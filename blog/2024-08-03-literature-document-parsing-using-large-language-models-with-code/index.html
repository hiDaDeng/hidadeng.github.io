<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>实验 | 使用本地大模型从论文PDF中提取结构化信息 | 大邓和他的PYTHON</title>
<meta name="keywords" content="Python, 大模型, ollama, 文献整理" />
<meta name="description" content="非结构文本、图片、视频等数据是待挖掘的数据矿藏， 在经管、社科等研究领域中谁拥有了从非结构提取结构化信息的能力，谁就拥有科研上的数据优势。正则表达式是一种强大的文档解析工具，但它们常常难以应对现实世界文档的复杂性和多变性。而随着chatGPT这类LLM的出现，为我们提供了更强大、更灵活的方法来处理多种类型的文档结构和内容类型。
 代码 | 使用本地大模型从文本中提取结构化信息 实验 | 使用本地大模型DIY制作单词书教案PDF  为方便理解和实验，今天再新增一个案例，即论文处理的场景为例

一、任务 从海量的论文pdf文件中批量提取出
 论文标题 出版年份 作者 联系作者 抽象的 摘要  1.1 为何选择LLM，而不是正则表达式 在灵活性、上下文理解能力、维护和可扩展性三方面， 我们对比一下LLM和正则表达式
   方面 LLM 正则表达式     灵活性 能够自动理解和适应各种文档结构，并且无论位于文档的什么位置，都能够识别相关信息。 需要每个文档结构都有特定的模式，当给定的文档偏离预期的格式时就会失败。   上下文理解 对每个文档的含义有细致的理解，从而可以更准确地提取相关信息。 无需理解上下文或含义即可匹配模式。   维护和可扩展性 可以轻松适应新的文档类型，只需在初始提示中进行最少的更改，从而使其更具可扩展性。 需要随着文档格式的变化而不断更新。添加对新类型信息的支持需要编写一个全新的正则表达式。    综上， 选择LLM更适合做「从论文PDF中提取信息」这一任务。
1.2 工作流程 为了方便实验，让我们以论文处理的场景为例，下图是使用LLM批量提取论文中元信息的工作流程。
工作流程总体上有三个主要组成部分：输入、处理和输出。
 首先，提交文件（在本例中为PDF格式的科研论文）进行处理。 处理组件的第一个模块从每个 PDF 中提取原始数据，并将其与包含大型语言模型指令的提示相结合，以有效地提取数据。 然后，大型语言模型使用提示来提取所有元数据。 对于每个PDF，最终结果以JSON格式保存，可用于进一步分析。  二、准备工作 2.1 安装ollama 点击前往网站 https://ollama.">
<meta name="author" content="大邓">
<link rel="canonical" href="/blog/2024-08-03-literature-document-parsing-using-large-language-models-with-code/" />
<meta name="baidu-site-verification" content="codeva-TJUe6nOmGr" />
<meta name="360-site-verification" content="6b9b733ec558a1bb12cee8aa82f2529e" />
<meta name="sogou-site-verification" content="dZHPIorOhK" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.89.4" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="实验 | 使用本地大模型从论文PDF中提取结构化信息" />
<meta property="og:description" content="非结构文本、图片、视频等数据是待挖掘的数据矿藏， 在经管、社科等研究领域中谁拥有了从非结构提取结构化信息的能力，谁就拥有科研上的数据优势。正则表达式是一种强大的文档解析工具，但它们常常难以应对现实世界文档的复杂性和多变性。而随着chatGPT这类LLM的出现，为我们提供了更强大、更灵活的方法来处理多种类型的文档结构和内容类型。
 代码 | 使用本地大模型从文本中提取结构化信息 实验 | 使用本地大模型DIY制作单词书教案PDF  为方便理解和实验，今天再新增一个案例，即论文处理的场景为例

一、任务 从海量的论文pdf文件中批量提取出
 论文标题 出版年份 作者 联系作者 抽象的 摘要  1.1 为何选择LLM，而不是正则表达式 在灵活性、上下文理解能力、维护和可扩展性三方面， 我们对比一下LLM和正则表达式
   方面 LLM 正则表达式     灵活性 能够自动理解和适应各种文档结构，并且无论位于文档的什么位置，都能够识别相关信息。 需要每个文档结构都有特定的模式，当给定的文档偏离预期的格式时就会失败。   上下文理解 对每个文档的含义有细致的理解，从而可以更准确地提取相关信息。 无需理解上下文或含义即可匹配模式。   维护和可扩展性 可以轻松适应新的文档类型，只需在初始提示中进行最少的更改，从而使其更具可扩展性。 需要随着文档格式的变化而不断更新。添加对新类型信息的支持需要编写一个全新的正则表达式。    综上， 选择LLM更适合做「从论文PDF中提取信息」这一任务。
1.2 工作流程 为了方便实验，让我们以论文处理的场景为例，下图是使用LLM批量提取论文中元信息的工作流程。
工作流程总体上有三个主要组成部分：输入、处理和输出。
 首先，提交文件（在本例中为PDF格式的科研论文）进行处理。 处理组件的第一个模块从每个 PDF 中提取原始数据，并将其与包含大型语言模型指令的提示相结合，以有效地提取数据。 然后，大型语言模型使用提示来提取所有元数据。 对于每个PDF，最终结果以JSON格式保存，可用于进一步分析。  二、准备工作 2.1 安装ollama 点击前往网站 https://ollama." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blog/2024-08-03-literature-document-parsing-using-large-language-models-with-code/" />
<meta property="og:image" content="/images/blog/document-parsing-using-llm.jpg" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2024-08-03T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2024-08-03T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="/images/blog/document-parsing-using-llm.jpg" />
<meta name="twitter:title" content="实验 | 使用本地大模型从论文PDF中提取结构化信息"/>
<meta name="twitter:description" content="非结构文本、图片、视频等数据是待挖掘的数据矿藏， 在经管、社科等研究领域中谁拥有了从非结构提取结构化信息的能力，谁就拥有科研上的数据优势。正则表达式是一种强大的文档解析工具，但它们常常难以应对现实世界文档的复杂性和多变性。而随着chatGPT这类LLM的出现，为我们提供了更强大、更灵活的方法来处理多种类型的文档结构和内容类型。
 代码 | 使用本地大模型从文本中提取结构化信息 实验 | 使用本地大模型DIY制作单词书教案PDF  为方便理解和实验，今天再新增一个案例，即论文处理的场景为例

一、任务 从海量的论文pdf文件中批量提取出
 论文标题 出版年份 作者 联系作者 抽象的 摘要  1.1 为何选择LLM，而不是正则表达式 在灵活性、上下文理解能力、维护和可扩展性三方面， 我们对比一下LLM和正则表达式
   方面 LLM 正则表达式     灵活性 能够自动理解和适应各种文档结构，并且无论位于文档的什么位置，都能够识别相关信息。 需要每个文档结构都有特定的模式，当给定的文档偏离预期的格式时就会失败。   上下文理解 对每个文档的含义有细致的理解，从而可以更准确地提取相关信息。 无需理解上下文或含义即可匹配模式。   维护和可扩展性 可以轻松适应新的文档类型，只需在初始提示中进行最少的更改，从而使其更具可扩展性。 需要随着文档格式的变化而不断更新。添加对新类型信息的支持需要编写一个全新的正则表达式。    综上， 选择LLM更适合做「从论文PDF中提取信息」这一任务。
1.2 工作流程 为了方便实验，让我们以论文处理的场景为例，下图是使用LLM批量提取论文中元信息的工作流程。
工作流程总体上有三个主要组成部分：输入、处理和输出。
 首先，提交文件（在本例中为PDF格式的科研论文）进行处理。 处理组件的第一个模块从每个 PDF 中提取原始数据，并将其与包含大型语言模型指令的提示相结合，以有效地提取数据。 然后，大型语言模型使用提示来提取所有元数据。 对于每个PDF，最终结果以JSON格式保存，可用于进一步分析。  二、准备工作 2.1 安装ollama 点击前往网站 https://ollama."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "/blog/"
    }
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "实验 | 使用本地大模型从论文PDF中提取结构化信息",
      "item": "/blog/2024-08-03-literature-document-parsing-using-large-language-models-with-code/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "实验 | 使用本地大模型从论文PDF中提取结构化信息",
  "name": "实验 | 使用本地大模型从论文PDF中提取结构化信息",
  "description": "非结构文本、图片、视频等数据是待挖掘的数据矿藏， 在经管、社科等研究领域中谁拥有了从非结构提取结构化信息的能力，谁就拥有科研上的数据优势。正则表达式是一种强大的文档解析工具，但它们常常难以应对现实世界文档的复杂性和多变性。而随着chatGPT这类LLM的出现，为我们提供了更强大、更灵活的方法来处理多种类型的文档结构和内容类型。\n 代码 | 使用本地大模型从文本中提取结构化信息 实验 | 使用本地大模型DIY制作单词书教案PDF  为方便理解和实验，今天再新增一个案例，即论文处理的场景为例\n\n一、任务 从海量的论文pdf文件中批量提取出\n 论文标题 出版年份 作者 联系作者 抽象的 摘要  1.1 为何选择LLM，而不是正则表达式 在灵活性、上下文理解能力、维护和可扩展性三方面， 我们对比一下LLM和正则表达式\n   方面 LLM 正则表达式     灵活性 能够自动理解和适应各种文档结构，并且无论位于文档的什么位置，都能够识别相关信息。 需要每个文档结构都有特定的模式，当给定的文档偏离预期的格式时就会失败。   上下文理解 对每个文档的含义有细致的理解，从而可以更准确地提取相关信息。 无需理解上下文或含义即可匹配模式。   维护和可扩展性 可以轻松适应新的文档类型，只需在初始提示中进行最少的更改，从而使其更具可扩展性。 需要随着文档格式的变化而不断更新。添加对新类型信息的支持需要编写一个全新的正则表达式。    综上， 选择LLM更适合做「从论文PDF中提取信息」这一任务。\n1.2 工作流程 为了方便实验，让我们以论文处理的场景为例，下图是使用LLM批量提取论文中元信息的工作流程。\n工作流程总体上有三个主要组成部分：输入、处理和输出。\n 首先，提交文件（在本例中为PDF格式的科研论文）进行处理。 处理组件的第一个模块从每个 PDF 中提取原始数据，并将其与包含大型语言模型指令的提示相结合，以有效地提取数据。 然后，大型语言模型使用提示来提取所有元数据。 对于每个PDF，最终结果以JSON格式保存，可用于进一步分析。  二、准备工作 2.1 安装ollama 点击前往网站 https://ollama.",
  "keywords": [
    "Python", "大模型", "ollama", "文献整理"
  ],
  "articleBody": "非结构文本、图片、视频等数据是待挖掘的数据矿藏， 在经管、社科等研究领域中谁拥有了从非结构提取结构化信息的能力，谁就拥有科研上的数据优势。正则表达式是一种强大的文档解析工具，但它们常常难以应对现实世界文档的复杂性和多变性。而随着chatGPT这类LLM的出现，为我们提供了更强大、更灵活的方法来处理多种类型的文档结构和内容类型。\n 代码 | 使用本地大模型从文本中提取结构化信息 实验 | 使用本地大模型DIY制作单词书教案PDF  为方便理解和实验，今天再新增一个案例，即论文处理的场景为例\n\n一、任务 从海量的论文pdf文件中批量提取出\n 论文标题 出版年份 作者 联系作者 抽象的 摘要  1.1 为何选择LLM，而不是正则表达式 在灵活性、上下文理解能力、维护和可扩展性三方面， 我们对比一下LLM和正则表达式\n   方面 LLM 正则表达式     灵活性 能够自动理解和适应各种文档结构，并且无论位于文档的什么位置，都能够识别相关信息。 需要每个文档结构都有特定的模式，当给定的文档偏离预期的格式时就会失败。   上下文理解 对每个文档的含义有细致的理解，从而可以更准确地提取相关信息。 无需理解上下文或含义即可匹配模式。   维护和可扩展性 可以轻松适应新的文档类型，只需在初始提示中进行最少的更改，从而使其更具可扩展性。 需要随着文档格式的变化而不断更新。添加对新类型信息的支持需要编写一个全新的正则表达式。    综上， 选择LLM更适合做「从论文PDF中提取信息」这一任务。\n1.2 工作流程 为了方便实验，让我们以论文处理的场景为例，下图是使用LLM批量提取论文中元信息的工作流程。\n工作流程总体上有三个主要组成部分：输入、处理和输出。\n 首先，提交文件（在本例中为PDF格式的科研论文）进行处理。 处理组件的第一个模块从每个 PDF 中提取原始数据，并将其与包含大型语言模型指令的提示相结合，以有效地提取数据。 然后，大型语言模型使用提示来提取所有元数据。 对于每个PDF，最终结果以JSON格式保存，可用于进一步分析。  二、准备工作 2.1 安装ollama 点击前往网站 https://ollama.com/ ，下载ollama软件，支持win、Mac、linux\n2.2 下载LLM ollama软件目前支持多种大模型， 如阿里的（qwen、qwen2）、meta的(llama3、llama3.1)， 本文选择最近新出的模型 llama3.1\n以llama3.1为例，根据自己电脑显存性能， 选择适宜的版本。如果不知道选什么，那就试着安装，不合适不能用再删除即可。\n打开电脑命令行cmd(mac是terminal), 网络是连网状态，执行模型下载(安装)命令\nollama pull llama3.1 等待 llama3.1:8b 下载完成。\n2.3 安装python包 在python中调用ollama服务，需要ollama包。\n打开电脑命令行cmd(mac是terminal), 网络是连网状态，执行安装命令\npip3 install ollama \n2.4 启动ollama服务 在Python中调用本地ollama服务，需要先启动本地ollama服务， 打开电脑命令行cmd(mac是terminal), 执行\nollama serve Run\n2024/08/03 14:52:24 routes.go:1011: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/Users/deng/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]\" time=2024-08-03T14:52:24.742+08:00 level=INFO source=images.go:725 msg=\"total blobs: 18\" time=2024-08-03T14:52:24.742+08:00 level=INFO source=images.go:732 msg=\"total unused blobs removed: 0\" time=2024-08-03T14:52:24.743+08:00 level=INFO source=routes.go:1057 msg=\"Listening on 127.0.0.1:11434 (version 0.1.44)\" time=2024-08-03T14:52:24.744+08:00 level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/ollama4239159529/runners time=2024-08-03T14:52:24.772+08:00 level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [metal]\" time=2024-08-03T14:52:24.796+08:00 level=INFO source=types.go:71 msg=\"inference compute\" id=0 library=metal compute=\"\" driver=0.0 name=\"\" total=\"72.0 GiB\" available=\"72.0 GiB\" cmd(mac是terminal)看到如上的信息，说明本地ollama服务已开启。\n三、实验 3.1 代码结构 点击下载本文 实验代码\nproject | - Extract_Metadata_With_Large_Language_Models.ipynb - prompts |--- scientific_papers_prompt.txt - data |--- 1706.03762v7.pdf |--- 2301.09056v1.pdf - extracted_metadata/ \n project文件夹 是根文件夹，包含 ipynb代码文件、 prompts文件夹、data文件夹、extracted_metadata文件夹 prompts文件夹 有txt文件格式的提示信息 data文件夹 存储着实验论文pdf数据 extracted_metadata文件夹 目前为空，将存储从论文pdf中提取的元信息，以 json 文件格式存储  3.2 提示工程 我们需要从论文pdf中提取\n 论文标题 出版年份 作者 联系作者 抽象的 摘要  这是我设计的提示， 该提示存储在 prompts/scientific_papers_prompt.txt 中。\n科学研究论文： --- {document} --- 您是分析科学研究论文的专家。 请仔细阅读上面提供的研究论文，并提取以下关键信息： 从研究论文中提取以下六 (6) 个属性： - 论文标题：研究论文的全名 - 出版年份：论文发表的年份 - 作者：论文所有作者的全名 - 作者联系方式：字典列表，其中每个字典包含每个作者的以下键： - 姓名：作者的全名 - 机构：作者的机构隶属关系 - 电子邮件：作者的电子邮件地址（如果提供） - 摘要：论文摘要的全文 - 摘要总结：用 2-3 句话简洁地总结摘要，突出重点 指南： - 提取的信息应属实，并准确无误。 - 除摘要外，应极其简洁，摘要应完整复制。 - 提取的实体应该是独立的，并且不需要论文的其余部分就能轻松理解。 - 如果论文中缺少任何属性，请将该字段留空，而不是猜测。 - 对于摘要总结，重点介绍研究的主要目标、方法和主要发现。 - 对于作者联系方式，请为每个作者创建一个条目，即使缺少一些信息。如果没有提供作者的电子邮件或机构，请在字典中将该字段留空。 以 JSON 格式回答。 JSON 应包含 6 个键：\"PaperTitle\", \"PublicationYear\", \"Authors\", \"AuthorContact\", \"Abstract\", \"SummaryAbstract\"。 \"AuthorContact\"字段应该是字典列表格式。 \n3.2 提取信息 读取 data/1706.03762v7.pdf， 提取该论文首页中感兴趣的6个信息，如\n%%time import ollama import cntext as ct #cntext版本为2.1.2，非开源， #需联系大邓372335839获取 #我们感兴趣的信息在论文的第一页，所以这里粗糙的选择前4000个字符。 paper_content = ct.read_pdf('data/1706.03762v7.pdf')[:4000] prompt_content = open('prompts/scientific_papers_prompt.txt', encoding='utf-8').read() response = ollama.chat(model='llama3.1:8b', messages = [ {'role': 'system', 'content': prompt_content}, {'role': 'user', 'content': paper_content} ]) result = response['message']['content'] result = eval(result.split('```\\n')[1].split('\\n```')[0]) result Run\nCPU times: user 3.5 ms, sys: 2.13 ms, total: 5.63 ms Wall time: 11.8 s {'PaperTitle': 'Attention Is All You Need', 'PublicationYear': 2017, 'Authors': ['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Łukasz Kaiser', 'Illia Polosukhin'], 'AuthorContact': [{'Name': 'Ashish Vaswani', 'Institution': 'Google Brain', 'Email': 'avaswani@google.com'}, {'Name': 'Noam Shazeer', 'Institution': 'Google Brain', 'Email': 'noam@google.com'}, {'Name': 'Niki Parmar', 'Institution': 'Google Research', 'Email': 'nikip@google.com'}, {'Name': 'Jakob Uszkoreit', 'Institution': 'Google Research', 'Email': 'usz@google.com'}, {'Name': 'Llion Jones', 'Institution': 'Google Research', 'Email': 'llion@google.com'}, {'Name': 'Aidan N. Gomez', 'Institution': 'University of Toronto', 'Email': 'aidan@cs.toronto.edu'}, {'Name': 'Łukasz Kaiser', 'Institution': 'Google Brain', 'Email': 'lukaszkaiser@google.com'}, {'Name': 'Illia Polosukhin', 'Institution': '', 'Email': 'illia.polosukhin@gmail.com'}], 'Abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.', 'SummaryAbstract': '本文提出了一种新的Transformer模型，基于注意力机制，抛弃了递归和卷积等复杂方法。该模型在机器翻译任务上表现出优异的效果，并且可以更好地并行化和训练。'} 从运行结果看， 摘要Abstract 的提取不够准确，有一定的遗漏。\n3.3 封装成函数extract_info 实验成功，我们将其封装为函数extract_info ，因为LLM返回的内容的格式存在不确定性， 所以为了保证函数尽可能的成功的运行出结果，这里我设置了异常处理机制。\nimport ollama import cntext as ct #cntext版本为2.1.2，非开源， #需联系大邓372335839获取 def extract_info(paper_content, prompt_content, max_retries=3): for attempt in range(max_retries + 1): try: response = ollama.chat( model='llama3.1:8b', messages=[ {'role': 'system', 'content': prompt_content}, {'role': 'user', 'content': paper_content} ] ) result = response['message']['content'] result = eval(result.split('```\\n')[1].split('\\n```')[0]) return result except Exception as e: if attempt  max_retries: print(f\"An error occurred: {e}. Retrying ({attempt + 1}/{max_retries + 1})...\") else: raise e #我们感兴趣的信息在论文的第一页，所以这里粗糙的选择前4000个字符。 paper_content = ct.read_pdf('data/1706.03762v7.pdf')[:4000] prompt_content = open('prompts/scientific_papers_prompt.txt', encoding='utf-8').read() result = extract_info(paper_content, prompt_content) result 运行结果与之前无异，为节约板面空间，这里就不展示result了。\n3.4 批量提取 假设data文件夹内有成百上千的发票(实际上只有一张发票)， 对data文件夹进行批量信息提取，结果存储为csv。\n%%time import os #cntext版本为2.1.3，非开源，需联系大邓372335839获取 import cntext as ct import pandas as pd import jsonlines #当前代码所在的代码文件与data文件夹处于同一个文件夹内 #获取data内所有pdf的路径 pdf_files = [f'data/{file}' for file in os.listdir('data') if '.pdf' in file] prompt_content = open('prompts/scientific_papers_prompt.txt', encoding='utf-8').read() for pdf_file in pdf_files: paper_content = ct.read_pdf(pdf_file)[:4000] dict_data = extract_info(paper_content, prompt_content) jsonf = pdf_file.replace('data', 'extracted_metadata').replace('pdf', 'jsonl') with jsonlines.open(jsonf, 'w') as jf: jf.write(dict_data) Run\nCPU times: user 919 ms, sys: 14.8 ms, total: 933 ms Wall time: 24.6 s \n四、讨论 本文简要概述了 LLM 在从复杂文档中提取元数据方面的应用，提取的 json 数据可以存储在非关系数据库中以供进一步分析。\nLLM 和 Regex 在内容提取方面各有优缺点，应根据用例明智地应用每种方法。希望本简短教程能帮助您获得新技能。\n精选内容  LIST | 可供社科(经管)领域使用的数据集汇总 LIST | 社科(经管)数据挖掘文献资料汇总 网络爬虫 | 使用scrapegraph-ai(大模型方案)自动采集网页数据 推荐 | 文本分析库cntext2.x使用手册 付费视频课 | Python实证指标构建与文本分析  ",
  "wordCount" : "664",
  "inLanguage": "en",
  "image":"/images/blog/document-parsing-using-llm.jpg","datePublished": "2024-08-03T00:00:00Z",
  "dateModified": "2024-08-03T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "大邓"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/blog/2024-08-03-literature-document-parsing-using-large-language-models-with-code/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "大邓和他的PYTHON",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon.ico"
    }
  }
}
</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SFGQCREQ9X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SFGQCREQ9X');
</script>



<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=PT+Serif" rel="stylesheet">
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="/" accesskey="h" title="大邓和他的PYTHON (Alt + H)" target="_blank">大邓和他的PYTHON</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="/about/" title="关于" target="_blank">
                    <span>关于</span>
                </a>
            </li>
            <li>
                <a href="/blog" title="博文" target="_blank">
                    <span>博文</span>
                </a>
            </li>
            <li>
                <a href="/search/" title="搜索" target="_blank">
                    <span>搜索</span>
                </a>
            </li>
            <li>
                <a href="/tags/" title="标签" target="_blank">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="/blog/management_python_course/" title="课程" target="_blank">
                    <span>课程</span>
                </a>
            </li>
            <li>
                <a href="/blog/the_text_analysis_list_about_ms/" title="文献" target="_blank">
                    <span>文献</span>
                </a>
            </li>
            <li>
                <a href="/blog/datasets_available_for_management_science/" title="数据" target="_blank">
                    <span>数据</span>
                </a>
            </li>
            <li>
                <a href="/blog/2024-04-27-cntext2x-usage-tutorial/" title="cntext2.x" target="_blank">
                    <span>cntext2.x</span>
                </a>
            </li>
            <li>
                <a href="/index.xml" title="RSS" target="_blank">
                    <span>RSS</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/hiDaDeng/hidadeng.github.io/discussions/" title="留言" target="_blank">
                    <span>留言</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
onload="renderMathInElement(document.body);"></script>




<article class="post-single">
  <header class="post-header">
    
    <div class="breadcrumbs"><a href="/" target="_blank">Home</a>&nbsp;»&nbsp;<a href="/blog/" target="_blank">Blogs</a></div>
    <h1 class="post-title">
      实验 | 使用本地大模型从论文PDF中提取结构化信息
    </h1>
    <div class="post-meta"><span title='2024-08-03 00:00:00 +0000 UTC'>2024-08-03</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;大邓

</div>
  </header> 
<figure class="entry-cover"><a href="/images/blog/document-parsing-using-llm.jpg" target="_blank"
            rel="noopener noreferrer"><img loading="lazy" src="/images/blog/document-parsing-using-llm.jpg" alt=""></a>
        
</figure>
<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on twitter"
        href="https://twitter.com/intent/tweet/?text=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af&amp;url=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f&amp;hashtags=LLM%2c%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f&amp;title=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af&amp;summary=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af&amp;source=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on reddit"
        href="https://reddit.com/submit?url=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f&title=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on facebook"
        href="https://facebook.com/sharer/sharer.php?u=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on whatsapp"
        href="https://api.whatsapp.com/send?text=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af%20-%20%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on telegram"
        href="https://telegram.me/share/url?text=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af&amp;url=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
    
</div>
<aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e4%b8%80%e4%bb%bb%e5%8a%a1" aria-label="一、任务">一、任务</a><ul>
                            
                    <li>
                        <a href="#11-%e4%b8%ba%e4%bd%95%e9%80%89%e6%8b%a9llm%e8%80%8c%e4%b8%8d%e6%98%af%e6%ad%a3%e5%88%99%e8%a1%a8%e8%be%be%e5%bc%8f" aria-label="1.1 为何选择LLM，而不是正则表达式">1.1 为何选择LLM，而不是正则表达式</a></li>
                    <li>
                        <a href="#12-%e5%b7%a5%e4%bd%9c%e6%b5%81%e7%a8%8b" aria-label="1.2 工作流程">1.2 工作流程</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e4%ba%8c%e5%87%86%e5%a4%87%e5%b7%a5%e4%bd%9c" aria-label="二、准备工作">二、准备工作</a><ul>
                            
                    <li>
                        <a href="#21-%e5%ae%89%e8%a3%85ollama" aria-label="2.1 安装ollama">2.1 安装ollama</a></li>
                    <li>
                        <a href="#22-%e4%b8%8b%e8%bd%bdllm" aria-label="2.2 下载LLM">2.2 下载LLM</a></li>
                    <li>
                        <a href="#23-%e5%ae%89%e8%a3%85python%e5%8c%85" aria-label="2.3 安装python包">2.3 安装python包</a></li>
                    <li>
                        <a href="#24-%e5%90%af%e5%8a%a8ollama%e6%9c%8d%e5%8a%a1" aria-label="2.4 启动ollama服务">2.4 启动ollama服务</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e4%b8%89%e5%ae%9e%e9%aa%8c" aria-label="三、实验">三、实验</a><ul>
                            
                    <li>
                        <a href="#31-%e4%bb%a3%e7%a0%81%e7%bb%93%e6%9e%84" aria-label="3.1 代码结构">3.1 代码结构</a></li>
                    <li>
                        <a href="#32-%e6%8f%90%e7%a4%ba%e5%b7%a5%e7%a8%8b" aria-label="3.2 提示工程">3.2 提示工程</a></li>
                    <li>
                        <a href="#32-%e6%8f%90%e5%8f%96%e4%bf%a1%e6%81%af" aria-label="3.2 提取信息">3.2 提取信息</a></li>
                    <li>
                        <a href="#33-%e5%b0%81%e8%a3%85%e6%88%90%e5%87%bd%e6%95%b0extract_info" aria-label="3.3 封装成函数extract_info">3.3 封装成函数extract_info</a></li>
                    <li>
                        <a href="#34-%e6%89%b9%e9%87%8f%e6%8f%90%e5%8f%96" aria-label="3.4 批量提取">3.4 批量提取</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e5%9b%9b%e8%ae%a8%e8%ae%ba" aria-label="四、讨论">四、讨论</a></li>
                    <li>
                        <a href="#%e7%b2%be%e9%80%89%e5%86%85%e5%ae%b9" aria-label="精选内容">精选内容</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><p>非结构文本、图片、视频等数据是待挖掘的数据矿藏， 在经管、社科等研究领域中谁拥有了<em><strong>从非结构提取结构化信息的能力</strong></em>，谁就拥有科研上的数据优势。正则表达式是一种强大的文档解析工具，但它们常常难以应对现实世界文档的复杂性和多变性。而随着chatGPT这类LLM的出现，为我们提供了更强大、更灵活的方法来处理多种类型的文档结构和内容类型。</p>
<ul>
<li><a href="https://textdata.cn/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/">代码 | 使用本地大模型从文本中提取结构化信息</a></li>
<li><a href="https://textdata.cn/blog/2024-07-10-using-large-language-model-to-build-diy-dictionary/">实验 | 使用本地大模型DIY制作单词书教案PDF</a></li>
</ul>
<p>为方便理解和实验，今天再新增一个案例，即论文处理的场景为例</p>
<p><br><br></p>
<h2 id="一任务">一、任务<a hidden class="anchor" aria-hidden="true" href="#一任务">#</a></h2>
<p>从海量的论文pdf文件中批量提取出</p>
<ul>
<li>论文标题</li>
<li>出版年份</li>
<li>作者</li>
<li>联系作者</li>
<li>抽象的</li>
<li>摘要</li>
</ul>
<br>
<h3 id="11-为何选择llm而不是正则表达式">1.1 为何选择LLM，而不是正则表达式<a hidden class="anchor" aria-hidden="true" href="#11-为何选择llm而不是正则表达式">#</a></h3>
<p>在灵活性、上下文理解能力、维护和可扩展性三方面， 我们对比一下LLM和正则表达式</p>
<table>
<thead>
<tr>
<th>方面</th>
<th>LLM</th>
<th>正则表达式</th>
</tr>
</thead>
<tbody>
<tr>
<td>灵活性</td>
<td>能够自动理解和适应各种文档结构，并且无论位于文档的什么位置，都能够识别相关信息。</td>
<td>需要每个文档结构都有特定的模式，当给定的文档偏离预期的格式时就会失败。</td>
</tr>
<tr>
<td>上下文理解</td>
<td>对每个文档的含义有细致的理解，从而可以更准确地提取相关信息。</td>
<td>无需理解上下文或含义即可匹配模式。</td>
</tr>
<tr>
<td>维护和可扩展性</td>
<td>可以轻松适应新的文档类型，只需在初始提示中进行最少的更改，从而使其更具可扩展性。</td>
<td>需要随着文档格式的变化而不断更新。添加对新类型信息的支持需要编写一个全新的正则表达式。</td>
</tr>
</tbody>
</table>
<p>综上， 选择LLM更适合做「从论文PDF中提取信息」这一任务。</p>
<br>
<h3 id="12-工作流程">1.2 工作流程<a hidden class="anchor" aria-hidden="true" href="#12-工作流程">#</a></h3>
<p>为了方便实验，让我们以论文处理的场景为例，下图是使用LLM批量提取论文中元信息的工作流程。</p>
<p><img loading="lazy" src="img/00-document-parsing.png" alt=""  />
</p>
<p>工作流程总体上有三个主要组成部分：输入、处理和输出。</p>
<ul>
<li>首先，提交文件（在本例中为PDF格式的科研论文）进行处理。</li>
<li>处理组件的第一个模块从每个 PDF 中提取原始数据，并将其与包含大型语言模型指令的提示相结合，以有效地提取数据。</li>
<li>然后，大型语言模型使用提示来提取所有元数据。</li>
<li>对于每个PDF，最终结果以JSON格式保存，可用于进一步分析。</li>
</ul>
<br>
<br>
<h2 id="二准备工作">二、准备工作<a hidden class="anchor" aria-hidden="true" href="#二准备工作">#</a></h2>
<h3 id="21-安装ollama">2.1 安装ollama<a hidden class="anchor" aria-hidden="true" href="#21-安装ollama">#</a></h3>
<p>点击前往网站 <a href="https://ollama.com/">https://ollama.com/</a> ，下载ollama软件，支持win、Mac、linux</p>
<p><img loading="lazy" src="img/02-ollama-gui.png" alt=""  />
</p>
<br>
<h3 id="22-下载llm">2.2 下载LLM<a hidden class="anchor" aria-hidden="true" href="#22-下载llm">#</a></h3>
<p>ollama软件目前支持多种大模型， 如阿里的（qwen、qwen2）、meta的(llama3、llama3.1)，  本文选择最近新出的模型 llama3.1</p>
<p><img loading="lazy" src="img/03-ollama-model.png" alt=""  />
</p>
<br>
<p>以llama3.1为例，根据自己电脑显存性能， 选择适宜的版本。如果不知道选什么，那就试着安装，不合适不能用再删除即可。</p>
<p><img loading="lazy" src="img/04-ollama-llama3.png" alt=""  />
</p>
<br>
<p>打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行模型下载(安装)命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ollama pull llama3.1
</code></pre></div><p>等待 <strong>llama3.1:8b</strong> 下载完成。</p>
<br>
<h3 id="23-安装python包">2.3 安装python包<a hidden class="anchor" aria-hidden="true" href="#23-安装python包">#</a></h3>
<p>在python中调用ollama服务，需要ollama包。</p>
<p>打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行安装命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install ollama
</code></pre></div><br>
<h3 id="24-启动ollama服务">2.4 启动ollama服务<a hidden class="anchor" aria-hidden="true" href="#24-启动ollama服务">#</a></h3>
<p>在Python中调用本地ollama服务，需要先启动本地ollama服务， 打开电脑命令行cmd(mac是terminal), 执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ollama serve
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2024/08/03 14:52:24 routes.go:1011: INFO server config env=&#34;map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/Users/deng/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]&#34;
time=2024-08-03T14:52:24.742+08:00 level=INFO source=images.go:725 msg=&#34;total blobs: 18&#34;
time=2024-08-03T14:52:24.742+08:00 level=INFO source=images.go:732 msg=&#34;total unused blobs removed: 0&#34;
time=2024-08-03T14:52:24.743+08:00 level=INFO source=routes.go:1057 msg=&#34;Listening on 127.0.0.1:11434 (version 0.1.44)&#34;
time=2024-08-03T14:52:24.744+08:00 level=INFO source=payload.go:30 msg=&#34;extracting embedded files&#34; dir=/var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/ollama4239159529/runners
time=2024-08-03T14:52:24.772+08:00 level=INFO source=payload.go:44 msg=&#34;Dynamic LLM libraries [metal]&#34;
time=2024-08-03T14:52:24.796+08:00 level=INFO source=types.go:71 msg=&#34;inference compute&#34; id=0 library=metal compute=&#34;&#34; driver=0.0 name=&#34;&#34; total=&#34;72.0 GiB&#34; available=&#34;72.0 GiB&#34;
</code></pre></div><p>cmd(mac是terminal)看到如上的信息，说明本地ollama服务已开启。</p>
<br>
<br>
<h2 id="三实验">三、实验<a hidden class="anchor" aria-hidden="true" href="#三实验">#</a></h2>
<h3 id="31-代码结构">3.1 代码结构<a hidden class="anchor" aria-hidden="true" href="#31-代码结构">#</a></h3>
<p>点击下载本文 <a href="project.zip">实验代码</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">project
   |
  - Extract_Metadata_With_Large_Language_Models.ipynb
  - prompts
       |--- scientific_papers_prompt.txt
  - data
      |--- 1706.03762v7.pdf
      |--- 2301.09056v1.pdf
  - extracted_metadata/
</code></pre></div><br>
<ul>
<li><em><strong>project文件夹</strong></em> 是根文件夹，包含 <em><strong>ipynb代码文件</strong></em>、 <em><strong>prompts文件夹</strong></em>、<em><strong>data文件夹</strong></em>、<em><strong>extracted_metadata文件夹</strong></em></li>
<li><em><strong>prompts文件夹</strong></em> 有txt文件格式的提示信息</li>
<li><em><strong>data文件夹</strong></em> 存储着实验论文pdf数据</li>
<li><em><strong>extracted_metadata文件夹</strong></em> 目前为空，将存储从论文pdf中提取的元信息，以 json 文件格式存储</li>
</ul>
<br>
<h3 id="32-提示工程">3.2 提示工程<a hidden class="anchor" aria-hidden="true" href="#32-提示工程">#</a></h3>
<p>我们需要从论文pdf中提取</p>
<ul>
<li>论文标题</li>
<li>出版年份</li>
<li>作者</li>
<li>联系作者</li>
<li>抽象的</li>
<li>摘要</li>
</ul>
<p>这是我设计的提示， 该提示存储在 <em><strong>prompts/scientific_papers_prompt.txt</strong></em> 中。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">科学研究论文</span><span class="err">：</span>
<span class="o">---</span> 
<span class="p">{</span><span class="n">document</span><span class="p">}</span> 
<span class="o">---</span>

<span class="n">您是分析科学研究论文的专家</span><span class="err">。</span> <span class="n">请仔细阅读上面提供的研究论文</span><span class="err">，</span><span class="n">并提取以下关键信息</span><span class="err">：</span>

<span class="n">从研究论文中提取以下六</span> <span class="p">(</span><span class="mi">6</span><span class="p">)</span> <span class="n">个属性</span><span class="err">：</span>
<span class="o">-</span> <span class="n">论文标题</span><span class="err">：</span><span class="n">研究论文的全名</span>
<span class="o">-</span> <span class="n">出版年份</span><span class="err">：</span><span class="n">论文发表的年份</span>
<span class="o">-</span> <span class="n">作者</span><span class="err">：</span><span class="n">论文所有作者的全名</span>
<span class="o">-</span> <span class="n">作者联系方式</span><span class="err">：</span><span class="n">字典列表</span><span class="err">，</span><span class="n">其中每个字典包含每个作者的以下键</span><span class="err">：</span>
  <span class="o">-</span> <span class="n">姓名</span><span class="err">：</span><span class="n">作者的全名</span>
  <span class="o">-</span> <span class="n">机构</span><span class="err">：</span><span class="n">作者的机构隶属关系</span>
  <span class="o">-</span> <span class="n">电子邮件</span><span class="err">：</span><span class="n">作者的电子邮件地址</span><span class="err">（</span><span class="n">如果提供</span><span class="err">）</span>
<span class="o">-</span> <span class="n">摘要</span><span class="err">：</span><span class="n">论文摘要的全文</span>
<span class="o">-</span> <span class="n">摘要总结</span><span class="err">：</span><span class="n">用</span> <span class="mi">2</span><span class="o">-</span><span class="mi">3</span> <span class="n">句话简洁地总结摘要</span><span class="err">，</span><span class="n">突出重点</span>

<span class="n">指南</span><span class="err">：</span>
<span class="o">-</span> <span class="n">提取的信息应属实</span><span class="err">，</span><span class="n">并准确无误</span><span class="err">。</span>
<span class="o">-</span> <span class="n">除摘要外</span><span class="err">，</span><span class="n">应极其简洁</span><span class="err">，</span><span class="n">摘要应完整复制</span><span class="err">。</span>
<span class="o">-</span> <span class="n">提取的实体应该是独立的</span><span class="err">，</span><span class="n">并且不需要论文的其余部分就能轻松理解</span><span class="err">。</span>
<span class="o">-</span> <span class="n">如果论文中缺少任何属性</span><span class="err">，</span><span class="n">请将该字段留空</span><span class="err">，</span><span class="n">而不是猜测</span><span class="err">。</span>
<span class="o">-</span> <span class="n">对于摘要总结</span><span class="err">，</span><span class="n">重点介绍研究的主要目标</span><span class="err">、</span><span class="n">方法和主要发现</span><span class="err">。</span>
<span class="o">-</span> <span class="n">对于作者联系方式</span><span class="err">，</span><span class="n">请为每个作者创建一个条目</span><span class="err">，</span><span class="n">即使缺少一些信息</span><span class="err">。</span><span class="n">如果没有提供作者的电子邮件或机构</span><span class="err">，</span><span class="n">请在字典中将该字段留空</span><span class="err">。</span>

<span class="n">以</span> <span class="n">JSON</span> <span class="n">格式回答</span><span class="err">。</span> <span class="n">JSON</span> <span class="n">应包含</span> <span class="mi">6</span> <span class="n">个键</span><span class="err">：</span><span class="s2">&#34;PaperTitle&#34;</span><span class="p">,</span> <span class="s2">&#34;PublicationYear&#34;</span><span class="p">,</span> <span class="s2">&#34;Authors&#34;</span><span class="p">,</span> <span class="s2">&#34;AuthorContact&#34;</span><span class="p">,</span> <span class="s2">&#34;Abstract&#34;</span><span class="p">,</span> <span class="s2">&#34;SummaryAbstract&#34;</span><span class="err">。</span> <span class="s2">&#34;AuthorContact&#34;</span><span class="n">字段应该是字典列表格式</span><span class="err">。</span>
</code></pre></div><br>
<h3 id="32-提取信息">3.2 提取信息<a hidden class="anchor" aria-hidden="true" href="#32-提取信息">#</a></h3>
<p>读取 <em><strong>data/1706.03762v7.pdf</strong></em>， 提取该论文首页中感兴趣的6个信息，如</p>
<p><img loading="lazy" src="img/6-paper.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>

<span class="kn">import</span> <span class="nn">ollama</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>  
<span class="c1">#cntext版本为2.1.2，非开源， #需联系大邓372335839获取</span>

<span class="c1">#我们感兴趣的信息在论文的第一页，所以这里粗糙的选择前4000个字符。</span>
<span class="n">paper_content</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;data/1706.03762v7.pdf&#39;</span><span class="p">)[:</span><span class="mi">4000</span><span class="p">]</span>
<span class="n">prompt_content</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;prompts/scientific_papers_prompt.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;llama3.1:8b&#39;</span><span class="p">,</span> 
                       <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
                           <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">prompt_content</span><span class="p">},</span>
                           <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">paper_content</span><span class="p">}</span>
                       <span class="p">])</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
<span class="n">result</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;```</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">```&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">result</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">CPU times: user 3.5 ms, sys: 2.13 ms, total: 5.63 ms
Wall time: 11.8 s


{&#39;PaperTitle&#39;: &#39;Attention Is All You Need&#39;,
 &#39;PublicationYear&#39;: 2017,
 &#39;Authors&#39;: [&#39;Ashish Vaswani&#39;,
  &#39;Noam Shazeer&#39;,
  &#39;Niki Parmar&#39;,
  &#39;Jakob Uszkoreit&#39;,
  &#39;Llion Jones&#39;,
  &#39;Aidan N. Gomez&#39;,
  &#39;Łukasz Kaiser&#39;,
  &#39;Illia Polosukhin&#39;],
 &#39;AuthorContact&#39;: [{&#39;Name&#39;: &#39;Ashish Vaswani&#39;,
   &#39;Institution&#39;: &#39;Google Brain&#39;,
   &#39;Email&#39;: &#39;avaswani@google.com&#39;},
  {&#39;Name&#39;: &#39;Noam Shazeer&#39;,
   &#39;Institution&#39;: &#39;Google Brain&#39;,
   &#39;Email&#39;: &#39;noam@google.com&#39;},
  {&#39;Name&#39;: &#39;Niki Parmar&#39;,
   &#39;Institution&#39;: &#39;Google Research&#39;,
   &#39;Email&#39;: &#39;nikip@google.com&#39;},
  {&#39;Name&#39;: &#39;Jakob Uszkoreit&#39;,
   &#39;Institution&#39;: &#39;Google Research&#39;,
   &#39;Email&#39;: &#39;usz@google.com&#39;},
  {&#39;Name&#39;: &#39;Llion Jones&#39;,
   &#39;Institution&#39;: &#39;Google Research&#39;,
   &#39;Email&#39;: &#39;llion@google.com&#39;},
  {&#39;Name&#39;: &#39;Aidan N. Gomez&#39;,
   &#39;Institution&#39;: &#39;University of Toronto&#39;,
   &#39;Email&#39;: &#39;aidan@cs.toronto.edu&#39;},
  {&#39;Name&#39;: &#39;Łukasz Kaiser&#39;,
   &#39;Institution&#39;: &#39;Google Brain&#39;,
   &#39;Email&#39;: &#39;lukaszkaiser@google.com&#39;},
  {&#39;Name&#39;: &#39;Illia Polosukhin&#39;,
   &#39;Institution&#39;: &#39;&#39;,
   &#39;Email&#39;: &#39;illia.polosukhin@gmail.com&#39;}],
 &#39;Abstract&#39;: &#39;The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.&#39;,
 &#39;SummaryAbstract&#39;: &#39;本文提出了一种新的Transformer模型，基于注意力机制，抛弃了递归和卷积等复杂方法。该模型在机器翻译任务上表现出优异的效果，并且可以更好地并行化和训练。&#39;}
</code></pre></div><p>从运行结果看， 摘要<em><strong>Abstract</strong></em> 的提取不够准确，有一定的遗漏。</p>
<br>
<h3 id="33-封装成函数extract_info">3.3 封装成函数extract_info<a hidden class="anchor" aria-hidden="true" href="#33-封装成函数extract_info">#</a></h3>
<p>实验成功，我们将其封装为函数<em><strong>extract_info</strong></em> ，因为LLM返回的内容的格式存在不确定性， 所以为了保证函数尽可能的成功的运行出结果，这里我设置了异常处理机制。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">ollama</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>  
<span class="c1">#cntext版本为2.1.2，非开源， #需联系大邓372335839获取</span>


<span class="k">def</span> <span class="nf">extract_info</span><span class="p">(</span><span class="n">paper_content</span><span class="p">,</span> <span class="n">prompt_content</span><span class="p">,</span> <span class="n">max_retries</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_retries</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="s1">&#39;llama3.1:8b&#39;</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
                    <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">prompt_content</span><span class="p">},</span>
                    <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">paper_content</span><span class="p">}</span>
                <span class="p">]</span>
            <span class="p">)</span>

            <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
            <span class="n">result</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;```</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">```&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">result</span>
        
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">attempt</span> <span class="o">&lt;</span> <span class="n">max_retries</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;An error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. Retrying (</span><span class="si">{</span><span class="n">attempt</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">max_retries</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">)...&#34;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">e</span>


<span class="c1">#我们感兴趣的信息在论文的第一页，所以这里粗糙的选择前4000个字符。</span>
<span class="n">paper_content</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;data/1706.03762v7.pdf&#39;</span><span class="p">)[:</span><span class="mi">4000</span><span class="p">]</span>
<span class="n">prompt_content</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;prompts/scientific_papers_prompt.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">extract_info</span><span class="p">(</span><span class="n">paper_content</span><span class="p">,</span> <span class="n">prompt_content</span><span class="p">)</span>
<span class="n">result</span>
</code></pre></div><p>运行结果与之前无异，为节约板面空间，这里就不展示result了。</p>
<br>
<h3 id="34-批量提取">3.4 批量提取<a hidden class="anchor" aria-hidden="true" href="#34-批量提取">#</a></h3>
<p>假设data文件夹内有成百上千的发票(实际上只有一张发票)， 对data文件夹进行批量信息提取，结果存储为csv。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="c1">#cntext版本为2.1.3，非开源，需联系大邓372335839获取</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">jsonlines</span>

<span class="c1">#当前代码所在的代码文件与data文件夹处于同一个文件夹内</span>
<span class="c1">#获取data内所有pdf的路径</span>
<span class="n">pdf_files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;data/</span><span class="si">{</span><span class="n">file</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.pdf&#39;</span> <span class="ow">in</span> <span class="n">file</span><span class="p">]</span>
<span class="n">prompt_content</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;prompts/scientific_papers_prompt.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="k">for</span> <span class="n">pdf_file</span> <span class="ow">in</span> <span class="n">pdf_files</span><span class="p">:</span>
    <span class="n">paper_content</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="n">pdf_file</span><span class="p">)[:</span><span class="mi">4000</span><span class="p">]</span>
    <span class="n">dict_data</span> <span class="o">=</span> <span class="n">extract_info</span><span class="p">(</span><span class="n">paper_content</span><span class="p">,</span> <span class="n">prompt_content</span><span class="p">)</span>
    <span class="n">jsonf</span> <span class="o">=</span> <span class="n">pdf_file</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;extracted_metadata&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;pdf&#39;</span><span class="p">,</span> <span class="s1">&#39;jsonl&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">jsonlines</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">jsonf</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">jf</span><span class="p">:</span>
        <span class="n">jf</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">dict_data</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">CPU times: user 919 ms, sys: 14.8 ms, total: 933 ms
Wall time: 24.6 s
</code></pre></div><p><img loading="lazy" src="img/05-2result-json.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四讨论">四、讨论<a hidden class="anchor" aria-hidden="true" href="#四讨论">#</a></h2>
<p>本文简要概述了 LLM 在从复杂文档中提取元数据方面的应用，提取的 json 数据可以存储在非关系数据库中以供进一步分析。</p>
<p>LLM 和 Regex 在内容提取方面各有优缺点，应根据用例明智地应用每种方法。希望本简短教程能帮助您获得新技能。</p>
<br>
<br>
<h2 id="精选内容">精选内容<a hidden class="anchor" aria-hidden="true" href="#精选内容">#</a></h2>
<ul>
<li><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集汇总</a></li>
<li><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)数据挖掘文献资料汇总</a></li>
<li><a href="https://textdata.cn/blog/2024-06-16-scrapegraph-ai/">网络爬虫 | 使用scrapegraph-ai(大模型方案)自动采集网页数据</a></li>
<li><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">推荐 | 文本分析库cntext2.x使用手册</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>


  </div>

  <footer class="post-footer">
      <ul class="post-tags">
        <b>Tags:  &nbsp;</b>
        <li><a href="/tags/llm/" target='_blank'>LLM</a></li>
        <li><a href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/" target='_blank'>文本分析</a></li>
      </ul>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on twitter"
        href="https://twitter.com/intent/tweet/?text=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af&amp;url=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f&amp;hashtags=LLM%2c%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f&amp;title=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af&amp;summary=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af&amp;source=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on reddit"
        href="https://reddit.com/submit?url=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f&title=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on facebook"
        href="https://facebook.com/sharer/sharer.php?u=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on whatsapp"
        href="https://api.whatsapp.com/send?text=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af%20-%20%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 实验 | 使用本地大模型从论文PDF中提取结构化信息 on telegram"
        href="https://telegram.me/share/url?text=%e5%ae%9e%e9%aa%8c%20%7c%20%e4%bd%bf%e7%94%a8%e6%9c%ac%e5%9c%b0%e5%a4%a7%e6%a8%a1%e5%9e%8b%e4%bb%8e%e8%ae%ba%e6%96%87PDF%e4%b8%ad%e6%8f%90%e5%8f%96%e7%bb%93%e6%9e%84%e5%8c%96%e4%bf%a1%e6%81%af&amp;url=%2fblog%2f2024-08-03-literature-document-parsing-using-large-language-models-with-code%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
    
</div>




  </footer><script src="https://utteranc.es/client.js"
        repo="hiDaDeng/hidadeng.github.io"
        issue-term="pathname"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
</script>

  
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="/">大邓和他的PYTHON</a></span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'Copy';

        function copyingDone() {
            copybutton.innerText = 'Copied!';
            setTimeout(() => {
                copybutton.innerText = 'Copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>


    
    
</body>

</html>
