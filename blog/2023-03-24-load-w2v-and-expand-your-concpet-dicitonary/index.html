<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 | 大邓和他的PYTHON</title>
<meta name="keywords" content="Python, 数据集, dataset, 经济管理, 词向量, 词嵌入, 态度, 偏见, 组织文化, 旅游目的地形象, 品牌形象" />
<meta name="description" content="相关内容  LIST | 社科(经管)文本挖掘文献汇总 LIST | 文本分析代码汇总 LIST | 可供社科(经管)领域使用的数据集 Python实证指标构建与文本分析 使用3751w专利申请数据集按年份(按省份)训练词向量 预训练模型 | 使用1000w专利摘要训练word2vec模型，可用于开发词典  相关文献
[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216. [1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27. [3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156&#43;11&#43;19-21. [4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020 
一、训练 1.1 导入mda数据 读取 数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析
import pandas as pd df = pd.read_excel(&#39;mda01-22.csv.gz&#39;, compression=&#39;gzip&#39;) #gz解压后读取csv #df = pd.read_excel(&#39;mda01-22.csv&#39;) print(len(df)) df.head() Run
55439 1.2 构造语料 从 mda01-22.xlsx 数据中抽取出所有文本，写入到 mda01-22.txt">
<meta name="author" content="大邓">
<link rel="canonical" href="/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/" />
<meta name="baidu-site-verification" content="codeva-q71uASYfGi" />
<meta name="360-site-verification" content="6b9b733ec558a1bb12cee8aa82f2529e" />
<meta name="sogou-site-verification" content="dZHPIorOhK" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.89.4" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型" />
<meta property="og:description" content="相关内容  LIST | 社科(经管)文本挖掘文献汇总 LIST | 文本分析代码汇总 LIST | 可供社科(经管)领域使用的数据集 Python实证指标构建与文本分析 使用3751w专利申请数据集按年份(按省份)训练词向量 预训练模型 | 使用1000w专利摘要训练word2vec模型，可用于开发词典  相关文献
[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216. [1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27. [3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156&#43;11&#43;19-21. [4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020 
一、训练 1.1 导入mda数据 读取 数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析
import pandas as pd df = pd.read_excel(&#39;mda01-22.csv.gz&#39;, compression=&#39;gzip&#39;) #gz解压后读取csv #df = pd.read_excel(&#39;mda01-22.csv&#39;) print(len(df)) df.head() Run
55439 1.2 构造语料 从 mda01-22.xlsx 数据中抽取出所有文本，写入到 mda01-22.txt" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/" />
<meta property="og:image" content="/images/blog/mda-analysis-ipad.jpg" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2023-03-24T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2023-03-24T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="/images/blog/mda-analysis-ipad.jpg" />
<meta name="twitter:title" content="词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型"/>
<meta name="twitter:description" content="相关内容  LIST | 社科(经管)文本挖掘文献汇总 LIST | 文本分析代码汇总 LIST | 可供社科(经管)领域使用的数据集 Python实证指标构建与文本分析 使用3751w专利申请数据集按年份(按省份)训练词向量 预训练模型 | 使用1000w专利摘要训练word2vec模型，可用于开发词典  相关文献
[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216. [1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27. [3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156&#43;11&#43;19-21. [4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020 
一、训练 1.1 导入mda数据 读取 数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析
import pandas as pd df = pd.read_excel(&#39;mda01-22.csv.gz&#39;, compression=&#39;gzip&#39;) #gz解压后读取csv #df = pd.read_excel(&#39;mda01-22.csv&#39;) print(len(df)) df.head() Run
55439 1.2 构造语料 从 mda01-22.xlsx 数据中抽取出所有文本，写入到 mda01-22.txt"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "/blog/"
    }
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "词向量(付费) | 使用MD\u0026A2001-2022语料训练Word2Vec模型",
      "item": "/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "词向量(付费) | 使用MD\u0026A2001-2022语料训练Word2Vec模型",
  "name": "词向量(付费) | 使用MD\u0026A2001-2022语料训练Word2Vec模型",
  "description": "相关内容  LIST | 社科(经管)文本挖掘文献汇总 LIST | 文本分析代码汇总 LIST | 可供社科(经管)领域使用的数据集 Python实证指标构建与文本分析 使用3751w专利申请数据集按年份(按省份)训练词向量 预训练模型 | 使用1000w专利摘要训练word2vec模型，可用于开发词典  相关文献\n[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216. [1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27. [3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21. [4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020 \n一、训练 1.1 导入mda数据 读取 数据集 | 2001-2022年A股上市公司年报\u0026amp;管理层讨论与分析\nimport pandas as pd df = pd.read_excel(\u0026#39;mda01-22.csv.gz\u0026#39;, compression=\u0026#39;gzip\u0026#39;) #gz解压后读取csv #df = pd.read_excel(\u0026#39;mda01-22.csv\u0026#39;) print(len(df)) df.head() Run\n55439 1.2 构造语料 从 mda01-22.xlsx 数据中抽取出所有文本，写入到 mda01-22.txt",
  "keywords": [
    "Python", "数据集", "dataset", "经济管理", "词向量", "词嵌入", "态度", "偏见", "组织文化", "旅游目的地形象", "品牌形象"
  ],
  "articleBody": "相关内容  LIST | 社科(经管)文本挖掘文献汇总 LIST | 文本分析代码汇总 LIST | 可供社科(经管)领域使用的数据集 Python实证指标构建与文本分析 使用3751w专利申请数据集按年份(按省份)训练词向量 预训练模型 | 使用1000w专利摘要训练word2vec模型，可用于开发词典  相关文献\n[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216. [1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27. [3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21. [4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020 \n一、训练 1.1 导入mda数据 读取 数据集 | 2001-2022年A股上市公司年报\u0026管理层讨论与分析\nimport pandas as pd df = pd.read_excel('mda01-22.csv.gz', compression='gzip') #gz解压后读取csv #df = pd.read_excel('mda01-22.csv') print(len(df)) df.head() Run\n55439 1.2 构造语料 从 mda01-22.xlsx 数据中抽取出所有文本，写入到 mda01-22.txt\nwith open('mda01-22.txt', 'a+', encoding='utf-8') as f: text = ''.join(df['text']) f.write(text) \n1.3 配置cntext环境 使用2.1.1版本 cntext 库(该版本暂不开源，需付费购买)。 将得到的 cntext-2.1.1-py3-none-any.whl 文件放置于电脑桌面， win系统打开cmd(Mac打开terminal)， 输入如下命令(将工作环境切换至桌面)\ncd desktop 个别Win用户如无效，试试cd Desktop 。\n继续在cmd (terminal) 中执行如下命令安装cntext2.1.1\npip3 install distinctiveness pip3 install cntext-2.1.1-py3-none-any.whl \n1.4 训练word2vec 设置模型参数配置\n mda01-22 使用2001-2022年度mda数据训练 200 嵌入的维度数，即每个词的向量长度是200 6 词语上下文的窗口是6  %%time #程序结束后，可查看总的运行时间 import cntext as ct w2v = ct.W2VModel(corpus_file='mda01-22.txt') w2v.train(vector_size=200, window_size=6, min_count=6, save_dir='Word2Vec') Run\nBuilding prefix dict from the default dictionary ... Start Preprocessing Corpus... Dumping model to file cache /var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/jieba.cache Loading model cost 0.278 seconds. Prefix dict has been built successfully. Start Training! This may take a while. Please be patient... Training word2vec model took 3532 seconds Note: The Word2Vec model has been saved to output/Word2Vec CPU times: user 1h 30min 45s, sys: 30.1 s, total: 1h 31min 15s Wall time: 58min 57s 经过不到两个小时时间， 训练出的中国A股市场词向量模型(如下截图)，词汇量 914058， 模型文件 1.49G。模型可广泛用于经济管理等领域概念(情感)词典的构建或扩展。\n mda01-22.200.6.bin mda01-22.200.6.bin.syn1neg.npy mda01-22.200.6.bin.wv.vectors.npy  为什么这样确定200和6，可以看这篇 词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总\n二、导入模型 需要用到两个自定义函数load_w2v、expand_dictionary，源代码太长，为了提高阅读体验， 放在文末。大家记得用这两个函数前一定要先导入。点击代码\n#先导入load_w2v、expand_dictionary函数源代码 #读取模型文件 w2v_model = load_w2v(w2v_path='Word2Vec/mda01-22.200.6.bin') w2v_model Loading word2vec model...  注意 之前购买过mda01-22.100.6.bin的可以留意下， 和 是有区别的。\n\n三、w2v_model的使用  查看词汇量 查询某词向量 查看多个词的均值向量  更多内容，建议查看下gensim库的文档\n#词汇量 len(w2v_model.wv.index_to_key) Run\n914058  #查询某词的词向量 w2v_model.wv.get_vector('创新') Run\narray([-1.36441350e-01, -2.02002168e+00, -1.49168205e+00, 2.65202689e+00, 1.49721682e+00, 2.14851022e+00, -1.54925853e-01, -2.25241160e+00, -3.58773202e-01, 1.54530525e+00, -7.62950361e-01, -9.77181852e-01, 6.70365512e-01, -3.20203233e+00, 3.18079638e+00, 1.66510820e+00, 9.80131567e-01, 1.62199986e+00, 1.80585206e+00, 4.08179426e+00, -1.26518166e+00, 3.75929743e-01, 5.72038591e-01, 1.16134119e+00, 2.55617023e+00, -2.25110960e+00, -2.61538339e+00, -5.71992218e-01, 8.70356798e-01, -1.85045290e+00, -2.85597444e-01, -9.15628672e-01, -2.03667688e+00, 2.11716801e-01, 2.94088912e+00, -2.32688546e+00, 2.20858502e+00, 8.81347775e-01, -7.99135566e-01, -8.61206651e-01, -4.45446587e+00, -1.73757005e+00, -3.36678886e+00, -2.82611530e-02, -1.62726247e+00, -8.49750221e-01, 4.13731128e-01, -1.62519825e+00, 3.03865957e+00, -1.39746085e-01, 8.22233260e-01, -7.97697455e-02, 1.72468078e+00, 2.94929433e+00, 9.72453177e-01, -1.12741642e-01, 8.18425417e-01, -9.05264139e-01, 2.61516261e+00, 8.02830994e-01, 2.40420485e+00, 8.85799348e-01, -1.08665645e+00, 8.21912348e-01, -4.39456075e-01, -2.57663131e+00, 2.38062453e+00, -4.58515882e-01, 2.12767506e+00, -2.01356173e-01, 2.71096081e-01, 9.51708496e-01, -3.05705309e+00, -6.06385887e-01, -1.38406023e-01, 2.36809158e+00, -2.49158549e+00, 2.71105647e+00, -3.07211792e-03, 1.04273570e+00, 1.44201803e+00, -5.65704823e-01, 2.85488725e-01, 1.43495277e-01, -1.39421299e-01, 9.24086392e-01, 4.25374925e-01, -1.56690669e+00, 1.67641795e+00, -1.03729677e+00, -1.45472065e-01, -2.11022258e+00, -1.81541741e+00, -8.66766050e-02, 8.72350857e-02, 1.17173791e+00, -3.07721123e-02, 5.84330797e-01, 1.47265148e+00, -1.76913440e+00, -8.48391712e-01, -3.25056529e+00, 7.14846313e-01, -2.98076987e-01, 1.13966620e+00, -1.42698896e+00, 6.93505168e-01, -2.04717040e+00, -1.53559577e+00, 1.01942134e+00, -1.58283603e+00, 9.08654630e-01, -1.90529859e+00, -9.43309963e-01, 4.12964225e-01, -2.50713086e+00, -4.24056143e-01, -4.10613680e+00, 3.60615468e+00, -4.19765860e-01, -2.41174579e+00, 6.80675328e-01, 2.99834704e+00, 1.05610855e-01, -7.84325838e-01, 3.24065971e+00, -1.85072863e+00, -2.12448812e+00, -2.83468294e+00, -5.77759802e-01, -3.13433480e+00, -6.91670418e-01, 2.99401569e+00, -5.16145706e-01, 9.09552336e-01, -5.52680910e-01, -2.88360894e-01, 1.11991334e+00, -1.11737549e+00, 1.15479147e+00, -4.63319182e-01, 1.38351321e+00, -3.02179503e+00, 1.24334955e+00, 1.93393975e-01, -8.27962995e-01, -2.37227559e+00, -9.26931739e-01, 6.72517180e-01, 1.27736795e+00, 1.98695862e+00, 1.41960573e+00, -3.73892736e+00, -3.14201683e-01, -7.19093859e-01, 1.86080355e-02, -2.68105698e+00, 1.04344964e+00, 9.46133554e-01, -2.06151366e+00, -2.84214950e+00, 1.17004764e+00, 1.24577022e+00, -1.10806060e+00, 9.93207514e-01, 8.46789181e-01, -3.09691691e+00, 2.12616014e+00, -1.49274826e+00, -1.53214395e+00, -9.95470941e-01, 1.23463202e+00, -2.18907285e+00, -4.94913310e-01, 2.80939412e+00, 1.68149090e+00, 1.48991072e+00, 3.83729649e+00, 4.72325265e-01, 1.37606680e+00, 2.14257884e+00, 3.18186909e-01, 5.98093605e+00, 1.46744043e-01, -2.37729326e-01, 1.20463884e+00, -1.55812174e-01, -5.03088772e-01, 4.53981996e-01, 1.95544350e+00, -2.32564354e+00, -4.09389853e-01, 1.89125270e-01, 2.62835431e+00, 9.81123984e-01, -9.51041043e-01, -1.14294410e-01, 1.10983588e-01, 9.30419266e-02, -9.84693542e-02], dtype=float32)  #查询多个词的词向量 w2v_model.wv.get_mean_vector(['创新', '研发']) Ruj\narray([ 0.03019853, -0.01928307, -0.05371316, 0.00053774, 0.02516318, 0.10103251, -0.03914721, -0.08307559, 0.00444389, 0.09456791, -0.05761364, -0.03459097, 0.04394419, -0.10181106, 0.1418381 , 0.05334964, 0.01820264, 0.01493831, 0.01626587, 0.17402864, -0.02859601, 0.04538149, 0.03768233, 0.05431981, 0.15405464, -0.03632693, -0.08566202, -0.00595666, 0.08378439, -0.11071078, -0.05904576, -0.06451955, -0.1076955 , 0.05141645, 0.11710279, -0.09403889, 0.08633652, -0.06743232, 0.00328483, 0.01589498, -0.11226317, -0.05367877, -0.057222 , -0.00685401, -0.04531868, -0.02090884, 0.01426806, -0.04787309, 0.1325518 , -0.00498158, 0.01912023, -0.02292867, 0.08855374, 0.07697155, 0.01407153, -0.02378988, 0.03745927, 0.00889686, 0.12555045, 0.04007044, 0.06247196, 0.04912657, -0.06158784, 0.06346396, 0.00197599, -0.04995281, 0.05125345, -0.01584197, 0.07572784, 0.02580263, -0.02904062, -0.0008835 , -0.08365948, -0.05539802, -0.07523517, 0.04622741, -0.12007375, 0.05453204, -0.02054051, 0.02937108, 0.10272598, -0.0089594 , 0.05172383, 0.00588922, -0.0010917 , 0.02603476, -0.01580217, -0.07810815, 0.06964722, -0.04709972, -0.0316673 , -0.05055645, -0.05096703, 0.02772727, -0.03495743, 0.09567484, -0.0071935 , -0.01266821, 0.00074132, -0.07593331, -0.02928162, -0.12574387, 0.02437552, -0.0228716 , -0.03047204, -0.03948782, 0.07722469, -0.07440004, -0.00951135, 0.05531401, -0.03240326, 0.00389662, -0.05632257, -0.05030375, 0.02883579, -0.06157173, 0.00584065, -0.16594191, 0.1108149 , -0.00243916, -0.09964953, 0.02029083, 0.03522225, -0.01167114, -0.04048527, 0.08301719, -0.04682562, -0.0714631 , -0.07355815, -0.0496731 , -0.05303175, -0.03625978, 0.06879813, -0.09117774, 0.0323513 , -0.01808765, -0.01746182, 0.02472609, -0.00873791, -0.00951474, -0.02176155, 0.02394484, -0.07035318, 0.10963078, 0.01004294, -0.02269555, -0.09929934, -0.02897175, 0.02157164, 0.05608977, 0.09083252, -0.00525982, -0.09866816, -0.02736895, -0.02923711, 0.05582205, -0.04462272, 0.01932517, 0.04468061, 0.00317996, -0.04182415, 0.03061792, 0.04278665, 0.02939183, 0.03475334, -0.00898206, -0.08902986, 0.08294971, -0.00942507, -0.02125597, -0.01008157, 0.04477865, -0.08366893, -0.00074587, 0.08328778, 0.02653155, 0.04581301, 0.10532658, -0.04637942, 0.04722971, 0.06853952, -0.00235328, 0.18312256, -0.0457427 , 0.00874868, 0.08945092, -0.01135547, -0.04203002, 0.02408407, 0.0594779 , -0.05467811, 0.01946783, 0.07095537, 0.04226222, -0.0018304 , -0.00086302, 0.04624099, 0.01009499, 0.04783599, 0.02535392], dtype=float32)  有了每个词或者概念的向量，可以结合cntext旧版本单语言模型内的态度偏见的度量。\n\n四、扩展词典 做词典法的文本分析，最重要的是有自己的领域词典。之前受限于技术难度，文科生的我也一直在用形容词的通用情感词典。现在依托word2vec技术， 可以加速人工构建的准确率和效率。\n下面是在 mda01-22.200.6.bin 上做的词典扩展测试，函数expand_dictionary会根据种子词选取最准确的topn个词。\n#短视主义词 实验 expand_dictionary(wv=w2v_model.wv, seedwords=['抓紧', '立刻', '月底', '年底', '年终', '争取', '力争'], topn=30) Run\n['抓紧', '立刻', '月底', '年底', '年终', '争取', '力争', '争取', '力争', '年底', '月底', '3月底', '尽快', '上半年', '努力争取', '年内实现', '抓紧', '工作争取', '尽早', '6月底', '工作力争', '7月份', '年底完成', '确保', '早日', '有望', '全力', '创造条件', '3月份', '加紧', '力争实现', '力争今年', '月底前', '10月底', '4月份', '继续', '月初']  expand_dictionary(wv=w2v_model.wv, seedwords=['团结', '拼搏', '克服', '勇攀高峰', '友善', '进取'], topn=30) Run\n['团结', '拼搏', '克服', '勇攀高峰', '友善', '进取', '拼搏', '艰苦奋斗', '团结拼搏', '勇于担当', '锐意进取', '勇气', '团结', '团结奋进', '团结一致', '顽强拼搏', '上下一心', '实干', '拼搏进取', '积极进取', '奋力拼搏', '奋进', '坚定信念', '团结一心', '精诚团结', '顽强', '踏实', '团结协作', '求真务实', '团结奋斗', '奋发有为', '同心协力', '脚踏实地', '开拓进取', '进取', '勇于']  expand_dictionary(wv=w2v_model.wv, seedwords=['创新', '科技', '研发', '技术', '标准'], topn=30) Run\n['创新', '科技', '研发', '技术', '标准', '技术创新', '技术研发', '先进技术', '关键技术', '创新性', '前沿技术', '科技创新', '技术应用', '产品开发', '自主创新', '新技术', '科研', '产品研发', '自主研发', '技术开发', '工艺技术', '技术标准', '基础研究', '集成创新', '核心技术', '成熟技术', '研发创新', '理论技术', '前沿技术研发', '工艺', '科技成果', '技术研究', '标准制定', '技术装备', '技术相结合']  expand_dictionary(wv=w2v_model.wv, seedwords=['竞争', '竞争力'], topn=30) Run\n['竞争', '竞争力', '竞争能力', '市场竞争', '竞争优势', '市场竞争力', '竞', '竞争实力', '激烈竞争', '参与市场竞争', '国际竞争', '市场竞争能力', '竞争态势', '市场竞争优势', '行业竞争', '综合竞争力', '竞争对手', '未来市场竞争', '产品竞争力', '之间竞争', '核心竞争力', '参与竞争', '核心竞争能力', '竞争日趋激烈', '国际化竞争', '国际竞争力', '竟争力', '市场化竞争', '同质化竞争', '竞争力关键', '价格竞争', '整体竞争力']  expand_dictionary(wv=w2v_model.wv, seedwords=['疫情', '扩散', '防控', '反复', '冲击'], topn=30) Run\n['疫情', '扩散', '防控', '反复', '冲击', '蔓延', '疫情', '疫情爆发', '疫情冲击', '新冠疫情', '肆虐', '新冠肺炎', '疫情蔓延', '本次疫情', '散发', '疫情扩散', '疫情影响', '疫情反复', '疫情传播', '肺炎疫情', '国内疫情', '击', '各地疫情', '疫情全球', '疫情多点', '全球疫情', '持续蔓延', '多点散发', '疫情导致', '疫情暴发', '病毒疫情', '疫情持续', '疫情初期', '疫情出现', '防控措施']  expand_dictionary(wv=w2v_model.wv, seedwords=['旧', '老', '后', '落后'], topn=30) Run\n['旧', '老', '后', '落后', '老', '旧', '陈旧', '老旧', '淘汰', '低效率', '低效', '部分老旧', '进行改造', '老旧设备', '工艺落后', '设备陈旧', '能耗高', '更新改造', '落后工艺', '技术落后', '改造', '翻新', '简陋', '旧设备', '拆除', '现象严重', '原有', '相对落后', '产能淘汰', '加快淘汰', '搬', '替换', '大批', '迁']  \n五、源代码 from gensim.models import KeyedVectors from pathlib import Path def load_w2v(w2v_path): \"\"\" Load word2vec model Args: w2v_path (str): path of word2vec model Returns: model: word2vec model \"\"\" print('Loading word2vec model...') model = KeyedVectors.load(w2v_path) return model def expand_dictionary(wv, seedwords, topn=100): \"\"\" According to the seed word file, select the top n words with the most similar semantics and save them in the directory save_dir. Args: wv (Word2VecKeyedVectors): the word embedding model seedwords (list): 种子词 topn (int, optional): Set the number of most similar words to retrieve to topn. Defaults to 100. save_dir (str, optional): the directory to save the candidate words. Defaults to 'Word2Vec'. Returns: \"\"\" simidx_scores = [] similars_candidate_idxs = [] #the candidate words of seedwords dictionary = wv.key_to_index seedidxs = [] #transform word to index for seed in seedwords: if seed in dictionary: seedidx = dictionary[seed] seedidxs.append(seedidx) for seedidx in seedidxs: # sims_words such as [('by', 0.99984), ('or', 0.99982), ('an', 0.99981), ('up', 0.99980)] sims_words = wv.similar_by_word(seedidx, topn=topn) #Convert words to index and store them similars_candidate_idxs.extend([dictionary[sim[0]] for sim in sims_words]) similars_candidate_idxs = set(similars_candidate_idxs) for idx in similars_candidate_idxs: score = wv.n_similarity([idx], seedidxs) simidx_scores.append((idx, score)) simidxs = [w[0] for w in sorted(simidx_scores, key=lambda k:k[1], reverse=True)] simwords = [str(wv.index_to_key[idx]) for idx in simidxs][:topn] resultwords = [] resultwords.extend(seedwords) resultwords.extend(simwords) return resultwords \n六、获取模型 内容创作不易， 本文为付费内容，\n- 100元 2001-2022年报 \u0026 管理层讨论与分析 - 100元 cntext-2.1.1-py3-none-any.whl - 100元 Word2Vec相关模型文件(mda01-22.200.6.bin) - 200元 - 2001-2022年报 \u0026 管理层讨论与分析 - cntext-2.1.1-py3-none-any.whl - Word2Vec相关模型文件(mda01-22.200.6.bin) 加微信 372335839， 备注「姓名-学校-专业-word2vec」\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ",
  "wordCount" : "1164",
  "inLanguage": "en",
  "image":"/images/blog/mda-analysis-ipad.jpg","datePublished": "2023-03-24T00:00:00Z",
  "dateModified": "2023-03-24T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "大邓"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "大邓和他的PYTHON",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon.ico"
    }
  }
}
</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SFGQCREQ9X"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SFGQCREQ9X');
</script>



<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=PT+Serif" rel="stylesheet">
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="/" accesskey="h" title="大邓和他的PYTHON (Alt + H)" target="_blank">大邓和他的PYTHON</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="/about/" title="关于" target="_blank">
                    <span>关于</span>
                </a>
            </li>
            <li>
                <a href="/blog" title="博文" target="_blank">
                    <span>博文</span>
                </a>
            </li>
            <li>
                <a href="/blog/2024-04-27-cntext2x-usage-tutorial/" title="cntext2" target="_blank">
                    <span>cntext2</span>
                </a>
            </li>
            <li>
                <a href="/search/" title="搜索" target="_blank">
                    <span>搜索</span>
                </a>
            </li>
            <li>
                <a href="/tags/" title="标签" target="_blank">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="/blog/management_python_course/" title="课程" target="_blank">
                    <span>课程</span>
                </a>
            </li>
            <li>
                <a href="/blog/datasets_available_for_management_science/" title="数据" target="_blank">
                    <span>数据</span>
                </a>
            </li>
            <li>
                <a href="/index.xml" title="RSS" target="_blank">
                    <span>RSS</span>
                </a>
            </li>
            <li>
                <a href="/support/" title="打赏" target="_blank">
                    <span>打赏</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
onload="renderMathInElement(document.body);"></script>




<article class="post-single">
  <header class="post-header">
    
    <div class="breadcrumbs"><a href="/" target="_blank">Home</a>&nbsp;»&nbsp;<a href="/blog/" target="_blank">Blogs</a></div>
    <h1 class="post-title">
      词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型
    </h1>
    <div class="post-meta"><span title='2023-03-24 00:00:00 +0000 UTC'>2023-03-24</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;大邓

</div>
  </header> 
<figure class="entry-cover"><a href="/images/blog/mda-analysis-ipad.jpg" target="_blank"
            rel="noopener noreferrer"><img loading="lazy" src="/images/blog/mda-analysis-ipad.jpg" alt=""></a>
        
</figure>
<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on twitter"
        href="https://twitter.com/intent/tweet/?text=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b&amp;url=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f&amp;hashtags=%e6%95%b0%e6%8d%ae%e9%9b%86%2c%e8%af%8d%e5%90%91%e9%87%8f%2c%e7%bb%8f%e6%b5%8e%e7%ae%a1%e7%90%86%2c%e4%bb%98%e8%b4%b9%e5%86%85%e5%ae%b9">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f&amp;title=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b&amp;summary=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b&amp;source=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on reddit"
        href="https://reddit.com/submit?url=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f&title=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on facebook"
        href="https://facebook.com/sharer/sharer.php?u=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on whatsapp"
        href="https://api.whatsapp.com/send?text=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b%20-%20%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on telegram"
        href="https://telegram.me/share/url?text=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b&amp;url=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
    
</div>
<aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e7%9b%b8%e5%85%b3%e5%86%85%e5%ae%b9" aria-label="相关内容">相关内容</a></li>
                    <li>
                        <a href="#%e4%b8%80%e8%ae%ad%e7%bb%83" aria-label="一、训练">一、训练</a><ul>
                            
                    <li>
                        <a href="#11-%e5%af%bc%e5%85%a5mda%e6%95%b0%e6%8d%ae" aria-label="1.1 导入mda数据">1.1 导入mda数据</a></li>
                    <li>
                        <a href="#12-%e6%9e%84%e9%80%a0%e8%af%ad%e6%96%99" aria-label="1.2 构造语料">1.2 构造语料</a></li>
                    <li>
                        <a href="#13-%e9%85%8d%e7%bd%aecntext%e7%8e%af%e5%a2%83" aria-label="1.3 配置cntext环境">1.3 配置cntext环境</a></li>
                    <li>
                        <a href="#14-%e8%ae%ad%e7%bb%83word2vec" aria-label="1.4 训练word2vec">1.4 训练word2vec</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e4%ba%8c%e5%af%bc%e5%85%a5%e6%a8%a1%e5%9e%8b" aria-label="二、导入模型">二、导入模型</a></li>
                    <li>
                        <a href="#%e6%b3%a8%e6%84%8f" aria-label="注意">注意</a><ul>
                            
                    <li>
                        <a href="#%e4%b8%89w2v_model%e7%9a%84%e4%bd%bf%e7%94%a8" aria-label="三、w2v_model的使用">三、w2v_model的使用</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e5%9b%9b%e6%89%a9%e5%b1%95%e8%af%8d%e5%85%b8" aria-label="四、扩展词典">四、扩展词典</a></li>
                    <li>
                        <a href="#%e4%ba%94%e6%ba%90%e4%bb%a3%e7%a0%81" aria-label="五、源代码">五、源代码</a></li>
                    <li>
                        <a href="#%e5%85%ad%e8%8e%b7%e5%8f%96%e6%a8%a1%e5%9e%8b" aria-label="六、获取模型">六、获取模型</a></li>
                    <li>
                        <a href="#%e5%b9%bf%e8%80%8c%e5%91%8a%e4%b9%8b" aria-label="广而告之">广而告之</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h2 id="相关内容">相关内容<a hidden class="anchor" aria-hidden="true" href="#相关内容">#</a></h2>
<ul>
<li><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)文本挖掘文献汇总</a></li>
<li><a href="https://textdata.cn/blog/text_analysis_code_list_about_ms/">LIST | 文本分析代码汇总</a></li>
<li><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">Python实证指标构建与文本分析</a></li>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">使用3751w专利申请数据集按年份(按省份)训练词向量</a></li>
<li><a href="https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/">预训练模型 | 使用1000w专利摘要训练word2vec模型，可用于开发词典</a></li>
</ul>
<p>相关文献</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216.
[1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.
[3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
</code></pre></div><p><br><br></p>
<h2 id="一训练">一、训练<a hidden class="anchor" aria-hidden="true" href="#一训练">#</a></h2>
<h3 id="11-导入mda数据">1.1 导入mda数据<a hidden class="anchor" aria-hidden="true" href="#11-导入mda数据">#</a></h3>
<p>读取 <a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/"><strong>数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析</strong></a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;mda01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="c1">#gz解压后读取csv</span>
<span class="c1">#df = pd.read_excel(&#39;mda01-22.csv&#39;)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">55439
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<h3 id="12-构造语料">1.2 构造语料<a hidden class="anchor" aria-hidden="true" href="#12-构造语料">#</a></h3>
<p>从 <strong>mda01-22.xlsx</strong> 数据中抽取出所有文本，写入到 <strong>mda01-22.txt</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;mda01-22.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="13-配置cntext环境">1.3 配置cntext环境<a hidden class="anchor" aria-hidden="true" href="#13-配置cntext环境">#</a></h3>
<p>使用2.1.1版本 cntext 库(该版本暂不开源，需付费购买)。 将得到的 <strong>cntext-2.1.1-py3-none-any.whl</strong> 文件放置于电脑桌面，  win系统打开<strong>cmd</strong>(Mac打开terminal)， 输入如下命令(将工作环境切换至桌面)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>个别Win用户如无效，试试<code>cd Desktop</code> 。</p>
<p>继续在cmd (terminal) 中执行如下命令安装cntext2.1.1</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl 
</code></pre></div><br>
<h3 id="14-训练word2vec">1.4 训练word2vec<a hidden class="anchor" aria-hidden="true" href="#14-训练word2vec">#</a></h3>
<p>设置模型参数配置</p>
<ul>
<li>mda01-22 使用2001-2022年度mda数据训练</li>
<li>200 嵌入的维度数，即每个词的向量长度是200</li>
<li>6 词语上下文的窗口是6</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>  <span class="c1">#程序结束后，可查看总的运行时间</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;mda01-22.txt&#39;</span><span class="p">)</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="s1">&#39;Word2Vec&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Building prefix dict from the default dictionary ...
Start Preprocessing Corpus...
Dumping model to file cache /var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/jieba.cache
Loading model cost 0.278 seconds.
Prefix dict has been built successfully.
Start Training! This may take a while. Please be patient...

Training word2vec model took 3532 seconds

Note: The Word2Vec model has been saved to output/Word2Vec

CPU times: user 1h 30min 45s, sys: 30.1 s, total: 1h 31min 15s
Wall time: 58min 57s
</code></pre></div><p>经过不到两个小时时间， 训练出的中国A股市场词向量模型(如下截图)，词汇量 914058， 模型文件 1.49G。模型可广泛用于经济管理等领域概念(情感)词典的构建或扩展。</p>
<ul>
<li><strong>mda01-22.200.6.bin</strong></li>
<li><strong>mda01-22.200.6.bin.syn1neg.npy</strong></li>
<li><strong>mda01-22.200.6.bin.wv.vectors.npy</strong></li>
</ul>
<p><img loading="lazy" src="img/pretained-screen.png" alt=""  />
</p>
<p>为什么这样确定200和6，可以看这篇 <a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></p>
<br>
<br>
<h2 id="二导入模型">二、导入模型<a hidden class="anchor" aria-hidden="true" href="#二导入模型">#</a></h2>
<p>需要用到两个自定义函数load_w2v、expand_dictionary，源代码太长，为了提高阅读体验， 放在文末。大家记得用这两个函数前一定要先导入。<a href="mda_pretained_model_code.ipynb"><strong>点击代码</strong></a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#先导入load_w2v、expand_dictionary函数源代码</span>


<span class="c1">#读取模型文件</span>
<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">load_w2v</span><span class="p">(</span><span class="n">w2v_path</span><span class="o">=</span><span class="s1">&#39;Word2Vec/mda01-22.200.6.bin&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span>
</code></pre></div><pre><code>Loading word2vec model...
&lt;gensim.models.word2vec.Word2Vec at 0x310dd9990&gt;
</code></pre>
<br>
<h2 id="注意">注意<a hidden class="anchor" aria-hidden="true" href="#注意">#</a></h2>
<p>之前购买过mda01-22.100.6.bin的可以留意下， &lt;gensim.models.word2vec.Word2Vec&gt;和&lt;gensim.models.keyedvectors.KeyedVectors&gt;
是有区别的。</p>
<p><br><br></p>
<h3 id="三w2v_model的使用">三、w2v_model的使用<a hidden class="anchor" aria-hidden="true" href="#三w2v_model的使用">#</a></h3>
<ul>
<li>查看词汇量</li>
<li>查询某词向量</li>
<li>查看多个词的均值向量</li>
</ul>
<p>更多内容，建议查看下gensim库的文档</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#词汇量</span>
<span class="nb">len</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>914058  
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查询某词的词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;创新&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>array([-1.36441350e-01, -2.02002168e+00, -1.49168205e+00,  2.65202689e+00,
        1.49721682e+00,  2.14851022e+00, -1.54925853e-01, -2.25241160e+00,
       -3.58773202e-01,  1.54530525e+00, -7.62950361e-01, -9.77181852e-01,
        6.70365512e-01, -3.20203233e+00,  3.18079638e+00,  1.66510820e+00,
        9.80131567e-01,  1.62199986e+00,  1.80585206e+00,  4.08179426e+00,
       -1.26518166e+00,  3.75929743e-01,  5.72038591e-01,  1.16134119e+00,
        2.55617023e+00, -2.25110960e+00, -2.61538339e+00, -5.71992218e-01,
        8.70356798e-01, -1.85045290e+00, -2.85597444e-01, -9.15628672e-01,
       -2.03667688e+00,  2.11716801e-01,  2.94088912e+00, -2.32688546e+00,
        2.20858502e+00,  8.81347775e-01, -7.99135566e-01, -8.61206651e-01,
       -4.45446587e+00, -1.73757005e+00, -3.36678886e+00, -2.82611530e-02,
       -1.62726247e+00, -8.49750221e-01,  4.13731128e-01, -1.62519825e+00,
        3.03865957e+00, -1.39746085e-01,  8.22233260e-01, -7.97697455e-02,
        1.72468078e+00,  2.94929433e+00,  9.72453177e-01, -1.12741642e-01,
        8.18425417e-01, -9.05264139e-01,  2.61516261e+00,  8.02830994e-01,
        2.40420485e+00,  8.85799348e-01, -1.08665645e+00,  8.21912348e-01,
       -4.39456075e-01, -2.57663131e+00,  2.38062453e+00, -4.58515882e-01,
        2.12767506e+00, -2.01356173e-01,  2.71096081e-01,  9.51708496e-01,
       -3.05705309e+00, -6.06385887e-01, -1.38406023e-01,  2.36809158e+00,
       -2.49158549e+00,  2.71105647e+00, -3.07211792e-03,  1.04273570e+00,
        1.44201803e+00, -5.65704823e-01,  2.85488725e-01,  1.43495277e-01,
       -1.39421299e-01,  9.24086392e-01,  4.25374925e-01, -1.56690669e+00,
        1.67641795e+00, -1.03729677e+00, -1.45472065e-01, -2.11022258e+00,
       -1.81541741e+00, -8.66766050e-02,  8.72350857e-02,  1.17173791e+00,
       -3.07721123e-02,  5.84330797e-01,  1.47265148e+00, -1.76913440e+00,
       -8.48391712e-01, -3.25056529e+00,  7.14846313e-01, -2.98076987e-01,
        1.13966620e+00, -1.42698896e+00,  6.93505168e-01, -2.04717040e+00,
       -1.53559577e+00,  1.01942134e+00, -1.58283603e+00,  9.08654630e-01,
       -1.90529859e+00, -9.43309963e-01,  4.12964225e-01, -2.50713086e+00,
       -4.24056143e-01, -4.10613680e+00,  3.60615468e+00, -4.19765860e-01,
       -2.41174579e+00,  6.80675328e-01,  2.99834704e+00,  1.05610855e-01,
       -7.84325838e-01,  3.24065971e+00, -1.85072863e+00, -2.12448812e+00,
       -2.83468294e+00, -5.77759802e-01, -3.13433480e+00, -6.91670418e-01,
        2.99401569e+00, -5.16145706e-01,  9.09552336e-01, -5.52680910e-01,
       -2.88360894e-01,  1.11991334e+00, -1.11737549e+00,  1.15479147e+00,
       -4.63319182e-01,  1.38351321e+00, -3.02179503e+00,  1.24334955e+00,
        1.93393975e-01, -8.27962995e-01, -2.37227559e+00, -9.26931739e-01,
        6.72517180e-01,  1.27736795e+00,  1.98695862e+00,  1.41960573e+00,
       -3.73892736e+00, -3.14201683e-01, -7.19093859e-01,  1.86080355e-02,
       -2.68105698e+00,  1.04344964e+00,  9.46133554e-01, -2.06151366e+00,
       -2.84214950e+00,  1.17004764e+00,  1.24577022e+00, -1.10806060e+00,
        9.93207514e-01,  8.46789181e-01, -3.09691691e+00,  2.12616014e+00,
       -1.49274826e+00, -1.53214395e+00, -9.95470941e-01,  1.23463202e+00,
       -2.18907285e+00, -4.94913310e-01,  2.80939412e+00,  1.68149090e+00,
        1.48991072e+00,  3.83729649e+00,  4.72325265e-01,  1.37606680e+00,
        2.14257884e+00,  3.18186909e-01,  5.98093605e+00,  1.46744043e-01,
       -2.37729326e-01,  1.20463884e+00, -1.55812174e-01, -5.03088772e-01,
        4.53981996e-01,  1.95544350e+00, -2.32564354e+00, -4.09389853e-01,
        1.89125270e-01,  2.62835431e+00,  9.81123984e-01, -9.51041043e-01,
       -1.14294410e-01,  1.10983588e-01,  9.30419266e-02, -9.84693542e-02],
      dtype=float32)
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查询多个词的词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_mean_vector</span><span class="p">([</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;研发&#39;</span><span class="p">])</span>
</code></pre></div><p>Ruj</p>
<pre><code>array([ 0.03019853, -0.01928307, -0.05371316,  0.00053774,  0.02516318,
        0.10103251, -0.03914721, -0.08307559,  0.00444389,  0.09456791,
       -0.05761364, -0.03459097,  0.04394419, -0.10181106,  0.1418381 ,
        0.05334964,  0.01820264,  0.01493831,  0.01626587,  0.17402864,
       -0.02859601,  0.04538149,  0.03768233,  0.05431981,  0.15405464,
       -0.03632693, -0.08566202, -0.00595666,  0.08378439, -0.11071078,
       -0.05904576, -0.06451955, -0.1076955 ,  0.05141645,  0.11710279,
       -0.09403889,  0.08633652, -0.06743232,  0.00328483,  0.01589498,
       -0.11226317, -0.05367877, -0.057222  , -0.00685401, -0.04531868,
       -0.02090884,  0.01426806, -0.04787309,  0.1325518 , -0.00498158,
        0.01912023, -0.02292867,  0.08855374,  0.07697155,  0.01407153,
       -0.02378988,  0.03745927,  0.00889686,  0.12555045,  0.04007044,
        0.06247196,  0.04912657, -0.06158784,  0.06346396,  0.00197599,
       -0.04995281,  0.05125345, -0.01584197,  0.07572784,  0.02580263,
       -0.02904062, -0.0008835 , -0.08365948, -0.05539802, -0.07523517,
        0.04622741, -0.12007375,  0.05453204, -0.02054051,  0.02937108,
        0.10272598, -0.0089594 ,  0.05172383,  0.00588922, -0.0010917 ,
        0.02603476, -0.01580217, -0.07810815,  0.06964722, -0.04709972,
       -0.0316673 , -0.05055645, -0.05096703,  0.02772727, -0.03495743,
        0.09567484, -0.0071935 , -0.01266821,  0.00074132, -0.07593331,
       -0.02928162, -0.12574387,  0.02437552, -0.0228716 , -0.03047204,
       -0.03948782,  0.07722469, -0.07440004, -0.00951135,  0.05531401,
       -0.03240326,  0.00389662, -0.05632257, -0.05030375,  0.02883579,
       -0.06157173,  0.00584065, -0.16594191,  0.1108149 , -0.00243916,
       -0.09964953,  0.02029083,  0.03522225, -0.01167114, -0.04048527,
        0.08301719, -0.04682562, -0.0714631 , -0.07355815, -0.0496731 ,
       -0.05303175, -0.03625978,  0.06879813, -0.09117774,  0.0323513 ,
       -0.01808765, -0.01746182,  0.02472609, -0.00873791, -0.00951474,
       -0.02176155,  0.02394484, -0.07035318,  0.10963078,  0.01004294,
       -0.02269555, -0.09929934, -0.02897175,  0.02157164,  0.05608977,
        0.09083252, -0.00525982, -0.09866816, -0.02736895, -0.02923711,
        0.05582205, -0.04462272,  0.01932517,  0.04468061,  0.00317996,
       -0.04182415,  0.03061792,  0.04278665,  0.02939183,  0.03475334,
       -0.00898206, -0.08902986,  0.08294971, -0.00942507, -0.02125597,
       -0.01008157,  0.04477865, -0.08366893, -0.00074587,  0.08328778,
        0.02653155,  0.04581301,  0.10532658, -0.04637942,  0.04722971,
        0.06853952, -0.00235328,  0.18312256, -0.0457427 ,  0.00874868,
        0.08945092, -0.01135547, -0.04203002,  0.02408407,  0.0594779 ,
       -0.05467811,  0.01946783,  0.07095537,  0.04226222, -0.0018304 ,
       -0.00086302,  0.04624099,  0.01009499,  0.04783599,  0.02535392],
      dtype=float32)
</code></pre>
<p>有了每个词或者概念的向量，可以结合cntext旧版本单语言模型内的态度偏见的度量。</p>
<p><br><br></p>
<h2 id="四扩展词典">四、扩展词典<a hidden class="anchor" aria-hidden="true" href="#四扩展词典">#</a></h2>
<p>做词典法的文本分析，最重要的是有自己的领域词典。之前受限于技术难度，文科生的我也一直在用形容词的通用情感词典。现在依托word2vec技术， 可以加速人工构建的准确率和效率。</p>
<p>下面是在 mda01-22.200.6.bin 上做的词典扩展测试，函数expand_dictionary会根据种子词选取最准确的topn个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#短视主义词  实验</span>
<span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;抓紧&#39;</span><span class="p">,</span> <span class="s1">&#39;立刻&#39;</span><span class="p">,</span> <span class="s1">&#39;月底&#39;</span><span class="p">,</span> <span class="s1">&#39;年底&#39;</span><span class="p">,</span> <span class="s1">&#39;年终&#39;</span><span class="p">,</span> <span class="s1">&#39;争取&#39;</span><span class="p">,</span> <span class="s1">&#39;力争&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['抓紧',
 '立刻',
 '月底',
 '年底',
 '年终',
 '争取',
 '力争',
 '争取',
 '力争',
 '年底',
 '月底',
 '3月底',
 '尽快',
 '上半年',
 '努力争取',
 '年内实现',
 '抓紧',
 '工作争取',
 '尽早',
 '6月底',
 '工作力争',
 '7月份',
 '年底完成',
 '确保',
 '早日',
 '有望',
 '全力',
 '创造条件',
 '3月份',
 '加紧',
 '力争实现',
 '力争今年',
 '月底前',
 '10月底',
 '4月份',
 '继续',
 '月初']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;团结&#39;</span><span class="p">,</span> <span class="s1">&#39;拼搏&#39;</span><span class="p">,</span>  <span class="s1">&#39;克服&#39;</span><span class="p">,</span>  <span class="s1">&#39;勇攀高峰&#39;</span><span class="p">,</span>  <span class="s1">&#39;友善&#39;</span><span class="p">,</span>  <span class="s1">&#39;进取&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['团结',
 '拼搏',
 '克服',
 '勇攀高峰',
 '友善',
 '进取',
 '拼搏',
 '艰苦奋斗',
 '团结拼搏',
 '勇于担当',
 '锐意进取',
 '勇气',
 '团结',
 '团结奋进',
 '团结一致',
 '顽强拼搏',
 '上下一心',
 '实干',
 '拼搏进取',
 '积极进取',
 '奋力拼搏',
 '奋进',
 '坚定信念',
 '团结一心',
 '精诚团结',
 '顽强',
 '踏实',
 '团结协作',
 '求真务实',
 '团结奋斗',
 '奋发有为',
 '同心协力',
 '脚踏实地',
 '开拓进取',
 '进取',
 '勇于']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;科技&#39;</span><span class="p">,</span>  <span class="s1">&#39;研发&#39;</span><span class="p">,</span>  <span class="s1">&#39;技术&#39;</span><span class="p">,</span>  <span class="s1">&#39;标准&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['创新',
 '科技',
 '研发',
 '技术',
 '标准',
 '技术创新',
 '技术研发',
 '先进技术',
 '关键技术',
 '创新性',
 '前沿技术',
 '科技创新',
 '技术应用',
 '产品开发',
 '自主创新',
 '新技术',
 '科研',
 '产品研发',
 '自主研发',
 '技术开发',
 '工艺技术',
 '技术标准',
 '基础研究',
 '集成创新',
 '核心技术',
 '成熟技术',
 '研发创新',
 '理论技术',
 '前沿技术研发',
 '工艺',
 '科技成果',
 '技术研究',
 '标准制定',
 '技术装备',
 '技术相结合']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;竞争&#39;</span><span class="p">,</span> <span class="s1">&#39;竞争力&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['竞争',
 '竞争力',
 '竞争能力',
 '市场竞争',
 '竞争优势',
 '市场竞争力',
 '竞',
 '竞争实力',
 '激烈竞争',
 '参与市场竞争',
 '国际竞争',
 '市场竞争能力',
 '竞争态势',
 '市场竞争优势',
 '行业竞争',
 '综合竞争力',
 '竞争对手',
 '未来市场竞争',
 '产品竞争力',
 '之间竞争',
 '核心竞争力',
 '参与竞争',
 '核心竞争能力',
 '竞争日趋激烈',
 '国际化竞争',
 '国际竞争力',
 '竟争力',
 '市场化竞争',
 '同质化竞争',
 '竞争力关键',
 '价格竞争',
 '整体竞争力']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;疫情&#39;</span><span class="p">,</span> <span class="s1">&#39;扩散&#39;</span><span class="p">,</span> <span class="s1">&#39;防控&#39;</span><span class="p">,</span> <span class="s1">&#39;反复&#39;</span><span class="p">,</span> <span class="s1">&#39;冲击&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['疫情',
 '扩散',
 '防控',
 '反复',
 '冲击',
 '蔓延',
 '疫情',
 '疫情爆发',
 '疫情冲击',
 '新冠疫情',
 '肆虐',
 '新冠肺炎',
 '疫情蔓延',
 '本次疫情',
 '散发',
 '疫情扩散',
 '疫情影响',
 '疫情反复',
 '疫情传播',
 '肺炎疫情',
 '国内疫情',
 '击',
 '各地疫情',
 '疫情全球',
 '疫情多点',
 '全球疫情',
 '持续蔓延',
 '多点散发',
 '疫情导致',
 '疫情暴发',
 '病毒疫情',
 '疫情持续',
 '疫情初期',
 '疫情出现',
 '防控措施']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;旧&#39;</span><span class="p">,</span> <span class="s1">&#39;老&#39;</span><span class="p">,</span> <span class="s1">&#39;后&#39;</span><span class="p">,</span> <span class="s1">&#39;落后&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['旧',
 '老',
 '后',
 '落后',
 '老',
 '旧',
 '陈旧',
 '老旧',
 '淘汰',
 '低效率',
 '低效',
 '部分老旧',
 '进行改造',
 '老旧设备',
 '工艺落后',
 '设备陈旧',
 '能耗高',
 '更新改造',
 '落后工艺',
 '技术落后',
 '改造',
 '翻新',
 '简陋',
 '旧设备',
 '拆除',
 '现象严重',
 '原有',
 '相对落后',
 '产能淘汰',
 '加快淘汰',
 '搬',
 '替换',
 '大批',
 '迁']
</code></pre>
<p><br><br></p>
<h2 id="五源代码">五、源代码<a hidden class="anchor" aria-hidden="true" href="#五源代码">#</a></h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>


<span class="k">def</span> <span class="nf">load_w2v</span><span class="p">(</span><span class="n">w2v_path</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    Load word2vec model
</span><span class="s2">
</span><span class="s2">    Args:
</span><span class="s2">        w2v_path (str): path of word2vec model
</span><span class="s2">
</span><span class="s2">    Returns:
</span><span class="s2">        model: word2vec model
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loading word2vec model...&#39;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">seedwords</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    According to the seed word file, select the top n words with the most similar semantics and save them in the directory save_dir.
</span><span class="s2">    
</span><span class="s2">    Args:
</span><span class="s2">        wv (Word2VecKeyedVectors): the word embedding model
</span><span class="s2">        seedwords (list): 种子词
</span><span class="s2">        topn (int, optional): Set the number of most similar words to retrieve to topn. Defaults to 100.
</span><span class="s2">        save_dir (str, optional): the directory to save the candidate words. Defaults to &#39;Word2Vec&#39;.
</span><span class="s2">    
</span><span class="s2">    Returns:
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">simidx_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">similars_candidate_idxs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#the candidate words of seedwords</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span>
    <span class="n">seedidxs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#transform word to index</span>
    <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seedwords</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
            <span class="n">seedidx</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">seed</span><span class="p">]</span>
            <span class="n">seedidxs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seedidx</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">seedidx</span> <span class="ow">in</span> <span class="n">seedidxs</span><span class="p">:</span>
        <span class="c1"># sims_words such as [(&#39;by&#39;, 0.99984), (&#39;or&#39;, 0.99982), (&#39;an&#39;, 0.99981), (&#39;up&#39;, 0.99980)]</span>
        <span class="n">sims_words</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">seedidx</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="n">topn</span><span class="p">)</span>
        <span class="c1">#Convert words to index and store them</span>
        <span class="n">similars_candidate_idxs</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">dictionary</span><span class="p">[</span><span class="n">sim</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="k">for</span> <span class="n">sim</span> <span class="ow">in</span> <span class="n">sims_words</span><span class="p">])</span>
    <span class="n">similars_candidate_idxs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">similars_candidate_idxs</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">similars_candidate_idxs</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="n">idx</span><span class="p">],</span> <span class="n">seedidxs</span><span class="p">)</span>
        <span class="n">simidx_scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">score</span><span class="p">))</span>
    <span class="n">simidxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">simidx_scores</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>

    <span class="n">simwords</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">simidxs</span><span class="p">][:</span><span class="n">topn</span><span class="p">]</span>

    <span class="n">resultwords</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">resultwords</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">seedwords</span><span class="p">)</span>
    <span class="n">resultwords</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">simwords</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">resultwords</span>
</code></pre></div><p><br><br></p>
<h2 id="六获取模型">六、获取模型<a hidden class="anchor" aria-hidden="true" href="#六获取模型">#</a></h2>
<p>内容创作不易， 本文为付费内容，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 100元    2001-2022年报 &amp; 管理层讨论与分析

- 100元    cntext-2.1.1-py3-none-any.whl

- 100元   Word2Vec相关模型文件(mda01-22.200.6.bin)

- 200元   
    - 2001-2022年报 &amp; 管理层讨论与分析
    - cntext-2.1.1-py3-none-any.whl  
    - Word2Vec相关模型文件(mda01-22.200.6.bin)
</code></pre></div><p>加微信 372335839， 备注「姓名-学校-专业-word2vec」</p>
<p><br><br></p>
<h2 id="广而告之">广而告之<a hidden class="anchor" aria-hidden="true" href="#广而告之">#</a></h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>


  </div>

  <footer class="post-footer">
      <ul class="post-tags">
        <b>Tags:  &nbsp;</b>
        <li><a href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" target='_blank'>数据集</a></li>
        <li><a href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/" target='_blank'>词向量</a></li>
        <li><a href="/tags/%E7%BB%8F%E6%B5%8E%E7%AE%A1%E7%90%86/" target='_blank'>经济管理</a></li>
        <li><a href="/tags/%E4%BB%98%E8%B4%B9%E5%86%85%E5%AE%B9/" target='_blank'>付费内容</a></li>
      </ul>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on twitter"
        href="https://twitter.com/intent/tweet/?text=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b&amp;url=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f&amp;hashtags=%e6%95%b0%e6%8d%ae%e9%9b%86%2c%e8%af%8d%e5%90%91%e9%87%8f%2c%e7%bb%8f%e6%b5%8e%e7%ae%a1%e7%90%86%2c%e4%bb%98%e8%b4%b9%e5%86%85%e5%ae%b9">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f&amp;title=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b&amp;summary=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b&amp;source=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on reddit"
        href="https://reddit.com/submit?url=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f&title=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on facebook"
        href="https://facebook.com/sharer/sharer.php?u=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on whatsapp"
        href="https://api.whatsapp.com/send?text=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b%20-%20%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型 on telegram"
        href="https://telegram.me/share/url?text=%e8%af%8d%e5%90%91%e9%87%8f%28%e4%bb%98%e8%b4%b9%29%20%7c%20%e4%bd%bf%e7%94%a8MD%26A2001-2022%e8%af%ad%e6%96%99%e8%ae%ad%e7%bb%83Word2Vec%e6%a8%a1%e5%9e%8b&amp;url=%2fblog%2f2023-03-24-load-w2v-and-expand-your-concpet-dicitonary%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
    
</div>




  </footer><script src="https://utteranc.es/client.js"
        repo="hiDaDeng/hidadeng.github.io"
        issue-term="pathname"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
</script>

  
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="/">大邓和他的PYTHON</a></span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'Copy';

        function copyingDone() {
            copybutton.innerText = 'Copied!';
            setTimeout(() => {
                copybutton.innerText = 'Copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>


    
    
</body>

</html>
