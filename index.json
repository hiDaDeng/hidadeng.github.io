[{"content":"文献类 个人感觉本文的精华就在这里了，读几篇文章就能对文本分析在社科（经管）的应用有深刻的了解。\n 读完本文你就了解什么是文本分析 JM2022综述 | 黄金领域: 为营销研究(新洞察)采集网络数据 转载 | 金融学文本大数据挖掘方法与研究进展 转载 | 社会计算驱动的社会科学研究方法 转载 | 大数据驱动的「社会经济地位」分析研究综述 转载 | 国外会计文本信息实证研究述评与展望 文献汇总 | 量化历史学与经济学研究 视频分享 | 文本分析在经济管理研究中的应用 视频分享 | 会计领域中的Python文本分析 视频分享 | Python数据挖掘与金融科技  文本分析在市场营销研究中的应用 近年《管理世界》《管理科学学报》使用文本分析论文 文本相似 | Lazy Prices公司年报内容变动预示重大风险 管理世界 | 使用LM中文金融词典对年报进行语调分析 管理世界 | 使用文本分析\u0026amp;机器学习测量短视主义 管理世界 | 使用 经营讨论与分析 测量 企业数字化 中国工业经济 | MD\u0026amp;A信息含量指标构建代码实现 金融研究 | 央行货币政策文本相似度计算与可视化 营销研究中文本分析应用概述(含案例及代码) 文本可读性研究及应用清单 多维度、细粒度情感词库的核心思想与建设过程概述 使用 textsta t库计算文本可读性 计算文本的语言具体性 | 以JCR2021论文为例 PNAS | 文本网络分析\u0026amp;文化桥梁 Python 代码实现 PNAS | 历史语言记录揭示了近几十年来认知扭曲的激增 PNAS | 情侣分手3个月前就有预兆！聊天记录还能反映分手后遗症 PNAS | 词汇熟悉度对线上参与和资金筹集的预测性效用 PNAS | 使用语义距离测量一个人的创新力(发散思维)得分 词嵌入测量不同群体对某概念的态度(偏见) 转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用 转载 | 从符号到嵌入：计算社会科学的两种文本表示 文献汇总 | 词嵌入 与 社会科学中的偏见(态度) Management Science | 使用网络算法识别创新的颠覆性与否  代码类 本部分选取了与社科相关性较强的文本分析代码或第三方库。\n Python语法入门 | 含视频代码 训练\u0026amp;使用 Glove 语言模型， 可度量刻板印象等 BERTopic库 | 使用预训练模型做话题建模 BERTopic | 使用推特数据构建 动态主题模型模 KeyBERT | 关键词发现库 Top2Vec | 主题建模和语义搜索库 豆瓣影评 | 探索词向量妙处 Python | 词移距离(Word Mover\u0026rsquo;s Distance) 中文金融情感词典 FinBERT | 金融文本BERT模型，可情感分析、识别ESG和FLS类型 如何正确读入文本数据不乱码(解决文本乱码问题) 30天Python编程学习挑战 预训练词向量模型的方法、应用场景、变体延伸与实践总结 天** | 使用selenium做数据采集 百度指数 | 使用qdata采集百度指数 LIWC vs Python | 文本分析之词典词频法略讲(含代码) 欢迎各位向cntext库分享情感词典 cntext库 | 关于DUTIR被污染解决办法 EmoBank | 中文维度情感词典 文本分析 | 中国企业高管团队创新注意力 在会计研究中使用Python进行文本分析 Python与文化分析入门 免费社科类Python编程课程列表 PyPlutchik库 | 可视化文本的情绪轮(情绪指纹) tomotopy库 | 速度最快的LDA主题模型 使用scipy实现层次聚类分析 推荐系统与协同过滤、奇异值分解 cntext库 | 中文情感分析包 Asent库 | 英文文本数据情感分析 sentence-transformer库 | 句子语义向量化 tfidf有权重的情感分析 Shifterator库 | 词移图分辨两文本用词风格差异 Pandas小案例 | 对某公司同年的某指标批量汇总 Label-Studio|多媒体数据标注工具 使用Pandas处理文本数据 doccano|为机器学习建模做数据标注 whatlies库 | 可视化词向量 WordBias库 | 发现偏见(刻板印象)的交互式工具 karateclub库 | 计算社交网络中节点的向量 causalinference库 | 使用Python做因果推断 机器学习实战 | 信用卡欺诈检测 实战 | 构建基于客户细分的 K-Means 聚类算法！ sklearnex库 | 让你的scikit-learn代码加速百倍 nlp-roadmap | 文本分析知识点思维脑图 Maigret库 | 查询某用户名在各平台网站的使用情况 R语言 | ggplot2简明绘图之散点图 R语言 | ggplot2简明绘图之直方图 R语言 | ggplot2简明绘图之动态图 R语言 | 使用posterdown包制作学术会议海报 R语言 | 使用ggsci包绘制sci风格图表 R语言 | ggpubr包让数据可视化更加优雅 R语言 | 让统计更easy的easystats集合包 R语言 | 使用shiny的reactive表达式写应用程序 R语言 | 使用stargazer包输出格式化回归结果 R语言 | 使用word2vec词向量模型 Latex | 为Rmarkdown配置tinytex环境 Faker库 | 生成实验数据 可视化 | 绘制《三体》人物关系网络图 LovelyPlots库 | 格式化科学论文、论文和演示文稿的可视化图形 5个你或许不知道的pandas数据导入技巧  数据集 数据集这块比较少，各位如果有资源，欢迎留言分享或者邮箱thunderhit@qq.com联系我。\n 1850万条 | 世界地图POI兴趣点数据集 1.5G数据集 | 200万条Indiegogo众筹项目信息 12G数据集 | 23w条Kickstarter项目信息 魔搭 | 在线中文AI模型开源社区 数据集 | 90w条中国上市公司高管数据 400M数据集 | 上交所年报excel文件 数据集 | 585w企业工商注册信息 10G数据集 | YelpDaset酒店管理类数据集 17G数据集 | 企业社会责任报告数据集 27G数据集 | 使用Python对27G招股说明书进行文本分析 70G数据集 | 上交所定期报告数据集 14G数据集 | 2007-2021年A股上市公司年度报告（txt文件） ChineseSemanticKB | 中文语义常用词典 DomainWordsDict | 领域词库构建方法与68领域、916万级专业词库分享 小规模金融并购、投资事件图谱设计概述与数据构成解析 中文词向量资源汇总 \u0026amp; 使用方法 NLP资源 | 汽车、金融等9大领域预训练词向量模型下载资源 数据集 | 多语言对齐词向量预训练模型 Google Books Ngram Viewer显示英文词汇历史使用趋势 十万级 | 多领域因果事件对数据集对外开源 历时语料 | 新闻联播文字版等数据源分享   广而告之  长期征稿 长期招募小伙伴 付费视频课程 | Python实证指标构建与文本分析 大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与直播课。 如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的录播课。 如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读有偿说明  ","permalink":"/blog/the_text_analysis_list_about_ms/","summary":"文献类 个人感觉本文的精华就在这里了，读几篇文章就能对文本分析在社科（经管）的应用有深刻的了解。\n 读完本文你就了解什么是文本分析 JM2022综述 | 黄金领域: 为营销研究(新洞察)采集网络数据 转载 | 金融学文本大数据挖掘方法与研究进展 转载 | 社会计算驱动的社会科学研究方法 转载 | 大数据驱动的「社会经济地位」分析研究综述 转载 | 国外会计文本信息实证研究述评与展望 文献汇总 | 量化历史学与经济学研究 视频分享 | 文本分析在经济管理研究中的应用 视频分享 | 会计领域中的Python文本分析 视频分享 | Python数据挖掘与金融科技  文本分析在市场营销研究中的应用 近年《管理世界》《管理科学学报》使用文本分析论文 文本相似 | Lazy Prices公司年报内容变动预示重大风险 管理世界 | 使用LM中文金融词典对年报进行语调分析 管理世界 | 使用文本分析\u0026amp;机器学习测量短视主义 管理世界 | 使用 经营讨论与分析 测量 企业数字化 中国工业经济 | MD\u0026amp;A信息含量指标构建代码实现 金融研究 | 央行货币政策文本相似度计算与可视化 营销研究中文本分析应用概述(含案例及代码) 文本可读性研究及应用清单 多维度、细粒度情感词库的核心思想与建设过程概述 使用 textsta t库计算文本可读性 计算文本的语言具体性 | 以JCR2021论文为例 PNAS | 文本网络分析\u0026amp;文化桥梁 Python 代码实现 PNAS | 历史语言记录揭示了近几十年来认知扭曲的激增 PNAS | 情侣分手3个月前就有预兆！聊天记录还能反映分手后遗症 PNAS | 词汇熟悉度对线上参与和资金筹集的预测性效用 PNAS | 使用语义距离测量一个人的创新力(发散思维)得分 词嵌入测量不同群体对某概念的态度(偏见) 转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用 转载 | 从符号到嵌入：计算社会科学的两种文本表示 文献汇总 | 词嵌入 与 社会科学中的偏见(态度) Management Science | 使用网络算法识别创新的颠覆性与否  代码类 本部分选取了与社科相关性较强的文本分析代码或第三方库。","title":"置顶推荐 | 社科(经管)文本分析快速指南"},{"content":" 文本分析与会计 资料下载   数据挖掘一般会遇到两个难题\n  如何从网络中高效地 采集数据？\n批量下载、汇总、清洗、整理\n  如何从文本数据中抽取文本信息(变量)？\n情感、客观性、主观性、偏见\n    目录 1. 认识Python  学Python的n理由 Python是一门语言 数据类型与语法 如何写Python代码  ​\n2. Text as Data   角色-Producer/Receiver\n  机制-Reflects/Impacts\n  目的-Predict/Understanding\n  方法-定性vs定量\n  文本分析的常用指标\n  3. 文本特征工程  词袋法(文档向量) 词向量 文档向量化 词向量  4. 文本分析指标 5. 文本分析应用案例(会计) \n一、认识Python 1.1 学Python的n理由   简单\n  用户多\n  能做很多有意思的事\n 自动化办公  群发邮件 自动生成报表   网络爬虫  在线秒杀 下载音频、视频pdf报告 明星的微博有新消息后邮箱提醒你   数据分析 可视化 机器学习 物联网 制作网站    1.2 Python是一门语言 There is such a number a, if a is greater than or equal to 0, we will print a; if a is less than 0, we will print -a. \n1.3 数据类型与语法    英语 Python 例如     单词 数据类型 数字、列表、字符串、字典等   语法 逻辑语句 if条件判断语句、for循环语句等       数据类型 例子     数字 age = 25   字符串 intro = \u0026quot;Hello, my name is ...\u0026quot;   列表 my_list = [1, 2, 3]   字典 ages = {'David': 25, 'Mark':30}   空值 None   布尔值 True, False    1.4 如何写Python代码 实现一定功能, 代码一般由三部分组成\n 数据类型 逻辑语句 相应功能Python包(库)  #数据类型 ability = 1 scale = 1.01 records = [] #逻辑语句 days = range(1, 365) for day in days: ability = ability*scale records.append(ability) #相应的库  import matplotlib.pyplot as plt %matplotlib inline plt.figure(figsize=(10, 8)) plt.plot(days, records) plt.title(\u0026#39;Day day Up in one field!\u0026#39;) \n二、Text as Data  2.1 Producer/Receiver 本节讨论的是涉及到文本的形形色色的角色\n文本信息的==producer== 与 ==receiver==，涵盖 ==个人、公司(组织)、国家(社会)==三个层面。\n2.2 Reflects/Impacts  编码解码理论\n文化研究之父斯图亚特·霍尔（Stuart Hall）在《电视话语中的编码和解码》（Encoding and decoding inthe television discourse）一文中提出了“编码解码”理论。\n 编码（encoding），信息传播者将所传递的讯息、意图或观点，转化为具有特定规则的代码。 解码（decoding），信息接受者，将上述代码按特定规则进行解读。   需要注意的是文本的 ==反映Reflects== 和==影响Impacts==并不是非此即彼，往往会同时起作用。\n   \u0026mdash; 研究目的 自变量     Reflects 文本可以反映producer的一些特质，帮助研究者理解producer。\n例如试图挖掘producer的个性personality或隶属于什么社会团体。 了解公司的品牌个性；\n年报含有未来业绩表现的线索；\n消费者们在品牌社区的言语能更深的投射出消费者对品牌的态度；\n而更宏大的层面，文本也能反映出文化差异。\n了解消费者是否喜欢新产品，消费者如何看待品牌，消费者最看重什么   Affects 知道文本如何影响receiver，receiver会有什么样的行为和选择。 检验文本是否以及如何导致消费者诸如购买、分享和卷入行为。\n广告会塑造消费者的消费行为\n消费者杂志会扭曲消费者产品分类感知\n电影剧本会影响观众的反应    2.3 Predict/Understanding 使用文本数据的目的是\n   \u0026mdash; Reflects Affects 目的 应用 难点     Predict 预测 producer的状态、特性、性格等 预测 receiver阅读、分享和购买行为 相比搞清楚作用机制(因果关系)，更关心预测的准确性。 什么消费者最喜欢贷款;\n什么电影会大火;\n未来股市走向;\n 文本数据可以生成成千上万的特征(相当于变量x1，x2\u0026hellip;xn)，而文本数据记录数甚至可能少于特征数。\n为了解决这个为题，使用新的特征分类方法，减少特征数量，又有可能存在拟合问题。   Understanding 为什么当人们压抑的时候会使用特殊人称。 来理解为何带有情绪的文本会更容易被阅读和分享 理解为什么事情发生以及如何发生的这类研究往往会用到心理学、社会学的实证方法，旨在理解某个文本特征会导致什么后续结果，以及为什么产生这样的后果。 消费者怎样表达会如何影响口碑;\n为何某些推文会被挑中分享？\n歌曲为何变火？\n品牌如何让消费者忠诚？ 找出观测数据背后的因果关系。相应的，该领域的工作可能会强调实验数据，以允许对关键的独立变量进行操作。\n另一个挑战是解释文本特征之间的关系。    2.4 定性/定量 经过刚刚定量技术的介绍，现在对定性与定量粗略做个对比。\n   定性/量 分析方法 优点 缺点     定性（text as text） 质性（扎根） 依靠研究者领域知识，可以对少量的数据做出深刻洞见。 难以应对大规模数据；\n编码过程并不能保证唯一；   定量 textual data(text as data) 明显的文本特征，如词频、可阅读性 标准如一;\n适合大规模文本挖掘；\n纷繁复杂中涌现出潜在规律 需要破坏文本的结构，丧失了部分信息量    2.5 文本指标 词典法，对某个词、某类词(词典)的统计个数多少，。特点容易理解，简单，实施性强。\n 数量； 如文本长度(e.g., Godes and Mayzlin 2004; Moe and Trusov2011) 主观性； 情感得分，情感词词典(e.g., Godes and Silva 2012; Moe and Schweidel 2012; Ying, Feinberg and Wedel 2006)· 客观性，如方差、信息墒(e.g., Godes and Mayzlin 2004).  A 产品不错， 包装破损， 态度很好， 综合还是推荐大家购买! [5,1,5,4] B产品垃圾，使用垃圾， 包装破损， 差评!!  [1, 1, 1, 1] A的方差更大，信息量更客观公正。   实体词词频； 例如“电脑”商品的在线评论中“电脑”出现次数会远多于其他词。 可读性；阅读难易程度，根据词典或词的字母数测量 不确定性；经济政策不确定性词典 偏见，态度；将每个词看做向量，对向量进行计算  \n三、文本特征工程 3.1 文档向量化 3.1.1 词袋法 以**词典法(语料中所有词均列入词典)**为基础，文档向量化\n3.1.2 one-hot 与词袋非常类似的算法还有one-hot\n3.1.3 tf-idf 不止考虑出现次数，还要考虑词语出现场景的可诊断性\n3.2 词向量 Docs =[\u0026#34;Mom is a happy woman\u0026#34;, \u0026#34;Dad is happy.!\u0026#34;] 词典中带顺序[Mom, is a happy woman dad]\n   技术 技术 维度类比 任务 例子     字典法（词频） 数个数 原子 统计每句话里的名词个数 sent_num1 = 2\nsent_num2 = 1   词袋法 bag of words\none-hot\nTf-idf 分子 转化为词向量, 计算两个句子相似度。 vec1 = [1, 1, 1, 1, 1, 0]\nvec2 = [0, 1, 0, 1, 0, 1]\nsimilarity = cosine(vec1, vec2)   词嵌入 word2vec、\nglove等 中子、质子、电子 词语相似度。(语义上大小相近，方向相反) mom = [0.2, 0.7, 0.1]dad = [0.3, 0.5, -0.2]    有意思的是，词嵌入Embeddings，尤其是glove，通过一定的向量化运算，可以挖掘出人类留下的认知信息，如态度、偏见等。词嵌入模型训练的方式不同，能做不同的计算。\n3.2.1 按群体 将数据按照producer划分，对每类producer的文本数据分别训练词嵌入模型\n3.2.2 按时间 将时间分为不同时间段，对每个时间段内的文本数据分别训练词嵌入模型\n\n四、技术对比 从左向右，自动化程度越来越高； 相对而言， 后期人工介入的越来越少。\n   技术 描述 优点 缺点 应用领域 Python包     主题分析 人工编码 使用参与者自己的话语或者构念来挖掘数据，对少量文本理解的更深入 属于时间、劳动密集型任务，不适合大规模数据。\n由于不同的编码人员有不同的经历和偏好，编码过程的标准不可靠 社会学、管理学    字典法 统计文本中词语的出现个数(占比) 允许对研究的数据进行定量分析，有标准，规格唯一 采用的词典应尽量与研究问题适应，词典适配性问题突出。情感分析，形容词词典。 管理学 jieba   词袋法 文本向量化 编码标准稳定简单，扩展性强 编码过程忽略词语的先后顺序；舍弃了一些信息量 管理学 jieba\nscikit-learn   监督学习 文本分类 允许事先定义编码规则；逻辑简单 需要高质量的标注数据(工作量大)；特征词太多，训练的模型很容易过拟合。 计算机学、政治学、管理学 scikit-learn   无监督学习 主题建模\nLDA话题模型 在没有人工标注的情况下，加速了数据的“标注”或“分类” “标注”是机器按照数字特征进行的分组，需要研究者解读才可以赋予“标准“意义；训练过程需要大量的调参 计算机学、政治学、管留学 scikit-learn   自然语言处理 考虑词语上下文语境顺序，word2vec、glove等 计算机自动化；可分析语义 大多数模型是人类无法解读的黑箱；\n虽然代码编程量小，但训练代码耗时巨大 计算科学；市场营销；心理学 gensim\n等    \n五、文本分析论文解读 5.1 应用    论文 定性 词典 向量     胡楠, 薛付婧 and 王昊楠, 2021. 管理者短视主义影响企业长期投资吗———基于文本分析和机器学习. 管理世界, 37(5), pp.139-156.  Y Y   Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. The Journal of Finance, 75(3), pp.1371-1415.  Y Y   王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性\u0026ndash;基于 Kickstarter 的实证研究. 管理世界, (5), pp.81-98. Y Y     5.2 分析步骤    步骤 任务 Python     1. 研究问题 Produce/Receive 、 Reflects/Impact\n管理层短视特质x -\u0026gt; 企业资本支出和研发支出y    2. 数据收集 巨潮资讯网； 所有 A 股；\n2007~2018 年年度财务报告文件 Python网络爬虫   3. 设计构念 短视主义词有哪些\n训练word2vec模型，找到”尽快“近义词，\n如、”尽早“、”抓紧“、”力争“、”加紧“ word2vec   4. 测量构念 统计不同年报中MD\u0026amp;A中的短视主义词出现占比 词典法   5. 计量建模 计算 x 与y之间的关系     \n相关文献 冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用.南开管理评论1-27\n沈艳, 陈赟, \u0026amp; 黄卓. (2019). 文本大数据分析在经济学和金融学中的应用: 一个文献综述. 经济学 (季刊), 18(4), 1153-1186.\nBerger, J., Humphreys, A., Ludwig, S., Moe, W.W., Netzer, O. and Schweidel, D.A., 2020. Uniting the tribes: Using text for marketing insight. Journal of Marketing, 84(1), pp.1-25.\nKenneth Benoit. July 16, 2019. “Text as Data: An Overview” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. Handbook of Research Methods in Political Science and International Relations. Thousand Oaks: Sage.\nAnand, V., Bochkay, K., Chychyla, R. and Leone, A.J., 2020. Using Python for text analysis in accounting research. Vic Anand, Khrystyna Bochkay, Roman Chychyla and Andrew Leone (2020),\u0026quot; Using Python for Text Analysis in Accounting Research\u0026quot;, Foundations and Trends® in Accounting, 14(3-4), pp.128-359.\nCohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. The Journal of Finance, 75(3), pp.1371-1415.\n胡楠, 薛付婧 and 王昊楠, 2021. 管理者短视主义影响企业长期投资吗———基于文本分析和机器学习. 管理世界, 37(5), pp.139-156.\n王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性\u0026ndash;基于 Kickstarter 的实证研究. 管理世界, (5), pp.81-98.\nBanks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. “A review of best practice recommendations for text analysis in R (and a user-friendly app).” Journal of Business and Psychology 33, no. 4 (2018): 445-459.\nCohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. The Journal of Finance, 75(3), pp.1371-1415.\n徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/accountingtext/","summary":"文本分析与会计 资料下载   数据挖掘一般会遇到两个难题\n  如何从网络中高效地 采集数据？\n批量下载、汇总、清洗、整理\n  如何从文本数据中抽取文本信息(变量)？\n情感、客观性、主观性、偏见\n    目录 1. 认识Python  学Python的n理由 Python是一门语言 数据类型与语法 如何写Python代码  ​\n2. Text as Data   角色-Producer/Receiver\n  机制-Reflects/Impacts\n  目的-Predict/Understanding\n  方法-定性vs定量\n  文本分析的常用指标\n  3. 文本特征工程  词袋法(文档向量) 词向量 文档向量化 词向量  4. 文本分析指标 5. 文本分析应用案例(会计) \n一、认识Python 1.1 学Python的n理由   简单\n  用户多","title":"置顶推荐 | Python文本分析与会计(视频) "},{"content":"\n肖浩,詹雷,王征.国外会计文本信息实证研究述评与展望[J].外国经济与管理,2016,38(09):93-112.\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2023-01-12-review_about_accounting_text_mining/","summary":"\n肖浩,詹雷,王征.国外会计文本信息实证研究述评与展望[J].外国经济与管理,2016,38(09):93-112.\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"转载 | 国外会计文本信息实证研究述评与展望"},{"content":"如果您以前曾在 Python 中使用过 Pandas，您就会知道可以通过多种不同的方式导入表格数据。\nPandas 可能是使用最广泛的用于加载、操作和导出数据的 Python 包之一。\n虽然您可能熟悉使用 pandas 读取和写入数据的许多不同方法，但您可能没有意识到一些可能并不像您想象的那么明显的快捷方式/技巧。 这可以使导入数据变得更加容易和快速。这篇博文仅探讨了其中的五种技术.\n1. 从url导入csv 您可能非常熟悉 pd.read_csv() 方法可以导入csv文件的， 其实ta还可以通过 HTTP 从 URL 读取数据。\n网络访问 CSV 文件，而不是在本地存储所有文件， 可以省去手动下载内容的麻烦。\nimport pandas as pd url = \u0026#39;csv文件的网络连接\u0026#39; df = pd.read_csv(url) \n2. 导入html网页中的表格 如果您曾经阅读过 pandas 文档，您可能会遇到 pd.read_html() 方法。 与 pd.read_csv() 非常相似，它也具有从 URL 导出数据的附加功能。 让这更酷的是，您可以提供一个包含许多表格的网站的 URL，它会抓取所有表格。\n例如，考虑维基百科文章“英国经济”。 这篇文章可以被抓取如下。\nurl = \u0026#39;https://en.wikipedia.org/wiki/Economy_of_the_United_Kingdom\u0026#39; df_tables = pd.read_html(url) \n3. JSON规范化 有时，当您处理 JSON 数据时， JSON 内往往包含多层嵌套。 对于要转换为表格数据的 JSON 对象，它们需要被展平（嵌套深度为 1）。\npandas 提供了一种使用 pd.json_normalize() 方法来执行此操作的方法。例如，考虑以下 JSON 对象（在 pandas 文档中用作示例）\ndata = [ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Cole Volk\u0026#34;, \u0026#34;fitness\u0026#34;: {\u0026#34;height\u0026#34;: 130, \u0026#34;weight\u0026#34;: 60}, }, {\u0026#34;name\u0026#34;: \u0026#34;Mark Reg\u0026#34;, \u0026#34;fitness\u0026#34;: {\u0026#34;height\u0026#34;: 130, \u0026#34;weight\u0026#34;: 60}}, { \u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Faye Raker\u0026#34;, \u0026#34;fitness\u0026#34;: {\u0026#34;height\u0026#34;: 130, \u0026#34;weight\u0026#34;: 60}, }, ] data Run\n[{'id': 1, 'name': 'Cole Volk', 'fitness': {'height': 130, 'weight': 60}}, {'name': 'Mark Reg', 'fitness': {'height': 130, 'weight': 60}}, {'id': 2, 'name': 'Faye Raker', 'fitness': {'height': 130, 'weight': 60}}]  data包含多层嵌套对象的属性（在本例中为“健身”）。 使用 pd.json_normalize() 可以折叠成表格。\n注意：可以添加可选参数 max_level 以指定要折叠的最大嵌套级别数。 默认情况下，它将标准化所有级别。\ndf = pd.json_normalize(data) df Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id name fitness.height fitness.weight     0 1.0 Cole Volk 130 60   1 NaN Mark Reg 130 60   2 2.0 Faye Raker 130 60     4. 从剪贴板导入 pd.read_clipboard() 方法可用于捕获存储在剪贴板上的任何数据。\n默认情况下，此方法接受正则表达式 \\s+ 作为分隔值的分隔符。 或者，您可以使用自己的正则表达式。\ndf = pd.read_clipboard(sep=\u0026#39;\\\\s+\u0026#39;) \n5. 从Excel导入 假设有人向您发送了一些存储在 excel 电子表格中的数据，可以使用 pd.read_excel() 方法读取数据，就好像它是一个简单的 CSV 文件一样。\n要记住的一件事是，电子表格软件允许用户使用单个文件中的工作表来分割多个电子表格。 使用 sheet_name 参数可以实现从 特定工作表 访问数据。\ndf = pd.read_excel(\u0026#39;my_spreadsheet.xlsx\u0026#39;, sheet_name=\u0026#39;Sheet 1\u0026#39;) \n广而告之  长期征稿 长期招募小伙伴 支持开票 | Python实证指标构建与文本分析  ","permalink":"/blog/2023-01-08-pandas-5-trips-you-may-or-not-may-know/","summary":"如果您以前曾在 Python 中使用过 Pandas，您就会知道可以通过多种不同的方式导入表格数据。\nPandas 可能是使用最广泛的用于加载、操作和导出数据的 Python 包之一。\n虽然您可能熟悉使用 pandas 读取和写入数据的许多不同方法，但您可能没有意识到一些可能并不像您想象的那么明显的快捷方式/技巧。 这可以使导入数据变得更加容易和快速。这篇博文仅探讨了其中的五种技术.\n1. 从url导入csv 您可能非常熟悉 pd.read_csv() 方法可以导入csv文件的， 其实ta还可以通过 HTTP 从 URL 读取数据。\n网络访问 CSV 文件，而不是在本地存储所有文件， 可以省去手动下载内容的麻烦。\nimport pandas as pd url = \u0026#39;csv文件的网络连接\u0026#39; df = pd.read_csv(url) \n2. 导入html网页中的表格 如果您曾经阅读过 pandas 文档，您可能会遇到 pd.read_html() 方法。 与 pd.read_csv() 非常相似，它也具有从 URL 导出数据的附加功能。 让这更酷的是，您可以提供一个包含许多表格的网站的 URL，它会抓取所有表格。\n例如，考虑维基百科文章“英国经济”。 这篇文章可以被抓取如下。\nurl = \u0026#39;https://en.wikipedia.org/wiki/Economy_of_the_United_Kingdom\u0026#39; df_tables = pd.read_html(url) \n3. JSON规范化 有时，当您处理 JSON 数据时， JSON 内往往包含多层嵌套。 对于要转换为表格数据的 JSON 对象，它们需要被展平（嵌套深度为 1）。","title":"5个你或许不知道的pandas数据导入技巧"},{"content":"信息含量 由于每个公司的 MD\u0026amp;A 中不仅包括公司经营状况等历史信息， 也包括与其他公司相似的信息， 如外部环境、市场格局、风险因素等内容。 因此， 本文参考 Hanley and Hoberg （ 2010 ）， 从行业和市场两个维度来考察和定义公司 MD\u0026amp;A 中的信息含量。\n 市场因素， 所有上市公司都处于相同的宏观经济环境、风险因素和政治、政策背景之下； 行业因素， 同一行业中的各上市公司又面临着相似的产业政策、竞争环境和市场特征。  由此可见， 每个上市公司 MD\u0026amp;A 信息不可避免地在某种程度上与同行业其他上市公司以及市场其他行业上市公司存在一定的相似性， 甚至某些公司可能直接参考其他公司 MD\u0026amp;A 的表述。 可以将与行业其他公司或其他行业的公司重复或相似的信息定义为不具有信息含量的内容，同时将不同的信息定义为真正具有信息含量的内容，简称为信息含量。\n 孟庆斌, 杨俊华, and 鲁冰. \u0026ldquo;管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究.\u0026rdquo; 中国工业经济 12 (2017): 132-150.\n 摘要 本文采用文本向量化的方法， 对 2007—2015 年中国 A 股上市公司年报的管理层讨论与分析（MD\u0026amp;A）所披露的信息含量加以度量， 研究其对股价崩盘风险的影响。 研究发现， MD\u0026amp;A 的信息含量越高，未来股价崩盘风险越低。 将 MD\u0026amp;A 进一步划分为回顾部分和展望部分后发现，仅有展望部分中的信息含量能够显著降低未来股价崩盘风险。 在控制内生性问题之后，本文的结论依然成立。 本文还分别从文本可读性和信息不对称的角度出发，研究它们对二者关系的影响。 结果表明，信息的可读性越高，信息不对称程度越高，展望部分的信息含量对股价崩盘风险的降低作用越大。 在重新定义股价崩盘风险的计算区间以及控制股价同步性之后， MD\u0026amp;A 展望部分的信息含量依然能够显著降低股价崩盘风险， 表明本文的结论是稳健的。 本文从文本信息的角度丰富了股价崩盘风险影响因素的研究， 同时也从增量信息的角度完善了 MD\u0026amp;A 信息有用性的研究，具有重要的理论和现实意义。\n样本选择和处理 本文选取 2007 — 2015 年中国上市公司年报中的 MD\u0026amp;A 信息作为研究样本。 之所以选取 2007 年作为样本的起点， 是因为从 2007 年开始， MD\u0026amp;A 在企业定期报告中的披露要求已经较为完善， 而且 2007 年是中国会计准则国际趋同的重要时点， 新制定的《企业会计准则》已经开始实施， 为避免前后会计准则差异而产生的影响， 因此选取 2007 年作为样本区间的起点。\n本文所使用的上市公司年度报告均来自于巨潮资讯网。 数据处理过程如下：\n（ 1 ）剔除金融行业、 ST 和 *ST 类企业， 以及上市时间不足一年的企业。 （ 2 ）从 MD\u0026amp;A 的内容中分别提取回顾和展望部分， 保存为回顾信息文件和展望信息文件， 部分无法抓取出的年报通过手工收集处理。 （ 3 ）文本处理-文本向量化。 借鉴 Hanley and Hoberg （ 2010 ）的研究思路， 将每个 MD\u0026amp;A 文本通过向量的 形式表示出来， 其每个元素为文本中的每个词语出现的频率。 例如， 假设某 MD\u0026amp;A 文本中包含 10000 个词， 则该文本对应一个 10000×1 维的向量。 举一个简单的例子来描述文本向量化的过程： 在两个简化的 MD\u0026amp;A 文本中， 一个包含“我们生产土豆和生产玉米”， 另一个包含“我们生产家具”， 剔除连词“和”、代词“我们”之后， 只剩下“生产”、“土豆”、“玉米”、“家具”这 4 个词。 那么， 在第一个 MD\u0026amp;A 文本中， “生产”、“土豆”和“玉米”分别出现了 2 次、 1 次和 1 次， 而“家具”出现 0 次， 所以该 文本的向量为 {2 ， 1 ， 1 ， 0} ， 同样得到第二个文本的向量为 {1 ， 0 ， 0 ， 1} 。 （ 4 ）向量标准化。 对于向量化的文本， 仍需解决文本长度不同导致的结果不可比问题。 一般来说， 某一个词在长文本中重复出现的次数较多， 在短文本中重复出现的次数较少， 但并不能因此说 长文本比短文本的信息量大。 为此， 本文进一步将这些向量进行标准化处理， 即将该向量除以文本 中单词的总数， 得到标准化后的向量。 在上面的例子中， 两个公司的标准化之后的向量就成为了 {0.50 ， 0.25 ， 0.25 ， 0} 和 {0.50 ， 0 ， 0 ， 0.50} 。 \n文件目录 管理层讨论信息含量/ ├── 信息含量计算代码.ipynb ├── data/ │ ├── 行业代码.xlsx │ └── mda10-20.xlsx ├── mda_infor.csv ├── output/ │ └── 2020/ │ ├── 000002.csv │ ├── 000004.csv │ ├── 000005.csv │ ├── 000006.csv │ ├── ... │ └── 2019/ │ ├── 000002.csv │ ├── 000004.csv │ ├── 000005.csv │ ├── 000006.csv │ ├── ... │ └── 2018/ │ ├── 000002.csv │ ├── 000004.csv │ ├── 000005.csv │ ├── 000006.csv │ ├── 000006.csv │ ├── ... │ └── ... \n一、导入数据 这里准备了2010-2020年A股经营讨论与分析内容和行业代码数据。\nimport pandas as pd # converters 强制声明该列为字符串， 防止股票代码 被程序识别为数字， # 完整数据370+M  df = pd.read_excel(\u0026#39;data/mda10-20.xlsx\u0026#39;, converters={\u0026#39;股票代码\u0026#39;: str}) #上市公司行业信息 ind_info_df = pd.read_excel(\u0026#39;data/行业代码.xlsx\u0026#39;, converters={\u0026#39;股票代码\u0026#39;: str}) df = pd.merge(df, ind_info_df, on=[\u0026#39;股票代码\u0026#39;, \u0026#39;会计年度\u0026#39;], how=\u0026#39;inner\u0026#39;) #显示前5行 df.head() Run\n# 剔除金融行业处理 df = df[~df[\u0026#39;行业代码\u0026#39;].str.contains(\u0026#34;J\u0026#34;)] df = df[~df[\u0026#39;公司简称\u0026#39;].str.contains(\u0026#34;ST\u0026#34;)] df.head() Run\n\n二、以2020年为例 写代码先局部后整体，以2020年为例，如果2020年可以成功计算出信息含量，则可以for循环推广到所有股票所有年份。本章节需要做\n 选定某年份，以2020年为例 定义transform函数，用于处理「经营讨论与分析内容」字段内的内容。 文本向量化，向量标准化。  2.1 选定2020年 df_per_year = df[df[\u0026#39;会计年度\u0026#39;]==2020] df_per_year.reset_index(inplace=True) df_per_year.head() Run\n2.2 定义transform函数 定义transform函数，该函数可以处理「经营讨论与分析内容」字段内容，使其:\n 只保留中文内容 剔除停用词 整理为用空格间隔的字符串(类西方语言文本格式)  之后应用 transform函数， 使用apply方法， 处理 df_per_year[\u0026lsquo;经营讨论与分析内容\u0026rsquo;] 。\nimport cntext as ct import jieba import re stopwords = ct.load_pkl_dict(\u0026#39;STOPWORDS.pkl\u0026#39;)[\u0026#39;STOPWORDS\u0026#39;][\u0026#39;chinese\u0026#39;] def transform(text): #只保留md\u0026amp;a中的中文内容 text = \u0026#39;\u0026#39;.join(re.findall(\u0026#39;[\\u4e00-\\u9fa5]+\u0026#39;, text)) #剔除停用词 words = [w for w in jieba.cut(text) if w not in stopwords] #整理为用空格间隔的字符串(类西方语言文本格式) return \u0026#39; \u0026#39;.join(words) df_per_year[\u0026#39;clean_text\u0026#39;] = df_per_year[\u0026#39;经营讨论与分析内容\u0026#39;].apply(transform) Building prefix dict from the default dictionary ... Loading model from cache /var/folders/sc/3mnt5tgs419_hk7s16gq61p80000gn/T/jieba.cache Loading model cost 0.556 seconds. Prefix dict has been built successfully.  2.3 文本向量化 本小节要做:\n 文本向量化 向量标准化 合并多个字段为新的df  先将df_per_year[\u0026lsquo;clean_text\u0026rsquo;] 向量化，代码如下\nfrom sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer(min_df=0.05, max_df=0.5) # 生成稀疏bow矩阵 dtm_per_year = cv.fit_transform(df_per_year[\u0026#39;clean_text\u0026#39;]) dtm_per_year = pd.DataFrame(dtm_per_year.toarray()) dtm_per_year Run\nimport numpy as np #向量标准化 dtm_per_year = dtm_per_year.apply(lambda row: row/np.sum(row), axis=1) dtm_per_year Run\n#合并多个字段为新的df dtm_per_year = pd.concat([df_per_year[[\u0026#39;股票代码\u0026#39;, \u0026#39;会计年度\u0026#39;, \u0026#39;行业代码\u0026#39;]], dtm_per_year], axis=1) dtm_per_year.head() Run\n\n三、计算2020年行业向量、市场向量 计算2020年所有公司的市场向量、行业向量。这里\nimport os import pandas as pd for idx in range(len(dtm_per_year)): code = dtm_per_year.loc[idx, \u0026#39;股票代码\u0026#39;] ind = dtm_per_year.loc[idx, \u0026#39;行业代码\u0026#39;] year = dtm_per_year.loc[idx, \u0026#39;会计年度\u0026#39;] ind_freq = dtm_per_year[dtm_per_year[\u0026#39;行业代码\u0026#39;]==ind][dtm_per_year[\u0026#39;股票代码\u0026#39;]!=code].iloc[:, 3:].mean(axis=0) market_freq = dtm_per_year[dtm_per_year[\u0026#39;行业代码\u0026#39;]!=ind].iloc[:, 3:].mean(axis=0) dtm_per_year_melted = dtm_per_year.melt(id_vars=[\u0026#39;股票代码\u0026#39;, \u0026#39;会计年度\u0026#39;, \u0026#39;行业代码\u0026#39;], var_name=\u0026#39;word_id\u0026#39;, value_name=\u0026#39;word_freq\u0026#39;) corporate_df = pd.DataFrame({\u0026#39;word_id\u0026#39;: dtm_per_year_melted[dtm_per_year_melted[\u0026#39;股票代码\u0026#39;]==code][\u0026#39;word_id\u0026#39;].values, \u0026#39;word_freq\u0026#39;: dtm_per_year_melted[dtm_per_year_melted[\u0026#39;股票代码\u0026#39;]==code][\u0026#39;word_freq\u0026#39;].values, \u0026#39;ind_freq\u0026#39;: ind_freq, \u0026#39;market_freq\u0026#39;:market_freq}) corporate_df[\u0026#39;股票代码\u0026#39;] = code corporate_df[\u0026#39;行业代码\u0026#39;] = ind corporate_df[\u0026#39;会计年度\u0026#39;] = year corporate_df.reset_index(inplace=True) corporate_df = corporate_df[[\u0026#39;股票代码\u0026#39;, \u0026#39;行业代码\u0026#39;, \u0026#39;会计年度\u0026#39;, \u0026#39;word_id\u0026#39;, \u0026#39;word_freq\u0026#39;, \u0026#39;ind_freq\u0026#39;, \u0026#39;market_freq\u0026#39;]] if not os.path.exists(\u0026#39;output/{year}\u0026#39;.format(year=year)): os.mkdir(\u0026#39;output/{year}\u0026#39;.format(year=year)) corporate_df.to_csv(\u0026#39;output/{year}/{code}.csv\u0026#39;.format(year=year, code=code), index=False) \n四、计算2010-2020年所有公司行业向量、市场向量 信息含量的定义。 由于每个公司的 MD\u0026amp;A 中不仅包括公司经营状况等历史信息， 也包括与其他公司相似的信息， 如外部环境、市场格局、风险因素等内容。 因此， 本文参考 Hanley and Hoberg （ 2010 ）， 从行业和市场两个维度来考察和定义公司 MD\u0026amp;A 中的信息含量。\n 市场因素， 所有上市公司都处于相同的宏观经济环境、风险因素和政治、政策背景之下； 行业因素， 同一行业中的各上市公司又面临着相似的产业政策、竞争环境和市场特征。  由此可见， 每个上市公司 MD\u0026amp;A 信息不可避免地在某种程度上与同行业其他上市公司以及市场其他行业上市公司存在一定的相似性， 甚至某些公司可能直接参考其他公司 MD\u0026amp;A 的表述。\n参考文中截图行业向量、市场向量计算方法，有如下代码。该部分代码运行较慢，全部运行下来大约10小时。\nfrom sklearn.feature_extraction.text import CountVectorizer import numpy as np import pandas as pd import cntext as ct import jieba import os import re stopwords = ct.load_pkl_dict(\u0026#39;STOPWORDS.pkl\u0026#39;)[\u0026#39;STOPWORDS\u0026#39;][\u0026#39;chinese\u0026#39;] def transform(text): #只保留md\u0026amp;a中的中文内容 text = \u0026#39;\u0026#39;.join(re.findall(\u0026#39;[\\u4e00-\\u9fa5]+\u0026#39;, text)) #剔除停用词 words = [w for w in jieba.cut(text) if w not in stopwords] #整理为用空格间隔的字符串(类西方语言文本格式) return \u0026#39; \u0026#39;.join(words) # converters 强制声明该列为字符串， 防止股票代码 被程序识别为数字， df = pd.read_excel(\u0026#39;data/mda10-20.xlsx\u0026#39;, converters={\u0026#39;股票代码\u0026#39;: str}) #上市公司行业信息 ind_info_df = pd.read_excel(\u0026#39;data/行业代码.xlsx\u0026#39;, converters={\u0026#39;股票代码\u0026#39;: str}) df = pd.merge(df, ind_info_df, on=[\u0026#39;股票代码\u0026#39;, \u0026#39;会计年度\u0026#39;], how=\u0026#39;inner\u0026#39;) # 剔除金融行业处理 df = df[~df[\u0026#39;行业代码\u0026#39;].str.contains(\u0026#34;J\u0026#34;)] df = df[~df[\u0026#39;公司简称\u0026#39;].str.contains(\u0026#34;ST\u0026#34;)] for year in [2010, 2011, 2012, 2013]: #for year in set(df[\u0026#39;会计年度\u0026#39;].values): df_per_year = df[df[\u0026#39;会计年度\u0026#39;]==year] df_per_year.reset_index(inplace=True) df_per_year[\u0026#39;clean_text\u0026#39;] = df_per_year[\u0026#39;经营讨论与分析内容\u0026#39;].apply(transform) cv = CountVectorizer(min_df=0.05, max_df=0.5) # 生成稀疏bow矩阵 dtm_per_year = cv.fit_transform(df_per_year[\u0026#39;clean_text\u0026#39;]) dtm_per_year = pd.DataFrame(dtm_per_year.toarray()) dtm_per_year = dtm_per_year.apply(lambda row: row/np.sum(row), axis=1) dtm_per_year = pd.concat([df_per_year[[\u0026#39;股票代码\u0026#39;, \u0026#39;会计年度\u0026#39;, \u0026#39;行业代码\u0026#39;]], dtm_per_year], axis=1) for idx in range(len(dtm_per_year)): code = dtm_per_year.loc[idx, \u0026#39;股票代码\u0026#39;] ind = dtm_per_year.loc[idx, \u0026#39;行业代码\u0026#39;] year = dtm_per_year.loc[idx, \u0026#39;会计年度\u0026#39;] ind_freq = dtm_per_year[dtm_per_year[\u0026#39;行业代码\u0026#39;]==ind][dtm_per_year[\u0026#39;股票代码\u0026#39;]!=code].iloc[:, 3:].mean(axis=0) market_freq = dtm_per_year[dtm_per_year[\u0026#39;行业代码\u0026#39;]!=ind].iloc[:, 3:].mean(axis=0) dtm_per_year_melted = dtm_per_year.melt(id_vars=[\u0026#39;股票代码\u0026#39;, \u0026#39;会计年度\u0026#39;, \u0026#39;行业代码\u0026#39;], var_name=\u0026#39;word_id\u0026#39;, value_name=\u0026#39;word_freq\u0026#39;) corporate_df = pd.DataFrame({ \u0026#39;word_id\u0026#39;: dtm_per_year_melted[dtm_per_year_melted[\u0026#39;股票代码\u0026#39;]==code][\u0026#39;word_id\u0026#39;].values, \u0026#39;word_freq\u0026#39;: dtm_per_year_melted[dtm_per_year_melted[\u0026#39;股票代码\u0026#39;]==code][\u0026#39;word_freq\u0026#39;].values, \u0026#39;ind_freq\u0026#39;: ind_freq, \u0026#39;market_freq\u0026#39;:market_freq}) corporate_df[\u0026#39;股票代码\u0026#39;] = code corporate_df[\u0026#39;行业代码\u0026#39;] = ind corporate_df[\u0026#39;会计年度\u0026#39;] = year corporate_df.reset_index(inplace=True) corporate_df = corporate_df[[\u0026#39;股票代码\u0026#39;, \u0026#39;行业代码\u0026#39;, \u0026#39;会计年度\u0026#39;, \u0026#39;word_id\u0026#39;, \u0026#39;word_freq\u0026#39;, \u0026#39;ind_freq\u0026#39;, \u0026#39;market_freq\u0026#39;]] if not os.path.exists(\u0026#39;output/{year}\u0026#39;.format(year=year)): os.mkdir(\u0026#39;output/{year}\u0026#39;.format(year=year)) corporate_df.to_csv(\u0026#39;output/{year}/{code}.csv\u0026#39;.format(year=year, code=code), index=False) \n五、标准信息、信息含量 以2020年000002为例，计算其标准信息、信息含量。计算成功后，再计算所有年份所有上市公司 md\u0026amp;a的标准信息、信息含量。\n原文除了计算md\u0026amp;a，还将md\u0026amp;a区分为回顾过去、展望未来两部分，并分别计算了对应的标准信息、信息含量。这里只计算md\u0026amp;a的标准信息、信息含量。\n这里使用Python的统计模型statsmodels库OLS来计算标准信息和信息含量。\nimport pandas as pd csv_df = pd.read_csv(\u0026#39;output/2020/000002.csv\u0026#39;, converters={\u0026#39;股票代码\u0026#39;: str}) csv_df.head() Run\n#更改字段名 csv_df.columns = [\u0026#39;股票代码\u0026#39;, \u0026#39;行业代码\u0026#39;, \u0026#39;会计年度\u0026#39;, \u0026#39;word_id\u0026#39;, \u0026#39;Norm\u0026#39;, \u0026#39;Norm_Ind\u0026#39;, \u0026#39;Norm_Market\u0026#39;] csv_df.head() Run import statsmodels.formula.api as smf #因变量Norm #解释变量Norm_Ind、 Norm_Market formula = \u0026#39;Norm ~ Norm_Ind + Norm_Market\u0026#39; model = smf.ols(formula, data=csv_df) result = model.fit() print(result.summary()) Run\nOLS Regression Results ============================================================================== Dep. Variable: Norm R-squared: 0.319 Model: OLS Adj. R-squared: 0.318 Method: Least Squares F-statistic: 763.9 Date: Fri, 06 Jan 2023 Prob (F-statistic): 6.88e-273 Time: 06:37:27 Log-Likelihood: 17334. No. Observations: 3269 AIC: -3.466e+04 Df Residuals: 3266 BIC: -3.464e+04 Df Model: 2 Covariance Type: nonrobust =============================================================================== coef std err t P\u0026gt;|t| [0.025 0.975] ------------------------------------------------------------------------------- Intercept 1.164e-05 2.86e-05 0.407 0.684 -4.44e-05 6.77e-05 Norm_Ind 0.7486 0.020 37.460 0.000 0.709 0.788 Norm_Market 0.2133 0.064 3.327 0.001 0.088 0.339 ============================================================================== Omnibus: 4804.262 Durbin-Watson: 2.026 Prob(Omnibus): 0.000 Jarque-Bera (JB): 4165802.983 Skew: 8.425 Prob(JB): 0.00 Kurtosis: 177.069 Cond. No. 3.05e+03 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 3.05e+03. This might indicate that there are strong multicollinearity or other numerical problems. \n#标准信息 standard_info = result.params.Norm_Ind + result.params.Norm_Market #信息含量 informative_content = sum(abs(result.resid)) print(\u0026#39;000002标准信息: {}\u0026#39;.format(standard_info)) print(\u0026#39;000002信息含量: {}\u0026#39;.format(informative_content)) Run\n000002标准信息: 0.9619640977801796 000002信息含量: 1.2750713760886476 \n既然能成功计算某年某公司的标准信息、信息含量，现在推广到所有年份所有公司，计算结果存储为一个csv文件。\nimport os import csv import statsmodels.formula.api as smf import re #结果存储到mda_infor.csv with open(\u0026#39;mda_infor.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) as csvf: fieldnames = [\u0026#39;股票代码\u0026#39;, \u0026#39;会计年度\u0026#39;, \u0026#39;标准信息\u0026#39;, \u0026#39;信息含量\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() year_dirs = os.listdir(\u0026#39;output\u0026#39;) year_dirs = [y for y in year_dirs if \u0026#39;DS\u0026#39; not in y] for year_dir in year_dirs: code_csvfs = [\u0026#39;output/{year}/{csvf}\u0026#39;.format(year=year_dir, csvf=f) for f in os.listdir(\u0026#39;output/{}\u0026#39;.format(year_dir))] code_csvfs = [f for f in code_csvfs if \u0026#39;DS\u0026#39; not in f] for csvf in code_csvfs: try: csv_df = pd.read_csv(csvf, converters={\u0026#39;股票代码\u0026#39;: str}) csv_df.columns = [\u0026#39;股票代码\u0026#39;, \u0026#39;行业代码\u0026#39;, \u0026#39;会计年度\u0026#39;, \u0026#39;word_id\u0026#39;, \u0026#39;Norm\u0026#39;, \u0026#39;Norm_Ind\u0026#39;, \u0026#39;Norm_Market\u0026#39;] formula = \u0026#39;Norm ~ Norm_Ind + Norm_Market\u0026#39; model = smf.ols(formula, data=csv_df) result = model.fit() #标准信息 standard_info = result.params.Norm_Ind + result.params.Norm_Market #信息含量 informative_content = sum(abs(result.resid)) data = {\u0026#39;股票代码\u0026#39;: re.findall(\u0026#39;\\d{6}\u0026#39;, csvf)[0], \u0026#39;会计年度\u0026#39;: re.findall(\u0026#39;\\d{4}\u0026#39;, csvf)[0], \u0026#39;标准信息\u0026#39;: standard_info, \u0026#39;信息含量\u0026#39;: informative_content} writer.writerow(data) except: pass \n最后 读取生成的 \u0026lsquo;mda_infor.csv\u0026rsquo; 文件，欣赏一下 标准信息、信息含量\nimport pandas as pd df = pd.read_csv(\u0026#39;mda_infor.csv\u0026#39;, converters={\u0026#39;股票代码\u0026#39;: str}) df.head() Run\n需要注意，原文选取 2007 — 2015 年中国上市公司年报中的 MD\u0026amp;A 信息作为研究样本。 之所以选取 2007 年作为样本的起点， 是因为从 2007 年开始， MD\u0026amp;A 在企业定期报告中的披露要求已经较为完善， 而且 2007 年是中国会计准则国际趋同的重要时点， 新制定的《企业会计准则》已经开始实施， 为避免前后会计准则差异而产生的影响， 因此选取 2007 年作为样本区间的起点。\nmda_infor.csv含有2010-2020年的数据，如要复现原文，需要注意筛选数据。\nprint(\u0026#39;mda_infor.csv记录数:\u0026#39;,len(df)) Run\nmda_infor.csv记录数: 30173 \n数据代码获取 撰写本文代码耗时一个工作日，程序运行又额外耗时 10 小时。原创不易，请勿白嫖，转发集赞 50+，保留 2 小时，加微信 372335839， 备注「姓名-学校-专业-信息含量」。已购买 支持开票 | Python实证指标构建与文本分析 课程学员，可私信大邓，直接获取本文数据、代码。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2023-01-06-mda_informative_content/","summary":"信息含量 由于每个公司的 MD\u0026amp;A 中不仅包括公司经营状况等历史信息， 也包括与其他公司相似的信息， 如外部环境、市场格局、风险因素等内容。 因此， 本文参考 Hanley and Hoberg （ 2010 ）， 从行业和市场两个维度来考察和定义公司 MD\u0026amp;A 中的信息含量。\n 市场因素， 所有上市公司都处于相同的宏观经济环境、风险因素和政治、政策背景之下； 行业因素， 同一行业中的各上市公司又面临着相似的产业政策、竞争环境和市场特征。  由此可见， 每个上市公司 MD\u0026amp;A 信息不可避免地在某种程度上与同行业其他上市公司以及市场其他行业上市公司存在一定的相似性， 甚至某些公司可能直接参考其他公司 MD\u0026amp;A 的表述。 可以将与行业其他公司或其他行业的公司重复或相似的信息定义为不具有信息含量的内容，同时将不同的信息定义为真正具有信息含量的内容，简称为信息含量。\n 孟庆斌, 杨俊华, and 鲁冰. \u0026ldquo;管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究.\u0026rdquo; 中国工业经济 12 (2017): 132-150.\n 摘要 本文采用文本向量化的方法， 对 2007—2015 年中国 A 股上市公司年报的管理层讨论与分析（MD\u0026amp;A）所披露的信息含量加以度量， 研究其对股价崩盘风险的影响。 研究发现， MD\u0026amp;A 的信息含量越高，未来股价崩盘风险越低。 将 MD\u0026amp;A 进一步划分为回顾部分和展望部分后发现，仅有展望部分中的信息含量能够显著降低未来股价崩盘风险。 在控制内生性问题之后，本文的结论依然成立。 本文还分别从文本可读性和信息不对称的角度出发，研究它们对二者关系的影响。 结果表明，信息的可读性越高，信息不对称程度越高，展望部分的信息含量对股价崩盘风险的降低作用越大。 在重新定义股价崩盘风险的计算区间以及控制股价同步性之后， MD\u0026amp;A 展望部分的信息含量依然能够显著降低股价崩盘风险， 表明本文的结论是稳健的。 本文从文本信息的角度丰富了股价崩盘风险影响因素的研究， 同时也从增量信息的角度完善了 MD\u0026amp;A 信息有用性的研究，具有重要的理论和现实意义。\n样本选择和处理 本文选取 2007 — 2015 年中国上市公司年报中的 MD\u0026amp;A 信息作为研究样本。 之所以选取 2007 年作为样本的起点， 是因为从 2007 年开始， MD\u0026amp;A 在企业定期报告中的披露要求已经较为完善， 而且 2007 年是中国会计准则国际趋同的重要时点， 新制定的《企业会计准则》已经开始实施， 为避免前后会计准则差异而产生的影响， 因此选取 2007 年作为样本区间的起点。","title":"中国工业经济 | MD\u0026A信息含量指标构建代码实现"},{"content":"\n么晓明, 丁世昌, 赵涛, 黄宏, 罗家德, and 傅晓明. \u0026ldquo;大数据驱动的社会经济地位分析研究综述.\u0026rdquo; 计算机科学 49, no. 4 (2022): 80-87.\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-12-30-review-about-socioeconomic-status-analysis/","summary":"\n么晓明, 丁世昌, 赵涛, 黄宏, 罗家德, and 傅晓明. \u0026ldquo;大数据驱动的社会经济地位分析研究综述.\u0026rdquo; 计算机科学 49, no. 4 (2022): 80-87.\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"转载 | 大数据驱动的「社会经济地位」分析研究综述"},{"content":"LovelyPlots库 可以很好地格式化科学论文、论文和演示文稿的可视化图形，同时使它们在 Adobe Illustrator 中完全可编辑。\n此外，.svg 导出选项允许图形自动调整其字体以适应您文档的字体。 例如，导入到 .tex 文件中的 .svg 图形将自动生成为您的 .tex 文件中使用的文本字体。\n安装 pip3 install LovelyPlots \n样例 代码 import matplotlib.pyplot as plt import numpy as np def MB_speed(v, m, T): \u0026#34;\u0026#34;\u0026#34;Maxwell-Boltzmann speed distribution for speeds\u0026#34;\u0026#34;\u0026#34; kB = 1.38e-23 return ( (m / (2 * np.pi * kB * T)) ** 1.5 * 4 * np.pi * v**2 * np.exp(-m * v**2 / (2 * kB * T)) ) def plot_dist( temperatures, v, mass=85 * 1.66e-27, pparam={\u0026#34;xlabel\u0026#34;: \u0026#34;Speed\u0026#34;, \u0026#34;ylabel\u0026#34;: \u0026#34;Speed distribution\u0026#34;}, save_name=\u0026#34;\u0026#34;, ): fig, ax = plt.subplots() for T in temperatures: fv = MB_speed(v, mass, T) ax.plot(v, fv, label=f\u0026#34;T={T}K\u0026#34;) ax.legend() ax.set(**pparam) fig.savefig(save_name) v = np.arange(0, 800, 10) temperatures = [i for i in range(100, 500, 75)] styles = [ [ \u0026#34;ipynb\u0026#34;, \u0026#34;use_mathtext\u0026#34;, ], [ \u0026#34;ipynb\u0026#34;, \u0026#34;use_mathtext\u0026#34;, \u0026#34;colors5-light\u0026#34;, ], [ \u0026#34;ipynb\u0026#34;, \u0026#34;use_mathtext\u0026#34;, \u0026#34;colors10-ls\u0026#34;, ], [ \u0026#34;ipynb\u0026#34;, \u0026#34;use_mathtext\u0026#34;, \u0026#34;colors10-markers\u0026#34;, ], [\u0026#34;classic\u0026#34;], [\u0026#34;ipynb\u0026#34;], ] names = [ \u0026#34;out/ipynb+use_mathtext.png\u0026#34;, \u0026#34;out/ipynb+use_mathtext+colors5-light.png\u0026#34;, \u0026#34;out/ipynb+use_mathtext+colors10-ls.png\u0026#34;, \u0026#34;out/ipynb+use_mathtext+colors10-markers.png\u0026#34;, \u0026#34;out/classicmpl.png\u0026#34;, \u0026#34;out/ipynb.png\u0026#34;, ] for style, name in zip(styles, names): with plt.style.context(style): plot_dist(temperatures, v, save_name=name) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-12-10-lovelyplots/","summary":"LovelyPlots库 可以很好地格式化科学论文、论文和演示文稿的可视化图形，同时使它们在 Adobe Illustrator 中完全可编辑。\n此外，.svg 导出选项允许图形自动调整其字体以适应您文档的字体。 例如，导入到 .tex 文件中的 .svg 图形将自动生成为您的 .tex 文件中使用的文本字体。\n安装 pip3 install LovelyPlots \n样例 代码 import matplotlib.pyplot as plt import numpy as np def MB_speed(v, m, T): \u0026#34;\u0026#34;\u0026#34;Maxwell-Boltzmann speed distribution for speeds\u0026#34;\u0026#34;\u0026#34; kB = 1.38e-23 return ( (m / (2 * np.pi * kB * T)) ** 1.5 * 4 * np.pi * v**2 * np.exp(-m * v**2 / (2 * kB * T)) ) def plot_dist( temperatures, v, mass=85 * 1.","title":"LovelyPlots库 | 格式化科学论文、论文和演示文稿的可视化图形"},{"content":"1978-2019.4, 585w中国大陆企业注册信息\n文末有 enterprise-registration-data-of-chinese-mainland.csv 数据获取方式。\nimport pandas as pd df = pd.read_csv(\u0026#39;enterprise-registration-data-of-chinese-mainland.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, #忽略有问题的记录 on_bad_lines=\u0026#39;skip\u0026#39;) df.head() Run\n#剔除大邓广告 df = df[df[\u0026#39;企业类型\u0026#39;]!=\u0026#39;公众号: 大邓和他的Python\u0026#39;] df.head() Run\n#记录 print(\u0026#39;记录数: \u0026#39;, len(df)) # print(\u0026#39;字段有: \u0026#39;, df.columns) Run\n\u0026#39;记录数: 12756270\u0026#39; \u0026#39;字段有: [\u0026#39;企业名称\u0026#39;, \u0026#39;统一社会信用代码\u0026#39;, \u0026#39;注册日期\u0026#39;, \u0026#39;企业类型\u0026#39;, \u0026#39;法人代表\u0026#39;, \u0026#39;注册资金\u0026#39;, \u0026#39;经营范围\u0026#39;, \u0026#39;所在省份\u0026#39;, \u0026#39;地区\u0026#39;, \u0026#39;注册地址\u0026#39;]\u0026#39; \n但数据可能会有重复，这里以企业名称作为唯一标识，可以查看真实的数据量\nprint(\u0026#39;真实记录数: \u0026#39;, len(set(df[\u0026#39;企业名称\u0026#39;]))) Run\n\u0026#39;真实记录数: 5888382\u0026#39; \n二、如何将多个csv汇总到一个csv中？ 那么这个enterprise-registration-data-of-chinese-mainland.csv怎么来的？\n原始的数据集结构\n先局部实验成功后，推广到整体。\n 获取路径列表 尝试读取任意一个csv文件 尝试合并两个df 合并所有csv到一个文件内  2.1 获取路径列表 import os #大邓电脑为Mac #Mac容易在文件夹中生成奇怪的.DS_Store #该操作为获取文件夹列表，同时剔除.DS_Store y_dirs = [di for di in os.listdir(\u0026#39;csv\u0026#39;) if \u0026#39;.DS_Store\u0026#39;!=di] for y_dir in y_dirs: #在年份文件夹内有很多csv文件 fs = [\u0026#39;csv/{}/{}\u0026#39;.format(y_dir, fn) for fn in os.listdir(\u0026#39;csv/{}\u0026#39;.format(y_dir)) if \u0026#39;.csv\u0026#39; in fn] print(fs) print() Run\n...... ...... [\u0026#39;csv/2013/河南.csv\u0026#39;, \u0026#39;csv/2013/青海.csv\u0026#39;, \u0026#39;csv/2013/河北.csv\u0026#39;, \u0026#39;csv/2013/浙江.csv\u0026#39;, \u0026#39;csv/2013/内蒙古.csv\u0026#39;, \u0026#39;csv/2013/辽宁.csv\u0026#39;, \u0026#39;csv/2013/天津.csv\u0026#39;, \u0026#39;csv/2013/福建.csv\u0026#39;, \u0026#39;csv/2013/吉林.csv\u0026#39;, \u0026#39;csv/2013/西藏.csv\u0026#39;, \u0026#39;csv/2013/四川.csv\u0026#39;, \u0026#39;csv/2013/云南.csv\u0026#39;, \u0026#39;csv/2013/宁夏.csv\u0026#39;, \u0026#39;csv/2013/新疆.csv\u0026#39;, \u0026#39;csv/2013/安徽.csv\u0026#39;, \u0026#39;csv/2013/重庆.csv\u0026#39;, \u0026#39;csv/2013/贵州.csv\u0026#39;, \u0026#39;csv/2013/湖南.csv\u0026#39;, \u0026#39;csv/2013/海南.csv\u0026#39;, \u0026#39;csv/2013/湖北.csv\u0026#39;, \u0026#39;csv/2013/江西.csv\u0026#39;, \u0026#39;csv/2013/广东.csv\u0026#39;, \u0026#39;csv/2013/北京.csv\u0026#39;, \u0026#39;csv/2013/山西.csv\u0026#39;, \u0026#39;csv/2013/上海.csv\u0026#39;, \u0026#39;csv/2013/陕西.csv\u0026#39;, \u0026#39;csv/2013/黑龙江.csv\u0026#39;, \u0026#39;csv/2013/甘肃.csv\u0026#39;, \u0026#39;csv/2013/江苏.csv\u0026#39;, \u0026#39;csv/2013/山东.csv\u0026#39;, \u0026#39;csv/2013/广西.csv\u0026#39;] [\u0026#39;csv/2014/河南.csv\u0026#39;, \u0026#39;csv/2014/青海.csv\u0026#39;, \u0026#39;csv/2014/河北.csv\u0026#39;, \u0026#39;csv/2014/浙江.csv\u0026#39;, \u0026#39;csv/2014/内蒙古.csv\u0026#39;, \u0026#39;csv/2014/辽宁.csv\u0026#39;, \u0026#39;csv/2014/天津.csv\u0026#39;, \u0026#39;csv/2014/福建.csv\u0026#39;, \u0026#39;csv/2014/吉林.csv\u0026#39;, \u0026#39;csv/2014/西藏.csv\u0026#39;, \u0026#39;csv/2014/四川.csv\u0026#39;, \u0026#39;csv/2014/云南.csv\u0026#39;, \u0026#39;csv/2014/宁夏.csv\u0026#39;, \u0026#39;csv/2014/新疆.csv\u0026#39;, \u0026#39;csv/2014/安徽.csv\u0026#39;, \u0026#39;csv/2014/重庆.csv\u0026#39;, \u0026#39;csv/2014/贵州.csv\u0026#39;, \u0026#39;csv/2014/湖南.csv\u0026#39;, \u0026#39;csv/2014/海南.csv\u0026#39;, \u0026#39;csv/2014/湖北.csv\u0026#39;, \u0026#39;csv/2014/江西.csv\u0026#39;, \u0026#39;csv/2014/广东.csv\u0026#39;, \u0026#39;csv/2014/北京.csv\u0026#39;, \u0026#39;csv/2014/山西.csv\u0026#39;, \u0026#39;csv/2014/上海.csv\u0026#39;, \u0026#39;csv/2014/陕西.csv\u0026#39;, \u0026#39;csv/2014/黑龙江.csv\u0026#39;, \u0026#39;csv/2014/甘肃.csv\u0026#39;, \u0026#39;csv/2014/江苏.csv\u0026#39;, \u0026#39;csv/2014/山东.csv\u0026#39;, \u0026#39;csv/2014/广西.csv\u0026#39;] [\u0026#39;csv/2015/河南.csv\u0026#39;, \u0026#39;csv/2015/青海.csv\u0026#39;, \u0026#39;csv/2015/河北.csv\u0026#39;, \u0026#39;csv/2015/浙江.csv\u0026#39;, \u0026#39;csv/2015/内蒙古.csv\u0026#39;, \u0026#39;csv/2015/辽宁.csv\u0026#39;, \u0026#39;csv/2015/天津.csv\u0026#39;, \u0026#39;csv/2015/福建.csv\u0026#39;, \u0026#39;csv/2015/吉林.csv\u0026#39;, \u0026#39;csv/2015/西藏.csv\u0026#39;, \u0026#39;csv/2015/四川.csv\u0026#39;, \u0026#39;csv/2015/云南.csv\u0026#39;, \u0026#39;csv/2015/宁夏.csv\u0026#39;, \u0026#39;csv/2015/新疆.csv\u0026#39;, \u0026#39;csv/2015/安徽.csv\u0026#39;, \u0026#39;csv/2015/重庆.csv\u0026#39;, \u0026#39;csv/2015/贵州.csv\u0026#39;, \u0026#39;csv/2015/湖南.csv\u0026#39;, \u0026#39;csv/2015/海南.csv\u0026#39;, \u0026#39;csv/2015/湖北.csv\u0026#39;, \u0026#39;csv/2015/江西.csv\u0026#39;, \u0026#39;csv/2015/广东.csv\u0026#39;, \u0026#39;csv/2015/北京.csv\u0026#39;, \u0026#39;csv/2015/山西.csv\u0026#39;, \u0026#39;csv/2015/上海.csv\u0026#39;, \u0026#39;csv/2015/陕西.csv\u0026#39;, \u0026#39;csv/2015/黑龙江.csv\u0026#39;, \u0026#39;csv/2015/甘肃.csv\u0026#39;, \u0026#39;csv/2015/江苏.csv\u0026#39;, \u0026#39;csv/2015/山东.csv\u0026#39;, \u0026#39;csv/2015/广西.csv\u0026#39;] ..... ..... \n2.2 尝试读取任意一个csv文件 import pandas as pd df1 = pd.read_csv(\u0026#39;csv/2012/辽宁.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, on_bad_lines=\u0026#39;skip\u0026#39;) df1.head(2) Run\ndf2 = pd.read_csv(\u0026#39;csv/2013/青海.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, on_bad_lines=\u0026#39;skip\u0026#39;) df2.head(2) Run\n2.3 尝试合并两个df 两个df垂直方向堆积，不增加字段种类，所以选择 pd.concat函数。\ndf12 = pd.concat([df1, df2], axis=0) df12.head(2) Run\n#检查记录数 print(len(df1)) print(len(df2)) print(len(df12)) Run\n10246 4417 14663 \n2.4 合并所有csv到一个文件内 将步骤1、2、3代码整理，汇总\nimport os import pandas as pd #存df列表 dfs = [] #文件路径列表 y_dirs = [di for di in os.listdir(\u0026#39;csv\u0026#39;) if \u0026#39;.DS_Store\u0026#39;!=di] for y_dir in y_dirs: csvfs = [\u0026#39;csv/{}/{}\u0026#39;.format(y_dir, fn) for fn in os.listdir(\u0026#39;csv/{}\u0026#39;.format(y_dir)) if \u0026#39;.csv\u0026#39; in fn] for csvf in csvfs: #读取csv，得到df df = pd.read_csv(csvf, encoding=\u0026#39;utf-8\u0026#39;, on_bad_lines=\u0026#39;skip\u0026#39;) #存入df列表 dfs.append(df) #合并dfs为alldf alldf = pd.concat(dfs, axis=0) #导出为data.csv alldf.to_csv(\u0026#39;enterprise-registration-data-of-chinese-mainland.csv\u0026#39;, index=False) \n三、数据获取 转发本文至朋友圈集赞50+， 加微信372335839， 备注【姓名-学校-专业-1200w工商】获取本文数据。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-12-07-585w-chinese-enterprise-registration-data/","summary":"1978-2019.4, 585w中国大陆企业注册信息\n文末有 enterprise-registration-data-of-chinese-mainland.csv 数据获取方式。\nimport pandas as pd df = pd.read_csv(\u0026#39;enterprise-registration-data-of-chinese-mainland.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, #忽略有问题的记录 on_bad_lines=\u0026#39;skip\u0026#39;) df.head() Run\n#剔除大邓广告 df = df[df[\u0026#39;企业类型\u0026#39;]!=\u0026#39;公众号: 大邓和他的Python\u0026#39;] df.head() Run\n#记录 print(\u0026#39;记录数: \u0026#39;, len(df)) # print(\u0026#39;字段有: \u0026#39;, df.columns) Run\n\u0026#39;记录数: 12756270\u0026#39; \u0026#39;字段有: [\u0026#39;企业名称\u0026#39;, \u0026#39;统一社会信用代码\u0026#39;, \u0026#39;注册日期\u0026#39;, \u0026#39;企业类型\u0026#39;, \u0026#39;法人代表\u0026#39;, \u0026#39;注册资金\u0026#39;, \u0026#39;经营范围\u0026#39;, \u0026#39;所在省份\u0026#39;, \u0026#39;地区\u0026#39;, \u0026#39;注册地址\u0026#39;]\u0026#39; \n但数据可能会有重复，这里以企业名称作为唯一标识，可以查看真实的数据量\nprint(\u0026#39;真实记录数: \u0026#39;, len(set(df[\u0026#39;企业名称\u0026#39;]))) Run\n\u0026#39;真实记录数: 5888382\u0026#39; \n二、如何将多个csv汇总到一个csv中？ 那么这个enterprise-registration-data-of-chinese-mainland.csv怎么来的？\n原始的数据集结构\n先局部实验成功后，推广到整体。\n 获取路径列表 尝试读取任意一个csv文件 尝试合并两个df 合并所有csv到一个文件内  2.1 获取路径列表 import os #大邓电脑为Mac #Mac容易在文件夹中生成奇怪的.","title":"数据集 | 585w企业工商注册信息"},{"content":"Kickstarter介绍 Kickstarter于2009年4月在美国纽约成立，是一个专为具有创意方案的企业筹资的众筹网站平台。\nkickstarter平台的运作方式相对来说比较简单而有效：该平台的用户一方是有创新意渴望进行创作和创造的人，另一方则是愿意为他们出资金的人，然后见证新发明新创作新产品的出现。kickstarter网站的创意性活动包括：音乐，网页设计，平面设计，动画，作家以及所有有能力创造以及影响他人的活动。\n\n12G数据集 2016年3月 写好的kickstarter爬虫，每月执行一次。截止2022年11月， 所有压缩文件累积11.42G。文末有数据获取方式\n\n参考论文 该数据集研究价值，可用于研究市场营销、创新创业、信息管理等， 部分使用kickstarter作为研究对象的论文。\n[1]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.*管理世界*.2016;5:81-98. [2]Dai, Hengchen and Dennis J. Zhang. “Prosocial Goal Pursuit in Crowdfunding: Evidence from Kickstarter.” Journal of Marketing Research 56 (2019): 498 - 517. [3]Gafni, H., Marom, D.M., Robb, A.M., \u0026amp; Sade, O. (2020). Gender Dynamics in Crowdfunding (Kickstarter): Evidence on Entrepreneurs, Backers, and Taste-Based Discrimination*. Review of Finance. [4]Jensen, Lasse Skovgaard and Ali Gürcan Özkil. “Identifying challenges in crowdfunded product development: a review of Kickstarter projects.” Design Science 4 (2018): n. pag. \n查看数据 任意选择一个zip文件解压会得到json文件，注意 不同json文件不太一样，所以本文的代码可能要有调整。\nimport pandas as pd #读取任意一个zip解压得到的csv文件 df = pd.read_json(\u0026#39;data/Kickstarter_2022-06-09T03_20_03_365Z.json\u0026#39;, lines=True) df.head() Run\nlen(df) Run\n230346 \n# 选中projects字段 projects = df[\u0026#39;data\u0026#39;] projects Run\n0 {\u0026#39;id\u0026#39;: 947118202, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/029... 1 {\u0026#39;id\u0026#39;: 426094497, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/029... 2 {\u0026#39;id\u0026#39;: 44835253, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/034/... 3 {\u0026#39;id\u0026#39;: 1001767271, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/03... 4 {\u0026#39;id\u0026#39;: 1880345176, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/03... ... 230341 {\u0026#39;id\u0026#39;: 676753351, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/012... 230342 {\u0026#39;id\u0026#39;: 1579378115, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/02... 230343 {\u0026#39;id\u0026#39;: 1281094926, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/02... 230344 {\u0026#39;id\u0026#39;: 783009016, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/012... 230345 {\u0026#39;id\u0026#39;: 324368296, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/012... Name: data, Length: 230346, dtype: object \n#查看第一行，data列 df.loc[0, \u0026#39;data\u0026#39;] Run\n{\u0026#39;id\u0026#39;: 947118202, \u0026#39;photo\u0026#39;: {\u0026#39;key\u0026#39;: \u0026#39;assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png\u0026#39;, \u0026#39;full\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=560\u0026amp;h=315\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=26209d432871ad2e9cca642527c291d9\u0026#39;, \u0026#39;ed\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=352\u0026amp;h=198\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=db82255e6639d5951506e0f2ed4d7d8b\u0026#39;, \u0026#39;med\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=272\u0026amp;h=153\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=f7b43116136000c8efa892bdbdd2d956\u0026#39;, \u0026#39;little\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=208\u0026amp;h=117\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=a52e3c34066a020e040c517c614a8b36\u0026#39;, \u0026#39;small\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=160\u0026amp;h=90\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=6c5f1c254119ffe914b50250f8e2899f\u0026#39;, \u0026#39;thumb\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=48\u0026amp;h=27\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=0222f379ed51059eb73adc7436f07b1e\u0026#39;, \u0026#39;1024x576\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=1024\u0026amp;h=576\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=d01546c5e88f3f47e0dddc48b5dce9df\u0026#39;, \u0026#39;1536x864\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=1552\u0026amp;h=873\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=f47505da909a374642906e6d418474e7\u0026#39;}, \u0026#39;name\u0026#39;: \u0026#39;Paint Rogue\u0026#39;, \u0026#39;blurb\u0026#39;: \u0026#39;Roguelike | Platformer | Shooter\u0026#39;, \u0026#39;goal\u0026#39;: 5000, \u0026#39;pledged\u0026#39;: 5268.22, \u0026#39;state\u0026#39;: \u0026#39;successful\u0026#39;, \u0026#39;slug\u0026#39;: \u0026#39;paint-rogue\u0026#39;, \u0026#39;disable_communication\u0026#39;: False, \u0026#39;country\u0026#39;: \u0026#39;AU\u0026#39;, \u0026#39;country_displayable_name\u0026#39;: \u0026#39;Australia\u0026#39;, \u0026#39;currency\u0026#39;: \u0026#39;AUD\u0026#39;, \u0026#39;currency_symbol\u0026#39;: \u0026#39;$\u0026#39;, \u0026#39;currency_trailing_code\u0026#39;: True, \u0026#39;deadline\u0026#39;: 1594247312, \u0026#39;state_changed_at\u0026#39;: 1594247312, \u0026#39;created_at\u0026#39;: 1591152439, \u0026#39;launched_at\u0026#39;: 1591655312, \u0026#39;staff_pick\u0026#39;: False, \u0026#39;is_starrable\u0026#39;: False, \u0026#39;backers_count\u0026#39;: 42, \u0026#39;static_usd_rate\u0026#39;: 0.69681992, \u0026#39;usd_pledged\u0026#39;: \u0026#39;3671.0006389424\u0026#39;, \u0026#39;converted_pledged_amount\u0026#39;: 3657, \u0026#39;fx_rate\u0026#39;: 0.7200616400000001, \u0026#39;usd_exchange_rate\u0026#39;: 0.69423473, \u0026#39;current_currency\u0026#39;: \u0026#39;USD\u0026#39;, \u0026#39;usd_type\u0026#39;: \u0026#39;international\u0026#39;, \u0026#39;creator\u0026#39;: {\u0026#39;id\u0026#39;: 1018782761, \u0026#39;name\u0026#39;: \u0026#39;Andrew Von Stieglitz\u0026#39;, \u0026#39;is_registered\u0026#39;: None, \u0026#39;is_email_verified\u0026#39;: None, \u0026#39;chosen_currency\u0026#39;: None, \u0026#39;is_superbacker\u0026#39;: None, \u0026#39;avatar\u0026#39;: {\u0026#39;thumb\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2\u0026amp;w=40\u0026amp;h=40\u0026amp;fit=crop\u0026amp;v=1554207776\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=bf4ce960e83b57310b93c40dda68e213\u0026#39;, \u0026#39;small\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2\u0026amp;w=80\u0026amp;h=80\u0026amp;fit=crop\u0026amp;v=1554207776\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=a862ab30490c90cd08186f448884142d\u0026#39;, \u0026#39;medium\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2\u0026amp;w=160\u0026amp;h=160\u0026amp;fit=crop\u0026amp;v=1554207776\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=38923ac11699d68a7aae93ce126b97b6\u0026#39;}, \u0026#39;urls\u0026#39;: {\u0026#39;web\u0026#39;: {\u0026#39;user\u0026#39;: \u0026#39;https://www.kickstarter.com/profile/1018782761\u0026#39;}, \u0026#39;api\u0026#39;: {\u0026#39;user\u0026#39;: \u0026#39;https://api.kickstarter.com/v1/users/1018782761?signature=1654832212.14f9df54b2643f080ad98cacb07314f94757d9c1\u0026#39;}}}, \u0026#39;location\u0026#39;: {\u0026#39;id\u0026#39;: 1105779, \u0026#39;name\u0026#39;: \u0026#39;Sydney\u0026#39;, \u0026#39;slug\u0026#39;: \u0026#39;sydney-au\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;Sydney, AU\u0026#39;, \u0026#39;displayable_name\u0026#39;: \u0026#39;Sydney, AU\u0026#39;, \u0026#39;localized_name\u0026#39;: \u0026#39;Sydney\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;AU\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;NSW\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;Town\u0026#39;, \u0026#39;is_root\u0026#39;: False, \u0026#39;expanded_country\u0026#39;: \u0026#39;Australia\u0026#39;, \u0026#39;urls\u0026#39;: {\u0026#39;web\u0026#39;: {\u0026#39;discover\u0026#39;: \u0026#39;https://www.kickstarter.com/discover/places/sydney-au\u0026#39;, \u0026#39;location\u0026#39;: \u0026#39;https://www.kickstarter.com/locations/sydney-au\u0026#39;}, \u0026#39;api\u0026#39;: {\u0026#39;nearby_projects\u0026#39;: \u0026#39;https://api.kickstarter.com/v1/discover?signature=1654814982.2fcf49a7b611d4414d14b1dbe41ac53623192e6a\u0026amp;woe_id=1105779\u0026#39;}}}, \u0026#39;category\u0026#39;: {\u0026#39;id\u0026#39;: 35, \u0026#39;name\u0026#39;: \u0026#39;Video Games\u0026#39;, \u0026#39;analytics_name\u0026#39;: \u0026#39;Video Games\u0026#39;, \u0026#39;slug\u0026#39;: \u0026#39;games/video games\u0026#39;, \u0026#39;position\u0026#39;: 7, \u0026#39;parent_id\u0026#39;: 12, \u0026#39;parent_name\u0026#39;: \u0026#39;Games\u0026#39;, \u0026#39;color\u0026#39;: 51627, \u0026#39;urls\u0026#39;: {\u0026#39;web\u0026#39;: {\u0026#39;discover\u0026#39;: \u0026#39;http://www.kickstarter.com/discover/categories/games/video%20games\u0026#39;}}}, \u0026#39;profile\u0026#39;: {\u0026#39;id\u0026#39;: 4007060, \u0026#39;project_id\u0026#39;: 4007060, \u0026#39;state\u0026#39;: \u0026#39;active\u0026#39;, \u0026#39;state_changed_at\u0026#39;: 1594267960, \u0026#39;name\u0026#39;: \u0026#39;Paint Rogue\u0026#39;, \u0026#39;blurb\u0026#39;: \u0026#39;Roguelike | Platformer | Shooter\u0026#39;, \u0026#39;background_color\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;text_color\u0026#39;: \u0026#39;ffffff\u0026#39;, \u0026#39;link_background_color\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;link_text_color\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;link_text\u0026#39;: \u0026#39;Follow along!\u0026#39;, \u0026#39;link_url\u0026#39;: \u0026#39;https://www.kickstarter.com/projects/1018782761/paint-rogue/\u0026#39;, \u0026#39;show_feature_image\u0026#39;: True, \u0026#39;background_image_opacity\u0026#39;: 0.5700000000000001, \u0026#39;background_image_attributes\u0026#39;: {\u0026#39;id\u0026#39;: 29758105, \u0026#39;image_urls\u0026#39;: {\u0026#39;default\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/758/105/971e42c0e19ca75fbae0943aa874c3c2_original.png?ixlib=rb-4.0.2\u0026amp;w=1600\u0026amp;fit=max\u0026amp;v=1594267934\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=b91907c9e125e206a11a1bcef322c142\u0026#39;, \u0026#39;baseball_card\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/758/105/971e42c0e19ca75fbae0943aa874c3c2_original.png?ixlib=rb-4.0.2\u0026amp;w=460\u0026amp;fit=max\u0026amp;v=1594267934\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=e7af2286d1f74a51672fbb6060ad43c8\u0026#39;}}, \u0026#39;should_show_feature_image_section\u0026#39;: False, \u0026#39;feature_image_attributes\u0026#39;: {\u0026#39;image_urls\u0026#39;: {\u0026#39;default\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=1552\u0026amp;h=873\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=f47505da909a374642906e6d418474e7\u0026#39;, \u0026#39;baseball_card\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=560\u0026amp;h=315\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=26209d432871ad2e9cca642527c291d9\u0026#39;}}}, \u0026#39;spotlight\u0026#39;: True, \u0026#39;urls\u0026#39;: {\u0026#39;web\u0026#39;: {\u0026#39;project\u0026#39;: \u0026#39;https://www.kickstarter.com/projects/1018782761/paint-rogue?ref=discovery_category_newest\u0026#39;, \u0026#39;rewards\u0026#39;: \u0026#39;https://www.kickstarter.com/projects/1018782761/paint-rogue/rewards\u0026#39;}}, \u0026#39;source_url\u0026#39;: \u0026#39;https://www.kickstarter.com/discover/categories/games/video%20games\u0026#39;} \n字段 以第一条为例，查看每条众筹项目数据中的字段，\ndf.loc[0, \u0026#39;data\u0026#39;].keys() Run，运行结果#为后期加入的字段解释\ndict_keys([ \u0026#39;id\u0026#39;, \u0026#39;photo\u0026#39;, #id、图片链接 \u0026#39;name\u0026#39;, \u0026#39;blurb\u0026#39;, #项目名 \u0026#39;goal\u0026#39;, #项目筹资目标金额 \u0026#39;pledged\u0026#39;, \u0026#39;state\u0026#39;, #项目状态 \u0026#39;slug\u0026#39;, \u0026#39;disable_communication\u0026#39;, \u0026#39;country\u0026#39;, \u0026#39;country_displayable_name\u0026#39;, #国家 \u0026#39;currency\u0026#39;, \u0026#39;currency_symbol\u0026#39;, \u0026#39;currency_trailing_code\u0026#39;, #货币 \u0026#39;deadline\u0026#39;, \u0026#39;state_changed_at\u0026#39;, #项目筹资截止时间(时间戳格式) \u0026#39;created_at\u0026#39;, #项目创建时间(时间戳格式) \u0026#39;launched_at\u0026#39;, #项目上架时间(时间戳格式) \u0026#39;staff_pick\u0026#39;, \u0026#39;is_starrable\u0026#39;, \u0026#39;backers_count\u0026#39;, #资助人数 \u0026#39;static_usd_rate\u0026#39;, \u0026#39;usd_pledged\u0026#39;, \u0026#39;converted_pledged_amount\u0026#39;, \u0026#39;fx_rate\u0026#39;, \u0026#39;usd_exchange_rate\u0026#39;, \u0026#39;current_currency\u0026#39;, \u0026#39;usd_type\u0026#39;, \u0026#39;creator\u0026#39;, #项目发起人信息 \u0026#39;location\u0026#39;, #地址 \u0026#39;category\u0026#39;, #项目所属类目信息 \u0026#39;profile\u0026#39;, #项目基本信息 \u0026#39;spotlight\u0026#39;, \u0026#39;urls\u0026#39;, #项目链接 \u0026#39;source_url\u0026#39;]) \n以第一条数据为例，依次查看这几个字段的信息\n#众筹项目具名 print(\u0026#39;项目名\u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;name\u0026#39;], end=\u0026#39;\\n\\n\u0026#39;) print(\u0026#39;项目链接\\n\u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;urls\u0026#39;], end=\u0026#39;\\n\\n\u0026#39;) #众筹项目的目标总金额 print(\u0026#39;目标总金额: {goal}{currency}\u0026#39;.format(goal=df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;goal\u0026#39;], currency=df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;currency\u0026#39;])) Run\n项目名 Paint Rogue 项目链接 {\u0026#39;web\u0026#39;: {\u0026#39;project\u0026#39;: \u0026#39;https://www.kickstarter.com/projects/1018782761/paint-rogue?ref=discovery_category_newest\u0026#39;, \u0026#39;rewards\u0026#39;: \u0026#39;https://www.kickstarter.com/projects/1018782761/paint-rogue/rewards\u0026#39;}} 目标总金额: 5000AUD \n#众筹项目发起人信息 print(\u0026#39;项目发起人信息\\n\u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;creator\u0026#39;], end=\u0026#39;\\n\\n\u0026#39;) print(\u0026#39;项目基本信息\\n\u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;profile\u0026#39;], end=\u0026#39;\\n\\n\u0026#39;) #众筹项目坐标 print(\u0026#39;地址: \u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;location\u0026#39;], end=\u0026#39;\\n\\n\u0026#39;) #众筹项目货币 print(\u0026#39;货币:\u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;currency\u0026#39;], end=\u0026#39;\\n\\n\u0026#39;) #众筹项目所在国家 print(\u0026#39;所在国家: \u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;country_displayable_name\u0026#39;], end=\u0026#39;\\n\\n\u0026#39;) Run\n项目发起人信息 {\u0026#39;id\u0026#39;: 1018782761, \u0026#39;name\u0026#39;: \u0026#39;Andrew Von Stieglitz\u0026#39;, \u0026#39;is_registered\u0026#39;: None, \u0026#39;is_email_verified\u0026#39;: None, \u0026#39;chosen_currency\u0026#39;: None, \u0026#39;is_superbacker\u0026#39;: None, \u0026#39;avatar\u0026#39;: {\u0026#39;thumb\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2\u0026amp;w=40\u0026amp;h=40\u0026amp;fit=crop\u0026amp;v=1554207776\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=bf4ce960e83b57310b93c40dda68e213\u0026#39;, \u0026#39;small\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2\u0026amp;w=80\u0026amp;h=80\u0026amp;fit=crop\u0026amp;v=1554207776\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=a862ab30490c90cd08186f448884142d\u0026#39;, \u0026#39;medium\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2\u0026amp;w=160\u0026amp;h=160\u0026amp;fit=crop\u0026amp;v=1554207776\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=38923ac11699d68a7aae93ce126b97b6\u0026#39;}, \u0026#39;urls\u0026#39;: {\u0026#39;web\u0026#39;: {\u0026#39;user\u0026#39;: \u0026#39;https://www.kickstarter.com/profile/1018782761\u0026#39;}, \u0026#39;api\u0026#39;: {\u0026#39;user\u0026#39;: \u0026#39;https://api.kickstarter.com/v1/users/1018782761?signature=1654832212.14f9df54b2643f080ad98cacb07314f94757d9c1\u0026#39;}}} 项目基本信息 {\u0026#39;id\u0026#39;: 4007060, \u0026#39;project_id\u0026#39;: 4007060, \u0026#39;state\u0026#39;: \u0026#39;active\u0026#39;, \u0026#39;state_changed_at\u0026#39;: 1594267960, \u0026#39;name\u0026#39;: \u0026#39;Paint Rogue\u0026#39;, \u0026#39;blurb\u0026#39;: \u0026#39;Roguelike | Platformer | Shooter\u0026#39;, \u0026#39;background_color\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;text_color\u0026#39;: \u0026#39;ffffff\u0026#39;, \u0026#39;link_background_color\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;link_text_color\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;link_text\u0026#39;: \u0026#39;Follow along!\u0026#39;, \u0026#39;link_url\u0026#39;: \u0026#39;https://www.kickstarter.com/projects/1018782761/paint-rogue/\u0026#39;, \u0026#39;show_feature_image\u0026#39;: True, \u0026#39;background_image_opacity\u0026#39;: 0.5700000000000001, \u0026#39;background_image_attributes\u0026#39;: {\u0026#39;id\u0026#39;: 29758105, \u0026#39;image_urls\u0026#39;: {\u0026#39;default\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/758/105/971e42c0e19ca75fbae0943aa874c3c2_original.png?ixlib=rb-4.0.2\u0026amp;w=1600\u0026amp;fit=max\u0026amp;v=1594267934\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=b91907c9e125e206a11a1bcef322c142\u0026#39;, \u0026#39;baseball_card\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/758/105/971e42c0e19ca75fbae0943aa874c3c2_original.png?ixlib=rb-4.0.2\u0026amp;w=460\u0026amp;fit=max\u0026amp;v=1594267934\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=e7af2286d1f74a51672fbb6060ad43c8\u0026#39;}}, \u0026#39;should_show_feature_image_section\u0026#39;: False, \u0026#39;feature_image_attributes\u0026#39;: {\u0026#39;image_urls\u0026#39;: {\u0026#39;default\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=1552\u0026amp;h=873\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=f47505da909a374642906e6d418474e7\u0026#39;, \u0026#39;baseball_card\u0026#39;: \u0026#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2\u0026amp;crop=faces\u0026amp;w=560\u0026amp;h=315\u0026amp;fit=crop\u0026amp;v=1592675400\u0026amp;auto=format\u0026amp;frame=1\u0026amp;q=92\u0026amp;s=26209d432871ad2e9cca642527c291d9\u0026#39;}}} 地址: {\u0026#39;id\u0026#39;: 1105779, \u0026#39;name\u0026#39;: \u0026#39;Sydney\u0026#39;, \u0026#39;slug\u0026#39;: \u0026#39;sydney-au\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;Sydney, AU\u0026#39;, \u0026#39;displayable_name\u0026#39;: \u0026#39;Sydney, AU\u0026#39;, \u0026#39;localized_name\u0026#39;: \u0026#39;Sydney\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;AU\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;NSW\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;Town\u0026#39;, \u0026#39;is_root\u0026#39;: False, \u0026#39;expanded_country\u0026#39;: \u0026#39;Australia\u0026#39;, \u0026#39;urls\u0026#39;: {\u0026#39;web\u0026#39;: {\u0026#39;discover\u0026#39;: \u0026#39;https://www.kickstarter.com/discover/places/sydney-au\u0026#39;, \u0026#39;location\u0026#39;: \u0026#39;https://www.kickstarter.com/locations/sydney-au\u0026#39;}, \u0026#39;api\u0026#39;: {\u0026#39;nearby_projects\u0026#39;: \u0026#39;https://api.kickstarter.com/v1/discover?signature=1654814982.2fcf49a7b611d4414d14b1dbe41ac53623192e6a\u0026amp;woe_id=1105779\u0026#39;}}} 货币: AUD 所在国家: Australia \n#众筹项目创建时间 print(\u0026#39;项目创建时间: \u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;created_at\u0026#39;]) #众筹项目上架时间 print(\u0026#39;项目上架时间: \u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;launched_at\u0026#39;]) #众筹项目截止时间 print(\u0026#39;项目截止时间: \u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;deadline\u0026#39;]) Run\n项目创建时间: 1591152439 项目上架时间: 1591655312 项目截止时间: 1594247312 \n时间戳转日期 1591152439是时间戳，以某时间点距1970之间的秒数作为时间。\n#时间戳转日期 import datetime def timestamp2str(timestamp): d = datetime.datetime.fromtimestamp(timestamp) return \u0026#39;{year}-{month}-{day}{hour}:{minute}:{second}\u0026#39;.format(year=d.year, month=d.month, day=d.day, hour=d.hour, minute=d.minute, second=d.second) print(\u0026#39;创建时间\u0026#39;, timestamp2str(1591152439)) print(\u0026#39;上架时间\u0026#39;, timestamp2str(1591655312)) print(\u0026#39;截止时间\u0026#39;, timestamp2str(1594247312)) Run\n创建时间 2020-6-3 10:47:19 上架时间 2020-6-9 6:28:32 截止时间 2020-7-9 6:28:32 \n#众筹项目产品 所属类目信息 print(\u0026#39;众筹类目:\u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;category\u0026#39;], end=\u0026#39;\\n\\n\u0026#39;) # 众筹类目根链接 print(\u0026#39;众筹类目根链接:\u0026#39;, df.loc[0, \u0026#39;data\u0026#39;][\u0026#39;source_url\u0026#39;]) Run\n众筹类目: {\u0026#39;id\u0026#39;: 35, \u0026#39;name\u0026#39;: \u0026#39;Video Games\u0026#39;, \u0026#39;analytics_name\u0026#39;: \u0026#39;Video Games\u0026#39;, \u0026#39;slug\u0026#39;: \u0026#39;games/video games\u0026#39;, \u0026#39;position\u0026#39;: 7, \u0026#39;parent_id\u0026#39;: 12, \u0026#39;parent_name\u0026#39;: \u0026#39;Games\u0026#39;, \u0026#39;color\u0026#39;: 51627, \u0026#39;urls\u0026#39;: {\u0026#39;web\u0026#39;: {\u0026#39;discover\u0026#39;: \u0026#39;http://www.kickstarter.com/discover/categories/games/video%20games\u0026#39;}}} 众筹类目根链接: https://www.kickstarter.com/discover/categories/games/video%20games \n数据获取方法 转发分享至朋友圈，集赞50+, 加微信 372335839 ， 备注「姓名-学校-专业-Kickstarter」\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-12-04-kickstarters_dataset/","summary":"Kickstarter介绍 Kickstarter于2009年4月在美国纽约成立，是一个专为具有创意方案的企业筹资的众筹网站平台。\nkickstarter平台的运作方式相对来说比较简单而有效：该平台的用户一方是有创新意渴望进行创作和创造的人，另一方则是愿意为他们出资金的人，然后见证新发明新创作新产品的出现。kickstarter网站的创意性活动包括：音乐，网页设计，平面设计，动画，作家以及所有有能力创造以及影响他人的活动。\n\n12G数据集 2016年3月 写好的kickstarter爬虫，每月执行一次。截止2022年11月， 所有压缩文件累积11.42G。文末有数据获取方式\n\n参考论文 该数据集研究价值，可用于研究市场营销、创新创业、信息管理等， 部分使用kickstarter作为研究对象的论文。\n[1]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.*管理世界*.2016;5:81-98. [2]Dai, Hengchen and Dennis J. Zhang. “Prosocial Goal Pursuit in Crowdfunding: Evidence from Kickstarter.” Journal of Marketing Research 56 (2019): 498 - 517. [3]Gafni, H., Marom, D.M., Robb, A.M., \u0026amp; Sade, O. (2020). Gender Dynamics in Crowdfunding (Kickstarter): Evidence on Entrepreneurs, Backers, and Taste-Based Discrimination*. Review of Finance. [4]Jensen, Lasse Skovgaard and Ali Gürcan Özkil. “Identifying challenges in crowdfunded product development: a review of Kickstarter projects.","title":"12G数据集 |  23w条Kickstarter项目信息"},{"content":"在本文中将使用 BERTopic 库，对美国前总统 Trump 推特数据集，构建动态主题模型 DTM(Dynamic Topic Modeling)，可视化文档数据集中不同主题随时间的演变(变迁)。文末有代码下载方式\n安装 为保证代码可复现，保证你我电脑中 bertopic 版本一致，先查看大邓电脑的 bertopic 版本\nimport bertopic #本文bertopic版本 bertopic.__version__ Run\n'0.12.0'  #推荐指定版本安装； #!pip3 install bertopic==0.12.0 #不指定版本安装 !pip3 install bertopic \n导入数据 这里准备了twitter账号 @realDonalTrump 中 2021年的推特数据， 点击下载 数据及代码。\n 我们只分析原推特，不分析每条推特的回复。 因为要分析推特随时间的主题变化，需要准备 推特 及对应的 推文时间  import re import pandas as pd from datetime import datetime # 导入数据 trump_df = pd.read_csv(\u0026#39;trump_twitter_2021.csv\u0026#39;) trump_df.head() Run\n\u0026lt;div\u0026gt; \u0026lt;style scoped\u0026gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } \u0026lt;/style\u0026gt; \u0026lt;table border=\u0026#34;1\u0026#34; class=\u0026#34;dataframe\u0026#34;\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr style=\u0026#34;text-align: right;\u0026#34;\u0026gt; \u0026lt;th\u0026gt;\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;id\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;text\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;isRetweet\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;isDeleted\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;device\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;favorites\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;retweets\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;date\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;isFlagged\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;0\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;98454970654916608\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Republicans and Democrats have both created ou...\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;TweetDeck\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;49\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;255\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2011-08-02 18:07:48\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;1\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;1234653427789070336\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;I was thrilled to be back in the Great city of...\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Twitter for iPhone\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;73748\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;17404\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2020-03-03 01:34:50\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;2\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;1218010753434820614\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;RT @CBS_Herridge: READ: Letter to surveillance...\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;t\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Twitter for iPhone\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;0\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;7396\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2020-01-17 03:22:47\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;3\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;1304875170860015617\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;The Unsolicited Mail In Ballot Scam is a major...\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Twitter for iPhone\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;80527\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;23502\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2020-09-12 20:10:58\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;4\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;1218159531554897920\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;RT @MZHemingway: Very friendly telling of even...\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;t\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;Twitter for iPhone\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;0\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;9081\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2020-01-17 13:13:59\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;f\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \n预处理  使用正则表达式 清除推文中的http链接 剔除@符 使用正则表达式 剔除 非英文字符  import re #预处理函数clean_text def clean_text(text): text = re.sub(\u0026#34;http\\S+\u0026#34;, \u0026#34;\u0026#34;, text).lower() text = \u0026#34; \u0026#34;.join([w for w in text.split() if w[0]!=\u0026#39;@\u0026#39;]) text = re.sub(\u0026#34;[^a-zA-Z]+\u0026#34;, \u0026#34; \u0026#34;, text).lower() return text test_text = \u0026#39;hello @Apple, https://apple.com 李John\u0026#39; #验证函数有效性 clean_text(text=test_text) Run\n'hello john'   对text字段使用预处理函数 clean_text 只保留原推文 准备推特tweets和时间戳 timestamps  #清洗字段text trump_df[\u0026#39;text\u0026#39;] = trump_df[\u0026#39;text\u0026#39;].apply(clean_text) #只保留特朗普原推文(剔除特朗普的Retweet) #推文内容不能为”“ trump_df = trump_df.loc[(trump_df[\u0026#39;isRetweet\u0026#39;] == \u0026#34;f\u0026#34;) \u0026amp; (trump_df[\u0026#39;text\u0026#39;] != \u0026#34;\u0026#34;), :] #准备tweets及对应的timestamps tweets = trump_df[\u0026#39;text\u0026#39;].to_list() timestamps = trump_df[\u0026#39;date\u0026#39;].to_list() tweets[0] Run\n'republicans and democrats have both created our economic problems '  初始化BERTopic 在模型初始化阶段，使用所有推文数据， 会忽略时间维度。 该步骤会把所有时间段中出现的主题都提前训练识别出来。\nfrom bertopic import BERTopic #大邓这里，运行了不到1小时 #特朗普比较活跃，什么内容都会参与，所以这里设置一个话题数下限为35，话题数上限不设置 topic_model = BERTopic(min_topic_size=35, verbose=True) topics, _ = topic_model.fit_transform(tweets) Run\nDownloading: 0%| | 0.00/1.18k [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/190 [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/10.6k [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/612 [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/116 [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/39.3k [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/349 [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/90.9M [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/53.0 [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/112 [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/466k [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/350 [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/13.2k [00:00\u0026lt;?, ?B/s] Downloading: 0%| | 0.00/232k [00:00\u0026lt;?, ?B/s] Batches: 0%| | 0/1418 [00:00\u0026lt;?, ?it/s] 2022-12-04 22:04:02,964 - BERTopic - Transformed documents to Embeddings 2022-12-04 22:05:13,606 - BERTopic - Reduced dimensionality 2022-12-04 22:05:17,814 - BERTopic - Clustered reduced embeddings \n抽取出所有的话题\nfreq = topic_model.get_topic_info() #话题总数 print(len(freq)) freq.head(10) Run\n169 \u0026lt;div\u0026gt; \u0026lt;style scoped\u0026gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } \u0026lt;/style\u0026gt; \u0026lt;table border=\u0026#34;1\u0026#34; class=\u0026#34;dataframe\u0026#34;\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr style=\u0026#34;text-align: right;\u0026#34;\u0026gt; \u0026lt;th\u0026gt;\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Topic\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Count\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;0\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;-1\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;15098\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;-1_the_to_is_of\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;1\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;0\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;3182\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;0_run_president_trump_donald\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;2\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;1\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;1821\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;1_crowd_carolina_join_thank\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;3\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;2\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;1084\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2_golf_course_doral_scotland\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;4\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;3\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;1030\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;3_border_wall_immigration_mexico\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;5\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;4\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;811\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;4_china_trade_tariffs_chinese\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;6\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;5\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;642\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;5_obamacare_healthcare_repeal_website\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;7\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;6\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;638\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;6_hillary_clinton_crooked_she\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;8\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;7\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;607\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;7_amp_it_you_to\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;9\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;8\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;562\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;8_media_fake_news_failing\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \n-1 意识是所有的离群点(异类)推文，应该被忽略掉。接下来让我们看一下 Topic-4 的特征词及其权重\n#topic-4的特征词及权重 topic_model.get_topic(4) Run\n[(\u0026#39;china\u0026#39;, 0.05289416225000891), (\u0026#39;tariffs\u0026#39;, 0.024471004754487165), (\u0026#39;trade\u0026#39;, 0.02437576425026641), (\u0026#39;chinese\u0026#39;, 0.013643270667358017), (\u0026#39;us\u0026#39;, 0.011206804363719649), (\u0026#39;farmers\u0026#39;, 0.01113584970813823), (\u0026#39;our\u0026#39;, 0.010197907480148342), (\u0026#39;deal\u0026#39;, 0.010014612658730073), (\u0026#39;we\u0026#39;, 0.009043537683534882), (\u0026#39;countries\u0026#39;, 0.00901653033214627)] \n在二维空间中使用 Intertopic Distance Map 可视化所有主题。该图可以让我们继续创建 DTM 前，判断主题数设置的是否充分够用。\nfig = topic_model.visualize_topics() fig Run\n渲染的可视化文件太大，这里感兴趣的可以 点击查看动态效果图\n构建DTM 在 构建动态主题模型 前， 不同时间段中出现的主题需要预先都训练好。\n docs 文档数据，对应于本文的 tweets timestamps 时间戳，对应于本文的 timestamps global_tuning 是否将某个主题在 时间t 的主题表示向量 与 其全局主题表示向量 进行平均 evolution_tuning 是否将某个主题在 时间t 的主题表示向量 与 该主题在时间t-1 的主题表示向量 进行平均 nr_bins 时间段内含有的时间戳(点)数量。在数千个不同的时间戳中提取主题在计算上是低效的, 可以合并 20 个时间戳为一个时间段  topics_over_time = topic_model.topics_over_time(docs=tweets, timestamps=timestamps, global_tuning=True, evolution_tuning=True, nr_bins=20) topics_over_time Run\n\u0026lt;div\u0026gt; \u0026lt;style scoped\u0026gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } \u0026lt;/style\u0026gt; \u0026lt;table border=\u0026#34;1\u0026#34; class=\u0026#34;dataframe\u0026#34;\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr style=\u0026#34;text-align: right;\u0026#34;\u0026gt; \u0026lt;th\u0026gt;\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Topic\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Words\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Frequency\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Timestamp\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;0\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;-1\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;donald, keychain, champion, trump, contest\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;20\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2009-04-30 12:30:07.596999936\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;-1_the_to_is_of\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;1\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;0\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;donald, execute, imagination, step, randal\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;9\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2009-04-30 12:30:07.596999936\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;0_run_president_trump_donald\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;2\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;2\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;begun, schedule, ahead, international, scotland\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;1\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2009-04-30 12:30:07.596999936\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2_golf_course_doral_scotland\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;3\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;3\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;cling, wallflower, persona, walls, rather\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;1\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2009-04-30 12:30:07.596999936\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;3_border_wall_immigration_mexico\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;4\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;10\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;independence, safe, here, enjoy, happy\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;1\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2009-04-30 12:30:07.596999936\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;10_veterans_honor_heroes_our\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;...\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;...\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;...\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;...\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;...\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;...\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;1880\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;162\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;ratings, fredo, frank, bad, based\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2020-06-09 07:29:57.849999872\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;162_ratings_machine_show_sided\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;1881\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;163\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;yes, no, way, absolutely,\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2020-06-09 07:29:57.849999872\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;163_yes_no_absolutely_way\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;1882\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;164\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;twitter, trending, section, trends, conservative\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;13\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2020-06-09 07:29:57.849999872\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;164_twitter_trending_conservative_sectio...\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;1883\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;165\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;york, eaten, hell, new, blasio\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;4\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2020-06-09 07:29:57.849999872\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;165_york_ny_new_wonerful\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;1884\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt;167\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;mixing, courthouse, mocked, notes, prosecuted\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;3\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;2020-06-09 07:29:57.849999872\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;167_jury_judge_guilty_foreperson\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;p\u0026gt;1885 rows × 5 columns\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \n可视化DTM #模型中一共有169个主题，这里显示前Top10的主题的演变 topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=10) Run\n渲染的可视化文件太大，这里感兴趣的可以 点击查看动态效果图\n获取本文代码 点击获取 数据及代码\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-12-03-dynamic_topic_model_with_bertopic/","summary":"在本文中将使用 BERTopic 库，对美国前总统 Trump 推特数据集，构建动态主题模型 DTM(Dynamic Topic Modeling)，可视化文档数据集中不同主题随时间的演变(变迁)。文末有代码下载方式\n安装 为保证代码可复现，保证你我电脑中 bertopic 版本一致，先查看大邓电脑的 bertopic 版本\nimport bertopic #本文bertopic版本 bertopic.__version__ Run\n'0.12.0'  #推荐指定版本安装； #!pip3 install bertopic==0.12.0 #不指定版本安装 !pip3 install bertopic \n导入数据 这里准备了twitter账号 @realDonalTrump 中 2021年的推特数据， 点击下载 数据及代码。\n 我们只分析原推特，不分析每条推特的回复。 因为要分析推特随时间的主题变化，需要准备 推特 及对应的 推文时间  import re import pandas as pd from datetime import datetime # 导入数据 trump_df = pd.read_csv(\u0026#39;trump_twitter_2021.csv\u0026#39;) trump_df.head() Run\n\u0026lt;div\u0026gt; \u0026lt;style scoped\u0026gt; .dataframe tbody tr th:only-of-type { vertical-align: middle; } .","title":"BERTopic | 使用推特数据构建动态主题模型"},{"content":"Boegershausen, Johannes, Hannes Datta, Abhishek Borah, and Andrew Stephen. \u0026ldquo;Fields of gold: Scraping web data for marketing insights.\u0026rdquo; Journal of Marketing (2022).\n本文是JM中少有的技术流综述文，阅读起来晦涩难懂，我们就大概知道怎么回事， 查看有没有自己感兴趣的研究(方法)即可。该文作者为该综述专门开发了一个 web-scraping.org 的网站,截图如下\n  摘要 市场营销学者越来越多使用网络爬虫和API接口，从互联网收集数据。尽管网络数据得到广泛使用，但很少有学者关注收集过程中面临的各种挑战。研究人员如何确保采集的数据集是有效的？ 虽然现有资源强调提取网络数据的技术细节，但作者提出了一种新的方法框架，重点是提高其有效性。特别是，该框架强调解决有效性问题， 需要在数据采集的三个阶段(选择数据源、设计数据收集和提取数据)联合考虑技术和法律/伦理问题。作者进一步审查了营销Top5期刊上300 篇使用网络数据的论文，并总结提出了如何使用网络数据促进营销研究。本文最后指出了未来研究的方向，高价值的网络数据源和新方法。\nKeywords：\n- web scraping - application programming interface, API - crawling - validity - user-generated content - social media big data \n一、网络数据的魅力 社会和商业生活的加速数字化创造了数量空前的消费者和企业行为数字痕迹。 每分钟，全球用户在 Google 上进行 570 万次搜索，进行 600 万次商业交易，并在 Instagram 上分享6.5万张照片（Statista 2021）。 由此产生的网络数据——规模庞大、形式多样，而且通常可以在互联网上公开访问——对于那些想要量化消费、深入了解企业行为并跟踪难以或昂贵地观察社会活动的营销学者来说，这是一个潜在的金矿 . 网络数据对营销研究的重要性反映在越来越多的有影响力的出版物中，涵盖消费者文化理论、消费者心理学、实证建模和营销策略等。\n整理了 营销领域 top 5 期刊( JM、JMR、JCR、JCP、MS) 的 313 篇论文 ，经过整理绘制图-1（Figure1）， 使用网络数据进行研究的量呈现快速上涨的趋势。使用网络数据的论文占比，从2010年的4%提升到2020年的15%。 者313篇论文，数据的获取方式统计\n **59% 的论文使用了 网络爬虫 采集数据 12% 的论文使用API收集数据 9% 的论文同时使用了网络爬虫和API 20% 使用人工从网站手动复制粘贴数据  使用 网络数据 的论文，平均被引用次数 7.55， 远高于 非网络数据 的 3.90。\n使用网络数据做新研究，大致有4种实现路径\n 研究新现象，新场景  网络世界产生的不同于现实世界的情景，可以研究新现象   繁荣生态价值  比如，对亚马逊评论数据进行研究，研究发现可以帮助亚马逊平台进行改善。   促进方法论进步  文本、图片、音频、视频等   提高测量效果(快、准、好、全)  借助一些API，可以对已有的数据集增加新的信息量。 例如，日期数据，结合HolidayAPI，可以查看日期的节假日信息 给定日期和IP地址，使用Weather Underground可以查看天气信息    \n二、数据采集的方法框架 在使用 **网络爬虫 和 API ** 自动收集网络数据时，研究人员通常会在 **研究有效性、技术可行性和法律/伦理风险 **1 三者间权衡利弊得失，研究人员如何解决这些权衡，通过增强或破坏 统计结论有效性、内部有效性、结构有效性和外部有效性 来塑造研究结果的可信度（Shadish、Cook 和 Campbell 2002）。\n本文开发了一个方法框架，为使用 网络爬虫 和 API 自动收集网络数据提供指导。图 2（Figure 2） 涵盖三个关键阶段\n 数据源选择 设计方案  从网站中抽取哪些信息 采集频率，即 每天(周/月)重复运行一次爬虫，得到面板数据   执行数据采集  如何改善爬虫运行效率 如何处理原始信息，完整的保存为原始格式html、json，还是只抽取存储当前想要的字段    研究人员通常从一组广泛的潜在数据源开始，并根据三个关键考虑因素（有效性、技术可行性和法律/道德风险）剔除其中一些数据源。这三个考虑因素出现在倒金字塔的角落，底部的有效性强调其重要性。鉴于在收集最终数据集之前难以预测其确切特征，研究人员在设计、原型化和完善数据收集时经常重新考虑这些因素。未能解决技术或法律/伦理问题可能意味着网络数据无法有意义地告知研究问题。\n2.1 数据源面临的挑战(解决办法)  探索潜在网络数据源  由于网络资源在质量、稳定性和可检索性方面存在巨大差异，研究人员可能倾向于只考虑主要或熟悉的平台。 对数据世界的彻底探索允许令人信服的理论检验和识别可能难以以其他方式注意到的新颖的、新兴的营销现象。   考虑网络爬虫的替代方案  由于网络抓取是最流行的网络数据提取方法，研究人员可能会忽视其他提取数据的方法。 API 提供了一种记录和授权的方式来获取许多来源的 Web 数据。 一些来源还提供现成的数据集。 使用此类替代方案可以节省时间并最大限度地减少法律风险。   将数据与场景结合对应起来  Web 数据通常没有大量的文档。 尽早识别潜在相关的背景信息对于研究的相关性和有效性至关重要。     2.2 设计数据采集方案  从页面抽取什么信息，从有效性、合法、技术可行性 三个方面论证。 如何进行数据抽样？ 以什么频率(每天、周、月)进行数据采集  2.3 执行数据采集  如何改善爬虫运行效率 如何监控数据质量 整理数据文档(记录)   部分参考文献 [1]Allard, Thomas, Lea H. Dunn, and Katherine White. \u0026#34;Negative reviews, positive impact: Consumer empathetic responding to unfair word of mouth.\u0026#34; Journal of Marketing 84, no. 4 (2020): 86-108. [2]Gao, Weihe, Li Ji, Yong Liu, and Qi Sun. \u0026#34;Branding cultural products in international markets: a study of hollywood movies in China.\u0026#34; Journal of Marketing 84, no. 3 (2020): 86-105. [3]Reich, Taly, and Sam J. Maglio. \u0026#34;Featuring mistakes: The persuasive impact of purchase mistakes in online reviews.\u0026#34; Journal of Marketing 84, no. 1 (2020): 52-65. [4]Lee, Jeffrey K., and Ann Kronrod. \u0026#34;The strength of weak-tie consensus language.\u0026#34; Journal of Marketing Research 57, no. 2 (2020): 353-374. [5]Matz, Sandra C., Cristina Segalin, David Stillwell, Sandrine R. Müller, and Maarten W. Bos. \u0026#34;Predicting the personal appeal of marketing images using computational methods.\u0026#34; Journal of Consumer Psychology 29, no. 3 (2019): 370-390. [6]Dai, Hengchen, and Dennis J. Zhang. \u0026#34;Prosocial goal pursuit in crowdfunding: Evidence from kickstarter.\u0026#34; Journal of Marketing Research 56, no. 3 (2019): 498-517. [7]Luffarelli, Jonathan, Mudra Mukesh, and Ammara Mahmood. \u0026#34;Let the logo do the talking: The influence of logo descriptiveness on brand equity.\u0026#34; Journal of Marketing Research 56, no. 5 (2019): 862-878. [8]Bond, Samuel D., Stephen X. He, and Wen Wen. \u0026#34;Speaking for “free”: Word of mouth in free-and paid-product settings.\u0026#34; Journal of Marketing Research 56, no. 2 (2019): 276-290. [9]Han, Kyuhong, Jihye Jung, Vikas Mittal, Jinyong Daniel Zyung, and Hajo Adam. \u0026#34;Political identity and financial risk taking: Insights from social dominance orientation.\u0026#34; Journal of Marketing Research 56, no. 4 (2019): 581-601. [10]Netzer, Oded, Alain Lemaire, and Michal Herzenstein. \u0026#34;When words sweat: Identifying signals for loan default in the text of loan applications.\u0026#34; Journal of Marketing Research 56, no. 6 (2019): 960-980. [11]Toubia, Olivier, Garud Iyengar, Renée Bunnell, and Alain Lemaire. \u0026#34;Extracting features of entertainment products: A guided latent dirichlet allocation approach informed by the psychology of media consumption.\u0026#34; Journal of Marketing Research 56, no. 1 (2019): 18-36. [12]Van Laer, Tom, Jennifer Edson Escalas, Stephan Ludwig, and Ellis A. Van Den Hende. \u0026#34;What happens in Vegas stays on TripAdvisor? A theory and technique to understand narrativity in consumer reviews.\u0026#34; Journal of Consumer Research 46, no. 2 (2019): 267-285. [13]Zhong, Ning, and David A. Schweidel. \u0026#34;Capturing changes in social media content: A multiple latent changepoint topic model.\u0026#34; Marketing Science 39, no. 4 (2020): 827-846. [14]Colicev, Anatoli, Ashwin Malshe, Koen Pauwels, and Peter O\u0026#39;Connor. \u0026#34;Improving consumer mindset metrics and shareholder value through social media: The different roles of owned and earned media.\u0026#34; Journal of Marketing 82, no. 1 (2018): 37-56. [15]Liu, Xuan, Savannah Wei Shi, Thales Teixeira, and Michel Wedel. \u0026#34;Video content marketing: The making of clips.\u0026#34; Journal of Marketing 82, no. 4 (2018): 86-101. [16]Liu, Jia, and Olivier Toubia. \u0026#34;A semantic approach for estimating consumer content preferences from online search queries.\u0026#34; Marketing Science 37, no. 6 (2018): 930-952. [17]Nam, Hyoryung, Yogesh V. Joshi, and P. K. Kannan. \u0026#34;Harvesting brand information from social tags.\u0026#34; Journal of Marketing 81, no. 4 (2017): 88-108. [18]Packard, Grant, and Jonah Berger. \u0026#34;How language shapes word of mouth\u0026#39;s impact.\u0026#34; Journal of Marketing Research 54, no. 4 (2017): 572-588. \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-12-03-scraping-web-data-for-marketing-insights/","summary":"Boegershausen, Johannes, Hannes Datta, Abhishek Borah, and Andrew Stephen. \u0026ldquo;Fields of gold: Scraping web data for marketing insights.\u0026rdquo; Journal of Marketing (2022).\n本文是JM中少有的技术流综述文，阅读起来晦涩难懂，我们就大概知道怎么回事， 查看有没有自己感兴趣的研究(方法)即可。该文作者为该综述专门开发了一个 web-scraping.org 的网站,截图如下\n  摘要 市场营销学者越来越多使用网络爬虫和API接口，从互联网收集数据。尽管网络数据得到广泛使用，但很少有学者关注收集过程中面临的各种挑战。研究人员如何确保采集的数据集是有效的？ 虽然现有资源强调提取网络数据的技术细节，但作者提出了一种新的方法框架，重点是提高其有效性。特别是，该框架强调解决有效性问题， 需要在数据采集的三个阶段(选择数据源、设计数据收集和提取数据)联合考虑技术和法律/伦理问题。作者进一步审查了营销Top5期刊上300 篇使用网络数据的论文，并总结提出了如何使用网络数据促进营销研究。本文最后指出了未来研究的方向，高价值的网络数据源和新方法。\nKeywords：\n- web scraping - application programming interface, API - crawling - validity - user-generated content - social media big data \n一、网络数据的魅力 社会和商业生活的加速数字化创造了数量空前的消费者和企业行为数字痕迹。 每分钟，全球用户在 Google 上进行 570 万次搜索，进行 600 万次商业交易，并在 Instagram 上分享6.5万张照片（Statista 2021）。 由此产生的网络数据——规模庞大、形式多样，而且通常可以在互联网上公开访问——对于那些想要量化消费、深入了解企业行为并跟踪难以或昂贵地观察社会活动的营销学者来说，这是一个潜在的金矿 . 网络数据对营销研究的重要性反映在越来越多的有影响力的出版物中，涵盖消费者文化理论、消费者心理学、实证建模和营销策略等。","title":"JM2022综述 | 黄金领域: 为营销研究(新洞察)采集网络数据"},{"content":"周涛,高馨,罗家德.社会计算驱动的社会科学研究方法[J].社会学研究,2022,37(05):130-155+228-229.\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-12-03-social-computing-methodology-about-big-data-and-artificial-intelligence/","summary":"周涛,高馨,罗家德.社会计算驱动的社会科学研究方法[J].社会学研究,2022,37(05):130-155+228-229.\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"社会学研究 | 社会计算驱动的社会科学研究方法"},{"content":"今天逛B站，看到《三体》三部曲全部人物关系数据可视化视频\n 于是动手写了今天的技术文。绘制人物网络关系图，需要有\n 节点数据； 节点名、节点属性 边数据； source、target、weight 可视化工具(Gephi软件或Python的可视化包)  本文代码 点击下载\n\n节点-人物数据 绘制人物关系网络图，首先需要有网络节点数据\n name 人物名称 desc 人物简介 stage 人物出现在三体小说的哪个阶段  import pandas as pd nodes_info_df = pd.read_excel(\u0026#39;data/三体人物.xlsx\u0026#39;) nodes_info_df Run\n\n边-人物关系数据 将节点数据依次在三体小说文本中按行进行检索，如果每行同时出现两个人物，两个人物会构建一个边。人物关系网络图可以用gephi软件进行绘制， 绘制需要两个csv文件，即\n 三体_nodes.csv 三体_edges.csv  实现代码如下\nimport jieba import codecs import csv for name in df[\u0026#39;name\u0026#39;].tolist(): jieba.add_word(name) nodes = {}\t# 姓名字典，保存人物，该字典的键为人物名称，值为该人物在全文中出现的次数 relationships = {}\t# 关系字典，保存人物关系的有向边，该字典的键为有向边的起点，值为一个字典edge，edge的键是有向边的终点，值是有向边的权值，代表两个人物之间联系的紧密程度 lineNodes = []\t# 每段内人物关系，是一个缓存变量，保存对每一段分词得到当前段中出现的人物名称，lineName[i]是一个列表，列表中存储第i段中出现过的人物 with open(\u0026#34;data/三体.txt\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: for line in f.readlines(): words = jieba.lcut(line)\t# 分词并返回该词词性 lineNodes.append([])\t# 为新读入的一段添加人物名称列表 for word in words: if word in df[\u0026#39;name\u0026#39;].tolist(): lineNodes[-1].append(word)\t# 为当前段的环境增加一个人物 if nodes.get(word) is None: nodes[word] = 0 relationships[word] = {} nodes[word] += 1\t# 该人物出现次数加 1 # explore relationships 对于 lineNames 中每一行，我们为该行中出现的所有人物两两相连。如果两个人物之间尚未有边建立，则将新建的边权值设为 1，否则将已存在的边的权值加 1。这种方法将产生很多的冗余边，这些冗余边将在最后处理。 for line in lineNodes:\t# 对于每一段 for node1 in line:\tfor node2 in line:\t# 每段中的任意两个人 if node1 == node2: continue if relationships[node1].get(node2) is None:\t# 若两人尚未同时出现则新建项 relationships[node1][node2]= 1 else: relationships[node1][node2] = relationships[node1][node2]+ 1\t# 两人共同出现次数加 1 # output 将已经建好的 names 和 relationships 输出到文本，以方便 gephi 可视化处理。输出边的过程中可以过滤可能是冗余的边，这里假设共同出现次数少于 3 次的是冗余边，则在输出时跳过这样的边。输出的节点集合保存为 busan_node.txt ，边集合保存为 busan_edge.node 。 with open(\u0026#34;output/三体_nodes.csv\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as csvf1: writer1 = csv.DictWriter(csvf1, fieldnames=[\u0026#39;id\u0026#39;, \u0026#39;label\u0026#39;, \u0026#39;weight\u0026#39;]) writer1.writeheader() for node, times in nodes.items(): line = {\u0026#39;id\u0026#39;: node, \u0026#39;label\u0026#39;: node, \u0026#39;weight\u0026#39;: times} writer1.writerow(line) with open(\u0026#34;output/三体_edges.csv\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as csvf2: writer2 = csv.DictWriter(csvf2, fieldnames=[\u0026#39;source\u0026#39;, \u0026#39;target\u0026#39;, \u0026#39;weight\u0026#39;]) writer2.writeheader() for node, edges in relationships.items(): for target, weight in edges.items(): if weight\u0026gt;3: line = {\u0026#39;source\u0026#39;: node, \u0026#39;target\u0026#39;:target, \u0026#39;weight\u0026#39;:w} writer2.writerow(line) \n查看 三体_nodes.csv 文件样式\nnodes_df = pd.read_csv(\u0026#39;output/三体_nodes.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) nodes_df Run\n查看 三体_edges.csv 文件样式\nedges_df = pd.read_csv(\u0026#39;output/三体_edges.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) edges_df Run\n\n绘制关系图 本文不讲解gephi，感兴趣的可以去B站搜gephi使用方法。 实现可视化，除了gephi， 也可以使用pyechart库实现不错的可视化效果。\nimport numpy as np from pyecharts import options as opts from pyecharts.charts import Graph from pyecharts.globals import CurrentConfig, NotebookType CurrentConfig.NOTEBOOK_TYPE = NotebookType.JUPYTER_NOTEBOOK nodes_data = [] edges_data = [] categories_data = [] for node, weight in zip(nodes_df.label, nodes_df.weight): category = nodes_info_df[nodes_info_df[\u0026#39;name\u0026#39;]==node][\u0026#39;stage\u0026#39;].values[0] ##使用weight作为圆点的值，除以10，防止圆点太大占满整个屏幕. node_info = {\u0026#34;name\u0026#34;: str(node), \u0026#34;symbolSize\u0026#34;: weight/10, \u0026#39;category\u0026#39;: category, \u0026#34;value\u0026#34;: weight/10} nodes_data.append(node_info) for ix, source, target in zip(edges_df.index, edges_df.source, edges_df.target): edges_data.append({\u0026#34;id\u0026#34;: ix, \u0026#34;source\u0026#34;: source, \u0026#34;target\u0026#34;: target}) for category in set(df[\u0026#39;stage\u0026#39;].tolist()): categories_data.append({\u0026#39;name\u0026#39;: category}) c = ( Graph(init_opts=opts.InitOpts(width=\u0026#34;1000px\u0026#34;, height=\u0026#34;600px\u0026#34;)) .add( \u0026#34;\u0026#34;, nodes=nodes_data, links=edges_data, categories=categories_data, layout=\u0026#34;circular\u0026#34;, is_rotate_label=True, linestyle_opts=opts.LineStyleOpts(color=\u0026#34;source\u0026#34;, curve=0.3), label_opts=opts.LabelOpts(position=\u0026#34;right\u0026#34;), ) .set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;三体人物关系图\u0026#34;), legend_opts=opts.LegendOpts(orient=\u0026#34;vertical\u0026#34;, pos_left=\u0026#34;2%\u0026#34;, pos_top=\u0026#34;20%\u0026#34;), ) ) c.render(\u0026#39;output/三体_graph.html\u0026#39;) c.render_notebook() Run\n\r\rAwesome-pyecharts\r\r\r\r\r\rvar chart_e97ab300531a4c0ca1574681e578906a = echarts.init(\rdocument.getElementById('e97ab300531a4c0ca1574681e578906a'), 'white', {renderer: 'canvas'});\rvar option_e97ab300531a4c0ca1574681e578906a = {\r\"animation\": true,\r\"animationThreshold\": 2000,\r\"animationDuration\": 1000,\r\"animationEasing\": \"cubicOut\",\r\"animationDelay\": 0,\r\"animationDurationUpdate\": 300,\r\"animationEasingUpdate\": \"cubicOut\",\r\"animationDelayUpdate\": 0,\r\"color\": [\r\"#c23531\",\r\"#2f4554\",\r\"#61a0a8\",\r\"#d48265\",\r\"#749f83\",\r\"#ca8622\",\r\"#bda29a\",\r\"#6e7074\",\r\"#546570\",\r\"#c4ccd3\",\r\"#f05b72\",\r\"#ef5b9c\",\r\"#f47920\",\r\"#905a3d\",\r\"#fab27b\",\r\"#2a5caa\",\r\"#444693\",\r\"#726930\",\r\"#b2d235\",\r\"#6d8346\",\r\"#ac6767\",\r\"#1d953f\",\r\"#6950a1\",\r\"#918597\"\r],\r\"series\": [\r{\r\"type\": \"graph\",\r\"layout\": \"circular\",\r\"symbolSize\": 10,\r\"circular\": {\r\"rotateLabel\": true\r},\r\"force\": {\r\"repulsion\": 50,\r\"edgeLength\": 50,\r\"gravity\": 0.2\r},\r\"label\": {\r\"show\": true,\r\"position\": \"right\",\r\"margin\": 8\r},\r\"lineStyle\": {\r\"show\": true,\r\"width\": 1,\r\"opacity\": 1,\r\"curveness\": 0.3,\r\"type\": \"solid\",\r\"color\": \"source\"\r},\r\"roam\": true,\r\"draggable\": false,\r\"focusNodeAdjacency\": true,\r\"data\": [\r{\r\"name\": \"\\u6c6a\\u6dfc\",\r\"symbolSize\": 64.1,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 64.1\r},\r{\r\"name\": \"\\u53f2\\u5f3a\",\r\"symbolSize\": 24.7,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 24.7\r},\r{\r\"name\": \"\\u5e38\\u4f1f\\u601d\",\r\"symbolSize\": 9.4,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 9.4\r},\r{\r\"name\": \"\\u6768\\u51ac\",\r\"symbolSize\": 6.5,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 6.5\r},\r{\r\"name\": \"\\u4e01\\u4eea\",\r\"symbolSize\": 17.2,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 17.2\r},\r{\r\"name\": \"\\u7533\\u7389\\u83f2\",\r\"symbolSize\": 4.9,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 4.9\r},\r{\r\"name\": \"\\u8c46\\u8c46\",\r\"symbolSize\": 0.3,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.3\r},\r{\r\"name\": \"\\u9b4f\\u6210\",\r\"symbolSize\": 3.0,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 3.0\r},\r{\r\"name\": \"\\u6f58\\u5bd2\",\r\"symbolSize\": 4.7,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 4.7\r},\r{\r\"name\": \"\\u674e\\u7476\",\r\"symbolSize\": 0.2,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.2\r},\r{\r\"name\": \"\\u53f6\\u6587\\u6d01\",\r\"symbolSize\": 47.2,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 47.2\r},\r{\r\"name\": \"\\u6960\\u6960\",\r\"symbolSize\": 0.3,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.3\r},\r{\r\"name\": \"\\u6d0b\\u6d0b\",\r\"symbolSize\": 0.1,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.1\r},\r{\r\"name\": \"\\u54aa\\u54aa\",\r\"symbolSize\": 0.1,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.1\r},\r{\r\"name\": \"\\u6c99\\u745e\\u5c71\",\r\"symbolSize\": 2.5,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 2.5\r},\r{\r\"name\": \"\\u53f6\\u54f2\\u6cf0\",\r\"symbolSize\": 3.0,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 3.0\r},\r{\r\"name\": \"\\u767d\\u6c90\\u9716\",\r\"symbolSize\": 2.9,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 2.9\r},\r{\r\"name\": \"\\u9a6c\\u94a2\",\r\"symbolSize\": 0.5,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.5\r},\r{\r\"name\": \"\\u7a0b\\u4e3d\\u534e\",\r\"symbolSize\": 0.9,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.9\r},\r{\r\"name\": \"\\u53f6\\u6587\\u96ea\",\r\"symbolSize\": 0.4,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.4\r},\r{\r\"name\": \"\\u96f7\\u5fd7\\u6210\",\r\"symbolSize\": 3.5,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 3.5\r},\r{\r\"name\": \"\\u6768\\u536b\\u5b81\",\r\"symbolSize\": 7.8,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 7.8\r},\r{\r\"name\": \"\\u5a01\\u5c14\\u900a\",\r\"symbolSize\": 0.4,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.4\r},\r{\r\"name\": \"\\u58a8\\u5b50\",\r\"symbolSize\": 3.1,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 3.1\r},\r{\r\"name\": \"\\u5f90\\u51b0\\u51b0\",\r\"symbolSize\": 0.7,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.7\r},\r{\r\"name\": \"\\u4f0a\\u6587\\u65af\",\r\"symbolSize\": 8.8,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 8.8\r},\r{\r\"name\": \"\\u62c9\\u83f2\\u5c14\",\r\"symbolSize\": 0.1,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.1\r},\r{\r\"name\": \"\\u6838\\u5f39\\u5973\\u5b69\",\r\"symbolSize\": 0.3,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.3\r},\r{\r\"name\": \"\\u9f50\\u730e\\u5934\",\r\"symbolSize\": 0.5,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.5\r},\r{\r\"name\": \"\\u5927\\u51e4\",\r\"symbolSize\": 1.1,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 1.1\r},\r{\r\"name\": \"\\u9ea6\\u514b\",\r\"symbolSize\": 0.8,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.8\r},\r{\r\"name\": \"\\u65af\\u5766\\u987f\",\r\"symbolSize\": 2.3,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 2.3\r},\r{\r\"name\": \"1379\\u53f7\\u76d1\\u542c\\u5458\",\r\"symbolSize\": 0.5,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.5\r},\r{\r\"name\": \"\\u5143\\u9996\",\r\"symbolSize\": 7.9,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 7.9\r},\r{\r\"name\": \"\\u667a\\u5b50\",\r\"symbolSize\": 46.4,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 46.4\r},\r{\r\"name\": \"\\u6797\\u4e91\",\r\"symbolSize\": 0.1,\r\"category\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"value\": 0.1\r},\r{\r\"name\": \"\\u7f57\\u8f91\",\r\"symbolSize\": 129.4,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 129.4\r},\r{\r\"name\": \"\\u5434\\u5cb3\",\r\"symbolSize\": 6.1,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 6.1\r},\r{\r\"name\": \"\\u7ae0\\u5317\\u6d77\",\r\"symbolSize\": 31.6,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 31.6\r},\r{\r\"name\": \"\\u96f7\\u5fb7\\u5c14\",\r\"symbolSize\": 1.0,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 1.0\r},\r{\r\"name\": \"\\u743c\\u65af\",\r\"symbolSize\": 0.9,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.9\r},\r{\r\"name\": \"\\u6590\\u5179\\u7f57\\u5c06\\u519b\",\r\"symbolSize\": 0.7,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.7\r},\r{\r\"name\": \"\\u5f20\\u63f4\\u671d\",\r\"symbolSize\": 4.3,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 4.3\r},\r{\r\"name\": \"\\u6768\\u664b\\u6587\",\r\"symbolSize\": 3.4,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 3.4\r},\r{\r\"name\": \"\\u82d7\\u798f\\u5168\",\r\"symbolSize\": 2.0,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 2.0\r},\r{\r\"name\": \"\\u53f2\\u6653\\u660e\",\r\"symbolSize\": 3.5,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 3.5\r},\r{\r\"name\": \"\\u4f3d\\u5c14\\u8bfa\\u592b\",\r\"symbolSize\": 0.1,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.1\r},\r{\r\"name\": \"\\u767d\\u84c9\",\r\"symbolSize\": 2.4,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 2.4\r},\r{\r\"name\": \"\\u574e\\u7279\",\r\"symbolSize\": 7.6,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 7.6\r},\r{\r\"name\": \"\\u8428\\u4f0a\",\r\"symbolSize\": 5.3,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 5.3\r},\r{\r\"name\": \"\\u5c71\\u6749\\u60e0\\u5b50\",\r\"symbolSize\": 6.4,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 6.4\r},\r{\r\"name\": \"\\u6797\\u683c\",\r\"symbolSize\": 4.8,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 4.8\r},\r{\r\"name\": \"\\u4f3d\\u5c14\\u5b81\",\r\"symbolSize\": 3.5,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 3.5\r},\r{\r\"name\": \"\\u827e\\u4f26\",\r\"symbolSize\": 2.1,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 2.1\r},\r{\r\"name\": \"\\u4e95\\u4e0a\\u5b8f\\u4e00\",\r\"symbolSize\": 0.6,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.6\r},\r{\r\"name\": \"\\u5e84\\u989c\",\r\"symbolSize\": 12.5,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 12.5\r},\r{\r\"name\": \"\\u54c8\\u91cc\\u65af\",\r\"symbolSize\": 0.3,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.3\r},\r{\r\"name\": \"\\u5f20\\u7fd4\",\r\"symbolSize\": 1.1,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 1.1\r},\r{\r\"name\": \"\\u5f20\\u5ef6\",\r\"symbolSize\": 0.5,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.5\r},\r{\r\"name\": \"\\u6653\\u8679\",\r\"symbolSize\": 0.1,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.1\r},\r{\r\"name\": \"\\u51ef\\u745f\\u7433\",\r\"symbolSize\": 0.1,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.1\r},\r{\r\"name\": \"\\u718a\\u6587\",\r\"symbolSize\": 1.5,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 1.5\r},\r{\r\"name\": \"\\u90ed\\u6b63\\u660e\",\r\"symbolSize\": 0.3,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.3\r},\r{\r\"name\": \"\\u4e1c\\u65b9\\u5ef6\\u7eea\",\r\"symbolSize\": 8.4,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 8.4\r},\r{\r\"name\": \"\\u80af\\u535a\\u58eb\",\r\"symbolSize\": 0.4,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.4\r},\r{\r\"name\": \"\\u7f57\\u5bbe\\u900a\\u5c06\\u519b\",\r\"symbolSize\": 0.3,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.3\r},\r{\r\"name\": \"\\u5217\\u6587\",\r\"symbolSize\": 0.8,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.8\r},\r{\r\"name\": \"\\u4e95\\u4e0a\\u660e\",\r\"symbolSize\": 1.1,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 1.1\r},\r{\r\"name\": \"\\u897f\\u5b50\",\r\"symbolSize\": 2.9,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 2.9\r},\r{\r\"name\": \"\\u8d75\\u946b\",\r\"symbolSize\": 2.1,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 2.1\r},\r{\r\"name\": \"\\u674e\\u7ef4\",\r\"symbolSize\": 2.0,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 2.0\r},\r{\r\"name\": \"\\u84dd\\u897f\",\r\"symbolSize\": 3.0,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 3.0\r},\r{\r\"name\": \"\\u65af\\u79d1\\u7279\",\r\"symbolSize\": 2.3,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 2.3\r},\r{\r\"name\": \"\\u6cd5\\u624e\\u5170\",\r\"symbolSize\": 3.1,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 3.1\r},\r{\r\"name\": \"\\u72c4\\u5965\\u4f26\\u5a1c\",\r\"symbolSize\": 5.2,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 5.2\r},\r{\r\"name\": \"\\u4e91\\u5929\\u660e\",\r\"symbolSize\": 31.6,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 31.6\r},\r{\r\"name\": \"\\u5f20\\u533b\\u751f\",\r\"symbolSize\": 1.6,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 1.6\r},\r{\r\"name\": \"\\u8001\\u674e\",\r\"symbolSize\": 2.4,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 2.4\r},\r{\r\"name\": \"\\u80e1\\u6587\",\r\"symbolSize\": 2.3,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 2.3\r},\r{\r\"name\": \"\\u7a0b\\u5fc3\",\r\"symbolSize\": 151.7,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 151.7\r},\r{\r\"name\": \"\\u4f55\\u535a\\u58eb\",\r\"symbolSize\": 2.1,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 2.1\r},\r{\r\"name\": \"\\u4e8e\\u7ef4\\u6c11\",\r\"symbolSize\": 0.4,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 0.4\r},\r{\r\"name\": \"\\u67ef\\u66fc\\u7433\",\r\"symbolSize\": 1.0,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 1.0\r},\r{\r\"name\": \"\\u4e54\\u4f9d\\u5a1c\",\r\"symbolSize\": 0.3,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 0.3\r},\r{\r\"name\": \"\\u516c\\u5143\\u4eba\",\r\"symbolSize\": 1.6,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 1.6\r},\r{\r\"name\": \"\\u6bd5\\u4e91\\u5cf0\",\r\"symbolSize\": 2.4,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 2.4\r},\r{\r\"name\": \"\\u66f9\\u5f6c\",\r\"symbolSize\": 8.8,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 8.8\r},\r{\r\"name\": \"\\u4f0a\\u4e07\",\r\"symbolSize\": 0.5,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 0.5\r},\r{\r\"name\": \"\\u5b89\\u4e1c\\u8bfa\\u592b\",\r\"symbolSize\": 0.4,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 0.4\r},\r{\r\"name\": \"\\u97e6\\u65af\\u7279\",\r\"symbolSize\": 2.8,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 2.8\r},\r{\r\"name\": \"\\u6234\\u6587\",\r\"symbolSize\": 1.1,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 1.1\r},\r{\r\"name\": \"\\u6734\\u4e49\\u541b\",\r\"symbolSize\": 0.7,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 0.7\r},\r{\r\"name\": \"\\u827e\\u514b\",\r\"symbolSize\": 2.5,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 2.5\r},\r{\r\"name\": \"\\u8587\\u62c9\",\r\"symbolSize\": 1.9,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 1.9\r},\r{\r\"name\": \"\\u5218\\u6653\\u660e\",\r\"symbolSize\": 0.2,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 0.2\r},\r{\r\"name\": \"\\u5173\\u4e00\\u5e06\",\r\"symbolSize\": 26.2,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 26.2\r},\r{\r\"name\": \"\\u5f17\\u96f7\\u65af\",\r\"symbolSize\": 3.8,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 3.8\r},\r{\r\"name\": \"\\u79cb\\u539f\\u73b2\\u5b50\",\r\"symbolSize\": 0.2,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 0.2\r},\r{\r\"name\": \"\\u891a\\u5ca9\",\r\"symbolSize\": 0.5,\r\"category\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"value\": 0.5\r},\r{\r\"name\": \"\\u5353\\u6587\",\r\"symbolSize\": 1.0,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 1.0\r},\r{\r\"name\": \"\\u6df1\\u6c34\\u738b\\u5b50\",\r\"symbolSize\": 3.4,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 3.4\r},\r{\r\"name\": \"\\u9732\\u73e0\\u516c\\u4e3b\",\r\"symbolSize\": 3.1,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 3.1\r},\r{\r\"name\": \"\\u7a7a\\u7075\\u753b\\u5e08\",\r\"symbolSize\": 2.4,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 2.4\r},\r{\r\"name\": \"\\u9488\\u773c\\u753b\\u5e08\",\r\"symbolSize\": 3.1,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 3.1\r},\r{\r\"name\": \"\\u957f\\u5e06\",\r\"symbolSize\": 2.3,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 2.3\r},\r{\r\"name\": \"\\u5df4\\u52d2\\u83ab\",\r\"symbolSize\": 0.8,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 0.8\r},\r{\r\"name\": \"\\u6770\\u68ee\",\r\"symbolSize\": 2.4,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 2.4\r},\r{\r\"name\": \"\\u5a01\\u7eb3\\u5c14\",\r\"symbolSize\": 1.3,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 1.3\r},\r{\r\"name\": \"\\u9ad8way\",\r\"symbolSize\": 2.1,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 2.1\r},\r{\r\"name\": \"\\u5e03\\u83b1\\u5c14\",\r\"symbolSize\": 0.2,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 0.2\r},\r{\r\"name\": \"\\u6b4c\\u8005\",\r\"symbolSize\": 4.9,\r\"category\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\",\r\"value\": 4.9\r}\r],\r\"categories\": [\r{\r\"name\": \"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\"\r},\r{\r\"name\": \"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\"\r},\r{\r\"name\": \"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\"\r}\r],\r\"edgeLabel\": {\r\"show\": false,\r\"position\": \"top\",\r\"margin\": 8\r},\r\"edgeSymbol\": [\rnull,\rnull\r],\r\"edgeSymbolSize\": 10,\r\"links\": [\r{\r\"id\": 0,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u53f2\\u5f3a\"\r},\r{\r\"id\": 1,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u5e38\\u4f1f\\u601d\"\r},\r{\r\"id\": 2,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u6768\\u51ac\"\r},\r{\r\"id\": 3,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u4e01\\u4eea\"\r},\r{\r\"id\": 4,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u7533\\u7389\\u83f2\"\r},\r{\r\"id\": 5,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u9b4f\\u6210\"\r},\r{\r\"id\": 6,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u6f58\\u5bd2\"\r},\r{\r\"id\": 7,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 8,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u6c99\\u745e\\u5c71\"\r},\r{\r\"id\": 9,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u58a8\\u5b50\"\r},\r{\r\"id\": 10,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u6768\\u536b\\u5b81\"\r},\r{\r\"id\": 11,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u5f90\\u51b0\\u51b0\"\r},\r{\r\"id\": 12,\r\"source\": \"\\u6c6a\\u6dfc\",\r\"target\": \"\\u65af\\u5766\\u987f\"\r},\r{\r\"id\": 13,\r\"source\": \"\\u53f2\\u5f3a\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 14,\r\"source\": \"\\u53f2\\u5f3a\",\r\"target\": \"\\u5e38\\u4f1f\\u601d\"\r},\r{\r\"id\": 15,\r\"source\": \"\\u53f2\\u5f3a\",\r\"target\": \"\\u65af\\u5766\\u987f\"\r},\r{\r\"id\": 16,\r\"source\": \"\\u53f2\\u5f3a\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 17,\r\"source\": \"\\u53f2\\u5f3a\",\r\"target\": \"\\u574e\\u7279\"\r},\r{\r\"id\": 18,\r\"source\": \"\\u53f2\\u5f3a\",\r\"target\": \"\\u53f2\\u6653\\u660e\"\r},\r{\r\"id\": 19,\r\"source\": \"\\u5e38\\u4f1f\\u601d\",\r\"target\": \"\\u53f2\\u5f3a\"\r},\r{\r\"id\": 20,\r\"source\": \"\\u5e38\\u4f1f\\u601d\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 21,\r\"source\": \"\\u5e38\\u4f1f\\u601d\",\r\"target\": \"\\u7ae0\\u5317\\u6d77\"\r},\r{\r\"id\": 22,\r\"source\": \"\\u6768\\u51ac\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 23,\r\"source\": \"\\u6768\\u51ac\",\r\"target\": \"\\u4e01\\u4eea\"\r},\r{\r\"id\": 24,\r\"source\": \"\\u6768\\u51ac\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 25,\r\"source\": \"\\u6768\\u51ac\",\r\"target\": \"\\u9f50\\u730e\\u5934\"\r},\r{\r\"id\": 26,\r\"source\": \"\\u6768\\u51ac\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 27,\r\"source\": \"\\u4e01\\u4eea\",\r\"target\": \"\\u6768\\u51ac\"\r},\r{\r\"id\": 28,\r\"source\": \"\\u4e01\\u4eea\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 29,\r\"source\": \"\\u4e01\\u4eea\",\r\"target\": \"\\u667a\\u5b50\"\r},\r{\r\"id\": 30,\r\"source\": \"\\u4e01\\u4eea\",\r\"target\": \"\\u7ae0\\u5317\\u6d77\"\r},\r{\r\"id\": 31,\r\"source\": \"\\u4e01\\u4eea\",\r\"target\": \"\\u897f\\u5b50\"\r},\r{\r\"id\": 32,\r\"source\": \"\\u7533\\u7389\\u83f2\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 33,\r\"source\": \"\\u7533\\u7389\\u83f2\",\r\"target\": \"\\u9b4f\\u6210\"\r},\r{\r\"id\": 34,\r\"source\": \"\\u7533\\u7389\\u83f2\",\r\"target\": \"\\u6f58\\u5bd2\"\r},\r{\r\"id\": 35,\r\"source\": \"\\u9b4f\\u6210\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 36,\r\"source\": \"\\u9b4f\\u6210\",\r\"target\": \"\\u7533\\u7389\\u83f2\"\r},\r{\r\"id\": 37,\r\"source\": \"\\u6f58\\u5bd2\",\r\"target\": \"\\u7533\\u7389\\u83f2\"\r},\r{\r\"id\": 38,\r\"source\": \"\\u6f58\\u5bd2\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 39,\r\"source\": \"\\u6f58\\u5bd2\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 40,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 41,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u6768\\u51ac\"\r},\r{\r\"id\": 42,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u6c99\\u745e\\u5c71\"\r},\r{\r\"id\": 43,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u53f6\\u54f2\\u6cf0\"\r},\r{\r\"id\": 44,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u767d\\u6c90\\u9716\"\r},\r{\r\"id\": 45,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u7a0b\\u4e3d\\u534e\"\r},\r{\r\"id\": 46,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u53f6\\u6587\\u96ea\"\r},\r{\r\"id\": 47,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u6768\\u536b\\u5b81\"\r},\r{\r\"id\": 48,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u96f7\\u5fd7\\u6210\"\r},\r{\r\"id\": 49,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u6f58\\u5bd2\"\r},\r{\r\"id\": 50,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u4f0a\\u6587\\u65af\"\r},\r{\r\"id\": 51,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u9f50\\u730e\\u5934\"\r},\r{\r\"id\": 52,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u5927\\u51e4\"\r},\r{\r\"id\": 53,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u667a\\u5b50\"\r},\r{\r\"id\": 54,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 55,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u8428\\u4f0a\"\r},\r{\r\"id\": 56,\r\"source\": \"\\u53f6\\u6587\\u6d01\",\r\"target\": \"\\u5e84\\u989c\"\r},\r{\r\"id\": 57,\r\"source\": \"\\u6c99\\u745e\\u5c71\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 58,\r\"source\": \"\\u6c99\\u745e\\u5c71\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 59,\r\"source\": \"\\u53f6\\u54f2\\u6cf0\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 60,\r\"source\": \"\\u53f6\\u54f2\\u6cf0\",\r\"target\": \"\\u6768\\u536b\\u5b81\"\r},\r{\r\"id\": 61,\r\"source\": \"\\u767d\\u6c90\\u9716\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 62,\r\"source\": \"\\u7a0b\\u4e3d\\u534e\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 63,\r\"source\": \"\\u53f6\\u6587\\u96ea\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 64,\r\"source\": \"\\u96f7\\u5fd7\\u6210\",\r\"target\": \"\\u6768\\u536b\\u5b81\"\r},\r{\r\"id\": 65,\r\"source\": \"\\u96f7\\u5fd7\\u6210\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 66,\r\"source\": \"\\u6768\\u536b\\u5b81\",\r\"target\": \"\\u96f7\\u5fd7\\u6210\"\r},\r{\r\"id\": 67,\r\"source\": \"\\u6768\\u536b\\u5b81\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 68,\r\"source\": \"\\u6768\\u536b\\u5b81\",\r\"target\": \"\\u53f6\\u54f2\\u6cf0\"\r},\r{\r\"id\": 69,\r\"source\": \"\\u6768\\u536b\\u5b81\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 70,\r\"source\": \"\\u58a8\\u5b50\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 71,\r\"source\": \"\\u5f90\\u51b0\\u51b0\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 72,\r\"source\": \"\\u4f0a\\u6587\\u65af\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 73,\r\"source\": \"\\u4f0a\\u6587\\u65af\",\r\"target\": \"\\u9ea6\\u514b\"\r},\r{\r\"id\": 74,\r\"source\": \"\\u4f0a\\u6587\\u65af\",\r\"target\": \"\\u667a\\u5b50\"\r},\r{\r\"id\": 75,\r\"source\": \"\\u9f50\\u730e\\u5934\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 76,\r\"source\": \"\\u9f50\\u730e\\u5934\",\r\"target\": \"\\u6768\\u51ac\"\r},\r{\r\"id\": 77,\r\"source\": \"\\u9f50\\u730e\\u5934\",\r\"target\": \"\\u5927\\u51e4\"\r},\r{\r\"id\": 78,\r\"source\": \"\\u5927\\u51e4\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 79,\r\"source\": \"\\u5927\\u51e4\",\r\"target\": \"\\u9f50\\u730e\\u5934\"\r},\r{\r\"id\": 80,\r\"source\": \"\\u9ea6\\u514b\",\r\"target\": \"\\u4f0a\\u6587\\u65af\"\r},\r{\r\"id\": 81,\r\"source\": \"\\u65af\\u5766\\u987f\",\r\"target\": \"\\u53f2\\u5f3a\"\r},\r{\r\"id\": 82,\r\"source\": \"\\u65af\\u5766\\u987f\",\r\"target\": \"\\u6c6a\\u6dfc\"\r},\r{\r\"id\": 83,\r\"source\": \"\\u5143\\u9996\",\r\"target\": \"\\u667a\\u5b50\"\r},\r{\r\"id\": 84,\r\"source\": \"\\u667a\\u5b50\",\r\"target\": \"\\u5143\\u9996\"\r},\r{\r\"id\": 85,\r\"source\": \"\\u667a\\u5b50\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 86,\r\"source\": \"\\u667a\\u5b50\",\r\"target\": \"\\u4e01\\u4eea\"\r},\r{\r\"id\": 87,\r\"source\": \"\\u667a\\u5b50\",\r\"target\": \"\\u4f0a\\u6587\\u65af\"\r},\r{\r\"id\": 88,\r\"source\": \"\\u667a\\u5b50\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 89,\r\"source\": \"\\u667a\\u5b50\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 90,\r\"source\": \"\\u667a\\u5b50\",\r\"target\": \"\\u6234\\u6587\"\r},\r{\r\"id\": 91,\r\"source\": \"\\u667a\\u5b50\",\r\"target\": \"\\u5f17\\u96f7\\u65af\"\r},\r{\r\"id\": 92,\r\"source\": \"\\u667a\\u5b50\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 93,\r\"source\": \"\\u667a\\u5b50\",\r\"target\": \"\\u5173\\u4e00\\u5e06\"\r},\r{\r\"id\": 94,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u6768\\u51ac\"\r},\r{\r\"id\": 95,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u53f2\\u5f3a\"\r},\r{\r\"id\": 96,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u7ae0\\u5317\\u6d77\"\r},\r{\r\"id\": 97,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u767d\\u84c9\"\r},\r{\r\"id\": 98,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u574e\\u7279\"\r},\r{\r\"id\": 99,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u8428\\u4f0a\"\r},\r{\r\"id\": 100,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u4f3d\\u5c14\\u5b81\"\r},\r{\r\"id\": 101,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u5e84\\u989c\"\r},\r{\r\"id\": 102,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u667a\\u5b50\"\r},\r{\r\"id\": 103,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 104,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u6797\\u683c\"\r},\r{\r\"id\": 105,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u5f20\\u7fd4\"\r},\r{\r\"id\": 106,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u5c71\\u6749\\u60e0\\u5b50\"\r},\r{\r\"id\": 107,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u718a\\u6587\"\r},\r{\r\"id\": 108,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u53f2\\u6653\\u660e\"\r},\r{\r\"id\": 109,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 110,\r\"source\": \"\\u7f57\\u8f91\",\r\"target\": \"\\u5f17\\u96f7\\u65af\"\r},\r{\r\"id\": 111,\r\"source\": \"\\u5434\\u5cb3\",\r\"target\": \"\\u7ae0\\u5317\\u6d77\"\r},\r{\r\"id\": 112,\r\"source\": \"\\u7ae0\\u5317\\u6d77\",\r\"target\": \"\\u5434\\u5cb3\"\r},\r{\r\"id\": 113,\r\"source\": \"\\u7ae0\\u5317\\u6d77\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 114,\r\"source\": \"\\u7ae0\\u5317\\u6d77\",\r\"target\": \"\\u5e38\\u4f1f\\u601d\"\r},\r{\r\"id\": 115,\r\"source\": \"\\u7ae0\\u5317\\u6d77\",\r\"target\": \"\\u4e01\\u4eea\"\r},\r{\r\"id\": 116,\r\"source\": \"\\u7ae0\\u5317\\u6d77\",\r\"target\": \"\\u4e1c\\u65b9\\u5ef6\\u7eea\"\r},\r{\r\"id\": 117,\r\"source\": \"\\u7ae0\\u5317\\u6d77\",\r\"target\": \"\\u84dd\\u897f\"\r},\r{\r\"id\": 118,\r\"source\": \"\\u6590\\u5179\\u7f57\\u5c06\\u519b\",\r\"target\": \"\\u6797\\u683c\"\r},\r{\r\"id\": 119,\r\"source\": \"\\u5f20\\u63f4\\u671d\",\r\"target\": \"\\u6768\\u664b\\u6587\"\r},\r{\r\"id\": 120,\r\"source\": \"\\u5f20\\u63f4\\u671d\",\r\"target\": \"\\u82d7\\u798f\\u5168\"\r},\r{\r\"id\": 121,\r\"source\": \"\\u5f20\\u63f4\\u671d\",\r\"target\": \"\\u53f2\\u6653\\u660e\"\r},\r{\r\"id\": 122,\r\"source\": \"\\u6768\\u664b\\u6587\",\r\"target\": \"\\u5f20\\u63f4\\u671d\"\r},\r{\r\"id\": 123,\r\"source\": \"\\u6768\\u664b\\u6587\",\r\"target\": \"\\u82d7\\u798f\\u5168\"\r},\r{\r\"id\": 124,\r\"source\": \"\\u82d7\\u798f\\u5168\",\r\"target\": \"\\u5f20\\u63f4\\u671d\"\r},\r{\r\"id\": 125,\r\"source\": \"\\u82d7\\u798f\\u5168\",\r\"target\": \"\\u6768\\u664b\\u6587\"\r},\r{\r\"id\": 126,\r\"source\": \"\\u53f2\\u6653\\u660e\",\r\"target\": \"\\u5f20\\u63f4\\u671d\"\r},\r{\r\"id\": 127,\r\"source\": \"\\u53f2\\u6653\\u660e\",\r\"target\": \"\\u53f2\\u5f3a\"\r},\r{\r\"id\": 128,\r\"source\": \"\\u53f2\\u6653\\u660e\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 129,\r\"source\": \"\\u767d\\u84c9\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 130,\r\"source\": \"\\u574e\\u7279\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 131,\r\"source\": \"\\u574e\\u7279\",\r\"target\": \"\\u53f2\\u5f3a\"\r},\r{\r\"id\": 132,\r\"source\": \"\\u574e\\u7279\",\r\"target\": \"\\u5f20\\u7fd4\"\r},\r{\r\"id\": 133,\r\"source\": \"\\u8428\\u4f0a\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 134,\r\"source\": \"\\u8428\\u4f0a\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 135,\r\"source\": \"\\u8428\\u4f0a\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 136,\r\"source\": \"\\u5c71\\u6749\\u60e0\\u5b50\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 137,\r\"source\": \"\\u6797\\u683c\",\r\"target\": \"\\u6590\\u5179\\u7f57\\u5c06\\u519b\"\r},\r{\r\"id\": 138,\r\"source\": \"\\u6797\\u683c\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 139,\r\"source\": \"\\u6797\\u683c\",\r\"target\": \"\\u80af\\u535a\\u58eb\"\r},\r{\r\"id\": 140,\r\"source\": \"\\u4f3d\\u5c14\\u5b81\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 141,\r\"source\": \"\\u5e84\\u989c\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 142,\r\"source\": \"\\u5e84\\u989c\",\r\"target\": \"\\u53f6\\u6587\\u6d01\"\r},\r{\r\"id\": 143,\r\"source\": \"\\u5f20\\u7fd4\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 144,\r\"source\": \"\\u5f20\\u7fd4\",\r\"target\": \"\\u574e\\u7279\"\r},\r{\r\"id\": 145,\r\"source\": \"\\u718a\\u6587\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 146,\r\"source\": \"\\u4e1c\\u65b9\\u5ef6\\u7eea\",\r\"target\": \"\\u7ae0\\u5317\\u6d77\"\r},\r{\r\"id\": 147,\r\"source\": \"\\u4e1c\\u65b9\\u5ef6\\u7eea\",\r\"target\": \"\\u84dd\\u897f\"\r},\r{\r\"id\": 148,\r\"source\": \"\\u80af\\u535a\\u58eb\",\r\"target\": \"\\u6797\\u683c\"\r},\r{\r\"id\": 149,\r\"source\": \"\\u5217\\u6587\",\r\"target\": \"\\u4e95\\u4e0a\\u660e\"\r},\r{\r\"id\": 150,\r\"source\": \"\\u4e95\\u4e0a\\u660e\",\r\"target\": \"\\u5217\\u6587\"\r},\r{\r\"id\": 151,\r\"source\": \"\\u897f\\u5b50\",\r\"target\": \"\\u4e01\\u4eea\"\r},\r{\r\"id\": 152,\r\"source\": \"\\u8d75\\u946b\",\r\"target\": \"\\u674e\\u7ef4\"\r},\r{\r\"id\": 153,\r\"source\": \"\\u674e\\u7ef4\",\r\"target\": \"\\u8d75\\u946b\"\r},\r{\r\"id\": 154,\r\"source\": \"\\u84dd\\u897f\",\r\"target\": \"\\u7ae0\\u5317\\u6d77\"\r},\r{\r\"id\": 155,\r\"source\": \"\\u84dd\\u897f\",\r\"target\": \"\\u4e1c\\u65b9\\u5ef6\\u7eea\"\r},\r{\r\"id\": 156,\r\"source\": \"\\u84dd\\u897f\",\r\"target\": \"\\u65af\\u79d1\\u7279\"\r},\r{\r\"id\": 157,\r\"source\": \"\\u65af\\u79d1\\u7279\",\r\"target\": \"\\u84dd\\u897f\"\r},\r{\r\"id\": 158,\r\"source\": \"\\u6cd5\\u624e\\u5170\",\r\"target\": \"\\u72c4\\u5965\\u4f26\\u5a1c\"\r},\r{\r\"id\": 159,\r\"source\": \"\\u72c4\\u5965\\u4f26\\u5a1c\",\r\"target\": \"\\u6cd5\\u624e\\u5170\"\r},\r{\r\"id\": 160,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u5f20\\u533b\\u751f\"\r},\r{\r\"id\": 161,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u8001\\u674e\"\r},\r{\r\"id\": 162,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u80e1\\u6587\"\r},\r{\r\"id\": 163,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 164,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u4f55\\u535a\\u58eb\"\r},\r{\r\"id\": 165,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u4e8e\\u7ef4\\u6c11\"\r},\r{\r\"id\": 166,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u667a\\u5b50\"\r},\r{\r\"id\": 167,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u6df1\\u6c34\\u738b\\u5b50\"\r},\r{\r\"id\": 168,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u7a7a\\u7075\\u753b\\u5e08\"\r},\r{\r\"id\": 169,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u957f\\u5e06\"\r},\r{\r\"id\": 170,\r\"source\": \"\\u4e91\\u5929\\u660e\",\r\"target\": \"\\u5173\\u4e00\\u5e06\"\r},\r{\r\"id\": 171,\r\"source\": \"\\u5f20\\u533b\\u751f\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 172,\r\"source\": \"\\u5f20\\u533b\\u751f\",\r\"target\": \"\\u8001\\u674e\"\r},\r{\r\"id\": 173,\r\"source\": \"\\u8001\\u674e\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 174,\r\"source\": \"\\u8001\\u674e\",\r\"target\": \"\\u5f20\\u533b\\u751f\"\r},\r{\r\"id\": 175,\r\"source\": \"\\u80e1\\u6587\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 176,\r\"source\": \"\\u80e1\\u6587\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 177,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 178,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u80e1\\u6587\"\r},\r{\r\"id\": 179,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u4f55\\u535a\\u58eb\"\r},\r{\r\"id\": 180,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u4e8e\\u7ef4\\u6c11\"\r},\r{\r\"id\": 181,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u67ef\\u66fc\\u7433\"\r},\r{\r\"id\": 182,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u8428\\u4f0a\"\r},\r{\r\"id\": 183,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u516c\\u5143\\u4eba\"\r},\r{\r\"id\": 184,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u667a\\u5b50\"\r},\r{\r\"id\": 185,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u6bd5\\u4e91\\u5cf0\"\r},\r{\r\"id\": 186,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 187,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u5f17\\u96f7\\u65af\"\r},\r{\r\"id\": 188,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u66f9\\u5f6c\"\r},\r{\r\"id\": 189,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u6770\\u68ee\"\r},\r{\r\"id\": 190,\r\"source\": \"\\u7a0b\\u5fc3\",\r\"target\": \"\\u5173\\u4e00\\u5e06\"\r},\r{\r\"id\": 191,\r\"source\": \"\\u4f55\\u535a\\u58eb\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 192,\r\"source\": \"\\u4f55\\u535a\\u58eb\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 193,\r\"source\": \"\\u4e8e\\u7ef4\\u6c11\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 194,\r\"source\": \"\\u4e8e\\u7ef4\\u6c11\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 195,\r\"source\": \"\\u67ef\\u66fc\\u7433\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 196,\r\"source\": \"\\u516c\\u5143\\u4eba\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 197,\r\"source\": \"\\u6bd5\\u4e91\\u5cf0\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 198,\r\"source\": \"\\u6bd5\\u4e91\\u5cf0\",\r\"target\": \"\\u66f9\\u5f6c\"\r},\r{\r\"id\": 199,\r\"source\": \"\\u66f9\\u5f6c\",\r\"target\": \"\\u6bd5\\u4e91\\u5cf0\"\r},\r{\r\"id\": 200,\r\"source\": \"\\u66f9\\u5f6c\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 201,\r\"source\": \"\\u97e6\\u65af\\u7279\",\r\"target\": \"\\u6234\\u6587\"\r},\r{\r\"id\": 202,\r\"source\": \"\\u97e6\\u65af\\u7279\",\r\"target\": \"\\u5173\\u4e00\\u5e06\"\r},\r{\r\"id\": 203,\r\"source\": \"\\u97e6\\u65af\\u7279\",\r\"target\": \"\\u5353\\u6587\"\r},\r{\r\"id\": 204,\r\"source\": \"\\u6234\\u6587\",\r\"target\": \"\\u97e6\\u65af\\u7279\"\r},\r{\r\"id\": 205,\r\"source\": \"\\u6234\\u6587\",\r\"target\": \"\\u667a\\u5b50\"\r},\r{\r\"id\": 206,\r\"source\": \"\\u827e\\u514b\",\r\"target\": \"\\u8587\\u62c9\"\r},\r{\r\"id\": 207,\r\"source\": \"\\u8587\\u62c9\",\r\"target\": \"\\u827e\\u514b\"\r},\r{\r\"id\": 208,\r\"source\": \"\\u5173\\u4e00\\u5e06\",\r\"target\": \"\\u97e6\\u65af\\u7279\"\r},\r{\r\"id\": 209,\r\"source\": \"\\u5173\\u4e00\\u5e06\",\r\"target\": \"\\u5353\\u6587\"\r},\r{\r\"id\": 210,\r\"source\": \"\\u5173\\u4e00\\u5e06\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 211,\r\"source\": \"\\u5173\\u4e00\\u5e06\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 212,\r\"source\": \"\\u5173\\u4e00\\u5e06\",\r\"target\": \"\\u667a\\u5b50\"\r},\r{\r\"id\": 213,\r\"source\": \"\\u5f17\\u96f7\\u65af\",\r\"target\": \"\\u7a0b\\u5fc3\"\r},\r{\r\"id\": 214,\r\"source\": \"\\u5f17\\u96f7\\u65af\",\r\"target\": \"\\u667a\\u5b50\"\r},\r{\r\"id\": 215,\r\"source\": \"\\u5f17\\u96f7\\u65af\",\r\"target\": \"\\u7f57\\u8f91\"\r},\r{\r\"id\": 216,\r\"source\": \"\\u5353\\u6587\",\r\"target\": \"\\u5173\\u4e00\\u5e06\"\r},\r{\r\"id\": 217,\r\"source\": \"\\u5353\\u6587\",\r\"target\": \"\\u97e6\\u65af\\u7279\"\r},\r{\r\"id\": 218,\r\"source\": \"\\u6df1\\u6c34\\u738b\\u5b50\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 219,\r\"source\": \"\\u6df1\\u6c34\\u738b\\u5b50\",\r\"target\": \"\\u9732\\u73e0\\u516c\\u4e3b\"\r},\r{\r\"id\": 220,\r\"source\": \"\\u9732\\u73e0\\u516c\\u4e3b\",\r\"target\": \"\\u6df1\\u6c34\\u738b\\u5b50\"\r},\r{\r\"id\": 221,\r\"source\": \"\\u9732\\u73e0\\u516c\\u4e3b\",\r\"target\": \"\\u957f\\u5e06\"\r},\r{\r\"id\": 222,\r\"source\": \"\\u7a7a\\u7075\\u753b\\u5e08\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 223,\r\"source\": \"\\u7a7a\\u7075\\u753b\\u5e08\",\r\"target\": \"\\u957f\\u5e06\"\r},\r{\r\"id\": 224,\r\"source\": \"\\u957f\\u5e06\",\r\"target\": \"\\u9732\\u73e0\\u516c\\u4e3b\"\r},\r{\r\"id\": 225,\r\"source\": \"\\u957f\\u5e06\",\r\"target\": \"\\u4e91\\u5929\\u660e\"\r},\r{\r\"id\": 226,\r\"source\": \"\\u957f\\u5e06\",\r\"target\": \"\\u7a7a\\u7075\\u753b\\u5e08\"\r},\r{\r\"id\": 227,\r\"source\": \"\\u6770\\u68ee\",\r\"target\": \"\\u7a0b\\u5fc3\"\r}\r]\r}\r],\r\"legend\": [\r{\r\"data\": [\r\"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\",\r\"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\",\r\"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\"\r],\r\"selected\": {\r\"\\u4e09\\u4f531-\\u5730\\u7403\\u5f80\\u4e8b\": true,\r\"\\u4e09\\u4f532-\\u9ed1\\u6697\\u68ee\\u6797\": true,\r\"\\u4e09\\u4f533-\\u6b7b\\u795e\\u6c38\\u751f\": true\r},\r\"show\": true,\r\"left\": \"2%\",\r\"top\": \"20%\",\r\"orient\": \"vertical\",\r\"padding\": 5,\r\"itemGap\": 10,\r\"itemWidth\": 25,\r\"itemHeight\": 14\r}\r],\r\"tooltip\": {\r\"show\": true,\r\"trigger\": \"item\",\r\"triggerOn\": \"mousemove|click\",\r\"axisPointer\": {\r\"type\": \"line\"\r},\r\"showContent\": true,\r\"alwaysShowContent\": false,\r\"showDelay\": 0,\r\"hideDelay\": 100,\r\"textStyle\": {\r\"fontSize\": 14\r},\r\"borderWidth\": 0,\r\"padding\": 5\r},\r\"title\": [\r{\r\"text\": \"\\u4e09\\u4f53\\u4eba\\u7269\\u5173\\u7cfb\\u56fe\",\r\"padding\": 5,\r\"itemGap\": 10\r}\r]\r};\rchart_e97ab300531a4c0ca1574681e578906a.setOption(option_e97ab300531a4c0ca1574681e578906a);\r\r\r\r\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-29-santi-relationship-visualization-with-pyecharts/","summary":"今天逛B站，看到《三体》三部曲全部人物关系数据可视化视频\n 于是动手写了今天的技术文。绘制人物网络关系图，需要有\n 节点数据； 节点名、节点属性 边数据； source、target、weight 可视化工具(Gephi软件或Python的可视化包)  本文代码 点击下载\n\n节点-人物数据 绘制人物关系网络图，首先需要有网络节点数据\n name 人物名称 desc 人物简介 stage 人物出现在三体小说的哪个阶段  import pandas as pd nodes_info_df = pd.read_excel(\u0026#39;data/三体人物.xlsx\u0026#39;) nodes_info_df Run\n\n边-人物关系数据 将节点数据依次在三体小说文本中按行进行检索，如果每行同时出现两个人物，两个人物会构建一个边。人物关系网络图可以用gephi软件进行绘制， 绘制需要两个csv文件，即\n 三体_nodes.csv 三体_edges.csv  实现代码如下\nimport jieba import codecs import csv for name in df[\u0026#39;name\u0026#39;].tolist(): jieba.add_word(name) nodes = {}\t# 姓名字典，保存人物，该字典的键为人物名称，值为该人物在全文中出现的次数 relationships = {}\t# 关系字典，保存人物关系的有向边，该字典的键为有向边的起点，值为一个字典edge，edge的键是有向边的终点，值是有向边的权值，代表两个人物之间联系的紧密程度 lineNodes = []\t# 每段内人物关系，是一个缓存变量，保存对每一段分词得到当前段中出现的人物名称，lineName[i]是一个列表，列表中存储第i段中出现过的人物 with open(\u0026#34;data/三体.txt\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: for line in f.","title":"可视化 | 绘制《三体》人物关系网络图"},{"content":"\r有时候学习新的数据分析方法时，需要构造一些小样本的实验数据，手动构造比较麻烦，这时候可以使用faker库。\n\n安装\rpip install Faker\r\n\r快速上手\rfrom faker import Faker\r#设定语言，默认生成的是英文数据\r#fake = Faker()\rfake = Faker(locale=\u0026#39;zh_CN\u0026#39;)\r#伪造姓名\rfake.name()\r## \u0026#39;罗辉\u0026#39;\r\n生成地址数据\nfake.address()\r## \u0026#39;湖北省张家港市大东王街v座 601815\u0026#39;\r\n生成公司数据\nfake.company_prefix()\r## \u0026#39;银嘉\u0026#39;\r\n\r自定义\r例如生成一个工作经历的实验数据，可以自定义公司名集合，从中随机抽取。\nfrom faker import Faker\rfake = Faker()\rmy_word_list = [\r\u0026#39;华为\u0026#39;,\u0026#39;小米\u0026#39;,\u0026#39;三星\u0026#39;,\r\u0026#39;海尔\u0026#39;,\u0026#39;宝马\u0026#39;,\u0026#39;保洁\u0026#39;,\r\u0026#39;中铁\u0026#39;,\u0026#39;中通\u0026#39;,\u0026#39;京东\u0026#39;,\r\u0026#39;阿里\u0026#39;,\u0026#39;百度\u0026#39;,\u0026#39;腾讯\u0026#39;]\rwork_experiences = fake.sentence(ext_word_list=my_word_list, nb_words=5)\rwork_experiences\r## \u0026#39;京东 华为 中通.\u0026#39;\r\n\r设定随机性\r由于faker属于随机生成数据的包，如果不限定状态， 每次运行相同的代码，随机生成的数据是不一样的。\nfrom faker import Faker\rfake = Faker(\u0026#39;zh_CN\u0026#39;)\r#设定随机状态\rFaker.seed(4321)\rprint(fake.name())\r## 王鑫\r\n\n\r广而告之\r\r长期征稿\r长期招募小伙伴\r付费视频课 | Python实证指标构建与文本分析\r\r\r","permalink":"/blog/2022-11-25-faker-generate-test-data/","summary":"有时候学习新的数据分析方法时，需要构造一些小样本的实验数据，手动构造比较麻烦，这时候可以使用faker库。\n\n安装\rpip install Faker\r\n\r快速上手\rfrom faker import Faker\r#设定语言，默认生成的是英文数据\r#fake = Faker()\rfake = Faker(locale=\u0026#39;zh_CN\u0026#39;)\r#伪造姓名\rfake.name()\r## \u0026#39;罗辉\u0026#39;\r\n生成地址数据\nfake.address()\r## \u0026#39;湖北省张家港市大东王街v座 601815\u0026#39;\r\n生成公司数据\nfake.company_prefix()\r## \u0026#39;银嘉\u0026#39;\r\n\r自定义\r例如生成一个工作经历的实验数据，可以自定义公司名集合，从中随机抽取。\nfrom faker import Faker\rfake = Faker()\rmy_word_list = [\r\u0026#39;华为\u0026#39;,\u0026#39;小米\u0026#39;,\u0026#39;三星\u0026#39;,\r\u0026#39;海尔\u0026#39;,\u0026#39;宝马\u0026#39;,\u0026#39;保洁\u0026#39;,\r\u0026#39;中铁\u0026#39;,\u0026#39;中通\u0026#39;,\u0026#39;京东\u0026#39;,\r\u0026#39;阿里\u0026#39;,\u0026#39;百度\u0026#39;,\u0026#39;腾讯\u0026#39;]\rwork_experiences = fake.sentence(ext_word_list=my_word_list, nb_words=5)\rwork_experiences\r## \u0026#39;京东 华为 中通.\u0026#39;\r\n\r设定随机性\r由于faker属于随机生成数据的包，如果不限定状态， 每次运行相同的代码，随机生成的数据是不一样的。\nfrom faker import Faker\rfake = Faker(\u0026#39;zh_CN\u0026#39;)\r#设定随机状态\rFaker.","title":"Faker库 | 生成实验数据"},{"content":"90w条中国上市公司高管简历，数据源-新浪财经，统计的日期范围1990-2021年。\n\n相关论文 这里粘贴部分应用高管数据论文\n- 何瑛,于文蕾,戴逸驰,王砚羽.高管职业经历与企业创新[J].管理世界,2019,35(11):174-192. - 杨林,和欣,顾红芳.高管团队经验、动态能力与企业战略突变：管理自主权的调节效应[J].管理世界,2020,36(06):168-188+201+252. - 周楷唐,麻志明,吴联生.高管学术经历与公司债务融资成本[J].经济研究,2017,52(07):169-183. - 陆瑶,张叶青,黎波,赵浩宇.高管个人特征与公司业绩——基于机器学习的经验证据[J].管理科学学报,2020,23(02):120-140. - 柳光强,孔高文.高管经管教育背景与企业内部薪酬差距[J].会计研究,2021,(03):110-121. - 郑建明,孙诗璐,李金甜.高管文化背景与企业债务成本——基于劳模文化的视角[J].会计研究,2021,(03):137-145. \n数据集字段  数据集的字段含，大多是从「个人简历」中计算衍生出来的。\n - ID - 姓名 - 证券代码 - 统计截止日期 - 个人简历 - 国籍 - 籍贯 - 籍贯所在地区代码 - 出生地 - 出生地所在地区代码 - 性别 - 年龄 - 毕业院校 - 学历 1=中专及中专以下； 2=大专； 3=本科； 4=硕士研究生； 5=博士研究生； 6=其他（以其他形式公布的学历，如荣誉博士、函授等）； 7=MBA/EMBA - 专业 - 职称 - 是否领取薪酬 - 报告期报酬总额 - 年末持股数 - 是否高管团队成员 - 是否董事会成员 - 是否独立董事 - 是否兼任董事长和CEO - 是否监事 - 具体职务 \n读取数据  数据文件 高管数据.xlsx 强制某几个字段的数据类型 将字段 「统计截止日期」 转化为 datetime 类型  import pandas as pd # 导入数据， df = pd.read_excel(\u0026#39;高管数据.xlsx\u0026#39;, #保证这两个字段是字符串格式 converters={\u0026#39;证券代码\u0026#39;: str, \u0026#39;ID\u0026#39;: str}) #将字段「统计截止日期」 整理为datetime格式 df[\u0026#39;统计截止日期\u0026#39;] = pd.to_datetime(df[\u0026#39;统计截止日期\u0026#39;]) #显示前1条记录 df.head(1) Run\ndf.columns Run\nIndex([\u0026#39;ID\u0026#39;, \u0026#39;姓名\u0026#39;, \u0026#39;证券代码\u0026#39;, \u0026#39;统计截止日期\u0026#39;, \u0026#39;个人简历\u0026#39;, \u0026#39;国籍\u0026#39;, \u0026#39;籍贯\u0026#39;, \u0026#39;籍贯所在地区代码\u0026#39;, \u0026#39;出生地\u0026#39;, \u0026#39;出生地所在地区代码\u0026#39;, \u0026#39;性别\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;毕业院校\u0026#39;, \u0026#39;学历\u0026#39;, \u0026#39;专业\u0026#39;, \u0026#39;职称\u0026#39;, \u0026#39;是否领取薪酬\u0026#39;, \u0026#39;报告期报酬总额\u0026#39;, \u0026#39;津贴\u0026#39;, \u0026#39;年末持股数\u0026#39;, \u0026#39;是否高管团队成员\u0026#39;, \u0026#39;是否董事会成员\u0026#39;, \u0026#39;是否独立董事\u0026#39;, \u0026#39;是否兼任董事长和CEO\u0026#39;, \u0026#39;是否监事\u0026#39;, \u0026#39;具体职务\u0026#39;], dtype=\u0026#39;object\u0026#39;) \n数据集记录数共\nlen(df) Run\n900887 \n数据统计日期范围自 1990年12月10日 至 2021年7月19日\ndf[\u0026#39;统计截止日期\u0026#39;].sort_values() Run\n900886 1990-12-10 900884 1990-12-10 900883 1990-12-10 900882 1990-12-10 900881 1990-12-10 ... 59734 2021-07-19 59733 2021-07-19 59731 2021-07-19 59736 2021-07-19 59742 2021-07-19 Name: 统计截止日期, Length: 900887, dtype: datetime64[ns] \n数据集字段 有\ndf.columns Run\nIndex([\u0026#39;ID\u0026#39;, \u0026#39;姓名\u0026#39;, \u0026#39;证券代码\u0026#39;, \u0026#39;统计截止日期\u0026#39;, \u0026#39;个人简历\u0026#39;, \u0026#39;国籍\u0026#39;, \u0026#39;籍贯\u0026#39;, \u0026#39;籍贯所在地区代码\u0026#39;, \u0026#39;出生地\u0026#39;, \u0026#39;出生地所在地区代码\u0026#39;, \u0026#39;性别\u0026#39;, \u0026#39;年龄\u0026#39;, \u0026#39;毕业院校\u0026#39;, \u0026#39;学历\u0026#39;, \u0026#39;专业\u0026#39;, \u0026#39;职称\u0026#39;, \u0026#39;是否领取薪酬\u0026#39;, \u0026#39;报告期报酬总额\u0026#39;, \u0026#39;津贴\u0026#39;, \u0026#39;年末持股数\u0026#39;, \u0026#39;是否高管团队成员\u0026#39;, \u0026#39;是否董事会成员\u0026#39;, \u0026#39;是否独立董事\u0026#39;, \u0026#39;是否兼任董事长和CEO\u0026#39;, \u0026#39;是否监事\u0026#39;, \u0026#39;具体职务\u0026#39;], dtype=\u0026#39;object\u0026#39;) \n截止统计日期时大于90岁的高管 记录有\ndf[df[\u0026#39;年龄\u0026#39;]\u0026gt;90] \n后续待分享 何瑛,于文蕾,戴逸驰,王砚羽.高管职业经历与企业创新[J].管理世界,2019,35(11):174-192.\n 摘要:管理的本质是一种实践,在某些情形下,阅历比简历更重要,丰富的职业经历有助于企业高管形成多元化的思维结构、广阔的管理视野、丰富的社会资源和过人的胆识,也是塑造复合型人才的重要路径。本文基于行为金融理论和高层梯队理论,手工搜集整理了2007～2016年中国沪深A股上市公司高管职业经历独特数据集,从职能部门、企业、行业、组织机构和地域类型五个维度构建了复合型职业经历的衡量指标——职业经历丰富度指数,对CEO职业经历与企业创新的影响因素和影响机理进行理论解释、数据分析和验证。研究结果表明:CEO职业经历越丰富,企业创新水平越高,其中跨企业经历对创新水平的影响最为显著,其次是跨行业经历和跨组织机构经历,跨职能部门经历和跨地域经历对企业创新水平的影响最小;影响因素方面,基于公司内外部治理的视角发现,市场化程度越低、企业融资约束程度越低时,CEO职业经历丰富度对企业创新水平的促进作用越明显,国有企业CEO职业经历丰富度对企业创新水平的促进作用更强,而股权制衡度对CEO职业经历丰富度与企业创新水平的调节作用不明显;影响机理方面,CEO复合型职业经历主要是通过丰富高管的社会网络资源以及增强高管的风险偏好倾向,从而提升企业的创新水平。本文的研究结论拓展了企业创新影响因素及高管职业经历经济后果领域的相关文献,将复合型人才的影响从国家宏观层面拓展到企业微观层面,为企业高层次人才的招聘和选拔提供新的证据支持。 中提到高管的创新\n 高管，一般是有多个企业经历的， 如何将高管职业经历转化为可以计算和比较的 高管职业经历向量 呢？\n后续有机会，大邓会使用该数据集 生成 高管职业经历向量 , 有了向量可以\n 计算高管之间的相似度 企业高管团队异质性，计算高管向量之间的距离 \u0026hellip;  \n数据获取 转发集赞 30+ ， 加微信 372335839, 备注【姓名-学校-专业-高管数据集】获取本数据集。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-25-senior-manager-resume-dataset/","summary":"90w条中国上市公司高管简历，数据源-新浪财经，统计的日期范围1990-2021年。\n\n相关论文 这里粘贴部分应用高管数据论文\n- 何瑛,于文蕾,戴逸驰,王砚羽.高管职业经历与企业创新[J].管理世界,2019,35(11):174-192. - 杨林,和欣,顾红芳.高管团队经验、动态能力与企业战略突变：管理自主权的调节效应[J].管理世界,2020,36(06):168-188+201+252. - 周楷唐,麻志明,吴联生.高管学术经历与公司债务融资成本[J].经济研究,2017,52(07):169-183. - 陆瑶,张叶青,黎波,赵浩宇.高管个人特征与公司业绩——基于机器学习的经验证据[J].管理科学学报,2020,23(02):120-140. - 柳光强,孔高文.高管经管教育背景与企业内部薪酬差距[J].会计研究,2021,(03):110-121. - 郑建明,孙诗璐,李金甜.高管文化背景与企业债务成本——基于劳模文化的视角[J].会计研究,2021,(03):137-145. \n数据集字段  数据集的字段含，大多是从「个人简历」中计算衍生出来的。\n - ID - 姓名 - 证券代码 - 统计截止日期 - 个人简历 - 国籍 - 籍贯 - 籍贯所在地区代码 - 出生地 - 出生地所在地区代码 - 性别 - 年龄 - 毕业院校 - 学历 1=中专及中专以下； 2=大专； 3=本科； 4=硕士研究生； 5=博士研究生； 6=其他（以其他形式公布的学历，如荣誉博士、函授等）； 7=MBA/EMBA - 专业 - 职称 - 是否领取薪酬 - 报告期报酬总额 - 年末持股数 - 是否高管团队成员 - 是否董事会成员 - 是否独立董事 - 是否兼任董事长和CEO - 是否监事 - 具体职务","title":"数据集 | 90w条中国上市公司高管数据"},{"content":"Glove可以捕捉到词语在语料库中的全局语义信息和类比信息， 据此基于语义向量计算刻板印象、文化变迁等，Glove模型在计算社会科学中拥有很大的应用潜力。\n训练Glove模型有两种实现方式\n C语言； https://nlp.stanford.edu/projects/glove/ Python语言；mittens、glove-python  方法比较    方法 优点 缺点     C语言 速度快，现成的代码工具 源代码仅支持英文, 需要付出较高的学习成本才能改动支持中文。 对文科生小白而言，门槛高   Python语言 mittens、glove-python等包语法简洁, 易上手 对文科生还是有一定的门槛，代码运行速度慢         不考虑性能约束条件，更多地考虑易用性，大邓简化了Python代码，将其内置到了cntext库。\n对词向量、词嵌入感兴趣的童鞋，可以阅读下列相关资料\n 转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用 转载 | 从符号到嵌入：计算社会科学的两种文本表示  GloVe代码 cntext支持中英文， 只需要7行代码，可完成导入数据、训练模型、保存结果。 这里以三体小说数据为例， 使用 data/santi.txt 。\n需要注意， santi.txt文件内文本是已经分词处理过的。这样可以在english这类西方语言模式下使用空格来区分词语的边界。\n 如果使用英文数据，下面代码只需要更改数据文件的路径即可。\n import cntext as ct import os #设置语言和项目文件夹路径 model = ct.Glove(cwd=os.getcwd(), lang=\u0026#39;english\u0026#39;) #导入语料 model.create_vocab(file=\u0026#39;data/santi.txt\u0026#39;, min_count=5) #构建词语共现矩阵 model.cooccurrence_matrix() #设置词嵌入模型的向量维度、迭代数 model.train_embeddings(vector_size=50, max_iter=25) #存储模型 model.save(model_name=\u0026#39;santi_glove_model\u0026#39;) Run\nBuilding prefix dict from the default dictionary ... Step 1/4: ...Create vocabulary for Glove. Dumping model to file cache C:\\Users\\Deng\\AppData\\Local\\Temp\\jieba.cache Loading model cost 0.628 seconds. Prefix dict has been built successfully. Step 2/4: ...Create cooccurrence matrix. Step 3/4: ...Train glove embeddings. Note, this part takes a long time to run Iteration 20: error 64925132.71550 Step 3/4: ... Finish! Use 316.91 s Step 4/4: ... Save the glove embeddings to a txt file \n导入GloVe预训练模型 训练好的GloVe模型是txt文件，可以使用gensim导入。\nfrom gensim.models import KeyedVectors # 导入GloVe模型文件 model = KeyedVectors.load_word2vec_format(\u0026#39;output/Glove/santi_glove_model.txt\u0026#39;, no_header=True) #查看某词的词向量 model.get_vector(\u0026#39;宇宙\u0026#39;) Run\narray([ 0.6618259 , 0.60663235, 0.9849417 , -1.028956 , 1.0711069 , -0.8875306 , -0.52833366, -1.0125595 , -0.9628481 , 1.0356479 , 0.8595257 , 0.7454354 , -1.0468111 , -0.26285014, -1.0310447 , 0.9906805 , 0.05825566, -0.85581344, -0.9932533 , -1.020438 , 1.0495061 , -0.6973389 , 0.49099424, -0.80775315, 0.64256483, 1.0157642 , 1.0135043 , -1.0131834 , 0.17376372, 0.89585054, 0.30890268, 0.798895 , 0.6653925 , 0.908629 , -1.048273 , -0.35683677, 0.06306187, -1.0267074 , -1.0494691 , 0.42172813, 0.24005401, 0.5934993 , -0.0696691 , -1.0360557 , -0.9797269 , 1.0205714 , -0.376359 , -1.0501183 , 1.0415571 , -0.9312968 ], dtype=float32) \n模型的使用 语料中所有的词语都是维度相同的向量，可以根据向量计算找近义词、反义词。可参考 之前分享的 豆瓣影评 | 探索词向量妙处\n参考资料  冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27. William L. Hamilton, Jure Leskovec, and Dan Jurafsky. ACL 2016. Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.  https://nlp.stanford.edu/projects/glove/   https://github.com/hiDaDeng/cntext  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-22-glove-embeddings-model/","summary":"Glove可以捕捉到词语在语料库中的全局语义信息和类比信息， 据此基于语义向量计算刻板印象、文化变迁等，Glove模型在计算社会科学中拥有很大的应用潜力。\n训练Glove模型有两种实现方式\n C语言； https://nlp.stanford.edu/projects/glove/ Python语言；mittens、glove-python  方法比较    方法 优点 缺点     C语言 速度快，现成的代码工具 源代码仅支持英文, 需要付出较高的学习成本才能改动支持中文。 对文科生小白而言，门槛高   Python语言 mittens、glove-python等包语法简洁, 易上手 对文科生还是有一定的门槛，代码运行速度慢         不考虑性能约束条件，更多地考虑易用性，大邓简化了Python代码，将其内置到了cntext库。\n对词向量、词嵌入感兴趣的童鞋，可以阅读下列相关资料\n 转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用 转载 | 从符号到嵌入：计算社会科学的两种文本表示  GloVe代码 cntext支持中英文， 只需要7行代码，可完成导入数据、训练模型、保存结果。 这里以三体小说数据为例， 使用 data/santi.txt 。\n需要注意， santi.txt文件内文本是已经分词处理过的。这样可以在english这类西方语言模式下使用空格来区分词语的边界。\n 如果使用英文数据，下面代码只需要更改数据文件的路径即可。\n import cntext as ct import os #设置语言和项目文件夹路径 model = ct.Glove(cwd=os.getcwd(), lang=\u0026#39;english\u0026#39;) #导入语料 model.","title":"训练\u0026使用Glove语言模型， 可度量刻板印象等"},{"content":"FinBERT介绍   FinBERT， 是使用49亿词的英文金融语料库数据，生成的BERT预训练语言模型。语料库上大小为 49亿个词。\n 公司报告 10-K 和 10-Q：25亿个词 电话会议记录：13亿个词 分析师报告：11亿个词  FinBERT开发者在多个金融 NLP 任务上对 FinBERT 预训练模型进行了微调，均优于传统机器学习模型、深度学习模型和微调 BERT 模型。 所有经过微调的 FinBERT 模型都公开托管在 Huggingface 🤗。 目前支持包括情绪分析、ESG 分类、前瞻性陈述 (FLS) 分类。\nHuang, Allen H., Hui Wang, and Yi Yang. \u0026#34;FinBERT: A large language model for extracting information from financial text.\u0026#34; Contemporary Accounting Research (2022). 摘要（翻译）: 我们开发了 FinBERT，这是一种适用于金融领域的最先进的大型语言模型。我们表明，FinBERT 结合了金融知识，可以更好地总结金融文本中的上下文信息。使用分析报告中研究人员标记的句子样本，我们证明 FinBERT 大大优于 Loughran 和 McDonald 词典以及其他机器学习算法，包括朴素贝叶斯、支持向量机、随机森林、卷积神经网络和长短期记忆，在情感分类中。我们的结果表明，FinBERT 擅长识别其他算法错误标记为中性的句子的正面或负面情绪，这可能是因为它使用了金融文本中的上下文信息。我们发现，FinBERT 优于其他算法，以及 Google 的原始双向编码器表示形式来自 transformers (BERT) 模型，当训练样本量较小且文本中包含一般文本中不常用的金融词时，这种优势尤为突出。 FinBERT 在识别与环境、社会和治理问题相关的讨论方面也优于其他模型。最后，我们表明，与 FinBERT 相比，其他方法低估了收益电话会议的文本信息量至少 18%。我们的结果对学术研究人员、投资专业人士和金融市场监管机构具有重要意义。 \nFinBERT功能 具体来说，FinBERT有以下内容：\n FinBERT-Pretrained： 针对大规模金融文本的预训练 FinBERT 模型。 FinBERT-Sentiment： 用于情感分类任务。 FinBERT-ESG： 用于 ESG 分类任务。 FinBERT-FLS： 用于前瞻性陈述（FLS）分类任务。  环境配置 pip install transformers==4.18.0 本次实验使用的transformers版本为\nimport transformers transformers.__version__ Run\n4.18.0 \n代码下载 点击下载\n\n一、情感分析 金融文本情绪可以调动管理者、信息中介和投资者的观点和意见, 因此分析金融文本情感(情绪)是有价值的。 FinBERT-Sentiment 是一个 FinBERT 模型，它根据标准普尔 500 家公司的分析师报告中的 10,000 个手动注释的句子进行了Fine-tune(微调)。\n Fine-Tune微调 是 深度学习的一种语言处理技术，可以在前人（已有）的语言模型文件基础上加入少量新场景的文本数据进行更新训练，生成出新场景的语言模型。\n  输入：金融文本。 输出：Positive, Neutral or Negative.  from transformers import BertTokenizer, BertForSequenceClassification, pipeline #首次运行，因为会下载FinBERT模型，耗时会比较久 senti_finbert = BertForSequenceClassification.from_pretrained(\u0026#39;yiyanghkust/finbert-tone\u0026#39;,num_labels=3) senti_tokenizer = BertTokenizer.from_pretrained(\u0026#39;yiyanghkust/finbert-tone\u0026#39;) senti_nlp = pipeline(\u0026#34;text-classification\u0026#34;, model=senti_finbert, tokenizer=senti_tokenizer) \n使用3条测试文本进行测试\n# 待分析的文本数据 senti_results = senti_nlp([\u0026#39;growth is strong and we have plenty of liquidity.\u0026#39;, \u0026#39;there is a shortage of capital, and we need extra financing.\u0026#39;, \u0026#39;formulation patents might protect Vasotec to a limited extent.\u0026#39;]) senti_results Run\n[{\u0026#39;label\u0026#39;: \u0026#39;Positive\u0026#39;, \u0026#39;score\u0026#39;: 1.0}, {\u0026#39;label\u0026#39;: \u0026#39;Negative\u0026#39;, \u0026#39;score\u0026#39;: 0.9952379465103149}, {\u0026#39;label\u0026#39;: \u0026#39;Neutral\u0026#39;, \u0026#39;score\u0026#39;: 0.9979718327522278}] \n二、ESG分类 ESG 分析可以帮助投资者确定企业的长期可持续性并识别相关风险。 FinBERT-ESG 是一个 FinBERT 模型，根据来自公司 ESG 报告和年度报告的 2,000 个手动注释句子进行微调。\n 输入：金融文本。 输出：Environmental, Social, Governance or None.  from transformers import BertTokenizer, BertForSequenceClassification, pipeline esg_finbert = BertForSequenceClassification.from_pretrained(\u0026#39;yiyanghkust/finbert-esg\u0026#39;,num_labels=4) esg_tokenizer = BertTokenizer.from_pretrained(\u0026#39;yiyanghkust/finbert-esg\u0026#39;) esg_nlp = pipeline(\u0026#34;text-classification\u0026#34;, model=esg_finbert, tokenizer=esg_tokenizer) \n使用3条测试文本进行测试\nesg_results = esg_nlp([\u0026#39;Managing and working to mitigate the impact our operations have on the environment is a core element of our business.\u0026#39;, \u0026#39;Rhonda has been volunteering for several years for a variety of charitable community programs.\u0026#39;, \u0026#39;Cabot\\\u0026#39;s annual statements are audited annually by an independent registered public accounting firm.\u0026#39;, \u0026#39;As of December 31, 2012, the 2011 Term Loan had a principal balance of $492.5 million.\u0026#39;]) esg_results Run\n[{\u0026#39;label\u0026#39;: \u0026#39;Environmental\u0026#39;, \u0026#39;score\u0026#39;: 0.9805498719215393}, {\u0026#39;label\u0026#39;: \u0026#39;Social\u0026#39;, \u0026#39;score\u0026#39;: 0.9906041026115417}, {\u0026#39;label\u0026#39;: \u0026#39;Governance\u0026#39;, \u0026#39;score\u0026#39;: 0.6738430857658386}, {\u0026#39;label\u0026#39;: \u0026#39;None\u0026#39;, \u0026#39;score\u0026#39;: 0.9960240125656128}] \n三、FLS识别 前瞻性陈述 (FLS) 告知投资者经理人对公司未来事件或结果的信念和意见。 从公司报告中识别前瞻性陈述可以帮助投资者进行财务分析。 FinBERT-FLS 是一个 FinBERT 模型，它基于罗素 3000 家公司年报的管理讨论和分析部分的 3,500 个手动注释的句子进行了微调。\n 输入：金融文本。 输出：Specific-FLS(特定 FLS) , Non-specific FLS(非特定 FLS), Not-FLS(非 FLS)。  from transformers import BertTokenizer, BertForSequenceClassification, pipeline fls_finbert = BertForSequenceClassification.from_pretrained(\u0026#39;yiyanghkust/finbert-fls\u0026#39;,num_labels=3) fls_tokenizer = BertTokenizer.from_pretrained(\u0026#39;yiyanghkust/finbert-fls\u0026#39;) fls_nlp = pipeline(\u0026#34;text-classification\u0026#34;, model=fls_finbert, tokenizer=fls_tokenizer) \n使用3条测试文本进行测试\nfls_results = fls_nlp([\u0026#39;we expect the age of our fleet to enhance availability and reliability due to reduced downtime for repairs.\u0026#39;, \u0026#39;on an equivalent unit of production basis, general and administrative expenses declined 24 percent from 1994 to $.67 per boe.\u0026#39;, \u0026#39;we will continue to assess the need for a valuation allowance against deferred tax assets considering all available evidence obtained in future reporting periods.\u0026#39;]) fls_results Run\n[{\u0026#39;label\u0026#39;: \u0026#39;Specific FLS\u0026#39;, \u0026#39;score\u0026#39;: 0.7727874517440796}, {\u0026#39;label\u0026#39;: \u0026#39;Not FLS\u0026#39;, \u0026#39;score\u0026#39;: 0.9905241131782532}, {\u0026#39;label\u0026#39;: \u0026#39;Non-specific FLS\u0026#39;, \u0026#39;score\u0026#39;: 0.975904107093811}] \n文档及引用说明   文档github地址 https://github.com/yya518/FinBERT\n  作者博客: https://yya518.github.io/research\n  Huang, Allen H., Hui Wang, and Yi Yang. \u0026ldquo;FinBERT: A large language model for extracting information from financial text.\u0026rdquo; Contemporary Accounting Research (2022).\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-17-finbert-finance-bert-model/","summary":"FinBERT介绍   FinBERT， 是使用49亿词的英文金融语料库数据，生成的BERT预训练语言模型。语料库上大小为 49亿个词。\n 公司报告 10-K 和 10-Q：25亿个词 电话会议记录：13亿个词 分析师报告：11亿个词  FinBERT开发者在多个金融 NLP 任务上对 FinBERT 预训练模型进行了微调，均优于传统机器学习模型、深度学习模型和微调 BERT 模型。 所有经过微调的 FinBERT 模型都公开托管在 Huggingface 🤗。 目前支持包括情绪分析、ESG 分类、前瞻性陈述 (FLS) 分类。\nHuang, Allen H., Hui Wang, and Yi Yang. \u0026#34;FinBERT: A large language model for extracting information from financial text.\u0026#34; Contemporary Accounting Research (2022). 摘要（翻译）: 我们开发了 FinBERT，这是一种适用于金融领域的最先进的大型语言模型。我们表明，FinBERT 结合了金融知识，可以更好地总结金融文本中的上下文信息。使用分析报告中研究人员标记的句子样本，我们证明 FinBERT 大大优于 Loughran 和 McDonald 词典以及其他机器学习算法，包括朴素贝叶斯、支持向量机、随机森林、卷积神经网络和长短期记忆，在情感分类中。我们的结果表明，FinBERT 擅长识别其他算法错误标记为中性的句子的正面或负面情绪，这可能是因为它使用了金融文本中的上下文信息。我们发现，FinBERT 优于其他算法，以及 Google 的原始双向编码器表示形式来自 transformers (BERT) 模型，当训练样本量较小且文本中包含一般文本中不常用的金融词时，这种优势尤为突出。 FinBERT 在识别与环境、社会和治理问题相关的讨论方面也优于其他模型。最后，我们表明，与 FinBERT 相比，其他方法低估了收益电话会议的文本信息量至少 18%。我们的结果对学术研究人员、投资专业人士和金融市场监管机构具有重要意义。","title":"FinBERT | 金融文本BERT模型，可情感分析、识别ESG和FLS类型"},{"content":"乱码由来 相信大家使用Python读取txt、csv文件时，经常遇到UnicodeDecodeError错误，从字面意思看是 编码解码问题 。 关于编码解码相关知识推荐看B站柴知道制作的视频，了解\n**锟斤拷�⊠是怎样炼成的\u0026mdash;\u0026mdash;中文显示\u0026quot;⼊\u0026quot;门指南【**柴知道】\n 遇到乱码 在Python中，遇到文件读取乱码的可能性很大，例如\nimport pandas as pd df = pd.read_csv(\u0026#39;twitter_sentiment.csv\u0026#39;) df.head() Run\nc:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\io\\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\r...\rpandas\\_libs\\parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()\rpandas\\_libs\\parsers.pyx in pandas._libs.parsers.raise_parser_error()\rUnicodeDecodeError: 'utf-8' codec can't decode byte 0xbd in position 10717: illegal multibyte sequence\r 可以看到报错, 遇到 UnicodeDecodeError 问题\n使用encoding参数 常见的解决办法是在函数内加入encoding参数，中文最常见的编码方式有gbk、gb2312、utf-8。\nimport pandas as pd #这里依次试验gbk、gb2312、utf-8 df = pd.read_csv(\u0026#39;twitter_sentiment.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) df.head() 依然出现 UnicodeDecodeError 问题 。所以encoding遇到很不常见的编码方式。\n获取encoding参数 先获取 twitter_sentiment.csv 文件的编码方式，再进行读取操作。\nimport chardet #读取为二进制数据 binary_data = open(\u0026#39;twitter_sentiment.csv\u0026#39;, \u0026#39;rb\u0026#39;).read() #传给chardet.detect，稍等片刻 chardet.detect(binary_data) Run\n{'encoding': 'Windows-1252', 'confidence': 0.7291192008535122, 'language': ''}\r 由此得知该文件的编码方式为 Windows-1252 ， 重新更改即可成功读入数据\n成功读取 获取了正确的编码 encoding='Windows-1252'， 就能正确的读入数据\nimport pandas as pd #这里依次试验gbk、gb2312、utf-8 df = pd.read_csv(\u0026#39;twitter_sentiment.csv\u0026#39;, encoding=\u0026#39;Windows-1252\u0026#39;) df.head() Run\n    ItemID Sentiment SentimentText     0 1 0 is so sad for my APL frie...   1 2 0 I missed the New Moon trail...   2 3 1 omg its already 7:30 :O   3 4 0 .. Omgaga. Im sooo im gunna CRy. I'...   4 5 0 i think mi bf is cheating on me!!! ...    广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-16-how-to-fix-string-unicode-decode-error/","summary":"乱码由来 相信大家使用Python读取txt、csv文件时，经常遇到UnicodeDecodeError错误，从字面意思看是 编码解码问题 。 关于编码解码相关知识推荐看B站柴知道制作的视频，了解\n**锟斤拷�⊠是怎样炼成的\u0026mdash;\u0026mdash;中文显示\u0026quot;⼊\u0026quot;门指南【**柴知道】\n 遇到乱码 在Python中，遇到文件读取乱码的可能性很大，例如\nimport pandas as pd df = pd.read_csv(\u0026#39;twitter_sentiment.csv\u0026#39;) df.head() Run\nc:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\io\\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\r...\rpandas\\_libs\\parsers.","title":"如何正确读入文本数据不乱码(解决文本乱码问题)"},{"content":" 姚加权,张锟澎,罗平.金融学文本大数据挖掘方法与研究进展[J].经济学动态,2020(04):143-158.\n 摘要 在金融学领域的传统实证研究中，所用数据多局限于财务报表和股票市场数据等结构化数据。而在大数据时代，计算机技术的进步使得数据类型不断丰富，研究者开始将非结构化的文本大数据引入到金融学领域的研究中，其主要包括上市公司披露文本、财经媒体报道、社交网络文本、网络搜索指数以及 P2P 网络借贷文本等，并对 文本可读性、语气语调、相似性 以及 语义特征 展开研究。本文首先介绍了金融学领域文本大数据挖掘步骤和方法，描述了语料获取、预处理过程、文档表示以及文档的特征抽取；然后根据不同的文本信息来源，梳理了金融学文本大数据的研究进展；最后对未来金融学文本大数据的研究方法和研究内容进行了展望。\n关键词：文本大数据 文本分析 机器学习 深度学习 数据挖掘\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-16-literature-review-textmining-in-finance-yao2020/","summary":" 姚加权,张锟澎,罗平.金融学文本大数据挖掘方法与研究进展[J].经济学动态,2020(04):143-158.\n 摘要 在金融学领域的传统实证研究中，所用数据多局限于财务报表和股票市场数据等结构化数据。而在大数据时代，计算机技术的进步使得数据类型不断丰富，研究者开始将非结构化的文本大数据引入到金融学领域的研究中，其主要包括上市公司披露文本、财经媒体报道、社交网络文本、网络搜索指数以及 P2P 网络借贷文本等，并对 文本可读性、语气语调、相似性 以及 语义特征 展开研究。本文首先介绍了金融学领域文本大数据挖掘步骤和方法，描述了语料获取、预处理过程、文档表示以及文档的特征抽取；然后根据不同的文本信息来源，梳理了金融学文本大数据的研究进展；最后对未来金融学文本大数据的研究方法和研究内容进行了展望。\n关键词：文本大数据 文本分析 机器学习 深度学习 数据挖掘\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"转载 | 金融学文本大数据挖掘方法与研究进展"},{"content":"\n传统测量 被试者创造力 存在耗费时间、主观性太强、缺乏客观性，且所得到的分值是不稳定的，无法跨时间、文化、群体进行分值比较。该研究分析了创新力的两大理论，即联系理论和执行理论，即创新力是包含思维的广度和深度两方面。\n 联系理论(广度) 负责搜寻所有可能方案的集合，增加集合的规模，体现思维的广度。 执行理论(深度) 负责寻找最佳方案，并将方案落实执行，体现思维的深度。  结合Glove词嵌入技术，将每个词理解为一个技术或知识，两词语语义越相似，发散性越低。\n文中让被试按照一定规则，随意填写10个名词，使用其中7个有效词语测量被试的创新力(发散性)思维。可以简单的把7个词理解为知识或者技术，7个词语会形成21种词语对(组合)。最后求均值可以测量出被试词语对的语义距离体现创新发散性的强度。文末含案例代码\nOlson, J.A., Nahas, J., Chmoulevitch, D., Cropper, S.J. and Webb, M.E., 2021. Naming unrelated words predicts creativity. Proceedings of the National Academy of Sciences, 118(25), p.e2022340118. \n一、摘要 一些理论认为，有 创造力 的人能够产生更多 发散性 的想法。如果这是正确的，简单地让被试写 N 个不相关的单词，然后测量这N个词的语义距离， 作为 #发散思维 的客观衡量标准。为了验证这一假设，我们要求 8,914 名参与者说出 10 个彼此尽可能不同的单词。\n然后计算算法估计单词之间的平均语义距离；相关词（例如 cat 和 dog）比不相关词（例如 cat 和 thimble）的距离更短。我们预测，产生更大语义距离的人也会在传统的创造力测量中得分更高。\n在研究 1 中，我们发现语义距离与两个广泛使用的创造力测量（替代用途任务和桥接关联差距任务）之间存在中度至强相关性。在研究 2 中，参与者来自 98 个国家，语义距离仅因基本人口变量而略有不同。在一系列已知可预测创造力的问题上，语义距离与表现之间也存在正相关关系。\n总体而言， 语义距离 与已建立的 创造力测量 的相关性至少与这些测量彼此之间的相关性一样强。 因此，在我们所说的发散关联任务中命名不相关的词可以作为发散思维的简短、可靠和客观的衡量标准。\n二、创新力理论 想出 3 个尽可能不同的词。根据两种主要的创造力理论 (1, 2)，选择这些词依赖于产生 #远程联想 ，同时抑制 #常见联想 。\n#联想理论 (Associative Theory)认为，有创造力的人具有语义记忆结构，可以更容易地链接远程元素 (3-6)。\n#执行理论 (Executive Theory) 侧重于自上而下的注意力控制；创造性的解决方案来自于监测和抑制共同的联想 (2, 7)。\n基于这些理论，我们假设 填写n个无关单词的任务 可以可靠地衡量 #语言创造力 。 创造力有两个主要的心理成分， 收敛思维和发散思维，它们在产生创意输出时协同工作。收敛性思维任务衡量评估多种刺激并得出最适当响应的能力，例如问题的最佳解决方案 (3, 8-10)。这些任务往往更容易得分，因为只有一小部分正确答案。相比之下，发散思维任务通常使用开放式问题来衡量一个人产生各种解决方案的能力 (11-13)。它们通常需要更长的回答(文本)，因此更难客观评分。\n三、创新力测量 3.1 替代用途任务 最常见的发散思维测量是 替代用途任务 Alternative Uses Task (14, 15)，在该任务中，参与者生成常见物体的用途，例如回形针或鞋子。使用常用的评分方法 (16)，评分者然后根据三个组成部分来判断回答：\n 灵活性，产生的不同用途类别的数量； 独创性，每次使用相对于样本的其余部分的稀有程度，这对创造力特别重要（17、18）；和 流畅度，一共产生了多少次使用。  3.2 离散联系任务 本研究作者开发了 离散联系任务 (Divergent Association Task， DAT) 的网站， 填写你想到的10个不相关词语， 创造力越丰富的人，填写的词语语义距离往往会更远。\nhttps://www.datcreativity.com/\n被试填写10个单词的规则  只能填写英文单词 只能是名词(如事情、物体、概念) 不能填 专有名词（例如，特定的人或地点） 不能填写 专业词（比如技术词） 自己思考这些词，不要只看周围环境的物体。  DAT算法实现  使用Glove预训练模型 选前7个词(一共10个词)， 存在 21个词对（组合） 对21词对， 分别计算词向量的余弦距离，分别乘以100。最终求均值得到DAT得分。   下图是大邓第二次填写得到的DAT得分，第一次只超过了6%的人，这方法第一次准，再测就知道如何提高DAT得分。\n DAT得分范围0-200， 得分为0可能是7个有效词之间语义相同，而得分200可能是有效词之间彼此语义完全不相同。实践中，得分大多处于65~90之间，且很少超过100。\n 词嵌入技术可以把每个词转化为等长的向量，而不同词语共处于相同的语义空间中。常见的词嵌入技术有word2vec、Glove、flastText等，因为最近有学者在 替代用途任务(Alternative Uses Task）中用过Glove算法，本文采用Glove算法。本研究使用的Glove预训练模型来自Common Crawl Corpus项目，该项目拥有数十亿网页文本数据。\n为了提供冗余， 只采用 被试者 填写的前7个词作为有效单词(DAT的被试需要填写10个词)。DAT得分是这些词之间的语义距离的平均值，具体计算方法， 7个词两两相关的组合有 42种组合， 选择其中最有可能的 21 个语义组合。\n 四、实验 这种发散思维的操作化是基于创造力的联想和执行控制理论。 更高的分数将显示出更大的能力来利用更远程的关联 (3-5) 或抑制过度相关的关联 (2, 7)。\n在研究 1 中，我们通过将 DAT 与其他两种创造力测量方法进行比较来检验这一假设：替代用途任务 (15) 和桥接关联差距任务 (36)。 在研究 2 中，我们测试了这些分数如何随人口统计而变化，以及它们是否与更大数据集中与发散性思维相关的其他测量值相关 (9, 37)。 这些研究评估了语义距离是否可以作为发散思维的可靠指标。 五、讨论 研究结果表面， 让被试简单的填写10个不想管单词的任务可以作为 测量发散思维 的可靠衡量标准。在研究中， 将这项任务的表现与已有的两种创造力量表做了比较，具有很高的相关性。\n总体而言支持了语义发散性，尽管这种联系背后的确切机制尚不清楚，但在创新力最主要的两个理论，即联想理论或执行理论 的联系网络中衡量网络的范围或效率。\nDAT算法表现稳定，方差不随人口统计特征变化出现显著性变化（研究2），可以在跨年龄、跨性别的情况下应用。\n5.1 DAT的优点  操作简单，快捷，客观，节约了大量的人力时间，又能保证客观性。 得分绝对，可比较，可以用于测量不同群体(种族、文化、性别、年龄)的创造力得分。 对被试友好，一般一两分钟即可完成。  5.2 DAT的不足  创造力有发散性和执行力，发散性负责搜选所有方案集合的规模，而执行力是从方案集中选出最优方案并将其执行。DAT测量的仅仅是发散性思维。 被试可能通过填写稀奇的词语提高DAT得分。 只有短短几分钟，被试可能很难短时间内了解实验规则。  5.3 未来展望 DAT得分取决于Glove模型、语料库(数据集), 更新词模型或语料库，被试的DAT得分会发生变化。为简单起见，本研究使用免费的预训练模型， 通过一些努力，未来研究者可以对不同时期，不同国家的语料库来训练Glove模型。随着特定单词关联或多或少的联系， 更新的模型将会自动考虑这些变化，这将允许DAT得分跨越文化跨越时代，进行创新力的比较。\n\n代码 代码的文档说明请点击 github仓库地址 https://github.com/jayolson/divergent-association-task 查看。这里仅粘贴作者源代码，源代码需要配置好才可运行。\nimport dat ## 从 https://nlp.stanford.edu/projects/glove/ 下载Glove模型 model = dat.Model(\u0026#34;glove.840B.300d.txt\u0026#34;, \u0026#34;words.txt\u0026#34;) # 验证词语，如输入的是词组，代码会将其转为连线形式的单词 print(model.validate(\u0026#34;cul de sac\u0026#34;)) Run\ncul-de-sac \n计算两个词语之间的语义距离\nprint(model.distance(\u0026#34;cat\u0026#34;, \u0026#34;dog\u0026#34;)) print(model.distance(\u0026#34;cat\u0026#34;, \u0026#34;thimble\u0026#34;)) Run\n0.1983 0.8787 \n计算词对的DAT得分（语义cosine距离*100）\nprint(model.dat([\u0026#34;cat\u0026#34;, \u0026#34;dog\u0026#34;], 2)) print(model.dat([\u0026#34;cat\u0026#34;, \u0026#34;thimble\u0026#34;], 2)) Run\n19.83 87.87 \n假设有三个人分别都填写10个词，选其前7个词作为有效词。有效词如下，\nlow = [\u0026#34;arm\u0026#34;, \u0026#34;eyes\u0026#34;, \u0026#34;feet\u0026#34;, \u0026#34;hand\u0026#34;, \u0026#34;head\u0026#34;, \u0026#34;leg\u0026#34;, \u0026#34;body\u0026#34;] average = [\u0026#34;bag\u0026#34;, \u0026#34;bee\u0026#34;, \u0026#34;burger\u0026#34;, \u0026#34;feast\u0026#34;, \u0026#34;office\u0026#34;, \u0026#34;shoes\u0026#34;, \u0026#34;tree\u0026#34;] high = [\u0026#34;hippo\u0026#34;, \u0026#34;jumper\u0026#34;, \u0026#34;machinery\u0026#34;, \u0026#34;prickle\u0026#34;, \u0026#34;tickets\u0026#34;, \u0026#34;tomato\u0026#34;, \u0026#34;violin\u0026#34;] # Compute the DAT score (transformed average cosine distance of first 7 valid words) print(model.dat(low)) # 50 print(model.dat(average)) # 78 print(model.dat(high)) # 95 Run\n50 78 95 需要注意pnas作者公开的代码只能用在英文，且无法自己训练Glove模型。如果想基于自有数据集（中文、英文），训练自有Glove模型，需要学习\n 如何训练Glove模型 如何导入训练好的Glove模型 如何计算中英文dat得分  相关知识点已更新至我的录播课课程 付费视频课 | Python实证指标构建与文本分析\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/","summary":"传统测量 被试者创造力 存在耗费时间、主观性太强、缺乏客观性，且所得到的分值是不稳定的，无法跨时间、文化、群体进行分值比较。该研究分析了创新力的两大理论，即联系理论和执行理论，即创新力是包含思维的广度和深度两方面。\n 联系理论(广度) 负责搜寻所有可能方案的集合，增加集合的规模，体现思维的广度。 执行理论(深度) 负责寻找最佳方案，并将方案落实执行，体现思维的深度。  结合Glove词嵌入技术，将每个词理解为一个技术或知识，两词语语义越相似，发散性越低。\n文中让被试按照一定规则，随意填写10个名词，使用其中7个有效词语测量被试的创新力(发散性)思维。可以简单的把7个词理解为知识或者技术，7个词语会形成21种词语对(组合)。最后求均值可以测量出被试词语对的语义距离体现创新发散性的强度。文末含案例代码\nOlson, J.A., Nahas, J., Chmoulevitch, D., Cropper, S.J. and Webb, M.E., 2021. Naming unrelated words predicts creativity. Proceedings of the National Academy of Sciences, 118(25), p.e2022340118. \n一、摘要 一些理论认为，有 创造力 的人能够产生更多 发散性 的想法。如果这是正确的，简单地让被试写 N 个不相关的单词，然后测量这N个词的语义距离， 作为 #发散思维 的客观衡量标准。为了验证这一假设，我们要求 8,914 名参与者说出 10 个彼此尽可能不同的单词。\n然后计算算法估计单词之间的平均语义距离；相关词（例如 cat 和 dog）比不相关词（例如 cat 和 thimble）的距离更短。我们预测，产生更大语义距离的人也会在传统的创造力测量中得分更高。\n在研究 1 中，我们发现语义距离与两个广泛使用的创造力测量（替代用途任务和桥接关联差距任务）之间存在中度至强相关性。在研究 2 中，参与者来自 98 个国家，语义距离仅因基本人口变量而略有不同。在一系列已知可预测创造力的问题上，语义距离与表现之间也存在正相关关系。\n总体而言， 语义距离 与已建立的 创造力测量 的相关性至少与这些测量彼此之间的相关性一样强。 因此，在我们所说的发散关联任务中命名不相关的词可以作为发散思维的简短、可靠和客观的衡量标准。","title":"PNAS | 使用语义距离测量一个人的创新力(发散思维)得分"},{"content":"关于ModelScope ModelScope社区成立于 2022 年 6 月，是一个模型开源社区及创新平台，由阿里巴巴达摩院，联合CCF开源发展委员会，共同作为项目发起方。\n 社区联合国内AI领域合作伙伴与高校机构，致力于通过开放的社区合作，构建深度学习相关的模型开源，并开源相关模型服务创新技术，推动模型应用生态的繁荣发展。\n 期待ModelScope会有不一样的表现。\n与ModelScope类似的网站有\n 国际 huggingface是较早将AI模型开源的网站，用户群体庞大，社区内有丰富的数据集、模型，文档详实。 国内 百度飞桨是国内AI模型开源较好的网站，用户群体较大，更新活跃，但是文档质量。。。  目前ModelScope刚刚上线不久，模型和数据集都不怎么多\n 名词解释 ModelScope平台是以模型为中心的模型开源社区，与模型的使用相关，您需要先了解如下概念。\n   基础概念 定义     任务 任务（Task）指某一领域具体的应用，以用于完成特定场景的任务。例如图像分类、文本生成、语音识别等，您可根据任务的输入输出找到适合您的应用场景的任务类型，通过任务的筛选来查找您所需的模型。   模型 模型（Model）是指一个具体的模型实例，包括模型网络结构和相应参数。ModelScope平台提供丰富的模型信息供用户体验与使用。   模型库 模型库（Modelhub）是指对模型进行存储、版本管理和相关操作的模型服务，用户上传和共享的模型将存储至ModelScope的模型库中，同时用户也可在Model hub中创建属于自己的模型存储库，并沿用平台提供的模型库管理功能进行模型管理。   数据集 数据集（Dataset）是方便共享及访问的数据集合，可用于算法训练、测试、验证，通常以表格形式出现。按照模态可划分为文本、图像、音频、视频、多模态等。   数据集库 数据集库（Datasethub）用于集中管理数据，支持模型进行训练、预测等，使各类型数据具备易访问、易管理、易共享的特点。   ModelScope Library ModelScope Library是ModelScope平台自研的一套Python Library框架，通过调用特定的方法，用户可以只写短短的几行代码，就可以完成模型的推理、训练和评估等任务，也可以在此基础上快速进行二次开发，实现自己的创新想法。    一、模型探索 首先访问平台网址https://www.modelscope.cn/models， 您将看见平台上已有的所有公开模型，根据任务筛选或者关键词搜索可查找您感兴趣的模型。\n二、环境准备 2.1 本地开发环境 如果您需要在本地运行模型，需要进行相应的环境安装准备，包括：\n 安装python环境。支持python3，不支持python2，建议3.7版本及以上。我们推荐您使用Anaconda进行安装。 安装深度学习框架。ModelScope Library目前支持Tensorflow，Pytorch两大深度学习框架进行模型训练、推理。您可根据模型所需的框架选择适合的框架进行安装。 安装ModelScope Library。我们提供两种安装方式，您可选择适合的方式进行安装。  pip安装。ModelScope提供了根据不同领域的安装包，您可根据对应的模型选择所需的安装包。 使用源码安装。 更完整的安装信息参考：环境安装指南。    2.2 在线Notebook 若您觉得本地安装较为复杂， ModelScope平台也提供在线的运行环境，您可直接在Notebook中运行，Notebook中提供官方镜像无需自主进行环境安装，更加方便快捷，推荐大家使用！\n注意：该功能需要您登录后使用，新用户注册ModelScope账号并完成阿里云账号绑定后即可获得免费算力资源，详情请参阅免费额度说明 。\n三、2分钟跑通模型推理 若您准备好本地环境或者已经打开一个Notebook的预装环境实例，则根据下述代码可对该模型进行推理。 使用modelscope pipeline接口只需要两步，同样以上述中文分词模型（damo/nlp_structbert_word-segmentation_chinese-base）为例简单说明：\n首先根据task实例化一个pipeline对象\nfrom modelscope.pipelines import pipeline word_segmentation = pipeline(\u0026#39;word-segmentation\u0026#39;,model=\u0026#39;damo/nlp_structbert_word-segmentation_chinese-base\u0026#39;) 输入数据，拿到结果\ninput_str = \u0026#39;今天天气不错，适合出去游玩\u0026#39; print(word_segmentation(input_str)) Run\n{\u0026#39;output\u0026#39;: \u0026#39;今天 天气 不错 ， 适合 出去 游玩\u0026#39;} \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-09-chinese-modelscope-open-source/","summary":"关于ModelScope ModelScope社区成立于 2022 年 6 月，是一个模型开源社区及创新平台，由阿里巴巴达摩院，联合CCF开源发展委员会，共同作为项目发起方。\n 社区联合国内AI领域合作伙伴与高校机构，致力于通过开放的社区合作，构建深度学习相关的模型开源，并开源相关模型服务创新技术，推动模型应用生态的繁荣发展。\n 期待ModelScope会有不一样的表现。\n与ModelScope类似的网站有\n 国际 huggingface是较早将AI模型开源的网站，用户群体庞大，社区内有丰富的数据集、模型，文档详实。 国内 百度飞桨是国内AI模型开源较好的网站，用户群体较大，更新活跃，但是文档质量。。。  目前ModelScope刚刚上线不久，模型和数据集都不怎么多\n 名词解释 ModelScope平台是以模型为中心的模型开源社区，与模型的使用相关，您需要先了解如下概念。\n   基础概念 定义     任务 任务（Task）指某一领域具体的应用，以用于完成特定场景的任务。例如图像分类、文本生成、语音识别等，您可根据任务的输入输出找到适合您的应用场景的任务类型，通过任务的筛选来查找您所需的模型。   模型 模型（Model）是指一个具体的模型实例，包括模型网络结构和相应参数。ModelScope平台提供丰富的模型信息供用户体验与使用。   模型库 模型库（Modelhub）是指对模型进行存储、版本管理和相关操作的模型服务，用户上传和共享的模型将存储至ModelScope的模型库中，同时用户也可在Model hub中创建属于自己的模型存储库，并沿用平台提供的模型库管理功能进行模型管理。   数据集 数据集（Dataset）是方便共享及访问的数据集合，可用于算法训练、测试、验证，通常以表格形式出现。按照模态可划分为文本、图像、音频、视频、多模态等。   数据集库 数据集库（Datasethub）用于集中管理数据，支持模型进行训练、预测等，使各类型数据具备易访问、易管理、易共享的特点。   ModelScope Library ModelScope Library是ModelScope平台自研的一套Python Library框架，通过调用特定的方法，用户可以只写短短的几行代码，就可以完成模型的推理、训练和评估等任务，也可以在此基础上快速进行二次开发，实现自己的创新想法。    一、模型探索 首先访问平台网址https://www.modelscope.cn/models， 您将看见平台上已有的所有公开模型，根据任务筛选或者关键词搜索可查找您感兴趣的模型。\n二、环境准备 2.1 本地开发环境 如果您需要在本地运行模型，需要进行相应的环境安装准备，包括：\n 安装python环境。支持python3，不支持python2，建议3.7版本及以上。我们推荐您使用Anaconda进行安装。 安装深度学习框架。ModelScope Library目前支持Tensorflow，Pytorch两大深度学习框架进行模型训练、推理。您可根据模型所需的框架选择适合的框架进行安装。 安装ModelScope Library。我们提供两种安装方式，您可选择适合的方式进行安装。  pip安装。ModelScope提供了根据不同领域的安装包，您可根据对应的模型选择所需的安装包。 使用源码安装。 更完整的安装信息参考：环境安装指南。    2.","title":"魔搭 | 中文AI模型开源社区"},{"content":"不同的领域词库能够支持不同的文本分析任务，也是支撑领域NLP处理的必备资源。\n因此，本文介绍一个涵盖68个领域、共计916万词的专业词典知识库，可用于文本分类、知识增强、领域词汇库扩充等自然语言处理应用。\n在利用学习模型进行自然语言处理任务时候，领域词汇可以作为一项重要的领域特征加入到模型当中，可以提升领域性模型的性能。\n地址：https://github.com/liuhuanyong/DomainWordsDict\n项目概述 DomainWordsDict, Chinese words dict that contains more than 68 domains, which can be used as text classification、knowledge enhance task。涵盖68个领域、共计916万词的专业词典知识库，可用于文本分类、知识增强、领域词汇库扩充等自然语言处理应用。在利用学习模型进行自然语言处理任务时候，领域词汇可以作为一项重要的领域特征加入到模型当中，可以提升领域性模型的性能。\n项目由来 1、领域性是自然语言处理中十分重要的一类问题，不同的领域之间在文本形式、用词、表达上都存在差异。而领域词汇作为一个领域的表示是用来区分领域的常规手段，例如，在没有标注语料进行有监督的领域文本分类中，利用领域关键词进行匹配、计数、排序的方式即可以完成这一任务。\n2、当前，纵观中文开放语言资源，并未有出现较大规模的领域性资源，如领域的wordembedding词向量、领域的关键词库。而这一资源在传统方法进行文本处理具有较大价值。\n为了填补这一空白以及对领域性词库进行基础语言资源建设，本项目被提出。\n数据来源 通过对领域垂直网站的解析、领域文本的特征词提取，近几年来对领域词典的收集与整理，人工清洗等处理工作之后， 最终形成了数学科学、人力招聘、天文科学、餐饮食品、外语学习等共计68个领域，共计916万词的较大规模领域词汇库。\n数据介绍 数据放在data文件夹下，共68个txt文件，每个文件以领域的名称命名。每个文件中的每一行包括两列(以tab符分开)，分别代表词语名称以及对应的权重。文件中的词语按照权重从大到小的方式排列，权重越高，该词对于领域的代表性或区分能力就越强。在使用的过程中，我们可以设定具体的权重域值在选用不同的词语来用于特定任务。\n词典样例    序号 领域 个数 举例     1 数学科学 17,287 定义域、值域、半群、悖论、本原多项式、闭包、变换群、边连通度、不变因子、差集、超滤子、存在量词、代数、代数闭域、单射   2 人力招聘 447,606 销售代表、产品经理、销售经理、电话销售、阿里云、客户经理、销售精英、销售工程师、集团总部、销售主管、商务专员、客服专员、课程顾问、Manager、销售助理   3 天文科学 4,135 天体化学、天体力学、白洞、本星系群、不规则星系、垂直圈、地面天文学、第一宇宙速度、动力学宇宙学、方位天文学、高能天体物理学、观测宇宙学、光学天文学、航海天文学、航天动力学   4 餐饮食品 201,163 六堡茶、参龟汤、婆参扒大鸭、虾籽大乌参、红扒鱼肚、蛤什蟆汤、参女、五盖山米茶、皖西黄大茶、涌溪火青、夫妻肺片、普洱茶、六堡散茶、漆蜡妙食、鱼羊鲜   5 外语学习 1,150 褒义、被动语态、比较级、贬义、表语、表语从句、宾补、宾格、宾语、宾语从句、并列复合句、并列句、并列连词、不定代词、不定冠词   6 电影影视 114,577 小查和寇弟的游轮生活、邪斗邪、邪完再邪、和莎莫的五百天、比利曼蒂和死神的大反击、融入那芒芒的大海里、阿扎泽勒、圣棱的星光、他们也忒不仗义了、吉姆贾木许、神探伽俐略、仔表栖议甲弟、叶卡捷琳堡音乐戏剧剧院、锁叶和属于自己的春天、神奇四侠   7 环境科学 7,891 层积云、热岛效应、单区电除尘器、逆温、卷层云、卷积云、卷云、低压槽、厄尔尼诺现象、副热带高压、高压脊、锢囚锋、积雨云、蒙古气旋、气候变化   8 钢铁冶金 89,114 炉料、炉衬、梭车、偏心轮、炉渣流动性、锰矿、软熔带、炉顶、耐热钢、脱碳、除尘风机、连续采煤机、锚杆机、扒渣、齿轮钢   9 印刷印染 464 包装薄膜、包装防伪、标识防伪、玻璃油墨、产品防伪、打样机、电晕笔、分切机、复合机、覆膜机、功能薄膜、挂历印刷、刮墨刀、海报印刷、降解薄膜   10 美容美发 9,662 阿莎露、娜莎迪、内眦赘皮、重睑术、艾茜凯、洗得你净、兰芝、欧莱雅、虾青素、削刀式划剪、扛薄剪、碧欧泉、高丝、赫莲娜、护肤   11 法律诉讼 62,717 行政处罚、送达、书证、行政诉讼法、行政复议、指定管辖、合议庭、第三人、二审、民事诉讼法、诉讼时效、合并审理、法定代表人、司法解释、刑事诉讼法   12 计算机业 55,037 字符串、排序、标识符、队列、访问级别、局部变量、安全类、安全内核、安全识别、安全域、绑定、包过滤、保留字、备份与恢复、编辑程序   13 水利工程 30,584 拦污栅、电动蝶阀、管件、检查井、启闭机、手动蝶阀、手动葫芦、消力池、闸室、帮扎、包箍、边墩、草袋、承插管、粗料石   14 手机数码 10,955 阿尔卡特、金立、奥盛、天时达、大显、首信、多普达、萨基姆、斯达康、索尼爱立信、摩奇、奥乐、柏卡、爱立信、迪比特   15 音乐歌曲 8,276 月亮代表我的心、倒带、搁浅、回到过去、简单爱、菊花台、蒲公英的约定、日不落、听妈妈的话、退后、夜曲、音高、蔡依林、陈好、陈慧琳   16 地产开发 14,708 锦绣花园、东方家园、东方巴黎、和平小区、红河小区、华侨饭店、金河小区、凯旋城、临江花园、世纪花园、世纪嘉园、世茂滨江新城、银河小区、中山花园、丽景苑   17 汉语言学 32,8050 长虫、背时、胰子、刺挠、旮旯、上该、要得、茅房、邋遢、落雨、二杆子、爹爹、宝气、日白、棒老二   18 医药医学 54,9008 毒邪内闭证、参附注射液、邪盛正衰、夹蛇龟肉、夹蛇龟、直接盖髓术间接盖髓术干髓术、参附汤、盖革缪勒计数器、荜澄茄、大敦、南沙参、白虎加人参汤、桡尺近侧关节、归地参术汤、地骨皮   19 网络文学 95,331 竺氏三姐弟、参神契、明道参神契、元婴、缩地术、朱苹、地行术、胖都都、似模似样、水晶血龙参、渡劫、盖运聪、和氏之璧、詹姆、沐王府   20 休闲活动 59,186 说、哪府并哪县、佛挡杀佛、罢罢罢来休休休、万、番、拾玉镯、打金枝、军、吾乃江东小霸王孙伯符、丁广和、会、了、贾、得   21 交通运输 27,230 信号机、北京铁路局、沈阳铁路局、车辆段、车务段、工务段、哈尔滨铁路局、岔心、列车长、蒸汽机车、济南铁路局、南昌铁路局、上海铁路局、郑州铁路局、abs防抱死刹车系统   22 矿业勘探 20,817 捕收剂、矿车、带式输送机、化合水、精煤、炼焦煤、煤仓、煤泥、起泡剂、选煤厂、闭路破碎、粗磨、粗碎、二分器、翻车机   23 地点名称 1,338,275 新村、和平村、胜利村、新建村、太平村、向阳村、团结村、新华村、东山村、前进村、劲霸男装、红旗村、东风村、以纯、光明村   24 船舶工程 5,424 舱壁板、风暴扶手、锚链管、锚链筒、白昼信号灯、车钟、充放电板、电笛、舵杆、舵机、舵叶、发电机屏、海图灯、空气断路器、雷达应答器   25 敏感用词 13,595 抢盐、AV、hz、sm、PK、PX、C4、usk、flg、GCD、gcd、GHB、rfa、sex、TND   26 旅游交通 52,848 报国寺、本溪水洞、大佛寺、大明寺、独乐寺、夫子庙、观音山、广胜寺、寒山寺、黑龙潭、极乐寺、蠡园、隆兴寺、鲁迅故居、明孝陵   27 机械工程 9,164 磨床、铣床、滚子链、键槽、蜗杆、蜗轮、镗床、脱碳、保持架、齿距、齿宽、传动链、大齿轮、导程、碟形弹簧   28 考古挖掘 5,713 二里头文化、辛店文化、朱开沟文化、夏家店文化、彭头山文化、齐家文化、二里岗文化、石家河文化、贾湖骨笛、云纹铜禁、郑振香、半坡遗址、大明宫遗址、阿房宫遗址、汉长安城遗址   29 人文政治 13,189 坚持改革开放、坚持和完善人民代表大会制度、建设社会主义法治国家、我代表中共中央、向香港特别行政区同胞、高举中国特色社会主义伟大旗帜、转变发展方式、多种所有制经济共同发展的基本经济制度、改革开放以来、开辟了中国特色社会主义道路、毛泽东思想、维护世界和平与促进共同发展、病有所医、劳有所得、老有所养   30 电力电气 50,429 高压侧、厂用电率、模芯和模套对准中心调整、栅差、空侧、氢侧、裕度、奥科勃纶、掺的石英光纤、缩颈模、辗页辗页橡辗页塑、揭大盖、拉线模模孔光洁度模角等光学检测仪、铁芯、有功功率   31 网络游戏 52,2150 虎窟佛调行、此佛彼佛、伽邪舍多链、伽那格毒手蛇使、长剑、藏千邪、吸血、金创药、破甲、布鞋、藏邪、长邪、长邪带、长邪护手、长邪戒   32 纺织服装 28,111 平纹布、风衣、夹克、九分裤、里料、棉绳、耳仔、西裤、氨纶、蝙蝠袖、插肩袖、翻领、口袋、裤长、立领   33 办公文教 6,135 硒鼓、李浩东、王嘉豪、彩喷纸、充电辊、磁辊、定影膜、定影组件、粉盒、分离爪、感光鼓、鼓芯、加热组件、墨粉、色带   34 组织机构 369,709 酒店名称、软件学院、校医院、管理学院、北京工业大学、广东工业大学、广东外语外贸大学、广州大学、广州中医药大学、华南理工大学、华南师范大学、网络中心、广州美术学院、河北大学、华南农业大学   35 化学化工 40,316 苛化度苛化作用苛化率、没食子酸、奎哪啶红、助色团、单分子反应、瞞、电子亲合势、阿伏伽德罗数、丁咯地尔、盐酸丁咯地尔、乙烯、磁量子数、副价、均裂、量子数   36 诗词歌赋 772,992 泊思禅寺呈廖明略其地盖干越寺在琵琶洲上、送矰吴尉并属寄声吴交代尉比行余亦行追作、裴纶著作见期行日延宿所居既至裴已行因书寄、送常宁吴尉并属寄声吴交代尉比行余亦行追作、次韵追和钱穆父内翰勰赵伯坚大卿令铄游颍湖、蕃有诗谢萧伯和见访伯和和之节推丈见而同作、僮有弹鹭置池上者予解其缚纵之而不去盖不饮、与硕父沈弟伯仲晚行河堤硕父欲作小亭于其上、王虞部惠佳篇叙述昔与湘潭亡弟游从仍以亡弟、舟中咏落景余清晖轻桡弄溪渚之句盖孟浩然耶、自道场山至何山读故人洪舜俞内翰诗刻追和、龟胜寺枸杞大如椽陈日华发其根而枯堂犹以地、永丰祝子益和予诗见寄许见访以长句谢之且贤、次韵德美碧感旧之什且约胡广仲伯逢季丘来会、舟宿南尉岸下夜夕不寐思丁老小山戏成长韵   37 社会科学 19,231 第三人、承包合同、出资、法定代表人、国有资产、合伙协议、原件、不当得利、法学家、国际经济法、国际私法、民事诉讼法、善意第三人、违法、刑法   38 军事情报 76,249 狙击步枪、突击步枪、无情角斗士的邪纹护手、型潜艇、无情角斗士的邪纹长裤、阿史那弥射、级驱逐舰、胡庆余堂牌参参胶囊、步兵战车、无壳弹、级潜艇、自动步枪、莫折大提起义、勒伯勒东、叶卡捷琳堡号核潜艇   39 农林牧渔 38,611 广叶参属、大参属、毒参属、佛肚苣苔属、革叶荠属、吡弗咯菌素、大苞鞘花属、大丁草属、大爪草属、女菀属、莎菀属、绿尾大蚕蛾、围绿单爪鳃金龟、咯菌腈、天山邪蒿属   40 文学名著 235,996 种柳成行夹流水、皂盖朱轮别似空、豸角当邪触、太虚幻境、满铺着寂寞和黑暗、自惟朴且疏、张生马瘦衣且单、早寒风摵摵、浣溪沙、探春、生命也是这般的一瞥么、并仰空若思、叶尾娜、秦女休行、自弄还自罢   41 新番动漫 152,984 醞、骗、苢、颯、黨、芭、賩、问、覫、鳧、莫、韧、颙、鋨、駩   42 网络用语 23,972 冰天雪地掩面泪奔、好苦、举手、看好你哦、困揉眼睛、来呀挑衅、脸红掩面、列队、皿哪里跑、摸摸头、人击掌、呜呜呜、凹凸曼、拜托啦人、抱抱   43 市场购物 6,3732 缪缪、艾拓、盖奇、杰恩万堡、卡莎布兰卡、娜尔思、阿莎琪、阿她琪、阿枝、艾盟、艾娜斯、艾茜芬、奥联金盟、奥倩、奥诗裳   44 电子工程 6,107 传输线、电偶极子、介电损耗、居里温度、特征阻抗、插入损耗、分频器、椭圆极化、无源网络、线极化、有源网络、圆极化、驻波比、按比例缩小、暗电流   45 金融财经 605,698 L.P.、Ltd.、LLC、数在校数、学生数在校数、Inc.、Limited、质押、LP、拓日新能、拓尔思、国债、中国平安、中国人寿、中信证券   46 古代历史 114,701 大畜利貞不家食吉、恒星什宿度、虞陆张骆陆吾朱传、棍噶扎勒参、大庶长、琅邪、孙和、刺史、召陵之盟、巴而术阿而忒的斤、阿史那思摩、莫折大提、仆射、石重贵、陆景   47 世界哲学 20,627 费尔巴哈、阶级性、经院哲学、拉布里奥拉、李卜克内西、两点论、空亡、纳甲、天刑、阿多诺、鲍威尔、伯恩施坦、布哈林、不可知论、布洛赫   48 通信工程 3,814 单稽指令、未说娩量、中止的椎、副帧、视见区、无线网卡、集合差、工厂说瞄、侧音、数位叠加和、符号差、单地址指令、单地址信息、单赋值语言、单钮鼠标器   49 人物名称 1,572,202 醞、骗、苢、颯、黨、芭、张鑫、賩、问、李娟、李莉、覫、刘佳、莫、鳧   50 世界宗教 132,295 大阿阇黎佛智足、佛前佛后难、佛世差摩竭、东方亦有阿閦鞞佛、鞞侈遮罗那三般那、萨婆僧伽三摩地伽兰地、前佛后佛、第十八祖伽邪舍多、阿悉多伽那、佛不见身知是佛、拘那含牟尼佛、邻阿伽色、阿伽色、大须弥佛、大焰肩佛   51 地理测绘 53,610 侧分泌说、均变说、夹石、底栖的、单钭的、单栅笔石、风棱石、溺谷、无结构腐殖体、大陆车阀说、底辟构造、陆间裂谷系、地柱说、出射角、红帘石   52 民间习俗 1,365 八字官星太多、八字无比劫、八字无官星、八字有比劫、八字有官星、白虎持势、白腊金、背禄逐马、比肩劫财与地支、比肩劫财与天干、比劫、比劫帮身、比劫夺财、比劫克财、壁上土   53 书法艺术 28,266 三击掌、拾玉镯、单弦、苏州弹词、打金枝、副净、慢板、四进士、武松打虎、北京琴书、二人转、河南坠子、湖北大鼓、天津时调、高凤翰   54 期货期权 1,300 热卷、铜、铝、螺纹钢、锡、锌、镍、PTA、动力煤、菜粕、豆粕、硅铁、玉米淀粉、PVC   55 土木工程 56,720 等参单元等参数单元等参元、填石、盖梁、拱脚、嵌岩桩、无侧限抗压强度、翼缘、抹角、似棱体、似棱体公式、单墩单墩、刚度比劲度比、内排水系统、伸缩缝、弯曲刚度抗弯劲度   56 安全工程 4,051 安全性能、疏散时间、隔离栅、灭火器材、安全标志、安全标准、安全防护、安全功能、安全认证、安全设备、安全准则、不安全行为、不安全状态、地下开采、二氧化碳灭火器   57 材料包装 1,473 焊接面、奥丽斯纹、百花纹、百家姓纹、白牛皮、宝石兰、本白、编织纹、彩胶、茶花纹、充皮纸、大鳄鱼纹、大玫瑰纹、灯笼纹、蝶影纹   58 教育教学 111,449 旅游管理、江西师范大学、电子信息科学与技术、西南交通大学、郑州大学、教务处、见恶如探汤、麦比乌斯圈、3、西北大学、北方工业大学、北京第二外国语学院、北京工商大学、北京工业大学、北京化工大学   59 家居装饰 8,668 车脚、搭脚仔凳、汉代陶柜、什景灯、折屏、转屏、眠之堡、松堡王国、阿里斯顿、九牧、林内、奥地雅、白夹竹、大边和抹头、大唐合盛   60 工业设计 7,150 急流槽、弯沉、压实度、沉降缝、底基层、钢筋笼、钢筋砼、浆砌片石、路堑、路缘石、清表、松铺厚度、圆管涵、锥坡、产品设计   61 物理科学 12,989 临界指数、布儒斯特角、产生算符、场点、狄拉克方程、对易、对易关系、反射光栅、夫琅禾费衍射、光阑、归一化、基态、角放大率、勒让德变换、洛伦兹变换   62 体育运动 48,602 奥伦塞彭特、范那佛洛、彼得伯勒联、法恩伯勒、什鲁斯伯里城、大蒙特基奥、哈万特和滑铁卢村、索尔兹伯里城、奥吉贾奥佛埃、奥锡拉库扎、奥伦塞、奇彭纳姆城、艾思莫茨、班伯里联、佛特   63 航空航天 682 副翼、起落架、襟翼、升降舵、油滤、那拉提、戀攀椀、挀栀愀渀最、猀栀愀渀最、被释放、不安全事件、部分功率、不工作、不亮、不能复位   64 建筑装潢 32,826 夹景、侧脚、丁顺隔皮砌式、副景、屏石、栅顶、尾景、佛座、普柏枋、寻杖合角造、侧天窗、单材拱、单风道系统、盖瓦、和玺彩画   65 广告传媒 166 编辑机、彩喷纸、充气模型、促销台、灯箱布、分支器、挂历、光发射机、广告板、广告机、广告牌、广告旗杆、广告条幅、光接收机、光学摄像机   66 汽车行业 10,294 杂物箱、机油泵、倒车镜、倒车雷达、脚踏板、分离轴承、半轴、换挡杆、节温器、转向灯开关、保险杠、滚针轴承、活塞销、通气塞、油箱盖   67 管理科学 20,751 不孕不育、预防成本、最低库存、最高库存、安全班前会、安全标志使用导则、安全色、班前会、班前会记录、搬运分析、班组建设、备品备件、闭环、闭环与关闭、必要动作   68 动植生物 314,030 鳉、尾棘无壳侧鳃、无壳侧鳃属、单序波缘大参、单刺侧红糠虾、非洲侧颈龟属、六结侧颈龟、六峰侧颈龟、南美侧颈龟属、奥氏抖尾地雀、无角陶塞特羊、显脉大参、短梗大参、盖革氏离子计数、蓝无壳侧鳃    项目总结 1、本项目开放了一个涵盖68个领域，带有行业代表性权重的领域词库，规模达到了916万词，是目前开放词典资源中较大规模的一个，填补了一定的空缺。 2，领域词汇库的构建和开放，是一项基础、必要且重要的工作。可以通过领域开放文本进行挖掘，如基于垂直网站解析、文本特征词提取等诸多方法来实现。 3，关于领域词汇知识库的构建方法和理论，可以参考之前写的博客《领域词汇知识库的类型、可用资源与构建技术漫谈》：https://blog.csdn.net/lhy2014/article/details/103995629。 4，语言资源、经典词库的构建，与目前盛行的深度学习自然语言处理并行不悖。将已构建好的领域词库或者知识库融合到深度学习模型当中，是一个很好的前进方向。需要且必要地关注底层语义资源的建设。\n关于作者 刘焕勇，liuhuanyong，现任360人工智能研究院算法专家，前中科院软件所工程师，主要研究方向为知识图谱、事件图谱在实际业务中的落地应用。\n得语言者得天下，得语言资源者，分得天下，得语言逻辑者，争得天下。\n 个人主页：https://liuhuanyong.github.io 个人公众号：老刘说NLP  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-07-domainwordsdict-liuhuanyong/","summary":"不同的领域词库能够支持不同的文本分析任务，也是支撑领域NLP处理的必备资源。\n因此，本文介绍一个涵盖68个领域、共计916万词的专业词典知识库，可用于文本分类、知识增强、领域词汇库扩充等自然语言处理应用。\n在利用学习模型进行自然语言处理任务时候，领域词汇可以作为一项重要的领域特征加入到模型当中，可以提升领域性模型的性能。\n地址：https://github.com/liuhuanyong/DomainWordsDict\n项目概述 DomainWordsDict, Chinese words dict that contains more than 68 domains, which can be used as text classification、knowledge enhance task。涵盖68个领域、共计916万词的专业词典知识库，可用于文本分类、知识增强、领域词汇库扩充等自然语言处理应用。在利用学习模型进行自然语言处理任务时候，领域词汇可以作为一项重要的领域特征加入到模型当中，可以提升领域性模型的性能。\n项目由来 1、领域性是自然语言处理中十分重要的一类问题，不同的领域之间在文本形式、用词、表达上都存在差异。而领域词汇作为一个领域的表示是用来区分领域的常规手段，例如，在没有标注语料进行有监督的领域文本分类中，利用领域关键词进行匹配、计数、排序的方式即可以完成这一任务。\n2、当前，纵观中文开放语言资源，并未有出现较大规模的领域性资源，如领域的wordembedding词向量、领域的关键词库。而这一资源在传统方法进行文本处理具有较大价值。\n为了填补这一空白以及对领域性词库进行基础语言资源建设，本项目被提出。\n数据来源 通过对领域垂直网站的解析、领域文本的特征词提取，近几年来对领域词典的收集与整理，人工清洗等处理工作之后， 最终形成了数学科学、人力招聘、天文科学、餐饮食品、外语学习等共计68个领域，共计916万词的较大规模领域词汇库。\n数据介绍 数据放在data文件夹下，共68个txt文件，每个文件以领域的名称命名。每个文件中的每一行包括两列(以tab符分开)，分别代表词语名称以及对应的权重。文件中的词语按照权重从大到小的方式排列，权重越高，该词对于领域的代表性或区分能力就越强。在使用的过程中，我们可以设定具体的权重域值在选用不同的词语来用于特定任务。\n词典样例    序号 领域 个数 举例     1 数学科学 17,287 定义域、值域、半群、悖论、本原多项式、闭包、变换群、边连通度、不变因子、差集、超滤子、存在量词、代数、代数闭域、单射   2 人力招聘 447,606 销售代表、产品经理、销售经理、电话销售、阿里云、客户经理、销售精英、销售工程师、集团总部、销售主管、商务专员、客服专员、课程顾问、Manager、销售助理   3 天文科学 4,135 天体化学、天体力学、白洞、本星系群、不规则星系、垂直圈、地面天文学、第一宇宙速度、动力学宇宙学、方位天文学、高能天体物理学、观测宇宙学、光学天文学、航海天文学、航天动力学   4 餐饮食品 201,163 六堡茶、参龟汤、婆参扒大鸭、虾籽大乌参、红扒鱼肚、蛤什蟆汤、参女、五盖山米茶、皖西黄大茶、涌溪火青、夫妻肺片、普洱茶、六堡散茶、漆蜡妙食、鱼羊鲜   5 外语学习 1,150 褒义、被动语态、比较级、贬义、表语、表语从句、宾补、宾格、宾语、宾语从句、并列复合句、并列句、并列连词、不定代词、不定冠词   6 电影影视 114,577 小查和寇弟的游轮生活、邪斗邪、邪完再邪、和莎莫的五百天、比利曼蒂和死神的大反击、融入那芒芒的大海里、阿扎泽勒、圣棱的星光、他们也忒不仗义了、吉姆贾木许、神探伽俐略、仔表栖议甲弟、叶卡捷琳堡音乐戏剧剧院、锁叶和属于自己的春天、神奇四侠   7 环境科学 7,891 层积云、热岛效应、单区电除尘器、逆温、卷层云、卷积云、卷云、低压槽、厄尔尼诺现象、副热带高压、高压脊、锢囚锋、积雨云、蒙古气旋、气候变化   8 钢铁冶金 89,114 炉料、炉衬、梭车、偏心轮、炉渣流动性、锰矿、软熔带、炉顶、耐热钢、脱碳、除尘风机、连续采煤机、锚杆机、扒渣、齿轮钢   9 印刷印染 464 包装薄膜、包装防伪、标识防伪、玻璃油墨、产品防伪、打样机、电晕笔、分切机、复合机、覆膜机、功能薄膜、挂历印刷、刮墨刀、海报印刷、降解薄膜   10 美容美发 9,662 阿莎露、娜莎迪、内眦赘皮、重睑术、艾茜凯、洗得你净、兰芝、欧莱雅、虾青素、削刀式划剪、扛薄剪、碧欧泉、高丝、赫莲娜、护肤   11 法律诉讼 62,717 行政处罚、送达、书证、行政诉讼法、行政复议、指定管辖、合议庭、第三人、二审、民事诉讼法、诉讼时效、合并审理、法定代表人、司法解释、刑事诉讼法   12 计算机业 55,037 字符串、排序、标识符、队列、访问级别、局部变量、安全类、安全内核、安全识别、安全域、绑定、包过滤、保留字、备份与恢复、编辑程序   13 水利工程 30,584 拦污栅、电动蝶阀、管件、检查井、启闭机、手动蝶阀、手动葫芦、消力池、闸室、帮扎、包箍、边墩、草袋、承插管、粗料石   14 手机数码 10,955 阿尔卡特、金立、奥盛、天时达、大显、首信、多普达、萨基姆、斯达康、索尼爱立信、摩奇、奥乐、柏卡、爱立信、迪比特   15 音乐歌曲 8,276 月亮代表我的心、倒带、搁浅、回到过去、简单爱、菊花台、蒲公英的约定、日不落、听妈妈的话、退后、夜曲、音高、蔡依林、陈好、陈慧琳   16 地产开发 14,708 锦绣花园、东方家园、东方巴黎、和平小区、红河小区、华侨饭店、金河小区、凯旋城、临江花园、世纪花园、世纪嘉园、世茂滨江新城、银河小区、中山花园、丽景苑   17 汉语言学 32,8050 长虫、背时、胰子、刺挠、旮旯、上该、要得、茅房、邋遢、落雨、二杆子、爹爹、宝气、日白、棒老二   18 医药医学 54,9008 毒邪内闭证、参附注射液、邪盛正衰、夹蛇龟肉、夹蛇龟、直接盖髓术间接盖髓术干髓术、参附汤、盖革缪勒计数器、荜澄茄、大敦、南沙参、白虎加人参汤、桡尺近侧关节、归地参术汤、地骨皮   19 网络文学 95,331 竺氏三姐弟、参神契、明道参神契、元婴、缩地术、朱苹、地行术、胖都都、似模似样、水晶血龙参、渡劫、盖运聪、和氏之璧、詹姆、沐王府   20 休闲活动 59,186 说、哪府并哪县、佛挡杀佛、罢罢罢来休休休、万、番、拾玉镯、打金枝、军、吾乃江东小霸王孙伯符、丁广和、会、了、贾、得   21 交通运输 27,230 信号机、北京铁路局、沈阳铁路局、车辆段、车务段、工务段、哈尔滨铁路局、岔心、列车长、蒸汽机车、济南铁路局、南昌铁路局、上海铁路局、郑州铁路局、abs防抱死刹车系统   22 矿业勘探 20,817 捕收剂、矿车、带式输送机、化合水、精煤、炼焦煤、煤仓、煤泥、起泡剂、选煤厂、闭路破碎、粗磨、粗碎、二分器、翻车机   23 地点名称 1,338,275 新村、和平村、胜利村、新建村、太平村、向阳村、团结村、新华村、东山村、前进村、劲霸男装、红旗村、东风村、以纯、光明村   24 船舶工程 5,424 舱壁板、风暴扶手、锚链管、锚链筒、白昼信号灯、车钟、充放电板、电笛、舵杆、舵机、舵叶、发电机屏、海图灯、空气断路器、雷达应答器   25 敏感用词 13,595 抢盐、AV、hz、sm、PK、PX、C4、usk、flg、GCD、gcd、GHB、rfa、sex、TND   26 旅游交通 52,848 报国寺、本溪水洞、大佛寺、大明寺、独乐寺、夫子庙、观音山、广胜寺、寒山寺、黑龙潭、极乐寺、蠡园、隆兴寺、鲁迅故居、明孝陵   27 机械工程 9,164 磨床、铣床、滚子链、键槽、蜗杆、蜗轮、镗床、脱碳、保持架、齿距、齿宽、传动链、大齿轮、导程、碟形弹簧   28 考古挖掘 5,713 二里头文化、辛店文化、朱开沟文化、夏家店文化、彭头山文化、齐家文化、二里岗文化、石家河文化、贾湖骨笛、云纹铜禁、郑振香、半坡遗址、大明宫遗址、阿房宫遗址、汉长安城遗址   29 人文政治 13,189 坚持改革开放、坚持和完善人民代表大会制度、建设社会主义法治国家、我代表中共中央、向香港特别行政区同胞、高举中国特色社会主义伟大旗帜、转变发展方式、多种所有制经济共同发展的基本经济制度、改革开放以来、开辟了中国特色社会主义道路、毛泽东思想、维护世界和平与促进共同发展、病有所医、劳有所得、老有所养   30 电力电气 50,429 高压侧、厂用电率、模芯和模套对准中心调整、栅差、空侧、氢侧、裕度、奥科勃纶、掺的石英光纤、缩颈模、辗页辗页橡辗页塑、揭大盖、拉线模模孔光洁度模角等光学检测仪、铁芯、有功功率   31 网络游戏 52,2150 虎窟佛调行、此佛彼佛、伽邪舍多链、伽那格毒手蛇使、长剑、藏千邪、吸血、金创药、破甲、布鞋、藏邪、长邪、长邪带、长邪护手、长邪戒   32 纺织服装 28,111 平纹布、风衣、夹克、九分裤、里料、棉绳、耳仔、西裤、氨纶、蝙蝠袖、插肩袖、翻领、口袋、裤长、立领   33 办公文教 6,135 硒鼓、李浩东、王嘉豪、彩喷纸、充电辊、磁辊、定影膜、定影组件、粉盒、分离爪、感光鼓、鼓芯、加热组件、墨粉、色带   34 组织机构 369,709 酒店名称、软件学院、校医院、管理学院、北京工业大学、广东工业大学、广东外语外贸大学、广州大学、广州中医药大学、华南理工大学、华南师范大学、网络中心、广州美术学院、河北大学、华南农业大学   35 化学化工 40,316 苛化度苛化作用苛化率、没食子酸、奎哪啶红、助色团、单分子反应、瞞、电子亲合势、阿伏伽德罗数、丁咯地尔、盐酸丁咯地尔、乙烯、磁量子数、副价、均裂、量子数   36 诗词歌赋 772,992 泊思禅寺呈廖明略其地盖干越寺在琵琶洲上、送矰吴尉并属寄声吴交代尉比行余亦行追作、裴纶著作见期行日延宿所居既至裴已行因书寄、送常宁吴尉并属寄声吴交代尉比行余亦行追作、次韵追和钱穆父内翰勰赵伯坚大卿令铄游颍湖、蕃有诗谢萧伯和见访伯和和之节推丈见而同作、僮有弹鹭置池上者予解其缚纵之而不去盖不饮、与硕父沈弟伯仲晚行河堤硕父欲作小亭于其上、王虞部惠佳篇叙述昔与湘潭亡弟游从仍以亡弟、舟中咏落景余清晖轻桡弄溪渚之句盖孟浩然耶、自道场山至何山读故人洪舜俞内翰诗刻追和、龟胜寺枸杞大如椽陈日华发其根而枯堂犹以地、永丰祝子益和予诗见寄许见访以长句谢之且贤、次韵德美碧感旧之什且约胡广仲伯逢季丘来会、舟宿南尉岸下夜夕不寐思丁老小山戏成长韵   37 社会科学 19,231 第三人、承包合同、出资、法定代表人、国有资产、合伙协议、原件、不当得利、法学家、国际经济法、国际私法、民事诉讼法、善意第三人、违法、刑法   38 军事情报 76,249 狙击步枪、突击步枪、无情角斗士的邪纹护手、型潜艇、无情角斗士的邪纹长裤、阿史那弥射、级驱逐舰、胡庆余堂牌参参胶囊、步兵战车、无壳弹、级潜艇、自动步枪、莫折大提起义、勒伯勒东、叶卡捷琳堡号核潜艇   39 农林牧渔 38,611 广叶参属、大参属、毒参属、佛肚苣苔属、革叶荠属、吡弗咯菌素、大苞鞘花属、大丁草属、大爪草属、女菀属、莎菀属、绿尾大蚕蛾、围绿单爪鳃金龟、咯菌腈、天山邪蒿属   40 文学名著 235,996 种柳成行夹流水、皂盖朱轮别似空、豸角当邪触、太虚幻境、满铺着寂寞和黑暗、自惟朴且疏、张生马瘦衣且单、早寒风摵摵、浣溪沙、探春、生命也是这般的一瞥么、并仰空若思、叶尾娜、秦女休行、自弄还自罢   41 新番动漫 152,984 醞、骗、苢、颯、黨、芭、賩、问、覫、鳧、莫、韧、颙、鋨、駩   42 网络用语 23,972 冰天雪地掩面泪奔、好苦、举手、看好你哦、困揉眼睛、来呀挑衅、脸红掩面、列队、皿哪里跑、摸摸头、人击掌、呜呜呜、凹凸曼、拜托啦人、抱抱   43 市场购物 6,3732 缪缪、艾拓、盖奇、杰恩万堡、卡莎布兰卡、娜尔思、阿莎琪、阿她琪、阿枝、艾盟、艾娜斯、艾茜芬、奥联金盟、奥倩、奥诗裳   44 电子工程 6,107 传输线、电偶极子、介电损耗、居里温度、特征阻抗、插入损耗、分频器、椭圆极化、无源网络、线极化、有源网络、圆极化、驻波比、按比例缩小、暗电流   45 金融财经 605,698 L.","title":"DomainWordsDict | 领域词库构建方法与68领域、916万级专业词库分享"},{"content":"作者 刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。\n就职于360人工智能研究院、曾就职于中国科学院软件研究所。\n老刘说NLP，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。\n开放文本中蕴含着大量的逻辑性知识，以刻画事物之间逻辑传导关系的逻辑类知识库是推动知识推理发展的重要基础。 因果抽取是一个十分有趣的话题，研发大规模逻辑推理知识库有助于支持实体或事件等传导驱动决策任务，而目前尚未有开源的因果事件对出现，为了弥补这一空缺，本文对外开源一个面向多领域的十万级因果事件对数据集，可以自行转成因果关系图谱，展开更多有趣实验，供大家一起参考。 地址：https://github.com/liuhuanyong/CausalDataset\n一、因果抽取常用方法 我们在《事件图谱技术：因果关系事件对抽取常用方法的解析与动手实践》中讲述了因果抽取的方法，从传统模式规则、语义分析、依存句法、序列标注四种方式进行实践，并配上实现项目进行讲解，这涵盖了当前因果事件抽取的常用方式。\n地址： https://github.com/liuhuanyong/CausalityEventExtraction\n1.1 基于模式匹配的因果事件对提取 基于模式匹配的方式，是进行因果抽取的入门级以及兜底方式，充分利用好语言学知识，具有显式标记的因果关联词、因果表达句式进行归纳，并配以正则表达式实现，可以有效地提取出大量的因果事件对。\n1.2 基于语义角色的因果事件抽取 基于触发词模式匹配的方法无法捕捉因果事件之间的关联关系，因此可以借助依存句法分析以及语义角色标注的方式进行处理。\n以因果关系触发词为核心动作，首先从语义角色方面找寻该触发词动作的实施对象和受事对象，将实施对象作为原因事件，将受事对象作为结果事件，并根据词性过滤事件；\n1.3 基于依存句法的因果事件抽取 由于自然语言处理的复杂性，LTP中未能对一些子句中的因果关系触发词进行语义角色标注，或者只标注了一部分，即A0和A1未同时被标注出来，因此利用依存句法分析来抽取此类情况下的因果事件对。\n1.4 基于序列标注的因果抽取 针对基于规则的因果抽取模型中的不足，可以使用基于Bert微调的序列标注模型。在序列标签的设计上，模型的序列标签采用BIO标签体系，标签类型主要为cause、triger、effect。 为了能方便地根据标签结果进行因果三元组组合，在设计标签体系时也对单因果、多因果进行了区分，分别设置为multi-cause、multi-effect。\n\n二、基于多领域文本数据集的因果事件对 为了得到多领域因果事件对，我们以清华大学开源的文本分类数据集THUnews，THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。\n其在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。满足了多领域性的需求。\n数据地址：http://thuctc.thunlp.org/#中文文本分类数据集THUCNews\n训练因果抽取识别模型，最终去重得到了100,688条因果关系对，通过对频次进行统计，可以过滤出质量较高的因果对，下面显示了格式为原因事件@结构事件\\t出现频次格式下的数据样例。 投资风险巨大@本金全部亏损 248 用户友好界面@模式帮助用户选择场景 38 政策消息面和技术面所有信息@交易者预期变 37 磨砂表面处理@触感更佳 31 加上F2大光圈和丰富手动功能@机器推出受到消费者广泛关注 26 金属材质设计@整体造型更具品质感 25 商务机型中并常见@上下边框显得厚 23 顶盖采用工程塑料制成配@笔记本外壳防滑耐磨 19 取消传统曲面过度@iPhone4底部扬声器变得硕大 17 准专业机型GRDIGITALII和GX200电子水平仪功能引进@使用R10拍摄高楼山水 16 镜头位移减震功能以及闪光灯控制系统@低光照下拍摄照片时噪 14 像素触摸式液晶屏幕@操控方面人性化 14 采用直线条形式边框风格@整体看上去大气 14 像素摄像头镶嵌屏幕上方@视频聊天方便 14 \n2.1 关于“地震”相关的因果事件对 日本东北部海域发生里氏大地震@重大人员伤亡和财产损失 6 日本东北部海域发生里氏地震@重大人员伤亡和财产损失 5 印尼西爪哇省附近印度洋海域发生里氏地震@人死亡人受伤 4 智利中南部城市康塞普西翁附近发生里氏强烈地震@重大人员伤亡 3 智利发生里氏地震@重大人员伤亡和财产损失 3 东部凡省发生强烈地震@死亡人数 3 上周五地震中受损核反应堆发生爆炸@核工业相关公司股票 3 日本大地震@金融市场动 3 最近地震和海啸灾害中复苏@日元汇率下跌 3 日本东北部大地震@全球关注 2 汶川地震期间捐款数目@高度关注 2 \n2.2 与“贬值”相关的因果事件对 虚拟道具贬值@广范围用户付费意愿越来越低 3 流动性过剩加剧@贬值趋势 3 日本核泄露事件@外资产贬值 3 全球性经济复苏以及贬值流动性过剩@全球商品价格出现暴涨 3 朝鲜进行货币贬值@市场经济瘫痪 2 欧洲主权债务危机深化和亚洲国家货币贬值@日本有警惕金融资本市场动荡 2 游戏公司滥发虚拟物品@玩家虚拟物品贬值 2 住房价格贬值@全球经济下滑形势演变成 2 中长期内贬值@资金撤离资产 2 持续贬值和人民币升值预期@中国内地成为资金洼地 2 韩元贬值@进口商品价格上升 2 货币大体上呈贬值趋势@国际油价名义价格走高 2 朱广沪时期大面积召人@国家队贬值 1 \n2.3 与“恋爱”相关的因果事件对 恋爱观婚姻观@观众极大兴趣 2 恋爱问题@学生意外伤害事 2 人相知相惜@恋爱温度始终保持合适系数 1 持人大爆钱包@恋爱故事 1 来美丽密令恋爱线人电影@陆毅闪耀大银幕上 1 李成儒和小演员侯角恋爱往事@媒体关注 1 歌曲转换过渡上显得流畅@听起来实在如男女恋爱中不伦恋 1 抓紧时间南京谈恋爱@台上台下哄笑 1 公司安排工作@没时间恋爱 1 强打精神去面对@恋爱没有兴趣 1 \n总结 本文以清华大学开源的文本分类数据集THUnews，对外开源了一个面向多领域的十万级因果事件对数据集，并介绍了常用技术方法。当然，数据的质量也有不足之处，规模不大，可以加以改善。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-07-chinese-casual-text-datasets/","summary":"作者 刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。\n就职于360人工智能研究院、曾就职于中国科学院软件研究所。\n老刘说NLP，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。\n开放文本中蕴含着大量的逻辑性知识，以刻画事物之间逻辑传导关系的逻辑类知识库是推动知识推理发展的重要基础。 因果抽取是一个十分有趣的话题，研发大规模逻辑推理知识库有助于支持实体或事件等传导驱动决策任务，而目前尚未有开源的因果事件对出现，为了弥补这一空缺，本文对外开源一个面向多领域的十万级因果事件对数据集，可以自行转成因果关系图谱，展开更多有趣实验，供大家一起参考。 地址：https://github.com/liuhuanyong/CausalDataset\n一、因果抽取常用方法 我们在《事件图谱技术：因果关系事件对抽取常用方法的解析与动手实践》中讲述了因果抽取的方法，从传统模式规则、语义分析、依存句法、序列标注四种方式进行实践，并配上实现项目进行讲解，这涵盖了当前因果事件抽取的常用方式。\n地址： https://github.com/liuhuanyong/CausalityEventExtraction\n1.1 基于模式匹配的因果事件对提取 基于模式匹配的方式，是进行因果抽取的入门级以及兜底方式，充分利用好语言学知识，具有显式标记的因果关联词、因果表达句式进行归纳，并配以正则表达式实现，可以有效地提取出大量的因果事件对。\n1.2 基于语义角色的因果事件抽取 基于触发词模式匹配的方法无法捕捉因果事件之间的关联关系，因此可以借助依存句法分析以及语义角色标注的方式进行处理。\n以因果关系触发词为核心动作，首先从语义角色方面找寻该触发词动作的实施对象和受事对象，将实施对象作为原因事件，将受事对象作为结果事件，并根据词性过滤事件；\n1.3 基于依存句法的因果事件抽取 由于自然语言处理的复杂性，LTP中未能对一些子句中的因果关系触发词进行语义角色标注，或者只标注了一部分，即A0和A1未同时被标注出来，因此利用依存句法分析来抽取此类情况下的因果事件对。\n1.4 基于序列标注的因果抽取 针对基于规则的因果抽取模型中的不足，可以使用基于Bert微调的序列标注模型。在序列标签的设计上，模型的序列标签采用BIO标签体系，标签类型主要为cause、triger、effect。 为了能方便地根据标签结果进行因果三元组组合，在设计标签体系时也对单因果、多因果进行了区分，分别设置为multi-cause、multi-effect。\n\n二、基于多领域文本数据集的因果事件对 为了得到多领域因果事件对，我们以清华大学开源的文本分类数据集THUnews，THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。\n其在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。满足了多领域性的需求。\n数据地址：http://thuctc.thunlp.org/#中文文本分类数据集THUCNews\n训练因果抽取识别模型，最终去重得到了100,688条因果关系对，通过对频次进行统计，可以过滤出质量较高的因果对，下面显示了格式为原因事件@结构事件\\t出现频次格式下的数据样例。 投资风险巨大@本金全部亏损 248 用户友好界面@模式帮助用户选择场景 38 政策消息面和技术面所有信息@交易者预期变 37 磨砂表面处理@触感更佳 31 加上F2大光圈和丰富手动功能@机器推出受到消费者广泛关注 26 金属材质设计@整体造型更具品质感 25 商务机型中并常见@上下边框显得厚 23 顶盖采用工程塑料制成配@笔记本外壳防滑耐磨 19 取消传统曲面过度@iPhone4底部扬声器变得硕大 17 准专业机型GRDIGITALII和GX200电子水平仪功能引进@使用R10拍摄高楼山水 16 镜头位移减震功能以及闪光灯控制系统@低光照下拍摄照片时噪 14 像素触摸式液晶屏幕@操控方面人性化 14 采用直线条形式边框风格@整体看上去大气 14 像素摄像头镶嵌屏幕上方@视频聊天方便 14 \n2.1 关于“地震”相关的因果事件对 日本东北部海域发生里氏大地震@重大人员伤亡和财产损失 6 日本东北部海域发生里氏地震@重大人员伤亡和财产损失 5 印尼西爪哇省附近印度洋海域发生里氏地震@人死亡人受伤 4 智利中南部城市康塞普西翁附近发生里氏强烈地震@重大人员伤亡 3 智利发生里氏地震@重大人员伤亡和财产损失 3 东部凡省发生强烈地震@死亡人数 3 上周五地震中受损核反应堆发生爆炸@核工业相关公司股票 3 日本大地震@金融市场动 3 最近地震和海啸灾害中复苏@日元汇率下跌 3 日本东北部大地震@全球关注 2 汶川地震期间捐款数目@高度关注 2","title":"十万级 | 多领域因果事件对数据集对外开源"},{"content":"作者 刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。\n就职于360人工智能研究院、曾就职于中国科学院软件研究所。\n老刘说NLP，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。\n事件图谱是当前的一个十分有趣的话题，我们在前面的事件图谱系列文章中对事件图谱进行了论述。\n例如文章《技术思考：面向落地应用的事件类图谱划分、关键问题及其与知识图谱的对比辨析》、《事件图谱应用：智能金融与情报分析中的七大应用潜在场景概述》、《事件图谱技术：基于触发词的事件句识别方法与关键流程总结》等。\n同样本着技术具像化的原则，为了让大家对具体事件图谱有个清晰的直观的认识，本文我们介绍一个自建的金融事件图谱，涵盖并购和投资两大类事件类型，从金融事件图谱设计概述、投资事件图谱数据介绍以及并购事件图谱数据介绍三个角度进行论述，供大家一起参考。\n一、金融事件图谱设计概述 事件知识图谱EKG（event knowledge graph）是当前事件类图谱的一种，在这里，我更倾向于认为这个图谱本身更倾向于为一个事件知识库，而非实体知识图谱。\n事件知识图谱的工作主要围绕事件知识本身进行展开，关注点在于事件内部信息，如ACE中的8大类事件，将这几类事件中的信息进行抽取和填充就能够得到一个以特定事件类型作为分类标准的事件知识库，如婚姻事件库、爆炸事件库等。\n而相对应的，领域事件图谱显得更为重要，金融领域作为一个需求较为明显的领域，其建模能力更具代表性，例如，我们可以对事件图谱进行本体定义：\n   事件类型 事件要素 事件关系     投资事件 融资方、投资方、金额、轮次、融资时间、所属行业 顺承/时序   并购事件 并购方、被并购方、并购状态、所属行业、涉及股权、并购开始时间、并购结束时间、是否VC/PE支持 顺承/时序    在这样一个本体框架之下，我们要构建起一个事件图谱，可以有两种方式：\n  从已经结构化好的数据源中直接获取。例如，目前针对投融资领域已经出现了许多垂类网站，如投资界、IT橘子中直接获取，并做清洗。这种方式最为快捷，但受制于人，其中的数据有限，并存在字段不全的问题。当我们想建成一个实时动态的金融事件图谱库，在捕捉实时数据时，及时处理时候，就需要采用抽取的思路。\n  基于模型的非结构化文本抽取。为了避免方法1带来的拿来主义缺陷，我们可以转换为标准的事件抽取任务，针对实时的实时新闻流，进行论元识别、事件要素抽取。\n  例如，给定文本：\n8日消息，总部位于墨西哥的在线批发平台Miferia获得了700万美元种子轮融资，该轮融资由贝恩资本风险投资公司和Tiger Global共同领投。Miferia批发平台将墨西哥的独立零售店与化妆品、食品和饮料以及家居装饰等类别的品牌联系起来。该平台拥有来自500多个品牌的数千种产品，每周有30多个新品牌上线。（Latamlist） 我们可以从中检测出融资事件：\n8月8日消息，总部位于墨西哥的在线批发平台Miferia获得了700万美元种子轮融资，该轮融资由贝恩资本风险投资公司和Tiger Global共同领投。 并识别出一下结构化信息：\n融资方：Miferia 金额：700万美元 轮次：种子轮以及投资方贝恩资本风险投资公司、Tiger Global； 融资时间：8月8日、所属行业：在线批发等信息 下图展示了一个金融领域的一个典型投资领域事件图谱：\n其中包括“君度德瑞、新余凯信投资、深圳市立德富盈投资等投资信濠光电20%股权”、“东莞中科中广基金（领投）、中广创投、紫宸创投等投资信濠光电5%股权”两个投资事件，每个投资事件由投资方、融资方、金额、日期、轮次几个事件要素构成，而若以一个融资方为中心进行融资历程的刻画，就可以根据日期发展的先后顺序，在两个事件之间形成一条边。\n需要注意的是，现在的事件抽取任务中，是不包含事件名称的抽取的，但如果要星辰恶搞事件图谱，就必须保证该事件的唯一性和友好性，可以使用md5值来表示，但并不直观，图中给出了一个较好的例子，用一个短句来表示。\n二、投资事件图谱数据介绍 我们以投资界为数据源，通过解析整理，形成了9093条投资事件，包括融资方、投资方、金额、轮次、融资时间、所属行业共5个要素。\n数据样例：\n{ \u0026#34;name\u0026#34;:\u0026#34;苏州聚源铸芯创投基金（领投）、创世一期、高捷资本等投资英彼森\u0026#34;, \u0026#34;event_type\u0026#34;:\u0026#34;投资事件\u0026#34;, \u0026#34;融资方\u0026#34;:\u0026#34;英彼森半导体（珠海）有限公司\u0026#34;, \u0026#34;投资方\u0026#34;:[ \u0026#34;聚源资本\u0026#34;, \u0026#34;高捷资本\u0026#34;, \u0026#34;创世伙伴\u0026#34;, \u0026#34;绿河投资\u0026#34;, \u0026#34;珠海科技创投\u0026#34; ], \u0026#34;金额\u0026#34;:\u0026#34;RMB数亿\u0026#34;, \u0026#34;轮次\u0026#34;:\u0026#34;A轮\u0026#34;, \u0026#34;融资时间\u0026#34;:\u0026#34;2021年06月29日\u0026#34;, \u0026#34;所属行业\u0026#34;:\u0026#34;半导体及电子设备-半导体\u0026#34; } { \u0026#34;name\u0026#34;:\u0026#34;Esta Investments、DD Asset Holdings、DST China EC XI等投资滴滴集团6.08%股权\u0026#34;, \u0026#34;event_type\u0026#34;:\u0026#34;投资事件\u0026#34;, \u0026#34;融资方\u0026#34;:\u0026#34;滴滴\u0026#34;, \u0026#34;投资方\u0026#34;:[ \u0026#34;Esta Investments\u0026#34;, \u0026#34;腾讯投资\u0026#34;, \u0026#34;THL A11\u0026#34;, \u0026#34;纪源资本\u0026#34;, \u0026#34;数字天空技术\u0026#34; ], \u0026#34;金额\u0026#34;:\u0026#34;USD7.5亿\u0026#34;, \u0026#34;轮次\u0026#34;:\u0026#34;B轮\u0026#34;, \u0026#34;融资时间\u0026#34;:\u0026#34;2014年12月02日\u0026#34;, \u0026#34;所属行业\u0026#34;:\u0026#34;电信及增值业务-无线互联网服务\u0026#34; } \n三、并购事件图谱数据介绍 同样的，我们得到了3865条并购事件数据，包括并购方、被并购方、并购状态、所属行业、涉及股权、并购开始时间、并购结束时间以及是否VC/PE支持等事件要素。\n数据样例：\n{ \u0026#34;name\u0026#34;:\u0026#34;友睦口腔收购友睦三九60%股权\u0026#34;, \u0026#34;event_type\u0026#34;:\u0026#34;并购事件\u0026#34;, \u0026#34;并购方\u0026#34;:\u0026#34;深圳市友睦口腔股份有限公司\u0026#34;, \u0026#34;被并购方\u0026#34;:\u0026#34;深圳友睦三九口腔门诊部有限公司\u0026#34;, \u0026#34;并购状态\u0026#34;:\u0026#34;已完成\u0026#34;, \u0026#34;所属行业\u0026#34;:\u0026#34;生物技术/医疗健康-医疗服务\u0026#34;, \u0026#34;涉及股权\u0026#34;:\u0026#34;60.00 %\u0026#34;, \u0026#34;并购开始时间\u0026#34;:\u0026#34;2017年03月21日\u0026#34;, \u0026#34;并购结束时间\u0026#34;:\u0026#34;2017年03月21日\u0026#34;, \u0026#34;是否VC/PE支持\u0026#34;:\u0026#34;是\u0026#34; } { \u0026#34;name\u0026#34;:\u0026#34;我享科技收购我享网络\u0026#34;, \u0026#34;event_type\u0026#34;:\u0026#34;并购事件\u0026#34;, \u0026#34;并购方\u0026#34;:\u0026#34;上海我享网络信息科技股份有限公司\u0026#34;, \u0026#34;被并购方\u0026#34;:\u0026#34;上海我享网络科技有限公司\u0026#34;, \u0026#34;并购状态\u0026#34;:\u0026#34;已完成\u0026#34;, \u0026#34;所属行业\u0026#34;:\u0026#34;互联网-电子商务-C2C\u0026#34;, \u0026#34;涉及股权\u0026#34;:\u0026#34;N/A\u0026#34;, \u0026#34;并购开始时间\u0026#34;:\u0026#34;2017年03月01日\u0026#34;, \u0026#34;并购结束时间\u0026#34;:\u0026#34;2017年03月24日\u0026#34;, \u0026#34;是否VC/PE支持\u0026#34;:\u0026#34;是\u0026#34; } \n总结 本文我们介绍l额一个自建的金融事件图谱，涵盖并购和投资两大类事件类型，从金融事件图谱设计概述、投资事件图谱数据介绍以及并购事件图谱数据介绍三个角度进行论述，这对加深我们对事件图谱的具象化认识具有一定的意义。\n关于具体的数据，可以关注 公众号：老刘说NLP，并加入技术社区，与技术社区的朋友一同分享获取。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-07-financial-invest-merge/","summary":"作者 刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。\n就职于360人工智能研究院、曾就职于中国科学院软件研究所。\n老刘说NLP，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。\n事件图谱是当前的一个十分有趣的话题，我们在前面的事件图谱系列文章中对事件图谱进行了论述。\n例如文章《技术思考：面向落地应用的事件类图谱划分、关键问题及其与知识图谱的对比辨析》、《事件图谱应用：智能金融与情报分析中的七大应用潜在场景概述》、《事件图谱技术：基于触发词的事件句识别方法与关键流程总结》等。\n同样本着技术具像化的原则，为了让大家对具体事件图谱有个清晰的直观的认识，本文我们介绍一个自建的金融事件图谱，涵盖并购和投资两大类事件类型，从金融事件图谱设计概述、投资事件图谱数据介绍以及并购事件图谱数据介绍三个角度进行论述，供大家一起参考。\n一、金融事件图谱设计概述 事件知识图谱EKG（event knowledge graph）是当前事件类图谱的一种，在这里，我更倾向于认为这个图谱本身更倾向于为一个事件知识库，而非实体知识图谱。\n事件知识图谱的工作主要围绕事件知识本身进行展开，关注点在于事件内部信息，如ACE中的8大类事件，将这几类事件中的信息进行抽取和填充就能够得到一个以特定事件类型作为分类标准的事件知识库，如婚姻事件库、爆炸事件库等。\n而相对应的，领域事件图谱显得更为重要，金融领域作为一个需求较为明显的领域，其建模能力更具代表性，例如，我们可以对事件图谱进行本体定义：\n   事件类型 事件要素 事件关系     投资事件 融资方、投资方、金额、轮次、融资时间、所属行业 顺承/时序   并购事件 并购方、被并购方、并购状态、所属行业、涉及股权、并购开始时间、并购结束时间、是否VC/PE支持 顺承/时序    在这样一个本体框架之下，我们要构建起一个事件图谱，可以有两种方式：\n  从已经结构化好的数据源中直接获取。例如，目前针对投融资领域已经出现了许多垂类网站，如投资界、IT橘子中直接获取，并做清洗。这种方式最为快捷，但受制于人，其中的数据有限，并存在字段不全的问题。当我们想建成一个实时动态的金融事件图谱库，在捕捉实时数据时，及时处理时候，就需要采用抽取的思路。\n  基于模型的非结构化文本抽取。为了避免方法1带来的拿来主义缺陷，我们可以转换为标准的事件抽取任务，针对实时的实时新闻流，进行论元识别、事件要素抽取。\n  例如，给定文本：\n8日消息，总部位于墨西哥的在线批发平台Miferia获得了700万美元种子轮融资，该轮融资由贝恩资本风险投资公司和Tiger Global共同领投。Miferia批发平台将墨西哥的独立零售店与化妆品、食品和饮料以及家居装饰等类别的品牌联系起来。该平台拥有来自500多个品牌的数千种产品，每周有30多个新品牌上线。（Latamlist） 我们可以从中检测出融资事件：\n8月8日消息，总部位于墨西哥的在线批发平台Miferia获得了700万美元种子轮融资，该轮融资由贝恩资本风险投资公司和Tiger Global共同领投。 并识别出一下结构化信息：\n融资方：Miferia 金额：700万美元 轮次：种子轮以及投资方贝恩资本风险投资公司、Tiger Global； 融资时间：8月8日、所属行业：在线批发等信息 下图展示了一个金融领域的一个典型投资领域事件图谱：\n其中包括“君度德瑞、新余凯信投资、深圳市立德富盈投资等投资信濠光电20%股权”、“东莞中科中广基金（领投）、中广创投、紫宸创投等投资信濠光电5%股权”两个投资事件，每个投资事件由投资方、融资方、金额、日期、轮次几个事件要素构成，而若以一个融资方为中心进行融资历程的刻画，就可以根据日期发展的先后顺序，在两个事件之间形成一条边。\n需要注意的是，现在的事件抽取任务中，是不包含事件名称的抽取的，但如果要星辰恶搞事件图谱，就必须保证该事件的唯一性和友好性，可以使用md5值来表示，但并不直观，图中给出了一个较好的例子，用一个短句来表示。\n二、投资事件图谱数据介绍 我们以投资界为数据源，通过解析整理，形成了9093条投资事件，包括融资方、投资方、金额、轮次、融资时间、所属行业共5个要素。\n数据样例：\n{ \u0026#34;name\u0026#34;:\u0026#34;苏州聚源铸芯创投基金（领投）、创世一期、高捷资本等投资英彼森\u0026#34;, \u0026#34;event_type\u0026#34;:\u0026#34;投资事件\u0026#34;, \u0026#34;融资方\u0026#34;:\u0026#34;英彼森半导体（珠海）有限公司\u0026#34;, \u0026#34;投资方\u0026#34;:[ \u0026#34;聚源资本\u0026#34;, \u0026#34;高捷资本\u0026#34;, \u0026#34;创世伙伴\u0026#34;, \u0026#34;绿河投资\u0026#34;, \u0026#34;珠海科技创投\u0026#34; ], \u0026#34;金额\u0026#34;:\u0026#34;RMB数亿\u0026#34;, \u0026#34;轮次\u0026#34;:\u0026#34;A轮\u0026#34;, \u0026#34;融资时间\u0026#34;:\u0026#34;2021年06月29日\u0026#34;, \u0026#34;所属行业\u0026#34;:\u0026#34;半导体及电子设备-半导体\u0026#34; } { \u0026#34;name\u0026#34;:\u0026#34;Esta Investments、DD Asset Holdings、DST China EC XI等投资滴滴集团6.","title":"小规模金融并购、投资事件图谱设计概述与数据构成解析"},{"content":"\n关于作者 刘焕勇，liuhuanyong，现任360人工智能研究院算法专家，前中科院软件所工程师，主要研究方向为知识图谱、事件图谱在实际业务中的落地应用。\n得语言者得天下，得语言资源者，分得天下，得语言逻辑者，争得天下。\n 个人主页：https://liuhuanyong.github.io 个人公众号：老刘说NLP  当前，以预训练语言模型PLM+fintune的自然语言处理范式可谓十分火热，有大量的文章在宣传这类方法，包括梳理以NNLM为起点的整个预训练方法的发展史。\n当前工业界，主要使用的预训练模型包括两种，一种是以wordvec为代表的预训练词向量，另一种是以BERT为代表的预训练语言模型。前者通常作为词语表示输入的初始化，后接NN/CNN/LSTM等编码层，后者既可以同样后接，也可以直接接上softmax/crf/span-pointer等进行解码。\n本文，主要围绕预训练词向量模型这一主题，对当前预训练词向量模型的常用方法、评估与应用方式、领域的迁移变体、当前开放的训练工具和词向量文件进行介绍，并以word2vec和fasttext为例，展示一个词向量训练的案例，做到理论与实践相结合。\n\n一、预训练词向量模型方法 自从进入2010年以来，神经语言模型就逐渐进入人们眼球，以NNLM为典型最初代表的神经网络模型，极大的推动了NLP这一领域的发展。\n实际上，早期词向量的研究通常来源于语言模型，比如NNLM和RNNLM，其主要目的是语言模型，而词向量只是一个副产物。著名的harris分布式假说提供了一个局部统计信息的理论基础。\n下面就选择其中三种典型进行介绍。\n1.1 word2vec word2vec是2013年Google开源的一款用于词向量计算的工具，通过内置的语言模型训练目标，可以将中间层得到的向量权重矩阵进行抽离，形成每个词对应的向量化表示，包括CBOW、Skip-gram两种方式，前者通过周围词来预测中心词，后者以中心词来预测上下文。\n经典的wordvec结构包括输入层、隐藏层和输出层，其计算流程为：\n1、输入层存储上下文单词的onehot。假设单词向量空间dim为V，上下文单词个数为C。\n2、所有onehot分别乘以共享的输入权重矩阵W。V*N矩阵，N为自己设定的数，初始化权重矩阵W 。\n3、所得的向量 相加求平均作为隐层向量, size为1*N。\n4、乘以输出权重矩阵W' N*V。\n5、得到向量1*V，经过激活函数处理得到V-dim概率分布。\n6、Hierarchical Softmax分类，概率最大的index所指示的单词为预测出的中间词与预测值的onehot做比较，根据误差更新权重矩阵。\n这个W矩阵就是所有单词的word embedding，任何一个单词的onehot乘以这个矩阵都将得到自己的词向量。\n通常，在训练词向量时候，会根据语料的大小来选择相应的训练方法。例如，针对小型的数据集，可以用CBOW算法，该方法对于很多分布式信息进行了平滑处理，将一整段上下文信息视为一个单一观察量，对于小型的数据集，这一处理是有帮助的。相比之下，大型数据集，可以用Skip-Gram模型，该方法将每个“上下文-目标词汇”的组合视为一个新观察量，这种做法在大型数据集中会更为有效。\n1.2 fasttext fastText是Facebook于2016年开源的一个词向量计算和文本分类工具。将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。包括字符级n-gram特征的引入以及分层Softmax分类两种。\n与CBOW一样，原本的fastText模型包括输入层、隐含层、输出层，输入都是多个经向量表示的单词，输出都是一个特定的目标，隐含层都是对多个词向量的叠加平均。不同的是，CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档，CBOW的输入单词被onehot编码过，fastText的输入特征是经embedding化的，CBOW的输出是目标词汇，fastText的输出是文档对应的类标。\n而如果将该类标替换成中间目标词，那么就可以得到wordvec的升级版，即单纯的词向量模型。例如，word2vec把语料库中的每个单词当成原子的，它会为每个单词生成一个向量。这忽略了单词内部的形态特征。\nfasttext使用了字符级别的n-grams来表示一个单词。对于单词“apple”，假设n的取值为3，则它的trigram有“\u0026lt;ap”, “app”, “ppl”, “ple”, “le\u0026gt;”，其中，\u0026lt;表示前缀，\u0026gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，可以用这5个trigram的向量叠加来表示“apple”的词向量。\n因此，因为它们的n-gram可以和其它词共享，对于训练词库之外的单词，能够解决或者oov词，这也是在当前很多文本分类、推荐场景中会优先选用fastText作为训练方法。\n1.3 Glove GloVe是斯坦福团队于2014年提出一个词向量方法，全名叫“Global Vectors”，直接利用全局的统计信息进行训练。\n与上述两种方式靠滑动窗口来制造局部上下文不同，GloVe会用到全局的词语之间共现的统计信息，即词的出现次数，词对之间的共现概率，形成共现概率矩阵，并试图生成词向量来毕竟共现概率，利用Word2Vec的skip-gram算法的高性能来解决LDA的计算量复杂问题。\n因此，我们可以发现，Glove需要事先统计共现概率，这也让其通常被认为是无监督学习，实际上glove还是有label的，即共现次数。与wordvec还有一处不同的是，损失函数是最小平方损失函数，权重可以做映射变换。\n\n二、预训练词向量的训练参数 词向量模型的超参数很多，不同的参数选择会取得不同的效果，并且，word2vec中有几个大家提的比较多的问题。以gensim-word2vec为例，包括以下参数：\n sentences： 可以是一个list，对于大语料集，可使用BrownCorpus,Text8Corpus或LineSentence构建； sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法； size： 特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百； window： 表示当前词与预测词在一个句子中的最大距离是多少； alpha: 学习速率； seed： 用于随机数发生器。与初始化词向量有关； min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5； max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制； sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)；workers参数控制训练的并行数； hs: 如果为1则会采用hierarchical softmax技巧。如果设置为0（defaut），则negative sampling会被使用； negative: 如果\u0026gt;0,则会采用negativesamping，用于设置多少个noise words； cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defaut）则采用均值。只有使用CBOW的时候才起作用； hashfxn： hash函数来初始化权重。默认使用python的hash函数； iter： 迭代次数，默认为5； trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RUE_DISCARD,utis.RUE_KEEP或者utis.RUE_DEFAUT的函数； sorted_vocab： 如果为1（defaut），则在分配word index 的时候会先对单词基于频率降序排序； batch_words： 每一批的传递给线程的单词的数量，默认为10000。  不过，如此多的参数不一定能跳得过来，因此通常会集中在以下常规参数：\n\n三、预训练词向量的评估与应用 预训练词向量生产出来，需要进行性能的评估。这方面的方法包括基于评测集，或者基于具体业务使用，用业务的指标来进行评估。\n3.1 预训练词向量的评估 学术上，词向量的质量通常由类比问题任务进行评估。如CA-translated包含了三个语义问题和134个中文词。CA8 是专门为中文语言设计的。它包含了 17813 个类比问题，覆盖了综合的词法和语义关联。\n工业，则使用词向量来代替之前随机生成的词向量文件，来对自然语言处理中的文本/情感分类、实体识别、关系抽取等任务进行评估。\n3.2 预训练词向量的应用 预训练词向量文件最大的价值在于解决了一个词语的初始化稠密表示，在解决当前以数值化为输入的深度或机器学习模型第一部的同时，还保留了一个词的区别性特征。\n一方面，当前词向量可以用于近义词挖掘的重要来源，通过某个词，通过计算词与其他词之间的相似度，并设定阈值，可以迭代挖掘出大量的相关词【过程中要注意语义漂移】。而这个词，直接就可以用于当前的搜索查询扩展、领域词构建等场景。进一步的，在模型方面，还可以作为EDA数据增强工作中的重要补充。\n另一方面，词向量可以用于当前无监督文本表示的重要方法，通过对文本进行分词，然后找到词语对应的向量，通过向量叠加的方式可以快速得到一个文本的向量表示，这一表示在诸如情感分析、句子相似度计算等任务中是实际有效的，基于文本表示，也可以进一步提升文本分类、聚类、相似query召回等使用场景性能，甚至很形象的成为了当前业务模型的baseline或者兜底模型。\n\n四、预训练词向量的变体延伸 4.1 gramEmbedding 共现信息，是cbow以及skipgram的基础，其本质在于通过周围词来建模中心词或者用中心词来建模周围词。因此，通过构造不同的共现信息，可以得到不同类型的向量形式。这里取了个名字叫gramembedding，用于表示专指文本的一系列embedding变体。\n例如，对于一个词来说，我们可以把词拆分为词word、n元序列ngram、汉字character，偏旁部首Radical，词性POS，依存关系dependency、拼音pinying。\n单元的共现，我们同样可以进行组合，例如，构造word-word，word-ngram、ngran-ngram等，得到上下文特征（单词、n-gram、字符等）等不同粒度的词向量。\n观察近几年的发展，词向量可以进一步分成偏旁部首向量、字符向量等。如香侬科技推出的glyce向量，引入汉字的字形特征。蚂蚁金服推出的cw2vec字符向量，将汉字拆解成偏旁、字件进行建模。\n当ngram中的n为1时，可以得到字向量，n为2或者更多时，则可以得到词向量等。fasttext中，就是得到了ngram的向量，并进行加和，得到一个OOV词语的向量进行表示。\n例如，基于skigram，分别设定词向量的维度及其他超参数，可以得到字向量,拼音向量，词向量，词性向量，通过上下文共现与PCA降维的方法可以得到依存向量。\n从下面的结果可以看出，词和字向量的效果看起来还不错。\n***********************字符向量************************ token:刘 (\u0026#39;李\u0026#39;, 0.7306396961212158),(\u0026#39;陈\u0026#39;, 0.7201231122016907) (\u0026#39;赵\u0026#39;, 0.6974461674690247),(\u0026#39;杨\u0026#39;, 0.6972213983535767) (\u0026#39;吴\u0026#39;, 0.6851627230644226),(\u0026#39;徐\u0026#39;, 0.6516467332839966) (\u0026#39;郭\u0026#39;, 0.6499480605125427),(\u0026#39;蔡\u0026#39;, 0.6175302267074585) (\u0026#39;郑\u0026#39;, 0.6092196106910706),(\u0026#39;孙\u0026#39;, 0.5950524210929871) token:丑 (\u0026#39;卯\u0026#39;, 0.6074919700622559),(\u0026#39;酉\u0026#39;, 0.5910211801528931) (\u0026#39;巳\u0026#39;, 0.5581363439559937),(\u0026#39;戌\u0026#39;, 0.43932047486305237) (\u0026#39;戊\u0026#39;, 0.41449615359306335),(\u0026#39;壬\u0026#39;, 0.40456631779670715) (\u0026#39;謤\u0026#39;, 0.367109090089798),(\u0026#39;绯\u0026#39;, 0.3643313944339752), (\u0026#39;寅\u0026#39;, 0.36351141333580017),(\u0026#39;旽\u0026#39;, 0.3549465537071228) ***********************依存向量************************ dependency rel:ATT (\u0026#39;COO\u0026#39;, 0.14239487051963806),(\u0026#39;ADV\u0026#39;, -0.16987691819667816) (\u0026#39;RAD\u0026#39;, -0.2357601821422577),(\u0026#39;HED\u0026#39;, -0.2401314228773117) (\u0026#39;SBV\u0026#39;, -0.25625932216644287),(\u0026#39;WP\u0026#39;, -0.27165737748146057) (\u0026#39;LAD\u0026#39;, -0.2902592420578003),(\u0026#39;POB\u0026#39;, -0.2990782558917999) (\u0026#39;VOB\u0026#39;, -0.37553706765174866),(\u0026#39;IOB\u0026#39;, -0.6669262647628784) dependency rel:POB (\u0026#39;IOB\u0026#39;, 0.16698899865150452),(\u0026#39;DBL\u0026#39;, 0.16678886115550995) (\u0026#39;FOB\u0026#39;, 0.1657436639070511),(\u0026#39;CMP\u0026#39;, 0.14784857630729675) (\u0026#39;VOB\u0026#39;, 0.1461176574230194),(\u0026#39;SBV\u0026#39;, 0.08011472970247269) (\u0026#39;LAD\u0026#39;, -0.022307466715574265),(\u0026#39;WP\u0026#39;, -0.022942926734685898) (\u0026#39;HED\u0026#39;, -0.037264980375766754),(\u0026#39;RAD\u0026#39;, -0.042251598089933395) ***********************拼音向量************************ pinyin:wo (\u0026#39;shei\u0026#39;, 0.6129732131958008)(\u0026#39;ta\u0026#39;, 0.6081706285476685) (\u0026#39;nin\u0026#39;, 0.5819231867790222),(\u0026#39;！\u0026#39;, 0.5435523986816406) (\u0026#39;……\u0026#39;, 0.48428624868392944),(\u0026#39;ai\u0026#39;, 0.47832390666007996) (\u0026#39;o\u0026#39;, 0.4761071801185608),(\u0026#39;。』\u0026#39;, 0.4598163366317749) (\u0026#39;...\u0026#39;, 0.45207729935646057),(\u0026#39;ni\u0026#39;, 0.44975683093070984) pinyin:guo (\u0026#39;dang\u0026#39;, 0.3908974528312683),(\u0026#39;yuan\u0026#39;, 0.378823846578598) (\u0026#39;zu\u0026#39;, 0.35387369990348816),(\u0026#39;hua\u0026#39;, 0.3405681848526001) (\u0026#39;zheng\u0026#39;, 0.3355437219142914),(\u0026#39;yi\u0026#39;, 0.3333034813404083) (\u0026#39;ren\u0026#39;, 0.3194104731082916),(\u0026#39;jun\u0026#39;, 0.3187354505062103) (\u0026#39;hui\u0026#39;, 0.31342023611068726),(\u0026#39;xin\u0026#39;, 0.3096797466278076) ***********************词性向量************************ word postag:a (\u0026#39;d\u0026#39;, 0.7203904986381531),(\u0026#39;c\u0026#39;, 0.6124969720840454) (\u0026#39;v\u0026#39;, 0.4963228106498718),(\u0026#39;an\u0026#39;, 0.4531499147415161) (\u0026#39;uz\u0026#39;, 0.4459834396839142),(\u0026#39;ud\u0026#39;, 0.42059916257858276) (\u0026#39;r\u0026#39;, 0.4090540111064911),(\u0026#39;uj\u0026#39;, 0.4061364233493805) (\u0026#39;i\u0026#39;, 0.38707998394966125),(\u0026#39;l\u0026#39;, 0.3551557660102844) word postag:n (\u0026#39;b\u0026#39;, 0.7030695676803589),(\u0026#39;vn\u0026#39;, 0.490166038274765) (\u0026#39;p\u0026#39;, 0.4858315885066986),(\u0026#39;v\u0026#39;, 0.4499088227748871) (\u0026#39;nt\u0026#39;, 0.44155171513557434),(\u0026#39;f\u0026#39;, 0.26609259843826294) (\u0026#39;s\u0026#39;, 0.2639649212360382),(\u0026#39;l\u0026#39;, 0.24365971982479095) (\u0026#39;ns\u0026#39;, 0.2278469204902649),(\u0026#39;m\u0026#39;, 0.202927365899086) ***********************词向量************************ word:爱情 (\u0026#39;爱恋\u0026#39;, 0.6931096315383911),(\u0026#39;真爱\u0026#39;, 0.6897798776626587) (\u0026#39;婚姻\u0026#39;, 0.6540514826774597),(\u0026#39;浪漫爱情\u0026#39;, 0.6535360813140869) (\u0026#39;情感\u0026#39;, 0.6501022577285767),(\u0026#39;感情\u0026#39;, 0.6403399705886841) (\u0026#39;纯爱\u0026#39;, 0.6394841074943542),(\u0026#39;爱情故事\u0026#39;, 0.6282097101211548) (\u0026#39;校园爱情\u0026#39;, 0.6078493595123291),(\u0026#39;情爱\u0026#39;, 0.5976818799972534) word:创新 (\u0026#39;技术创新\u0026#39;, 0.7648976445198059),(\u0026#39;不断创新\u0026#39;, 0.7172579765319824) (\u0026#39;创新型\u0026#39;, 0.6573833227157593),(\u0026#39;创新能力\u0026#39;, 0.6533682942390442) (\u0026#39;创新性\u0026#39;, 0.6160774827003479),(\u0026#39;革新\u0026#39;, 0.6159394383430481) (\u0026#39;人才培养\u0026#39;, 0.6093565821647644),(\u0026#39;开拓创新\u0026#39;, 0.6015594601631165) (\u0026#39;探索\u0026#39;, 0.5987343788146973),(\u0026#39;技术革新\u0026#39;, 0.5949685573577881) 从上，也看到一些十分有趣的现象：\n1）依存向量，依存向量中可以看出，ATT作为定中关系，在依存关系中属于定中结构，COO(联合)，ADV(状中)的相似度要比主谓SBV，动宾VOB的相似度要高。另外，作为介宾的POB，相似的有IOB，DBL，FOB，这些关系均与宾语成分相关\n2）拼音向量，从wo，guo的拼音相似拼音来看，我们可以看到，这种相似的拼音更像是一种搭配， 很有意思，(词性参照jieba分词词性对照表)。\n3）词性向量，从a，n的相似词性来看，也似乎更像是一种搭配现象，或许有更好的解释。\n4.2 DomainEmbedding 为了更好的适配不同领域的任务，当前也有很多的公司或者任务会选择使用领域性的领域进行训练，以得到不同领域的词向量文件，这与当前各种领域的bert模型做法是类似的。当前出现了金融领域bert、法律领域的bert等。\n代表性的，2018年推出的Chinese-Word-Vectors中提供了包含经过数十种用各领域语料（百度百科、维基百科、人民日报 1947-2017、知乎、微博、文学、金融、古汉语等）训练的词向量，涵盖各领域，且包含多种训练设置。\n又如，当前PaddleNLP官方提供了61种可直接加载的预训练词向量，训练自多领域中英文语料、如百度百科、新闻语料、微博等，覆盖多种经典词向量模型（word2vec、glove、fastText）、涵盖不同维度、不同语料库大小。\n4.3 GraphEmbdding 经典的deepwalk以及node2vec也是借鉴word2vec思想，学习图节点嵌入的方法。并且成为当前推荐系统中的一个重量级使用方法。\n1、Deepwalk\n通过对图中的节点进行随机游走（主要考虑深度优先遍历），形成节点之间的游走序列，并将其作为上下文，后面接入skipgram形成节点向量，从构造上来看，就是首先利用random walk来表示图结构，然后利用skip-gram模型来更新学习节点表示。\n随机选取与其邻接的下一个结点，直至达到给定长度，这个长度作为一个参数进行指定，这个类似于word2vec中的window_size上下文窗口。\n2、node2vec\nnode2vec综合考虑了广度优先遍历（用于捕捉局部信息）和深度优先遍历（用于捕捉全局信息）的游走，提出二阶随机游走思想，解决内容相似和结构相似的问题。\n前者具有直接链接关系的两个节点，我们可以认为是内容相似的（例如两个灰色网站之间很有可能能够直接跳转，如图中的s1，s2等一阶邻居）、结构相似（例如周围邻居数量都很类似，如图中的s6和u节点，两个都有4个邻接，结构类似）。\n具体实现思路也很简单：\n我们从节点v转移到节点t，并且当前在节点t时，需要考虑下一个采样节点x。因此，可以设计一个节点到它的不同邻居的转移概率：\n其中，每一步采样都会有三种状态，分别对应于上图的0，1，2三种情况：\n 1）0代表如果t和x相等，那么采样的概率为1/p； 2）1代表t与x相连，采样的概率为1； 3）2代表t与x不相连，采样的概率为1/q**  式子中的参数p作为返回参数，控制重新采样上一步已访问节点的概率。参数q，作为出入参数，控制采样的方向。\n其中：\n 1）当q\u0026gt;1时，接下来采样的节点倾向于节点t，偏向于广度优先； 2）当q\u0026lt;1时，接下来采样的节点倾向于远离t，偏向于深度优先遍历。 3）当p\u0026gt;max(q,1)时，接下来采样的节点很大概率不是之前已访问节点，这一方法使得采样偏向深度优先； 4）当p\u0026lt;max(q,1)时，接下来采样的节点很大概率是之前已访问节点，这一方法使得采样偏向广度优先。  此外，在推荐场景中也有item2vec的类似延伸，例如协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达。\n\n五、预训练词向量的动手实操 纸上得来终觉浅，觉知此事要躬行，能够动手实践是加强对该概念理解的重要方式。预训练词向量，在流程上，应该包括全量训练和增量训练两种。前者可以在有大规模训练语料的情况下得到领域的向量，后者适用于小语料微调。 下面以gemsim中的wordvec和fasttext为例进行实践，大家可以看出其中的一些具体的步骤和结果。\n5.1 word2vec向量训练 1、构造训练语料 # coding = utf-8 import os import jieba import json cur = \u0026#39;/\u0026#39;.join(os.path.abspath(__file__).split(\u0026#39;/\u0026#39;)[:-1]) class Trainvec: def __init__(self): self.filepath = os.path.join(cur, \u0026#34;lawsuit.json\u0026#34;) self.update_filepath = os.path.join(cur, \u0026#34;duanzi.txt\u0026#34;) return def build_corpus(self): i = 0 train_path = open(os.path.join(cur, \u0026#34;train.txt\u0026#34;), \u0026#39;w+\u0026#39;) with open(self.filepath, \u0026#39;r\u0026#39;) as f: for line in f: i += 1 if not line.strip(): continue if i % 100 == 0: print(i) json_obj = json.loads(line.strip()) content = json_obj[\u0026#34;content\u0026#34;] text = \u0026#39;\\n\u0026#39;.join(content) cut_wds = jieba.lcut(text) train_path.write(\u0026#39; \u0026#39;.join(cut_wds) + \u0026#39;\\n\u0026#39;) train_path.close() return def build_update_corpus(self): i = 0 train_path = open(os.path.join(cur, \u0026#34;update.txt\u0026#34;), \u0026#39;w+\u0026#39;) with open(self.update_filepath, \u0026#39;r\u0026#39;) as f: for line in f: line = line.strip() i += 1 if not line.strip(): continue if i % 100 == 0: print(i) cut_wds = jieba.lcut(line) train_path.write(\u0026#39; \u0026#39;.join([i for i in cut_wds if i]) + \u0026#39;\\n\u0026#39;) train_path.close() return if __name__ == \u0026#39;__main__\u0026#39;: handler = Trainvec() #handler.build_corpus() handler.build_update_corpus() \n2、配置输入与输出路径 # -*- coding: utf-8 -*- import os import gensim from gensim.models import word2vec import logging cur = \u0026#39;/\u0026#39;.join(os.path.abspath(__file__).split(\u0026#39;/\u0026#39;)[:-1]) filepath = os.path.join(cur, \u0026#34;train.txt\u0026#34;) update_filepath = os.path.join(cur, \u0026#34;update.txt\u0026#34;) model_path = \u0026#34;wordvec.model\u0026#34; model_update_path = \u0026#34;wordvec_update.model\u0026#34; model_vec_path = \u0026#34;wordvec.bin\u0026#34; model_update_vec_path = \u0026#34;wordvec_update.bin\u0026#34; \n3、全量数据预训练 def full_train_embedding(): num_features = 100 min_word_count = 3 num_workers = 4 context = 5 downsampling = 1e-3 # 获取日志信息 logging.basicConfig(format=\u0026#39;%(asctime)s:%(levelname)s:%(message)s\u0026#39;, level=logging.INFO) # 加载分词后的文本，使用的是Text8Corpus类 sentences = word2vec.Text8Corpus(filepath) # 训练模型，部分参数如下 model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sg=1, sample=downsampling) #保存模型,除包含词-向量,还保存词频等训练所需信息 model.save(model_path) #保存词向量文件,保存的模型仅包含词-向量信息 model.wv.save_word2vec_format(model_vec_path, binary=True) return model 在保存过程中，存在两种方式，保存模型,除包含词-向量,还保存词频等训练所需信息，保存词向量文件,保存的模型仅包含词-向量信息。所以我们可以看到，词向量文件，确实是word2vec模型的副产物。\n4、增量数据预训练 增量训练，主要解决在新的文本上进行训练，也可以引入一些新的词，但这个时候，需要考虑到min_count这一过滤条件。 def update_train_embedding(): # 获取日志信息 logging.basicConfig(format=\u0026#39;%(asctime)s:%(levelname)s:%(message)s\u0026#39;, level=logging.INFO) # 加载新的训练数据 text = word2vec.LineSentence(update_filepath) # 加载旧模型 model = word2vec.Word2Vec.load(model_path) # 更新词汇表 model.build_vocab(text, update=True) # 训练数据 model.train(text, total_examples=model.corpus_count, epochs=model.epochs) # epoch=iter语料库的迭代次数；（默认为5） total_examples:句子数。 # 保存模型，是分成两个来训练 model.save(model_update_path) # 保存词向量文件 model.wv.save_word2vec_format(model_update_vec_path, binary=True) return model \n5、词向量结果测试 def test_model(): model = gensim.models.KeyedVectors.load_word2vec_format(\u0026#34;wordvec.model.bin\u0026#34;, binary=True) while 1: wd = input(\u0026#34;enter an word:\u0026#34;).strip() res = model.most_similar(wd) print(res) return words 通过运行，我们可以得到如下查询结果：\nenter an word:开心 [(\u0026#39;高兴\u0026#39;, 0.7237069606781006), (\u0026#39;有缘\u0026#39;, 0.7097823619842529), (\u0026#39;开了花\u0026#39;, 0.7021969556808472), (\u0026#39;玩得\u0026#39;, 0.6799882650375366), (\u0026#39;快乐\u0026#39;, 0.6698621511459351), (\u0026#39;不亦乐乎\u0026#39;, 0.668710470199585), (\u0026#39;鉴宝\u0026#39;, 0.6672042012214661), (\u0026#39;越聊\u0026#39;, 0.6671714782714844), (\u0026#39;爱玩\u0026#39;, 0.6659203767776489), (\u0026#39;着迷\u0026#39;, 0.6657696962356567)] enter an word:混蛋 [(\u0026#39;享福\u0026#39;, 0.9413065910339355), (\u0026#39;没良心\u0026#39;, 0.9331107139587402), (\u0026#39;怪不得\u0026#39;, 0.9317291975021362), (\u0026#39;养不活\u0026#39;, 0.9283043742179871), (\u0026#39;好惨\u0026#39;, 0.9255991578102112), (\u0026#39;看笑话\u0026#39;, 0.9251411557197571), (\u0026#39;逗我\u0026#39;, 0.9232471585273743), (\u0026#39;命苦\u0026#39;, 0.9226915836334229), (\u0026#39;别怪\u0026#39;, 0.921725332736969), (\u0026#39;我养\u0026#39;, 0.9205465316772461)] enter an word:巴嘎 KeyError: \u0026#34;word \u0026#39;巴嘎\u0026#39; not in vocabulary\u0026#34; 从上面我们可以看到，wordvec中对于词表外的词是无法查询的，为了缓解这一问题，可以通过训练时候的min_count参数调至1，以覆盖更多的词语，另一种则是进行增量训练。\n5.2 fasttext向量训练 与wordvec类似，fasttext也才用了类似的训练方法。\n1、全量数据训练 def full_train_embedding(): feature_size = 100 window_size = 5 min_count = 3 workers = 4 corpus_file = datapath(filepath) logging.basicConfig(format=\u0026#39;%(asctime)s:%(levelname)s:%(message)s\u0026#39;, level=logging.INFO) model = FastText(size=feature_size, min_count=min_count, window=window_size, workers=workers) model.build_vocab(corpus_file=corpus_file) model.train( corpus_file=corpus_file, epochs=model.epochs, total_examples=model.corpus_count, total_words=model.corpus_total_words ) model.save(model_path) #保存词向量文件,保存的模型仅包含词-向量信息 model.wv.save_word2vec_format(model_vec_path, binary=True) \n2、增量数据训练 def update_train_embedding(): # 获取日志信息 logging.basicConfig(format=\u0026#39;%(asctime)s:%(levelname)s:%(message)s\u0026#39;, level=logging.INFO) # 加载新的训练数据 text = word2vec.LineSentence(update_filepath) # 加载旧模型 model = FastText.load(model_path) # 更新词汇表 model.build_vocab(text, update=True) # 训练数据 model.train(text, total_examples=model.corpus_count, epochs=model.epochs) # epoch=iter语料库的迭代次数；（默认为5） total_examples:句子数。 # 保存模型，是分成两个来训练 model.save(model_update_path) # 保存词向量文件 model.wv.save_word2vec_format(model_update_vec_path, binary=True) return \n3、词向量结果测试 def test_model(): model = FastText.load(model_path) while 1: wd = input(\u0026#34;enter an word:\u0026#34;).strip() res = model.wv.most_similar(wd) print(res) return 通过执行，我们会得到以下查询结果：\nenter an word:开心 [(\u0026#39;开心果\u0026#39;, 0.7953568696975708), (\u0026#39;高兴\u0026#39;, 0.7377268671989441), (\u0026#39;郡县\u0026#39;, 0.6981974244117737), (\u0026#39;有缘\u0026#39;, 0.6916821002960205), (\u0026#39;折勾以\u0026#39;, 0.687650203704834), (\u0026#39;爱\u0026#39;, 0.684776782989502), (\u0026#39;愉快\u0026#39;, 0.6840348243713379), (\u0026#39;快乐\u0026#39;, 0.676334023475647), (\u0026#39;太高兴\u0026#39;, 0.6728817224502563), (\u0026#39;放心\u0026#39;, 0.6692144274711609)] enter an word:混蛋 [(\u0026#39;侯希辰\u0026#39;, 0.7582178115844727), (\u0026#39;舐\u0026#39;, 0.7578023672103882), (\u0026#39;走眼\u0026#39;, 0.7541716694831848), (\u0026#39;有眼\u0026#39;, 0.7511969804763794), (\u0026#39;贺应勤\u0026#39;, 0.7478049397468567), (\u0026#39;罗敏\u0026#39;, 0.747008204460144), (\u0026#39;郭守桥\u0026#39;, 0.7450246810913086), (\u0026#39;熊芳琴\u0026#39;, 0.7417726516723633), (\u0026#39;找死\u0026#39;, 0.741632342338562), (\u0026#39;许身\u0026#39;, 0.7414941787719727)] enter an word:巴嘎 [(\u0026#39;陈晓大爆\u0026#39;, 0.3896751403808594), (\u0026#39;董王勇\u0026#39;, 0.36747634410858154), (\u0026#39;李刚\u0026#39;, 0.34988462924957275), (\u0026#39;曾杰\u0026#39;, 0.34452974796295166), (\u0026#39;张文宾\u0026#39;, 0.3370075821876526), (\u0026#39;成浩\u0026#39;, 0.3369928300380707), (\u0026#39;刘晓静\u0026#39;, 0.3348349630832672), (\u0026#39;刘晓丹\u0026#39;, 0.3348219394683838), (\u0026#39;刘骏\u0026#39;, 0.32817351818084717), (\u0026#39;吴建明\u0026#39;, 0.32765522599220276)] 与上面的wordvec无法处理OOV问题不同，对于八嘎这一词，fasttext依旧可以推断出来，关于这个中间步骤，我们可以作为单独一个问题来说明。\n4、fasttext是如何解决oov问题的 通过对其源码进行阅读，可以发现fasttext针对OOV词的原始计算方式包括三个步骤，\n 1）抽取出每个词的N-grams; 2）与预先存好的n-grams词库进行匹配; 3）将匹配到的n-gram向量进行平均，实现如下：  from gensim.models.utils_any2vec import _save_word2vec_format, _load_word2vec_format, _compute_ngrams, _ft_hash def compute_ngrams(word, min_n, max_n): BOW, EOW = (\u0026#39;\u0026lt;\u0026#39;, \u0026#39;\u0026gt;\u0026#39;) # Used by FastText to attach to all words as prefix and suffix extended_word = BOW + word + EOW ngrams = [] for ngram_length in range(min_n, min(len(extended_word), max_n) + 1): for i in range(0, len(extended_word) - ngram_length + 1): ngrams.append(extended_word[i:i + ngram_length]) return ngrams def word_vec(self, word, use_norm=False): if word in self.vocab: return super(FastTextKeyedVectors, self).word_vec(word, use_norm) else: # from gensim.models.fasttext import compute_ngrams word_vec = np.zeros(self.vectors_ngrams.shape[1], dtype=np.float32) ngrams = _compute_ngrams(word, self.min_n, self.max_n) if use_norm: ngram_weights = self.vectors_ngrams_norm else: ngram_weights = self.vectors_ngrams ngrams_found = 0 for ngram in ngrams: ngram_hash = _ft_hash(ngram) % self.bucket if ngram_hash in self.hash2index: word_vec += ngram_weights[self.hash2index[ngram_hash]] ngrams_found += 1 if word_vec.any(): return word_vec / max(1, ngrams_found) else: # No ngrams of the word are present in self.ngrams raise KeyError(\u0026#39;all ngrams for word %sabsent from model\u0026#39; % word) 例如，通过滑动窗口的方式，设定最短ngram和最长ngram，可以得到ngram集合。\n\u0026gt;\u0026gt;\u0026gt; from gensim.models.utils_any2vec import * \u0026gt;\u0026gt;\u0026gt; ngrams = compute_ngrams(\u0026#39;好嗨哦\u0026#39;,min_n = 1,max_n =3) \u0026gt;\u0026gt;\u0026gt; ngrams [\u0026#39;\u0026lt;\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;嗨\u0026#39;, \u0026#39;哦\u0026#39;, \u0026#39;\u0026gt;\u0026#39;, \u0026#39;\u0026lt;好\u0026#39;, \u0026#39;好嗨\u0026#39;, \u0026#39;嗨哦\u0026#39;, \u0026#39;哦\u0026gt;\u0026#39;, \u0026#39;\u0026lt;好嗨\u0026#39;, \u0026#39;好嗨哦\u0026#39;, \u0026#39;嗨哦\u0026gt;\u0026#39;] 不过，可以看到的是，ngram中引入了“\u0026lt;”和“\u0026gt;”用于标记头和尾，这对于语言模型来说十分生动。\n\n六、开源词向量训练工具与预训文件 不必重复造轮子，当前已经陆续出现了一些代表性的预训练词向量工具和词向量资源，我们可以充分利用好。\n6.1 开源词向量训练工具  ngram2vec： https://github.com/zhezhaoa/ngram2vec/ word2vec： https://github.com/svn2github/word2vec fasttext： https://github.com/facebookresearch/fastText glove：https://github.com/stanfordnlp/GloVe  6.2 开源预训练词向量文件  https://github.com/Embedding/Chinese-Word-Vectors https://github.com/liuhuanyong/Word2Vector https://github.com/liuhuanyong/ChineseEmbedding  \n七、本文总结 本文，主要围绕预训练词向量模型这一主题，对当前预训练词向量模型的常用方法、评估与应用方式、领域的迁移变体、当前开放的训练工具和词向量文件进行介绍，并以word2vec和fasttext为例，展示一个词向量训练的案例，做到理论与实践相结合。\n关于预训练词向量相关的文章目前已经有很多，关于更为细致的解读，可以参考其他材料。预训练词向量是bert出现之前，NLP处理业务问题的标配，绝对称得上是一个里程碑的事件，并且开创了“万物皆可embdding”的时代。\n​实际上，词向量的发展也在一定程度上验证了当前nlp的进步。\n由最开始的基于one-hot、tf-idf、textrank等的bag-of-words，到LSA（SVD）、pLSA、LDA的主题模型词向量，再到word2vec、fastText、glove为代表的固定表征，最后到当前elmo、GPT、bert为代表的基于词向量的动态表征，都说明了语义建模中的动态属性和文本语境的多样性。\n不过，我们需要认识的是，在此类词向量中，虽然其本质仍然是语言模型，但是它的目标不是语言模型本身，而是词向量，其所作的一系列优化，其专注于词向量本身，因此做了许多优化来提高计算效率。\n例如，与NNLM相比，word2vec将词向量直接sum，不再拼接，并舍弃隐层；考虑到sofmax归一化需要遍历整个词汇表，采用hierarchical softmax 和negative sampling进行优化，前者生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；后者对每一个样本中每一个词都进行负例采样。\n最后，以当前一个新的观点来结尾：\n现在的预训练语言模型是下一代知识图谱，那么预训练词向量是什么？垫底型相关词库？大家可以想想。\n\n参考文献  https://baijiahao.baidu.com/sid=1600509930259553151 https://mp.weixin.qq.com/s/u8WZqlsIcGCU5BqPH23S4w https://www.jianshu.com/p/546d12898378/ https://www.jianshu.com/p/471d9bfbd72f https://zhuanlan.zhihu.com/p/32965521 https://mp.weixin.qq.com/s/DvbvYppeuMaPn84SRAINcQ  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-07-embeddings-theory-applicaiton-liuhuanyong/","summary":"关于作者 刘焕勇，liuhuanyong，现任360人工智能研究院算法专家，前中科院软件所工程师，主要研究方向为知识图谱、事件图谱在实际业务中的落地应用。\n得语言者得天下，得语言资源者，分得天下，得语言逻辑者，争得天下。\n 个人主页：https://liuhuanyong.github.io 个人公众号：老刘说NLP  当前，以预训练语言模型PLM+fintune的自然语言处理范式可谓十分火热，有大量的文章在宣传这类方法，包括梳理以NNLM为起点的整个预训练方法的发展史。\n当前工业界，主要使用的预训练模型包括两种，一种是以wordvec为代表的预训练词向量，另一种是以BERT为代表的预训练语言模型。前者通常作为词语表示输入的初始化，后接NN/CNN/LSTM等编码层，后者既可以同样后接，也可以直接接上softmax/crf/span-pointer等进行解码。\n本文，主要围绕预训练词向量模型这一主题，对当前预训练词向量模型的常用方法、评估与应用方式、领域的迁移变体、当前开放的训练工具和词向量文件进行介绍，并以word2vec和fasttext为例，展示一个词向量训练的案例，做到理论与实践相结合。\n\n一、预训练词向量模型方法 自从进入2010年以来，神经语言模型就逐渐进入人们眼球，以NNLM为典型最初代表的神经网络模型，极大的推动了NLP这一领域的发展。\n实际上，早期词向量的研究通常来源于语言模型，比如NNLM和RNNLM，其主要目的是语言模型，而词向量只是一个副产物。著名的harris分布式假说提供了一个局部统计信息的理论基础。\n下面就选择其中三种典型进行介绍。\n1.1 word2vec word2vec是2013年Google开源的一款用于词向量计算的工具，通过内置的语言模型训练目标，可以将中间层得到的向量权重矩阵进行抽离，形成每个词对应的向量化表示，包括CBOW、Skip-gram两种方式，前者通过周围词来预测中心词，后者以中心词来预测上下文。\n经典的wordvec结构包括输入层、隐藏层和输出层，其计算流程为：\n1、输入层存储上下文单词的onehot。假设单词向量空间dim为V，上下文单词个数为C。\n2、所有onehot分别乘以共享的输入权重矩阵W。V*N矩阵，N为自己设定的数，初始化权重矩阵W 。\n3、所得的向量 相加求平均作为隐层向量, size为1*N。\n4、乘以输出权重矩阵W' N*V。\n5、得到向量1*V，经过激活函数处理得到V-dim概率分布。\n6、Hierarchical Softmax分类，概率最大的index所指示的单词为预测出的中间词与预测值的onehot做比较，根据误差更新权重矩阵。\n这个W矩阵就是所有单词的word embedding，任何一个单词的onehot乘以这个矩阵都将得到自己的词向量。\n通常，在训练词向量时候，会根据语料的大小来选择相应的训练方法。例如，针对小型的数据集，可以用CBOW算法，该方法对于很多分布式信息进行了平滑处理，将一整段上下文信息视为一个单一观察量，对于小型的数据集，这一处理是有帮助的。相比之下，大型数据集，可以用Skip-Gram模型，该方法将每个“上下文-目标词汇”的组合视为一个新观察量，这种做法在大型数据集中会更为有效。\n1.2 fasttext fastText是Facebook于2016年开源的一个词向量计算和文本分类工具。将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。包括字符级n-gram特征的引入以及分层Softmax分类两种。\n与CBOW一样，原本的fastText模型包括输入层、隐含层、输出层，输入都是多个经向量表示的单词，输出都是一个特定的目标，隐含层都是对多个词向量的叠加平均。不同的是，CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档，CBOW的输入单词被onehot编码过，fastText的输入特征是经embedding化的，CBOW的输出是目标词汇，fastText的输出是文档对应的类标。\n而如果将该类标替换成中间目标词，那么就可以得到wordvec的升级版，即单纯的词向量模型。例如，word2vec把语料库中的每个单词当成原子的，它会为每个单词生成一个向量。这忽略了单词内部的形态特征。\nfasttext使用了字符级别的n-grams来表示一个单词。对于单词“apple”，假设n的取值为3，则它的trigram有“\u0026lt;ap”, “app”, “ppl”, “ple”, “le\u0026gt;”，其中，\u0026lt;表示前缀，\u0026gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，可以用这5个trigram的向量叠加来表示“apple”的词向量。\n因此，因为它们的n-gram可以和其它词共享，对于训练词库之外的单词，能够解决或者oov词，这也是在当前很多文本分类、推荐场景中会优先选用fastText作为训练方法。\n1.3 Glove GloVe是斯坦福团队于2014年提出一个词向量方法，全名叫“Global Vectors”，直接利用全局的统计信息进行训练。\n与上述两种方式靠滑动窗口来制造局部上下文不同，GloVe会用到全局的词语之间共现的统计信息，即词的出现次数，词对之间的共现概率，形成共现概率矩阵，并试图生成词向量来毕竟共现概率，利用Word2Vec的skip-gram算法的高性能来解决LDA的计算量复杂问题。\n因此，我们可以发现，Glove需要事先统计共现概率，这也让其通常被认为是无监督学习，实际上glove还是有label的，即共现次数。与wordvec还有一处不同的是，损失函数是最小平方损失函数，权重可以做映射变换。\n\n二、预训练词向量的训练参数 词向量模型的超参数很多，不同的参数选择会取得不同的效果，并且，word2vec中有几个大家提的比较多的问题。以gensim-word2vec为例，包括以下参数：\n sentences： 可以是一个list，对于大语料集，可使用BrownCorpus,Text8Corpus或LineSentence构建； sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法； size： 特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百； window： 表示当前词与预测词在一个句子中的最大距离是多少； alpha: 学习速率； seed： 用于随机数发生器。与初始化词向量有关； min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5； max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制； sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)；workers参数控制训练的并行数； hs: 如果为1则会采用hierarchical softmax技巧。如果设置为0（defaut），则negative sampling会被使用； negative: 如果\u0026gt;0,则会采用negativesamping，用于设置多少个noise words； cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defaut）则采用均值。只有使用CBOW的时候才起作用； hashfxn： hash函数来初始化权重。默认使用python的hash函数； iter： 迭代次数，默认为5； trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RUE_DISCARD,utis.","title":"预训练词向量模型的方法、应用场景、变体延伸与实践总结"},{"content":"使用 经营讨论与分析 数据，计算企业数字化指标, 相关论文:\n 吴非, 胡慧芷, 林慧妍, and 任晓怡. \u0026ldquo;企业数字化转型与资本市场表现——来自股票流动性的经验证据.\u0026rdquo; 管理世界 (2021). 宋德勇, 朱文博, and 丁海. \u0026ldquo;企业数字化能否促进绿色技术创新?.\u0026rdquo; 财经研究 48, no. 4 (2022).  数字化指标数分析结果以xlsx存储，如下图\n一、读取数据 完整md\u0026amp;a.xlsx数据集370M，覆盖33000+条 md\u0026amp;a记录。\n测试数据 small_test_data.xlsx 是从完整的数据中随机抽取的20条，结构基本一致。以测试数据为例，方便快速实验。\nimport pandas as pd # converters 强制声明该列为字符串， 防止股票代码 被程序识别为数字， df = pd.read_excel(\u0026#39;small_mda_data.xlsx\u0026#39;, converters={\u0026#39;股票代码\u0026#39;: str}) #显示前5行 df.head() 二、构建词典 下图是吴非等(2021)数字化指标的截图\n 后期，如果想自己扩展词典，可以初步筛选种子词(该篇论文的词表), 使用md\u0026amp;a语料文件(txt格式)， 结合cntext库的so-pmi或词向量方法，对数字化词典进行扩充。\n 这里我已将吴非等(2021)的词表内置到 cntext库（1.8.0版本）的 Chinese_Digitalization.pkl中。\n#安装cntext库 import cntext as ct Chinese_Digitalization_Info = ct.load_pkl_dict(\u0026#39;Chinese_Digitalization.pkl\u0026#39;) print(ct.__version__) print(Chinese_Digitalization_Info) Run\n1.8.0 {\u0026#39;Referer\u0026#39;: \u0026#39;吴非,胡慧芷,林慧妍,任晓怡. 企业数字化转型与资本市场表现——来自股票流动性的经验证据[J]. 管理世界,2021,37(07):130-144+10.\u0026#39;, \u0026#39;Desc\u0026#39;: \u0026#39;基于这篇论文，构建了中文数字化词典，含人工智能技术、大数据技术、云计算技术、区块链技术、数字技术应用等关键词列表。 \u0026#39;, \u0026#39;Chinese_Digitalization\u0026#39;: {\u0026#39;Artificial_Intelligence\u0026#39;: [\u0026#39;人工智能\u0026#39;, \u0026#39;商业智能\u0026#39;, \u0026#39;图像理解\u0026#39;, \u0026#39;投资决策辅助系统\u0026#39;, \u0026#39;智能数据分析, \u0026#39;智能机器人\u0026#39;, \u0026#39;机器学习\u0026#39;, \u0026#39;深度学习\u0026#39;, \u0026#39;语义搜索\u0026#39;, \u0026#39;生物识别技术\u0026#39;, \u0026#39;人脸识别\u0026#39;, \u0026#39;语音识别\u0026#39;, \u0026#39;身份验证\u0026#39;, \u0026#39;自动驾驶\u0026#39;, \u0026#39;自然语言处理\u0026#39;], \u0026#39;Big_Data\u0026#39;: [\u0026#39;大数据\u0026#39;, \u0026#39;数据挖掘\u0026#39;, \u0026#39;文本挖掘\u0026#39;, \u0026#39;数据可视化\u0026#39;, \u0026#39;异构数据\u0026#39;, \u0026#39;征信\u0026#39;, \u0026#39;增强现实\u0026#39;, \u0026#39;混合现实\u0026#39;, \u0026#39;虚拟现实\u0026#39;], \u0026#39;Cloud_Computing\u0026#39;: [\u0026#39;云计算\u0026#39;, \u0026#39;流计算\u0026#39;, \u0026#39;图计算\u0026#39;, \u0026#39;内存计算\u0026#39;, \u0026#39;多方安全计算\u0026#39;, \u0026#39;类脑计算\u0026#39;, \u0026#39;绿色计算\u0026#39;, \u0026#39;认知计算\u0026#39;, \u0026#39;融合架构\u0026#39;, \u0026#39;亿级并发\u0026#39;, \u0026#39;EB级存储\u0026#39;, \u0026#39;物联网\u0026#39;, \u0026#39;信息物理系统\u0026#39;], \u0026#39;Block_Chains\u0026#39;: [\u0026#39;区块链\u0026#39;, \u0026#39;数字货币\u0026#39;, \u0026#39;分布式计算\u0026#39;, \u0026#39;差分隐私技术\u0026#39;, \u0026#39;智能金融合约\u0026#39;], \u0026#39;Usage_of_Digitalization\u0026#39;: [\u0026#39;移动互联网\u0026#39;, \u0026#39;工业互联网\u0026#39;, \u0026#39;移动互联\u0026#39;, \u0026#39;互联网医疗\u0026#39;, \u0026#39;电子商务\u0026#39;, \u0026#39;移动支付\u0026#39;, \u0026#39;第三方支付\u0026#39;, \u0026#39;NFC支付\u0026#39;, \u0026#39;智能能源\u0026#39;, \u0026#39;B2B\u0026#39;, \u0026#39;B2C\u0026#39;, \u0026#39;C2B\u0026#39;, \u0026#39;C2C\u0026#39;, \u0026#39;O2O\u0026#39;, \u0026#39;网联\u0026#39;, \u0026#39;智能穿戴\u0026#39;, \u0026#39;智慧农业\u0026#39;, \u0026#39;智能交通\u0026#39;, \u0026#39;智能医疗\u0026#39;, \u0026#39;智能客服\u0026#39;, \u0026#39;智能家居\u0026#39;, \u0026#39;智能投顾\u0026#39;, \u0026#39;智能文旅\u0026#39;, \u0026#39;智能环保\u0026#39;, \u0026#39;智能电网\u0026#39;, \u0026#39;智能营销\u0026#39;, \u0026#39;数字营销\u0026#39;, \u0026#39;无人零售\u0026#39;, \u0026#39;互联网金融\u0026#39;, \u0026#39;数字金融\u0026#39;, \u0026#39;Fintech\u0026#39;, \u0026#39;金融科技\u0026#39;, \u0026#39;量化金融\u0026#39;, \u0026#39;开放银行\u0026#39;]}} \n三、定义数字化函数 目前，对于企业数字化水平的度量是相关研究的难点，现有文献主要有三种度量方法。\n 第一，祁怀锦等（2020）使用企业年末无形资产明细项中与数字经济相关部分的金额占无形资产总额的比例度量企业数字化程度。 第二，大量研究运用数字化相关关键词在年报中的词频数量或占比度量企业的数字化转型或数字化水平（赵宸宇，2021；袁淳等，2021）。 第三，相关研究采取问卷调查的方式获取企业的数字化水平数据（刘政等，2020）。  使用第二种方法，通过Python定义数字化函数，统计文本中数字化词语个数得到相应指标。\n 吴非等(2021管理世界)数字化指标的计算更复杂一些，在此基础上，剔除关键词前存在“没” “无” “不”等否定词语的表述，同时也剔除非本公司（包括公司的股东、客户、供应商、公司高管简介介绍在内）的“数 字化转型”关键词。\n import cntext as ct digtal_diction = ct.load_pkl_dict(\u0026#39;Chinese_Digitalization.pkl\u0026#39;)[\u0026#39;Chinese_Digitalization\u0026#39;] def digtal_function(text): #统计text中每类词的个数 res = ct.sentiment(text=text, diction=digtal_diction) return pd.Series(res) test_text = \u0026#39;经过技术人员不懈努力， 该企业在人工智能、大数据、云计算、工业互联网等领域有了一定的市场地位....\u0026#39; digtal_function(text=test_text) Run\nArtificial_Intelligence_num 1 Big_Data_num 1 Cloud_Computing_num 1 Block_Chains_num 0 Usage_of_Digitalization_num 1 stopword_num 11 word_num 24 sentence_num 1 dtype: int64 \n四、批量计算 使用apply方法，对 [经营讨论与分析内容] 列，进行 digtal_function 运算, 得到 res_df\n#结果返回为dataframe，数字代表的是每类词出现次数 res_df = df[\u0026#39;经营讨论与分析内容\u0026#39;].apply(digtal_function) res_df.head() Run\n参数解读\n Artificial_Intelligence_num\t人工智能技术词出现在md\u0026amp;a中的次数 Big_Data_num\t大数据技术词出现在md\u0026amp;a中的次数 Cloud_Computing_num\t云计算技术词出现在md\u0026amp;a中的次数 Block_Chains_num\t区块链技术词出现在md\u0026amp;a中的次数 Usage_of_Digitalization_num\t数字化应用技术词出现在md\u0026amp;a中的次数 stopword_num\t停用词出现在md\u0026amp;a中的次数 word_num\tmd\u0026amp;a中的总词数(md\u0026amp;a的长度) sentence_num md\u0026amp;a的句子数  五、结果整理 上一环节，将各种技术词出现次数加总，构建企业数字化词语出现个数， 并将其转为数字化指标(词频)。\n 由于这类数据具有典型的“右偏性”特征，后续在其他计量分析软件中需要将其进行对数化处理，从而得到刻画企业数字化转型的整体指标。\n res_df[\u0026#39;Digital_word_num\u0026#39;] = res_df[[\u0026#39;Artificial_Intelligence_num\u0026#39;, \u0026#39;Big_Data_num\u0026#39;, \u0026#39;Cloud_Computing_num\u0026#39;, \u0026#39;Block_Chains_num\u0026#39;, \u0026#39;Usage_of_Digitalization_num\u0026#39;]].sum(axis=1) # [数字化相关技术词] 在 [文本总词数] 中的占比 res_df[\u0026#39;Digital_Index\u0026#39;] = res_df[\u0026#39;Digital_word_num\u0026#39;]/res_df[\u0026#39;word_num\u0026#39;] res_df.head() Run\n六、保存结果 合并df 和 res_df\ndf2 = pd.concat([df, res_df], axis=1) df2.head() Run\n选中需要的字段，保存到 corporate_digitalization.xlsx 内\ndf[[\u0026#39;股票代码\u0026#39;, \u0026#39;公司简称\u0026#39;, \u0026#39;会计年度\u0026#39;, \u0026#39;Digital_Index\u0026#39;]].to_excel(\u0026#39;corporate_digitalization.xlsx\u0026#39;, index=False) 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-03-mda-measure-digitalization/","summary":"使用 经营讨论与分析 数据，计算企业数字化指标, 相关论文:\n 吴非, 胡慧芷, 林慧妍, and 任晓怡. \u0026ldquo;企业数字化转型与资本市场表现——来自股票流动性的经验证据.\u0026rdquo; 管理世界 (2021). 宋德勇, 朱文博, and 丁海. \u0026ldquo;企业数字化能否促进绿色技术创新?.\u0026rdquo; 财经研究 48, no. 4 (2022).  数字化指标数分析结果以xlsx存储，如下图\n一、读取数据 完整md\u0026amp;a.xlsx数据集370M，覆盖33000+条 md\u0026amp;a记录。\n测试数据 small_test_data.xlsx 是从完整的数据中随机抽取的20条，结构基本一致。以测试数据为例，方便快速实验。\nimport pandas as pd # converters 强制声明该列为字符串， 防止股票代码 被程序识别为数字， df = pd.read_excel(\u0026#39;small_mda_data.xlsx\u0026#39;, converters={\u0026#39;股票代码\u0026#39;: str}) #显示前5行 df.head() 二、构建词典 下图是吴非等(2021)数字化指标的截图\n 后期，如果想自己扩展词典，可以初步筛选种子词(该篇论文的词表), 使用md\u0026amp;a语料文件(txt格式)， 结合cntext库的so-pmi或词向量方法，对数字化词典进行扩充。\n 这里我已将吴非等(2021)的词表内置到 cntext库（1.8.0版本）的 Chinese_Digitalization.pkl中。\n#安装cntext库 import cntext as ct Chinese_Digitalization_Info = ct.load_pkl_dict(\u0026#39;Chinese_Digitalization.pkl\u0026#39;) print(ct.__version__) print(Chinese_Digitalization_Info) Run\n1.8.0 {\u0026#39;Referer\u0026#39;: \u0026#39;吴非,胡慧芷,林慧妍,任晓怡.","title":"管理世界 | 使用 经营讨论与分析 测量 企业数字化"},{"content":"一、招股说明书 从淘宝店花70元买的， 一共有27G， 百度网盘分享很不方便，且格式比较乱， 有txt、pdf等， 汇总整理至csv中。\n使用csv格式，只要定义相应的指标函数，就可以很方便得到相应文本变量。\n二、导入数据 截止2022.01.01，共有3630家公司，含公司名、股票代码、日期、标题、报告文本5个字段。\n 链接：https://pan.baidu.com/s/1pLZHDy0oXwcTiCakFb-KiA 提取码：e68l\n import pandas as pd df = pd.read_csv(\u0026#39;招股说明书.csv\u0026#39;) df.head() # 招股记录数 len(df) Run\n4630\r 三、定义指标函数 这里准备两个比较简单的指标，设计函数，可以理解为设计数据分析流水线某环节的输入和输出。\n 报告长度 情感得分 其他指标\u0026hellip;  例如\n3.1 报告长度函数  输入: 字符串 运算: 计算字符长度 输出: 数字  def length(text): return len(text) text = \u0026#39;你好啊\u0026#39; length(text) Run\n3\r 3.2 某类词个数  输入: 字符串 运算: 使用某种词典(成熟的或自己开发)，计算文本中正面词个数、负面词个数、总词数 输出: 数字  import cntext as ct #使用已有词典或自定义词典 diction = {\u0026#39;pos\u0026#39;: [\u0026#39;独家\u0026#39;, \u0026#39;进步\u0026#39;, \u0026#39;发展\u0026#39;, \u0026#39;稳定\u0026#39;, \u0026#39;卓越\u0026#39;, \u0026#39;提高\u0026#39;, \u0026#39;成功\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;丑闻\u0026#39;, \u0026#39;挪用\u0026#39;, \u0026#39;错过\u0026#39;, \u0026#39;不利\u0026#39;, \u0026#39;牺牲\u0026#39;, \u0026#39;干扰\u0026#39;, \u0026#39;过度\u0026#39;]} text = \u0026#39;公司在市场竞争中，主动发挥技术优势，取得了长足的发展。\u0026#39; ct.sentiment(text=text, diction=diction, lang=\u0026#39;chinese\u0026#39;) Run\n{'pos_num': 1,\r'neg_num': 0,\r'stopword_num': 7,\r'word_num': 16,\r'sentence_num': 1}\r import cntext as ct diction = {\u0026#39;pos\u0026#39;: [\u0026#39;独家\u0026#39;, \u0026#39;进步\u0026#39;, \u0026#39;发展\u0026#39;, \u0026#39;稳定\u0026#39;, \u0026#39;卓越\u0026#39;, \u0026#39;提高\u0026#39;, \u0026#39;成功\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;丑闻\u0026#39;, \u0026#39;挪用\u0026#39;, \u0026#39;错过\u0026#39;, \u0026#39;不利\u0026#39;, \u0026#39;牺牲\u0026#39;, \u0026#39;干扰\u0026#39;, \u0026#39;过度\u0026#39;]} def pos(text): #使用已有词典或自定义词典 res = ct.sentiment(text=text, diction=diction, lang=\u0026#39;chinese\u0026#39;) return res[\u0026#39;pos_num\u0026#39;] def neg(text): #使用已有词典或自定义词典 res = ct.sentiment(text=text, diction=diction, lang=\u0026#39;chinese\u0026#39;) return res[\u0026#39;neg_num\u0026#39;] text = \u0026#39;公司在市场竞争中，主动发挥技术优势，取得了长足的发展。\u0026#39; print(pos(text)) print(neg(text)) Run\n1\r0\r 四、批量运算 选中dataframe中某一列，使用apply应用某种计算函数。\n#确保text这列所有的数据均为字符串 #如果不是字符串，强制转化为字符串 df2[\u0026#39;text\u0026#39;] = df2[\u0026#39;text\u0026#39;].astype(str) df2[\u0026#39;Len\u0026#39;] = df2[\u0026#39;text\u0026#39;].apply(length) df2[\u0026#39;Pos\u0026#39;] = df2[\u0026#39;text\u0026#39;].apply(pos) df2[\u0026#39;Neg\u0026#39;] = df2[\u0026#39;text\u0026#39;].apply(neg) df2[\u0026#39;Senti\u0026#39;] = (df2[\u0026#39;Pos\u0026#39;]-df2[\u0026#39;Neg\u0026#39;])/(df2[\u0026#39;Pos\u0026#39;]+df2[\u0026#39;Neg\u0026#39;]) df2.head() Run\n五、保存 最后保存为csv、或xlsx，具体根据自己需要进行选择。\n df.to_csv() df.to_excel()  #df.to_csv(\u0026#39;result.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, index=False) df.to_excel(\u0026#39;result.xlsx\u0026#39;, index=False) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-11-02-27g-python-27g-a-share-market-prospectus/","summary":"一、招股说明书 从淘宝店花70元买的， 一共有27G， 百度网盘分享很不方便，且格式比较乱， 有txt、pdf等， 汇总整理至csv中。\n使用csv格式，只要定义相应的指标函数，就可以很方便得到相应文本变量。\n二、导入数据 截止2022.01.01，共有3630家公司，含公司名、股票代码、日期、标题、报告文本5个字段。\n 链接：https://pan.baidu.com/s/1pLZHDy0oXwcTiCakFb-KiA 提取码：e68l\n import pandas as pd df = pd.read_csv(\u0026#39;招股说明书.csv\u0026#39;) df.head() # 招股记录数 len(df) Run\n4630\r 三、定义指标函数 这里准备两个比较简单的指标，设计函数，可以理解为设计数据分析流水线某环节的输入和输出。\n 报告长度 情感得分 其他指标\u0026hellip;  例如\n3.1 报告长度函数  输入: 字符串 运算: 计算字符长度 输出: 数字  def length(text): return len(text) text = \u0026#39;你好啊\u0026#39; length(text) Run\n3\r 3.2 某类词个数  输入: 字符串 运算: 使用某种词典(成熟的或自己开发)，计算文本中正面词个数、负面词个数、总词数 输出: 数字  import cntext as ct #使用已有词典或自定义词典 diction = {\u0026#39;pos\u0026#39;: [\u0026#39;独家\u0026#39;, \u0026#39;进步\u0026#39;, \u0026#39;发展\u0026#39;, \u0026#39;稳定\u0026#39;, \u0026#39;卓越\u0026#39;, \u0026#39;提高\u0026#39;, \u0026#39;成功\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;丑闻\u0026#39;, \u0026#39;挪用\u0026#39;, \u0026#39;错过\u0026#39;, \u0026#39;不利\u0026#39;, \u0026#39;牺牲\u0026#39;, \u0026#39;干扰\u0026#39;, \u0026#39;过度\u0026#39;]} text = \u0026#39;公司在市场竞争中，主动发挥技术优势，取得了长足的发展。\u0026#39; ct.","title":"27G数据集 | 使用Python对27G招股说明书进行文本分析"},{"content":"熟悉IT的同学知道，下载工具有curl和wget。但是这类工具很难成功下载大体积的文件，今天分享的gdown可以帮我们解决这个问题。不过使用该工具的其那题是， 电脑可以科学地上网。\n安装 pip3 install gdown \n使用 gdown安装后有两种使用方法\n 命令行模式 代码模式  命令行模式 $ gdown --help usage: gdown [-h] [-V] [-O OUTPUT] [-q] [--fuzzy] [--id] [--proxy PROXY] [--speed SPEED] [--no-cookies] [--no-check-certificate] [--continue] [--folder] [--remaining-ok] url_or_id ... $ # 大文件 (~500MB) $ gdown https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ $ md5sum fcn8s_from_caffe.npz 256c2a8235c1c65e62e48d3284fbd384 \n代码模式 import gdown # 下载 网盘文件 url = \u0026#34;https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ\u0026#34; output = \u0026#34;fcn8s_from_caffe.npz\u0026#34; gdown.download(url, output, quiet=False) # 使用文件ID作为文件名 id = \u0026#34;0B9P1L--7Wd2vNm9zMTJWOGxobkU\u0026#34; gdown.download(id=id, output=output, quiet=False) # same as the above, and you can copy-and-paste a URL from Google Drive with fuzzy=True url = \u0026#34;https://drive.google.com/file/d/0B9P1L--7Wd2vNm9zMTJWOGxobkU/view?usp=sharing\u0026#34; gdown.download(url=url, output=output, quiet=False, fuzzy=True) # 下载 网盘文件夹 url = \u0026#34;https://drive.google.com/drive/folders/15uNXeRBIhVvZJIhL4yTw4IsStMhUaaxl\u0026#34; gdown.download_folder(url, quiet=True, use_cookies=False) # 使用文件夹ID作为文件夹名 id = \u0026#34;15uNXeRBIhVvZJIhL4yTw4IsStMhUaaxl\u0026#34; gdown.download_folder(id=id, quiet=True, use_cookies=False) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-10-31-gdown-googledriver/","summary":"熟悉IT的同学知道，下载工具有curl和wget。但是这类工具很难成功下载大体积的文件，今天分享的gdown可以帮我们解决这个问题。不过使用该工具的其那题是， 电脑可以科学地上网。\n安装 pip3 install gdown \n使用 gdown安装后有两种使用方法\n 命令行模式 代码模式  命令行模式 $ gdown --help usage: gdown [-h] [-V] [-O OUTPUT] [-q] [--fuzzy] [--id] [--proxy PROXY] [--speed SPEED] [--no-cookies] [--no-check-certificate] [--continue] [--folder] [--remaining-ok] url_or_id ... $ # 大文件 (~500MB) $ gdown https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ $ md5sum fcn8s_from_caffe.npz 256c2a8235c1c65e62e48d3284fbd384 \n代码模式 import gdown # 下载 网盘文件 url = \u0026#34;https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ\u0026#34; output = \u0026#34;fcn8s_from_caffe.npz\u0026#34; gdown.download(url, output, quiet=False) # 使用文件ID作为文件名 id = \u0026#34;0B9P1L--7Wd2vNm9zMTJWOGxobkU\u0026#34; gdown.download(id=id, output=output, quiet=False) # same as the above, and you can copy-and-paste a URL from Google Drive with fuzzy=True url = \u0026#34;https://drive.","title":"gdown库 |  从googleDriver下载大体积文件"},{"content":"\rtextstat可以对文本进行可读性计算，支持英文、德语、西班牙、意大利、荷兰语等。目前不支持中文，如做中文文本分析，可以考虑用cntext包。\nhttps://github.com/textstat/textstat\n\n任务\r今天在本文中，将学习三个知识点。\n读取csv数据文件\r选中csv中某列文本数据，依次使用apply方法，计算FOG、ARI、CLI等。\r综合三个可读性指标，构造可读性mean值\r\r\n\r安装\rpip3 install textstat\r\n\r读取数据\r点击下载实验数据data.csv\nimport pandas as pd\rimport textstat\r#设置dataframe显示的宽度\rpd.options.display.max_colwidth = 50\rdf = pd.read_csv(\u0026#39;data.csv\u0026#39;)\rdf\r## doc\r## 0 Playing games has always been thought to be im...\r## 1 the development of well-balanced and creative ...\r## 2 however, what part, if any, they should play i...\r## 3 of adults has never been researched that deepl...\r## 4 that playing games is every bit as important f...\r## 5 as for children. Not only is taking time out t...\r## 6 with our children and other adults valuable to...\r## 7 interpersonal relationships but is also a wond...\r## 8 to release built up tension.\r## 9 The language will be used for syllable calcula...\r\n\rSeries批操作\r使用apply方法对pd.Series类型的数据进行批操作\nextstat库有丰富的可读性方法，这里任选2个作为 批操作函数。\n\rFog textstat.gunning_fog(text)\rFlesch textstat.flesch_reading_ease(text)\r\rdf[\u0026#39;Fog\u0026#39;] = df[\u0026#39;doc\u0026#39;].apply(textstat.gunning_fog)\rdf[\u0026#39;Flesch\u0026#39;] = df[\u0026#39;doc\u0026#39;].apply(textstat.flesch_reading_ease)\rdf.head()\r## doc Fog Flesch\r## 0 Playing games has always been thought to be im... 4.00 78.25\r## 1 the development of well-balanced and creative ... 8.51 30.53\r## 2 however, what part, if any, they should play i... 4.40 94.15\r## 3 of adults has never been researched that deepl... 4.00 86.71\r## 4 that playing games is every bit as important f... 4.00 78.25\r\n\rDataFrame均值\r选中Fog、Flesch两列\n#查看df[[\u0026#39;Fog\u0026#39;, \u0026#39;Flesch\u0026#39;]]数据类型\rtype(df[[\u0026#39;Fog\u0026#39;, \u0026#39;Flesch\u0026#39;]])\r## \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt;\r#对这两个指标水平方向进行均值\rdf[\u0026#39;Mean\u0026#39;] = df[[\u0026#39;Fog\u0026#39;, \u0026#39;Flesch\u0026#39;]].mean(axis=1)\rdf.head()\r## doc Fog Flesch Mean\r## 0 Playing games has always been thought to be im... 4.00 78.25 41.125\r## 1 the development of well-balanced and creative ... 8.51 30.53 19.520\r## 2 however, what part, if any, they should play i... 4.40 94.15 49.275\r## 3 of adults has never been researched that deepl... 4.00 86.71 45.355\r## 4 that playing games is every bit as important f... 4.00 78.25 41.125\r\n\r存储\r存储到可读性.csv中\ndf.to_csv(\u0026#39;可读性.csv\u0026#39;, index=False)\r\n\r广而告之\r\r长期征稿\r长期招募小伙伴\r付费视频课 | Python实证指标构建与文本分析\r\r\r","permalink":"/blog/2022-10-22-textstats-readability/","summary":"textstat可以对文本进行可读性计算，支持英文、德语、西班牙、意大利、荷兰语等。目前不支持中文，如做中文文本分析，可以考虑用cntext包。\nhttps://github.com/textstat/textstat\n\n任务\r今天在本文中，将学习三个知识点。\n读取csv数据文件\r选中csv中某列文本数据，依次使用apply方法，计算FOG、ARI、CLI等。\r综合三个可读性指标，构造可读性mean值\r\r\n\r安装\rpip3 install textstat\r\n\r读取数据\r点击下载实验数据data.csv\nimport pandas as pd\rimport textstat\r#设置dataframe显示的宽度\rpd.options.display.max_colwidth = 50\rdf = pd.read_csv(\u0026#39;data.csv\u0026#39;)\rdf\r## doc\r## 0 Playing games has always been thought to be im...\r## 1 the development of well-balanced and creative ...\r## 2 however, what part, if any, they should play i...\r## 3 of adults has never been researched that deepl.","title":"使用textstat库计算文本可读性"},{"content":"2007-2021年A股上市公司年度报告, 整理不易，请转发分享。\n截图 获取  链接: https://pan.baidu.com/s/1jw6VGGAN9cxROoqWN2X4vw 提取码: g3cn\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-10-21-2007-2021-a-share-reports-dataset/","summary":"2007-2021年A股上市公司年度报告, 整理不易，请转发分享。\n截图 获取  链接: https://pan.baidu.com/s/1jw6VGGAN9cxROoqWN2X4vw 提取码: g3cn\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"14G数据集 | 2007-2021年A股上市公司年度报告（txt文件）"},{"content":"\r词嵌入方法（word2vec、glove等）可以将每个词的语义映射到n维空间，在n维空间中，词语间距离远近可以表征语义的远近。Kusner等人(2015)提出词移距离（word mover’s distance， 后文用WMD缩写代替）借助词语向量语义距离，实现两文档间的相似度计算，距离越小，相似度越高。在会计领域中的应用可以用来度量问答场景的答非所问的程度。\n\nWMD基础\r有两个文档\ndoc1 = \u0026quot;Obama speaks to the media in Illinois\u0026quot;\rdoc2 = \u0026quot;The President greets the press in Chicago.\u0026quot;\r词向量一般是高(n)维空间，这里把n压缩到2维空间，使用matplotlib绘图。两个文档中重要的词语彼此之间存在语义相似度，\n# Image from https://vene.ro/images/wmd-obama.png\rimport matplotlib.pyplot as plt\rimport matplotlib.image as mpimg\rimg = mpimg.imread(\u0026#39;img/wmd-obama.png\u0026#39;)\rimgplot = plt.imshow(img)\rplt.axis(\u0026#39;off\u0026#39;)\r## (-0.5, 2397.5, 1327.5, -0.5)\rplt.show()\r\n\r计算WMD步骤\r剔除文档中停止词，如the、a等无信息量词\r导入预训练好的词嵌入(word2vec)模型(网上资源比较多，如果数据量很大，也可以自己使用gensim训练自己的词向量)\r计算WMD\r\rfrom nltk.corpus import stopwords\rfrom nltk import download\rdownload(\u0026#39;stopwords\u0026#39;) # Download stopwords list.\rstop_words = stopwords.words(\u0026#39;english\u0026#39;)\rdef preprocess(sentence):\rreturn [w for w in sentence.lower().split() if w not in stop_words]\rdoc1 = \u0026quot;Obama speaks to the media in Illinois\u0026quot;\rdoc2 = \u0026quot;The President greets the press in Chicago.\u0026quot;\rdoc1 = preprocess(doc1)\rdoc2 = preprocess(doc2)\rprint(doc1)\rprint(doc2)\rRun\n[\u0026#39;obama\u0026#39;, \u0026#39;speaks\u0026#39;, \u0026#39;media\u0026#39;, \u0026#39;illinois\u0026#39;]\r[\u0026#39;president\u0026#39;, \u0026#39;greets\u0026#39;, \u0026#39;press\u0026#39;, \u0026#39;chicago.\u0026#39;]\r\n\r如果运行代码出现nltk问题，可以观看视频 https://www.bilibili.com/video/BV14A411i7DB\n\r下载谷歌新闻预训练模型(word2vec-google-news-300) ,这里可以使用我提供的百度网盘\n\r链接：https://pan.baidu.com/s/1yzGLcMsZl3u1zigTHLdc2Q\r提取码：l63f\n\r这里需要\nfrom gensim.models import KeyedVectors\rw2v_model = KeyedVectors.load(\u0026#39;GoogleNews-vectors-negative300.bin.gz\u0026#39;)\rwmd = w2v_model.wmdistance(doc1, doc2)\rprint(\u0026#39;distance :{wmd}\u0026#39;.format(wmd=wmd))\rRun\ndistance :0.8867237050133944\r\n\r参考文献\r\rKusner, Matt J., Yu Sun, Nicholas I. Kolkin and Kilian Q. Weinberger. “From Word Embeddings To Document Distances.” ICML (2015).\rhttps://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html\r\r\n\r广而告之\r\r长期征稿\r长期招募小伙伴\r付费视频课 | Python实证指标构建与文本分析\r\r\r","permalink":"/blog/2022-10-16-python-word-mover-s-distance/","summary":"词嵌入方法（word2vec、glove等）可以将每个词的语义映射到n维空间，在n维空间中，词语间距离远近可以表征语义的远近。Kusner等人(2015)提出词移距离（word mover’s distance， 后文用WMD缩写代替）借助词语向量语义距离，实现两文档间的相似度计算，距离越小，相似度越高。在会计领域中的应用可以用来度量问答场景的答非所问的程度。\n\nWMD基础\r有两个文档\ndoc1 = \u0026quot;Obama speaks to the media in Illinois\u0026quot;\rdoc2 = \u0026quot;The President greets the press in Chicago.\u0026quot;\r词向量一般是高(n)维空间，这里把n压缩到2维空间，使用matplotlib绘图。两个文档中重要的词语彼此之间存在语义相似度，\n# Image from https://vene.ro/images/wmd-obama.png\rimport matplotlib.pyplot as plt\rimport matplotlib.image as mpimg\rimg = mpimg.imread(\u0026#39;img/wmd-obama.png\u0026#39;)\rimgplot = plt.imshow(img)\rplt.axis(\u0026#39;off\u0026#39;)\r## (-0.5, 2397.5, 1327.5, -0.5)\rplt.show()\r\n\r计算WMD步骤\r剔除文档中停止词，如the、a等无信息量词\r导入预训练好的词嵌入(word2vec)模型(网上资源比较多，如果数据量很大，也可以自己使用gensim训练自己的词向量)\r计算WMD\r\rfrom nltk.corpus import stopwords\rfrom nltk import download\rdownload(\u0026#39;stopwords\u0026#39;) # Download stopwords list.","title":"Python | 词移距离(Word Mover's Distance)"},{"content":"介绍 Facebook研究者使用 fastText 算法，对维基百科(44种语言)语料数据进行了训练，最终生成了 44 种语言的对齐词向量。\n用途 wiki数据集有个优点，即由于众人分享、翻译，将不同语言的百科词条进行了翻译整理。所以facebook使用wiki训练对齐词向量有助于提升翻译准确性。与此同时，因为翻译者处于不同的语言和文化背景下，词条及词条内容必然蕴含着语言所特有的文化信息线索，有可能有助于我们挖掘跨语言的文化差异。例如中文词条护士和 英文词条nurse ，可以借助对齐词向量，比较护士这个群体在性别、种族等语义上的差异。\n之前分享过的内容\n 词嵌入测量不同群体对某概念的态度(偏见) 转载 | 大数据时代下社会科学研究方法的拓展\u0026mdash;\u0026mdash;基于词嵌入技术的文本分析的应用 转载 | 从符号到嵌入：计算社会科学的两种文本表示 文献汇总 | 词嵌入 与 社会科学中的偏见(态度)  不过fastText算法认为词语有不同的大小划分层次，从大到小分别是词语、词缀、字符等，使用 Joulin 等人 (2018) 中描述的 RCSLS 方法进行比对。\n   Code en-es es-en en-fr fr-en en-de de-en en-ru ru-en en-zh zh-en avg     Joulin et al. [1] 84.1 86.3 83.3 84.1 79.1 76.3 57.9 67.2 45.9 46.4 71.1   This implementation (10 epochs) 84.2 86.6 83.9 84.7 78.3 76.6 57.6 66.7 47.6 47.4 71.4   This implementation (unsup. model selection) 84.3 86.6 83.9 85.0 78.7 76.7 57.6 67.1 47.6 47.4 71.5    算法得出的词向量在西方，尤其是西欧语言之间进行语义对齐，效果可能更好。而中文、日语等汉字语言，是由偏旁部首组成，与西方字母语言还是存在一定差异。上表也可以看出中英语义对齐准确率47%， 而其他语言之间对齐准确率平均为71%。\n模型资源 https://fasttext.cc/docs/en/aligned-vectors.html\n对齐预训练向量模型下载链接\n           Afrikaans: text Arabic: text Bulgarian: text Bengali: text   Bosnian: text Catalan: text Czech: text Danish: text   German: text Greek: text English: text Spanish: text   Estonian: text Persian: text Finnish: text French: text   Hebrew: text Hindi: text Croatian: text Hungarian: text   Indonesian: text Italian: text Korean: text Lithuanian: text   Latvian: text Macedonian: text Malay: text Dutch: text   Norwegian: text Polish: text Portuguese: text Romanian: text   Russian: text Slovak: text Slovenian: text Albanian: text   Swedish: text Tamil: text Thai: text Tagalog: text   Turkish: text Ukrainian: text Vietnamese: text Chinese: text    格式 词向量默认使用的fastText格式\n 第一行给了词向量的维数 从第二行开始，每一行由词语及对应的词向量组成。 数值之间使用空格间隔  代码 导入模型 使用gensim导入fastText方法训练出的 预训练语言模型\nfrom gensim.models import KeyedVectors #导入刚刚下载的预训练模型 #该词向量模型300维 zh_w2v_model = KeyedVectors.load_word2vec_format(\u0026#39;wiki.zh.align.vec\u0026#39;, binary=False) #英文词向量模型5G，太大了。如果内存小于16G不要使用下面命令 #en_w2v_model = KeyedVectors.load_word2vec_format(\u0026#39;wiki.en.align.vec\u0026#39;, binary=False) 一旦导入成功，就可以进行向量计算。这里仅进行简单演示\n#获取某词的词向量 zh_w2v_model.get_vector(\u0026#39;护士\u0026#39;) Run\narray([ 0.0733, 0.0782, 0.0188, -0.0027, -0.0052,..., 0.0586, 0.0166,\r-0.1401, -0.0545, -0.0125, 0.0373, -0.0681, 0.063 ],\rdtype=float32)\r 在中文中， 护士职业的主要从业者为女性，反应在词向量相似度上，如下\nprint(zh_w2v_model.similarity(\u0026#39;护士\u0026#39;, \u0026#39;女性\u0026#39;)) print(zh_w2v_model.similarity(\u0026#39;护士\u0026#39;, \u0026#39;男性\u0026#39;)) Run\n0.4417011\r0.378651\r 更多w2v_model用法可参考 豆瓣影评 | 探索词向量妙处\n文献 如果使用了facebook的预训练词向量，请引用以下两篇文献。\n Joulin, Armand, Piotr Bojanowski, Tomas Mikolov, Hervé Jégou, and Edouard Grave. \u0026ldquo;Loss in translation: Learning bilingual word mapping with a retrieval criterion.\u0026rdquo; arXiv preprint arXiv:1804.07745 (2018). Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. \u0026ldquo;Enriching word vectors with subword information.\u0026rdquo; Transactions of the association for computational linguistics 5 (2017): 135-146.  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-10-16-aligned-word-vectors/","summary":"介绍 Facebook研究者使用 fastText 算法，对维基百科(44种语言)语料数据进行了训练，最终生成了 44 种语言的对齐词向量。\n用途 wiki数据集有个优点，即由于众人分享、翻译，将不同语言的百科词条进行了翻译整理。所以facebook使用wiki训练对齐词向量有助于提升翻译准确性。与此同时，因为翻译者处于不同的语言和文化背景下，词条及词条内容必然蕴含着语言所特有的文化信息线索，有可能有助于我们挖掘跨语言的文化差异。例如中文词条护士和 英文词条nurse ，可以借助对齐词向量，比较护士这个群体在性别、种族等语义上的差异。\n之前分享过的内容\n 词嵌入测量不同群体对某概念的态度(偏见) 转载 | 大数据时代下社会科学研究方法的拓展\u0026mdash;\u0026mdash;基于词嵌入技术的文本分析的应用 转载 | 从符号到嵌入：计算社会科学的两种文本表示 文献汇总 | 词嵌入 与 社会科学中的偏见(态度)  不过fastText算法认为词语有不同的大小划分层次，从大到小分别是词语、词缀、字符等，使用 Joulin 等人 (2018) 中描述的 RCSLS 方法进行比对。\n   Code en-es es-en en-fr fr-en en-de de-en en-ru ru-en en-zh zh-en avg     Joulin et al. [1] 84.1 86.3 83.3 84.1 79.1 76.3 57.9 67.2 45.9 46.4 71.1   This implementation (10 epochs) 84.","title":"数据集 | 多语言对齐词向量预训练模型"},{"content":"\rPython的gensim库可以训练和使用word2vec模型，R语言中也有与之对应的word2vec包。word2vec是词嵌入技术中最常用的一种技术，如果对词嵌入不太了解，可以阅读前文\n\r转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用\r转载 | 从符号到嵌入：计算社会科学的两种文本表示\r\r本文需要的R包\ninstall.packages(c(\u0026quot;word2vec\u0026quot;, \u0026quot;jiebaR\u0026quot;, \u0026quot;tidyverse\u0026quot;, \u0026quot;readtext\u0026quot;))\r\nword2vec包常用函数\r\rword2vec 使用文本数据训练word2vec模型\ras.matrix 获取词向量\rdoc2vec 获取文档向量\rpredict 获取\rwrite.word2vec 保存word2vec模型至文件\rread.word2vec 读取word2vec模型文件\r\r\n\r准备数据\r原始数据是从网站下载的 三体.txt, 未分词处理，现在需要\n读中文取txt数据\r保留标点符号，进行分词处理\r分词结果重新整理为类似英文(空格间隔词语的形式)字符串\r结果存入新的txt\r\rlibrary(jiebaR)\rlibrary(tidyverse)\rlibrary(word2vec)\r#导入数据\rtri_body \u0026lt;- readtext::readtext(\u0026#39;data/三体.txt\u0026#39;)$text #分词（保留标点符号）\rtokenizer \u0026lt;- worker(symbol=T)\rtri_words \u0026lt;- segment(tri_body, tokenizer)\r# 整理为英文格式（词语之间加空格）\rsegmented_text \u0026lt;- stringr::str_c(tri_words, collapse = \u0026quot; \u0026quot;) %\u0026gt;% c()\r#写入txt\rreadr::write_file(segmented_text, file=\u0026#39;data/santi.txt\u0026#39;)\r\n\r训练word2vec模型\rword2vec(\rx,\rtype = c(\u0026quot;cbow\u0026quot;, \u0026quot;skip-gram\u0026quot;),\rdim = 50,\rwindow = ifelse(type == \u0026quot;cbow\u0026quot;, 5L, 10L),\riter = 5L,\rlr = 0.05,\rmin_count = 5L,\rsplit = c(\u0026quot; \\n,.-!?:;/\\\u0026quot;#$%\u0026amp;\u0026#39;()*+\u0026lt;=\u0026gt;@[]\\\\^_`{|}~\\t\\v\\f\\r\u0026quot;, \u0026quot;.\\n?!\u0026quot;),\rstopwords = character(),\rthreads = 1L,\r...\r)\r\rx 英文文本数据txt文件(中文数据txt文件是分词后的txt文件，空格间隔词语)\rtype 训练方式，默认CBOW\rdim 词向量维度，默认50维\rwindow 词向量窗口，默认5\riter 训练迭代次数，默认5\rsplit 分词、分句对应的分隔符。\rlr 学习率，默认0.05\rmin_count 词语在语料中至少要出现5次(低于5次的词语，训练好的结果中没有该词语）\rstopwords 停用词表，默认空字符集\rthreads 并行加速，cpu核数，默认1。为了加速训练过程，可以使用 parallel::detectCores() 获得本电脑的核数\r\r#训练10维的词向量模型\rmodel \u0026lt;- word2vec(x = \u0026#39;data/santi.txt\u0026#39;, dim = 10, iter = 20, split = c(\u0026quot; \u0026quot;, \u0026quot;。？！；\u0026quot;),\rthreads = parallel::detectCores()) #并行，使用cpu多核加速\remb \u0026lt;- as.matrix(model)\r#显示6个词\rhead(emb)\r## [,1] [,2] [,3] [,4] [,5] [,6]\r## 煮 -1.02566934 -0.9271542 -0.42417252 -0.54280633 1.8847700 0.41640753\r## 报 -0.83992052 1.9440031 0.09093992 0.83522910 1.7909089 0.72149992\r## 悬空 -0.06369513 -1.3519955 -2.13137460 -0.06198586 0.6096401 1.32933748\r## 略 1.74687469 -0.4278547 -0.33822438 1.08505321 2.0168977 -0.07693915\r## 伏 -0.68947995 -1.4147453 -1.95522511 -0.39963767 0.5269030 0.30352208\r## 石柱 -0.40561640 -1.3643234 0.30329546 -0.94012892 2.1579018 0.79654717\r## [,7] [,8] [,9] [,10]\r## 煮 -1.1708908 -0.7624418 -0.6275516 1.2417521\r## 报 0.5235919 0.8448864 -0.2960095 -0.0773837\r## 悬空 0.1527163 -0.1337370 -0.1646384 1.1892601\r## 略 -0.3246748 -0.9813624 0.5045205 0.2771466\r## 伏 0.3166684 -1.4238008 -1.0167172 -0.0976937\r## 石柱 0.2237919 0.6933151 0.7412233 -0.7918702\r\n\r查看某词的vector\r查看词语 汪淼 的vector\nemb[\u0026quot;汪淼\u0026quot;,]\r## [1] -0.77559733 -0.90021265 0.66555792 -0.10277803 1.89924443 -0.88817298\r## [7] -1.32665634 -0.75938725 -0.09628224 1.18008399\r查看词语 地球 的vector\nemb[\u0026quot;地球\u0026quot;,]\r## [1] 0.29645494 -0.61688840 0.91209215 -0.64530188 0.62816381 -0.72807491\r## [7] 0.50655973 2.38137436 1.19238114 -0.09610342\r\n\rpredict()\r找到语料中，词语 罗辑 最相似的 20个词\npredict(model, \u0026#39;罗辑\u0026#39;, type=\u0026#39;nearest\u0026#39;, top_n = 20)\r## $罗辑\r## term1 term2 similarity rank\r## 1 罗辑 胡文 0.9744400 1\r## 2 罗辑 申玉菲 0.9678891 2\r## 3 罗辑 瓦季姆 0.9550550 3\r## 4 罗辑 狄奥伦娜 0.9518393 4\r## 5 罗辑 蓝西 0.9472395 5\r## 6 罗辑 护士 0.9471439 6\r## 7 罗辑 法扎兰 0.9458703 7\r## 8 罗辑 白艾思 0.9451101 8\r## 9 罗辑 坎特 0.9396626 9\r## 10 罗辑 白蓉 0.9387447 10\r## 11 罗辑 参谋长 0.9377206 11\r## 12 罗辑 弗雷斯 0.9369408 12\r## 13 罗辑 第一眼 0.9357565 13\r## 14 罗辑 父亲 0.9350463 14\r## 15 罗辑 多少次 0.9314436 15\r## 16 罗辑 门去 0.9291503 16\r## 17 罗辑 维德 0.9267251 17\r## 18 罗辑 褐蚁 0.9203902 18\r## 19 罗辑 刚 0.9200501 19\r## 20 罗辑 吴岳 0.9191605 20\r查看均值向量（多个词向量中心的）的10个近义词\nvectors \u0026lt;- emb[c(\u0026quot;汪淼\u0026quot;, \u0026quot;罗辑\u0026quot;, \u0026quot;叶文洁\u0026quot;), ]\rcentroid_vector \u0026lt;- colMeans(vectors)\rpredict(model, centroid_vector, type = \u0026quot;nearest\u0026quot;, top_n = 10)\r## term similarity rank\r## 1 罗辑 0.9185568 1\r## 2 狄奥伦娜 0.9104245 2\r## 3 文洁 0.9088279 3\r## 4 汪淼 0.9054156 4\r## 5 白艾思 0.9046930 5\r## 6 张翔 0.9026827 6\r## 7 尴尬 0.8952187 7\r## 8 庄颜 0.8952166 8\r## 9 皇帝 0.8949283 9\r## 10 父亲 0.8915347 10\r\n\rdoc2vec()\r\rdoc2vec(object, newdata, split = ” “)\r\robject word2vec模型对象\rnewdata 文档列表(用空格间隔的字符串列表)\rsplit 默认分隔符是空格\r\r\r将文档转为向量\ndocs \u0026lt;- c(\u0026quot;哦 ， 对不起 ， 汪 教授 。 这是 我们 史强 队长 。\u0026quot;, \u0026quot; 丁仪 博士 ， 您 能否 把 杨冬 的 遗书 给 汪 教授 看 一下 ？ \u0026quot;)\rdoc2vec(object=model, newdata = docs, split=\u0026#39; \u0026#39;)\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7]\r## [1,] -1.1769752 -0.1065619 0.1983950 1.734068 0.5478012 -0.8320528 -0.2387014\r## [2,] -0.4827189 0.0664595 -0.2119484 1.895074 0.6729840 -0.3008853 -0.6857539\r## [,8] [,9] [,10]\r## [1,] -0.5519856 -2.007002 0.4182127\r## [2,] -0.5976922 -2.130454 -0.4653725\r\n\r保存word2vec模型\r保存模型，一般有两个目的\n\r为了分享word2vec模型\r避免反复训练模型，节约数据分析时间\r\rword2vec::write.word2vec(x = model, #新建output文件夹，将模型存入output文件夹内\rfile = \u0026quot;output/santi_word2vec.bin\u0026quot;)\r## [1] TRUE\r\n\r导入预训练模型\r导入 output/santi_word2vec.bin 的预训练word2vec模型\npre_trained_model \u0026lt;- word2vec::read.word2vec(file = \u0026quot;output/santi_word2vec.bin\u0026quot;)\rpre_trained_emb \u0026lt;- as.matrix(pre_trained_model)\rhead(pre_trained_emb)\r## [,1] [,2] [,3] [,4] [,5] [,6]\r## 回荡 -1.9563367 -0.3099073 -1.2969902 -0.5719763 1.1507142 -0.05515177\r## 听证会 0.2756990 1.3702289 -1.3303705 -0.1827691 0.6622804 -1.92008448\r## 纲领 0.4495552 1.9311246 -0.5812275 -0.1470096 -0.2678985 -0.01694358\r## 很亮 0.3621844 -1.0048453 0.7036168 -2.0917876 0.6459805 1.18436253\r## 秒 1.9033701 1.6510324 -0.2616904 0.3671210 1.0618066 0.06588747\r## 杰森 -1.2904713 -1.2501229 0.3380587 0.8590797 1.6798494 -0.58775252\r## [,7] [,8] [,9] [,10]\r## 回荡 1.1082711 -0.2064489 -0.9264346 -0.7816723\r## 听证会 -1.0952694 0.6120903 -0.1326561 0.7252344\r## 纲领 -0.6097277 2.1051276 -0.2405726 -0.8808851\r## 很亮 0.1964065 -1.3926132 -0.4042619 -0.1645472\r## 秒 -0.8347995 0.2591044 0.3594093 1.1929117\r## 杰森 0.4941484 -1.1393189 -0.4687541 0.9951217\r\r","permalink":"/blog/2022-10-12-r-word2vec/","summary":"Python的gensim库可以训练和使用word2vec模型，R语言中也有与之对应的word2vec包。word2vec是词嵌入技术中最常用的一种技术，如果对词嵌入不太了解，可以阅读前文\n\r转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用\r转载 | 从符号到嵌入：计算社会科学的两种文本表示\r\r本文需要的R包\ninstall.packages(c(\u0026quot;word2vec\u0026quot;, \u0026quot;jiebaR\u0026quot;, \u0026quot;tidyverse\u0026quot;, \u0026quot;readtext\u0026quot;))\r\nword2vec包常用函数\r\rword2vec 使用文本数据训练word2vec模型\ras.matrix 获取词向量\rdoc2vec 获取文档向量\rpredict 获取\rwrite.word2vec 保存word2vec模型至文件\rread.word2vec 读取word2vec模型文件\r\r\n\r准备数据\r原始数据是从网站下载的 三体.txt, 未分词处理，现在需要\n读中文取txt数据\r保留标点符号，进行分词处理\r分词结果重新整理为类似英文(空格间隔词语的形式)字符串\r结果存入新的txt\r\rlibrary(jiebaR)\rlibrary(tidyverse)\rlibrary(word2vec)\r#导入数据\rtri_body \u0026lt;- readtext::readtext(\u0026#39;data/三体.txt\u0026#39;)$text #分词（保留标点符号）\rtokenizer \u0026lt;- worker(symbol=T)\rtri_words \u0026lt;- segment(tri_body, tokenizer)\r# 整理为英文格式（词语之间加空格）\rsegmented_text \u0026lt;- stringr::str_c(tri_words, collapse = \u0026quot; \u0026quot;) %\u0026gt;% c()\r#写入txt\rreadr::write_file(segmented_text, file=\u0026#39;data/santi.","title":"R语言 | 使用word2vec词向量模型"},{"content":" 转载自\n 作者: 潮享教育李老师 出处: https://blog.csdn.net/andyleo0111/article/details/118738910   一、问题诊断 安装python包出现报错: Microsoft Visual 14.0 or greater is required. 怎么办？\n使用Python下载第三方库，pip也更新了，镜像也使用了，网络也没有问题，但是还是下载报错了，出现一大片红色报错信息怎么办？？？\n要解决这个问题，总共分为三个步骤：\n第一步：查看报错信息\n![](img/01-microsoft visual c14.jpeg)\n第二步：根据报错信息，发现报错原因是因为缺少Microsoft Visual C++ 14.0\n报错提示：Microsoft Visual 14.0 or greater is required.\n第三步：下载、安装Microsoft Visual C++ 14.0即可完美解决问题！\n(如下链接下载即可，包含了默认文件，不用担心文件程序损坏和包丢失的问题，全自动安装成功)\n二、解决问题 2.1 下载软件 链接: https://pan.baidu.com/s/1c8tUHKmVYYTsueV-MVGNTQ 提取码: 5jic\n2.2 双击安装 下载之后解压文件，双击VisualCppBuildTools_Full.exe\n2.3 初始化文件\u0026hellip; 2.4 安装文件 2.5 安装文件中\u0026hellip; 2.6 问题解决，收工！！！ 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-10-09-visual_c++14_is_required/","summary":" 转载自\n 作者: 潮享教育李老师 出处: https://blog.csdn.net/andyleo0111/article/details/118738910   一、问题诊断 安装python包出现报错: Microsoft Visual 14.0 or greater is required. 怎么办？\n使用Python下载第三方库，pip也更新了，镜像也使用了，网络也没有问题，但是还是下载报错了，出现一大片红色报错信息怎么办？？？\n要解决这个问题，总共分为三个步骤：\n第一步：查看报错信息\n![](img/01-microsoft visual c14.jpeg)\n第二步：根据报错信息，发现报错原因是因为缺少Microsoft Visual C++ 14.0\n报错提示：Microsoft Visual 14.0 or greater is required.\n第三步：下载、安装Microsoft Visual C++ 14.0即可完美解决问题！\n(如下链接下载即可，包含了默认文件，不用担心文件程序损坏和包丢失的问题，全自动安装成功)\n二、解决问题 2.1 下载软件 链接: https://pan.baidu.com/s/1c8tUHKmVYYTsueV-MVGNTQ 提取码: 5jic\n2.2 双击安装 下载之后解压文件，双击VisualCppBuildTools_Full.exe\n2.3 初始化文件\u0026hellip; 2.4 安装文件 2.5 安装文件中\u0026hellip; 2.6 问题解决，收工！！！ 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"安装python包出现报错：Microsoft Visual 14.0 or greater is required. 怎么办？"},{"content":"Maigret 能检查各网站(应用) 某 用户名 是否注册，并从网页收集所有可用信息, 运行过程不需要 API 密钥。目前支持超过 2500 个站点检索（完整列表），默认针对 500 个热门站点按受欢迎程度降序启动搜索。\n主要功能  个人资料页面解析 个人信息提取 其他个人资料链接等。 通过新用户名和找到的其他 id 进行递归搜索 按标签搜索（网站类别、国家/地区）  安装 命令行中安装maigret包\npip3 install maigret \n使用 我自己有个账号名是hidadeng，就用hidadeng试试。\n为了解用户名hidadeng使用情况，报告结果存储于html和pdf。 在命令行中执行，\nmaigret hidadeng --html --pdf 命令行运行过程\n报告 maigret查询用户名hidadeng的使用情况、兴趣等结果可以绘制成报告。\n点击查看hidadeng报告\n效果挺准的，对hidadeng这个用户兴趣(coding、shopping)拿捏的也挺不错。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-10-08-find-sns-account-information-with-maigret/","summary":"Maigret 能检查各网站(应用) 某 用户名 是否注册，并从网页收集所有可用信息, 运行过程不需要 API 密钥。目前支持超过 2500 个站点检索（完整列表），默认针对 500 个热门站点按受欢迎程度降序启动搜索。\n主要功能  个人资料页面解析 个人信息提取 其他个人资料链接等。 通过新用户名和找到的其他 id 进行递归搜索 按标签搜索（网站类别、国家/地区）  安装 命令行中安装maigret包\npip3 install maigret \n使用 我自己有个账号名是hidadeng，就用hidadeng试试。\n为了解用户名hidadeng使用情况，报告结果存储于html和pdf。 在命令行中执行，\nmaigret hidadeng --html --pdf 命令行运行过程\n报告 maigret查询用户名hidadeng的使用情况、兴趣等结果可以绘制成报告。\n点击查看hidadeng报告\n效果挺准的，对hidadeng这个用户兴趣(coding、shopping)拿捏的也挺不错。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"Maigret库 | 查询某用户名在各平台网站的使用情况"},{"content":"文本分析(社科领域中与Python相关的)，对应于计算机领域是自然语言处理，两者范畴高度重叠，关系密切。但相关术语脉络比较庞杂，nlp-roadmap项目为我们梳理了相关概念，更有助于快速掌握文本分析。\n相关知识 NLP最相关的几个知识大类有统计学、机器学习、文本挖掘、自然语言处理\n统计学 机器学习 文本挖掘 自然语言处理NLP 参考 [1] ratsgo\u0026rsquo;s blog for textmining, ratsgo/ratsgo.github.io\n[2] (한국어) 텍스트 마이닝을 위한 공부거리들, lovit/textmining-tutorial\n[3] Christopher Bishop(2006). Pattern Recognition and Machine Learning\n[4] Young, T., Hazarika, D., Poria, S., \u0026amp; Cambria, E. (2017). Recent Trends in Deep Learning Based Natural Language Processing. arXiv preprint arXiv:1708.02709.\n[5] curated collection of papers for the nlp practitioner, mihail911/nlp-library\nAcknowledgement to ratsgo, lovit for creating great posts and lectures.\n作者  Tae Hwan Jung @graykode, Kyung Hee Univ CE(Undergraduate). Author Email : nlkey2022@gmail.com  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-10-08-nlp-roadmap/","summary":"文本分析(社科领域中与Python相关的)，对应于计算机领域是自然语言处理，两者范畴高度重叠，关系密切。但相关术语脉络比较庞杂，nlp-roadmap项目为我们梳理了相关概念，更有助于快速掌握文本分析。\n相关知识 NLP最相关的几个知识大类有统计学、机器学习、文本挖掘、自然语言处理\n统计学 机器学习 文本挖掘 自然语言处理NLP 参考 [1] ratsgo\u0026rsquo;s blog for textmining, ratsgo/ratsgo.github.io\n[2] (한국어) 텍스트 마이닝을 위한 공부거리들, lovit/textmining-tutorial\n[3] Christopher Bishop(2006). Pattern Recognition and Machine Learning\n[4] Young, T., Hazarika, D., Poria, S., \u0026amp; Cambria, E. (2017). Recent Trends in Deep Learning Based Natural Language Processing. arXiv preprint arXiv:1708.02709.\n[5] curated collection of papers for the nlp practitioner, mihail911/nlp-library\nAcknowledgement to ratsgo, lovit for creating great posts and lectures.","title":"nlp-roadmap | 文本分析知识点思维脑图"},{"content":" 作者: 猫哥\n波恩大学经济学专业， 计量经济学方向\n 在之前的推文中，我们介绍了如何在R语言中，用Shiny包编写应用程序App。在这篇推文中，我们将要介绍Shiny包中一类独特且重要的表达式：reactive expressions。\n在设计App时，我们希望App能够有较快的响应速度。但是，如果App的server函数中包含很多耗时较长的运算，那么应该怎么办呢？此时，就需要用到Reactive表达式。它能够控制程序中哪些部分需要更新运算，而哪些部分不需要更新，从而节省运算时间。\n案例：stockVis 此处，我们将通过一个案例来讲解reactive表达式。下图是stockVis的App界面，这个App能够帮助用户描绘相应股票的价格变动。感兴趣的读者可以根据此处的提示下载该App。\n根据上图可以看出，用户需要首先选择股票（Symbol），然后选择观察时期(Data range)，再选择绘制原股票价格还是对数化后的股票价格，最后再选择是否股价中的通胀进行矫正。\n在对这些选项进行了选择之后，stockVis首先用getSymbols函数从诸如Yahoo finance和Federal Reserve Bank of St. Louis这样的网站中下载金融数据到R，然后再用chartSeries将股价描绘出来。\nserver函数中，用来生成图形的程序如下。在分析了该程序之后，我们会发现一个问题。比如，当我们重新选中“Plot y axis on the log scale”，那么input$log的值就会变化，那么就会导致整个renderPlot重新进行运算。\noutput$plot \u0026lt;- renderPlot({ data \u0026lt;- getSymbols(input$symb, src = \u0026#34;yahoo\u0026#34;, from = input$dates[1], to = input$dates[2], auto.assign = FALSE) chartSeries(data, theme = chartTheme(\u0026#34;white\u0026#34;), type = \u0026#34;line\u0026#34;, log.scale = input$log, TA = NULL) }) 而renderPlot每次重新运算时，首先会重新用getSymbols抓去数据，然后用chartSeries重新画图。然而，用getSymbols从Yahoo等网站抓取数据所花费的时间并不是可以忽略不计的。另外，如果抓取的过于频繁，我们的IP地址会被屏蔽，这是网站将我们错判为机器人，也是网站应对爬虫的常用做法。最关键的是，当我们只是重新选择“Plot y axis on the log scale”，我们并不希望重新抓取数据，而是希望在原有数据的基础上对数化即可。\nReactive表达式 在遇到上述问题时，我们就需要用到reactive表达式。reactive表达式以ui函数里各种input变量作为输入。如下，reactive表达式的输入则是input$symb和input$dates。当input$symb和input$dates的值变化时，此处reactive表达式的输出结果才会发生变化，而不受input$log的影响。\ndataInput \u0026lt;- reactive({ getSymbols(input$symb, src = \u0026#34;yahoo\u0026#34;, from = input$dates[1], to = input$dates[2], auto.assign = FALSE) }) 所以，我们现在用上面的reactive表达式更新原先的程序，得到如下程序。dataInput()命令则是运行如上的reactive表达式。此时，如果我们只重新选择\u0026quot;Plot y axis on the log scale\u0026quot;，那么reactive表达式里的数据抓取过程并不会更新，而只有renderPlot里的log.scale参数会更新，这将节省程序运行时间，而且减少数据抓取次数，防止IP地址被封。\noutput$plot \u0026lt;- renderPlot({ chartSeries(dataInput(), theme = chartTheme(\u0026#34;white\u0026#34;), type = \u0026#34;line\u0026#34;, log.scale = input$log, TA = NULL) }) reactive表达式不止reactive()，还包括observe(), observeEvent()和eventReactive()等等，详见此处。另外，reactive表达式只能包装在特定的、允许reactive表达式的函数中，如此处的renderPlot，而不能被包装在plot函数中。render*类函数都允许reactive表达式。\n总结 Reactive表达式的功能可被简单地总结为以下步骤：\n 当你第一次运行程序时，reactive表达式会缓存运行结果。 当你下一次运行程序时，reactive表达式会自动检测输入值是否是最新的。在上例中，也就是说输入的input$symb和input$dates是否有变化。 如果有变化，则reactive会根据新输入值更新结果。 如果没变化，则reactive会直接使用缓存中的值。  参考文献 https://shiny.rstudio.com/tutorial/written-tutorial/lesson6/\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-10-07-r-shiny-reactive/","summary":"作者: 猫哥\n波恩大学经济学专业， 计量经济学方向\n 在之前的推文中，我们介绍了如何在R语言中，用Shiny包编写应用程序App。在这篇推文中，我们将要介绍Shiny包中一类独特且重要的表达式：reactive expressions。\n在设计App时，我们希望App能够有较快的响应速度。但是，如果App的server函数中包含很多耗时较长的运算，那么应该怎么办呢？此时，就需要用到Reactive表达式。它能够控制程序中哪些部分需要更新运算，而哪些部分不需要更新，从而节省运算时间。\n案例：stockVis 此处，我们将通过一个案例来讲解reactive表达式。下图是stockVis的App界面，这个App能够帮助用户描绘相应股票的价格变动。感兴趣的读者可以根据此处的提示下载该App。\n根据上图可以看出，用户需要首先选择股票（Symbol），然后选择观察时期(Data range)，再选择绘制原股票价格还是对数化后的股票价格，最后再选择是否股价中的通胀进行矫正。\n在对这些选项进行了选择之后，stockVis首先用getSymbols函数从诸如Yahoo finance和Federal Reserve Bank of St. Louis这样的网站中下载金融数据到R，然后再用chartSeries将股价描绘出来。\nserver函数中，用来生成图形的程序如下。在分析了该程序之后，我们会发现一个问题。比如，当我们重新选中“Plot y axis on the log scale”，那么input$log的值就会变化，那么就会导致整个renderPlot重新进行运算。\noutput$plot \u0026lt;- renderPlot({ data \u0026lt;- getSymbols(input$symb, src = \u0026#34;yahoo\u0026#34;, from = input$dates[1], to = input$dates[2], auto.assign = FALSE) chartSeries(data, theme = chartTheme(\u0026#34;white\u0026#34;), type = \u0026#34;line\u0026#34;, log.scale = input$log, TA = NULL) }) 而renderPlot每次重新运算时，首先会重新用getSymbols抓去数据，然后用chartSeries重新画图。然而，用getSymbols从Yahoo等网站抓取数据所花费的时间并不是可以忽略不计的。另外，如果抓取的过于频繁，我们的IP地址会被屏蔽，这是网站将我们错判为机器人，也是网站应对爬虫的常用做法。最关键的是，当我们只是重新选择“Plot y axis on the log scale”，我们并不希望重新抓取数据，而是希望在原有数据的基础上对数化即可。\nReactive表达式 在遇到上述问题时，我们就需要用到reactive表达式。reactive表达式以ui函数里各种input变量作为输入。如下，reactive表达式的输入则是input$symb和input$dates。当input$symb和input$dates的值变化时，此处reactive表达式的输出结果才会发生变化，而不受input$log的影响。\ndataInput \u0026lt;- reactive({ getSymbols(input$symb, src = \u0026#34;yahoo\u0026#34;, from = input$dates[1], to = input$dates[2], auto.","title":"R语言 | 使用shiny的reactive表达式写应用程序"},{"content":" 作者: 猫哥\n波恩大学经济学专业， 计量经济学方向\n 在实证分析过程中，各种回归模型十分常用，特别是参数模型。而且我们常常需要运行多种参数回归模型，并对参数估计值、显著性等统计性质进行比较。此时，如果用copy paste或者手动输入，那么则耗时耗力，且有一定错误率。如果你也有相同烦恼，那么stargazer包可以完美地解决你的烦恼。他可以快速、准确、清晰地输出回归结果。\n安装 与其它R包的安装方式无异，我们可以通过install.packages()和library()函数来下载和导入stargazer包。\ninstall.packages(\u0026#34;stargazer\u0026#34;) library(stargazer) \n描述性统计值 在估计回归模型之前，我们常常也需要对数据的描述性统计性质进行分析，以便了解变量的分布和变量间的联合分布，这对后续的回归模型设定具有重要意义。stargazer包能使我们方便地分析描述性统计值。我们此处以R内自带数据集attitude为例。\nstargazer(attitude, type = \u0026#34;text\u0026#34;) 运行上述程序后，我们就会得到下表。\n如果需要输出latex或者html格式，那么指定type=\u0026quot;latex\u0026quot;或者type=\u0026quot;html\u0026quot;即可。当然，我们还有其他一些重要的参数。\n title: 定义表格的标题 out: 定义导出文件的路径，文件类型可以是.tex, .txt, .htm或者.html。 flip: 默认为FALSE。当定义为TRUE时，则表示转置，即各变量按列排列。 digits: 定义输出数值的小数位数。  参数回归模型结果 此处，我们依然使用attitude数据包，以rating作为因变量，其他各变量作为协变量，用两个线性模型和一个Probit模型进行参数估计。\n## 2 OLS models linear.1 \u0026lt;- lm(rating ~ complaints + privileges + learning + raises + critical, data=attitude) linear.2 \u0026lt;- lm(rating ~ complaints + privileges + learning, data=attitude) ## create an indicator dependent variable, and run a probit model attitude$high.rating \u0026lt;- (attitude$rating \u0026gt; 70) probit.model \u0026lt;- glm(high.rating ~ learning + critical + advance, data=attitude, family = binomial(link = \u0026#34;probit\u0026#34;)) 在得到各模型后，我们可以运行以下程序汇总和输出三个模型的结果。\nstargazer(linear.1, linear.2, probit.model, type=\u0026#34;text\u0026#34;, title=\u0026#34;Regression Results\u0026#34;) 汇总和输出结果如下表。\n在输出回归结果时，我们也可以定义一些重要的参数。\n single.row: 默认值为FALSE。当取值为TRUE，表示将参数和t统计值或其它参数统计值放在一行，而不是分上下放置。 ci: 默认为FALSE。当取值为TRUE时，表示将在参数估计值后报告置信区间，而不是t统计值。 ci.level: 定义置信度水平。 omit.stat: 定义不需要报告的统计值。例如，omit.stat=c(\u0026quot;f\u0026quot;.\u0026quot;ser\u0026quot;)，表示不需要报告模型的F统计值和Residual Std. Error，即残差的标准误。 keep.stat: 定义需要报告的统计值。默认是报告所有的协变量。例如，keep.stat=\u0026quot;n\u0026quot;，表示只需要报告样本量。 keep: 定义需要包含在输出结果中的变量。当模型协变量数目很多，而我们又只关注个别变量的系数时，这个参数则能够起到作用。例如，keep=c(\u0026quot;complaints\u0026quot;,\u0026quot;learning\u0026quot;,\u0026quot;raises\u0026quot;,\u0026quot;critical\u0026quot;)。 order: 定义输出结果中变量的顺序。例如，order=c(\u0026quot;learning\u0026quot;, \u0026quot;raises\u0026quot;)。  相关系数表 stargazer包还能汇总整理相关系数表。以attitude数据集为例，运行以下程序即可获得相关系数表。\ncorrelation.matrix \u0026lt;- cor(attitude) stargazer(correlation.matrix, type=\u0026#34;text\u0026#34;) 参考文献 Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2. https://CRAN.R-project.org/package=stargazer\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-10-07-r-stargazer/","summary":"作者: 猫哥\n波恩大学经济学专业， 计量经济学方向\n 在实证分析过程中，各种回归模型十分常用，特别是参数模型。而且我们常常需要运行多种参数回归模型，并对参数估计值、显著性等统计性质进行比较。此时，如果用copy paste或者手动输入，那么则耗时耗力，且有一定错误率。如果你也有相同烦恼，那么stargazer包可以完美地解决你的烦恼。他可以快速、准确、清晰地输出回归结果。\n安装 与其它R包的安装方式无异，我们可以通过install.packages()和library()函数来下载和导入stargazer包。\ninstall.packages(\u0026#34;stargazer\u0026#34;) library(stargazer) \n描述性统计值 在估计回归模型之前，我们常常也需要对数据的描述性统计性质进行分析，以便了解变量的分布和变量间的联合分布，这对后续的回归模型设定具有重要意义。stargazer包能使我们方便地分析描述性统计值。我们此处以R内自带数据集attitude为例。\nstargazer(attitude, type = \u0026#34;text\u0026#34;) 运行上述程序后，我们就会得到下表。\n如果需要输出latex或者html格式，那么指定type=\u0026quot;latex\u0026quot;或者type=\u0026quot;html\u0026quot;即可。当然，我们还有其他一些重要的参数。\n title: 定义表格的标题 out: 定义导出文件的路径，文件类型可以是.tex, .txt, .htm或者.html。 flip: 默认为FALSE。当定义为TRUE时，则表示转置，即各变量按列排列。 digits: 定义输出数值的小数位数。  参数回归模型结果 此处，我们依然使用attitude数据包，以rating作为因变量，其他各变量作为协变量，用两个线性模型和一个Probit模型进行参数估计。\n## 2 OLS models linear.1 \u0026lt;- lm(rating ~ complaints + privileges + learning + raises + critical, data=attitude) linear.2 \u0026lt;- lm(rating ~ complaints + privileges + learning, data=attitude) ## create an indicator dependent variable, and run a probit model attitude$high.","title":"R语言 | 使用stargazer包输出格式化回归结果"},{"content":"\rGoogle Books Ngram Viewer 可以显示输入短语(你感兴趣的词组)在谷歌书籍中（例如，“British English”、“English Fiction”、“French”）中出现的频率变化趋势 。\n网址 https://books.google.com/ngrams/\n","permalink":"/blog/2022-09-27-r-ngramr/","summary":"Google Books Ngram Viewer 可以显示输入短语(你感兴趣的词组)在谷歌书籍中（例如，“British English”、“English Fiction”、“French”）中出现的频率变化趋势 。\n网址 https://books.google.com/ngrams/","title":"Google Books Ngram Viewer显示英文词汇历史使用趋势"},{"content":"\r\n\r\n\r\n","permalink":"/blog/2022-09-27-r-renderthis/","summary":"","title":"R语言 | renderthis包制作讲解动画、视频"},{"content":"easystats 是 R 包集合，可提供统一且easy的代码进行 统计分析。\n与 tidyverse 有何不同？ tidyverse包集合，是 R语言 中非常流行的软件包集合（ggplot、dplyr、tidyr\u0026hellip;\u0026hellip;），它也使 R 的使用更加容易。那么，您应该选择 tidyverse 还是 easystats？\n两个都选！\n事实上，这两个生态系统的设计考虑了非常不同的目标。 tidyverse 包主要用于 数据操作和探索。而 easystats 更关注分析的最后阶段：理解和解释您的结果，并在手稿或报告中报告它们。您绝对可以在 tidyverse 工作流程中使用 easystats 功能！\neasystats + tidyverse = ❤️\neasystats生态圈 每个 easystats 包都有不同的范围和用途。这意味着您最好的开始方式是探索并选择您认为可能对您有用的那些。但是，由于它们的构建考虑了\u0026quot;更大的图景\u0026quot;，您会意识到使用更多它们会创建一个流畅的工作流程，因为这些包旨在协同工作。理想情况下，这些软件包可以协同工作，涵盖统计分析和数据可视化的所有方面。\n report：📜 🎉 生成自动统计报告 correlation：🔗 相关性的多合一包 modelbased：📈 基于统计模型估计效果、组平均值和组间对比 bayestestR：👻 非常适合贝叶斯统计的初学者或专家 effectize: 🐉 计算、转换、解释和使用效果大小和标准化参数的索引 see ：🎨 创建漂亮结果可视化的绘图 parameters：📊 获取有关模型包含参数所有信息的表格 performance：💪模型的质量和性能指标（R2、ICC、LOO、AIC、BF、\u0026hellip;） insight：🔮 对于开发人员来说，一个包可以帮助您使用不同的模型和包 datawizard：🧙 清理和转换数据的魔法药水  截止2022-09-21， easystats生态中各个包的下载情况\n   Total insight bayestestR parameters performance datawizard effectsize correlation see modelbased report easystats     10,001,095 3,085,739 1,373,379 1,363,678 1,285,506 1,159,097 1,060,079 277,541 251,038 94,089 48,876 2,073    安装 install.packages(\u0026#34;easystats\u0026#34;) \n炫一下 easystats到底如何好用，今天只实验report包，该包号称 “从 R 到你的手稿”\n报告的主要目标是弥合 R 的输出与手稿中包含的格式化结果之间的差距。 它根据最佳实践指南（例如 APA 期刊风格）自动生成模型和数据框架的报告，确保结果报告的标准化和质量。\n生成iris数据集描述性统计信息的英文报告\nreport(iris) \n生成线性回归模型的报告\nlibrary(report) model \u0026lt;- lm(Sepal.Length ~ Species, data = iris) report(model) 更多内容请阅读 report包 的文档。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-09-21-r-easystats/","summary":"easystats 是 R 包集合，可提供统一且easy的代码进行 统计分析。\n与 tidyverse 有何不同？ tidyverse包集合，是 R语言 中非常流行的软件包集合（ggplot、dplyr、tidyr\u0026hellip;\u0026hellip;），它也使 R 的使用更加容易。那么，您应该选择 tidyverse 还是 easystats？\n两个都选！\n事实上，这两个生态系统的设计考虑了非常不同的目标。 tidyverse 包主要用于 数据操作和探索。而 easystats 更关注分析的最后阶段：理解和解释您的结果，并在手稿或报告中报告它们。您绝对可以在 tidyverse 工作流程中使用 easystats 功能！\neasystats + tidyverse = ❤️\neasystats生态圈 每个 easystats 包都有不同的范围和用途。这意味着您最好的开始方式是探索并选择您认为可能对您有用的那些。但是，由于它们的构建考虑了\u0026quot;更大的图景\u0026quot;，您会意识到使用更多它们会创建一个流畅的工作流程，因为这些包旨在协同工作。理想情况下，这些软件包可以协同工作，涵盖统计分析和数据可视化的所有方面。\n report：📜 🎉 生成自动统计报告 correlation：🔗 相关性的多合一包 modelbased：📈 基于统计模型估计效果、组平均值和组间对比 bayestestR：👻 非常适合贝叶斯统计的初学者或专家 effectize: 🐉 计算、转换、解释和使用效果大小和标准化参数的索引 see ：🎨 创建漂亮结果可视化的绘图 parameters：📊 获取有关模型包含参数所有信息的表格 performance：💪模型的质量和性能指标（R2、ICC、LOO、AIC、BF、\u0026hellip;） insight：🔮 对于开发人员来说，一个包可以帮助您使用不同的模型和包 datawizard：🧙 清理和转换数据的魔法药水  截止2022-09-21， easystats生态中各个包的下载情况\n   Total insight bayestestR parameters performance datawizard effectsize correlation see modelbased report easystats     10,001,095 3,085,739 1,373,379 1,363,678 1,285,506 1,159,097 1,060,079 277,541 251,038 94,089 48,876 2,073    安装 install.","title":"R语言 | 让统计更easy的easystats集合包"},{"content":" 今天分享一个R语言ggsci包，帮你绘制出美观舒服，又合期刊编辑审美风格的图表。\n\n安装 install.packages(\u0026quot;ggsci\u0026quot;) #remotes::install_github(\u0026quot;nanxstats/ggsci\u0026quot;) 使用 vignette(\"ggsci\")可以查看ggsci的vignette。\n\n 期刊风格 ggsci支持的主题风格主要参考自期刊、可视化库\n    风格 期刊、技术框架    NPG Nature旗下期刊(Nature Publishing Group)  AAAS Science旗下期刊()  NEJM 新英格兰医学杂志(New England Journal of Medicine)  Lancet 柳叶刀杂志(Lancet Oncology)  JAMA 美国医学学会杂志(Journal of the American Medical Association)  JCO 临床肿瘤学杂志(Journal of Clinical Oncology)  UCSCGB UCSC基因组浏览器(UCSC Genome Browser)  D3 d3.js  … …    \n 基本图表 这里准备一个常见的ggplot2图表\nlibrary(\u0026quot;ggsci\u0026quot;) library(\u0026quot;ggplot2\u0026quot;) library(\u0026quot;gridExtra\u0026quot;) data(\u0026quot;diamonds\u0026quot;) p1 = ggplot(subset(diamonds, carat \u0026gt;= 2.2), aes(x = table, y = price, colour = cut)) + geom_point(alpha = 0.7) + geom_smooth(method = \u0026quot;loess\u0026quot;, alpha = 0.05, size = 1, span = 1) + theme_bw() p2 = ggplot(subset(diamonds, carat \u0026gt; 2.2 \u0026amp; depth \u0026gt; 55 \u0026amp; depth \u0026lt; 70), aes(x = depth, fill = cut)) + geom_histogram(colour = \u0026quot;black\u0026quot;, binwidth = 1, position = \u0026quot;dodge\u0026quot;) + theme_bw() grid.arrange(p1, p2, ncol=2) \n Gallery NPG ature旗下期刊(Nature Publishing Group)配色风格\np1_npg = p1 + scale_color_npg() p2_npg = p2 + scale_fill_npg() grid.arrange(p1_npg, p2_npg, ncol = 2) \n AAAS Science旗下期刊(American Association for the Advancement of Science)配色风格\np1_aaas = p1 + scale_color_aaas() p2_aaas = p2 + scale_fill_aaas() grid.arrange(p1_aaas, p2_aaas, ncol = 2) \n NEJM 新英格兰医学杂志(New England Journal of Medicine)配色风格\np1_nejm = p1 + scale_color_nejm() p2_nejm = p2 + scale_fill_nejm() grid.arrange(p1_nejm, p2_nejm, ncol = 2)  Lancet 柳叶刀杂(Lancet)志配色风格\np1_lancet = p1 + scale_color_lancet() p2_lancet = p2 + scale_fill_lancet() grid.arrange(p1_lancet, p2_lancet, ncol = 2) \n JAMA 美国医学学会杂志(Journal of the American Medical Association)配色风格\np1_jama = p1 + scale_color_jama() p2_jama = p2 + scale_fill_jama() grid.arrange(p1_jama, p2_jama, ncol = 2) \n JCO 临床肿瘤学杂志(Journal of Clinical Oncology)配色风格\np1_jco = p1 + scale_color_jco() p2_jco = p2 + scale_fill_jco() grid.arrange(p1_jco, p2_jco, ncol = 2) \n D3 d3.js配色风格\np1_d3 = p1 + scale_color_d3() p2_d3 = p2 + scale_fill_d3() grid.arrange(p1_d3, p2_d3, ncol = 2) \n  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析   ","permalink":"/blog/2022-09-20-r-ggsci/","summary":"今天分享一个R语言ggsci包，帮你绘制出美观舒服，又合期刊编辑审美风格的图表。\n\n安装 install.packages(\u0026quot;ggsci\u0026quot;) #remotes::install_github(\u0026quot;nanxstats/ggsci\u0026quot;) 使用 vignette(\"ggsci\")可以查看ggsci的vignette。\n\n 期刊风格 ggsci支持的主题风格主要参考自期刊、可视化库\n    风格 期刊、技术框架    NPG Nature旗下期刊(Nature Publishing Group)  AAAS Science旗下期刊()  NEJM 新英格兰医学杂志(New England Journal of Medicine)  Lancet 柳叶刀杂志(Lancet Oncology)  JAMA 美国医学学会杂志(Journal of the American Medical Association)  JCO 临床肿瘤学杂志(Journal of Clinical Oncology)  UCSCGB UCSC基因组浏览器(UCSC Genome Browser)  D3 d3.js  … …    \n 基本图表 这里准备一个常见的ggplot2图表","title":"R语言 |  使用ggsci包绘制sci风格图表"},{"content":"\rHadley Wickham的ggplot2 是一个出色且灵活的包，用于在 R 中进行优雅的数据可视化。但是，默认生成的图需要一些格式才能发送它们以供发布。 此外，要自定义 ggplot，语法是负责的，这提高了没有高级 R 编程技能的研究人员的难度。\nggpubr包 提供了一些易于使用的功能，可以使用更简单的语法代码绘制出可供发表出版的图表。\n\n安装\rinstall.packages(\u0026quot;ggpubr\u0026quot;)\r\n## 折线图\nlibrary(ggpubr)\rdf \u0026lt;- data.frame(supp=rep(c(\u0026quot;VC\u0026quot;, \u0026quot;OJ\u0026quot;), each=3),\rdose=rep(c(\u0026quot;D0.5\u0026quot;, \u0026quot;D1\u0026quot;, \u0026quot;D2\u0026quot;),2),\rlen=c(6.8, 15, 33, 4.2, 10, 29.5))\r#print(df)\r#\u0026gt; supp dose len\r#\u0026gt; 1 VC D0.5 6.8\r#\u0026gt; 2 VC D1 15.0\r#\u0026gt; 3 VC D2 33.0\r#\u0026gt; 4 OJ D0.5 4.2\r#\u0026gt; 5 OJ D1 10.0\r#\u0026gt; 6 OJ D2 29.5\r# Plot \u0026quot;len\u0026quot; by \u0026quot;dose\u0026quot; and\r# Change line types and point shapes by a second groups: \u0026quot;supp\u0026quot;\rggline(df, x=\u0026quot;dose\u0026quot;, y=\u0026quot;len\u0026quot;,\rlinetype = \u0026quot;supp\u0026quot;, shape = \u0026quot;supp\u0026quot;)\r# Change colors\r# +++++++++++++++++++++\r# Change color by group: \u0026quot;supp\u0026quot;\r# Use custom color palette\rggline(df, x=\u0026quot;dose\u0026quot;, y=\u0026quot;len\u0026quot;,\rlinetype = \u0026quot;supp\u0026quot;, shape = \u0026quot;supp\u0026quot;,\rcolor = \u0026quot;supp\u0026quot;, palette = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;))\r\n\r散点图\r# Load data\rdata(\u0026quot;mtcars\u0026quot;)\rdf \u0026lt;- mtcars\rdf$cyl \u0026lt;- as.factor(df$cyl)\r#head(df[, c(\u0026quot;wt\u0026quot;, \u0026quot;mpg\u0026quot;, \u0026quot;cyl\u0026quot;)], 3)\r#\u0026gt; wt mpg cyl\r#\u0026gt; Mazda RX4 2.620 21.0 6\r#\u0026gt; Mazda RX4 Wag 2.875 21.0 6\r#\u0026gt; Datsun 710 2.320 22.8 4\r# Textual annotation\r# +++++++++++++++++\rdf$name \u0026lt;- rownames(df)\rggscatter(df, x = \u0026quot;wt\u0026quot;, y = \u0026quot;mpg\u0026quot;,\rcolor = \u0026quot;cyl\u0026quot;, palette = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;, \u0026quot;#FC4E07\u0026quot;),\rlabel = \u0026quot;name\u0026quot;, repel = TRUE)\r\n\r饼形图\rdf \u0026lt;- data.frame(\rgroup = c(\u0026quot;Male\u0026quot;, \u0026quot;Female\u0026quot;, \u0026quot;Child\u0026quot;),\rvalue = c(25, 25, 50))\r#head(df)\r#\u0026gt; group value\r#\u0026gt; 1 Male 25\r#\u0026gt; 2 Female 25\r#\u0026gt; 3 Child 50\r# Basic pie charts\r# ++++++++++++++++++++++++++++++++\rggpie(df, \u0026quot;value\u0026quot;, label = \u0026quot;group\u0026quot;)\rggpie(df, \u0026quot;value\u0026quot;, label = \u0026quot;group\u0026quot;, fill=\u0026quot;group\u0026quot;)\rggpie(df, \u0026quot;value\u0026quot;, label = \u0026quot;group\u0026quot;, fill=\u0026quot;group\u0026quot;, color=\u0026#39;white\u0026#39;)\rggpie(df, \u0026quot;value\u0026quot;, label = \u0026quot;group\u0026quot;, fill=\u0026quot;group\u0026quot;, palette = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;, \u0026quot;#FC4E07\u0026quot;),\rcolor=\u0026#39;white\u0026#39;)\rlabs \u0026lt;- paste0(df$group, \u0026quot; (\u0026quot;, df$value, \u0026quot;%)\u0026quot;)\r#\u0026gt; \u0026quot;Male (25%)\u0026quot; \u0026quot;Female (25%)\u0026quot; \u0026quot;Child (50%)\u0026quot; ggpie(df, \u0026quot;value\u0026quot;, label = labs, fill=\u0026quot;group\u0026quot;, palette = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;, \u0026quot;#FC4E07\u0026quot;),\rcolor=\u0026#39;white\u0026#39;)\rlabs \u0026lt;- paste0(df$group, \u0026quot; (\u0026quot;, df$value, \u0026quot;%)\u0026quot;)\r#\u0026gt; \u0026quot;Male (25%)\u0026quot; \u0026quot;Female (25%)\u0026quot; \u0026quot;Child (50%)\u0026quot; ggpie(df, \u0026quot;value\u0026quot;, label = labs, fill=\u0026quot;group\u0026quot;, lab.pos = \u0026quot;in\u0026quot;, lab.font = \u0026quot;white\u0026quot;,\rpalette = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;, \u0026quot;#FC4E07\u0026quot;),\rcolor=\u0026#39;white\u0026#39;)\r\n\r甜甜圈图\r#\u0026gt; head(df)\r#\u0026gt; group value\r#\u0026gt; 1 Male 25\r#\u0026gt; 2 Female 25\r#\u0026gt; 3 Child 50\r#\u0026gt; # Change the position and font color of labels\rggdonutchart(df, \u0026quot;value\u0026quot;, label = labs,\rlab.pos = \u0026quot;in\u0026quot;, lab.font = \u0026quot;white\u0026quot;,\rfill = \u0026quot;group\u0026quot;, color = \u0026quot;white\u0026quot;,\rpalette = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;, \u0026quot;#FC4E07\u0026quot;))\r\n\r点图\r# Load data\rdata(\u0026quot;mtcars\u0026quot;)\rdfm \u0026lt;- mtcars\r# Convert the cyl variable to a factor\rdfm$cyl \u0026lt;- as.factor(dfm$cyl)\r# Add the name colums\rdfm$name \u0026lt;- rownames(dfm)\r# Inspect the data\r#head(dfm[, c(\u0026quot;name\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;mpg\u0026quot;, \u0026quot;cyl\u0026quot;)])\r#\u0026gt; name wt mpg cyl\r#\u0026gt; Mazda RX4 Mazda RX4 2.620 21.0 6\r#\u0026gt; Mazda RX4 Wag Mazda RX4 Wag 2.875 21.0 6\r#\u0026gt; Datsun 710 Datsun 710 2.320 22.8 4\r#\u0026gt; Hornet 4 Drive Hornet 4 Drive 3.215 21.4 6\r#\u0026gt; Hornet Sportabout Hornet Sportabout 3.440 18.7 8\r#\u0026gt; Valiant Valiant 3.460 18.1 6\rggdotchart(dfm, x = \u0026quot;name\u0026quot;, y = \u0026quot;mpg\u0026quot;,\rcolor = \u0026quot;cyl\u0026quot;, # Color by groups\rpalette = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;, \u0026quot;#FC4E07\u0026quot;), # Custom color palette\rsorting = \u0026quot;ascending\u0026quot;, # Sort value in descending order\radd = \u0026quot;segments\u0026quot;, # Add segments from y = 0 to dots\rggtheme = theme_pubr() # ggplot2 theme\r)\r\n\r密度图\rset.seed(1234)\rwdata = data.frame(\rsex = factor(rep(c(\u0026quot;F\u0026quot;, \u0026quot;M\u0026quot;), each=200)),\rweight = c(rnorm(200, 55), rnorm(200, 58)))\r#head(wdata, 4)\r#\u0026gt; sex weight\r#\u0026gt; 1 F 53.79293\r#\u0026gt; 2 F 55.27743\r#\u0026gt; 3 F 56.08444\r#\u0026gt; 4 F 52.65430\r# Density plot with mean lines and marginal rug\r# :::::::::::::::::::::::::::::::::::::::::::::::::::\r# Change outline and fill colors by groups (\u0026quot;sex\u0026quot;)\r# Use custom palette\rggdensity(wdata, x = \u0026quot;weight\u0026quot;, color=\u0026#39;sex\u0026#39;)\rggdensity(wdata, x = \u0026quot;weight\u0026quot;, color=\u0026#39;sex\u0026#39;, add=\u0026#39;mean\u0026#39;)\rggdensity(wdata, x = \u0026quot;weight\u0026quot;, color=\u0026#39;sex\u0026#39;, add=\u0026#39;mean\u0026#39;, rug=TRUE)\rggdensity(wdata, x = \u0026quot;weight\u0026quot;, color=\u0026#39;sex\u0026#39;, add=\u0026#39;mean\u0026#39;, rug=TRUE, fill=\u0026#39;sex\u0026#39;)\rggdensity(wdata, x = \u0026quot;weight\u0026quot;, color=\u0026#39;sex\u0026#39;, add=\u0026#39;mean\u0026#39;, rug=TRUE, fill=\u0026#39;sex\u0026#39;, palette = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;))\r\n\r直方图\r# Histogram plot with mean lines and marginal rug\r# :::::::::::::::::::::::::::::::::::::::::::::::::::\r# Change outline and fill colors by groups (\u0026quot;sex\u0026quot;)\r# Use custom color palette\rgghistogram(wdata, x = \u0026quot;weight\u0026quot;,\radd = \u0026quot;mean\u0026quot;, rug = TRUE,\rcolor = \u0026quot;sex\u0026quot;, fill = \u0026quot;sex\u0026quot;,\rpalette = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;))\r\n\r箱图\r# Load data\rdata(\u0026quot;ToothGrowth\u0026quot;)\rdf \u0026lt;- ToothGrowth\r#head(df, 4)\r#\u0026gt; len supp dose\r#\u0026gt; 1 4.2 VC 0.5\r#\u0026gt; 2 11.5 VC 0.5\r#\u0026gt; 3 7.3 VC 0.5\r#\u0026gt; 4 5.8 VC 0.5\r# Box plots with jittered points\r# :::::::::::::::::::::::::::::::::::::::::::::::::::\r# Change outline colors by groups: dose\r# Use custom color palette\r# Add jitter points and change the shape by groups\rp \u0026lt;- ggboxplot(df, x = \u0026quot;dose\u0026quot;, y = \u0026quot;len\u0026quot;, add = \u0026quot;jitter\u0026quot;,\rcolor = \u0026quot;dose\u0026quot;, shape = \u0026quot;dose\u0026quot;, palette =c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;, \u0026quot;#FC4E07\u0026quot;))\rp\r\n\r条形图\r# Load data\rdata(\u0026quot;mtcars\u0026quot;)\rdfm \u0026lt;- mtcars\r# Convert the cyl variable to a factor\rdfm$cyl \u0026lt;- as.factor(dfm$cyl)\r# Add the name colums\rdfm$name \u0026lt;- rownames(dfm)\r# Inspect the data\r#head(dfm[, c(\u0026quot;name\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;mpg\u0026quot;, \u0026quot;cyl\u0026quot;)])\r#\u0026gt; name wt mpg cyl\r#\u0026gt; Mazda RX4 Mazda RX4 2.620 21.0 6\r#\u0026gt; Mazda RX4 Wag Mazda RX4 Wag 2.875 21.0 6\r#\u0026gt; Datsun 710 Datsun 710 2.320 22.8 4\r#\u0026gt; Hornet 4 Drive Hornet 4 Drive 3.215 21.4 6\r#\u0026gt; Hornet Sportabout Hornet Sportabout 3.440 18.7 8\r#\u0026gt; Valiant Valiant 3.460 18.1 6\rggbarplot(dfm, x = \u0026quot;name\u0026quot;, y = \u0026quot;mpg\u0026quot;,\rfill = \u0026quot;cyl\u0026quot;, # change fill color by cyl\rcolor = \u0026quot;white\u0026quot;, # Set bar border colors to white\rpalette = \u0026quot;jco\u0026quot;, # jco journal color palett. see ?ggpar\rsort.val = \u0026quot;desc\u0026quot;, # Sort the value in dscending order\rsort.by.groups = TRUE, # Sort inside each group\rx.text.angle = 90 # Rotate vertically x axis texts\r)\rggbarplot(dfm, x = \u0026quot;name\u0026quot;, y = \u0026quot;mpg\u0026quot;,\rfill = \u0026quot;cyl\u0026quot;, # change fill color by cyl\rcolor = \u0026quot;white\u0026quot;, # Set bar border colors to white\rpalette = \u0026quot;jco\u0026quot;, # jco journal color palett. see ?ggpar\rsort.val = \u0026quot;desc\u0026quot;, # Sort the value in dscending order\rsort.by.groups = TRUE, # Don\u0026#39;t sort inside each group\rx.text.angle = 90, # Rotate vertically x axis texts\rlegend.title = \u0026quot;CYL\u0026quot; # Set legend title\r)\r\r表格\r#Medium blue (mBlue) theme\rggtexttable(head(iris), rows = NULL, theme = ttheme(\u0026quot;mBlue\u0026quot;))\rmain.title \u0026lt;- \u0026quot;Edgar Anderson\u0026#39;s Iris Data\u0026quot;\rsubtitle \u0026lt;- paste0(\r\u0026quot;This famous (Fisher\u0026#39;s or Anderson\u0026#39;s) iris data set gives the measurements\u0026quot;,\r\u0026quot; in centimeters of the variables sepal length and width and petal length and width,\u0026quot;,\r\u0026quot; respectively, for 50 flowers from each of 3 species of iris.\u0026quot;,\r\u0026quot; The species are Iris setosa, versicolor, and virginica.\u0026quot;\r) %\u0026gt;%\rstrwrap(width = 80) %\u0026gt;%\rpaste(collapse = \u0026quot;\\n\u0026quot;)\rtab \u0026lt;- ggtexttable(head(iris), theme = ttheme(\u0026quot;light\u0026quot;))\rtab %\u0026gt;%\rtab_add_title(text = subtitle, face = \u0026quot;plain\u0026quot;, size = 10) %\u0026gt;%\rtab_add_title(text = main.title, face = \u0026quot;bold\u0026quot;, padding = unit(0.1, \u0026quot;line\u0026quot;)) %\u0026gt;%\rtab_add_footnote(text = \u0026quot;*Table created using ggpubr\u0026quot;, size = 10, face = \u0026quot;italic\u0026quot;)\r\n\r广而告之\r\r长期征稿\r长期招募小伙伴\r付费视频课 | Python实证指标构建与文本分析\r\r\r","permalink":"/blog/2022-09-20-r-ggplot2-ggpubr/","summary":"Hadley Wickham的ggplot2 是一个出色且灵活的包，用于在 R 中进行优雅的数据可视化。但是，默认生成的图需要一些格式才能发送它们以供发布。 此外，要自定义 ggplot，语法是负责的，这提高了没有高级 R 编程技能的研究人员的难度。\nggpubr包 提供了一些易于使用的功能，可以使用更简单的语法代码绘制出可供发表出版的图表。\n\n安装\rinstall.packages(\u0026quot;ggpubr\u0026quot;)\r\n## 折线图\nlibrary(ggpubr)\rdf \u0026lt;- data.frame(supp=rep(c(\u0026quot;VC\u0026quot;, \u0026quot;OJ\u0026quot;), each=3),\rdose=rep(c(\u0026quot;D0.5\u0026quot;, \u0026quot;D1\u0026quot;, \u0026quot;D2\u0026quot;),2),\rlen=c(6.8, 15, 33, 4.2, 10, 29.5))\r#print(df)\r#\u0026gt; supp dose len\r#\u0026gt; 1 VC D0.5 6.8\r#\u0026gt; 2 VC D1 15.0\r#\u0026gt; 3 VC D2 33.0\r#\u0026gt; 4 OJ D0.5 4.2\r#\u0026gt; 5 OJ D1 10.0\r#\u0026gt; 6 OJ D2 29.5\r# Plot \u0026quot;len\u0026quot; by \u0026quot;dose\u0026quot; and\r# Change line types and point shapes by a second groups: \u0026quot;supp\u0026quot;\rggline(df, x=\u0026quot;dose\u0026quot;, y=\u0026quot;len\u0026quot;,\rlinetype = \u0026quot;supp\u0026quot;, shape = \u0026quot;supp\u0026quot;)\r# Change colors\r# +++++++++++++++++++++\r# Change color by group: \u0026quot;supp\u0026quot;\r# Use custom color palette\rggline(df, x=\u0026quot;dose\u0026quot;, y=\u0026quot;len\u0026quot;,\rlinetype = \u0026quot;supp\u0026quot;, shape = \u0026quot;supp\u0026quot;,\rcolor = \u0026quot;supp\u0026quot;, palette = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;))","title":"R语言 | ggpubr包让数据可视化更加优雅"},{"content":"我对经济史不太懂，标题起的可能不一定恰当，通过历史数据挖掘构建有意思的新变量， 量化历史学 可能是个不错的借鉴思路。\n\n历史与经济  摘自 陈志武教授统筹量化历史研究获得创纪录研究经费支持\n 由陈志武教授统筹的跨领域研究项目获得大学教育资助委员会（教资会）辖下的研究资助局（研资局）6,732万港元（即逾850万美元） 的拨款资助，再加上香港大学的支持，此单一研究项目共获得7,480万港元 （即逾950万美元），资助金额创学院自2001年成立以来的新高。项目研究团队由来自五间教资会资助院校的成员及合作方组成，包括香港大学（港大）、香港中文大学 （中大）、香港科技大学 （科大）、岭南大学（岭大）及香港浸会大学（浸大），另外亦有来自牛津大学及中国人民大学的学者。项目成员包括：港大陈志武 （项目统筹人）、Jed O. Kaplan、梁其姿、林晨和马驰骋、白营（中大）、康文林（科大）、林展 （人民大学）、刘光临（岭大）以及马德斌 （牛津）。（按英文字母顺序排列）\n题为 “量化历史研究：追寻现代中国发展的根源” 的研究项目获得研资局辖下第十轮 “卓越学科领域计划” 的资助，旨在透过在香港大学成立 “量化历史研究中心” ，作为协调和开展 “量化中国历史” 研究的核心学术机构，致力加深对中国历史发展的认识，挖掘历史新知识，促进历史教学，引导政策制定，改善商界运营模式。\n学院金融学讲座教授及郑裕彤基金教授（金融学）陈志武指出：“我们很荣幸能够获得今年度卓越学科领域计划的拨款，这支持并肯定了我们为多层面中国历史研究所作出的努力。中国蕴藏丰富的历史档案和考古挖掘，其规模在世界乃独一无二， 内容几乎涵盖中国社会的所有方面：从政治到商业、法律和监管、犯罪和动乱、家庭和宗族、文化和习俗、宗教和社会组织，以及科学。近年，这些档案被数码化，为量化历史学家提供前所未有的机会去全面重新审视中国历史的各方面。作为中国的一部分，香港具有语言、文化和人力资源的优势去建立整体而全面的中国量化历史，我们深信成立量化历史研究中心，将推动香港成为全球量化历史研究的领导者。”\n\n更多资料  鉴古识今 – 从「科举考试」分析中国经济发展\nTing Chen, James Kai-sing Kung, Chicheng Ma, Long Live Keju! The Persistent Effects of China’s Civil Examination System, The Economic Journal, Volume 130, Issue 631, October 2020, Pages 2030–2064, https://doi.org/10.1093/ej/ueaa043   Zhiwu Chen, Chicheng Ma, Andrew J Sinclair, Banking on the Confucian Clan: Why China Developed Financial Markets so Late, The Economic Journal, Volume 132, Issue 644, May 2022, Pages 1378–1413, https://doi.org/10.1093/ej/ueab082   从权贵到富贵： 中国传世明画产权变动与社会流动性研究，960-1911    陈志武 \u0026ndash; 史前文明摇篮的长久影响 \u0026mdash;你的故乡是兴是衰在四千多年前就确定？    李中清|《大数据与中国社会经济史》   陈春声 | 统计分析方法在史学研究中的应用   地理信息系统（GIS）与中国历史研究   周欣平：大数据与社会科学和人文科学研究  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-09-19-quantitative-history-economic/","summary":"我对经济史不太懂，标题起的可能不一定恰当，通过历史数据挖掘构建有意思的新变量， 量化历史学 可能是个不错的借鉴思路。\n\n历史与经济  摘自 陈志武教授统筹量化历史研究获得创纪录研究经费支持\n 由陈志武教授统筹的跨领域研究项目获得大学教育资助委员会（教资会）辖下的研究资助局（研资局）6,732万港元（即逾850万美元） 的拨款资助，再加上香港大学的支持，此单一研究项目共获得7,480万港元 （即逾950万美元），资助金额创学院自2001年成立以来的新高。项目研究团队由来自五间教资会资助院校的成员及合作方组成，包括香港大学（港大）、香港中文大学 （中大）、香港科技大学 （科大）、岭南大学（岭大）及香港浸会大学（浸大），另外亦有来自牛津大学及中国人民大学的学者。项目成员包括：港大陈志武 （项目统筹人）、Jed O. Kaplan、梁其姿、林晨和马驰骋、白营（中大）、康文林（科大）、林展 （人民大学）、刘光临（岭大）以及马德斌 （牛津）。（按英文字母顺序排列）\n题为 “量化历史研究：追寻现代中国发展的根源” 的研究项目获得研资局辖下第十轮 “卓越学科领域计划” 的资助，旨在透过在香港大学成立 “量化历史研究中心” ，作为协调和开展 “量化中国历史” 研究的核心学术机构，致力加深对中国历史发展的认识，挖掘历史新知识，促进历史教学，引导政策制定，改善商界运营模式。\n学院金融学讲座教授及郑裕彤基金教授（金融学）陈志武指出：“我们很荣幸能够获得今年度卓越学科领域计划的拨款，这支持并肯定了我们为多层面中国历史研究所作出的努力。中国蕴藏丰富的历史档案和考古挖掘，其规模在世界乃独一无二， 内容几乎涵盖中国社会的所有方面：从政治到商业、法律和监管、犯罪和动乱、家庭和宗族、文化和习俗、宗教和社会组织，以及科学。近年，这些档案被数码化，为量化历史学家提供前所未有的机会去全面重新审视中国历史的各方面。作为中国的一部分，香港具有语言、文化和人力资源的优势去建立整体而全面的中国量化历史，我们深信成立量化历史研究中心，将推动香港成为全球量化历史研究的领导者。”\n\n更多资料  鉴古识今 – 从「科举考试」分析中国经济发展\nTing Chen, James Kai-sing Kung, Chicheng Ma, Long Live Keju! The Persistent Effects of China’s Civil Examination System, The Economic Journal, Volume 130, Issue 631, October 2020, Pages 2030–2064, https://doi.org/10.1093/ej/ueaa043   Zhiwu Chen, Chicheng Ma, Andrew J Sinclair, Banking on the Confucian Clan: Why China Developed Financial Markets so Late, The Economic Journal, Volume 132, Issue 644, May 2022, Pages 1378–1413, https://doi.","title":"文献汇总 | 量化历史学与经济学研究"},{"content":"\n赵耀龙,巢子豪.历史GIS的研究现状和发展趋势[J].地球信息科学学报,2020,22(05):929-944.\n 赵耀龙，华南师范大学地理科学学院教授、博士生导师、副院长。研究方向为地理信息科学与技术、空间综合人文学与社会科学。 巢子豪，华南师范大学地理科学学院GIS专业博士研究生，研究方向为空间藏学。  摘要 历史地理信息系统（历史GIS）属于地理信息系统（GIS）与历史学相结合的交叉研究领域，它有机地集成了GIS的技术方法、地理学家的空间视角和历史学家的时间视角，量化历史时期的地理过程并构建相应的时空模型，为面向未来的科学预测提供研究基础。历史GIS兴起于20世纪90年代中期，给GIS学科、历史地理学和历史学都带来了新的研究机遇与活力，并表现出强劲的发展势头，但也存在一些尚待解决的问题。近年来，历史GIS向社会提供了日益丰富的历史地理信息服务，并逐渐跨越系统的技术层面，向着科学层面纵深发展。本文通过对国内外相关文献的梳理与分析，回顾了历史GIS产生的背景，从数字化、数据模型、数据库建设与系统开发、空间分析和可视化5个方面综述了国内外历史GIS的研究现状。最后，从历史资料的空间化与数字化、历史地理时空大数据、历史地理空间框架构建及历史地理信息服务、历史地理时空过程及模型构建、历史地理信息科学和技术学科体系的形成等角度展望了历史GIS的发展趋势，以期为历史GIS未来的发展提供新的研究思路。\n关键词: 历史GIS 研究现状 发展趋势 空间化 历史地理时空大数据 历史地理空间框架 历史地理信息服务 历史地理信息科学与技术\n1. 引言 地理学与历史学分属于对空间差异、时间分化进行研究的学科，而对历史的研究不可能完全脱离地理空间，通过侧重于对人类生活中有形环境的研究，地理学日益进入到历史学的研究领域，并产生了相应的交叉研究学科。历史地理学即是这样一门研究人类历史时期地理环境变化及其规律的学科。传统历史地理学的研究方法主要集中于历史文献资料的归纳和演绎，缺乏现代科学技术手段的应用，较难对统计计量科学意义上的规律进行探讨。随着信息化、智慧化技术的迅猛发展和地球信息科学的兴起，历史地理学在迎来发展机遇的同时，也面临着诸多挑战。如何推动传统历史地理学中研究方法与科学思维在新时代的发展与创新，便是亟待解决的问题之一。地理信息系统（GIS）应用现代数字化技术手段解决时间和空间问题，在信息化和智慧化浪潮中得到了快速发展，并广泛应用于诸多学科领域，包括历史学和地理学，历史GIS应运而生。历史GIS集合了GIS的技术特点、地理学家的空间视角和历史学家的时间视角，通过GIS的技术方法将空间和时间视角相结合，研究时空尺度上的变化模式。\n历史GIS常被应用于历史地理数据的管理、处理、空间分析、可视化表达等方面，为历史学研究提供了更加广泛的地理空间视角，并发挥出越来越重要的研究价值。①为历史学特别是历史地理文献中的描述性信息提供了空间化和定量化处理方法。②用于不同来源、不同类型、不同结构历史地理数据资源的有效整合，如书面文档、古地图、历史地图、现场调查结果、考古文物等。通过建立历史GIS专题数据库，实现不同历史地理数据在同一个时空框架内的综合管理，有助于历史研究的条理性与系统性。③将空间分析的概念引入历史研究，让历史学家们重新审视历史学中空间信息的价值和社会关系中空间要素的重要性，从历史事件的地理空间位置和时间角度出发，了解其时空演化过程和机制。④为传统历史研究提供了空间可视化的表达方法，通过动态地图、三维GIS(3D GIS)等手段生动直观地展现历史数据的空间变化过程。此外，历史GIS还促进了跨学科研究的发展，为来自历史学、地理学、测绘学、计算机科学与技术等领域的学者们提供了合作的空间；GIS与历史学的结合还可以发现传统方法难以发现的历史问题，促进历史学领域潜在信息的挖掘，并帮助检测传统数据的合理性与正确性。\n历史GIS的出现无疑为传统的历史地理研究注入了新的活力，也为GIS研究本身提供了广阔的拓展空间。然而，目前历史GIS研究中也存在一些尚待解决的问题。例如，特定历史地理数据的缺失或难以量化，导致GIS功能难以充分发挥；部分GIS的研究方法并不完全适用于历史问题的研究，需要根据具体问题进行修正；历史GIS中大量空间和属性数据的高精度、规范化处理，以及数据库建设的高难度和高成本也是必须面对的实际问题；研究人员的知识结构难以满足历史GIS的多学科知识背景需求，历史学者往往将GIS理解为绘制地图的软件，难以在研究中挖掘出深层的时空信息等。这些问题在一定程度上制约了历史学研究中GIS作用的发挥。另外，随着GIS逐步从信息系统的技术层面走向信息服务的科学层面，历史GIS于历史地理学而言也不再仅仅是技术的实现，有必要构建历史地理信息科学与技术的学科体系，并为社会提供日益丰富的历史地理信息服务。\n有学者从不同学科视角讨论了我国历史GIS在不同时期的发展态势。如潘威等于2012年回顾了GIS引入我国历史地理学界的10年发展历程；2018年，潘威提出了“数字人文”背景和大数据理念下我国历史地理信息化的应对策略；张萍回顾了GIS对中国历史研究的贡献和存在的问题。这些成果为历史GIS研究提供了重要参考。为全面借鉴国内外历史GIS的研究经验，本文在回顾历史GIS产生背景的基础上，从数字化、数据模型、数据库与系统开发、空间分析和可视化5个方面梳理了国内外历史GIS的发展历程和研究进展，提出历史GIS未来值得重点关注的发展趋势，以期推动历史GIS 的健康发展。\n2. 历史GIS产生的背景 2.1 国际上历史GIS 产生的背景 历史地理学界提出将研究对象的时间和空间属性相结合始于20世纪初期。英国历史地理学家Darby于1936年提出的“横剖面法”，以时间序列中多个地理空间剖面展示了空间随着时间发生的变化。20世纪50年代，历史地理学界兴起了“定量革命”，不再局限于定性描述的研究方法，而是强调对研究对象的定量分析。20世纪80年代，数据库技术开始与历史学研究相结合。同一时期，GIS技术得到飞速发展，并被引入历史研究领域，开始用以研究历史事件发展进程、区域经济开发、聚落成长变迁、人群地域特征、灾害或疫病的空间演进等。但该类研究更多关注的是地表状况的变迁，忽略了历史地理学关心的历史现象更迭背后所隐藏的原因等问题，并且历史文献在此类研究中并未被使用，因此，该类研究不能算作GIS应用于历史地理学领域的真正开端。\n学者们普遍认为历史GIS兴起于20世纪90年代中期，认为该领域的开山之作是1994年Goerke的《Coordinate for Historical Maps》一书，该书整理了1994 年于欧洲大学研究所举办的研讨会展示的文章。当时历史学正处于对GIS的初步了解阶段，历史GIS发展较为缓慢。这一观点在Ogborn的文章中得到了体现，该文综述了1997年历史地理领域的研究进展，但并未提及历史GIS。之后，由于GIS在历史研究领域的潜力，历史GIS经历了一个相对快速发展时期。1998年和1999年社会科学历史协会组织的历史GIS专题会议中提到了学者们热情地参与到GIS等新方法的使用中，参与者们认为新工具的使用同样意味着创新。Holdsworth在其2002年的文章中提及历史GIS，认为其是历史地理学领域一种新兴的发展趋势。Baker将历史GIS描述为历史地理学领域一项“有趣而又富有挑战的发展”。\n对历史GIS更多、更全面的研究主要体现在2000年以后，这一时期出现了许多以历史GIS 为研究重点的出版物。Knowles在2000—2008年出版了历史GIS相关专著2部和期刊论文2篇，包括于2002年的《Past Place: GIS for History》和2008年与Hillier一同出版的《Placing history: how maps，spatial data， and GIS are changing historical scho larship》。Gregory在2003年发表了有关历史数据服务的《Guide to Practice in GIS》，并于2007 年和Ell一同出版了《Historical GIS: technolo gies， methodologies and scholarship》一书，进一步普及了历史GIS的理念、技术及相关应用。von Lünen和Travis于2013年出版了著作《GIS and History:Epistemologies，Reflections，and Conside rations》该书汇集了对多个学术领域专家的采访，分享了他们多年的经验和历史学者们为何应该接受GIS的思考。期刊方面，《Social Science History》在2000年的第24卷第3期中首先推出了历史GIS的专刊。《History and Computing》和《Historical Geography》也分别在2003年第13卷第1期和2005年第33卷中推出了历史GIS专刊。诸多刊物与文章的问世标志着历史GIS进入了快速发展时期。历史地理学者Knowles承认历史GIS的巨大进步，但也指出历史GIS距最初的期望值还相距甚远。并且GIS为历史研究带来了哪些东西，其局限又有哪些，这些都没有形成普遍可接受的观点。\n2.2 国内历史GIS 产生的背景 拥有悠久的历史文明使我国在古代地理学领域长期处于东亚领先地位，但在历史地理学领域日本起步更早，且领先于我国。“历史地理”这一名词于20世纪初由日本传入我国。1935 年“，历史地理”被顾颉刚、谭其骧创办的《禹贡》杂志用作英文刊名，由此扎根于国内学术界。2000年，冯仁国指出了国内历史地理学发展所遇到的问题，即历史地理学领域的技术手段滞后，与地理学主流发展相距甚远，仍停留在利用传统地图学法和事件枚举法来进行研究分析的阶段。同年，针对这一问题，中国历史地理国际研讨会在云南大学召开，会上着重讨论了领域内新兴的GIS技术。葛剑雄于会议中指出，需凭借现代化的高科技手段来提高历史地理学研究的精度。编绘数字历史地图，创建中国历史GIS的工作开始提上日程。相较于其他国家，我国发展历史GIS具有得天独厚的优势，所拥有的不同历史时期的气候、人口、经济、耕地、赋税等连续历史文字记录为发展历史GIS奠定了坚实的数据基础。\n有学者认为，我国历史GIS的开拓者为复旦大学历史地理研究中心的满志敏，其在历史自然地理领域尝试将GIS与历史文献相结合。满志敏于2000 年发表的文章《光绪三年北方大旱的气候背景》被视为是国内GIS与历史地理研究相结合的开山之作。该文基于历史文献资料构建专题空间模型，用于模拟历史时期的地理现象，有着较高的研究起点。2001年，复旦大学历史地理研究所与美国哈佛大学等机构合作，在谭其骧主编的《中国历史地图集》基础上，开发了“中国历史地理信息系统”（China Historical Geographic Information System，CHGIS），建立了一套中国历史时期连续变化、开放的基础地理信息数据库。这一项目的启动标志着我国历史地理学信息化时代的到来。2007年，葛全胜等在总结我国现代历史地理学不同的发展阶段时提到了信息技术的引入时期，表达了当时历史地理学界对提高研究成果定量化程度和精确性的迫切需求，并介绍了历史GIS在国内取得的突破性进展。自此，历史GIS的研究在国内逐步展开。然而，仍有学者并没有真正理解历史GIS研究及技术背后的思考与判断，而是将其认为是由计算机操作的绘图工具，可见学界对历史GIS的认知仍不深刻。\n与国外期刊相继推出历史GIS专刊相比，国内期刊对历史GIS的重视程度相对较低。截至目前，未有期刊出版过历史GIS专刊，相关文章也相对较少。\n近年来，历史GIS的发展日益受到国内学术界的普遍重视。中国地理学会历史地理专业委员会和复旦大学历史地理研究中心于2015年举办了第一届“历史地理信息系统HGIS沙龙”，之后每年举办一次。中国地理信息科学理论与方法学术年会也曾多次组织了历史GIS分会场。\n##3. 历史GIS研究的现状\n3.1 历史地理信息的数字化 地图是历史地理学的重要组成部分，其载体形式经历了从传统的石质、帛质、纸质等到数字化的转变。GIS具有空间表现力强、信息存储量大的特点，能够实现把历史上传统的地理空间信息描述方法和内容，转移到以地理空间坐标为基础的现代地图上，以数字化的形式再现。历史地理信息的数字化即是对历史地理信息由纸质或其它载体转化为数字形式的技术过程，有助于推动历史地理信息的深度挖掘和利用。\n在国外，Spence于2000年在其专著中论述了历史地理信息数字化的潜力。他将伦敦17世纪90年代的行政地理学数据与税收数据相结合进行数字化，使用专题地图来探索这一时期伦敦地理的不同领域，包括营业租金、家庭密度与租金、按性别的家庭分布等，发现了数据中隐藏的空间模式。Baily等致力于将英国20世纪30至40年代开展的首次土地利用测绘图进行数字化，并评价了该半自动化程序的数字化精度。Scholzel 等开发了新的算法和数字化软件，给历史地理信息的数字化技术带来了革新，实现了对投影信息模糊的历史地图进行数字化的功能。被引入历史地理信息的数字化领域后，GIS也被广泛用于数字化历史地图集的制作。GIS技术与传统制图法的结合，既保证了制图质量又节省了时间和人工成本。并且，通过建立以行政边界为典型代表的不同类型数据层，将其与不同主题的属性数据相结合来制作数字化历史地图集已成为历史地理信息数字化的一种新模式。多个国家或地区已对既有的国家历史地图集进行了部分数字化工作。其中，美国的“世界电子图书馆”（WorldDigital Library Map Section）不仅存储有世界各地的历史地图千余件，还以电子地图插件的形式标识出历史地图所在的地理位置，以表格的形式记录了历史地图的属性信息。“欧洲历史地图资料库”(History and Maps of Europe，Euralas)以21幅历史地图表示欧洲和地中海盆地公元1世纪至20世纪的政治景观。“非洲历史动态地图集”(Animated Atlas of African History) 以动态地图的形式展现了非洲大陆1879—2002年的领土变更、政治体系变化、暴力冲突和经济人口等历史信息。“加拿大历史地图集”(Historical Atlas of Canada)在制作第二卷时引入GIS 技术，以地图、文字、图表的形式展现了加拿大19世纪和20世纪不同的历史主题与事件。“南亚电子图书馆”网站将“印度王国地名地图集”中的历史地理信息进行数字化。另外，展示专题信息的历史地图集数字化工作也相继展开。如展示19世纪40年代爱尔兰饥荒对当地劣质住宅影响等信息的“爱尔兰饥荒数据地图集”(The Atlas of Irish Famine Data)、含有德国及其部分周边地区地名信息的“德意志帝国地图集”、描绘有法国与印第安战争战场详情的“法国－印第安人战争地图”(Maps of the French and Indian War)等。\n我国历史地理信息的数字化工作起步于复旦大学历史地理研究中心和哈佛大学等机构联合开展的“中国历史地理信息系统”项目，其数字化的阶段性成果已共享于CHGIS数据库网站。北京大历史地理研究中心与北京市测绘设计研究院合作的“北京历史数字地图”是我国第一项城市区域历史地图的数字化成果，应用GIS技术对《北京历史地图集》进行了数字化。自此，历史地图数字化工作在我国多个城市展开《天津城市历史地图集》《广西历史地图集》《沈阳市历史地图》《杭州市历史地图》《上海市历史地图》等的数字化工作相继提上日程，并已顺利完成。近年来，国内诸多学者在大比例尺历史地图数字化领域也进行了探索。上海交通大学对日本科学书院1998 年出版的《中国大陆五万分之一地图集成》一书中共计4088 幅民国历史地图进行了扫描、地理校准等数字化工作，存储于“中国历史地图地理信息系统”。韩昭庆对利用西方测绘方法绘制的康熙《皇舆全览图》进行了数字化，形成一套以1710年代为时间界面的国家基础历史地理数据库和地名数据库。史磊则尝试利用GIS技术，对民国初年河南省1:10万地形图进行数字化保护。在技术领域，谭瑛等探讨了基于数字化技术的历史地图空间解译方法，从历史地图中提取隐含的空间要素和内涵信息，进行了分类提取、类型分层、数字化转译。\n我国历史悠久，历史地图或古地图遗存众多，在数字化、信息化的时代潮流中，越来越多的历地理信息数字化工作被提上日程。历史GIS的数字化功能为历史学及相关学科的研究提供了便利。历史地理信息数字化需求的增加也促进了历史GIS数字化技术的革新与进步。\n3.2 历史地理信息的数据模型 GIS的数据模型是根据程式化、概念化的策略将现实世界发生的现象抽象化，可通过地理层面的点线面或离散连续场进行构建。数据模型中应定义数据类型、存取方式、关系、操作和规则，以维持数据库和信息系统的完整。如何设计一个坚固、连续的数据模型是历史GIS数据库及平台建设的关键问题之一，数据模型设计的好坏直接关系到历史GIS数据库及平台建设的成败。目前，国内外历史GIS在构建数据模型时多以时空数据模型为基础。时空数据模型也是实现不同尺度、不同时序空间数据互动与融合的基础，通过依赖于时间的表达方式来组织管理时态地理数据、属性、空间和时间语义更完整的地理数据模型，可重现历史状态，追踪历史变化以及预测未来趋势。基础、通用的历史GIS数据模型是历史GIS的基石，就像矢量数据模型之于GIS一样重要，决定了历史GIS 时空分析和表达能力的强弱。\n国外，Langran较早做出了将时间与GIS相结合的研究尝试，他总结了时空立方体、时空复合、基态修正和时空快照模型这4种时空数据模型，并在应用实例中强调了时间分量的重要性。随后，Peuquet和Duan归纳并比较分析了基于事件的时空数据模型为代表的9种主要时空数据模型。Griffiths等针对当时的对象数据模型普遍无法整合时间与空间属性的弊端，提出了名为Tripod的空间—历史数据模型，该模型基于组件的设计使其内部的空间、时间和历史信息三者得到有机整合。在与复旦大学合作的CHGIS项目中，Berman在原有三维空间数据的基础上增加了时间维度，形成四维空间。CHGIS的数据模型满足了在特定时间点搜寻所有地理单元，通过对象特征类型筛选结果，展示某一地点随时间变化等的要求。Euratlas历史地图集网站的数据模型是不同时间点政治景观及其互相关系的集合，按等级划分为不同的政治领域，高等级领域由低等级领域构成，从低到高四级行政实体分别为：省级、一级行政单元、国家、超国家行政实体。\n国内学者在历史地理信息数据模型领域的研究颇丰。徐志红等提出了基于事件语义的时态地理信息系统模型，并应用该模型进行了历史数据的查询及历史数据的回溯和再现。顾国民等针对历史GIS平台中时空数据的存储和显示问题进行研究，在已有时空模型的研究基础上，选用并改进了面向对象的数据模型，提出了更加适合历史GIS平台的时空数据模型。Chen等根据代表性数据的属性设计了针对历史地理名称的时空数据模型，建设了研究中国历史与文化的时空框架。于靖提出了基于地名生命周期的城市历史地名时空数据模型，解决了以可视化方式展示并查询时间截面地名等相关问题。史先瑞分析了基础历史地理信息的时空变化特征，改进基态修正数据模型，提出了一种基于对象的基础历史地理信息的数据组织方法。胡迪等认为现有的历史地理信息数据模型主要面向某一专题的历史地理数据库或信息系统，通用性差。他从地理与历史双重视角出发，以时间、地点、人物、事件历史4个要素为基础将历史信息抽象为历史人物、历史事件、历史地物和历史场景，以及关系和经历等要素，提出了一种通用的历史地理信息数据模型。历史地理信息数据模型的研究正随着各式历史地理信息数据库及系统需求的增加而日益深入。\n3.3 历史地理信息的数据库建设与系统开发 历史地理空间数据的采集耗时较长，过程繁琐冗长，且往往是整个历史GIS建设过程中耗资最大的一个环节。历史地理信息数据库的建立不仅要考虑到建设成本，还要顾及到日后的管理与维护。历史地理信息数据库一旦建成，其实际需求常常超越其最初的构想，因此，设计者需考虑到诸如数据记录、元数据和数据的长期保存等问题，以便历史地理信息数据库价值得到充分展现。\n世界上多个国家和地区已构建或正在建设本国的国家历史地理信息系统。美国地理家协会(The Association of American Geographers，AAG) 网站提供了世界各国主要的历史GIS目录。其中，由朴次茅斯大学主持的“大英历史地理信息系统”(Great Britain Historical Geographic Information System) 始建于1994年，主要提供了英国历史时期不同尺度的行政区划及区划内的人口统计信息。明尼苏达大学的明尼苏达人口研究中心的“国家历史地理信息系统”(National Historical Geographic Information System)在美国国家科学基金会资助下开发，其创建的最初意图为构建一个基础架构以支持国家范围内不同信息的专题制图。该系统汇集了1790年至今美国全国范围内的人口、住房、农业、经济统计数据和相应的历史边界矢量数据。“比利时历史地理信息系统”(Belgian Historical Geographic Information System，BHGIS)由根特大学于20世纪90年代开始建设，储存了1800—1963 年比利时国内的人口、工业、农业及贸易数据。悉尼大学开展的“时间地图计划”(TimeMap)开始于1996年，可通过网络索引的方式获取分布式资源、交互式地图、时间序列数据以及地图动画，是国际上首个基于时间序列的交互式与分布式相结合的制图系统。国际上著名的国家历史地理信息系统还包括了加拿大渥太华大学开展的“加拿大世纪研究项目”(Canadian Century Research Infrastructure)，韩国高丽大学开发的“南韩历史信息系统”(South Korea Historical GIS)，欧洲史研究所与德国梅茵兹应用技术大学联合开发的“德意志历史地理信息系统”(Historical Geographic Information System Germany)，荷兰拉德堡德大学开发的“荷兰地理信息系统”(Netherlands Geographic Information System) 等。\n相较于国家层面的历史地理信息平台，根据不同专题建设的历史地理信息平台数据来源更广，针对性更强。目前，世界范围内的专题历史地理信息平台较多，如哈佛大学地理分析中心开发的展现非洲历史地图与重要事件的“非洲地图(AfricaMap)”，越南佛教大学开发的可展示越南佛教场所等佛教文化的“越南佛教文化地图集网站”(Cultural Atlas of Vietnam-ese Buddhism)，悉尼大学研发的可视化“柬埔寨吴哥窟文化遗产历史地理信息平台”(Living with Heritage)，爱尔兰梅努斯大学开发的“爱尔兰人口变化地图集平台”(Irish Population Change Atlas)和“爱尔兰饥荒地图集平台”(The Atlas of Irish Famine Data) ，Migliaccio 等组织开发的意大利历史地震数据管理原型系统等。\n我国的历史地理信息平台建设起步较晚，但发展迅速。国家层面的历史地理信息平台以“中国历史地理信息系统”和“中华文明之时空基础框架”最为著名。由复旦大学历史地理研究中心与美国哈佛大学等合作的“中国历史地理信息系统”(CHGIS)项目起始于2001年，致力于建设成为一个连续变化的中国历史时期基础地理信息数据库平台，为研究人员提供根据特定时间和地点的GIS数据查询、统计、下载、空间分析、专题制图与建模服务。CHGIS的历史空间数据来源于复旦大学历史地理研究中心的原始资料，目前已有空间数据的时间跨度为公元前221年至公元1911年。“中华文明之时空基础框架”(Chinese Civilization in Time and Space，CCTS)由中国台湾地区的中央研究院于2002 年研发完成。该系统存储了我国大量历史地图及主题化的属性信息。其中，历史地图以谭其骧先生主编的《中华历史地图集》为基础，提供有上古至清代逾两千年的中国历代疆域图等基本历史地图，并辅之持续整理的各类其它历史地图、遥感影像等。\n除了国家历史地理信息平台，国内不同专题的历史地理信息平台也层出不穷。台湾中央研究院的“台湾历史文化地图系统”以历史文献、地名资料与古今地图为基础资料，搭建起了数字化的台湾时空基础架构平台。香港中文大学地球信息科学研究所研发的“民国时期北京都市文化历史地理信息数据库”，考察了由民国成立至抗战时期的北京市在多元文化激荡之下所呈现的文化变迁空间模式。首都师范大学历史地理研究中心与陕西师范大学出版社联合开发的“丝绸之路历史地理信息开放平台”，建立了一套自汉代张骞打通西域以来至1949年以前丝绸之路沿线逐年连续变化的基础历史地理数据库。华中科技大学建立了展现汉语方言分布与演化的“中国历史方言地理信息系统”。中国地图出版社与中国科学院遥感与数字地球研究所在中国31个主要城市古地图数字化成果的基础上，建成了可共享的“中国城市历史地理信息平台”。天津大学建筑学院建立了多角度还原明长城军事聚落历史地理原貌的“明长城军事聚落历史地理信息库”。同时，陕西理工大学的“蜀道历史地理信息系统”、华南师范大学与西藏大学联合开发的“藏语方言时空数据共享服务平台”等多个历史地理信息系统平台也处于研发之中。此外，国内诸多城市也有展现地方历史文化的地理信息系统。如，北京市测绘设计研究院开发的“北京历史文化地理信息系统”、上海师范大学主持的“国际化大都市（上海）历史人文地理信息系统”、南京大学开发的“六朝建康地名信息系统”、西安市城市规划设计研究院设计的“大西安区域历史地理信息共享服务平台”等。\n目前，历史地理信息平台的建设正日益向着专题化、区域化的方向发展。同时，方兴未艾的历史地理信息平台建设也促进了数据库技术和基于历史地理信息平台的专题研究进步\n3.4 历史地理信息的空间分析 Gatrell将空间分析定义为内部相关的3大主题的研究：空间布局、时空过程和空间预测。空间位置信息及其随时间的变化是历史地理研究的核心，而空间分析可以定量的方式研究历史地理领域的时空现象和过程。空间分析的方法虽然不仅限于GIS，但GIS中属性数据与地理坐标相关联的数据模型使其非常适合于定量研究。历史研究有其特殊性，并不是所有的GIS空间分析方法都能直接应用于该领域，需要对其做相应调整。\nGIS在历史地理学领域的空间分析方法可以分为3大类，即：点模式分析、附有属性数据的点线面数据分析和栅格数据分析。其中，最基本的点模式分析从研究对象空间特征的角度出发，探寻点群的空间布局，不涉及其属性信息。如Longley等基于点模式使用平均中心与标准距离法研究了1850—1960年加拿大安大略省伦敦市住宅用地、公共用地、商业用地和工业用地4种土地利用类型的变化模式。其它适用于历史GIS点模式研究方法还包括了密度平滑法、距离衰退模型、地理分析机等。探索点线面数据中属性数据的空间模式，有单变量和多变量2类分析方法。局部空间自相关法是单变量分析的典型方法之一。地理加权回归是一种通过空间权重矩阵进行回归分析的多变量分析方法。Gregory 和Ell同时使用了局部空间自相关和地理加权回归分析了19世纪40年代末期爱尔兰大饥荒后的爱尔兰各地的人口减少情况。相较于矢量数据，栅格数据更适合表示连续的表面，处理没有清晰边界的不确定数据。Bartley 和Campbel收集了英格兰14 世纪上半叶的6000 多名土地私有人去世后的财产清单，根据位置信息生成点数据，并插值生成连续的栅格表面，制作了英格兰黑死病前的土地利用专题地图，进行了私有草地分布密度等分析。\n近年来，国外学者使用GIS的空间分析功能在宗教、语言、军事、城市、疾病、遗迹、自然景观等历史地理相关研究中做了诸多尝试。Ayhan 等基于GIS 和空间统计学的方法研究了不同年份土耳其伊兹密尔清真寺的空间分布与城市发展的关系，结果表明该市清真寺的时空分布模式与城市发展模式相似。Blaxter基于核密度估计等方法追溯了挪威语言文化的时空扩散过程。Skaloš等基于数字化后的历史军事测量图和正射影像图，改良了已有的方法，研究捷克部分地区土地覆盖的长期变化情况，其方法更适用于时间尺度超过250年的长期景观变化研究。Keti从18世纪现代罗马城历史地图集中获取信息，根据不同的主题信息对不同的城市现象进行分析与解译。Bezymennyi等收集了1913—2012年乌克兰家畜炭疽病爆发位置数据，分析其时空分布模式和爆发的热点区域，并进一步研究一个世纪以来该疾病爆发地的变化趋势。Blanco等从1870—1960年的古地图中提取出5800多个马德里的水利遗产点数据，将其位置信息、时间信息和属性信息存入数据库，使用GIS 分析其多年的分布特征，得出了水利遗产日益受到城市化影响而破坏的结论。Madricardo和Donnici根据城市历史档案中的环境记录重建了意大利威尼斯舄湖从初建到现在的景观变化，并与描绘有舄湖详细信息的历史地图进行对比，揭示出舄湖景观变化对城市的影响。\n从现有研究成果来看，历史GIS最早在国内萌芽时，空间分析功能即被应用于历史水文、历史气候、历史土地覆被、历史海岸线等多个自然地理研究领域。如潘威、满志敏等基于GIS技术构建格网体系，对近百年来上海市三角洲河网的复杂形态进行了研究。葛全胜等专注于历史气候变化及其带来的影响，先后研究了历史气候变化对农作物影响的时空分异，以及对红火蚁潜在分布区的影响。何凡能、方修琦等多次提到用GIS中空间格局网络化的方法重建历史耕地、森林等土地覆被，并分析其空间特征。张晓祥、康育龙等利用数字海岸线分析系统分别分析了历史时期江苏和杭州湾海岸线的演化方式及变化速率。GIS空间分析在历史人文地理领域也有着更加广泛的应用。其中，城市和村落空间形态与格局特征的历史演变是近年来最常被关注的领域。诸多学者基于历史文献、古地图等资料中提供的文字、图像信息，以国家级或地方历史文化名城名镇名村、历史时期城池、历史时期城市工农业用地，民国时期北京城内学校等为研究对象，利用GIS的缓冲区分析、叠加分析、统计分类分析、格网方法等GIS空间分析方法定量化研究其时空布局与形态变化过程，并结合史实分析其内在机理。历史文化与GIS的结合同样是研究的热门领域，包含有民族分布、宗教演变、方言发展、文化名人足迹等研究。杨宇亮等以GIS的技术方法对藏彝走廊与汉族走廊的民族格局开展研究，不仅描述了二者与地理空间的关联性，还反映出了民族的迁徙过程。韩冰利用地理编码技术和谷歌卫星影像获取浙江省1269座佛寺的矢量多边形、中心点数据、面积规模以及布局特征，并基于GIS对其进行了全局自相关分析、系统聚类分析等研究。张义将汉语置于一个广阔的时空背景中，直观地考察方言特征历时的演变与分布、相互影响的程度以及各方言间的亲疏度问题，并在此基础上综合考察中国历史时空坐标中的汉语方言演变轨迹。曾莹利用GIS空间化和数字化方法收集整理资料，设计空间数据库，并基于GIS分析方法探索了江西古代书院及文化名人的时空特征。与此同时，得益于GIS强大的空间分析能力和数据建模能力，诸多学者已经将GIS成功地运用于考古学领域。张海在其专著《GIS与考古学空间分析》一书中指出，考古学景观强调地表空间构成的连续性、区域性和综合性，这3大特征使得GIS技术十分适宜于考古学的景观分析。目前，作为考古学研究的一项新兴支撑技术，GIS中的数字地形分析、空间叠置分析、缓冲区分析和空间形态分析等空间分析方法常被用于研究考古遗址的空间分布规律，并用以探究遗址分布的倾向性及其与地理环境的关系。另外，国内学者还尝试将GIS应用于历史文化景观重现、文物保护、历史人口分布研究、古建筑价值评价与保护等历史人文地理领域，如首都师范大学的“丝绸之路历史地理信息系统建设”等。\nGIS中的诸多空间分析方法在历史地理领域已得到了较为广泛的应用，挖掘了传统历史地理研究中未曾发现的潜在信息。然而，历史GIS的空间分析不应该仅仅是GIS方法在历史地理领域的常规应用，随着历史GIS在历史学界的日益普及，越来越多的学者改进了GIS方法以适应不同历史研究的需求，不断丰富着历史GIS空间分析方法的理论。\n3.5 历史地理信息的可视化 历史地理信息的可视化将空间、历史与属性等要素结合，在空间可视化的基础上表达时空变化与发展过程，直观反映时空对象在不同历史时间的各个状态与动态演化。GIS与历史地理空间数据结合可以反映变化的实体与现象，能够描述时空对象的产生、发展、演化与消亡的全过程。历史地理信息的可视化展现不仅仅只有纸质形式的地图，随着计算机技术的发展，越来越多的新技术与GIS相结合，传统的历史地理信息表现形式得到了颠覆性的革新。目前，GIS可视化方法中的3D GIS、动画、多媒体影像等多种技术已在历史地理研究中得到了应用。\n3D情景的构造是目前历史地理信息可视化领域的一大热点。国外在3D GIS方面的发展已较为成熟，并与全球导航卫星系统GNSS、遥感、激光雷达等其他相关技术结合，运用于城市历史风貌、历史地形、历史古迹的还原等方面。日本的Sadahiro等使用DragonFly快速三维制图引擎，在其中叠加东京市的数字高程模型(DEM)、高分辨卫星影像和东京市的历史影像数据，实现了从不同角度和位置浏览东京城市历史风貌的功能。Harris基于数字地形模型(DTM)探索西弗吉尼亚芒兹维尔附近的格雷夫克里克土墩的史前地形。在历史考古学领域，越来越多的学者倾向于借助激光雷达和3D GIS技术进行研究，还原历史遗迹的原始面貌。尽管动画的方式生动且高效，但在历史研究领域中有效利用动画技术的实例较少。历史研究中较长的时间尺度决定了要想实现历史地理信息的动态呈现必须减少可视化对象本身的多样性和复杂性。动画、多媒体技术与GIS的结合在历史自然地理和历史人文地理两方面均有涉及。Cunfer与Rastner等分别运用GIS的动画功能探索美国大平原干旱尘暴区和Findelengletscher冰川随时间的变化。Siekierska 和Armenakis则借助多媒体方法展现加拿大历史上领土的演变。虚拟现实地理信息系统(VR-GIS)等其他可视化技术的综合应用同样对历史地理研究做出了重要贡献，其真实感和互动方式使历史地理信息的显示和观察更加生动和方便，但仍处于发展的早期阶段。Howey 和Brouwer认为考古GIS研究必将受到“数字人文”领域数据、方法与技术的影响，并提及了应用无人机影像进行“数字化故事叙述”、数字化重建和VR等技术在该领域的前景。Pérez-Martín等整合了手绘稿、照片、数字正射影像、计算机图形等多媒体图件影像，在数字摄影测量工作站、全球导航卫星系统、计算机辅助设计、和电脑动画等前沿技术的支持下，以一种新的形式展现了西班牙历史时期的风车情况。\n在国内，基于历史文献的可视化研究也是学们较多涉及的研究领域。李海萍等以清代历史地图为例，参照国内现有的地图符号标准，研究并设计了基于GIS的清代历史地图符号库。朱锁玲和王明峰应用GIS技术实现了对《方志物产》中物产分布、物产传播等相关数据的管理和可视化制图。王哲梳理了史料中历史空间数据的可视化方法，探讨了未来经济史研究领域利用可视化手段的可能性。多名学者也在历史事件可视化角度展开过研究。王占刚等利用事件时态树结构实现历史事件时空过程可视化，并以Adobe Flex为工具开发实现了历史事件时空过程可视化算法。陈敏颉等探讨了战争历史事件可视化策略，开展了基于历史事件运动轨迹的可视化表达方法的分类研究。王加胜等利用统计图表、社会网络和GIS技术对南沙群岛历史事件演化的时空特征进行了可视化表达。GIS 与VR、多媒体等技术结合对历史古迹的三维重建也是国内历史地理信息可视化研究的热点之一。GIS三维重建的重点主要集中在古代城市、历史名迹、考古遗址等的空间格局与构造的复原上。另外，历史地理信息可视化的研究多与信息系统共生，并成为系统研究和展示的重要部分。如实现家族成员迁徙路线可视化的“族谱地理信息系统”和采用地图动画、序列地图、时间轴地图、三维场景模拟等表达人物、事件、时间、地点等多要素时空信息的“三国历史地理信息系统”。目前，关于历史变化的可视化表达技术仍需进一步深入研究。\n4. 历史GIS的发展趋势 GIS被引入历史学和历史地理学以来，解决了一些传统历史和历史地理研究中无法解决的技术和空间分析问题，得出的研究成果不仅可以为部分历史问题作出基于空间视角的解释，也在一定程度上推动着信息化时代历史学和历史地理学研究范式的创新。GIS与历史相关研究的结合日趋紧密，随着数字人文和信息化、智慧化时代的推进，其未来的发展又会呈现出新的趋势。\n（1）构建历史资料的空间化和数字化体系，推动历史地理时空大数据的建设。\n随着历史GIS的发展，诸多不同历史时期和不同形式的历史文献会越来越多地进入历史GIS学者的视野。历史文献资料与GIS结合程度的加深，必将推动GIS研究中地理空间的时间范围扩展并不断向前延伸。历史GIS研究需要对大量不同领域、不同尺度、不同类型的历史资料进行空间化和数字化，诸如古地图、历史地图、遥感影像、地名志、考古遗址、图版照片、历代史书资料、郡县府志、家谱、游记、诗赋等，构建历史地理时空大数据。但历史地理信息和现代地理信息有着不同的特征，比如在时态、比例尺、精度等方面，历史资料具有不同于现代地理信息的表现手法。历史资料的特殊性和差异性决定了历史资料空间化和数字化工作的困难，诸如投影、误差、接边等突出和常见问题，始终困扰着历史GIS研究者。需要梳理历史学研究对历史资料空间化和数字化的需求特点，总结历史资料的空间化和数字化技术要求，形成一套历史资料空间化和数字化的技术体系，以推动历史地理时空大数据的建设。\n（2）建设历史地理空间框架，推动历史地理信息服务的普及。\n历史GIS发展过程中产生了大量的历史地理信息数字化产品，不同学者和研究机构也建设了不同专题的历史GIS数据库及系统平台。然而，随着历史GIS的进一步发展，不同来源、不同尺度、不同类型的历史地理信息数字化产品往往缺乏统一基准、无法实现共享等问题逐渐突显出来。建设历史地理空间框架，是解决这一问题的重要途径。历史地理空间框架可为所有与地理位置相关的历史地理信息数据及产品提供一个统一的时空定位基准，为定位、嵌入或配准各类图形、图像、文本、视频、音频信息提供一个时空多维载体，可实现多元数据的无缝连接和整合，保证历史地理空间数据的一致性、兼容性和可转换性，使用户能够按照地理坐标或空间位置集成、检索、展示所关心的历史时期的自然、社会、经济、环境等信息。历史地理空间框架建设的宗旨是向社会公众尤其是科研人员提供易获得且通用的历史地理信息服务。历史地理信息服务的普及不仅便于科研人员研究工作的开展，也有利于社会公众了解历史地理和历史文化信息，有利于增强文化自信，从而进一步实现历史GIS 的社会价值。\n（3）加强历史地理过程及模型研究，推动历史地理信息科学与技术学科体系的形成。\n目前，国内外历史GIS的研究主要集中于历史地理信息的时空格局，揭示历史地理时空过程及模型构建的研究相对较少。历史地理学是研究历史时期地理环境发展、变化及其规律的科学，主要任务是探明历史时期的地理过程并构建相应的地理过程模型，从而为面向未来的科学预测提供研究基础。历史地理过程模型是抽象地表示不同历史时期地理环境状态或其组成要素特征的实物模型或数学统计模型，是理解和预测不同尺度历史地理系统格局和过程的重要研究方法。但目前的历史GIS研究成果和历史地理学的专业需求之间还存在着较大的差距。因此，充分利用历史地理时空大数据，通过数据挖掘和多源数据融合，进行历史地理过程的模型构建、模拟及重现，是未来历史GIS研究的重要发展方向之一。历史地理过程及其模型构建的研究，将推动历史GIS由信息系统向信息科学的转换，推动历史地理信息科学和技术学科体系的形成。\n5. 结语 和GIS学科本身相比，历史GIS的起步较晚，发展也较为缓慢，但已在历史地理信息的数字化、数据模型、数据库建设和系统开发、空间分析及可视化等方面取得了显著的研究进展。GIS作为一种研究手段和思维方式，越来越多地参与到历史学和历史地理研究中，在继承传统定性描述方法的基础上，以多样化的研究方法将历史学和历史地理学带入了定量化、信息化和智慧化时代。\n得益于我国悠久历史长河中留存的大量历史文献古籍，国家层面的重视，以及历史学和历史地理学等学科领域发展的迫切需要，近年来我国的历史GIS研究取得了一定的成就，但也存在着较为明显的差距。怎样解决好历史地理数据不确定性问题？怎样充分发挥我国现存典籍多的优势？怎样促进历史地理学与GIS等新兴学科的融合发展？怎样通过构建历史地理空间框架做好历史地理信息服务的普及？如何通过历史地理过程模型的构建推动历史地理信息科学与技术学科体系的形成？这些问题的解决将成为促进我国历史GIS事业进一步发展的推动力。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-09-19-history-gis/","summary":"赵耀龙,巢子豪.历史GIS的研究现状和发展趋势[J].地球信息科学学报,2020,22(05):929-944.\n 赵耀龙，华南师范大学地理科学学院教授、博士生导师、副院长。研究方向为地理信息科学与技术、空间综合人文学与社会科学。 巢子豪，华南师范大学地理科学学院GIS专业博士研究生，研究方向为空间藏学。  摘要 历史地理信息系统（历史GIS）属于地理信息系统（GIS）与历史学相结合的交叉研究领域，它有机地集成了GIS的技术方法、地理学家的空间视角和历史学家的时间视角，量化历史时期的地理过程并构建相应的时空模型，为面向未来的科学预测提供研究基础。历史GIS兴起于20世纪90年代中期，给GIS学科、历史地理学和历史学都带来了新的研究机遇与活力，并表现出强劲的发展势头，但也存在一些尚待解决的问题。近年来，历史GIS向社会提供了日益丰富的历史地理信息服务，并逐渐跨越系统的技术层面，向着科学层面纵深发展。本文通过对国内外相关文献的梳理与分析，回顾了历史GIS产生的背景，从数字化、数据模型、数据库建设与系统开发、空间分析和可视化5个方面综述了国内外历史GIS的研究现状。最后，从历史资料的空间化与数字化、历史地理时空大数据、历史地理空间框架构建及历史地理信息服务、历史地理时空过程及模型构建、历史地理信息科学和技术学科体系的形成等角度展望了历史GIS的发展趋势，以期为历史GIS未来的发展提供新的研究思路。\n关键词: 历史GIS 研究现状 发展趋势 空间化 历史地理时空大数据 历史地理空间框架 历史地理信息服务 历史地理信息科学与技术\n1. 引言 地理学与历史学分属于对空间差异、时间分化进行研究的学科，而对历史的研究不可能完全脱离地理空间，通过侧重于对人类生活中有形环境的研究，地理学日益进入到历史学的研究领域，并产生了相应的交叉研究学科。历史地理学即是这样一门研究人类历史时期地理环境变化及其规律的学科。传统历史地理学的研究方法主要集中于历史文献资料的归纳和演绎，缺乏现代科学技术手段的应用，较难对统计计量科学意义上的规律进行探讨。随着信息化、智慧化技术的迅猛发展和地球信息科学的兴起，历史地理学在迎来发展机遇的同时，也面临着诸多挑战。如何推动传统历史地理学中研究方法与科学思维在新时代的发展与创新，便是亟待解决的问题之一。地理信息系统（GIS）应用现代数字化技术手段解决时间和空间问题，在信息化和智慧化浪潮中得到了快速发展，并广泛应用于诸多学科领域，包括历史学和地理学，历史GIS应运而生。历史GIS集合了GIS的技术特点、地理学家的空间视角和历史学家的时间视角，通过GIS的技术方法将空间和时间视角相结合，研究时空尺度上的变化模式。\n历史GIS常被应用于历史地理数据的管理、处理、空间分析、可视化表达等方面，为历史学研究提供了更加广泛的地理空间视角，并发挥出越来越重要的研究价值。①为历史学特别是历史地理文献中的描述性信息提供了空间化和定量化处理方法。②用于不同来源、不同类型、不同结构历史地理数据资源的有效整合，如书面文档、古地图、历史地图、现场调查结果、考古文物等。通过建立历史GIS专题数据库，实现不同历史地理数据在同一个时空框架内的综合管理，有助于历史研究的条理性与系统性。③将空间分析的概念引入历史研究，让历史学家们重新审视历史学中空间信息的价值和社会关系中空间要素的重要性，从历史事件的地理空间位置和时间角度出发，了解其时空演化过程和机制。④为传统历史研究提供了空间可视化的表达方法，通过动态地图、三维GIS(3D GIS)等手段生动直观地展现历史数据的空间变化过程。此外，历史GIS还促进了跨学科研究的发展，为来自历史学、地理学、测绘学、计算机科学与技术等领域的学者们提供了合作的空间；GIS与历史学的结合还可以发现传统方法难以发现的历史问题，促进历史学领域潜在信息的挖掘，并帮助检测传统数据的合理性与正确性。\n历史GIS的出现无疑为传统的历史地理研究注入了新的活力，也为GIS研究本身提供了广阔的拓展空间。然而，目前历史GIS研究中也存在一些尚待解决的问题。例如，特定历史地理数据的缺失或难以量化，导致GIS功能难以充分发挥；部分GIS的研究方法并不完全适用于历史问题的研究，需要根据具体问题进行修正；历史GIS中大量空间和属性数据的高精度、规范化处理，以及数据库建设的高难度和高成本也是必须面对的实际问题；研究人员的知识结构难以满足历史GIS的多学科知识背景需求，历史学者往往将GIS理解为绘制地图的软件，难以在研究中挖掘出深层的时空信息等。这些问题在一定程度上制约了历史学研究中GIS作用的发挥。另外，随着GIS逐步从信息系统的技术层面走向信息服务的科学层面，历史GIS于历史地理学而言也不再仅仅是技术的实现，有必要构建历史地理信息科学与技术的学科体系，并为社会提供日益丰富的历史地理信息服务。\n有学者从不同学科视角讨论了我国历史GIS在不同时期的发展态势。如潘威等于2012年回顾了GIS引入我国历史地理学界的10年发展历程；2018年，潘威提出了“数字人文”背景和大数据理念下我国历史地理信息化的应对策略；张萍回顾了GIS对中国历史研究的贡献和存在的问题。这些成果为历史GIS研究提供了重要参考。为全面借鉴国内外历史GIS的研究经验，本文在回顾历史GIS产生背景的基础上，从数字化、数据模型、数据库与系统开发、空间分析和可视化5个方面梳理了国内外历史GIS的发展历程和研究进展，提出历史GIS未来值得重点关注的发展趋势，以期推动历史GIS 的健康发展。\n2. 历史GIS产生的背景 2.1 国际上历史GIS 产生的背景 历史地理学界提出将研究对象的时间和空间属性相结合始于20世纪初期。英国历史地理学家Darby于1936年提出的“横剖面法”，以时间序列中多个地理空间剖面展示了空间随着时间发生的变化。20世纪50年代，历史地理学界兴起了“定量革命”，不再局限于定性描述的研究方法，而是强调对研究对象的定量分析。20世纪80年代，数据库技术开始与历史学研究相结合。同一时期，GIS技术得到飞速发展，并被引入历史研究领域，开始用以研究历史事件发展进程、区域经济开发、聚落成长变迁、人群地域特征、灾害或疫病的空间演进等。但该类研究更多关注的是地表状况的变迁，忽略了历史地理学关心的历史现象更迭背后所隐藏的原因等问题，并且历史文献在此类研究中并未被使用，因此，该类研究不能算作GIS应用于历史地理学领域的真正开端。\n学者们普遍认为历史GIS兴起于20世纪90年代中期，认为该领域的开山之作是1994年Goerke的《Coordinate for Historical Maps》一书，该书整理了1994 年于欧洲大学研究所举办的研讨会展示的文章。当时历史学正处于对GIS的初步了解阶段，历史GIS发展较为缓慢。这一观点在Ogborn的文章中得到了体现，该文综述了1997年历史地理领域的研究进展，但并未提及历史GIS。之后，由于GIS在历史研究领域的潜力，历史GIS经历了一个相对快速发展时期。1998年和1999年社会科学历史协会组织的历史GIS专题会议中提到了学者们热情地参与到GIS等新方法的使用中，参与者们认为新工具的使用同样意味着创新。Holdsworth在其2002年的文章中提及历史GIS，认为其是历史地理学领域一种新兴的发展趋势。Baker将历史GIS描述为历史地理学领域一项“有趣而又富有挑战的发展”。\n对历史GIS更多、更全面的研究主要体现在2000年以后，这一时期出现了许多以历史GIS 为研究重点的出版物。Knowles在2000—2008年出版了历史GIS相关专著2部和期刊论文2篇，包括于2002年的《Past Place: GIS for History》和2008年与Hillier一同出版的《Placing history: how maps，spatial data， and GIS are changing historical scho larship》。Gregory在2003年发表了有关历史数据服务的《Guide to Practice in GIS》，并于2007 年和Ell一同出版了《Historical GIS: technolo gies， methodologies and scholarship》一书，进一步普及了历史GIS的理念、技术及相关应用。von Lünen和Travis于2013年出版了著作《GIS and History:Epistemologies，Reflections，and Conside rations》该书汇集了对多个学术领域专家的采访，分享了他们多年的经验和历史学者们为何应该接受GIS的思考。期刊方面，《Social Science History》在2000年的第24卷第3期中首先推出了历史GIS的专刊。《History and Computing》和《Historical Geography》也分别在2003年第13卷第1期和2005年第33卷中推出了历史GIS专刊。诸多刊物与文章的问世标志着历史GIS进入了快速发展时期。历史地理学者Knowles承认历史GIS的巨大进步，但也指出历史GIS距最初的期望值还相距甚远。并且GIS为历史研究带来了哪些东西，其局限又有哪些，这些都没有形成普遍可接受的观点。","title":"转载 | 历史GIS的研究现状和发展趋势"},{"content":"报名信息   时间：2022.10.03 ~ 2022.10.04\n  地点: 小鹅通平台（线上直播）\n  报名咨询: 17816181460（同微信）（汪老师）\n  报名费：2500元\n 单位：杭州国商智库信息技术服务有限公司 开户银行： 中国银行杭州大学城支行 银行账户：6232636200100260588    简介 在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！\n在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。大数据时代，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：\n 网络爬虫 解决 如何从网络世界中高效地 采集数据？ 文本分析 解决 如何从杂乱的文本数据中 构建指标？  为方便大家感受到文本数据的魅力，按照是否采用某项技术(爬虫、词频、词袋、w2v建词典、w2v认知变迁)，从五个维度标记代表性的7篇论文。\n   文献 爬虫 定性 词频 词袋 W2V建词典 W2V认知变迁     王伟 , 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性\u0026ndash;基于 Kickstarter 的实证研究. 管理世界, (5), pp.81-98. Y Y Y      语言具体性如何影响顾客满意度\nPackard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” Journal of Consumer Research 47, no. 5 (2021): 787-806.   Y      Wang, Quan, Beibei Li, and Param Vir Singh. \u0026ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.\u0026rdquo; Information Systems Research 29, no. 2 (2018): 273-291. Y   Y     文本相似度\nCohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. The Journal of Finance, 75(3), pp.1371-1415.    Y     胡楠, 薛付婧 and 王昊楠, 2021. 管理者短视主义影响企业长期投资吗———基于文本分析和机器学习. 管理世界, 37(5), pp.139-156.   Y  Y    Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, The Review of Financial Studies, 2020    Y Y    女性就职高管改变组织内性别偏见\nLawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. \u0026ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.\u0026rdquo; Proceedings of the National Academy of Sciences 119, no. 9 (2022): e2026443119.      Y    主讲老师 大邓，哈尔滨工业大学(HIT)管理学院信息管理系统方向在读博士。在多所大学分享数据采集和文本分析。运营公众号：大邓和他的Python，主要分享Python、爬虫、文本分析、机器学习等内容。\n一、入门语法  Python跟英语一样是一门语言 数据类型之字符串 数据类型之列表元组集合 数据类型之字典 数据类型之布尔值、None 逻辑语句(if\u0026amp;for\u0026amp;tryexcept) 列表推导式 理解函数 常用的内置函数 os路径库 内置库csv文件库 常见错误汇总  二、数据采集  网络爬虫原理 寻找网址规律 获取网页-requests库 pyquery库解析html网页 案例: 豆瓣小说 json库解析json网页 案例: 豆瓣电影 案例: 微博 案例: 文件下载 案例: 上市公司定期报告pdf批量下载 区分动态网站与静态网站  三、文本初识  从信息传播视角重新认识文本 读取各类文件中的数据 案例: 识别图片中的文本 数据清洗re库 案例: 将多个数据文件汇总至一个csv文件 案例: 中文jieba分词、词频统计、制作词云图 案例: 使用共现(word2vec)法扩展情感词典 案例: 使用词典做情感分析(无权重) 案例: 数据分析pandas库快速入门 案例: 使用pandas对excel中的文本进行情感分析  四、文本进阶  文本分析与机器学习 特征工程-认识词袋法、one-hot、Tf-Idf、word2vec 将文档转为机器可处理的向量 案例: 使用情感词典和tf-idf做情感分析（有权重） 案例: 在线评论文本分类 案例: 使用文本相似性识别变化(政策连续性) 案例: Kmeans聚类算法、LDA话题模型 文本中的人类记忆(认知) 如何测量人类认知偏见(刻板印象) 案例: 词向量模型的使用方法-豆瓣影评 文本分析在经管社科领域中的应用概述  参考文献 [1]沈艳, 陈赟 and 黄卓, 2019. 文本大数据分析在经济学和金融学中的应用: 一个文献综述. *经济学 (季刊)*, *18*(4), pp.1153-1186. [2]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html [3]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.*管理世界*.2016;5:81-98. [4]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21. [5]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020 [6]Loughran T, McDonald B. Textual analysis in accounting and finance: A survey[J]. *Journal of Accounting Research*, 2016, 54(4): 1187-1230. Author links open overlay panelComputational socioeconomics [7]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026#34;Uniting the tribes: Using text for marketing insight.\u0026#34; *Journal of Marketing* 84, no. 1 (2020): 1-25. [8]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. \u0026#34;A review of best practice recommendations for text analysis in R (and a user-friendly app).\u0026#34; *Journal of Business and Psychology* 33, no. 4 (2018): 445-459. [9]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. \u0026#34;Lazy prices.\u0026#34; *The Journal of Finance* 75, no. 3 (2020): 1371-1415. [10]孟庆斌, 杨俊华, 鲁冰. 管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J]. *中国工业经济*, 2017 (12): 132-150. [11]Wang, Quan, Beibei Li, and Param Vir Singh. \u0026#34;Copycats vs. Original Mobile Apps: A Machine Learning Copycat-Detection Method and Empirical Analysis.\u0026#34; *Information Systems Research* 29.2 (2018): 273-291. [12]Hoberg, Gerard, and Gordon Phillips. 2016, Text-based network industries and endogenous product differentiation,?*Journal of Political Economy* 124, 1423-1465 [13]Loughran, Tim, and Bill McDonald. \u0026#34;When is a liability not a liability? Textual analysis, dictionaries, and 10‐Ks.\u0026#34; *The Journal of Finance* 66, no. 1 (2011): 35-65. [14]Fairclough, Norman. 2003. Analysing discourse: Textual analysis for social research (Psychology Press) [15]Grimmer, Justin, and Brandon M Stewart. 2013, Text as data: The promise and pitfalls of automatic content analysis methods for political texts, *Political analysis*21, 267-297. [16]Markowitz, D. M., \u0026amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18). [17]Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” Journal of Consumer Research 47, no. 5 (2021): 787-806. [18]Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156. [19]Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. \u0026#34;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.\u0026#34; *Proceedings of the National Academy of Sciences* 119, no. 9 (2022): e2026443119. ","permalink":"/blog/2022-09-19-text-mining-in-ms-workshop/","summary":"报名信息   时间：2022.10.03 ~ 2022.10.04\n  地点: 小鹅通平台（线上直播）\n  报名咨询: 17816181460（同微信）（汪老师）\n  报名费：2500元\n 单位：杭州国商智库信息技术服务有限公司 开户银行： 中国银行杭州大学城支行 银行账户：6232636200100260588    简介 在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！\n在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。大数据时代，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：\n 网络爬虫 解决 如何从网络世界中高效地 采集数据？ 文本分析 解决 如何从杂乱的文本数据中 构建指标？  为方便大家感受到文本数据的魅力，按照是否采用某项技术(爬虫、词频、词袋、w2v建词典、w2v认知变迁)，从五个维度标记代表性的7篇论文。\n   文献 爬虫 定性 词频 词袋 W2V建词典 W2V认知变迁     王伟 , 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性\u0026ndash;基于 Kickstarter 的实证研究. 管理世界, (5), pp.81-98. Y Y Y      语言具体性如何影响顾客满意度","title":"国庆直播 | Python实证指标与文本分析"},{"content":"妙言是一款有望取代Typora的markdown编辑器，使用体验特别美妙，遗憾的是目前仅支持Mac平台。\n特点  🏂 妙：纯本地使用、安全、语法高亮、黑暗模式、源文件保存、国际化、演示模式、PPT 模式、单独编辑模式、文档自动排版、文档导出、内部跳转、图床、LaTeX、Mermaid、PlantUML、Markmap 脑图 🎊 美：极简的设计风格，文件夹 + 文件列表 + 编辑器方式 3 列模式 🚄 快：使用 Swift5 原生开发，相比 Web 套壳方式性能体验好 🥛 简：很轻巧，纯编辑器输入体验，众多快捷键助你快人一步  首次使用 从 GitHub Releases 中 下载 最新的 dmg 安装包。\n可以在 iCloud 或根目录下创建一个 MiaoYan 的文件夹，打开妙言的设置，将默认存储地址修改成这个。 点击妙言左上角新增文件夹的图标，创建好自己的文档分类文件夹，就可以开始使用了。 同样假如你不习惯默认的字体，可以在设置中修改成其他的正常字体。\n快捷键 窗口操作\n command + 1：收起展开目录 command + 2：收起展开文档列表 command + 3：切换编辑和预览 command + 4：切换到演示模式 command + option + m：全局唤起/隐藏妙言  文档操作\n command + n：新建文档 command + r：重命名文档 command + d：复制文档 command + o：单独打开文档 command + delete：删除文档 command + shift + n：新建文件夹 command + shift + l：自动排版 command + option + r：在 Finder 中显示 command + option + i：显示字数等文档属性 command + option + p：启动妙言 PPT 预览  🏂 此外还有很多快捷键 👆🏻 👇🏻 👈🏻 👉🏻 等着爱折腾的你去寻找~\n妙言 PPT  新朋友默认初始化会生成模版，如果是老朋友需升级到 1.0，可以 Copy 此文件 到妙言玩一玩。 执行 command + option + p 可以启动妙言 PPT 预览，同时你也可以选中文档点击右键，选择 妙言 PPT 打开。 只有在有 --- 分隔符标志的文档中，才可启用 PPT 模式，演示过程中你可以 回车键 预览演讲大纲，ESC 键可退出 PPT 模式。 你可以使用 HTML 来自定义效果，更多复杂的用法可以参考 Reveal 文档。  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-09-14-miaoyan-markdown/","summary":"妙言是一款有望取代Typora的markdown编辑器，使用体验特别美妙，遗憾的是目前仅支持Mac平台。\n特点  🏂 妙：纯本地使用、安全、语法高亮、黑暗模式、源文件保存、国际化、演示模式、PPT 模式、单独编辑模式、文档自动排版、文档导出、内部跳转、图床、LaTeX、Mermaid、PlantUML、Markmap 脑图 🎊 美：极简的设计风格，文件夹 + 文件列表 + 编辑器方式 3 列模式 🚄 快：使用 Swift5 原生开发，相比 Web 套壳方式性能体验好 🥛 简：很轻巧，纯编辑器输入体验，众多快捷键助你快人一步  首次使用 从 GitHub Releases 中 下载 最新的 dmg 安装包。\n可以在 iCloud 或根目录下创建一个 MiaoYan 的文件夹，打开妙言的设置，将默认存储地址修改成这个。 点击妙言左上角新增文件夹的图标，创建好自己的文档分类文件夹，就可以开始使用了。 同样假如你不习惯默认的字体，可以在设置中修改成其他的正常字体。\n快捷键 窗口操作\n command + 1：收起展开目录 command + 2：收起展开文档列表 command + 3：切换编辑和预览 command + 4：切换到演示模式 command + option + m：全局唤起/隐藏妙言  文档操作\n command + n：新建文档 command + r：重命名文档 command + d：复制文档 command + o：单独打开文档 command + delete：删除文档 command + shift + n：新建文件夹 command + shift + l：自动排版 command + option + r：在 Finder 中显示 command + option + i：显示字数等文档属性 command + option + p：启动妙言 PPT 预览  🏂 此外还有很多快捷键 👆🏻 👇🏻 👈🏻 👉🏻 等着爱折腾的你去寻找~","title":"妙言 | 轻灵的Markdown笔记本伴你写出妙言"},{"content":"Slides  \n背景 在经管研究中，往往会涉及很多文本数据的编码。但是做研究面临两个问题:\n难题1- 数据量大 量太大，以至于废人力所能及。\n时代发展，体现在数据上的特点就是数据大爆炸，过去做经管研究，使用访谈等研究方法，收录的文本内容，规模大多停留在M级。但是现在大数据时代，研究对象相关的文本数据，G级的数据量也是很常见的。\n难题2- 格式乱 信息存储技术发展，有应用不同场景的不同数据存储格式。数据可能是pdf、txt、docx，也可能是音频、视频等转录的文件。如果快捷整理，这也是个难点。\n难题3-难编码 数据量少，可以人工阅读对数据进行理解和编码。但是当数据量大到无法处理的级别后，选择何种算法、各种算法技术的优缺点如何把握，对经管学者也是一个需要攻克的的技术难题。\n难度大，但因为文本涉及的主体错综复杂，千丝万缕，所以可以研究很多对象。如个人、组织、社会之间的交互。\n\n编码解码理论 斯图亚特·霍尔在《电视话语的编码和解码》提出 『编码-解码理论』。该理论形成于70年代冷战时期，冷战中不两大阵营为了维护各自的社会稳定，为了在意识形态宣传中取胜，都在宣传工作中投入了重金。\n当时的宣传工具是单向的广播模式，媒体作为统治阶级的喉舌，要将统治阶级的偏好、价值观等进行加工，生产相应意识形态内容。\n而普罗大众，作为内容的接受者， 一成长于该特定意识形态的社会，同时又有一定的自我意识，所以对于一个宣传内容可能会有三种反应，表里都认同、表认同里不认同、表里都不认同。\n使用文本想清楚两个问题 - How text reflects its Sender？ - How text impacts its Receiver？ 使用文本明晰三个角度 我做的研究使用的文本数据，涉及哪些角色、作用力方向、感兴趣的内容。\n 角色: Sender or Receiver 方向: Reflect or Impact 内容: Sender的意识(认知、偏好、\u0026hellip;) vs Receiver的意识(认知、偏好、\u0026hellip;)  下面是经管领域研究部分汇总，每个学者根据自己学科研究对象，应该能在4*4的矩阵中找到自己对应的位置\n Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.\u0026rdquo; Journal of Marketing 84, no. 1 (2020): 1-25.\n \n人工编码与机器编码 做研究需要有干净的数据做实证分析，最为理想的是表数据，例如excel文件，每一行代表一条记录，每一列代表一个字段。编码的作用就是将非机构化的、脏乱的数据整理为干净整洁的表数据。\n要明确编码方法的优点和缺点，在合理的适用范围使用。对于文本数据的编码，需要理解人工和机器两种编码方式的优缺点\n    分析方法 优点 缺点     人工编码 质性（扎根） 少量数据，深刻洞见。 难以应对大数据；\n编码标准不统一；   机器编码 词频、向量相似度、向量距离 标准如一;\n适合大规模文本挖掘； 需要破坏文本的结构，\n丧失了部分信息量    \n机器编码-将文本转为数字或向量   符号法(每个词对应一个数字)\n 词典(词频)法 词袋法、TF-IDF    词嵌入(每个词对应一个向量)\n  符号法算法假设词语彼此是语义不相关的，目的是把 文本 转为某个数字或向量。\n而词嵌入算法假设不同的词语是由n维个语义组成的线性组合，目的是把 词语 转为向量。\n符号法 符号法就是数某个词或某类词的出现次数(或占比)。符合法是计算机NLP领域的专业叫法，在经管社科领域，最常见的文本分析软件LIWC其实也是符号法。而LIWC全(Linguistic Inquiry and Word Count，即语义查询与词频统计。\n符号法的应用    概念 测量方法     认真(努力) 测量文本中词语的个数   情感 使用情感词典，统计文本中正面词占比   可读性 文本中高难度(或专业性)词占比   客观性 文本中某个值的方差，如情感\n- A产品不错， 包装破损， 态度很好， 综合还是推荐大家购买! [5, 1, 5, 4]\n- B产品垃圾，使用垃圾， 包装破损， 差评!! [1, 1, 1, 1]\nA的方差更大，更客观   相似性(政策稳定性) cosine(text_vector1, text_vector2)   \u0026hellip; \u0026hellip;    词嵌入 词嵌入技术有 Word2Vec、Glove，这类技术是挖掘出每个词的上下文语境，通俗的说法就是让计算机，对同样的文章数据，做千万次、上亿次完形填空。这样每个词语都有独特的上下文语义，并以n维向量形式表示，所以词嵌入也可以称之为词向量。\n向量模型有近义词相近、概念类似的平行两个特点。分别举几个例子，方便大家理解。\n语义空间是n维，为了便于理解，将其压缩至二维空间。中学的向量大家都比较熟悉，在二维坐标中空间中，两个点的连线可以组成新的向量，相同的向量是平行的。\n而在下图的2维语义空间中，good、best语义更接近，所以空间距离更近。同理bad、worst更近。\n而vector(good, best)、vector(bad, worst)这两个向量均表示原形-\u0026gt;最高级, 语义向量会近似平行。\n同理， vector(good, bad)、 vector(best, worst)两个向量表示 好-\u0026gt;差，语义向量也会近似平行。\n词嵌入与认知 刚刚词嵌入的语义空间中的几个例子，其实就体现了语言的记忆。语义记录了使用该语言的人的记忆。不同的组织，对于同一种概念，会有不同的偏好。例如， Nature2022使用大规模语料数据训练出的词向量，发现语言中残存着人类的某些认知记忆。\n通过构建概念词组对儿，在空间中投影，就可以挖掘出词语的在该概念中的分值。例如，使用\n SMALL = [small, tiny, little\u0026hellip;] BIG = [big, mega, large\u0026hellip;]  每个词都是一个n维的向量，SMALL或BIG都能计算出一个均值向量。大家记得中学的向量投影不，Nature2022就使用这个朴素的方法测量每个动物名称所蕴含的人类尺寸认知。\n Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Nature Human Behaviour, pp.1-13.\n 技术对比 这里做个表格对比，大家自己感受下三种技术的异同。\n   技术 技术 维度类比 任务 例子     符号法-字典（词频） 数个数 原子 统计每句话里的名词个数 sent_num1 = 2\nsent_num2 = 1   符号法-词袋 bag of words\none-hot\nTf-idf 分子 转化为词向量, 计算两个句子相似度。 vec1 = [1, 1, 1, 1, 1, 0]\nvec2 = [0, 1, 0, 1, 0, 1]\nsimilarity = cosine(vec1, vec2)   词嵌入 word2vec、\nglove等 中子、质子、电子 词语相似度。(语义上大小相近，方向相反; 态度、偏见) mom = [0.2, 0.7, 0.1]dad = [0.3, 0.5, -0.2]    \n经管-文本分析-文献 在这里我把技术细分为词频、词袋、w2v建词典、w2v认知变迁四个维度，整理了经管7篇论文。大家可以阅读这7篇论文，掌握文本分析的应用场景。\n   文献 定性 词频 词袋 W2V建词典 W2V认知变迁     王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性\u0026ndash;基于 Kickstarter 的实证研究. 管理世界, (5), pp.81-98. Y Y      语言具体性如何影响顾客满意度\nPackard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” Journal of Consumer Research 47, no. 5 (2021): 787-806.  Y      Wang, Quan, Beibei Li, and Param Vir Singh. \u0026ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.\u0026rdquo; Information Systems Research 29, no. 2 (2018): 273-291.   Y     文本相似度\nCohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. The Journal of Finance, 75(3), pp.1371-1415.   Y     胡楠, 薛付婧 and 王昊楠, 2021. 管理者短视主义影响企业长期投资吗———基于文本分析和机器学习. 管理世界, 37(5), pp.139-156.  Y  Y    Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, The Review of Financial Studies, 2020   Y Y    女性就职高管改变组织内性别偏见\nLawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. \u0026ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.\u0026rdquo; Proceedings of the National Academy of Sciences 119, no. 9 (2022): e2026443119.     Y   使用词嵌入技术，量化近百年以来性别和族群的刻板印象\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. \u0026ldquo;Word embeddings quantify 100 years of gender and ethnic stereotypes.\u0026rdquo; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.     Y    \n案例 案例1-众筹语言风格 王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性\u0026ndash;基于 Kickstarter 的实证研究. 管理世界, (5), pp.81-98.\n 众筹融资效果决定着众筹平台的兴衰。 众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。 而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。 不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。 首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。\n然后，借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。\n最后，建立语言说服风格对项目筹资影响的计量模型，并对 Kickstarter 平台上的 128345 个项目进行实证分析。 总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。\n 案例2 山寨 vs 原创 Wang, Quan, Beibei Li, and Param Vir Singh. \u0026ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.\u0026rdquo; Information Systems Research 29, no. 2 (2018): 273-291.\n 进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原创app或模仿app，可检测两种模仿者的方法：欺骗性和非欺骗性。\n根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的5,141个开发人员的10,100个动作游戏应用程序样本中，模仿app对原创app需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原创app的需求。\n结果表明，从总体上讲，模仿app对原创app需求的影响在统计上是微不足道的。我们的研究通过提供一种识别模仿app的方法，并提供模仿app对原创app需求影响的证据，为越来越多的移动应用消费文献做出了贡献。\n 案例3 Lazy prices文本相似性 Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. The Journal of Finance, 75(3), pp.1371-1415.\n 之前的研究认为，尽管投资者一次对包含重大变化的财务报表的发布作出了迅时反应，但随着时间的流逝，这种公告作用是会减弱的(Brown and Tucker, 2011 and Feldman et al., 2010)。这表示10-K报告会随着时间推移，信息价值大打折扣。尽管我们复现了这个事实，即与常规文件的变更没有重大的公告效应，但我们认为，前人的研究忽略了更重要部分(如MD\u0026amp;A)对对资产价格的影响。\n  确切的说，并不是报告的披露效应的信息价值变低了，而是投资者越来越难以发现报告中微妙的信息变化， 比如因为报告变得越来越冗杂。投资者只有看到某些新闻后，才会逐渐意识到之前公司报告内容变化的的真正价值。\n使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对报告进行积极更改时，这种行为蕴含着公司未来运营的重要信号。\n财务报告的语言和结构的变化也对公司的未来收益产生重大影响：做空\u0026quot;变化\u0026quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。\n 案例3-女性就职高管改变组织内性别刻板印象 PNAS2022 Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. \u0026ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.\u0026rdquo; Proceedings of the National Academy of Sciences 119, no. 9 (2022): e2026443119.\n 女性在领导职位上的代表性仍然不足。这种代表性不足至少部分是由将男性与成就导向的代理特征（例如，自信和果断）联系起来的性别刻板印象驱动的。这些刻板印象在语言中得到表达和延续，女性被描述的方式比男性少。目前的研究表明，任命女性担任高层管理人员可以减轻这些以语言表达的根深蒂固的刻板印象。我们使用自然语言处理技术分析了超过 43,000 份包含 12.3 亿字的文档，发现聘用女性首席执行官和董事会成员与组织使用语言的变化有关，因此女性的语义变得更类似于代理的语义。换句话说，雇用女性担任领导职务有助于将女性与对领导成功至关重要的特征联系起来。重要的是，我们的研究结果表明，通过增加女性代表来改变组织语言可能会为女性提供摆脱双重束缚的途径：当女性领导人被任命担任权力职位时，女性与代理的积极方面（例如，独立和自信）在语言上，不以减少与社区的联系（例如，善良和关怀）为代价。总而言之，我们的研究结果表明，女性代表不仅是目的，而且是系统地改变阴险的性别刻板印象并克服女性被认为是有能力或可爱的权衡的一种手段。\n本文使用的词向量， 刻画研究对象的文化认知，是依对象依时间而变化的。\n 案例4- 使用词嵌入技术，量化近百年以来性别和族群的刻板印象 PNAS2018 {{ \u0026lt; bilibili BV1b4411X7i1 \u0026gt;}}\n关于合作与交流 如果正在推进的项目，需要用到文本分析，欢迎交流与合作 ~\n可加微信372335839， 备注【姓名-学校-专业】, 说明来意。如果想系统获取技术细节，课程 付费视频课 | Python实证指标构建与文本分析 内都有的技术代码和讲解，欢迎了解。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-09-08-dufe-text-mining-in-ms/","summary":"Slides  \n背景 在经管研究中，往往会涉及很多文本数据的编码。但是做研究面临两个问题:\n难题1- 数据量大 量太大，以至于废人力所能及。\n时代发展，体现在数据上的特点就是数据大爆炸，过去做经管研究，使用访谈等研究方法，收录的文本内容，规模大多停留在M级。但是现在大数据时代，研究对象相关的文本数据，G级的数据量也是很常见的。\n难题2- 格式乱 信息存储技术发展，有应用不同场景的不同数据存储格式。数据可能是pdf、txt、docx，也可能是音频、视频等转录的文件。如果快捷整理，这也是个难点。\n难题3-难编码 数据量少，可以人工阅读对数据进行理解和编码。但是当数据量大到无法处理的级别后，选择何种算法、各种算法技术的优缺点如何把握，对经管学者也是一个需要攻克的的技术难题。\n难度大，但因为文本涉及的主体错综复杂，千丝万缕，所以可以研究很多对象。如个人、组织、社会之间的交互。\n\n编码解码理论 斯图亚特·霍尔在《电视话语的编码和解码》提出 『编码-解码理论』。该理论形成于70年代冷战时期，冷战中不两大阵营为了维护各自的社会稳定，为了在意识形态宣传中取胜，都在宣传工作中投入了重金。\n当时的宣传工具是单向的广播模式，媒体作为统治阶级的喉舌，要将统治阶级的偏好、价值观等进行加工，生产相应意识形态内容。\n而普罗大众，作为内容的接受者， 一成长于该特定意识形态的社会，同时又有一定的自我意识，所以对于一个宣传内容可能会有三种反应，表里都认同、表认同里不认同、表里都不认同。\n使用文本想清楚两个问题 - How text reflects its Sender？ - How text impacts its Receiver？ 使用文本明晰三个角度 我做的研究使用的文本数据，涉及哪些角色、作用力方向、感兴趣的内容。\n 角色: Sender or Receiver 方向: Reflect or Impact 内容: Sender的意识(认知、偏好、\u0026hellip;) vs Receiver的意识(认知、偏好、\u0026hellip;)  下面是经管领域研究部分汇总，每个学者根据自己学科研究对象，应该能在4*4的矩阵中找到自己对应的位置\n Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.","title":"视频分享 | 文本分析在经管研究中的应用"},{"content":" 颠覆式创新是一个很火的概念，在创新创业、科学学等研究中，每个专利、论文的正文中都会引用关系，而引用关系会构成一个引用网络。\n那么创新如何从网络形态进行区分，如何计算网络节点的创新程度，本文列举两篇与此相关的论文，分别是 Management science 和 Science 。\n\n文献摘要 Funk, Russell J., and Jason Owen-Smith. “A dynamic network measure of technological change.” Management science 63, no. 3 (2017): 791-817.\n该文使用网络分析方法研究技术变革，论文认为 颠覆性的新发明，通过将发明者的注意力转移到或远离这些发明所依赖的知识，来重塑相互关联的技术网络。即更广的视野或更久远的视角，往往有利于颠覆性创新的产生。基于该思路，本文开发了新发明的颠覆性与否的计算指标cdindex。我们将这些指标应用于大学研究商业化的分析，并发现 联邦研究资金推动校园产生颠覆性创新，而商业联系会有利于巩固现状的创新。通过量化新技术，我们提出的指数允许基于专利的创新研究捕捉概念上重要的现象， 这些现象无法通过既定措施检测到。该测量方法提供了支持创新、创业、技术战略、科学政策和社会网络理论研究的理论发展的经验见解。\n Abstract: This article outlines a network approach to the study of technological change. We propose that new inventions reshape networks of interlinked technologies by shifting inventors’ attention to or away from the knowledge on which those inventions build. Using this approach, we develop novel indexes of the extent to which a new invention consolidates or destabilizes existing technology streams. We apply these indexes in analyses of university research commercialization and ﬁnd that, although federal research funding pushes campuses to create inventions that are more destabilizing, deeper commercial ties lead them to produce technologies that consolidate the status quo. By quantifying the eﬀects that new technologies have on their predecessors, the indexes we propose allow patent-based studies of innovation to capture conceptually important phenomena that are not detectable with established measures. The measurement approach presented here oﬀers empirical insights that support theoretical development in studies of innovation, entrepreneurship, technology strategy, science policy, and social network theory.\n \nWu, Lingfei, Dashun Wang, and James A. Evans. “Large teams develop and small teams disrupt science and technology.” Nature 566, no. 7744 (2019): 378-382.\n当今科学和技术最普遍的趋势之一是各个领域的大型团队的增长，因为孤独的研究人员和小型团队的流行程度正在减少 。团队规模的增加归因于科学活动的专业化、通信技术的改进 或需要跨学科解决方案的现代问题的复杂性。团队规模的这种转变引发了一个问题，即大团队所产生的科技特征是否以及如何不同于小团队。分析了 1954-2014 年期间超过 6500 万篇论文、专利和软件产品，证明在此期间，较小的团队倾向于将拉长到更大的时间尺度，借鉴过去，用新的想法和机会来颠覆科学和技术；而较大的团队倾向于聚焦于当前流行的，完善当前现有的。不论团队大小，均对于蓬勃发展的科学技术生态至关重要，并表明，为实现这一目标，科学政策应旨在支持团队规模的多样性。\n Abstract: One of the most universal trends in science and technology today is the growth of large teams in all areas, as solitary researchers and small teams diminish in prevalence. Increases in team size have been attributed to the specialization of scientific activities, improvements in communication technology, or the complexity of modern problems that require interdisciplinary solutions.This shift in team size raises the question of whether and how the character of the science and technology produced by large teams differs from that of small teams. Here we analyse more than 65 million papers, patents and software products that span the period 1954–2014, and demonstrate that across this period smaller teams have tended to disrupt science and technology with new ideas and opportunities, whereas larger teams have tended to develop existing ones. Work from larger teams builds on morerecent and popular developments, and attention to their work comes immediately. By contrast, contributions by smaller teams search more deeply into the past, are viewed as disruptive to science and technology and succeed further into the future—if at all. Observed differences between small and large teams are magnified for higherimpact work, with small teams known for disruptive work and large teams for developing work. Differences in topic and research design account for a small part of the relationship between team size and disruption; most of the effect occurs at the level of the individual, as people move between smaller and larger teams. These results demonstrate that both small and large teams are essential to a flourishing ecology of science and technology, and suggest that, to achieve this, science policies should aim to support a diversity of team sizes.\n \n 算法对比 我没阅读两篇论文，仅就颠覆性与否的计算方法和图例，感觉算法实现差不多。\n上图为2017年Management Science的插图\n \n上图为2019年Nature的插图\n \n 代码数据 下面分别为Management2017和Nature2019的主页，均含数据和代码。\n\n\n\n\n 算法实现 按照时间优先原则，本文就只分享Management2017论文作者Funk, Russell开源了cdindex库 (开发语言C和Python) ，安装\n\npip3 install cdindex 将Management2017 cdindex算法图 标注为如下图， 下图中左右两个网络节点是相同的，只需构造一套节点，两套边数据即可完成实验。\n\n我们就直接上代码\nimport cdindex import datetime #节点，理解为专利号或者论文doi号；同时节点有先后时间属性 vertices = [{\u0026quot;name\u0026quot;: \u0026quot;x1\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1990, 1, 1)}, {\u0026quot;name\u0026quot;: \u0026quot;x2\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1990, 1, 1)}, {\u0026quot;name\u0026quot;: \u0026quot;x3\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1990, 1, 1)}, {\u0026quot;name\u0026quot;: \u0026quot;x4\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1990, 1, 1)}, {\u0026quot;name\u0026quot;: \u0026quot;y\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1991, 1, 1)}, {\u0026quot;name\u0026quot;: \u0026quot;z1\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1995, 1, 1)}, {\u0026quot;name\u0026quot;: \u0026quot;z2\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1995, 1, 1)}, {\u0026quot;name\u0026quot;: \u0026quot;z3\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1995, 1, 1)}, {\u0026quot;name\u0026quot;: \u0026quot;z4\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1995, 1, 1)}, {\u0026quot;name\u0026quot;: \u0026quot;z5\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1995, 1, 1)}, {\u0026quot;name\u0026quot;: \u0026quot;z6\u0026quot;, \u0026quot;time\u0026quot;: datetime.datetime(1995, 1, 1)}] #edges_1边关系 #edges_1中的y为颠覆型 edges_1 = [{\u0026quot;source\u0026quot;: \u0026quot;z1\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z2\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z3\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z4\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z5\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z6\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;y\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x1\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;y\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x2\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;y\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x3\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;y\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x4\u0026quot;}] #edges_2边关系 #edges_2中的y为巩固型 edges_2 = [{\u0026quot;source\u0026quot;: \u0026quot;z1\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z2\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z3\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z4\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z5\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z6\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;y\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;y\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x1\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;y\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x2\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;y\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x3\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;y\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x4\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z1\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x1\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z2\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x1\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z3\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x2\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z4\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x3\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z5\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x3\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z5\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x4\u0026quot;}, {\u0026quot;source\u0026quot;: \u0026quot;z6\u0026quot;, \u0026quot;target\u0026quot;: \u0026quot;x4\u0026quot;}] # 构建两个网络 graph1 = cdindex.Graph() #颠覆型 graph2 = cdindex.Graph() #发展型 # 添加节点 for vertex in vertices: graph1.add_vertex(vertex[\u0026quot;name\u0026quot;], cdindex.timestamp_from_datetime(vertex[\u0026quot;time\u0026quot;])) graph2.add_vertex(vertex[\u0026quot;name\u0026quot;], cdindex.timestamp_from_datetime(vertex[\u0026quot;time\u0026quot;])) # 添加引用关系 for edge in edges_1: graph1.add_edge(edge[\u0026quot;source\u0026quot;], edge[\u0026quot;target\u0026quot;]) for edge in edges_2: graph2.add_edge(edge[\u0026quot;source\u0026quot;], edge[\u0026quot;target\u0026quot;]) #y研究发布后1825天内，引用y的论文(专利)列入网络。 t_delta = int(datetime.timedelta(days=1825).total_seconds()) #计算cdindex得分 score1 = graph1.cdindex(\u0026quot;y\u0026quot;, t_delta) score2 = graph2.cdindex(\u0026quot;y\u0026quot;, t_delta) print(\u0026#39;左侧-网络中的y节点的cdinex得分: {}, 节点y 为颠覆性创新\u0026#39;.format(score1)) ## 左侧-网络中的y节点的cdinex得分: 1.0, 节点y 为颠覆性创新 \nprint(\u0026#39;右侧-网络中的y节点的cdinex得分: {}, 节点y 为发展性创新\u0026#39;.format(score2)) ## 右侧-网络中的y节点的cdinex得分: -1.0, 节点y 为发展性创新 \n cdindex 对比Python的结果，与论文计算过程，完全一致。cdindex内部实现我不太熟悉，如果想了解cdindex内部实现，可前往 https://github.com/russellfunk/cdindex 阅读cdindex库的源码。  ","permalink":"/blog/2022-09-07-management-science-disrupt-science-and-technology/","summary":"颠覆式创新是一个很火的概念，在创新创业、科学学等研究中，每个专利、论文的正文中都会引用关系，而引用关系会构成一个引用网络。\n那么创新如何从网络形态进行区分，如何计算网络节点的创新程度，本文列举两篇与此相关的论文，分别是 Management science 和 Science 。\n\n文献摘要 Funk, Russell J., and Jason Owen-Smith. “A dynamic network measure of technological change.” Management science 63, no. 3 (2017): 791-817.\n该文使用网络分析方法研究技术变革，论文认为 颠覆性的新发明，通过将发明者的注意力转移到或远离这些发明所依赖的知识，来重塑相互关联的技术网络。即更广的视野或更久远的视角，往往有利于颠覆性创新的产生。基于该思路，本文开发了新发明的颠覆性与否的计算指标cdindex。我们将这些指标应用于大学研究商业化的分析，并发现 联邦研究资金推动校园产生颠覆性创新，而商业联系会有利于巩固现状的创新。通过量化新技术，我们提出的指数允许基于专利的创新研究捕捉概念上重要的现象， 这些现象无法通过既定措施检测到。该测量方法提供了支持创新、创业、技术战略、科学政策和社会网络理论研究的理论发展的经验见解。\n Abstract: This article outlines a network approach to the study of technological change. We propose that new inventions reshape networks of interlinked technologies by shifting inventors’ attention to or away from the knowledge on which those inventions build.","title":"ManagementScience | 使用网络算法识别创新的颠覆性与否"},{"content":"  原文: https://www.miriamheiss.com/posts/histogram-ggplot/\n ggplot() 函数对任何数据科学家都是必不可少的, ta是一种非常简单的绘图函数。刚开始接触可能看起来很难， 不过不要害怕，因为一旦学了基础知识，一切都会变得清晰！ 让我们开始！\n之前分享过 R语言 | ggplot2简明绘图之散点图,是以散点图为例简单讲解ggplot2的绘图，今天我们将以直方图作为主讲图形。\n直方图是另一种ggplot2常用的图形，与散点图类似，也是分多个图层进行逐层绘制。\n\n准备 导入本文要用到的包\nlibrary(tidyverse)  ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.3.6 ✔ purrr 0.3.4 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.9 ## ✔ tidyr 1.2.0 ✔ stringr 1.4.1 ## ✔ readr 2.1.2 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(primer.data) #准备数据 library(showtext) ## Loading required package: sysfonts ## Loading required package: showtextdb showtext_auto() #显示中文 #install.packages(\u0026quot;MetBrewer\u0026quot;) library(MetBrewer) #配色包 \n 选择数据 使用data()函数可以查看现有的数据集有哪些，\ndata() 这里选择nobel，使用View(nobel)可以在新打开一个窗口，方便了解数据\nView(nobel) \n 画布gglot 画画需要画布，对于数据分析的绘图也是同理。导入相关R包后， 用ggplot函数构造一个画布。因为还没设定数据，所以这是一个空画布\nggplot() 我们将使用 nobel数据集，传入数据的代码ggplot(data=nobel)\nggplot(data=nobel) 画布看起来依然是空白的，不要紧张。理解这个之前类比PS这类绘图软件，将修图工作看做是很多个图层的叠加。现在我们使用时依然在最底层的ggplot图层，在ggplot函数内添加mapping=aes()参数，准备添加x轴、y轴、color。的图层。\nggplot(data=nobel, mapping=aes()) 注意了，现在图层即将发生变化。我们选择设置x轴 aes(x=year)\n x轴 year  ggplot(data=nobel, mapping=aes(x=year)) 现在我们将开始添加高层次的图层，也会显示越来越多的信息。\n\n 添加geom 现在添加geom层，该层是通过 + 构建在ggplot层之上。这里使用 geom_histogram() 绘制直方图，\nggplot(data=nobel, mapping=aes(x=year))+ geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 不错，接下来添加color\n\n fill和color 按照学科对每个时期的诺奖进行专业分类，使用aes中的fill参数。\nggplot(data=nobel, mapping=aes(x=year, fill=field))+ geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 但同一时期，不同专业之间没有边界区分，容易混乱。这里设置 geom_histogram() 的 color=\"white\"。\nggplot(data=nobel, mapping=aes(x=year, fill=field))+ geom_histogram(color=\u0026quot;white\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. \n 更改配色 更改geom层的颜色，所以该层紧贴geom层，且在geom层之上。设置方法可以使用 scale_fill_manual() 即可。scale_fill_munual() 中的values可以传入颜色字符串。\nggplot(data=nobel, mapping=aes(x=year, fill=field))+ scale_fill_manual(values=c(\u0026quot;red\u0026quot;, \u0026quot;orange\u0026quot;, \u0026quot;yellow\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;purple\u0026quot;))+ geom_histogram(color=\u0026quot;white\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 漂亮！ 这些颜色真的很明艳， scale_fill_munual() 还可以选择十六进制颜色字符串进行颜色自定义\nggplot(data = nobel, mapping = aes(x = year, fill = field)) + scale_fill_manual(values = c(\u0026quot;#f73c39\u0026quot;, \u0026quot;#f79b39\u0026quot;, \u0026quot;#f7ee39\u0026quot;, \u0026quot;#228c14\u0026quot;, \u0026quot;#1e80c7\u0026quot;, \u0026quot;#7c148c\u0026quot;)) + geom_histogram(color = \u0026quot;white\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. \n 配色包MetBrewer 对于我们普通人而言， 不需要记住那么多颜色，只需要在配色方案中选择好看的配色即可。 MetBrewer是R语言的配色包，在文章开头已经提前导入了。下图是MetBrewer的配色方案，选择一种配色方案的名字,如Signac\nggplot(data = nobel, mapping = aes(x = year, fill = field)) + #选择Signac配色方案，使用其中6种颜色 scale_fill_manual(values = met.brewer(\u0026#39;Signac\u0026#39;, 6)) + geom_histogram(color = \u0026quot;white\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. \n 标签labs 现在我们需要用labs() 函数给图片添加标签图层。例如title、subtitle、caption、x、y、legend。\nggplot(data = nobel, mapping = aes(x = year, fill = field)) + scale_fill_manual(values = met.brewer(\u0026quot;Signac\u0026quot;, 6)) + geom_histogram(color = \u0026quot;white\u0026quot;) + labs(title = \u0026quot;Nobel prize laureate numbers have gone up in past 50 years\u0026quot;, subtitle = \u0026quot;Physics, Medicine, and Chemistry have largest numbers of laureates\u0026quot;, x = \u0026quot;Year\u0026quot;, y = \u0026quot;Number of laureates\u0026quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 现在x轴、y轴、标题都是大写，需要将field也大写。这里在labs(fill=‘Year’)更改year为Year\nggplot(data = nobel, mapping = aes(x = year, fill = field)) + scale_fill_manual(values = met.brewer(\u0026quot;Signac\u0026quot;, 6)) + geom_histogram(color = \u0026quot;white\u0026quot;) + labs(title = \u0026quot;Nobel prize laureate numbers have gone up in past 50 years\u0026quot;, subtitle = \u0026quot;Physics, Medicine, and Chemistry have largest numbers of laureates\u0026quot;, x = \u0026quot;Year\u0026quot;, y = \u0026quot;Number of laureates\u0026quot;, fill=\u0026#39;Field\u0026#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. \n 中文 默认ggplot2不支持中文，为了能显示中文，使用showtext包。前文已提前导入并初始化\nlibrary(showtext) #支持中文 showtext_auto() 运行中文的代码\n#把学科转为中文 nobel2 \u0026lt;- nobel %\u0026gt;% mutate( field = case_when(field==\u0026#39;Chemistry\u0026#39; ~ \u0026#39;化学\u0026#39;, field==\u0026#39;Economics\u0026#39; ~ \u0026#39;经济学\u0026#39;, field==\u0026#39;Medicine\u0026#39; ~ \u0026#39;经济学\u0026#39;, field==\u0026#39;Peace\u0026#39; ~ \u0026#39;和平\u0026#39;, field==\u0026#39;Physics\u0026#39; ~ \u0026#39;物理学\u0026#39;, field==\u0026#39;Literature\u0026#39; ~ \u0026#39;文学\u0026#39;)) #绘图 ggplot(data = nobel2, mapping = aes(x = year, fill = field)) + scale_fill_manual(values = met.brewer(\u0026quot;Signac\u0026quot;, 6)) + geom_histogram(color = \u0026quot;white\u0026quot;) + labs(title = \u0026quot;过去50年诺贝尔奖得主人数一直保持增长趋势\u0026quot;, subtitle = \u0026quot;物理学、医学和化学的获奖者人数最多\u0026quot;, x = \u0026quot;年份\u0026quot;, y = \u0026quot;获奖人数\u0026quot;, fill=\u0026#39;领域\u0026#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. \n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析   ","permalink":"/blog/2022-09-04-r-ggplot2-histogram/","summary":"原文: https://www.miriamheiss.com/posts/histogram-ggplot/\n ggplot() 函数对任何数据科学家都是必不可少的, ta是一种非常简单的绘图函数。刚开始接触可能看起来很难， 不过不要害怕，因为一旦学了基础知识，一切都会变得清晰！ 让我们开始！\n之前分享过 R语言 | ggplot2简明绘图之散点图,是以散点图为例简单讲解ggplot2的绘图，今天我们将以直方图作为主讲图形。\n直方图是另一种ggplot2常用的图形，与散点图类似，也是分多个图层进行逐层绘制。\n\n准备 导入本文要用到的包\nlibrary(tidyverse)  ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.3.6 ✔ purrr 0.3.4 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.9 ## ✔ tidyr 1.2.0 ✔ stringr 1.4.1 ## ✔ readr 2.1.2 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(primer.","title":"R语言 |  ggplot2简明绘图之直方图"},{"content":"        原文: https://www.miriamheiss.com/posts/graphing-with-ggplot/\n ggplot() 函数对任何数据科学家都是必不可少的, ta是一种非常简单的绘图函数。刚开始接触可能看起来很难， 不过不要害怕，因为一旦学了基础知识，一切都会变得清晰！ 让我们开始！\nggplotly函数可以直接创建动态交互图标， 该函数内置于ggplot()中。 今天我们要绘制动态散点图\n\n准备 导入本文要用到的包\nlibrary(tidyverse) library(gapminder) #准备数据 library(MetBrewer) #配色包 library(plotly) #动态图 \n quick-start 只要会ggplot，只增多一行代码即可绘制出动态图。鼠标悬浮处会显示一个黑色弹出框。\nlibrary(plotly) library(dplyr) library(gapminder) #选出2007年的数据 gapminder_2007 \u0026lt;- gapminder %\u0026gt;% filter(year==\u0026quot;2007\u0026quot;) %\u0026gt;% dplyr::select(-year) p \u0026lt;- ggplot(data = gapminder_2007, mapping = aes(x=gdpPercap, y=lifeExp, size = pop)) + geom_point(alpha=0.7) #将静态图转为动态图 ggplotly(p)  {\"x\":{\"data\":[{\"x\":[974.5803384,5937.029526,6223.367465,4797.231267,12779.37964,34435.36744,36126.4927,29796.04834,1391.253792,33692.60508,1441.284873,3822.137084,7446.298803,12569.85177,9065.800825,10680.79282,1217.032994,430.0706916,1713.778686,2042.09524,36319.23501,706.016537,1704.063724,13171.63885,4959.114854,7006.580419,986.1478792,277.5518587,3632.557798,9645.06142,1544.750112,14619.22272,8948.102923,22833.30851,35278.41874,2082.481567,6025.374752,6873.262326,5581.180998,5728.353514,12154.08975,641.3695236,690.8055759,33207.0844,30470.0167,13206.48452,752.7497265,32170.37442,1327.60891,27538.41188,5186.050003,942.6542111,579.231743,1201.637154,3548.330846,39724.97867,18008.94444,36180.78919,2452.210407,3540.651564,11605.71449,4471.061906,40675.99635,25523.2771,28569.7197,7320.880262,31656.06806,4519.461171,1463.249282,1593.06548,23348.13973,47306.98978,10461.05868,1569.331442,414.5073415,12057.49928,1044.770126,759.3499101,12451.6558,1042.581557,1803.151496,10956.99112,11977.57496,3095.772271,9253.896111,3820.17523,823.6856205,944,4811.060429,1091.359778,36797.93332,25185.00911,2749.320965,619.6768924,2013.977305,49357.19017,22316.19287,2605.94758,9809.185636,4172.838464,7408.905561,3190.481016,15389.92468,20509.64777,19328.70901,7670.122558,10808.47561,863.0884639,1598.435089,21654.83194,1712.472136,9786.534714,862.5407561,47143.17964,18678.31435,25768.25759,926.1410683,9269.657808,28821.0637,3970.095407,2602.394995,4513.480643,33859.74835,37506.41907,4184.548089,28718.27684,1107.482182,7458.396327,882.9699438,18008.50924,7092.923025,8458.276384,1056.380121,33203.26128,42951.65309,10611.46299,11415.80569,2441.576404,3025.349798,2280.769906,1271.211593,469.7092981],\"y\":[43.828,76.423,72.301,42.731,75.32,81.235,79.829,75.635,64.062,79.441,56.728,65.554,74.852,50.728,72.39,73.005,52.295,49.58,59.723,50.43,80.653,44.741,50.651,78.553,72.961,72.889,65.152,46.462,55.322,78.782,48.328,75.748,78.273,76.486,78.332,54.791,72.235,74.994,71.338,71.878,51.579,58.04,52.947,79.313,80.657,56.735,59.448,79.406,60.022,79.483,70.259,56.007,46.388,60.916,70.198,82.208,73.338,81.757,64.698,70.65,70.964,59.545,78.885,80.745,80.546,72.567,82.603,72.535,54.11,67.297,78.623,77.588,71.993,42.592,45.678,73.952,59.443,48.303,74.241,54.467,64.164,72.801,76.195,66.803,74.543,71.164,42.082,62.069,52.906,63.785,79.762,80.204,72.899,56.867,46.859,80.196,75.64,65.483,75.537,71.752,71.421,71.688,75.563,78.098,78.746,76.442,72.476,46.242,65.528,72.777,63.062,74.002,42.568,79.972,74.663,77.926,48.159,49.339,80.941,72.396,58.556,39.613,80.884,81.701,74.143,78.4,52.517,70.616,58.42,69.819,73.923,71.777,51.542,79.425,78.242,76.384,73.747,74.249,73.422,62.698,42.384,43.487],\"text\":[\"gdpPercap: 974.5803\nlifeExp: 43.828\npop: 31889923\",\"gdpPercap: 5937.0295\nlifeExp: 76.423\npop: 3600523\",\"gdpPercap: 6223.3675\nlifeExp: 72.301\npop: 33333216\",\"gdpPercap: 4797.2313\nlifeExp: 42.731\npop: 12420476\",\"gdpPercap: 12779.3796\nlifeExp: 75.320\npop: 40301927\",\"gdpPercap: 34435.3674\nlifeExp: 81.235\npop: 20434176\",\"gdpPercap: 36126.4927\nlifeExp: 79.829\npop: 8199783\",\"gdpPercap: 29796.0483\nlifeExp: 75.635\npop: 708573\",\"gdpPercap: 1391.2538\nlifeExp: 64.062\npop: 150448339\",\"gdpPercap: 33692.6051\nlifeExp: 79.441\npop: 10392226\",\"gdpPercap: 1441.2849\nlifeExp: 56.728\npop: 8078314\",\"gdpPercap: 3822.1371\nlifeExp: 65.554\npop: 9119152\",\"gdpPercap: 7446.2988\nlifeExp: 74.852\npop: 4552198\",\"gdpPercap: 12569.8518\nlifeExp: 50.728\npop: 1639131\",\"gdpPercap: 9065.8008\nlifeExp: 72.390\npop: 190010647\",\"gdpPercap: 10680.7928\nlifeExp: 73.005\npop: 7322858\",\"gdpPercap: 1217.0330\nlifeExp: 52.295\npop: 14326203\",\"gdpPercap: 430.0707\nlifeExp: 49.580\npop: 8390505\",\"gdpPercap: 1713.7787\nlifeExp: 59.723\npop: 14131858\",\"gdpPercap: 2042.0952\nlifeExp: 50.430\npop: 17696293\",\"gdpPercap: 36319.2350\nlifeExp: 80.653\npop: 33390141\",\"gdpPercap: 706.0165\nlifeExp: 44.741\npop: 4369038\",\"gdpPercap: 1704.0637\nlifeExp: 50.651\npop: 10238807\",\"gdpPercap: 13171.6388\nlifeExp: 78.553\npop: 16284741\",\"gdpPercap: 4959.1149\nlifeExp: 72.961\npop: 1318683096\",\"gdpPercap: 7006.5804\nlifeExp: 72.889\npop: 44227550\",\"gdpPercap: 986.1479\nlifeExp: 65.152\npop: 710960\",\"gdpPercap: 277.5519\nlifeExp: 46.462\npop: 64606759\",\"gdpPercap: 3632.5578\nlifeExp: 55.322\npop: 3800610\",\"gdpPercap: 9645.0614\nlifeExp: 78.782\npop: 4133884\",\"gdpPercap: 1544.7501\nlifeExp: 48.328\npop: 18013409\",\"gdpPercap: 14619.2227\nlifeExp: 75.748\npop: 4493312\",\"gdpPercap: 8948.1029\nlifeExp: 78.273\npop: 11416987\",\"gdpPercap: 22833.3085\nlifeExp: 76.486\npop: 10228744\",\"gdpPercap: 35278.4187\nlifeExp: 78.332\npop: 5468120\",\"gdpPercap: 2082.4816\nlifeExp: 54.791\npop: 496374\",\"gdpPercap: 6025.3748\nlifeExp: 72.235\npop: 9319622\",\"gdpPercap: 6873.2623\nlifeExp: 74.994\npop: 13755680\",\"gdpPercap: 5581.1810\nlifeExp: 71.338\npop: 80264543\",\"gdpPercap: 5728.3535\nlifeExp: 71.878\npop: 6939688\",\"gdpPercap: 12154.0897\nlifeExp: 51.579\npop: 551201\",\"gdpPercap: 641.3695\nlifeExp: 58.040\npop: 4906585\",\"gdpPercap: 690.8056\nlifeExp: 52.947\npop: 76511887\",\"gdpPercap: 33207.0844\nlifeExp: 79.313\npop: 5238460\",\"gdpPercap: 30470.0167\nlifeExp: 80.657\npop: 61083916\",\"gdpPercap: 13206.4845\nlifeExp: 56.735\npop: 1454867\",\"gdpPercap: 752.7497\nlifeExp: 59.448\npop: 1688359\",\"gdpPercap: 32170.3744\nlifeExp: 79.406\npop: 82400996\",\"gdpPercap: 1327.6089\nlifeExp: 60.022\npop: 22873338\",\"gdpPercap: 27538.4119\nlifeExp: 79.483\npop: 10706290\",\"gdpPercap: 5186.0500\nlifeExp: 70.259\npop: 12572928\",\"gdpPercap: 942.6542\nlifeExp: 56.007\npop: 9947814\",\"gdpPercap: 579.2317\nlifeExp: 46.388\npop: 1472041\",\"gdpPercap: 1201.6372\nlifeExp: 60.916\npop: 8502814\",\"gdpPercap: 3548.3308\nlifeExp: 70.198\npop: 7483763\",\"gdpPercap: 39724.9787\nlifeExp: 82.208\npop: 6980412\",\"gdpPercap: 18008.9444\nlifeExp: 73.338\npop: 9956108\",\"gdpPercap: 36180.7892\nlifeExp: 81.757\npop: 301931\",\"gdpPercap: 2452.2104\nlifeExp: 64.698\npop: 1110396331\",\"gdpPercap: 3540.6516\nlifeExp: 70.650\npop: 223547000\",\"gdpPercap: 11605.7145\nlifeExp: 70.964\npop: 69453570\",\"gdpPercap: 4471.0619\nlifeExp: 59.545\npop: 27499638\",\"gdpPercap: 40675.9964\nlifeExp: 78.885\npop: 4109086\",\"gdpPercap: 25523.2771\nlifeExp: 80.745\npop: 6426679\",\"gdpPercap: 28569.7197\nlifeExp: 80.546\npop: 58147733\",\"gdpPercap: 7320.8803\nlifeExp: 72.567\npop: 2780132\",\"gdpPercap: 31656.0681\nlifeExp: 82.603\npop: 127467972\",\"gdpPercap: 4519.4612\nlifeExp: 72.535\npop: 6053193\",\"gdpPercap: 1463.2493\nlifeExp: 54.110\npop: 35610177\",\"gdpPercap: 1593.0655\nlifeExp: 67.297\npop: 23301725\",\"gdpPercap: 23348.1397\nlifeExp: 78.623\npop: 49044790\",\"gdpPercap: 47306.9898\nlifeExp: 77.588\npop: 2505559\",\"gdpPercap: 10461.0587\nlifeExp: 71.993\npop: 3921278\",\"gdpPercap: 1569.3314\nlifeExp: 42.592\npop: 2012649\",\"gdpPercap: 414.5073\nlifeExp: 45.678\npop: 3193942\",\"gdpPercap: 12057.4993\nlifeExp: 73.952\npop: 6036914\",\"gdpPercap: 1044.7701\nlifeExp: 59.443\npop: 19167654\",\"gdpPercap: 759.3499\nlifeExp: 48.303\npop: 13327079\",\"gdpPercap: 12451.6558\nlifeExp: 74.241\npop: 24821286\",\"gdpPercap: 1042.5816\nlifeExp: 54.467\npop: 12031795\",\"gdpPercap: 1803.1515\nlifeExp: 64.164\npop: 3270065\",\"gdpPercap: 10956.9911\nlifeExp: 72.801\npop: 1250882\",\"gdpPercap: 11977.5750\nlifeExp: 76.195\npop: 108700891\",\"gdpPercap: 3095.7723\nlifeExp: 66.803\npop: 2874127\",\"gdpPercap: 9253.8961\nlifeExp: 74.543\npop: 684736\",\"gdpPercap: 3820.1752\nlifeExp: 71.164\npop: 33757175\",\"gdpPercap: 823.6856\nlifeExp: 42.082\npop: 19951656\",\"gdpPercap: 944.0000\nlifeExp: 62.069\npop: 47761980\",\"gdpPercap: 4811.0604\nlifeExp: 52.906\npop: 2055080\",\"gdpPercap: 1091.3598\nlifeExp: 63.785\npop: 28901790\",\"gdpPercap: 36797.9333\nlifeExp: 79.762\npop: 16570613\",\"gdpPercap: 25185.0091\nlifeExp: 80.204\npop: 4115771\",\"gdpPercap: 2749.3210\nlifeExp: 72.899\npop: 5675356\",\"gdpPercap: 619.6769\nlifeExp: 56.867\npop: 12894865\",\"gdpPercap: 2013.9773\nlifeExp: 46.859\npop: 135031164\",\"gdpPercap: 49357.1902\nlifeExp: 80.196\npop: 4627926\",\"gdpPercap: 22316.1929\nlifeExp: 75.640\npop: 3204897\",\"gdpPercap: 2605.9476\nlifeExp: 65.483\npop: 169270617\",\"gdpPercap: 9809.1856\nlifeExp: 75.537\npop: 3242173\",\"gdpPercap: 4172.8385\nlifeExp: 71.752\npop: 6667147\",\"gdpPercap: 7408.9056\nlifeExp: 71.421\npop: 28674757\",\"gdpPercap: 3190.4810\nlifeExp: 71.688\npop: 91077287\",\"gdpPercap: 15389.9247\nlifeExp: 75.563\npop: 38518241\",\"gdpPercap: 20509.6478\nlifeExp: 78.098\npop: 10642836\",\"gdpPercap: 19328.7090\nlifeExp: 78.746\npop: 3942491\",\"gdpPercap: 7670.1226\nlifeExp: 76.442\npop: 798094\",\"gdpPercap: 10808.4756\nlifeExp: 72.476\npop: 22276056\",\"gdpPercap: 863.0885\nlifeExp: 46.242\npop: 8860588\",\"gdpPercap: 1598.4351\nlifeExp: 65.528\npop: 199579\",\"gdpPercap: 21654.8319\nlifeExp: 72.777\npop: 27601038\",\"gdpPercap: 1712.4721\nlifeExp: 63.062\npop: 12267493\",\"gdpPercap: 9786.5347\nlifeExp: 74.002\npop: 10150265\",\"gdpPercap: 862.5408\nlifeExp: 42.568\npop: 6144562\",\"gdpPercap: 47143.1796\nlifeExp: 79.972\npop: 4553009\",\"gdpPercap: 18678.3144\nlifeExp: 74.663\npop: 5447502\",\"gdpPercap: 25768.2576\nlifeExp: 77.926\npop: 2009245\",\"gdpPercap: 926.1411\nlifeExp: 48.159\npop: 9118773\",\"gdpPercap: 9269.6578\nlifeExp: 49.339\npop: 43997828\",\"gdpPercap: 28821.0637\nlifeExp: 80.941\npop: 40448191\",\"gdpPercap: 3970.0954\nlifeExp: 72.396\npop: 20378239\",\"gdpPercap: 2602.3950\nlifeExp: 58.556\npop: 42292929\",\"gdpPercap: 4513.4806\nlifeExp: 39.613\npop: 1133066\",\"gdpPercap: 33859.7484\nlifeExp: 80.884\npop: 9031088\",\"gdpPercap: 37506.4191\nlifeExp: 81.701\npop: 7554661\",\"gdpPercap: 4184.5481\nlifeExp: 74.143\npop: 19314747\",\"gdpPercap: 28718.2768\nlifeExp: 78.400\npop: 23174294\",\"gdpPercap: 1107.4822\nlifeExp: 52.517\npop: 38139640\",\"gdpPercap: 7458.3963\nlifeExp: 70.616\npop: 65068149\",\"gdpPercap: 882.9699\nlifeExp: 58.420\npop: 5701579\",\"gdpPercap: 18008.5092\nlifeExp: 69.819\npop: 1056608\",\"gdpPercap: 7092.9230\nlifeExp: 73.923\npop: 10276158\",\"gdpPercap: 8458.2764\nlifeExp: 71.777\npop: 71158647\",\"gdpPercap: 1056.3801\nlifeExp: 51.542\npop: 29170398\",\"gdpPercap: 33203.2613\nlifeExp: 79.425\npop: 60776238\",\"gdpPercap: 42951.6531\nlifeExp: 78.242\npop: 301139947\",\"gdpPercap: 10611.4630\nlifeExp: 76.384\npop: 3447496\",\"gdpPercap: 11415.8057\nlifeExp: 73.747\npop: 26084662\",\"gdpPercap: 2441.5764\nlifeExp: 74.249\npop: 85262356\",\"gdpPercap: 3025.3498\nlifeExp: 73.422\npop: 4018332\",\"gdpPercap: 2280.7699\nlifeExp: 62.698\npop: 22211743\",\"gdpPercap: 1271.2116\nlifeExp: 42.384\npop: 11746035\",\"gdpPercap: 469.7093\nlifeExp: 43.487\npop: 12311143\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,0,1)\",\"opacity\":0.7,\"size\":[6.70929835135528,4.73930360976545,6.77527173254923,5.598900411863,7.07528369289545,6.12061399094626,5.2515712457991,4.1508288847182,10.1588656081799,5.44107849786917,5.24035330707835,5.33385387656466,4.86531670289218,4.40395768399738,10.9497220287062,5.16855282584858,5.7356199392208,5.26901439581437,5.72211801075065,5.9564773609874,6.77784403625228,4.8422259478741,5.42852631166044,5.86681801798997,22.6771653543307,7.23282911414924,4.15169850163956,7.9562662998771,4.76713328307226,4.81182340745332,5.97611663817011,4.85794694387522,5.52260397354913,5.42769965273683,4.97410786436283,4.06305719383881,5.35122378867242,5.69571313317988,8.43636871732964,5.1306777542305,4.08813627550005,4.90865391372741,8.32592567062337,4.94778140481952,7.84043376621969,4.36262596871136,4.41454467512776,8.49809123744673,6.25770233060098,5.46648280699294,5.61021330347734,5.40445197132612,4.3666011970112,5.27919109958551,5.18415329737334,5.13475345805415,5.40514308400654,3.9460291298415,21.1203680953071,11.5573968303758,8.11057131553749,6.49879524068774,4.8085649745634,5.07824026167466,7.74130405516196,4.6155666029278,9.65077587363648,5.03869129857133,6.87649613589243,6.28100350114513,7.4168446506104,4.56983839481089,4.78354392302204,4.480300710223,4.68010775450673,5.03693920169262,6.0461636385333,5.66517797748393,6.36196165910748,5.5697344023561,4.69148323166173,4.31315004312066,9.20063061292984,4.63065651672448,4.14203032052794,6.79437672410524,6.09253246174489,7.36876387589759,4.48845333089755,6.56775273775666,5.88528441607637,4.80944439195868,4.99737544049698,5.6338762963756,9.82271393327331,4.87472139425141,4.68175365529605,10.5466615830696,4.68733172329479,5.10307852744492,6.55670348636785,8.74086596862592,7.00115515259145,5.46138101214586,4.78640121163658,4.18215900239245,6.22484391610449,5.3111594952501,3.77952755905512,6.50384061768477,5.58747695964146,5.42123845033248,5.04848038390226,4.86541785279541,4.97176812735327,4.47964255788798,5.33382085391144,7.22380829377829,7.0812884751231,6.11737587407916,7.1561065468969,4.282361176813,5.32616181946541,5.19097245574501,6.05493528307165,6.27409491409121,6.98520027409899,7.97119993399054,5.00028803920838,4.26132881438327,5.43159101796171,8.16356363915206,6.580769078434,7.83015990857845,12.807919245623,4.71746231550028,6.42738713411734,8.57951329437571,4.79655097614072,6.22127948111281,5.54798462578679,5.59074371141117],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":26.2283105022831,\"r\":7.30593607305936,\"b\":40.1826484018265,\"l\":37.2602739726027},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-2176.430056865,51811.172085565],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"10000\",\"20000\",\"30000\",\"40000\",\"50000\"],\"tickvals\":[0,10000,20000,30000,40000,50000],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"10000\",\"20000\",\"30000\",\"40000\",\"50000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"gdpPercap\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[37.4635,84.7525],\"tickmode\":\"array\",\"ticktext\":[\"40\",\"50\",\"60\",\"70\",\"80\"],\"tickvals\":[40,50,60,70,80],\"categoryorder\":\"array\",\"categoryarray\":[\"40\",\"50\",\"60\",\"70\",\"80\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"lifeExp\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"title\":{\"text\":\"pop\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"6d507cad2d7b\":{\"x\":{},\"y\":{},\"size\":{},\"type\":\"scatter\"}},\"cur_data\":\"6d507cad2d7b\",\"visdat\":{\"6d507cad2d7b\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}  画布gglot 用ggplot函数构造一个画布, 使用gapminder数据集\n 将gdpPercap设定为x轴 将lifeExp设定为y轴  ggplot(data = gapminder_2007, mapping = aes(x = gdpPercap, y = lifeExp)) 现在我们将开始添加高层次的图层，会显示越来越多的信息。\n\n 添加geom 现在添加geom层，该层是通过 + 构建在ggplot层之上。这里使用 geom_ponit() 绘制散点图，\nggplot(data = gapminder_2007, mapping = aes(x = gdpPercap, y = lifeExp))+ geom_point() #透明度 生成的图仅仅是黑白图，为了美观，可以设置为更靓丽的颜色。\n\n aes(color) 二维空间显示三维信息， - x轴 gdpPercap - y轴 lifeExp - color continent\nggplot(data = gapminder_2007, mapping = aes(x = gdpPercap, #gdp y = lifeExp, #预期寿命 color=continent))+ #大陆 geom_point()  现在可以清晰地看出哪些大陆更加富裕、更加健康, 要是配色再好看点就好了。\n\n 调整配色 在 [https://hidadeng.github.io/blog/2022-09-04-r-ggplot2-histogram/]中首次使用了MetBrewer配色包。\n这里会用5个具有区分度的颜色，我们选择 Lakota 配色方案。同时，在图中绘制对应的人口规模，以size方式显示。\nggplot(data = gapminder_2007, mapping = aes(x = gdpPercap, #gdp y = lifeExp, #预期寿命 size = pop, #人口 color=continent))+ #大陆 geom_point()+ scale_color_manual(values=met.brewer(\u0026quot;Lakota\u0026quot;, 5)) 为了让数据更加平滑，对x轴取对数\nggplot(data = gapminder_2007, mapping = aes(x = gdpPercap, #gdp y = lifeExp, #预期寿命 size = pop, #人口 color=continent))+ #大陆 geom_point()+ scale_color_manual(values = met.brewer(\u0026quot;Lakota\u0026quot;, 5))+ scale_x_log10(breaks = c(400, 4000, 40000)) \n 更换主题 使用 theme_bw 更改绘图的主题风格\nggplot(data = gapminder_2007, mapping = aes(x = gdpPercap, #gdp y = lifeExp, #预期寿命 size = pop, #人口 color=continent))+ #大陆 geom_point()+ scale_color_manual(values = met.brewer(\u0026quot;Lakota\u0026quot;, 5))+ scale_x_log10(breaks = c(400, 4000, 40000))+ theme_bw() \n 标签labs p2 \u0026lt;- ggplot(data = gapminder_2007, mapping = aes(x = gdpPercap, #gdp y = lifeExp, #预期寿命 size = pop, #人口 color=continent, #大陆 text = country))+ #国家 geom_point()+ #透明度 scale_color_manual(values = met.brewer(\u0026quot;Lakota\u0026quot;, 5))+ scale_x_log10(breaks = c(400, 4000, 40000))+ theme_bw()+ labs(title = \u0026quot;Gdp and Life Expectancy in 142 countries\u0026quot;, subtitle = \u0026quot;Most of Africa is sick and poor,\\nwhile Europe is thriving and rich\u0026quot;, x = \u0026quot;Gdp Per Capita\u0026quot;, y = \u0026quot;Life Expectancy\u0026quot;, fill = \u0026quot;Continent\u0026quot;, size = \u0026quot;Population\u0026quot;) p2 使用ggplotly将p2从静态图转为动态图， 鼠标悬浮处弹出圆圈对应的国家。\nggplotly(p2, tooltip = \u0026quot;text\u0026quot;)  {\"x\":{\"data\":[{\"x\":[3.79402544509666,3.68099065562682,3.1587498285201,4.09933015629768,3.08530235219712,2.63353984732021,3.31007599301138,2.848814873635,3.23148583131337,2.99394204506796,2.44334414004637,3.56021253312911,3.18885823537455,3.31858116595608,3.74672610693845,4.08472243900371,2.80710831941093,2.83935583435696,4.12078722664154,2.87665060639063,3.1230701584329,2.9743524119827,2.76285235400399,3.16531831968284,3.19571467604724,2.61753222691212,4.08125724466051,3.01902074595738,2.88044194586588,3.0181100381011,3.25603221652431,4.03969130967891,3.58208328429303,2.91576148434958,3.68224081186337,2.79216530171644,3.30405457229069,3.88480230343241,2.93605531183875,3.20369500466057,3.23362351372365,2.93577962517273,2.96667714274902,3.96706370233863,3.41537321502711,3.65451158423759,3.04433674769752,2.94594592049266,3.85082524675123,3.02382022004224,3.10421784482941,2.67182915731664],\"y\":[72.301,42.731,56.728,50.728,52.295,49.58,50.43,44.741,50.651,65.152,46.462,55.322,48.328,54.791,71.338,51.579,58.04,52.947,56.735,59.448,60.022,56.007,46.388,54.11,42.592,45.678,73.952,59.443,48.303,54.467,64.164,72.801,71.164,42.082,52.906,56.867,46.859,76.442,46.242,65.528,63.062,42.568,48.159,49.339,58.556,39.613,52.517,58.42,73.923,51.542,42.384,43.487],\"text\":[\"Algeria\",\"Angola\",\"Benin\",\"Botswana\",\"Burkina Faso\",\"Burundi\",\"Cameroon\",\"Central African Republic\",\"Chad\",\"Comoros\",\"Congo, Dem. Rep.\",\"Congo, Rep.\",\"Cote d'Ivoire\",\"Djibouti\",\"Egypt\",\"Equatorial Guinea\",\"Eritrea\",\"Ethiopia\",\"Gabon\",\"Gambia\",\"Ghana\",\"Guinea\",\"Guinea-Bissau\",\"Kenya\",\"Lesotho\",\"Liberia\",\"Libya\",\"Madagascar\",\"Malawi\",\"Mali\",\"Mauritania\",\"Mauritius\",\"Morocco\",\"Mozambique\",\"Namibia\",\"Niger\",\"Nigeria\",\"Reunion\",\"Rwanda\",\"Sao Tome and Principe\",\"Senegal\",\"Sierra Leone\",\"Somalia\",\"South Africa\",\"Sudan\",\"Swaziland\",\"Tanzania\",\"Togo\",\"Tunisia\",\"Uganda\",\"Zambia\",\"Zimbabwe\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(4,163,189,1)\",\"opacity\":1,\"size\":[6.77527173254923,5.598900411863,5.24035330707835,4.40395768399738,5.7356199392208,5.26901439581437,5.9564773609874,4.8422259478741,5.42852631166044,4.15169850163956,7.9562662998771,4.76713328307226,5.97611663817011,4.06305719383881,8.43636871732964,4.08813627550005,4.90865391372741,8.32592567062337,4.36262596871136,4.41454467512776,6.25770233060098,5.40445197132612,4.3666011970112,6.87649613589243,4.480300710223,4.68010775450673,5.03693920169262,6.0461636385333,5.66517797748393,5.5697344023561,4.69148323166173,4.31315004312066,6.79437672410524,6.09253246174489,4.48845333089755,5.6338762963756,9.82271393327331,4.18215900239245,5.3111594952501,3.77952755905512,5.58747695964146,5.04848038390226,5.33382085391144,7.22380829377829,7.1561065468969,4.282361176813,6.98520027409899,5.00028803920838,5.43159101796171,6.580769078434,5.54798462578679,5.59074371141117],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(4,163,189,1)\"}},\"hoveron\":\"points\",\"name\":\"Africa\",\"legendgroup\":\"Africa\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4.10650977201769,3.58230625931688,3.95740617340857,4.56013669242519,4.11963981438709,3.84550611104731,3.98430499758718,3.95173097080165,3.77998406330737,3.83716291957619,3.75802981175127,3.71483669984604,3.0797733480287,3.55002410662006,3.86456330372846,4.07836889751077,3.43922544380543,3.99163295350919,3.6204315729333,3.86975405900188,4.28620284789525,4.25547776303394,4.632979883279,4.02577526370184,4.05750656798335],\"y\":[75.32,65.554,72.39,80.653,78.553,72.889,78.782,78.273,72.235,74.994,71.878,70.259,60.916,70.198,72.567,76.195,72.899,75.537,71.752,71.421,78.746,69.819,78.242,76.384,73.747],\"text\":[\"Argentina\",\"Bolivia\",\"Brazil\",\"Canada\",\"Chile\",\"Colombia\",\"Costa Rica\",\"Cuba\",\"Dominican Republic\",\"Ecuador\",\"El Salvador\",\"Guatemala\",\"Haiti\",\"Honduras\",\"Jamaica\",\"Mexico\",\"Nicaragua\",\"Panama\",\"Paraguay\",\"Peru\",\"Puerto Rico\",\"Trinidad and Tobago\",\"United States\",\"Uruguay\",\"Venezuela\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(240,190,61,1)\",\"opacity\":1,\"size\":[7.07528369289545,5.33385387656466,10.9497220287062,6.77784403625228,5.86681801798997,7.23282911414924,4.81182340745332,5.52260397354913,5.35122378867242,5.69571313317988,5.1306777542305,5.61021330347734,5.27919109958551,5.18415329737334,4.6155666029278,9.20063061292984,4.99737544049698,4.68733172329479,5.10307852744492,6.55670348636785,4.78640121163658,4.26132881438327,12.807919245623,4.71746231550028,6.42738713411734],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(240,190,61,1)\"}},\"hoveron\":\"points\",\"name\":\"Americas\",\"legendgroup\":\"Americas\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[2.98881764549129,4.47415867018597,3.143406361057,3.23395473727517,3.6954041667466,4.59906367268864,3.38955773121288,3.54908318988216,4.06467188236304,3.65041068311967,4.40693643553008,4.50045697098462,3.65508665949928,3.20223362703668,4.3682522837381,4.67492531406996,4.01957563822304,4.09522710701315,3.49076900592376,2.97497199429807,3.03796794384465,4.34862010618943,3.41596567540354,3.50385616482709,4.33555481758265,4.67341887079622,3.59880094359832,3.62164856315343,4.45815837772449,3.87264545733796,3.38767031916547,3.4807755960326,3.35808147396803],\"y\":[43.828,75.635,64.062,59.723,72.961,82.208,64.698,70.65,70.964,59.545,80.745,82.603,72.535,67.297,78.623,77.588,71.993,74.241,66.803,62.069,63.785,75.64,65.483,71.688,72.777,79.972,72.396,74.143,78.4,70.616,74.249,73.422,62.698],\"text\":[\"Afghanistan\",\"Bahrain\",\"Bangladesh\",\"Cambodia\",\"China\",\"Hong Kong, China\",\"India\",\"Indonesia\",\"Iran\",\"Iraq\",\"Israel\",\"Japan\",\"Jordan\",\"Korea, Dem. Rep.\",\"Korea, Rep.\",\"Kuwait\",\"Lebanon\",\"Malaysia\",\"Mongolia\",\"Myanmar\",\"Nepal\",\"Oman\",\"Pakistan\",\"Philippines\",\"Saudi Arabia\",\"Singapore\",\"Sri Lanka\",\"Syria\",\"Taiwan\",\"Thailand\",\"Vietnam\",\"West Bank and Gaza\",\"Yemen, Rep.\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(147,30,24,1)\",\"opacity\":1,\"size\":[6.70929835135528,4.1508288847182,10.1588656081799,5.72211801075065,22.6771653543307,5.13475345805415,21.1203680953071,11.5573968303758,8.11057131553749,6.49879524068774,5.07824026167466,9.65077587363648,5.03869129857133,6.28100350114513,7.4168446506104,4.56983839481089,4.78354392302204,6.36196165910748,4.63065651672448,7.36876387589759,6.56775273775666,4.68175365529605,10.5466615830696,8.74086596862592,6.50384061768477,4.86541785279541,6.11737587407916,6.05493528307165,6.27409491409121,7.97119993399054,8.57951329437571,4.79655097614072,6.22127948111281],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(147,30,24,1)\"}},\"hoveron\":\"points\",\"name\":\"Asia\",\"legendgroup\":\"Asia\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[3.77356920876217,4.55782580062218,4.52753459151331,3.87194045947979,4.02860349094832,4.16492428251317,4.35856884462783,4.54750911081685,4.52123074598147,4.48387269225076,4.50745611553652,4.43993889119905,4.25548825822109,4.55847803563392,4.60933819969144,4.45590597950278,3.96632461957789,4.56582342810493,4.69335042800253,4.18723649435127,4.31195820191596,4.03376444688129,3.990628940935,4.27133768016358,4.41108505321789,4.4597100052953,4.52968372605252,4.57410560181062,3.92728187211911,4.52118074293136],\"y\":[76.423,79.829,79.441,74.852,73.005,75.748,76.486,78.332,79.313,80.657,79.406,79.483,73.338,81.757,78.885,80.546,74.543,79.762,80.196,75.563,78.098,72.476,74.002,74.663,77.926,80.941,80.884,81.701,71.777,79.425],\"text\":[\"Albania\",\"Austria\",\"Belgium\",\"Bosnia and Herzegovina\",\"Bulgaria\",\"Croatia\",\"Czech Republic\",\"Denmark\",\"Finland\",\"France\",\"Germany\",\"Greece\",\"Hungary\",\"Iceland\",\"Ireland\",\"Italy\",\"Montenegro\",\"Netherlands\",\"Norway\",\"Poland\",\"Portugal\",\"Romania\",\"Serbia\",\"Slovak Republic\",\"Slovenia\",\"Spain\",\"Sweden\",\"Switzerland\",\"Turkey\",\"United Kingdom\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(218,121,1,1)\",\"opacity\":1,\"size\":[4.73930360976545,5.2515712457991,5.44107849786917,4.86531670289218,5.16855282584858,4.85794694387522,5.42769965273683,4.97410786436283,4.94778140481952,7.84043376621969,8.49809123744673,5.46648280699294,5.40514308400654,3.9460291298415,4.8085649745634,7.74130405516196,4.14203032052794,5.88528441607637,4.87472139425141,7.00115515259145,5.46138101214586,6.22484391610449,5.42123845033248,4.97176812735327,4.47964255788798,7.0812884751231,5.32616181946541,5.19097245574501,8.16356363915206,7.83015990857845],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(218,121,1,1)\"}},\"hoveron\":\"points\",\"name\":\"Europe\",\"legendgroup\":\"Europe\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4.53700472145269,4.40114211228811],\"y\":[81.235,80.204],\"text\":[\"Australia\",\"New Zealand\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(36,125,63,1)\",\"opacity\":1,\"size\":[6.12061399094626,4.80944439195868],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(36,125,63,1)\"}},\"hoveron\":\"points\",\"name\":\"Oceania\",\"legendgroup\":\"Oceania\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":43.7625570776256,\"r\":7.30593607305936,\"b\":40.1826484018265,\"l\":37.2602739726027},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\"Gdp and Life Expectancy in 142 countries\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[2.33084382564856,4.80585074240034],\"tickmode\":\"array\",\"ticktext\":[\"400\",\"4000\",\"40000\"],\"tickvals\":[2.60205999132796,3.60205999132796,4.60205999132796],\"categoryorder\":\"array\",\"categoryarray\":[\"400\",\"4000\",\"40000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Gdp Per Capita\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[37.4635,84.7525],\"tickmode\":\"array\",\"ticktext\":[\"40\",\"50\",\"60\",\"70\",\"80\"],\"tickvals\":[40,50,60,70,80],\"categoryorder\":\"array\",\"categoryarray\":[\"40\",\"50\",\"60\",\"70\",\"80\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"Life Expectancy\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":0.66417600664176,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"title\":{\"text\":\"continent\nPopulation\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"6d507d3656d6\":{\"x\":{},\"y\":{},\"size\":{},\"colour\":{},\"text\":{},\"type\":\"scatter\"}},\"cur_data\":\"6d507d3656d6\",\"visdat\":{\"6d507d3656d6\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]} \n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析   ","permalink":"/blog/2022-09-04-r-ggplot2-ggplotly/","summary":"原文: https://www.miriamheiss.com/posts/graphing-with-ggplot/\n ggplot() 函数对任何数据科学家都是必不可少的, ta是一种非常简单的绘图函数。刚开始接触可能看起来很难， 不过不要害怕，因为一旦学了基础知识，一切都会变得清晰！ 让我们开始！\nggplotly函数可以直接创建动态交互图标， 该函数内置于ggplot()中。 今天我们要绘制动态散点图\n\n准备 导入本文要用到的包\nlibrary(tidyverse) library(gapminder) #准备数据 library(MetBrewer) #配色包 library(plotly) #动态图 \n quick-start 只要会ggplot，只增多一行代码即可绘制出动态图。鼠标悬浮处会显示一个黑色弹出框。\nlibrary(plotly) library(dplyr) library(gapminder) #选出2007年的数据 gapminder_2007 \u0026lt;- gapminder %\u0026gt;% filter(year==\u0026quot;2007\u0026quot;) %\u0026gt;% dplyr::select(-year) p \u0026lt;- ggplot(data = gapminder_2007, mapping = aes(x=gdpPercap, y=lifeExp, size = pop)) + geom_point(alpha=0.7) #将静态图转为动态图 ggplotly(p)  {\"x\":{\"data\":[{\"x\":[974.5803384,5937.029526,6223.367465,4797.231267,12779.37964,34435.36744,36126.4927,29796.04834,1391.253792,33692.60508,1441.284873,3822.137084,7446.298803,12569.85177,9065.800825,10680.79282,1217.032994,430.0706916,1713.778686,2042.09524,36319.23501,706.016537,1704.063724,13171.63885,4959.114854,7006.580419,986.1478792,277.5518587,3632.557798,9645.06142,1544.750112,14619.22272,8948.102923,22833.30851,35278.41874,2082.481567,6025.374752,6873.262326,5581.180998,5728.353514,12154.08975,641.3695236,690.8055759,33207.0844,30470.0167,13206.48452,752.7497265,32170.37442,1327.60891,27538.41188,5186.050003,942.6542111,579.231743,1201.637154,3548.330846,39724.97867,18008.94444,36180.78919,2452.210407,3540.651564,11605.71449,4471.061906,40675.99635,25523.2771,28569.7197,7320.880262,31656.06806,4519.461171,1463.249282,1593.06548,23348.13973,47306.98978,10461.05868,1569.331442,414.5073415,12057.49928,1044.770126,759.3499101,12451.6558,1042.581557,1803.151496,10956.99112,11977.57496,3095.772271,9253.896111,3820.17523,823.6856205,944,4811.060429,1091.359778,36797.93332,25185.00911,2749.320965,619.6768924,2013.977305,49357.19017,22316.19287,2605.94758,9809.185636,4172.838464,7408.905561,3190.481016,15389.92468,20509.64777,19328.70901,7670.122558,10808.47561,863.0884639,1598.435089,21654.83194,1712.472136,9786.534714,862.5407561,47143.17964,18678.31435,25768.25759,926.1410683,9269.657808,28821.0637,3970.095407,2602.394995,4513.480643,33859.74835,37506.41907,4184.548089,28718.27684,1107.482182,7458.396327,882.9699438,18008.50924,7092.923025,8458.276384,1056.380121,33203.26128,42951.65309,10611.46299,11415.80569,2441.576404,3025.349798,2280.769906,1271.211593,469.7092981],\"y\":[43.828,76.423,72.301,42.731,75.32,81.235,79.829,75.635,64.062,79.441,56.728,65.554,74.852,50.728,72.39,73.005,52.295,49.58,59.723,50.43,80.653,44.741,50.651,78.553,72.961,72.889,65.152,46.462,55.322,78.782,48.328,75.748,78.273,76.486,78.332,54.791,72.235,74.994,71.338,71.878,51.579,58.04,52.947,79.313,80.657,56.735,59.448,79.406,60.022,79.483,70.259,56.007,46.388,60.916,70.198,82.208,73.338,81.757,64.698,70.65,70.964,59.545,78.885,80.745,80.546,72.567,82.603,72.535,54.11,67.297,78.623,77.588,71.993,42.592,45.678,73.952,59.443,48.303,74.241,54.467,64.164,72.801,76.195,66.803,74.543,71.164,42.082,62.069,52.906,63.785,79.762,80.204,72.899,56.867,46.859,80.196,75.64,65.483,75.537,71.752,71.421,71.688,75.563,78.098,78.746,76.442,72.476,46.242,65.528,72.777,63.062,74.002,42.568,79.972,74.663,77.926,48.159,49.339,80.941,72.396,58.556,39.613,80.884,81.701,74.143,78.4,52.517,70.616,58.42,69.819,73.923,71.777,51.542,79.425,78.242,76.384,73.747,74.249,73.422,62.698,42.384,43.487],\"text\":[\"gdpPercap: 974.5803\nlifeExp: 43.828\npop: 31889923\",\"gdpPercap: 5937.0295\nlifeExp: 76.423","title":"R语言 | ggplot2简明绘图之动态图"},{"content":"  原文: https://www.miriamheiss.com/posts/graphing-with-ggplot/\n ggplot() 函数对任何数据科学家都是必不可少的, ta是一种非常简单的绘图函数。刚开始接触可能看起来很难， 不过不要害怕，因为一旦学了基础知识，一切都会变得清晰！ 让我们开始！\n准备 在这里，我需要导入本节需要的包。 tidyverse 包括八个包，其中之一是 ggplot2。 primer.data包 拥有比R 内置的更多的数据集。\nlibrary(ggplot2) library(primer.data) #准备数据 library(showtext) ## Loading required package: sysfonts ## Loading required package: showtextdb showtext_auto() #显示中文  画布gglot 画画需要画布，对于数据分析的绘图也是同理。导入相关R包后， 用ggplot函数构造一个画布。因为还没设定数据，所以这是一个空画布\nggplot() 我们将使用nhanes数据集，传入数据的代码ggplot(data=nhanes)\nggplot(data=nhanes) 画布看起来依然是空白的，不要紧张。理解这个之前类比PS这类绘图软件，将修图工作看做是很多个图层的叠加。现在我们使用时依然在最底层的ggplot图层，在ggplot函数内添加mapping=aes()参数，准备添加x轴、y轴、color。的图层。\nAesthetic mappings审美映射。\nggplot(data=nhanes, mapping=aes()) 注意了，现在图层即将发生变化。我们选择设置x轴、y轴、color的字段。\n x轴 height身高 y轴 weight体重 color gender性别  ggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender)) 现在我们将开始添加高层次的图层，也会显示越来越多的信息。\n\n 添加geom 现在添加geom层(geom是geomeric缩写)，该层是通过 + 构建在ggplot层之上。这里使用 geom_point() 绘制散点图，\nggplot(data=nhanes, mapping = aes( x=height, y=weight, color=gender))+ geom_point() ## Warning: Removed 366 rows containing missing values (geom_point). Wow! 不错的开始，不过这个图中的点互相之间重叠的有一点点严重，需要设定点的大小size和透明度alpha来控制重叠效果。\nggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.3, size=0.5) ## Warning: Removed 366 rows containing missing values (geom_point). much better! 但能否按性别，分别绘制男、女的散点图。\n\n 分面facet 接下来添加一个分面函数 facet_wrap。该函数会分别生成男性分面、女性分面\nggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.3, size=0.5)+ facet_wrap(~gender) ## Warning: Removed 366 rows containing missing values (geom_point). 现在我们有了两个分面图\n\n 添加第二个geom 现在我们需要添加一个趋势线，可以使用 geom_smooth() 函数，因为geom_smooth和geom_point都是geom层的函数，理所当然它俩比 facet_wrap层更近一些。为了让趋势线更明显，将散点的透明度设置的更浅，比如0.1\nggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.1, size=0.5)+ geom_smooth()+ facet_wrap(~gender) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; and formula \u0026#39;y ~ s(x, bs = \u0026quot;cs\u0026quot;)\u0026#39; ## Warning: Removed 366 rows containing non-finite values (stat_smooth). ## Warning: Removed 366 rows containing missing values (geom_point). 现在，我们想让趋势线更平滑一些。在geom_smooth()中，我们会设置 method=\"loess\"以使得趋势线更平滑。 formula=y~x表示y的变化与x有关。\nggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.1, size=0.5)+ geom_smooth(method=\u0026quot;loess\u0026quot;, formula=y~x)+ facet_wrap(~gender) ## Warning: Removed 366 rows containing non-finite values (stat_smooth). ## Warning: Removed 366 rows containing missing values (geom_point).  标签labs 现在我们需要用labs()函数给图片添加标签图层。例如title、subtitle、caption、x、y、legend。\nggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.1, size=0.5)+ geom_smooth(method=\u0026quot;loess\u0026quot;, formula=y~x)+ facet_wrap(~gender)+ labs(title = \u0026quot;Heights in the U.S.\u0026quot;, subtitle = \u0026quot;On average, men weigh more and are taller than women\u0026quot;) ## Warning: Removed 366 rows containing non-finite values (stat_smooth). ## Warning: Removed 366 rows containing missing values (geom_point). 现在有了正副标题，横纵坐标没有数量单位，不太nice，这里更改为 Height(cm)、Weight(kg)\nggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.1, size=0.5)+ geom_smooth(method=\u0026quot;loess\u0026quot;, formula=y~x)+ facet_wrap(~gender)+ labs(title = \u0026quot;Heights in the U.S.\u0026quot;, subtitle = \u0026quot;On average, men weigh more and are taller than women\u0026quot;, x=\u0026quot;Height (cm)\u0026quot;, y=\u0026quot;Weight (kg)\u0026quot;) ## Warning: Removed 366 rows containing non-finite values (stat_smooth). ## Warning: Removed 366 rows containing missing values (geom_point). Awesome! 但图例Lengend中的 gender 依然是小写，我希望改为大写。我们知道x、y、color分别对应height、weight、gender，所以如果更改gender，需要设置的是color。\nggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.1, size=0.5)+ geom_smooth(method=\u0026quot;loess\u0026quot;, formula=y~x)+ facet_wrap(~gender)+ labs(title = \u0026quot;Heights in the U.S.\u0026quot;, subtitle = \u0026quot;On average, men weigh more and are taller than women\u0026quot;, x=\u0026quot;Height (cm)\u0026quot;, y=\u0026quot;Weight (kg)\u0026quot;, color=\u0026quot;Gender\u0026quot;) ## Warning: Removed 366 rows containing non-finite values (stat_smooth). ## Warning: Removed 366 rows containing missing values (geom_point). 但是看到这个图片时，其他人会想原始数据是啥情况，怎么来的。这时候我们需要告诉大家nhances数据集来自于 National Health and Nutrition Examination Survey。通过设置labs的caption参数即可。\nggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.1, size=0.5)+ geom_smooth(method=\u0026quot;loess\u0026quot;, formula=y~x)+ facet_wrap(~gender)+ labs(title = \u0026quot;Heights in the U.S.\u0026quot;, subtitle = \u0026quot;On average, men weigh more and are taller than women\u0026quot;, x=\u0026quot;Height (cm)\u0026quot;, y=\u0026quot;Weight (kg)\u0026quot;, color=\u0026quot;Gender\u0026quot;, caption=\u0026quot;Source: National Health and Nutrition Examination Survey\u0026quot;) ## Warning: Removed 366 rows containing non-finite values (stat_smooth). ## Warning: Removed 366 rows containing missing values (geom_point). \n 更改配色 绘图已经相当完整，但geom层的散点颜色可能不是咱的最爱，如何设置颜色呢？\n更改geom层的颜色，所以该层紧贴geom层，且在geom层之上。设置方法使用 scale_color_manual() 即可。scale_color_munual() 中的values可以传入颜色十六进制的字符串，还可以传入颜色字符串。\nggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.1, size=0.5)+ geom_smooth(method=\u0026quot;loess\u0026quot;, formula=y~x)+ scale_color_manual(values=c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;))+ facet_wrap(~gender)+ labs(title = \u0026quot;Heights in the U.S.\u0026quot;, subtitle = \u0026quot;On average, men weigh more and are taller than women\u0026quot;, x=\u0026quot;Height (cm)\u0026quot;, y=\u0026quot;Weight (kg)\u0026quot;, color=\u0026quot;Gender\u0026quot;, caption=\u0026quot;Source: National Health and Nutrition Examination Survey\u0026quot;) ## Warning: Removed 366 rows containing non-finite values (stat_smooth). ## Warning: Removed 366 rows containing missing values (geom_point). \n 设置主题  theme_bw() theme_dark() theme_gray() theme_light() theme_minimal()  g \u0026lt;- ggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.1, size=0.5)+ geom_smooth(method=\u0026quot;loess\u0026quot;, formula=y~x)+ scale_color_manual(values=c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;))+ facet_wrap(~gender)+ labs(title = \u0026quot;Heights in the U.S.\u0026quot;, subtitle = \u0026quot;On average, men weigh more and are taller than women\u0026quot;, x=\u0026quot;Height (cm)\u0026quot;, y=\u0026quot;Weight (kg)\u0026quot;, color=\u0026quot;Gender\u0026quot;, caption=\u0026quot;Source: National Health and Nutrition Examination Survey\u0026quot;) g + theme_minimal() ## Warning: Removed 366 rows containing non-finite values (stat_smooth). ## Warning: Removed 366 rows containing missing values (geom_point). \n 保存 ggsave( filename = \u0026quot;scatter.png\u0026quot;, plot = g, width = 10, height = 8, dpi = 100, device = \u0026quot;png\u0026quot; ) ## Warning: Removed 366 rows containing non-finite values (stat_smooth). ## Warning: Removed 366 rows containing missing values (geom_point). \n 中文问题 默认ggplot2不支持中文，为了能显示中文，需要使用showtext包\nlibrary(ggplot2) library(primer.data) #提供数据 library(showtext) #支持中文 showtext_auto() ggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender))+ geom_point(alpha=0.1, size=0.5)+ geom_smooth(method=\u0026quot;loess\u0026quot;, formula=y~x)+ scale_color_manual(values=c(\u0026quot;magenta\u0026quot;, \u0026quot;blue\u0026quot;))+ facet_wrap(~gender)+ labs(title = \u0026quot;美国身高\u0026quot;, subtitle = \u0026quot;平均而言，男性群体的身高会高于女性群体\u0026quot;, x=\u0026quot;身高(cm)\u0026quot;, y=\u0026quot;体重(kg)\u0026quot;, color=\u0026quot;性别\u0026quot;, caption=\u0026quot;数据源: National Health and Nutrition Examination Survey\u0026quot;) ## Warning: Removed 366 rows containing non-finite values (stat_smooth). ## Warning: Removed 366 rows containing missing values (geom_point). \n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析   ","permalink":"/blog/2022-09-04-r-ggplot2-scatter/","summary":"原文: https://www.miriamheiss.com/posts/graphing-with-ggplot/\n ggplot() 函数对任何数据科学家都是必不可少的, ta是一种非常简单的绘图函数。刚开始接触可能看起来很难， 不过不要害怕，因为一旦学了基础知识，一切都会变得清晰！ 让我们开始！\n准备 在这里，我需要导入本节需要的包。 tidyverse 包括八个包，其中之一是 ggplot2。 primer.data包 拥有比R 内置的更多的数据集。\nlibrary(ggplot2) library(primer.data) #准备数据 library(showtext) ## Loading required package: sysfonts ## Loading required package: showtextdb showtext_auto() #显示中文  画布gglot 画画需要画布，对于数据分析的绘图也是同理。导入相关R包后， 用ggplot函数构造一个画布。因为还没设定数据，所以这是一个空画布\nggplot() 我们将使用nhanes数据集，传入数据的代码ggplot(data=nhanes)\nggplot(data=nhanes) 画布看起来依然是空白的，不要紧张。理解这个之前类比PS这类绘图软件，将修图工作看做是很多个图层的叠加。现在我们使用时依然在最底层的ggplot图层，在ggplot函数内添加mapping=aes()参数，准备添加x轴、y轴、color。的图层。\nAesthetic mappings审美映射。\nggplot(data=nhanes, mapping=aes()) 注意了，现在图层即将发生变化。我们选择设置x轴、y轴、color的字段。\n x轴 height身高 y轴 weight体重 color gender性别  ggplot(data=nhanes, mapping=aes( x=height, y=weight, color=gender)) 现在我们将开始添加高层次的图层，也会显示越来越多的信息。\n\n 添加geom 现在添加geom层(geom是geomeric缩写)，该层是通过 + 构建在ggplot层之上。这里使用 geom_point() 绘制散点图，\nggplot(data=nhanes, mapping = aes( x=height, y=weight, color=gender))+ geom_point() ## Warning: Removed 366 rows containing missing values (geom_point).","title":"R语言 |  ggplot2简明绘图之散点图"},{"content":" 使用officedown在Rmarkdown中写PPT/Word。本文只以PPT为例简单演示，不做细节讲解。\n安装 install.packages(c(\u0026quot;officedown\u0026quot;, \u0026quot;rvg\u0026quot;)) \n 新建Rmd 按照截图， 会自动新建一个Rmd模板。 如果你自己想改动添加内容，自己改就行不用怕，大不了重新新建一个空白模板。\n\n 渲染 内容Okay后，点击渲染\n渲染时会自动弹开MS PowerPoint\n\n 相关资料  https://www.miriamheiss.com/posts/graphing-with-ggplot/ https://ardata-fr.github.io/officeverse/index.html  \n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析   ","permalink":"/blog/2022-09-04-officedown-pptdocx/","summary":" 使用officedown在Rmarkdown中写PPT/Word。本文只以PPT为例简单演示，不做细节讲解。\n安装 install.packages(c(\u0026quot;officedown\u0026quot;, \u0026quot;rvg\u0026quot;)) \n 新建Rmd 按照截图， 会自动新建一个Rmd模板。 如果你自己想改动添加内容，自己改就行不用怕，大不了重新新建一个空白模板。\n\n 渲染 内容Okay后，点击渲染\n渲染时会自动弹开MS PowerPoint\n\n 相关资料  https://www.miriamheiss.com/posts/graphing-with-ggplot/ https://ardata-fr.github.io/officeverse/index.html  \n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析   ","title":"R语言 |  使用officedown包在Rmarkdown中制作PPT"},{"content":"  原文: https://shilaan.rbind.io/post/academic-conference-posters-using-posterdown/\n 如果你认识我，你就会知道我喜欢用 R语言 做任何事情。 当需要创建我的第一个学术会议海报时，我知道我不想浪费任何时间在不同的软件环境之间移动或复制粘贴和手动格式化文本、表格、图形和结果。\n相反，我想使用 RMarkdown 生成一个完全可复制且格式精美的会议海报。 原来; R 包 posterdown 使这变得非常简单！ 在这篇文章中，我将简要向您展示如何使用 posterdown 包创建自己的海报。\n要先睹为快，您可以在此处找到我的第一个 posterdown 创建的 海报 以及此处的基础代码。\n\n安装 install.packages(\u0026quot;posterdown\u0026quot;) \n 新建Rmd 按照下面步骤点击从模板新建Rmarkdown File -\u0026gt; Net File -\u0026gt; R Markdown -\u0026gt; From Template\n从中选择posterdown模板，有三种可供选择的子模板，即 - Posterdown HTML - Posterdown betterland - Posterdown Betterport\n大致的样式如下\n虽然 HTML 模板看起来更像是经典的科学海报，但 Betterland 和 Betterport 模板创建的海报具有大量空间，专门用于呈现高传递性的(take-away)信息。 后两者的区别在于，Betterland 是横向的，而 Betterport 是纵向的。\n\n 个性化 当选择了模板，就可以在Rmd文件内修改内容。如果对字号、字体等不太满意，还可以在修改yaml部分参数。\n最后点击Knit即可渲染出学术会议海报\n\n 参考资料  https://shilaan.rbind.io/post/academic-conference-posters-using-posterdown https://github.com/brentthorne/posterdown  \n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析   ","permalink":"/blog/2022-09-04-posterdown/","summary":"原文: https://shilaan.rbind.io/post/academic-conference-posters-using-posterdown/\n 如果你认识我，你就会知道我喜欢用 R语言 做任何事情。 当需要创建我的第一个学术会议海报时，我知道我不想浪费任何时间在不同的软件环境之间移动或复制粘贴和手动格式化文本、表格、图形和结果。\n相反，我想使用 RMarkdown 生成一个完全可复制且格式精美的会议海报。 原来; R 包 posterdown 使这变得非常简单！ 在这篇文章中，我将简要向您展示如何使用 posterdown 包创建自己的海报。\n要先睹为快，您可以在此处找到我的第一个 posterdown 创建的 海报 以及此处的基础代码。\n\n安装 install.packages(\u0026quot;posterdown\u0026quot;) \n 新建Rmd 按照下面步骤点击从模板新建Rmarkdown File -\u0026gt; Net File -\u0026gt; R Markdown -\u0026gt; From Template\n从中选择posterdown模板，有三种可供选择的子模板，即 - Posterdown HTML - Posterdown betterland - Posterdown Betterport\n大致的样式如下\n虽然 HTML 模板看起来更像是经典的科学海报，但 Betterland 和 Betterport 模板创建的海报具有大量空间，专门用于呈现高传递性的(take-away)信息。 后两者的区别在于，Betterland 是横向的，而 Betterport 是纵向的。\n\n 个性化 当选择了模板，就可以在Rmd文件内修改内容。如果对字号、字体等不太满意，还可以在修改yaml部分参数。\n最后点击Knit即可渲染出学术会议海报","title":"R语言 |  使用posterdown包制作学术会议海报"},{"content":" \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析   ","permalink":"/blog/2022-09-04-geospatial-plotting/","summary":" \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析   ","title":"R语言 | 绘制中国地图"},{"content":"R圈谢一辉大神开发的 tinytex包，是一个强悍精干的Tex软件包。如果接触过LaTex的同学应该知道，配置好Latex的环境需要几个G，十分占用空间。其次，TinyTex将LaTex包的配置、安装、渲染简化为几个函数。\n配置TinyTex 安装和维护 TinyTex环境十分简单，首先我们安装好R语言的tinytex包，之后用该包安装TinyTex软件。考虑到 Tex资源站CTAN部署在海外，使用该站点下载资源包会很慢，所以这里配置为国内清华站点。\n#安装R语言中的tinytex包 install.packages(\u0026#34;tinytex\u0026#34;) #使用tinytex包安装TinyTex tinytex::install_tinytex() #设置资源镜像，加速资源的下载安装 tinytex::tlmgr_repo(\u0026#39;http://mirrors.tuna.tsinghua.edu.cn/CTAN/\u0026#39;) #tinytex::tl_pkgs() #查看安装的latex包 #length(tinytex::tl_pkgs()) #tinytex::tinytex_root() # 查看默认安装路径 \n编译 LaTex文档是一团源代码，我们需要将其编译(渲染)为PDF文件。\n#首先记得设置当前工作路径 setwd('tex文件所在的文件') tinytex::pdflatex('英文tex文件路径') tinytex::xelatex('中英文tex文件路径') #推荐 tinytex::latexmk('中英文tex文件路径') #推荐  我们经常跟中英文一起打交道，所以编译方法使用xelatex。 pdflatex或xelatex渲染过程可能会缺一些包，需要手动安装。而latexmk会在编译过程中自动安装缺失的包，不过中文最后还是\n TinyTex缺点是初学者编译慢，用的越多越久，越好用，工具使用的收益曲线是向上的。而TexLive、MacTex等使用难度是水平的。\n \n维护 TinyTeX 的主要维护工作应该是安装 LaTeX 包以及更新。\n 如果编译 PDF 时出现缺失 LaTeX 包的错误信息，可以用命令行 tinytex::tlmgr_search() 根据缺失文件名搜索包名，并用 tinytex::tlmgr_install() 安装缺失的包； 命令 tinytex::tlmgr_update() 可以更新整个 TeX Live 系统。R 用户可以使用 R 包 tinytex 中的相应函数，如：  tinytex::tlmgr_search(\u0026#39;framed.sty\u0026#39;) # 搜索包含 framed.sty 文件的 LaTeX 包 tinytex::tlmgr_install(\u0026#39;framed\u0026#39;) # 安装 framed 包 tinytex::tlmgr_update() # 更新 TeX Live 如果你想降低这些搜包、装包操作，可以考虑像 TexLive 那样装几个G。谢一辉大神今年推送了一个新资源，只需要安装4000个包2G的即可绑定常用资源包。\n设置环境变量 TINYTEX_INSTALLER=TinyTeX-2, 之后运行以下代码\n# increase the download timeout to 2 hours just in case the # default 1 hour is not enough (you can further increase it) options(timeout = 7200) tinytex::install_tinytex(bundle = \u0026#39;TinyTeX-2\u0026#39;) 勘察一下电脑内latex包数量\nlength(tinytex::tl_pkgs()) Wow~~，达到4197!! 如果你愿意TinyTex，也可以变成HugeTex。最后再简单的工具，如果只看依然会看着容易用着难，还是要多动手的。\n\n学习资料   https://yihui.org/tinytex/cn/\n  https://yihui.org/tinytex/\n  https://yihui.org/en/2022/05/tinytex-full/\n  https://github.com/rstudio/tinytex-releases/\n  \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-09-01-how_to_use_tinytex/","summary":"R圈谢一辉大神开发的 tinytex包，是一个强悍精干的Tex软件包。如果接触过LaTex的同学应该知道，配置好Latex的环境需要几个G，十分占用空间。其次，TinyTex将LaTex包的配置、安装、渲染简化为几个函数。\n配置TinyTex 安装和维护 TinyTex环境十分简单，首先我们安装好R语言的tinytex包，之后用该包安装TinyTex软件。考虑到 Tex资源站CTAN部署在海外，使用该站点下载资源包会很慢，所以这里配置为国内清华站点。\n#安装R语言中的tinytex包 install.packages(\u0026#34;tinytex\u0026#34;) #使用tinytex包安装TinyTex tinytex::install_tinytex() #设置资源镜像，加速资源的下载安装 tinytex::tlmgr_repo(\u0026#39;http://mirrors.tuna.tsinghua.edu.cn/CTAN/\u0026#39;) #tinytex::tl_pkgs() #查看安装的latex包 #length(tinytex::tl_pkgs()) #tinytex::tinytex_root() # 查看默认安装路径 \n编译 LaTex文档是一团源代码，我们需要将其编译(渲染)为PDF文件。\n#首先记得设置当前工作路径 setwd('tex文件所在的文件') tinytex::pdflatex('英文tex文件路径') tinytex::xelatex('中英文tex文件路径') #推荐 tinytex::latexmk('中英文tex文件路径') #推荐  我们经常跟中英文一起打交道，所以编译方法使用xelatex。 pdflatex或xelatex渲染过程可能会缺一些包，需要手动安装。而latexmk会在编译过程中自动安装缺失的包，不过中文最后还是\n TinyTex缺点是初学者编译慢，用的越多越久，越好用，工具使用的收益曲线是向上的。而TexLive、MacTex等使用难度是水平的。\n \n维护 TinyTeX 的主要维护工作应该是安装 LaTeX 包以及更新。\n 如果编译 PDF 时出现缺失 LaTeX 包的错误信息，可以用命令行 tinytex::tlmgr_search() 根据缺失文件名搜索包名，并用 tinytex::tlmgr_install() 安装缺失的包； 命令 tinytex::tlmgr_update() 可以更新整个 TeX Live 系统。R 用户可以使用 R 包 tinytex 中的相应函数，如：  tinytex::tlmgr_search(\u0026#39;framed.sty\u0026#39;) # 搜索包含 framed.sty 文件的 LaTeX 包 tinytex::tlmgr_install(\u0026#39;framed\u0026#39;) # 安装 framed 包 tinytex::tlmgr_update() # 更新 TeX Live 如果你想降低这些搜包、装包操作，可以考虑像 TexLive 那样装几个G。谢一辉大神今年推送了一个新资源，只需要安装4000个包2G的即可绑定常用资源包。","title":"Latex | 为Rmarkdown配置tinytex环境"},{"content":"Rmarkdown是R语言的代码文件类型，能渲染成html、pdf、doc等文件。但需要用到复杂的YAML进行参数设置，太难记住了。不过有了ymlthis包， 难度大大降低。\n安装ymlthis knit渲染效果一般由Rmarkdown开头的YAML设置。形如\ntitle: \u0026#34;Rmarkdown用法\u0026#34; author: \u0026#34;大邓\u0026#34; date: \u0026#34;2022-07-21\u0026#34; output: html_document: #输出html toc: true #生成目录 toc_float: true #目录浮动 theme: readable #readable主题 number_sections: yes #章节自动排序号 df_print: paged #html中的data.frame数据能翻页 yaml比较难记住，很容易设置错误，可以安装 ymlthis包，该包可以帮我们省去记忆之苦。\ninstall.packages(\u0026#34;ymlthis\u0026#34;) \n配置YAML ymlthis安装成功后，就可以使用该快捷工具。在工具栏点击 Addins\n找到 Write New R Markdown or YAML File\n复杂的 html渲染参数设置，如标题、作者、渲染效果、目录、主题等，瞬间变的简单起来。\n最后，.Rmd文件渲染为html文件的操作方法。\n广而告之  长期征稿 长期招募小伙伴 付费视频课程 | Python实证指标构建与文本分析 大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与直播课。 如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的录播课。 如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读有偿说明  ","permalink":"/blog/ymlthis_rmarkdown_ymal/","summary":"Rmarkdown是R语言的代码文件类型，能渲染成html、pdf、doc等文件。但需要用到复杂的YAML进行参数设置，太难记住了。不过有了ymlthis包， 难度大大降低。\n安装ymlthis knit渲染效果一般由Rmarkdown开头的YAML设置。形如\ntitle: \u0026#34;Rmarkdown用法\u0026#34; author: \u0026#34;大邓\u0026#34; date: \u0026#34;2022-07-21\u0026#34; output: html_document: #输出html toc: true #生成目录 toc_float: true #目录浮动 theme: readable #readable主题 number_sections: yes #章节自动排序号 df_print: paged #html中的data.frame数据能翻页 yaml比较难记住，很容易设置错误，可以安装 ymlthis包，该包可以帮我们省去记忆之苦。\ninstall.packages(\u0026#34;ymlthis\u0026#34;) \n配置YAML ymlthis安装成功后，就可以使用该快捷工具。在工具栏点击 Addins\n找到 Write New R Markdown or YAML File\n复杂的 html渲染参数设置，如标题、作者、渲染效果、目录、主题等，瞬间变的简单起来。\n最后，.Rmd文件渲染为html文件的操作方法。\n广而告之  长期征稿 长期招募小伙伴 付费视频课程 | Python实证指标构建与文本分析 大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与直播课。 如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的录播课。 如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读有偿说明  ","title":"ymlthis包 | 为Rmarkdown文件轻松设置YMAL"},{"content":"之前一直没有好的办法采集，最近发现一个神奇的qdata包可以采集百度指数不太好采集。\n安装 打开命令行(cmd、terminal)，\n# 避免与pycryptodome冲突 pip3 uninstall pycrypto #安装最新的qdata pip3 install --upgrade qdata  \n百度指数 qdata包内置百度指数的省份(城市)对应的地区代码，以省份代码为例\nfrom qdata.baidu_index import PROVINCE_CODE, CITY_CODE PROVINCE_CODE Run\n{'山东': '901', '贵州': '902', '江西': '903', '重庆': '904', '内蒙古': '905', '湖北': '906', '辽宁': '907', '湖南': '908', .... '甘肃': '925', '新疆': '926', '河南': '927', '安徽': '928', '山西': '929', '海南': '930', '台湾': '931', '西藏': '932', '香港': '933', '澳门': '934'}  \n案例 采集\n 时间段 2022-05-01 ~ 2022-08-01 地区 山东 关键词 ['疫情', '锻炼', '居家']  的百度指数数据。\n\n准备你的cookie 在命令行 Python环境 下运行\nfrom qdata.baidu_login import get_cookie_by_qr_login get_cookie_by_qr_login() 上方代码运行结束后，弹出一个二维码窗体。\n使用百度相关app，笔者使用 百度网盘app 扫码, 命令行内出现了一串字符串就是cookie。\n代码 import time from qdata.baidu_index import PROVINCE_CODE from qdata.baidu_index import get_search_index import csv province = \u0026#39;山东\u0026#39; keywords = [\u0026#39;网购\u0026#39;, \u0026#39;居家\u0026#39;] start_dt = \u0026#39;2022-07-01\u0026#39; end_dt = \u0026#39;2022-08-01\u0026#39; #你的cookie cookie = \u0026#39;你的cookie\u0026#39; #数据存储于data文件夹内 with open(\u0026#39;data/{}.csv\u0026#39;.format(province), \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) as csvf: fieldnames = [\u0026#39;province\u0026#39;, \u0026#39;type\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;index\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() for info in get_search_index(keywords_list = [keywords], start_date = start_dt, end_date = end_dt, area = PROVINCE_CODE[province], cookies = cookie): data = {\u0026#39;province\u0026#39;: province, \u0026#39;type\u0026#39;: info[\u0026#39;type\u0026#39;], \u0026#39;date\u0026#39;: info[\u0026#39;date\u0026#39;], \u0026#39;index\u0026#39;: info[\u0026#39;index\u0026#39;]} print(data) writer.writerow(data) Run\n{'province': '山东', 'type': 'all', 'date': '2022-07-01', 'index': '200'} {'province': '山东', 'type': 'all', 'date': '2022-07-02', 'index': '148'} {'province': '山东', 'type': 'all', 'date': '2022-07-03', 'index': '257'} ... {'province': '山东', 'type': 'pc', 'date': '2022-07-01', 'index': '59'} {'province': '山东', 'type': 'pc', 'date': '2022-07-02', 'index': '0'} {'province': '山东', 'type': 'pc', 'date': '2022-07-03', 'index': '118'} ... {'province': '山东', 'type': 'wise', 'date': '2022-07-01', 'index': '141'} {'province': '山东', 'type': 'wise', 'date': '2022-07-02', 'index': '148'} {'province': '山东', 'type': 'wise', 'date': '2022-07-03', 'index': '139'}  type字段的含义\n all 信息来自 PC+移动 pc 信息来自 PC wise 信息来自 移动  山东 2022-07-01 PC+移动 的指数是 200， 刚好等于 pc59+移动141 ，也等于 网购65+居家135 。\n最后数据存储于data文件夹内，如下图。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/qdata_collect_baidu_index/","summary":"之前一直没有好的办法采集，最近发现一个神奇的qdata包可以采集百度指数不太好采集。\n安装 打开命令行(cmd、terminal)，\n# 避免与pycryptodome冲突 pip3 uninstall pycrypto #安装最新的qdata pip3 install --upgrade qdata  \n百度指数 qdata包内置百度指数的省份(城市)对应的地区代码，以省份代码为例\nfrom qdata.baidu_index import PROVINCE_CODE, CITY_CODE PROVINCE_CODE Run\n{'山东': '901', '贵州': '902', '江西': '903', '重庆': '904', '内蒙古': '905', '湖北': '906', '辽宁': '907', '湖南': '908', .... '甘肃': '925', '新疆': '926', '河南': '927', '安徽': '928', '山西': '929', '海南': '930', '台湾': '931', '西藏': '932', '香港': '933', '澳门': '934'}  \n案例 采集\n 时间段 2022-05-01 ~ 2022-08-01 地区 山东 关键词 ['疫情', '锻炼', '居家']  的百度指数数据。","title":"百度指数 | 使用qdata采集百度指数"},{"content":" 作者:张延丰 哈工程在读博士\n 近年来，企业社会责任（csr)已成为全球学术界研究的热点。国内外各大顶刊都先后刊登了多篇关于csr的文章，比如《企业绿色创新实践如何破解“和谐共生”难题？》（发表于管理世界）、《负责任的国际投资：ESG与中国OFDI》（发表于经济研究）、《Is my company really doing good? Factors influencing employees' evaluation of the authenticity of their company\u0026rsquo;s corporate social responsibility engagement》（发表于JBR）等。这些文章核心变量的构建大都基于对企业社会责任报告的内容分析和挖掘。比如《企业绿色创新实践如何破解“和谐共生”难题？》的被解释变量（绿色创新）以及部分解释变量（二元合法性和伦理型领导）。可见，社会责任报告对于我们研究esg至关重要。因此，接下来小编就带大家爬取深交所上市公司历年的社会责任报告，希望能够给大家带来一些帮助。\n获取数据集 采集4000多个pdf文件。经过数据清洗，将20G的pdf数据，汇总整理到170M的csv文件内。\n数据整理不易，如需获取本数据集，请转发本文至朋友圈集赞满30+， 加微信【372335839】，备注【深圳ESG数据集】\n\n一、构建网络爬虫 数据采集分为多个步骤\n 找网址规律(GET or POST), 构造url参数 伪装请求，防止被封 构造csv，存储信心 执行整个爬虫  1.1 url 打开X交所的 http://www.szse.cn/disclosure/listed/notice/ ，同时打开浏览器开发者工具network面板，在截图左侧输入框输入关键词 『社会责任报告』，按下回车。\n此时开发者工具network面板出现很多网络交换信息， 点击检查发现下图\n发现该页面数据是POST请求，网址为\nhttp://www.szse.cn/api/disc/announcement/annList?random=random参数 1.2 headers 同时也能发现伪装头参数，现将两个重要信息整理为\nurl = \u0026#39;http://www.szse.cn/api/disc/announcement/annList?random={}\u0026#39;.format(random.random()) #伪装头 headers = {\u0026#39;Accept\u0026#39;: \u0026#39;application/json, text/javascript, */*; q=0.01\u0026#39;, \u0026#39;Accept-Encoding\u0026#39;: \u0026#39;gzip, deflate\u0026#39;, \u0026#39;Accept-Language\u0026#39;: \u0026#39;zh-CN,zh;q=0.9,en;q=0.8\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Host\u0026#39;: \u0026#39;www.szse.cn\u0026#39;, \u0026#39;Origin\u0026#39;: \u0026#39;http://www.szse.cn\u0026#39;, \u0026#39;Proxy-Connection\u0026#39;: \u0026#39;close\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;http://www.szse.cn/disclosure/listed/fixed/index.html\u0026#39;, \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\u0026#39;, \u0026#39;X-Request-Type\u0026#39;: \u0026#39;ajax\u0026#39;, \u0026#39;X-Requested-With\u0026#39;: \u0026#39;XMLHttpRequest\u0026#39;} 1.3 data参数 POST请求需要构造data参数，在开发者对应于payload, 整理为Python格式\n\n1.4 preview 看到左侧渲染后的数据，同时也能在开发者工具network面板看到肉眼背后的源数据。我们使用preview预览截图再次确认网址规律没有问题。\nkeyword = \u0026#39;社会责任报告\u0026#39; page = 1 #post方法参数 payload ={\u0026#34;seDate\u0026#34;: [\u0026#34;\u0026#34;,\u0026#34;\u0026#34;], \u0026#34;searchKey\u0026#34;: [keyword], \u0026#34;channelCode\u0026#34;: [\u0026#34;listedNotice_disc\u0026#34;], \u0026#34;pageSize\u0026#34;: 50, \u0026#34;pageNum\u0026#34;: page} 1.5 csv 现在已经把爬虫最重要的工作做完了，剩下的就是想办法构造出csv，并将数据存入csv。\n#定义csv字段，存储PDF链接信息至data/esg_links.csv csvf = open(\u0026#39;data/esg_links.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;code\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;pdf_link\u0026#39;, \u0026#39;size\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() test_data = {\u0026#39;pdf_link\u0026#39;: \u0026#39;测试pdf文件链接\u0026#39;, \u0026#39;code\u0026#39;: \u0026#39;测试股票代码\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;股票名称\u0026#39;, \u0026#39;title\u0026#39;: \u0026#39;报告名称\u0026#39;, \u0026#39;date\u0026#39;: \u0026#39;发布日期\u0026#39;, \u0026#39;size\u0026#39;: \u0026#39;pdf文件字节大小\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;数据id\u0026#39;} writer.writerow(test_data) #关闭csv csvf.close() \n二、爬虫代码(完整) 当你看到本文时，该完整代码很有可能会随着网站变化而失效。不要悲伤难过， 按照爬虫思路自己diy即可。如果没有爬虫基础，学习 大邓的B站爬虫视频 ，\n自己懂爬虫原理diy代码，比改别人的代码来的更容易。将前面的准备工作组织起来, 就形成了下面的完整代码\nimport requests import random import json import time import csv keyword = \u0026#34;社会责任报告\u0026#34; #定义csv字段，存储PDF链接信息至data/esg_links.csv csvf = open(\u0026#39;data/esg_links.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;code\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;pdf_link\u0026#39;, \u0026#39;size\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() #伪装头 headers = {\u0026#39;Accept\u0026#39;: \u0026#39;application/json, text/javascript, */*; q=0.01\u0026#39;, \u0026#39;Accept-Encoding\u0026#39;: \u0026#39;gzip, deflate\u0026#39;, \u0026#39;Accept-Language\u0026#39;: \u0026#39;zh-CN,zh;q=0.9,en;q=0.8\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Host\u0026#39;: \u0026#39;www.szse.cn\u0026#39;, \u0026#39;Origin\u0026#39;: \u0026#39;http://www.szse.cn\u0026#39;, \u0026#39;Proxy-Connection\u0026#39;: \u0026#39;close\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;http://www.szse.cn/disclosure/listed/fixed/index.html\u0026#39;, \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\u0026#39;, \u0026#39;X-Request-Type\u0026#39;: \u0026#39;ajax\u0026#39;, \u0026#39;X-Requested-With\u0026#39;: \u0026#39;XMLHttpRequest\u0026#39;} page = 1 #post方法参数 payload ={\u0026#34;seDate\u0026#34;: [\u0026#34;\u0026#34;,\u0026#34;\u0026#34;], \u0026#34;searchKey\u0026#34;: [keyword], \u0026#34;channelCode\u0026#34;: [\u0026#34;listedNotice_disc\u0026#34;], \u0026#34;pageSize\u0026#34;: 50, \u0026#34;pageNum\u0026#34;: page} #发起请求 url = \u0026#39;http://www.szse.cn/api/disc/announcement/annList?random={}\u0026#39;.format(random.random()) resp = requests.post(url, headers=headers, data=json.dumps(payload)) #当data关键词有对应的非空列表，循环一直进行。 while resp.json()[\u0026#39;data\u0026#39;]: payload[\u0026#39;pageNum\u0026#39;] = page resp = requests.post(url, headers=headers, data=json.dumps(payload)) esgs = resp.json()[\u0026#39;data\u0026#39;] for esg in esgs: #以字典样式写入csv data = {\u0026#39;pdf_link\u0026#39;: \u0026#39;http://disc.static.szse.cn/download\u0026#39;+ esg[\u0026#39;attachPath\u0026#39;], #为防止股票代码被exel等软件识别为数字，特转为字符串，并加sz标识。 \u0026#39;code\u0026#39;: \u0026#39;sz\u0026#39;+str(esg[\u0026#39;secCode\u0026#39;][0]), \u0026#39;name\u0026#39;: esg[\u0026#39;secName\u0026#39;][0], \u0026#39;title\u0026#39;: esg[\u0026#39;title\u0026#39;], \u0026#39;date\u0026#39;: esg[\u0026#39;publishTime\u0026#39;], \u0026#39;size\u0026#39;: esg[\u0026#39;attachSize\u0026#39;], \u0026#39;id\u0026#39;: esg[\u0026#39;id\u0026#39;]} writer.writerow(data) page = page + 1 #关闭csv csvf.close() \n三、查看csv 使用pandas读取 data/esg_links.csv,\nimport pandas as pd df = pd.read_csv(\u0026#39;esg_links.csv\u0026#39;) df.head() len(df) Run\n4392 一共有4392条 「企业社会责任」 的报告数据。\n\n四、批量下载 下载就简单多了， 直接使用定义好的爬虫代码。\nimport requests def download(url, file): \u0026#34;\u0026#34;\u0026#34; 下载多媒体及文件 url： 多媒体文件链接（结尾有文件格式名） file: 存储文件的路径（结尾有文件格式名） \u0026#34;\u0026#34;\u0026#34; resp = requests.get(url) #获取到二进制数据 binarydata = resp.content #以二进制形式将数据流存入fname中 with open(file, \u0026#39;wb\u0026#39;) as f: f.write(binarydata) i = 0 for link, title in zip(df[\u0026#39;pdf_link\u0026#39;], df[\u0026#39;title\u0026#39;]): print(i, title) download(url=link, file=\u0026#39;data/{}.pdf\u0026#39;.format(title)) i = i + 1 Run\n0 山河药辅：山河药辅2021年度社会责任报告 1 新 希 望：2021年企业社会责任报告（英文版） 2 天原股份：宜宾天原集团股份有限公司社会责任报告 3 五 粮 液：2021年度社会责任报告（英文版） 4 中兵红箭：2021年度社会责任报告\t...... ...... 148 苏宁环球：2021年社会责任报告 149 蓝色光标：2021年度企业社会责任报告 150 开尔新材：2021年度社会责任报告 151 中顺洁柔：2021年社会责任报告 ...... ...... 4391 闽东电力：2006年度社会责任报告 4392 阳光发展：2006年度社会责任报告书 采集过程中，被封锁在所难免，所以记得每次停止采集的位置，在csv中删除该位置之前的数据。然后重新运行代码即可。\n注意 即时解决以上问题，可能遇到奇怪的问题。比如\n检查发现相比其他几百kb的pdf，问题文件大小只有几kb。问题可能是被网站封锁或网络不稳定导致，标记好问题pdf的链接，重新批量下载一遍。\n\n汇总至csv 很多企业社会责任报告是图片合成的，所以这里的pdf体积很大。将data文件夹中的4000多个pdf汇总至esg_data.csv中，能节约出电脑内存空间，也方便后续数据分析。\nfrom pdfdocx import read_pdf import os import csv i = 1 #新建esg_data.csv，用于存储企业社会责任报告数据 with open(\u0026#39;esg_data.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) as csvf: fieldnames = [\u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;code\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;pdf_link\u0026#39;, \u0026#39;size\u0026#39;, \u0026#39;report_content\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() files = os.listdir(\u0026#39;data\u0026#39;) for file in files: record_of_df = df[df[\u0026#39;title\u0026#39;]==file.replace(\u0026#39;.pdf\u0026#39;, \u0026#39;\u0026#39;)].iloc[0, :] data = record_of_df.to_dict() print(i, file) data[\u0026#39;report_content\u0026#39;] = read_pdf(\u0026#39;data/{}\u0026#39;.format(file)) writer.writerow(data) i = i + 1 最后，数据从20G的data文件夹(4000多个PDF)压缩为一个170M的esg_data.csv文件。\nesg_reports_df = pd.read_csv(\u0026#39;esg_data.csv\u0026#39;) esg_reports_df.head() len(esg_reports_df) Run\n4346 \n五、相关文献 [1]解学梅, \u0026amp; 朱琪玮. (2021). 企业绿色创新实践如何破解 “和谐共生” 难题?. 管理世界, 37(1), 128-149. [2]谢红军 \u0026amp; 吕雪.(2022).负责任的国际投资：ESG与中国OFDI. 经济研究(03),83-99. [3]Schaefer, S. D., Terlutter, R., \u0026amp; Diehl, S. (2019). Is my company really doing good? Factors influencing employees\u0026#39; evaluation of the authenticity of their company\u0026#39;s corporate social responsibility engagement. Journal of business research, 101, 128-143. \n六、其他(广告)  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/coporate_social_responsibility_datasets/","summary":"作者:张延丰 哈工程在读博士\n 近年来，企业社会责任（csr)已成为全球学术界研究的热点。国内外各大顶刊都先后刊登了多篇关于csr的文章，比如《企业绿色创新实践如何破解“和谐共生”难题？》（发表于管理世界）、《负责任的国际投资：ESG与中国OFDI》（发表于经济研究）、《Is my company really doing good? Factors influencing employees' evaluation of the authenticity of their company\u0026rsquo;s corporate social responsibility engagement》（发表于JBR）等。这些文章核心变量的构建大都基于对企业社会责任报告的内容分析和挖掘。比如《企业绿色创新实践如何破解“和谐共生”难题？》的被解释变量（绿色创新）以及部分解释变量（二元合法性和伦理型领导）。可见，社会责任报告对于我们研究esg至关重要。因此，接下来小编就带大家爬取深交所上市公司历年的社会责任报告，希望能够给大家带来一些帮助。\n获取数据集 采集4000多个pdf文件。经过数据清洗，将20G的pdf数据，汇总整理到170M的csv文件内。\n数据整理不易，如需获取本数据集，请转发本文至朋友圈集赞满30+， 加微信【372335839】，备注【深圳ESG数据集】\n\n一、构建网络爬虫 数据采集分为多个步骤\n 找网址规律(GET or POST), 构造url参数 伪装请求，防止被封 构造csv，存储信心 执行整个爬虫  1.1 url 打开X交所的 http://www.szse.cn/disclosure/listed/notice/ ，同时打开浏览器开发者工具network面板，在截图左侧输入框输入关键词 『社会责任报告』，按下回车。\n此时开发者工具network面板出现很多网络交换信息， 点击检查发现下图\n发现该页面数据是POST请求，网址为\nhttp://www.szse.cn/api/disc/announcement/annList?random=random参数 1.2 headers 同时也能发现伪装头参数，现将两个重要信息整理为\nurl = \u0026#39;http://www.szse.cn/api/disc/announcement/annList?random={}\u0026#39;.format(random.random()) #伪装头 headers = {\u0026#39;Accept\u0026#39;: \u0026#39;application/json, text/javascript, */*; q=0.01\u0026#39;, \u0026#39;Accept-Encoding\u0026#39;: \u0026#39;gzip, deflate\u0026#39;, \u0026#39;Accept-Language\u0026#39;: \u0026#39;zh-CN,zh;q=0.9,en;q=0.8\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Host\u0026#39;: \u0026#39;www.","title":"17G数据 | 企业社会责任报告数据集"},{"content":" 使用「R语言」 的 blogdown + hugo + + githubPages 搭建静态博客系统，使用用 Rstudio 专注于写作。\nHugo Hugo是一种特别受欢迎的静态站点框架，基于Go语言开发，建站速度和上手难度都很亲民。\n\n Blogdown 而 blogdown 是 R语言的建站包， 可以在Rstudio内一边写文档笔记一边渲染出html文件。下图是Rstudio 截图\n Rstudio左上角 为博客文档撰写区(代码、文档、图片) Rstudio左下方 为命令行区域 Rstudio右下方 为博客文档渲染效果  \n对了， blogdown作者 谢益辉 是一位中国人，他同时也是 bookdown、 tinytex 等包的作者。 学习R语言的同学应该很熟悉的。目前很多 R语言 优秀的文档都是使用 bookdown包，比如\n\n 工作流程(旧) 从21年5月起，大邓第一次使用 Hugo + Gihub Page 建立个人博客站，之后又更换为 hugo-papermod主题；新增留言功能。界面美观，代码复制很方便，适合公众号关注者浏览学习。但对大邓而言，工作流程异常繁琐\njupyter notebook 写代码文档，测试代码可运行。 从 jupyter 中下载为 markdown 文档，编辑用的工具是Typora。 Hugo命令行更新本地(电脑内)的站点仓库。 使用 Github Desktop 推送至Github Page  操作步骤中，夹杂着大量的命令行操作，我也记不住这些命令行，是单独存放在一个markdown笔记中，用的时候复制一下。命令行给大家看看\n#切换至电脑博客仓库 cd /Users/大邓/Desktop/Blog/Github #启动本地服务，浏览器测试 hugo server -t PaperMod --buildDrafts #生成站点，待推送至github hugo --theme=PaperMod --baseUrl=\u0026#39;/\u0026#39; --buildDrafts \n总之就是复杂，最近用 R语言 发现有 blogdown包 可以将其中的步骤压缩， 主要工作集中在 Rstudio 和 Github Desktop。\n\n 新工作流程 在 Rstudio 内新建 Rmarkdown 文档，内部可进行 R语言(或Python) 的代码撰写 编译 Rmarkdown文档 为 html， Github Desktop 推送至仓库，更新站点。  需要用到的命令，仅仅有以下4个\n #新建博客 .Rmd文件 blogdown::new_post() #启动本地服务，一遍写文档，网页渲染效果随时可见 blogdown::serve_site() #停止本地服务 blogdown::stop_server() #将.Rmd文件渲染为html等站点文件 blogdown::build_site() \n 本文参考资料  blogdown Docs Hugo-PaperMod 用 R 语言的 blogdown+hugo+netlify+github 建博客  \n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析   ","permalink":"/blog/blogdown_for_hugo_website/","summary":"使用「R语言」 的 blogdown + hugo + + githubPages 搭建静态博客系统，使用用 Rstudio 专注于写作。\nHugo Hugo是一种特别受欢迎的静态站点框架，基于Go语言开发，建站速度和上手难度都很亲民。\n\n Blogdown 而 blogdown 是 R语言的建站包， 可以在Rstudio内一边写文档笔记一边渲染出html文件。下图是Rstudio 截图\n Rstudio左上角 为博客文档撰写区(代码、文档、图片) Rstudio左下方 为命令行区域 Rstudio右下方 为博客文档渲染效果  \n对了， blogdown作者 谢益辉 是一位中国人，他同时也是 bookdown、 tinytex 等包的作者。 学习R语言的同学应该很熟悉的。目前很多 R语言 优秀的文档都是使用 bookdown包，比如\n\n 工作流程(旧) 从21年5月起，大邓第一次使用 Hugo + Gihub Page 建立个人博客站，之后又更换为 hugo-papermod主题；新增留言功能。界面美观，代码复制很方便，适合公众号关注者浏览学习。但对大邓而言，工作流程异常繁琐\njupyter notebook 写代码文档，测试代码可运行。 从 jupyter 中下载为 markdown 文档，编辑用的工具是Typora。 Hugo命令行更新本地(电脑内)的站点仓库。 使用 Github Desktop 推送至Github Page  操作步骤中，夹杂着大量的命令行操作，我也记不住这些命令行，是单独存放在一个markdown笔记中，用的时候复制一下。命令行给大家看看\n#切换至电脑博客仓库 cd /Users/大邓/Desktop/Blog/Github #启动本地服务，浏览器测试 hugo server -t PaperMod --buildDrafts #生成站点，待推送至github hugo --theme=PaperMod --baseUrl=\u0026#39;/\u0026#39; --buildDrafts","title":"blogdown包 | 使用R语言维护Hugo静态网站"},{"content":" 任务 使用R语言，对多个文件夹内的数百个txt汇总到一个csv文件内。\n\n 数据集 01-21年，全国各地市政府工作报告数据集。\n\n 任务分解 使用list.files获取文件路径列表 定义需要的函数  使用readtext::readtext()函数读取报告文本 年份函数、省份函数  对每个文件路径，根据2得到三个字段信息，构造tibble结构； 步骤2和步骤3使用bind_cols合并成一个tibble readr::write_csv()函数存至data.csv 审查data.csv  数据存在province文件内, 该点击下载该数据集\n1. txt路径列表 使用 list.files函数查看\n 文件夹路径列表 文件路径列表  province内的文件夹路径列表\ndirs \u0026lt;- list.files(\u0026#39;province\u0026#39;, full.names = TRUE) head(dirs) ## [1] \u0026quot;province/上海\u0026quot; \u0026quot;province/云南\u0026quot; \u0026quot;province/内蒙古\u0026quot; \u0026quot;province/北京\u0026quot; ## [5] \u0026quot;province/吉林\u0026quot; \u0026quot;province/四川\u0026quot; 所有省份文件夹内的文件路径列表\nfiles \u0026lt;- list.files(dirs, full.names = TRUE) head(files) ## [1] \u0026quot;province/上海/2003年上海政府工作报告.txt\u0026quot; ## [2] \u0026quot;province/上海/2004年上海政府工作报告.txt\u0026quot; ## [3] \u0026quot;province/上海/2005年上海政府工作报告.txt\u0026quot; ## [4] \u0026quot;province/上海/2006年上海政府工作报告.txt\u0026quot; ## [5] \u0026quot;province/上海/2007年上海政府工作报告.txt\u0026quot; ## [6] \u0026quot;province/上海/2008年上海政府工作报告.txt\u0026quot; 共有617个txt文件\nlength(files) ## [1] 617 \n 2.1 readtext读取txt 使用 readtext::readtext 批量读取 多个txt\ntxts_df \u0026lt;- readtext::readtext(files) head(txts_df) ## readtext object consisting of 6 documents and 0 docvars. ## # Description: df [6 × 2] ## doc_id text ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2003年上海政府工作报告.txt \u0026quot;\\\u0026quot; 各位代表， 现在\\\u0026quot;...\u0026quot; ## 2 2004年上海政府工作报告.txt \u0026quot;\\\u0026quot; 各位代表：\\n\\n \\\u0026quot;...\u0026quot; ## 3 2005年上海政府工作报告.txt \u0026quot;\\\u0026quot;各位代表：\\n\\n　现\\\u0026quot;...\u0026quot; ## 4 2006年上海政府工作报告.txt \u0026quot;\\\u0026quot;各位代表：\\n　上海\\\u0026quot;...\u0026quot; ## 5 2007年上海政府工作报告.txt \u0026quot;\\\u0026quot;　政府工作报告\\n　\\\u0026quot;...\u0026quot; ## 6 2008年上海政府工作报告.txt \u0026quot;\\\u0026quot;\\n\\t政府工作报告\\n\\n\\\u0026quot;...\u0026quot; 检查text字段长度，是否为617.\nlength(txts_df[[\u0026#39;text\u0026#39;]]) ## [1] 617 \n 2.2 定义功能函数 数据整理到一个csv，我们想保存四个字段，分别是\n txt文件名 年份 省(市)名 工作报告内容  年份和省份需要通过定义函数实现~\ns\u0026lt;-basename(\u0026quot;province/上海/2003年上海政府工作报告.txt\u0026quot;) substr(s, 1, 4) ## [1] \u0026quot;2003\u0026quot; library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.3.6 ✔ purrr 0.3.4 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.9 ## ✔ tidyr 1.2.0 ✔ stringr 1.4.0 ## ✔ readr 2.1.2 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() year_func \u0026lt;- function(filepath){ year \u0026lt;- filepath %\u0026gt;% basename() %\u0026gt;% substr(1, 4) return (year) } name_func \u0026lt;- function(file){ file \u0026lt;- basename(file) name \u0026lt;- gsub(\u0026#39;政府工作报告.txt\u0026#39;, \u0026#39;\u0026#39;, file) name \u0026lt;- stringr::str_sub(name, start=6) return (name) } file \u0026lt;- \u0026quot;province/上海/2003年上海政府工作报告.txt\u0026quot; year_func(file) ## [1] \u0026quot;2003\u0026quot; name_func(file) ## [1] \u0026quot;上海\u0026quot; txts_df 是一个特殊的tibble数据类型。 现在需要构造年份、省份函数，获取另外一个tibble。\nyear_province_df \u0026lt;- tibble( year = year_func(txts_df$doc_id), province = lapply(txts_df$doc_id, name_func) %\u0026gt;% unlist() ) head(year_province_df) ## # A tibble: 6 × 2 ## year province ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2003 上海 ## 2 2004 上海 ## 3 2005 上海 ## 4 2006 上海 ## 5 2007 上海 ## 6 2008 上海 \n 4. 合并两个tibble cbind_rows()合并两个tibble\nres_df \u0026lt;- bind_cols(year_province_df, txts_df) head(res_df) ## # A tibble: 6 × 4 ## year province doc_id text ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2003 上海 2003年上海政府工作报告.txt \u0026quot; 各位代表， 现在，我代表上海市人… ## 2 2004 上海 2004年上海政府工作报告.txt \u0026quot; 各位代表：\\n\\n 现在，我代表上… ## 3 2005 上海 2005年上海政府工作报告.txt \u0026quot;各位代表：\\n\\n　现在，我代表上海… ## 4 2006 上海 2006年上海政府工作报告.txt \u0026quot;各位代表：\\n　上海市国民经济和社… ## 5 2007 上海 2007年上海政府工作报告.txt \u0026quot;　政府工作报告\\n　――2007年1月2… ## 6 2008 上海 2008年上海政府工作报告.txt \u0026quot;\\n\\t政府工作报告\\n\\n\\t——2008年1月2… \n 5. 存入csv 使用 write.table(x, file, sep) 写入data.csv\n x 待存储数据对象 file csv文件路径 delim 分割符  ?readr::write_csv readr::write_csv(x=res_df, file=\u0026#39;data.csv\u0026#39;, col_names=T)  \n 6. 检查data.csv 尝试读取 data.csv\ndf \u0026lt;- readr::read_csv(\u0026#39;data.csv\u0026#39;) ## Rows: 617 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (3): province, doc_id, text ## dbl (1): year ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(df) ## # A tibble: 6 × 4 ## year province doc_id text ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 2003 上海 2003年上海政府工作报告.txt \u0026quot;各位代表， 现在，我代表上海市人民… ## 2 2004 上海 2004年上海政府工作报告.txt \u0026quot;各位代表：\\n\\n 现在，我代表上海… ## 3 2005 上海 2005年上海政府工作报告.txt \u0026quot;各位代表：\\n\\n　现在，我代表上海… ## 4 2006 上海 2006年上海政府工作报告.txt \u0026quot;各位代表：\\n　上海市国民经济和社… ## 5 2007 上海 2007年上海政府工作报告.txt \u0026quot;　政府工作报告\\n　――2007年1月2… ## 6 2008 上海 2008年上海政府工作报告.txt \u0026quot;\\n\\t政府工作报告\\n\\n\\t——2008年1月2…   ","permalink":"/blog/multiple_txts_into_csv/","summary":"任务 使用R语言，对多个文件夹内的数百个txt汇总到一个csv文件内。\n\n 数据集 01-21年，全国各地市政府工作报告数据集。\n\n 任务分解 使用list.files获取文件路径列表 定义需要的函数  使用readtext::readtext()函数读取报告文本 年份函数、省份函数  对每个文件路径，根据2得到三个字段信息，构造tibble结构； 步骤2和步骤3使用bind_cols合并成一个tibble readr::write_csv()函数存至data.csv 审查data.csv  数据存在province文件内, 该点击下载该数据集\n1. txt路径列表 使用 list.files函数查看\n 文件夹路径列表 文件路径列表  province内的文件夹路径列表\ndirs \u0026lt;- list.files(\u0026#39;province\u0026#39;, full.names = TRUE) head(dirs) ## [1] \u0026quot;province/上海\u0026quot; \u0026quot;province/云南\u0026quot; \u0026quot;province/内蒙古\u0026quot; \u0026quot;province/北京\u0026quot; ## [5] \u0026quot;province/吉林\u0026quot; \u0026quot;province/四川\u0026quot; 所有省份文件夹内的文件路径列表\nfiles \u0026lt;- list.files(dirs, full.names = TRUE) head(files) ## [1] \u0026quot;province/上海/2003年上海政府工作报告.txt\u0026quot; ## [2] \u0026quot;province/上海/2004年上海政府工作报告.txt\u0026quot; ## [3] \u0026quot;province/上海/2005年上海政府工作报告.txt\u0026quot; ## [4] \u0026quot;province/上海/2006年上海政府工作报告.txt\u0026quot; ## [5] \u0026quot;province/上海/2007年上海政府工作报告.","title":"使用R将多个txt汇总到一个csv文件中"},{"content":"   renv特性 renv 包可帮助您为 R 项目创建可重现的环境。 使用 renv 使 R语言项目更：\n  『隔离的』：为一个项目安装新的或更新的包不会破坏其他项目，反之亦然。 那是因为 renv 为每个项目提供了自己的私有包。\n  『便携的』：轻松将项目从一台计算机传输到另一台计算机，甚至跨不同平台。 renv 可以很容易地安装项目所依赖的包。\n  『可重现的』： renv 记录项目所依赖的确切软件包版本，并确保无论您走到哪里都可以安装这些确切版本。\n  \n安装 从CRAN安装最新的 renv包\ninstall.packages(\u0026#34;renv\u0026#34;) \n用法 renv::init() 使用 renv::init() 来初始化新(或已有)项目的R环境。通过初始化可以为项目建立独立的私有包，确保项目所需包都能得到安装。\nrenv::snapshot() 保存项目状态， 项目用到的R包会记录在lockfile中，被称为renv.lock。\nrenv::restore() 之后，如果需要将项目文件夹转移到新的电脑， 可以通过 renv::restore() 来重装 lockfile文件记录到(项目需要的)的R包。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/renv_environment_management_for_r/","summary":"   renv特性 renv 包可帮助您为 R 项目创建可重现的环境。 使用 renv 使 R语言项目更：\n  『隔离的』：为一个项目安装新的或更新的包不会破坏其他项目，反之亦然。 那是因为 renv 为每个项目提供了自己的私有包。\n  『便携的』：轻松将项目从一台计算机传输到另一台计算机，甚至跨不同平台。 renv 可以很容易地安装项目所依赖的包。\n  『可重现的』： renv 记录项目所依赖的确切软件包版本，并确保无论您走到哪里都可以安装这些确切版本。\n  \n安装 从CRAN安装最新的 renv包\ninstall.packages(\u0026#34;renv\u0026#34;) \n用法 renv::init() 使用 renv::init() 来初始化新(或已有)项目的R环境。通过初始化可以为项目建立独立的私有包，确保项目所需包都能得到安装。\nrenv::snapshot() 保存项目状态， 项目用到的R包会记录在lockfile中，被称为renv.lock。\nrenv::restore() 之后，如果需要将项目文件夹转移到新的电脑， 可以通过 renv::restore() 来重装 lockfile文件记录到(项目需要的)的R包。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"renv包 | R语言的项目环境管理库"},{"content":"使用IPython的display可以将pdf内容渲染到jupyter内，代码如下\npdf文件 本地pdf文件，可以在jupyter内显示\nfrom IPython.display import IFrame IFrame(\u0026#39;管理者短视主义影响企业长期投资吗_基于文本分析和机器学习_胡楠.pdf\u0026#39;, width=800, height=450) pdf链接 from IPython.display import IFrame IFrame(\u0026#39;https://arxiv.org/pdf/1406.2661.pdf\u0026#39;, width=800, height=450) 长期征稿  点击了解投稿   招募小伙伴  点击加入我们   文本分析视频课 想轻松而快捷的深刻了解一个领域，看视频(直播)学习是一个不错的方式。\n  大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与直播课。\n  如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的录播课。\n  如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读有偿说明\n   点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/render_pdf_content_in_jupyter/","summary":"使用IPython的display可以将pdf内容渲染到jupyter内，代码如下\npdf文件 本地pdf文件，可以在jupyter内显示\nfrom IPython.display import IFrame IFrame(\u0026#39;管理者短视主义影响企业长期投资吗_基于文本分析和机器学习_胡楠.pdf\u0026#39;, width=800, height=450) pdf链接 from IPython.display import IFrame IFrame(\u0026#39;https://arxiv.org/pdf/1406.2661.pdf\u0026#39;, width=800, height=450) 长期征稿  点击了解投稿   招募小伙伴  点击加入我们   文本分析视频课 想轻松而快捷的深刻了解一个领域，看视频(直播)学习是一个不错的方式。\n  大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与直播课。\n  如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的录播课。\n  如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读有偿说明\n   点击上方图片购买课程   点击进入详情页","title":"在jupyter中显示pdf内容"},{"content":"标量(位)图 matplotlib能绘制出美观的图，是一个挺好用的绘图包，但是在jupyter内渲染的图很丑，有种模糊感。\nimport matplotlib.pyplot as plt import numpy as np # Data for plotting t = np.arange(0.0, 2.0, 0.01) s = 1 + np.sin(2 * np.pi * t) fig, ax = plt.subplots() ax.plot(t, s) ax.set(xlabel=\u0026#39;time (s)\u0026#39;, ylabel=\u0026#39;voltage (mV)\u0026#39;, title=\u0026#39;About as simple as it gets, folks\u0026#39;) ax.grid() plt.show() ​\n矢量图 实际上，jupyter notebook内加上一行配置，就能让matplotlib输出矢量图(放大缩小不改变图片样子)。\n%config InlineBackend.figure_format = \u0026#39;svg\u0026#39; 配置后，浏览器会渲染出更精细化的图，如下图所示\nimport matplotlib import matplotlib.pyplot as plt import numpy as np %matplotlib inline %config InlineBackend.figure_format = \u0026#39;svg\u0026#39; # Data for plotting t = np.arange(0.0, 2.0, 0.01) s = 1 + np.sin(2 * np.pi * t) fig, ax = plt.subplots() ax.plot(t, s) ax.set(xlabel=\u0026#39;time (s)\u0026#39;, ylabel=\u0026#39;voltage (mV)\u0026#39;, title=\u0026#39;About as simple as it gets, folks\u0026#39;) ax.grid() plt.show() 保存 savefig可以保存为.pdf、.eps、 .svg、 .png等不同格式文件。 其中.pdf 或者 .eps 能方便地插入到 latex 中！\nimport matplotlib import matplotlib.pyplot as plt import numpy as np %matplotlib inline %config InlineBackend.figure_format = \u0026#39;svg\u0026#39; # Data for plotting t = np.arange(0.0, 2.0, 0.01) s = 1 + np.sin(2 * np.pi * t) fig, ax = plt.subplots() ax.plot(t, s) ax.set(xlabel=\u0026#39;time (s)\u0026#39;, ylabel=\u0026#39;voltage (mV)\u0026#39;, title=\u0026#39;About as simple as it gets, folks\u0026#39;) ax.grid() #矢量图svg plt.savefig(fname=\u0026#39;三角函数图.svg\u0026#39;) #标量图设置dpi高一点，输出的图精致一些 #plt.savefig(fname=\u0026#39;三角函数图.png\u0026#39;, dpi=300) #plt.savefig(fname=\u0026#39;三角函数图.pdf\u0026#39;, dpi=300) #plt.savefig(fname=\u0026#39;三角函数图.svg\u0026#39;, dpi=300) ​\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/matplotlib_config_settings_in_jupyter/","summary":"标量(位)图 matplotlib能绘制出美观的图，是一个挺好用的绘图包，但是在jupyter内渲染的图很丑，有种模糊感。\nimport matplotlib.pyplot as plt import numpy as np # Data for plotting t = np.arange(0.0, 2.0, 0.01) s = 1 + np.sin(2 * np.pi * t) fig, ax = plt.subplots() ax.plot(t, s) ax.set(xlabel=\u0026#39;time (s)\u0026#39;, ylabel=\u0026#39;voltage (mV)\u0026#39;, title=\u0026#39;About as simple as it gets, folks\u0026#39;) ax.grid() plt.show() ​\n矢量图 实际上，jupyter notebook内加上一行配置，就能让matplotlib输出矢量图(放大缩小不改变图片样子)。\n%config InlineBackend.figure_format = \u0026#39;svg\u0026#39; 配置后，浏览器会渲染出更精细化的图，如下图所示\nimport matplotlib import matplotlib.pyplot as plt import numpy as np %matplotlib inline %config InlineBackend.figure_format = \u0026#39;svg\u0026#39; # Data for plotting t = np.","title":"改善matplotlib在jupyter内的渲染效果"},{"content":" 作者: 张延丰\n哈工程管工在读博士，擅长数据采集\u0026amp;挖掘。\n文末可获取代码下载链接\n 马云在接受CNBC（美国消费者新闻与商业频道）采访时提出：“整个世界将变成数据，我认为这还是只是数据时代的开始。新浪潮即将来临，很多就业机会将被夺走。有些人会赶上潮流，变得更加富有和成功。但是对于那些落后的人，未来将是痛苦的。”就小编看来，这种说法在人文社科研究当中也同样适用。在当前数以万计甚至数以十万计研究样本“遍地走”的时代，若我们还拘泥于传统的“小样本”研究（比如样本量为100多的调查问卷数据等），不仅难以跟随时代的脚步，还会逐渐丧失学术竞争力、从而被时代淘汰。那么，究竟该如何获取属于自己的大样本数据呢？今天小编就带大家用selenium库来爬取国内某知名第三方企业信息平台（天**）的企业工商信息。\n\n一、自动打开网站页面 首先，数据爬取的第一步是利用selenium库启动浏览器，打开我们的目标网站。部分代码\n# @Author : Jacob-ZHANG import requests,base64 from PIL import Image import csv,re from selenium import webdriver import time,random #1启动浏览器。 #win browser=webdriver.Chrome(executable_path=\u0026#39;driver/chromedriver.exe\u0026#39;) #mac #browser=webdriver.Chrome(executable_path=\u0026#39;driver/chromedriver\u0026#39;) #2加入这个脚本可以避免被识别 browser.execute_cdp_cmd(\u0026#34;Page.addScriptToEvaluateOnNewDocument\u0026#34;, { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; Object.defineProperty(navigator, \u0026#39;webdriver\u0026#39;, { get: () =\u0026gt; undefined }) \u0026#34;\u0026#34;\u0026#34; }) #3延迟10s启动 browser.implicitly_wait(10) #4利用谷歌浏览器打开目标网页 browser.get(\u0026#39;https://pro.xxxxxx.com/searchx\u0026#39;) #5将窗口最大化 browser.maximize_window() #6给网页一些时间加载 time.sleep(random.randint(1, 2)) ... ... ... 非常简单，如下所示。\n需要说明的是：\n 第一，selenium库距今已经有近20年的历史，各类网站、浏览器大多都能对它进行识别，因此为了避免被反爬，我可以通过加入上述脚本（#2）来防止网站的识别。 第二，因为在完成#4的操作，也即利用浏览器打开网页后，显示的网页页面并非是最大化的窗口，这导致页面元素存在难以定位、从而报错的情况，因此我们在此处加上了窗口最大化的操作（#5）。 第三，在以后的操作里，类似#6的代码会频频出现，这主要是因为受限于网速，不得不为网页加载提供更多时间。  \n二、模拟登陆 天**反扒的第一关便是需要登录才能够查看具体的页面信息。相比于利用复杂JS逆向技术完成登陆而言，利用selenium库模拟人的操作、从而实现网站自动化登陆的做法则显得更为简单易行。从下图来看，我们需要利用selenium库来完成“点击密码登录（切换到密码登录页面，也即下图所示页面）-向账号对话框内输入账号-向密码对话框内输入密码-向验证码对话框内输入验证码-点击登录”等一系列操作后，才能登录到网站的信息页面，获取自己要想的数据。\n对于网站的登录我们提供了以下两种方法：一种是自动化登录；另一种则是手动登录。\n2.1 自动化登录 首先，我们先来看看较为复杂的自动化登录。要想实现网页的自动化登录，其关键在于利用 「外部力量」 来识别验证码并完成导入。具体而言，我们首先需要定位验证码在网页当中的元素位置，其次利用截图软件根据验证码元素位置来截取验证码图片，再次利用外部库对验证码图片进行识别，最后将识别出的验证码录入对话框。自动化登录的具体过程可以分为 get_code_image函数 和 parse_code函数 两个步骤进行，具体代码如下所示。其中，验证码的解析小编是调用了百度AI的开源库进行的。另外，需要注意的是，利用selenium库打开的登录页面一开始是不显示验证码的，必须向账号框和密码框输入内容以后，它才会显示验证码。因此，对于验证码的识别和录入，我们将它放在了所有操作中的最后部分。\nparse_code函数\ndef parse_code(): #用百度API解析图片 request_url = \u0026#34;https://aip.baidubce.com/rest/2.0/ocr/v1/numbers\u0026#34; f = open(\u0026#39;temp/验证码.png\u0026#39;, \u0026#39;rb\u0026#39;) img = base64.b64encode(f.read()) params = {\u0026#34;image\u0026#34;: img} access_token = \u0026#39;24.a7fbbfb9dcab2e1054cc827f09d09234.2592000.1625930266.282335-19004069\u0026#39; request_url = request_url + \u0026#34;?access_token=\u0026#34; + access_token headers = {\u0026#39;content-type\u0026#39;: \u0026#39;application/x-www-form-urlencoded\u0026#39;} response = requests.post(request_url, data=params, headers=headers) #得到解析结果 dictionary=response.json() #得到验证码 yanzhengma=dictionary[\u0026#39;words_result\u0026#39;][0][\u0026#39;words\u0026#39;] #录入验证码 browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;web-content\u0026#34;]/div/div[2]/div[3]/form/div[6]/input\u0026#39;).send_keys(yanzhengma) # 点击登录按钮 time.sleep(random.randint(1, 2)) browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;web-content\u0026#34;]/div/div[2]/div[3]/form/div[8]\u0026#39;).click() \nparse_code函数\ndef get_code_image(): # 1启动浏览器。 browser = webdriver.Chrome() # 2加入这个脚本可以避免被识别 browser.execute_cdp_cmd(\u0026#34;Page.addScriptToEvaluateOnNewDocument\u0026#34;, { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; Object.defineProperty(navigator, \u0026#39;webdriver\u0026#39;, { get: () =\u0026gt; undefined }) \u0026#34;\u0026#34;\u0026#34;}) # 3延迟10s启动 browser.implicitly_wait(10) # 4利用谷歌浏览器打开目标网页 browser.get(\u0026#39;https://pro.xxxxxx.com/searchx\u0026#39;) # 5将窗口最大化 browser.maximize_window() # 6给网页一些时间加载 time.sleep(random.randint(1, 2)) #将页面从快捷登录切换到密码登录 browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;web-content\u0026#34;]/div/div[2]/div[3]/div[1]/div[2]\u0026#39;).click() time.sleep(0.5) #输入账号 browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;web-content\u0026#34;]/div/div[2]/div[3]/form/div[2]/input\u0026#39;).send_keys( \u0026#39;******\u0026#39;) time.sleep(random.randint(1, 2)) # 输入密码 browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;web-content\u0026#34;]/div/div[2]/div[3]/form/div[4]/input\u0026#39;).send_keys(\u0026#39;*******\u0026#39;) time.sleep(random.randint(1, 2)) browser.save_screenshot(\u0026#39;temp/屏幕.png\u0026#39;)#截图整个页面】 #定位验证码x,y坐标 left_angle=browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;web-content\u0026#34;]/div/div[2]/div[3]/form/div[6]/img\u0026#39;).location image=browser.find_element_by_xpath(\u0026#39;//*[@id=\u0026#34;web-content\u0026#34;]/div/div[2]/div[3]/form/div[6]/img\u0026#39;) #获取验证码的长和宽 size=image.size #设定我们需要截取的位置 rangle = (int(left_angle[\u0026#39;x\u0026#39;]), int(left_angle[\u0026#39;y\u0026#39;] ), int(left_angle[\u0026#39;x\u0026#39;] + size[\u0026#39;width\u0026#39;] + 230), int(left_angle[\u0026#39;y\u0026#39;] + size[\u0026#39;height\u0026#39;] + 300)) #打开截图 open_image=Image.open(\u0026#39;temp/屏幕.png\u0026#39;) #从图片中截取我们需要的的区域 jietu=open_image.crop(rangle) jietu.save(\u0026#39;temp/验证码.png\u0026#39;) 接下来，我们再来看看如何实现手动登录。相比于自动化登录，手动登录的操作更为简单。具体地，我们只要在完成自动打开网站页面的代码后加入input()函数，然后自己手动向网站的对话框内输入账号、密码、验证码并点击登录，就可以进入到网站的信息页面。\n\n三、获取自己想要的数据 完成登陆之后，就会跳转到如下页面。然后，大家就可以根据自己的目标继续撰写属于自己的“个性化代码”了。下面，小编以获取31个省市的特定类型的企业数据为例，给大家分享一下自己获取数据的过程。\n其实，代码撰写的逻辑很简单。首先要做的就是先选中我们要爬取的目标城市。下来就是根据自己的需求来定制个性化的筛选标准。以小编自己的需求为例，先通过点击高级模式，向企业名称对话框里输入关键词，比如医院（当然，大家也可以通过限定行业来挑选目标）；然后，去掉机构类型中已勾选的企业，选择事业单位；接下来，勾选全部企业类型；最后点击查看结果。\n至此就完成了筛选，得到了满足我们要求的所有企业（见下图1）。接下来，我们要做的就是遍历每一页里的每一家企业，然后获取企业页面信息（见下图2）中自己想要的数据了。\n由于后续代码较长，就不在这里一一列举了。 有需要的小伙伴可以在后台留言，然后向xx索取。\n\n代码下载 对于初学者而言，直接上手可能比较难，建议先收藏本文，待熟练掌握爬虫可以实验本文的代码。\n如果想复现本文代码，需熟悉\n python基础语法 selenium驱动器driver的适配 百度api注册 涉及开发者工具xpath定位  本文教程\u0026amp;代码免费分享，但作者时间和精力宝贵，无法做到一一指导，尽请包涵。\n 代码链接: https://pan.baidu.com/s/1VmNoTN8hFRCBI9g770mUCw 提取码: ob2j\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/tian_ya_cha_spider_code/","summary":"作者: 张延丰\n哈工程管工在读博士，擅长数据采集\u0026amp;挖掘。\n文末可获取代码下载链接\n 马云在接受CNBC（美国消费者新闻与商业频道）采访时提出：“整个世界将变成数据，我认为这还是只是数据时代的开始。新浪潮即将来临，很多就业机会将被夺走。有些人会赶上潮流，变得更加富有和成功。但是对于那些落后的人，未来将是痛苦的。”就小编看来，这种说法在人文社科研究当中也同样适用。在当前数以万计甚至数以十万计研究样本“遍地走”的时代，若我们还拘泥于传统的“小样本”研究（比如样本量为100多的调查问卷数据等），不仅难以跟随时代的脚步，还会逐渐丧失学术竞争力、从而被时代淘汰。那么，究竟该如何获取属于自己的大样本数据呢？今天小编就带大家用selenium库来爬取国内某知名第三方企业信息平台（天**）的企业工商信息。\n\n一、自动打开网站页面 首先，数据爬取的第一步是利用selenium库启动浏览器，打开我们的目标网站。部分代码\n# @Author : Jacob-ZHANG import requests,base64 from PIL import Image import csv,re from selenium import webdriver import time,random #1启动浏览器。 #win browser=webdriver.Chrome(executable_path=\u0026#39;driver/chromedriver.exe\u0026#39;) #mac #browser=webdriver.Chrome(executable_path=\u0026#39;driver/chromedriver\u0026#39;) #2加入这个脚本可以避免被识别 browser.execute_cdp_cmd(\u0026#34;Page.addScriptToEvaluateOnNewDocument\u0026#34;, { \u0026#34;source\u0026#34;: \u0026#34;\u0026#34;\u0026#34; Object.defineProperty(navigator, \u0026#39;webdriver\u0026#39;, { get: () =\u0026gt; undefined }) \u0026#34;\u0026#34;\u0026#34; }) #3延迟10s启动 browser.implicitly_wait(10) #4利用谷歌浏览器打开目标网页 browser.get(\u0026#39;https://pro.xxxxxx.com/searchx\u0026#39;) #5将窗口最大化 browser.maximize_window() #6给网页一些时间加载 time.sleep(random.randint(1, 2)) ... ... ... 非常简单，如下所示。\n需要说明的是：\n 第一，selenium库距今已经有近20年的历史，各类网站、浏览器大多都能对它进行识别，因此为了避免被反爬，我可以通过加入上述脚本（#2）来防止网站的识别。 第二，因为在完成#4的操作，也即利用浏览器打开网页后，显示的网页页面并非是最大化的窗口，这导致页面元素存在难以定位、从而报错的情况，因此我们在此处加上了窗口最大化的操作（#5）。 第三，在以后的操作里，类似#6的代码会频频出现，这主要是因为受限于网速，不得不为网页加载提供更多时间。  \n二、模拟登陆 天**反扒的第一关便是需要登录才能够查看具体的页面信息。相比于利用复杂JS逆向技术完成登陆而言，利用selenium库模拟人的操作、从而实现网站自动化登陆的做法则显得更为简单易行。从下图来看，我们需要利用selenium库来完成“点击密码登录（切换到密码登录页面，也即下图所示页面）-向账号对话框内输入账号-向密码对话框内输入密码-向验证码对话框内输入验证码-点击登录”等一系列操作后，才能登录到网站的信息页面，获取自己要想的数据。","title":"天**  |  使用selenium做数据采集"},{"content":"引言 『中文情绪银行』 (Chinese EmoBank)是由人工标注产生的 中文维度情感词典 ，含效价valence和唤醒度arousal两个维度。\n 效价valence，可测量出文本中的积极/消极情感程度。 唤醒度arousal，可测量文本中平静/兴奋状态的程度。  该词典包括\n CVAW(Chinese valence-arousal words)， 5512词 CVAP(Chinese valence-arousal phrases)， 含2998词组 语料CVAS(Chinese valence-arousal sentences) 含2582个单句 语料CVAT(Chinese valence-arousal texts） 2969个句子  需要注意该词典是繁体中文词典，经过繁体转简体，已将CVAW嵌入到最新的cntext包。\npip3 install --upgrade cntext \n 本文图片来源于\nhttp://nlp.innobic.yzu.edu.tw/resources/ChineseEmoBank.html\n CVAW(Chinese valence-arousal words)    Word Valence_Mean Arousal_Mean Valence_SD Arousal_SD     乏味 3.4 3.0 0.800 1.414   放鬆 6.2 2.0 0.748 0.894   勝利 7.8 7.2 0.748 1.166   痛苦 2.4 6.8 0.490 0.748    CVAP(Chinese valence-arousal phrases )    Modifier Type Phrase Valence_Mean Arousal_Mean Valence_SD Arousal_SD     deg 十分有趣 8.222 7.063 0.533 0.390   mod 應該開心 5.986 5.350 0.242 0.456   neg 不喜歡 3.033 5.788 0.481 0.605   neg_deg 沒有太難過 4.478 4.675 0.413 0.538    CVAS(Chinese valence-arousal sentences)    Text Valence_Mean Arousal_Mean Valence_SD Arousal_SD     這是我觀賞過的最令人驚歎的演出。 7.000 7.750 0.000 0.433   簡直是人生惡夢的開端。 2.600 6.750 0.490 0.829   從小我經常覺得現實很無聊。 3.667 4.333 0.471 0.471   過去他們很輕鬆地賺錢。 5.667 4.000 1.247 0.816    CVAT(Chinese valence-arousal texts)    Text Valence_Mean Arousal_Mean Valence_SD Arousal_SD Category     很多車主抱怨新車怠速抖動嚴重\u0026mdash;-冷車時更嚴重。 3.250 5.667 1.090 1.054 Car   房間裏黴味，煙味撲鼻，沒有窗戶通風，骯髒的地毯上的斑斑點點的污蹟，令人觸目驚心。 1.889 6.875 0.737 0.927 Hotel   CPU顯卡也完全夠用，接口也非常齊全，總體來說很滿意！ 7.143 5.000 0.350 0.816 Laptop   飛安帶來更多保障，也提供旅客更安心的服務品質。 7.000 4.222 0.535 1.133 News    文献 如果用到Chinese EmoBank词典，请注明出处。\nLung-Hao Lee, Jian-Hong Li and Liang-Chih Yu, \u0026ldquo;Chinese EmoBank: Building Valence-Arousal Resources for Dimensional Sentiment Analysis,\u0026rdquo; ACM Trans. Asian and Low-Resource Language Information Processing, vol. 21, no. 4, article 65, 2022.\nLiang-Chih Yu, Lung-Hao Lee, Shuai Hao, Jin Wang, Yunchao He, Jun Hu, K. Robert Lai, and Xuejie Zhang. 2016. \u0026ldquo;Building Chinese affective resources in valence-arousal dimensions. In Proceedings of NAACL/HLT-16, pages 540-545.\n代码 import cntext as ct ct.load_pkl_dict(\u0026#39;ChineseEmoBank.pkl\u0026#39;) Run\n{\u0026#39;Referer-1\u0026#39;: \u0026#39;Lee, Lung-Hao, Jian-Hong Li, and Liang-Chih Yu. \u0026#34;Chinese EmoBank: Building Valence-Arousal Resources for Dimensional Sentiment Analysis.\u0026#34; Transactions on Asian and Low-Resource Language Information Processing 21, no. 4 (2022): 1-18.\u0026#39;, \u0026#39;Referer-2\u0026#39;: \u0026#39;Liang-Chih Yu, Lung-Hao Lee, Shuai Hao, Jin Wang, Yunchao He, Jun Hu, K. Robert Lai, and Xuejie Zhang. 2016. \u0026#34;Building Chinese affective resources in valence-arousal dimensions. In Proceedings of NAACL/HLT-16, pages 540-545.\u0026#39;, \u0026#39;Desc\u0026#39;: \u0026#39;Chinese Sentiment Dictionary, includes 「valence」「arousal」. In cntext, we only take single word into account, ignore phrase.\u0026#39;, \u0026#39;ChineseEmoBank\u0026#39;: word valence arousal 0 不可思议 5.4 7.2 1 不平 3.6 5.8 2 不甘 3.2 6.4 3 不安 3.8 5.4 4 不利 3.6 5.6 ... ... ... ... 5505 黏闷 2.8 5.6 5506 黏腻 2.7 5.8 5507 艳丽 5.8 4.5 5508 苗条 6.7 3.8 5509 修长 7.0 4.5 \nChineseEmoBank的CVAW词典(Chinese valence-arousal words)原有 5512词，经过繁体转简体处理，得到5510个词。\ndiction_df = ct.load_pkl_dict(\u0026#39;ChineseEmoBank.pkl\u0026#39;)[\u0026#39;ChineseEmoBank\u0026#39;] diction_df Run\n测量一段文本的valence和arousal，\ntext = \u0026#39;很多车主抱怨新车怠速抖动严重---冷车时更严重。\u0026#39; help(ct.sentiment_by_weight) Run\nHelp on function sentiment_by_weight in module cntext.stats: sentiment_by_weight(text, diction, params, lang=\u0026#39;english\u0026#39;) calculate the occurrences of each sentiment category words in text; the complex influence of intensity adverbs and negative words on emotion is not considered. :param text: text sring :param diction: sentiment dictionary dataframe with weight.； :param params: set sentiment category weight, such as params=[\u0026#39;valence\u0026#39;, \u0026#39;arousal\u0026#39;] :param lang: \u0026#34;chinese\u0026#34; or \u0026#34;english\u0026#34;; default lang=\u0026#34;english\u0026#34; :return: \n计算文本text中chinese_emobank词两维度的汇总得分，得到valence、arousal、word_num\ntext = \u0026#39;很多车主抱怨新车怠速抖动严重---冷车时更严重。\u0026#39; ct.sentiment_by_weight(text = text, diction = diction_df, params = [\u0026#39;valence\u0026#39;, \u0026#39;arousal\u0026#39;], lang = \u0026#39;chinese\u0026#39;) Run\n{\u0026#39;valence\u0026#39;: 14.8, \u0026#39;arousal\u0026#39;: 24.8, \u0026#39;word_num\u0026#39;: 13}  valence是句子中各个chinese_emobank词valence得分的加总。 arousal是句子中各个chinese_emobank词arousal得分的加总。 word_num是句子中的词语数(含标点符号)，短文本的情况下，word_num会不太准确，长文本情况下无限接近真实词语数。  需要注意，文本越长，valence和arousal指标应该会越大。使用这两个指标时，需要结合word_num进行均值处理，即\nValence = valence/word_num Arousal = arousal/word_num 这里未做均值处理，尽量保留文本的原始信息。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/chinese_emobank/","summary":"引言 『中文情绪银行』 (Chinese EmoBank)是由人工标注产生的 中文维度情感词典 ，含效价valence和唤醒度arousal两个维度。\n 效价valence，可测量出文本中的积极/消极情感程度。 唤醒度arousal，可测量文本中平静/兴奋状态的程度。  该词典包括\n CVAW(Chinese valence-arousal words)， 5512词 CVAP(Chinese valence-arousal phrases)， 含2998词组 语料CVAS(Chinese valence-arousal sentences) 含2582个单句 语料CVAT(Chinese valence-arousal texts） 2969个句子  需要注意该词典是繁体中文词典，经过繁体转简体，已将CVAW嵌入到最新的cntext包。\npip3 install --upgrade cntext \n 本文图片来源于\nhttp://nlp.innobic.yzu.edu.tw/resources/ChineseEmoBank.html\n CVAW(Chinese valence-arousal words)    Word Valence_Mean Arousal_Mean Valence_SD Arousal_SD     乏味 3.4 3.0 0.800 1.414   放鬆 6.2 2.0 0.748 0.894   勝利 7.8 7.","title":"EmoBank | 中文维度情感词典"},{"content":"网上有一些繁体中文资源不能直接利用，通过chinese-convertor库，我们可以进行中文繁简互换。\n安装 pip install chinese-converter \n快速上手 import chinese_converter chinese_converter.to_traditional(\u0026#34;中国\u0026#34;) Run\n中國 \nchinese_converter.to_simplified(\u0026#34;中國\u0026#34;) Run\n中国 \n长期征稿  点击了解投稿   招募小伙伴  点击加入我们   了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/chinese_converter/","summary":"网上有一些繁体中文资源不能直接利用，通过chinese-convertor库，我们可以进行中文繁简互换。\n安装 pip install chinese-converter \n快速上手 import chinese_converter chinese_converter.to_traditional(\u0026#34;中国\u0026#34;) Run\n中國 \nchinese_converter.to_simplified(\u0026#34;中國\u0026#34;) Run\n中国 \n长期征稿  点击了解投稿   招募小伙伴  点击加入我们   了解课程  点击上方图片购买课程   点击进入详情页","title":"chinese-converter | 中文繁简互换Python库"},{"content":"课程介绍 在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！\n在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：\n 网络爬虫技术 解决如何从网络世界中高效地采集数据？ 文本分析技术 解决如何从杂乱的文本数据中抽取实证指标(情绪、不确定、态度、认知等变量)？  授课方式  线上直播（电脑端与手机端皆可播放，回放十天）。 开课前会建立讲师微信群并发布最新学习资料，群聊长期有效，助教全程跟随。 第一时段-在线讲座 2022.8.16~17 上午\u0026amp;下午 第二时段-论文指导 2022.8.24 下午  Python实证指标构建与文本分析课程结束一周后 半天时间 学员利用一周的时间用Python收集、整理数据、文本分析，撰写一个初步的论文与老师交流，老师一对一地指导如何修改文本数据挖掘的实证论文。    费用与优惠  报名总费用2500元（包含在线讲座费用2000元、论文指导费用500元、课后长期答疑以及全部讲义以及代码等资料） 个人报名优惠：报名两科9折；三科8折；四科及以上7.5折；老朋友9折；凭本人学生证报名可再减200元/人。 团队报名优惠：三人成团及以上9折；五人成团及以上8折。 7月10日之前报名可享每人优惠100元。 各项优惠叠加不超过总价的7.5折。  报名方式  从即日起可加老师微信咨询与报名。 17816181460（同微信）（汪老师）  缴费方式  扫码付款 添加汪老师微信获取,支持公务卡支付  对公转账  单位：杭州国商智库信息技术服务有限公司 开户银行：中国银行杭州大学城支行 银行账户：6232636200100260588  \n内容安排 一、Python语法入门  Python跟英语一样是一门语言 数据类型之字符串 数据类型之列表元组集合 数据类型之字典 数据类型之布尔值、None 逻辑语句(if\u0026amp;for\u0026amp;tryexcept) 列表推导式 理解函数 常用的内置函数 os路径库 内置库csv文件库 常见错误汇总  二、数据采集  网络爬虫原理 寻找网址规律 获取网页-requests库 pyquery库解析html网页 案例 1：豆瓣小说 json库解析json网页 案例 2：豆瓣电影 案例 3：微博 案例 4： 批量下载文档、多媒体文件 案例 5：上市公司定期报告pdf批量下载 区分动态网站与静态网站  三、文本分析入门  文本分析在经管领域中的应用 读取文件中的数据(txt、pdf、docx、xlsx、csv) 数据清洗re库-从文本中抽取姓名、年龄、电话、数字等各种信息 案例 6：如何将多个文件中的数据整理到一个excel中 中文jieba分词 案例 7：词频统计、制作词云图 案例 8：共现法扩展情感词典 案例 9：词向量word2vec扩展情感词典 案例 10：中文情感分析(无权重词典法) 数据分析pandas库快速入门 案例 11：使用pandas对excel中的文本进行情感分析 案例 12: 计算地图中两点(经纬度)距离及方位角  四、机器学习  了解机器学习 理解特征工程 文本特征工程-将文本转化为机器可处理的数字向量 认识词袋法、one-hot、Tf-Idf、word2vec 案例 13：使用tf-idf进行情感分析（有权重词典法） 案例 14： 使用标注工具对文本数据进行标注 案例 15：在线评论文本分类 文本相似性计算 案例 16：使用文本相似性识别变化(政策连续性) 案例 17：Kmeans聚类算法 案例 18：LDA话题模型 案例 19: 识别图片中的文本 python爬虫、文本分析、机器学习等技术在论文中的应用赏析  五、词嵌入与认知  词嵌入 豆瓣影评-gensim导入词向量模型 认知偏见(刻板印象) 总结: 文本分析在经管领域中的应用概述  \n文本分析应用案例 参照两篇论文的摘要，可以通过场景化等的方式帮助我们迅速理解上面两个问题。摘要部分的加粗内容是论文用到的分析技术，在我们的课程中均有与之对应的知识点和代码。\n王伟,陈伟,祝效国,王洪伟.众筹融资成功率与语言风格的说服性——基于Kickstarter的实证研究[J].管理世界,2016(05):81-98.\n 摘要：众筹融资效果决定着众筹平台的兴衰。众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。\n然后，借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。\n最后，建立语言说服风格对项目筹资影响的计量模型，并对 Kickstarter 平台上的 128345 个项目进行实证分析。总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。\n 胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.\n 在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间 导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现，年报 MD\u0026amp;A 中披露的“短期视域” 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴。\n Wang, Quan, Beibei Li, and Param Vir Singh. \u0026ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.\u0026rdquo; Information Systems Research 29, no. 2 (2018): 273-291.\n 摘要：尽管移动应用程序市场的增长为移动应用程序开发人员创新提供了巨大的市场机会和经济诱因，但它也不可避免地刺激了模仿者开发盗版软件。原始应用的从业人员和开发人员声称，模仿者窃取了原始应用的想法和潜在需求，并呼吁应用平台对此类模仿者采取行动。令人惊讶的是，很少有严格的研究来分析模仿者是否以及如何影响原始应用的需求。\n进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原始或模仿者并检测两种模仿者的方法：欺骗性和非欺骗性。\n根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的5,141个开发人员的10,100个动作游戏应用程序样本中，模仿应用程序对原始应用程序需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原件的需求。\n结果表明，从总体上讲，模仿者对原始移动应用程序需求的影响在统计上是微不足道的。我们的研究通过提供一种识别模仿者的方法，并提供模仿者对原始应用需求的影响的证据，为越来越多的移动应用消费文献做出了贡献。\n Markowitz, D. M., \u0026amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).\n 处理流畅性等元认知框架通常表明人们对简单和通用的语言的反应比复杂和技术性语言更有利。与复杂的信息相比，人们更容易处理简单和非技术性的信息，因此会更多地与目标进行互动。在涵盖 12 个现场样本（总 n = 1,064,533）的两项研究中，我们通过展示人们在付出时间和注意力时更多地使用非技术语言（例如，简单的在线语言往往会获得更多社交信息）来建立并复制这种越简单越好的现象订婚）。然而，人们在捐款时会对复杂的语言做出反应（例如，慈善捐赠活动和赠款摘要中的复杂语言往往会收到更多的钱）。这一证据表明，人们根据时间或金钱目标以不同的方式使用复杂语言的启发式方法。这些结果强调语言是社会和心理过程的镜头，以及大规模测量文本模式的计算方法。\n 文献汇总 [1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html [2]沈艳,陈赟,黄卓．文本大数据分析在经济学和金融学中的应用：一个文献综述[EB/OL].http://www.ccer.pku.edu.cn/yjcg/tlg/242968.htm,2018-11-19 [3]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.*管理世界*.2016;5:81-98. [4]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21. [5]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020 [6]Kenneth Benoit. July 16, 2019. “Text as Data: An Overview.” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. Handbook of Research Methods in Political Science and International Relations. Thousand Oaks: Sage. [7]Loughran T, McDonald B. Textual analysis in accounting and finance: A survey[J]. *Journal of Accounting Research*, 2016, 54(4): 1187-1230. Author links open overlay panelComputational socioeconomics [8]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026#34;Uniting the tribes: Using text for marketing insight.\u0026#34; *Journal of Marketing* 84, no. 1 (2020): 1-25. [9]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. \u0026#34;A review of best practice recommendations for text analysis in R (and a user-friendly app).\u0026#34; *Journal of Business and Psychology* 33, no. 4 (2018): 445-459. [10]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. \u0026#34;Lazy prices.\u0026#34; *The Journal of Finance* 75, no. 3 (2020): 1371-1415. [11]孟庆斌, 杨俊华, 鲁冰. 管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J]. *中国工业经济*, 2017 (12): 132-150. [12]Wang, Quan, Beibei Li, and Param Vir Singh. \u0026#34;Copycats vs. Original Mobile Apps: A Machine Learning Copycat-Detection Method and Empirical Analysis.\u0026#34; *Information Systems Research* 29.2 (2018): 273-291. [13]Hoberg, Gerard, and Gordon Phillips. 2016, Text-based network industries and endogenous product differentiation,?*Journal of Political Economy* 124, 1423-1465 [14]Loughran, Tim, and Bill McDonald. \u0026#34;When is a liability not a liability? Textual analysis, dictionaries, and 10‐Ks.\u0026#34; *The Journal of Finance* 66, no. 1 (2011): 35-65. [15]Fairclough, Norman. 2003. Analysing discourse: Textual analysis for social research (Psychology Press) [16]Grimmer, Justin, and Brandon M Stewart. 2013, Text as data: The promise and pitfalls of automatic content analysis methods for political texts, *Political analysis*21, 267-297. [17]Markowitz, D. M., \u0026amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18). [18]Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” Journal of Consumer Research 47, no. 5 (2021): 787-806. [19]Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156. \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022_summer_workshop/","summary":"课程介绍 在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！\n在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：\n 网络爬虫技术 解决如何从网络世界中高效地采集数据？ 文本分析技术 解决如何从杂乱的文本数据中抽取实证指标(情绪、不确定、态度、认知等变量)？  授课方式  线上直播（电脑端与手机端皆可播放，回放十天）。 开课前会建立讲师微信群并发布最新学习资料，群聊长期有效，助教全程跟随。 第一时段-在线讲座 2022.8.16~17 上午\u0026amp;下午 第二时段-论文指导 2022.8.24 下午  Python实证指标构建与文本分析课程结束一周后 半天时间 学员利用一周的时间用Python收集、整理数据、文本分析，撰写一个初步的论文与老师交流，老师一对一地指导如何修改文本数据挖掘的实证论文。    费用与优惠  报名总费用2500元（包含在线讲座费用2000元、论文指导费用500元、课后长期答疑以及全部讲义以及代码等资料） 个人报名优惠：报名两科9折；三科8折；四科及以上7.5折；老朋友9折；凭本人学生证报名可再减200元/人。 团队报名优惠：三人成团及以上9折；五人成团及以上8折。 7月10日之前报名可享每人优惠100元。 各项优惠叠加不超过总价的7.5折。  报名方式  从即日起可加老师微信咨询与报名。 17816181460（同微信）（汪老师）  缴费方式  扫码付款 添加汪老师微信获取,支持公务卡支付  对公转账  单位：杭州国商智库信息技术服务有限公司 开户银行：中国银行杭州大学城支行 银行账户：6232636200100260588  \n内容安排 一、Python语法入门  Python跟英语一样是一门语言 数据类型之字符串 数据类型之列表元组集合 数据类型之字典 数据类型之布尔值、None 逻辑语句(if\u0026amp;for\u0026amp;tryexcept) 列表推导式 理解函数 常用的内置函数 os路径库 内置库csv文件库 常见错误汇总  二、数据采集  网络爬虫原理 寻找网址规律 获取网页-requests库 pyquery库解析html网页 案例 1：豆瓣小说 json库解析json网页 案例 2：豆瓣电影 案例 3：微博 案例 4： 批量下载文档、多媒体文件 案例 5：上市公司定期报告pdf批量下载 区分动态网站与静态网站  三、文本分析入门  文本分析在经管领域中的应用 读取文件中的数据(txt、pdf、docx、xlsx、csv) 数据清洗re库-从文本中抽取姓名、年龄、电话、数字等各种信息 案例 6：如何将多个文件中的数据整理到一个excel中 中文jieba分词 案例 7：词频统计、制作词云图 案例 8：共现法扩展情感词典 案例 9：词向量word2vec扩展情感词典 案例 10：中文情感分析(无权重词典法) 数据分析pandas库快速入门 案例 11：使用pandas对excel中的文本进行情感分析 案例 12: 计算地图中两点(经纬度)距离及方位角  四、机器学习  了解机器学习 理解特征工程 文本特征工程-将文本转化为机器可处理的数字向量 认识词袋法、one-hot、Tf-Idf、word2vec 案例 13：使用tf-idf进行情感分析（有权重词典法） 案例 14： 使用标注工具对文本数据进行标注 案例 15：在线评论文本分类 文本相似性计算 案例 16：使用文本相似性识别变化(政策连续性) 案例 17：Kmeans聚类算法 案例 18：LDA话题模型 案例 19: 识别图片中的文本 python爬虫、文本分析、机器学习等技术在论文中的应用赏析  五、词嵌入与认知  词嵌入 豆瓣影评-gensim导入词向量模型 认知偏见(刻板印象) 总结: 文本分析在经管领域中的应用概述","title":"2022暑期工作坊 | Python实证指标构建与文本分析"},{"content":"Asent 是一个新的Python情感分析库， 依据情感词典，按照一定的规则，可用于评判词语、句子、文档的情感信息(正、负)。\n目前与情感有关的规则有\n 否定（即“不高兴”） 加强词（“非常高兴”） 对比共轭（即“但是”） 其他强调标记，如感叹号、大小写和问号。  Asent目前仅支持英语、丹麦、挪威、瑞典4种语言。\n安装配置 学习课程之前，需要先下载并配置spacy模型， https://github.com/explosion/spacy-models/releases\npip3 install spacy==3.2.0 pip3 install asent==0.4.2 #下载en_core_web_lg-3.3.0-py3-none-any.whl到桌面 #下载链接: https://pan.baidu.com/s/13hFWFjy9uRxzC-9lqrp7SQ 提取码: em8l  #然后使用如下安装命令 pip3 install Desktop/en_core_web_lg-3.2.0-py3-none-any.whl \n快速上手 以下将带您逐步了解情绪是如何计算的。\n首先，我们需要一个 spaCy 管道，并且我们需要向其中添加 asent 管道。\nimport asent import spacy # load spacy pipeline nlp = spacy.load(\u0026#34;en_core_web_lg\u0026#34;) # add the rule-based sentiment model nlp.add_pipe(\u0026#34;asent_en_v1\u0026#34;) Run\n\u0026lt;asent.component.Asent at 0x7fd6b3243130\u0026gt;  效价和极性 如下所示， token的效价信息来自于人工标注的词典。例如I am not very happy中词语happy的人类情感评分是2.7。\n首先我们查看每个词语对应的效价。\ndoc = nlp(\u0026#34;I am not very happy.\u0026#34;) for token in doc: print(token, \u0026#34;\\t\u0026#34;, token._.valence) Run\nI 0.0 am 0.0 not 0.0 very 0.0 happy 2.7 . 0.0  在该语境中， happy前面有否定词not修饰，所以情感极性方面应该被看做消极的。一般否定词和副词可以将形容词的情感进行反转和放大(缩小)。\nfor token in doc: print(token._.polarity) Run\npolarity=0.0 token=I span=I polarity=0.0 token=am span=am polarity=0.0 token=not span=not polarity=0.0 token=very span=very polarity=-2.215 token=happy span=not very happy polarity=0.0 token=. span=.  注意到， 词语在happy拥有-2.215的极性分，该分是由not very happy确定的。\n可视化 asent拥有多种情感极性可视化的方法\nasent.visualize(doc, style=\u0026#34;prediction\u0026#34;) asent.visualize(doc, style=\u0026#34;analysis\u0026#34;) for sentence in doc.sents: print(sentence._.polarity) neg=0.391 neu=0.609 pos=0.0 compound=-0.4964 span=I am not very happy.  doc._.polarity DocPolarityOutput(neg=0.391, neu=0.609, pos=0.0, compound=-0.4964)  doc2 = nlp(\u0026#34;I am not very happy.I am very very happy.It is awesome!!\u0026#34;) print(\u0026#39;doc2情感极性信息: \u0026#39;, doc2._.polarity) print() print(\u0026#39;doc2情感得分:\u0026#39;, doc2._.polarity.compound) doc2情感极性信息: neg=0.13 neu=0.536 pos=0.333 compound=0.2794 doc2情感得分: 0.279353567721562  #每个句子的情感极性信息 for sentence in doc2.sents: print(sentence._.polarity) neg=0.391 neu=0.609 pos=0.0 compound=-0.4964 span=I am not very happy. neg=0.0 neu=0.539 pos=0.461 compound=0.6453 span=I am very very happy. neg=0.0 neu=0.461 pos=0.539 compound=0.6892 span=It is awesome!!  #每个句子的情感得分 for sentence in doc2.sents: print(sentence._.polarity.compound) -0.4964238981617178 0.6452764659402158 0.689208135386188  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/asent_sentiment_analysis/","summary":"Asent 是一个新的Python情感分析库， 依据情感词典，按照一定的规则，可用于评判词语、句子、文档的情感信息(正、负)。\n目前与情感有关的规则有\n 否定（即“不高兴”） 加强词（“非常高兴”） 对比共轭（即“但是”） 其他强调标记，如感叹号、大小写和问号。  Asent目前仅支持英语、丹麦、挪威、瑞典4种语言。\n安装配置 学习课程之前，需要先下载并配置spacy模型， https://github.com/explosion/spacy-models/releases\npip3 install spacy==3.2.0 pip3 install asent==0.4.2 #下载en_core_web_lg-3.3.0-py3-none-any.whl到桌面 #下载链接: https://pan.baidu.com/s/13hFWFjy9uRxzC-9lqrp7SQ 提取码: em8l  #然后使用如下安装命令 pip3 install Desktop/en_core_web_lg-3.2.0-py3-none-any.whl \n快速上手 以下将带您逐步了解情绪是如何计算的。\n首先，我们需要一个 spaCy 管道，并且我们需要向其中添加 asent 管道。\nimport asent import spacy # load spacy pipeline nlp = spacy.load(\u0026#34;en_core_web_lg\u0026#34;) # add the rule-based sentiment model nlp.add_pipe(\u0026#34;asent_en_v1\u0026#34;) Run\n\u0026lt;asent.component.Asent at 0x7fd6b3243130\u0026gt;  效价和极性 如下所示， token的效价信息来自于人工标注的词典。例如I am not very happy中词语happy的人类情感评分是2.7。\n首先我们查看每个词语对应的效价。\ndoc = nlp(\u0026#34;I am not very happy.","title":"Asent库 | 英文文本数据情感分析"},{"content":" 第一届中国研究生金融科技创新大赛讲座\n2022/06/24 13:43\n录制文件：https://dwz.win/ayS8\n  另类数据与投资算法  信息通信技术的创新、互联网和移动终端的普及，产生了了大量的区别于 传统财务数据的新型数据，这类非财务数据具有数据量大、实时性高、颗粒度细及“原始”等特点，影响着资本市场，在投资领域的应用受到了越来越多的关注。投资者可以用较低的成本获取大量的数据和信息，对这类信息进行筛选、分析，辅助制定投资决策。\n能否选择一种尚未在资本市场广泛使用的另类数据，利用合适的算法把该数据应用于 A 股市场投资当中，并寻找合适的算法解决方案，研究其在投资中的价值，并构建出可行性的投资方案？\n 另类数据alternative data 大数据思维， 快、多、大、异。\n另类大数据产生的更多更快，与传统指标相关性小，能提供更多的信息增益。\n另类数据alternative data主要包含以下三种:\n   另类数据 包括 结构化 类型 python技术     个人产生的数据 社交媒体帖子、产品评论、互联网搜索趋势等 非结构 网页 爬虫   由业务流程产生的数据 公司工商数据、专利数据、尾气数据、招聘数据、商业交易、事件数据、招标数据、阿里巴巴、京东、美团等电商平台数据、app排行榜、直播和搜索指数数据等 结构化 数字 爬虫   传感器产生的数据 卫星图像数据、行人和车辆流量、船舶位置等，地图数据。 非结构 图像 图片分析   第三方数据 分析师研报情感数据、一致性预期。 结构 数字 付费    国内提供另类数据的开源网站有:\n tushare 付费 akshare 免费  文本 文化研究之父斯图亚特·霍尔（Stuart Hall）在《电视话语中的编码和解码》（Encoding and decoding inthe television discourse）一文中提出了“编码解码”理论。\n 编码（encoding），信息传播者将所传递的讯息、意图或观点，转化为具有特定规则的代码。 解码（decoding），信息接受者，将上述代码按特定规则进行解读。  信息传播学的编码解码理论\n   角度 解释 难度 python库     信息检索 新闻咨询中是否出现某类信息(某类词) 低 re、jieba   情感分析 文本中正面词与负面词含量的对比 低 jieba、nltk   文本相似度 两文本向量化后的cosine余弦值的 中 jieba、scikit-learn   文本分类 标注数据，使用文本数据做类别预测(利好、利空) 中 scikit-learn   词向量 - 不同主体对同一概念的认知(偏见、刻板印象)等。\n- 同一主体对不同概念的认知。 高 gensim          文本相似度提前预警股价暴跌。  Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. \u0026ldquo;Lazy prices.\u0026rdquo; The Journal of Finance 75, no. 3 (2020): 1371-1415.\n 图片 OCR图像识别，识别有没有、有多少。\n 停车场汽车停放量，识别有多少量车，预测沃尔玛等商超的经营情况  音频、视频  演讲音频转文本，用到文本分析，度量讲话的语气语调。  金融资讯舆情分析  新闻舆情作为金融投资市场上的重要信息可以及时披露上市公司的经营状 况或股价异动情况，常常可作为投资决策的重要参考，但市场中海量的舆情信息难以通过人工的方式逐一分析，往往只能主观挑选某些个人认为比较重要的 新闻媒体进行舆情的跟踪，并忽略和抛弃其他新闻媒体的舆情信息，这极有可能遗漏掉一部分有价值的重要信息。\n请各参赛队伍根据赛方提供的上市公司新闻资讯数据，利用深度学习、自然语言处理算法进行建模分析，及时、准确地判断新闻资讯的 舆情倾向（利好、中性、利空等）\n 新闻中的可以挖掘的金融指标\n  分析师情绪 买在分歧，卖在一致。\n  新闻情绪 机构、媒体、散户。\n  测度算法 使用文本分析对咨询中的舆情倾向（利好、中性、利空等）进行分析。\n   算法 功能 类比      词典法 把文档转为某个数。\n例如政府工作报告中提到\u0026quot;创新\u0026quot;、\u0026ldquo;创业\u0026quot;的个数。 原子    机器学习 把 文档 转为 vector 分子    词嵌入 比机器学习更深入彻底，将word看做vector。工程师，含有男性、技术、高薪。。。 夸克           需要的技术   词典法-构造金融情感词典\n  共现法，上下文共同出现。\n  词向量法\n    ML做文本分类\n  构造金融词典 共现法 物以类聚，词以群分。近义词更容易出现在同一个上下文中。\n以「利好」「利空」为例\n 人工选定「利好」「利空」初始词 构建语料内的词语共现矩阵 得到与「利好」「利空」共现得分较高的前n个候选词 分别输出到txt内 人工筛查剔除  词向量 以「利好」「利空」为例\n 人工选定「利好」「利空」初始词 训练语料内的词向量模型 得到与「利好」「利空」向量相似度较高的前n个候选词 分别输出到txt内 人工筛选剔除  ML做预测(利好1、利空0)步骤  \u0026hellip;(标注数据) 导入数据 数据清洗(剔除停用词，杂乱字符等) 特征工程（文本转化为向量） 将数据分为训练集和测试集 选择某种ML算法训练模型 评价模型  \nML算法 机器学习算法分为 监督式 和 非监督式。本节特指监督式，即同时含有x1, x2,\u0026hellip;xn和y.\nML训练出的模型，实际上是通过数据，学习 y=f(x1, x2, \u0026hellip;xn)中的 f。\n   监督学习算法 代码导入方法     回归 from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\n\u0026hellip;   K近邻 from sklearn.neighbors import KNeighborsClassifier\n\u0026hellip;   支持向量机 from sklearn.svm import SVC\u0026hellip;   决策树 from sklearn.tree import DecisionTreeClassifier\u0026hellip;    \n投保反欺诈模型 机器学习可以根据丰富的数据和监控模型，对数据进行多重处理分析，建立实时反欺诈规则和模型，结合当前用户特征，实时识别用户欺诈行为。\n请参赛队伍在了解投保信息收集的基础上，基于机器学习技术，对投保过程中的信息进行收集和分析，从数据中提取客户多维度异常模式，探索大数据反欺诈规则，实现异常识别功能，提前检测投保人在交易过程中是否有欺诈行为，识别可能的欺诈行为，减少欺诈损害。\n 了解投保信息收集的基础上\u0026ndash;\u0026gt;提取新的x\n ML做预测步骤  \u0026hellip;(标注数据) 导入数据 数据清洗(剔除停用词，杂乱字符等) 特征工程（构造并加入新的x） 将数据分为训练集和测试集 选择某种ML算法训练模型 评价模型     监督学习算法 代码导入方法     回归 from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\n\u0026hellip;   K近邻 from sklearn.neighbors import KNeighborsClassifier\n\u0026hellip;   支持向量机 from sklearn.svm import SVC\u0026hellip;   决策树 from sklearn.tree import DecisionTreeClassifier\u0026hellip;    Kaggle代码 \n公募产品个性化推荐系统  在客户需求升级和金融市场的竞争环境下，数字化运营将是未来金融机构核心竞争力的来源，是构筑差异化优势的重要手段。\n请参赛队伍结合金融行业的数字化运营需求，根据赛方提供的公募基金资讯数据、风险等级数据、用户行为点击序列、公募产品详情页的停留时长、公募产品的自选收藏等行为数据集，运用机器学习、深度学习、推荐算法等科技手段，分析预测用户的下一个兴趣点，在满足风险合规的条件下为合适的用户找到合适的产品。\n 方法论基础 假设: 相似的人 喜欢做 相似的事情\n有三种推荐算法\n   推荐系统算法思想 解释 特征向量化     Demographic Filtering 相似人口特征的人 喜欢 相似的事(物) 将人向量化。[age、gendre、salary、consume、地理、、、]   Content Based Filtering 如果一个人喜欢某个特定事(物)，他或她也会喜欢与它相似的项目。 将事物向量化   Collaborative Filtering 协同(联合) 人与事(物) 的 配对匹配 存在模式 用户-评价-矩阵    Collaborative Filtering | user-item-matrix 以用户影评为例，挖掘构造出用户、产品的特点(特征向量）。\n冷启动问题 如果某个用户，没有任何影评数据，如何预测该用户的偏好？\n思路: 依然假设物以类聚，人以群分。\n公募基金公司 有历史记录\n   user 类型 个人风险偏好考试 金额     User1 (age/gender/edu/addr/intro) 债券 保守 5000   User2 (age/gender/edu/addr/intro) 股票 激进 10000   \u0026hellip; .. .. ..    本文之外 长期征稿  点击了解投稿   招募小伙伴  点击加入我们   文本分析视频课 想轻松而快捷的深刻了解一个领域，看视频(直播)学习是一个不错的方式。\n  大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与直播课。\n  如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的录播课。\n  如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读有偿说明\n   点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/fintech_quant_with_python/","summary":"第一届中国研究生金融科技创新大赛讲座\n2022/06/24 13:43\n录制文件：https://dwz.win/ayS8\n  另类数据与投资算法  信息通信技术的创新、互联网和移动终端的普及，产生了了大量的区别于 传统财务数据的新型数据，这类非财务数据具有数据量大、实时性高、颗粒度细及“原始”等特点，影响着资本市场，在投资领域的应用受到了越来越多的关注。投资者可以用较低的成本获取大量的数据和信息，对这类信息进行筛选、分析，辅助制定投资决策。\n能否选择一种尚未在资本市场广泛使用的另类数据，利用合适的算法把该数据应用于 A 股市场投资当中，并寻找合适的算法解决方案，研究其在投资中的价值，并构建出可行性的投资方案？\n 另类数据alternative data 大数据思维， 快、多、大、异。\n另类大数据产生的更多更快，与传统指标相关性小，能提供更多的信息增益。\n另类数据alternative data主要包含以下三种:\n   另类数据 包括 结构化 类型 python技术     个人产生的数据 社交媒体帖子、产品评论、互联网搜索趋势等 非结构 网页 爬虫   由业务流程产生的数据 公司工商数据、专利数据、尾气数据、招聘数据、商业交易、事件数据、招标数据、阿里巴巴、京东、美团等电商平台数据、app排行榜、直播和搜索指数数据等 结构化 数字 爬虫   传感器产生的数据 卫星图像数据、行人和车辆流量、船舶位置等，地图数据。 非结构 图像 图片分析   第三方数据 分析师研报情感数据、一致性预期。 结构 数字 付费    国内提供另类数据的开源网站有:\n tushare 付费 akshare 免费  文本 文化研究之父斯图亚特·霍尔（Stuart Hall）在《电视话语中的编码和解码》（Encoding and decoding inthe television discourse）一文中提出了“编码解码”理论。","title":"视频分享| Python数据挖掘与金融科技 "},{"content":"今天分享的这篇论文通过 有道翻译 这一简单有效的的方式汉化了 LM英文金融词典，并使用 数词语数个数 的方式构造了管理层语调这个指标。\n文献 曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.\n本文代码实现的视频讲解已更新至付费课 Python实证指标构建与文本分析 中。\n 点击上方图片购买课程   本文代码 点击下载\n摘要 基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在**「口是心非」** 的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与 「年报语调」 相关。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。\n关键词 年报; 语调管理; 内部人交易; 信息不对称;\n代码  年报数据 data/reports.csv LM金融词典 需要更新cntext库至于1.7.3及以上版本  \n语调指标  算法1 该年报内 「积极词汇数 与消极词汇数 之差」 占 「年报总词汇数」 的比例； 算法2 （积极词汇数-消极词汇数）/（ 积极词汇数+消极词汇数）  import pandas as pd df = pd.read_csv(\u0026#39;data/reports.csv\u0026#39;) df.head() Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  company year content     0 四川长虹 2017 2017 年，面对复杂多变的外部环境和多重叠加的困难挑战，公司聚焦用户与产品，强化消费洞...   1 江苏吴中 2014 2014 年，正值公司成立二十周年，上市十五周年，在董事会带领下，公司经营管理团队与全体...   2 联美控股 2017 报告期内，公司实现营业收入 2,376,375,380.44 元，同比增长 16.24%，营...   3 华海药业 2016 第三节\\t公司业务概要\\n\\n一、 报告期内公司所从事的主要业务、经营模式及行业情况说明\\n...   4 江泉实业 2014 报告期内，全球经济形势复杂多变、复苏进程缓慢；我国宏观经济进入增速放缓、结构调整加剧的新...     len(df) Run\n500  \ncntext 安装cntext\n!pip3 install cntext \nimport cntext as ct print(ct.__version__) Run\n1.7.3 \n查看内置词典\nct.dict_pkl_list() Run\n[\u0026#39;DUTIR.pkl\u0026#39;, \u0026#39;HOWNET.pkl\u0026#39;, \u0026#39;Chinese_Loughran_McDonald_Financial_Sentiment.pkl\u0026#39;, \u0026#39;sentiws.pkl\u0026#39;, \u0026#39;ChineseFinancialFormalUnformalSentiment.pkl\u0026#39;, \u0026#39;ANEW.pkl\u0026#39;, \u0026#39;LSD2015.pkl\u0026#39;, \u0026#39;NRC.pkl\u0026#39;, \u0026#39;geninqposneg.pkl\u0026#39;, \u0026#39;HuLiu.pkl\u0026#39;, \u0026#39;Loughran_McDonald_Financial_Sentiment.pkl\u0026#39;, \u0026#39;AFINN.pkl\u0026#39;, \u0026#39;ADV_CONJ.pkl\u0026#39;, \u0026#39;STOPWORDS.pkl\u0026#39;] \nclm = ct.load_pkl_dict(\u0026#39;Chinese_Loughran_McDonald_Financial_Sentiment.pkl\u0026#39;) #print(clm) print(\u0026#39;clm关键词: \u0026#39;,clm.keys()) print() print(\u0026#39;Desc: \u0026#39;, clm[\u0026#39;Desc\u0026#39;]) print() print(\u0026#39;Referer: \u0026#39;, clm[\u0026#39;Referer\u0026#39;]) print() print(\u0026#39;Chinese_Loughran_McDonald_Financial_Sentiment词典含2类情感词\\n，分别是\u0026#39;, clm[\u0026#39;Chinese_Loughran_McDonald_Financial_Sentiment\u0026#39;].keys()) Run\nclm关键词: dict_keys([\u0026#39;Chinese_Loughran_McDonald_Financial_Sentiment\u0026#39;, \u0026#39;Desc\u0026#39;, \u0026#39;Referer\u0026#39;]) Desc: 参照该论文，cntext库使用百度翻译、有道翻译对LM词典进行汉化处理。原文使用的有道翻译、金山词霸。 Referer: 曾庆生, 周波, 张程, and 陈信元. \u0026#34;年报语调与内部人交易: 表里如一还是口是心非?.\u0026#34; 管理世界 34, no. 09 (2018): 143-160. Chinese_Loughran_McDonald_Financial_Sentiment词典含2类情感词 ，分别是 dict_keys([\u0026#39;negative\u0026#39;, \u0026#39;positive\u0026#39;]) \ncntext语调的实现  算法1 该年报内 「积极词汇数 与消极词汇数 之差」 占 「年报总词汇数」 的比例； 算法2 （积极词汇数-消极词汇数）/（ 积极词汇数+消极词汇数）  import cntext as ct diction = {\u0026#39;pos\u0026#39;: [\u0026#39;高兴\u0026#39;, \u0026#39;快乐\u0026#39;, \u0026#39;分享\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;难过\u0026#39;, \u0026#39;悲伤\u0026#39;]} text = \u0026#39;我今天得奖了，很高兴，我要将快乐分享大家。\u0026#39; ct.sentiment(text=text, diction=diction, lang=\u0026#39;chinese\u0026#39;) Run\n{\u0026#39;pos_num\u0026#39;: 3, \u0026#39;neg_num\u0026#39;: 0, \u0026#39;stopword_num\u0026#39;: 8, \u0026#39;word_num\u0026#39;: 14, \u0026#39;sentence_num\u0026#39;: 1} \nimport cntext as ct diction = ct.load_pkl_dict(\u0026#39;Chinese_Loughran_McDonald_Financial_Sentiment.pkl\u0026#39;)[\u0026#39;Chinese_Loughran_McDonald_Financial_Sentiment\u0026#39;] def tone(text): res = ct.sentiment(text=text, diction=diction, lang=\u0026#39;chinese\u0026#39;) return pd.Series(res) #第一个年报的语调 tone(text=df[\u0026#39;content\u0026#39;].tolist()[0]) Run\nnegative_num 67 positive_num 76 stopword_num 462 word_num 1831 sentence_num 52 dtype: int64 \n选中文本列content， 对content整体实施tone计算，\n#计算tone语调 tone_df = df[\u0026#39;content\u0026#39;].apply(tone) tone_df.head() Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  negative_num positive_num stopword_num word_num sentence_num     0 67 76 462 1831 52   1 32 59 372 1266 34   2 18 33 178 816 19   3 81 114 1055 3619 90   4 27 17 134 453 16      算法1 该年报内 「积极词汇数 与消极词汇数 之差」 占 「年报总词汇数」 的比例； 算法2 （积极词汇数-消极词汇数）/（ 积极词汇数+消极词汇数）  将得到的正、负面、总词数分别按照算法1和算法2进行计算。\n# 算法1 tone_df[\u0026#39;tone1\u0026#39;] = (tone_df[\u0026#39;positive_num\u0026#39;]-tone_df[\u0026#39;negative_num\u0026#39;])/tone_df[\u0026#39;word_num\u0026#39;] #tone_df[\u0026#39;tone1\u0026#39;] = (tone_df[\u0026#39;positive_num\u0026#39;]-tone_df[\u0026#39;negative_num\u0026#39;])/(tone_df[\u0026#39;word_num\u0026#39;]+1) #算法2 tone_df[\u0026#39;tone2\u0026#39;] = (tone_df[\u0026#39;positive_num\u0026#39;]-tone_df[\u0026#39;negative_num\u0026#39;])/(tone_df[\u0026#39;positive_num\u0026#39;]+tone_df[\u0026#39;negative_num\u0026#39;]) #tone_df[\u0026#39;tone2\u0026#39;] = (tone_df[\u0026#39;positive_num\u0026#39;]-tone_df[\u0026#39;negative_num\u0026#39;])/(tone_df[\u0026#39;positive_num\u0026#39;]+tone_df[\u0026#39;negative_num\u0026#39;]+1) tone_df.head() Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  negative_num positive_num stopword_num word_num sentence_num tone1 tone2     0 67 76 462 1831 52 0.004915 0.062937   1 32 59 372 1266 34 0.021327 0.296703   2 18 33 178 816 19 0.018382 0.294118   3 81 114 1055 3619 90 0.009119 0.169231   4 27 17 134 453 16 -0.022075 -0.227273     将结果存储到xlsx中\ntone_df.to_excel(\u0026#39;output/管理层-语调分析.xlsx\u0026#39;) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/manager_tone_analysis_with_lm/","summary":"今天分享的这篇论文通过 有道翻译 这一简单有效的的方式汉化了 LM英文金融词典，并使用 数词语数个数 的方式构造了管理层语调这个指标。\n文献 曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.\n本文代码实现的视频讲解已更新至付费课 Python实证指标构建与文本分析 中。\n 点击上方图片购买课程   本文代码 点击下载\n摘要 基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在**「口是心非」** 的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与 「年报语调」 相关。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。\n关键词 年报; 语调管理; 内部人交易; 信息不对称;\n代码  年报数据 data/reports.csv LM金融词典 需要更新cntext库至于1.7.3及以上版本  \n语调指标  算法1 该年报内 「积极词汇数 与消极词汇数 之差」 占 「年报总词汇数」 的比例； 算法2 （积极词汇数-消极词汇数）/（ 积极词汇数+消极词汇数）  import pandas as pd df = pd.read_csv(\u0026#39;data/reports.csv\u0026#39;) df.head() Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .","title":"管理世界 | 使用LM中文金融词典对年报进行语调分析"},{"content":"本文资料 点击下载本文资料\n词典污染了 实在抱歉，大邓的粗心导致词典DUTIR被污染。大家如果使用cntext中的DUTIR，麻烦更新至1.7.2版本。\nimport cntext as ct print(ct.__version__) dutir = ct.load_pkl_dict(\u0026#39;DUTIR.pkl\u0026#39;) dutir Run\n1.7.1 {\u0026#39;DUTIR\u0026#39;: {\u0026#39;哀\u0026#39;: [\u0026#39;怀想\u0026#39;, \u0026#39;治丝而棼\u0026#39;, \u0026#39;伤害\u0026#39;,...], \u0026#39;好\u0026#39;: [\u0026#39;进贤黜奸\u0026#39;, \u0026#39;清醇\u0026#39;, \u0026#39;放达\u0026#39;, ...], \u0026#39;惊\u0026#39;: [\u0026#39;惊奇不已\u0026#39;, \u0026#39;魂惊魄惕\u0026#39;, \u0026#39;海外奇谈\u0026#39;,...], \u0026#39;惧\u0026#39;: [\u0026#39;忸忸怩怩\u0026#39;, \u0026#39;谈虎色变\u0026#39;, \u0026#39;手忙脚乱\u0026#39;,...], \u0026#39;乐\u0026#39;: [\u0026#39;百龄眉寿\u0026#39;, \u0026#39;娱心\u0026#39;, \u0026#39;如意\u0026#39;,...], \u0026#39;怒\u0026#39;: [\u0026#39;饮恨吞声\u0026#39;, \u0026#39;扬眉瞬目\u0026#39;,...], \u0026#39;恶\u0026#39;: [\u0026#39;出逃\u0026#39;, \u0026#39;鱼肉百姓\u0026#39;, \u0026#39;移天易日\u0026#39;,...]}, \u0026#39;Desc\u0026#39;: \u0026#39;大连理工大学情感本体库，细粒度情感词典。含七大类情绪，依次是哀, 好, 惊, 惧, 乐, 怒, 恶\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.\u0026#39;} 七大类情绪有:\ndutir[\u0026#39;DUTIR\u0026#39;].keys() Run\ndict_keys([\u0026#39;哀\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;惊\u0026#39;, \u0026#39;惧\u0026#39;, \u0026#39;乐\u0026#39;, \u0026#39;怒\u0026#39;, \u0026#39;恶\u0026#39;]) 下面是Bug!\nfor key in dutir[\u0026#39;DUTIR\u0026#39;].keys(): if \u0026#39;开心\u0026#39; in dutir[\u0026#39;DUTIR\u0026#39;][key]: print(\u0026#39;「开心」出现在情绪【{}】词表中\u0026#39;.format(key)) Run\n「开心」出现在情绪【乐】词表中 「开心」出现在情绪【恶】词表中 词语「开心」同时出现在情绪【乐】和【恶】\nDUTIR词典 在网上找到大连理工大学情感本体文献、词典xlsx文件。\n制作方法，把 21 种小情绪汇总到喜怒哀乐等七大类情绪中。词典被污染，很可能是我汇总过程中出的问题。\nimport pandas as pd df = pd.read_excel(\u0026#39;大连理工大学中文情感词汇本体.xlsx\u0026#39;) df.head() Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  词语 词性种类 词义数 词义序号 情感分类 强度 极性 辅助情感分类 强度.1 极性.1 Unnamed: 10 Unnamed: 11     0 脏乱 adj 1.0 1.0 NN 7 2 NaN NaN NaN NaN NaN   1 糟报 adj 1.0 1.0 NN 5 2 NaN NaN NaN NaN NaN   2 早衰 adj 1.0 1.0 NE 5 2 NaN NaN NaN NaN NaN   3 责备 verb 1.0 1.0 NN 5 2 NaN NaN NaN NaN NaN   4 贼眼 noun 1.0 1.0 NN 5 2 NaN NaN NaN NaN NaN     汇总七类情绪 #乐 le_cates = [\u0026#39;PA\u0026#39;, \u0026#39;PE\u0026#39;] #好 hao_cates = [\u0026#39;PD\u0026#39;, \u0026#39;PH\u0026#39;, \u0026#39;PG\u0026#39;, \u0026#39;PB\u0026#39;, \u0026#39;PK\u0026#39;] # 怒 nu_cates = [\u0026#39;NA\u0026#39;] #哀 ai_cates = [\u0026#39;NB\u0026#39;, \u0026#39;NJ\u0026#39;, \u0026#39;NH\u0026#39;, \u0026#39;PF\u0026#39;] # 惧 ju_cates = [\u0026#39;NI\u0026#39;, \u0026#39;NC\u0026#39;, \u0026#39;NG\u0026#39;] # 恶 wu_cates = [\u0026#39;NE\u0026#39;, \u0026#39;ND\u0026#39;, \u0026#39;NN\u0026#39;, \u0026#39;NK\u0026#39;, \u0026#39;NL\u0026#39;] # 惊 jing_cates = [\u0026#39;PC\u0026#39;] def emotion(cates): dfs = [] for cate in cates: sdf = df[df[\u0026#39;情感分类\u0026#39;]==cate] dfs.append(sdf) res_df = pd.concat(dfs, axis=0) return res_df[\u0026#39;词语\u0026#39;].tolist() # 情绪【乐】的词语有： le_words = emotion(cates=le_cates) print(le_words[:10]) Run\n[\u0026#39;瑞雪\u0026#39;, \u0026#39;神采\u0026#39;, \u0026#39;喜人\u0026#39;, \u0026#39;怡悦\u0026#39;, \u0026#39;进益\u0026#39;, \u0026#39;奏凯\u0026#39;, \u0026#39;鸾凤和鸣\u0026#39;, \u0026#39;特等\u0026#39;, \u0026#39;欢快\u0026#39;, \u0026#39;如意\u0026#39;] \n制作DUTIR.pkl 将DUTIR介绍、文献出处、对应的词典汇总到字典，并制作生成DUTIR.pkl文件\ndutir = dict() dutir[\u0026#39;乐\u0026#39;] = senti(cates=ju_cates) dutir[\u0026#39;好\u0026#39;] = senti(cates=hao_cates) dutir[\u0026#39;怒\u0026#39;] = senti(cates=nu_cates) dutir[\u0026#39;哀\u0026#39;] = senti(cates=ai_cates) dutir[\u0026#39;惧\u0026#39;] = senti(cates=ju_cates) dutir[\u0026#39;恶\u0026#39;] = senti(cates=e_cates) dutir[\u0026#39;惊\u0026#39;] = senti(cates=jing_cates) data = {\u0026#39;DUTIR\u0026#39;: dutir, \u0026#39;Desc\u0026#39;: \u0026#39;大连理工大学情感本体库，细粒度情感词典。含七大类情绪，依次是哀, 好, 惊, 惧, 乐, 怒, 恶\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.\u0026#39;} import pickle with open(\u0026#39;DUTIR.pkl\u0026#39;, \u0026#39;wb\u0026#39;) as f: pickle.dump(data, f) \n更新cntext 解决DUTIR词典问题， 需更新至1.7.9版本。\npip3 install cntext==1.7.9\n现在我们检查下刚刚的问题\nimport cntext as ct print(ct.__version__) dutir = ct.load_pkl_dict(\u0026#39;DUTIR.pkl\u0026#39;) for key in dutir[\u0026#39;DUTIR\u0026#39;].keys(): if \u0026#39;开心\u0026#39; in dutir[\u0026#39;DUTIR\u0026#39;][key]: print(\u0026#39;「开心」只出现在情绪【{}】词表中\u0026#39;.format(key)) Run\n1.7.9 「开心」只出现在情绪【恶】词表中 \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/fixed_dutir_bug/","summary":"本文资料 点击下载本文资料\n词典污染了 实在抱歉，大邓的粗心导致词典DUTIR被污染。大家如果使用cntext中的DUTIR，麻烦更新至1.7.2版本。\nimport cntext as ct print(ct.__version__) dutir = ct.load_pkl_dict(\u0026#39;DUTIR.pkl\u0026#39;) dutir Run\n1.7.1 {\u0026#39;DUTIR\u0026#39;: {\u0026#39;哀\u0026#39;: [\u0026#39;怀想\u0026#39;, \u0026#39;治丝而棼\u0026#39;, \u0026#39;伤害\u0026#39;,...], \u0026#39;好\u0026#39;: [\u0026#39;进贤黜奸\u0026#39;, \u0026#39;清醇\u0026#39;, \u0026#39;放达\u0026#39;, ...], \u0026#39;惊\u0026#39;: [\u0026#39;惊奇不已\u0026#39;, \u0026#39;魂惊魄惕\u0026#39;, \u0026#39;海外奇谈\u0026#39;,...], \u0026#39;惧\u0026#39;: [\u0026#39;忸忸怩怩\u0026#39;, \u0026#39;谈虎色变\u0026#39;, \u0026#39;手忙脚乱\u0026#39;,...], \u0026#39;乐\u0026#39;: [\u0026#39;百龄眉寿\u0026#39;, \u0026#39;娱心\u0026#39;, \u0026#39;如意\u0026#39;,...], \u0026#39;怒\u0026#39;: [\u0026#39;饮恨吞声\u0026#39;, \u0026#39;扬眉瞬目\u0026#39;,...], \u0026#39;恶\u0026#39;: [\u0026#39;出逃\u0026#39;, \u0026#39;鱼肉百姓\u0026#39;, \u0026#39;移天易日\u0026#39;,...]}, \u0026#39;Desc\u0026#39;: \u0026#39;大连理工大学情感本体库，细粒度情感词典。含七大类情绪，依次是哀, 好, 惊, 惧, 乐, 怒, 恶\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.\u0026#39;} 七大类情绪有:\ndutir[\u0026#39;DUTIR\u0026#39;].keys() Run\ndict_keys([\u0026#39;哀\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;惊\u0026#39;, \u0026#39;惧\u0026#39;, \u0026#39;乐\u0026#39;, \u0026#39;怒\u0026#39;, \u0026#39;恶\u0026#39;]) 下面是Bug!\nfor key in dutir[\u0026#39;DUTIR\u0026#39;].","title":"cntext库 | 关于DUTIR被污染解决办法"},{"content":"「scikit-learn」作为经典的机器学习框架，其运算速度一直广受用户的诟病。今天分享一个新包 「sklearnex」，可以在不改变原有代码的基础上，获得数十倍甚至上千倍的效率提升。\nsklearnex简介 借助英特尔® Extension for Scikit-learn，您可以加速您的 Scikit-learn 应用程序，并且不需修改 Scikit-Learn原有代码结构。 英特尔® Extension for Scikit-learn是一款免费软件 AI 加速器，可带来超过 10-100 倍的加速。\n安装sklearnex pip3 install notebook scikit-learn-intelex \n快速上手 scikit-learn原代码 import numpy as np from sklearn.cluster import KMeans X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]) kmeans = KMeans(n_clusters=2, random_state=0).fit(X) print(f\u0026#34;kmeans.labels_ = {kmeans.labels_}\u0026#34;) Run\nkmeans.labels_ = [1 1 1 0 0 0] \n加速代码 import numpy as np from sklearnex import patch_sklearn patch_sklearn() #启动加速补丁 # 加速补丁放置于sklearn之前 from sklearn.cluster import KMeans X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]) kmeans = KMeans(n_clusters=2, random_state=0).fit(X) print(f\u0026#34;kmeans.labels_ = {kmeans.labels_}\u0026#34;) Run\nkmeans.labels_ = [1 1 1 0 0 0] \n去掉加速 一般使用sklearn的环境是jupyter内，如果使用了sklearnex的加速功能，那么整个jupyter文件都是加速环境。如果想回到常规速度，可以在机器学习算法之前使用unpatch_sklearn()回到sklearn正常速度\nfrom sklearnex import unpatch_sklearn unpatch_sklearn() # Re-import scikit-learn algorithms after the unpatch: from sklearn.cluster import KMeans \n效率对比 按照官方说法，越强劲的CPU可以获得的性能提升比例也会更高，下图是官方在Intel Xeon Platinum 8275CL处理器下测试了一系列算法后得出的性能提升结果，不仅可以提升训练速度，还可以提升模型推理预测速度，在某些场景下甚至达到数千倍的性能提升。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/speed_up_sklearn_code_with_sklearnex/","summary":"「scikit-learn」作为经典的机器学习框架，其运算速度一直广受用户的诟病。今天分享一个新包 「sklearnex」，可以在不改变原有代码的基础上，获得数十倍甚至上千倍的效率提升。\nsklearnex简介 借助英特尔® Extension for Scikit-learn，您可以加速您的 Scikit-learn 应用程序，并且不需修改 Scikit-Learn原有代码结构。 英特尔® Extension for Scikit-learn是一款免费软件 AI 加速器，可带来超过 10-100 倍的加速。\n安装sklearnex pip3 install notebook scikit-learn-intelex \n快速上手 scikit-learn原代码 import numpy as np from sklearn.cluster import KMeans X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]) kmeans = KMeans(n_clusters=2, random_state=0).fit(X) print(f\u0026#34;kmeans.labels_ = {kmeans.labels_}\u0026#34;) Run\nkmeans.labels_ = [1 1 1 0 0 0] \n加速代码 import numpy as np from sklearnex import patch_sklearn patch_sklearn() #启动加速补丁 # 加速补丁放置于sklearn之前 from sklearn.","title":"sklearnex库 | 让你的scikit-learn代码加速百倍"},{"content":"管理世界 曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.DOI:10.19744/j.cnki.11-1235/f.2018.09.012.\n本文代码实现\n 摘要: 基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在\u0026quot;口是心非\u0026quot;的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与年报语调相关。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。\n**关键词：**年报; 语调管理; 内部人交易; 信息不对称;\n 洪永淼,汪寿阳.大数据如何改变经济学研究范式？[J].管理世界,2021,37(10):40-55+72+56.DOI:10.19744/j.cnki.11-1235/f.2021.0153.\n 摘要： 本文首先从经济学视角探讨大数据给经济学实证研究所带来的范式变革,包括从理性经济人到非完全理性经济人,从孤立的经济人到互相关联的社会经济人,从代表性经济人到异质性经济主体,以及从经济分析到经济社会活动的系统分析。然后,从方法论视角讨论大数据给经济学实证研究方法所带来的变革,包括从模型驱动到数据驱动,从参数不确定性到模型不确定性,从无偏估计到有偏估计,从低维建模到高维建模,从低频数据到高频甚至实时数据,从结构化数据到非结构化数据,从传统结构化数据到新型结构化数据,以及从人工分析到智能分析等。大数据引起的经济学研究范式与研究方法变革,正在深刻重塑经济学发展方向,不但加强了经济学实证研究范式的趋势,而且还进一步突破了现代西方经济学的一些基本假设的局限性,使经济学研究日益呈现出科学化、严谨化、精细化、多元化(跨学科)与系统化的趋势,并且与社会科学其他领域在方法论上日益趋同。中国大数据资源,为从中国经济实践中总结经济发展规律,从中国特殊性中凝练可复制的经济发展模式,从而构建具有深厚学理基础的原创性中国经济理论体系,提供了一个得天独厚的\u0026quot;富矿\u0026quot;。\n关键词：\t大数据;文本分析;机器学习;研究范式;研究方法;反身性;\n 张宗新,吴钊颖.媒体情绪传染与分析师乐观偏差——基于机器学习文本分析方法的经验证据[J].管理世界,2021,37(01):170-185+11+20-22.DOI:10.19744/j.cnki.11-1235/f.2021.0011.\n 摘要： 本文利用2013～2017年上市公司的百度新闻报道作为文本,运用机器学习文本分析方法测算情绪倾向得分,考察了媒体情绪对分析师预测行为的影响及其传染机制与风险后果。研究发现:(1)媒体乐观情绪会显著正向影响分析师盈利预测的乐观偏差度;(2)媒体情绪通过\u0026quot;分析师有限关注\u0026quot;与\u0026quot;投资者情绪\u0026quot;两条路径来影响分析师预测的乐观倾向;(3)分析师乐观情绪和媒体乐观情绪均会加剧股价波动及尾部风险,且分析师乐观情绪是媒体情绪影响股价波动的传导路径;(4)明星分析师与非明星分析师均会受到媒体情绪的感染,前者理性程度相对更高但其行为对股价波动冲击更为明显。本研究对于规范媒体行为,矫正分析师过度乐观偏差,合理引导理性投资具有重要意义。\n关键词：\t媒体报道情绪;分析师乐观偏差;股价波动;有限理性;\n 胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.DOI:10.19744/j.cnki.11-1235/f.2021.0070.\n **摘要：**在可持续发展战略导向下,秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基石。然而,作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间导向理论,提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系,并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现,年报MD\u0026amp;A中披露的\u0026quot;短期视域\u0026quot;语言能够反映管理者内在的短视主义特质,管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时,管理者短视主义对这些长期投资的负向影响越易受到抑制。最终,管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析,对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时,本文将文本分析和机器学习方法引入管理者短视主义的研究,为未来该领域的研究提供了参考和借鉴。\n关键词： 管理者短视;长期投资;文本分析;机器学习;\n 底璐璐,罗勇根,江伟,陈灿.客户年报语调具有供应链传染效应吗？——企业现金持有的视角[J].管理世界,2020,36(08):148-163.DOI:10.19744/j.cnki.11-1235/f.2020.0124.\n 摘要： 利用我国供应商企业前五名上市客户及其管理层语调的文本数据,本文考察了跨企业关系情形下客户年报语调对供应商企业现金持有决策的影响。研究结果发现,客户的年报语调越消极,供应商企业则会持有更多的现金,表明客户年报净负面语调在供应链上存在传染效应。进一步的研究发现,非国有性质、相对议价能力较低的供应商企业现金持有与客户年报净负面语调的正相关关系分别显著强于国有性质、相对议价能力较高的供应商企业。此外,当客户融资融券程度较高时,客户年报净负面语调对供应商企业现金持有的正向影响会有所增强。本文的研究不仅在考察跨企业情形下企业现金持有的影响因素以及客户文本信息的经济后果两个方面弥补了国内外现有研究的不足,而且对于企业如何进行现金持有决策提供了一定的经验证据与参考,这对于管理供应链相关风险,推动我国企业的供应链整合进而提升我国企业的全球竞争力具有重要的启示意义。\n关键词：\t年报语调;现金持有;供应链传染;文本分析;\n 林晚发,赵仲匡,宋敏.管理层讨论与分析的语调操纵及其债券市场反应[J].管理世界,2022,38(01):164-180.DOI:10.19744/j.cnki.11-1235/f.2022.0012.\n **摘要: **本文研究了管理层讨论与分析（MD\u0026amp;A）语调的操纵行为及其债券市场反应。研究发现,MD\u0026amp;A异常积极语调与预警Z值负相关,债务重组正相关,这表明MD\u0026amp;A异常积极语调暗示了企业较高的未来风险,这与语调的信息增量解释相悖,因此MD\u0026amp;A异常积极语调更可能是操纵的结果。进一步研究发现,MD\u0026amp;A异常积极语调越大,债券信用评级越高,且该正向关系在与评级机构利益冲突大、信息透明度低的公司子样本中更显著;此外,债券投资者能够识别语调操纵行为,但随着债券市场公众投资者的参与,MD\u0026amp;A异常积极语调与债券信用利差之间呈现出一定的负向关系,且这种负向关系在信息透明度低的企业组中更加显著。本文较早使用中国资本市场数据度量了MD\u0026amp;A异常积极语调,且证实这种异常语调是管理层操纵的结果,并探讨了MD\u0026amp;A语调操纵对于债券市场信息效率的影响,相关结论对于完善MD\u0026amp;A文本信息披露监管法规、提高评级机构独立性以及提升债券市场信息效率具有重要启示。\n关键词：\nMD＆A语调操纵; 利益冲突; 债券信用评级; 债券信用利差;\n \n管理科学学报 马长峰, 陈志娟, 张顺明. 基于文本大数据分析的会计和金融研究综述[J]. 管理科学学报, 2020, 23(9):12..\n **摘要：**作为一种非结构化数据,文本大数据最近十年深刻影响会计学和金融学研究.这种影响体现在两类文献:第一类以信息为中心,将文本分析技术用于信息的品质(可读性)和数量(文本信息含量),信息披露和市场异象等方面的研究;第二类与信息无关,主要是利用文本大数据分析技术构建全新指标,例如基于文本分析的公司竞争力,创新和经济政策不确定性等新变量,梳理上述文献研究脉络,揭示文本分析技术的优缺点,并且指出在会计和金融领域应用文本大数据技术的研究面临的挑战和机遇。\n**关键词：**可读性; 信息; 欺诈; 创新; 经济政策不确定性\n 杨晓兰,王伟超,高媚.股市政策对股票市场的影响——基于投资者社会互动的视角[J].管理科学学报,2020,v.23;No.187(01):15-32.\n 摘要： 本文将影响股市的政策分为五类,检验股市的政策效应;并以新浪财经博客为投资者之间社会互动的媒介,利用文本挖掘技术和社会网络研究方法,构建反映投资者之间社会互动程度、情绪属性以及社会网络中心程度的变量,探讨社会互动对股市政策效应的影响.实证研究表明,舆论导向政策对股市收益率存在显著的正向影响;证券供给需求性政策、货币政策显著提高股市波动率,市场创新与市场交易制度显著降低市场波动率.同时,投资者对专业性政策的解读显著依赖于社会互动,社会互动会放大货币政策对股市收益率的正向影响,加剧证券供给需求性政策对股市波动的影响,平缓市场创新与市场交易制度对股市波动的影响,而不影响舆论导向政策对股市产生的效应.\n关键词：\t政策;社交网络;社会互动;股票市场;文本挖掘;\n 赵子夜,杨庆,杨楠.言多必失?管理层报告的样板化及其经济后果[J].管理科学学报,2019,22(03):53-70.\n **摘要：**样板化报告在古今中外都有广泛的运用,但报告者面临两难:一方面,样板化有利于规避披露风险;但另一方面,样板化又不利于传递内部信息.那么,投资者如何评价中国上市公司的报告的样板化程度?以中国上市公司的管理层讨论与分析的文字为样本,用公司t期和t-1期报告的纵向文本相似度以及本公司和其他公司同期的报告的平均横向相似度来衡量样板化的水平,并考察了其经济后果.检验结果表明,纵向样板化的经济后果呈现相机抉择性,当公司财务风险高（亏损、微利或者被出具非标准审计意见）时,信息效应占优,样板化的报告引发负面的市场评价,而当公司财务风险较低,风险效应占优,样板化的报告则引发市场的好评.另一方面,报告横向样板化则引起了整体的负面评价.在调节效应方面,纵向样板化的经济后果受公司创新、特质信息、董事长权力和停牌次数的影响,横向样板化的经济后果则受到公司独立董事的社会网络位置的影响.综合结果表明,公司管理层讨论与分析的横向样板化,以及在高财务风险条件下的纵向样本化都会因信息披露不足而引起负面的经济后果.\n关键词： 管理层报告;样板化;文本分析;经济后果;\n 卞世博, 管之凡, 阎志鹏. 答非所问与市场反应:基于业绩说明会的研究[J]. 管理科学学报, 2021, 24(4):18.\n 摘要: 对上市公司业绩说明会中投资者与管理层问答互动中管理层答非所问的现象进行了研究.本文以中小板和创业板上市公司召开的业绩说明会作为研究样本,利用文本分析方法对业绩说明会中管理层在回答投资者提问时答非所问的程度进行度量,进而实证分析了管理层的答非所问与市场反应和公司未来业绩表现之间的可能关联.结果 发现:在控制其它因素之后,管理层的答非所问与市场反应之间呈现显著的负相关关系,即公司管理层的答非所问程度越高,随后公司股票的市场表现则就会越差,并且对于那些低分析师关注的公司尤为明显;而在公司未来业绩表现方面,管理层答非所问的程度越高,则公司未来的业绩表现则会越差.。\n**关键词：**业绩说明会; 答非所问; 市场反应; 未来业绩\n 逯东, 宋昕倍. 媒体报道,上市公司年报可读性与融资约束[J]. 管理科学学报, 2021, 24(12):17..\n **摘要：**采用文本分析方法,深入考察了上市公司年报可读性与融资约束的关系,并考虑媒体报道这一外部信息的调节效应研究发现,上市公司的年报可读性越低,其面临的融资约束越高;媒体报道的增多可以弱化年报可读性与融资约束的关系,且媒体报道情绪越正向,其调节作用越显著.进一步分析发现:机构投资者持股比例较高能减弱年报可读性和融资约束的关系;当年报可读性较低时,媒体报道的信息效应更为显著;只有官方媒体和地方媒体的报道数量与正向报道情绪能够显著缓解年报可读性低带来的融资约束;同时,较低的年报可读性是通过提高融资成本路径来加大公司的融资约束,且使得公司未来的融资方式呈现出内部融资增加,外部融资减少的特点.从融资约束角度拓展了关于财务报告文本信息披露质量的研究,并揭示了媒体报道如何有效改善内部信息披露不足的作用机理,为企业如何通过改善内,外部的信息环境来缓解自身的融资困境提供了理论依据。\n**关键词：**年报可读性；融资约束；媒体报道；文本分析\n 姚加权, 冯绪, 王赞钧,等. 语调,情绪及市场影响:基于金融情绪词典[J]. 管理科学学报, 2021, 24(5):21.\n **摘要：**金融文本的语调与情绪含有上市公司管理层以及个体投资者表达的情感信息,并对股票市场产生影响.通过词典重组和深度学习算法构建了适用于正式文本与非正式文本的金融领域中文情绪词典,并基于词典构建了上市公司的年报语调和社交媒体情绪指标.构建的年报语调指标和社交媒体情绪指标能有效地预测上市公司股票的收益率,成交量,波动率和非预期盈余等市场因素,并优于基于其他广泛使用情绪词典构建的指标.此外,年报语调指标和社交媒体情绪指标对上市公司的股价崩盘风险具有显著的预测作用.为文本大数据在金融市场的应用提供了分析工具,也为大数据时代的金融市场预测和监管等活动提供了决策支持。\n**关键词：**情绪词典；语调；投资者情绪；市场影响\n 姜富伟, 马甜, 张宏伟. 高风险低收益? 基于机器学习的动态CAPM模型解释[J]. 管理科学学报, 2021.\n **摘要：**我国股票市场存在高风险股票反而伴随较低收益的低风险定价异象,这有悖于传统资产定价理论.本文使用宏观经济和微观企业特征构建了六百多个变量的宏微观混合大数据集,并结合多种经典机器学习算法开发了基于大数据和机器学习的智能动态CAPM模型,检验了时变系统性风险对我国股市收益解释能力.实证结果表明:本文的智能动态CAPM定价模型能够显著解释我国股市低风险定价异象;随机森林等非线性机器学习算法表现最佳;影响股票时变系统风险的主要因素是市场类因子,基本面因子居次.本文对于我国股市系统性风险测度,动态资产定价模型构建和金融与大数据和人工智能融合创新有重要理论与实践指导意义.\n**关键词：**系统性风险; 动态CAPM; 机器学习; 金融大数据\n 陆瑶, 张叶青, 黎波,等. 高管个人特征与公司业绩——基于机器学习的经验证据[J]. 管理科学学报, 2020, 23(2):21.\n 摘要： 在目前的公司治理文献中,大部分的高管特征研究一方面仅关注单一的高管特征与公司业绩之间的关联,缺乏全面的高管特征分析;另一方面主要围绕因果推断进行研究,缺乏从预测能力出发的系统定量的结论.本文首次采用机器学习算法中的Boosting回归树,全面考察了多维度高管特征对公司业绩的预测性.以我国2008年～2016年的上市公司为样本,研究了高管的多维个人特征是否能预测公司业绩,并进一步分析了对公司业绩预测能力较强的高管个人特征及其预测模式.研究发现:1)整体而言,在我国公司CEO和董事长的特征对公司业绩的预测能力较弱;2)在众多高管个人特征之中,高管持股比例和年龄对公司业绩的预测能力较强;3)高管持股比例和年龄与公司业绩之间的关联都呈现出非线性的特点,与以往的理论较为吻合.本研究不仅利用机器学习方法从一个更为全面的视角对中国的高管特征进行了研究,也为公司高管聘任和激励机制设计等方面提供了有益的启发.\n**关键词：**机器学习；Boosting回归树；公司治理；公司业绩\n 吴武清, 赵越, 闫嘉文,等. 分析师文本语调会影响股价同步性吗?\u0026ndash;基于利益相关者行为的中介效应检验[J]. 管理科学学报, 2020, 23(9):19.\n **摘要：**文章考察了分析师研究报告的文本语调对股价同步性的影响与作用机制.首先爬取2006年至2018年中国A股上市公司377644份分析师研究报告,随机选出10434句文本并人工分为积极,中性,消极三类形成语料库,以此训练11种机器学习方法并比较各方法的预测准确性,最终选择朴素贝叶斯方法估计出分析师研究报告的文本语调.实证分析发现,分析师积极的文本语调显著降低了所追踪公司的股价同步性.这一结果与已有多数研究结论不同,但在做空机制欠发达的中国资本市场,个体选择性知觉理论为此提供了很好的解释.进一步地,中介效应检验结果表明,分析师积极的文本语调通过激励公司发布更多公告,引导机构投资者买入和吸引其他分析师发布研究报告,显著降低了股价同步性.该研究对于投资者关注研报语调指标,上市公司加强信息披露,政府部门完善资本市场制度均具有重要启示。\n**关键词：**分析师文本语调; 股价同步性; 朴素贝叶斯; 选择性知觉; 中介效应\n 刘冠男, 曲金铭, 李小琳,等. 基于深度强化学习的救护车动态重定位调度研究[J]. 管理科学学报, 2020, 23(2):15.\n **摘要：**救护车是挽救患者生命的重要医疗资源,合理调配有限的救护车资源可以降低呼叫响应时间,提高医疗服务水平.本文面向救护车动态重定位调度问题,提出了一种基于强化学习的调度策略结构.为解决传统强化学习所面临的高维状态空间的挑战,本文基于深度Q值网络(DQN)方法,提出了一种考虑多种调度交互因子的算法RedCon-DQN,以在给定环境状态下得到最优的重定位调度策略.在此基础上,本文还提出了急救网络弹性概念,以评估各站点对全局救护优化目标的影响力.最后,基于南京市2016年～2017年的实际救护车呼叫及响应数据,构造了环境交互模拟器.在模拟器中通过大规模数据实验,验证了模型得到的调度策略相比已有方法的优越性,并分析了不同时段下调度策略的有效性及其特点.\n**关键词：**强化学习; DQN; 救护车调度; 重定位\n 黄丽华, 何晓, 卢向华. 企业在线社群内容组合策略的影响研究[J]. 管理科学学报, 2020, 23(2):15..\n **摘要：**现代企业通过建立在线社群实现与消费者的互动,希望在向消费者提供服务的同时进行更好的营销,然而如何提供在线社群中的营销与服务内容却是一大难题.本文在营销—服务二元理论的基础上,提出了在线社群内容二元性的平衡维度与结合维度概念,并研究平衡维度与结合维度如何影响销售业绩与消费者的满意度.结合机器学习方法,本文发现,平衡维度对消费者满意度和销售绩效有提高作用,但是,结合维度对消费者满意度及企业绩效的影响呈倒U型;另外,企业员工的技能水平对内容二元性策略的效果有着显著的调节作用.研究结论对企业理解在线社群中的营销内容与服务内容之间的二元关系,以及内容提供策略的价值机制有重要的指导意义。\n**关键词：**在线社群; 内容二元性; 销售绩效; 消费者满意度\n 部慧,解峥,李佳鸿,吴俊杰.基于股评的投资者情绪对股票市场的影响[J].管理科学学报,2018,v.21;No.166(04):86-101.\n 摘要： 探讨投资者情绪对我国股票市场的影响.为刻画投资者情绪,基于东方财富网股吧帖文与朴素贝叶斯方法,提出融合股评看涨看跌预期和投资者关注程度的投资者情绪度量指标.进一步,利用Granger因果检验、瞬时Granger因果检验、跨期回归分析等方法,探讨了投资者情绪对我国股票收益率、交易量和波动性是否具有预测能力及影响.实证结果揭示:虽然投资者情绪对股票市场收益率、交易量和波动性均无预测能力,但投资者情绪对股票收益率和交易量有当期影响;开盘前非交易时段的股评情绪对开盘价具有预测力,开盘后交易时段的股评情绪对收盘价和日交易量具有更显著的影响.此外,股票收益率是投资者情绪的Granger原因,即投资者情绪的形成依赖于前期市场收益率.这些实证结果为深入理解参与股吧评论的交易者的行为以及行为对市场产生的影响提供了证据.\n关键词: 投资者情绪; 噪声交易者; 文本挖掘; Granger因果检验;\n \n##金融研究\n姜富伟, 胡逸驰, \u0026amp; 黄楠. (2021). 央行货币政策报告文本信息, 宏观经济与股票市场. 金融研究, 492(6), 95-113.\n **摘要: ** 本文利用金融情感词典和文本分析技术,分析中国人民银行货币政策执行报告的文本情绪、文本相似度和文本可读性等多维文本信息,刻画央行货币政策执行报告的文本特征,探究货币政策报告的文本信息与宏观经济和股票市场的关系。实证研究发现,货币政策报告的文本情绪的改善会引起显著为正的股票市场价格反应,报告文本相似度的增加会引起股票市场波动性的显著降低,报告可读性对公布后股票市场的波动性影响不显著。货币政策报告文本情绪还与诸多宏观经济指标显著相关。进一步研究发现,引起股票市场显著反应的是报告文本情绪中反映货币政策指引的部分,而反映宏观经济历史状态的部分对股票市场的影响不显著。本文从文本大数据分析角度证明了我国央行沟通的有效性,对国内央行沟通相关研究形成了有益补充。\n关键词: 文本情绪分析 中央银行沟通 股票市场 宏观经济\n 孙彤, 薛爽, \u0026amp; 崔庆慧. (2021). 企业家前台化影响企业价值吗?——基于新浪微博的实证证据. 金融研究, 491(5), 189-206.\n 摘要: 互联网时代信息传递成本和沟通成本显著降低。微博作为自媒体的主要代表之一,为企业家从企业的幕后走向台前提供了一条便捷的途径。本文以信息传递理论为基础,利用新浪微博数据,检验了企业家前台化行为对企业价值的影响。实证结果表明:(1)企业家发布微博这一前台化行为有助于提升企业价值。从对价值影响的路径看,企业家微博发布后,企业经营活动现金流增加,系统性风险降低;(2)针对企业家微博进行文本分析,发现企业家微博中个性化微博比例越高、“艾特”人数越多或者微博内容中正向语调比例越高,对企业价值的正向影响越显著;(3)相对于信息不对称程度较低的企业,信息不对称程度较高的企业中企业家更倾向于发布微博。上述实证结果说明自媒体对缓解企业、企业家与投资者之间的信息不对称具有一定作用,为企业家前台化决策及路径选择提供了参考。\n关键词: 企业家 前台化 微博 企业价值 信息传递\n 阮睿, 孙宇辰, 唐悦, \u0026amp; 聂辉华. (2021). 资本市场开放能否提高企业信息披露质量?——基于 “沪港通” 和年报文本挖掘的分析. 金融研究, 488(2), 188-206.\n 摘要: 提高信息披露质量对于改善上市公司治理结构和保护股东权益具有重要意义。本文利用2014年开通的“沪港通”机制这一准自然实验,研究资本市场开放是否提高了企业的信息披露质量。从2010-2019年A股上市公司年报文本中提炼可读性指标衡量信息披露质量,使用匹配和双重差分方法进行实证研究,发现“沪港通”机制实施以后,标的公司(纳入“沪港通”的A股上市公司)的信息披露质量显著提高。这一结论对不同的估计方法、样本区间及控制变量组均保持稳健。异质性分析表明,对于盈余操纵水平较高、股价信息含量较低的企业,资本市场开放能够更好地改善其信息披露质量。本文丰富了资本市场开放对企业行为和绩效影响的实证研究,为继续推进资本市场开放政策提供了理论依据。\n关键词: 沪港通 资本市场开放 信息披露 文本分析\n 李哲, \u0026amp; 王文翰. (2021). “多言寡行” 的环境责任表现能否影响银行信贷获取——基于 “言” 和 “行” 双维度的文本分析. 金融研究, 498(12), 116-132.\n 摘要: 基于我国推行绿色信贷的政策背景,本文考察了企业“多言寡行”的环境责任表现能否影响银行的信贷决策。研究发现:(1)从总体来看,“多言寡行”的环境责任表现有助于企业获取更多的银行借款。(2)相比于长期银行借款,“多言寡行”对于短期银行借款的正向影响更为明显。(3)《关于构建绿色金融体系的指导意见》的出台抑制了“多言寡行”对银行借款的正向影响。(4)进一步分析发现,相比于环境责任表现“少言多行”以及“少言寡行”的企业,企业“多言寡行”的环境责任表现对于银行的信贷资源具有显著的正向影响;“多言寡行”对银行借款的正向影响在无背景关联、价值较低以及市场环境更差的企业中更为明显。本文有助于信贷机构认识到绿色信贷政策面临的执行风险,为确保绿色信贷的健康发展提供了新的决策参考。 关键词: 环境责任表现 绿色金融 绿色信贷 文本分析\n 潘健平, 潘越, \u0026amp; 马奕涵. (2019). 以 “合” 为贵? 合作文化与企业创新. 金融研究, 463(1), 148-167.\n 摘要: 本文以2006-2015年沪深A股非金融上市公司为样本,基于上市公司网站对于企业文化的叙述和年报董事会报告两份本文,采用文本分析方法,构建两个度量企业合作文化强弱的指标,并研究企业合作文化对企业创新产出和创新效率的影响。研究发现,企业文化越强调合作,企业的创新产出越多,创新效率越高。这一结论在采用增加控制变量、利用水稻播种面积作为工具变量以及以董事长的非正常离职事件为冲击进行PSM-DID等多种方法后仍然稳健。渠道检验的结果显示,合作文化是通过提高企业内部员工的凝聚力和促进企业的“产学研”合作这两种渠道来促进企业创新。进一步的研究表明,合作文化的促进作用在竞争性行业以及地区信任程度和产业集群程度较高的地区中尤为显著。本文不仅从微观层面揭示企业文化对公司财务行为的影响机理,丰富和补充了当前方兴未艾的“文化与金融”研究,而且为国家制定建设社会主义文化强国的方针战略提供理论基础和实证支持。\n关键词: 合作 企业文化 企业创新\n 彭红枫, \u0026amp; 林川. (2018). 言之有物: 网络借贷中语言有用吗?——来自人人贷借款描述的经验证据. 金融研究, 461(11), 133-153.\n 摘要: 本文以“人人贷”平台的388522条借款标的为样本,基于借款描述文本构造P2P网络借贷词典,并探究文本中六种类型词语比重对网络借贷行为的影响,实证结果表明:首先,各类词语比重发出的信号对贷款人的投资决策有显著影响,积极类词语和金融类词语比重与借款成功率呈正相关,消极类词语比重、强语气词语比重和弱语气词语比重均与借款成功率呈负相关关系;其次,不同年龄层次和不同收入水平的借款人提供的描述性文本中词语信号对贷款人行为的影响存在较大差异,而性别差异和学历高低基本不影响词语信号作用的发挥;最后,各类词语比重发出的质量信号是部分有效的,金融类词语比重发出的信号有效且被投资者正确识别,强语气词语比重发出的信号同样有效却未被投资者准确识别,其他类别词语比重不是有效质量信号。\n关键词: 网络借贷 文本分析 信号理论\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/research_with_tm_in_chinese_top_ms_journal/","summary":"管理世界 曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.DOI:10.19744/j.cnki.11-1235/f.2018.09.012.\n本文代码实现\n 摘要: 基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在\u0026quot;口是心非\u0026quot;的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与年报语调相关。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。\n**关键词：**年报; 语调管理; 内部人交易; 信息不对称;\n 洪永淼,汪寿阳.大数据如何改变经济学研究范式？[J].管理世界,2021,37(10):40-55+72+56.DOI:10.19744/j.cnki.11-1235/f.2021.0153.\n 摘要： 本文首先从经济学视角探讨大数据给经济学实证研究所带来的范式变革,包括从理性经济人到非完全理性经济人,从孤立的经济人到互相关联的社会经济人,从代表性经济人到异质性经济主体,以及从经济分析到经济社会活动的系统分析。然后,从方法论视角讨论大数据给经济学实证研究方法所带来的变革,包括从模型驱动到数据驱动,从参数不确定性到模型不确定性,从无偏估计到有偏估计,从低维建模到高维建模,从低频数据到高频甚至实时数据,从结构化数据到非结构化数据,从传统结构化数据到新型结构化数据,以及从人工分析到智能分析等。大数据引起的经济学研究范式与研究方法变革,正在深刻重塑经济学发展方向,不但加强了经济学实证研究范式的趋势,而且还进一步突破了现代西方经济学的一些基本假设的局限性,使经济学研究日益呈现出科学化、严谨化、精细化、多元化(跨学科)与系统化的趋势,并且与社会科学其他领域在方法论上日益趋同。中国大数据资源,为从中国经济实践中总结经济发展规律,从中国特殊性中凝练可复制的经济发展模式,从而构建具有深厚学理基础的原创性中国经济理论体系,提供了一个得天独厚的\u0026quot;富矿\u0026quot;。\n关键词：\t大数据;文本分析;机器学习;研究范式;研究方法;反身性;\n 张宗新,吴钊颖.媒体情绪传染与分析师乐观偏差——基于机器学习文本分析方法的经验证据[J].管理世界,2021,37(01):170-185+11+20-22.DOI:10.19744/j.cnki.11-1235/f.2021.0011.\n 摘要： 本文利用2013～2017年上市公司的百度新闻报道作为文本,运用机器学习文本分析方法测算情绪倾向得分,考察了媒体情绪对分析师预测行为的影响及其传染机制与风险后果。研究发现:(1)媒体乐观情绪会显著正向影响分析师盈利预测的乐观偏差度;(2)媒体情绪通过\u0026quot;分析师有限关注\u0026quot;与\u0026quot;投资者情绪\u0026quot;两条路径来影响分析师预测的乐观倾向;(3)分析师乐观情绪和媒体乐观情绪均会加剧股价波动及尾部风险,且分析师乐观情绪是媒体情绪影响股价波动的传导路径;(4)明星分析师与非明星分析师均会受到媒体情绪的感染,前者理性程度相对更高但其行为对股价波动冲击更为明显。本研究对于规范媒体行为,矫正分析师过度乐观偏差,合理引导理性投资具有重要意义。\n关键词：\t媒体报道情绪;分析师乐观偏差;股价波动;有限理性;\n 胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.DOI:10.19744/j.cnki.11-1235/f.2021.0070.\n **摘要：**在可持续发展战略导向下,秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基石。然而,作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间导向理论,提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系,并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现,年报MD\u0026amp;A中披露的\u0026quot;短期视域\u0026quot;语言能够反映管理者内在的短视主义特质,管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时,管理者短视主义对这些长期投资的负向影响越易受到抑制。最终,管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析,对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时,本文将文本分析和机器学习方法引入管理者短视主义的研究,为未来该领域的研究提供了参考和借鉴。\n关键词： 管理者短视;长期投资;文本分析;机器学习;\n 底璐璐,罗勇根,江伟,陈灿.客户年报语调具有供应链传染效应吗？——企业现金持有的视角[J].管理世界,2020,36(08):148-163.DOI:10.19744/j.cnki.11-1235/f.2020.0124.\n 摘要： 利用我国供应商企业前五名上市客户及其管理层语调的文本数据,本文考察了跨企业关系情形下客户年报语调对供应商企业现金持有决策的影响。研究结果发现,客户的年报语调越消极,供应商企业则会持有更多的现金,表明客户年报净负面语调在供应链上存在传染效应。进一步的研究发现,非国有性质、相对议价能力较低的供应商企业现金持有与客户年报净负面语调的正相关关系分别显著强于国有性质、相对议价能力较高的供应商企业。此外,当客户融资融券程度较高时,客户年报净负面语调对供应商企业现金持有的正向影响会有所增强。本文的研究不仅在考察跨企业情形下企业现金持有的影响因素以及客户文本信息的经济后果两个方面弥补了国内外现有研究的不足,而且对于企业如何进行现金持有决策提供了一定的经验证据与参考,这对于管理供应链相关风险,推动我国企业的供应链整合进而提升我国企业的全球竞争力具有重要的启示意义。\n关键词：\t年报语调;现金持有;供应链传染;文本分析;\n 林晚发,赵仲匡,宋敏.管理层讨论与分析的语调操纵及其债券市场反应[J].管理世界,2022,38(01):164-180.DOI:10.19744/j.cnki.11-1235/f.2022.0012.\n **摘要: **本文研究了管理层讨论与分析（MD\u0026amp;A）语调的操纵行为及其债券市场反应。研究发现,MD\u0026amp;A异常积极语调与预警Z值负相关,债务重组正相关,这表明MD\u0026amp;A异常积极语调暗示了企业较高的未来风险,这与语调的信息增量解释相悖,因此MD\u0026amp;A异常积极语调更可能是操纵的结果。进一步研究发现,MD\u0026amp;A异常积极语调越大,债券信用评级越高,且该正向关系在与评级机构利益冲突大、信息透明度低的公司子样本中更显著;此外,债券投资者能够识别语调操纵行为,但随着债券市场公众投资者的参与,MD\u0026amp;A异常积极语调与债券信用利差之间呈现出一定的负向关系,且这种负向关系在信息透明度低的企业组中更加显著。本文较早使用中国资本市场数据度量了MD\u0026amp;A异常积极语调,且证实这种异常语调是管理层操纵的结果,并探讨了MD\u0026amp;A语调操纵对于债券市场信息效率的影响,相关结论对于完善MD\u0026amp;A文本信息披露监管法规、提高评级机构独立性以及提升债券市场信息效率具有重要启示。\n关键词：\nMD＆A语调操纵; 利益冲突; 债券信用评级; 债券信用利差;\n \n管理科学学报 马长峰, 陈志娟, 张顺明. 基于文本大数据分析的会计和金融研究综述[J]. 管理科学学报, 2020, 23(9):12..\n **摘要：**作为一种非结构化数据,文本大数据最近十年深刻影响会计学和金融学研究.这种影响体现在两类文献:第一类以信息为中心,将文本分析技术用于信息的品质(可读性)和数量(文本信息含量),信息披露和市场异象等方面的研究;第二类与信息无关,主要是利用文本大数据分析技术构建全新指标,例如基于文本分析的公司竞争力,创新和经济政策不确定性等新变量,梳理上述文献研究脉络,揭示文本分析技术的优缺点,并且指出在会计和金融领域应用文本大数据技术的研究面临的挑战和机遇。\n**关键词：**可读性; 信息; 欺诈; 创新; 经济政策不确定性\n 杨晓兰,王伟超,高媚.股市政策对股票市场的影响——基于投资者社会互动的视角[J].管理科学学报,2020,v.23;No.187(01):15-32.\n 摘要： 本文将影响股市的政策分为五类,检验股市的政策效应;并以新浪财经博客为投资者之间社会互动的媒介,利用文本挖掘技术和社会网络研究方法,构建反映投资者之间社会互动程度、情绪属性以及社会网络中心程度的变量,探讨社会互动对股市政策效应的影响.实证研究表明,舆论导向政策对股市收益率存在显著的正向影响;证券供给需求性政策、货币政策显著提高股市波动率,市场创新与市场交易制度显著降低市场波动率.同时,投资者对专业性政策的解读显著依赖于社会互动,社会互动会放大货币政策对股市收益率的正向影响,加剧证券供给需求性政策对股市波动的影响,平缓市场创新与市场交易制度对股市波动的影响,而不影响舆论导向政策对股市产生的效应.\n关键词：\t政策;社交网络;社会互动;股票市场;文本挖掘;","title":"近年《管理世界》《管理科学学报》《金融研究》使用文本分析论文"},{"content":"   引言 总有一些你不认识的人，知道你想知道的东西。『大邓和他的Python』 或许可以成为一座桥梁，在大数据时代，促使不同背景、不同方向的学者学术灵感相互碰撞，迸发出更多的可能性。\n『大邓和他的Python』 鼓励分享 Python/R技术、文本(数据)分析、经管科研(含Python或R)等内容。目的只有一个，让数据科学在社会科学中更接地气。\n内容选题 未来公众号的选题内容规划\n 网络爬虫(数据采集) 文本、音频、视频、文件等数据处理 机器学习、自然语言处理 经管、社科领域，借助数据挖掘的研究和技术 Python相关技术分享 其他(待定)  稿件要求   文章确系个人原创作品，未曾在公开渠道发表， 如为其他平台已发表或待发表的文章，请明确标注\n  稿件建议以 markdown 格式撰写， 示例链接: https://pan.baidu.com/s/1ZpvWhrGGbah71YbkW-7pjg 提取码: upuc\n  投递邮件发送至 thunderhit@qq.com\n  作者福利   尊重原作者署名权，并将为每篇被采纳的原创首发稿件， 提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算。\n  如作者内容分享成体系，文稿质量高，公众号可组织付费直播课。\n  ","permalink":"/blog/call_for_paper/","summary":"   引言 总有一些你不认识的人，知道你想知道的东西。『大邓和他的Python』 或许可以成为一座桥梁，在大数据时代，促使不同背景、不同方向的学者学术灵感相互碰撞，迸发出更多的可能性。\n『大邓和他的Python』 鼓励分享 Python/R技术、文本(数据)分析、经管科研(含Python或R)等内容。目的只有一个，让数据科学在社会科学中更接地气。\n内容选题 未来公众号的选题内容规划\n 网络爬虫(数据采集) 文本、音频、视频、文件等数据处理 机器学习、自然语言处理 经管、社科领域，借助数据挖掘的研究和技术 Python相关技术分享 其他(待定)  稿件要求   文章确系个人原创作品，未曾在公开渠道发表， 如为其他平台已发表或待发表的文章，请明确标注\n  稿件建议以 markdown 格式撰写， 示例链接: https://pan.baidu.com/s/1ZpvWhrGGbah71YbkW-7pjg 提取码: upuc\n  投递邮件发送至 thunderhit@qq.com\n  作者福利   尊重原作者署名权，并将为每篇被采纳的原创首发稿件， 提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算。\n  如作者内容分享成体系，文稿质量高，公众号可组织付费直播课。\n  ","title":"长期征稿"},{"content":" 作者: 小猴子\n公众号: 机器学习研习院\n 本文旨在使用 XGBoost、随机森林、KNN、逻辑回归、SVM 和决策树解决分类问题\n案例简介 假设你受雇于帮助一家信用卡公司检测潜在的欺诈案件，你的工作是确保客户不会因未购买的商品而被收取费用。给你一个包含人与人之间交易的数据集，他们是欺诈与否的信息，并要求你区分它们。我们的最终目的是通过构建分类模型来对欺诈交易进行分类区分来解决上述情况。\n代码下载 点击下载\n对于这个案例，所需要用到的主要模块是处理数据的 Pandas、处理数组的 NumPy、用于数据拆分、构建和评估分类模型的 scikit-learn，最后是用于 xgboost 分类器模型算法的 xgboost 包。\n导入数据 关于数据： 我们将要使用的数据是 Kaggle 信用卡欺诈检测数据集。它包含特征 V1 到 V28，是 PCA 获得的主要成分，并忽略对构建模型没有用的时间特征。其余的特征是包含交易总金额的\u0026quot;金额\u0026quot;特征和包含交易是否为欺诈案件的\u0026quot;类别\u0026quot;特征。\n现在使用\u0026rsquo;pd.read_csv\u0026rsquo;方法导入数据，并查看部分数据样例。\nKaggle 信用卡欺诈检测数据集: https://www.kaggle.com/mlg-ulb/creditcardfraud\nimport pandas as pd df = pd.read_csv(\u0026#39;creditcard.csv\u0026#39;) df.drop(\u0026#39;Time\u0026#39;, axis = 1, inplace = True) df.head() Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class     0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 0.090794 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 149.62 0   1 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 -0.166974 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 2.69 0   2 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 0.207643 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 378.66 0   3 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 -0.054952 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 123.50 0   4 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 0.753074 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 69.99 0    5 rows × 30 columns\n 接下来将进行一些数据预处理和探索性数据分析（EDA）。\n探索性数据分析 看看数据集中有多少欺诈案件和非欺诈案件。此外，还计算整个记录交易中欺诈案件的百分比。\nfrom termcolor import colored as cl cases = len(df) nonfraud_count = len(df[df.Class == 0]) fraud_count = len(df[df.Class == 1]) fraud_percentage = round(fraud_count/nonfraud_count*100, 2) print(cl(\u0026#39;CASE COUNT\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;--------------------------------------------\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;Total number of cases are {}\u0026#39;.format(cases), attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;Number of Non-fraud cases are {}\u0026#39;.format(nonfraud_count), attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;Number of Non-fraud cases are {}\u0026#39;.format(fraud_count), attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;Percentage of fraud cases is {}\u0026#39;.format(fraud_percentage), attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;--------------------------------------------\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) Run\n\u001b[1mCASE COUNT\u001b[0m \u001b[1m--------------------------------------------\u001b[0m \u001b[1mTotal number of cases are 284807\u001b[0m \u001b[1mNumber of Non-fraud cases are 284315\u001b[0m \u001b[1mNumber of Non-fraud cases are 492\u001b[0m \u001b[1mPercentage of fraud cases is 0.17\u001b[0m \u001b[1m--------------------------------------------\u001b[0m  我们可以看到，在 284,807 个样本中，只有 492 个欺诈案例，仅占样本总数的 0.17% 。所以，可以说我们正在处理的数据是高度不平衡的数据，需要在建模和评估时谨慎处理。\n接下来，我们将使用 Python 中的**\u0026ldquo;describe\u0026rdquo;**方法获取欺诈和非欺诈交易金额数据的统计视图。\nnonfraud_cases = df[df.Class == 0] fraud_cases = df[df.Class == 1] print(cl(\u0026#39;CASE AMOUNT STATISTICS\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;--------------------------------------------\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;NON-FRAUD CASE AMOUNT STATS\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) print(nonfraud_cases.Amount.describe()) print(cl(\u0026#39;--------------------------------------------\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;FRAUD CASE AMOUNT STATS\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) print(fraud_cases.Amount.describe()) print(cl(\u0026#39;--------------------------------------------\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) Run\n\u001b[1mCASE AMOUNT STATISTICS\u001b[0m \u001b[1m--------------------------------------------\u001b[0m \u001b[1mNON-FRAUD CASE AMOUNT STATS\u001b[0m count 284315.000000 mean 88.291022 std 250.105092 min 0.000000 25% 5.650000 50% 22.000000 75% 77.050000 max 25691.160000 Name: Amount, dtype: float64 \u001b[1m--------------------------------------------\u001b[0m \u001b[1mFRAUD CASE AMOUNT STATS\u001b[0m count 492.000000 mean 122.211321 std 256.683288 min 0.000000 25% 1.000000 50% 9.250000 75% 105.890000 max 2125.870000 Name: Amount, dtype: float64 \u001b[1m--------------------------------------------\u001b[0m  在查看统计数据时，可以看到与其余变量相比，\u0026quot;金额\u0026quot; 变量中的值变化很大。为了减少其广泛的值，我们可以使用 python 中的 \u0026ldquo;StandardScaler()\u0026rdquo; 方法对其进行标准化。\nfrom sklearn.preprocessing import StandardScaler sc = StandardScaler() amount = df[\u0026#39;Amount\u0026#39;].values df[\u0026#39;Amount\u0026#39;] = sc.fit_transform(amount.reshape(-1, 1)) print(cl(df[\u0026#39;Amount\u0026#39;].head(10), attrs = [\u0026#39;bold\u0026#39;])) Run\n\u001b[1m0 0.244964 1 -0.342475 2 1.160686 3 0.140534 4 -0.073403 5 -0.338556 6 -0.333279 7 -0.190107 8 0.019392 9 -0.338516 Name: Amount, dtype: float64\u001b[0m  特征选择和数据集拆分 在这个过程中，定义自变量 (X) 和因变量 (Y)。使用定义的变量将数据分成训练集和测试集，进一步用于建模和评估。可以使用 python 中的 \u0026ldquo;train_test_split\u0026rdquo; 算法轻松拆分数据。\nfrom sklearn.model_selection import train_test_split # DATA SPLIT X = df.drop(\u0026#39;Class\u0026#39;, axis = 1).values y = df[\u0026#39;Class\u0026#39;].values X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state = 0) print(cl(\u0026#39;X_train samples : \u0026#39;, attrs = [\u0026#39;bold\u0026#39;]), X_train[:1]) print(cl(\u0026#39;X_test samples : \u0026#39;, attrs = [\u0026#39;bold\u0026#39;]), X_test[0:1]) print(cl(\u0026#39;y_train samples : \u0026#39;, attrs = [\u0026#39;bold\u0026#39;]), y_train[0:20]) print(cl(\u0026#39;y_test samples : \u0026#39;, attrs = [\u0026#39;bold\u0026#39;]), y_test[0:20]) Run\n\u001b[1mX_train samples : \u001b[0m [[-1.11504743 1.03558276 0.80071244 -1.06039825 0.03262117 0.85342216 -0.61424348 -3.23116112 1.53994798 -0.81690879 -1.30559201 0.1081772 -0.85960958 -0.07193421 0.90665563 -1.72092961 0.79785322 -0.0067594 1.95677806 -0.64489556 3.02038533 -0.53961798 0.03315649 -0.77494577 0.10586781 -0.43085348 0.22973694 -0.0705913 -0.30145418]] \u001b[1mX_test samples : \u001b[0m [[-0.32333357 1.05745525 -0.04834115 -0.60720431 1.25982115 -0.09176072 1.1591015 -0.12433461 -0.17463954 -1.64440065 -1.11886302 0.20264731 1.14596495 -1.80235956 -0.24717793 -0.06094535 0.84660574 0.37945439 0.84726224 0.18640942 -0.20709827 -0.43389027 -0.26161328 -0.04665061 0.2115123 0.00829721 0.10849443 0.16113917 -0.19330595]] \u001b[1my_train samples : \u001b[0m [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \u001b[1my_test samples : \u001b[0m [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  到目前为止，已经做好了构建分类模型所需的所有准备。\n模型建立 这里构建六种不同类型的分类模型，即决策树、K-最近邻 (KNN)、逻辑回归、支持向量机 (SVM)、随机森林和 XGBoost。虽然我们还可以使用更多其他的模型，但我们选用的是用于解决分类问题的最流行模型。所有这些模型构建均比较方便，都可以使用 scikit-learn 包提供的算法来构建。仅对于 XGBoost 模型，将使用 xgboost 包。接下来在 python 中实现这些模型，所使用的算法可能需要花费一定的时间来实现。\nfrom sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from xgboost import XGBClassifier # MODELING # 1. Decision Tree tree_model = DecisionTreeClassifier(max_depth = 4, criterion = \u0026#39;entropy\u0026#39;) tree_model.fit(X_train, y_train) tree_yhat = tree_model.predict(X_test) # 2. K-Nearest Neighbors n = 5 knn = KNeighborsClassifier(n_neighbors = n) knn.fit(X_train, y_train) knn_yhat = knn.predict(X_test) # 3. Logistic Regression lr = LogisticRegression() lr.fit(X_train, y_train) lr_yhat = lr.predict(X_test) # 4. SVM  svm = SVC() svm.fit(X_train, y_train) svm_yhat = svm.predict(X_test) # 5. Random Forest Tree rf = RandomForestClassifier(max_depth = 4) rf.fit(X_train, y_train) rf_yhat = rf.predict(X_test) # 6. XGBoost xgb = XGBClassifier(max_depth = 4) xgb.fit(X_train, y_train) xgb_yhat = xgb.predict(X_test) 至此我们构建了从决策树模型到 XGBoost 模型的六种不同类型的分类模型。\n在决策树模型中，使用 \u0026ldquo;DecisionTreeClassifier\u0026rdquo; 算法来构建模型。在算法中，设置 \u0026ldquo;max_depth=4\u0026rdquo;，意味着允许树最大分裂四次，\u0026ldquo;criterion = \u0026lsquo;entropy\u0026rdquo;，与**\u0026ldquo;max_depth\u0026rdquo;**最相似，但决定何时分裂停止分裂树。最后拟合模型后将预测值存储到 \u0026ldquo;tree_yhat\u0026rdquo; 变量中。\n在K-最近邻 (KNN)中，使用 \u0026ldquo;KNeighborsClassifier\u0026rdquo; 算法构建了模型，并设置 \u0026ldquo;n_neighbors=5\u0026rdquo;。 \u0026lsquo;n_neighbors\u0026rsquo; 的值是随机选择的，其实可以通过迭代一系列值来有目的地选择，然后拟合模型后将预测值存储到 \u0026ldquo;knn_yhat\u0026rdquo; 变量中。\n逻辑回归的代码没有什么可解释的，因为我使用 \u0026ldquo;LogisticRegression\u0026rdquo; 算法并全部使用默认值，并拟合模型后将预测值存储到 \u0026ldquo;lr_yhat\u0026rdquo; 变量中。\n使用\u0026quot;SVC\u0026quot;算法构建了支持向量机模型，并且同样使用默认值，并且默认内核就是我们所希望用到的模型，即\u0026quot;rbf\u0026quot;内核。之后，我们在拟合模型后将预测值存储到 \u0026ldquo;svm_yhat\u0026rdquo; 中。\n接下来使用 \u0026ldquo;RandomForestClassifier\u0026rdquo; 算法构建的随机森林模型，设置参数 \u0026ldquo;max_depth=4\u0026rdquo;，就像构建决策树模型的方式一样。最后在拟合模型后将预测值存储到 \u0026ldquo;rf_yhat\u0026rdquo; 中。请记住，决策树和随机森林之间的主要区别在于，决策树使用整个数据集来构建单个模型，而随机森林使用随机选择的特征来构建多个模型。这就是为什么很多情况下选择使用随机森林模型而不是决策树的原因。\n最后是 XGBoost 模型。使用 xgboost 包提供的 \u0026ldquo;XGBClassifier\u0026rdquo; 算法构建模型。设置 \u0026ldquo;max_depth=4\u0026rdquo;，最后在拟合模型后将预测值存储到 \u0026ldquo;xgb_yhat\u0026rdquo; 中。\n至此，我们成功构建了六种分类模型，为了便于理解，对代码进行了简单解释。接下来需要评估每个模型，并找到最适合我们案例的模型。\n模型评估 之前有提到过，我们将使用 scikit-learn 包提供的评估指标来评估我们构建的模型。在此过程中的主要目标是为给定案例找到最佳模型。这里将使用的评估指标是准确度评分指标、f1 评分指标，及混淆矩阵。\n准确率 准确率是最基本的评价指标之一，广泛用于评价分类模型。准确率分数的计算方法很简单，就是将模型做出的正确预测的数量除以模型做出的预测总数（可以乘以 100 将结果转换为百分比）。一般可以表示为：\n准确度分数 = 正确预测数 / 总预测数\n我们检查我们所构建的六种不同分类模型的准确率分数。要在 python 中完成，我们可以使用 scikit-learn 包提供的 \u0026ldquo;accuracy_score\u0026rdquo; 方法。\nfrom sklearn.metrics import accuracy_score # 1. Accuracy score print(cl(\u0026#39;ACCURACY SCORE\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;Accuracy score of the Decision Tree model is {}\u0026#39; .format(accuracy_score(y_test, tree_yhat)), attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;Accuracy score of the KNN model is {}\u0026#39; .format(accuracy_score(y_test, knn_yhat)), attrs = [\u0026#39;bold\u0026#39;], color = \u0026#39;green\u0026#39;)) print(cl(\u0026#39;Accuracy score of the Logistic Regression model is {}\u0026#39; .format(accuracy_score(y_test, lr_yhat)), attrs = [\u0026#39;bold\u0026#39;], color = \u0026#39;red\u0026#39;)) print(cl(\u0026#39;Accuracy score of the SVM model is {}\u0026#39; .format(accuracy_score(y_test, svm_yhat)), attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;Accuracy score of the Random Forest Tree model is {}\u0026#39; .format(accuracy_score(y_test, rf_yhat)), attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;Accuracy score of the XGBoost model is {}\u0026#39; .format(accuracy_score(y_test, xgb_yhat)), attrs = [\u0026#39;bold\u0026#39;])) Run\n\u001b[1mACCURACY SCORE\u001b[0m \u001b[1mAccuracy score of the Decision Tree model is 0.9993679997191109\u001b[0m \u001b[1m\u001b[32mAccuracy score of the KNN model is 0.9995259997893332\u001b[0m \u001b[1m\u001b[31mAccuracy score of the Logistic Regression model is 0.9991924440855307\u001b[0m \u001b[1mAccuracy score of the SVM model is 0.9993153330290369\u001b[0m \u001b[1mAccuracy score of the Random Forest Tree model is 0.9993153330290369\u001b[0m \u001b[1mAccuracy score of the XGBoost model is 0.9994908886626171\u001b[0m  根据准确性评分评估指标来看，KNN 模型为最准确的模型，而 Logistic 回归模型最不准确。然而，当我们对每个模型的结果进行四舍五入时，得到 99% 的准确性，这看是一个非常好的分数。\nF1-score F1-score 或 F-score 是用于评估分类模型的最流行的评估指标之一。它可以简单地定义为模型的准确率和召回率的调和平均值。它的计算方法是将 模型的精度和召回率的乘积除以模型的精度和召回率相加得到的值，最后乘以 2 得到的值。可以表示为：\nF1-score = 2( (精度 * 召回率) / (精度 + 召回率) )\n可以使用 scikit-learn 包提供的 \u0026ldquo;f1_score\u0026rdquo; 方法轻松计算 F1-score 。\nfrom sklearn.metrics import f1_score # 2. F1 score print(cl(\u0026#39;F1 SCORE\u0026#39;, attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;F1 score of the Decision Tree model is {}\u0026#39; .format(f1_score(y_test, tree_yhat)), attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;F1 score of the KNN model is {}\u0026#39; .format(f1_score(y_test, knn_yhat)), attrs = [\u0026#39;bold\u0026#39;], color = \u0026#39;green\u0026#39;)) print(cl(\u0026#39;F1 score of the Logistic Regression model is {}\u0026#39; .format(f1_score(y_test, lr_yhat)), attrs = [\u0026#39;bold\u0026#39;], color = \u0026#39;red\u0026#39;)) print(cl(\u0026#39;F1 score of the SVM model is {}\u0026#39; .format(f1_score(y_test, svm_yhat)), attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;F1 score of the Random Forest Tree model is {}\u0026#39; .format(f1_score(y_test, rf_yhat)), attrs = [\u0026#39;bold\u0026#39;])) print(cl(\u0026#39;F1 score of the XGBoost model is {}\u0026#39; .format(f1_score(y_test, xgb_yhat)), attrs = [\u0026#39;bold\u0026#39;])) Run\n\u001b[1mF1 SCORE\u001b[0m \u001b[1mF1 score of the Decision Tree model is 0.8105263157894738\u001b[0m \u001b[1m\u001b[32mF1 score of the KNN model is 0.8571428571428572\u001b[0m \u001b[1m\u001b[31mF1 score of the Logistic Regression model is 0.7356321839080459\u001b[0m \u001b[1mF1 score of the SVM model is 0.7771428571428572\u001b[0m \u001b[1mF1 score of the Random Forest Tree model is 0.7796610169491525\u001b[0m \u001b[1mF1 score of the XGBoost model is 0.8449197860962566\u001b[0m  模型的排名几乎与之前的评估指标相似。在 F1-score 评估指标的基础上，KNN 模型再次夺得第一，Logistic 回归模型仍然是最不准确的模型。\n混淆矩阵 通常，混淆矩阵是分类模型的可视化，显示模型与原始结果相比预测结果的程度。通常，预测结果存储在一个变量中，然后将其转换为相关表。使用相关表，以热图的形式绘制混淆矩阵。尽管有多种内置方法可以可视化混淆矩阵，但我们将从零开始定义和可视化它，以便更好地理解。\n# 3. Confusion Matrix # defining the plot function def plot_confusion_matrix(cm, classes, title, normalize = False, cmap = plt.cm.Blues): title = \u0026#39;Confusion Matrix of {}\u0026#39;.format(title) from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt import itertools import numpy as np if normalize: cm = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis] plt.imshow(cm, interpolation = \u0026#39;nearest\u0026#39;, cmap = cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation = 45) plt.yticks(tick_marks, classes) fmt = \u0026#39;.2f\u0026#39; if normalize else \u0026#39;d\u0026#39; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment = \u0026#39;center\u0026#39;, color = \u0026#39;white\u0026#39; if cm[i, j] \u0026gt; thresh else \u0026#39;black\u0026#39;) plt.tight_layout() plt.ylabel(\u0026#39;True label\u0026#39;) plt.xlabel(\u0026#39;Predicted label\u0026#39;) # Compute confusion matrix for the models tree_matrix = confusion_matrix(y_test, tree_yhat, labels = [0, 1]) # Decision Tree knn_matrix = confusion_matrix(y_test, knn_yhat, labels = [0, 1]) # K-Nearest Neighbors lr_matrix = confusion_matrix(y_test, lr_yhat, labels = [0, 1]) # Logistic Regression svm_matrix = confusion_matrix(y_test, svm_yhat, labels = [0, 1]) # Support Vector Machine rf_matrix = confusion_matrix(y_test, rf_yhat, labels = [0, 1]) # Random Forest Tree xgb_matrix = confusion_matrix(y_test, xgb_yhat, labels = [0, 1]) # XGBoost # Plot the confusion matrix plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (6, 6) Decision tree tree_cm_plot = plot_confusion_matrix(tree_matrix, classes = [\u0026#39;Non-Default(0)\u0026#39;,\u0026#39;Default(1)\u0026#39;], normalize = False, title = \u0026#39;Decision Tree\u0026#39;) plt.savefig(\u0026#39;tree_cm_plot.png\u0026#39;) plt.show() ​ ​\nK-Nearest Neighbors knn_cm_plot = plot_confusion_matrix(knn_matrix, classes = [\u0026#39;Non-Default(0)\u0026#39;,\u0026#39;Default(1)\u0026#39;], normalize = False, title = \u0026#39;KNN\u0026#39;) plt.savefig(\u0026#39;knn_cm_plot.png\u0026#39;) plt.show() ​ ​\nLogistic regression lr_cm_plot = plot_confusion_matrix(lr_matrix, classes = [\u0026#39;Non-Default(0)\u0026#39;,\u0026#39;Default(1)\u0026#39;], normalize = False, title = \u0026#39;Logistic Regression\u0026#39;) plt.savefig(\u0026#39;lr_cm_plot.png\u0026#39;) plt.show() ​ ​\nSupport Vector Machine svm_cm_plot = plot_confusion_matrix(svm_matrix, classes = [\u0026#39;Non-Default(0)\u0026#39;,\u0026#39;Default(1)\u0026#39;], normalize = False, title = \u0026#39;SVM\u0026#39;) plt.savefig(\u0026#39;svm_cm_plot.png\u0026#39;) plt.show() ​ ​\nRandom Forest rf_cm_plot = plot_confusion_matrix(rf_matrix, classes = [\u0026#39;Non-Default(0)\u0026#39;,\u0026#39;Default(1)\u0026#39;], normalize = False, title = \u0026#39;Random Forest Tree\u0026#39;) plt.savefig(\u0026#39;rf_cm_plot.png\u0026#39;) plt.show() ​ ​\nXGBoost xgb_cm_plot = plot_confusion_matrix(xgb_matrix, classes = [\u0026#39;Non-Default(0)\u0026#39;,\u0026#39;Default(1)\u0026#39;], normalize = False, title = \u0026#39;XGBoost\u0026#39;) plt.savefig(\u0026#39;xgb_cm_plot.png\u0026#39;) plt.show() ​ ​\n混淆矩阵理解： 以XGBoost模型的混淆矩阵为例。\n  第一行。 第一行是测试集中实际欺诈值为0的交易。可以计算，其中56861笔欺诈值为0。在这56861笔非欺诈交易中，分类器正确预测了其中的56854笔为 0 和 预测了其中 7 为 1。这意味着，对于 56854 笔非欺诈交易，测试集中的实际流失值为 0，分类器也正确预测为 0。可以说我们的模型已经对非欺诈交易进行了分类交易还不错。\n  第二行。 看起来有 101 笔交易的欺诈值为 1。分类器正确预测其中 79 笔为 1，错误预测值为 0 的 22 笔。错误预测值可以视为模型的错误。\n  在比较所有模型的混淆矩阵时可以看出，K-Nearest Neighbors 模型在从非欺诈交易中分类欺诈交易方面做得非常好，其次是 XGBoost 模型。所以可以得出结论，最适合本次案例的模型是 K-Nearest Neighbors 模型，可以忽略的模型是 Logistic 回归模型。\n写在最后 经过一连串的过程，我们已经成功构建了从决策树模型到XGBoost模型的六种不同类型的分类模型。随后使用评估指标评估了每个模型，并选择了最适合给定案例的模型。\n在本文中，我们只选用了6个相对流行的模型，其实还有更多模型需要探索。此外，虽然我们很轻松地在 python 中可行地构建了模型，但是每个模型背后都有很多的数学和统计数据，在有精力的情况下，可以去了解下这么模型背后的数学推理。\n参考资料 [1] Kaggle 信用卡欺诈检测数据集: https://www.kaggle.com/mlg-ulb/creditcardfraud\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/ml_credit_card_fraud_detection/","summary":"作者: 小猴子\n公众号: 机器学习研习院\n 本文旨在使用 XGBoost、随机森林、KNN、逻辑回归、SVM 和决策树解决分类问题\n案例简介 假设你受雇于帮助一家信用卡公司检测潜在的欺诈案件，你的工作是确保客户不会因未购买的商品而被收取费用。给你一个包含人与人之间交易的数据集，他们是欺诈与否的信息，并要求你区分它们。我们的最终目的是通过构建分类模型来对欺诈交易进行分类区分来解决上述情况。\n代码下载 点击下载\n对于这个案例，所需要用到的主要模块是处理数据的 Pandas、处理数组的 NumPy、用于数据拆分、构建和评估分类模型的 scikit-learn，最后是用于 xgboost 分类器模型算法的 xgboost 包。\n导入数据 关于数据： 我们将要使用的数据是 Kaggle 信用卡欺诈检测数据集。它包含特征 V1 到 V28，是 PCA 获得的主要成分，并忽略对构建模型没有用的时间特征。其余的特征是包含交易总金额的\u0026quot;金额\u0026quot;特征和包含交易是否为欺诈案件的\u0026quot;类别\u0026quot;特征。\n现在使用\u0026rsquo;pd.read_csv\u0026rsquo;方法导入数据，并查看部分数据样例。\nKaggle 信用卡欺诈检测数据集: https://www.kaggle.com/mlg-ulb/creditcardfraud\nimport pandas as pd df = pd.read_csv(\u0026#39;creditcard.csv\u0026#39;) df.drop(\u0026#39;Time\u0026#39;, axis = 1, inplace = True) df.head() Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }","title":"机器学习实战 | 信用卡欺诈检测"},{"content":"前几天刚刚分享 LIWC vs Python | 文本分析之词典词频法略讲(含代码)，借鉴LIWC，我觉得中文也需要有社科类的中文情感词典库，如果能汇聚已发表论文中的中文情感词典，如用户生成内容UGC那样，那么中文文本分析也会变的容易。下图是LIWC用户分享词典界面。\nLIWC用户分享词典 没有购买LIWC是看不到截图中的「USER-CREATED LIWC DICTIONARIES」。涉及版权，英文词典文件不作分享，一起尊重知识。\n中文领域有很多发表出来的各研究领域的情感词典，如果有词典推荐，欢迎thunderhit@qq.com联系我，我可以将词典整理为cntext内置格式。\n假设cntext内置词典丰富了，使用cntext做如下文本分析操作。\n案例: cntext操作 cntext内置词典 import cntext as ct #cntext版本 print(\u0026#39;cntext版本: {}\u0026#39;.format(ct.__version__)) #查看cntext内置词典 ct.dict_pkl_list() Run\n\u0026#39;cntext版本: 1.7.1\u0026#39; [\u0026#39;DUTIR.pkl\u0026#39;, \u0026#39;HOWNET.pkl\u0026#39;, \u0026#39;sentiws.pkl\u0026#39;, \u0026#39;ChineseFinancialFormalUnformalSentiment.pkl\u0026#39;, \u0026#39;ANEW.pkl\u0026#39;, \u0026#39;LSD2015.pkl\u0026#39;, \u0026#39;NRC.pkl\u0026#39;, \u0026#39;geninqposneg.pkl\u0026#39;, \u0026#39;HuLiu.pkl\u0026#39;, \u0026#39;AFINN.pkl\u0026#39;, \u0026#39;ADV_CONJ.pkl\u0026#39;, \u0026#39;LoughranMcDonald.pkl\u0026#39;, \u0026#39;STOPWORDS.pkl\u0026#39;, \u0026#39;concreteness.pkl\u0026#39;] \n导入内置pkl词典 cntext内词典正在规范化，理想的规范词典应该含有词语列表、Desc简介和Referer参考文献三部分。例如，大连理工大学情感本体库词典DUTIR.pkl\ndutir = ct.load_pkl_dict(\u0026#39;DUTIR.pkl\u0026#39;) dutir Run\n{\u0026#39;DUTIR\u0026#39;: {\u0026#39;哀\u0026#39;: [\u0026#39;怀想\u0026#39;, \u0026#39;治丝而棼\u0026#39;, \u0026#39;伤害\u0026#39;,...], \u0026#39;好\u0026#39;: [\u0026#39;进贤黜奸\u0026#39;, \u0026#39;清醇\u0026#39;, \u0026#39;放达\u0026#39;, ...], \u0026#39;惊\u0026#39;: [\u0026#39;惊奇不已\u0026#39;, \u0026#39;魂惊魄惕\u0026#39;, \u0026#39;海外奇谈\u0026#39;,...], \u0026#39;惧\u0026#39;: [\u0026#39;忸忸怩怩\u0026#39;, \u0026#39;谈虎色变\u0026#39;, \u0026#39;手忙脚乱\u0026#39;,...], \u0026#39;乐\u0026#39;: [\u0026#39;百龄眉寿\u0026#39;, \u0026#39;娱心\u0026#39;, \u0026#39;如意\u0026#39;,...], \u0026#39;怒\u0026#39;: [\u0026#39;饮恨吞声\u0026#39;, \u0026#39;扬眉瞬目\u0026#39;,...], \u0026#39;恶\u0026#39;: [出逃\u0026#39;, \u0026#39;鱼肉百姓\u0026#39;, \u0026#39;移天易日\u0026#39;,...]}, \u0026#39;Desc\u0026#39;: \u0026#39;大连理工大学情感本体库，细粒度情感词典。含七大类情绪，依次是哀, 好, 惊, 惧, 乐, 怒, 恶\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.\u0026#39;} dutir返回了\n 词典数据 Desc词典介绍 Referer词典文献出处  用cntext做情感计算 情感分析，统计文本中某类词出现个数，使用cntext.sentiment函数即可实现。\nsentiment(text, diction, lang=\u0026lsquo;chinese\u0026rsquo;)\n text: 文本字符串 diction: 情感词典 lang: 语言类型，\u0026ldquo;chinese\u0026rdquo; or \u0026ldquo;english\u0026rdquo;; 默认lang=\u0026ldquo;chinese\u0026rdquo;  import cntext as ct #自定义词典 diy_dict = {\u0026#39;pos\u0026#39;: [\u0026#39;高兴\u0026#39;, \u0026#39;快乐\u0026#39;, \u0026#39;分享\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;难过\u0026#39;, \u0026#39;悲伤\u0026#39;], \u0026#39;adv\u0026#39;: [\u0026#39;很\u0026#39;, \u0026#39;特别\u0026#39;]} #cntext内置词典-DUTIR dutir = ct.load_pkl_dict(\u0026#39;DUTIR.pkl\u0026#39;)[\u0026#39;DUTIR\u0026#39;] text = \u0026#39;我今天得奖了，很高兴，我要将快乐分享大家。\u0026#39; #使用diy_dict做情感分析 print(ct.sentiment(text=text, diction=diy_dict, lang=\u0026#39;chinese\u0026#39;) #使用DUTIR做情感分析  print(ct.sentiment(text=text, diction=dutir, lang=\u0026#39;chinese\u0026#39;)) Run\n{\u0026#39;pos_num\u0026#39;: 3, \u0026#39;neg_num\u0026#39;: 0, \u0026#39;adv_num\u0026#39;: 1, \u0026#39;stopword_num\u0026#39;: 8, \u0026#39;word_num\u0026#39;: 14, \u0026#39;sentence_num\u0026#39;: 1} {\u0026#39;哀_num\u0026#39;: 0, \u0026#39;好_num\u0026#39;: 0, \u0026#39;惊_num\u0026#39;: 0, \u0026#39;惧_num\u0026#39;: 0, \u0026#39;乐_num\u0026#39;: 2, \u0026#39;怒_num\u0026#39;: 0, \u0026#39;恶_num\u0026#39;: 0, \u0026#39;stopword_num\u0026#39;: 8, \u0026#39;word_num\u0026#39;: 14, \u0026#39;sentence_num\u0026#39;: 1} \nLIWC用户分享词典 以下内容整理自LIWC网站，我添加了doi及中文翻译。由于没有阅读每个词典对应的文献，词典简介翻译可能会有差错。\n以下词典仅仅是介绍，有疑惑的可以点击doi，找到对应论文进行理解。\n由于版权问题，词典文件资源不作分享。\n   Dictionary Desc Author Date DOI     Absolutist Measure absolutist thinking in texts (eg, always, never)衡量文本中的绝对主义思维（例如，always、never） Al-Mosaiwi \u0026amp; Johnstone 2018 https://doi.org/10.1177/2167702617747074   Age_Stereotypes Reflects eight broadly-defined stereotypes identified in past research as descriptive of older adults,such as impaired, despondent, shrew, recluse, vulnerable, golden, grandparent, conservative\n反映过去研究中确定的八种广泛定义的刻板印象(用于描述老年人)，例如“受损、沮丧、泼妇、隐士、脆弱、黄金、祖父母、保守” Jessica Remedios 2010 https://doi.org/10.1080/15298860903054175   Agitation\u0026amp;Dejection Based on studies linking promotion versus prevention focus with the emotions “Agitation” and “Dejection”\n基于将促进与预防重点与情绪“激动”和“沮丧”联系起来的研究 Johnsen et al. 2014 https://doi.org/10.2147/PRBM.S54947   Behavioral_Activation Captures linguistic indicators of planning and participation in enjoyable activities\n捕捉规划和参与愉快活动的语言指标 Burkhardt et al. 2021 https://doi.org/10.2196/28244   Big_Two Measure the degree to which a person is thinking in terms of Agency/Communion.\n衡量一个人在机构/交流方面的思考程度。 Pietraszkiewicz et al. 2019 https://doi.org/10.1002/ejsp.2561   Brand_Personality Assesses Aaker’s five brand personality dimensions as well as 42 personality trait norms\n评估 Aaker 的五个品牌个性维度以及 42 个个性特征规范 Opoku et al. 2008 https://doi.org/10.1080/08841240802100386   Controversial_Terms A lexicon of terms that range in their degree of controversiality, particularly in terms of their use in the media.\n具有争议程度的术语词典，特别是在媒体中的使用方面。 Mejova et al. 2014 http://arxiv.org/abs/1409.8152   Corporate_Social_Responsibility Reveals four dimensions of corporate social responsibility\n揭示企业社会责任的四个维度 Nadra Pencle \u0026amp; Irina Mălăescu 2016 https://doi.org/10.2308/jeta-51615   Cost_Benefit Measures language related to perceived costs and benefits that result from a decision or behavior.\n衡量与决策或行为导致的感知成本和收益相关的语言。 Michael McCullough 2006 https://doi.org/10.1037/0022-006X.74.5.887   Creativity\u0026amp;Innovation Language describing creation and/or innovation\n描述创造和/或创新的语言 Neufeld and Gaucher 2017    Crovitz_Innovator_Identification Identify “innovators” and “non-innovators” using Hebert F. Crovitz’s 42 relational words\n使用 Hebert F. Crovitz 的 42 个相关词识别“创新者”和“非创新者” Greco et al. 2021 https://doi.org/10.1007/s11135-020-01038-x   extended_Moral_Foundations_Dictionary(eMFD) The eMFD, unlike previous methods, is constructed from text annotations generated by a large sample of human coders.\n与以前的方法不同，eMFD 是由大量人类编码人员生成的文本注释构成的。 Hopp et al. 2021 https://doi.org/10.3758/s13428-020-01433-0   Foresight Measures the degree to which anticipation/foresight occurs. That is, words pointing to indicate where things are heading (often on the basis of recurrent behaviors).\n衡量预期/预见发生的程度。 也就是说，指向事物前进方向的词语（通常基于反复出现的行为）。 Robert Hogenraad 2020 https://doi.org/10.1007/s11135-020-01071-w   Imagination Digital lexicon of 627 entries relative to imagination and transfiguration, i.e., words pointing to the unbelievable and whatever is beyond the real.\n与想象和变形相关的 627 个条目的数字词典，即指向令人难以置信的事物和超越真实事物的词语。 Robert Hogenraad 2019 https://doi.org/10.1007/s11135-018-0813-7   Global_Citizen A dictionary to assess language usage related to global citizenship\n用于评估与全球公民相关的语言使用情况的词典 Stephen Reysen et al. 2014 https://doi.org/10.4018/ijcbpl.2014100101   Grant_Evaluation Captures categories relevant to scientific grant review (ability, achievement, agentic, research, standout, pos eval, neg eval)\n捕获与科学资助审查相关的类别（能力、成就、代理、研究、杰出、正面、负面） Kaatz et al. 2015 https://doi.org/10.1097/ACM.0000000000000442   Home_Perceptions Calculates the frequency of words describing clutter, a sense of the home as unfinished, restful words, and nature words\n计算描述杂乱、未完成的家感、宁静的词和自然词的频率 Saxbe \u0026amp; Repetti 2022-01-01 https://doi.org/10.1177/0146167209352864   Invective Dictionary Use this dictionary to detect invective language in narrative A. T. Panter 2022-01-01    Linguistic_Category_Model A computerized LCM analysis method使用这本词典检测叙事中的谩骂语言 Yi-Tai Seih 2017 https://doi.org/10.1177/0261927X16657855   Loughran_McDonald_Financial_Sentiment Dictionary for measuring positive and negative sentiment specifically in financial texts.This is the 2018 version of the dictionary.专门用于衡量金融文本中正面和负面情绪的字典。这是 2018 年版的字典。 Loughran \u0026amp; McDonald 2011 https://doi.org/10.1111/j.1540-6261.2010.01625.x   Masculine_and_Feminine List of masculine and feminine words from Gaucher et al. (2011)Gaucher 等人的男性化和女性化词列表。 (2011) Maureen McCusker 2011 https://doi.org/10.1037/a0022530   Mindfulness Two categories of mindfulness language describing the mindfulness state and the more encompassing “mindfulness journey”描述正念状态的两类正念语言和更全面的“正念之旅” Collins et al. 2009 https://doi.org/10.1037/a0017579   Mind_Perception Measures linguistic use of mind perception (words related to “agency” and “experience”) in naturalistic settings在自然主义环境中测量心理感知（与“agency”和“experience”相关的词）的语言使用 Schweitzer \u0026amp; Waytz 2020 https://doi.org/10.1037/xge0001013   Moral_Foundations_v2.0 An updated version of the Moral Foundations Dictionary that is recommended over the original by its creators.道德词典的更新版本，由其创建者推荐。 Jeremy Frimer 2019 https://doi.org/10.1016/j.jrp.2019.103906   Moral_Justification Measures variation in justification content (deontological, consequentialist, or emotive) as a function of moral foundations衡量辩护内容（道义论、后果论或情感论）随道德基础的变化 Wheeler \u0026amp; Laham 2016 https://doi.org/10.1177/0146167216653374   Personal_Values_Dictionary Measures the 10 Schwartz Values (and 4 higher-order value dimensions).测量 10 个 Schwartz 值（和 4 个高阶值维度）。 Ponizovskiy et al. 2020 https://doi.org/10.1002/per.2294   Prosocial_Words Calculates the density of prosocial words in anything that a person says计算一个人所说的任何内容中亲社会词的密度 Jeremy Frimer 2022-01-01 https://doi.org/10.1073/pnas.1500355112   Regulatory_Mode Locomotion and Assessment States of Goal Pursuit目标追求的运动和评估状态 Dana Kanze, Mark A. Conley, and E. Tory Higgins 2019 https://doi.org/10.1016/j.obhdp.2019.04.002   Security_Language Provides a reference for the comparative study of security-related linguistic repertoires in political texts (speeches, policy documents, etc.).为政治文本（演讲、政策文件等）中与安全相关的语言库的比较研究提供参考。 Stephane Baele \u0026amp; Olivier Sterck 2014 https://doi.org/10.1111/1467-9248.12147   Self-Care Measures the degree to which self-care words are used (e.g., diet, yoga)衡量自我保健词的使用程度（例如，饮食、瑜伽） Xunyi Wang et al. 2018 https://doi.org/10.1093/jamia/ocy012   Stereotype_Content A stereotype content dictionary, made using a semi-automated method, to capture the Stereotype Content Model in text使用半自动化方法制作的刻板印象内容字典，用于捕获文本中的刻板印象内容模型 Nicolas et al. 2022-01-01 https://doi.org/10.1002/ejsp.2724   Stress A dictionary used to measure psychological stress. Created based on the LIWC2007 English Dictionary.用来测量心理压力的字典。 根据 LIWC2007 英语词典创建。 Wei Wang et al. 2022-01-01 https://doi.org/10.1111/apps.12065   Well_Being Words that might indicate the presence of purpose or meaning可能表明存在目的或意义的词 Ratner et al. 2019 https://doi.org/10.1080/10888691.2019.1659140    分享词典 中文领域有很多发表出来的各研究领域的情感词典，如果有词典推荐，欢迎thunderhit@qq.com联系我，我会将词典整理为cntext内置格式。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/share_your_dict_to_cntext/","summary":"前几天刚刚分享 LIWC vs Python | 文本分析之词典词频法略讲(含代码)，借鉴LIWC，我觉得中文也需要有社科类的中文情感词典库，如果能汇聚已发表论文中的中文情感词典，如用户生成内容UGC那样，那么中文文本分析也会变的容易。下图是LIWC用户分享词典界面。\nLIWC用户分享词典 没有购买LIWC是看不到截图中的「USER-CREATED LIWC DICTIONARIES」。涉及版权，英文词典文件不作分享，一起尊重知识。\n中文领域有很多发表出来的各研究领域的情感词典，如果有词典推荐，欢迎thunderhit@qq.com联系我，我可以将词典整理为cntext内置格式。\n假设cntext内置词典丰富了，使用cntext做如下文本分析操作。\n案例: cntext操作 cntext内置词典 import cntext as ct #cntext版本 print(\u0026#39;cntext版本: {}\u0026#39;.format(ct.__version__)) #查看cntext内置词典 ct.dict_pkl_list() Run\n\u0026#39;cntext版本: 1.7.1\u0026#39; [\u0026#39;DUTIR.pkl\u0026#39;, \u0026#39;HOWNET.pkl\u0026#39;, \u0026#39;sentiws.pkl\u0026#39;, \u0026#39;ChineseFinancialFormalUnformalSentiment.pkl\u0026#39;, \u0026#39;ANEW.pkl\u0026#39;, \u0026#39;LSD2015.pkl\u0026#39;, \u0026#39;NRC.pkl\u0026#39;, \u0026#39;geninqposneg.pkl\u0026#39;, \u0026#39;HuLiu.pkl\u0026#39;, \u0026#39;AFINN.pkl\u0026#39;, \u0026#39;ADV_CONJ.pkl\u0026#39;, \u0026#39;LoughranMcDonald.pkl\u0026#39;, \u0026#39;STOPWORDS.pkl\u0026#39;, \u0026#39;concreteness.pkl\u0026#39;] \n导入内置pkl词典 cntext内词典正在规范化，理想的规范词典应该含有词语列表、Desc简介和Referer参考文献三部分。例如，大连理工大学情感本体库词典DUTIR.pkl\ndutir = ct.load_pkl_dict(\u0026#39;DUTIR.pkl\u0026#39;) dutir Run\n{\u0026#39;DUTIR\u0026#39;: {\u0026#39;哀\u0026#39;: [\u0026#39;怀想\u0026#39;, \u0026#39;治丝而棼\u0026#39;, \u0026#39;伤害\u0026#39;,...], \u0026#39;好\u0026#39;: [\u0026#39;进贤黜奸\u0026#39;, \u0026#39;清醇\u0026#39;, \u0026#39;放达\u0026#39;, ...], \u0026#39;惊\u0026#39;: [\u0026#39;惊奇不已\u0026#39;, \u0026#39;魂惊魄惕\u0026#39;, \u0026#39;海外奇谈\u0026#39;,...], \u0026#39;惧\u0026#39;: [\u0026#39;忸忸怩怩\u0026#39;, \u0026#39;谈虎色变\u0026#39;, \u0026#39;手忙脚乱\u0026#39;,...], \u0026#39;乐\u0026#39;: [\u0026#39;百龄眉寿\u0026#39;, \u0026#39;娱心\u0026#39;, \u0026#39;如意\u0026#39;,.","title":"欢迎各位向cntext库分享情感词典"},{"content":"客群细分对于企业了解目标受众非常重要。根据受众群体的不同，我们可以给采取不同的营销策略。目前有许多无监督的机器学习算法可以帮助公司识别他们的用户群并创建消费群体。\n在本文中，我将分享一种目前比较流行的 K-Means 聚类的无监督学习技术。K-Means的目标是将所有可用的数据分组为彼此不同的不重叠的子组。K-Means聚类是数据科学家用来帮助公司进行客户细分的常用技术。\n在本文中，你将了解以下内容：\n K-Means聚类的数据预处理 从头构建K-Means聚类算法 用于评估聚类模型性能的指标 可视化构建簇类 簇类构建的解读与分析  代码下载 点击下载\n预备知识 在开始之前安装以下库：pandas、numpy、matplotlib、seaborn、sciket learn、kneed。完成后，我们就可以开始制作模型了！\n本文中要的数据集可以文末下载，运行以下代码行以导入必要的库并读取数据集：\nimport pandas as pd df = pd.read_csv(\u0026#39;Mall_Customers.csv\u0026#39;) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  CustomerID Gender Age Annual Income (k$) Spending Score (1-100)     0 1 Male 19 15 39   1 2 Male 21 15 81   2 3 Female 20 16 6   3 4 Female 23 16 77   4 5 Female 31 17 40     数据集中有五个变量。CustomerID是数据集中每个客户的唯一标识符，我们可以删除这个变量。它没有为我们提供任何有用的集群信息。由于 gender 是一个分类变量，它需要编码并转换成数字。\n在输入模型之前，其他所有变量都将按正态分布进行缩放。我们将标准化这些变量，平均值为0，标准偏差为1。\n标准化变量 首先，让我们标准化数据集中的所有变量，使它们在相同的范围内。\nfrom sklearn.preprocessing import StandardScaler col_names = [\u0026#39;Annual Income (k$)\u0026#39;, \u0026#39;Age\u0026#39;, \u0026#39;Spending Score (1-100)\u0026#39;] features = df[col_names] scaler = StandardScaler().fit(features.values) features = scaler.transform(features.values) scaled_features = pd.DataFrame(features, columns = col_names) scaled_features.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Annual Income (k$) Age Spending Score (1-100)     0 -1.738999 -1.424569 -0.434801   1 -1.738999 -1.281035 1.195704   2 -1.700830 -1.352802 -1.715913   3 -1.700830 -1.137502 1.040418   4 -1.662660 -0.563369 -0.395980     我们可以看到所有的变量都被转换了，现在都以零为中心。\n热编码 变量\u0026quot;gender\u0026quot;是分类变量，我们需要把它转换成一个数值变量，可以用pd.get_dummies()来处理。\ngender = df[\u0026#39;Gender\u0026#39;] newdf = scaled_features.join(gender) newdf = pd.get_dummies(newdf, prefix=None, prefix_sep=\u0026#39;_\u0026#39;, dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None) newdf = newdf.drop([\u0026#39;Gender_Male\u0026#39;], axis=1) newdf.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Annual Income (k$) Age Spending Score (1-100) Gender_Female     0 -1.738999 -1.424569 -0.434801 0   1 -1.738999 -1.281035 1.195704 0   2 -1.700830 -1.352802 -1.715913 1   3 -1.700830 -1.137502 1.040418 1   4 -1.662660 -0.563369 -0.395980 1     可以看到，性别变量已经发生了变化，从数据框中删除了“Gender_Male”。这是因为不需要再保留变量了。\n建立聚类模型 让我们构建一个 K-means 聚类模型，并将其拟合到数据集中的所有变量上，我们用肘部图可视化聚类模型的性能，它会告诉我们在构建模型时使用的「最佳聚类数」。\nfrom sklearn.cluster import KMeans import matplotlib.pyplot as plt SSE = [] for cluster in range(1,10): kmeans = KMeans(n_clusters = cluster, init=\u0026#39;k-means++\u0026#39;) kmeans.fit(newdf) SSE.append(kmeans.inertia_) # converting the results into a dataframe and plotting them frame = pd.DataFrame({\u0026#39;Cluster\u0026#39;:range(1,10), \u0026#39;SSE\u0026#39;:SSE}) plt.figure(figsize=(12,6)) plt.plot(frame[\u0026#39;Cluster\u0026#39;], frame[\u0026#39;SSE\u0026#39;], marker=\u0026#39;o\u0026#39;) plt.xlabel(\u0026#39;Number of clusters\u0026#39;) plt.ylabel(\u0026#39;Inertia\u0026#39;) Text(0, 0.5, 'Inertia')  ​ ​\n根据上面的「肘部图」，我们可以看到最佳聚类数为「4」\n轮廓系数 轮廓系数或轮廓分数是用于评估该算法创建的簇的质量的方法。轮廓分数在-1到+1之间。轮廓分数越高，模型越好。轮廓分数度量同一簇中所有数据点之间的距离。这个距离越小，轮廓分数就越好。\n让我们计算一下我们刚刚建立的模型的轮廓分数：\nfrom sklearn.metrics import silhouette_score # First, build a model with 4 clusters kmeans = KMeans(n_clusters = 4, init=\u0026#39;k-means++\u0026#39;) kmeans.fit(newdf) # Now, print the silhouette score of this model print(silhouette_score(newdf, kmeans.labels_, metric=\u0026#39;euclidean\u0026#39;)) 0.35027020434653977  轮廓线得分约为「0.35」。这是一个不错的模型，但我们可以做得更好，并尝试获得更高的簇群分离。\n在我们尝试这样做之前，让我们将刚刚构建的聚类可视化，以了解模型的运行情况：\nclusters = kmeans.fit_predict(newdf.iloc[:,1:]) newdf[\u0026#34;label\u0026#34;] = clusters fig = plt.figure(figsize=(21,10)) ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) ax.scatter(newdf[\u0026#39;Age\u0026#39;][newdf.label == 0], newdf[\u0026#34;Annual Income (k$)\u0026#34;][newdf.label == 0], df[\u0026#34;Spending Score (1-100)\u0026#34;][newdf.label == 0], c=\u0026#39;blue\u0026#39;, s=60) ax.scatter(newdf[\u0026#39;Age\u0026#39;][newdf.label == 1], newdf[\u0026#34;Annual Income (k$)\u0026#34;][newdf.label == 1], newdf[\u0026#34;Spending Score (1-100)\u0026#34;][newdf.label == 1], c=\u0026#39;red\u0026#39;, s=60) ax.scatter(newdf[\u0026#39;Age\u0026#39;][newdf.label == 2], newdf[\u0026#34;Annual Income (k$)\u0026#34;][newdf.label == 2], df[\u0026#34;Spending Score (1-100)\u0026#34;][newdf.label == 2], c=\u0026#39;green\u0026#39;, s=60) ax.scatter(newdf[\u0026#39;Age\u0026#39;][newdf.label == 3], newdf[\u0026#34;Annual Income (k$)\u0026#34;][newdf.label == 3], newdf[\u0026#34;Spending Score (1-100)\u0026#34;][newdf.label == 3], c=\u0026#39;orange\u0026#39;, s=60) ax.view_init(30, 185) plt.show() ​ ​\n从上图可以看出，簇类分离度不是很大。红点与蓝色混合，绿色与黄色重叠，这与轮廓分数一起向我们表明该模型表现不佳。现在，让我们创建一个比这个模型具有更好集群可分离性的新模型。\n建立聚类模型2 对于这个模型，让我们做一些特征选择。我们可以使用一种叫做主成分分析（PCA）的技术。\nPCA 是一种帮助我们降低数据集维数的技术。现在，让我们在数据集上运行PCA：\nfrom sklearn.decomposition import PCA pca = PCA(n_components=4) principalComponents = pca.fit_transform(newdf) features = range(pca.n_components_) plt.bar(features, pca.explained_variance_ratio_, color=\u0026#39;black\u0026#39;) plt.xlabel(\u0026#39;PCA features\u0026#39;) plt.ylabel(\u0026#39;variance %\u0026#39;) plt.xticks(features) PCA_components = pd.DataFrame(principalComponents) ​ ​\n这张图表显示了每个主成分分析的组成，以及它的方差。我们可以看到前两个主成分解释了大约70%的数据集方差。我们可以将这两个组件输入到模型中再次构建模型，并选择要使用的簇的数量\nks = range(1, 10) inertias = [] for k in ks: model = KMeans(n_clusters=k) model.fit(PCA_components.iloc[:,:2]) inertias.append(model.inertia_) plt.plot(ks, inertias, \u0026#39;-o\u0026#39;, color=\u0026#39;black\u0026#39;) plt.xlabel(\u0026#39;number of clusters, k\u0026#39;) plt.ylabel(\u0026#39;inertia\u0026#39;) plt.xticks(ks) plt.show() ​\n同样，看起来「最佳簇数是4」。我们可以用4个簇来计算此模型的轮廓分数：\nmodel = KMeans(n_clusters=4) model.fit(PCA_components.iloc[:,:2]) # silhouette score print(silhouette_score(PCA_components.iloc[:,:2], model.labels_, metric=\u0026#39;euclidean\u0026#39;)) 0.6025604455573874  这个模型的轮廓分数是「0.42」，这比我们之前创建的模型要好。我们可以像前面一样可视化此模型：\nmodel = KMeans(n_clusters=4) clusters = model.fit_predict(PCA_components.iloc[:,:2]) newdf[\u0026#34;label\u0026#34;] = clusters fig = plt.figure(figsize=(21,10)) ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) ax.scatter(newdf.Age[newdf.label == 0], newdf[\u0026#34;Annual Income (k$)\u0026#34;][newdf.label == 0], newdf[\u0026#34;Spending Score (1-100)\u0026#34;][newdf.label == 0], c=\u0026#39;blue\u0026#39;, s=60) ax.scatter(newdf.Age[newdf.label == 1], newdf[\u0026#34;Annual Income (k$)\u0026#34;][newdf.label == 1], newdf[\u0026#34;Spending Score (1-100)\u0026#34;][newdf.label == 1], c=\u0026#39;red\u0026#39;, s=60) ax.scatter(newdf.Age[newdf.label == 2], newdf[\u0026#34;Annual Income (k$)\u0026#34;][newdf.label == 2], newdf[\u0026#34;Spending Score (1-100)\u0026#34;][newdf.label == 2], c=\u0026#39;green\u0026#39;, s=60) ax.scatter(newdf.Age[newdf.label == 3], newdf[\u0026#34;Annual Income (k$)\u0026#34;][newdf.label == 3], newdf[\u0026#34;Spending Score (1-100)\u0026#34;][newdf.label == 3], c=\u0026#39;orange\u0026#39;, s=60) ax.view_init(30, 185) plt.show() ​ ​\n模型1与模型2 让我们比较一下这个模型和第一个模型的聚类可分性：\n第二个模型中的簇比第一个模型中的簇分离得好得多。此外，第二个模型的轮廓分数要高得多。基于这些原因，我们可以选择第二个模型进行分析。\n聚类分析 首先，让我们将簇类映射回数据集，并查看数据帧。\ndf = pd.read_csv(\u0026#39;Mall_Customers.csv\u0026#39;) df = df.drop([\u0026#39;CustomerID\u0026#39;],axis=1) # map back clusters to dataframe pred = model.predict(PCA_components.iloc[:,:2]) frame = pd.DataFrame(df) frame[\u0026#39;cluster\u0026#39;] = pred frame.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Gender Age Annual Income (k$) Spending Score (1-100) cluster     0 Male 19 15 39 3   1 Male 21 15 81 0   2 Female 20 16 6 3   3 Female 23 16 77 0   4 Female 31 17 40 3     数据帧中的每一行现在都分配给一个集群。要比较不同群集的属性，请查找每个群集上所有变量的平均值：\navg_df = df.groupby([\u0026#39;cluster\u0026#39;], as_index=False).mean() avg_df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  cluster Age Annual Income (k$) Spending Score (1-100)     0 0 25.521739 26.304348 78.565217   1 1 51.681818 62.125000 33.750000   2 2 32.904762 84.380952 80.500000   3 3 26.659574 53.106383 40.042553     如果我们将这些簇可视化，我们可以更容易地解释它们。运行以下代码以获得每个变量的不同可视化效果：\nimport seaborn as sns sns.barplot(x=\u0026#39;cluster\u0026#39;,y=\u0026#39;Age\u0026#39;,data=avg_df) \u0026lt;AxesSubplot:xlabel='cluster', ylabel='Age'\u0026gt;  ​ ​\nsns.barplot(x=\u0026#39;cluster\u0026#39;,y=\u0026#39;Spending Score (1-100)\u0026#39;,data=avg_df) \u0026lt;AxesSubplot:xlabel='cluster', ylabel='Spending Score (1-100)'\u0026gt;  ​ ​\nsns.barplot(x=\u0026#39;cluster\u0026#39;,y=\u0026#39;Annual Income (k$)\u0026#39;,data=avg_df) \u0026lt;AxesSubplot:xlabel='cluster', ylabel='Annual Income (k$)'\u0026gt;  ​ ​\ndf2 = pd.DataFrame(df.groupby([\u0026#39;cluster\u0026#39;,\u0026#39;Gender\u0026#39;])[\u0026#39;Gender\u0026#39;].count()) df2.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   Gender   cluster Gender      0 Female 14   Male 9   1 Female 47   Male 41   2 Female 23     各细分市场的主要特点\n簇类0:\n 年平均收入高，支出低。 平均年龄在40岁左右，性别以男性为主。  簇类1：\n 中低收入，平均消费能力。 平均年龄在50岁左右，性别以女性为主。  簇类2：\n 平均收入低，消费分数高。 平均年龄在25岁左右，性别以女性为主。  簇类3：\n 平均收入高，消费分数高。 平均年龄在30岁左右，性别以女性为主。  值得注意的是，计算年龄中位数将有助于更好地了解每个集群内的年龄分布。\n而且，女性在整个数据集中的代表性更高，这就是为什么大多数集群中女性的数量比男性多。我们可以找到每个性别相对于整个数据集中的数字的百分比，以便更好地了解性别分布。\n为每个簇类构建角色 作为一名数据科学家，能够用你的分析讲述一个故事是一项重要的技能，这将帮助你的客户或利益相关者更容易理解你的发现。下面是一个基于创建的簇类构建消费者角色的示例：\n簇类0\n这个角色由对金钱非常谨慎的中年人组成。尽管与所有其他群体中的个人相比，他们的平均收入最高，但花费最少。这可能是因为他们有经济责任——比如为孩子的高等教育存钱。\n建议：促销、优惠券和折扣代码将吸引这一领域的个人，因为他们倾向于少花钱。\n簇类1\n这部分人包括一个年龄较大的群体。他们挣的少，花的少，而且可能正在为退休储蓄。\n建议：针对这些人的营销可以向这一领域的人推广医疗保健相关产品。\n簇类2\n这一部分由较年轻的年龄组组成。这部分人最有可能是第一批求职者。与其他人相比，他们赚的钱最少。然而，这些人都是热情的年轻人，他们喜欢过上好的生活方式，而且往往超支消费。\n建议：由于这些年轻人花费很多，给他们提供旅游优惠券或酒店折扣可能是个好主意。为他们提供折扣的顶级服装和化妆品品牌也将很好地为这一部分。\n簇类3\n这部分人是由中年人组成的。这些人努力工作，积累了大量财富。他们也花大量的钱来过好的生活。\n建议：由于他们的消费能力和人口结构，这些人很可能会寻找房产购买或投资。\n结论 在本文中，我已经详细的建立了一个用于客户细分的 K-Means 聚类模型。我们还探讨了聚类分析，并分析了每个聚类中个体的行为。最后，我们看了一些可以根据集群中每个人的属性提供的业务建议。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/customer_segment_with_kmeans/","summary":"客群细分对于企业了解目标受众非常重要。根据受众群体的不同，我们可以给采取不同的营销策略。目前有许多无监督的机器学习算法可以帮助公司识别他们的用户群并创建消费群体。\n在本文中，我将分享一种目前比较流行的 K-Means 聚类的无监督学习技术。K-Means的目标是将所有可用的数据分组为彼此不同的不重叠的子组。K-Means聚类是数据科学家用来帮助公司进行客户细分的常用技术。\n在本文中，你将了解以下内容：\n K-Means聚类的数据预处理 从头构建K-Means聚类算法 用于评估聚类模型性能的指标 可视化构建簇类 簇类构建的解读与分析  代码下载 点击下载\n预备知识 在开始之前安装以下库：pandas、numpy、matplotlib、seaborn、sciket learn、kneed。完成后，我们就可以开始制作模型了！\n本文中要的数据集可以文末下载，运行以下代码行以导入必要的库并读取数据集：\nimport pandas as pd df = pd.read_csv(\u0026#39;Mall_Customers.csv\u0026#39;) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  CustomerID Gender Age Annual Income (k$) Spending Score (1-100)     0 1 Male 19 15 39   1 2 Male 21 15 81   2 3 Female 20 16 6   3 4 Female 23 16 77   4 5 Female 31 17 40     数据集中有五个变量。CustomerID是数据集中每个客户的唯一标识符，我们可以删除这个变量。它没有为我们提供任何有用的集群信息。由于 gender 是一个分类变量，它需要编码并转换成数字。","title":"实战 | 构建基于客户细分的 K-Means 聚类算法！"},{"content":"代码下载 点击下载本文代码\nTitle 中国企业高管团队创新注意力 对 如何影响企业创新活动？ 公司治理的调节作用研究\n Chen, Shouming, Miao Bu, Sibin Wu, and Xin Liang. \u0026ldquo;How does TMT attention to innovation of Chinese firms influence firm innovation activities? A study on the moderating role of corporate governance.\u0026rdquo; Journal of Business Research 68, no. 5 (2015): 1127-1135.\n 摘要 本文借鉴高层梯队理论，探讨了高管团队创新注意力对中国企业创新活动的影响。本文预测 高管团队创新注意力 对企业创新活动的影响受到公司治理特征的调节作用。进一步利用从2006年至2011年6年间394家中国制造企业收集的1747个公司年度观察数据，实证检验上述假设。\n研究结果表明:企业高管团队创新注意力与企业专利申请之间存在正相关关系，且当企业为民营企业、董事会规模较大或独立董事较少时，该正相关关系更强。\nTMTAI指标构建 高管团队创新注意力TMTAI（ TMT attention to innovation）：利用6个创新相关的关键词对该指标进行测量，可以使用词典词频法， 计算TMTAI词在年报中的词频。\ninnovations = [\u0026#39;知识产权\u0026#39;, \u0026#39;自主创新\u0026#39;, \u0026#39;专利保护\u0026#39;, \u0026#39;专利侵权\u0026#39;, \u0026#39;技术创新\u0026#39;, \u0026#39;核心技术\u0026#39;] \n待分析的数据 原论文使用2006-2011年中国上市企业年报， 这里我自己随便找了点年报数据。\nimport pandas as pd df = pd.read_csv(\u0026#39;reports.csv\u0026#39;) df.head() 写代码，一定秉承先简单，再复杂，先局部后整体。只要在具体的局部成功了，就可以推而广之。\n那么我们拿出一条文本，对一条文本做高管团队创新注意力tmtai词语的计算\nimport cntext as ct import jieba tmtai_words = [\u0026#39;知识产权\u0026#39;, \u0026#39;自主创新\u0026#39;, \u0026#39;专利保护\u0026#39;, \u0026#39;专利侵权\u0026#39;, \u0026#39;技术创新\u0026#39;, \u0026#39;核心技术\u0026#39;] tmtai_dict = {\u0026#39;tmtai\u0026#39;: tmtai_words} #加入自定义词典，放置文本被错分 for w in tmtai_words: jieba.add_word(w) # 我瞎编的 test_text = \u0026#39;我们公司尊重知识产权，但也要避免专利侵权，在下一阶段会加强自主创新，培育核心技术\u0026#39; pd.Series(ct.sentiment(text=test_text, diction=tmtai_dict, lang=\u0026#39;chinese\u0026#39;)) Run\ntmtai_num 4 stopword_num 9 word_num 19 sentence_num 1 dtype: int64 实验成功，接下来就可以推广到所有text这一列\nimport cntext as ct import jieba tmtai_words = [\u0026#39;知识产权\u0026#39;, \u0026#39;自主创新\u0026#39;, \u0026#39;专利保护\u0026#39;, \u0026#39;专利侵权\u0026#39;, \u0026#39;技术创新\u0026#39;, \u0026#39;核心技术\u0026#39;] tmtai_dict = {\u0026#39;tmtai\u0026#39;: tmtai_words} #加入自定义词典，放置文本被错分 for w in tmtai_words: jieba.add_word(w) def tmtai_count(text): return pd.Series(ct.sentiment(text=text, diction=tmtai_dict, lang=\u0026#39;chinese\u0026#39;)) #选中text这列，统计其中每条文本中tmtai词出现次数 tdf = df[\u0026#39;text\u0026#39;].apply(tmtai_count) tdf.head() #合并新旧两个dataframe result_df = pd.concat([df, tdf], axis=1) #tmtai指标是 词频，因此需要tmtai_num/word_num result_df[\u0026#39;tmtai_score\u0026#39;] = result_df[\u0026#39;tmtai_num\u0026#39;]/result_df[\u0026#39;word_num\u0026#39;] result_df.head() Run\n查看结果，最后一列出现了我们感兴趣的 tmtai_score 指标\nresult_df.head() Run\ntmtai_score平均分\n#500家公司tmtai指标平均值 result_df[\u0026#39;tmtai_score\u0026#39;].mean() Run\n0.0004088675846679111 \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/how_chinese_tmtai_impact_corporate_inovation/","summary":"代码下载 点击下载本文代码\nTitle 中国企业高管团队创新注意力 对 如何影响企业创新活动？ 公司治理的调节作用研究\n Chen, Shouming, Miao Bu, Sibin Wu, and Xin Liang. \u0026ldquo;How does TMT attention to innovation of Chinese firms influence firm innovation activities? A study on the moderating role of corporate governance.\u0026rdquo; Journal of Business Research 68, no. 5 (2015): 1127-1135.\n 摘要 本文借鉴高层梯队理论，探讨了高管团队创新注意力对中国企业创新活动的影响。本文预测 高管团队创新注意力 对企业创新活动的影响受到公司治理特征的调节作用。进一步利用从2006年至2011年6年间394家中国制造企业收集的1747个公司年度观察数据，实证检验上述假设。\n研究结果表明:企业高管团队创新注意力与企业专利申请之间存在正相关关系，且当企业为民营企业、董事会规模较大或独立董事较少时，该正相关关系更强。\nTMTAI指标构建 高管团队创新注意力TMTAI（ TMT attention to innovation）：利用6个创新相关的关键词对该指标进行测量，可以使用词典词频法， 计算TMTAI词在年报中的词频。\ninnovations = [\u0026#39;知识产权\u0026#39;, \u0026#39;自主创新\u0026#39;, \u0026#39;专利保护\u0026#39;, \u0026#39;专利侵权\u0026#39;, \u0026#39;技术创新\u0026#39;, \u0026#39;核心技术\u0026#39;]","title":"文本分析 | 中国企业高管团队创新注意力"},{"content":" 本文原理介绍翻译自 https://www.liwc.app/help/howitworks\n对比及Python代码主要是自创作\n LIWC是一种付费的文本分析软件，在学界知名度挺高的。今天翻译了LIWC: how it works https://www.liwc.app/help/howitworks ，通过LIWC来侧面加深对 词典情感分析 的理解。\n词频：可靠的指标 语言查询和词数统计 (LIWC「Linguistic Inquiry and Word Count」) 的核心逻辑来自数十年的科学研究表明，人们的语言可以提供极其丰富心理状态信息，包括情绪、思维方式和社会关注点。有时，这些见解是相当明显和直截了当的。例如，如果某人使用了很多像 「happy、excited、elated」 这样的词，他们可能会感到快乐，我们可以使用这些信息来可靠地估计他们当前的情绪状态。然而，言语行为和心理之间的关系往往不那么明显。例如，更自信、社会地位更高的人倾向于使用相对较高的 「you」 词，而使用 「me」词的频率相对较低。在这里，数十年的实证研究——尤其是使用 LIWC 作为科学工具的研究——也为我们提供了理解、解释和量化心理、社会和行为现象的专业方法。\n但作为算法，实际上主要的计算方法是词频。而这点，借助Python可以完成从数据清洗到数据分析全部过程。\n丰富的词典 LIWC-22 带有 100 多个内置字典，用于捕捉人们的社会和心理状态。每本词典都包含一系列单词、词干、表情符号和其他特定的语言结构，这些结构已被识别为反映感兴趣的心理类别。例如，「认知过程cognitive processes」词典包括 1,000 多个条目，这些条目反映了一个人何时通过一般和更具体的方式积极处理信息。 「从属关系affiliation」词典包括超过 350 个条目，这些条目反映了一个人与他人联系的需要，其中包括 「community」 和 「together」 等词。\nLIWC 读取给定文本并将文本中的每个单词与字典单词列表进行比较，并计算文本中与每个字典类别匹配的总单词的百分比。例如，如果 LIWC 使用内置的 LIWC-22 词典分析包含 1000 个单词的单个语音，它可能会发现其中 50 个单词与积极情绪有关，10 个单词与从属关系有关。 LIWC 会将这些数字转换为百分比：5.0% 的积极情绪和 1.0% 的从属关系。\n请注意，许多 LIWC-22 类别是按层次结构组织的。根据定义，所有愤怒的词都被归类为负面情绪词，而负面情绪词又被归类为情绪词。另请注意，同一个词可能会被分类在多个字典中。例如，「celebrate」一词在积极情绪和成就词典中都有。\n下图是liwc用户上传分享的自定义词典，目前有77个。好像需要购买liwc服务，才能下载里面的文件\n文本越长越好 不要忘记，LIWC 和所有文本分析工具一样，是一种相对粗糙的工具。它有时会在识别和计算单个单词时出错。考虑一下「mad」这个词——一个在愤怒词典中被计算在内的词。通常，今天，「mad」这个词确实反映了某种程度的愤怒。然而，有时它表达了喜悦（「he\u0026rsquo;s mad for her.」）或精神不稳定（「mad as hatter」）。幸运的是，这很少成为问题，因为 LIWC 利用了语言使用的概率模型。是的，在给定的句子中，「mad」这个词可能被用来表达积极的情绪。然而，如果作者实际上正在经历积极情绪，他们通常会倾向于使用一个以上的积极情绪词，并且很可能很少使用其他愤怒词，这应该会导致积极情绪得分高而愤怒得分低。要记住的重要一点是，您分析的单词越多，结果就越值得信赖。 10,000 字的文本比 100 字的文本产生的结果可靠得多。任何少于 25-50 个单词的文本都应该以一定的怀疑态度来看待。\n至此翻译结束\n简单对比：Python与LIWC    工具 简介 算法 优势 劣势     Python 编程语言 词频(典)法、词嵌入法 接近全能, 可以用Python搞定从数据采集、清洗、分析全流程\n可以把最新前沿应用到自己研究中 (nature、science、pnas相关文本分析方法的论文会大多会开源自己的Python代码)。 有一定的学习门槛\n   LIWC 软件 主要是词典法 学界认可\n内置丰富的词典, 拿来即用。 不够灵活， 对中文支持不友好，内置词典几乎全是西方语言。    考虑数据清洗 综合来看，如果只使用 词频(词典)法 统计某一构念相关词语在文中出现的占比， LIWC 较 Python和R等编程语言有微弱优势。这里需要说明一下，完整的文本(数据)分析包含采集、清洗、分析。其中清洗部分工作量是最大的，数据科学家有个形象的统计，认为清洗占整个数据分析工作量的70%左右。\nLIWC的上游环节往往需要借助Python和R等其他语言对原始数据做数据清洗和整理。\n如果数据分析的代码量一共有100行，那么清洗的代码可能有70行，数据分析的代码只需再写30行。为了数据清洗任务，你可能不得不学Python，之后可再用LIWC；也可以 LIWC\u0026amp;Python一起用。\n好消息 大家可能觉得 词频(词典)法 算法过于粗暴， 通过对LIWC工作原理了解，我们知道LIWC软件底层算法也是词频(词典)法。\n现在大家应该对 词频(词典)法 有了新的认识，更加有理论自信，技术自信。而Python对这种算法的运行其实很擅长的，\ncntext是我一直在开发更新的一个包，一直想将常见的文本分析代码工作量压缩至 个位行数。\n功能模块含\n  stats 文本统计指标\n 词频统计 可读性 内置pkl词典 情感分析    dictionary构建词表(典)\n Sopmi 互信息扩充词典法 W2Vmodels 词向量扩充词典法 Glove Glove词向量模型    similarity 文本相似度\n  cos相似度\n  jaccard相似度\n  编辑距离相似度\n  mind 计算文本中的认知方向（态度、偏见）\n  比如对一条测试数据test_text， 使用 词频(词典)法 做情感分析，代码量不到5行\nimport cntext as ct # 自定义情感词典 diction = {\u0026#39;pos\u0026#39;: [\u0026#39;高兴\u0026#39;, \u0026#39;快乐\u0026#39;, \u0026#39;分享\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;难过\u0026#39;, \u0026#39;悲伤\u0026#39;]} # 测试数据 test_text = \u0026#39;我今天得奖了，很高兴，我要将快乐分享大家。\u0026#39; # 情感计算 ct.sentiment(text=test_text, diction=diction, lang=\u0026#39;chinese\u0026#39;) Run\n{\u0026#39;pos_num\u0026#39;: 3, \u0026#39;neg_num\u0026#39;: 0, \u0026#39;stopword_num\u0026#39;: 8, \u0026#39;word_num\u0026#39;: 14, \u0026#39;sentence_num\u0026#39;: 1} \n即时对一个csv或excel文件，某一列文本做情感分析，代码量不超过10行。我们先看一下数据\nimport pandas as pd df = pd.read_csv(\u0026#39;test_sentiment_texts.csv\u0026#39;) df.head() Run\n对text列做情感分析，使用自定义情感词典\nimport pandas as pd import cntext as ct # 导入自定义情感词典 diction = {\u0026#39;pos\u0026#39;: [\u0026#39;高兴\u0026#39;, \u0026#39;快乐\u0026#39;, \u0026#39;分享\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;难过\u0026#39;, \u0026#39;悲伤\u0026#39;]} # 情感计算 def diy_senti(text): return pd.Series(ct.sentiment(text=text, diction=diction, lang=\u0026#39;chinese\u0026#39;)) #读取数据 df = pd.read_csv(\u0026#39;test_sentiment_texts.csv\u0026#39;) #选中text列，对该列进行情感计算，得到dataframe senti_df = df[\u0026#39;text\u0026#39;].apply(diy_senti) #将df和senti_df两个dataframe合并 result_df = pd.concat([df, senti_df], axis=1) #存储 \u0026amp; 显示结果 result_df.to_csv(\u0026#39;result_of_sentiment_texts.csv\u0026#39;) result_df.head() Run\n本文代码 点击下载\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/liwc_python_text_mining/","summary":"本文原理介绍翻译自 https://www.liwc.app/help/howitworks\n对比及Python代码主要是自创作\n LIWC是一种付费的文本分析软件，在学界知名度挺高的。今天翻译了LIWC: how it works https://www.liwc.app/help/howitworks ，通过LIWC来侧面加深对 词典情感分析 的理解。\n词频：可靠的指标 语言查询和词数统计 (LIWC「Linguistic Inquiry and Word Count」) 的核心逻辑来自数十年的科学研究表明，人们的语言可以提供极其丰富心理状态信息，包括情绪、思维方式和社会关注点。有时，这些见解是相当明显和直截了当的。例如，如果某人使用了很多像 「happy、excited、elated」 这样的词，他们可能会感到快乐，我们可以使用这些信息来可靠地估计他们当前的情绪状态。然而，言语行为和心理之间的关系往往不那么明显。例如，更自信、社会地位更高的人倾向于使用相对较高的 「you」 词，而使用 「me」词的频率相对较低。在这里，数十年的实证研究——尤其是使用 LIWC 作为科学工具的研究——也为我们提供了理解、解释和量化心理、社会和行为现象的专业方法。\n但作为算法，实际上主要的计算方法是词频。而这点，借助Python可以完成从数据清洗到数据分析全部过程。\n丰富的词典 LIWC-22 带有 100 多个内置字典，用于捕捉人们的社会和心理状态。每本词典都包含一系列单词、词干、表情符号和其他特定的语言结构，这些结构已被识别为反映感兴趣的心理类别。例如，「认知过程cognitive processes」词典包括 1,000 多个条目，这些条目反映了一个人何时通过一般和更具体的方式积极处理信息。 「从属关系affiliation」词典包括超过 350 个条目，这些条目反映了一个人与他人联系的需要，其中包括 「community」 和 「together」 等词。\nLIWC 读取给定文本并将文本中的每个单词与字典单词列表进行比较，并计算文本中与每个字典类别匹配的总单词的百分比。例如，如果 LIWC 使用内置的 LIWC-22 词典分析包含 1000 个单词的单个语音，它可能会发现其中 50 个单词与积极情绪有关，10 个单词与从属关系有关。 LIWC 会将这些数字转换为百分比：5.0% 的积极情绪和 1.0% 的从属关系。\n请注意，许多 LIWC-22 类别是按层次结构组织的。根据定义，所有愤怒的词都被归类为负面情绪词，而负面情绪词又被归类为情绪词。另请注意，同一个词可能会被分类在多个字典中。例如，「celebrate」一词在积极情绪和成就词典中都有。\n下图是liwc用户上传分享的自定义词典，目前有77个。好像需要购买liwc服务，才能下载里面的文件\n文本越长越好 不要忘记，LIWC 和所有文本分析工具一样，是一种相对粗糙的工具。它有时会在识别和计算单个单词时出错。考虑一下「mad」这个词——一个在愤怒词典中被计算在内的词。通常，今天，「mad」这个词确实反映了某种程度的愤怒。然而，有时它表达了喜悦（「he\u0026rsquo;s mad for her.」）或精神不稳定（「mad as hatter」）。幸运的是，这很少成为问题，因为 LIWC 利用了语言使用的概率模型。是的，在给定的句子中，「mad」这个词可能被用来表达积极的情绪。然而，如果作者实际上正在经历积极情绪，他们通常会倾向于使用一个以上的积极情绪词，并且很可能很少使用其他愤怒词，这应该会导致积极情绪得分高而愤怒得分低。要记住的重要一点是，您分析的单词越多，结果就越值得信赖。 10,000 字的文本比 100 字的文本产生的结果可靠得多。任何少于 25-50 个单词的文本都应该以一定的怀疑态度来看待。","title":"LIWC vs Python  | 文本分析之词典词频法略讲(含代码)"},{"content":"截止今日，「公众号: 大邓和他的Python」聚集了27000位Python爱好者。首先要感谢大家的信任和支持！\n引言 数据挖掘在社科、经管等领域中的应用已成为潮流和趋势。数据采集、数据分析、文本编码(清洗)、机器学习、深度学习等，借助Python一门语言可以全部搞定。互联网时代下，海量的、不规则的数据散落在各处，等待着大家去整理去探索。科学研究的进展离不开测量方法和工具的革新，在大数据时代，掌握Python会让我们的实证研究选题更广、更深、更新。\n随着Python技术社区发展，针对不规则数据的处理，如网页文本、报告pdf、图片、音频、视频， 技术的可行性和有用性越来越高，学习Python的价值也在越来越大。大邓一直在学习和分享Python，如果要学习高质量的内容，还得翻出去查看英文社区资料。\n一人行快，众人行远。聚集热爱编程(Python、R)、数据挖掘技术、社科(经管)科研的小伙伴，一起做技术分享。\n内容规划 未来公众号的选题内容规划\n 网络爬虫(数据采集) 文本、音频、视频、文件等数据处理 机器学习、自然语言处理 经管、社科领域，借助数据挖掘的研究和技术 Python相关技术分享 其他(待定)  招募小伙伴 小伙伴气质   有Python、R基础\n  对数据分析感兴趣\n  高校社科(经管)专业 在读硕博\n  工作内容 结合小伙伴兴趣、特长，划定工作内容：\n  技术：python或R相关资料收集、选题、代码整理。\n  运营：新媒体运营、社群、内容运营\n  福利   尊重原作者署名权，并将为每篇被采纳的原创首发稿件， 提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算。\n  如作者内容分享成体系，文稿质量高，公众号可组织付费直播课。\n   根据小伙伴的工作绩效，大邓会从公众号产生的收益中，发放一定的补贴。\n 参与流程   准备个人word简历：个人信息、科研经历、兴趣爱好、考/保研经历、技术分享等\n  投递个人简历至 thunderhit@qq.com\n  ","permalink":"/blog/we_need_you/","summary":"截止今日，「公众号: 大邓和他的Python」聚集了27000位Python爱好者。首先要感谢大家的信任和支持！\n引言 数据挖掘在社科、经管等领域中的应用已成为潮流和趋势。数据采集、数据分析、文本编码(清洗)、机器学习、深度学习等，借助Python一门语言可以全部搞定。互联网时代下，海量的、不规则的数据散落在各处，等待着大家去整理去探索。科学研究的进展离不开测量方法和工具的革新，在大数据时代，掌握Python会让我们的实证研究选题更广、更深、更新。\n随着Python技术社区发展，针对不规则数据的处理，如网页文本、报告pdf、图片、音频、视频， 技术的可行性和有用性越来越高，学习Python的价值也在越来越大。大邓一直在学习和分享Python，如果要学习高质量的内容，还得翻出去查看英文社区资料。\n一人行快，众人行远。聚集热爱编程(Python、R)、数据挖掘技术、社科(经管)科研的小伙伴，一起做技术分享。\n内容规划 未来公众号的选题内容规划\n 网络爬虫(数据采集) 文本、音频、视频、文件等数据处理 机器学习、自然语言处理 经管、社科领域，借助数据挖掘的研究和技术 Python相关技术分享 其他(待定)  招募小伙伴 小伙伴气质   有Python、R基础\n  对数据分析感兴趣\n  高校社科(经管)专业 在读硕博\n  工作内容 结合小伙伴兴趣、特长，划定工作内容：\n  技术：python或R相关资料收集、选题、代码整理。\n  运营：新媒体运营、社群、内容运营\n  福利   尊重原作者署名权，并将为每篇被采纳的原创首发稿件， 提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算。\n  如作者内容分享成体系，文稿质量高，公众号可组织付费直播课。\n   根据小伙伴的工作绩效，大邓会从公众号产生的收益中，发放一定的补贴。\n 参与流程   准备个人word简历：个人信息、科研经历、兴趣爱好、考/保研经历、技术分享等\n  投递个人简历至 thunderhit@qq.com\n  ","title":"招募小伙伴"},{"content":"svgartista https://svgartista.net/\nsvg动画制作网站\nrawgraphs https://app.rawgraphs.io/\n免费制作炫酷的可视化图表，在底层，该工具使用 D3.js(一个出色的 JavaScript 可视化库)，可数据进行可视化。\nRAWGraphs 团队提供了一系列关于如何使用该工具的视频教程。\njiffyreader https://www.jiffyreader.com/\n通过对单词局部位置加粗，让读者更好更快阅读英文。安装该插件后，理论上能让大脑处理速度加快10+倍。\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly18/","summary":"svgartista https://svgartista.net/\nsvg动画制作网站\nrawgraphs https://app.rawgraphs.io/\n免费制作炫酷的可视化图表，在底层，该工具使用 D3.js(一个出色的 JavaScript 可视化库)，可数据进行可视化。\nRAWGraphs 团队提供了一系列关于如何使用该工具的视频教程。\njiffyreader https://www.jiffyreader.com/\n通过对单词局部位置加粗，让读者更好更快阅读英文。安装该插件后，理论上能让大脑处理速度加快10+倍。\n了解课程  点击上方图片购买课程   点击进入详情页","title":"TechWeekly-18 每周有趣有用的技术分享"},{"content":"R和Python都是数据分析利器，最好能一起使用，发挥各自的优势。\n在Jupyter中，通常是默认支持Python的，如何配置才能让R也能运行呢？\n配置步骤 step 1 安装好R软件，打开命令行，输入R回车，调出R环境\nR 在R环境中运行\ninstall.packages(c(\u0026#39;repr\u0026#39;, \u0026#39;IRdisplay\u0026#39;, \u0026#39;evaluate\u0026#39;, \u0026#39;crayon\u0026#39;, \u0026#39;pbdZMQ\u0026#39;, \u0026#39;devtools\u0026#39;, \u0026#39;uuid\u0026#39;, \u0026#39;digest\u0026#39;)) \nstep 2 继续在在R环境中运行\ndevtools::install_github(\u0026#39;IRkernel/IRkernel\u0026#39;) \nstep 3 继续在R环境中运行\nIRkernel::installspec(user=FALSE) \n调用Jupyter 重新打开命令行，执行\njupyter notebook 弹出的界面中拥有了R的kernel\n新建一个R的jupyter文件，运行下方代码\nlibrary(\u0026#34;ggsci\u0026#34;) library(\u0026#34;ggplot2\u0026#34;) library(\u0026#34;gridExtra\u0026#34;) data(\u0026#34;diamonds\u0026#34;) p1 \u0026lt;- ggplot( subset(diamonds, carat \u0026gt;= 2.2), aes(x = table, y = price, colour = cut) ) + geom_point(alpha = 0.7) + geom_smooth(method = \u0026#34;loess\u0026#34;, alpha = 0.05, size = 1, span = 1) + theme_bw() p2 \u0026lt;- ggplot( subset(diamonds, carat \u0026gt; 2.2 \u0026amp; depth \u0026gt; 55 \u0026amp; depth \u0026lt; 70), aes(x = depth, fill = cut) ) + geom_histogram(colour = \u0026#34;black\u0026#34;, binwidth = 1, position = \u0026#34;dodge\u0026#34;) + theme_bw() p1_npg \u0026lt;- p1 + scale_color_npg() p2_npg \u0026lt;- p2 + scale_fill_npg() grid.arrange(p1_npg, p2_npg, ncol = 2) Run\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/run_r_in_jupyter/","summary":"R和Python都是数据分析利器，最好能一起使用，发挥各自的优势。\n在Jupyter中，通常是默认支持Python的，如何配置才能让R也能运行呢？\n配置步骤 step 1 安装好R软件，打开命令行，输入R回车，调出R环境\nR 在R环境中运行\ninstall.packages(c(\u0026#39;repr\u0026#39;, \u0026#39;IRdisplay\u0026#39;, \u0026#39;evaluate\u0026#39;, \u0026#39;crayon\u0026#39;, \u0026#39;pbdZMQ\u0026#39;, \u0026#39;devtools\u0026#39;, \u0026#39;uuid\u0026#39;, \u0026#39;digest\u0026#39;)) \nstep 2 继续在在R环境中运行\ndevtools::install_github(\u0026#39;IRkernel/IRkernel\u0026#39;) \nstep 3 继续在R环境中运行\nIRkernel::installspec(user=FALSE) \n调用Jupyter 重新打开命令行，执行\njupyter notebook 弹出的界面中拥有了R的kernel\n新建一个R的jupyter文件，运行下方代码\nlibrary(\u0026#34;ggsci\u0026#34;) library(\u0026#34;ggplot2\u0026#34;) library(\u0026#34;gridExtra\u0026#34;) data(\u0026#34;diamonds\u0026#34;) p1 \u0026lt;- ggplot( subset(diamonds, carat \u0026gt;= 2.2), aes(x = table, y = price, colour = cut) ) + geom_point(alpha = 0.7) + geom_smooth(method = \u0026#34;loess\u0026#34;, alpha = 0.05, size = 1, span = 1) + theme_bw() p2 \u0026lt;- ggplot( subset(diamonds, carat \u0026gt; 2.","title":"在jupyter内运行R代码"},{"content":"在前面的文章中，我们介绍了关于词向量的一些基础理论和训练方法，本文主要开放汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等9大领域预训练词向量，以及字符、依存、拼音与词性4类预训练向量地址，供大家一起使用。\n一、汽车、房产等9大领域预训练词向量 通过收集多文本分类语料库，对汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等多个领域文本进行词向量训练，得到了如下预训练词向量的结果：\n   领域类型 模型类型 关键词集合 词的规模     汽车 word_vector_auto.model.bin 117,510 200   房产 word_vector_house.model.bin 145,287 200   教育 word_vector_edu.model.bin 242,874 200   社会 word_vector_society.model.bin 221,395 200   娱乐 word_vector_ent.model.bin 230,665 200   体育 word_vector_sports.model.bin 95724 200   金融 word_vector_finance.model.bin 284035 200   科技 word_vector_tech.model.bin 108188 200   游戏 word_vector_games.model.bin 100821 200    开放地址：\nhttps://pan.baidu.com/s/1jEHFoAmVXlB67Q28-CeTvw 密码: 1pa6\n二、预训练字符、依存、拼音与词性向量 通过对字符、依存、拼音与词性进行切分，使用同样的方式，可以得到相应的预训练词向量。\n   向量名称 向量含义 词数 维度 例子     de_vec_10 依存关系向量 13 10 SBV, ATT   pinyin_vec_300 汉语拼音向量 146242 300 ni, hao   postag_vec_30 汉语词性向量 59 300 n,v,a,d   token_vec_300 汉语字向量 20029 300 刘,焕,勇   word_vec_300 汉语词向量 673266 300 刘焕勇    开放地址：\nhttps://github.com/liuhuanyong/ChineseEmbedding\n向量效果：\n***********************字符向量************************ token:刘 (\u0026#39;李\u0026#39;, 0.7306396961212158),(\u0026#39;陈\u0026#39;, 0.7201231122016907) (\u0026#39;赵\u0026#39;, 0.6974461674690247),(\u0026#39;杨\u0026#39;, 0.6972213983535767) (\u0026#39;吴\u0026#39;, 0.6851627230644226),(\u0026#39;徐\u0026#39;, 0.6516467332839966) (\u0026#39;郭\u0026#39;, 0.6499480605125427),(\u0026#39;蔡\u0026#39;, 0.6175302267074585) (\u0026#39;郑\u0026#39;, 0.6092196106910706),(\u0026#39;孙\u0026#39;, 0.5950524210929871) token:丑 (\u0026#39;卯\u0026#39;, 0.6074919700622559),(\u0026#39;酉\u0026#39;, 0.5910211801528931) (\u0026#39;巳\u0026#39;, 0.5581363439559937),(\u0026#39;戌\u0026#39;, 0.43932047486305237) (\u0026#39;戊\u0026#39;, 0.41449615359306335),(\u0026#39;壬\u0026#39;, 0.40456631779670715) (\u0026#39;謤\u0026#39;, 0.367109090089798),(\u0026#39;绯\u0026#39;, 0.3643313944339752), (\u0026#39;寅\u0026#39;, 0.36351141333580017),(\u0026#39;旽\u0026#39;, 0.3549465537071228) ***********************依存向量************************ dependency rel:ATT (\u0026#39;COO\u0026#39;, 0.14239487051963806),(\u0026#39;ADV\u0026#39;, -0.16987691819667816) (\u0026#39;RAD\u0026#39;, -0.2357601821422577),(\u0026#39;HED\u0026#39;, -0.2401314228773117) (\u0026#39;SBV\u0026#39;, -0.25625932216644287),(\u0026#39;WP\u0026#39;, -0.27165737748146057) (\u0026#39;LAD\u0026#39;, -0.2902592420578003),(\u0026#39;POB\u0026#39;, -0.2990782558917999) (\u0026#39;VOB\u0026#39;, -0.37553706765174866),(\u0026#39;IOB\u0026#39;, -0.6669262647628784) dependency rel:POB (\u0026#39;IOB\u0026#39;, 0.16698899865150452),(\u0026#39;DBL\u0026#39;, 0.16678886115550995) (\u0026#39;FOB\u0026#39;, 0.1657436639070511),(\u0026#39;CMP\u0026#39;, 0.14784857630729675) (\u0026#39;VOB\u0026#39;, 0.1461176574230194),(\u0026#39;SBV\u0026#39;, 0.08011472970247269) (\u0026#39;LAD\u0026#39;, -0.022307466715574265),(\u0026#39;WP\u0026#39;, -0.022942926734685898) (\u0026#39;HED\u0026#39;, -0.037264980375766754),(\u0026#39;RAD\u0026#39;, -0.042251598089933395) ***********************拼音向量************************ pinyin:wo (\u0026#39;shei\u0026#39;, 0.6129732131958008)(\u0026#39;ta\u0026#39;, 0.6081706285476685) (\u0026#39;nin\u0026#39;, 0.5819231867790222),(\u0026#39;！\u0026#39;, 0.5435523986816406) (\u0026#39;……\u0026#39;, 0.48428624868392944),(\u0026#39;ai\u0026#39;, 0.47832390666007996) (\u0026#39;o\u0026#39;, 0.4761071801185608),(\u0026#39;。』\u0026#39;, 0.4598163366317749) (\u0026#39;...\u0026#39;, 0.45207729935646057),(\u0026#39;ni\u0026#39;, 0.44975683093070984) pinyin:guo (\u0026#39;dang\u0026#39;, 0.3908974528312683),(\u0026#39;yuan\u0026#39;, 0.378823846578598) (\u0026#39;zu\u0026#39;, 0.35387369990348816),(\u0026#39;hua\u0026#39;, 0.3405681848526001) (\u0026#39;zheng\u0026#39;, 0.3355437219142914),(\u0026#39;yi\u0026#39;, 0.3333034813404083) (\u0026#39;ren\u0026#39;, 0.3194104731082916),(\u0026#39;jun\u0026#39;, 0.3187354505062103) (\u0026#39;hui\u0026#39;, 0.31342023611068726),(\u0026#39;xin\u0026#39;, 0.3096797466278076) ***********************词性向量************************ word postag:a (\u0026#39;d\u0026#39;, 0.7203904986381531),(\u0026#39;c\u0026#39;, 0.6124969720840454) (\u0026#39;v\u0026#39;, 0.4963228106498718),(\u0026#39;an\u0026#39;, 0.4531499147415161) (\u0026#39;uz\u0026#39;, 0.4459834396839142),(\u0026#39;ud\u0026#39;, 0.42059916257858276) (\u0026#39;r\u0026#39;, 0.4090540111064911),(\u0026#39;uj\u0026#39;, 0.4061364233493805) (\u0026#39;i\u0026#39;, 0.38707998394966125),(\u0026#39;l\u0026#39;, 0.3551557660102844) word postag:n (\u0026#39;b\u0026#39;, 0.7030695676803589),(\u0026#39;vn\u0026#39;, 0.490166038274765) (\u0026#39;p\u0026#39;, 0.4858315885066986),(\u0026#39;v\u0026#39;, 0.4499088227748871) (\u0026#39;nt\u0026#39;, 0.44155171513557434),(\u0026#39;f\u0026#39;, 0.26609259843826294) (\u0026#39;s\u0026#39;, 0.2639649212360382),(\u0026#39;l\u0026#39;, 0.24365971982479095) (\u0026#39;ns\u0026#39;, 0.2278469204902649),(\u0026#39;m\u0026#39;, 0.202927365899086) ***********************词向量************************ word:爱情 (\u0026#39;爱恋\u0026#39;, 0.6931096315383911),(\u0026#39;真爱\u0026#39;, 0.6897798776626587) (\u0026#39;婚姻\u0026#39;, 0.6540514826774597),(\u0026#39;浪漫爱情\u0026#39;, 0.6535360813140869) (\u0026#39;情感\u0026#39;, 0.6501022577285767),(\u0026#39;感情\u0026#39;, 0.6403399705886841) (\u0026#39;纯爱\u0026#39;, 0.6394841074943542),(\u0026#39;爱情故事\u0026#39;, 0.6282097101211548) (\u0026#39;校园爱情\u0026#39;, 0.6078493595123291),(\u0026#39;情爱\u0026#39;, 0.5976818799972534) word:创新 (\u0026#39;技术创新\u0026#39;, 0.7648976445198059),(\u0026#39;不断创新\u0026#39;, 0.7172579765319824) (\u0026#39;创新型\u0026#39;, 0.6573833227157593),(\u0026#39;创新能力\u0026#39;, 0.6533682942390442) (\u0026#39;创新性\u0026#39;, 0.6160774827003479),(\u0026#39;革新\u0026#39;, 0.6159394383430481) (\u0026#39;人才培养\u0026#39;, 0.6093565821647644),(\u0026#39;开拓创新\u0026#39;, 0.6015594601631165) (\u0026#39;探索\u0026#39;, 0.5987343788146973),(\u0026#39;技术革新\u0026#39;, 0.5949685573577881) 关于作者 老刘，刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。\n就职于360人工智能研究院、曾就职于中国科学院软件研究所。\n老刘说NLP，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pretained_nlp_models/","summary":"在前面的文章中，我们介绍了关于词向量的一些基础理论和训练方法，本文主要开放汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等9大领域预训练词向量，以及字符、依存、拼音与词性4类预训练向量地址，供大家一起使用。\n一、汽车、房产等9大领域预训练词向量 通过收集多文本分类语料库，对汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等多个领域文本进行词向量训练，得到了如下预训练词向量的结果：\n   领域类型 模型类型 关键词集合 词的规模     汽车 word_vector_auto.model.bin 117,510 200   房产 word_vector_house.model.bin 145,287 200   教育 word_vector_edu.model.bin 242,874 200   社会 word_vector_society.model.bin 221,395 200   娱乐 word_vector_ent.model.bin 230,665 200   体育 word_vector_sports.model.bin 95724 200   金融 word_vector_finance.model.bin 284035 200   科技 word_vector_tech.model.bin 108188 200   游戏 word_vector_games.model.bin 100821 200    开放地址：","title":"NLP资源 | 汽车、金融等9大领域预训练词向量模型下载资源"},{"content":"朋友圈转发截图生成工具 点击这里使用\n本工具仅可用于个人应付各类强制要求转发朋友圈的情形，请不要用于以下用途：\n 将生成的截图用于造谣诽谤、微商宣传等非法或令人反感的用途 使用各种方式大量批量生成截图 将源代码或生成的截图有偿售卖 以 “关注○○，发送○○获取工具地址” 等方式为自己的公众号引流（想要推荐的话，在正文里留下地址或二维码之类的就没问题～）  这个小工具是什么？\n一个因为不喜欢也不想往朋友圈发某些不得不发的废文而做出来的摸鱼产物。\n生成的截图中，文字部分能否出现表情符号？\n微信中除 Emoji 之外的其它表情符号实际上是特定的文本，复制后粘贴到别处时就可以看到，在微信中则显示为表情符号。\n比如这个表情，从简体中文版微信中发送的实际上是[再见]，繁体中文是[再見]，英文是[Bye]。即使不选择表情符号而是手动输入[再见]、[再見]或[Bye]（任选一个输入即可，和微信的语言设置无关），发送后它们都会显示为。\n在这个小工具的“正文”处输入表情符号对应的文本，生成截图时也会被替换成对应的表情符号～\n目前仅支持简体中文、繁体中文、英文版微信的表情文本。从使用其他语言的微信发送一条带有表情的消息然后复制到这里，生成的截图中不一定会出现对应的表情符号。\nCleanMyWechat 自动删除 PC 端微信自动下载的大量文件、视频、图片等数据内容，解放一年几十 G 的空间占用。\n该工具不会删除文字的聊天记录，请放心使用。请给个 Star 吧，非常感谢！\n现已经支持 Windows 系统中的所有微信版本。\nhttps://github.com/blackboxo/CleanMyWechat\n清理Mac中的微信数据\n同时按下command+shift+G三个键， 输入\u0026amp;回车\n~/Library/Containers/com.tencent.xinWeChat/Data/Library/Application Support/com.tencent.xinWeChat 在我的mac中，微信软件的缓存也达到7.42G。Mac的空间十分珍贵，直接删掉，怕以后数据比较难恢复，我采取的办法是转移至移动硬盘。\n正则表达式解析 可以测试正则表达式匹配效果，而且将表达式匹配功能以流程图的方式展示出来，对初学regex的小白非常友好。\nhttps://devtoolcafe.com/tools/regex#!flags=img\u0026amp;re=\nB站视频下载 https://github.com/leiurayer/downkyi\n哔哩下载姬（DownKyi）是一个简单易用的哔哩哔哩视频下载工具，具有简洁的界面，流畅的操作逻辑。哔哩下载姬可以下载几乎所有的B站视频，并输出mp4格式的文件；采用Aria下载器多线程下载，采用FFmpeg对视频进行混流、提取音视频等操作。目前仅支持Win\n 支持二维码登录 支持视频、番剧、剧集、电影、课程下载 支持8K、4K、HDR、杜比视界、杜比全景声下载 支持用户收藏夹、订阅、稍后再看、历史记录下载 支持弹幕下载、样式设置 支持字幕下载 支持封面下载 支持自定义文件命名 支持断点续传 支持Aria2c 支持下载历史记录保存 支持av、BV互转 支持弹幕发送者查询 支持音视频分离 支持去水印 支持检查更新  Musicn 🎵 一个下载高品质音乐的命令行工具\nhttps://github.com/zonemeen/musicn\nexping https://exping.world/\n标记你的品味地图 让你的地图更有价\nbookmark https://www.bookmark.style/ 将任意链接转为美观的、带二维码样式的分享卡片\nOrion 浏览器 一种基于 Webkit 的新浏览器，只能用于 MacOS 和 iOS，但是能安装 Chrome 的插件，目前还是测试版。据说体验非常好，胜过 Safari。\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly17/","summary":"朋友圈转发截图生成工具 点击这里使用\n本工具仅可用于个人应付各类强制要求转发朋友圈的情形，请不要用于以下用途：\n 将生成的截图用于造谣诽谤、微商宣传等非法或令人反感的用途 使用各种方式大量批量生成截图 将源代码或生成的截图有偿售卖 以 “关注○○，发送○○获取工具地址” 等方式为自己的公众号引流（想要推荐的话，在正文里留下地址或二维码之类的就没问题～）  这个小工具是什么？\n一个因为不喜欢也不想往朋友圈发某些不得不发的废文而做出来的摸鱼产物。\n生成的截图中，文字部分能否出现表情符号？\n微信中除 Emoji 之外的其它表情符号实际上是特定的文本，复制后粘贴到别处时就可以看到，在微信中则显示为表情符号。\n比如这个表情，从简体中文版微信中发送的实际上是[再见]，繁体中文是[再見]，英文是[Bye]。即使不选择表情符号而是手动输入[再见]、[再見]或[Bye]（任选一个输入即可，和微信的语言设置无关），发送后它们都会显示为。\n在这个小工具的“正文”处输入表情符号对应的文本，生成截图时也会被替换成对应的表情符号～\n目前仅支持简体中文、繁体中文、英文版微信的表情文本。从使用其他语言的微信发送一条带有表情的消息然后复制到这里，生成的截图中不一定会出现对应的表情符号。\nCleanMyWechat 自动删除 PC 端微信自动下载的大量文件、视频、图片等数据内容，解放一年几十 G 的空间占用。\n该工具不会删除文字的聊天记录，请放心使用。请给个 Star 吧，非常感谢！\n现已经支持 Windows 系统中的所有微信版本。\nhttps://github.com/blackboxo/CleanMyWechat\n清理Mac中的微信数据\n同时按下command+shift+G三个键， 输入\u0026amp;回车\n~/Library/Containers/com.tencent.xinWeChat/Data/Library/Application Support/com.tencent.xinWeChat 在我的mac中，微信软件的缓存也达到7.42G。Mac的空间十分珍贵，直接删掉，怕以后数据比较难恢复，我采取的办法是转移至移动硬盘。\n正则表达式解析 可以测试正则表达式匹配效果，而且将表达式匹配功能以流程图的方式展示出来，对初学regex的小白非常友好。\nhttps://devtoolcafe.com/tools/regex#!flags=img\u0026amp;re=\nB站视频下载 https://github.com/leiurayer/downkyi\n哔哩下载姬（DownKyi）是一个简单易用的哔哩哔哩视频下载工具，具有简洁的界面，流畅的操作逻辑。哔哩下载姬可以下载几乎所有的B站视频，并输出mp4格式的文件；采用Aria下载器多线程下载，采用FFmpeg对视频进行混流、提取音视频等操作。目前仅支持Win\n 支持二维码登录 支持视频、番剧、剧集、电影、课程下载 支持8K、4K、HDR、杜比视界、杜比全景声下载 支持用户收藏夹、订阅、稍后再看、历史记录下载 支持弹幕下载、样式设置 支持字幕下载 支持封面下载 支持自定义文件命名 支持断点续传 支持Aria2c 支持下载历史记录保存 支持av、BV互转 支持弹幕发送者查询 支持音视频分离 支持去水印 支持检查更新  Musicn 🎵 一个下载高品质音乐的命令行工具\nhttps://github.com/zonemeen/musicn\nexping https://exping.world/\n标记你的品味地图 让你的地图更有价","title":"TechWeekly-17 每周有趣有用的技术分享"},{"content":"JCR2021一篇软件介绍，支持中英文在内的多种语言。\n Hovy, D., Melumad, S. and Inman, J.J., 2021. Wordify: a tool for discovering and differentiating consumer vocabularies. Journal of Consumer Research, 48(3), pp.394-414.\n 摘要: 这项工作介绍了一个免费易用的在线文本分析工具Wordify，用于了解”在不同上下文中，消费者的单词使用如何变化“。Wordify 使用随机逻辑回归 (RLR) 来识别最能区分来自不同预分类文本的用词差异，例如男性与女性撰写的帖子用词差异，或好评与差评的用词差异。我们提供了说明性示例，以展示该工具如何用于多种用途，例如 (1) 揭示消费者在智能手机和 PC 上撰写评论时使用的独特词汇，(2) 发现推文中使用的词语在假定的支持者和反对有争议的广告，以及 (3) 扩展基于字典的情绪测量工具的字典。我们凭经验表明，Wordify 的 RLR 算法在区分词汇方面比支持向量机和卡方选择器表现更好，同时在计算时间上具有显着优势。还讨论了 Wordify 与其他文本分析工具的结合使用，例如概率主题建模和情感分析，以更深入地了解语言在消费者行为中的作用。\n关键词：文本分析，自然语言处理，语言，情感分析\n本地wordify配置 作者在github公开了wordify的代码，仓库地址 https://github.com/MilaNLProc/wordify-webapp-streamlit\n大致的使用步骤\n wordify要配置spacy语言模型，配置方法参照以前分享的spacy产业级自然语言处理包 到github仓库下载代码，解压至桌面 打开命令行, 执行命令cd desktop/wordify-webapp-streamlit-main 命令行执行pip3 install -r requirements.txt 命令行执行streamlit run app.py， 此时命令行中出现本地服务地址(类似于网站)，浏览器打开这个地址即可  本地配置比较有难度，建议使用在线版https://wordify.unibocconi.it/\n在线展示网站 网址 https://wordify.unibocconi.it/\n使用方法   表格文件需含两个字段名，分别为text和label, 中文数据需要先为用空格间隔词语的文本样式。中文样例文件\n  表格文件支持csv、xlsx、tsv、parquet，10M以内。数据上传成功后，页面会发生变化\n  在线页面在运行时一定不要刷新，这样会中断数据分析的过程\n  Wordify 的性能取决于文件中各个文本的长度。\n  点评 以往的文本分析思路，大多无视混杂效应Confound，主要从文本中抽取一些变量，如情感值，用于后期计量建模，试图挖掘文本指标(如情感值)与Outcome之间的因果关系。\n 混杂效应，例如研究推文正负面情感对网友点击行为的影响。\n研究的机制可以简化为不同的文本情感\u0026ndash;\u0026gt;产生不同的网友点击\n但是有可能不全是情感影响了网友的点击，作者存在性别差异，女性比男性更容易表达积极文本信息，而且在互联网世界女性比男性可能更有吸引力。\n 论文中没提及Confound效应，但粗略浏览下，wordify创新地考虑了confound场景，通过文本分析，看看不同群体用词的差异。\nwordify的缺点本地版配置太难，网页版运行太慢。0.6M的中文数据，我等了20min，还是没有跑出结果，果断关闭在线网页。总之感觉没有文中说的那么易用，门槛还是太高了。有耐心的朋友，如果感兴趣，可以去试试。\n如果研究考虑文本的confound效应，可以参考causalnlp包，虽然配置难，但是运行速度还是有保证。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/jcr_wordify/","summary":"JCR2021一篇软件介绍，支持中英文在内的多种语言。\n Hovy, D., Melumad, S. and Inman, J.J., 2021. Wordify: a tool for discovering and differentiating consumer vocabularies. Journal of Consumer Research, 48(3), pp.394-414.\n 摘要: 这项工作介绍了一个免费易用的在线文本分析工具Wordify，用于了解”在不同上下文中，消费者的单词使用如何变化“。Wordify 使用随机逻辑回归 (RLR) 来识别最能区分来自不同预分类文本的用词差异，例如男性与女性撰写的帖子用词差异，或好评与差评的用词差异。我们提供了说明性示例，以展示该工具如何用于多种用途，例如 (1) 揭示消费者在智能手机和 PC 上撰写评论时使用的独特词汇，(2) 发现推文中使用的词语在假定的支持者和反对有争议的广告，以及 (3) 扩展基于字典的情绪测量工具的字典。我们凭经验表明，Wordify 的 RLR 算法在区分词汇方面比支持向量机和卡方选择器表现更好，同时在计算时间上具有显着优势。还讨论了 Wordify 与其他文本分析工具的结合使用，例如概率主题建模和情感分析，以更深入地了解语言在消费者行为中的作用。\n关键词：文本分析，自然语言处理，语言，情感分析\n本地wordify配置 作者在github公开了wordify的代码，仓库地址 https://github.com/MilaNLProc/wordify-webapp-streamlit\n大致的使用步骤\n wordify要配置spacy语言模型，配置方法参照以前分享的spacy产业级自然语言处理包 到github仓库下载代码，解压至桌面 打开命令行, 执行命令cd desktop/wordify-webapp-streamlit-main 命令行执行pip3 install -r requirements.txt 命令行执行streamlit run app.py， 此时命令行中出现本地服务地址(类似于网站)，浏览器打开这个地址即可  本地配置比较有难度，建议使用在线版https://wordify.unibocconi.it/\n在线展示网站 网址 https://wordify.unibocconi.it/\n使用方法   表格文件需含两个字段名，分别为text和label, 中文数据需要先为用空格间隔词语的文本样式。中文样例文件","title":"Wordify | 发现和区分消费者词汇的工具"},{"content":"karateclub是小规模图挖掘研究的一把瑞士军刀， 可以对图形结构化数据进行无监督学习。\n 首先，可以计算出节点、图的特征向量 其次，它包括多种重叠和非重叠的社区发现方法。  代码下载 click to download\n数据格式 karateclub假设用户提供的用于节点嵌入和社区检测的 NetworkX 图具有以下重要属性：\n 节点用整数索引 节点索引从零开始，索引是连续的  节点的属性矩阵可以提供为 scipy sparse 和 numpy 数组。返回的社区成员字典和嵌入矩阵使用相同的数字连续索引。\n安装 pip3 install karateclub \n准备数据 import pandas as pd df = pd.read_csv(\u0026#39;karate_club_graph.csv\u0026#39;) print(df.columns) print() print(df.head().to_markdown()) print() edges = list(zip(df[\u0026#39;src\u0026#39;], df[\u0026#39;tgt\u0026#39;])) print(edges) Run\nIndex(['src', 'tgt'], dtype='object') | | src | tgt | |---:|------:|------:| | 0 | 0 | 1 | | 1 | 0 | 2 | | 2 | 0 | 3 | | 3 | 0 | 4 | | 4 | 0 | 5 | [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 10), (0, 11), (0, 12), (0, 13), (0, 17), (0, 19), (0, 21), (0, 31), (1, 2), (1, 3), (1, 7), (1, 13), (1, 17), (1, 19), (1, 21), (1, 30), (2, 3), (2, 7), (2, 8), (2, 9), (2, 13), (2, 27), (2, 28), (2, 32), (3, 7), (3, 12), (3, 13), (4, 6), (4, 10), (5, 6), (5, 10), (5, 16), (6, 16), (8, 30), (8, 32), (8, 33), (9, 33), (13, 33), (14, 32), (14, 33), (15, 32), (15, 33), (18, 32), (18, 33), (19, 33), (20, 32), (20, 33), (22, 32), (22, 33), (23, 25), (23, 27), (23, 29), (23, 32), (23, 33), (24, 25), (24, 27), (24, 31), (25, 31), (26, 29), (26, 33), (27, 33), (28, 31), (28, 33), (29, 32), (29, 33), (30, 32), (30, 33), (31, 32), (31, 33), (32, 33)]  import networkx as nx graph = nx.Graph() graph.add_edges_from(edges) nx.draw(graph) Run\n​ ​\n社区发现 现在让我们使用LabelPropagation算法来发现网络中的社区结构。\nfrom karateclub import LabelPropagation model = LabelPropagation() model.fit(graph) cluster_membership = model.get_memberships() cluster_membership Run\n{23: 8, 33: 8, 5: 10, 7: 1, 28: 31, 4: 10, 3: 1, 31: 31, 20: 8, 19: 1, 6: 10, 32: 8, 29: 8, 9: 1, 14: 8, 2: 1, 0: 1, 17: 1, 25: 31, 22: 8, 11: 1, 13: 1, 1: 1, 24: 31, 15: 8, 18: 8, 26: 8, 27: 8, 16: 10, 12: 1, 30: 8, 21: 1, 8: 8, 10: 10}  在有34个节点的图中，发现了4个社区，分别是1、8、10、31。\nNode embeddings 计算节点的向量。​使用 Diff2vec 拟合数据的节点嵌入(向量)，具有少量维度、每个源节点的扩散和短欧拉游走。\nfrom karateclub import Diff2Vec model = Diff2Vec(diffusion_number=2, diffusion_cover=20, dimensions=5) model.fit(graph) X = model.get_embedding() X.shape Run\n(34, 5)  X Run\narray([[ 1.3687179 , -0.33502993, -0.3294797 , 0.40154558, 1.0270709 ], [ 0.88167036, -0.3201618 , -0.34293872, 0.41519755, 0.71964073], [ 0.8756805 , -0.21934716, -0.33261183, 0.33785722, 0.51631075], [ 0.9768452 , -0.39260587, -0.39460638, 0.28851682, 0.8665034 ], [ 0.4809215 , -0.28729865, -0.19276802, 0.22588767, 0.07305563], [ 0.5580538 , -0.28137547, -0.1947159 , 0.23712516, 0.49257705], [ 0.23477663, 0.04262228, 0.07154325, 0.02909669, 0.33999097], [ 1.1882199 , -0.21742308, -0.26985615, 0.44171503, 0.6679048 ], [ 1.0287609 , -0.27409104, -0.04119629, 0.30143994, 0.704676 ], [ 0.5700088 , -0.26341844, 0.01560158, -0.08039217, 0.41796318], [ 0.5753763 , -0.2242508 , -0.1795436 , 0.0705331 , 0.46571913], [ 0.46763912, -0.17108741, -0.22459361, 0.03058788, 0.05998428], [ 0.5500626 , -0.12745889, -0.28661036, 0.16889155, 0.48200938], [ 0.6217582 , -0.10251168, -0.0713837 , 0.13550574, 0.60422456], [ 0.9797377 , -0.46282482, -0.09380057, 0.2749968 , 0.7020155 ], [ 0.38830167, -0.30841848, -0.20950563, -0.02130592, 0.0836651 ], [ 0.57225037, -0.04150235, -0.1246101 , 0.06918757, 0.23083903], [ 0.6431406 , -0.04898892, -0.05708801, 0.1311793 , 0.46377632], [ 0.541667 , -0.16031542, -0.33119023, 0.10385639, 0.39525154], [ 0.65543544, -0.27534947, -0.28757 , 0.2080029 , 0.5288213 ], [ 0.46381798, -0.07729273, -0.09209982, 0.11292508, 0.36836028], [ 0.53826964, -0.09915172, -0.09243581, 0.15036733, 0.5449071 ], [ 0.31599265, -0.22078821, -0.02872767, 0.07436654, 0.28573534], [ 1.0706906 , -0.27783617, -0.16653039, 0.2631594 , 0.6408689 ], [ 0.67875004, -0.34441757, -0.10262538, 0.2588695 , 0.38405937], [ 0.41786563, -0.10344986, -0.19508548, 0.19657765, 0.22006002], [ 0.7855942 , -0.27200857, 0.02204541, 0.09168041, 0.42220354], [ 0.7773458 , -0.11727296, -0.24145149, 0.04537854, 0.5737133 ], [ 0.75732976, -0.314953 , -0.15383345, 0.02065313, 0.51843405], [ 0.7226543 , -0.31919608, -0.18878649, 0.15413427, 0.42012522], [ 0.43411565, -0.17342259, -0.28042233, 0.26853496, 0.49947587], [ 1.1565564 , -0.36802933, -0.12613232, 0.32381424, 0.75113887], [ 1.1192797 , -0.162529 , -0.17195942, 0.39265418, 0.83656436], [ 1.2231556 , -0.5336606 , -0.14015286, 0.14054438, 0.5695296 ]], dtype=float32)  \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/karateclub_tutorial/","summary":"karateclub是小规模图挖掘研究的一把瑞士军刀， 可以对图形结构化数据进行无监督学习。\n 首先，可以计算出节点、图的特征向量 其次，它包括多种重叠和非重叠的社区发现方法。  代码下载 click to download\n数据格式 karateclub假设用户提供的用于节点嵌入和社区检测的 NetworkX 图具有以下重要属性：\n 节点用整数索引 节点索引从零开始，索引是连续的  节点的属性矩阵可以提供为 scipy sparse 和 numpy 数组。返回的社区成员字典和嵌入矩阵使用相同的数字连续索引。\n安装 pip3 install karateclub \n准备数据 import pandas as pd df = pd.read_csv(\u0026#39;karate_club_graph.csv\u0026#39;) print(df.columns) print() print(df.head().to_markdown()) print() edges = list(zip(df[\u0026#39;src\u0026#39;], df[\u0026#39;tgt\u0026#39;])) print(edges) Run\nIndex(['src', 'tgt'], dtype='object') | | src | tgt | |---:|------:|------:| | 0 | 0 | 1 | | 1 | 0 | 2 | | 2 | 0 | 3 | | 3 | 0 | 4 | | 4 | 0 | 5 | [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 10), (0, 11), (0, 12), (0, 13), (0, 17), (0, 19), (0, 21), (0, 31), (1, 2), (1, 3), (1, 7), (1, 13), (1, 17), (1, 19), (1, 21), (1, 30), (2, 3), (2, 7), (2, 8), (2, 9), (2, 13), (2, 27), (2, 28), (2, 32), (3, 7), (3, 12), (3, 13), (4, 6), (4, 10), (5, 6), (5, 10), (5, 16), (6, 16), (8, 30), (8, 32), (8, 33), (9, 33), (13, 33), (14, 32), (14, 33), (15, 32), (15, 33), (18, 32), (18, 33), (19, 33), (20, 32), (20, 33), (22, 32), (22, 33), (23, 25), (23, 27), (23, 29), (23, 32), (23, 33), (24, 25), (24, 27), (24, 31), (25, 31), (26, 29), (26, 33), (27, 33), (28, 31), (28, 33), (29, 32), (29, 33), (30, 32), (30, 33), (31, 32), (31, 33), (32, 33)]  import networkx as nx graph = nx.","title":"karateclub库 | 计算社交网络中节点的向量"},{"content":"\n旧版cntext入口\n中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等\n github地址 https://github.com/hidadeng/cntext pypi地址 https://pypi.org/project/cntext/ 视频课-Python网络爬虫与文本数据分析  功能模块含\n stats 文本统计指标  词频统计 可读性 内置pkl词典 情感分析   dictionary 构建词表(典)  Sopmi 互信息扩充词典法 W2Vmodels 词向量扩充词典法 Glove Glove词嵌入模型   similarity 文本相似度  cos相似度 jaccard相似度 编辑距离相似度   mind.py 计算文本中的认知方向（态度、偏见）  代码下载 click to download\n安装 pip install cntext \nQuickStart import cntext as ct help(ct) Run\nHelp on package cntext: NAME cntext PACKAGE CONTENTS mind dictionary similarity stats \n一、stats 目前stats内置的函数有\n readability 文本可读性 term_freq 词频统计函数 dict_pkl_list 获取cntext内置词典列表(pkl格式) load_pkl_dict 导入pkl词典文件 sentiment 情感分析  import cntext as ct text = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。\u0026#39; ct.term_freq(text, lang=\u0026#39;chinese\u0026#39;) Run\nCounter({\u0026#39;看待\u0026#39;: 1, \u0026#39;网文\u0026#39;: 1, \u0026#39;作者\u0026#39;: 1, \u0026#39;黑客\u0026#39;: 1, \u0026#39;大佬\u0026#39;: 1, \u0026#39;盗号\u0026#39;: 1, \u0026#39;改文因\u0026#39;: 1, \u0026#39;万分\u0026#39;: 1, \u0026#39;惭愧\u0026#39;: 1, \u0026#39;停\u0026#39;: 1}) \n1.1 readability 文本可读性，指标越大，文章复杂度越高，可读性越差。\nreadability(text, lang=\u0026lsquo;chinese\u0026rsquo;)\n text: 文本字符串数据 lang: 语言类型，\u0026ldquo;chinese\u0026quot;或\u0026quot;english\u0026rdquo;，默认\u0026quot;chinese\u0026quot;  中文可读性 算法参考自\n 徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.\n readability1 \u0026mdash;每个分句中的平均字数 readability2 \u0026mdash;每个句子中副词和连词所占的比例 readability3 \u0026mdash;参考Fog Index， readability3=(readability1+readability2)×0.5   ​\n以上三个指标越大，都说明文本的复杂程度越高，可读性越差。\ntext1 = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。\u0026#39; ct.readability(text1, lang=\u0026#39;chinese\u0026#39;) Run\n{\u0026#39;readability1\u0026#39;: 28.0, \u0026#39;readability2\u0026#39;: 0.15789473684210525, \u0026#39;readability3\u0026#39;: 14.078947368421053} \n句子中的符号变更会影响结果\ntext2 = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。\u0026#39; ct.readability(text2, lang=\u0026#39;chinese\u0026#39;) Run\n{\u0026#39;readability1\u0026#39;: 27.0, \u0026#39;readability2\u0026#39;: 0.16666666666666666, \u0026#39;readability3\u0026#39;: 13.583333333333334} \n1.2 term_freq 词频统计函数，返回Counter类型\nimport cntext as ct text = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。\u0026#39; ct.term_freq(text, lang=\u0026#39;chinese\u0026#39;) Run\nCounter({\u0026#39;看待\u0026#39;: 1, \u0026#39;网文\u0026#39;: 1, \u0026#39;作者\u0026#39;: 1, \u0026#39;黑客\u0026#39;: 1, \u0026#39;大佬\u0026#39;: 1, \u0026#39;盗号\u0026#39;: 1, \u0026#39;改文因\u0026#39;: 1, \u0026#39;万分\u0026#39;: 1, \u0026#39;惭愧\u0026#39;: 1, \u0026#39;停\u0026#39;: 1}) \n1.3 dict_pkl_list 获取cntext内置词典列表(pkl格式)\nimport cntext as ct # 获取cntext内置词典列表(pkl格式) ct.dict_pkl_list() Run\n[\u0026#39;DUTIR.pkl\u0026#39;, \u0026#39;HOWNET.pkl\u0026#39;, \u0026#39;sentiws.pkl\u0026#39;, \u0026#39;ChineseFinancialFormalUnformalSentiment.pkl\u0026#39;, \u0026#39;ANEW.pkl\u0026#39;, \u0026#39;LSD2015.pkl\u0026#39;, \u0026#39;NRC.pkl\u0026#39;, \u0026#39;geninqposneg.pkl\u0026#39;, \u0026#39;HuLiu.pkl\u0026#39;, \u0026#39;AFINN.pkl\u0026#39;, \u0026#39;ADV_CONJ.pkl\u0026#39;, \u0026#39;LoughranMcDonald.pkl\u0026#39;, \u0026#39;STOPWORDS.pkl\u0026#39;, \u0026#39;concreteness.pkl\u0026#39;] 词典对应关系, 部分情感词典资料整理自 quanteda.sentiment\n   pkl文件 词典 语言 功能     DUTIR.pkl 大连理工大学情感本体库 中文 七大类情绪，哀, 好, 惊, 惧, 乐, 怒, 恶   HOWNET.pkl 知网Hownet词典 中文 正面词、负面词   sentiws.pkl SentimentWortschatz (SentiWS) 英文 正面词、负面词；\n效价   ChineseFinancialFormalUnformalSentiment.pkl 金融领域正式、非正式；积极消极 中文 formal-pos、\nformal-neg；\nunformal-pos、\nunformal-neg   ANEW.pkl 英语单词的情感规范Affective Norms for English Words (ANEW) 英文 词语效价信息   LSD2015.pkl Lexicoder Sentiment Dictionary (2015) 英文 正面词、负面词   NRC.pkl NRC Word-Emotion Association Lexicon 英文 细粒度情绪词；   geninqposneg.pkl      HuLiu.pkl Hu\u0026amp;Liu (2004)正、负情感词典 英文 正面词、负面词   AFINN.pkl 尼尔森 (2011) 的“新 ANEW”效价词表 英文 情感效价信息valence   LoughranMcDonald.pkl 会计金融LM词典 英文 金融领域正、负面情感词   ADV_CONJ.pkl 副词连词 中文    STOPWORDS.pkl  中、英 停用词   concreteness.pkl Brysbaert, M., Warriner, A. B., \u0026amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911 English word \u0026amp; concreateness score    注意:   如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”\n  如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue\n  1.4 load_pkl_dict 导入pkl词典文件，返回字典样式数据。\nimport cntext as ct # 导入pkl词典文件, print(ct.load_pkl_dict(\u0026#39;DUTIR.pkl\u0026#39;)) Run\n{\u0026#39;DUTIR\u0026#39;: {\u0026#39;哀\u0026#39;: [\u0026#39;怀想\u0026#39;, \u0026#39;治丝而棼\u0026#39;, ...], \u0026#39;好\u0026#39;: [\u0026#39;进贤黜奸\u0026#39;, \u0026#39;清醇\u0026#39;, \u0026#39;放达\u0026#39;, ...], \u0026#39;惊\u0026#39;: [\u0026#39;惊奇不已\u0026#39;, \u0026#39;魂惊魄惕\u0026#39;, \u0026#39;海外奇谈\u0026#39;,...], \u0026#39;惧\u0026#39;: [\u0026#39;忸忸怩怩\u0026#39;, \u0026#39;谈虎色变\u0026#39;, \u0026#39;手忙脚乱\u0026#39;, \u0026#39;刿目怵心\u0026#39;,...], \u0026#39;乐\u0026#39;: [\u0026#39;百龄眉寿\u0026#39;, \u0026#39;娱心\u0026#39;, \u0026#39;如意\u0026#39;, \u0026#39;喜糖\u0026#39;,...], \u0026#39;怒\u0026#39;: [\u0026#39;饮恨吞声\u0026#39;, \u0026#39;扬眉瞬目\u0026#39;,...], \u0026#39;恶\u0026#39;: [\u0026#39;出逃\u0026#39;, \u0026#39;鱼肉百姓\u0026#39;, \u0026#39;移天易日\u0026#39;,] } \n1.5 sentiment sentiment(text, diction, lang=\u0026lsquo;chinese\u0026rsquo;) 使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，\n text: 待分析中文文本 diction: 情感词字典； lang: 语言类型，\u0026ldquo;chinese\u0026quot;或\u0026quot;english\u0026rdquo;，默认\u0026quot;chinese\u0026quot;  import cntext as ct text = \u0026#39;我今天得奖了，很高兴，我要将快乐分享大家。\u0026#39; ct.sentiment(text=text, diction=ct.load_pkl_dict(\u0026#39;DUTIR.pkl\u0026#39;)[\u0026#39;DUTIR\u0026#39;], lang=\u0026#39;chinese\u0026#39;) Run\n{\u0026#39;哀_num\u0026#39;: 0, \u0026#39;好_num\u0026#39;: 0, \u0026#39;惊_num\u0026#39;: 0, \u0026#39;惧_num\u0026#39;: 0, \u0026#39;乐_num\u0026#39;: 2, \u0026#39;怒_num\u0026#39;: 0, \u0026#39;恶_num\u0026#39;: 0, \u0026#39;stopword_num\u0026#39;: 8, \u0026#39;word_num\u0026#39;: 14, \u0026#39;sentence_num\u0026#39;: 1} 如果不适用pkl词典，可以自定义自己的词典，例如\ndiction = {\u0026#39;pos\u0026#39;: [\u0026#39;高兴\u0026#39;, \u0026#39;快乐\u0026#39;, \u0026#39;分享\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;难过\u0026#39;, \u0026#39;悲伤\u0026#39;], \u0026#39;adv\u0026#39;: [\u0026#39;很\u0026#39;, \u0026#39;特别\u0026#39;]} text = \u0026#39;我今天得奖了，很高兴，我要将快乐分享大家。\u0026#39; ct.sentiment(text=text, diction=diction, lang=\u0026#39;chinese\u0026#39;) Run\n{\u0026#39;pos_num\u0026#39;: 3, \u0026#39;neg_num\u0026#39;: 0, \u0026#39;adv_num\u0026#39;: 1, \u0026#39;stopword_num\u0026#39;: 8, \u0026#39;word_num\u0026#39;: 14, \u0026#39;sentence_num\u0026#39;: 1} \n1.6 sentiment_by_valence sentiment函数默认所有情感词权重均为1，只需要统计文本中情感词的个数，即可得到文本情感得分。\nsentiment_by_valence(text, diction, lang=\u0026lsquo;english\u0026rsquo;)函数考虑了词语的效价(valence)\n text 待输入文本 diction 带效价的词典，DataFrame格式。 lang 语言类型\u0026rsquo;chinese' 或 \u0026lsquo;english\u0026rsquo;，默认\u0026rsquo;english'  这里我们以文本具体性度量为例， concreteness.pkl 整理自 Brysbaert2014的文章。\n Brysbaert, M., Warriner, A. B., \u0026amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911\n import cntext as ct # load the concreteness.pkl dictionary file concreteness_df = ct.load_pkl_dict(\u0026#39;concreteness.pkl\u0026#39;) concreteness_df.head() Run\n    word valence     0 roadsweeper 4.85   1 traindriver 4.54   2 tush 4.45   3 hairdress 3.93   4 pharmaceutics 3.77    先看一条文本的具体性度量\nreply = \u0026#34;I\u0026#39;ll go look for that\u0026#34; score=ct.sentiment_by_valence(text=reply, diction=concreteness_df, lang=\u0026#39;english\u0026#39;) score Run\n1.85 \n很多条文本的具体性度量\nemployee_replys = [\u0026#34;I\u0026#39;ll go look for that\u0026#34;, \u0026#34;I\u0026#39;ll go search for that\u0026#34;, \u0026#34;I\u0026#39;ll go search for that top\u0026#34;, \u0026#34;I\u0026#39;ll go search for that t-shirt\u0026#34;, \u0026#34;I\u0026#39;ll go look for that t-shirt in grey\u0026#34;, \u0026#34;I\u0026#39;ll go search for that t-shirt in grey\u0026#34;] for idx, reply in enumerate(employee_replys): score=ct.sentiment_by_valence(text=reply, diction=concreteness_df, lang=\u0026#39;english\u0026#39;) template = \u0026#34;Concreteness Score: {score:.2f}| Example-{idx}: {exmaple}\u0026#34; print(template.format(score=score, idx=idx, exmaple=reply)) ct.sentiment_by_valence(text=text, diction=concreteness_df, lang=\u0026#39;english\u0026#39;) Run\nConcreteness Score: 1.55 | Example-0: I\u0026#39;ll go look for that Concreteness Score: 1.55 | Example-1: I\u0026#39;ll go search for that Concreteness Score: 1.89 | Example-2: I\u0026#39;ll go search for that top Concreteness Score: 2.04 | Example-3: I\u0026#39;ll go search for that t-shirt Concreteness Score: 2.37 | Example-4: I\u0026#39;ll go look for that t-shirt in grey Concreteness Score: 2.37 | Example-5: I\u0026#39;ll go search for that t-shirt in grey \n\n二、dictionary 本模块用于构建词表(典),含\n SoPmi 共现法扩充词表(典) W2VModels 词向量word2vec扩充词表(典)  2.1 SoPmi SoPmi 共现法\nimport cntext as ct import os sopmier = ct.SoPmi(cwd=os.getcwd(), input_txt_file=\u0026#39;data/sopmi_corpus.txt\u0026#39;, #原始数据，您的语料 seedword_txt_file=\u0026#39;data/sopmi_seed_words.txt\u0026#39;, #人工标注的初始种子词 lang=\u0026#39;chinese\u0026#39; ) sopmier.sopmi() Run\nStep 1/4:...Preprocess Corpus ... Step 2/4:...Collect co-occurrency information ... Step 3/4:...Calculate mutual information ... Step 4/4:...Save candidate words ... Finish! used 44.49 s \n2.2 W2VModels W2VModels 词向量\n特别要注意代码需要设定lang语言参数\nimport cntext as ct import os #初始化模型,需要设置lang参数。 model = ct.W2VModels(cwd=os.getcwd(), lang=\u0026#39;english\u0026#39;) #语料数据 w2v_corpus.txt model.train(input_txt_file=\u0026#39;data/w2v_corpus.txt\u0026#39;) #根据种子词，筛选出没类词最相近的前100个词 model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/integrity.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/innovation.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/quality.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/respect.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/teamwork.txt\u0026#39;, topn=100) Run\nStep 1/4:...Preprocess corpus ... Step 2/4:...Train word2vec model used 174 s Step 3/4:...Prepare similar candidates for each seed word in the word2vec model... Step 4/4 Finish! Used 187 s Step 3/4:...Prepare similar candidates for each seed word in the word2vec model... Step 4/4 Finish! Used 187 s Step 3/4:...Prepare similar candidates for each seed word in the word2vec model... Step 4/4 Finish! Used 187 s Step 3/4:...Prepare similar candidates for each seed word in the word2vec model... Step 4/4 Finish! Used 187 s Step 3/4:...Prepare similar candidates for each seed word in the word2vec model... Step 4/4 Finish! Used 187 s \n需要注意 训练出的w2v模型可以后续中使用。\nfrom gensim.models import KeyedVectors w2v_model = KeyedVectors.load(w2v.model路径) #找出word的词向量 #w2v_model.get_vector(word) #更多w2_model方法查看 #help(w2_model) 例如本代码，运行生成的结果路径output/w2v_candi_words/w2v.model\nfrom gensim.models import KeyedVectors w2v_model = KeyedVectors.load(\u0026#39;output/w2v_candi_words/w2v.model\u0026#39;) w2v_model.most_similar(\u0026#39;innovation\u0026#39;) Run\n[(\u0026#39;technology\u0026#39;, 0.689210832118988), (\u0026#39;infrastructure\u0026#39;, 0.669672966003418), (\u0026#39;resources\u0026#39;, 0.6695448160171509), (\u0026#39;talent\u0026#39;, 0.6627111434936523), (\u0026#39;execution\u0026#39;, 0.6549549102783203), (\u0026#39;marketing\u0026#39;, 0.6533523797988892), (\u0026#39;merchandising\u0026#39;, 0.6504817008972168), (\u0026#39;diversification\u0026#39;, 0.6479553580284119), (\u0026#39;expertise\u0026#39;, 0.6446896195411682), (\u0026#39;digital\u0026#39;, 0.6326863765716553)] \n#获取词向量 w2v_model.get_vector(\u0026#39;innovation\u0026#39;) Run\narray([-0.45616838, -0.7799563 , 0.56367606, -0.8570078 , 0.600359 , -0.6588043 , 0.31116748, -0.11956959, -0.47599426, 0.21840936, -0.02268819, 0.1832016 , 0.24452794, 0.01084935, -1.4213187 , 0.22840202, 0.46387577, 1.198386 , -0.621511 , -0.51598716, 0.13352732, 0.04140598, -0.23470387, 0.6402956 , 0.20394802, 0.10799981, 0.24908689, -1.0117126 , -2.3168423 , -0.0402851 , 1.6886286 , 0.5357047 , 0.22932841, -0.6094084 , 0.4515793 , -0.5900931 , 1.8684244 , -0.21056202, 0.29313338, -0.221067 , -0.9535679 , 0.07325 , -0.15823542, 1.1477109 , 0.6716076 , -1.0096023 , 0.10605699, 1.4148282 , 0.24576302, 0.5740349 , 0.19984631, 0.53964925, 0.41962907, 0.41497853, -1.0322098 , 0.01090925, 0.54345983, 0.806317 , 0.31737605, -0.7965337 , 0.9282971 , -0.8775608 , -0.26852605, -0.06743863, 0.42815775, -0.11774074, -0.17956367, 0.88813037, -0.46279573, -1.0841943 , -0.06798118, 0.4493006 , 0.71962464, -0.02876493, 1.0282255 , -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922, -0.16753489, 0.14065655, -1.8609751 , 0.03587054, 1.2792674 , 1.2732009 , -0.74120265, -0.98000383, 0.4521185 , -0.26387128, 0.37045383, 0.3680011 , 0.7197629 , -0.3570571 , 0.8016917 , 0.39243212, -0.5027844 , -1.2106236 , 0.6412354 , -0.878307 ], dtype=float32) \n2.3 co_occurrence_matrix 词共现矩阵\nimport cntext as ct documents = [\u0026#34;I go to school every day by bus .\u0026#34;, \u0026#34;i go to theatre every night by bus\u0026#34;] ct.co_occurrence_matrix(documents, window_size=2, lang=\u0026#39;english\u0026#39;) documents2 = [\u0026#34;编程很好玩\u0026#34;, \u0026#34;Python是最好学的编程\u0026#34;] ct.co_occurrence_matrix(documents2, window_size=2, lang=\u0026#39;chinese\u0026#39;) \n2.4 Glove 构建Glove词嵌入模型，使用英文数据data/brown_corpus.txt\nimport cntext as ct import os model = ct.Glove(cwd=os.getcwd(), lang=\u0026#39;english\u0026#39;) model.create_vocab(file=\u0026#39;data/brown_corpus.txt\u0026#39;, min_count=5) model.cooccurrence_matrix() model.train_embeddings(vector_size=50, max_iter=25) model.save() Run\nStep 1/4: ...Create vocabulary for Glove. Step 2/4: ...Create cooccurrence matrix. Step 3/4: ...Train glove embeddings. Note, this part takes a long time to run Step 3/4: ... Finish! Use 175.98 s 生成的Glove词嵌入文件位于output/Glove 。\n\n三、similarity 四种相似度计算函数\n cosine_sim(text1, text2) cos余弦相似 jaccard_sim(text1, text2) jaccard相似 minedit_sim(text1, text2) 最小编辑距离相似度； simple_sim(text1, text2) 更改变动算法  算法实现参考自 Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.\nimport cntext as ct text1 = \u0026#39;编程真好玩编程真好玩\u0026#39; text2 = \u0026#39;游戏真好玩编程真好玩啊\u0026#39; print(ct.cosine_sim(text1, text2)) print(ct.jaccard_sim(text1, text2)) print(ct.minedit_sim(text1, text2)) print(ct.simple_sim(text1, text2)) Run\n0.82 0.67 2.00 0.87 \n四、Text2Mind 词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。\n- 多个对象在某概念的远近，职业与性别，某个职业是否存在亲近男性，而排斥女性\n- 多个对象在某概念的分量(fen，一声)的多少， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆\n这两种认知分别可以用向量距离、向量语义投影计算得来。\n tm.sematic_distance(words, c_words1, c_words2) 向量距离 tm.sematic_projection(words, c_words1, c_words2) 向量语义投影  4.1 tm.sematic_distance(words, c_words1, c_words2) 分别计算words与c_words1、c_words2语义距离，返回距离差值。\n例如\nmale_concept = [\u0026#39;male\u0026#39;, \u0026#39;man\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;him\u0026#39;] female_concept = [\u0026#39;female\u0026#39;, \u0026#39;woman\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;her\u0026#39;] software_engineer_concept = [\u0026#39;engineer\u0026#39;, \u0026#39;programming\u0026#39;, \u0026#39;software\u0026#39;] d1 = distance(male_concept, software_engineer_concept) d2 = distance(female_concept, software_engineer_concept) 如果d1-d2\u0026lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。\n换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。\n下载glove_w2v.6B.100d.txt链接: https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw 提取码: 72l0\nimport cntext as ct #Note: this is a word2vec format model tm = ct.Text2Mind(w2v_model_path=\u0026#39;glove_w2v.6B.100d.txt\u0026#39;) engineer = [\u0026#39;program\u0026#39;, \u0026#39;software\u0026#39;, \u0026#39;computer\u0026#39;] mans = [\u0026#34;man\u0026#34;, \u0026#34;he\u0026#34;, \u0026#34;him\u0026#34;] womans = [\u0026#34;woman\u0026#34;, \u0026#34;she\u0026#34;, \u0026#34;her\u0026#34;] #在语义空间中，工程师更接近于男人，而不是女人。 #in semantic space, engineer is closer to man, other than woman. tm.sematic_distance(words=animals, c_words1=mans, c_words2=womans) Run\n-0.38 -0.38 意味着工程师更接近于男人，而不是女人。\n4.2 tm.sematic_projection(words, c_words1, c_words2) 语义投影，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。\n分值越大，word越位于c_words2一侧。\n下图是语义投影示例图，本文算法和图片均来自 \u0026ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Nature Human Behaviour, pp.1-13.\u0026rdquo;\n例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。\n以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。\nanimals = [\u0026#39;mouse\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;horse\u0026#39;, \u0026#39;pig\u0026#39;, \u0026#39;whale\u0026#39;] smalls = [\u0026#34;small\u0026#34;, \u0026#34;little\u0026#34;, \u0026#34;tiny\u0026#34;] bigs = [\u0026#34;large\u0026#34;, \u0026#34;big\u0026#34;, \u0026#34;huge\u0026#34;] # In size conception, mouse is smallest, horse is biggest. # 在大小概念上，老鼠最小，马是最大的。 tm.sematic_projection(words=animals, c_words1=smalls, c_words2=bigs) Run\n[(\u0026#39;mouse\u0026#39;, -1.68), (\u0026#39;cat\u0026#39;, -0.92), (\u0026#39;pig\u0026#39;, -0.46), (\u0026#39;whale\u0026#39;, -0.24), (\u0026#39;horse\u0026#39;, 0.4)] 在这几个动物尺寸的感知上，人类觉得老鼠体型是最小，马的体型是最大。\n\n引用说明 如果研究中使用cntext，请使用以下格式进行引用\napalike Deng X., Nan P. (2022). cntext: a Python tool for text mining (version 1.7.9). DOI: 10.5281/zenodo.7063523 URL: https://github.com/hiDaDeng/cntext bibtex @misc{YourReferenceHere, author = {Deng, Xudong and Nan, Peng}, doi = {10.5281/zenodo.7063523}, month = {9}, title = {cntext: a Python tool for text mining}, url = {https://github.com/hiDaDeng/cntext}, year = {2022} } endnote %0 Generic %A Deng, Xudong %A Nan, Peng %D 2022 %K text mining %K text analysi %K social science %K management science %K semantic analysis %R 10.5281/zenodo.7063523 %T cntext: a Python tool for text mining %U https://github.com/hiDaDeng/cntext \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/cntext_tutorial/","summary":"旧版cntext入口\n中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等\n github地址 https://github.com/hidadeng/cntext pypi地址 https://pypi.org/project/cntext/ 视频课-Python网络爬虫与文本数据分析  功能模块含\n stats 文本统计指标  词频统计 可读性 内置pkl词典 情感分析   dictionary 构建词表(典)  Sopmi 互信息扩充词典法 W2Vmodels 词向量扩充词典法 Glove Glove词嵌入模型   similarity 文本相似度  cos相似度 jaccard相似度 编辑距离相似度   mind.py 计算文本中的认知方向（态度、偏见）  代码下载 click to download\n安装 pip install cntext \nQuickStart import cntext as ct help(ct) Run\nHelp on package cntext: NAME cntext PACKAGE CONTENTS mind dictionary similarity stats","title":"cntext库 |  Python文本分析包更新"},{"content":" 内容摘自\n刘焕勇博客: https://liuhuanyong.github.io/\n原文地址: https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q\n公众号: 老刘说NLP\n word2vec、glove是两种静态的词向量模型，即每个词语只有一个固定的向量表示。但在不同语境中，词语的语义会发生变化，按道理词向量也应该动态调整。相比word2vec、glove生成的静态词向量， BERT是一种动态的技术，可以根据上下文情景，得到语义变化的词向量。\nHuggingFace网站提供了简易可用的数据集、丰富的预训练语言模型， 通过sentence-transformer库，我们可以使用HuggingFace内的预训练模型，得到不同情景的文本的语义向量。\nHuggingFace网站 https://huggingface.co/\n动态句向量 sentence-transformer框架提供了一种简便的方法来计算句子和段落的向量表示（也称为句子嵌入）\n安装 pip3 install -U sentence-transformers \n代码 click to download the code\n使用huggingface中的distiluse-base-multilingual-cased与训练模型，\nfrom sentence_transformers import SentenceTransformer, util model = SentenceTransformer(\u0026#39;distiluse-base-multilingual-cased\u0026#39;) 第一次运行上方的代码，需要运行一定的时间用于下载。下载完成后，我们使用同种语义的中英文句子，分别计算得到emb1和emb2两个句向量\nemb1 = model.encode(\u0026#39;Natural language processing is a hard task for human\u0026#39;) emb2 = model.encode(\u0026#39;自然语言处理对于人类来说是个困难的任务\u0026#39;) emb1 Run\narray([ 2.58186590e-02, 4.65703346e-02, 4.25276496e-02, -1.67875513e-02, 5.56012690e-02, -3.44308838e-02, -6.53978735e-02, 1.77450478e-02, -3.47155109e-02, 2.86140274e-02, 2.48657260e-02, 7.94188876e-04, 5.09755425e-02, -1.76107027e-02, -1.04308855e-02, 7.61642214e-03, ... 4.28482369e-02, 1.76657233e-02, -5.83355911e-02, 1.92921527e-03, 2.81221420e-02, 5.24400780e-03, 2.10703332e-02, 7.96715263e-03, -6.80630878e-02, -2.05304120e-02, -2.43293475e-02, -1.87458862e-02], dtype=float32) 在distiluse-base-multilingual-cased这种模型中， 不同语言的同义句应该具有类似的语义，那么cos相似度应该是很大的。越接近于1越相似；越接近于0，越不相似。\ncos_sim = util.pytorch_cos_sim(emb1, emb2) cos_sim Run\ntensor([[0.8960]]) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/sentence-transformer-tutorial/","summary":"内容摘自\n刘焕勇博客: https://liuhuanyong.github.io/\n原文地址: https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q\n公众号: 老刘说NLP\n word2vec、glove是两种静态的词向量模型，即每个词语只有一个固定的向量表示。但在不同语境中，词语的语义会发生变化，按道理词向量也应该动态调整。相比word2vec、glove生成的静态词向量， BERT是一种动态的技术，可以根据上下文情景，得到语义变化的词向量。\nHuggingFace网站提供了简易可用的数据集、丰富的预训练语言模型， 通过sentence-transformer库，我们可以使用HuggingFace内的预训练模型，得到不同情景的文本的语义向量。\nHuggingFace网站 https://huggingface.co/\n动态句向量 sentence-transformer框架提供了一种简便的方法来计算句子和段落的向量表示（也称为句子嵌入）\n安装 pip3 install -U sentence-transformers \n代码 click to download the code\n使用huggingface中的distiluse-base-multilingual-cased与训练模型，\nfrom sentence_transformers import SentenceTransformer, util model = SentenceTransformer(\u0026#39;distiluse-base-multilingual-cased\u0026#39;) 第一次运行上方的代码，需要运行一定的时间用于下载。下载完成后，我们使用同种语义的中英文句子，分别计算得到emb1和emb2两个句向量\nemb1 = model.encode(\u0026#39;Natural language processing is a hard task for human\u0026#39;) emb2 = model.encode(\u0026#39;自然语言处理对于人类来说是个困难的任务\u0026#39;) emb1 Run\narray([ 2.58186590e-02, 4.65703346e-02, 4.25276496e-02, -1.67875513e-02, 5.56012690e-02, -3.44308838e-02, -6.53978735e-02, 1.77450478e-02, -3.47155109e-02, 2.86140274e-02, 2.48657260e-02, 7.94188876e-04, 5.09755425e-02, -1.76107027e-02, -1.","title":"sentence-transformer库 | 句子语义向量化"},{"content":"使用pyecharts绘制社交网络关系图，直接上代码。\n代码 点击下载代码\nbase from pyecharts import options as opts from pyecharts.charts import Graph nodes = [ {\u0026#34;name\u0026#34;: \u0026#34;结点1\u0026#34;, \u0026#34;symbolSize\u0026#34;: 10}, {\u0026#34;name\u0026#34;: \u0026#34;结点2\u0026#34;, \u0026#34;symbolSize\u0026#34;: 20}, {\u0026#34;name\u0026#34;: \u0026#34;结点3\u0026#34;, \u0026#34;symbolSize\u0026#34;: 30}, {\u0026#34;name\u0026#34;: \u0026#34;结点4\u0026#34;, \u0026#34;symbolSize\u0026#34;: 40}, {\u0026#34;name\u0026#34;: \u0026#34;结点5\u0026#34;, \u0026#34;symbolSize\u0026#34;: 50}, {\u0026#34;name\u0026#34;: \u0026#34;结点6\u0026#34;, \u0026#34;symbolSize\u0026#34;: 40}, {\u0026#34;name\u0026#34;: \u0026#34;结点7\u0026#34;, \u0026#34;symbolSize\u0026#34;: 30}, {\u0026#34;name\u0026#34;: \u0026#34;结点8\u0026#34;, \u0026#34;symbolSize\u0026#34;: 20}, ] links = [] for i in nodes: for j in nodes: links.append({\u0026#34;source\u0026#34;: i.get(\u0026#34;name\u0026#34;), \u0026#34;target\u0026#34;: j.get(\u0026#34;name\u0026#34;)}) ( Graph() .add(\u0026#34;\u0026#34;, nodes, links, repulsion=8000) .set_global_opts(title_opts=opts.TitleOpts(title=\u0026#34;Graph-基本示例\u0026#34;)) .render(\u0026#34;graph_base.html\u0026#34;) ) Run\ngraph_base\nweibo import json from pyecharts import options as opts from pyecharts.charts import Graph with open(\u0026#34;data/weibo.json\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: j = json.load(f) nodes, links, categories, cont, mid, userl = j ( Graph() .add( \u0026#34;\u0026#34;, nodes, links, categories, repulsion=50, linestyle_opts=opts.LineStyleOpts(curve=0.2), label_opts=opts.LabelOpts(is_show=False), ) .set_global_opts( legend_opts=opts.LegendOpts(is_show=False), title_opts=opts.TitleOpts(title=\u0026#34;Graph-微博转发关系图\u0026#34;), ) .render(\u0026#34;graph_weibo.html\u0026#34;) ) Run\ngraph_weibo\nnpm import pyecharts.options as opts from pyecharts.charts import Graph \u0026#34;\u0026#34;\u0026#34; Gallery 使用 pyecharts 1.1.0 参考地址: https://echarts.apache.org/examples/editor.html?c=graph-npm 目前无法实现的功能: 1、暂无 \u0026#34;\u0026#34;\u0026#34; with open(\u0026#34;data/npmdepgraph.min10.json\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: data = json.load(f) nodes = [ { \u0026#34;x\u0026#34;: node[\u0026#34;x\u0026#34;], \u0026#34;y\u0026#34;: node[\u0026#34;y\u0026#34;], \u0026#34;id\u0026#34;: node[\u0026#34;id\u0026#34;], \u0026#34;name\u0026#34;: node[\u0026#34;label\u0026#34;], \u0026#34;symbolSize\u0026#34;: node[\u0026#34;size\u0026#34;], \u0026#34;itemStyle\u0026#34;: {\u0026#34;normal\u0026#34;: {\u0026#34;color\u0026#34;: node[\u0026#34;color\u0026#34;]}}, } for node in data[\u0026#34;nodes\u0026#34;] ] edges = [ {\u0026#34;source\u0026#34;: edge[\u0026#34;sourceID\u0026#34;], \u0026#34;target\u0026#34;: edge[\u0026#34;targetID\u0026#34;]} for edge in data[\u0026#34;edges\u0026#34;] ] ( Graph(init_opts=opts.InitOpts(width=\u0026#34;1600px\u0026#34;, height=\u0026#34;800px\u0026#34;)) .add( series_name=\u0026#34;\u0026#34;, nodes=nodes, links=edges, layout=\u0026#34;none\u0026#34;, is_roam=True, is_focusnode=True, label_opts=opts.LabelOpts(is_show=False), linestyle_opts=opts.LineStyleOpts(width=0.5, curve=0.3, opacity=0.7), ) .set_global_opts(title_opts=opts.TitleOpts(title=\u0026#34;NPM Dependencies\u0026#34;)) .render(\u0026#34;npm_dependencies.html\u0026#34;) ) Run\nnpm_dependencies\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pyecharts_graph_vis/","summary":"使用pyecharts绘制社交网络关系图，直接上代码。\n代码 点击下载代码\nbase from pyecharts import options as opts from pyecharts.charts import Graph nodes = [ {\u0026#34;name\u0026#34;: \u0026#34;结点1\u0026#34;, \u0026#34;symbolSize\u0026#34;: 10}, {\u0026#34;name\u0026#34;: \u0026#34;结点2\u0026#34;, \u0026#34;symbolSize\u0026#34;: 20}, {\u0026#34;name\u0026#34;: \u0026#34;结点3\u0026#34;, \u0026#34;symbolSize\u0026#34;: 30}, {\u0026#34;name\u0026#34;: \u0026#34;结点4\u0026#34;, \u0026#34;symbolSize\u0026#34;: 40}, {\u0026#34;name\u0026#34;: \u0026#34;结点5\u0026#34;, \u0026#34;symbolSize\u0026#34;: 50}, {\u0026#34;name\u0026#34;: \u0026#34;结点6\u0026#34;, \u0026#34;symbolSize\u0026#34;: 40}, {\u0026#34;name\u0026#34;: \u0026#34;结点7\u0026#34;, \u0026#34;symbolSize\u0026#34;: 30}, {\u0026#34;name\u0026#34;: \u0026#34;结点8\u0026#34;, \u0026#34;symbolSize\u0026#34;: 20}, ] links = [] for i in nodes: for j in nodes: links.append({\u0026#34;source\u0026#34;: i.get(\u0026#34;name\u0026#34;), \u0026#34;target\u0026#34;: j.get(\u0026#34;name\u0026#34;)}) ( Graph() .add(\u0026#34;\u0026#34;, nodes, links, repulsion=8000) .","title":"PyEcharts库 | 绘制社交关系网络图"},{"content":"Simple Transformers 库基于 HuggingFace 的 Transformers 库，可让您快速训练和评估 Transformer 模型， 初始化、训练和评估模型只需要 3 行代码。\n安装 pip3 install simpletransformers Simple Transformer 模型在构建时考虑了特定的自然语言处理 (NLP) 任务。 每个这样的模型都配备了旨在最适合它们打算执行的任务的特性和功能。 使用 Simple Transformers 模型的高级过程遵循相同的模式。\n 初始化一个特定于任务的模型 2.用train_model()训练模型 使用 eval_model() 评估模型 使用 predict() 对（未标记的）数据进行预测  但是，不同模型之间存在必要的差异，以确保它们非常适合其预期任务。 关键差异通常是输入/输出数据格式和任何任务特定功能/配置选项的差异。 这些都可以在每个任务的文档部分中找到。\n当前实现的特定于任务的“Simple Transformer”模型及其任务如下所示。\n   Task Model     Binary and multi-class text classification文本二分类、多分类 ClassificationModel   Conversational AI (chatbot training)对话机器人训练 ConvAIModel   Language generation语言生成 LanguageGenerationModel   Language model training/fine-tuning语言模型训练、微调 LanguageModelingModel   Multi-label text classification多类别文本分类 MultiLabelClassificationModel   Multi-modal classification (text and image data combined)多模态分类 MultiModalClassificationModel   Named entity recognition命名实体识别 NERModel   Question answering问答 QuestionAnsweringModel   Regression回归 ClassificationModel   Sentence-pair classification句对分类 ClassificationModel   Text Representation Generation文本表征生成 RepresentationModel   Document Retrieval文档抽取 RetrievalModel     有关如何使用这些模型的更多信息，请参阅 docs 中的相关部分。 示例脚本可以在 examples 目录中找到。 有关项目的最新更改，请参阅 Changelog。  生成句子嵌入 使用huggingface网站https://huggingface.co/ 提供的模型\n 英文模型 bert-base-uncased 中文模型 bert-base-chinese  from simpletransformers.language_representation import RepresentationModel sentences = [\u0026#34;Machine Learning and Deep Learning are part of AI\u0026#34;, \u0026#34;Data Science will excel in future\u0026#34;] #it should always be a list model = RepresentationModel( model_type=\u0026#34;bert\u0026#34;, model_name=\u0026#34;bert-base-uncased\u0026#34;, #英文模型 use_cuda=False) sentence_vectors = model.encode_sentences(sentences, combine_strategy=\u0026#34;mean\u0026#34;) print(sentence_vectors.shape) print(sentence_vectors) Run\n(2, 768) array([[-0.10800573, 0.19615649, -0.10756102, ..., -0.26362818, 0.56403756, -0.30985302], [ 0.0201617 , -0.19381572, 0.4360792 , ..., -0.2979438 , 0.04984972, -0.702381 ]], dtype=float32) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/simple_transformer/","summary":"Simple Transformers 库基于 HuggingFace 的 Transformers 库，可让您快速训练和评估 Transformer 模型， 初始化、训练和评估模型只需要 3 行代码。\n安装 pip3 install simpletransformers Simple Transformer 模型在构建时考虑了特定的自然语言处理 (NLP) 任务。 每个这样的模型都配备了旨在最适合它们打算执行的任务的特性和功能。 使用 Simple Transformers 模型的高级过程遵循相同的模式。\n 初始化一个特定于任务的模型 2.用train_model()训练模型 使用 eval_model() 评估模型 使用 predict() 对（未标记的）数据进行预测  但是，不同模型之间存在必要的差异，以确保它们非常适合其预期任务。 关键差异通常是输入/输出数据格式和任何任务特定功能/配置选项的差异。 这些都可以在每个任务的文档部分中找到。\n当前实现的特定于任务的“Simple Transformer”模型及其任务如下所示。\n   Task Model     Binary and multi-class text classification文本二分类、多分类 ClassificationModel   Conversational AI (chatbot training)对话机器人训练 ConvAIModel   Language generation语言生成 LanguageGenerationModel   Language model training/fine-tuning语言模型训练、微调 LanguageModelingModel   Multi-label text classification多类别文本分类 MultiLabelClassificationModel   Multi-modal classification (text and image data combined)多模态分类 MultiModalClassificationModel   Named entity recognition命名实体识别 NERModel   Question answering问答 QuestionAnsweringModel   Regression回归 ClassificationModel   Sentence-pair classification句对分类 ClassificationModel   Text Representation Generation文本表征生成 RepresentationModel   Document Retrieval文档抽取 RetrievalModel     有关如何使用这些模型的更多信息，请参阅 docs 中的相关部分。 示例脚本可以在 examples 目录中找到。 有关项目的最新更改，请参阅 Changelog。  生成句子嵌入 使用huggingface网站https://huggingface.","title":"SimpleTransformers库 | 使用BERT实现文本向量化"},{"content":"geopy geopy 是几个流行的地理编码网络服务的 Python 客户端。\ngeopy 使 Python 开发人员可以使用第三方地理编码器和其他数据源轻松定位全球地址、城市、国家和地标的坐标。\ngeopy 包括用于 OpenStreetMap Nominatim、Google Geocoding API (V3) 的地理编码器类和许多 其他地理编码服务。 完整列表可在 Geocoders 文档部分 上找到。 地理编码器类位于 geopy.geocoders。\ngeopy 针对 CPython（版本 3.5、3.6、3.7、3.8、3.9）和 PyPy3 进行了测试。 geopy 1.x 系列还支持 CPython 2.7、3.4 和 PyPy2。\n安装 pip3 install geopy Geocoding 要将查询地理定位到地址和坐标：\n首先需要在https://www.openstreetmap.org/注册账号，注册一个app名。注册好的app名用于填充user_agent\nfrom geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=\u0026#34;specify_your_app_name_here\u0026#34;) location = geolocator.geocode(\u0026#34;175 5th Avenue NYC\u0026#34;) print(location.address) print((location.latitude, location.longitude)) print(location.raw) Run\nFlatiron Building, 175, 5th Avenue, Flatiron, New York, NYC, New York, ... (40.7410861, -73.9896297241625) {\u0026#39;place_id\u0026#39;: \u0026#39;9167009604\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;attraction\u0026#39;, ...} \n为了找到地址对应的经纬度\nfrom geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=\u0026#34;specify_your_app_name_here\u0026#34;) location = geolocator.reverse(\u0026#34;52.509669, 13.376294\u0026#34;) print(location.address) print((location.latitude, location.longitude)) print(location.raw) Run\nPotsdamer Platz, Mitte, Berlin, 10117, Deutschland, European Union (52.5094982, 13.3765983) {\u0026#39;place_id\u0026#39;: \u0026#39;654513\u0026#39;, \u0026#39;osm_type\u0026#39;: \u0026#39;node\u0026#39;, ...} \n测量距离 Geopy 可以使用 geodesic distance 或 great-circle distance 计算两点之间的测地线距离 Great-circle_distance），默认的测地线距离可用作函数 geopy.distance.distance。\n这是测地线距离的示例用法，采用一对 (lat, lon) 元组：\nfrom geopy.distance import geodesic newport_ri = (41.49008, -71.312796) cleveland_oh = (41.499498, -81.695391) print(geodesic(newport_ri, cleveland_oh).miles) Run\n538.390445368 \n使用great-cricle距离算法，同时采用一对 (lat, lon) 元组：\nfrom geopy.distance import great_circle newport_ri = (41.49008, -71.312796) cleveland_oh = (41.499498, -81.695391) print(great_circle(newport_ri, cleveland_oh).miles) Run\n536.997990696 \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/geopy_distance_calculate/","summary":"geopy geopy 是几个流行的地理编码网络服务的 Python 客户端。\ngeopy 使 Python 开发人员可以使用第三方地理编码器和其他数据源轻松定位全球地址、城市、国家和地标的坐标。\ngeopy 包括用于 OpenStreetMap Nominatim、Google Geocoding API (V3) 的地理编码器类和许多 其他地理编码服务。 完整列表可在 Geocoders 文档部分 上找到。 地理编码器类位于 geopy.geocoders。\ngeopy 针对 CPython（版本 3.5、3.6、3.7、3.8、3.9）和 PyPy3 进行了测试。 geopy 1.x 系列还支持 CPython 2.7、3.4 和 PyPy2。\n安装 pip3 install geopy Geocoding 要将查询地理定位到地址和坐标：\n首先需要在https://www.openstreetmap.org/注册账号，注册一个app名。注册好的app名用于填充user_agent\nfrom geopy.geocoders import Nominatim geolocator = Nominatim(user_agent=\u0026#34;specify_your_app_name_here\u0026#34;) location = geolocator.geocode(\u0026#34;175 5th Avenue NYC\u0026#34;) print(location.address) print((location.latitude, location.longitude)) print(location.raw) Run\nFlatiron Building, 175, 5th Avenue, Flatiron, New York, NYC, New York, .","title":"geopy库 | 地理编码计算距离"},{"content":"Taxi GPS数据处理 B站-同济小旭学长\n 代码下载 download the code\n在这个例子中，我们将介绍如何使用 TransBigData 包来高效处理 Taxi GPS 数据。首先，导入 TransBigData 并使用 pandas 读取数据\n!pip3 install mapclassify==2.4.3 !pip3 install -U transbigdata==0.4.7 !pip3 install keplergl==0.3.2 import transbigdata as tbd import pandas as pd import geopandas as gpd import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) # Read data data = pd.read_csv(\u0026#39;data/TaxiData-Sample.csv\u0026#39;, header=None) data.columns = [\u0026#39;VehicleNum\u0026#39;, \u0026#39;Time\u0026#39;, \u0026#39;Lng\u0026#39;, \u0026#39;Lat\u0026#39;, \u0026#39;OpenStatus\u0026#39;, \u0026#39;Speed\u0026#39;] data.head() # Read the GeoDataFrame of the study area sz = gpd.read_file(r\u0026#39;data/sz.json\u0026#39;) sz.crs = None sz.head() fig = plt.figure(1, (8, 3), dpi=150) ax1 = plt.subplot(111) sz.plot(ax=ax1) plt.xticks([], fontsize=10) plt.yticks([], fontsize=10); 数据预处理 TransBigData 集成了几种常用的数据预处理方法。 使用tbd.clean_outofshape方法，给定研究区域的数据和GeoDataFrame，可以删除研究区域外的数据。 tbd.clean_taxi_status 方法可以过滤掉乘客状态（OpenStatus）瞬时变化的数据。 使用预处理方式时，需要将对应的列名作为参数传入：\n# Data Preprocessing # Delete the data outside of the study area data = tbd.clean_outofshape(data, sz, col=[\u0026#39;Lng\u0026#39;, \u0026#39;Lat\u0026#39;], accuracy=500) # Delete the data with instantaneous changes in passenger status data = tbd.clean_taxi_status(data, col=[\u0026#39;VehicleNum\u0026#39;, \u0026#39;Time\u0026#39;, \u0026#39;OpenStatus\u0026#39;]) \n数据网格化 数据分布最基本的表达方式是地理网格的形式；\n数据网格化后，每个 GPS 数据点映射到相应的网格。 对于数据网格化，首先需要确定网格化参数（可以理解为定义一个网格坐标系）：\n# 数据网格化 # 定义边界并生成网格参数（经纬度） bounds = [113.6, 22.4, 114.8, 22.9] params = tbd.area_to_params(bounds, accuracy=500) print(params) {'slon': 113.6, 'slat': 22.4, 'deltalon': 0.004872390756896538, 'deltalat': 0.004496605206422906, 'theta': 0, 'method': 'rect', 'gridsize': 500}  获得网格化参数后，下一步就是将 GPS 映射到其对应的网格上。\n使用 tbd.GPS_to_grids，它将生成 LONCOL 列和 LATCOL 列。 两列一起可以指定一个网格：\n# Mapping GPS data to grids data[\u0026#39;LONCOL\u0026#39;], data[\u0026#39;LATCOL\u0026#39;] = tbd.GPS_to_grid(data[\u0026#39;Lng\u0026#39;], data[\u0026#39;Lat\u0026#39;], params) data.head() 计算每个网格中的数据量：\n# Aggregate data into grids datatest = data.groupby([\u0026#39;LONCOL\u0026#39;, \u0026#39;LATCOL\u0026#39;])[\u0026#39;VehicleNum\u0026#39;].count().reset_index() datatest.head() 将生成网格的geometry并将其转换为 GeoDataFrame\n# Generate the geometry for grids datatest[\u0026#39;geometry\u0026#39;] = tbd.grid_to_polygon([datatest[\u0026#39;LONCOL\u0026#39;], datatest[\u0026#39;LATCOL\u0026#39;]], params) # Change it into GeoDataFrame # import geopandas as gpd datatest = gpd.GeoDataFrame(datatest) datatest.head() 绘制生成的网格：\n# Plot the grids fig = plt.figure(1, (16, 6), dpi=300) ax1 = plt.subplot(111) # tbd.plot_map(plt, bounds, zoom=10, style=4) datatest.plot(ax=ax1, column=\u0026#39;VehicleNum\u0026#39;, legend=True) plt.xticks([], fontsize=10) plt.yticks([], fontsize=10) plt.title(\u0026#39;Counting of Taxi GPS Trajectory Points\u0026#39;, fontsize=12); ​ ​\n# Plot the grids fig = plt.figure(1, (16, 6), dpi=300) # 确定图形高为6，宽为8；图形清晰度 ax1 = plt.subplot(111) datatest.plot(ax=ax1, column=\u0026#39;VehicleNum\u0026#39;, legend=True, scheme=\u0026#39;quantiles\u0026#39;) # plt.legend(fontsize=10) plt.xticks([], fontsize=10) plt.yticks([], fontsize=10) plt.title(\u0026#39;Counting of Taxi GPS Trajectory Points\u0026#39;, fontsize=12); ​ ​\n# Plot the grids fig = plt.figure(1, (16, 6), dpi=150) # 确定图形高为6，宽为8；图形清晰度 ax1 = plt.subplot(111) datatest.plot(ax=ax1, column=\u0026#39;VehicleNum\u0026#39;, legend=True, cmap=\u0026#39;OrRd\u0026#39;, scheme=\u0026#39;quantiles\u0026#39;) # plt.legend(fontsize=10) plt.xticks([], fontsize=10) plt.yticks([], fontsize=10) plt.title(\u0026#39;Counting of Taxi GPS Trajectory Points\u0026#39;, fontsize=12); ​ ​\nOrigin-destination(OD) 提取和汇总出租车行程 使用 tbd.taxigps_to_od 方法并传入对应的列名来提取出租车行程 OD：\n# Extract taxi OD from GPS data oddata = tbd.taxigps_to_od(data,col = [\u0026#39;VehicleNum\u0026#39;, \u0026#39;Time\u0026#39;, \u0026#39;Lng\u0026#39;, \u0026#39;Lat\u0026#39;, \u0026#39;OpenStatus\u0026#39;]) oddata 聚合提取的 OD 并生成 LineString GeoDataFrame\n# Gridding and aggragate data od_gdf = tbd.odagg_grid(oddata, params) od_gdf.head() # Plot the grids fig = plt.figure(1, (16, 6), dpi=150) # 确定图形高为6，宽为8；图形清晰度 ax1 = plt.subplot(111) # data_grid_count.plot(ax=ax1, column=\u0026#39;VehicleNum\u0026#39;, legend=True, cmap=\u0026#39;OrRd\u0026#39;, scheme=\u0026#39;quantiles\u0026#39;) od_gdf.plot(ax=ax1, column=\u0026#39;count\u0026#39;, legend=True, scheme=\u0026#39;quantiles\u0026#39;) plt.xticks([], fontsize=10) plt.yticks([], fontsize=10) plt.title(\u0026#39;OD Trips\u0026#39;, fontsize=12); ​ ​\n将 OD 聚合成多边形 TransBigData 还提供了将 OD 聚合成多边形的方法\n# Aggragate OD data to polygons  # without passing gridding parameters, the algorithm will map the data  # to polygons directly using their coordinates od_gdf = tbd.odagg_shape(oddata, sz, round_accuracy=6) fig = plt.figure(1, (16, 6), dpi=150) # 确定图形高为6，宽为8；图形清晰度 ax1 = plt.subplot(111) od_gdf.plot(ax=ax1, column=\u0026#39;count\u0026#39;) plt.xticks([], fontsize=10) plt.yticks([], fontsize=10) plt.title(\u0026#39;OD Trips\u0026#39;, fontsize=12); 基于 Matplotlib 的地图绘制 TransBigData 还在 matplotlib 中提供底图加载。 在使用此方法之前，您需要设置您的 mapboxtoken 和底图的存储位置，请参见：此链接。tbd.plot_map 添加底图和 tbd.plotscale 以添加比例和指南针：\n# Create figure fig = plt.figure(1, (10, 10), dpi=300) ax = plt.subplot(111) plt.sca(ax) # Load basemap tbd.plot_map(plt, bounds, zoom=12, style=4) # Define an ax for colorbar cax = plt.axes([0.05, 0.33, 0.02, 0.3]) plt.title(\u0026#39;OD\\nMatrix\u0026#39;) plt.sca(ax) # Plot the OD od_gdf.plot(ax=ax, vmax=100, column=\u0026#39;count\u0026#39;, cax=cax, legend=True) # Plot the polygons sz.plot(ax=ax, edgecolor=(0, 0, 0, 1), facecolor=(0, 0, 0, 0.2), linewidths=0.5) # Add compass and scale tbd.plotscale(ax, bounds=bounds, textsize=10, compasssize=1, accuracy=2000, rect=[0.06, 0.03], zorder=10) plt.axis(\u0026#39;off\u0026#39;) plt.xlim(bounds[0], bounds[2]) plt.ylim(bounds[1], bounds[3]) plt.show() ​ ​\n## 提取出租车轨迹 使用`the.taxigps_traj_point`方法，输入GPS数据和OD数据，可以提取轨迹点 data_deliver, data_idle = tbd.taxigps_traj_point(data,oddata,col=[\u0026#39;VehicleNum\u0026#39;, \u0026#39;Time\u0026#39;, \u0026#39;Lng\u0026#39;, \u0026#39;Lat\u0026#39;, \u0026#39;OpenStatus\u0026#39;]) data_deliver.head() data_idle.head() 生成轨迹图\ntraj_deliver = tbd.points_to_traj(data_deliver) traj_deliver.plot(); ​ ​\ntraj_idle = tbd.points_to_traj(data_idle[data_idle[\u0026#39;OpenStatus\u0026#39;] == 0]) traj_idle.plot() ​ ​\n轨迹可视化Trajectories visualization TransBigData 的内置可视化功能利用了可视化包 keplergl ，使用简单代码即可在 Jupyter notebook 上交互式地显示轨迹数据。\n要使用此方法，请为 python 安装 keplergl 包：\n pip3 install keplergl\n 安装后，需要按照 link 配置jupyter notebook\nVisualization of trajectory data:\ntbd.visualization_trip(data_deliver) Processing trajectory data... Generate visualization... User Guide: https://docs.kepler.gl/docs/keplergl-jupyter  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/trans_big_data_examples_taxi_data/","summary":"Taxi GPS数据处理 B站-同济小旭学长\n 代码下载 download the code\n在这个例子中，我们将介绍如何使用 TransBigData 包来高效处理 Taxi GPS 数据。首先，导入 TransBigData 并使用 pandas 读取数据\n!pip3 install mapclassify==2.4.3 !pip3 install -U transbigdata==0.4.7 !pip3 install keplergl==0.3.2 import transbigdata as tbd import pandas as pd import geopandas as gpd import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) # Read data data = pd.read_csv(\u0026#39;data/TaxiData-Sample.csv\u0026#39;, header=None) data.columns = [\u0026#39;VehicleNum\u0026#39;, \u0026#39;Time\u0026#39;, \u0026#39;Lng\u0026#39;, \u0026#39;Lat\u0026#39;, \u0026#39;OpenStatus\u0026#39;, \u0026#39;Speed\u0026#39;] data.head() # Read the GeoDataFrame of the study area sz = gpd.","title":"TransBigData库 | 出租车GPS数据处理"},{"content":"B站看到大牛刘知远关于文本分析在计算社会科学领域应用的分享，解答了我对文本表示的疑惑，看完了能对文本的特征工程加深理解，同时也能更清晰未来如何借助计算机科学技术开展社会科学研究。\n 全文摘抄自\nChen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.\n  摘要 计算社会科学（CSS），旨在利用计算方法来解决社会科学问题，是一个新兴和快速发展的领域。 CSS 的研究是数据驱动的，并且显着受益于在线用户生成内容和社交网络的可用性，其中包含用于调查的富文本和网络数据。然而，这些大规模、多模态的数据也给研究人员带来了很大的挑战：如何有效地表示数据以挖掘我们想要的 CSS 含义？为了探索答案，我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示，并为每个方案介绍了一系列典型的方法。随后，我们基于对来自 6 个涉及 CSS 的顶级场所的 400 多篇研究文章的调查，展示了上述表示的应用。从这些应用程序的统计数据中，我们挖掘出每种表示的强度，并发现基于嵌入的表示在过去十年中出现并获得越来越多的关注的趋势。最后，我们讨论了几个关键挑战和未来方向的开放性问题。本调查旨在为 CSS 研究人员提供对数据表示的更深入理解和更明智的应用。\n关键词：计算社会科学；基于符号的表示；基于嵌入的表示；社交网络\n一、计算社会学数据分析流程 其中第二步，数据表示目前有两大类表示(特征工程)方法\n 基于符号的文本表示；符号可以是单词(或词组)，也可以是概念(如正面情感、负面情感) 基于嵌入(分布式)的文本表示；相比于符号法，将词(词组)看做一个点。嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(king)-v(queen)约等于v(man)-v(woman)  二、基于符号的文本表示 基于符号的文本表示一般来说默认词语是不可分的符号，每个词能根据词频统计出现次数的多与少，或是否存在。\n2.1 词语层面   基于词频表示\n 是否出现，出现标位1，反之标位0。 出现多少，词语出现几次，标为几个。    基于特征表示，如每个词带有权重(得分)\n  基于网络表示，如词语共现网络(矩阵)\n  2.2 句子层面   基于词频的表示\n one-hot 将文本转为向量，向量中每个数，词语出现标位1，反之标位0 bag-of-words，将文本转为向量，向量中每个数，词语出现n次标记为n n-grams，对词组的处理，将词组看做一个单词(整体)。 Tf-Idf ,该算法分为tf和idf两部分。其中tf与bag-of-words类似，考虑词语出现次数。而idf还考虑词语在语料中出现场景的稀缺性程度。    基于语法特征，如句法依存关系，类似于英语语法，将句子分为主谓宾、动词、名词等。\n  词典法，如使用正、负情感词典，对文本数据进行情感分析，可以得到pos和neg的各自得分\n  三、基于嵌入的文本表示 3.1词语层面 嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(best)-v(good)约等于v(worst)-v(bad)\n3.2 句子层面 词语是向量，那么由词语组成的句子也会加权得到一个向量。含有相似话题或含义相近的句子在多维向量空间中会比较接近。\n四、任务分类：文本的用法 有了文本数据，刚刚解决了如何表示文本。接下来，需要明确，我们使用文本目的是为了做哪类分析，得到哪些信息。有8种常见的文本分析图式\n 描述性。如随时间推移，词频的发展趋势是变大的 相关性。 聚类。如lda话题分析、k-means聚类 相似度。两个文档转为向量后，可以通过cosine计算相似度 分类。机器学习分类，判断某文本隶属于哪个类别 回归。例如根据文本，判断某件事发生的概率 语言模型。 排序。  五、发文趋势-符号vs嵌入 基于上一节中对应用程序的介绍，可以观察到基于符号和基于嵌入的表示在 计算社会科学中都得到了相当大的采用。为了明确研究它们的覆盖范围，我们计算了每年使用两种表示中的一种或两种的作品数量，如图 17 所示。通过比较nature、science、pnas三大顶级期刊，我们可以发现使用基于嵌入表示的文章比例在过去几年中逐渐。这表明越来越多的 计算社会科学文章 已经考虑并受益于基于嵌入表示。\n图 18 显示了在 计算机领域ACL、WWW 和 KDD 的会议上中，发现使用基于嵌入的表示的文章数量已大大超过使用基于符号的表示的文章数量。然而，与图 17 相比，计算机科学会议中基于嵌入的表示的数量与三个多学科期刊之间存在很大差距。\n总而言之，在过去十年中，基于嵌入的表示已经出现并在 计算社会科学 中发挥着越来越重要的作用。\n六、趋势解读 基于它们的内部机制和现有应用，对趋势解读，我们总结出以下三个关键点。\n基于符号的表示因其明确性和可解释性而擅长描述和关系的任务。\n基于符号的表示中的每个值都表示一定的人类可读的含义，因此我们可以直接使用它来观察数据的分布，以及提取对象之间的关系。例如，基于频率的词表示用于观察文化变化并捕捉新闻中提及次数与公司股票交易量之间的关系。虽然基于主题模型的表示和一些基于神经的表示在一定程度上具有实际意义，但它们对于社会科学研究人员来说仍然是模糊的并且不那么引人注目。\n由于神经网络具有强大的拟合数据和提取深度语义的能力，基于嵌入的表示在预测（例如分类和回归）和相似性任务中表现更好。一方面，神经网络通过大规模神经元的连接实现高效的输入输出映射功能。另一方面，通过多层网络的构建，实现深层语义和抽象概念的提取。现有研究表明，深层捕获相对于浅层更抽象的特征。诸如社会偏见和道德化之类的抽象概念都可以通过基于嵌入的表示来很好地衡量。虽然我们提到基于符号的表示可以通过一些定义的符号来代表抽象概念，但这种表示仍然是部分和肤浅的，很难捕捉到它们的全貌。\n基于嵌入的表示需要更少的人力。基于符号的表示通常需要大量的专家知识来定义研究对象的特征，这是劳动密集型的。此外，对于一些没有充分特征的抽象概念或对象，它们的表现将受到限制。与它们不同的是，基于嵌入的表示是从数据中自动提取的，几乎不需要人工干预，甚至可以补充人类知识。例如，可以使用神经网络来自动恢复丢失的巴比伦文本，这即使对专家来说也是具有挑战性的。此外，基于嵌入的表示可以在没有手动定义的情况下描述语言的复杂性和歧义性。\n七、未来展望 尽管在过去十年中出现了从符号到嵌入的趋势，但仍有许多挑战和悬而未决的问题有待探索。展望未来，我们列出了一些与计算社会科学 中的数据表示相关的基本和潜在的未来方向。\n预训练的语言模型。近年来，预训练的语言模型受到了相当大的关注，并在处理文本数据方面取得了巨大的成功 [100, 240]。这些模型从百科全书和书籍等海量文本数据中学习丰富的语义信息，仅在下游任务中进行微调以实现有效的基于嵌入的表示。因此，对于 计算社会科学，我们可以借助预训练的语言模型获得更通用、更健壮的文本表示。与从传统神经网络模型中学习的表示相比，这些表示不仅可以更广泛、更准确地从文本中分析社会现象，而且还可以减少那些需要大量标记数据的任务的人工注释。\n图神经网络。通过消息传递机制，图神经网络 [461] 可以同时有效地对网络拓扑和节点/边缘特征（例如文本信息）进行建模，从而提供一个统一的框架来利用来自异构来源的信息。 计算社会科学 中的许多场景需要处理社交网络以及个人特征。因此，图神经网络技术在 计算社会科学 研究中具有很大的应用潜力，可以学习融合文本和网络信息的表示。事实上，计算机科学中的各种应用，例如自然语言处理 [418] 和推荐系统 [439]，已经采用图神经网络进行建模。\n设计为预测和相似性。基于嵌入的表示以丰富和深层次的语义而闻名，而基于符号的表示通常保留在部分和浅层语义中。同时，基于嵌入的表示擅长预测和相似性的任务。因此，为了充分利用嵌入中的强语义，鼓励 计算社会科学 研究人员尽可能将研究问题设计为预测或相似性任务。例如，我们可以将社会偏见问题设计为性别词和中性词嵌入之间的相似性度量 [59, 133]。此外，人类语言的复杂性可以设计为一项预测任务，它以语言模型为指标查看单词或句子的预测概率[155]。\n可解释性。诚然，基于嵌入的方法的一个缺点是缺乏可解释性。这个问题会损害与道德、安全或隐私相关的决策关键系统的应用。尽管嵌入模型，尤其是神经网络模型的可解释性尚未完全解决，但计算机科学领域的研究人员已经做出了一些努力，以提高基于神经模型的可解释性 [16]。因此，利用基于嵌入的模型和可解释性分析方法进行有效和（部分）可解释的预测将是一个有趣的方向。\n结论 计算社会科学作为一个新兴且有前途的跨学科领域，近年来吸引了相当多的研究兴趣。 计算社会科学 研究中广泛使用两种主要类型的数据，即文本数据和网络数据。在本次调查中，我们首先将数据表示总结为基于符号和基于嵌入的表示，并在构建这些表示时进一步介绍典型的方法。之后，我们基于来自 6 个经典期刊和会议的 400 多篇高被引文献，对这两类表示的应用进行了全面回顾。根据对这些应用的统计，发现了 计算社会科学 中基于嵌入的文本和网络表示正在出现和增长的趋势，我们进一步讨论了其中的原因。最后，我们提出了 计算社会科学 中的四个挑战和未解决的问题，它们是需要探索的基本和潜在方向。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/from_sysbol_to_embeddings_in_computational_social_science/","summary":"B站看到大牛刘知远关于文本分析在计算社会科学领域应用的分享，解答了我对文本表示的疑惑，看完了能对文本的特征工程加深理解，同时也能更清晰未来如何借助计算机科学技术开展社会科学研究。\n 全文摘抄自\nChen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.\n  摘要 计算社会科学（CSS），旨在利用计算方法来解决社会科学问题，是一个新兴和快速发展的领域。 CSS 的研究是数据驱动的，并且显着受益于在线用户生成内容和社交网络的可用性，其中包含用于调查的富文本和网络数据。然而，这些大规模、多模态的数据也给研究人员带来了很大的挑战：如何有效地表示数据以挖掘我们想要的 CSS 含义？为了探索答案，我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示，并为每个方案介绍了一系列典型的方法。随后，我们基于对来自 6 个涉及 CSS 的顶级场所的 400 多篇研究文章的调查，展示了上述表示的应用。从这些应用程序的统计数据中，我们挖掘出每种表示的强度，并发现基于嵌入的表示在过去十年中出现并获得越来越多的关注的趋势。最后，我们讨论了几个关键挑战和未来方向的开放性问题。本调查旨在为 CSS 研究人员提供对数据表示的更深入理解和更明智的应用。\n关键词：计算社会科学；基于符号的表示；基于嵌入的表示；社交网络\n一、计算社会学数据分析流程 其中第二步，数据表示目前有两大类表示(特征工程)方法\n 基于符号的文本表示；符号可以是单词(或词组)，也可以是概念(如正面情感、负面情感) 基于嵌入(分布式)的文本表示；相比于符号法，将词(词组)看做一个点。嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(king)-v(queen)约等于v(man)-v(woman)  二、基于符号的文本表示 基于符号的文本表示一般来说默认词语是不可分的符号，每个词能根据词频统计出现次数的多与少，或是否存在。\n2.1 词语层面   基于词频表示\n 是否出现，出现标位1，反之标位0。 出现多少，词语出现几次，标为几个。    基于特征表示，如每个词带有权重(得分)","title":"转载 | 从符号到嵌入：计算社会科学的两种文本表示"},{"content":"\n项目地址 https://github.com/Embedding/Chinese-Word-Vectors\nChinese-Word-Vectors项目提供超过100种中文词向量，其中包括不同的表示方式（稠密SGNS和稀疏PPMI）、不同的上下文特征（词、N元组、字等等）、以及不同的训练语料。获取预训练词向量非常方便，下载后即可用于下游任务。\n参考文献 如果使用了本项目的词向量和CA8数据集请进行如下引用：\nShen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, Analogical Reasoning on Chinese Morphological and Semantic Relations, ACL 2018.\n@InProceedings{P18-2023, author = \u0026#34;Li, Shen and Zhao, Zhe and Hu, Renfen and Li, Wensi and Liu, Tao and Du, Xiaoyong\u0026#34;, title = \u0026#34;Analogical Reasoning on Chinese Morphological and Semantic Relations\u0026#34;, booktitle = \u0026#34;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\u0026#34;, year = \u0026#34;2018\u0026#34;, publisher = \u0026#34;Association for Computational Linguistics\u0026#34;, pages = \u0026#34;138--143\u0026#34;, location = \u0026#34;Melbourne, Australia\u0026#34;, url = \u0026#34;http://aclweb.org/anthology/P18-2023\u0026#34; } \n不同领域 下列词向量基于不同的表示方式、不同的上下文特征以及不同领域的语料训练而成。\nWord2vec / Skip-Gram with Negative Sampling (SGNS)  语料 上下文特征  词 词 + N元组 词 + 字 词 + 字 + N元组  Baidu Encyclopedia 百度百科 300d 300d 300d 300d / PWD: 5555  Wikipedia_zh 中文维基百科 300d 300d 300d 300d  People's Daily News 人民日报 300d 300d 300d 300d  Sogou News 搜狗新闻 300d 300d 300d 300d  Financial News 金融新闻 300d 300d 300d 300d  Zhihu_QA 知乎问答  300d 300d 300d 300d  Weibo 微博 300d 300d 300d 300d  Literature 文学作品 300d 300d / PWD: z5b4 300d 300d / PWD: yenb  Complete Library in Four Sections\n四库全书* 300d 300d NAN NAN  Mixed-large 综合\nBaidu Netdisk / Google Drive  300d\n300d   300d\n300d   300d\n300d   300d\n300d    Positive Pointwise Mutual Information (PPMI)  语料 上下文特征  词 词 + N元组 词 + 字 词 + 字 + N元组  Baidu Encyclopedia 百度百科 Sparse Sparse Sparse Sparse  Wikipedia_zh 中文维基百科 Sparse Sparse Sparse Sparse  People's Daily News 人民日报 Sparse Sparse Sparse Sparse  Sogou News 搜狗新闻 Sparse Sparse Sparse Sparse  Financial News 金融新闻 Sparse Sparse Sparse Sparse  Zhihu_QA 知乎问答  Sparse Sparse Sparse Sparse  Weibo 微博 Sparse Sparse Sparse Sparse  Literature 文学作品 Sparse Sparse Sparse Sparse  Complete Library in Four Sections\n四库全书* Sparse Sparse NAN NAN   Mixed-large 综合 Sparse Sparse Sparse Sparse   *由于古汉语中绝大部份词均为单字词，因此只需字向量。\n语料 项目花费了大量精力来收集了来自多个领域的语料。所有的文本数据均移除了html和xml标记，仅保留了纯文本。之后采用了HanLP(v_1.5.3)对文本进行了分词。此外，我们将繁体中文用Open Chinese Convert (OpenCC)转换为了简体中文。更详细的语料信息如下所示：\n语料 大小 词数量 词汇量 详情  Baidu Encyclopedia\n百度百科 4.1G 745M 5422K 中文百科\nhttps://baike.baidu.com/  Wikipedia_zh\n中文维基百科 1.3G 223M 2129K 中文维基百科\nhttps://dumps.wikimedia.org/  People's Daily News\n人民日报 3.9G 668M 1664K 人民日报新闻数据(1946-2017)\nhttp://data.people.com.cn/  Sogou News\n搜狗新闻 3.7G 649M 1226K Sogou labs的新闻数据\nhttp://www.sogou.com/labs/  Financial News\n金融新闻 6.2G 1055M 2785K 从多个网站收集到的金融新闻  Zhihu_QA\n知乎问答 2.1G 384M 1117K 中文问答数据\nhttps://www.zhihu.com/  Weibo\n微博 0.73G 136M 850K NLPIR Lab提供的微博数据\nhttp://www.nlpir.org/wordpress/download/weibo.7z  Literature\n文学作品 0.93G 177M 702K 8599篇现代文学作品  Mixed-large\n综合 22.6G 4037M 10653K 上述所有数据的汇总  Complete Library in Four Sections\n四库全书 1.5G 714M 21.8K 目前最大的古代文献汇总   上述统计结果中，所有词都被计算在内，包括低频词。 导入模型(代码) 例如我下载了多个词模型，下载得到bz2结尾的文件名，例如sgns.financial.bigram.bz2。\n使用方式\nfrom gensim.models.keyedvectors import KeyedVectors #以金融sgns.financial.bigram.bz2为例 model = KeyedVectors.load_word2vec_format(\u0026#39;embeddings/sgns.financial.bigram.bz2\u0026#39;, binary=False, unicode_errors=\u0026#39;ignore\u0026#39;) model Run\n\u0026lt;gensim.models.keyedvectors.KeyedVectors at 0x7fe7fad79d60\u0026gt; \nmodel.get_vector(\u0026#39;投资\u0026#39;) Run\narray([-0.084635, 0.890228, -0.23223 , -0.308985, 0.058241, 0.458777, -0.152547, -0.413471, 0.269701, -0.078043, -0.4155 , 0.074735, 0.35714 , 0.103431, 0.601784, -0.390854, 0.814801, -0.122664, -1.076744, 0.516941, -0.293319, -0.310251, -0.407794, 0.003898, -0.210962, 0.378095, -0.345955, -0.223848, 0.700162, 0.207644, 0.426249, -0.272832, -0.110305, -0.701062, -0.173407, -0.172121, -0.682592, 0.593414, 0.279591, -0.408284, -0.166693, 0.753402, 0.037375, 0.141865, -0.246024, -0.108663, -0.225255, -0.856601, 0.381026, 0.401248, 0.012108, -0.126305, -0.374255, 0.728795, 0.219549, -0.354029, -0.353131, 0.064867, 0.49565 , -0.503267, -0.304075, 0.145036, 0.688948, 0.063382, -0.223243, 0.474251, 0.80543 , 0.683178, 0.118159, 0.408411, -0.020066, 0.009045, -0.135446, -0.069633, 0.206357, 0.482845, -0.075307, 0.06433 , -0.112367, 0.011816, 0.87427 , -0.120287, -0.31036 , 0.369985, 0.560386, -0.215248, 0.389631, 0.042943, -0.319149, 0.951551, -0.335188, 0.642246, -0.55546 , 0.322397, 0.659618, -0.213124, 0.346696, -0.342239, 0.31479 , 0.078533, -0.345148, 0.815577, -0.530134, 0.303419, -0.158916, -0.190564, 0.436046, -0.112251, -0.339966, 0.253645, 0.181076, 0.122875, -0.310951, -0.126253, 1.641405, 0.357906, 0.165796, 0.398656, -0.330591, 0.20328 , -0.077191, -0.421248, -0.078504, -0.734519, 0.146212, 0.535727, 0.014134, 0.040322, -0.44809 , -0.758205, -0.151237, 0.248258, -0.319704, 0.656033, -0.518857, 0.932356, -1.01786 , -0.46354 , 0.160921, -0.243597, 0.106666, -0.03404 , 0.010672, 0.260243, 0.899813, 0.171735, -0.108209, -0.009843, -0.18113 , 0.302494, 0.187285, 0.064669, -0.502041, -0.724377, -0.294312, -0.522256, 0.334543, 0.740455, -0.357653, 0.540747, 0.256146, 0.513839, 1.116628, -0.626111, 0.505574, 0.089774, -0.381137, -0.282352, -0.457542, 0.198909, 0.313638, 0.560809, 0.25295 , 0.878158, -0.289311, -0.629047, 0.011103, 0.041058, -0.291302, -0.014001, -0.027697, -0.445817, -0.070086, 0.159816, -0.120071, 1.280489, -0.108866, 0.01586 , -0.505574, -0.679772, -0.343165, 0.595633, 0.438108, -0.364066, -0.393667, 0.442285, 0.24979 , -0.191607, 0.425692, 0.535577, -0.480332, -0.737461, 0.588498, -0.380264, 0.151292, 0.077519, -0.221384, 0.699436, 0.401642, 0.509026, -0.411141, 0.206719, -0.097051, -0.451834, -0.825617, 0.602984, 0.2853 , 0.46055 , 0.96472 , 0.322712, -0.373446, 0.207944, 0.236688, 0.566523, 0.037644, 1.241091, 0.025682, 0.373211, 0.097712, -0.195355, 0.264579, -0.072992, -0.121629, 0.041688, 0.213666, 0.329652, -0.015182, 0.396307, 0.117955, 0.119577, -0.334761, -0.135917, 0.409983, 0.512367, -0.292204, 0.302897, -0.325733, 0.383173, -0.92419 , -0.377535, -0.059801, -0.606275, -0.240482, 0.054021, -0.581386, -0.555691, 0.158354, 0.103765, 0.107681, 0.248877, -0.597925, 0.193332, 0.844085, 0.00584 , 0.041622, -0.111235, 0.617778, 0.234883, -0.09562 , 0.408324, -0.107121, 0.717875, 0.674794, 0.127214, -0.178357, 0.331436, 0.417898, -0.650833, -0.428309, -0.576132, 0.210533, -0.057879, -0.578397, 0.468586, 0.103365, -0.403216, -0.398776, 0.094514, -0.130387, 0.628187, -0.463082, -0.951649, 0.561544, 0.118903, 0.448327, -0.171685, -0.672348, 0.069471, 0.556452, -0.335425], dtype=float32) \nmodel.similar_by_key(\u0026#39;投资\u0026#39;) Run\n[(\u0026#39;长期投资\u0026#39;, 0.5135656595230103), (\u0026#39;投资规模\u0026#39;, 0.5089880228042603), (\u0026#39;智百扬\u0026#39;, 0.49565914273262024), (\u0026#39;投资总额\u0026#39;, 0.4955061078071594), (\u0026#39;洛辉\u0026#39;, 0.489188551902771), (\u0026#39;337409\u0026#39;, 0.48917514085769653), (\u0026#39;洛盛\u0026#39;, 0.4819018244743347), (\u0026#39;洛腾\u0026#39;, 0.4728960692882538), (\u0026#39;394150\u0026#39;, 0.4704836308956146), (\u0026#39;投资额\u0026#39;, 0.4685181975364685)] \nmodel.similar_by_key(\u0026#39;风险\u0026#39;) Run\n[(\u0026#39;提示\u0026#39;, 0.6549968123435974), (\u0026#39;经营风险\u0026#39;, 0.6316577792167664), (\u0026#39;景气衰退\u0026#39;, 0.544153094291687), (\u0026#39;风险分析\u0026#39;, 0.5439289212226868), (\u0026#39;遇宏观\u0026#39;, 0.5435716509819031), (\u0026#39;信用风险\u0026#39;, 0.5345730185508728), (\u0026#39;承受能力\u0026#39;, 0.5291797518730164), (\u0026#39;防范\u0026#39;, 0.5271924138069153), (\u0026#39;系统性\u0026#39;, 0.5178108811378479), (\u0026#39;不确定性\u0026#39;, 0.5173759460449219)] 向量运行效果还行，感兴趣的同学也可以根据自己的数据训练word2vec模型，训练及使用的办法参照文章\n豆瓣影评 | 探索词向量妙处\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/embeddings_resource_usage_method/","summary":"项目地址 https://github.com/Embedding/Chinese-Word-Vectors\nChinese-Word-Vectors项目提供超过100种中文词向量，其中包括不同的表示方式（稠密SGNS和稀疏PPMI）、不同的上下文特征（词、N元组、字等等）、以及不同的训练语料。获取预训练词向量非常方便，下载后即可用于下游任务。\n参考文献 如果使用了本项目的词向量和CA8数据集请进行如下引用：\nShen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, Analogical Reasoning on Chinese Morphological and Semantic Relations, ACL 2018.\n@InProceedings{P18-2023, author = \u0026#34;Li, Shen and Zhao, Zhe and Hu, Renfen and Li, Wensi and Liu, Tao and Du, Xiaoyong\u0026#34;, title = \u0026#34;Analogical Reasoning on Chinese Morphological and Semantic Relations\u0026#34;, booktitle = \u0026#34;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\u0026#34;, year = \u0026#34;2018\u0026#34;, publisher = \u0026#34;Association for Computational Linguistics\u0026#34;, pages = \u0026#34;138--143\u0026#34;, location = \u0026#34;Melbourne, Australia\u0026#34;, url = \u0026#34;http://aclweb.","title":"中文词向量资源汇总 \u0026 使用方法"},{"content":"整理到csv中 将70G定期报告披露数据集下载\n链接: https://pan.baidu.com/s/1oboFUswiAMdA_Wn3xCh6YQ 提取码: g7bd\nimport os import pdfdocx import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) import pandas as pd import re def clean(text): return re.sub(\u0026#39;\\s\u0026#39;, \u0026#39; \u0026#39;, text) #文件夹列表 dirs = [d for d in os.listdir(\u0026#39;reports\u0026#39;) if \u0026#39;DS\u0026#39; not in d] for di in dirs: datas = [] files = [f for f in os.listdir(\u0026#39;reports/{d}\u0026#39;.format(d=di)) if \u0026#39;z\u0026#39; in f] for file in files: try: file = \u0026#39;reports/{di}/{f}\u0026#39;.format(di=di, f=file) code = file.split(\u0026#39;/\u0026#39;)[-1].split(\u0026#39;_\u0026#39;)[0] year = re.findall(\u0026#39;_(\\d{4})_\u0026#39;, file.split(\u0026#39;/\u0026#39;)[-1])[0] text = clean(pdfdocx.read_pdf(file)) data = {\u0026#39;code\u0026#39;: code, \u0026#39;text\u0026#39;: text, \u0026#39;year\u0026#39;:year} datas.append(data) except: pass df = pd.DataFrame(datas, columns=[\u0026#39;code\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;text\u0026#39;]) #将每家公司的年报导出到csv中 df.to_csv(\u0026#39;data.csv\u0026#39;, index=False, mode=\u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) #读取 ndf = pd.read_csv(\u0026#39;data.csv\u0026#39;) #去重 ndf.drop_duplicates(inplace=True) #清洗 ndf = ndf[ndf.code=!=\u0026#39;code\u0026#39;] #导出到xlsx ndf.to_excel(\u0026#39;data.xlsx\u0026#39;, index=False) \n导入数据 excel数据下载链接: https://pan.baidu.com/s/1r4YRyxb7bTsx-_ayT4GDKQ 提取码: ew4v\nimport pandas as pd df = pd.read_excel(\u0026#39;data.xlsx\u0026#39;) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  code year text     0 603859 2017 2017年半年度报告1/116公司代码：603859公司简称：能科股份能科节能技术股份有限公...   1 603859 2019 2019年半年度报告1/141公司代码：603859公司简称：能科股份能科科技股份有限公司2...   2 603859 2018 2018年半年度报告1/120公司代码：603859公司简称：能科股份能科科技股份有限公司2...   3 601500 2017 2017年半年度报告1/114公司代码：601500公司简称：通用股份江苏通用科技股份有限公...   4 601500 2019 2019年半年度报告1/140公司代码：601500公司简称：通用股份江苏通用科技股份有限公...     查看数据量 len(df) 16984  公司数 df.code.nunique() 1476  含有的年份 sorted(df.year.unique()) [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]  每家公司年报数 数据集中，平均每家公司的年报数\navg = len(df)/df.code.nunique() round(avg, 2) 11.51  说明 数据是19年获取的，数据不全，下载过程中有部分pdf是破损的文件。\n大家可以尝试该数据集训练会计年报词向量，看看有没有有趣的应用。\n本数据可作探索实验性质，如果想在会计领域深入挖掘，建议找更全更精准的数据集。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/sh_market/","summary":"整理到csv中 将70G定期报告披露数据集下载\n链接: https://pan.baidu.com/s/1oboFUswiAMdA_Wn3xCh6YQ 提取码: g7bd\nimport os import pdfdocx import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) import pandas as pd import re def clean(text): return re.sub(\u0026#39;\\s\u0026#39;, \u0026#39; \u0026#39;, text) #文件夹列表 dirs = [d for d in os.listdir(\u0026#39;reports\u0026#39;) if \u0026#39;DS\u0026#39; not in d] for di in dirs: datas = [] files = [f for f in os.listdir(\u0026#39;reports/{d}\u0026#39;.format(d=di)) if \u0026#39;z\u0026#39; in f] for file in files: try: file = \u0026#39;reports/{di}/{f}\u0026#39;.format(di=di, f=file) code = file.split(\u0026#39;/\u0026#39;)[-1].split(\u0026#39;_\u0026#39;)[0] year = re.","title":"将年报数据汇总至xlsx文件中"},{"content":"本文要点\n 读取csv cntext训练词向量模型 cntext扩展pos、neg词典 导入词向量模型 运用词向量模型  代码下载 链接: https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg 提取码: og9t\n\n一、读取数据 import pandas as pd df = pd.read_csv(\u0026#39;douban.csv\u0026#39;) df.head() print(\u0026#34;电影 : {}部\u0026#34;.format(df.Movie_Name_CN.nunique())) print(\u0026#34;评论 : {}条\u0026#34;.format(len(df))) 电影 : 28 部 评论 : 2125056 条  二、训练模型 使用cntext库训练词向量word2vec模型,这里我把csv数据整理为txt\nfrom cntext import W2VModels import os #训练word2vec模型 model = W2VModels(cwd=os.getcwd()) #语料数据 model.train(input_txt_file=\u0026#39;douban.txt\u0026#39;) Step 1/4:...预处理 语料 ... Step 2/4:...训练 word2vec模型 耗时 2001 s cntext可以用于扩展词典\nmodel.find(seedword_txt_file=\u0026#39;pos.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;neg.txt\u0026#39;, topn=100) Step 3/4:...准备 每个seed在word2vec模型中的相似候选词... Step 4/4 完成! 耗时 2302 s Step 3/4:...准备 每个seed在word2vec模型中的相似候选词... Step 4/4 完成! 耗时 2303 s 在代码所在文件夹内可以找到\n output/w2v_candi_words/w2v.model 新的 pos.txt 新的 neg.txt  新的pos.txt是对pos.txt词典的扩展。\n三、导入w2v模型 有的时候数据量特别大，模型训练十分不易。\n这时，保存已训练好的模型，不止下次不用再同样的数据再次训练，也可分享给其他人使用。\n训练结束后，在代码所在文件夹内可以找到 output/w2v_candi_words/w2v.model\nfrom gensim.models import KeyedVectors w2v_model = KeyedVectors.load(\u0026#39;output/w2v_candi_words/w2v.model\u0026#39;) w2v_model \u0026lt;gensim.models.keyedvectors.KeyedVectors at 0x7face0574880\u0026gt;  w2v_models数据类型为KeyedVectors， 在本文中使用w2v_models代指KeyedVectors\n四、玩转词向量 用户级的数据(如在线评论)感觉生成的向量会准一些，词向量的方向，近义反义在向量中都有体现。\n例如本文使用的是28部电影的2125056条影评， 一般评论内容包含电影相关信息，如电影题材、是否值的观影等。\n而在我们训练出模型w2v_models存在一些常用的方法\n w2v_model.get_vector(key) 获取key的词向量 w2v_model.most_similar_to_given(key1, keys_list) 从 keys_list 中获取与 key1 最相似的词 w2v_model.n_similarity(ws1, ws2) 两组词ws1, ws2 的相似度 w2v_model.closer_than(key1, key2) 更接近于key1的词向量(相比于key2) w2v_model.most_similar(positive, negative) 找出与positive同方向，与negative反向相反的词。  4.1 get_vector(key) w2v_model.get_vector(key) 获取key的词向量\n#获取某词语的向量 w2v_model.get_vector(\u0026#39;给力\u0026#39;) array([ 0.06488553, 0.74188954, 0.25468495, 0.89755714, 1.8139195 , -0.6950082 , 0.24339403, -1.2188634 , 0.543618 , -0.9988698 , 0.27471313, 0.9325699 , -0.5860608 , -0.5081917 , 1.6423215 , -0.0490295 , -0.3927043 , 0.659067 , 0.03185922, -1.021391 , -1.3214804 , -0.28208104, -0.7819419 , -0.30637202, -1.5944146 , -0.12383854, -0.70463836, 0.45689437, 1.223081 , -1.9453759 , -0.5538997 , -0.9750523 , -0.10031194, -0.9568689 , 0.30341247, 1.1102395 , 0.667315 , -1.1600997 , -0.26674765, -0.55144155, -0.3246094 , 0.82902473, -0.47339582, -0.9009957 , 1.7722464 , 0.28959563, -0.03453476, 0.4786787 , -0.48074463, -0.23090109, -0.49390873, 0.71246386, 2.1557336 , 2.4899387 , -0.51481706, 0.5579966 , -0.6973235 , -1.1408254 , 0.72495663, -1.0326954 , -0.5455598 , 0.98941576, -1.2155218 , -0.9088408 , 1.9184568 , -0.21800426, -1.2009395 , 0.29684314, 1.3672423 , -2.269391 , 0.6188098 , -0.02714545, -0.44811317, 1.4397241 , -1.0594722 , -0.08088647, -0.13015983, -0.99255013, 0.62044877, 2.5046496 , 0.4054545 , -0.38767585, -0.6956541 , 0.22991426, 0.5928579 , -0.12684819, -0.17408212, 0.25033692, -1.4419957 , -0.27390227, 1.166638 , -0.00624323, -1.6046506 , 2.1633575 , -0.395548 , -1.1297956 , -3.1474566 , 0.38729438, -2.0434535 , -1.5511289 ], dtype=float32)  4.2 most_similar_to_given(key1, keys_list) 从 keys_list 中获取与 key1 最相似的词。例如在212w影评中，从'爱情', '悬疑', '飞船', '历史', '战争'找出最接近'太空'，最后返回'飞船'\n#从 `keys_list` 中获取与 `key1` 最相似的 `key`。 w2v_model.most_similar_to_given(key1=\u0026#39;太空\u0026#39;, keys_list=[\u0026#39;爱情\u0026#39;, \u0026#39;悬疑\u0026#39;, \u0026#39;飞船\u0026#39;, \u0026#39;历史\u0026#39;, \u0026#39;战争\u0026#39;]) '飞船'  4.3 w2v_model.n_similarity(ws1, ws2) 两组词ws1, ws2 的相似度。\nfrom sklearn.metrics.pairwise import cosine_similarity cosine_similarity([w2v_model.get_vector(\u0026#39;理想\u0026#39;)], [w2v_model.get_vector(\u0026#39;现实\u0026#39;)])[0][0] 0.5371934  #cosine算法 w2v_model.n_similarity([\u0026#39;理想\u0026#39;], [\u0026#39;现实\u0026#39;]) 0.5371934  #计算两组键之间的余弦相似度。 w2v_model.n_similarity([\u0026#39;给力\u0026#39;, \u0026#39;精彩\u0026#39;, \u0026#39;赞\u0026#39;, \u0026#39;推荐\u0026#39;], [\u0026#39;无聊\u0026#39;, \u0026#39;尴尬\u0026#39;, \u0026#39;垃圾\u0026#39;]) 0.35008422  w2v_model.n_similarity([\u0026#39;理想\u0026#39;, \u0026#39;梦想\u0026#39;], [\u0026#39;现实\u0026#39;, \u0026#39;生活\u0026#39;]) 0.48020104  4.4 w2v_model.closer_than(key1, key2) 更接近于key1的词向量(相比于key2)\n#获取所有更接近 `key1` 的键，而不是 `key2` 。 w2v_model.closer_than(key1=\u0026#39;理想\u0026#39;, key2=\u0026#39;现实\u0026#39;) ['梦想', '妥协', '追梦', '愿望', '骨感']  4.5 w2v_model.most_similar(positive, negative) 找出与positive同方向，与negative反向相反的词。\nw2v_model.most_similar(positive=[\u0026#39;给力\u0026#39;, \u0026#39;精彩\u0026#39;, \u0026#39;过瘾\u0026#39;], negative=[\u0026#39;垃圾\u0026#39;], topn=10) [('激动人心', 0.6859163045883179), ('惊心动魄', 0.6767394542694092), ('带感', 0.6723690032958984), ('惊险刺激', 0.667783796787262), ('刺激', 0.6445038318634033), ('燃', 0.6429688930511475), ('爽快', 0.6287934184074402), ('带劲', 0.6254130005836487), ('爽', 0.624543309211731), ('酣畅淋漓', 0.6140543818473816)]  4.6 类比king-man+woman~queen 每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。\n这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。\n这两个词相减，按感觉应该得到的是性别方向，雄性-\u0026gt;雌性。\ngender_direction_1 = vector(man)-vector(woman)\ngender_direction_2 = vector(king)-vector(queen)\n那两个性别方向应该近似，假设这里将其gender_direction_1=gender_direction_2，则对于公式中任意一个词，都可以由等式中的其他三个词经过运算得到。例如\nvector(queen) = vector(king)-vector(man)+vector(woman)\n这里构造了一个情绪的公式，计算如下\n# 开心 - 难过 ~= 享受 - d a = w2v_model.get_vector(\u0026#39;开心\u0026#39;) b = w2v_model.get_vector(\u0026#39;难过\u0026#39;) c = w2v_model.get_vector(\u0026#39;享受\u0026#39;) #d = a-b+c w2v_model.similar_by_vector(a-b+c) [('享受', 0.7833479046821594), ('开心', 0.6825607419013977), ('愉快', 0.6298696994781494), ('娱乐', 0.6215130090713501), ('感官', 0.6085000038146973), ('图个', 0.6052624583244324), ('图一乐', 0.6039161682128906), ('休闲', 0.60273677110672), ('视觉享受', 0.6006160378456116), ('轻松愉快', 0.5961319804191589)]  很遗憾，d没有运算出煎熬之类的词语，但好在都是形容词，而且是快乐居多的形容词，类别是对的，就是方向是反的。\n词向量总结 需要注意的是经典的运算king-man+woman~queen来自glove模型，而不是本文使用的word2vec模型。两者相同点，glove与word2vec均为词嵌入embeddings技术。区别在于glove获取的词的全局语义空间，而word2vec一般是某个词前后n个词(例如前后5个词)范围内的语义。做概念四则运算，以后如可能，建议用glove。\n此外，即时使用glove，尽量使用概念的词组均值向量。首先要训练数据要存在这些人类认知的线索。其次，认知概念往往不是由一个词决定的，可能需要相关的很多词。例如人类社会中的雄雌(没有贬义，包含了男女在内的概念)，\n 雄性概念词有他、男人、男孩、父亲、爷爷、爸爸、姥爷... 雌性概念词有她、女人、女孩、母亲、奶奶、妈妈、姥姥... 国王概念词有查理n世、乔治、路易... 女王概念词有伊丽莎白n世、维多利亚女王、叶卡捷琳娜二世...  或许改成概念向量四则运算，公式可能更容易成立。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/douban_w2v/","summary":"本文要点\n 读取csv cntext训练词向量模型 cntext扩展pos、neg词典 导入词向量模型 运用词向量模型  代码下载 链接: https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg 提取码: og9t\n\n一、读取数据 import pandas as pd df = pd.read_csv(\u0026#39;douban.csv\u0026#39;) df.head() print(\u0026#34;电影 : {}部\u0026#34;.format(df.Movie_Name_CN.nunique())) print(\u0026#34;评论 : {}条\u0026#34;.format(len(df))) 电影 : 28 部 评论 : 2125056 条  二、训练模型 使用cntext库训练词向量word2vec模型,这里我把csv数据整理为txt\nfrom cntext import W2VModels import os #训练word2vec模型 model = W2VModels(cwd=os.getcwd()) #语料数据 model.train(input_txt_file=\u0026#39;douban.txt\u0026#39;) Step 1/4:...预处理 语料 ... Step 2/4:...训练 word2vec模型 耗时 2001 s cntext可以用于扩展词典\nmodel.find(seedword_txt_file=\u0026#39;pos.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;neg.txt\u0026#39;, topn=100) Step 3/4:...准备 每个seed在word2vec模型中的相似候选词... Step 4/4 完成!","title":"豆瓣影评 | 探索词向量妙处"},{"content":"词嵌入做为一种词向量模型，可以从文本中计算出隐含的上下文情景信息，态度及偏见。通过词向量距离的测算，就可以间接测得不同群体对某概念(组织、群体、品牌、地域等)的态度偏见。偏见(刻板印象)的介绍有\n大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用\n计算机科学家，正研究如何在AI中减弱甚至剔除刻板印象；但在社会科学领域，接受已有数据中存在的刻板印象，在数据中测量Bias，发现Bias，应用Bias，也能更好的认识和改造社会 。今天介绍一个挺好玩的工具WordBias。\nWordBias WordBias：一种用于发现词嵌入偏见(刻板印象)的交互式可视化工具， 旨在探索子群体（intersectional groups，直译为交叉群体）（如黑人女性、黑人穆斯林男性等）在词嵌入中的编码偏见。 我们的工具认为一个词与一个交叉组相关联，例如“Christian Males”，如果它与它的每个构成子集（Christians 和 Males）密切相关。 我们的工具旨在为专家提供有效的审核工具，为非专家提供教育工具，并增强领域专家的可访问性。\n 例如对“黑人男性”的刻板印象，是由“男人”和“黑人”两类刻板印象加上一些其他线索组成的。\n所以这里intersectional groups，直译为交叉群体, 感觉不太好理解， 我把intersectional groups理解为群体中的子群体。个人理解，不一定合理，欢迎留言。\n   Read paper \n  视频演示(5min)\n  在线Demo\n  安装   下载这个仓库\n  命令行切换至WordBias文件夹,安装依赖包\ncd Path_of_WordBias_Directory pip3 install -r req.txt   运行WordBias，命令行执行\npython3 app.py   在浏览器中打开打开链接\nhttps://localhost:6999   浏览器中会出现界面，如下图\n界面 上图为WordBias的可视化界面。 图片可以分为3部分：\n(A) 控制面板提供选择要投影到平行坐标图上的单词的选项\n(B) 主视图显示所选单词（蓝线）沿不同偏见类型（轴）的偏差分数\n(C) 搜索面板使用户能够搜索单词并显示搜索/刷新结果。 案例1-极端主义 在上图A位置选择恐怖主义类别词Extremism\n在图中B位置，可以看到这些负面词在不同维度上存在不同的偏见。\n 性别: 这类词主要倾向于男性 地区： 这类词倾向于伊斯兰地区 年龄: 这类词倾向于年轻人 经济: 这类词倾向于贫穷  这表明 Word2vec 嵌入包含对穆斯林地区的贫困男性存在偏见。 案例2-pretty/beautifull 根据WordBias，描述女性美丽，可能不同的词使用范围不太一样。\n在年龄维度，pretty更适合描述小女生，而beautifull适合成熟女性。\n岁月从不败美人，说的就是beautifull woman吧。\n论文 使用到wordbias，请注明出处\n@inproceedings{ghai2021wordbias, title={WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings}, author={Ghai, Bhavya and Hoque, Md Naimul and Mueller, Klaus}, booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems}, pages={1--7}, year={2021} } 基于训练好的词嵌入模型，WordBias计算每个词与性别、宗教等不同社会分类（类别词典）的偏见分数（关联系数），研究者定义了多个类别，如子类别，\n   类 子类 词表     Gender Male he, son, his, him, father, man, boy, himself, male, brother, sons, fathers, men, boys, males, brothers, uncle, uncles, nephew, nephews   Gender Femal she, daughter, hers, her, mother, woman, girl, herself, female, sister, daughters, mothers, women, girls, sisters, aunt, aunts, niece, nieces   Age Young Taylor, Jamie, Daniel, Aubrey, Alison, Miranda, Jacob, Arthur, Aaron, Ethan   Age Old Ruth, William, Horace, Mary, Susie, Amy, John, Henry, Edward, Elizabeth   Religion Islam allah, ramadan, turban, emir, salaam, sunni, koran, imam, sultan, prophet, veil, ayatollah, shiite, mosque, islam, sheik, muslim, muhammad   Religion Christainity baptism, messiah, catholicism, resurrection, christianity, salvation, protestant, gospel, trinity, jesus, christ, christian, cross, catholic, church   Race Black black, blacks, Black, Blacks, African, african, Afro   Race White white, whites, White, Whites, Caucasian, caucasian, European, european, Anglo   Economic Rich rich, richer, richest, affluence, advantaged, wealthy, costly, exorbitant, expensive, exquisite, extravagant, flush, invaluable, lavish, luxuriant, luxurious, luxury, moneyed, opulent, plush, precious, priceless, privileged, prosperous, classy   Economic Poor poor, poorer, poorest, poverty, destitude, needy, impoverished, economical, inexpensive, ruined, cheap, penurious, underprivileged, penniless, valueless, penury, indigence, bankrupt, beggarly, moneyless, insolvent    其中偏见分数使用了Relative Norm Difference算法。设向量g1、g2分别表示一个类别中的两个子群体(如黑人，g1黑女 g2黑男) ，给定一个词w， 分别计算w与g1、g2的距离。如果不等距，则表示存在刻板印象，距离差值越大，偏见得分(BiasScore)越深。\n$$𝐵𝑖𝑎𝑠S𝑐𝑜𝑟𝑒(𝑤) = 𝑐𝑜𝑠𝑖𝑛𝑒D𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑤, 𝑔1) − 𝑐𝑜𝑠𝑖𝑛𝑒D𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑤, 𝑔2) $$\n然后使用新颖的交互式界面将它们可视化。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/wordbias/","summary":"词嵌入做为一种词向量模型，可以从文本中计算出隐含的上下文情景信息，态度及偏见。通过词向量距离的测算，就可以间接测得不同群体对某概念(组织、群体、品牌、地域等)的态度偏见。偏见(刻板印象)的介绍有\n大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用\n计算机科学家，正研究如何在AI中减弱甚至剔除刻板印象；但在社会科学领域，接受已有数据中存在的刻板印象，在数据中测量Bias，发现Bias，应用Bias，也能更好的认识和改造社会 。今天介绍一个挺好玩的工具WordBias。\nWordBias WordBias：一种用于发现词嵌入偏见(刻板印象)的交互式可视化工具， 旨在探索子群体（intersectional groups，直译为交叉群体）（如黑人女性、黑人穆斯林男性等）在词嵌入中的编码偏见。 我们的工具认为一个词与一个交叉组相关联，例如“Christian Males”，如果它与它的每个构成子集（Christians 和 Males）密切相关。 我们的工具旨在为专家提供有效的审核工具，为非专家提供教育工具，并增强领域专家的可访问性。\n 例如对“黑人男性”的刻板印象，是由“男人”和“黑人”两类刻板印象加上一些其他线索组成的。\n所以这里intersectional groups，直译为交叉群体, 感觉不太好理解， 我把intersectional groups理解为群体中的子群体。个人理解，不一定合理，欢迎留言。\n   Read paper \n  视频演示(5min)\n  在线Demo\n  安装   下载这个仓库\n  命令行切换至WordBias文件夹,安装依赖包\ncd Path_of_WordBias_Directory pip3 install -r req.txt   运行WordBias，命令行执行\npython3 app.py   在浏览器中打开打开链接\nhttps://localhost:6999   浏览器中会出现界面，如下图\n界面 上图为WordBias的可视化界面。 图片可以分为3部分：\n(A) 控制面板提供选择要投影到平行坐标图上的单词的选项\n(B) 主视图显示所选单词（蓝线）沿不同偏见类型（轴）的偏差分数\n(C) 搜索面板使用户能够搜索单词并显示搜索/刷新结果。 案例1-极端主义 在上图A位置选择恐怖主义类别词Extremism","title":"WordBias库 | 发现偏见(刻板印象)的交互式工具"},{"content":"Python实证指标构建与文本分析  点击上方图片购买课程   概览 为何要学Python？ 在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！\n在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：\n 网络爬虫技术 解决 如何从网络世界中高效地 采集数据？ 文本分析技术 解决 如何从杂乱的文本数据中实证指标(情感、态度、刻板印象等)？  发票事项 如需发票，请先加微信372335839， 咨询发票细节，再作购买\n- 企业名称：哈尔滨所以然信息技术有限公司 - 企业税号：91230109MABT7KBC4M - 银行账户: 6228400176412884160 - 开户行: 中国农业银行哈尔滨香坊支行 课程纲要  课程目标： 掌握Python语法、网络爬虫、数据分析Pandas、文本分析、机器学习、词嵌入与认知 核心知识点： 爬虫原理及应用、 非结构化文本数据挖掘的思路及方法、机器学习应用等 环境配置: 本文使用Anaconda作为Python的软件安装包，注意安装过程中勾选Add Anaconda 3.x to PATH 课件资料： 本课程全部使用jupyter notebook文件作为课程课件  课程特色  接地气： 以经管学术需求为导向， 将Python分为语法篇、采集数据篇、文本分析篇、机器学习篇四大部分 好理解： 知识点力求通俗易懂，少了晦涩的计算机术语，多了通俗易懂的使用场景和实战讲解 上手快： 所有知识点均有可重复使用的代码块，犹如一块块的积木，课后您可以根据分析需要，快速搭建出自己的Python代码 技术新： 最新词嵌入，可挖掘文本中的态度、偏见、刻板印象等。  经管-经典文本分析方法 在这里我把技术细分为词频、词袋、w2v建词典、w2v认知变迁四个维度，这四大技术方法在本课程中均有体现。为了直观了解课程价值，这里附上7篇文献，大家可以购课前以做参考。\n   文献 定性 词频 词袋 W2V建词典 W2V认知变迁     王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性\u0026ndash;基于 Kickstarter 的实证研究. 管理世界, (5), pp.81-98. Y Y      语言具体性如何影响顾客满意度\nPackard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” Journal of Consumer Research 47, no. 5 (2021): 787-806.  Y      Wang, Quan, Beibei Li, and Param Vir Singh. \u0026ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.\u0026rdquo; Information Systems Research 29, no. 2 (2018): 273-291.   Y     文本相似度\nCohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. The Journal of Finance, 75(3), pp.1371-1415.   Y     胡楠, 薛付婧 and 王昊楠, 2021. 管理者短视主义影响企业长期投资吗———基于文本分析和机器学习. 管理世界, 37(5), pp.139-156.  Y  Y    Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, The Review of Financial Studies, 2020   Y Y    女性就职高管改变组织内性别偏见\nLawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. \u0026ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.\u0026rdquo; Proceedings of the National Academy of Sciences 119, no. 9 (2022): e2026443119.     Y   使用词嵌入技术，量化近百年以来性别和族群的刻板印象\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. \u0026ldquo;Word embeddings quantify 100 years of gender and ethnic stereotypes.\u0026rdquo; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.     Y    一、课件下载   课程介绍\n  Win中的Anaconda软件配置\n  Mac中的Anaconda软件配置\n  二、Python语法入门  Python跟英语一样是一门语言 数据类型之字符串 数据类型之列表元组集合 数据类型之字典 数据类型之布尔值、None 逻辑语句(if\u0026amp;for\u0026amp;tryexcept) 列表推导式 理解函数 常用的内置函数 内置库文件路径pathlib库 内置库csv文件库 内置库正则表达式re库 初学python常出错误汇总  三、数据采集  网络爬虫原理 网络访问requests库 网页解析pyquery库 「案例」 豆瓣读书 「案例」 Boss直聘 如何解析json数据 「案例」 豆瓣电影 「案例」 京东商城 「案例」 用爬虫下载文档及多媒体文件 「案例」 上市公司定期报告pdf批量下载 「案例」 上交所招股说明pdf批量下载 「案例」 深交所招股说明pdf批量下载 爬虫知识点总结  四、数据分析  Pandas基础知识 数据去重与缺失值处理 合并数据 重塑数据 选取表中指定记录(行) 选取表中指定字段(列) 描述性统计 在表中创建新字段(列) 批操作apply与agg 透视表pivot_table 数据分组groupby 时间序列时间点创建 日期数据的dt属性 日期行索引操作(选取指定日期的数据) 时间序列date_range 时间序列重采样resample 时间序列时间窗口rolling 「案例」 Kaggle titanic数据集探索性分析 「案例」 Boss直聘Python岗位分析  五、初识文本分析   从编码/解码视角重新理解文本\n  读取不同格式文件中的数据\n  如何将多个年报整理到一个excel中\n  「案例」 中文分词及数据清洗\n  「案例」 词频统计\u0026amp;词云图\n  「案例」 共现法扩展情感词典(领域词典)\n  「案例」 词向量word2vec扩展领域词典\n  「案例」 中文情感分析(词典法)\n  cntext库 情感分析代码操作\n  「案例」 对excel中的文本进行情感分析 91\n  「案例」: 语言具体性与心理距离 | 以JCR2021论文为例\n  「案例」: 使用LM金融词典对年报进行「语调分析」 | 2018管理世界\n  「案例」: 使用md\u0026amp;a数据测量企业数字化 | 管理世界、财经研究\n  「案例」: 使用md\u0026amp;a数据构建标准信息、信息含量 | 中国工业经济\n  六、机器学习与文本分析  了解机器学习ML 使用机器学习做文本分析的流程 scikit-learn机器学习库简介 文本特征抽取(特征工程) 「案例」 在线评论文本分类 使用标注工具对数据进行标注 「案例」 计算文本情感分析(有权重) 「案例」 文本相似性计算 「案例」 使用文本相似性识别变化(政策连续性) 「案例」 央行货币政策报告文本相似度计算与可视化 | 金融研究 「案例」 Kmeans聚类算法 「案例」 LDA话题模型 使用机器学习从图片中提取文本信息  七、词嵌入与认知  词嵌入原理及应用概述 「案例」 豆瓣影评-训练词向量\u0026amp;使用词向量 「案例」 使用词向量做话题建模 「案例」 认知指标(态度、偏见等)的测量 总结-文本分析在社科(经管)领域中的应用  \n相关应用 参照两篇论文的摘要，可以通过场景化等的方式帮助我们迅速理解上面两个问题。摘要部分的加粗内容是论文用到的分析技术，在我们的课程中均有与之对应的知识点和代码。\n曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.\n该文汉化了LM金融词典，并使用LM中文词典进行语调分析。 课程已整理了 LM中英文词典 及 对应代码。  摘要: 基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在**「口是心非」** 的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与 「年报语调」 相关。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。\n关键词： 年报; 语调管理; 内部人交易; 信息不对称;\n 王伟,陈伟,祝效国,王洪伟.众筹融资成功率与语言风格的说服性——基于Kickstarter的实证研究[J].管理世界,2016(05):81-98.\n 摘要：众筹融资效果决定着众筹平台的兴衰。众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。\n然后，借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。\n最后，建立语言说服风格对项目筹资影响的计量模型，并对 Kickstarter 平台上的 128345 个项目进行实证分析。总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。\n 胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.\n 摘要 : 在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间 导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现，年报 MD\u0026amp;A 中披露的「短期视域」 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴。\n关键词: 管理者短视; 长期投资; 文本分析; 机器学习\n 相关文献 在这里我把技术细分为词频、词袋、w2v建词典、w2v认知变迁四个维度，整理了经管7篇论文。大家可以阅读这7篇论文，掌握文本分析的应用场景。\n   文献 定性 词频 词袋 W2V建词典 W2V认知变迁     王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性\u0026ndash;基于 Kickstarter 的实证研究. 管理世界, (5), pp.81-98. Y Y      语言具体性如何影响顾客满意度\nPackard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” Journal of Consumer Research 47, no. 5 (2021): 787-806.  Y      Wang, Quan, Beibei Li, and Param Vir Singh. \u0026ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.\u0026rdquo; Information Systems Research 29, no. 2 (2018): 273-291.   Y     文本相似度\nCohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. The Journal of Finance, 75(3), pp.1371-1415.   Y     文本相似度\n姜富伟,胡逸驰,黄楠.央行货币政策报告文本信息、宏观经济与股票市场[J].金融研究,2021,(06):95-113.   Y     胡楠, 薛付婧 and 王昊楠, 2021. 管理者短视主义影响企业长期投资吗———基于文本分析和机器学习. 管理世界, 37(5), pp.139-156.  Y  Y    Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, The Review of Financial Studies, 2020   Y Y    女性就职高管改变组织内性别偏见\nLawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. \u0026ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.\u0026rdquo; Proceedings of the National Academy of Sciences 119, no. 9 (2022): e2026443119.     Y   使用词嵌入技术，量化近百年以来性别和族群的刻板印象\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. \u0026ldquo;Word embeddings quantify 100 years of gender and ethnic stereotypes.\u0026rdquo; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.     Y    [1]沈艳, 陈赟, \u0026amp; 黄卓. (2019). 文本大数据分析在经济学和金融学中的应用: 一个文献综述. 经济学 (季刊), 18(4), 1153-1186. [2]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.*管理世界*.2016;5:81-98. [3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21. [4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020 [5]Loughran T, McDonald B. Textual analysis in accounting and finance: A survey[J]. *Journal of Accounting Research*, 2016, 54(4): 1187-1230. Author links open overlay panelComputational socioeconomics [6]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026#34;Uniting the tribes: Using text for marketing insight.\u0026#34; *Journal of Marketing* 84, no. 1 (2020): 1-25. [7]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. \u0026#34;Lazy prices.\u0026#34; *The Journal of Finance* 75, no. 3 (2020): 1371-1415. [8]孟庆斌, 杨俊华, 鲁冰. 管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J]. *中国工业经济*, 2017 (12): 132-150. [9]Wang, Quan, Beibei Li, and Param Vir Singh. \u0026#34;Copycats vs. Original Mobile Apps: A Machine Learning Copycat-Detection Method and Empirical Analysis.\u0026#34; *Information Systems Research* 29.2 (2018): 273-291. [10]Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” _Journal of Consumer Research_ 47, no. 5 (2021): 787-806. [11]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27. [12]曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160. [13]彭红枫, \u0026amp; 林川. (2018). 言之有物: 网络借贷中语言有用吗?——来自人人贷借款描述的经验证据[J]. 金融研究, 461(11), 133-153. [14]吴非, 胡慧芷, 林慧妍, and 任晓怡. “企业数字化转型与资本市场表现——来自股票流动性的经验证据[J].” 管理世界 (2021). [15]姜富伟,胡逸驰,黄楠.央行货币政策报告文本信息、宏观经济与股票市场[J].金融研究,2021,(06):95-113. ","permalink":"/blog/management_python_course/","summary":"Python实证指标构建与文本分析  点击上方图片购买课程   概览 为何要学Python？ 在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！\n在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：\n 网络爬虫技术 解决 如何从网络世界中高效地 采集数据？ 文本分析技术 解决 如何从杂乱的文本数据中实证指标(情感、态度、刻板印象等)？  发票事项 如需发票，请先加微信372335839， 咨询发票细节，再作购买\n- 企业名称：哈尔滨所以然信息技术有限公司 - 企业税号：91230109MABT7KBC4M - 银行账户: 6228400176412884160 - 开户行: 中国农业银行哈尔滨香坊支行 课程纲要  课程目标： 掌握Python语法、网络爬虫、数据分析Pandas、文本分析、机器学习、词嵌入与认知 核心知识点： 爬虫原理及应用、 非结构化文本数据挖掘的思路及方法、机器学习应用等 环境配置: 本文使用Anaconda作为Python的软件安装包，注意安装过程中勾选Add Anaconda 3.x to PATH 课件资料： 本课程全部使用jupyter notebook文件作为课程课件  课程特色  接地气： 以经管学术需求为导向， 将Python分为语法篇、采集数据篇、文本分析篇、机器学习篇四大部分 好理解： 知识点力求通俗易懂，少了晦涩的计算机术语，多了通俗易懂的使用场景和实战讲解 上手快： 所有知识点均有可重复使用的代码块，犹如一块块的积木，课后您可以根据分析需要，快速搭建出自己的Python代码 技术新： 最新词嵌入，可挖掘文本中的态度、偏见、刻板印象等。  经管-经典文本分析方法 在这里我把技术细分为词频、词袋、w2v建词典、w2v认知变迁四个维度，这四大技术方法在本课程中均有体现。为了直观了解课程价值，这里附上7篇文献，大家可以购课前以做参考。\n   文献 定性 词频 词袋 W2V建词典 W2V认知变迁     王伟, 陈伟, 祝效国 and 王洪伟, 2016.","title":"Python实证指标构建与文本分析"},{"content":"大家好，五一劳动节假期我们将迎来了新的一期“结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘”工作坊，欢迎大家报名参加。我们将分七次为大家介绍本次工作坊的详细内容，敬请期待。\n关于我们 为推动我国经济、统计等社会科学量化研究方法学习与应用，培养和训练社会科学相关领域的青年学者、硕博士研究生，促进社会科学相关领域研究方法科学化规范化，“结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘”五一工作坊为广大学者提供了一个高水平学术交流、研究方法普及与研究经验分享的平台。工作坊采用模块式教学方法，不仅侧重经济、统计等社会科学量化基本方法的介绍，而且更加注重研究设计与研究选题训练，注重理论实践相结合，培养学员社会科学量化分析研究的综合能力。\n结构模型又称为结构计量模型，是将经济学模型和统计模型结合，用于估计描述现实的深层参数，模拟现实世界，以便合理地评估政策效果的实证工具。结构模型通过建立引起因果关系的数据生成具体方式（机制）的模型来解决简化型中的问题。模型中明确地指明了一些重要的外部因素（如政策）是如何影响通过某些参数来影响参与人决策的，那么通过改变这些外部因素并结合现有数据所估计出来的参数，结构模型便可以提供一系列反事实推断，对政策的制定有重要的意义。政策评估需要建立在理解对政策不变的“深层”参数之上。在结构式方法中，理论和实证的联系是紧密的。由于其建模技术的优雅和深刻，不仅是当今经济政策评估领域的前沿，也是发展经济理论的有力武器，在世界顶级期刊中，采用结构模型建模的文章引起广泛关注和引用，为所在学科的理论发展和政策评估带来深刻影响。\n实证研究过程中学者普遍面临数据获取、清洗和编码的两大问题。在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用计量分析方法对数据进行分析。但大数据时代，网络数据成为亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两大问题，即：①从网络世界中高效地采集数据 ②从纷杂的文本数据抽取文本信息。\n在获取数据及文本信息后，需要使用计量方法对数据进行分析处理。Stata、ArcGIS、Matlab等软件功能日益强大，理论也与时俱进。前沿分析固然可能会给你的Paper加分，但不理解其理论依据，会导致前沿方法的滥用, 使你的研究大为失色。\nDSGE，全称是dynamic stochastic general equilibrium，即动态随机一般均衡模型。是目前在宏观经济学研究占重要地位（甚至是主导地位）的模型方法，主要用于讨论经济增长、经济周期以及讨论政策工具效果（财政和货币政策）。我们需要对DSGE的深入学习。\n为此，本次五一工作坊特别邀请七位走在理论实证、数据分析前沿的学者，为广大热爱经济学的学生、青年教师，讲解当下前沿模型的核心思想，基于Matlab、ArcGIS、Stata、Python等软件的实践操作。在这个知识与思想碰撞的时代，我们将与您分享最前沿的思想与实践技巧！为您带来最前沿计量经济理论与扎实操作并重的高质量课程。\n工作坊详情页 由刘文革老师总筹划、谢杰老师协调发起工作坊，工作坊由7位老师分讲。\n点击下方链接，进入课程详情页，每门课程费用2000元(邓建鹏老师课程1000元)。\n 结构模型(一) -邹建文(中南财经政法大学) 结构模型(二) -邓建鹏(上海财经大学) DSGE-王文甫(四川大学) Stata实证前沿(一)-王非(中国人民大学) Stata实证前沿(二)-司继春(上海对外经贸大学) 空间计量-李光勤(安徽财经大学) Python数据挖掘-邓旭东(哈尔滨工业大学)  授课方式  时间  2022年五一期间（具体时间待定） 每天6小时（8:30 — 11:30；14:00 — 17:00）+ 30分钟答疑（部分课程晚间18:30-21:30进行）   地点: 小鹅通平台（线上直播） 每门课程2000元，视频保留10天；邓建鹏老师课程1000元  \n报名信息 全国高等院校及研究机构从事经济科学研究的青年师生。尤其适合那些希望掌握高级实证方法，提升量化研究设计能力和国家课题申报能力的研究者。\n费用  每门课程2000元(每位老师讲授一门)；邓建鹏老师课程1000元  优惠政策  个人报名优惠：报名两位老师的课程9折；三位老师的课程8折；四位及以上老师的课程7.5折；老学员9折；学生优惠200元/人。 团队报名优惠：三人成团及以上9折；五人成团及以上8折。  报名时间 从即日起\n报名咨询  17816181460（同微信）（汪老师）  缴费信息  单位：杭州国商智库信息技术服务有限公司 开户银行： 中国银行杭州大学城支行 银行账户：6232636200100260588  ","permalink":"/blog/2022-05-workshop/","summary":"大家好，五一劳动节假期我们将迎来了新的一期“结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘”工作坊，欢迎大家报名参加。我们将分七次为大家介绍本次工作坊的详细内容，敬请期待。\n关于我们 为推动我国经济、统计等社会科学量化研究方法学习与应用，培养和训练社会科学相关领域的青年学者、硕博士研究生，促进社会科学相关领域研究方法科学化规范化，“结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘”五一工作坊为广大学者提供了一个高水平学术交流、研究方法普及与研究经验分享的平台。工作坊采用模块式教学方法，不仅侧重经济、统计等社会科学量化基本方法的介绍，而且更加注重研究设计与研究选题训练，注重理论实践相结合，培养学员社会科学量化分析研究的综合能力。\n结构模型又称为结构计量模型，是将经济学模型和统计模型结合，用于估计描述现实的深层参数，模拟现实世界，以便合理地评估政策效果的实证工具。结构模型通过建立引起因果关系的数据生成具体方式（机制）的模型来解决简化型中的问题。模型中明确地指明了一些重要的外部因素（如政策）是如何影响通过某些参数来影响参与人决策的，那么通过改变这些外部因素并结合现有数据所估计出来的参数，结构模型便可以提供一系列反事实推断，对政策的制定有重要的意义。政策评估需要建立在理解对政策不变的“深层”参数之上。在结构式方法中，理论和实证的联系是紧密的。由于其建模技术的优雅和深刻，不仅是当今经济政策评估领域的前沿，也是发展经济理论的有力武器，在世界顶级期刊中，采用结构模型建模的文章引起广泛关注和引用，为所在学科的理论发展和政策评估带来深刻影响。\n实证研究过程中学者普遍面临数据获取、清洗和编码的两大问题。在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用计量分析方法对数据进行分析。但大数据时代，网络数据成为亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两大问题，即：①从网络世界中高效地采集数据 ②从纷杂的文本数据抽取文本信息。\n在获取数据及文本信息后，需要使用计量方法对数据进行分析处理。Stata、ArcGIS、Matlab等软件功能日益强大，理论也与时俱进。前沿分析固然可能会给你的Paper加分，但不理解其理论依据，会导致前沿方法的滥用, 使你的研究大为失色。\nDSGE，全称是dynamic stochastic general equilibrium，即动态随机一般均衡模型。是目前在宏观经济学研究占重要地位（甚至是主导地位）的模型方法，主要用于讨论经济增长、经济周期以及讨论政策工具效果（财政和货币政策）。我们需要对DSGE的深入学习。\n为此，本次五一工作坊特别邀请七位走在理论实证、数据分析前沿的学者，为广大热爱经济学的学生、青年教师，讲解当下前沿模型的核心思想，基于Matlab、ArcGIS、Stata、Python等软件的实践操作。在这个知识与思想碰撞的时代，我们将与您分享最前沿的思想与实践技巧！为您带来最前沿计量经济理论与扎实操作并重的高质量课程。\n工作坊详情页 由刘文革老师总筹划、谢杰老师协调发起工作坊，工作坊由7位老师分讲。\n点击下方链接，进入课程详情页，每门课程费用2000元(邓建鹏老师课程1000元)。\n 结构模型(一) -邹建文(中南财经政法大学) 结构模型(二) -邓建鹏(上海财经大学) DSGE-王文甫(四川大学) Stata实证前沿(一)-王非(中国人民大学) Stata实证前沿(二)-司继春(上海对外经贸大学) 空间计量-李光勤(安徽财经大学) Python数据挖掘-邓旭东(哈尔滨工业大学)  授课方式  时间  2022年五一期间（具体时间待定） 每天6小时（8:30 — 11:30；14:00 — 17:00）+ 30分钟答疑（部分课程晚间18:30-21:30进行）   地点: 小鹅通平台（线上直播） 每门课程2000元，视频保留10天；邓建鹏老师课程1000元  \n报名信息 全国高等院校及研究机构从事经济科学研究的青年师生。尤其适合那些希望掌握高级实证方法，提升量化研究设计能力和国家课题申报能力的研究者。\n费用  每门课程2000元(每位老师讲授一门)；邓建鹏老师课程1000元  优惠政策  个人报名优惠：报名两位老师的课程9折；三位老师的课程8折；四位及以上老师的课程7.5折；老学员9折；学生优惠200元/人。 团队报名优惠：三人成团及以上9折；五人成团及以上8折。  报名时间 从即日起\n报名咨询  17816181460（同微信）（汪老师）  缴费信息  单位：杭州国商智库信息技术服务有限公司 开户银行： 中国银行杭州大学城支行 银行账户：6232636200100260588  ","title":"结构模型|DSGE|Stata实证前沿|空间计量|Python数据挖掘2022五一工作坊"},{"content":"词嵌入 前几天刚刚分享了，\n大数据时代下社会科学研究方法的拓展—基于词嵌入技术的文本分析的应用\n人类在书信、网络论坛留下语言、文字的过程中，也留下了自己的偏见、态度等主观认知信息（偏见、态度）。\n词嵌入做为一种词向量模型，可以从文本中计算出隐含的上下文情景信息，态度及偏见。通过词向量距离的测算，就可以间接测得不同群体对某概念(组织、群体、品牌、地域等)的态度偏见。感觉词嵌入技术用处很大，最近整理了下pnas、nature、science中的文献，对了，相当部分的pnas关于词嵌入的论文经常会提供原始数据及代码。\n目前有些Python库可以使用词嵌入模型展示人类认知偏见， 如:\n whatlies parallax wordbias  相关文献   冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html\n  Kozlowski, A.C., Taddy, M. and Evans, J.A., 2019. The geometry of culture: Analyzing the meanings of class through word embeddings. American Sociological Review, 84(5), pp.905-949.\n  Toubia, O., Berger, J. and Eliashberg, J., 2021. How quantifying the shape of stories predicts their success. Proceedings of the National Academy of Sciences, 118(26).\n  Caliskan A, Bryson JJ, Narayanan A. Semantics derived automatically from language corpora contain human-like biases. Science. 2017;356: 183–186.\n  Garg N, Schiebinger L, Jurafsky D, Zou J. Word embeddings quantify 100 years of gender and ethnic stereotypes . Proceedings of the National Academy of Sciences. 2018. pp. E3635–E3644. doi:10.1073/pnas.1720347115\n  Garg, N., Schiebinger, L., Jurafsky, D. and Zou, J., 2018. Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16), pp.E3635-E3644.\n  Peng, H., Ke, Q., Budak, C., Romero, D.M. and Ahn, Y.Y., 2021. Neural embeddings of scholarly periodicals reveal complex disciplinary organizations. Science Advances, 7(17), p.eabb9004.\n  Waller, I. and Anderson, A., 2021. Quantifying social organization and political polarization in online platforms. Nature, 600(7888), pp.264-268.\n  Arseniev-Koehler, A., Cochran, S.D., Mays, V.M., Chang, K.W. and Foster, J.G., 2022. Integrating topic modeling and word embedding to characterize violent deaths. Proceedings of the National Academy of Sciences, 119(10), p.e2108801119.\n  Bollen, J., Ten Thij, M., Breithaupt, F., Barron, A.T., Rutter, L.A., Lorenzo-Luaces, L. and Scheffer, M., 2021. Historical language records reveal a surge of cognitive distortions in recent decades. Proceedings of the National Academy of Sciences, 118(30).\n  Kim, L., Smith, D.S., Hofstra, B. and McFarland, D.A., 2022. Gendered knowledge in fields and academic careers. Research Policy, 51(1), p.104411.\n  Lawson, M.A., Martin, A.E., Huda, I. and Matz, S.C., 2022. Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language. Proceedings of the National Academy of Sciences, 119(9), p.e2026443119.\n  Brady, W.J., McLoughlin, K., Doan, T.N. and Crockett, M.J., 2021. How social learning amplifies moral outrage expression in online social networks. Science Advances, 7(33), p.eabe5641.\n  Bailey, A.H., Williams, A. and Cimpian, A., 2022. Based on billions of words on the internet, people= men. Science Advances, 8(13), p.eabm2463.\n  Lewis, M. and Lupyan, G., 2020. Gender stereotypes are reflected in the distributional structure of 25 languages. Nature human behaviour, 4(10), pp.1021-1028.\n  Schramowski, P., Turan, C., Andersen, N., Rothkopf, C.A. and Kersting, K., 2022. Large pre-trained language models contain human-like biases of what is right and wrong to do. Nature Machine Intelligence, 4(3), pp.258-268.\n  Costa-jussà, M.R., 2019. An analysis of gender bias studies in natural language processing. Nature Machine Intelligence, 1(11), pp.495-496.\n  Rodman, E., 2020. A timely intervention: Tracking the changing meanings of political concepts with word vectors. Political Analysis, 28(1), pp.87-111.\n  Bhatia, S., 2017. Associative judgment and vector space semantics. Psychological review, 124(1), p.1.\n  Kurdi, B., Mann, T.C., Charlesworth, T.E. and Banaji, M.R., 2019. The relationship between implicit intergroup attitudes and beliefs. Proceedings of the National Academy of Sciences, 116(13), pp.5862-5871.\n  Charlesworth, T.E., Yang, V., Mann, T.C., Kurdi, B. and Banaji, M.R., 2021. Gender stereotypes in natural language: Word embeddings show robust consistency across child and adult language corpora of more than 65 million words. Psychological Science, 32(2), pp.218-240.\n  Bhatia, S., 2019. Predicting risk perception: New insights from data science. Management Science, 65(8), pp.3800-3823.\n  Rheault, L. and Cochrane, C., 2020. Word embeddings for the analysis of ideological placement in parliamentary corpora. Political Analysis, 28(1), pp.112-133.\n  Yang, K., Lau, R.Y. and Abbasi, A., 2022. Getting Personal: A Deep Learning Artifact for Text-Based Measurement of Personality. Information Systems Research.\n  Rodman, E., 2020. A timely intervention: Tracking the changing meanings of political concepts with word vectors. Political Analysis, 28(1), pp.87-111.\n  Margulis, E.H., Wong, P.C., Turnbull, C., Kubit, B.M. and McAuley, J.D., 2022. Narratives imagined in response to instrumental music reveal culture-bounded intersubjectivity. Proceedings of the National Academy of Sciences, 119(4).\n  Thompson, B., Roberts, S.G. and Lupyan, G., 2020. Cultural influences on word meanings revealed through large-scale semantic alignment. Nature Human Behaviour, 4(10), pp.1029-1038.\n  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/literatureembeddings/","summary":"词嵌入 前几天刚刚分享了，\n大数据时代下社会科学研究方法的拓展—基于词嵌入技术的文本分析的应用\n人类在书信、网络论坛留下语言、文字的过程中，也留下了自己的偏见、态度等主观认知信息（偏见、态度）。\n词嵌入做为一种词向量模型，可以从文本中计算出隐含的上下文情景信息，态度及偏见。通过词向量距离的测算，就可以间接测得不同群体对某概念(组织、群体、品牌、地域等)的态度偏见。感觉词嵌入技术用处很大，最近整理了下pnas、nature、science中的文献，对了，相当部分的pnas关于词嵌入的论文经常会提供原始数据及代码。\n目前有些Python库可以使用词嵌入模型展示人类认知偏见， 如:\n whatlies parallax wordbias  相关文献   冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html\n  Kozlowski, A.C., Taddy, M. and Evans, J.A., 2019. The geometry of culture: Analyzing the meanings of class through word embeddings. American Sociological Review, 84(5), pp.905-949.\n  Toubia, O., Berger, J. and Eliashberg, J., 2021. How quantifying the shape of stories predicts their success. Proceedings of the National Academy of Sciences, 118(26).","title":"文献汇总 | 词嵌入 与 社会科学中的偏见(态度)"},{"content":"前不久分享了一篇JCR2018的综述 营销研究中文本分析应用概述(含案例及代码) \n最近看到一篇JCR2021的实证 语言具体性如何影响消费者态度 ，研究者从一个现象, 即消费者可以通过感知店员的表达具体(例如，更多的名词而非代词)，判断店员是否用心倾听自己的需求。这有点像三十年前， 在服务态度不好的百货商场，店员往往爱答不理。\n大邓作为消费者，相比1、2、3三种表达方式，我会更喜欢会觉得4、5、6句子中使用**较多细节、具体词的店员。**而简短表达，代词过多，表明店员连嘴都懒得张口服务我，似乎对我们的切身需求关注不足，态度好不端正的感觉。文中对店员言语具体性表达给出了建议及原因，例子如下图。\n 结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘|2022五一工作坊 想随时随地系统学习Python文本分析，可以选择  Python网络爬虫与文本分析 | 2021录播课(虽是录播，但章节更多一些)。   更喜欢有互动感通过直播学习，可以考虑  Python网络爬虫与文本分析 | 2022五一直播    我找了三篇文本具体性的论文，文章结尾附有具体性的Python案例代码，希望能对大家有帮助。\n语言具体性 语言具体性Concreteness描述了一个词在多大程度上是指一个实际的、有形的或“真实的”实体，以一种更具体、更熟悉、更容易被眼睛或心灵感知的方式描述对象和行为（即，可想象或生动；Brysbaert, Warriner, and Kuperman 2014; Semin and Fiedler 1988). 我找了三篇文本具体性的论文，文章结尾附有具体性的Python案例代码，希望能对大家有帮助。\n具体性词典 Brysbaert, Warriner, A. B., \u0026amp; Kuperman, V. (2014) 找4000人，网络众包标注，开发了英文40000词的具体性词典。下图是对应的词典excel文件，字段Conc.M就是对应词语的具体性得分。\n心理距离与语言具体性 Snefjella, Bryor, and Victor Kuperman(2015)挖掘了心理距离与语言具体性之间的数学关系， 第一次将心理距离看做连续性变量进行度量(而之前的研究几乎只把心理距离设置为高、低二分类变量)，计算过程使用了Brysbaert2014的语言具体性词典度量。\n实验结果与我们认知相吻合，基本上心理距离越大， 具体性得分越小；反之，也成立。下面我列出在地理、时间、社会三个维度的量化可视化结果。\n地理维度 时间维度 社会维度 代码实现-以JCR为例 消费者经常对客户服务感到沮丧。 但是语言的简单转变是否有助于提高客户满意度？ 我们认为，语言具体性linguistic concreteness——员工在与客户交谈时使用的词语的有形性tangibility、具体性speciﬁcity或可想象性imaginability——可以塑造消费者的态度和行为。 五项研究，包括对两个不同领域环境中超过 1,000 次真实消费者-员工互动的文本分析，表明当员工与他们具体交谈时，客户会更满意、更愿意购买和购买。 这是因为客户推断使用更具体语言的员工正在倾听（即关注并理解他们的需求)。 这些发现加深了对语言如何塑造消费者行为的理解，揭示了具体性影响人们感知的心理机制，并为管理者帮助提高客户满意度提供了一种直接的方法。\n假设我们作为消费者，看到员工对同一个意思使用如下不同表达，\n相比4、5、6这三种表达方式，会觉得句子中使用**较多代词的店员懒得说话(态度不端正)。**而使用较多名词和形容词，会表明店员关注我们的切身需求。这篇JCR就是从这个角度切入的研究。\nJCR文中具体性计算说明\nWe computed a concreteness score for each conversational turn (averaging across all words in that turn) and for each conversational participant (averaging across all words over all their turns). Results were the same whether or not stop words commonly excluded from linguistics analyses (e.g., but, and) were included. We report results excluding stop words. 按照我的理解， 设计如下算法\n 对文本(会话)使用nltk分词，得到词语列表 在具体性词典中查询对应的具体性得分 得到文本的具体性得分(句子所有词的具体性得分加总除以词数)  方法一 import pandas as pd from nltk.tokenize import word_tokenize #JCR文中使用的Paetzold2016的词典 # Paetzold2016文中的词典下载链接失效。这里使用Brysbaert2014的词典 df = pd.read_excel(\u0026#34;Concreteness_ratings_Brysbaert_et_al_BRM.xlsx\u0026#34;) from nltk.tokenize import word_tokenize def query_concreteness(word): \u0026#34;\u0026#34;\u0026#34; 查询word的具体性得分 \u0026#34;\u0026#34;\u0026#34; try: return df[df[\u0026#34;Word\u0026#34;]==word][\u0026#39;Conc.M\u0026#39;].values[0] except: return 0 def concreteness_score(text): \u0026#34;\u0026#34;\u0026#34; 计算文本的具体性得分 \u0026#34;\u0026#34;\u0026#34; score = 0 text = text.lower() try: words = word_tokenize(text) except: print(\u0026#39;你的电脑nltk没配置好，请观看视频https://www.bilibili.com/video/BV14A411i7DB\u0026#39;) words = text.split(\u0026#39; \u0026#39;) for word in words: try: score += query_concreteness(word=word) except: score += 0 return score/len(words) # 案例 employee_replys = [\u0026#34;I\u0026#39;ll go look for that\u0026#34;, \u0026#34;I\u0026#39;ll go search for that\u0026#34;, \u0026#34;I\u0026#39;ll go search for that top\u0026#34;, \u0026#34;I\u0026#39;ll go search for that t-shirt\u0026#34;, \u0026#34;I\u0026#39;ll go look for that t-shirt in grey\u0026#34;, \u0026#34;I\u0026#39;ll go search for that t-shirt in grey\u0026#34;] for idx, reply in enumerate(employee_replys): score=concreteness_score(reply) template = \u0026#34;Concreteness Score: {score:.2f}| Example-{idx}: {exmaple}\u0026#34; print(template.format(score=score, idx=idx, exmaple=reply)) Run\nConcreteness Score: 1.55 | Example-0: I\u0026#39;ll go look for that Concreteness Score: 1.55 | Example-1: I\u0026#39;ll go search for that Concreteness Score: 1.89 | Example-2: I\u0026#39;ll go search for that top Concreteness Score: 2.04 | Example-3: I\u0026#39;ll go search for that t-shirt Concreteness Score: 2.37 | Example-4: I\u0026#39;ll go look for that t-shirt in grey Concreteness Score: 2.37 | Example-5: I\u0026#39;ll go search for that t-shirt in grey 员工的表达越具体，具体性得分越高。\n跟JCR中的得分不一样，但是案例的得分趋势是一致的。基本上从上至下，每个员工回复对应的具体性得分越来越高。\n方法二 cntext内置了效价情感分析函数和Concreteness词典，因此本任务实际上可以用cntext完成。\npip3 install cntext==1.7.7 代码\nimport cntext as ct # load the concreteness.pkl dictionary file concreteness_df = ct.load_pkl_dict(\u0026#39;Concreteness.pkl\u0026#39;)[\u0026#39;Concreteness\u0026#39;] concreteness_df.head() Run\n    word valence     0 roadsweeper 4.85   1 traindriver 4.54   2 tush 4.45   3 hairdress 3.93   4 pharmaceutics 3.77    reply = \u0026#34;I\u0026#39;ll go look for that\u0026#34; score=ct.sentiment_by_valence(text=reply, diction=concreteness_df, lang=\u0026#39;english\u0026#39;) score Run\n1.85 \nemployee_replys = [\u0026#34;I\u0026#39;ll go look for that\u0026#34;, \u0026#34;I\u0026#39;ll go search for that\u0026#34;, \u0026#34;I\u0026#39;ll go search for that top\u0026#34;, \u0026#34;I\u0026#39;ll go search for that t-shirt\u0026#34;, \u0026#34;I\u0026#39;ll go look for that t-shirt in grey\u0026#34;, \u0026#34;I\u0026#39;ll go search for that t-shirt in grey\u0026#34;] for idx, reply in enumerate(employee_replys): score=ct.sentiment_by_valence(text=reply, diction=concreteness_df, lang=\u0026#39;english\u0026#39;) template = \u0026#34;Concreteness Score: {score:.2f}| Example-{idx}: {exmaple}\u0026#34; print(template.format(score=score, idx=idx, exmaple=reply)) ct.sentiment_by_valence(text=text, diction=concreteness_df, lang=\u0026#39;english\u0026#39;) Run\nConcreteness Score: 1.55 | Example-0: I\u0026#39;ll go look for that Concreteness Score: 1.55 | Example-1: I\u0026#39;ll go search for that Concreteness Score: 1.89 | Example-2: I\u0026#39;ll go search for that top Concreteness Score: 2.04 | Example-3: I\u0026#39;ll go search for that t-shirt Concreteness Score: 2.37 | Example-4: I\u0026#39;ll go look for that t-shirt in grey Concreteness Score: 2.37 | Example-5: I\u0026#39;ll go search for that t-shirt in grey \n代码获取 点击下载本文代码\n  结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘|2022五一工作坊\n  想随时随地系统学习Python文本分析，可以选择\n Python网络爬虫与文本分析 | 2021录播课(虽是录播，但章节更多一些)。    更喜欢有互动感通过直播学习，可以考虑\n Python网络爬虫与文本分析 | 2022五一直播    相关文献 Brysbaert, M., Warriner, A. B., \u0026amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911\nSnefjella, Bryor, and Victor Kuperman. \u0026ldquo;Concreteness and psychological distance in natural language use.\u0026rdquo; Psychological science 26, no. 9 (2015): 1449-1460.\nPaetzold, G. H., and L. Specia (2016), “Inferring Psycholinguistic Properties of Words,” in Proceedings of the North American Association for Computational Linguistics-Human Language Technologies 2016, 435–40.\nPackard, Grant, and Jonah Berger. \u0026ldquo;How concrete language shapes customer satisfaction.\u0026rdquo; Journal of Consumer Research 47, no. 5 (2021): 787-806.\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/jcr_concreteness_computation/","summary":"前不久分享了一篇JCR2018的综述 营销研究中文本分析应用概述(含案例及代码) \n最近看到一篇JCR2021的实证 语言具体性如何影响消费者态度 ，研究者从一个现象, 即消费者可以通过感知店员的表达具体(例如，更多的名词而非代词)，判断店员是否用心倾听自己的需求。这有点像三十年前， 在服务态度不好的百货商场，店员往往爱答不理。\n大邓作为消费者，相比1、2、3三种表达方式，我会更喜欢会觉得4、5、6句子中使用**较多细节、具体词的店员。**而简短表达，代词过多，表明店员连嘴都懒得张口服务我，似乎对我们的切身需求关注不足，态度好不端正的感觉。文中对店员言语具体性表达给出了建议及原因，例子如下图。\n 结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘|2022五一工作坊 想随时随地系统学习Python文本分析，可以选择  Python网络爬虫与文本分析 | 2021录播课(虽是录播，但章节更多一些)。   更喜欢有互动感通过直播学习，可以考虑  Python网络爬虫与文本分析 | 2022五一直播    我找了三篇文本具体性的论文，文章结尾附有具体性的Python案例代码，希望能对大家有帮助。\n语言具体性 语言具体性Concreteness描述了一个词在多大程度上是指一个实际的、有形的或“真实的”实体，以一种更具体、更熟悉、更容易被眼睛或心灵感知的方式描述对象和行为（即，可想象或生动；Brysbaert, Warriner, and Kuperman 2014; Semin and Fiedler 1988). 我找了三篇文本具体性的论文，文章结尾附有具体性的Python案例代码，希望能对大家有帮助。\n具体性词典 Brysbaert, Warriner, A. B., \u0026amp; Kuperman, V. (2014) 找4000人，网络众包标注，开发了英文40000词的具体性词典。下图是对应的词典excel文件，字段Conc.M就是对应词语的具体性得分。\n心理距离与语言具体性 Snefjella, Bryor, and Victor Kuperman(2015)挖掘了心理距离与语言具体性之间的数学关系， 第一次将心理距离看做连续性变量进行度量(而之前的研究几乎只把心理距离设置为高、低二分类变量)，计算过程使用了Brysbaert2014的语言具体性词典度量。\n实验结果与我们认知相吻合，基本上心理距离越大， 具体性得分越小；反之，也成立。下面我列出在地理、时间、社会三个维度的量化可视化结果。\n地理维度 时间维度 社会维度 代码实现-以JCR为例 消费者经常对客户服务感到沮丧。 但是语言的简单转变是否有助于提高客户满意度？ 我们认为，语言具体性linguistic concreteness——员工在与客户交谈时使用的词语的有形性tangibility、具体性speciﬁcity或可想象性imaginability——可以塑造消费者的态度和行为。 五项研究，包括对两个不同领域环境中超过 1,000 次真实消费者-员工互动的文本分析，表明当员工与他们具体交谈时，客户会更满意、更愿意购买和购买。 这是因为客户推断使用更具体语言的员工正在倾听（即关注并理解他们的需求)。 这些发现加深了对语言如何塑造消费者行为的理解，揭示了具体性影响人们感知的心理机制，并为管理者帮助提高客户满意度提供了一种直接的方法。","title":"计算文本的语言具体性 | 以JCR2021论文为例"},{"content":"活动预告  想随时随地系统学习Python文本分析，可以选择  Python网络爬虫与文本分析 | 2021录播课(虽是录播，但章节更多一些)。   更喜欢有互动感通过直播学习，可以考虑  Python网络爬虫与文本分析 | 2022五一直播    文献 本文全文摘自\n冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html\n词嵌入亮点 词嵌入技术是文本分析中技术含量较高，可从文本中测量出人类认知信息。而即时是有一定学习能力的人，当阅读大量文本很难察觉文中是否有内置(预置)的信息，如作者的偏见、态度、刻板印象，等人类复杂认知。词嵌入技术可以将这类难以察觉的线索挖掘、测量。\n摘要 在大数据时代的背景下，基于大数据的分析处理技术为以“数据驱动”的社会科学研究创造了新的发 展契机。其中，词嵌入(Word Embeddings)技术借势大数据浪潮，以其高效的词表征能力和强大的迁移学习能力在文本分析领域受到越来越多的关注。不同于传统的文本分析路径，词嵌入技术不仅实现了对非结构化文本数据的表征，还保留了丰富的语义信息，可以实现对跨时间、跨文化文本中深层次文化信息的挖掘， 极大丰富了传统的社会科学实证的研究方法。文章总结了词嵌入技术的基本原理及特点，系统地梳理了词 嵌入技术的六大应用主题：社会偏见、概念联想、语义演变、组织关系、文本情感和个体决策机制。随后， 文章归纳了词嵌入技术的基本应用流程。词嵌入技术还面临文本数据的选择、中文文本的分词处理、单词 语义信息的表征层次三种挑战，文章归纳了相应的应对思路与方法。最后，基于词嵌入技术的强大适应能 力，未来研究可以进一步关注该技术在管理领域的应用前景，包括政策效应评估、用户推荐系统、品牌管 理、企业关系管理、组织内部管理、中国传统智慧与管理问题六个方面。\n关键词: 词嵌入；自然语言处理；文本分析；社会科学；管理领域应用\n\n引言 作为人类开展文化交流和情感沟通的基本载体，语言承担了重要的信息交换功能。借助 于各类语言表达形式，人们将诸如知觉、思维、态度和情感等复杂的心理活动转化成特定的 语言。 而作为语言的典型载体之一，文本既能够在个体层面上反映人们的内心活动，也能 够在组织和社会层面上反映集体文化。 因此，从文本内容挖掘个体深层次的心理活动和 人类社会的文化沿革是社会科学的基本研究路径。\n长期以来，在社会科学尤其是管理学和心理学等领域，实证研究多以针对实验、问卷和结构化的二手数据的量化分析为主导，而对于非结构化的文本材料(如访谈记录)仍以质性分析为主。[5] 在大数据时代，“数据+行为+交叉学科”已成为社会科学发展的必然方向。而计算社会科学的兴起则为我们理解人类行为、探讨社会现象提供了新的研究素材、视角和手段[6] 。随着互联网技术的飞速发展，人们在网络上发表大量包含思维、情感、观点的文本信息，这些井喷式爆发的文本为“以数据驱动”的社会科学研究提供了可及的信息来源。若能对之加以利用，无疑将拓宽社会科学研究的方法。[7 - 9] 然而，社会科学领域的传统文本研究方法以人工编码为主，其时间投入过大、成本较高、客观性相对较弱等不足极大地限制了文本数据 [10] 在实证研究中的应用。 所幸的是，以自然语言处理(Natural Language Processing; NLP)为核心的计算机文本分析技术(Computerized Text Analysis)的发展为大数据文本在社会科学领域 中的应用带来了契机。 “词”作为文本的最小单元，是计算机进行文本分析的基础。在自然语言处理领域，“词” 主要以向量(vector)的形式表示。而词嵌入(Word Embeddings)技术，即是一种可以把高维词 向量映射进低维向量空间，以此来实现词义理解的计算机文本分析技术。相较于其他自然语 言技术，词嵌入技术不仅展现出了高效的学习能力，而且允许计算机从更高的意义单元(即 目标词的上下文)出发理解词义、刻画“词”之间的相对关系，因此逐渐成为了自然语言处理的重要工具，在管理学、心理学等社会科学领域取得了丰富的研究进展。相比于传统的、以人工编码和词频统计为主导的文本分析方式，词嵌入的独特优势在于：\n 借助计算机分析技术，可以在短时间内、以较低成本，实现对大规模文本数据的高效处理； 在挖掘文本特征和理解文本内容时，更多地依赖文本自身的分布规律，具有较强的客观性，其背后“数据驱动”的分析逻辑也使这一技术在探索性研究中具有较大的应用优势； 面对跨时间、跨文化比较的研究话题、以及在挖掘社会学、行为学变量及变量关系等领域展现出广阔的应用前景。  词嵌入技术已在社会科学领域得到了广泛的应用，主要包括：社会偏见 、概念联想[14] 、关系网络和判断机制[16] 等六大主题，大量研究见诸 Nature、Science、PNAS、Academy Management Journal、American Sociological Review、Management Science 等国际期刊。反观国内的社会科学领域，词嵌入方法的应用价值还未得到足够重视和讨论。据此，本文通过介绍词嵌入技术的基本原理、梳理国外社会科学领域对词嵌入的应用情况，以期帮助国内社会科学研究者了解该技术独特的应用价值，推动词嵌入技术在大数据时代背景下对我国社会科学研究方法的丰富和推动。具体而言，本文：\n 梳理了词嵌入技术原理，以帮助学者深入了解词嵌入在文本分析方面的技术优势； 梳理了现有文献中社会科学研究者们利用这一技术的相关应用研究，展现了在面对实际问题时可以如何利用词嵌入技术进行实证分析，以帮助学者了解该技术的适用领域； 总结词嵌入技术的基本应用流程，提供方法指引； 归纳了词嵌入技术面临的三方面挑战 ——文本数据的选择、中文文本的分词处理、语义信息表征，并提出了相应的应对思路； 从政策效应 评估、用户推荐系统、品牌管理、企业关系管理、组织管理与中国传统智慧和管理问题这六个方面出发，探讨词嵌入技术在以管理为代表的社会科学研究中的应用潜力和价值，以期丰富大数据时代背景下我国的管理科学研究方法。  \n一、词嵌入技术的基本原理 不同于基于词频统计的文本分析方法，词嵌入技术的核心特征在于从文本的全局语义信息出发对“词”进行表征学习， 即大规模利用文本中“词”的上下文信息，将文本词汇映射至高维向量空间以实现词的向量化表示，使得词向量之间既保留着“词”在语义层面的关联，又满足向量所适用的代数运算性质。 在此基础上，通过度量词向量之间的几何关系(即“距 离”)便能够刻画“词”在现实语义中的关系。 进一步地，我们利用词与词之间这种可被量化的语义关系来探讨社会科学领域下的概念之间的相似性或相关性，并由此反映特定的社会文化和认知现象，乃至刻画社会、心理变量与其它行为变量间的相关关系。因此，词嵌入技术的应用主要包含两大步骤，即首先利用词嵌入模型从文本数据中获得对“词”的向量表征，再计算词向量距离进行相关性分析。\n1. 1 “词”的向量表征 “词”的向量化表征是计算机进行文本分析的基础，也是词嵌入技术的本质属性。纵观计算机文本分析的历史，词向量的表征方法主要经历了以下两个发展阶段：\n第一个阶段： 假设词语之间相互独立\n 第一个阶段是从词典出发、基于词频统计规则对“词”的离散型表征。例如，热向量编码(one-hot vector)通过\n建立基于目标文本(“猫很可爱，狗也很可爱”)的分词词典({“猫”: 0，“狗”: 1，“也”: 2，“很”: 3，“可爱”: 4})，将每个“词”都表示为一个向量，使其维度与词典长度相当，且每个元素取值为0 或 1。\n“猫” = (1, 0, 0, 0, 0) “狗” = (0, 1, 0, 0, 0) “也” = (0, 0, 1, 0, 0) “很” = (0, 0, 0, 1, 0) “可爱” = (0, 0, 0, 0, 1)) 这一类词表征方法虽然简单直观，但是在面对大规模文本时，词典长度的激增易造成参数空间的“维度灾难”问题(Curse of Dimension)① 。并且，粗糙的信息表征思路忽视了“词”的频率、上下文以及“词”之间的关联，使得这一类词向量无法反映“词”的语义信息。\n第二阶段: 认为词语之间有千丝万缕的联系\n 为了提升词向量的表征质量，Deerwester 等[20] 主张从更高的文本意义单元理解文本词汇的含义。由此，分布式表征(distributional representation)[18] 成为了第二阶段的词表征方法。分布式假设(distributional hypothesis)是分布式表征方法的理论支撑，也是词嵌入技术背后的核心逻辑基础——即上下文相似的“词”拥有相似的或相关的语义，它能够反映了人类的语言使用习惯，也符合人的现实认知逻辑。[16,18,21] 人们倾向于对具有相似或者相关特征的对象产生认知关联，体现在文本层面则是相近的语言表达或高度的共现频率，即相似的上下文语境。基于此，通过分析目标词与其上下文词汇之间的统计分布规律可以学习到目标词的众多文本信息，使得词表征结果囊括文本语境的特征。因此，分布式表征的思想被广泛应用于后续的语义学习中，成为了词嵌入技术的基本逻辑。\n其中，较为出色且经典的是 Mikolov 等人在 2013 年提出的 Word2Vec 模型，[18] 它标志 着 词嵌入模型 的 正 式 诞 生 。除了 Word2Vec 词嵌入模型外， Pennington 等 [23] 提出了同样具有高效学习能力的 GloVe(Global Vectors for Word Representation)学习框架，通过对词共现矩阵的矩阵分解，实 现对“词”的表征。此外，为了提升训练速度、适应海量文本学习，后 多对于词的分布式表征的改进算法，包括：fastText 算法、 谷歌的 ELMo(Embedding from Language Models)语言模型[25] 和 BERT(Bidirectional Encoder Representation from Transform) 语言模型[26]等。\n在将文本信息嵌入进每一个“词”之后，我们获得了“词”的向量表示，即在词向量空间中的位置，而词与词之间的语义关联可以通过向量空间中点与点的位置距离来反映。词嵌入技 [27,28] 术下的向量具有两项重要的几何性质——“聚类”(clustering)和“并行”(parallel)。 其中，\n “聚类”性质是指，现实语义相近的“词”在向量空间中的位置也相近。 例如， “挪威”与“瑞 典”的词向量更接近，而“意大利”和“德国”的词向量更接近。 而“并行”性质是指，向量空 间中的词向量之间满足基本的代数运算性质，且这种运算逻辑基本符合“词”的现实语义逻辑。  [18] 例如，从语义逻辑来看，“国王(King)”和“王后(Queen)”的区别平行于“男人(Man)”和“女人 (Woman)”的区别，反映到对应词向量上即可以得到“King−Man+Woman=Queen”的代数形式。\n综合以上内容可知，词嵌入虽然聚焦在“词”这一最小的文本单位上，但是看到的是丰富的全局文本语义信在“词”上的投射和体现。这不仅与传统的、基于词频的文本分析方法在逻辑上有着本质的区别，更能够为文本分析提供更深刻、更生动的洞察，构成了词嵌入文本分析技术在大数据时代的社会科学领域相关研究中的独特应用优势。\n\n1.2 词向量的距离计算 在词嵌入领域，词向量间的“距离”是词与词之间相关程度的度量指标，是分析概念之间的相关性的基础。词嵌入技术将文本中的“词”映射为 N 维欧式空间中的“点”，“词”在空间中 的位置坐标即用其对应的 N 维向量来标识。由于点的位置反映了词语的语义，因而点与点之间的空间距离即反映了词与词之间的语义相似性，对词向量进行特定的代数运算(如加减、 内积等)能够用以度量词与词之间、概念与概念之间乃至文档与文档之间的相关性。\n1.2.1 词与词之间的距离 设在 n 维语义空间中，单词 A 和 B 分别对应词向量 vA=vA1,\u0026hellip;, vAn 和 vB=vB1,\u0026hellip;, vBn， vA 与 vB 之间的距离计算方式主要有以下两种：\n 余弦相似度(cosine similarity)：。余弦相似度衡量词向量 vA 和 vB 之间的向量夹角的余弦值，其取值范围为[-1,1]。余弦相似度取值为 0，则代表单词 A 和 B 之间不存在语义关系；而取值越靠近 1，表明单词 A 和 B 之间具有正相关性；反之，取值越 靠近-1，则表明单词 A 和 B 之间具有负相关性 欧式距离(euclidean distance)：欧式距离越小表明单词 A 和单词 B 在词向量空间中的位置越近，之间的语义关系越强；反之，欧式距离越大表明单 词 A 和 B 语义关系越弱。  1.2.2 概念与概念之间的距离 在词嵌入分析领域，一个概念是由一系列“相关词”组合而成，例如，“女性”概念可以通过“女人”、“女生”、“母亲”等名词来表达。而在对比不同属性概念(如“女性”vs.“男性”与“智慧”)之间的相关性时，需要逐个计算概念间的相对距离(“女性”与“智慧”的距离 vs.“男性”与“智慧”的距离)。Garg 等[13] 、Caliskan 等[17] 分别构建了以下两种相对距离的计算方法，并为众多后续研究所采用：\n  相对范数差函数(relative norm distance)：\n $$vm∈Mvm−vA−vm−vB $$    该函数用于衡量两项目标词概念与某一项特征概念的相对距离。其中，M 代表特征概念(如“智慧”)，vm 为所属概念的相关词向量(如“聪明”)；vA 和 vB 分别代表两类目标词向量(如“男性”vs.“女性”)。该函数的含义为：在“男性”和“女性”两类群体中，哪一类群体与“智慧”这一概念更相关。若函数值为正，则代表“女性”与“智慧”更相近；若函数值为负，则代表“男性”与“智慧”更相近；若函数值靠近 0，则表明“智慧”不存在明显的性别偏向。\n  词嵌入相关性检验 (Word Embeddings Association Test; WEAT)：\n $$s(X,Y,A,B)=vx∈Xs(vx,A,B)−vy∈Ys(vy,A,B)$$ $$s(vw,A,B)=meanva∈Acos(vw,va)−meanvb∈Bcos(vw,vb)$$ 该框架用于衡量两组目标词 X,Y(Target Words，如“男性”vs.“女性”)与两组属性词 A,B(Attribute Words，如“事业”vs.“家庭”)在语义上的相对距离差异，其中 vw 为所属概念的相关词向量(如在描述“男性”概念时，人们往往会使用“男生”、“父亲”、“男人”等词语)。s(vw,A,B)表示单词 vw 与两类属性词 A 和 B 的相对距离，正值代表其与 A 属性距离更近、语义更相关，反之则反；而 s(X,Y,A,B)则衡量了两项目标词 X,Y 和两项属性词 A,B 相对距离的差异，即在 “ 男性 ” 和 “ 女性 ” 两类群体中， 哪一类群体与 “ 事业 ” 或 “ 家庭 ” 的文化概念更相关。 若s(X,Y,A,B)为正值，则表明相比于“女性”，“男性”与“事业”的语义相关性更高，反之则表明“女性”与“事业”的语义相关性更高。此外，WEAT 框架还提供了相应结果的显著性检验方式以及效应量指标。    文档与文档之间的距离—词移距离(Word Mover’s Distance) minT≥0i,j=1nTij∙ci,j, s.t. jnTij=di , ∀iϵ1,\u0026hellip;,n\n  除了概念间的相关性分析，我们可以通过文档间的相似性来探讨如文本主题、个体及组织之间的相关性问题，Kusner 等[31] 提出“词移距离”这一计算方法能够实现文档间的相似 性度量。词移距离即一个向量空间移动到另一向量空间所需的最小距离，通过对两个向量语 义空间中所有词向量间的欧式距离进行加权求和，以此来衡量两个文本间的相似性，如上式 所示。其中，c(i, j)为词向量间的欧式距离；Tij 为词向量之间的权重(由 TF-IDF② 计算加权值)。 函数值越大代表两个文本的相关程度越低，反之越高。\n  \n二、词嵌入技术的优势 传统的社会科学研究通常需要借助科学实验、社会调查和人工编码等方法，依赖于专家学者的领域知识和实践直觉，存在主观性较强、耗时、耗资源的缺点。 另外，传统的社会科学研究局限于小样本数据和历史数据的不足，通常关注当下的、有限范围的社会情景， 难以进行跨时间、跨文化的分析。 反观，以词嵌入为代表的计算机化的文本分析：\n 可处理大文本数据，不仅极大地节省人力和时间投入，而且可以拓宽现有社会科学研究的素材； 能够利用现有数据和先验知识改进学习算法，可拓展性和重复性强； 能依据文本内在的分布规律和领域知识，挖掘人们内隐层面的认知信息，结果更加客观真实； 能够从大规模文本中挖掘代表整体社会的认知，尤其擅长进行跨时间、跨文化的文本研究，结论不仅具有广泛的样本代表性，而且可以展示相关文化概念、思想观念等研究对象的纵向历时演化。 这些优点极大地丰富了社会科学的研究方法，拓展了社会科学的研究视野。传统的社会科学 研究方法与词嵌入技术的具体区别见表 1。  表 1 传统的社会科学研究与词嵌入技术的区别\n   对比维度 传统的社会科学研究路径 基于词嵌入技术的社会科学研究路径     研究工具 问卷、访谈、实验、案例分析等 Word2Vec、GloVe 等词嵌入模型，以及词向量、概念及文本的相关性计算   方法依据 基于实践经验和严格的理论推断，依赖于特定学科内专家学者的领域知识和实践直觉，是以人为中心的研究方法；围绕研究假设进行数据检验的分析路径 基于语言文本来理解文化概念和思想观念，综合利用社会科学理论、计算机科学等探讨社会、心理和行为层面的问题，是人智与计算机相结合的分析方法 ；不依赖严格的假设，利用数据挖掘展开探索性的研究   检验标准 大部分研究结论缺少严格客观的评断标准，主观性较强 有多项较为成熟的指标及评价流程，具体包括：检验词嵌入模型的训练效度 ( 模型在特定的测试任务集上的表现 ) 和检验研究结论的外部效度(将结论与其他社会调查数据、其他研究方法的结果展开对比)   数据来源 调研记录、实验数据、二手数据、文献等；受限于成本投入，数据来源较为单一且体量较小 数据来源广泛，能够熟练处理包括会议记录、网络文本、新闻书籍等非结构化的文本数据；在处理大规模、跨文化、跨时间的文本数据时有极大的优势   信息层次 以基于自我报告的外显认知为主，在获取被访者内隐认知时依赖于间接的方法设计；研究较大地依赖于样本选择，结论在跨时间、跨文化上的代表性有限 允许研究者直接挖掘文本所反映的内隐社会认知；研究较多从社会、集体层面的文本范围着手，结论具有较强的代表性和普适性   其他特点 应用过程中耗时长、成本较高；主观性较强；结论的可复现性较弱 针对大数据文本的无监督训练、时间人力投入小；客观性较强；结论的可复现性较强    \n三、词嵌入技术在社会科学领域的应用话题 3.1 社会偏见/刻板印象 文本语言能够反映人类对世界的认知和态度，基于词嵌入的文本分析方法可以有效地挖掘社会偏见和刻板印象。Garg 等[13] 采用词嵌入技术分析了来自纽约时报、谷歌新闻、谷歌图书及美国历史文本库(Corpus of Historical American English; COHA③ )的文本数据，揭示了 1900～1990 年间美国社会在性别和种族两大议题上的刻板印象及其历史变化。Garg 等[13] 首先以十年为单位将文本数据分为 9 份，并针对每一份文本数据使用词嵌入技术，以获得“词” 的向量表征。然后，他们整理了相应概念的单词列表，其中包括：与“性别”维度相关的“男性”词汇[如 he]和“女性”词汇[如 she]；“白人”词汇[如 Harris]、“亚裔”词汇[如 Huang]和“拉丁 裔”词汇[如 Ruiz]。进一步，他们构建了相对范数差(relative norm difference)函数，用以计算 一组词向量(如“男性”vs.“女性”)与目标词(如专业工作)的相对距离，以此度量社会刻板印象 的程度。Garg 等[13] 发现近百年间在美国社会的认知中始终存在着较为明显的性别偏见和种 族偏见。例如，“女性”往往和护士、保洁、舞者、秘书等职业联系更为紧密，而与工程师、 木匠、技术人员等职业更为疏远。此外，亚裔姓名和教授、科学家、化学家和工程师等学术职位联系更紧密，白人姓名往往与警察、统计学家、摄影家等职位联系更紧密。通过纵向分析概念间距离随时间推移的动态演化，Garg 等[13] 发现，这两类刻板印象随着时间呈减弱势 态，结合相关社会科学理论和历史事件，他们进一步提出 60、70 年代的美国民权运动是改善刻板印象的关键事件，而基于词嵌入的概念间相对距离也如实地反映了美国民权运动对于 国民性别、种族认知的深刻影响。 作为文化的产物之一，歌曲也能反映社会认知中的偏见/刻板印象。Boghrati 和 Berger[12] 利用 Word2Vec 词嵌入技术，挖掘了自 1965 年至 2018 年的近 60 年间美国公告榜(Billboard) 上流行歌曲歌词文本中所隐含的性别偏见。一方面，他们在流行音乐中发现了显著的“厌女症(misogyny)”刻板印象(如图 2)，相比“男性”词汇，人们更少将“女性”词汇和“能力/ 成功/热情”等具有积极属性的词汇相关联。但另一方面，歌曲中所反映的性别偏见随时间的推移呈现逐渐减弱的趋势。进一步地，Boghrati 和 Berger[12] 通过控制歌曲创作者的性别因素，发现男性作词人是影响歌曲“厌女症”现象变化的关键因素。\n为了检测词嵌入技术是否能够有效挖掘文本中的社会偏见，Caliskan 等[17] 对比了词嵌入模型和内隐联想测试(Implicit Association Test; IAT)④ ——一种社会科学领域中最常用的社会 偏见/刻板印象等内隐认知的测量方法—的差异。虽然 IAT 能够有效捕捉个体的社会偏见的内隐认知，但是该方法需要严格的实验环境、耗时较长且测量样本往往受到时间和空间的局限。Caliskan 等[17] 利用 GloVe 词嵌入模型构建了 WEAT(Word Embedding Association Test) 分析框架，并利用这一框架研究了 8 项内隐认知，如“科学-男性”和“艺术-女性”，并将其与 基于 IAT 的研究结果进行了对比。[17] 他们的研究表明，基于词嵌入技术的结论与基于 IAT 的结论具有高度且稳定的一致性。Caliskan 等[17] 进一步指出，在未来的内隐态度研究中，词嵌入技术不仅能够作为 IAT 测量方法的补充，更能够在样本代表性、研究成本和研究视野等方面展现出 IAT 所不具备的独特优势。\n除了探讨社会偏见与刻板印象的跨时特征之外，词嵌入技术还适用于跨文化的对比分析。 Defranza 等 [33] 利用词嵌入技术探讨了不同地域在性别偏见方面的程度差异。 他们利用 fastText 模型和 WEAT 分析框架， 从 49 类不同语种的文本中分别挖掘不同社会文化下的性别偏见现象。 结果显示，当一个地区的性别语言(Gendered language)——即语言中更加区分词汇的性别属性和使用者的性别身份(如泰语、芬兰语)更强时，该地区的性别偏见更加明显(图 3)，具体表现为男性与积极属性词汇的关联性更强，而女性与积极属性的联系更弱。 这一结果在一定程度上验证了萨皮尔-沃夫假说(Sapir-Whorf hypothesis)，即语言能够塑造的思维和认知。\n\n3.2 文化认知 历史无法复刻，但承载历史痕迹的文本资料能够帮助人们窥探特定时代背景下的文化内涵。Kozlowski 等[28] 利用词嵌入技术分析了自 1900～1999 年来 100 余年间公开发表的书籍， 探讨了 20 世纪美国社会对七大等级文化(财富、道德、职业、性别、教育、品味、身份地位) 的共识和演变规律。首先，作者利用 Word2Vec 模型，构建了一系列标度等级文化维度(如“贫 穷-富有”、“道德-不道德”、“男性-女性”等)的词向量空间(如“性别-财富”、“职业-道德”、“品 味-职业”、“教育-地位”等)。进一步，作者将一系列目标词分别映射进相应的等级维度空间， 以此标度这些目标词的多元等级属性。例如，在被映射进如图 4 所展示的“性别—财富”等级 维度空间后，“volleyball(排球运动)”一词表现出更靠近“feminine(女性气质)”和“rich(富有)”的 等级特征。此外，标度不同等级维度的向量之间的夹角也具有一定的社会文化含义。例如， “教育”与“道德”和“品味”的相关性较强且保持历时的稳定性，但与“职业”这一维度的相关性 相对较弱，这说明教育水平的提升能够提升人的修养和品味，但对职位状态和等级(如“失业” 和“就业”)的影响较小。Kozlowski 等[28] 的研究结果表明，词向量不仅可以反映特定概念间的语义关联和相互作用，还能够用于探讨多元文化维度之间的语义结构，从而推动实现更高层次构念的表征。\n作为文化概念的关键形式，社会认知(social cognition)是人们对各种社会刺激的综合加工过程， 是人们的社会动机系统和社会情感系统形成变化的基础 。\n社会认知包括社会信念 (social belief)和社会态度(social attitude)。而根据内隐—外显双系统理论， 社会认知可进一步分为外显社会认知(explicit social cognition)和内隐社会认知(implicit social cognition) 两类\n 外显社会认知强调个体可以通过自省的方式(如问卷法)报告的社会认知 内隐社会认知描述个体无法内省的、潜意识层面的社会认知。  然而，受限于现有对内隐认知的挖掘手段，有关内隐信念 (Implicit belief，如“亚洲人很聪明”)和内隐态度(Implicit attitude，如“我喜欢亚洲人”)的关系，已有研究要么将其混为一谈，要么将其作为互不干涉的独立构念。为了厘清该问题，Kurdi 等[14] 利用基于预训练的 fastText 词嵌入技术，分析了内隐态度和内隐信念的关联，并对比其与外显态度和外显信念的差异。具体而言，基于被试自我报告的实验结果显示，个体的外显态度与外显信念存在方向上的不一致性，例如，亚裔群体常被白人给予负面的评价(外显态 度)，但在智商、能力方面被认为有突出优势(外显信念)；而基于词嵌入技术的分析结果则表 明内隐态度与内隐信念具有一致性，内隐态度驱使内隐信念的产生。[14 ] 例如，白人群体有 较高的自我评价(内隐态度)，也认为本群体的智商高于亚裔群体(内隐信念)。另外，Kurdi 等[14] 发现人们对同一类属性词的认知也存在差异，例如，人们倾向于认为“book-smart(有学 问的)”优于“street-smart(生存力强的)”。综合可知，词嵌入技术可以作为挖掘文化概念认知以及社会认知的有效工具。\n3.3 语义内涵演变 语言的涵义会随着时代发生改变，而词嵌入技术的一大突出优势即表现为处理跨时段的大文本数据。文本语言的运用具有系统规律性，[35] 词嵌入模型训练生成的词向量能够有效 表征词的语义信息，通过针对来自不同历史时段的文本训练词嵌入模，有助于学者在时间维度上分析词义演变。Hamilton 等[36] 通过测量目标词的向量表示的时间位移值，来描述历史文本中高频词汇与多义词汇的词义历时变化。例如，如图 5 所示，“gay”作为一个多义词，在 1900 年代的文本中和“cheerful(开心)”和“frolicsome(玩闹)”词义更接近，而到 1990 年代则与“homosexual(同性恋)”、“lesbian(女性同性恋)”等更接近。进一步，Hamilton 等[36] 依据词嵌入技术的分析结果，提出了两条语义演化法则：(1)一致性，高频词汇会保持词义相对一致的历时演化规律；(2)新颖性，多义词汇的语义演化会更加快速。\n除了对普遍意义上的词汇含义的演变规律进行探讨，某些具体词汇的演化情况同样引起了学者们的关注，尤其是那些能反映特定文化背景或时代发展的词汇。 Rodman 等[11] 基于1855～2016 年间的纽约时报、路透社报道、美联社报道三大新闻文本集，挖掘并追踪了美国一个多世纪以来围绕“平等”一词的词义演变，其中包含了使用环境、指代对象等。他们发现，在上世纪 50 年代之前，即美国民权运动前，“平等”的词义与“社会”话题相关的词汇的关联度整体较高，但随着历史发展呈现减弱趋势，这一结果与美国民权运动前普遍存在的社会不公平现象相符合(如种族歧视)。而自上世纪 70 年代，即民权运动基本结束以来，“社会”与“经济”、“教育”等概念下的词汇的关联程度不断增强，反映了本世纪以来美国社会对“教育公平”，“经济公平”等热门话题的高度关注。可见，基于词嵌入的文本分析能够敏锐地捕捉到社会文化演变的信息线索，并能为社会、文化等领域的运动发展提供预示。\n\n3.4 文本情感分析 文本信息不仅包含词义，还表达情感。现有部分词嵌入模型在关注语义语法层面的表征 的同时，还进一步关注了词的情感信息，尤其是某些单词具有相似的上下文环境但所含的情 感态度截然相反 ( 如 “good” 和 “bad”) 。 例如， Tang 等 [37] 提出了 情感嵌入模型 (Sentiment Embeddings)，该模型不仅可以如词嵌入模型一样反映词义信息，还能识别词所包含的情感 信息，进而可以对文本(如在线评论)中的情感信息做出推断。\n情感的丰富性和语言的灵活性使得单词在不同文本环境下会呈现出差异化的情感特征。 例如，“I’m gonna put something offensive to some people.”中的“offensive(冒犯)”一词在该句话 中带有消极负面的情绪；而“#FSU offensive coordinator Sanders coached for Tennessee 1st [37] #BCS title game.”中的“offensive(矛盾)”一词则不含明显的情感信息。 Xiang 和 Zhou[38] 藉此 指出，在推断词的情感信息时加入对文本 主题(topic)的考量将有效提高情感推断的准确性。 Ren 等[39] 基于 Word2Vec 词嵌入模型，结合主题模型(Latent Dirichlet Allocation; LDA)的方法， 提出了主题增强的词嵌入模型(Topic-Enhanced Word Embeddings; TEWE)。作者使用支持向 量机(Support Vector Machine; SVM)作为文本情感分类器，发现 TEWE 模型在文本情感分类 任务有突出表现。 例如，该模型能更有效地区分含有负面情感态度的词汇(如 insane)与正 面情感词汇(如 sane)的差异。在此基础上，Xiong 等[40] 进一步考虑了文本情感信息的多元化 特征 ， 构建了多元层次情感词嵌入模型 (Multi-level Sentiment-enriched Word Embeddings; MSWE)。他们发现，在标注“积极[#happy; :-)]—消极[#angry; :-(]”的情感标签任务中，MSWE 情感嵌入模型能够实现 85.75%的分类水平，表明基于词的情感表征能够更有效地挖掘文本 背后的情感。\n\n3.5 组织关系分析 词嵌入技术在挖掘社会内隐认知方面展现出强大的效力，因而词嵌入技术可用于挖掘不 同组织在价值观和意识形态层面的关联，以此作为组织关系的推断依据。在此思路基础上， Spirling 和 Rodriguez[41] 采用 GloVe 和 Word2Vec 的词嵌入模型，分析了美国共和党和民主党两大政党在其各自的公开发言稿中对部分政治议题的所表达的态度。例如，对于“堕胎”议题， 两政党的理解存在较大争议：民主党认为“堕胎”是一种自愿选择，而共和党认为“堕胎”与“绝 育”、“公平”的话题相关；对于“税收”议题，两政党的理解则存在更多共识。由此可见，词嵌入技术不仅可以帮助我们了解政党组织在哪些政治议题上存在冲突，还可进一步衡量不同党派的政治关系。\nRheault 和 Cochrane[42] 分析了英国、加拿大和美国 20 世纪以来的议会记录文本，并依 据政党派别关系构建了“党派嵌入”(Party Embeddings)模型。学者利用词嵌入模型量化了不同党派在不同年代的议会观点中与特定“意识形态”维度(如自由 vs.保守、北部 vs.南部)的相关性，从而对比不同党派组织的意识形态差异。从整体上看，美国民主党的意识形态更靠近自 由派思想(如“民权”、“种族”、“枪支管控”)，而美国共和党的意识形态更具保守派和南部 [42] 州色彩(如“官僚”、“果农”、“烟草”)，且两党的意识形态差异随着时间不断扩大。 同样地， 对于加拿大，新民主党派与联盟党在意识形态上的政治冲突最为严重，魁北克政团与联邦主 [42] 义政团在事关“联盟”的政治议题上观点不同。 Pomeroy 等[15] 利用 GloVe 词嵌入模型分析了各个国家在联合国论坛的公开演讲文本，并使用词移距离(Word Mover’s Distance)来量化国 家讲演文本间的总体相似性，以此来反映国家立场及偏好的相似性。 作者发现，基于词嵌入技术的分析结果能够很好地反映国家间的政治关系。例如，虽然土耳其和希腊两国在投 票议程中表现出态度相似性(即一致的国家关系)，但实则两国在当年发生了边境军事冲突， 而这一冲突能从两国的联合国讲演文本中捕捉到线索。本研究指出，有关词嵌入在主体网络 关系的应用集中于党派关系和国际关系的研究，未来研究还可以考虑分析其他情景下的主体关系，如社交网络关系、品牌竞争关系、组织内部关系等。\n\n3.6 个体的判断与决策心理 决策结果和决策信息线索之间具有表征关系，因而词嵌入技术能够通过挖掘概念间的内在关联，在一定程度上揭示个体在决策任务中的思维过程和决策依据。Bhatia[16] 在自然语言处理的框架下，验证了以往决策研究中的相关性判断机制，即人们在进行判断性任务(如：“A 多大可能属于 B?”)时，会出于直觉性心理去思考问题与选项间的相关性或相似性，并以 作为判断依据。具体而言，作者综合 Word2Vec、CCA、GloVe 几项词嵌入技术，基于谷歌 新闻和 GigaWord 文本库⑤ 训练生成词向量。进一步，作者通过对句子中每个“词”的向量求 取平均值，分别对判断问题(如“在以下的两座德国城市中，哪一个人口最多？”)与选项(如“汉堡”和“科隆”)实现表征。作者依据两者间的语义相关性来预测答案选项的概率分布，并据此模拟一般决策者的选择。例如，针对上述问题，基于词嵌入模型的预测结果为“汉堡”，与被试的选择高度相似。此外，词嵌入模型在其它测试任务(如经典的“Linda 问题”⑥ )下也预测了 [16] 决策者的选择倾向， 这一现象与代表性启发理论(representativeness heuristic)——一种依赖人的相关性感知进行识别和判断的心理决策过程——相符。这说明词嵌入模型在很大程度上 能够解释人的相关性判断机制， 甚至对其中常见的认知偏差 ， 如 合取谬误 (conjunction fallacy)⑦ 、基础概率忽略(base rate neglect)⑧ 也能够予以反映。[16] 实证结果表明，词嵌入技术为我们理解人的直觉性判断心理提供了信息参考，能够帮助实现相对精准的决策预测。\n另外，个体的风险感知和风险判断也是个体决策研究中的重要组成部分。Bhatia[43] 利用词嵌入技术探讨了人们面对各类风险源时的风险评估机制。作者通过基于谷歌新闻文本的预 训练 Word2Vec 模型，量化了不同风险源(技术性风险源：“新兴技术”、“能源”等；活动性风 险源：“运动”、“职业”等)与相关概念的语义联系，进一步 揭示了人们进行风险评估时的知 识表征内容(即内隐联想)。例如，当评估药物风险时，人们在潜意识里会联想到“毒品(drug)”、 “无序(disorder)”等具有高风险含义的概念(如图 6a 的词云图)；而评估运动风险时，人们容易 联想到“碰撞(crash)”、“斗争(combat)”等风险事件(如图 6b)。作者弥补了以往有关风险评估 的研究方法中难以预测样本外数据(如新型风险源)的缺陷，展现了词嵌入技术在理解和预测个体判断决策机制中的应用优势。\n ⑥ “Linda 问题”是指“琳达，31 岁，单身，一位直率又聪明的女士，主修哲学。在学生时代，它就对歧视 问题和社会公正问题较为关心，还参加了反核示威游行。请问琳达更有可能是下面哪种情况？”有两个选 项：“A.琳达是银行出纳；B.琳达是银行出纳，同时她还积极参与女权运动”。相比于 A 选项，B 选项所 塑造的女性形象更贴近问题所提供的信息，因而人们会倾向于选择 B 选项。\n⑦合取谬误是指人们总是认为两个事件的联合出现比只出现其中一件事的可能性要大。以“Linda 问题”为例， 人们会更多地选择 B，虽然从实际概率角度来讲，B 选项的概率应低于 A 选项。\n⑧基础概率忽略是人们在进行主观概率判断时倾向于使用当下的具体信息而忽略掉一般常识的现象。\n \n四、词嵌入分析的基本流程 词向量的表征学习存在两条路径：一是采用本地化的训练模型(local-trained model)。二是使用预训练的词嵌入模型(pre-trained model)。针对第一条路径，通常需要经历如下预处理和模型训练步骤(见图 7)：\n4.1 选择合适的语料库(corpus) 语料库是用于训练词嵌入模型的文本集，“词”的表征效果以及后续的相关性分析依赖于训练文本的规模、质量及其所处的语言环境。对文本语料的选择需严格依研究者的具体问题而定，使研究主题/情景与文本主题/情景相对应，[41] 进而推动单词间的语义关系聚焦于特定的领域和视角上。例如，以探讨社会文化现象的研究可以选择新闻时报、社交媒体动态等社会文本作为主要语料；探讨消费者心理及行为的研究则可以将线上评论等商业沟通文本作为语料；关注组织行为的研究则以会议 记录、公司年报等组织内部的官方文本为主。\n文本数据的获取主要有以下三种途径：\n 第一， 国内外由政府、企业及其他组织或个人提供的公开的、已初步整理规范的文本数据库。如， 人民日报文本集(1946 年至今) ⑬ 、谷歌图书(包含 1500 年～2012 年期间公开出版的书籍，约占人类历史所有出版书目总数的 6%) ⑭ 、亚马逊评论集(包含 1996 年～2018 年亚马逊平台用 户对近 30 个产品品类的超过 2 亿条评论) ⑮ 、维基百科数据库(包含来自 400 多万篇文章的近 19 亿个单词的维基百科全文) ⑯ 等。 第二，借助“爬虫”程序收集文本数据。根据研究需要， 研究者可以在特定的网站上爬取一定的文本。例如，众筹平台的项目申报文本材料、微博平台的历时推文、论坛用户间的互动文本、企业员工在 Glassdoor ⑰ 等职业资讯网的日志评论等文本。 第三，纸质版文本转换成电子文本。必要时，研究者还可以将纸质文本录入为电子文 本形式，如员工日记、会议记录、线下心理咨询文稿等。  4.2 语料预处理(pre-processing) 常规的预处理流程包括：\n 删除与文本内容无关的标点符号、特殊字符(如：数字，空格符，分行符，“©”)和其它停用词(如：代词、连词)。 此外，中文文本的预处理中还需要对文本分词 (segmentation)，从而将语料处理成由“词”这一最小的文本分析单位所构成的列表(如将语 句“我很开心”分词为“我”/“很”/“开心”)。现今常用的中文分词工具有“Jieba”、“HanLP”、 “THULAC”、“TopWORDS”等。  4.3 模型训练 在预处理后的语料文本中训练词嵌入模型，最终实现文本词的向量表征。当前主流的词嵌入模型有 Word2Vec、GloVe、fastText 等， 而在 Python 环境下，大量与自然语言处理相关的成熟的开源工具包(如，Gensim)中提供了相关的算法模块，并允许研究者对相关参数(词向量的维度、单词上下文的观测窗口的大小 等)进行调整。\n此外，基于词嵌入模型的迁移学习能力，也可直接使用预训练的词嵌入模型(如，谷歌 的 GloVe 和 BERT )，从而获得基于其它大型语料库充分训练得到的词向量表示，并根据自身的文本特征对模型或表征结果进行微调(finetune)。但无论采用何种词向量表征路径， 在正式的词向量相关性分析之前，都有必要对词嵌入模型的训练结果，即词向量的表征效度进行评估。常见的评估方式是通过与人工标注的词相关性评分进行对比，检验二者是否一致， 以此判断词嵌入模型是否能够捕捉一般化的语义关系。目前，已有大量成熟的针对“词对” (word pairs)相似性的人工标注的测试集，如 MEN-3000(英文)[47] 、Wordsim240/297(中文)[48 ] [12,41]。\n词嵌入模型训练完成后，可以进一步依据研究目的进行词向量间的“距离”的几何计算， 主要包括词列表构建、相关性计算、有效性检验和稳健性检验四个步骤 。\n (1)构建词列表。 在词嵌入的文本分析中，特定概念通常由一系列近义词或同属性词列表构成。例如，在 Garg 等[13] 的研究中，他们构建的“男性”概念词列表包括“male”、“men”、“father”、“brother”等 20 个单词。 (2)计算词向量的相关性。针对具体研究问题，衡量“词”或者概念之间的语义关联(即 词向量间的“距离”)，主要包括余弦相似度(cosine similarity)、欧式距离(euclidean distance)两 种基本的计算方法(详见上文第一部分内容)。 (3)有效性检验。针对词嵌入的分析结果，我们 有必要进行进一步的检验，以保证结论的可靠性以及方法的有效性。具体包括两类检验方法： ①与对应年代的相关社会调查数据 (如，社会职业性别占比调查、社会偏见大调查 ) [13,49-51] 进行比对，以检验词与词的相关程度、变化是否与相应的指标数据、社会事件相吻合； ②与其它研究方法进行对比，如内隐联想测试(IAT)、主题模型(LDA)，以检验词嵌入模型能 否重复已有研究结果。[11,14] (4)稳健性检验。作为一种无监督的探索性分析方法，词嵌入的 分析结果会因文本、模型等因素的不同而产生差异。通过变换词嵌入模型、参数、文本语料 或相关性计算方法，以检验研究结果的一致性。稳健性程度越高则代表基于词嵌入分析方法 的结论可靠性越高。  \n五、词嵌入分析方法的挑战与应对 作为一种计算机化的文本分析方法，词嵌入技术在文本数据的预处理、文本表征效果等方面有一定的条件限制，因而该技术面临着以下几个方面的挑战。学界一直致力于词嵌入分析方法的完善，并就下述问题提出了针对性的应对方案和解决思路，详见表 3。\n(1)词嵌入技术的分析效果依赖于文本数据的体量、质量和语言环境。 ①通常来说，文本数据规模越大越有利于词嵌入学习和提取更充分的语义信息，[11] 而体量较小的文本，可能会限制词嵌入模型的训练效果，难以将文本的全局语义信息嵌入到单词上。[11,18] 对此，Rodman[11] 提出了两种解决思路：一是采用预训练的词嵌入模型。预训练词向量通常基于超大型文本数据训练而来， 使模型具备较好且广泛的语义表征能力。 二是采取“自举法”(Bootstrapping)⑱ 。通过该方法生成规模更大的文本数据集，并针对不同的抽样过程生成不同的词向量结果，随后求取其平均值以增强词向量的稳定性和有效性。\n②除此以外，文本语料的选择应考虑到其依存的文本情景、社会文化环境等背景信息。文本所依存的语言环境、文化观念和观点立场在很大程度上影响着文本词汇的分布方式(即单词上下文)，因此利用不同文化背景的文本训练词嵌入模型，可能会产生不同的研究结论。Spirling 和 Rodriguez[41] 对美国国会议事文本的分析所示，不同的党派组织对同一政治 议题的理解存在“冲突”(如，“堕胎(abortion)” \u0026amp; “福利(welfare)”)。再如，在研究组织员工的 幸福观时，企业的官方书面文本(更具指导性和应然性)以及员工的口述文本(更具真实性)可 能潜藏不同的结论。然而，如何权衡和选择合适的语料、如何处理不同文本下的结论不一致 等问题仍缺乏统一的解决标准。就学者的普遍实践来看，文本选择需要“有的放矢”，即依据 研究问题对文本的背景信息(如，表达视角、代表阶层、文本性质 )进行必要地分析和筛选， 在扩大语料库规模和类型的同时也要尽可能使之聚焦在同一视角和语境上。[11,28,41] (2)“词”是词嵌入分析的基本单位，中文文本的分析需要预先进行“分词”处理。\n对于某些特定领域的文本而言，如专业学术文章、古代汉语文本等，由于其文本内容及结构与标准的训练语料存在较大差异，[46] 使得文本分词的过程存在一定困难。近年来，大量学者就优化文本预处理技术展开了探讨，例如，Deng 等[46] 开发了“TopWORDS”分词软件，在小型训练文本中实现了部分低频词的精确识别，亦能处理含有大量未知专业语汇的文本，该方法的有效性在古汉语文本的分词任务上得到了进一步验证。在突破文本预处理中的困难后，可以应用词嵌入的实践流程对文言文展开分析，探讨古代社会文背景下的社会科学课题，如围绕权力、阶层、性别、宗教等的社会规范以及其它社会价值观念的演变。\n(3)在由“词”构成的文本结构中，词与词之间的组合搭配能够创造出更加丰富且抽象的语义信息，这一类信息难以通过词向量间简单的结构化公式运算来体现。 词嵌入技术对文本中的“词”展开语义分析，所建构的是词与词之间的关联，侧重于表达“单词级别”的语义信息。因此，基于词向量的简单几何计算难以直接反映 “单词级别”以外的语义信息，[18,5 2 ] 如词组概念、段落含义、文本主题等。学界也在积极探索“组合式分布语义”的实现方法，即如何利用词表征的组合实现对短语、段落和文档的有效表示。[18,53 ] 在自然语言处理领域，以LDA 为代表的主题模型从整个语言系统分布中学习“词”的含义，侧重于建模词与文档的关系，体现的是词的主题信息。相比于词嵌入模型下的单词之间的关联，包含主题特征的词向量之间能够反映相对丰富的语义关联。鉴于词嵌入模型具有较强的扩展能力，大量学者针对词嵌入模型的算法和训练过程进行优化，即将有关文本整体特征的信息或其他的领域知识融入词嵌入的学习过程。例如，Liu 等[54] 基于 Word2Vec 词嵌入模型，并结合 LDA 算法，使词向量包含更多的主题特征，如 “apple(苹果)”在电子产品的背景信息下表示“苹果公司”，而在食品背景信息下表示“苹果”这 类水果。此外，词嵌入技术的基本原理在文本表征领域也得到了长足发展。例如， Le 和 Mikolov[55] 将 Word2Vec(skip-gram)的算法运用至句子和短文本的表征学习；词嵌入模型界的 新秀——BERT 模型，能够有效表征句子等“单词级别”以上的文本语义概念，推动了对更 高文本单位的关系层面的理解。\n(4)传统的社会科学研究方法具备词嵌入技术所无法提供的分析视角，尤其是相对于“文本细读法(close reading)”，词嵌入这一计算机化的分析方法难以捕捉更加细微的语义差别。\n基于上下文分布来表示单词的方法也难以学习单词的细粒度语义， 例如，同义词、反义词、多义词、上下位词等词义的区分和表征还有待优化。据此，相关学者提出利用有监督的学习过程，在词嵌入的算法层面引入某些先验知识库(如，描述词义关联信息的“WordNet”语义网[5 7 ] )，帮助模型更好地捕捉单词多元的属性信息，从而有助于避免词嵌入表征词义的逻辑偏离实践认知。[19]\n\n六、词嵌入技术在管理学领域的应用展望 6.1 政策效应评估 已有政策效应评估主要采用定量分析工具(如，双差分法)分析显性的数据指标(如，人均收入)，而对于政策的隐式效应(如，社会心理、文化认知)的判断还较为局限。本研究提出， 词嵌入技术可以结合因果推断，[5 8 ] 分析公共政策的有效性。具体而言，词嵌入技术可以研 究：①政策对社会文化认知的影响，如“2020 年禁塑令”实施对公民“环保”、“健康”等概念的 认知的影响；“2021 年惠游湖北”政策对武汉城市污名化的缓释作用等。②衡量政策创新性， 探讨政策带来的创新性影响。例如，Perren 和 Sapsed[5 9 ] 分析了英国在过去近 50 年间的议会 记录，发现在其实施“国家创新计划”之后，“创新”这一概念在社会文本的出现频率显著上升， 有关科技领域的词汇与“创新”一词的共现频率显著增加。未来研究可以利用词嵌入技术来挖 掘“创新”与具体领域，例如，“知识”、“科技”、“法律”的相关度，并比对其在政策实施节点 前后的变化。\n6.2 用户推荐系统 以往的线上推荐系统多基于用户行为数据来判断个体偏好， 以矩阵分解 (Matrix Factorization)和协同过滤(Collaborative Filtering)为代表的技术被广泛应用于线上推荐场景。 然而相比上述几项技术，词嵌入能实现更高水平的用户偏好预测， [ 60 , 6 1 ] 这意味着文本数据 将是我们了解用户的重要渠道。现如今，电商评论和社交媒体的动态文本记录了大量消费者 认知、态度及其它表现个人特征的信息。利用词嵌入技术，平台能够挖掘用户对特定领域话 题的偏好、对产品的偏好和评价，以及用户之间的相似特征等。例如，通过计算“产品/品牌” 与积极、消极属性词汇的相对“距离”，[13] 来衡量个体用户的内隐偏好和真实的消费感受，并 据此展开产品推荐和广告投放。\n6.3 数“智”品牌管理 鉴于词嵌入技术在挖掘社会内隐认知上的突出表现，因而能用于刻画企业—消费者关系， 辅助企业的品牌管理战略。 借助词嵌入的分析方法， 企业可以透过用户生成文本 (user-generated content)(如，社交媒体、网络论坛和线上评论)纵观消费者对企业品牌形象的 态度、评价，也可以用于挖掘影响消费者满意度的关键因素和市场潜在需求。[8, 6 2 ] 作为品牌 形象的内核，企业品牌个性(brand personality)及其历时演变同样也可以使用词嵌入的分析方 法对其进行挖掘，[12] 从不同时期的社会文本中测量相应的语义关系，即品牌与个性维度间 的相关性(如，品牌与“真诚型”vs.“粗犷型”)。在跨文化视角下，词嵌入技术能够帮助企业考 察不同文化背景下的市场对其品牌的认知差异，并据此助推企业品牌的形象定位与国际化进 程。[6 3 ] 再如，利用词嵌入技术能够帮助企业动态追踪新产品的市场评价，为企业评估产品 的市场表现提供新的分析工具。其它的相关话题，诸如品牌依恋(brand attachment)、品牌文 化(brand culture)和品牌联想(brand association)等研究也将受益于词嵌入的分析方法。\n6.4 企业关系管理 文本作为企业对外传达信息、价值观，以及企业间进行交流的重要载体之一，蕴含了大量的、足以表征企业特征的信息。以往从文本层面探讨企业关系的研究相对较少，而利用词嵌入的基本原理，未来研究可以考虑利用文本来刻画企业间的关系(如竞争、合作、信任等)， 进而更有效地描绘企业在网络中的嵌入式角色(embedded role)。例如，学界一直致力于研究 企业间关系网络的结构特点及其对企业绩效、企业间联合研发效率的潜在影响，探讨了社会 网络嵌入视角下的企业组织的合作范式。例如，基于知识理论视角，企业合作网络的形成及 演变动机在很大程度上取决于知识的互补性与相似性特征。[64 ] 未来研究可以利用文本刻画 企业的关系网络及其节点特征，探索企业的合作策略和市场战略。尤其在信息不对称的商业环境下，基于文本的社会关系分析能够为企业的战略伙伴选择、市场表现等提供新的分析路 径和信息参考。\n6.5 组织内部管理 在管理学领域，有关组织行为的研究大多依赖于问卷访谈和自然观察等形式的调查方法， 以及基于组织管理目标开展特定的田野实验。这些研究路径在理解和预测个体行为的过程中 存在较强的主观性和外显性，难以挖掘组织成员真实的内隐认知。此外，以往对组织场景内 的文本分析在很大程度上受限于专家学者的领域知识和实践经验，耗时、低效且准确率低。 本研究提出，词嵌入技术可以用于分析组织内成员的心理及行为规律，通过挖掘组织内的文本(如会议记录、员工评述、领导讲演文本)，揭示员工的内隐认知信息(如动机、信念、情绪)， 甚至包括领导力(leadership)、员工创新力、员工的组织支持感(organizational support)和企业文化等主题。\n6.6 中国传统智慧与管理问题 社会科学研究者不仅需要关注当下的社会情景，也需要从历史中洞察现象、以史为鉴。 中国社会文化背景下的众多管理问题、思想乃至组织行为领域的话题，均能够从历史事件中窥知和借鉴。例如，Huang 等[6 5 ] 基于《资治通鉴》这一古籍中的记载，探讨了中国家族式企 业内的领导—员工关系。他们以古代的皇帝与太子间的关继承案例作为样本，并在长时间的 人工阅读和变量编码后发现，家庭组织在权力转移的过程中，子女继任的可能性与其父母在 位者对其的压制行为存在“U 型”关系。同样地，词嵌入技术可以挖掘在位者对继任者的评价， 以此判断与继任可能性之间的关系。此外，通过对《二十四史》展开词嵌入分析，也能帮助学者了解中国各个朝代的管理层在应对人事、外部环境、组织治理等方面的管理思想与策略。 对此，本文展望利用词嵌入方法对中华古籍文本展开必要的分析，挖掘诸如组织领导风格、 组织文化、组织竞争力、管理者与下属间关系、人员激励政策等研究话题，进而探索中国本土的管理智慧和组织话题。[65]\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/wordembeddingsinsocialscience/","summary":"活动预告  想随时随地系统学习Python文本分析，可以选择  Python网络爬虫与文本分析 | 2021录播课(虽是录播，但章节更多一些)。   更喜欢有互动感通过直播学习，可以考虑  Python网络爬虫与文本分析 | 2022五一直播    文献 本文全文摘自\n冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html\n词嵌入亮点 词嵌入技术是文本分析中技术含量较高，可从文本中测量出人类认知信息。而即时是有一定学习能力的人，当阅读大量文本很难察觉文中是否有内置(预置)的信息，如作者的偏见、态度、刻板印象，等人类复杂认知。词嵌入技术可以将这类难以察觉的线索挖掘、测量。\n摘要 在大数据时代的背景下，基于大数据的分析处理技术为以“数据驱动”的社会科学研究创造了新的发 展契机。其中，词嵌入(Word Embeddings)技术借势大数据浪潮，以其高效的词表征能力和强大的迁移学习能力在文本分析领域受到越来越多的关注。不同于传统的文本分析路径，词嵌入技术不仅实现了对非结构化文本数据的表征，还保留了丰富的语义信息，可以实现对跨时间、跨文化文本中深层次文化信息的挖掘， 极大丰富了传统的社会科学实证的研究方法。文章总结了词嵌入技术的基本原理及特点，系统地梳理了词 嵌入技术的六大应用主题：社会偏见、概念联想、语义演变、组织关系、文本情感和个体决策机制。随后， 文章归纳了词嵌入技术的基本应用流程。词嵌入技术还面临文本数据的选择、中文文本的分词处理、单词 语义信息的表征层次三种挑战，文章归纳了相应的应对思路与方法。最后，基于词嵌入技术的强大适应能 力，未来研究可以进一步关注该技术在管理领域的应用前景，包括政策效应评估、用户推荐系统、品牌管 理、企业关系管理、组织内部管理、中国传统智慧与管理问题六个方面。\n关键词: 词嵌入；自然语言处理；文本分析；社会科学；管理领域应用\n\n引言 作为人类开展文化交流和情感沟通的基本载体，语言承担了重要的信息交换功能。借助 于各类语言表达形式，人们将诸如知觉、思维、态度和情感等复杂的心理活动转化成特定的 语言。 而作为语言的典型载体之一，文本既能够在个体层面上反映人们的内心活动，也能 够在组织和社会层面上反映集体文化。 因此，从文本内容挖掘个体深层次的心理活动和 人类社会的文化沿革是社会科学的基本研究路径。\n长期以来，在社会科学尤其是管理学和心理学等领域，实证研究多以针对实验、问卷和结构化的二手数据的量化分析为主导，而对于非结构化的文本材料(如访谈记录)仍以质性分析为主。[5] 在大数据时代，“数据+行为+交叉学科”已成为社会科学发展的必然方向。而计算社会科学的兴起则为我们理解人类行为、探讨社会现象提供了新的研究素材、视角和手段[6] 。随着互联网技术的飞速发展，人们在网络上发表大量包含思维、情感、观点的文本信息，这些井喷式爆发的文本为“以数据驱动”的社会科学研究提供了可及的信息来源。若能对之加以利用，无疑将拓宽社会科学研究的方法。[7 - 9] 然而，社会科学领域的传统文本研究方法以人工编码为主，其时间投入过大、成本较高、客观性相对较弱等不足极大地限制了文本数据 [10] 在实证研究中的应用。 所幸的是，以自然语言处理(Natural Language Processing; NLP)为核心的计算机文本分析技术(Computerized Text Analysis)的发展为大数据文本在社会科学领域 中的应用带来了契机。 “词”作为文本的最小单元，是计算机进行文本分析的基础。在自然语言处理领域，“词” 主要以向量(vector)的形式表示。而词嵌入(Word Embeddings)技术，即是一种可以把高维词 向量映射进低维向量空间，以此来实现词义理解的计算机文本分析技术。相较于其他自然语 言技术，词嵌入技术不仅展现出了高效的学习能力，而且允许计算机从更高的意义单元(即 目标词的上下文)出发理解词义、刻画“词”之间的相对关系，因此逐渐成为了自然语言处理的重要工具，在管理学、心理学等社会科学领域取得了丰富的研究进展。相比于传统的、以人工编码和词频统计为主导的文本分析方式，词嵌入的独特优势在于：\n 借助计算机分析技术，可以在短时间内、以较低成本，实现对大规模文本数据的高效处理； 在挖掘文本特征和理解文本内容时，更多地依赖文本自身的分布规律，具有较强的客观性，其背后“数据驱动”的分析逻辑也使这一技术在探索性研究中具有较大的应用优势； 面对跨时间、跨文化比较的研究话题、以及在挖掘社会学、行为学变量及变量关系等领域展现出广阔的应用前景。  词嵌入技术已在社会科学领域得到了广泛的应用，主要包括：社会偏见 、概念联想[14] 、关系网络和判断机制[16] 等六大主题，大量研究见诸 Nature、Science、PNAS、Academy Management Journal、American Sociological Review、Management Science 等国际期刊。反观国内的社会科学领域，词嵌入方法的应用价值还未得到足够重视和讨论。据此，本文通过介绍词嵌入技术的基本原理、梳理国外社会科学领域对词嵌入的应用情况，以期帮助国内社会科学研究者了解该技术独特的应用价值，推动词嵌入技术在大数据时代背景下对我国社会科学研究方法的丰富和推动。具体而言，本文：","title":"转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用"},{"content":" 这个笔记本是教程中的部分方法\n Dzyabura, El Kihal and Peres (2020), \u0026ldquo;Image Analytics in Marketing\u0026rdquo;, in The Handbook of Market Research, Ch 14, Editors: Christian Homburg, Martin Klarmann, Arnd Vomberg. Springer, 2021.\n 安装 代码是用 Python 编写的。 用于编程和运行代码的理想界面是通过 Anaconda 的 Jupyter Notebook。\n除了标准 Anaconda 库之外，还需要安装几个额外的库。 我们在代码中标记它们。\n!pip3 install opencv-contrib-python==4.5.5.64 !pip3 install opencv-python==4.5.5.62 !pip3 install scikit-image==0.18.3 import numpy as np import pandas as pd import os import random import cv2 # needs to be installed separately import matplotlib.pyplot as plt from skimage.io import imread, imshow import matplotlib.image as mpimg %matplotlib inline \n预定义特征提取 我们从可以从图像中提取基本特征,如亮度、颜色等。\n首先，我们加载一张图片并显示它。\nimg = mpimg.imread(\u0026#39;azreali.jpg\u0026#39;) imgplot = plt.imshow(img) plt.show()   颜色提取 灰度直方图 颜色是基本的图像特征。 以下是创建颜色直方图的一些示例，这些直方图捕获图像的颜色组成。\n将彩色招照片转为灰度照片\nimg_gray = imread(\u0026#39;azreali.jpg\u0026#39;,as_gray=True) imshow(img_gray) \u0026lt;matplotlib.image.AxesImage at 0x7fd5ffe7c910\u0026gt;  ​  \n​\n我们可以为灰度图像创建一个颜色直方图，计算每个像素的强度，范围在 0 为黑色和 256 为白色之间。\n你能猜出直方图的样子吗？\nimage = cv2.imread(\u0026#39;azreali.jpg\u0026#39;) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) hist = cv2.calcHist([gray], [0], None, [256], [0, 256]) plt.figure() plt.title(\u0026#34;Grayscale Histogram\u0026#34;) plt.xlabel(\u0026#34;Bins\u0026#34;) plt.ylabel(\u0026#34;# of Pixels\u0026#34;) plt.plot(hist) plt.xlim([0, 256]) (0.0, 256.0)  ​   ​\nRGB直方图 实际上，直方图向右倾斜。 图片主要包含刻度亮侧的像素和相对较少的暗像素。\n接下来，我们可以为 RGB 颜色空间创建颜色直方图，0 表示颜色的最低强度，256 表示最高强度。\n你能猜出三座塔图片中的主要颜色是什么吗？\ncolor = (\u0026#39;b\u0026#39;,\u0026#39;g\u0026#39;,\u0026#39;r\u0026#39;) for i,col in enumerate(color): histr = cv2.calcHist([image],[i],None,[256],[0,256]) plt.plot(histr,color = col) plt.xlim([0,256]) plt.show()   确实。 图片中有很多蓝色，直方图检测到这些蓝色像素。\n今天讲的都是很简单的图片处理，图片是一种比文本体积更大的文件数据，受限制于个人技术水平以及电脑性能，大邓无法展示机器学习、深度学习的图片分析算法。感兴趣的同学可以阅读论文，了解图片分析在营销中的新应用新进展。\n Dzyabura, El Kihal and Peres (2020), \u0026ldquo;Image Analytics in Marketing\u0026rdquo;, in The Handbook of Market Research, Ch 14, Editors: Christian Homburg, Martin Klarmann, Arnd Vomberg. Springer, 2021.\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/image_analytics_in_marketing_code_examples_book_chapter/","summary":"这个笔记本是教程中的部分方法\n Dzyabura, El Kihal and Peres (2020), \u0026ldquo;Image Analytics in Marketing\u0026rdquo;, in The Handbook of Market Research, Ch 14, Editors: Christian Homburg, Martin Klarmann, Arnd Vomberg. Springer, 2021.\n 安装 代码是用 Python 编写的。 用于编程和运行代码的理想界面是通过 Anaconda 的 Jupyter Notebook。\n除了标准 Anaconda 库之外，还需要安装几个额外的库。 我们在代码中标记它们。\n!pip3 install opencv-contrib-python==4.5.5.64 !pip3 install opencv-python==4.5.5.62 !pip3 install scikit-image==0.18.3 import numpy as np import pandas as pd import os import random import cv2 # needs to be installed separately import matplotlib.","title":"营销研究中的图像分析"},{"content":"越来越多的社交网络学者， 为测量情绪， 基于心理学家 Robert Plutchik 提出的模型（通常简称为“Plutchik轮”，人类的情绪一共有8大类）制作了大量的情绪可视化作品。在某种程度上，Plutchik轮可以看做情绪指纹，例如不同的电影题材在8类情绪的分布是不一样的。\n今天介绍 PyPlutchik，这是一个 Python 库，专门用于在文本或语料库中可视化 Plutchik 的情绪。 PyPlutchik 绘制 Plutchik 的花朵，每个情感花瓣的大小取决于语料库中检测到或注释了多少情感，也代表每个情感花瓣的三个强度程度。\n安装 pip3 install pyplutchik \nUsage from pyplutchik import plutchik emotions = {\u0026#39;joy\u0026#39;: 0.6, \u0026#39;trust\u0026#39;: 0.4, \u0026#39;fear\u0026#39;: 0.1, \u0026#39;surprise\u0026#39;: 0.7, \u0026#39;sadness\u0026#39;: 0.1, \u0026#39;disgust\u0026#39;: 0.5, \u0026#39;anger\u0026#39;: 0.4, \u0026#39;anticipation\u0026#39;: 0.6} plutchik(emotions) \n功能特性 PyPlutchik 提供了一个即插即用的工具，用于在文本或语料库中定量表示 Plutchik 的情绪。 它尊重 Plutchik 轮中每个花瓣的原始颜色和空间位移。\n在 Pyplutchik 中，用户可以只传递一个字典作为唯一参数，其中字典的键必须是 8 种基本情绪。 每个值必须是 ∈ [0, 1]。\n每类情绪存在三种强度，下表是Pyplutchik的8大类情绪三种强度汇总。\n用户还可以给每个情绪传入一个长度是3的列表，依次表示情绪在低、中、高三种强度的程度，数值0 和 1 之间。\n   Lower intensity Emotion Higher intensity     Annoyance Anger Rage   Interest Anticipation Vigilance   Serenity Joy Ecstasy   Acceptance Trust Admiration   Apprehension Fear Terror   Distraction Surprise Amazement   Pensiveness Sadness Grief   Boredom Disgust Loathing    PyPlutchik 也可表征用户数据中的主要二元、次要二元、二元和相反的情绪。 它会自动从字典的关键字中了解用户想要显示什么样的二元组。\n绘图技巧 可以专注于情绪子集，不会忽视其他情绪，将情绪列表作为参数“highlight_emotions”的值传递：\n我们可以比较同一亚马逊语料库的不同子组，将我们的可视化并排放置，并仅突出显示愤怒、厌恶和恐惧的花瓣，以轻松发现这些负面情绪在 5 星评论中的表现如何低于 1 星评论 .\n或者可以隐藏坐标、刻度和标签，只绘制花瓣，使用参数 show_coordinates = False 和 show_ticklabels = False。下图是imdb不同电影的情绪轮\n查看文档 有关所有参数的文档和示例库，请参见文档\n文档下载\n🔥 PyPlutchik 2.0 即将推出！ 新功能包括从文本中提取情感并检查非主题词典。 敬请期待……🔥\n说明 如果使用PyPlutchik，请在文献中说明，格式如下\nSemeraro A, Vilella S, Ruffo G (2021) PyPlutchik: Visualising and comparing emotion-annotated corpora PLOS ONE 16(9):e0256503.https://doi.org/10.1371/journal.pone.0256503 \nSemeraro的研究 Semeraro, Alfonso, Salvatore Vilella, Giancarlo Ruffo, and Massimo Stella. \u0026#34;Writing about COVID-19 vaccines: Emotional profiling unravels how mainstream and alternative press framed AstraZeneca, Pfizer and vaccination campaigns.\u0026#34; *arXiv preprint arXiv:2201.07538* (2022).  摘要: 自 2020 年 11 月宣布 COVID-19 疫苗以来，媒体和社交媒体对 COVID-19 疫苗进行了大量辩论。由于大多数研究都集中在社交媒体中的 COVID-19 虚假信息上，与其他来源相比，主流新闻媒体如何构建 COVID-19 叙述很少受到关注。为了填补这一空白，我们使用认知网络科学和自然语言处理来重建 5745 条关于 COVID-19 疫苗的新闻的随时间变化的语义和情感框架。我们的数据集涵盖了 8 个月内的 17 个网点，其中包括在 Facebook（500 万总股数）和 Twitter（20 万股总股数）上大量转发的意大利新闻文章。我们发现主流消息来源构建“疫苗/疫苗”的总体概念的方式始终具有高度的信任/预期和较少的厌恶。在替代来源构建 COVID-19 疫苗的方式中，这些情绪严重缺失。在疫苗的特定实例中发现了更多差异。另类新闻包括以强烈的悲伤来描述阿斯利康疫苗的标题，而主流标题中没有。与“阿斯利康”相比，主流新闻最初将“辉瑞”与副作用（例如“过敏”、“反应”、“发烧”）相关联更多。随着后一种疫苗的暂停，在 2021 年 3 月 15 日，我们发现了一种语义/情感转变：即使是主流文章标题都将“阿斯利康”框定为在语义上更丰富与副作用的负面关联，而“辉瑞”则经历了积极的效价转变，主要与其更高的效率有关。血栓形成与可怕的概念联想一起进入了疫苗的框架，而死亡这个词经历了情感转变，在替代标题中转向恐惧，在主流标题中失去了希望的内涵，缺乏预期。我们的发现揭示了媒体采用的围绕 COVID-19 疫苗的情感叙述的关键方面，强调了了解替代媒体和主流媒体如何报道疫苗接种新闻的必要性。 Keywords: natural language processing, text analysis, complex networks, cognitive network science, COVID-19, COVID-19 vaccines   广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pyplutchik_emotion_circle/","summary":"越来越多的社交网络学者， 为测量情绪， 基于心理学家 Robert Plutchik 提出的模型（通常简称为“Plutchik轮”，人类的情绪一共有8大类）制作了大量的情绪可视化作品。在某种程度上，Plutchik轮可以看做情绪指纹，例如不同的电影题材在8类情绪的分布是不一样的。\n今天介绍 PyPlutchik，这是一个 Python 库，专门用于在文本或语料库中可视化 Plutchik 的情绪。 PyPlutchik 绘制 Plutchik 的花朵，每个情感花瓣的大小取决于语料库中检测到或注释了多少情感，也代表每个情感花瓣的三个强度程度。\n安装 pip3 install pyplutchik \nUsage from pyplutchik import plutchik emotions = {\u0026#39;joy\u0026#39;: 0.6, \u0026#39;trust\u0026#39;: 0.4, \u0026#39;fear\u0026#39;: 0.1, \u0026#39;surprise\u0026#39;: 0.7, \u0026#39;sadness\u0026#39;: 0.1, \u0026#39;disgust\u0026#39;: 0.5, \u0026#39;anger\u0026#39;: 0.4, \u0026#39;anticipation\u0026#39;: 0.6} plutchik(emotions) \n功能特性 PyPlutchik 提供了一个即插即用的工具，用于在文本或语料库中定量表示 Plutchik 的情绪。 它尊重 Plutchik 轮中每个花瓣的原始颜色和空间位移。\n在 Pyplutchik 中，用户可以只传递一个字典作为唯一参数，其中字典的键必须是 8 种基本情绪。 每个值必须是 ∈ [0, 1]。\n每类情绪存在三种强度，下表是Pyplutchik的8大类情绪三种强度汇总。\n用户还可以给每个情绪传入一个长度是3的列表，依次表示情绪在低、中、高三种强度的程度，数值0 和 1 之间。\n   Lower intensity Emotion Higher intensity     Annoyance Anger Rage   Interest Anticipation Vigilance   Serenity Joy Ecstasy   Acceptance Trust Admiration   Apprehension Fear Terror   Distraction Surprise Amazement   Pensiveness Sadness Grief   Boredom Disgust Loathing    PyPlutchik 也可表征用户数据中的主要二元、次要二元、二元和相反的情绪。 它会自动从字典的关键字中了解用户想要显示什么样的二元组。","title":"PyPlutchik库 | 可视化文本的情绪轮(情绪指纹)"},{"content":"代码下载  链接:https://pan.baidu.com/s/1vJohEJ0pc6t4PBK04PiZbg 密码:t7a6\n whatlies 可以与spacy语言模型结合，可视化词向量。安装zh_core_web_md、en_core_web_md和whatlies。具体文档可以查看https://github.com/RasaHQ/whatlies\n!pip3 install zh_core_web_md-3.0.0-py3-none-any.whl !pip3 install en_core_web_md-3.0.0-py3-none-any.whl !pip3 install whatlies \n快速上手 spacy模型中的词向量均为几十上百维度的词向量，通过压缩映射至二维空间后，横坐标man，纵坐标woman，就可以将词语的性别倾向可视化出来。\n词向量语言模型会学习到人类的刻板印象，\n大数据时代下社会科学研究方法的拓展—基于词嵌入技术的文本分析的应用\n词嵌入测量不同群体对某概念的态度(偏见)\n例如nurse是女性，doctor是男性。\n制作两维度画轴，其中以woman作纵轴，man作横轴。 nurse、queen一般更多的是女性从业者，因此更接近y轴。 king国王多为男性，所以更接近x轴。\n至于动物，女性喜欢养猫，男性喜欢养狗，所以也能体现出词语的性别倾向。\nfrom whatlies import EmbeddingSet from whatlies.language import SpacyLanguage lang = SpacyLanguage(\u0026#34;en_core_web_md\u0026#34;) words = [\u0026#34;cat\u0026#34;, \u0026#34;dog\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;kitten\u0026#34;, \u0026#34;man\u0026#34;, \u0026#34;woman\u0026#34;, \u0026#34;king\u0026#34;, \u0026#34;queen\u0026#34;, \u0026#34;doctor\u0026#34;, \u0026#34;nurse\u0026#34;] emb = EmbeddingSet(*[lang[w] for w in words]) emb.plot_interactive(x_axis=emb[\u0026#34;man\u0026#34;], y_axis=emb[\u0026#34;woman\u0026#34;])   whatlies也可以对中文进行操作。\nfrom whatlies import EmbeddingSet from whatlies.language import SpacyLanguage zh_lang = SpacyLanguage(\u0026#34;zh_core_web_md\u0026#34;) zh_words = [\u0026#34;猫\u0026#34;, \u0026#34;狗\u0026#34;, \u0026#34;鱼\u0026#34;, \u0026#34;鲤鱼\u0026#34;, \u0026#34;男人\u0026#34;, \u0026#34;女人\u0026#34;, \u0026#34;国王\u0026#34;, \u0026#34;王后\u0026#34;, \u0026#34;医生\u0026#34;, \u0026#34;护士\u0026#34;] zh_emb = EmbeddingSet(*[zh_lang[w] for w in zh_words]) zh_emb.plot_interactive(x_axis=zh_emb[\u0026#34;男人\u0026#34;], y_axis=zh_emb[\u0026#34;女人\u0026#34;])   广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/whatlies_word2vec/","summary":"代码下载  链接:https://pan.baidu.com/s/1vJohEJ0pc6t4PBK04PiZbg 密码:t7a6\n whatlies 可以与spacy语言模型结合，可视化词向量。安装zh_core_web_md、en_core_web_md和whatlies。具体文档可以查看https://github.com/RasaHQ/whatlies\n!pip3 install zh_core_web_md-3.0.0-py3-none-any.whl !pip3 install en_core_web_md-3.0.0-py3-none-any.whl !pip3 install whatlies \n快速上手 spacy模型中的词向量均为几十上百维度的词向量，通过压缩映射至二维空间后，横坐标man，纵坐标woman，就可以将词语的性别倾向可视化出来。\n词向量语言模型会学习到人类的刻板印象，\n大数据时代下社会科学研究方法的拓展—基于词嵌入技术的文本分析的应用\n词嵌入测量不同群体对某概念的态度(偏见)\n例如nurse是女性，doctor是男性。\n制作两维度画轴，其中以woman作纵轴，man作横轴。 nurse、queen一般更多的是女性从业者，因此更接近y轴。 king国王多为男性，所以更接近x轴。\n至于动物，女性喜欢养猫，男性喜欢养狗，所以也能体现出词语的性别倾向。\nfrom whatlies import EmbeddingSet from whatlies.language import SpacyLanguage lang = SpacyLanguage(\u0026#34;en_core_web_md\u0026#34;) words = [\u0026#34;cat\u0026#34;, \u0026#34;dog\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;kitten\u0026#34;, \u0026#34;man\u0026#34;, \u0026#34;woman\u0026#34;, \u0026#34;king\u0026#34;, \u0026#34;queen\u0026#34;, \u0026#34;doctor\u0026#34;, \u0026#34;nurse\u0026#34;] emb = EmbeddingSet(*[lang[w] for w in words]) emb.plot_interactive(x_axis=emb[\u0026#34;man\u0026#34;], y_axis=emb[\u0026#34;woman\u0026#34;])   whatlies也可以对中文进行操作。\nfrom whatlies import EmbeddingSet from whatlies.language import SpacyLanguage zh_lang = SpacyLanguage(\u0026#34;zh_core_web_md\u0026#34;) zh_words = [\u0026#34;猫\u0026#34;, \u0026#34;狗\u0026#34;, \u0026#34;鱼\u0026#34;, \u0026#34;鲤鱼\u0026#34;, \u0026#34;男人\u0026#34;, \u0026#34;女人\u0026#34;, \u0026#34;国王\u0026#34;, \u0026#34;王后\u0026#34;, \u0026#34;医生\u0026#34;, \u0026#34;护士\u0026#34;] zh_emb = EmbeddingSet(*[zh_lang[w] for w in zh_words]) zh_emb.","title":"whatlies库|可视化词向量"},{"content":"Bloxs是一个简单的 python 可交互的可视化包，可以帮助您以一种有吸引力的方式（以块形式）显示信息。 非常适合在笔记本中构建仪表板、报告和应用程序。\n它适用于：Jupyter Notebook、Google Colab、Deepnote、Kaggle Notebook、Mercury。\n安装 pip3 install bloxs \n快速上手 from bloxs import B B(1234, \u0026#34;Bloxs in notebook!\u0026#34;) 案例    Bloxs Code      B(1234, \u0026quot;Bloxs in notebook!\u0026quot;)    B(1999, \u0026quot;Percent change!\u0026quot;, percent_change=10)    B(\u0026quot;🎉🎉🎉\u0026quot;, \u0026quot;Works with emojis\u0026quot;)    B(\u0026quot;68%\u0026quot;, \u0026quot;Loading progress\u0026quot;, progress=68)    B(\u0026quot;68%\u0026quot;, \u0026quot;Loading progress\u0026quot;, progress=68, color=\u0026quot;green\u0026quot;) 颜色color参数可以设为\u0026quot;blue\u0026quot;, \u0026ldquo;red\u0026rdquo;, \u0026ldquo;green\u0026rdquo; 或十六进制表示 (例如\u0026quot;#fa33fa\u0026quot;)    B(\u0026quot;123\u0026quot;, \u0026quot;Display line chart\u0026quot;, points=[1,4,2,3,5,6])    B(\u0026quot;123\u0026quot;, \u0026quot;Display line chart\u0026quot;, points=[1,4,2,3,5,6], color=\u0026quot;red\u0026quot;)    B(\u0026quot;123\u0026quot;, \u0026quot;Display stepped chart\u0026quot;, points=[1,4,2,3,5,6], chart_type=\u0026quot;stepped\u0026quot;)    B(\u0026quot;123\u0026quot;, \u0026quot;Display bar chart\u0026quot;, points=[1,4,2,3,5,6], chart_type=\u0026quot;bar\u0026quot;)    B(\u0026quot;123\u0026quot;, \u0026quot;Display bar chart\u0026quot;, points=[1,4,2,3,5,6], chart_type=\u0026quot;bar\u0026quot;, color=\u0026quot;green\u0026quot;)    可以在一行内整合多个图\nB([ B(1999, \u0026#34;Percent change!\u0026#34;, percent_change=10), B(\u0026#34;🎉🎉🎉\u0026#34;, \u0026#34;Works with emojis\u0026#34;), B(\u0026#34;68%\u0026#34;, \u0026#34;Loading progress\u0026#34;, progress=68), B(1234, \u0026#34;Bloxs in notebook!\u0026#34;) ]) B([ B(\u0026#34;786\u0026#34;, \u0026#34;Display bar chart\u0026#34;, points=[1,4,2,3,5,6], chart_type=\u0026#34;bar\u0026#34;, color=\u0026#34;green\u0026#34;), B(\u0026#34;123\u0026#34;, \u0026#34;Display line chart\u0026#34;, points=[1,4,2,3,5,6], color=\u0026#34;red\u0026#34;), B(\u0026#34;123\u0026#34;, \u0026#34;Display stepped chart\u0026#34;, points=[1,4,2,3,5,6], chart_type=\u0026#34;stepped\u0026#34;) ]) 如果想在自己电脑中实验上述代码，可以点击 notebook 下载。\n结合Mercury用Bloxs Mercury 是一个用于将笔记本转换为交互式网络应用程序的框架。 它基于 YAML 配置将小部件添加到笔记本中。 下面展示了一个带有 bloxs 的笔记本，以及作为 Mercury 的 Web 应用程序的同一笔记本。\nMercury网页应用 Demo\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/bloxs_interactive_visualization/","summary":"Bloxs是一个简单的 python 可交互的可视化包，可以帮助您以一种有吸引力的方式（以块形式）显示信息。 非常适合在笔记本中构建仪表板、报告和应用程序。\n它适用于：Jupyter Notebook、Google Colab、Deepnote、Kaggle Notebook、Mercury。\n安装 pip3 install bloxs \n快速上手 from bloxs import B B(1234, \u0026#34;Bloxs in notebook!\u0026#34;) 案例    Bloxs Code      B(1234, \u0026quot;Bloxs in notebook!\u0026quot;)    B(1999, \u0026quot;Percent change!\u0026quot;, percent_change=10)    B(\u0026quot;🎉🎉🎉\u0026quot;, \u0026quot;Works with emojis\u0026quot;)    B(\u0026quot;68%\u0026quot;, \u0026quot;Loading progress\u0026quot;, progress=68)    B(\u0026quot;68%\u0026quot;, \u0026quot;Loading progress\u0026quot;, progress=68, color=\u0026quot;green\u0026quot;) 颜色color参数可以设为\u0026quot;blue\u0026quot;, \u0026ldquo;red\u0026rdquo;, \u0026ldquo;green\u0026rdquo; 或十六进制表示 (例如\u0026quot;#fa33fa\u0026quot;)    B(\u0026quot;123\u0026quot;, \u0026quot;Display line chart\u0026quot;, points=[1,4,2,3,5,6])    B(\u0026quot;123\u0026quot;, \u0026quot;Display line chart\u0026quot;, points=[1,4,2,3,5,6], color=\u0026quot;red\u0026quot;)    B(\u0026quot;123\u0026quot;, \u0026quot;Display stepped chart\u0026quot;, points=[1,4,2,3,5,6], chart_type=\u0026quot;stepped\u0026quot;)    B(\u0026quot;123\u0026quot;, \u0026quot;Display bar chart\u0026quot;, points=[1,4,2,3,5,6], chart_type=\u0026quot;bar\u0026quot;)    B(\u0026quot;123\u0026quot;, \u0026quot;Display bar chart\u0026quot;, points=[1,4,2,3,5,6], chart_type=\u0026quot;bar\u0026quot;, color=\u0026quot;green\u0026quot;)    可以在一行内整合多个图","title":"Bloxs包 | 可在notebook中使用的交互可视化包"},{"content":"本期TechWeekly主要是一些css、js类项目，可以起到点缀网站的效果。\n建议大家如果有时间，可以了解下html/css，很简单的。如果使用Hugo这类建站框架，可以自己修改下字体颜色，以符合自己的审美。以后会考虑在这个博客站点中用到，于是整理出来本期TechWeekly。\nAnimate.css https://animate.style/\nCSS网页动画库\n 点击图片访问Animate网站   Tabler https://github.com/tabler/tabler\nTabler 是基于 Bootstrap 构建的免费开源 HTML Dashboard UI Kit\n 点击图片访问tabler网站   Flowbite https://github.com/themesberg/flowbite\n使用 Tailwind CSS 构建的最受欢迎的交互式 UI 组件库\n 点击图片访问flowbite网站   SheetJs https://github.com/SheetJS/sheetjs\n  简化的电子表格\n  阅读、编辑和导出电子表格\n  适用于网络浏览器和服务器，\n  在 Office 365 中受 Microsoft 信任\n   点击图片访问网站   D3 https://github.com/d3/d3\nD3.js 是一个 JavaScript 可视化库。 D3 能使 HTML、SVG 和 CSS 等文件将数据可视化变为现实。\n 点击图片访问网站   下面是D3制作的样例\n 点击图片访问D3的Gallery    点击图片访问D3的Gallery    点击图片访问D3的Gallery   budibase https://github.com/Budibase/budibase\n在几分钟内构建组织内网的应用程序，支持Supports PostgreSQL, MySQL, MSSQL, MongoDB, Rest API, Docker, K8s\n 点击图片访问budibase网站   vitae https://pkg.mitchelloharawild.com/vitae/\n使用Rmarkdown制作个人简历\n 点击图片访问budibase网站   nextra https://github.com/shuding/nextra\nNextra 是一个 Next.js 和 MDX 驱动的无代码站点生成器。1分钟即可制作出下图的静态网站。\n 点击图片访问nextra网站   了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly16/","summary":"本期TechWeekly主要是一些css、js类项目，可以起到点缀网站的效果。\n建议大家如果有时间，可以了解下html/css，很简单的。如果使用Hugo这类建站框架，可以自己修改下字体颜色，以符合自己的审美。以后会考虑在这个博客站点中用到，于是整理出来本期TechWeekly。\nAnimate.css https://animate.style/\nCSS网页动画库\n 点击图片访问Animate网站   Tabler https://github.com/tabler/tabler\nTabler 是基于 Bootstrap 构建的免费开源 HTML Dashboard UI Kit\n 点击图片访问tabler网站   Flowbite https://github.com/themesberg/flowbite\n使用 Tailwind CSS 构建的最受欢迎的交互式 UI 组件库\n 点击图片访问flowbite网站   SheetJs https://github.com/SheetJS/sheetjs\n  简化的电子表格\n  阅读、编辑和导出电子表格\n  适用于网络浏览器和服务器，\n  在 Office 365 中受 Microsoft 信任\n   点击图片访问网站   D3 https://github.com/d3/d3\nD3.js 是一个 JavaScript 可视化库。 D3 能使 HTML、SVG 和 CSS 等文件将数据可视化变为现实。","title":"TechWeekly-16 每周有趣有用的技术分享"},{"content":"Deng\u0026rsquo;s StartPage 最近一年多，大邓之前一直使用自己diy的浏览器启动页, 最初是被该启动页的关键词搜索功能所惊艳。但用久了人总会喜新厌旧，但是每次打开浏览器，呈现的网址链接效果太繁杂了。而且大多数都是僵尸链接，平常很少点击的，反而常用的链接都设置成了快捷键。\n\n经过一番搜索，找到以下9个启动页，除了startpage7和startpage8外，大多数启动页都具有快捷关键词搜索方法， 大家根据自己喜好进行选择。\n快捷关键词搜索方法 4个启动页基本拥有相同的快捷关键词搜索方法\n 点击时钟，弹出日期框和倒计时信息。 搜索框支持command/搜索语法. 例如  在搜索框输入scholar/ python回车，就能实现在谷歌学术中搜python相关信息。类似的功能还有。\n   搜索框命令 目标网站 例子 作用     scholar/ google scholar scholar/ python 在谷歌学术中搜python相关信息   book/ 豆瓣读书 book/ python 在豆瓣读书中搜python相关书籍信息   zhihu/ 知乎 zhihu/ python 在知乎网站搜python相关信息   youtube/ youtube youtube/ python 在youtube搜索python相关视频   taobao/ 淘宝 taobao/ python 在淘宝搜python相关商品服务   jd/ 京东 jd/ python 在京东搜python相关商品服务   bilibili/ B站 bilibili/ python 在B站搜索python相关视频   github/ github github/ python 在github上搜python相关仓库代码等信息   mail/ qq邮箱 mail/ 默认打开qq邮箱   medium/ Medium medium/ python 在medium中搜python相关内容   arxiv/ arxiv arxiv/ python 在arXiv中搜python相关论文   img/ unsplash图片网 img/ sun 在unsplash中搜sun相关的图片   scihub/ scihub scihub/ 论文的doi 在scihub中根据论文doi寻找论文pdf资源    \n9个启动页 9个启动页搜索功能略微有些差异，并不是所有的都具备快捷关键词搜索方法。\n 第一类，有快捷关键词搜索方法，其中startpage1、2、3、5、6、9项目文件夹中使用了相同的搜索引擎js文件search-box.js 第二类，有快捷关键词搜索方法， startpage4使用特有的js文件定义了常用的关键词检索方法，配置文件为config.js； 第三类，没快捷关键词搜索方法， startpage7、8各自使用的自由搜索引擎。  startpage1   Demo\n  Code Download\n  拥有关键词快捷搜索功能。简单到项目文件夹中只有html、js、css及字体4个文件。\nStartpage2   Demo\n  Code Download\n  拥有关键词快捷搜索功能。如startpage1一般简单，项目文件夹中也只有html、js、css及字体4个文件。\nStartpage3   Demo\n  Code Download\n  拥有关键词快捷搜索功能。相比前两个，这个启动页拥有自动背景切换功能，选取的都是漂亮的油画。\nStartpage4   Demo\n  Code Download\n  拥有关键词快捷搜索功能。功能最强大，最炫酷的，大邓正在使用的启动页。\nStartpage5   Demo\n  Code Download\n  拥有关键词快捷搜索功能。上方的黑色波浪是动画，实时变化。\nStartpage6   Demo\n  Code Download\n  拥有关键词快捷搜索功能。\nStartpage7   Demo\n  Code Download\n  注意，startpage7没有关键词快捷搜索功能。\nStartpage8   Demo\n  Code Download\n  注意，startpage8没有关键词快捷搜索功能。\nStartpage9   Demo\n  Code Download\n  拥有关键词快捷搜索功能。\nCode使用方法 下载code压缩文件夹，以startpage1文件夹为例， 使用浏览器打开文件夹中的index.html文件，浏览器就是如下效果。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/dengstartpage/","summary":"Deng\u0026rsquo;s StartPage 最近一年多，大邓之前一直使用自己diy的浏览器启动页, 最初是被该启动页的关键词搜索功能所惊艳。但用久了人总会喜新厌旧，但是每次打开浏览器，呈现的网址链接效果太繁杂了。而且大多数都是僵尸链接，平常很少点击的，反而常用的链接都设置成了快捷键。\n\n经过一番搜索，找到以下9个启动页，除了startpage7和startpage8外，大多数启动页都具有快捷关键词搜索方法， 大家根据自己喜好进行选择。\n快捷关键词搜索方法 4个启动页基本拥有相同的快捷关键词搜索方法\n 点击时钟，弹出日期框和倒计时信息。 搜索框支持command/搜索语法. 例如  在搜索框输入scholar/ python回车，就能实现在谷歌学术中搜python相关信息。类似的功能还有。\n   搜索框命令 目标网站 例子 作用     scholar/ google scholar scholar/ python 在谷歌学术中搜python相关信息   book/ 豆瓣读书 book/ python 在豆瓣读书中搜python相关书籍信息   zhihu/ 知乎 zhihu/ python 在知乎网站搜python相关信息   youtube/ youtube youtube/ python 在youtube搜索python相关视频   taobao/ 淘宝 taobao/ python 在淘宝搜python相关商品服务   jd/ 京东 jd/ python 在京东搜python相关商品服务   bilibili/ B站 bilibili/ python 在B站搜索python相关视频   github/ github github/ python 在github上搜python相关仓库代码等信息   mail/ qq邮箱 mail/ 默认打开qq邮箱   medium/ Medium medium/ python 在medium中搜python相关内容   arxiv/ arxiv arxiv/ python 在arXiv中搜python相关论文   img/ unsplash图片网 img/ sun 在unsplash中搜sun相关的图片   scihub/ scihub scihub/ 论文的doi 在scihub中根据论文doi寻找论文pdf资源","title":"极简浏览器启动页StartPage分享"},{"content":"摘要 在过去的二十年里，可供营销研究人员分析的文本数据量呈指数级增长。 然而，书面语言充满了复杂的含义、歧义和细微差别。 营销研究人员如何才能将这种丰富的语言表示转化为可量化的数据以进行统计分析和建模？ 本章介绍了文本分析各种方法。 在涵盖了文本分析的一些基础知识后，总结和探索了在营销研究中的应用，如情感分析、主题建模和研究组织沟通，包括对产品发布的口碑反应的案例研究。\n本文相关资料  Christian Homburg, Martin Klarmann, and Arnd Vomberg. 2022. Handbook of Market Research.\nHumphreys A. (2019) Automated Text Analysis. In: Homburg C., Klarmann M., Vomberg A. (eds) Handbook of Market Research. Springer, Cham. https://doi.org/10.1007/978-3-319-05542-8_26-1\nHumphreys, Ashlee, and Rebecca Jen-Hui Wang. \u0026ldquo;Automated text analysis for consumer research.\u0026rdquo; Journal of Consumer Research 44.6 (2018): 1274-1306.\n 关键词   文本分析Text analysis\n  计算机辅助文本分析computer-assisted text analysis\n  自动内容分析automated content analysis\n  内容分析content analysis\n  主题建模topic modeling\n  情感分析sentiment analysis\n  LDA主题分析LDA\n  口碑word-of-mouth\n  一、引言 要了解自动文本分析(后文均简称文本分析)，首先要回顾一下它与传统内容分析的关系。内容分析是社会科学中用于系统地评估和分析消息内容的一种方法，通常以文本的形式，最早的内容分析传统可以追溯到 16 世纪的修道院生活，但现代内容分析是由 Max Weber (1924) 首次提出来研究新闻的。从那时起，社会学和传播学的学者就使用人类编码的内容分析来调查媒体内容的差异，描述传播随时间的趋势，揭示组织或个人关注的模式，并检查个人的态度、兴趣、意图或价值观或一组（例如，Berelson 1971；Gamson 和 Modigliani 1989）。\n传统的内容分析首先通过 Kassarjian (1977) 的方法大纲引入消费者行为领域，然后由 Kolbe 和 Burnett (1991) 进行再创新，以提高可靠性和客观性，主要关注计算编码间一致性的标准（参见也格雷森和锈 2001）。在消费者研究和营销中，传统的内容分析已被用于分析杂志广告（Belk 和 Pollay 1985）、直邮direct mail（Stevenson 和 Swayne 1999）、报纸文章（Garrett 1987）和口碑传播（Moore 2015；菲尔普斯等人 2004）。虽然文本分析可以提高传统内容分析的效率和可靠性，但它也有局限性。例如，计算机化的文本分析可能会遗漏文本中的细微之处，并且无法编码更精细的含义。\n文本分析并不是什么新鲜事，但自从个人计算机广泛采用以来，它已经变得更容易实现了。 General Inquirer (Stone 1966) 是最早用于消费者研究的计算机内容分析工具之一 (Kranz 1970)。 Kranz (1970) 对营销中内容分析涉及字典创建，但没有解决类别创建、有效性等。从那时起，在文本分析方面取得了长足的进步。\n二、文本分析的方法 在当前实践中，自动化文本分析基本上有词典法、机器学习法（分类方法、主题模型）。\n2.1 词典法 研究者测度文本中某些构念(概念)前，首先需要寻找到文本中存在的规律，通过构建单词列表或一组规则来完成(识别或)测量（Rayson 2009）。该方法已广泛用于社会科学，如消费者研究（Humphreys 和 Wang 2018）、心理学（Chung 和 Pennebaker 2013；Mehl 和 Gill 2008；Pennebaker 和 King 1999）、社会学（Van de Rijt 等人） . 2013) 和政治学 (Grimmer and Stewart 2013; Lasswell and Leites 1949)，因为词典法能够将理论构念转化为可测度的文本元素，并且算法具有可靠的透明度(相比而言，机器学习算法更像是黑箱)。另一方面，自下而上的方法在工程、计算机科学和营销科学中得到了更广泛的应用。营销策略借鉴了这两种方法，尽管基于字典的方法似乎更常见（Ertimur 和 CoskunerBalli 2015；Humphreys 2010；Ludwig 等人 2013；Packard 等人 2014）。\n基于字典的文本分析方法基于预先开发的单词列表或字典，用于计算文本中单词的出现次数。标准化词典可用于许多构造，例如情绪（例如，Hutto 和 Gilbert 2014）、与营销相关的构造，例如真实性和品牌个性（Kovács 等人 2013；Opoku 等人 2006），以及心理学中的许多标准概念(Pennebaker et al. 2001; Snefjella and Kuperman 2015) 和其他领域，如政治学 (Dunphy et al. 1974; Stone 1966)。除了使用标准字典外，许多研究人员选择创建自己的字典以适应特定的上下文，尽管只有在标准字典不可用时才应该这样做。\n字典创建最归纳的方法是根据研究问题和假设的相关类别，先从文档中按频率列出的所有单词和词组（Chung and Pennebaker 2013）。如果研究人员事先不知道哪些类别是相关的，则可以使用在词典开发之前，使用定性研究方法，编码过程中创建一组相关概念及对应的单词列表，（Humphreys 2010）。例如，为了研究报纸文章中与瑜伽行业有关的制度逻辑，Ertimur 和Coskuner-Balli (2015) 首先主轴编码了报纸文章和其他历史文本的数据集。通常，数据集中随机选取10-20% 的样本足以进行编码（Humphreys 和 Wang 2018），但研究人员应注意数据量，根据类别或时间段的不均匀性，相应地进行分层处理（Humphreys 2010）。\n字典创建的最演绎方法是从理论概念或类别创建一个词表。然而，应该注意研究人员和作家倾向于选择比文本数据中普遍存在的更抽象的词（Palmquist 等人，2009 年）。出于这个原因，仔细的后测验证是必要的，以确保词典结构的有效性。在清理和存储文本并创建字典后，研究人员使用 Python、Diction、LIWC、WordStat 或 R 等程序来执行计数。然后可以使用传统的统计包保存和分析数据。\n词典构建过程中，如果要进行后测验证，有很多方法检验方法。\n 彭内贝克等人 (2001) 推荐了一种验证字典的方法，但不推荐结果测量。在这里，三个研究助理将一个词视为该类别的代表或不代表，如果三个编码员中有两个同意，则保留该词。如果他们不这样做，则应从字典中删除该词。然后可以计算和报告字典类别的百分比协议，一般阈值与 Krippendorf 的 alpha 相似，高于 75%。 Weber (2005) 提出了一个饱和程序，研究人员从一个概念的 10 或 20 个实例中抽取样本，并让研究助理将它们编码为准确地代表类别（或不代表）。如果比率低于 80%，则应修改字典类别，直到达到阈值。 最后一种方法是将计算机编码的结果与来自两个或更多编码器的大量人工编码结果进行比较。为此，人们从数据集中选择一个随机样本（数量可能会因数据集的大小而异），然后人工编码人员根据类别描述对文本进行编码，并像在传统内容分析中一样计算可靠性。然后可以将其与计算机的附加“编码器”进行比较以产生相似度分数。尽管这种最终方法具有与传统内容分析比较的优势，但它并不总是必要的，并且在某些情况下会产生错误的结果。人类编码员能够理解计算机无法获得的微妙含义，同样，计算机能够一致且均匀地在整个数据集上对概念进行编码，而不会出现遗漏或偏见。出于这个原因，在某些情况下，将人类编码与计算机编码进行比较就像将苹果与橙子进行比较。  基于词典的分析已研究了很多领域的理论概念，例如情感emotion（Berger 和 Milkman 2012）、解释水平construal level（Snefjella 和 Kuperman 2015）、制度逻辑institutional logics（Ertimur 和 Coskuner-Balli 2015）、风险risk（Humphreys 和 Thompson 2014）、 言语行为speech acts（Ludwig et al. 2016; Villarroel Ordenes et al. 2017）和框架framing（Fiss and Hirsch 2005; Humphreys and Latour 2013; Jurafsky et al. 2014）。 可以通过基于词典的分析来探索各种上下文，例如产品和餐厅评论（Barasch 和 Berger 2014，Jurafsky 等人 2014；Kovács 等人 2013）、推文（Mogilner 等人 2010）、客户服务电话 (Packard et al. 2014)、博客 (Arsel and Bean 2013) 和新闻文章 (Humphreys 2010; Humphreys and Thompson 2014)。\n2.3 机器学习法 机器学习法又细分为分类算法和主题建模。\n2.3.1 分类方法 分类方法基于将文档分类为不同的“类型”，然后进一步描述（计算）文本中哪些元素为该条文本数据的成为某“类型”贡献了多少权重(可能性)。例如，Tirunillai 和 Tellis (2012) 使用分类来训练一个机器模型，以根据星级来识别正面和负面评论。研究者对训练数据集，使用朴素贝叶斯和支持向量机 (SVM) 分类器来找出哪些词可以预测星级，然后使用这些信息对整个评论集进行分类，达到精确度——这意味着他们的算法预测真阳性——68-85% 的时间，取决于产品类别。 Villarroel Ordenes 等人（2017 年）通过使用文本中显式和隐式情绪指标来衡量情绪和情绪强度，进一步完善情绪测量，还在 Tripadvisor、亚马逊和 Barnes and Noble 的一组星级评论上测试了他们的框架。分类模型算法的复杂程度各不相同；例如，这些方法的情绪准确性从 55% 到 96% 不等（Hutto and Gilbert 2014）。\n分类模型已被用于研究评论 (Tirunillai and Tellis 2012; Van Laer et al. 2017)、在线论坛 (Homburg et al. 2015)、电子邮件 (Ludwig et al. 2016) 和文学文本 (Boyd and Pennebaker 2015b; Plaisant 等人，2006 年）。例如，为了衡量留言板帖子的情绪，Homburg 等人（2015）对明确的正面和负面帖子的训练数据集进行分类。然后，他们使用情绪作为一个独立的衡量标准来了解公司参与实际上增加了多少积极的消费者情绪，发现参与的回报是递减的。\n2.3.2 主题建模 主题建模是一种方法，它首先将文本解析为离散的单词，然后假设单词出现是独立的情况下找到在统计上不太可能出现的共现模式（看不懂没关系，继续阅读）。通过这种方式，分析识别出可能由明显存在的词潜在表示的类别，然后标记这些词组以表示数据中有意义的概念或特征，就像在因子分析中所做的那样。例如，在酒店评论研究中，Mankad 等人。 (2016) 使用潜在狄利克雷分配 (LDA) 来识别用户 TripAdvisor 评论中出现的五个主题，将便利设施、位置、交易、价值和体验确定为评论者提到的关键主题。潜在语义分析 (LSA)、k-means 聚类 (Lee and Bradlow 2011)、概率潜在语义分析 (PLSA) 和 LDA (Blei et al. 2003) 都是主题建模的方法，其中 LDA 是最新和最常见的方法主题建模的分析方法。\nLDA 是一种分层贝叶斯模型，用于确定给定文档中存在的主题概率分布组合。 LDA主题建模前需要研究者设定主题的数量。假设主题的选择存在一定的概率分布，并且在该分布中选择表示该主题的词有一定的分布，LDA 会生成最终的主题列表（由该主题中的词列表表示）和给定主题在文档中的概率。尽管大多数方法都是基于单词或短语的，但 Büschken 和 Allenby (2016) 使用句子作为分析单位进行了 LDA 分析，并发现这产生的结果比基于单词的 LDA 更能预测评分。基于句子的模型假设句子中的所有单词都是同一主题的一部分，考虑到 Grice 的关系和方式准则（Grice 1975），这是合理的。 Büschken 和 Allenby (2016) 使用此模型从 Expedia 和 we8there.com 上的评论中识别意大利餐厅和酒店的主题。\nLDA 已在广泛的应用中使用（Büschken 和 Allenby 2016；Tirunillai 和 Tellis 2014）。与词典法一样，后测验证，在这种情况下，使用保留样本或其他预测技术（例如，外部 DV）是非常可取的。机器只会读取字面意思，因此同音异义词和包括讽刺在内的其他口语可能会出现问题，因为它们是过于笼统和过于具体的词。此外，仔细清理和准备文本可以减少错误，因为有时可以在数据收集期间添加文本标记（例如，页眉、页脚等）。\n三、文本分析的市场研究应用 3.1 情感分析 许多文本分析程序和从业者声称可以衡量情绪，但并不总是清楚这个关键指标的内涵。在讨论情感的文本分析之前，首先要讨论什么是情感、情感能体现文本中的什么信息。在大多数营销环境中，研究人员和从业者都对消费者对品牌、产品或服务的态度感兴趣。然而，态度是复杂的心理结构，不仅由情感组成，还包括认知信念和意图（Fishbein 和 Ajzen 1972）。此外，对于任何给定产品的最终购买态度和未来行为（如忠诚度）的重要性在很大程度上取决于上下文和参与（Petty 和 Cacioppo 1979）。人们在网上表达的态度可能并不能完全反映他们的潜在态度，他们选择表达的态度可能存在选择偏差，并且他们的行为也可能与他们所支持的态度不同。尽管如此，以情感表达的在线话语可以反映对品牌、产品或服务的某种潜在态度，并且重要的是可以影响其他消费者之间共享的社会共识。情绪已被证明可以预测电影销量（Krauss et al. 2008; Mestyán et al. 2013）和股票市场回报（Bollen et al. 2011; De Choudhury et al. 2008; Tirunillai and Tellis 2012）。\n情感得分的计算方法\n  大多数方法试图将文本分类或测量为具有积极、消极或有时是中性的情绪\n  另一些方法将其转化为净情绪，从积极的词中减去消极词（例如，Ludwig et al. 2013; Homburg et al. 2015 ）。\n  除了情感效价valence，情绪也可以有强度strength和确定性certainty。先前的研究使用了明确的、语义的情感指标以及隐含的、更实用的情感指标，如**言语行为speech acts（委托、断言和方向）**来成功测量情感强度（Villarroel Ordenes 等人，2017 年）。进一步研究表明，其他类型言语，如指示语 (Potts and Schwarz 2010) 和其他语用标记可以指示表达性内容，通常在产品评论中表达 (Constant et al. 2009)。\n使用预先开发的标准化词典是衡量跨上下文情绪的最可靠方法之一，因为这些词汇表已经开发并在广泛的文本数据上进行了测试。例如，VADAR(一种英文情感词典)使用带有基于规则的方法的字典来测量情绪。具体来说，Hutto 和 Gilbert (2014) 使用了基于以前标准化词典（如 LIWC 和 General Inquirer）的词典组合，但随后还开发了五个规则，这些规则考虑了句法和语法来衡量强度。使用词典法衡量情绪产生的准确率从 55% 到 96% 不等，具体取决于上下文（Hutto 和 Gilbert 2014）。 例如，Tirunillai 和 Tellis (2012) 使用星级来创建情绪分类系统，准确率为 68-85%。\n3.2 通过文本分析研究口碑 迄今为止，文本分析在营销研究中的主要用途是研究在线口碑传播。消费者总是通过人际交流来分享产品信息（Arndt 1967），这种交流已被证明比商业信息更有效（Brown 和 Reingen 1987；另见 Godes 和 Mayzlin 2004；Money 等人 1998）。然而，虽然口碑传播以前是通过面对面或电话进行的，但现在它可以在社交购物网站（Stephen 和 Toubia 2010）、社交媒体（Humphreys 2015）和第三方评论网站上看到并存档和平台。亚马逊上的产品评论、TripAdvisor 上的酒店评论和 Yelp 上的餐厅评论！都提供了营销见解，以更好地了解评级与销售额和股价之间的关系（Moe 和 Schweidel 2014；Schweidel 和 Moe 2014；Moe 和 Trusov 2011）。例如，Moe 和 Trusov (2011) 发现正面评论对销售有直接影响，但这种影响在某种程度上是短暂的，因为随着人们发布更多评分（即，帖子的社会动态导致评论变得相对更加负面）随着时间的推移）。此外，积极性可能因平台而异（Schweidel 和 Moe 2014；Villarroel Ordenes 等人 2017）。\n在线口碑可以通过测量情感效价、评论数量和评分分布的方差来表示（Godes 和 Mayzlin 2004）。评论数量和评分方差与现有的建模测量相对兼容，因为评论数可以聚合，评分的方差可以通过开始评级或其他用户输入来测量。情感效价虽然部分由星标衡量，但最好用情感衡量，这需要文本分析作为一种方法，将语言描述的非结构化数据转换为可纳入定量模型的数据。应该指出的是，除了情感效价之外，还有广泛的语言属性和语义内容可以为营销研究提供有用的信息（Humphreys and Wang 2018）。例如，Kovács 等人。 (2013) 表明，如果评论者在评论中提及真实性，即使在控制餐厅质量的情况下，餐厅的评分也会更高。\n情感在口碑传播中的作用是一个关键话题。在一项关于分享新闻文章的研究中，Berger 和 Milkman (2012) 发现积极情绪会增加病毒式传播，但文章中存在强烈的负面情绪（如愤怒或焦虑）也会增加病毒式传播。还通过使用代词的文本分析研究了发送者和言语上下文的影响。使用第一人称人称代词（“I”、“me”）的标准字典，Packard 和 Wooten（2013 年）发现，消费者通过口耳相传更多地自我提升，以表明对特定领域的了解。消费者也被证明通过在向大量观众广播时分享较少的负面情绪来进行自我展示，而不是对较小的观众进行窄播（Barasch 和 Berger 2014）。在像电影一样评估产品时，消费者在表达对品味的看法与对质量的看法时，更有可能使用指代自己的代词（Spiller 和 Belogolova 2016）。\n3.3 创建公司(产品)定位图、主题发现 文本分析可用于为品牌、公司或产品创建定位图，并根据特定类别中的属性可视化市场结构。**借助k-means 聚类算法或LDA 主题建模算法，可按一定的潜在逻辑(属性、品牌)，对文本中的常见词进行分组。**例如，为了从 Epinions.com 上的一组评论中创建相机市场结构的可视化，Lee 和 Bradlow (2011) 首先提取与特定属性（例如，电池寿命、照片质量）相关的短语，然后使用 k-means基于短语的相似性（计算为单词向量之间的余弦相似度）对短语进行聚类。分析发现消费者提到的和对消费者重要的属性存在差异，但在尺寸、设计和屏幕亮度等专家评论中却没有差异。同样，使用来自糖尿病论坛的文本数据，Netzer 等人。 (2012) 发现论坛上经常提到的几种副作用，但在 WebMD 之类的网站上却没有（例如，体重增加、肾脏问题）。\n主题模型与心理学理论之间是兼容的，例如语义记忆中的传播激活（Collins and Loftus 1975）。例如，人们是在一定的语义记忆中谈论相关的品牌。受到该想法启发，Netzer 等人 (2012) 使用 Edmunds.com 的评论为汽车品牌制作感知地图，并将其与使用销售(调查)数据测度的品牌转换感知地图进行比较。在此过程中，他们发现基于文本分析的结果与基于销售或调查数据的结果之间存在一些显着差异。例如，根据销售数据，韩国品牌的汽车与日本品牌没有关联。但是，根据文本数据，这些品牌被归为一类。这表明，虽然文本分析可以捕捉认知关联，但这些关联不一定会转化为品牌转换等行为（表 1）。\n   文本分析方法 数据源 应用领域 算法 相关案例     词典法 在线评论、论坛、新闻、公告、年报 情感(情绪)、心理(如解释水平)、品牌关注度、品牌价值、公司形象等 词频 Humphreys (2010), Berger and Milkman (2012), Packard et al. (2018)   分类法 在线评论、论坛、文献、推特、邮件 情感分析、欺诈识别、产品属性、市场结构 监督机器学习算法，如SVM、K近邻、朴素贝叶斯等 Homburg et al. (2015), Van Laer et al. (2018), Tirunillai and Tellis (2012)   主题模型 产品服务评论、在西安论坛 产品属性、定位图positioning、市场结构等 LDA、K-means Netzer et al. (2012), Lee and Bradlow (2006), Buschken and Allenby (2016)    3.4 组织和公司环境的测量 最后，文本分析可用于通过分析股东报告、新闻稿和其他营销传播来衡量组织关注度。这些研究主要基于基于词典的分析，并且经常创建词典而不是使用标准化词典来适应行业或原始背景和研究问题。例如，学者们开发了字典来研究 企业社会责任语言随时间的变化，以揭示发展中国家的差异（Gandolfo et al. 2016）。在对年度报告的分析中，Lee 等人 (2004) 发现，披露负面信息时，如果倾向于向内寻找原因，这类公司一年后的股价较高，这表明将责任归咎于公司控制因素的组织似乎比那些不责任的组织更有控制力，因此投资者对负面事件的印象更佳。\n企业环境也可以通过测量媒体，如报纸、杂志和贸易出版物来捕捉。例如，Humphreys (2010) 表明，制度和文化环境的变化使美国的赌场赌博业合法化。 Humphreys 和 Thompson（2014 年）研究了两次危机（埃克森和 BP 漏油）之后的风险认知环境，并发现媒体的叙述有助于遏制这些灾难之后的风险认知。 Ertimur 和 Coskuner-Balli（Ertimur 和 Coskuner-Balli 2015）追溯了瑜伽行业如何随着时间的推移而变化，形成了影响行业内品牌和定位的独特制度逻辑。\n3.5 处理文本数据的问题 尽管文本为分析消费者想法和市场战略领域研究提供了一个窗口，但在分析文本时仍有几个问题需要考虑。语言很少（如果有的话）遵循正态分布模式（Zipf 1932）。例如，像“a”、“he”和“there”这样的功能词在正常使用中占所有语言的 40% 左右。名词和动词等常用词占另外 59%，这些常用词中通常只有一小部分与研究问题相关。文本数据通常是左偏的（很多零），文档通常包含不同数量的单词，并且感兴趣的单词通常太少或太频繁出现而无法进行有意义的比较。由于这些原因，在计算出词频之后，研究人员通常会在统计分析之前对数据进行转换。此外，由于数据的非正态分布，许多检验（如 ANOVA）不适合。\n因此，文本信息几乎就是表示为文档中单词的百分比（例如，Ludwig et al. 2013），并且通常使用对数转换来解释偏度（Netzer et al. 2012），尽管使用了几种可能的转换（曼宁等人，2008 年）。 Tf-idf 是一种经常用于解释词频的度量，由整个数据集中单词的总体频率标准化（有关计算 tf*idf 的详细信息，请参阅 Salton 和 McGill 1983，以及伴随的转换选项）。\n用于测量共现的传统方法（例如 Pearson 相关性），必然导致一个问题，即数据集中存在大量的零（Netzer 等人，2012 年）。对此，研究人员经常使用余弦相似度或 Jaccard 距离来比较单词和文档。通常需要使用多种方法来计算共现的一系列稳健性检查，以确保结果不仅仅由于不经常或过于频繁出现的单词而出现（Monroe et al. 2009; Netzer et al. 2012）。例如，如果像“he他”这样的词很常见，那么与“airbag安全气囊”这样的不常见词相比，它可能与更多的词同时出现。然而，“安全气囊”这个词可能比“他”这样的人称代词更能概念可诊断性(信息量更大，更特别)。因为数据不是正态分布的，所以统计检验，例如 Mann-Whitney 检验，检验排名的显着性而不是绝对数，可以替代 ANOVA。\n四、扩展：专家和非专家之间，产品发布的口碑差异 本节介绍了一个文本分析案例，以词典法为例，将数据分析的过程分为六个阶段（转载自Humphreys and Wang (2018), Automated Text Analysis for Consumer Research, Journal of Consumer Research, 44(6), 1 (April), 1274–1306）。该案例研究消费者对 mp3 播放器/无线设备 Apple iTouch 产品发布的反应，在本文中只展示部分内容，展示从理论构念到文本分析大主要步骤。。\n   步骤 含义功能     1.识别研究问题 确定主题，以及与之对应的几个问题   2. 数据采集 识别数据源；\n在线数据库或新闻；对已有非文本数据(书籍、刊物)的数字化；网络爬虫；访谈   3. 定义构(概)念 定性分析数据的子样本; 为每个构念创建一个词典（单词列表）； 让编码人员检查和完善词典;初步实施字典以检查误报和漏报   4. 构念测量(计算) 对原始数据计算相关构念 ;\n根据研究问题，运行相关计算： 所有单词的百分比；\n时间段或类别中的单词百分比；所有编码单词的百分比； 二进制（“属于某构念”或“不属于某构念”）   5. 解读\u0026amp;分析 依文章、你那份对文本进行不同角度的分析；\n通过不同角度进行比较;\n对研究问题选择合适的统计方法:\n方差分析ANOVA；\n回归分析；相关分析；   6. 后测验证 提取子样本并由研究助理或研究人员编码，根据 Krippendorf 的 alpha 计算所构建的词典的优劣合格与否    Stage-1：提出研究问题 本研究提出了一个具体的问题：\n 在产品发布后，专家的反应与非专家的反应是否不同？ 此外，随着产品的扩散，专家组与非专家组的口碑反应如何变化？  专家的口口相传对产品采纳特别有影响，因此了解他们的观点会随着时间的推移以及与非专家群体的比较而发生变化是很重要的。 本研究选择的背景，即 Apple iTouch 的发布，是一个很好的研究案例，因为产品类别和评估产品的标准在推出时都是模棱两可的。\nStage-2: 数据收集 数据来自两个网站\n Amazon.com 亚马逊是一个销售从书籍到玩具的所有商品的网站，拥有广泛的受众； 来自Amazon的消费者评论被用来反映非专家或混合消费者的反应 CNET.com CNET 是一个专门用于技术的网站，并且可能拥有更专业的发帖人。来自CNET的用户评论被用来衡量专家的反应。  研究者收集了Amazon.com 和 CNET.com关于iTouch文本数据，采集的时间窗口从2007 年 9 月 5 日到 2009 年 11 月 6 日。“iPod Touch”的关键字搜索用于收集当时对该产品可用的所有客户评论的分析。对设备多个版本（第一代和第二代）的评论包括在分析中，并根据发布日期进行了细分。第一代 iPod Touch 于 2007 年 9 月 5 日发布，第二代于 2008 年 9 月 9 日发布。\n字段包括评论者(或发帖人)评论日期、发帖人姓名、评分、发帖人位置以及评论本身的文本都存储为单独的变量。从亚马逊收集了大约 204 个帖子，从 CNET 收集了 269 个帖子，产生的样本量足够高，可以在组之间进行统计比较。\nStage-3: 定义构念 信息处理领域的研究表明专家处理信息的方式与新手不同（Alba and Hutchinson 1987）。一般来说，\n 专家们更多地从认知角度看待产品，评估产品属性而不是好处或用途（Maheswaran 和 Sternthal 1990；Maheswaran 等人 1996；Sujan 1985）。 新手只使用刻板印象信息，专家则使用属性信息和刻板印象线索（Maheswaran 1994）。 专家能够同化吸收适应信息的类别模糊性，这意味着专家能更快地适应新产品（Meyers-Levy 和 Tybout 1989）。 专家还倾向于以抽象的、更高的解释水平来进行判断（Hong and Sternthal 2010）。  根据之前的研究，可以提出几个工作假设。我们希望进行的战略比较是关于专家与非专家如何评估产品以及这是否会随着时间而改变。首先，人们可能会期望专家会使用更多的认知语言，并且他们会更严格地评估设备。\nH1：专家会比新手使用更多的认知语言。\n其次，人们还期望专家会关注设备的功能，但非专家会更多地关注设备的使用（Maheswaran et al. 1996）。\nH2：专家会比非专家更多地讨论特征。\nH3：非专家会比专家更多地讨论好处和用途。\n第三，随着时间的推移，人们可能会预测专家将能够吸收模棱两可的产品功能(属性)，而非专家则不会。因为专家可以更容易地处理模棱两可的类别信息，并且由于他们具有更高的解释水平，所以人们会预测他们比新手更喜欢这种模棱两可的产品，并且会学会吸收模棱两可的信息。例如，在这种情况下，设备的存储空间使其难以分类（手机与 mp3 播放器）。人们会期望专家会更快地理解这种模糊性，并且随着时间的推移，他们对这一特征的阐述会减少。\nH4：随着时间的推移，专家将较少谈论模棱两可的属性（例如，存储空间），而非专家将继续讨论模棱两可的属性。\n最后，先前的研究表明，这些关注点的差异，专家在功能上和非专家在利益方面的差异，会对产品评级产生不同的影响.也就是说，非专家的评级将取决于对娱乐等福利的评估，但专家评级将更多地受到特征的影响。\nH5：评级将由非专家的利益驱动。\nH6：评级将由专家的特征驱动。\n这些只是在线口碑传播分析中可以探索的众多潜在假设中的一小部分。人们同样可以探索新技术的文化框架（Giesler 2008）或通过与博主进行产品评论来共同制作品牌传播（Kozinets 2010）。这里提出的问题——随着时间的推移，专家对新产品的反应是否与非专家不同？ – 旨在说明使用自动文本分析可以做什么，而不是对专业知识的心理属性进行严格测试。\n 在这个说明性案例中， H1 到 H6 的关键构念是已知的：专家和非专家、认知表达、情感、产品特征、收益。有一些构念的测量——认知语言和情感语言——可以通过已有的LIWC词典（Pennebaker et al. 2001）获得。然而，剩下的构念，如产品的特征和收益是特定于上下文的，需要专门构建字典。此外，可能还有其他特征将专家与非专家区分开来。因此，本研究的文本分析，我们采用词典法进行构念的测量。\nStage-4: 构念测量 对于此分析，Pennebaker 等人开发的标准 LIWC 词典。 (2001) 除了自定义字典外还使用了。Table3 显示了标准化和自定义词典中使用的类别。标准词典包括诸如“我”之类的人称代词的类别，诸如形容词之类的词类，诸如积极和消极情绪之类的心理测量学预先测试的类别，以及诸如休闲leisure、家庭family和与朋友相关的语言之类的内容相关类别。\n开发一个自定义词典来识别此处产品口碑数据的类别。研究人员不考虑不区分网站来源，从2个网站各选 10 条评论进行开放式编码。然后，从每个网站再选择 10 条评论并添加编码，直到达到饱和（Weber 2005）。总之，开发自定义词典所需的子样本是 60 条评论，每个网站有 30 条，约占所有评论的 11%。创建了十四个类别，每个类别平均包含六个单词。\n 这一步主要是构建与理论构念相适应的词典，通过不同构念的词表测量不同的构念。Table3中Category是不同的构念(Category)，对应的词表是Words，通过Words词语出现次数就能计算(测量)文本中不同构念(Category)。\n 评论的定性分析显示，发帖者倾向于从功能或美学角度谈论产品。因此，为与特征（例如 GPS、相机、硬盘、电池）和美学（例如，锐利、干净、性感、时尚）相关的词创建了字典类别。发帖人还反复关注设备的容量、产品的成本，并报告了他们在使用产品时遇到的问题。为这些问题中的每一个都创建了类别。因为可能有一些研究人员对产品用途的兴趣，并且因为发帖人经常提到娱乐和工作相关用途，所以为每种用途创建了类别。包括“大”与“小”的类别是因为先前的社会学理论表明 iPod 的成功来自其提供的过剩产品——大屏幕、过剩容量等（Sennett 2006）。当提到竞争产品时，创建了两个类别来计算，无论是在 Apple 品牌内部还是外部。\n字典类别由三位编码人员验证，他们建议包含和排除的单词。表 3 中可以找到每个字典类别的编码员之间的百分比一致性。alpha平均一致性为 90%。文本文件通过 LIWC 程序运行，首先使用标准字典，然后使用自定义字典。从三组数据创建了一个电子表格：(1) 直接从网站收集的评论数据（例如，发布日期、产品评级），(2) 标准字典的计算机结果， (3) 自定义词典计算结果。\n  例如测量评论文本的Social processes指标(词频)，实现计算的代码写法有很多种，以下仅为其中一种（代码仅供参考）\n#构建的自定义词典(词表) socialProcessWords= [\u0026#39;mate\u0026#39;, \u0026#39;talk\u0026#39;, \u0026#39;they\u0026#39;, \u0026#39;child\u0026#39;] #待分析的某条评论文本 comment = \u0026#39;Jim and Jam have a new iTouch.They talk the iTouch is wonderful.\u0026#39; #构念测量方法(统计评论文本中自定义词典词语出现的总次数) def calculate_SocialProcess(text): num = 0 words = text.lower().split(\u0026#39; \u0026#39;) for word in words: if word in socialProcessWords: num=num+1 return num #返回运行结果 socialProcessIndex = calculate_SocialProcess(text=comment) print(\u0026#34;social process index is {}\u0026#34;.format(socialProcessIndex)) Run\n2 最后 对这篇案例感兴趣的童鞋可以直接阅读原文, 对**python网络爬虫文本分析**感兴趣的童鞋，也可点击课程介绍。\n 点击上方图片购买课程   参考文献  Christian Homburg, Martin Klarmann, and Arnd Vomberg. 2022. Handbook of Market Research. Cham, Switzerland: Springer. https://search.ebscohost.com/login.aspx?direct=true\u0026amp;db=edsebk\u0026amp;AN=3112347\u0026amp;lang=zh-cn\u0026amp;site=eds-live.\nHumphreys A. (2019) Automated Text Analysis. In: Homburg C., Klarmann M., Vomberg A. (eds) Handbook of Market Research. Springer, Cham. https://doi.org/10.1007/978-3-319-05542-8_26-1\nHumphreys, Ashlee, and Rebecca Jen-Hui Wang. \u0026ldquo;Automated text analysis for consumer research.\u0026rdquo; Journal of Consumer Research 44.6 (2018): 1274-1306.\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/automate_text_analysis_in_market/","summary":"摘要 在过去的二十年里，可供营销研究人员分析的文本数据量呈指数级增长。 然而，书面语言充满了复杂的含义、歧义和细微差别。 营销研究人员如何才能将这种丰富的语言表示转化为可量化的数据以进行统计分析和建模？ 本章介绍了文本分析各种方法。 在涵盖了文本分析的一些基础知识后，总结和探索了在营销研究中的应用，如情感分析、主题建模和研究组织沟通，包括对产品发布的口碑反应的案例研究。\n本文相关资料  Christian Homburg, Martin Klarmann, and Arnd Vomberg. 2022. Handbook of Market Research.\nHumphreys A. (2019) Automated Text Analysis. In: Homburg C., Klarmann M., Vomberg A. (eds) Handbook of Market Research. Springer, Cham. https://doi.org/10.1007/978-3-319-05542-8_26-1\nHumphreys, Ashlee, and Rebecca Jen-Hui Wang. \u0026ldquo;Automated text analysis for consumer research.\u0026rdquo; Journal of Consumer Research 44.6 (2018): 1274-1306.\n 关键词   文本分析Text analysis\n  计算机辅助文本分析computer-assisted text analysis","title":"营销研究中文本分析应用概述(含案例及代码)"},{"content":"\n旧版cntext入口\n中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等\n github地址 https://github.com/hidadeng/cntext pypi地址 https://pypi.org/project/cntext/ 视频课-Python网络爬虫与文本数据分析  功能模块含\n stats 文本统计指标  词频统计 可读性 内置pkl词典 情感分析   dictionary 构建词表(典)  Sopmi 互信息扩充词典法 W2Vmodels 词向量扩充词典法   similarity 文本相似度  cos相似度 jaccard相似度 编辑距离相似度   mind.py 计算文本中的认知方向（态度、偏见）  安装 pip install cntext \nQuickStart import cntext as ct help(ct) Run\nHelp on package cntext: NAME cntext PACKAGE CONTENTS mind dictionary similarity stats \n一、stats 目前stats内置的函数有\n readability 文本可读性 term_freq 词频统计函数 dict_pkl_list 获取cntext内置词典列表(pkl格式) load_pkl_dict 导入pkl词典文件 sentiment 情感分析  import cntext as ct text = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。\u0026#39; ct.term_freq(text, lang=\u0026#39;chinese\u0026#39;) Run\nCounter({\u0026#39;看待\u0026#39;: 1, \u0026#39;网文\u0026#39;: 1, \u0026#39;作者\u0026#39;: 1, \u0026#39;黑客\u0026#39;: 1, \u0026#39;大佬\u0026#39;: 1, \u0026#39;盗号\u0026#39;: 1, \u0026#39;改文因\u0026#39;: 1, \u0026#39;万分\u0026#39;: 1, \u0026#39;惭愧\u0026#39;: 1, \u0026#39;停\u0026#39;: 1}) \n1.1 readability 文本可读性，指标越大，文章复杂度越高，可读性越差。\nreadability(text, lang=\u0026lsquo;chinese\u0026rsquo;)\n text: 文本字符串数据 lang: 语言类型，\u0026ldquo;chinese\u0026quot;或\u0026quot;english\u0026rdquo;，默认\u0026quot;chinese\u0026quot;  **中文可读性 ** 算法参考自\n 徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.\n readability1 \u0026mdash;每个分句中的平均字数 readability2 \u0026mdash;每个句子中副词和连词所占的比例 readability3 \u0026mdash;参考Fog Index， readability3=(readability1+readability2)×0.5   ​\n以上三个指标越大，都说明文本的复杂程度越高，可读性越差。\ntext1 = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。\u0026#39; ct.readability(text1, lang=\u0026#39;chinese\u0026#39;) Run\n{\u0026#39;readability1\u0026#39;: 28.0, \u0026#39;readability2\u0026#39;: 0.15789473684210525, \u0026#39;readability3\u0026#39;: 14.078947368421053} \n句子中的符号变更会影响结果\ntext2 = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。\u0026#39; ct.readability(text2, lang=\u0026#39;chinese\u0026#39;) Run\n{\u0026#39;readability1\u0026#39;: 27.0, \u0026#39;readability2\u0026#39;: 0.16666666666666666, \u0026#39;readability3\u0026#39;: 13.583333333333334} \n1.2 term_freq 词频统计函数，返回Counter类型\nimport cntext as ct text = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。\u0026#39; ct.term_freq(text, lang=\u0026#39;chinese\u0026#39;) Run\nCounter({\u0026#39;看待\u0026#39;: 1, \u0026#39;网文\u0026#39;: 1, \u0026#39;作者\u0026#39;: 1, \u0026#39;黑客\u0026#39;: 1, \u0026#39;大佬\u0026#39;: 1, \u0026#39;盗号\u0026#39;: 1, \u0026#39;改文因\u0026#39;: 1, \u0026#39;万分\u0026#39;: 1, \u0026#39;惭愧\u0026#39;: 1, \u0026#39;停\u0026#39;: 1}) \n1.3 dict_pkl_list 获取cntext内置词典列表(pkl格式)\nimport cntext as ct # 获取cntext内置词典列表(pkl格式) ct.dict_pkl_list() Run\n[\u0026#39;DUTIR.pkl\u0026#39;, \u0026#39;HOWNET.pkl\u0026#39;, \u0026#39;sentiws.pkl\u0026#39;, \u0026#39;ChineseFinancialFormalUnformalSentiment.pkl\u0026#39;, \u0026#39;ANEW.pkl\u0026#39;, \u0026#39;LSD2015.pkl\u0026#39;, \u0026#39;NRC.pkl\u0026#39;, \u0026#39;geninqposneg.pkl\u0026#39;, \u0026#39;HuLiu.pkl\u0026#39;, \u0026#39;AFINN.pkl\u0026#39;, \u0026#39;ADV_CONJ.pkl\u0026#39;, \u0026#39;LoughranMcDonald.pkl\u0026#39;, \u0026#39;STOPWORDS.pkl\u0026#39;] 词典对应关系, 部分情感词典资料整理自 quanteda.sentiment\n   pkl文件 词典 语言 功能     DUTIR.pkl 大连理工大学情感本体库 中文 七大类情绪，哀, 好, 惊, 惧, 乐, 怒, 恶   HOWNET.pkl 知网Hownet词典 中文 正面词、负面词   sentiws.pkl SentimentWortschatz (SentiWS) 英文 正面词、负面词；\n效价   ChineseFinancialFormalUnformalSentiment.pkl 金融领域正式、非正式；积极消极 中文 formal-pos、\nformal-neg；\nunformal-pos、\nunformal-neg   ANEW.pkl 英语单词的情感规范Affective Norms for English Words (ANEW) 英文 词语效价信息   LSD2015.pkl Lexicoder Sentiment Dictionary (2015) 英文 正面词、负面词   NRC.pkl NRC Word-Emotion Association Lexicon 英文 细粒度情绪词；   geninqposneg.pkl      HuLiu.pkl Hu\u0026amp;Liu (2004)正、负情感词典 英文 正面词、负面词   AFINN.pkl 尼尔森 (2011) 的“新 ANEW”效价词表 英文 情感效价信息valence   LoughranMcDonald.pkl 会计金融LM词典 英文 金融领域正、负面情感词   ADV_CONJ.pkl 副词连词 中文    STOPWORDS.pkl  中、英 停用词    注意:   如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”\n  如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue\n  1.4 load_pkl_dict 导入pkl词典文件，返回字典样式数据。\nimport cntext as ct # 导入pkl词典文件, print(ct.load_pkl_dict(\u0026#39;DUTIR.pkl\u0026#39;)) Run\n{\u0026#39;DUTIR\u0026#39;: {\u0026#39;哀\u0026#39;: [\u0026#39;怀想\u0026#39;, \u0026#39;治丝而棼\u0026#39;, ...], \u0026#39;好\u0026#39;: [\u0026#39;进贤黜奸\u0026#39;, \u0026#39;清醇\u0026#39;, \u0026#39;放达\u0026#39;, ...], \u0026#39;惊\u0026#39;: [\u0026#39;惊奇不已\u0026#39;, \u0026#39;魂惊魄惕\u0026#39;, \u0026#39;海外奇谈\u0026#39;,...], \u0026#39;惧\u0026#39;: [\u0026#39;忸忸怩怩\u0026#39;, \u0026#39;谈虎色变\u0026#39;, \u0026#39;手忙脚乱\u0026#39;, \u0026#39;刿目怵心\u0026#39;,...], \u0026#39;乐\u0026#39;: [\u0026#39;百龄眉寿\u0026#39;, \u0026#39;娱心\u0026#39;, \u0026#39;如意\u0026#39;, \u0026#39;喜糖\u0026#39;,...], \u0026#39;怒\u0026#39;: [\u0026#39;饮恨吞声\u0026#39;, \u0026#39;扬眉瞬目\u0026#39;,...], \u0026#39;恶\u0026#39;: [\u0026#39;出逃\u0026#39;, \u0026#39;鱼肉百姓\u0026#39;, \u0026#39;移天易日\u0026#39;,] } \n1.5 sentiment sentiment(text, diction, lang=\u0026lsquo;chinese\u0026rsquo;) 使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，\n text: 待分析中文文本 diction: 情感词字典； lang: 语言类型，\u0026ldquo;chinese\u0026quot;或\u0026quot;english\u0026rdquo;，默认\u0026quot;chinese\u0026quot;  import cntext as ct text = \u0026#39;我今天得奖了，很高兴，我要将快乐分享大家。\u0026#39; ct.sentiment(text=text, diction=ct.load_pkl_dict(\u0026#39;DUTIR.pkl\u0026#39;)[\u0026#39;DUTIR\u0026#39;], lang=\u0026#39;chinese\u0026#39;) Run\n{\u0026#39;哀_num\u0026#39;: 0, \u0026#39;好_num\u0026#39;: 0, \u0026#39;惊_num\u0026#39;: 0, \u0026#39;惧_num\u0026#39;: 0, \u0026#39;乐_num\u0026#39;: 2, \u0026#39;怒_num\u0026#39;: 0, \u0026#39;恶_num\u0026#39;: 0, \u0026#39;stopword_num\u0026#39;: 8, \u0026#39;word_num\u0026#39;: 14, \u0026#39;sentence_num\u0026#39;: 1} 如果不适用pkl词典，可以自定义自己的词典，例如\ndiction = {\u0026#39;pos\u0026#39;: [\u0026#39;高兴\u0026#39;, \u0026#39;快乐\u0026#39;, \u0026#39;分享\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;难过\u0026#39;, \u0026#39;悲伤\u0026#39;], \u0026#39;adv\u0026#39;: [\u0026#39;很\u0026#39;, \u0026#39;特别\u0026#39;]} text = \u0026#39;我今天得奖了，很高兴，我要将快乐分享大家。\u0026#39; ct.sentiment(text=text, diction=diction, lang=\u0026#39;chinese\u0026#39;) Run\n{\u0026#39;pos_num\u0026#39;: 3, \u0026#39;neg_num\u0026#39;: 0, \u0026#39;adv_num\u0026#39;: 1, \u0026#39;stopword_num\u0026#39;: 8, \u0026#39;word_num\u0026#39;: 14, \u0026#39;sentence_num\u0026#39;: 1} \n二、dictionary 本模块用于构建词表(典),含\n SoPmi 共现法扩充词表(典) W2VModels 词向量word2vec扩充词表(典)  2.1 SoPmi 共现法 import cntext as ct import os sopmier = ct.SoPmi(cwd=os.getcwd(), input_txt_file=\u0026#39;data/sopmi_corpus.txt\u0026#39;, #原始数据，您的语料 seedword_txt_file=\u0026#39;data/sopmi_seed_words.txt\u0026#39;, #人工标注的初始种子词 ) sopmier.sopmi() Run\nStep 1/4:...Preprocess Corpus ... Step 2/4:...Collect co-occurrency information ... Step 3/4:...Calculate mutual information ... Step 4/4:...Save candidate words ... Finish! used 44.49 s \n2.2 W2VModels 词向量 特别要注意代码需要设定lang语言参数\nimport cntext as ct import os #初始化模型,需要设置lang参数。 model = ct.W2VModels(cwd=os.getcwd(), lang=\u0026#39;english\u0026#39;) #语料数据 w2v_corpus.txt model.train(input_txt_file=\u0026#39;data/w2v_corpus.txt\u0026#39;) #根据种子词，筛选出没类词最相近的前100个词 model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/integrity.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/innovation.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/quality.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/respect.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/teamwork.txt\u0026#39;, topn=100) Run\nStep 1/4:...Preprocess corpus ... Step 2/4:...Train word2vec model used 174 s Step 3/4:...Prepare similar candidates for each seed word in the word2vec model... Step 4/4 Finish! Used 187 s Step 3/4:...Prepare similar candidates for each seed word in the word2vec model... Step 4/4 Finish! Used 187 s Step 3/4:...Prepare similar candidates for each seed word in the word2vec model... Step 4/4 Finish! Used 187 s Step 3/4:...Prepare similar candidates for each seed word in the word2vec model... Step 4/4 Finish! Used 187 s Step 3/4:...Prepare similar candidates for each seed word in the word2vec model... Step 4/4 Finish! Used 187 s \n需要注意 训练出的w2v模型可以后续中使用。\nfrom gensim.models import KeyedVectors w2v_model = KeyedVectors.load(w2v.model路径) #找出word的词向量 #w2v_model.get_vector(word) #更多w2_model方法查看 #help(w2_model) 例如本代码，运行生成的结果路径output/w2v_candi_words/w2v.model\nfrom gensim.models import KeyedVectors w2v_model = KeyedVectors.load(\u0026#39;output/w2v_candi_words/w2v.model\u0026#39;) w2v_model.most_similar(\u0026#39;innovation\u0026#39;) Run\n[(\u0026#39;technology\u0026#39;, 0.689210832118988), (\u0026#39;infrastructure\u0026#39;, 0.669672966003418), (\u0026#39;resources\u0026#39;, 0.6695448160171509), (\u0026#39;talent\u0026#39;, 0.6627111434936523), (\u0026#39;execution\u0026#39;, 0.6549549102783203), (\u0026#39;marketing\u0026#39;, 0.6533523797988892), (\u0026#39;merchandising\u0026#39;, 0.6504817008972168), (\u0026#39;diversification\u0026#39;, 0.6479553580284119), (\u0026#39;expertise\u0026#39;, 0.6446896195411682), (\u0026#39;digital\u0026#39;, 0.6326863765716553)] \n#获取词向量 w2v_model.get_vector(\u0026#39;innovation\u0026#39;) Run\narray([-0.45616838, -0.7799563 , 0.56367606, -0.8570078 , 0.600359 , -0.6588043 , 0.31116748, -0.11956959, -0.47599426, 0.21840936, -0.02268819, 0.1832016 , 0.24452794, 0.01084935, -1.4213187 , 0.22840202, 0.46387577, 1.198386 , -0.621511 , -0.51598716, 0.13352732, 0.04140598, -0.23470387, 0.6402956 , 0.20394802, 0.10799981, 0.24908689, -1.0117126 , -2.3168423 , -0.0402851 , 1.6886286 , 0.5357047 , 0.22932841, -0.6094084 , 0.4515793 , -0.5900931 , 1.8684244 , -0.21056202, 0.29313338, -0.221067 , -0.9535679 , 0.07325 , -0.15823542, 1.1477109 , 0.6716076 , -1.0096023 , 0.10605699, 1.4148282 , 0.24576302, 0.5740349 , 0.19984631, 0.53964925, 0.41962907, 0.41497853, -1.0322098 , 0.01090925, 0.54345983, 0.806317 , 0.31737605, -0.7965337 , 0.9282971 , -0.8775608 , -0.26852605, -0.06743863, 0.42815775, -0.11774074, -0.17956367, 0.88813037, -0.46279573, -1.0841943 , -0.06798118, 0.4493006 , 0.71962464, -0.02876493, 1.0282255 , -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922, -0.16753489, 0.14065655, -1.8609751 , 0.03587054, 1.2792674 , 1.2732009 , -0.74120265, -0.98000383, 0.4521185 , -0.26387128, 0.37045383, 0.3680011 , 0.7197629 , -0.3570571 , 0.8016917 , 0.39243212, -0.5027844 , -1.2106236 , 0.6412354 , -0.878307 ], dtype=float32) \n2.3 co_occurrence_matrix 词共现矩阵\nimport cntext as ct documents = [\u0026#34;I go to school every day by bus .\u0026#34;, \u0026#34;i go to theatre every night by bus\u0026#34;] ct.co_occurrence_matrix(documents, window_size=2, lang=\u0026#39;english\u0026#39;) documents2 = [\u0026#34;编程很好玩\u0026#34;, \u0026#34;Python是最好学的编程\u0026#34;] ct.co_occurrence_matrix(documents2, window_size=2, lang=\u0026#39;chinese\u0026#39;) \n三、similarity 四种相似度计算函数\n cosine_sim(text1, text2) cos余弦相似 jaccard_sim(text1, text2) jaccard相似 minedit_sim(text1, text2) 最小编辑距离相似度； simple_sim(text1, text2) 更改变动算法  算法实现参考自 Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.\nimport cntext as ct text1 = \u0026#39;编程真好玩编程真好玩\u0026#39; text2 = \u0026#39;游戏真好玩编程真好玩啊\u0026#39; print(ct.cosine_sim(text1, text2)) print(ct.jaccard_sim(text1, text2)) print(ct.minedit_sim(text1, text2)) print(ct.simple_sim(text1, text2)) Run\n0.82 0.67 2.00 0.87 \n四、Text2Mind 词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。\n- 多个对象在某概念的远近，职业与性别，某个职业是否存在亲近男性，而排斥女性\n- 多个对象在某概念的分量(fen，一声)的多少， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆\n这两种认知分别可以用向量距离、向量语义投影计算得来。\n tm.sematic_distance(words, c_words1, c_words2) 向量距离 tm.sematic_projection(words, c_words1, c_words2) 向量语义投影  4.1 tm.sematic_distance(words, c_words1, c_words2) 分别计算words与c_words1、c_words2语义距离，返回距离差值。\n例如\nmale_concept = [\u0026#39;male\u0026#39;, \u0026#39;man\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;him\u0026#39;] female_concept = [\u0026#39;female\u0026#39;, \u0026#39;woman\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;her\u0026#39;] software_engineer_concept = [\u0026#39;engineer\u0026#39;, \u0026#39;programming\u0026#39;, \u0026#39;software\u0026#39;] d1 = distance(male_concept, software_engineer_concept) d2 = distance(female_concept, software_engineer_concept) 如果d1-d2\u0026lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。\n换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。\n下载glove_w2v.6B.100d.txt链接: https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw 提取码: 72l0\nimport cntext as ct #Note: this is a word2vec format model tm = ct.Text2Mind(w2v_model_path=\u0026#39;glove_w2v.6B.100d.txt\u0026#39;) engineer = [\u0026#39;program\u0026#39;, \u0026#39;software\u0026#39;, \u0026#39;computer\u0026#39;] mans = [\u0026#34;man\u0026#34;, \u0026#34;he\u0026#34;, \u0026#34;him\u0026#34;] womans = [\u0026#34;woman\u0026#34;, \u0026#34;she\u0026#34;, \u0026#34;her\u0026#34;] #在语义空间中，工程师更接近于男人，而不是女人。 #in semantic space, engineer is closer to man, other than woman. tm.sematic_distance(words=animals, c_words1=mans, c_words2=womans) Run\n-0.38 \n4.2 tm.sematic_projection(words, c_words1, c_words2) 语义投影，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。\n分值越大，word越位于c_words2一侧。\n下图是语义投影示例图，本文算法和图片均来自 \u0026ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Nature Human Behaviour, pp.1-13.\u0026rdquo;\n例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。\n以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。\nanimals = [\u0026#39;mouse\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;horse\u0026#39;, \u0026#39;pig\u0026#39;, \u0026#39;whale\u0026#39;] smalls = [\u0026#34;small\u0026#34;, \u0026#34;little\u0026#34;, \u0026#34;tiny\u0026#34;] bigs = [\u0026#34;large\u0026#34;, \u0026#34;big\u0026#34;, \u0026#34;huge\u0026#34;] # In size conception, mouse is smallest, horse is biggest. # 在大小概念上，老鼠最小，马是最大的。 tm.sematic_projection(words=animals, c_words1=smalls, c_words2=bigs) Run\n[(\u0026#39;mouse\u0026#39;, -1.68), (\u0026#39;cat\u0026#39;, -0.92), (\u0026#39;pig\u0026#39;, -0.46), (\u0026#39;whale\u0026#39;, -0.24), (\u0026#39;horse\u0026#39;, 0.4)] \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/cntext_simplification/","summary":"旧版cntext入口\n中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等\n github地址 https://github.com/hidadeng/cntext pypi地址 https://pypi.org/project/cntext/ 视频课-Python网络爬虫与文本数据分析  功能模块含\n stats 文本统计指标  词频统计 可读性 内置pkl词典 情感分析   dictionary 构建词表(典)  Sopmi 互信息扩充词典法 W2Vmodels 词向量扩充词典法   similarity 文本相似度  cos相似度 jaccard相似度 编辑距离相似度   mind.py 计算文本中的认知方向（态度、偏见）  安装 pip install cntext \nQuickStart import cntext as ct help(ct) Run\nHelp on package cntext: NAME cntext PACKAGE CONTENTS mind dictionary similarity stats \n一、stats 目前stats内置的函数有\n readability 文本可读性 term_freq 词频统计函数 dict_pkl_list 获取cntext内置词典列表(pkl格式) load_pkl_dict 导入pkl词典文件 sentiment 情感分析  import cntext as ct text = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。\u0026#39; ct.","title":"cntext库 |  Python文本分析包更新"},{"content":"在科学研究中，数据的获取及分析是最重要的也是最棘手的三个环节！\n在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的三个问题：\n Python语法 解决 如何通过电脑自动化帮我们做事情？ 网络爬虫技术 解决 如何从网络世界中高效地 采集数据？ 文本分析技术 解决 如何从杂乱的文本数据中抽取文本信息(变量)？  使用说明 目前Python语法入门部分已免费开放，最近将相关教程代码视频整合到大邓的博客中，\n博客地址 https://hidadeng.github.io/\n点击Python语法入门进入语法学习界面，一共有15篇基础知识。大家按照顺序，从上往下学习即可。\n例如，点击Python是一种语言, 可以看到页面中\n 有B站视频 有课件下载地址 还有文档教程  此外，为了方便大家学习交流，博客开通了留言评论功能(有github账号的童靴才可以留言。)。建议大家注册个github账号(https://github.com/)，不止为了留言，更主要的是可以在github中找到自己需要的代码。\n\n教程纲要  课程目标： 掌握Python语法 核心知识点： 数据类型、逻辑语句、常用内置函数、常用内置库 环境配置: 安装anaconda，注意安装过程中勾选Add Anaconda to the system Path environment variable、Register Anaconda as the system python 3.x 课件资料： 本课程全部使用jupyter notebook文件作为课程课件，已全部上传至github中。  \nPython语法入门  Python跟英语一样是一门语言 环境配置  Mac配置 Win配置   数据类型-字符串 数据类型-列表元组集合 数据类型-字典 数据类型-布尔值、None 逻辑语句(if\u0026amp;for\u0026amp;tryexcept) 高级语法-列表推导式 高级语法-理解函数 常用内置函数 pathlib路径库 内置库csv文件库 正则表达式re库 常见错误汇总  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/dadeng_python_basic_tutorial/","summary":"在科学研究中，数据的获取及分析是最重要的也是最棘手的三个环节！\n在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的三个问题：\n Python语法 解决 如何通过电脑自动化帮我们做事情？ 网络爬虫技术 解决 如何从网络世界中高效地 采集数据？ 文本分析技术 解决 如何从杂乱的文本数据中抽取文本信息(变量)？  使用说明 目前Python语法入门部分已免费开放，最近将相关教程代码视频整合到大邓的博客中，\n博客地址 https://hidadeng.github.io/\n点击Python语法入门进入语法学习界面，一共有15篇基础知识。大家按照顺序，从上往下学习即可。\n例如，点击Python是一种语言, 可以看到页面中\n 有B站视频 有课件下载地址 还有文档教程  此外，为了方便大家学习交流，博客开通了留言评论功能(有github账号的童靴才可以留言。)。建议大家注册个github账号(https://github.com/)，不止为了留言，更主要的是可以在github中找到自己需要的代码。\n\n教程纲要  课程目标： 掌握Python语法 核心知识点： 数据类型、逻辑语句、常用内置函数、常用内置库 环境配置: 安装anaconda，注意安装过程中勾选Add Anaconda to the system Path environment variable、Register Anaconda as the system python 3.x 课件资料： 本课程全部使用jupyter notebook文件作为课程课件，已全部上传至github中。  \nPython语法入门  Python跟英语一样是一门语言 环境配置  Mac配置 Win配置   数据类型-字符串 数据类型-列表元组集合 数据类型-字典 数据类型-布尔值、None 逻辑语句(if\u0026amp;for\u0026amp;tryexcept) 高级语法-列表推导式 高级语法-理解函数 常用内置函数 pathlib路径库 内置库csv文件库 正则表达式re库 常见错误汇总  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"置顶推荐 | Python语法入门(含视频)"},{"content":" 这个周期表生成器的想法是创建一个关于 自然语言处理任务周期表 的博客。 在 Streamlit 的帮助下并受到此 Bokeh 图库示例 的启发，它成为了一个动态的创建者，可以根据您的元素周期表进行定制 ！\n大邓的作用仅仅是将其汉化，没有做布局上的新创新。\n下载 点击下载periodic-table-creator项目压缩文件夹\n下面是自定义的任务周期表示例。 在这种情况下：对于自然语言处理任务！\n安装\u0026amp;运行 打开命令行(终端),\npip3 install streamlit==1.8.1 pip3 install bokeh==2.4.1 #切换至项目文件夹periodic-table-creator cd periodic-table-creator #运行streamlit streamlit run periodic_table_creator.py 此时在命令行中会出现\n点击Local URL对应的链接，或者将该链接复制粘贴到浏览器，即可访问。\n导入数据 创建自己特有的任务周期表需要导入自由的csv数据。可以[点击这里] (periodic-table-creator/periodic_nlp.csv) 下载并查看数据格式，或者运行案例时， 点击按钮\u0026quot;Edit CSV text\u0026quot; 查看示例数据格式。\n 表的字符名(列名): atomicnumber;group;period;symbol;elementname;groupname;color;url;excerpt csv文件的分隔符使用英文格式下的 ;或, csv文件使用utf-8编码   点击上方图片购买课程   了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/periodic-table-creator/","summary":"这个周期表生成器的想法是创建一个关于 自然语言处理任务周期表 的博客。 在 Streamlit 的帮助下并受到此 Bokeh 图库示例 的启发，它成为了一个动态的创建者，可以根据您的元素周期表进行定制 ！\n大邓的作用仅仅是将其汉化，没有做布局上的新创新。\n下载 点击下载periodic-table-creator项目压缩文件夹\n下面是自定义的任务周期表示例。 在这种情况下：对于自然语言处理任务！\n安装\u0026amp;运行 打开命令行(终端),\npip3 install streamlit==1.8.1 pip3 install bokeh==2.4.1 #切换至项目文件夹periodic-table-creator cd periodic-table-creator #运行streamlit streamlit run periodic_table_creator.py 此时在命令行中会出现\n点击Local URL对应的链接，或者将该链接复制粘贴到浏览器，即可访问。\n导入数据 创建自己特有的任务周期表需要导入自由的csv数据。可以[点击这里] (periodic-table-creator/periodic_nlp.csv) 下载并查看数据格式，或者运行案例时， 点击按钮\u0026quot;Edit CSV text\u0026quot; 查看示例数据格式。\n 表的字符名(列名): atomicnumber;group;period;symbol;elementname;groupname;color;url;excerpt csv文件的分隔符使用英文格式下的 ;或, csv文件使用utf-8编码   点击上方图片购买课程   了解课程  点击上方图片购买课程   点击进入详情页","title":"streamlit|任务周期表生成器"},{"content":"doccano doccano是开源的数据标注工具，可以简化数据标注的难度。需要注意，市面上的机器学习课程一般都默认数据已标注，在此基础上讲机器学习。\n您还可以将 doccano 与您的脚本集成，因为它将功能公开为API。 doccano API是在局域网内的网址链接，多台设备可打开浏览、标注。\n一、标记流程 通过以下步骤开始和完成使用 doccano 的标签项目：\n 安装doccano。 运行doccano。 设置标签项目。 选择标签项目的类型， 配置项目设置。 导入数据集。 您还可以导入带标签的数据集。 给项目添加标注人员 给标志者定义标注工作文档说明 开始标记数据。 导出标记的数据集。  二、配置环境 2.1 安装 打开命令行（cmd、terminal）执行安装命令\npip3 install doccano \n2.2 运行doccano 在命令行（cmd、terminal）内依次执行\n#在电脑第一次运行的时候初始化doccano #只需设置一次，之后不用再运行该命令 doccano init #创建用户名及密码；例如现在有一个主管admin，两个标注员tom和jack #设置好用户，之后不用再运行该命令 doccano createuser --username admin --password pass doccano createuser --username tom --password pass doccano createuser --username jack --password pass #开启doccano服务 doccano webserver 完成上述操作后，另打开一个新的命令行，执行下列命令\ndoccano task \n三、案例 下面我们以外卖评论数据为例，对评论进行判断，标注为正、负面情感。点击下载data.csv\n3.1 创建项目 先登录用户名和密码，这里的admin是超级用户(权限最大)\n为创建项目，如图点击Create按钮。 根据需要选择合适的项目类别,这里选择的Text Classification，\n填写项目信息，例如项目名情感标注，简介、标注类型\n根据需要选择项目的功能需求，例如允许单标签，把数据打乱随机显示， 用户之间共享标注\n3.2 上传数据 创建项目后，点击Dataset按钮，点击Import dataset导入数据。我这里准备的是csv文件，拥有review和label两个字段。\n3.3 定义标签 点击左侧菜单中的“Labels”按钮来定义我们的标签。 我们应该看到标签编辑器页面。 在标签编辑器页面中，您可以通过指定标签文本、快捷键、背景颜色和文本颜色来创建标签。\n同理，可以定义负面neg标签。现在有了pos和neg两个标签。\n3.4 添加成员 点击左侧目录中的 Members 按钮，\n然后，选择“Add”按钮以显示表单。 使用您要添加到项目中的用户名和角色填写此表单。 然后，选择“Save”按钮。\n如果没有可供选择的成员，记得创建成员。形如doccano createuser --username tom --password pass\n3.5 开始标注 接下来，我们准备标注文本数据。 只需点击导航栏中的“Start annotation”按钮，我们就可以开始对文档进行批注了。\n3.6 导出数据 在注释步骤之后，我们可以下载标注后的数据。 转到“Dataset”页面，然后单击“操作”菜单中的“Export dataset”按钮。 选择导出格式后，单击“Export”。 您应该看到以下屏幕：\n到出的结果如下\nimport pandas as pd df = pd.read_csv(\u0026#39;all.csv\u0026#39;) df 3.7 导出数据 对了，当标注过程不同阶段，还看查看标注工作量等可视化信息\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/doccano_text_anotation/","summary":"doccano doccano是开源的数据标注工具，可以简化数据标注的难度。需要注意，市面上的机器学习课程一般都默认数据已标注，在此基础上讲机器学习。\n您还可以将 doccano 与您的脚本集成，因为它将功能公开为API。 doccano API是在局域网内的网址链接，多台设备可打开浏览、标注。\n一、标记流程 通过以下步骤开始和完成使用 doccano 的标签项目：\n 安装doccano。 运行doccano。 设置标签项目。 选择标签项目的类型， 配置项目设置。 导入数据集。 您还可以导入带标签的数据集。 给项目添加标注人员 给标志者定义标注工作文档说明 开始标记数据。 导出标记的数据集。  二、配置环境 2.1 安装 打开命令行（cmd、terminal）执行安装命令\npip3 install doccano \n2.2 运行doccano 在命令行（cmd、terminal）内依次执行\n#在电脑第一次运行的时候初始化doccano #只需设置一次，之后不用再运行该命令 doccano init #创建用户名及密码；例如现在有一个主管admin，两个标注员tom和jack #设置好用户，之后不用再运行该命令 doccano createuser --username admin --password pass doccano createuser --username tom --password pass doccano createuser --username jack --password pass #开启doccano服务 doccano webserver 完成上述操作后，另打开一个新的命令行，执行下列命令\ndoccano task \n三、案例 下面我们以外卖评论数据为例，对评论进行判断，标注为正、负面情感。点击下载data.csv\n3.1 创建项目 先登录用户名和密码，这里的admin是超级用户(权限最大)","title":"doccano|为机器学习建模做数据标注"},{"content":"说明 这个案例使用Streamlit 和 TimelineJS 制作， 通过时间线方式展示自然语言处理领域发展。大家可以将其改造为公司发展时间线、人生里程碑时间线等等。\n下载\u0026amp;运行  点击此处下载代码 https://hidadeng.github.io/blog/nlp-history-timeline/nlp-history-timeline.zip 解压nlp-history-timeline.zip文件夹，放置于桌面 打开命令行(终端)  #切换到nlp-history-timeline cd nlp-history-timeline #安装需要的包 pip3 install -r requirements.txt #运行timeline_app.py streamlit run timeline_app.py \n使用方式  点击打开案例网站  下面是网页的一个时间点\n自然语言处理发展时间线   你也可以浏览的原始数据文件，格式为json。下图可以看到json的是由\n 36个字典，即36个时间点 每个字典中有很多字段，如url图片链接、headline、year年份、text简介等。  时间线案例数据   \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/nlp-history-timeline/","summary":"说明 这个案例使用Streamlit 和 TimelineJS 制作， 通过时间线方式展示自然语言处理领域发展。大家可以将其改造为公司发展时间线、人生里程碑时间线等等。\n下载\u0026amp;运行  点击此处下载代码 https://hidadeng.github.io/blog/nlp-history-timeline/nlp-history-timeline.zip 解压nlp-history-timeline.zip文件夹，放置于桌面 打开命令行(终端)  #切换到nlp-history-timeline cd nlp-history-timeline #安装需要的包 pip3 install -r requirements.txt #运行timeline_app.py streamlit run timeline_app.py \n使用方式  点击打开案例网站  下面是网页的一个时间点\n自然语言处理发展时间线   你也可以浏览的原始数据文件，格式为json。下图可以看到json的是由\n 36个字典，即36个时间点 每个字典中有很多字段，如url图片链接、headline、year年份、text简介等。  时间线案例数据   \n了解课程  点击上方图片购买课程   点击进入详情页","title":"使用streamlit制作自然语言处理发展时间线页面"},{"content":"  作者：蘇宇暉（台科大管研所博士候選人）、羅凱揚（台科大企管系博士） 日期: 2020-12-14 绘图：彭煖蘋 出处: medium   “穿衣服是大学问”，不相信可以问问很多女生，每天出门前会不会为了今天要穿什麽衣服而伤脑筋？现在如果有一家公司帮您聘请一个专属的“穿衣顾问”，只在一开始收费120多元（二十块美金）的造型费，然后定期寄来已经帮您量身打造的时尚服饰，您愿意买单吗？不瞒您说，目前全世球已有三百五十万人接受美国一家叫做“Stitch Fix”的穿衣时尚订阅公司的服务。\n 点击浏览Stitch Fix网站   2011年6月，刚从哈佛大学商学院毕业的美日混血儿卡翠娜‧雷克（Katrina Lake），在美国旧金山成立时尚电商公司Stitch Fix。满脑子有趣想法的雷克，透过募集到的五十万美金，开始了她的创业之旅。短短不到七年的时间，到了2017年11月，Stitch Fix在美国Nasdaq上市。而卡翠娜‧雷克本人也成为2019年《福布斯》全美白手起家女富豪排行榜中的55名。\nStitch Fix的背后，其实是一家充分利用营销研究和营销数据科学，同时提供“穿衣时尚订阅”服务的新创公司。现在让我们来看看，Stitch Fix如何运作，如图-1所示。\n 图-1 StitchFix运作   消费者在登录Stitch Fix的网站首页时，不会看到像其他购物网站会有太多的商品展示，反而是介绍穿衣风格才是重点。而网站会有造型师来塑造消费者的风格，并且透过这种新的购物方式力邀消费者加入会员。因此，当消费者在Stitch Fix的网站注册时，Stitch Fix会请会员填答一份详细的问卷，包括顾客的基本资料、身高、尺码、喜欢的颜色、风格、经常出席的场合、甚至是预算等。\n接著，Stitch Fix每个月就会透过一个称为“订购盒子（Subscription Box）”的包裹，一次将五件服饰寄送给顾客。等到消费者收到包裹时，可以留下觉得满意的服饰，看不上眼或者不满意的服饰就再寄回给Stitch Fix。如果消费者将服饰全部留下，就会享受到折扣，反之，如果一件都不想买，就负担二十美元的包裹服务费。\n在美国，消费者要买衣服，往往得开车到购物中心或百货公司，买个两三件衣服总要花上半天时间。Stitch Fix一次寄来五件衣服（连同一张纸本问卷），其实也经过精算，因为如果一次寄太多件，消费者心理和预算上都难以承受。而Stitch Fix透过消费者所填答的电脑和纸本问卷，以及购买与退换货记录，利用机器学习算法对消费者喜好与需求进行预测，并结合设计师的搭配，给消费者定制化的建议。因为喜欢的衣服被留下，不喜欢的退回，Stitch Fix就很容易利用这些大量数据建立起消费者穿衣风格的“模型”。\n而为了进一步收集到更精准的数据，2017年，Stitch Fix推出了一款Style Shuffle的小游戏，让顾客针对不同的服饰或配件，简单回应喜爱或是不喜爱。借此更进一步收集消费者的偏好，并增加消费者的粘性。Stitch Fix后来并将触角伸向男性服饰以及儿童服饰。而大尺码的女性服饰更是其服务重点。\n通过收集大量消费者用户数据，以及不断优化的模型算法，并结合个人造型师和机器学习（AI）进行个性化推荐，让Stitch Fix的时尚订阅制服务，能够更精准地预测与满足消费者偏好的服饰及配件。据了解，截至2019年，该公司拥有8,000名员工，其中包括5,100名造型师和100多名数据科学家。\n从以上Stitch Fix的故事中，我们看到了营销研究与数据科学的完美搭配。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/stitchfix/","summary":"  作者：蘇宇暉（台科大管研所博士候選人）、羅凱揚（台科大企管系博士） 日期: 2020-12-14 绘图：彭煖蘋 出处: medium   “穿衣服是大学问”，不相信可以问问很多女生，每天出门前会不会为了今天要穿什麽衣服而伤脑筋？现在如果有一家公司帮您聘请一个专属的“穿衣顾问”，只在一开始收费120多元（二十块美金）的造型费，然后定期寄来已经帮您量身打造的时尚服饰，您愿意买单吗？不瞒您说，目前全世球已有三百五十万人接受美国一家叫做“Stitch Fix”的穿衣时尚订阅公司的服务。\n 点击浏览Stitch Fix网站   2011年6月，刚从哈佛大学商学院毕业的美日混血儿卡翠娜‧雷克（Katrina Lake），在美国旧金山成立时尚电商公司Stitch Fix。满脑子有趣想法的雷克，透过募集到的五十万美金，开始了她的创业之旅。短短不到七年的时间，到了2017年11月，Stitch Fix在美国Nasdaq上市。而卡翠娜‧雷克本人也成为2019年《福布斯》全美白手起家女富豪排行榜中的55名。\nStitch Fix的背后，其实是一家充分利用营销研究和营销数据科学，同时提供“穿衣时尚订阅”服务的新创公司。现在让我们来看看，Stitch Fix如何运作，如图-1所示。\n 图-1 StitchFix运作   消费者在登录Stitch Fix的网站首页时，不会看到像其他购物网站会有太多的商品展示，反而是介绍穿衣风格才是重点。而网站会有造型师来塑造消费者的风格，并且透过这种新的购物方式力邀消费者加入会员。因此，当消费者在Stitch Fix的网站注册时，Stitch Fix会请会员填答一份详细的问卷，包括顾客的基本资料、身高、尺码、喜欢的颜色、风格、经常出席的场合、甚至是预算等。\n接著，Stitch Fix每个月就会透过一个称为“订购盒子（Subscription Box）”的包裹，一次将五件服饰寄送给顾客。等到消费者收到包裹时，可以留下觉得满意的服饰，看不上眼或者不满意的服饰就再寄回给Stitch Fix。如果消费者将服饰全部留下，就会享受到折扣，反之，如果一件都不想买，就负担二十美元的包裹服务费。\n在美国，消费者要买衣服，往往得开车到购物中心或百货公司，买个两三件衣服总要花上半天时间。Stitch Fix一次寄来五件衣服（连同一张纸本问卷），其实也经过精算，因为如果一次寄太多件，消费者心理和预算上都难以承受。而Stitch Fix透过消费者所填答的电脑和纸本问卷，以及购买与退换货记录，利用机器学习算法对消费者喜好与需求进行预测，并结合设计师的搭配，给消费者定制化的建议。因为喜欢的衣服被留下，不喜欢的退回，Stitch Fix就很容易利用这些大量数据建立起消费者穿衣风格的“模型”。\n而为了进一步收集到更精准的数据，2017年，Stitch Fix推出了一款Style Shuffle的小游戏，让顾客针对不同的服饰或配件，简单回应喜爱或是不喜爱。借此更进一步收集消费者的偏好，并增加消费者的粘性。Stitch Fix后来并将触角伸向男性服饰以及儿童服饰。而大尺码的女性服饰更是其服务重点。\n通过收集大量消费者用户数据，以及不断优化的模型算法，并结合个人造型师和机器学习（AI）进行个性化推荐，让Stitch Fix的时尚订阅制服务，能够更精准地预测与满足消费者偏好的服饰及配件。据了解，截至2019年，该公司拥有8,000名员工，其中包括5,100名造型师和100多名数据科学家。\n从以上Stitch Fix的故事中，我们看到了营销研究与数据科学的完美搭配。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"营销科技 | 今天出门穿什么？时尚电商Stitch Fix"},{"content":" 点击   使用altair绘制漂亮的股票价格趋势图, 直接上代码。\n安装 !pip3 install altair \n案例数据 这里使用vega_datasets中提供的数据做测试，返回的结果是dataframe类型的数据。\nfrom vega_datasets import data def get_data(): source = data.stocks() source = source[source.date.gt(\u0026#34;2004-01-01\u0026#34;)] return source get_data()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  symbol date price     49 MSFT 2004-02-01 21.77   50 MSFT 2004-03-01 20.46   51 MSFT 2004-04-01 21.45   52 MSFT 2004-05-01 21.53   53 MSFT 2004-06-01 23.44   ... ... ... ...   555 AAPL 2009-11-01 199.91   556 AAPL 2009-12-01 210.73   557 AAPL 2010-01-01 192.06   558 AAPL 2010-02-01 204.62   559 AAPL 2010-03-01 223.02    364 rows × 3 columns\n 完整代码 import altair as alt import pandas as pd from vega_datasets import data from vega_datasets import data def get_data(): source = data.stocks() source = source[source.date.gt(\u0026#34;2004-01-01\u0026#34;)] return source def get_chart(df): hover = alt.selection_single( fields=[\u0026#34;date\u0026#34;], nearest=True, on=\u0026#34;mouseover\u0026#34;, empty=\u0026#34;none\u0026#34;, ) lines = ( alt.Chart(df, title=\u0026#34;股票价格趋势\u0026#34;) .mark_line() .encode( x=\u0026#34;date\u0026#34;, y=\u0026#34;price\u0026#34;, color=\u0026#34;symbol\u0026#34;, # strokeDash=\u0026#34;symbol\u0026#34;, ) ) # Draw points on the line, and highlight based on selection points = lines.transform_filter(hover).mark_circle(size=65) # Draw a rule at the location of the selection tooltips = ( alt.Chart(df) .mark_rule() .encode( x=\u0026#34;yearmonthdate(date)\u0026#34;, y=\u0026#34;price\u0026#34;, opacity=alt.condition(hover, alt.value(0.3), alt.value(0)), tooltip=[ alt.Tooltip(\u0026#34;date\u0026#34;, title=\u0026#34;日期\u0026#34;), alt.Tooltip(\u0026#34;price\u0026#34;, title=\u0026#34;价格 (USD)\u0026#34;), ], ) .add_selection(hover) ) return (lines + points + tooltips).interactive() # Original time series chart. Omitted `get_chart` for clarity chart = get_chart(get_data()) # Input annotations ANNOTATIONS = [ (\u0026#34;Mar 01, 2008\u0026#34;, \u0026#34;Pretty good day for GOOG\u0026#34;), (\u0026#34;Dec 01, 2007\u0026#34;, \u0026#34;Something\u0026#39;s going wrong for GOOG \u0026amp; AAPL\u0026#34;), (\u0026#34;Nov 01, 2008\u0026#34;, \u0026#34;Market starts again thanks to...\u0026#34;), (\u0026#34;Dec 01, 2009\u0026#34;, \u0026#34;Small crash for GOOG after...\u0026#34;), ] # Create a chart with annotations annotations_df = pd.DataFrame(ANNOTATIONS, columns=[\u0026#34;date\u0026#34;, \u0026#34;event\u0026#34;]) annotations_df.date = pd.to_datetime(annotations_df.date) annotations_df[\u0026#34;y\u0026#34;] = 0 annotation_layer = ( alt.Chart(annotations_df) .mark_text(size=15, text=ticker, dx=ticker_dx, dy=ticker_dy, align=\u0026#34;center\u0026#34;) .encode( x=\u0026#34;date:T\u0026#34;, y=alt.Y(\u0026#34;y:Q\u0026#34;), tooltip=[\u0026#34;event\u0026#34;], ) .interactive() ) (chart + annotation_layer).interactive() Run\n代码下载 出处 https://share.streamlit.io/streamlit/example-app-time-series-annotation/main\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/time-series-annotation/","summary":"点击   使用altair绘制漂亮的股票价格趋势图, 直接上代码。\n安装 !pip3 install altair \n案例数据 这里使用vega_datasets中提供的数据做测试，返回的结果是dataframe类型的数据。\nfrom vega_datasets import data def get_data(): source = data.stocks() source = source[source.date.gt(\u0026#34;2004-01-01\u0026#34;)] return source get_data()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  symbol date price     49 MSFT 2004-02-01 21.77   50 MSFT 2004-03-01 20.","title":"使用altair绘制漂亮的股票价格趋势图"},{"content":"Pandas非常善于处理大规模数据，支持将结果导出到CSV，Excel，HTML，json等文件中。 如果您想将 多种信息（excel、图片等）组合 到一个文档中，那么事情变得更加困难。 例如，如果要在一个Excel工作表上放置两个DataFrame，则需要使用相应的Excel库才能输出。 这当然是OK的，但走起来真不简单。 本文将介绍一种方法，将多种 信息（excel、图片等）组合 成一个 HTML模板 ，然后使用Jinja模板和WeasyPrint 再将其转换为独立的PDF文档。\n操作流程 使用Pandas将数据输出到Excel文件中的多个工作表或者用DataFrames创建多个Excel文件非常方便。 但是，如果您想将 多种信息（excel、图片等）组合 输出到一个文件中，那么直接从Pandas进行操作的方法并不多。 幸运的是，万能的python大法有很多选项可以帮助我们。\n在本文中，我将采用以下流程来创建多页PDF文档。\n  工具 我决定使用HTML作为模板语言，因为它可能是生成结构化数据最便捷的方法，支持各种格式数据（如图片、excel表）。我也认为每个人都知道（或可以弄清楚）足够的HTML知识来生成一个简单的报告。\n这个工作流程中最困难的部分是弄清楚如何将HTML呈现为PDF。我觉得还没有最佳的解决方案，但我选择了 WeasyPrint ，因为它仍然在积极维护，我发现我可以相对容易地使它工作。作为替代方案，我过去使用过xhtml2pdf，它的效果也很好，遗憾的是该库缺乏文档说明，但它已存在一段时间，并且确实可以从HTML生成PDF。\n数据 如上所述，我们将使用之前文章中的相同数据集。 为了使这一切成为一个独立的文章，下面是我如何导入数据,做描述性统计，及并生成关于CPU和软件销售的 数据透视表 。\n导入模块，并读入销售渠道信息。\nimport pandas as pd import numpy as np df = pd.read_excel(\u0026#39;salesfunnel.xlsx\u0026#39;) df.head() Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Account Name Rep Manager Product Quantity Price Status     0 714466 Trantow-Barrows Craig Booker Debra Henley CPU 1 30000 presented   1 714466 Trantow-Barrows Craig Booker Debra Henley Software 1 10000 presented   2 714466 Trantow-Barrows Craig Booker Debra Henley Maintenance 2 5000 pending   3 737550 Fritsch, Russel and Anderson Craig Booker Debra Henley CPU 1 35000 declined   4 146832 Kiehn-Spinka Daniel Hilton Debra Henley CPU 2 65000 won     对数据做透视表\nsales_report = pd.pivot_table(df, index=[\u0026#34;Manager\u0026#34;, \u0026#34;Rep\u0026#34;, \u0026#34;Product\u0026#34;], values=[\u0026#34;Price\u0026#34;, \u0026#34;Quantity\u0026#34;], aggfunc=[np.sum, np.mean], fill_value=0) sales_report.head() Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }  \n     sum mean      Price Quantity Price Quantity   Manager Rep Product         Debra Henley Craig Booker CPU 65000 2 32500 1.0   Maintenance 5000 2 5000 2.0   Software 10000 1 10000 1.0   Daniel Hilton CPU 105000 4 52500 2.0   Software 10000 1 10000 1.0     模板 DataFrame拥有to_html()的方法，可以将dataframe数据转化为含有HTML格式的字符串。\n但是随着您的报告变得越来越复杂或您选择将Jinja模板用于您的网络应用，jinja的这些功能将为您提供良好的服务。Jinja的另一个不错的功能是它包含多个内置过滤器，这些过滤器允许我们以Pandas中难以做到的方式格式化我们的一些数据。\n为了在我们的应用程序中使用Jinja，我们需要做三件事：\n1. 创建一个模板 2. 将变量添加到模板上下文中 3. 将模板渲染为HTML \n这是一个非常简单的模板，我们称之为myreport.html：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;{{ title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Sales Funnel Report - National\u0026lt;/h2\u0026gt; {{ national_pivot_table }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 上面html代码中拥有 {{title}} 和 {{national_pivot_table }} 这两个关键词。这两个关键词用来接收需要渲染的数据，比如图片对象、dataframe对象等。\n要填充这些变量，我们需要创建一个Jinja环境并获取我们的模板对象：\nfrom jinja2 import Environment, FileSystemLoader env = Environment(loader=FileSystemLoader(\u0026#39;.\u0026#39;)) template = env.get_template(\u0026#39;myreport.html\u0026#39;) 在上面的示例中，我假设模板位于当前目录中，但您可以将完整路径放到模板位置。\n另一个关键组件是 env 的创建。 这个变量是我们将数据（或对象）填充给模板的方式。 我们创建了一个名为 template_var 的字典，其中包含我们想要传递给模板的所有变量。\n请注意变量的名称如何与我们的模板匹配。即名称要与myreport.html中的{{}}夹住的关键词一样。\ntemplate_vars = {\u0026#34;title\u0026#34; : \u0026#34;Sales Funnel Report - National\u0026#34;, \u0026#34;national_pivot_table\u0026#34;: sales_report.to_html()} 最后一步是将 template_vars 渲染到html模板中，并输出为html字符串，最终我们将使用该html字符串来生成pdf报告。\nhtml_out = template.render(template_vars) 为简洁起见，我不会显示完整的HTML，但您应该明白这一点。\n生成pdf pdf报告的生成部分相当简单，这里用到weasyprint库\nfrom weasyprint import HTML HTML(string=html_out).write_pdf(\u0026#34;report.pdf\u0026#34;)   但是打开的pdf样式很简单，白底黑字并不美观。之所以这样是因为我们没有使用自定义样式表css，但是咱们不熟悉css，有一种简单的办法就是用成熟的css，这里我用的typography.css 作为填充表格时的样式表。这个css的优点有：\n1. 相对较小且易于理解 2. 可以在PDF引擎中运行而不会抛出错误和警告 3. 能让表格表格看起来很美观 让我们尝试使用我们更新的样式表重新渲染它：\nHTML(string=html_out).write_pdf(\u0026#39;beautiful_report.pdf\u0026#39;, stylesheets=[\u0026#34;typography.css\u0026#34;])   添加了一个stylesheets参数就让输出结果瞬间变得高端大气上档次。\n更多 如果想更加丰富的输出pdf报告，大家回去需要查阅jinja文档，找一些css样式表。\n下载 点击这里下载本文需要的实验数据及代码\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/weasyprint_auto_report/","summary":"Pandas非常善于处理大规模数据，支持将结果导出到CSV，Excel，HTML，json等文件中。 如果您想将 多种信息（excel、图片等）组合 到一个文档中，那么事情变得更加困难。 例如，如果要在一个Excel工作表上放置两个DataFrame，则需要使用相应的Excel库才能输出。 这当然是OK的，但走起来真不简单。 本文将介绍一种方法，将多种 信息（excel、图片等）组合 成一个 HTML模板 ，然后使用Jinja模板和WeasyPrint 再将其转换为独立的PDF文档。\n操作流程 使用Pandas将数据输出到Excel文件中的多个工作表或者用DataFrames创建多个Excel文件非常方便。 但是，如果您想将 多种信息（excel、图片等）组合 输出到一个文件中，那么直接从Pandas进行操作的方法并不多。 幸运的是，万能的python大法有很多选项可以帮助我们。\n在本文中，我将采用以下流程来创建多页PDF文档。\n  工具 我决定使用HTML作为模板语言，因为它可能是生成结构化数据最便捷的方法，支持各种格式数据（如图片、excel表）。我也认为每个人都知道（或可以弄清楚）足够的HTML知识来生成一个简单的报告。\n这个工作流程中最困难的部分是弄清楚如何将HTML呈现为PDF。我觉得还没有最佳的解决方案，但我选择了 WeasyPrint ，因为它仍然在积极维护，我发现我可以相对容易地使它工作。作为替代方案，我过去使用过xhtml2pdf，它的效果也很好，遗憾的是该库缺乏文档说明，但它已存在一段时间，并且确实可以从HTML生成PDF。\n数据 如上所述，我们将使用之前文章中的相同数据集。 为了使这一切成为一个独立的文章，下面是我如何导入数据,做描述性统计，及并生成关于CPU和软件销售的 数据透视表 。\n导入模块，并读入销售渠道信息。\nimport pandas as pd import numpy as np df = pd.read_excel(\u0026#39;salesfunnel.xlsx\u0026#39;) df.head() Run\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }","title":"使用WeasyPrint自动生成pdf报告文件"},{"content":"Rembg Rembg可以去掉图片中的背景，效果如下\n安装 CPU版\npip install rembg GPU版\npip install rembg[gpu] \n快速上手 安装成功后，可以在命令行中调动Rembg。如果只对单个图片进行处理\nrembg i path/to/input.png path/to/output.png 对多个图片文件处理(批处理)，\nrembg p path/to/input path/to/output \n在Python中使用 把图片读取为二进制数据\nfrom rembg import remove #待处理的图片路径 input_path = \u0026#39;input.png\u0026#39; #处理后存储的图片路径 output_path = \u0026#39;output.png\u0026#39; with open(input_path, \u0026#39;rb\u0026#39;) as i: with open(output_path, \u0026#39;wb\u0026#39;) as o: input = i.read() output = remove(input) o.write(output) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/rembg_remove_background_from_image/","summary":"Rembg Rembg可以去掉图片中的背景，效果如下\n安装 CPU版\npip install rembg GPU版\npip install rembg[gpu] \n快速上手 安装成功后，可以在命令行中调动Rembg。如果只对单个图片进行处理\nrembg i path/to/input.png path/to/output.png 对多个图片文件处理(批处理)，\nrembg p path/to/input path/to/output \n在Python中使用 把图片读取为二进制数据\nfrom rembg import remove #待处理的图片路径 input_path = \u0026#39;input.png\u0026#39; #处理后存储的图片路径 output_path = \u0026#39;output.png\u0026#39; with open(input_path, \u0026#39;rb\u0026#39;) as i: with open(output_path, \u0026#39;wb\u0026#39;) as o: input = i.read() output = remove(input) o.write(output) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"Rembg库 | 可以去掉图片背景的"},{"content":" 点击上方图片购买课程   paperless-ngx https://github.com/paperless-ngx/paperless-ngx\nPaperless-ngx 是一个文档管理系统，可将您的文献资料转换为可搜索的本地在线存档。\n  ggpubr https://github.com/kassambara/ggpubr\nHadley Wickham 的 ggplot2 是一个出色且灵活的包，用于在 R 中进行优雅的数据可视化。但是，默认生成的图需要一些精巧的设计才能用于学术发表。 此外，要自定义 ggplot，语法不透明，这增加了没有高级 R 编程技能的研究人员的难度。\n\u0026lsquo;ggpubr\u0026rsquo; 包提供了一些易于使用的功能，用于创建和定制基于 \u0026lsquo;ggplot2\u0026rsquo; 的发布就绪图。\nggdotchart(dfm, x = \u0026#34;name\u0026#34;, y = \u0026#34;mpg_z\u0026#34;, color = \u0026#34;cyl\u0026#34;, # Color by groups palette = c(\u0026#34;#00AFBB\u0026#34;, \u0026#34;#E7B800\u0026#34;, \u0026#34;#FC4E07\u0026#34;), # Custom color palette sorting = \u0026#34;descending\u0026#34;, # Sort value in descending order add = \u0026#34;segments\u0026#34;, # Add segments from y = 0 to dots add.params = list(color = \u0026#34;lightgray\u0026#34;, size = 2), # Change segment color and size group = \u0026#34;cyl\u0026#34;, # Order by groups dot.size = 6, # Large dot size label = round(dfm$mpg_z,1), # Add mpg values as dot labels font.label = list(color = \u0026#34;white\u0026#34;, size = 9, vjust = 0.5), # Adjust label parameters ggtheme = theme_pubr() # ggplot2 theme )+ geom_hline(yintercept = 0, linetype = 2, color = \u0026#34;lightgray\u0026#34;)   ##svgwrite\nhttps://github.com/mozman/svgwrite\n使用Python生成svg图片\nDiploma PDF Generator https://github.com/streamlit/example-app-pdf-report\n使用streamlit建立的学习证书生成器，大家感兴趣可以使用一下。需要注意的是，python环境最好是3.7版本，3.8、3.9运行该代码会出Bug。\n  PyScript 可以在html中嵌入可执行的Python代码。\nhttps://pyscript.net/\n  了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly15/","summary":"点击上方图片购买课程   paperless-ngx https://github.com/paperless-ngx/paperless-ngx\nPaperless-ngx 是一个文档管理系统，可将您的文献资料转换为可搜索的本地在线存档。\n  ggpubr https://github.com/kassambara/ggpubr\nHadley Wickham 的 ggplot2 是一个出色且灵活的包，用于在 R 中进行优雅的数据可视化。但是，默认生成的图需要一些精巧的设计才能用于学术发表。 此外，要自定义 ggplot，语法不透明，这增加了没有高级 R 编程技能的研究人员的难度。\n\u0026lsquo;ggpubr\u0026rsquo; 包提供了一些易于使用的功能，用于创建和定制基于 \u0026lsquo;ggplot2\u0026rsquo; 的发布就绪图。\nggdotchart(dfm, x = \u0026#34;name\u0026#34;, y = \u0026#34;mpg_z\u0026#34;, color = \u0026#34;cyl\u0026#34;, # Color by groups palette = c(\u0026#34;#00AFBB\u0026#34;, \u0026#34;#E7B800\u0026#34;, \u0026#34;#FC4E07\u0026#34;), # Custom color palette sorting = \u0026#34;descending\u0026#34;, # Sort value in descending order add = \u0026#34;segments\u0026#34;, # Add segments from y = 0 to dots add.","title":"TechWeekly-15 每周有趣有用的技术分享"},{"content":"下载本文代码 如何计算地图中两点距离及角度 例如北京市北纬40.1，东经116.6； 哈尔滨市北纬45.7567, 东经126.6424\n感兴趣的童鞋也可去看问题出处\n https://stackoverflow.com/questions/3932502/calculate-angle-between-two-latitude-longitude-points/51415608\n 在回答中，看到有地理信息系统Python包geographiclib，可以计算两点距离和角度。\n安装geographiclib https://geographiclib.sourceforge.io/1.52/python/\n!pip3 install geographiclib==1.52 \n计算距离 注意，在接下来的计算中，将北纬、东经表示为正数。将南纬、西经表示为负数。\ndef distance(beiwei1, dongjing1, beiwei2, dongjing2): \u0026#34;\u0026#34;\u0026#34; beiwei1: 地点1的纬度数，如果地点在北半球，北纬为正；反之为负。 dongjing1: 地点2的经度数， 如果位于东半球，东经数为正；反之为负 beiwei2: 地点2的纬度数 dongjing2: 地点2的经度数 \u0026#34;\u0026#34;\u0026#34; from geographiclib.geodesic import Geodesic geod = Geodesic.WGS84 g = geod.Inverse(beiwei1, dongjing1, beiwei2, dongjing2) distance = g[\u0026#39;s12\u0026#39;]/1000 return distance #北京Beijing与哈尔滨harbin distance(40.1, 116.6, 45.7567, 126.6424) Run\n1031.617791888516  方位角azimuth 两个地点在地图中所处的相对位置，称之为方位角azimuth。方位角最大度数360度，\n 0度表示正北 90度表示正西 180度表示正南 270度表示正西 360度表示正北  def azimuth(beiwei1, dongjing1, beiwei2, dongjing2): \u0026#34;\u0026#34;\u0026#34; beiwei1: 地点1的纬度数，如果地点在北半球，北纬为正；反之为负。 dongjing1: 地点2的经度数， 如果位于东半球，东经数为正；反之为负 beiwei2: 地点2的纬度数 dongjing2: 地点2的经度数 \u0026#34;\u0026#34;\u0026#34; from geographiclib.geodesic import Geodesic geod = Geodesic.WGS84 l = geod.InverseLine(beiwei1, dongjing1, beiwei2, dongjing2) s12 = distance(beiwei1, dongjing1, beiwei2, dongjing2) g = l.Position(s12, Geodesic.STANDARD | Geodesic.LONG_UNROLL) return g[\u0026#39;azi2\u0026#39;] #北京 哈尔滨 azimuth(40.1, 116.6, 45.7567, 126.6424) 56.03961942267271  按照azimuth定义，56度的解读为 哈尔滨位于北京的东北方向。\n同理可以计算西安与杭州\n#西安， 杭州 azimuth(34.2658, 108.9541, 30.2741, 120.1552) 115.1506923699206  杭州位于西安的115度，即东南方向\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/how_to_calculate_geo_distance_in_python/","summary":"下载本文代码 如何计算地图中两点距离及角度 例如北京市北纬40.1，东经116.6； 哈尔滨市北纬45.7567, 东经126.6424\n感兴趣的童鞋也可去看问题出处\n https://stackoverflow.com/questions/3932502/calculate-angle-between-two-latitude-longitude-points/51415608\n 在回答中，看到有地理信息系统Python包geographiclib，可以计算两点距离和角度。\n安装geographiclib https://geographiclib.sourceforge.io/1.52/python/\n!pip3 install geographiclib==1.52 \n计算距离 注意，在接下来的计算中，将北纬、东经表示为正数。将南纬、西经表示为负数。\ndef distance(beiwei1, dongjing1, beiwei2, dongjing2): \u0026#34;\u0026#34;\u0026#34; beiwei1: 地点1的纬度数，如果地点在北半球，北纬为正；反之为负。 dongjing1: 地点2的经度数， 如果位于东半球，东经数为正；反之为负 beiwei2: 地点2的纬度数 dongjing2: 地点2的经度数 \u0026#34;\u0026#34;\u0026#34; from geographiclib.geodesic import Geodesic geod = Geodesic.WGS84 g = geod.Inverse(beiwei1, dongjing1, beiwei2, dongjing2) distance = g[\u0026#39;s12\u0026#39;]/1000 return distance #北京Beijing与哈尔滨harbin distance(40.1, 116.6, 45.7567, 126.6424) Run\n1031.617791888516  方位角azimuth 两个地点在地图中所处的相对位置，称之为方位角azimuth。方位角最大度数360度，\n 0度表示正北 90度表示正西 180度表示正南 270度表示正西 360度表示正北  def azimuth(beiwei1, dongjing1, beiwei2, dongjing2): \u0026#34;\u0026#34;\u0026#34; beiwei1: 地点1的纬度数，如果地点在北半球，北纬为正；反之为负。 dongjing1: 地点2的经度数， 如果位于东半球，东经数为正；反之为负 beiwei2: 地点2的纬度数 dongjing2: 地点2的经度数 \u0026#34;\u0026#34;\u0026#34; from geographiclib.","title":"使用Python计算地图中两点距离及方位角"},{"content":" 点击本图下载本文项目代码   下载项目数据代码 地理空间数据分析已成为数据科学中的一个常见领域。对于地理空间数据科学来说也不例外。支持 GeoPandas、Shapely 和 Rasterio 等项目的社区使 Python 成为地理空间分析的首选。\nGreppo：快速简单的地理空间仪表板构建方法 Greppo 是一个用于构建地理空间网络应用程序的开源 Python 框架。 它提供了可随时使用的前端和后端组件作为函数，充当前端交互组件和绑定的后端变量之间的 API。 要了解有关心智模型的更多信息，请参阅此页面。\n无需太多前端、后端、Web 开发经验，您就可以在 5 分钟内使用 Python 构建和部署功能齐全的 Web 应用程序。\n GitHub存储库：https://github.com/greppo-io/greppo 文档：https://docs.greppo.io/ 网站：https://greppo.io/   在本教程中，我们将使用 Greppo 在 Python 中构建地理空间仪表板。 我们将通过设置环境、安装、导入数据、为我们的仪表板放入组件并为其提供服务来工作。 您将需要了解 Python、GeoPandas 和地理空间分析的基础知识。 入门…\n 首先要做的是安装本文需要的 Python 第三方库。\npip install greppo geopandas 下载本教程所需的数据集。 本教程的所有数据和代码都可以在这个 GitHub 存储库中找到：greppo-demo/vector-demo（数据源：https://github.com/openpolis/geojson-italy）\n编写仪表板脚本 我们首先为项目设置文件夹结构。 我将使用以下项目文件夹结构：\n└── vector-demo ├── app.py ├── cities.geojson ├── regions.geojson └── roads.geojson 该应用程序的主控脚本是 app.py。\nStep 0：启动服务器 启动服务器，请打开**终端（windows对应的是命令行cmd）**并按照这些说明进行操作。\n首先，终端中执行cd vector_demo将工作目录切换为 (cd) 项目文件夹 vector_demo。\n然后，终端执行greppo serve app.py, 启动服务器。\n在命令行中启动服务器   然后，您将看到 Uvicorn 服务器在指定位置（类似于链接）运行。 复制网址链接，并将其粘贴到浏览器中。 然后它将加载应用程序。 如果您对 app.py 进行了任何更改，您将在浏览器页面上看到对您的应用的更新更改。\nStep 1：构建应用程序的基本脚手架。 即导入 greppo 包，并在地图中添加 base_layer。\nfrom greppo import app app.base_layer( name=\u0026#34;Open Street Map\u0026#34;, visible=True, url=\u0026#34;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0026#34;, subdomains=None, attribution=\u0026#39;(C) OpenStreetMap contributors\u0026#39;, ) app.base_layer(provider=\u0026#34;CartoDB Positron\u0026#34;) 您需要先从 greppo 导入应用程序。 此应用程序对象将用作您的应用程序前端的接口，并为前端和后端之间的 2 路通信提供 API。\n  要将 base_layer 添加到您的应用程序地图，只需使用带有所需参数的应用程序 API 方法 app.base_layer()。 请参阅文档以了解什么是必需的和什么是可选的。 base_layer 方法提供了两种方法来指定基本瓦片层。 一种是使用 name 和 url 属性。 另一种方法是使用提供者的名称。\nStep 2：导入数据集并将其显示为叠加层。 使用 geopandas，我们可以将矢量数据集作为 GeoDataFrame 导入。 然后可以使用 app.vector_layer() 方法在 Web 应用程序上的地图上对其进行可视化。 在这里，我要导入三个不同的数据集，每个数据集都有多边形（意大利不同地区的边界）、线（意大利的主要高速公路）和点（意大利的主要城市）。\nimport geopandas as gpd from greppo import app regions = gpd.read_file(\u0026#34;./regions.geojson\u0026#34;) roads = gpd.read_file(\u0026#34;./roads.geojson\u0026#34;) cities = gpd.read_file(\u0026#34;./cities.geojson\u0026#34;) app.vector_layer( data = regions, name = \u0026#34;Regions of Italy\u0026#34;, description = \u0026#34;Polygons showing the boundaries of regions of Italy.\u0026#34;, style = {\u0026#34;fillColor\u0026#34;: \u0026#34;#4daf4a\u0026#34;}, ) app.vector_layer( data = roads, name = \u0026#34;Highways in Italy\u0026#34;, description = \u0026#34;Lines showing the major highways in Italy.\u0026#34;, style = {\u0026#34;color\u0026#34;: \u0026#34;#377eb8\u0026#34;}, ) app.vector_layer( data = cities, name = \u0026#34;Cities of Italy\u0026#34;, description = \u0026#34;Points showing the cities in Italy.\u0026#34;, style = {\u0026#34;color\u0026#34;: \u0026#34;#e41a1c\u0026#34;}, visible = True, ) 使用方法 app.vector_layer() 作为前端的 API，我们可以显示矢量数据。 您需要传入名称，其他参数是可选的。 虽然，将颜色或填充颜色传递给样式是区分和识别每一层的好习惯。 由于前端基于 Leaflet，所有样式都符合 Leaflet 的规范，可以在文档中找到。\n  您可以在此处找到 vector_layer 和样式的所有信息。 注意：您也可以制作 Choropleth 地图。 可以在此处找到其文档。\nStep 3：显示应用内文本、应用标题和应用描述 为了使 Web 应用程序有用，它需要携带一些文本以向其用户提供一些指导和上下文。 这些可以显示在组件旁边的侧边栏上。 使用 app.display() 方法可以在 web-app 上显示降价文本。 使用相同的方法，可以设置应用程序的标题和描述。\nfrom greppo import app app.display(name=\u0026#39;title\u0026#39;, value=\u0026#39;Vector demo\u0026#39;) app.display(name=\u0026#39;description\u0026#39;, value=\u0026#39;A Greppo demo app for vector data using GeoJSON data.\u0026#39;) text_1 = \u0026#34;\u0026#34;\u0026#34; ## About the web-app The dashboard shows the boundaries of the regions of Italy as polygons, the major arterial higways as lines and the major cities of each region as points. \u0026#34;\u0026#34;\u0026#34; app.display(name=\u0026#39;text-1\u0026#39;, value=text_1) app.display() 接受两个参数 name 和 value 。 虽然 value 承载要显示的文本，但 name 必须是唯一的，并带有文本的标识符。 如果 name=\u0026lsquo;title\u0026rsquo; 传递的值是应用程序的标题，如果 name=\u0026lsquo;description\u0026rsquo; 传递的值是应用程序的描述。 如果没有这两者中的任何一个，该应用程序将带有其默认标题和描述。 鼓励设置应用程序的标题和描述。\n  Step 4：将数据显示为图表 数据应用程序几乎没有将数据显示为图表。 Greppo 还允许您将数据显示为图表。 可以在文档中找到有关图表及其用法的信息。 这里，作为一个例子，展示了一个条形图，app.bar_chart()。\nfrom greppo import app import geopandas as gpd regions = gpd.read_file(\u0026#34;./regions.geojson\u0026#34;) roads = gpd.read_file(\u0026#34;./roads.geojson\u0026#34;) cities = gpd.read_file(\u0026#34;./cities.geojson\u0026#34;) app.display(name=\u0026#39;text-2\u0026#39;, value=\u0026#39;The following displays the count of polygons, lines and points as a barchart.\u0026#39;) app.bar_chart(name=\u0026#39;Geometry count\u0026#39;, description=\u0026#39;A bar-cart showing the count of each geometry-type in the datasets.\u0026#39;, x=[\u0026#39;polygons\u0026#39;, \u0026#39;lines\u0026#39;, \u0026#39;points\u0026#39;], y=[len(regions), len(roads), len(cities)], color=\u0026#39;#984ea3\u0026#39;) 图表所需的参数是名称、x 和 y 值。 可以添加描述和颜色来为应用程序用户提供更好的上下文。 名称和描述与图表一起显示。\n  结论 让我们全面了解一下我们在这里所做的事情。 我们的目标是使用 Greppo 创建一个地理空间 web 应用程序，以显示一些 GIS 矢量数据，并添加组件以为应用程序的用户提供更好的上下文。 这是该应用程序的完整代码：\nfrom greppo import app import geopandas as gpd app.display(name=\u0026#39;title\u0026#39;, value=\u0026#39;Vector demo\u0026#39;) app.display(name=\u0026#39;description\u0026#39;, value=\u0026#39;A Greppo demo app for vector data using GeoJSON data.\u0026#39;) app.base_layer( name=\u0026#34;Open Street Map\u0026#34;, visible=True, url=\u0026#34;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0026#34;, subdomains=None, attribution=\u0026#39;(C) OpenStreetMap contributors\u0026#39;, ) app.base_layer( provider=\u0026#34;CartoDB Positron\u0026#34;, ) regions = gpd.read_file(\u0026#34;./regions.geojson\u0026#34;) roads = gpd.read_file(\u0026#34;./roads.geojson\u0026#34;) cities = gpd.read_file(\u0026#34;./cities.geojson\u0026#34;) app.vector_layer( data=regions, name=\u0026#34;Regions of Italy\u0026#34;, description=\u0026#34;Polygons showing the boundaries of regions of Italy.\u0026#34;, style={\u0026#34;fillColor\u0026#34;: \u0026#34;#4daf4a\u0026#34;}, ) app.vector_layer( data=roads, name=\u0026#34;Highways in Italy\u0026#34;, description=\u0026#34;Lines showing the major highways in Italy.\u0026#34;, style={\u0026#34;color\u0026#34;: \u0026#34;#377eb8\u0026#34;}, ) app.vector_layer( data=cities, name=\u0026#34;Cities of Italy\u0026#34;, description=\u0026#34;Points showing the cities in Italy.\u0026#34;, style={\u0026#34;color\u0026#34;: \u0026#34;#e41a1c\u0026#34;}, visible=True, ) text_1 = \u0026#34;\u0026#34;\u0026#34; ## About the web-app The dashboard shows the boundaries of the regions of Italy as polygons, the major arterial higways as lines and the major cities of each region as points. \u0026#34;\u0026#34;\u0026#34; app.display(name=\u0026#39;text-1\u0026#39;, value=text_1) app.display(name=\u0026#39;text-2\u0026#39;, value=\u0026#39;The following displays the count of polygons, lines and points as a barchart.\u0026#39;) app.bar_chart(name=\u0026#39;Geometry count\u0026#39;, description=\u0026#39;A bar-cart showing the count of each geometry-type in the datasets.\u0026#39;, x=[\u0026#39;polygons\u0026#39;, \u0026#39;lines\u0026#39;, \u0026#39;points\u0026#39;], y=[len(regions), len(roads), len(cities)], color=\u0026#39;#984ea3\u0026#39;) 运行效果如下\n 点击本图下载本文项目代码   下载项目数据代码 \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/geospatial_with_greppo/","summary":"点击本图下载本文项目代码   下载项目数据代码 地理空间数据分析已成为数据科学中的一个常见领域。对于地理空间数据科学来说也不例外。支持 GeoPandas、Shapely 和 Rasterio 等项目的社区使 Python 成为地理空间分析的首选。\nGreppo：快速简单的地理空间仪表板构建方法 Greppo 是一个用于构建地理空间网络应用程序的开源 Python 框架。 它提供了可随时使用的前端和后端组件作为函数，充当前端交互组件和绑定的后端变量之间的 API。 要了解有关心智模型的更多信息，请参阅此页面。\n无需太多前端、后端、Web 开发经验，您就可以在 5 分钟内使用 Python 构建和部署功能齐全的 Web 应用程序。\n GitHub存储库：https://github.com/greppo-io/greppo 文档：https://docs.greppo.io/ 网站：https://greppo.io/   在本教程中，我们将使用 Greppo 在 Python 中构建地理空间仪表板。 我们将通过设置环境、安装、导入数据、为我们的仪表板放入组件并为其提供服务来工作。 您将需要了解 Python、GeoPandas 和地理空间分析的基础知识。 入门…\n 首先要做的是安装本文需要的 Python 第三方库。\npip install greppo geopandas 下载本教程所需的数据集。 本教程的所有数据和代码都可以在这个 GitHub 存储库中找到：greppo-demo/vector-demo（数据源：https://github.com/openpolis/geojson-italy）\n编写仪表板脚本 我们首先为项目设置文件夹结构。 我将使用以下项目文件夹结构：\n└── vector-demo ├── app.py ├── cities.geojson ├── regions.geojson └── roads.geojson 该应用程序的主控脚本是 app.","title":"在Python中使用Greppo构建的地理空间仪表板"},{"content":"LoveIt https://github.com/dillonzq/LoveIt\n比现在我用的PaperMod更简洁美观的hugo主题\n 点击上方图片,查看案例网站   DataWrapper https://www.datawrapper.de/\n可视化工具网站，操作简洁，好用\n 点击查看网站   frappe https://github.com/frappe/frappe\n消除 97.42%* 的软件开发工作\n 点击查看网站   Parrot_Paraphraser 每年上半年都是毕业季，学生都会为论文降重倍感煎熬。\n一个实用且功能丰富的句子改写（可以将低英文论文重复率）框架，以文本形式增强人类意图，为会话引擎构建健壮的 NLU 模型。 由 Prithiviraj Damodaran 创建。\nfrom parrot import Parrot import torch import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) \u0026#39;\u0026#39;\u0026#39; uncomment to get reproducable paraphrase generations def random_state(seed): torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) random_state(1234) \u0026#39;\u0026#39;\u0026#39; #Init models (make sure you init ONLY once if you integrate this to your code) parrot = Parrot(model_tag=\u0026#34;prithivida/parrot_paraphraser_on_T5\u0026#34;) phrases = [\u0026#34;Can you recommed some upscale restaurants in Newyork?\u0026#34;, \u0026#34;What are the famous places we should not miss in Russia?\u0026#34; ] for phrase in phrases: print(\u0026#34;-\u0026#34;*100) print(\u0026#34;Input_phrase: \u0026#34;, phrase) print(\u0026#34;-\u0026#34;*100) para_phrases = parrot.augment(input_phrase=phrase, use_gpu=False) for para_phrase in para_phrases: print(para_phrase) Run\n---------------------------------------------------------------------- Input_phrase: Can you recommed some upscale restaurants in Newyork? ---------------------------------------------------------------------- list some excellent restaurants to visit in new york city? what upscale restaurants do you recommend in new york? i want to try some upscale restaurants in new york? recommend some upscale restaurants in newyork? can you recommend some high end restaurants in newyork? can you recommend some upscale restaurants in new york? can you recommend some upscale restaurants in newyork? ---------------------------------------------------------------------- Input_phrase: What are the famous places we should not miss in Russia ---------------------------------------------------------------------- what should we not miss when visiting russia? recommend some of the best places to visit in russia? list some of the best places to visit in russia? can you list the top places to visit in russia? show the places that we should not miss in russia? list some famous places which we should not miss in russia? 中文不能直接用，不过也有解决思路。\n 先将中文转为英文 再使用parrot将英文转为多种英文表达 将多种英文表达转化为多种中文表达  Knot https://github.com/Lojii/Knot\nKnot 是一款iOS端抓包工具（没有其他的科学转发功能，单纯的抓包工具），包含所有完整代码(Network+UI)，使用MITM(中间人攻击)技术，用swift编写，目前实现了http(s)解析。已实现功能\n http、https流量抓取 流量解析、多格式导出、过程分析 可抓取局域网内其他设备与外界通信数据 CA证书安装、导出 中英文国际化 过滤规则配置、导出  pixel-punk-avatars https://github.com/pixegami/pixel-punk-avatars\n使用Python一键生成1000张nft样式的头像。\n  tabler https://github.com/tabler/tabler\nTabler 完全响应并与所有现代浏览器兼容。 由于其现代、用户友好的设计，您可以创建一个用户会喜欢的功能齐全的界面！ 选择您需要的布局和组件并对其进行自定义，以使您的设计一致且引人注目。 每个组件的创建都注重细节，使您的界面美观！ 给我演示\n  RPA-Python  点击查看rpa项目网站   RPA是英文机器人处理自动化（Robotic Process Automation）的简称。日常生活中有很多重复性的工作可以通过自动化脚本或软件实现。 我想起来大洋彼岸一位程序员。每次他的工作总是超额完成，多次在公司受到老板的表扬和提薪。\n知乎上有个类似的话题，\n 点击查看rpa项目网站   了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly14/","summary":"LoveIt https://github.com/dillonzq/LoveIt\n比现在我用的PaperMod更简洁美观的hugo主题\n 点击上方图片,查看案例网站   DataWrapper https://www.datawrapper.de/\n可视化工具网站，操作简洁，好用\n 点击查看网站   frappe https://github.com/frappe/frappe\n消除 97.42%* 的软件开发工作\n 点击查看网站   Parrot_Paraphraser 每年上半年都是毕业季，学生都会为论文降重倍感煎熬。\n一个实用且功能丰富的句子改写（可以将低英文论文重复率）框架，以文本形式增强人类意图，为会话引擎构建健壮的 NLU 模型。 由 Prithiviraj Damodaran 创建。\nfrom parrot import Parrot import torch import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) \u0026#39;\u0026#39;\u0026#39; uncomment to get reproducable paraphrase generations def random_state(seed): torch.manual_seed(seed) if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) random_state(1234) \u0026#39;\u0026#39;\u0026#39; #Init models (make sure you init ONLY once if you integrate this to your code) parrot = Parrot(model_tag=\u0026#34;prithivida/parrot_paraphraser_on_T5\u0026#34;) phrases = [\u0026#34;Can you recommed some upscale restaurants in Newyork?","title":"TechWeekly-14 每周有趣有用的技术分享"},{"content":"如何辨别出开悟的人？ 闲来无事，刷到讲开悟的视频， 里面提到智慧与记忆的关系。\n 开悟的人（有智慧的人）与普通人都有喜怒哀乐、七情六欲。在视频中，萨古鲁在这个视频里分享了为什么很多人无法开悟。\n太多的人因为年龄和经历的增长滋生傲慢，本质上这类行为隐含假设是【把记忆错当智慧】，他们习以为常，把记忆形成的惯性当做智慧，认为反惯性就是愚蠢的反智慧的。\n男人和女人吃同样的食物，男人不会变成女人，女人不会变成男人。猴子和人吃同样的食物，人不会变成猴子，猴子不会变成人。可见，我们的身体拥有记忆。其实我们的记忆有生命记忆，基因记忆，元素记忆，原子记忆；有意识、无意识层面上的记忆；言辞上和非言辞上的记忆，我们本身就是记忆的集合体。\n开悟的人，与普通人的区别是，摆脱肉体记忆对意识的影响，能够就是论事，实事求是，而不流于窠臼默守陈规。\n启示 大家都说保守不好，有好奇心拥抱变化是好的，但自己胆子小，讨厌变化，不愿意离开舒适区。\n今天看到的这个视频，让我站在更高的位置看透自己过往，自己的过往是记忆而不是智慧，不要把自己所有的记忆惯性应用于当下的问题。\n 做人要勤于思考，听得住他人劝，吸取有益合理的成分。 做事也要勤于思考，要认识事物的本来（本质），套用太祖的智慧“实事求是”。 给别人讲道理，尽量不要加个人断言，因为这样会掺杂很多记忆，容易让听者感到自己的三观被蹂躏践踏，最好能讲事物的本来（本质）。  了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/about_wisdom/","summary":"如何辨别出开悟的人？ 闲来无事，刷到讲开悟的视频， 里面提到智慧与记忆的关系。\n 开悟的人（有智慧的人）与普通人都有喜怒哀乐、七情六欲。在视频中，萨古鲁在这个视频里分享了为什么很多人无法开悟。\n太多的人因为年龄和经历的增长滋生傲慢，本质上这类行为隐含假设是【把记忆错当智慧】，他们习以为常，把记忆形成的惯性当做智慧，认为反惯性就是愚蠢的反智慧的。\n男人和女人吃同样的食物，男人不会变成女人，女人不会变成男人。猴子和人吃同样的食物，人不会变成猴子，猴子不会变成人。可见，我们的身体拥有记忆。其实我们的记忆有生命记忆，基因记忆，元素记忆，原子记忆；有意识、无意识层面上的记忆；言辞上和非言辞上的记忆，我们本身就是记忆的集合体。\n开悟的人，与普通人的区别是，摆脱肉体记忆对意识的影响，能够就是论事，实事求是，而不流于窠臼默守陈规。\n启示 大家都说保守不好，有好奇心拥抱变化是好的，但自己胆子小，讨厌变化，不愿意离开舒适区。\n今天看到的这个视频，让我站在更高的位置看透自己过往，自己的过往是记忆而不是智慧，不要把自己所有的记忆惯性应用于当下的问题。\n 做人要勤于思考，听得住他人劝，吸取有益合理的成分。 做事也要勤于思考，要认识事物的本来（本质），套用太祖的智慧“实事求是”。 给别人讲道理，尽量不要加个人断言，因为这样会掺杂很多记忆，容易让听者感到自己的三观被蹂躏践踏，最好能讲事物的本来（本质）。  了解课程  点击上方图片购买课程   点击进入详情页","title":"如何辨别出开悟的人？"},{"content":"问题场景 当遇到特别大的csv文件，例如500M，pandas读取会很慢，在之前教程中要使用其他包另类的语法。现在只需更新pandas，加一个参数即可。\n在本文中，我们将介绍：\n Pandas 的默认 CSV读数据， v1.4 中引入的更快、更并行的 CSV解析器。 一种不同的方法可以使事情变得更快。 读取 CSV，默认方式  碰巧有一个 850MB 的 CSV 文件，这是使用 Pandas 加载它的默认方式：\n## 更新pandas !pip3 install --upgrade pandas import pandas as pd df = pd.read_csv(\u0026#34;large.csv\u0026#34;) 下面是旧的pandas版本代码读数据所消耗时间\n0m13.245s\n使用pyarrow读csv 2022年1月份，刚刚发布的pandas1.4版本增加了arrow库的csv解析器。对了，该功能仍然处于试验阶段，不是默认解析器，但是足够快。下面是新版本用法\nimport pandas as pd df = pd.read_csv(\u0026#34;large.csv\u0026#34;, engine=\u0026#34;pyarrow\u0026#34;) 新版本代码读数据所消耗时间\n0m6.507s\n   CSV解析器 CPU运行时间     默认C 13.2s   PyArrow 6.5s    需要注意，如果你的数据分析项目已经使用了并行加速数据的读取，那么今天的新教程（pyarrow）并不会特别显著改善代码运行速度（十几倍加速），但至少可能提供有意义的加速（一倍左右）。\n重新思考 加载 CSV 基本上是一项繁重的工作：\n 需要分成几行。 需要用逗号分隔每一行。 需要处理字符串引用。 需要猜测（！）列的类型，除非您明确地将它们传递给 Pandas。 需要将字符串转换为整数、日期和其他非字符串类型。  所有这些都需要 CPU 时间。\n如果您从第三方获取 CSV，并且只处理一次，那么您对此无能为力。但是，如果您多次加载同一个 CSV 文件怎么办？或者，如果您是在数据处理管道的其他部分生成输入文件的人，该怎么办？\n您可以读取其他一些处理速度更快的文件格式，而不是读取 CSV。让我们看一个例子，使用 Parquet 数据格式。 Parquet 文件旨在快速读取：您不必像使用 CSV 那样进行大量解析。与 CSV 不同，其中列类型在文件读取时无需编码，在 Parquet 中，列的类型存储在实际文件中。\n首先，我们将 CSV 文件转换为 Parquet 文件；我们禁用压缩，因此我们正在与 CSV 进行更多的苹果对苹果的比较。当然，如果您是第一个生成文件的人，则不需要转换步骤，您可以直接将数据写入 Parquet。\nimport pandas as pd df = pd.read_csv(\u0026#34;large.csv\u0026#34;) df.to_parquet(\u0026#34;large.parquet\u0026#34;, compression=None) 现在我们的数据存在于large.parquet中，再读取large.parquet看看读取时间\nimport pandas as pd df = pd.read_parquet(\u0026#34;large.parquet\u0026#34;, engine=\u0026#34;fastparquet\u0026#34;) 2.441s\n   CSV解析器 CPU运行时间     默认C 13.2s   PyArrow 6.5s   fastparquet 2.4s    纯粹由 CPU 衡量，fastparquet 是迄今为止最快的。 这只是一个例子。 但显然阅读 Parquet 格式的效率要高得多。\n最好的文件格式不是 CSV CSV 是一种糟糕的格式。 除了解析效率低下之外，缺少类型数据意味着解析总是比具有实际列类型的结构化文件格式更容易出错和模棱两可。 因此，如果可以，请避免使用 CSV 并使用更好的格式，尽量选择其他高效类型，如 Parquet。\n如果你被 CSV 卡住了，考虑在 Pandas 1.4 中使用新的 PyArrow CSV 解析器； 你会得到很好的加速，特别是如果你的程序当前没有利用多个 CPU。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/read_csv_fast_with_pandas/","summary":"问题场景 当遇到特别大的csv文件，例如500M，pandas读取会很慢，在之前教程中要使用其他包另类的语法。现在只需更新pandas，加一个参数即可。\n在本文中，我们将介绍：\n Pandas 的默认 CSV读数据， v1.4 中引入的更快、更并行的 CSV解析器。 一种不同的方法可以使事情变得更快。 读取 CSV，默认方式  碰巧有一个 850MB 的 CSV 文件，这是使用 Pandas 加载它的默认方式：\n## 更新pandas !pip3 install --upgrade pandas import pandas as pd df = pd.read_csv(\u0026#34;large.csv\u0026#34;) 下面是旧的pandas版本代码读数据所消耗时间\n0m13.245s\n使用pyarrow读csv 2022年1月份，刚刚发布的pandas1.4版本增加了arrow库的csv解析器。对了，该功能仍然处于试验阶段，不是默认解析器，但是足够快。下面是新版本用法\nimport pandas as pd df = pd.read_csv(\u0026#34;large.csv\u0026#34;, engine=\u0026#34;pyarrow\u0026#34;) 新版本代码读数据所消耗时间\n0m6.507s\n   CSV解析器 CPU运行时间     默认C 13.2s   PyArrow 6.5s    需要注意，如果你的数据分析项目已经使用了并行加速数据的读取，那么今天的新教程（pyarrow）并不会特别显著改善代码运行速度（十几倍加速），但至少可能提供有意义的加速（一倍左右）。\n重新思考 加载 CSV 基本上是一项繁重的工作：","title":""},{"content":"astro https://github.com/withastro/astro\n构建高加载速度的网站，访问更快\nfocalboard https://github.com/mattermost/focalboard\nfocalboard是开源，本地使用目管理工具，可作为Notion的替代产品使用。\ntabby https://github.com/Eugeny/tabby\nTabby 是一名老外在 Github 开源的终端连接的工具，至今已经累积 20K+ star。\n 支持多平台，Windows、MacOS（Intel 芯片/M1 芯片）、Linux 都有对应的安装包的； 自带 SFTP 功能，能够与 Linux 系统传输文件； 炫酷的终端页面，简单易用，以及各种插件支持等  Echarts https://echarts.apache.org/examples/zh/index.html\n百度开发的，基于 JavaScript 的开源可视化图表库\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly13/","summary":"astro https://github.com/withastro/astro\n构建高加载速度的网站，访问更快\nfocalboard https://github.com/mattermost/focalboard\nfocalboard是开源，本地使用目管理工具，可作为Notion的替代产品使用。\ntabby https://github.com/Eugeny/tabby\nTabby 是一名老外在 Github 开源的终端连接的工具，至今已经累积 20K+ star。\n 支持多平台，Windows、MacOS（Intel 芯片/M1 芯片）、Linux 都有对应的安装包的； 自带 SFTP 功能，能够与 Linux 系统传输文件； 炫酷的终端页面，简单易用，以及各种插件支持等  Echarts https://echarts.apache.org/examples/zh/index.html\n百度开发的，基于 JavaScript 的开源可视化图表库\n了解课程  点击上方图片购买课程   点击进入详情页","title":"TechWeekly-13 每周有趣有用的技术分享"},{"content":"DatasetSearch https://datasetsearch.research.google.com/\n可以轻松地使用关键词搜索数据集。 此外，还可按日期，数据格式和使用权限等筛选条件过滤查询。\nKaggle 数据集 https://www.kaggle.com/datasets\nKaggle 是世界领先的平台，涉及所有数据科学相关的编程。允许用户查找和发布数据集，更具吸引力的是提供了其他人对该数据集的数据分析代码，可以通过案例化快速学习数据挖掘技术。\ndata.world https://data.world/\ndata.world是一个很少提及的集合。 它与Google数据集搜索引擎非常相似。\n当输入关键词查询时，它不仅显示数据集，还显示所需数据的子文件。 当查找诸如人口统计信息和地理位置集合之类的辅助数据时，强烈建议您使用\u0026quot;数据世界\u0026quot;。\nfivethirtyeight https://fivethirtyeight.com/\n著名数据分析网站名称538取自美国总统大选最后选举人团的538张选票，也就是决定总统选举结果的538票。\n该github项目存放了538网站大量的数据及分析代码。下面是538风格的可视化案例\ncongress-legislators https://github.com/unitedstates/congress-legislators\n美国国会议员，1789 年至今，采用 YAML/JSON/CSV 格式，以及委员会、总统和副总统。\nOpenAddresses https://github.com/openaddresses/openaddresses\n地址、地籍宗地和建筑足迹数据源的全球集合，开放且免费使用。\ndatasette https://github.com/simonw/datasette\nDatasette 是一个探索和发布数据的开源多功能工具。 可以帮助人们获取任何形状或大小的数据，并将其作为交互式、可探索的网站和随附的 API 发布。\n主要面向数据记者、博物馆馆长、档案管理员、地方政府、科学家、研究人员以及任何拥有希望与世界分享数据的人。\nLektor https://www.getlektor.com/\nLektor是一个静态网站生成器， 具有用于创建网站的**内容管理系统 (CMS) **和Web 框架功能。大多数静态站点生成器（例如Pelican、Hugo）都是以程序员为主要用户构建的。Lektor 试图通过提供类似于 Django 或 Wordpress 的管理面板来创建和更新站点内容，从而使非程序员更容易访问。\n\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly12/","summary":"DatasetSearch https://datasetsearch.research.google.com/\n可以轻松地使用关键词搜索数据集。 此外，还可按日期，数据格式和使用权限等筛选条件过滤查询。\nKaggle 数据集 https://www.kaggle.com/datasets\nKaggle 是世界领先的平台，涉及所有数据科学相关的编程。允许用户查找和发布数据集，更具吸引力的是提供了其他人对该数据集的数据分析代码，可以通过案例化快速学习数据挖掘技术。\ndata.world https://data.world/\ndata.world是一个很少提及的集合。 它与Google数据集搜索引擎非常相似。\n当输入关键词查询时，它不仅显示数据集，还显示所需数据的子文件。 当查找诸如人口统计信息和地理位置集合之类的辅助数据时，强烈建议您使用\u0026quot;数据世界\u0026quot;。\nfivethirtyeight https://fivethirtyeight.com/\n著名数据分析网站名称538取自美国总统大选最后选举人团的538张选票，也就是决定总统选举结果的538票。\n该github项目存放了538网站大量的数据及分析代码。下面是538风格的可视化案例\ncongress-legislators https://github.com/unitedstates/congress-legislators\n美国国会议员，1789 年至今，采用 YAML/JSON/CSV 格式，以及委员会、总统和副总统。\nOpenAddresses https://github.com/openaddresses/openaddresses\n地址、地籍宗地和建筑足迹数据源的全球集合，开放且免费使用。\ndatasette https://github.com/simonw/datasette\nDatasette 是一个探索和发布数据的开源多功能工具。 可以帮助人们获取任何形状或大小的数据，并将其作为交互式、可探索的网站和随附的 API 发布。\n主要面向数据记者、博物馆馆长、档案管理员、地方政府、科学家、研究人员以及任何拥有希望与世界分享数据的人。\nLektor https://www.getlektor.com/\nLektor是一个静态网站生成器， 具有用于创建网站的**内容管理系统 (CMS) **和Web 框架功能。大多数静态站点生成器（例如Pelican、Hugo）都是以程序员为主要用户构建的。Lektor 试图通过提供类似于 Django 或 Wordpress 的管理面板来创建和更新站点内容，从而使非程序员更容易访问。\n\n了解课程  点击上方图片购买课程   点击进入详情页","title":"TechWeekly-12 每周有趣有用的技术分享"},{"content":"接到大姐任务，要每天给小外甥出10以内的加减法习题。我寻思了下，还是写代码生成测试习题方便。\n为了自己偷懒，把习题和答案都生成，做完了让娃自己检查，我只检查他有没有做题就行了^_^\nimport random from datetime import datetime #加、减模板 add_formula = \u0026#39;{a}+ {b}= {c}\u0026#39; sub_formula = \u0026#39;{a}- {b}= {c}\u0026#39; formulas = set() for i in range(50): a=random.randint(0, 10) b=random.randint(0, 10) formulas.add(add_formula.format(a=a, b=b, c=a+b)) if a\u0026gt;=b: #小外甥这个岁数没有负数的概念，需要满足a\u0026gt;=b的减法 formulas.add(sub_formula.format(a=a, b=b, c=a-b)) #习题去重且有序，方便做题后自己检查对错 formulas = list(formulas) #保存到加减md中，方便导出pdf month = datetime.today().month day = datetime.today().day with open(\u0026#39;加减.md\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: f.write(\u0026#39;## {month}月{day}号 加减运算\\n\u0026#39;.format(month=month, day=day)) for fm in formulas: f.write(fm.split(\u0026#39;= \u0026#39;)[0] + \u0026#39; =\\n\\n\\n\u0026#39;) with open(\u0026#39;加减答案.md\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: f.write(\u0026#39;## {month}月{day}号 加减运算\\n\u0026#39;.format(month=month, day=day)) for fm in formulas: f.write(fm + \u0026#39;\\n\\n\u0026#39;) 然后共享到ipad内，刚刚开心了半个小时，小外甥做了10道题不到，我的pencil二代就完犊子了~~~~~\n 点击上方图片购买课程   了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/math_add_sub/","summary":"接到大姐任务，要每天给小外甥出10以内的加减法习题。我寻思了下，还是写代码生成测试习题方便。\n为了自己偷懒，把习题和答案都生成，做完了让娃自己检查，我只检查他有没有做题就行了^_^\nimport random from datetime import datetime #加、减模板 add_formula = \u0026#39;{a}+ {b}= {c}\u0026#39; sub_formula = \u0026#39;{a}- {b}= {c}\u0026#39; formulas = set() for i in range(50): a=random.randint(0, 10) b=random.randint(0, 10) formulas.add(add_formula.format(a=a, b=b, c=a+b)) if a\u0026gt;=b: #小外甥这个岁数没有负数的概念，需要满足a\u0026gt;=b的减法 formulas.add(sub_formula.format(a=a, b=b, c=a-b)) #习题去重且有序，方便做题后自己检查对错 formulas = list(formulas) #保存到加减md中，方便导出pdf month = datetime.today().month day = datetime.today().day with open(\u0026#39;加减.md\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: f.write(\u0026#39;## {month}月{day}号 加减运算\\n\u0026#39;.format(month=month, day=day)) for fm in formulas: f.write(fm.split(\u0026#39;= \u0026#39;)[0] + \u0026#39; =\\n\\n\\n\u0026#39;) with open(\u0026#39;加减答案.","title":"Python妙用|给小外甥生成10以内加减运算数学做作业"},{"content":"Python网络爬虫与文本分析 工作坊基本信息  2000元 2021年1月21-22日 小鹅通平台(线上直播） 每天6小时（8:30 — 11:30；14:00 — 17:00）+ 30分钟答疑  报名咨询  17816181460（同微信）（汪老师） 单位：杭州国商智库信息技术服务有限公司 开户银行： 中国银行杭州大学城支行 银行账户：6232636200100260588  课程纲要  课程目标： 掌握Python语法、网络爬虫、文本分析、机器学习的核心知识点和分析思路 核心知识点： 爬虫原理及应用、 非结构化文本数据挖掘的思路及方法、机器学习应用等 环境配置: 安装anaconda，注意安装过程中勾选Add Anaconda to the system Path environment variable、Register Anaconda as the system python 3.x 课件资料： 本课程全部使用jupyter notebook文件作为课程课件，开课前会将代码数据等相关资料发给各位  课程特色  接地气： 以经管学术需求为导向， 将Python分为语法篇、采集数据篇、文本分析篇、机器学习篇四大部分 好理解： 知识点力求通俗易懂，少了晦涩的计算机术语，多了通俗易懂的使用场景和实战讲解 上手快： 所有知识点均有可重复使用的代码块，犹如一块块的积木，课后您可以根据分析需要，快速搭建出自己的Python代码  \n课程目录 在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！\n在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：\n 网络爬虫技术 解决 如何从网络世界中高效地 采集数据？ 文本分析技术 解决 如何从杂乱的文本数据中抽取文本信息(变量)？  \n一、Python语法入门  Python跟英语一样是一门语言 数据类型之字符串 数据类型之列表元组集合 数据类型之字典 数据类型之布尔值、None 逻辑语句(if\u0026amp;for\u0026amp;tryexcept) 列表推导式 理解函数 常用的内置函数 内置库os路径库 内置库csv文件库 常见错误汇总  二、数据采集  网络爬虫原理 寻找网址规律 获取网页-requests库 pyquery库解析html网页 案例 1：豆瓣/大众点评 json库解析json网页 案例 2： 知乎 案例 3： 百度地图POI地点检索 案例 4： 下载文档、多媒体文件 案例 5：上市公司定期报告pdf批量下载 简易爬虫库smartscraper（了解） 区分动态网站与静态网站  三、初识文本分析  文本分析在经管领域中的应用 读取文件中的数据(txt、xlsx、csv) 数据清洗re库-从文本中抽取姓名、年龄、电话、数字等各种信息 案例 6：将多个txt文件整理到一个excel中 jieba分词包 spacy包 案例 7：词频统计\u0026amp;制作词云图 案例 8：金融文本语调（正式、非正式）分析 案例 9： 扩展情感词典- 共现法 案例 10： 扩展情感词典- 词向量word2vec 案例 11： 文本情感分析 数据分析pandas库快速入门 案例 12：使用pandas对excel中的文本进行情感分析  四、机器学习与文本分析  了解机器学习 使用机器学习做文本分析的流程 scikit-learn机器学习库简介 文本特征工程-将文本转化为机器可处理的数字向量 认识词袋法、one-hot、tf-idf、word2vec 案例 13： 文本情感分析（带权重tf-idf\u0026amp;情感词典） 案例 14： 文本数据标注工具 案例 15： 在线评论文本分类 文本相似性计算 案例 15：使用文本相似性识别变化(政策连续性) 案例 16：Kmeans聚类算法 案例 17：LDA话题模型 案例 18：使用预训练BERT词向量模型做主题分析 案例 19: 识别图片中的文本 python爬虫、文本分析、机器学习等技术在论文中的应用赏析  工作坊基本信息  2000元 2021年1月21-22日 小鹅通平台(线上直播） 每天6小时（8:30 — 11:30；14:00 — 17:00）+ 30分钟答疑  报名咨询  17816181460（同微信）（汪老师） 单位：杭州国商智库信息技术服务有限公司 开户银行： 中国银行杭州大学城支行 银行账户：6232636200100260588   点击上方图片购买课程   相关论文 参照论文的摘要，可以通过场景化等的方式帮助我们迅速理解上面两个问题。加粗内容是论文用到了python技术，在我们的课程中均有与之对应的知识点和代码。\n蔡庆丰,陈熠辉,林焜.信贷资源可得性与企业创新:激励还是抑制?——基于银行网点数据和金融地理结构的微观证据[J].经济研究,2020,v.55;No.637(10):124-140.\n 结合银行和企业的地理位置数据,运用Python编写的基于百度地图API的地理坐标匹配系统,获取上市公司办公地点周围的银行网点数量。\n 王伟,陈伟,祝效国,王洪伟.众筹融资成功率与语言风格的说服性——基于Kickstarter的实证研究[J].管理世界,2016(05):81-98.\n 摘要：众筹融资效果决定着众筹平台的兴衰。 众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。 而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。 不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。 首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。\n然后，借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。\n最后，建立语言说服风格对项目筹资影响的计量模型，并对 Kickstarter 平台上的 128345 个项目进行实证分析。 总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。\n 胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.\n 在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。 然而，作为企业掌舵人的管理者并非都具有长远的目光。 本文基于高层梯队理论和社会心理学中的时间 导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。 研究结果发现，年报 MD\u0026amp;A 中披露的“短期视域” 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。 当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越易受到抑制。 最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。 本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴。\n 姚加权,冯绪,王赞钧,纪荣嵘,张维.语调、情绪及市场影响:基于金融情绪词典[J].管理科学学报,2021,24(05):26-46.\n 金融文本的语调与情绪含有上市公司管理层以及个体投资者表达的情感信息 ， 并对股票市场产生影响 。 通过词典重组和深度学习算法构建了适用于正式文本 与 非正式文本的金融领域 中文情绪词典，并基于词典构建了上市公司的年报语调和社交媒体情绪指标构建的年报语调指标和社交媒体情绪指标能有效地预测上市公司股票的收益率 、成交量波动率和非预期盈余等市场因素 ， 并优于基于其他广泛使用情绪词典构建的指标。此外，年报语调指标和社交媒体情绪指标对上市公司的股价崩盘风险具有显著的预测作用。为文本大数据在金融市场的 应用提供了分析工具 ，也为大数据时代的金融市场预测和监管等活动提供了决策支持 。\n Wang, Quan, Beibei Li, and Param Vir Singh. \u0026ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.\u0026rdquo; Information Systems Research 29, no. 2 (2018): 273-291.\n 摘要: 尽管移动应用程序市场的增长为移动应用程序开发人员创新提供了巨大的市场机会和经济诱因，但它也不可避免地刺激了模仿者开发盗版软件。原始应用的从业人员和开发人员声称，模仿者窃取了原始应用的想法和潜在需求，并呼吁应用平台对此类模仿者采取行动。令人惊讶的是，很少有严格的研究来分析模仿者是否以及如何影响原始应用的需求。\n进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原始或模仿者并检测两种模仿者的方法：欺骗性和非欺骗性。\n根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的5,141个开发人员的10,100个动作游戏应用程序样本中，模仿应用程序对原始应用程序需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原件的需求。\n结果表明，从总体上讲，模仿者对原始移动应用程序需求的影响在统计上是微不足道的。我们的研究通过提供一种识别模仿者的方法，并提供模仿者对原始应用需求的影响的证据，为越来越多的移动应用消费文献做出了贡献。\n  点击上方图片购买课程   相关论文汇总 [1]沈艳,陈赟,黄卓.文本大数据分析在经济学和金融学中的应用:一个文献综述[J].经济学(季刊),2019,18(04):1153-1186.\n[2]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.管理世界.2016;5:81-98.\n[3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.\n[4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, The Review of Financial Studies,2020\n[5]Kenneth Benoit. July 16, 2019. “Text as Data: An Overview.” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. Handbook of Research Methods in Political Science and International Relations. Thousand Oaks: Sage.\n[6]Loughran T, McDonald B. Textual analysis in accounting and finance: A survey[J]. Journal of Accounting Research, 2016, 54(4): 1187-1230. Author links open overlay panelComputational socioeconomics\n[7]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.\u0026rdquo; Journal of Marketing 84, no. 1 (2020): 1-25.\n[8]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. \u0026ldquo;A review of best practice recommendations for text analysis in R (and a user-friendly app).\u0026rdquo; Journal of Business and Psychology 33, no. 4 (2018): 445-459.\n[9]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. \u0026ldquo;Lazy prices.\u0026rdquo; The Journal of Finance 75, no. 3 (2020): 1371-1415.\n[10]孟庆斌, 杨俊华, 鲁冰. 管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J]. 中国工业经济, 2017 (12): 132-150.\n[11]Wang, Quan, Beibei Li, and Param Vir Singh. \u0026ldquo;Copycats vs. Original Mobile Apps: A Machine Learning Copycat-Detection Method and Empirical Analysis.\u0026rdquo; Information Systems Research 29.2 (2018): 273-291.\n[12]Hoberg, Gerard, and Gordon Phillips. 2016, Text-based network industries and endogenous product differentiation,?Journal of Political Economy 124, 1423-1465\n[13]Loughran, Tim, and Bill McDonald. \u0026ldquo;When is a liability not a liability? Textual analysis, dictionaries, and 10‐Ks.\u0026rdquo; The Journal of Finance 66, no. 1 (2011): 35-65.\n[14]Fairclough, Norman. 2003. Analysing discourse: Textual analysis for social research (Psychology Press)\n[15]Grimmer, Justin, and Brandon M Stewart. 2013, Text as data: The promise and pitfalls of automatic content analysis methods for political texts, Political analysis21, 267-297.\n[16]Bollen, Johan, et al. \u0026ldquo;Historical language records reveal a surge of cognitive distortions in recent decades.\u0026rdquo; Proceedings of the National Academy of Sciences 118.30 (2021).\n[17]Markowitz, D. M., \u0026amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).\n[18]Bail, Christopher Andrew. \u0026ldquo;Combining natural language processing and network analysis to examine how advocacy organizations stimulate conversation on social media.\u0026rdquo; Proceedings of the National Academy of Sciences 113, no. 42 (2016): 11823-11828.\n[19]姚加权,冯绪,王赞钧,纪荣嵘,张维.语调、情绪及市场影响:基于金融情绪词典[J].管理科学学报,2021,24(05):26-46.\n录播课  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/2022_1st_workshop/","summary":"Python网络爬虫与文本分析 工作坊基本信息  2000元 2021年1月21-22日 小鹅通平台(线上直播） 每天6小时（8:30 — 11:30；14:00 — 17:00）+ 30分钟答疑  报名咨询  17816181460（同微信）（汪老师） 单位：杭州国商智库信息技术服务有限公司 开户银行： 中国银行杭州大学城支行 银行账户：6232636200100260588  课程纲要  课程目标： 掌握Python语法、网络爬虫、文本分析、机器学习的核心知识点和分析思路 核心知识点： 爬虫原理及应用、 非结构化文本数据挖掘的思路及方法、机器学习应用等 环境配置: 安装anaconda，注意安装过程中勾选Add Anaconda to the system Path environment variable、Register Anaconda as the system python 3.x 课件资料： 本课程全部使用jupyter notebook文件作为课程课件，开课前会将代码数据等相关资料发给各位  课程特色  接地气： 以经管学术需求为导向， 将Python分为语法篇、采集数据篇、文本分析篇、机器学习篇四大部分 好理解： 知识点力求通俗易懂，少了晦涩的计算机术语，多了通俗易懂的使用场景和实战讲解 上手快： 所有知识点均有可重复使用的代码块，犹如一块块的积木，课后您可以根据分析需要，快速搭建出自己的Python代码  \n课程目录 在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！\n在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：\n 网络爬虫技术 解决 如何从网络世界中高效地 采集数据？ 文本分析技术 解决 如何从杂乱的文本数据中抽取文本信息(变量)？","title":"即将开班 | 2022Python数据挖掘寒假工作坊"},{"content":" 点击上方图片购买课程   用于从各种网站下载图像/视频/音乐/文本等的桌面实用程序。\n下载安装 App版，仅支持Win\nhttps://github.com/KurtBestor/Hitomi-Downloader/releases/tag/v3.7h Chrome浏览器插件\nhttps://github.com/KurtBestor/Hitomi-Downloader/wiki/Chrome-Extension\n操作演示 功能特色  🍰 简洁用户接口 🚀 加速下载 📜 支持用户diy脚本 🧲 支持BitTorrent \u0026amp; Magnet 🎞️ 支持 M3U8 \u0026amp; MPD 格式视频下载  支持的站点Sites    Site URL     AfreecaTV https://afreecatv.com   ArtStation https://artstation.com   AsianSister https://asiansister.com   AsmHentai https://asmhentai.com   Avgle https://avgle.com   baraag.net https://baraag.net   半次元 https://bcy.net   BDSMlr https://bdsmlr.com   bilibili https://bilibili.com   ComicWalker https://comic-walker.com   Coub https://coub.com   Danbooru https://danbooru.donmai.us   Kakao Webtoon http://webtoon.kakao.com   DeviantArt https://deviantart.com   E(x)Hentai Galleries https://e-hentai.org\nhttps://exhentai.org   Facebook https://facebook.com   FC2 Video https://video.fc2.com   Flickr https://flickr.com   Gelbooru https://gelbooru.com   Hameln https://syosetu.org   hanime.tv https://hanime.tv   Hentai Foundry https://hentai-foundry.com   Hitomi.la https://hitomi.la   Hiyobi.me https://hiyobi.me   Imgur https://imgur.com   Instagram https://instagram.com   Iwara https://iwara.tv\nhttps://ecchi.iwara.tv   Jmana https://jmana.net   カクヨム https://kakuyomu.jp   LHScan https://loveheaven.net   Likee https://likee.video   Luscious https://luscious.net   MyReadingManga https://myreadingmanga.info   Naver Blog https://blog.naver.com   Naver Post https://post.naver.com   Naver Webtoon https://comic.naver.com   Naver TV https://tv.naver.com   nhentai https://nhentai.net   nhentai.com https://nhentai.com   Niconico http://nicovideo.jp   ニジエ https://nijie.info   Pawoo https://pawoo.net   Pinterest https://pinterest.com   Pixiv https://pixiv.net   pixivコミック https://comic.pixiv.net   Pornhub https://pornhub.com\nhttps://pornhubpremium.com   Rule34.xxx https://rule34.xxx   Sankaku Complex https://www.sankakucomplex.com\nhttps://chan.sankakucomplex.com\nhttps://idol.sankakucomplex.com   Soundcloud https://soundcloud.com   小説家になろう https://syosetu.com   TOKYO Motion https://tokyomotion.net   Tumblr https://tumblr.com   Twitch https://twitch.tv   Twitter https://twitter.com   Vimeo https://vimeo.com   V LIVE https://vlive.tv   Weibo https://weibo.com   WikiArt https://www.wikiart.org   xHamster https://xhamster.com   XNXX https://xnxx.com   XVideos https://xvideos.com   Yande.re https://yande.re   Youku https://youku.com   YouTube https://youtube.com   and more\u0026hellip; [Supported sites by youtube-dl](http://ytdl-org.github.io/youtube-dl/supportedsites.html    数据挖掘课  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/hitomi_downloader/","summary":"点击上方图片购买课程   用于从各种网站下载图像/视频/音乐/文本等的桌面实用程序。\n下载安装 App版，仅支持Win\nhttps://github.com/KurtBestor/Hitomi-Downloader/releases/tag/v3.7h Chrome浏览器插件\nhttps://github.com/KurtBestor/Hitomi-Downloader/wiki/Chrome-Extension\n操作演示 功能特色  🍰 简洁用户接口 🚀 加速下载 📜 支持用户diy脚本 🧲 支持BitTorrent \u0026amp; Magnet 🎞️ 支持 M3U8 \u0026amp; MPD 格式视频下载  支持的站点Sites    Site URL     AfreecaTV https://afreecatv.com   ArtStation https://artstation.com   AsianSister https://asiansister.com   AsmHentai https://asmhentai.com   Avgle https://avgle.com   baraag.net https://baraag.net   半次元 https://bcy.net   BDSMlr https://bdsmlr.com   bilibili https://bilibili.","title":"Hitomi|下载图像/视频/音乐/文本等的桌面实用程序"},{"content":"目前博客有以下几个功能特色\n Hugo 框架建站 PaperMod网站主题风格 MkDocs 生成技术文档 utterances 留言功能 浏览器首页 学术浏览器首页  网站仓库 博客的所有代码文件存储于hiDaDeng/hidadeng.github.io。大家如果想爬大邓的博客，速度还是太慢，可以直接从这里下载项目。 主题风格 博客之前换过很多种主题，有商务风、极客风，最后发现还是简单最好。而PaperMod不止简洁，还支持标签、搜索等功能。 技术文档 大邓课程培训Python快速入门基础教程，使用MkDocs框架生成技术文档，界面如图 留言功能 博客使用utterances调用github仓库资源，用于存储评论数据。评论系统有点慢，需要先有github账户才可以使用。日常大家如果对Python感兴趣，有什么好的想法、资料，欢迎在博客中留言。 学术浏览器首页 如果你也是经管背景，对Python感兴趣，可以点击收藏该学术首页\n如果想生成自己学科的学术首页，可以点击制作方法查看diy详情\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/blog_add_comments_function/","summary":"目前博客有以下几个功能特色\n Hugo 框架建站 PaperMod网站主题风格 MkDocs 生成技术文档 utterances 留言功能 浏览器首页 学术浏览器首页  网站仓库 博客的所有代码文件存储于hiDaDeng/hidadeng.github.io。大家如果想爬大邓的博客，速度还是太慢，可以直接从这里下载项目。 主题风格 博客之前换过很多种主题，有商务风、极客风，最后发现还是简单最好。而PaperMod不止简洁，还支持标签、搜索等功能。 技术文档 大邓课程培训Python快速入门基础教程，使用MkDocs框架生成技术文档，界面如图 留言功能 博客使用utterances调用github仓库资源，用于存储评论数据。评论系统有点慢，需要先有github账户才可以使用。日常大家如果对Python感兴趣，有什么好的想法、资料，欢迎在博客中留言。 学术浏览器首页 如果你也是经管背景，对Python感兴趣，可以点击收藏该学术首页\n如果想生成自己学科的学术首页，可以点击制作方法查看diy详情\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"博客已更新，现支持留言功能"},{"content":"doing_the_PhD https://github.com/shengyp/doing_the_PhD\n作者认为，博士旅程是一段围绕个人认知、能力与身心而开展的难得的修行。在这当中，内涵丰富、切实有效的指导性资源有益于将博士生们的独孤求索旅程带入新的境地，使其能够更有准备，更加勇敢，更持有信心地去面对随读博选择而伴生的不同研究任务与多方面的挑战，最终得以迭代升级，顺利修行。\n作者将上述提到的指导性资源划分为： [高考模式]、 [中美教育]、 [自救指南]、 [科学问题]、 [一个执着的决定]、 [如何开个好头]、 [好的研究想法如何产生]、 [如何读论文]、 [如何做实验]、 [如何写论文]、 [ArXiv]、 [如何投论文]、 [如何做Rebuttal]、 [如何审论文]、 [同行评审]、 [审稿状态]、 [如何做报告]、 [顶会存在的意义]、 [博士生的导师]、 [读博期间]、 [延期毕业问题]、 [博士学位的重要性]、 [博士学位论文]、 [学位论文答辩]、 [学位论文致谢]、 [毕业典礼]、 [高校排行榜]、 [名校光环]、 [学术界和工业界]、 [博士婚姻研究]、 [AI论文调研]、 [中文期刊]、 [期刊评价]、 [科研政策]、 [科研经费管理]、 [导师招生神奇要求]、 [我国博士研究生累积招生情况]、 [我国博士研究生培养结构]、 [有多“内卷”]、 [科研故事]、 [大厂经历]、 [灵魂拷问]、 [被博士后割韭菜了没]、 [洋博士们]、 [谁说科学没有国界]、 [毕业生的去向]、 [如何指导学生]、 [青椒的苦恼]、 [贤内助]、 [科技成果评价]、 [高校岗位设置与评价体制]、 [国家自然科学基金]、 [国家重点研发计划]、 [重大研究计划]、 [国家科技部政策]、 [重庆市科技政策]、 [教师法]、 [教师待遇]、 [谨慎破除“唯论文”现象]、 [把论文写在祖国大地上]、 [这些事不干]、 [毛主席话语]、 [严正声明]、 [症状]、 [英才陨落]、 [海里游荡]数个主题，期待更多的研究人员关注并给予建议。\nslidev https://cn.sli.dev/guide/why.html\n为开发者打造的演示文稿工具;Slidev 通过分离内容和视觉效果来弥补这一点。这使你能够一次专注于一件事，同时也能够重复使用社区中的主题。Slidev 并不寻求完全取代其他幻灯片制作工具。相反，它专注于迎合开发者社区的需求。\nbig_screen https://github.com/TurboWay/big_screen\n数据大屏可视化\n构建我的被动收入 https://www.bmpi.dev/\n博客主题是实现被动收入博主定下现在用25w长期投资15年达到200w；\n博主自己是全栈技术流，之前有金融行业从业经历。最惊艳的是博主的被动收入公式\n终身学习（认知杠杆）* 全栈技术（时间杠杆）* 投资理财（财务杠杆） =\u0026gt; 被动收入（人生杠杆） china_area_mysql https://github.com/kakuilan/china_area_mysql\n中国5级行政区域mysql库;\n全部共 758049 条\n 大陆数据共679237 条,其中  省/直辖市 31 市/州 342 县/区 3348 乡/镇 42757 村/社区 632759   港澳台数据共78812 条,其中  省/特区 3 港澳辖区 33 台湾市/县 23 台湾区/镇 371 台湾街道/村 78384    表结构 CREATETABLE`cnarea_2020`(`id`mediumint(7)unsignedNOTNULLAUTO_INCREMENT,`level`tinyint(1)unsignedNOTNULLCOMMENT\u0026#39;层级\u0026#39;,`parent_code`bigint(14)unsignedNOTNULLDEFAULT\u0026#39;0\u0026#39;COMMENT\u0026#39;父级行政代码\u0026#39;,`area_code`bigint(14)unsignedNOTNULLDEFAULT\u0026#39;0\u0026#39;COMMENT\u0026#39;行政代码\u0026#39;,`zip_code`mediumint(6)unsignedzerofillNOTNULLDEFAULT\u0026#39;000000\u0026#39;COMMENT\u0026#39;邮政编码\u0026#39;,`city_code`char(6)NOTNULLDEFAULT\u0026#39;\u0026#39;COMMENT\u0026#39;区号\u0026#39;,`name`varchar(50)NOTNULLDEFAULT\u0026#39;\u0026#39;COMMENT\u0026#39;名称\u0026#39;,`short_name`varchar(50)NOTNULLDEFAULT\u0026#39;\u0026#39;COMMENT\u0026#39;简称\u0026#39;,`merger_name`varchar(50)NOTNULLDEFAULT\u0026#39;\u0026#39;COMMENT\u0026#39;组合名\u0026#39;,`pinyin`varchar(30)NOTNULLDEFAULT\u0026#39;\u0026#39;COMMENT\u0026#39;拼音\u0026#39;,`lng`decimal(10,6)NOTNULLDEFAULT\u0026#39;0.000000\u0026#39;COMMENT\u0026#39;经度\u0026#39;,`lat`decimal(10,6)NOTNULLDEFAULT\u0026#39;0.000000\u0026#39;COMMENT\u0026#39;纬度\u0026#39;,PRIMARYKEY(`id`),UNIQUEKEY`uk_code`(`area_code`)USINGBTREE,KEY`idx_parent_code`(`parent_code`)USINGBTREE)ENGINE=MyISAMDEFAULTCHARSET=utf8COMMENT=\u0026#39;中国行政地区表\u0026#39;;\nmdvideo https://github.com/linqian02/mdvideo\nMDvideo，是一个桌面软件，自动将 Markdown 文档转成一段视频。 文档里面的视频、音频、图片网址，都会抓取后插入视频，还可以根据文字生成人工语音的旁白朗读。\nTR-Reading-List https://github.com/blcuicall/TR-Reading-List\n中英文文本可读性论文\u0026amp;工具列表\nawesome-seo https://github.com/madawei2699/awesome-seo\n不定期更新Google SEO学习及实战技术\n程序员应该访问的最佳网站中文版 https://github.com/tuteng/Best-websites-a-programmer-should-visit-zh\n在学习CS的时候有一些你必须知道的有用的站点来获取通知为了你的技术储备和学习新知识。这里是一个你应该访问的不是非常全面的一些站点的列表，这个列表会不断更新，只要我能得到链接，你也可以通过添加你知道的来为此做出贡献\nPersonGraphDataSet https://github.com/liuhuanyong/PersonGraphDataSet\n人物图谱数据集，近十万的人物关系图谱事实数据库，通过人物关系抽取算法抽取+人工整理得出，可用于人物关系搜索、查询、人物关系多跳问答，以及人物关系推理等场景提供基础数据。\nChainKnowledgeGraph https://github.com/liuhuanyong/ChainKnowledgeGraph\n产业链知识图谱包括A股上市公司、行业和产品共3类实体，包括上市公司所属行业关系、行业上级关系、产品上游原材料关系、产品下游产品关系、公司主营产品、产品小类共6大类。 上市公司4,654家，行业511个，产品95,559条、上游材料56,824条，上级行业480条，下游产品390条，产品小类52,937条，所属行业3,946条。\nEmail-newsletter-RSS https://github.com/alaskasquirrel/Email-newsletter-RSS\n邮箱 📧 newsletter RSS 荟萃\nnewsletter-list https://github.com/chasays/newsletter-list\n有趣，免费的 newsletter\nWorkingTime https://github.com/Robin970822/WorkingTime\n中国民间程序员自行搭建的作息数据平台，可了解各行业从业者的作息时间。\nSVG https://svgporn.com/\nSVG 意为可缩放矢量图形（Scalable Vector Graphics）,在放大或改变尺寸的情况下其图形质量不会有所损失。\n这个网站有常用的svg logo可供下载\n\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly11/","summary":"doing_the_PhD https://github.com/shengyp/doing_the_PhD\n作者认为，博士旅程是一段围绕个人认知、能力与身心而开展的难得的修行。在这当中，内涵丰富、切实有效的指导性资源有益于将博士生们的独孤求索旅程带入新的境地，使其能够更有准备，更加勇敢，更持有信心地去面对随读博选择而伴生的不同研究任务与多方面的挑战，最终得以迭代升级，顺利修行。\n作者将上述提到的指导性资源划分为： [高考模式]、 [中美教育]、 [自救指南]、 [科学问题]、 [一个执着的决定]、 [如何开个好头]、 [好的研究想法如何产生]、 [如何读论文]、 [如何做实验]、 [如何写论文]、 [ArXiv]、 [如何投论文]、 [如何做Rebuttal]、 [如何审论文]、 [同行评审]、 [审稿状态]、 [如何做报告]、 [顶会存在的意义]、 [博士生的导师]、 [读博期间]、 [延期毕业问题]、 [博士学位的重要性]、 [博士学位论文]、 [学位论文答辩]、 [学位论文致谢]、 [毕业典礼]、 [高校排行榜]、 [名校光环]、 [学术界和工业界]、 [博士婚姻研究]、 [AI论文调研]、 [中文期刊]、 [期刊评价]、 [科研政策]、 [科研经费管理]、 [导师招生神奇要求]、 [我国博士研究生累积招生情况]、 [我国博士研究生培养结构]、 [有多“内卷”]、 [科研故事]、 [大厂经历]、 [灵魂拷问]、 [被博士后割韭菜了没]、 [洋博士们]、 [谁说科学没有国界]、 [毕业生的去向]、 [如何指导学生]、 [青椒的苦恼]、 [贤内助]、 [科技成果评价]、 [高校岗位设置与评价体制]、 [国家自然科学基金]、 [国家重点研发计划]、 [重大研究计划]、 [国家科技部政策]、 [重庆市科技政策]、 [教师法]、 [教师待遇]、 [谨慎破除“唯论文”现象]、 [把论文写在祖国大地上]、 [这些事不干]、 [毛主席话语]、 [严正声明]、 [症状]、 [英才陨落]、 [海里游荡]数个主题，期待更多的研究人员关注并给予建议。","title":"TechWeekly-11| 每周有趣有用的技术分享"},{"content":" 点击上方图片购买课程    src: https://mp.weixin.qq.com/s/irYCS5gQVNu9vidqMy-WJA\n 有些时候，为了设定手机铃声或者发抖音视频，我们会耗费大量时间在剪辑音乐高潮部分上。那么这个音乐高潮的提取能不能自动化呢？当然可以。\n先来听听效果，孤芳自赏提取高潮后的部分：\n怎么样，是不是迫不及待想往下读了？不要急，让我们从原理开始慢慢讲起。\n原理简介 不知道大家有没有这样的体会，大部分时候，歌曲的高潮部分通常是重复次数最多的部分。因此我们可以根据这一个特征，提出我们的算法：\n 遍历整首歌曲。 将选定长度的部分与其他部分比较并计算相似度，以查看是否重复。 寻找重复次数较大、且间隔长的片段。  代码编写 为了避免造轮子，我们找到了别人已经做过的类似的项目：https://github.com/vivjay30/pychorus\n我们只需要分析这个源代码中最核心的部分，即求相似区段的源代码，就能知道它是不是符合我们的项目需求了：\n可以看到，这部分代码就是做了我们算法的第二步，进行了片段与片段之间的相似度计算。检测时用到的相似函数是这样的：\n这主要是因为歌曲由12个基本音符的帧的集合而组成，v1和v2是任意两段音乐的音符矢量，如果说两段音乐非常相似，那么右边的式子将接近于0. 如果说 1-右边的式子 得分非常高，则说明两段音乐非常相似。\n下面我们看看怎么使用这个项目求音乐高潮部分，其实非常简单。\n安装 pip3 install pychorus \n编写代码 实际上，这个包用起来可是相当简单，如果我们只是想单纯提取歌曲高潮部分：\nfrom pychorus import find_and_output_chorus chorus_start_sec = find_and_output_chorus(\u0026#34;你的音乐文件\u0026#34;, \u0026#34;提取结果的目标路径\u0026#34;, 要多少秒的高潮部分) 没错，两行代码就解决了。如果你想知道一些详细的细节，比如说输出相似矩阵或者结果可视化，建议阅读github中该项目的操作指令。下面让我们检验一下效果。\n效果检验 以《孤芳自赏》 为例，让我们试试这个提取器的功力。\n原曲：\n编写代码：\n# 提取音乐高潮部分 from pychorus import find_and_output_chorus input_file = \u0026#34;孤芳自赏.mp3\u0026#34; output_file = \u0026#34;孤芳自赏_high.wav\u0026#34; clip_length=40 chorus_start_sec = find_and_output_chorus(input_file, output_file, clip_length) \n数据挖掘课  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/pychorus/","summary":"点击上方图片购买课程    src: https://mp.weixin.qq.com/s/irYCS5gQVNu9vidqMy-WJA\n 有些时候，为了设定手机铃声或者发抖音视频，我们会耗费大量时间在剪辑音乐高潮部分上。那么这个音乐高潮的提取能不能自动化呢？当然可以。\n先来听听效果，孤芳自赏提取高潮后的部分：\n怎么样，是不是迫不及待想往下读了？不要急，让我们从原理开始慢慢讲起。\n原理简介 不知道大家有没有这样的体会，大部分时候，歌曲的高潮部分通常是重复次数最多的部分。因此我们可以根据这一个特征，提出我们的算法：\n 遍历整首歌曲。 将选定长度的部分与其他部分比较并计算相似度，以查看是否重复。 寻找重复次数较大、且间隔长的片段。  代码编写 为了避免造轮子，我们找到了别人已经做过的类似的项目：https://github.com/vivjay30/pychorus\n我们只需要分析这个源代码中最核心的部分，即求相似区段的源代码，就能知道它是不是符合我们的项目需求了：\n可以看到，这部分代码就是做了我们算法的第二步，进行了片段与片段之间的相似度计算。检测时用到的相似函数是这样的：\n这主要是因为歌曲由12个基本音符的帧的集合而组成，v1和v2是任意两段音乐的音符矢量，如果说两段音乐非常相似，那么右边的式子将接近于0. 如果说 1-右边的式子 得分非常高，则说明两段音乐非常相似。\n下面我们看看怎么使用这个项目求音乐高潮部分，其实非常简单。\n安装 pip3 install pychorus \n编写代码 实际上，这个包用起来可是相当简单，如果我们只是想单纯提取歌曲高潮部分：\nfrom pychorus import find_and_output_chorus chorus_start_sec = find_and_output_chorus(\u0026#34;你的音乐文件\u0026#34;, \u0026#34;提取结果的目标路径\u0026#34;, 要多少秒的高潮部分) 没错，两行代码就解决了。如果你想知道一些详细的细节，比如说输出相似矩阵或者结果可视化，建议阅读github中该项目的操作指令。下面让我们检验一下效果。\n效果检验 以《孤芳自赏》 为例，让我们试试这个提取器的功力。\n原曲：\n编写代码：\n# 提取音乐高潮部分 from pychorus import find_and_output_chorus input_file = \u0026#34;孤芳自赏.mp3\u0026#34; output_file = \u0026#34;孤芳自赏_high.wav\u0026#34; clip_length=40 chorus_start_sec = find_and_output_chorus(input_file, output_file, clip_length) \n数据挖掘课  点击上方图片购买课程   点击进入详情页","title":"pychorus | 3行代码提取音乐高潮部分"},{"content":" 点击上方图片购买课程   用于读取和写入图像数据的 Python 库；\nImageio 提供了一系列 example images，可以通过使用 \u0026lsquo;\u0026lsquo;imageio:chelsea.png\u0026rsquo;\u0026rsquo; 之类的 URI 来使用。 如果您的系统上尚不存在这些图像，则会自动下载这些图像。 因此，下面的大多数示例应该可以正常工作。\n读取图片 最常用的读取图片功能\nimport imageio as iio im = iio.imread(\u0026#39;imageio:chelsea.png\u0026#39;) print(im.shape) 如果图片文件是gif动图(gif是由多个静态图片组成的动图)\nimport imageio as iio im = iio.get_reader(\u0026#39;cat.gif\u0026#39;) for frame in im: print(frame.shape) # Each frame is a numpy matrix 如果gif存储于内存中（二进制数）\nimport imageio as iio #image_bytes二进制数据 im = iio.get_reader(image_bytes, \u0026#39;.gif\u0026#39;) 读取网图 Imageio可以从文件名、文件对象、zip压缩文件夹、字节流、图片链接中读取图片\nimport imageio as iio import visvis as vv im = iio.imread(\u0026#39;http://upload.wikimedia.org/wikipedia/commons/d/de/Wikipedia_Logo_1.0.png\u0026#39;) vv.imshow(im)  注意: 有时候代码运行不出结果，可以尝试 imageio.imread(imageio.core.urlopen(url).read(), '.gif').\n 读取文件夹中的图片 一种常见的情况是您想要读取文件夹中的所有图像，例如 进行科学分析，或者因为这些都是您的训练示例。 假设该文件夹仅包含图像文件，您可以这样读取它\nimport imageio as iio from pathlib import Path images = list() for file in Path(\u0026#34;path/to/folder\u0026#34;).iterdir(): im = iio.imread(file) images.append(im) \n迭代视频中的图片 视频也是由图片组成的，通过每秒钟播放几十帧（视频由很多张图片组成），静态的图展示出视频效果。\nimport imageio as iio reader = iio.get_reader(\u0026#39;imageio:cockatoo.mp4\u0026#39;) #for循环迭代 for i, im in enumerate(reader): print(\u0026#39;Mean of frame %iis %1.1f\u0026#39; % (i, im.mean())) \n官方文档 更多功能请查看官方文档 https://github.com/imageio/imageio\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/imageio/","summary":"点击上方图片购买课程   用于读取和写入图像数据的 Python 库；\nImageio 提供了一系列 example images，可以通过使用 \u0026lsquo;\u0026lsquo;imageio:chelsea.png\u0026rsquo;\u0026rsquo; 之类的 URI 来使用。 如果您的系统上尚不存在这些图像，则会自动下载这些图像。 因此，下面的大多数示例应该可以正常工作。\n读取图片 最常用的读取图片功能\nimport imageio as iio im = iio.imread(\u0026#39;imageio:chelsea.png\u0026#39;) print(im.shape) 如果图片文件是gif动图(gif是由多个静态图片组成的动图)\nimport imageio as iio im = iio.get_reader(\u0026#39;cat.gif\u0026#39;) for frame in im: print(frame.shape) # Each frame is a numpy matrix 如果gif存储于内存中（二进制数）\nimport imageio as iio #image_bytes二进制数据 im = iio.get_reader(image_bytes, \u0026#39;.gif\u0026#39;) 读取网图 Imageio可以从文件名、文件对象、zip压缩文件夹、字节流、图片链接中读取图片\nimport imageio as iio import visvis as vv im = iio.imread(\u0026#39;http://upload.wikimedia.org/wikipedia/commons/d/de/Wikipedia_Logo_1.0.png\u0026#39;) vv.","title":"Imageio | 读取和写入图像数据的Python库"},{"content":"Miller https://github.com/johnkerl/miller\nMiller 是一个命令行工具，用于查询、整形和重新格式化各种格式的数据文件，包括 CSV、TSV 和 JSON。\nTTS https://github.com/coqui-ai/TTS\nTTS 是一个用于高级文本到语音生成的库,可以根据你一段录音风格，迁移到其他任意语言文本，并将文本转为声音。\nexa https://the.exa.website/\n命令行ls的现代替代品。\nexa是一个改进的文件列表器，具有更多功能和更好的默认设置。它使用颜色来区分文件类型和元数据。它了解符号链接、扩展属性和Git。它很小，速度很快，只有一个二进制。\nLearn X in Y minutes https://learnxinyminutes.com/\n在 Y 分钟内学习 X; 来一次您最喜欢的语言的旋风之旅。\nDoodleCSS https://github.com/chr15m/DoodleCSS\n手绘风的html/css主题\nWiki https://www.modernwiki.app/\n现代维基百科, 重新设计的用户界面\nmatplotlib-cheatsheet 分初、中、高级，常用的matplotlib语法\n点击下载cheatsheet\nrssproxy https://rssproxy-v1.migor.org/\n将任意网址转为RSS源\nFluent-Reader 使用 Electron、React 和 Fluent UI 构建的现代桌面 RSS 阅读器\nhttps://github.com/yang991178/fluent-reader/\nddddocr https://github.com/sml2h3/ddddocr\n通用验证码识别OCR pypi版\nimport ddddocr ocr = ddddocr.DdddOcr(old=True) with open(\u0026#34;test.jpg\u0026#34;, \u0026#39;rb\u0026#39;) as f: image = f.read() res = ocr.classification(image) print(res) \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly10/","summary":"Miller https://github.com/johnkerl/miller\nMiller 是一个命令行工具，用于查询、整形和重新格式化各种格式的数据文件，包括 CSV、TSV 和 JSON。\nTTS https://github.com/coqui-ai/TTS\nTTS 是一个用于高级文本到语音生成的库,可以根据你一段录音风格，迁移到其他任意语言文本，并将文本转为声音。\nexa https://the.exa.website/\n命令行ls的现代替代品。\nexa是一个改进的文件列表器，具有更多功能和更好的默认设置。它使用颜色来区分文件类型和元数据。它了解符号链接、扩展属性和Git。它很小，速度很快，只有一个二进制。\nLearn X in Y minutes https://learnxinyminutes.com/\n在 Y 分钟内学习 X; 来一次您最喜欢的语言的旋风之旅。\nDoodleCSS https://github.com/chr15m/DoodleCSS\n手绘风的html/css主题\nWiki https://www.modernwiki.app/\n现代维基百科, 重新设计的用户界面\nmatplotlib-cheatsheet 分初、中、高级，常用的matplotlib语法\n点击下载cheatsheet\nrssproxy https://rssproxy-v1.migor.org/\n将任意网址转为RSS源\nFluent-Reader 使用 Electron、React 和 Fluent UI 构建的现代桌面 RSS 阅读器\nhttps://github.com/yang991178/fluent-reader/\nddddocr https://github.com/sml2h3/ddddocr\n通用验证码识别OCR pypi版\nimport ddddocr ocr = ddddocr.DdddOcr(old=True) with open(\u0026#34;test.jpg\u0026#34;, \u0026#39;rb\u0026#39;) as f: image = f.read() res = ocr.classification(image) print(res)","title":"TechWeekly-10| 每周有趣有用的技术分享"},{"content":"Tool_Kits 工具箱大全,主要是Python项目。\n涵盖：\n 网络爬虫 数据库 数据分析 机器学习 可视化 文本分析 GUI 自动化办公 其他  网络爬虫  requests 最好用的网络爬虫访问库 smartscraper 最简单的网络爬虫访问\u0026amp;解析库 weibo_crawler 最简单的微博爬虫 崔庆才大神发布的测试站点 gerapy-auto-extractor 爬虫页面智能解析库 TikTok-Api 抖音国际站爬虫库 rpa Python自动化操纵包 celery 可以用于制作爬虫访问队列 BeautifulSoup 最简单的网页解析库 pyquery 最简洁网页解析库 scrapy 最流行的爬虫框架 pyspider 国人开发的爬虫框架 selenium 浏览器自动化测试框架，可以用于爬虫反爬 playwright 微软开源的浏览器自动化测试框架 scylla 智能IP代理池，用于反爬 shreport 上海证券交易所上市公司定期报告下载 newspaper 新闻爬虫库，根据提供的url可以抽取出新闻标题、作者、关键词、总结，部分功能支持中文 ddddocr 通用验证码识别OCR pypi版  Web  pelican Python静态网站生成库 flask 可以开发网站、分享rest-api接口;流行度top2的web框架 streamlit、PyWebIO对Python小白最友好的的web库 fastapi web框架，高性能，易于学习，快速编写代码； PyWebIO 不需要编写HTML和JS代码，就可以构建简单的基于浏览器的GUI应用。 mkdocs 制作文档网站   数据库  PyMySQL Sqlite3 轻量级sql数据库(python内置库) pymongo 非关系型MongoDB库 redis Redis数据库 py2neo 对接Neo4J数据库的python库 datasette 探索和发布数据的开源多功能工具，主要面向数据记者、博物馆馆长、档案管理员、地方政府、科学家、研究人员以及任何拥有希望与世界分享数据的人。  数据分析  pandas 必须Python数据分析库，读取文件、预处理数据、分析、存储 SciencePlots 科学绘图的Python工具包 Orchest 创建数据科学工作量的工具。Orchest是一款Web数据科学工具，可在文件系统上运行 statsmodels Python的统计计量统计库 linearmodels 添加线性模型，包括statsmodels中缺少的工具变量和面板数据模型。 streamlit 快速搭建本地数据分析类Web应用 modin pandas加速库，接口语法与pandas高度一致 dask pandas加速库，接口语法与pandas高度一致 plydata pandas管道语法库 networkx 社交网络分析库  ​\n机器学习  vowpal wabbit 机器学习的前沿库 scikit-learn 机器学习必学库，支持有监督、无监督多种算法，含文本分析功能 Orange3 点击操作的机器学习分析软件， 可文本分析 doccano 文本数据标注工具 label-studio 最牛掰的文本数据标注工具  可视化  streamlit 快速搭建本地数据分析类Web应用 matplotlib Python中最万能绘图库，很少有ta画不出来的图；但语法较难、静态图 matplotx Matplotlib扩展库，可以提供更多样式，简化样式设定 seaborn 基于matplotlib开发的简化版可视化库， 一般的图可以用ta绘制； 高度定制仍需要结合matplotlib进行样式定制；静态图 plotnine ggplot2语法的Python可视化库， 可与plydata 库结合使用 pyecharts 国人开发并封装的动态可视化图绘制库; 中文文档 plotly 动态可视化图绘制库 bokeh 动态可视化图绘制库 SciencePlots 科研论文绘图，基于matplotlib datapane 数据分析报告生成 superset 开源商务智能分析可视化库 pyplutchik 文本可视化，可将文本情感信息按照plutchik轮样式可视化  文本分析  nltk 自然语言分析套件，对中文不友好 skift 使用scikit-learn语法封装了fastText功能的包。 kwx Python 中基于 BERT、LDA 和 TFIDF 的关键字提取 spacy 工业级自然语言模型库，支持中文 jieba 中文文本分词库 snownlp 中文情感分析库 gensim 最好用、最全的话题模型 cntext 中文文本分析库，含词频统计、情感分析、可视化 label-studio 最牛掰的文本数据标注工具 doccano 文本数据标注工具 textstat 文本可读性计算包(算法全，但仅支持英文) texthero 文本预处理、展示、可视化库，仅支持英文 textpipe 文本分析流水线 textplot 词语网络图 shifterator 通过让您查看单词使用方式的变化，单词移位可以帮助您进行从根本上更可解释的情感，熵和散度分析。量化不同单词对两个文本差异做出的贡献，以及它们如何发挥作用。 GuidedLDA 半监督LDA主题模型 corex_topic 层次非监督、半监督话题模型 BERTopic BERT话题模型 whatlies 词向量可视化 TextDescriptives 文本描述性统计,不支持中文 pdfdocx pdf、docx读取库 OCRmyPDF 为扫描的 PDF 文件添加了 OCR 文本层，允许对其进行搜索 Top2Vec 主题建模和语义搜索的算法, 自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。 适用于短文本; TextNet textnet将文档集表示为文档和单词的网络,为文本分析与可视化提供了新的可能性。 taguette 免费开源的定性研究工具  GUI窗体软件开发  tkinter Python内置的gui库 PySimpleGUI 最简单的gui开发库 pyqt5、pyside 最牛掰的gui软件开发库 DearPyGui 易于使用且功能强大的Python GUI框架，它提供了DearImGui的包装。 PyWebIO 快速构建 Web 应用的 Python 工具 kivy star数高达14k的gui库   自动化办公  zmail 自动化收发邮件管理库 pywinauto Windows电脑自动化Python库 WeasyPrint 自动化生产pdf报告  对PDF文件读取、更改、添加信息 selenium 浏览器自动化框架，可以自动化点击浏览器，完成某些工作 mkdocx python-docx 创建、修改docx文件库 python-ppt 创建、修改ppt文件库 openpyxl xlsx文件库 PyWebIO 不需要编写HTML和JS代码，就可以构建简单的基于浏览器的GUI应用。  其他  hiresearch 丢弃繁杂收藏夹，定义简洁办公的浏览器首页 reveal.js 最流行的幻灯片 slidev 编程人员使用的幻灯片 mkdocs 制作文档网站 mockoon 帮我们快速搭建 API 服务图形化界面工具 codepng 把代码转为美观的截图的website toad 金融风险评分卡；覆盖了建模全流程，从 EDA、特征工程、特征筛选 到 模型验证和评分卡转化 best-resume-ever Latex项目， 基于 Web 的简历模板，可以生成网页简历，然后用浏览器打印成 PDF 文件。 pychorus 将音频文件中的高潮部分剪辑出来的python包 imageio 用于读取和写入图像数据的 Python 库； rich 让命令行输出更美观简洁的Python包 textual rich作者开发的文本用户界面用户  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/python_tools/","summary":"Tool_Kits 工具箱大全,主要是Python项目。\n涵盖：\n 网络爬虫 数据库 数据分析 机器学习 可视化 文本分析 GUI 自动化办公 其他  网络爬虫  requests 最好用的网络爬虫访问库 smartscraper 最简单的网络爬虫访问\u0026amp;解析库 weibo_crawler 最简单的微博爬虫 崔庆才大神发布的测试站点 gerapy-auto-extractor 爬虫页面智能解析库 TikTok-Api 抖音国际站爬虫库 rpa Python自动化操纵包 celery 可以用于制作爬虫访问队列 BeautifulSoup 最简单的网页解析库 pyquery 最简洁网页解析库 scrapy 最流行的爬虫框架 pyspider 国人开发的爬虫框架 selenium 浏览器自动化测试框架，可以用于爬虫反爬 playwright 微软开源的浏览器自动化测试框架 scylla 智能IP代理池，用于反爬 shreport 上海证券交易所上市公司定期报告下载 newspaper 新闻爬虫库，根据提供的url可以抽取出新闻标题、作者、关键词、总结，部分功能支持中文 ddddocr 通用验证码识别OCR pypi版  Web  pelican Python静态网站生成库 flask 可以开发网站、分享rest-api接口;流行度top2的web框架 streamlit、PyWebIO对Python小白最友好的的web库 fastapi web框架，高性能，易于学习，快速编写代码； PyWebIO 不需要编写HTML和JS代码，就可以构建简单的基于浏览器的GUI应用。 mkdocs 制作文档网站   数据库  PyMySQL Sqlite3 轻量级sql数据库(python内置库) pymongo 非关系型MongoDB库 redis Redis数据库 py2neo 对接Neo4J数据库的python库 datasette 探索和发布数据的开源多功能工具，主要面向数据记者、博物馆馆长、档案管理员、地方政府、科学家、研究人员以及任何拥有希望与世界分享数据的人。  数据分析  pandas 必须Python数据分析库，读取文件、预处理数据、分析、存储 SciencePlots 科学绘图的Python工具包 Orchest 创建数据科学工作量的工具。Orchest是一款Web数据科学工具，可在文件系统上运行 statsmodels Python的统计计量统计库 linearmodels 添加线性模型，包括statsmodels中缺少的工具变量和面板数据模型。 streamlit 快速搭建本地数据分析类Web应用 modin pandas加速库，接口语法与pandas高度一致 dask pandas加速库，接口语法与pandas高度一致 plydata pandas管道语法库 networkx 社交网络分析库  ​","title":"大邓整理的有用Python项目"},{"content":"代码下载 \ntomotopy简介？ tomotopy 是 tomoto（主题建模工具）的 Python 扩展，它是用 C++ 编写的基于 Gibbs 采样的主题模型库。支持的主题模型包括 LDA、DMR、HDP、MG-LDA、PA 和 HPA， 利用现代 CPU 的矢量化来最大化速度。\nhttps://github.com/bab2min/tomotopy\n下图中同样的数据集， tomotopy迭代200次，gensim迭代10次的情况下， tomotopy与gensim耗时对比图，由此可见tomotopy训练主题模型速度之快。 当前版本的 tomotopy 支持的主题模型包括\n 潜在狄利克雷分配（LDAModel） 标记的 LDA（LLDA 模型） 部分标记的 LDA（PLDA 模型） 监督LDA（SLDA模型） Dirichlet 多项回归 (DMRModel) 广义狄利克雷多项回归 (GDMRModel) 分层狄利克雷过程 (HDPModel) 分层LDA（HLDA模型） 多粒 LDA（MGLDA 模型） 弹珠盘分配（PAModel） 分层 PA (HPAModel) 相关主题模型（CTModel） 动态主题模型 (DTModel) 基于伪文档的主题模型（PTModel）。  安装 !pip3 install tomotopy==0.12.2 !pip3 install pyLDAvis==3.3.1 目前，tomotopy 可以利用 AVX2、AVX 或 SSE2 SIMD 指令集来最大程度利用PC的性能。\nimport tomotopy as tp tp.isa Run\n'avx2'  如果 tp.isa 返回 None，则训练过程可能需要很长时间。\n1. 导入数据 准备一个自己很熟悉的数据disaster_news.csv，一共有332条，话题数K=5，（正常情况下K是需要探索的）。\nimport pandas as pd df = pd.read_csv(\u0026#39;disaster_news.csv\u0026#39;) df.head() 2. 整理数据 分词、去除停用词\nimport re import jieba from cntext import STOPWORDS_zh def segment(text): words = jieba.lcut(text) words = [w for w in words if w not in STOPWORDS_zh] return words test = \u0026#34;云南永善县级地震已致人伤间民房受损中新网月日电据云南昭通市防震减灾局官方网站消息截至日时云南昭通永善县级地震已造成人受伤其中重伤人轻伤人已全部送医院救治民房受损户间倒塌户间个乡镇所学校不同程度受损目前被损毁电力交通通讯设施已全部抢通修复当地已调拨帐篷顶紧急转移万人月日时分云南昭通永善县发生里氏级地震震源深度公里当地震感强烈此外成都等四川多地也有明显震感\u0026#34; print(segment(test)) ['云南', '永善县', '级', '地震', '已致', '伤间', '民房', '受损', '中新网', '日电', '云南', '昭通市', '防震', '减灾', '局', '官方网站', '消息', '日时', '云南', '昭通', '永善县', '级', '地震', '造成', '受伤', '重伤', '轻伤', '送', '医院', '救治', '民房', '受损', '户间', '倒塌', '户间', '乡镇', '学校', '不同', '程度', '受损', '目前', '损毁', '电力', '交通', '通讯', '设施', '抢通', '修复', '调拨', '帐篷', '顶', '紧急', '转移', '万人', '时分', '云南', '昭通', '永善县', '发生', '里氏', '级', '地震', '震源', '深度', '公里', '震感', '强烈', '成都', '四川', '多地', '明显', '震感']  df[\u0026#39;words\u0026#39;] = df[\u0026#39;text\u0026#39;].apply(segment) df.head() 3. 找到最佳K 正常的步骤应该认真对待这步，在一定区间范围内，根据模型得分找到合理的K。这里使用tomotopy提供的主题一致性coherence得分假装找一下。\n我们期望的图应该的topic coherence随着 number of topics增加而增加，然后到某个topic值趋于平稳。\ntomotopy每次运行得到的图形状不一样，为了保证运行结果具有可比性，设置了随机种子seed为555，你也可以根据需要改为自己需要的随机状态。(这里有点像炼丹)\ndef find_k(docs, min_k=1, max_k=20, min_df=2): #min_df 词语最少出现在2个文档中 import matplotlib.pyplot as plt scores = [] for k in range(min_k, max_k): #seed随机种子，保证在大邓这里运行结果与你运行的结果一样 mdl = tp.LDAModel(min_df=min_df, k=k, seed=555) for words in docs: if words: mdl.add_doc(words) mdl.train(20) coh = tp.coherence.Coherence(mdl) scores.append(coh.get_score()) #x = list(range(min_k, max_k - 1)) # 区间最右侧的值。注意：不能大于max_k #print(x) #print() plt.plot(range(min_k, max_k), scores) plt.xlabel(\u0026#34;number of topics\u0026#34;) plt.ylabel(\u0026#34;coherence\u0026#34;) plt.show() find_k(docs=df[\u0026#39;words\u0026#39;], min_k=1, max_k=10, min_df=2) 4. 训练lda 使用tomotopy的LDA模型， 话题数K=5\nimport tomotopy as tp #初始化LDA mdl = tp.LDAModel(k=5, min_df=2, seed=555) for words in df[\u0026#39;words\u0026#39;]: #确认words 是 非空词语列表 if words: mdl.add_doc(words=words) #训练 mdl.train() #查看每个topic feature words for k in range(mdl.k): print(\u0026#39;Top 10 words of topic #{}\u0026#39;.format(k)) print(mdl.get_topic_words(k, top_n=10)) print(\u0026#39;\\n\u0026#39;) Run\nTop 10 words of topic #0 [(\u0026#39;一辆\u0026#39;, 0.02751251682639122), (\u0026#39;事故\u0026#39;, 0.021704642102122307), (\u0026#39;记者\u0026#39;, 0.018342189490795135), (\u0026#39;死亡\u0026#39;, 0.01650812290608883), (\u0026#39;造成\u0026#39;, 0.014062701724469662), (\u0026#39;人员\u0026#39;, 0.013909862376749516), (\u0026#39;现场\u0026#39;, 0.013451346196234226), (\u0026#39;受伤\u0026#39;, 0.012687151320278645), (\u0026#39;相撞\u0026#39;, 0.011922957375645638), (\u0026#39;货车\u0026#39;, 0.011922957375645638)] ​ ​ Top 10 words of topic #1 ​ [(\u0026#39;学生\u0026#39;, 0.02709135226905346), (\u0026#39;食物中毒\u0026#39;, 0.02498047426342964), (\u0026#39;出现\u0026#39;, 0.019175563007593155), (\u0026#39;医院\u0026#39;, 0.016185153275728226), (\u0026#39;事件\u0026#39;, 0.013546556234359741), (\u0026#39;调查\u0026#39;, 0.013194743543863297), (\u0026#39;年月日\u0026#39;, 0.012842929922044277), (\u0026#39;治疗\u0026#39;, 0.012667023576796055), (\u0026#39;症状\u0026#39;, 0.011787491850554943), (\u0026#39;名\u0026#39;, 0.011259771883487701)] ​ ​ Top 10 words of topic #2 ​ [(\u0026#39;现场\u0026#39;, 0.018848909065127373), (\u0026#39;发生\u0026#39;, 0.01677251048386097), (\u0026#39;医院\u0026#39;, 0.015015557408332825), (\u0026#39;起火\u0026#39;, 0.014216942712664604), (\u0026#39;原因\u0026#39;, 0.012140544131398201), (\u0026#39;目前\u0026#39;, 0.012140544131398201), (\u0026#39;救治\u0026#39;, 0.01150165218859911), (\u0026#39;进行\u0026#39;, 0.011022482998669147), (\u0026#39;名\u0026#39;, 0.009425252676010132), (\u0026#39;火势\u0026#39;, 0.009265529923141003)] ​ ​ Top 10 words of topic #3 ​ [(\u0026#39;发生\u0026#39;, 0.03348556533455849), (\u0026#39;爆炸\u0026#39;, 0.022389251738786697), (\u0026#39;造成\u0026#39;, 0.019663840532302856), (\u0026#39;死亡\u0026#39;, 0.01713310182094574), (\u0026#39;受伤\u0026#39;, 0.016938429325819016), (\u0026#39;年月日\u0026#39;, 0.016354413703083992), (\u0026#39;轿车\u0026#39;, 0.012655640952289104), (\u0026#39;警方\u0026#39;, 0.012460969388484955), (\u0026#39;袭击\u0026#39;, 0.012266295962035656), (\u0026#39;事件\u0026#39;, 0.011487606912851334)] ​ ​ Top 10 words of topic #4 ​ [(\u0026#39;地震\u0026#39;, 0.047826822847127914), (\u0026#39;发生\u0026#39;, 0.03555167838931084), (\u0026#39;火灾\u0026#39;, 0.03140682727098465), (\u0026#39;时分\u0026#39;, 0.020885275676846504), (\u0026#39;级\u0026#39;, 0.015783920884132385), (\u0026#39;时间\u0026#39;, 0.013870910741388798), (\u0026#39;公里\u0026#39;, 0.013711493462324142), (\u0026#39;人员伤亡\u0026#39;, 0.013073823414742947), (\u0026#39;记者\u0026#39;, 0.013073823414742947), (\u0026#39;震感\u0026#39;, 0.012276736088097095)] \n查看话题模型信息\nmdl.summary() Run\n\u0026lt;BasicInfo\u0026gt;|LDAModel(currentversion:0.12.2)|332docs,29749words|TotalVocabs:8428,UsedVocabs:2984|Entropyofwords:7.10665|Entropyofterm-weightedwords:7.10665|RemovedVocabs:\u0026lt;NA\u0026gt;|\u0026lt;TrainingInfo\u0026gt;|Iterations:10,Burn-insteps:0|OptimizationInterval:10|Log-likelihoodperword:-7.79934|\u0026lt;InitialParameters\u0026gt;|tw:TermWeight.ONE|min_cf:0(minimumcollectionfrequencyofwords)|min_df:2(minimumdocumentfrequencyofwords)|rm_top:0(thenumberoftopwordstoberemoved)|k:5(thenumberoftopicsbetween1~32767)|alpha:[0.1](hyperparameterofDirichletdistributionfordocument-topic,givenasasingle`float`incaseofsymmetricpriorandasalistwithlength`k`of`float`incaseofasymmetricprior.)|eta:0.01(hyperparameterofDirichletdistributionfortopic-word)|seed:555(randomseed)|trainedinversion0.12.2|\u0026lt;Parameters\u0026gt;|alpha(Dirichletpriorontheper-documenttopicdistributions)|[0.71433650.68525130.750896160.62046770.7040125]|eta(Dirichletpriorontheper-topicworddistribution)|0.01|\u0026lt;Topics\u0026gt;|#0 (6513) : 一辆 事故 记者 死亡 造成 |#1 (5655) : 学生 食物中毒 出现 医院 事件 |#2 (6231) : 现场 发生 医院 起火 原因 |#3 (5107) : 发生 爆炸 造成 死亡 受伤 |#4 (6243) : 地震 发生 火灾 时分 级 topic解读 根据每个话题top10的特征词，5个话题解读为\n 交通事故| #0 (6513) : 一辆 事故 记者 死亡 造成 食品安全| #1 (5655) : 学生 食物中毒 出现 医院 事件 火灾新闻| #2 (6231) : 现场 发生 医院 起火 原因 恐怖袭击| #3 (5107) : 发生 爆炸 造成 死亡 受伤 地震灾害| #4 (6243) : 地震 发生 火灾 时分 级  5. 可视化 使用pyLDAvis\nimport pyLDAvis import numpy as np import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;, category=Warning) #在notebook显示 pyLDAvis.enable_notebook() #获取pyldavis需要的参数 topic_term_dists = np.stack([mdl.get_topic_word_dist(k) for k in range(mdl.k)]) doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs]) doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True) doc_lengths = np.array([len(doc.words) for doc in mdl.docs]) vocab = list(mdl.used_vocabs) term_frequency = mdl.used_vocab_freq prepared_data = pyLDAvis.prepare( topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, start_index=0, # tomotopy话题id从0开始，pyLDAvis话题id从1开始 sort_topics=False #注意：否则pyLDAvis与tomotopy内的话题无法一一对应。  ) #可视化结果存到html文件中 #pyLDAvis.save_html(prepared_data, \u0026#39;ldavis.html\u0026#39;) #notebook中显示 pyLDAvis.display(prepared_data) 6. 预测 预测某文档的话题\nimport jieba from cntext import STOPWORDS_zh #预测 doc = \u0026#39;云南永善县级地震已致伤间民房受损中新网日电云南昭通市防震减灾局官方网站消息日时云南昭通永善县级地震造成受伤重伤轻伤送医院救治民房受损户间倒塌户间乡镇学校不同程度受损目前损毁电力交通通讯设施抢通修复调拨帐篷顶紧急转移万人时分云南昭通永善县发生里氏级地震震源深度公里震感强烈成都四川多地明显震感\u0026#39; words = [w for w in jieba.lcut(doc) if w not in STOPWORDS_zh] #构造tomotopy需要的数据 doc_inst = mdl.make_doc(words=words) topic_dist, ll = mdl.infer(doc_inst) print(\u0026#34;Topic Distribution for Unseen Docs: \u0026#34;, topic_dist) Topic Distribution for Unseen Docs: [0.11645161 0.10240361 0.5342029 0.03622254 0.21071935]  列表长度为5， 列表第三个数值(topic #2)数值最大，该文本最大的可能性是topic #2\n补充: 指定主题特征词 如果对数据比较了解，已经知道有一些主题，可以把比较明显的词语分配给指定的topic_id。\nmdl = tp.LDAModel(k=5, min_df=2, seed=555) for words in df[\u0026#39;words\u0026#39;]: if words: mdl.add_doc(words) #把word相撞 分配给topic_0, 权重设置为1， 其他topic权重设置为0.1 #注意这里的range(5) 5是对应的k值 mdl.set_word_prior(\u0026#39;相撞\u0026#39;, [1.0 if k == 0 else 0.1 for k in range(5)]) #把word地震 分配给topic_1, 权重设置为1， 其他topic权重设置为0.1 mdl.set_word_prior(\u0026#39;地震\u0026#39;, [1.0 if k == 1 else 0.1 for k in range(5)]) #把word火灾 分配给topic_2, 权重设置为1， 其他topic权重设置为0.1 mdl.set_word_prior(\u0026#39;火灾\u0026#39;, [1.0 if k == 2 else 0.1 for k in range(5)]) #把word中毒 分配给topic_3, 权重设置为1， 其他topic权重设置为0.1 mdl.set_word_prior(\u0026#39;中毒\u0026#39;, [1.0 if k == 3 else 0.1 for k in range(5)]) #把word袭击 分配给topic_4, 权重设置为1， 其他topic权重设置为0.1 mdl.set_word_prior(\u0026#39;袭击\u0026#39;, [1.0 if k == 4 else 0.1 for k in range(5)]) mdl.train() mdl.summary() \u0026lt;Basic Info\u0026gt; | LDAModel (current version: 0.12.2) | 332 docs, 29749 words | Total Vocabs: 8428, Used Vocabs: 2984 | Entropy of words: 7.10665 | Entropy of term-weighted words: 7.10665 | Removed Vocabs: \u0026lt;NA\u0026gt; | \u0026lt;Training Info\u0026gt; | Iterations: 10, Burn-in steps: 0 | Optimization Interval: 10 | Log-likelihood per word: -7.72251 | \u0026lt;Initial Parameters\u0026gt; | tw: TermWeight.ONE | min_cf: 0 (minimum collection frequency of words) | min_df: 2 (minimum document frequency of words) | rm_top: 0 (the number of top words to be removed) | k: 5 (the number of topics between 1 ~ 32767) | alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.) | eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word) | seed: 555 (random seed) | trained in version 0.12.2 | \u0026lt;Parameters\u0026gt; | alpha (Dirichlet prior on the per-document topic distributions) | [0.7106193 0.60264444 0.5734784 0.71375024 0.6234263 ] | eta (Dirichlet prior on the per-topic word distribution) | 0.01 | \u0026lt;Topics\u0026gt; | #0 (6599) : 一辆 事故 死亡 发生 造成 | #1 (6087) : 地震 发生 级 公里 年月日 | #2 (5892) : 火灾 发生 现场 大火 起火 | #3 (6402) : 医院 学生 食物中毒 出现 名 | #4 (4769) : 事件 发生 袭击 人员 工作 |  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/tomotopy_is_the_fastest_topic_model/","summary":"代码下载 \ntomotopy简介？ tomotopy 是 tomoto（主题建模工具）的 Python 扩展，它是用 C++ 编写的基于 Gibbs 采样的主题模型库。支持的主题模型包括 LDA、DMR、HDP、MG-LDA、PA 和 HPA， 利用现代 CPU 的矢量化来最大化速度。\nhttps://github.com/bab2min/tomotopy\n下图中同样的数据集， tomotopy迭代200次，gensim迭代10次的情况下， tomotopy与gensim耗时对比图，由此可见tomotopy训练主题模型速度之快。 当前版本的 tomotopy 支持的主题模型包括\n 潜在狄利克雷分配（LDAModel） 标记的 LDA（LLDA 模型） 部分标记的 LDA（PLDA 模型） 监督LDA（SLDA模型） Dirichlet 多项回归 (DMRModel) 广义狄利克雷多项回归 (GDMRModel) 分层狄利克雷过程 (HDPModel) 分层LDA（HLDA模型） 多粒 LDA（MGLDA 模型） 弹珠盘分配（PAModel） 分层 PA (HPAModel) 相关主题模型（CTModel） 动态主题模型 (DTModel) 基于伪文档的主题模型（PTModel）。  安装 !pip3 install tomotopy==0.12.2 !pip3 install pyLDAvis==3.3.1 目前，tomotopy 可以利用 AVX2、AVX 或 SSE2 SIMD 指令集来最大程度利用PC的性能。\nimport tomotopy as tp tp.","title":"tomotopy | 速度最快的LDA主题模型"},{"content":"dvt Distant Viewing Toolkit for the Analysis of Visual Culture\n视觉文化分析的Python工具包\nhttps://github.com/distant-viewing/dvt\n远程电视工具包由几个 Python 包组成，旨在促进视觉文化的计算分析。 开始使用该工具包的最简单方法是在 Google 的协作 (Colab) 环境中运行该工具包。 这是一项免费使用的服务，允许您以最少的设置在远程服务器上运行 Python 代码。 只需选择以下链接之一即可开始：\n COLAB 笔记本演 https://colab.research.google.com/drive/1KxYziaozONxMZH8uUaf4PxbW6DyNiZ0k?usp=sharing  有关在您自己的机器上设置工具包的更多信息，请参阅 INSTALL.md。 以下页面提供了有关工具包和项目的更多信息：\n 搜索和发现界面示例：DVT 视频可视化 使用聚合元数据的示例分析：“两个网络时代情景喜剧的视觉风格” 项目理论：“远距离观察：分析大型视觉语料库”。 软件白皮书：用于分析视觉文化的 Python 包  示例 开发者提供的colab代码，我跑通了图片标注(例如识别不同身体部位)、视频转场(不同素材拼凑)\n   提示 本地配置难度极大，建议按照开发者提示，使用google colab环境。可能需要先配置好科学上网，才能使用\n COLAB 笔记本演 https://colab.research.google.com/drive/1KxYziaozONxMZH8uUaf4PxbW6DyNiZ0k?usp=sharing  \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/dvt/","summary":"dvt Distant Viewing Toolkit for the Analysis of Visual Culture\n视觉文化分析的Python工具包\nhttps://github.com/distant-viewing/dvt\n远程电视工具包由几个 Python 包组成，旨在促进视觉文化的计算分析。 开始使用该工具包的最简单方法是在 Google 的协作 (Colab) 环境中运行该工具包。 这是一项免费使用的服务，允许您以最少的设置在远程服务器上运行 Python 代码。 只需选择以下链接之一即可开始：\n COLAB 笔记本演 https://colab.research.google.com/drive/1KxYziaozONxMZH8uUaf4PxbW6DyNiZ0k?usp=sharing  有关在您自己的机器上设置工具包的更多信息，请参阅 INSTALL.md。 以下页面提供了有关工具包和项目的更多信息：\n 搜索和发现界面示例：DVT 视频可视化 使用聚合元数据的示例分析：“两个网络时代情景喜剧的视觉风格” 项目理论：“远距离观察：分析大型视觉语料库”。 软件白皮书：用于分析视觉文化的 Python 包  示例 开发者提供的colab代码，我跑通了图片标注(例如识别不同身体部位)、视频转场(不同素材拼凑)\n   提示 本地配置难度极大，建议按照开发者提示，使用google colab环境。可能需要先配置好科学上网，才能使用\n COLAB 笔记本演 https://colab.research.google.com/drive/1KxYziaozONxMZH8uUaf4PxbW6DyNiZ0k?usp=sharing  \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"dvt库 |  视觉文化分析的Python工具包"},{"content":"代码下载 Stargazer库介绍 R语言有一个stargazer包，可用于创建漂亮的、可发表的多元回归表。如今有Python化的stargazer库也可做类似的事。\n下图是使用stargazer生成的没有任何样式的原始示例的示例：\n 什么时候会用到这些？ 人们倾向于使用R版本的stargazer的主要情况是在学术论文中报告回归结果。 它允许您轻松地比较多个回归结果，这有助于比较具有实验影响的模型与没有影响的模型之间的结果。 这允许用户轻松查看系数的差异、统计显着性以及实验引入的新变量的影响。\n它目前支持 LaTeX 和 HTML 输出，stargazer开发者最终最表是也支持 Markdown 和 ASCII 文本。\n项目地址 https://github.com/mwburke/stargazer\n该库实现了原始包中的许多自定义功能。大多数示例可以在示例 jupyter notebook 中找到，功能函数完整列表如下： Stargazer库的函数  show_header：显示或隐藏模型头数据 show_model_numbers：显示或隐藏型号 custom_columns：自定义模型名称和模型分组 significance_levels：更改统计显着性阈值 significant_digits：更改有效数字的数量 show_confidence_intervals：显示置信区间，而不是方差 dependent_variable_name：重命名因变量 rename_covariates: 重命名协变量 covariate_order：重新排序协变量 reset_covariate_order：将协变量顺序重置为原始顺序 show_degrees_of_freedom：显示或隐藏自由度 custom_note_label：表格底部的标签注释部分 add_custom_notes：将自定义注释添加到表格底部的部分 add_line：向表格中添加自定义行 append_notes：显示或隐藏统计显着性阈值  这些功能与渲染类型无关，无论用户以 HTML、LaTeX 等格式输出都将应用\n安装 !pip3 install stargazer \nOLS回归 import pandas as pd from sklearn import datasets import statsmodels.api as sm from stargazer.stargazer import Stargazer diabetes = datasets.load_diabetes() df = pd.DataFrame(diabetes.data) df.columns = [\u0026#39;Age\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;BMI\u0026#39;, \u0026#39;ABP\u0026#39;, \u0026#39;S1\u0026#39;, \u0026#39;S2\u0026#39;, \u0026#39;S3\u0026#39;, \u0026#39;S4\u0026#39;, \u0026#39;S5\u0026#39;, \u0026#39;S6\u0026#39;] df[\u0026#39;target\u0026#39;] = diabetes.target est = sm.OLS(endog=df[\u0026#39;target\u0026#39;], exog=sm.add_constant(df[df.columns[0:4]])).fit() est2 = sm.OLS(endog=df[\u0026#39;target\u0026#39;], exog=sm.add_constant(df[df.columns[0:6]])).fit() stargazer = Stargazer([est, est2]) /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only x = pd.concat(x[::order], 1)  #分析结果渲染成html from IPython.display import display, HTML raw_str = stargazer.render_html() html = HTML(raw_str) display(html)   #分析结果渲染成latex stargazer.render_latex()   \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/stargazer/","summary":"代码下载 Stargazer库介绍 R语言有一个stargazer包，可用于创建漂亮的、可发表的多元回归表。如今有Python化的stargazer库也可做类似的事。\n下图是使用stargazer生成的没有任何样式的原始示例的示例：\n 什么时候会用到这些？ 人们倾向于使用R版本的stargazer的主要情况是在学术论文中报告回归结果。 它允许您轻松地比较多个回归结果，这有助于比较具有实验影响的模型与没有影响的模型之间的结果。 这允许用户轻松查看系数的差异、统计显着性以及实验引入的新变量的影响。\n它目前支持 LaTeX 和 HTML 输出，stargazer开发者最终最表是也支持 Markdown 和 ASCII 文本。\n项目地址 https://github.com/mwburke/stargazer\n该库实现了原始包中的许多自定义功能。大多数示例可以在示例 jupyter notebook 中找到，功能函数完整列表如下： Stargazer库的函数  show_header：显示或隐藏模型头数据 show_model_numbers：显示或隐藏型号 custom_columns：自定义模型名称和模型分组 significance_levels：更改统计显着性阈值 significant_digits：更改有效数字的数量 show_confidence_intervals：显示置信区间，而不是方差 dependent_variable_name：重命名因变量 rename_covariates: 重命名协变量 covariate_order：重新排序协变量 reset_covariate_order：将协变量顺序重置为原始顺序 show_degrees_of_freedom：显示或隐藏自由度 custom_note_label：表格底部的标签注释部分 add_custom_notes：将自定义注释添加到表格底部的部分 add_line：向表格中添加自定义行 append_notes：显示或隐藏统计显着性阈值  这些功能与渲染类型无关，无论用户以 HTML、LaTeX 等格式输出都将应用\n安装 !pip3 install stargazer \nOLS回归 import pandas as pd from sklearn import datasets import statsmodels.api as sm from stargazer.stargazer import Stargazer diabetes = datasets.","title":"Stargazer库 |  创建漂亮可发表的多元回归表"},{"content":" 公众号-新智元\n编辑: 桃子 小咸鱼\n 女朋友提出分手，你是不是后悔没有早点察觉？\n这不，对于分手前的有关迹象，科学家给出了答案：聊天记录！\n近日，美国德克萨斯大学的研究人员发表的一篇论文表明，通过分析情侣的聊天记录，可以找到情侣即将分手的证据。\n这篇 Language left behind on social media exposes the emotional and cognitive costs of a romantic breakup 已于12月20日发表在国际顶刊PNAS。\n分手前三个月，聊天便不正常了\n基于已有的数据，研究人员对6800名Reddit用户发布的1027541个帖子进行了文本分析。\n这些帖子涵盖了用户在过去2年里的Reddit发帖数据，这些帖子的内容不仅仅与他们的感情关系有关，也涵盖了他们生活中各个方面的内容。\n语言标记 表示「即将分手」的「语言标记」在分手发生前3个月出现的频次非常高，在分手的那一周达到峰值，并在6个月后恢复到正常基线。\n在表示「即将分手」的「语言标记」中，出现次数比较多的是「我」、「我们」这类字眼，以及暗含认知过程（cognitive processing ）的词汇（常常表示抑郁、高度集中的注意力、探究意义等）。\n分手前后Reddit用户语言模式的变化。第0周是每个用户在分手时公开披露分手的时间点。\n此外，「语言标记」中有关「分析思考」（analytic thinking）的词汇的出现频次会下降，与「分析思考」相关的词汇往往包含更多以个人为中心和非正式的语言。\n研究人员还发现，即使人们在与分手这类话题无关的群组中发帖，上述这种「语言标记」模式仍然存在。\n要注意了！\n与偶尔发一次帖的人相比，那些经常发布分手信息的人在分手一年之后，适应能力会变得很差。\n83%的人都会以第一人称发送第一次分手的信息，并且这些帖子详细描述了分手过程，甚至导致分手的原因和分手的后果都有。\n例如，一位网友通过回忆来讲述自己分手的故事：\n “Hey breakups, going through a rough one this week. The girl I\u0026rsquo;ve been seeing the last 7 months left me last Friday due to us not having common interests. Our relationship seemed just awesome and thought we were happy. We did get into a \u0026ldquo;routine\u0026rdquo; pretty fast and I was happy with it. Cook dinner, sex, watch movies together. I knew going into this with her she was a free spirited outdoorsy type and I am admittedly the opposite. But we sparked, and formed a relationship after a few months of talking. Things seemed great. We lived maybe 45 minutes away from each other and had different work schedules, so we only saw each other maybe 3 times a week, so it was sometimes hard to see each other\u0026hellip;”\n 分析思考和认知过程 尽管许多情侣关系解体模型都强调了分手过程中固有的认知思维，但没有一项研究能够实时跟踪认知过程。\n同时，研究人员在研究认知过程的变化时面临着几个艰巨的挑战，包括如何识别和测量感兴趣的认知动态。\n研究人员最近的工作确定了两种基于语言的通用思维模式：一是分析思考，另一是认知过程。\n说话以「我」、「我们」为焦点\n回想下，排除第三者关系，女友和你提出分手前，是不是经常会说「我\u0026hellip;.」\n正如研究人员指出，当分手发生时，人们会向内去理解发生关系破裂的原因，这有时会让人陷入沉思和情绪困扰 。\n在与抑郁、自杀、情绪剧变、消极和心理困扰相关的报告中，「我」这个词是最常见的。这表明 「我」的使用能够捕获内部的焦点和个人内部的关注。\n同样，在情侣关系研究中，在分手之前、之中和之后查看「我」字的使用可能是一种不错的方法，来跟踪人们在整个分手过程中的对自我关注和调整。\n在分手期间，人们可能会深究对方的前任伴侣和两人间的关系。\n「我们」一词，揭示了情侣间的关系承诺、继续关系的意图和解决问题的行为等等信息。\n而情侣之间更多地使用「我们」一词突出了成功的浪漫关系背后靠的是情侣间的相互依存性。\n但是，如果情侣间的关系变坏呢？\n一些研究发现，经常分享分手故事的人，如果开始频繁地使用「我们」这个词，这种现象就预示着他和他伴侣间的关系会变得更差。\n分手后遗症 在人们的社交生活与其在线状态交织在一起的时代，研究分手和其他个人心情动荡出现了新的方法。\n通过研究社交媒体帖子，研究人员已经发现了与人们情绪和心理状态相关的语言模式，例如抑郁症、创伤后应激障碍诊断和注意缺陷多动障碍症状。\n通过对社交媒体平台中人们的语言进行分析，研究人员最终可以追踪人们在分手时不断演变的心理过程。\n正如研究者指出，真正分手后会持续6个月的心理影响。\n分手后遗症，你有吗？\n数据下载 4.8G,含代码\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pnas_love_separate/","summary":"公众号-新智元\n编辑: 桃子 小咸鱼\n 女朋友提出分手，你是不是后悔没有早点察觉？\n这不，对于分手前的有关迹象，科学家给出了答案：聊天记录！\n近日，美国德克萨斯大学的研究人员发表的一篇论文表明，通过分析情侣的聊天记录，可以找到情侣即将分手的证据。\n这篇 Language left behind on social media exposes the emotional and cognitive costs of a romantic breakup 已于12月20日发表在国际顶刊PNAS。\n分手前三个月，聊天便不正常了\n基于已有的数据，研究人员对6800名Reddit用户发布的1027541个帖子进行了文本分析。\n这些帖子涵盖了用户在过去2年里的Reddit发帖数据，这些帖子的内容不仅仅与他们的感情关系有关，也涵盖了他们生活中各个方面的内容。\n语言标记 表示「即将分手」的「语言标记」在分手发生前3个月出现的频次非常高，在分手的那一周达到峰值，并在6个月后恢复到正常基线。\n在表示「即将分手」的「语言标记」中，出现次数比较多的是「我」、「我们」这类字眼，以及暗含认知过程（cognitive processing ）的词汇（常常表示抑郁、高度集中的注意力、探究意义等）。\n分手前后Reddit用户语言模式的变化。第0周是每个用户在分手时公开披露分手的时间点。\n此外，「语言标记」中有关「分析思考」（analytic thinking）的词汇的出现频次会下降，与「分析思考」相关的词汇往往包含更多以个人为中心和非正式的语言。\n研究人员还发现，即使人们在与分手这类话题无关的群组中发帖，上述这种「语言标记」模式仍然存在。\n要注意了！\n与偶尔发一次帖的人相比，那些经常发布分手信息的人在分手一年之后，适应能力会变得很差。\n83%的人都会以第一人称发送第一次分手的信息，并且这些帖子详细描述了分手过程，甚至导致分手的原因和分手的后果都有。\n例如，一位网友通过回忆来讲述自己分手的故事：\n “Hey breakups, going through a rough one this week. The girl I\u0026rsquo;ve been seeing the last 7 months left me last Friday due to us not having common interests.","title":"PNAS | 情侣分手3个月前就有预兆！聊天记录还能反映分手后遗症"},{"content":" Author:小云哥哥\nSrc: https://zhuanlan.zhihu.com/p/386454664\n 历史语言学家有两个基本任务。第一个任务是追溯相关语言的源头——所谓的“原始语言”，说得通俗一点就是推测一下祖先们是怎么说话的。但其实除非有时光机器，否则我们不可能知道祖先的发音，构拟原始语言的最终目的是使用一个自洽的系统去解释现代亲属语言的差异。这个任务是技术活儿，需要硬功夫，语言学家必须对这些语言的各方面都了如指掌，通过多年的时间真正理解这些语言的运作（尤其是音系和形态），而不是一上来就开始尬比较。第二任务是第一个任务的衍生产品。当我们能够解释亲属语言的差异以后，我们自然而然会发现有的语言差异较小，有的语言差异较大，我们会希望知道每种差异发生的时间顺序，从而推断出亲属语言是因循什么轨迹从原始语言中分化出来的。这就是语言的谱系。\n虽然传统的历史语言学取得了非常大的成功，但是语言学家毕竟是人，他们用人力研究为数众多的语言、处理浩如烟海的语料，总会出现这样那样的问题。比如说我们会在不少著作中看到语言学家前后标准不一致，或者分析过程描述不清晰透明等现象。有时候这些问题并不是有意为之，而是因为人确实无法预估那么多的事情，出错在所难免。\n于是，一部分语言学家开始认识到，我们需要一个机器协助的、量化的转变。人的大脑爱耍小聪明，更擅长处理复杂而特殊的个案，而机器更像一个奴隶，可以帮人类用统一的方法处理繁多和重复的工作。那么，历史语言学家的两个任务，机器可以协助我们解决哪一个呢？原始语言的构拟还是语言的谱系？\n事实上，这两个任务都需要很多的小聪明。如果原始语言的构拟是简单的音位比较，那么机器也许可以很快做出来。但实际操作上要比这个绕很多的弯儿，需要语言学家综合知识的灵活运用。比如索绪尔的喉音理论，就需要对梵语动词变位的深入理解，从而比较不同变位模式的内在一致，并且对音变的类型学有融会贯通的了解。这一切的运作，可能在索绪尔的脑子里一秒钟就能形成，而机器则不可能在短时间内完成喉音的构拟。我们引入机器是为了提高效率，而不是降低效率。因此，语言学家把目光转向了第二个任务，语言的谱系。历史语言学确定谱系的唯一标准是共同创新，但辨认共同创新实际上也需要深厚的研究功底，机器很难按照人类的方式分析。一个比较可行的办法是偏离历史语言学的原则，使用统计学的方式，构造出在统计学意义上最可能的谱系。\n在这篇文章中，我就用流水账的方式梳理一下机器协助的语言谱系分析的相关历史，尤其专注于贝叶斯谱系分析。因为是流水账，所以不会分小节，我也会省去所有赶客的公式和理论描述。\n语言谱系分析 较早使用统计学处理语言谱系的研究可以追溯到十九世纪前叶。不过现代的尝试最值得提的是二十世纪二十年代开始的一系列操作。波兰人类学家Czekanowski[1]在1928年收集了二十多个音系、形态和词汇上的特点，研究了包括立陶宛语、古教会斯拉夫语、哥特语、古爱尔兰语、拉丁语、希腊语、吠陀梵语、阿维斯陀语和亚美尼亚语相互之间的关系。他们得出的结果其中一个错误是认为哥特语与波罗地-斯拉夫语更为接近，而不是意大利-凯尔特语。1937年，加州大学的Kroeber和Chrétien[2]在前人的基础上，添加了新的数据（主要是特征性的音变和形态变化方面的数据），使参与比较的特征达到了74个。下图是Kroeber and Chrétien (1937)的统计分析\n Kroeber和Chrétien就是通过判断每一个特征是否在各种语言中出现，列出矩阵计算出各语言的相似度。他们的结果，至少从这九种语言来看，基本上与历史语言学的结果相符。但由于数据本身的局限性，他们的方法并没有被大规模地使用，并且遭到了一些批评。我不知道他们的计算是否用了机器，但是从他们并不复杂的公式来看，可能是笔算的。\n虽然这些早期的尝试寿命并不长，但是也为量化历史语言学定下了统计学的基调，尽管在数据选取上，名义上是使用了历史语言学的结论，但是并没有使用历史语言学的分类标准，而是把这些结论转化成可以用于统计学的数据。这也是从这以后，直至现在将尽一个世纪的趋势。\n La linguistique est la science statistique type ; les statisticiens le savent bien ; la plupart des linguistes l\u0026rsquo;ignorent encore. (Guiraud 1959: 15[3]) 语言学就是典型的统计科学；统计学家心里很清楚，大部分语言学家却不知道。\n \u0026lt;比如\u0026gt;\n基于词汇的语言谱系分析 1950年代，有一个长得有点喜感的中年男人，叫Morris Swadesh。他是一个美国的语言学家。身为一个历史语言学家，他并不把关注的重点放在音系和形态的变化上，而是更专注于词汇。词汇相对于音系和形态，显然是更容易操作的东西，毕竟它们就像拼好的积木，能让人一眼就辨认出来。Morris Swadesh (1909-1967)\n Swadesh认为不同语言中词汇的重合度很可能与语系的演化有关。这点很符合我们的直觉，基因关系较远的语言中，非同源的词汇理应越多。而且，他还假设词汇系统是按一定的速率变化的，我们只要以这个速率为基础，然后比较亲属语言的同源词的多寡，就能得到语言的谱系，同时我们还可以算出亲属语言的分裂时间。Swadesh (1950)[4]认为词汇的变化速率是每过1000年，一种语言想对于原本形态的同源词就会降低到原来的85%。后来这个百分比又被改为81%。 这个数字大概是基于古英语和现代英语的词汇变化确定的。\n核心词汇 我们不可能穷尽所有的词汇，所以就需要选取一些具有代表性的词汇来简化我们的研究。Swadesh整理出一份100词的词表，现在我们称为“核心词汇”或者“基本词汇”，包括身体部位、数字、颜色、基本动作等类别，这些词汇被认为是最不容易被借用的，有较大的概率是本土词汇。\n语言断代学（词汇统计学） 这么一来，如果我们发现两种亲属语言在核心词汇表上有81%的同源词，那么我们就可以认为这两种语言的分化时间是1000年。如果它们有81%×81%=65.61%的同源词，那么它们的分化时间就是2000年。这个方法我们称为Glottochronology，汉语称作“语言断代学”，它也是“词汇统计学”（lexicostatistics）的最主要方法之一。\n为了让故事更连续，我在这里删除了其它的研究方法，比如计算词汇间Levenshtein距离，有关这方面的内容，可以看这个回答。\n总而言之，从Swadesh开始，量化历史语言学基本上就在词汇之间徘徊，人们开始想尽办法从词汇中找到语言发展的轨迹。当然， 也有从音系/形态上考虑的（Ringe et al 2002）[5]，还有从类型学上考虑的（Dunn et al 2008）[6]，但始终无法摆脱或者撼动以词汇为基础的大趋势。\n语言断代学虽然在语言谱系分析的量化上取得了较大的进展，但最终仍被认为是失败的方法。这是因为它强制规定词汇有着固定的变化速率。这一基本假设从直觉上就不符合语言的发展历程，而且没有靠谱的研究去证明，反而很容易被证伪。比如说，我们使用语言年代学的模型，我们会得到格鲁吉亚语和明格列尔语的分化年代距今约1000年左右。但实际上，它们两个的分化年代要远早于公元四到五世纪（Bergsland and Vogt 1962）[7]。Swadesh本人也觉得这个方法有问题。所以逐渐人们也就不再使用语言年代学了。\n语言断代学最大的贡献不在于它得出的结论有多正确，而是让人们看到为语言分化断代的希望，通过语言的年代来研究人类史前史成为了可能，历史语言学不再是自娱自乐地谈论古人怎么说话，而一跃成为了人类历史研究中最重要的学科之一。\n比如说，Gray and Jordan (2000)[8]使用简约分析（parsimony analysis）计算出南岛语的谱系，测试了有关南岛语系起源的两个假说，“快车假说（express-train）”和“岛屿纠缠假说（entangled bank）”。他们发现快车假说与南岛语谱系树惊人吻合，从而确认了南岛语是从台湾省起源，扩散到南部各个岛屿的。\n因为有了成功的希望，所以尽管有很多语言学家对语言的断代嗤之以鼻，另一部分语言学家仍旧在探索着新的道路。我们在这里将跳过一些不太受欢迎的研究方法，比如Ringe et al (2002)和Nakhleh et al (2005)[9]的“完全谱系”（perfect phylogenies）。他们跟二十世纪二三十年代的那帮人类似，使用了音系和形态方面的语料来计算，当然他们的统计学方法要先进得多。只不过，他们处理语料的方式跟前人一样，基本上就是看哪一些特征在哪一些语言中存在，并没有具体到确切的实例。\n语言变化的时钟-宽松时钟 谱系分析始终只是历史语言学家的其中一个任务，更多的历史语言学家喜欢智力游戏，在构拟上下的功夫比较多，对于机器处理的谱系分析的热情没那么高。正在这时，那一边厢，生物学家们正在努力地发展更有效的断代方法。美国亚利桑那大学的演化生物学家Michael Sanderson就是其中一个代表人物。他从1997到2002发表了一系列的论文[10][11]，研究了一些已知的谱系树分支的年龄，认为DNA序列的发展确实是有既定的速率，这个速率是它们内在的“时钟”决定的，在不同的分支中，时钟走动的快慢是有区别的。如果我们把这个思想代入到语言学中，我们就知道，按照Swadesh的语言年代学的方法，词汇的发展被认为有统一的“时钟”，或者叫“分子时钟”（molecular clock），我们称为“严格时钟”（strict clock），而分子生物学的最新假设则是引入了“宽松时钟”（relaxed clock），换成语言学，则是认为词汇在不同语支的替换速率并不统一，而是各有各的速率。顺便一提，Swadesh的语言年代学比分子生物学中的“分子时钟”的提出（Zuckerkandl and Pauling 1965）[12]要早十年左右，但是生物学家在接受新鲜事物和创新方面要比语言学家快得多。在贝叶斯谱系分析中，Thorne et al (1998)[13]和Drummond et al (2006)[14]等人都对严格时钟的框架进行了批评和测试，并发现宽松时钟确实可以更好地模拟真实的演化过程。\n历史语言学最终还是再次向生物学靠拢了。2003年，Gray and Atkinson (2003)[15]在Nature上发表了一篇文章，他们使用了贝叶斯谱系分析计算出了印欧语的谱系树，并如同之前测试南岛民族的演化历程一样，这次他们也用谱系树测试了印欧语起源的两种假说，并表示语言的谱系支持原始印欧人是安纳托利亚的农民这一种看法。下图是Gray and Atkinson (2003)的印欧语谱系树\n 谱系分析算法 那么，语言的贝叶斯谱系分析究竟是怎么进行的呢？就像我们之前说的，词汇仍旧是基础。我们选取一个核心词汇表，然后把词汇表中的词汇翻译成我们需要解决的诸语言。当然，这一切都是建立在我们有合理理由怀疑这些语言是同属一个语系的前提下，否则我们得到的结果就没有意义。把词汇翻译成各种语言以后，我们就开始辨别同源词。我们把每一个义项下的同源词找出来，并把它们配成对儿。下图是词源词典编辑工具Edictor界面下的同源词辨认\n 同一个义项下，不同的语言可能呈现不同的词汇。比如汉语中，表示“EAT”这个义项的词汇在粤语和闽南语中都是来自“食”的同源词（粤语：si̍k，闽南语chia̍h），而普通话中则用“吃”来替代。那么单凭这一个词，我们用人脑都可以算出来，粤语和闽南语应该归在一个分支下，而普通话则应该属于另一个分支。\n我们就这样把数十甚至上百种语言的同源词都标记出来，并且把它们转化为机器可读的形式。那么什么样的形式机器才可读呢？机器是不会管你每个词是怎么发音的，它只想知道某两个词是不是同源词。所以你只需要告诉它哪些词是同源词，哪些不是，就可以了。如果两个词是同源词，那么就标记一个“1”，如果不是就标记一个“0”。所以你就要做一个像下图一样的东西，全是0和1，咱们看不懂，但是机器很容易看懂：\n 看到这里，大家就明白了。贝叶斯谱系分析的数据是“同源关系”，而不是同源词本身。我们把数据喂给电脑，接下来就让电脑处理吧。\n马尔可夫链蒙特卡洛 很多传统语言学家诟病，电脑处理的这个部分不透明，像在黑盒里操作一样，不放心把一切交给程序。为了解除一部分疑虑，我在这里解释一下究竟机器是怎么算谱系树的，当然，为了不赶客，下文中不会出现深奥的东西。\n机器在得到我们这些充满0和1的数据后，会开始使用贝叶斯定理，计算出一棵谱系树的可能性。它会先随机生成一棵谱系树，这棵谱系树正确反映语言谱系的概率可想而知是非常低的，但机器就会根据输入的数据，把这个概率算出来，先记下。然后它就会改变原树的形状，生成一棵新的树，再计算出这棵树正确反映语言谱系的概率算出来，与前一棵树的概率比较。如果前一棵树的概率比较小，那么我们就保留新的这棵树。如果前一棵树的概率较大，那么说明新树比旧树还要差，因此我们就会计算前后两个概率的比值（用新的概率除以旧的概率），得到的就是接受这棵新树的概率。然后机器会一直生成新的树，一直重复着相同的比较和计算，一般我们会让机器重复上千万次的计算，从而保证生成的每一棵树的概率达到一个较为稳定的值。这个过程有个名字，叫马尔可夫链蒙特卡洛（Markov chain Monte Carlo，MCMC）。大家可以看以下这篇文章，对其中的数学做了详细介绍：\nhttps://zhuanlan.zhihu.com/p/420214359\n而今听雨：MCMC与贝叶斯推断简介：从入门到放弃111 赞同 · 16 评论文章\n共识树 计算完了上千万次的树以后，还要进行一个步骤。就是我们需要把前边那些低概率的树删掉一点，或者说“烧掉”（burn-in），这样我们就可以排除掉那些比较糟糕的树。最终留下的带有稳定较高概率的树的集合，就是机器为我们输出的结果。所以，我们在众多有关贝叶斯谱系分析论文中看到的树，都不是一棵树，而是成千上万棵具有相近概率的树相互妥协的结果，我们称为“共识树”（consensus tree）。\n机器计算出的每棵树的分支都有着不同的长度。这些长度跟每一个分支末端的语言年龄是成正比的。也就是说，单凭这些分支的长度，我们只能知道语言之间年龄的比值，而我们想知道的却是它们精确到年的真正年龄。这就需要我们找到一个参考点，或者一个称为prior的东西。Prior可以是对得出最佳谱系有利的任何参考数据，而针对语言谱系的年龄，最理想的prior就是语言被记录的时间。比如，我们知道书面藏语是1300多年前被记录的，那么我们就为书面藏语标记1300年的年龄。这样的信息越多，那么计算出来的年龄就会越准确。软件会结合分支的长度与我们给出的年龄信息，推算出其它语言的年龄。这样我们带有年龄的谱系树就产生了。\nDensitree 即便有了年龄，共识树还是共识树，我们不能把它看作一棵单一的谱系树，这也是许多人看这类文章的误区。其实，除了这棵共识树，机器还能给我们提供另一种树，叫做Densitree。Densitree可以把所有谱系树中冲突的部分可视化，让我们看到究竟哪里出了问题。Densitree看起来还是很美观的， 是无数线条的集合。下图中展示的Sagart et al (2019)[16]汉藏语谱系的Densitree，显示了计算过程中出现的非树形结构。一个完美的树形结构中，每一种语言应该只被一条线连接，但是我们看到在这棵树上，有不少语言被深浅不一的线群连接了，比如比较严重的有Chepang、Tshangla、Dulong等语言。存在这一类非树状信息的一大原因在于我们没有完全正确地辨认同源词，而是被部分表面现象骗了，把借词也算成同源词，也提醒我们重新审视我们的同源词判定。Sagart et al (2019)汉藏语谱系的Densitree\n 所以，我们除了看共识树以外，还要注意看densitree，densitree里有更多有用的信息。大部分人对于贝叶斯谱系分析，或者任何谱系分析的诟病都是基于最后的结论，极少注意到这些研究的数据结构和分析方法，甚至连结论都没有看全。因此，我呼吁大家除了看短短的正文，还要注意看文章的补充材料。\n贝叶斯谱系分析是不是语言学？ 语言学的贝叶斯谱系分析基本上就是如上述方式进行的，希望这样的描述足够通俗易懂。如果你们看懂了，你们可能会产生这一个疑问：究竟贝叶斯谱系分析跟传统历史语言学的结合有多紧密？这样子做出的语言谱系，究竟是不是语言学？\n首先，我们应该明确，至少在语言学上，谱系分析的作用不是告诉我们确切的谱系，而是给我们一个有关语言谱系的参考，是辅助历史语言学研究的工具，而不能代替历史语言学本身。比如说，我们推测出的汉藏语系的谱系可以帮我们确立今后汉藏语系历史语言学研究的大方向，因为我们知道了哪些语言更可能属于同一分支，那么我们就可以根据这些线索和思路有针对性的研究。\n贝叶斯语言谱系分析全过程中跟历史语言学有关的部分当然是前期的数据准备过程。这一过程需要历史语言学家判断同源词。如果研究对象是一个我们了解得比较深入的语系，比如印欧语系，我们判断同源词的标准当然是严格遵守历史语言学的原则的。但如果是像汉藏语系这种我们基本不了解的语系，判断同源词的时候很大程度上是靠猜测，有经验的语言学家比没有经验的一般人猜测的准确率自然会高出不少，但也不能完全保证准确。判断同源词的过程必须主要由人工处理。虽然现在也有不少判断同源词的工具和程序，但这些工具大部分基于词汇的相似性，但同源词、尤其是庞大语系下相距较远语言中的同源词往往不相似。比如拉丁语的duus和亚美尼亚语的erk是同源词，除非能把所有的音变告诉机器，否则机器是不可能把它们俩判断为同源词的。对于超级大的语系，判断同源词的工作可能长达数月，也需要好几个历史语言学家的商量与合作。做好同源词的数据后，我们就把一切复杂的计算交给电脑，等它算个几天，这一部分就脱离了传统语言学，进行纯粹统计学的计算了。\n在得到谱系树之后，我们还可以进行后续的历史语言学研究，并把历史语言学的结论与贝叶斯谱系树进行比较。比如说，Birchall et al (2016)[17]就为Chapacruan语系的语言做了一个贝叶斯谱系分析，并同时使用音变创新手动得出了另一个谱系树，并对两棵树进行了比较研究，发现贝叶斯谱系分析得出的结论与手动做出的谱系树还是比较吻合的。又比如，在Sagart et al (2019)的汉藏语谱系发表后，项目成员又发表了一些后续的历史语言学研究与其遥相呼应，比如Lai et al (2020)[18]对西夏语谱系地位的研究，以及Jacques et Pellard (2021)[19]对羌缅语的分析。\n虽然贝叶斯谱系分析的前前后后都少不了历史语言学的工作，但两者始终没有完全融合在一起，在整个研究中交集并不多，而是有着明显的分工。这一个弱点也经常被人们攻击。而且，贝叶斯谱系分析直到今天，都在使用语言学家们较为不喜欢的核心词汇作为基础，而词汇绝不能与生物学中的DNA序列相提并论，音系和形态才可以。\n那么为什么我们坚持使用词汇呢？我在这里谈两个原因。\n第一，词汇被认为可以涵盖历史语言学的大部分工作，并且容易操作。我们判断同源词的时候，自然要考虑到音变的规律性和对应关系，有时甚至要倒推形态，有时还要进行简单的构拟，这些工作都体现在同源词的判别中，因此我们选用词汇，并不是完全无视传统历史语言学，而是因为词汇的比较是传统历史语言学的“精华”。\n第二，词汇的替换是可以无限进行下去的，而且词汇替换的速率已经被证明可以用一定的模型去模拟。而音变则是比较有方向性的，有的音变一旦发生，可能就没办法回头了，比如p \u0026gt; f的音变很容易发生，而f \u0026gt; p的音变则极少发生。另外，音变可以很快，也可以很慢，它们究竟能不能模拟也是一大问题。\n因此，大部分语言学家在谱系分析时，都在如何更好地标记词汇上下功夫。以同源词关系为基础的谱系分析可以在较大的语系下取得成功，但如果我们要研究时间深度较浅的小分支，很可能就没那么得心应手了。\n比如我们要研究官话的谱系，大部分官话的核心词汇都差不多，词汇替换的现象比较少，那么我们喂给机器的数据库可能大部分都是“1”，这样我们可能会得到许多平行的分支，而不是一棵有结构的树。用贝叶斯谱系分析做出的官话谱系，可能不会比白一平（2006）[20]用最大简约法做出的官话谱系进步多少。再者，目前的贝叶斯分析也并非能对大语系完全掌控，比如说，Gray and Atkinson (2003)的印欧语谱系树最让人看不过眼的一点就是斯拉夫语的分类，大家可以自行上滑到他们的印欧语谱系树上，找找波兰语在哪里。\n再举一个极端的例子，假设两种语言互相不能通话（发生了重要的音变），但所有核心词汇都是同源词，没有发生词汇替换，那么机器将认为这两种语言是同一种语言。尽管这样极端的情况在现实世界中不会发生，尽管每一种研究方法都有它的不足之处，但我们应该事先考虑到突发状况的解决办法。这就是未来我们需要解决的问题。\n贝叶斯谱系分析在语言学上的应用已经差不多二十年了，但这二十年间，研究方法上的突破并不显著。人们当然知道这样的分析存在的问题，但是实际研究上，却很难去解决。比如在词汇替换中，有一种情况可能只有词汇的一部分被替换了，那么我们究竟是赋“1”呢，还是赋“0”呢？Hill and List (2017)[21]倒是提出了一个解决方案，他们开发了“部分同源词”（partial cognate）的标记方法，这种方法支持把一个词拆开，只标记同源的部分。如下图中，缅语支诸语言的“羽毛”一词，都可以分析称两个词素，我们可以把这两个词素拆开，分别标记同源关系。\n 部分同源词的标记实际上已经向基于形态的谱系分析迈进了一步，虽然它并没有真正触及到复杂的形态变化，但至少在尝试为合成词的问题寻求合理的解决方法。部分同源词的标记通过实验证明是可行的，但是目前并没有很多真枪实弹的研究成果发表出来。\n如果没有部分同源词的标记，贝叶斯谱系分析其实已经开始变得有点无聊，即便有了部分同源词标记，也并不能把它的有趣续命太久，毕竟这一步迈得也不大。当我们知道一个方法远没有达到理想的程度，但又不断原地打转时，我们就会自然而然地感到焦虑。\n未来的贝叶斯谱系分析的重点必然在于我们处理数据的方式，如何融入更多的历史语言学原则是我们需要思考的。在上文中，我们已经提到，目前声称把语言学和贝叶斯谱系结合在一起的研究无非就是分别用贝叶斯做一个，再用语言学做一个，然后再进行定性的比较。这种方法是绝不能让人满意的。我们需要更加无缝的衔接。\n另外，回归到1930年代或者Ringe et al (2002)和Nakhleh et al (2005)的“完全谱系”那种基于音系和形态特征的谱系分析似乎也是不可取的。因为这些研究对具体数据的处理完全不够，仅仅是从前人的作品里选出一部分可能对分类有用的特征进行计算，这中间仍会有许多不清晰的地方。\n最理想的情景是从语料入手，自然地融入同源词判定以及音系、形态上的创新，让机器根据各语言创新的情况来计算出谱系树。这样不仅仅能大大增加研究的客观性和透明性——单纯的同源词判定的主观因素占比非常严重，而使用创新为依据可以让读者更直接地找出潜在的问题，而且可以让谱系分析有更强大的理论背景。\n最后 恰好，昨天（2021年7月6日），我在我们所的部门会议上谈到了这个问题，Gray and Atkinson (2003)的作者之一，Russell Gray，也是我们的部门主任，也谈了他的想法。他非常愿意看到新的贝叶斯谱系的方案，不过他承认即便在印欧语的谱系研究中，完全融合语言各层面的数据也是极难做到的。我解释道我不是想完全放弃以词汇的同源关系为基础的谱系分析，而是希望能通过音系和形态，去检验词汇同源关系所无法得到的细节。我的预感是，如果我们融入了音系/形态的创新，得出的结果中，非树状信号会大大减少，并帮我们检查同源词判定究竟在哪里出现了问题。\n流水账就写到这里吧。我想大家在这篇流水账中看到的中心思想，是通过量化谱系分析的发展史，看到研究方法一步一步的变迁，以及它们遇到的困难和存在的问题。我们应该知道，评价这类研究的重点在于它们的方法，而不仅仅局限于结论——因为结论必然是有问题的，即便我们得到了一棵完美全对的谱系树，它仍旧是存在问题的，因为它并非完全基于历史语言学理论，而很大程度基于概率，它的完美只是概率问题（有时候太漂亮的结果，也是我们担心的来源之一）。只有对数据处理的方法不断地改进，才有可能把我们带向最真实的谱系分析。\n谱系分析是历史语言学研究中的一个强大的辅助，尤其是它自带断代的特征，可以让我们更好地追溯人类的历史。因此今后的历史语言学家对机器协助的谱系研究会更加上心，争取让既有的历史语言学理论与新兴的技术更加默契地配合。这也是我的愿望列表上的一项，在今后数年的研究中会作出各种各样的尝试。\n参考  Jan Czekanowski, Na Marginesie Recenzji P. K. Moszyiskiego o Ksigtce: Wstep do Historji Slowian. Lud, Series II, vol. VII (1928). Kroeber, A., \u0026amp; Chrétien, C. (1937). Quantitative Classification of Indo-European Languages. Language, 13(2), 83-103. doi:10.2307/408715 Guiraud, Pierre (1959), Problèmes et méthodes de la statistique linguistique, D. Reidel, Publishing Company, Dordrecht, Holland. Swadesh, M. (1950). Salish internal relationships. International Journal of American Linguistics, 16(4), 157-167. Ringe, D., Warnow, T., \u0026amp; Taylor, A. (2002). Indo‐European and computational cladistics. Transactions of the philological society, 100(1), 59-129. Dunn, M., Levinson, S. C., Lindström, E., Reesink, G., \u0026amp; Terrill, A. (2008). Structural phylogeny in historical linguistics: Methodological explorations applied in Island Melanesia. Language, 710-759. Bergsland, K., \u0026amp; Vogt, H. (1962). On the validity of glottochronology. Current anthropology, 3(2), 115-153. Gray, R. D., \u0026amp; Jordan, F. M. Language trees support the express-train sequence of Austronesian expansion, 2000. Nature, 405, 1052. Nakhleh, L., Ringe, D., \u0026amp; Warnow, T. (2005). Perfect phylogenetic networks: A new methodology for reconstructing the evolutionary history of natural languages. Language, 382-420. Sanderson, M. J. (1997). A nonparametric approach to estimating divergence times in the absence of rate constancy. Molecular biology and evolution, 14(12), 1218-1231. Sanderson, M. 2002 Estimating absolute rates of evo- lution and divergence times: a penalized likelihood approach. Mol. Biol. Evol. 19, 101–109. Zuckerkandl, E., \u0026amp; Pauling, L. (1965). Evolutionary divergence and convergence in proteins. In Evolving genes and proteins (pp. 97-166). Academic Press. Thorne, J. L., Kishino, H., \u0026amp; Painter, I. S. (1998). Estimating the rate of evolution of the rate of molecular evolution. Molecular biology and evolution, 15(12), 1647-1657. Drummond, A. J., Ho, S. Y. W., Phillips, M. J. \u0026amp; Rambaut, A. 2006 Relaxed phylogenies and dating with confidence. PLoS Biol. 4, e88. 699 – 710. (doi:10.1371/ journal.pbio.0040088) Gray, R. D., \u0026amp; Atkinson, Q. D. (2003). Language-tree divergence times support the Anatolian theory of Indo-European origin. Nature, 426(6965), 435-439. Sagart, L., Jacques, G., Lai, Y., Ryder, R. J., Thouzeau, V., Greenhill, S. J., \u0026amp; List, J. M. (2019). Dated language phylogenies shed light on the ancestry of Sino-Tibetan. Proceedings of the National Academy of Sciences, 116(21), 10317-10322. Birchall, J., Dunn, M., \u0026amp; Greenhill, S. J. (2016). A combined comparative and phylogenetic analysis of the Chapacuran language family. International Journal of American Linguistics, 82(3), 255-284. Lai, Yunfan., Gong, Xun., Gates, Jesse. P., \u0026amp; Jacques, Guillaume. (2020). Tangut as a West Gyalrongic language. Folia Linguistica Historica, 54(s41), 171-203. Jacques, G., \u0026amp; Pellard, T. (2021). Phylogenies based on lexical innovations refute the Rung hypothesis. Diachronica, 38(1), 1-24. Baxter, W. H. (2006). Mandarin dialect phylogeny. Cahiers de linguistique-Asie orientale, 35(1), 71-114. Hill, N. W., \u0026amp; List, J. M. (2017, September). Challenges of annotation and analysis in computer-assisted language comparison: A case study on Burmish languages. In Yearbook of the Poznan Linguistic Meeting (Vol. 3, No. 1, pp. 47-76). De Gruyter Open.  录播课  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/quant_lang/","summary":"Author:小云哥哥\nSrc: https://zhuanlan.zhihu.com/p/386454664\n 历史语言学家有两个基本任务。第一个任务是追溯相关语言的源头——所谓的“原始语言”，说得通俗一点就是推测一下祖先们是怎么说话的。但其实除非有时光机器，否则我们不可能知道祖先的发音，构拟原始语言的最终目的是使用一个自洽的系统去解释现代亲属语言的差异。这个任务是技术活儿，需要硬功夫，语言学家必须对这些语言的各方面都了如指掌，通过多年的时间真正理解这些语言的运作（尤其是音系和形态），而不是一上来就开始尬比较。第二任务是第一个任务的衍生产品。当我们能够解释亲属语言的差异以后，我们自然而然会发现有的语言差异较小，有的语言差异较大，我们会希望知道每种差异发生的时间顺序，从而推断出亲属语言是因循什么轨迹从原始语言中分化出来的。这就是语言的谱系。\n虽然传统的历史语言学取得了非常大的成功，但是语言学家毕竟是人，他们用人力研究为数众多的语言、处理浩如烟海的语料，总会出现这样那样的问题。比如说我们会在不少著作中看到语言学家前后标准不一致，或者分析过程描述不清晰透明等现象。有时候这些问题并不是有意为之，而是因为人确实无法预估那么多的事情，出错在所难免。\n于是，一部分语言学家开始认识到，我们需要一个机器协助的、量化的转变。人的大脑爱耍小聪明，更擅长处理复杂而特殊的个案，而机器更像一个奴隶，可以帮人类用统一的方法处理繁多和重复的工作。那么，历史语言学家的两个任务，机器可以协助我们解决哪一个呢？原始语言的构拟还是语言的谱系？\n事实上，这两个任务都需要很多的小聪明。如果原始语言的构拟是简单的音位比较，那么机器也许可以很快做出来。但实际操作上要比这个绕很多的弯儿，需要语言学家综合知识的灵活运用。比如索绪尔的喉音理论，就需要对梵语动词变位的深入理解，从而比较不同变位模式的内在一致，并且对音变的类型学有融会贯通的了解。这一切的运作，可能在索绪尔的脑子里一秒钟就能形成，而机器则不可能在短时间内完成喉音的构拟。我们引入机器是为了提高效率，而不是降低效率。因此，语言学家把目光转向了第二个任务，语言的谱系。历史语言学确定谱系的唯一标准是共同创新，但辨认共同创新实际上也需要深厚的研究功底，机器很难按照人类的方式分析。一个比较可行的办法是偏离历史语言学的原则，使用统计学的方式，构造出在统计学意义上最可能的谱系。\n在这篇文章中，我就用流水账的方式梳理一下机器协助的语言谱系分析的相关历史，尤其专注于贝叶斯谱系分析。因为是流水账，所以不会分小节，我也会省去所有赶客的公式和理论描述。\n语言谱系分析 较早使用统计学处理语言谱系的研究可以追溯到十九世纪前叶。不过现代的尝试最值得提的是二十世纪二十年代开始的一系列操作。波兰人类学家Czekanowski[1]在1928年收集了二十多个音系、形态和词汇上的特点，研究了包括立陶宛语、古教会斯拉夫语、哥特语、古爱尔兰语、拉丁语、希腊语、吠陀梵语、阿维斯陀语和亚美尼亚语相互之间的关系。他们得出的结果其中一个错误是认为哥特语与波罗地-斯拉夫语更为接近，而不是意大利-凯尔特语。1937年，加州大学的Kroeber和Chrétien[2]在前人的基础上，添加了新的数据（主要是特征性的音变和形态变化方面的数据），使参与比较的特征达到了74个。下图是Kroeber and Chrétien (1937)的统计分析\n Kroeber和Chrétien就是通过判断每一个特征是否在各种语言中出现，列出矩阵计算出各语言的相似度。他们的结果，至少从这九种语言来看，基本上与历史语言学的结果相符。但由于数据本身的局限性，他们的方法并没有被大规模地使用，并且遭到了一些批评。我不知道他们的计算是否用了机器，但是从他们并不复杂的公式来看，可能是笔算的。\n虽然这些早期的尝试寿命并不长，但是也为量化历史语言学定下了统计学的基调，尽管在数据选取上，名义上是使用了历史语言学的结论，但是并没有使用历史语言学的分类标准，而是把这些结论转化成可以用于统计学的数据。这也是从这以后，直至现在将尽一个世纪的趋势。\n La linguistique est la science statistique type ; les statisticiens le savent bien ; la plupart des linguistes l\u0026rsquo;ignorent encore. (Guiraud 1959: 15[3]) 语言学就是典型的统计科学；统计学家心里很清楚，大部分语言学家却不知道。\n \u0026lt;比如\u0026gt;\n基于词汇的语言谱系分析 1950年代，有一个长得有点喜感的中年男人，叫Morris Swadesh。他是一个美国的语言学家。身为一个历史语言学家，他并不把关注的重点放在音系和形态的变化上，而是更专注于词汇。词汇相对于音系和形态，显然是更容易操作的东西，毕竟它们就像拼好的积木，能让人一眼就辨认出来。Morris Swadesh (1909-1967)\n Swadesh认为不同语言中词汇的重合度很可能与语系的演化有关。这点很符合我们的直觉，基因关系较远的语言中，非同源的词汇理应越多。而且，他还假设词汇系统是按一定的速率变化的，我们只要以这个速率为基础，然后比较亲属语言的同源词的多寡，就能得到语言的谱系，同时我们还可以算出亲属语言的分裂时间。Swadesh (1950)[4]认为词汇的变化速率是每过1000年，一种语言想对于原本形态的同源词就会降低到原来的85%。后来这个百分比又被改为81%。 这个数字大概是基于古英语和现代英语的词汇变化确定的。\n核心词汇 我们不可能穷尽所有的词汇，所以就需要选取一些具有代表性的词汇来简化我们的研究。Swadesh整理出一份100词的词表，现在我们称为“核心词汇”或者“基本词汇”，包括身体部位、数字、颜色、基本动作等类别，这些词汇被认为是最不容易被借用的，有较大的概率是本土词汇。\n语言断代学（词汇统计学） 这么一来，如果我们发现两种亲属语言在核心词汇表上有81%的同源词，那么我们就可以认为这两种语言的分化时间是1000年。如果它们有81%×81%=65.61%的同源词，那么它们的分化时间就是2000年。这个方法我们称为Glottochronology，汉语称作“语言断代学”，它也是“词汇统计学”（lexicostatistics）的最主要方法之一。\n为了让故事更连续，我在这里删除了其它的研究方法，比如计算词汇间Levenshtein距离，有关这方面的内容，可以看这个回答。\n总而言之，从Swadesh开始，量化历史语言学基本上就在词汇之间徘徊，人们开始想尽办法从词汇中找到语言发展的轨迹。当然， 也有从音系/形态上考虑的（Ringe et al 2002）[5]，还有从类型学上考虑的（Dunn et al 2008）[6]，但始终无法摆脱或者撼动以词汇为基础的大趋势。\n语言断代学虽然在语言谱系分析的量化上取得了较大的进展，但最终仍被认为是失败的方法。这是因为它强制规定词汇有着固定的变化速率。这一基本假设从直觉上就不符合语言的发展历程，而且没有靠谱的研究去证明，反而很容易被证伪。比如说，我们使用语言年代学的模型，我们会得到格鲁吉亚语和明格列尔语的分化年代距今约1000年左右。但实际上，它们两个的分化年代要远早于公元四到五世纪（Bergsland and Vogt 1962）[7]。Swadesh本人也觉得这个方法有问题。所以逐渐人们也就不再使用语言年代学了。","title":"量化历史语言学-贝叶斯语言谱系分析"},{"content":"dvt https://github.com/distant-viewing/dvt\nThe Distant TV Toolkit 由几个 Python 包组成，旨在促进视觉文化的计算分析。 开始使用该工具包的最简单方法是在 Google 的协作 (Colab) 环境中运行该工具包。 这是一项免费使用的服务，允许您以最少的设置在远程服务器上运行 Python 代码。\n OpenPifPaf   OpenPifPaf：用于语义关键点检测和时空关联的复合字段\n  OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association\n  许多图像分析任务需要准确了解人在图像框架内的位置。 我们可以通过使用关键点检测（远景工具包中包含的另一种算法）来获取有关帧中人物位置的更多信息。 在这里，我们检测图像中人物的关键点（与人体相关的点）\n sherlock https://github.com/sherlock-project/sherlock\n通过社交网络上的用户名追捕社交媒体帐户\nkivy https://github.com/kivy/kivy\n用 Python 编写的开源 UI 框架，可在 Windows、Linux、macOS、Android 和 iOS 上运行\n snscrape https://github.com/JustAnotherArchivist/snscrape\nPython 中的社交网络服务爬虫\npy-pkgs https://py-pkgs.org/\nPython 包是 Python 编程语言的核心元素，也是您在 Python 中创建有组织、可重用和可共享的代码的方式。 Python Packages 是一本开源书籍，描述了用于创建 Python 包的现代高效工作流程。\n cdlib https://github.com/GiulioRossetti/cdlib\nPython社区发现算法包\npapertime.app https://papertime.app/\n计算机科学论文摘要播音\nleafletjs https://leafletjs.com/\n网页html中可插入地图JS， 显示经纬度对应的地图\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly09/","summary":"dvt https://github.com/distant-viewing/dvt\nThe Distant TV Toolkit 由几个 Python 包组成，旨在促进视觉文化的计算分析。 开始使用该工具包的最简单方法是在 Google 的协作 (Colab) 环境中运行该工具包。 这是一项免费使用的服务，允许您以最少的设置在远程服务器上运行 Python 代码。\n OpenPifPaf   OpenPifPaf：用于语义关键点检测和时空关联的复合字段\n  OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association\n  许多图像分析任务需要准确了解人在图像框架内的位置。 我们可以通过使用关键点检测（远景工具包中包含的另一种算法）来获取有关帧中人物位置的更多信息。 在这里，我们检测图像中人物的关键点（与人体相关的点）\n sherlock https://github.com/sherlock-project/sherlock\n通过社交网络上的用户名追捕社交媒体帐户\nkivy https://github.com/kivy/kivy\n用 Python 编写的开源 UI 框架，可在 Windows、Linux、macOS、Android 和 iOS 上运行\n snscrape https://github.com/JustAnotherArchivist/snscrape\nPython 中的社交网络服务爬虫\npy-pkgs https://py-pkgs.org/\nPython 包是 Python 编程语言的核心元素，也是您在 Python 中创建有组织、可重用和可共享的代码的方式。 Python Packages 是一本开源书籍，描述了用于创建 Python 包的现代高效工作流程。","title":"TechWeekly-09| 每周有趣有用的技术分享"},{"content":"Introduction to Cultural Analytics \u0026amp; Python Designed by Melanie Walsh // Powered by Jupyter Book\n该项目仓库托管了在线教科书 Introduction to Cultural Analytics \u0026amp; Python 里的代码，主要面向人文社科人群设计的Python编程语言书\n这本书展示了如何使用 Python 来研究文化领域的数据，例如歌词、短篇小说、报纸文章、推文、Reddit 帖子和电影剧本。 它还介绍了网络抓取、API、主题建模、命名实体识别 (NER)、网络分析和映射等计算方法。\n这些材料最初是为了支持“文化分析简介：数据、计算和文化”，这是康奈尔大学和华盛顿大学教授的本科课程。\n## 章节列表 ## 数据集 ## 命令行 Python语法  安装Python 如何使用Jupyter Notebook 变量 数据类型 字符串方法 比较逻辑运算 循环 函数 文件编码 常见Python错误 Jupyter使用技巧   数据分析  Pandas基本操作 合并数据   数据采集  法律风险 网络爬虫 API 案例-采集歌词 案例-twitter 案例-Reddit   文本分析  TF-IDf 情感分析 话题模型 命名实体识别 词性   网络分析  bokeh网络可视化   资料下载 https://github.com/hiDaDeng/Intro-Cultural-Analytics/archive/refs/heads/master.zip\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/culture_analysis/","summary":"Introduction to Cultural Analytics \u0026amp; Python Designed by Melanie Walsh // Powered by Jupyter Book\n该项目仓库托管了在线教科书 Introduction to Cultural Analytics \u0026amp; Python 里的代码，主要面向人文社科人群设计的Python编程语言书\n这本书展示了如何使用 Python 来研究文化领域的数据，例如歌词、短篇小说、报纸文章、推文、Reddit 帖子和电影剧本。 它还介绍了网络抓取、API、主题建模、命名实体识别 (NER)、网络分析和映射等计算方法。\n这些材料最初是为了支持“文化分析简介：数据、计算和文化”，这是康奈尔大学和华盛顿大学教授的本科课程。\n## 章节列表 ## 数据集 ## 命令行 Python语法  安装Python 如何使用Jupyter Notebook 变量 数据类型 字符串方法 比较逻辑运算 循环 函数 文件编码 常见Python错误 Jupyter使用技巧   数据分析  Pandas基本操作 合并数据   数据采集  法律风险 网络爬虫 API 案例-采集歌词 案例-twitter 案例-Reddit   文本分析  TF-IDf 情感分析 话题模型 命名实体识别 词性   网络分析  bokeh网络可视化   资料下载 https://github.","title":"Python与文化分析入门"},{"content":"文化分析\u0026amp;Python https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html\n本书对应于康奈尔大学和华盛顿大学教授的本科课程《文化分析简介：数据、计算和文化》，主要介绍了如何使用 Python 来研究文化材料，例如歌词、短篇小说、报纸文章、推文、Reddit 帖子和电影剧本。 它还介绍了网络抓取、API、主题建模、命名实体识别 (NER)、网络分析和映射等计算方法。\n这些材料最初是为了支持“”，这是。\n 什么是文化分析？  文化分析是用计算方法研究文化。 文化是一个广义的术语，包括文学、历史、政治、艺术、音乐、社交媒体等等。 文化分析是数字人文和信息科学等领域不断发展的研究领域。\n 计算和推理思维:数据科学基础  Computational and Inferential Thinking: The Foundations of Data Science\n https://inferentialthinking.com/chapters/intro.html\n加州大学伯克利分校数据科学基础课程结合了三个观点：推理思维、计算思维和现实世界的相关性。 给定来自某些现实世界现象的数据，如何分析该数据以了解该现象？ 该课程教授计算机编程和统计推断方面的关键概念和技能，并结合对现实世界数据集的动手分析，包括经济数据、文档集、地理数据和社交网络。 它深入探讨了围绕数据分析的社会问题，例如隐私和设计。该课程是与伯克利数据科学部合作提供的。\n使用 PySAL 和 PyData 堆栈的地理数据科学¶ https://geographicdata.science/book/intro.html\n地理数据无处不在。总体而言，社会过程、物理环境和个人行为在其地理模式、结构和间隔方面表现出惊人的规律性。随着与这些系统相关的数据在范围、强度和深度上的增长，从位置等常见地理属性中提取有意义的见解变得更加重要，而且如何利用标准数据科学中不太常见的拓扑属性（例如关系）也变得更加重要。\n本书介绍了一种思考地理挑战的新方法， 如空间自相关(Spatial Autocorrelation)、点模式分析(Point Pattern Analysis)。使用地理分析和计算推理，它向读者展示了如何解开隐藏在数据中的新见解。本书围绕 Python 中可用的优秀数据科学环境构建，提供示例和工作分析供读者复制、适应、扩展和改进。\n 心理科学Python编程 https://lukas-snoek.com/introPy/index.html\n阿姆斯特丹大学为心理学硕士研究生开设的，是为期 4 周的“心理科学编程”课程的一部分，从两周的 R 编程开始，之后学生可以在剩下的两周内选择两个主题之一：高级 R 或 Python/PsychoPy（本课程） 。因此，本 Python/PsychoPy 课程假设学生熟悉基本的编程概念（例如条件、循环和函数）。 在本课程的第 1 周，我们将深入研究 Python 特定的主题（至少与 R 相比），例如面向对象编程和最重要的数据处理包（包括 pandas、numpy 和 matplotlib）。 在第 2 周，我们将讨论如何使用 PsychoPy 软件包通过其图形界面 (Builder) 和 Python 界面 (Coder) 创建实验。\nOpenPifPaf：用于语义关键点检测和时空关联的复合字段  OpenPifPaf: Composite Fields for Semantic Keypoint Detection and Spatio-Temporal Association   经济和金融领域的 Python 编程 https://quantecon.org/python-lectures/\n https://python.quantecon.org/intro.html\n 信用卡欺诈机器学习识别 https://fraud-detection-handbook.github.io/fraud-detection-handbook/Foreword.html\nML 技术在支付卡欺诈检测系统中的集成大大提高了它们更有效地检测欺诈的能力，并协助支付处理中介识别非法交易。尽管近年来欺诈交易的数量不断增加，但欺诈造成的损失百分比在 2016 年开始下降，这是与 ML 解决方案越来越多地采用相关的反向趋势。 [rep19]。除了帮助节省资金外，实施基于机器学习的欺诈检测系统如今已成为机构和公司赢得客户信任的必要条件。\n在这个用于卡欺诈检测的机器学习新领域中，一个被广泛认可和反复出现的问题是关于该主题 [LJ20,PP19,PL18,ZAM+16] 发表的大多数研究工作缺乏可重复性。一方面，支付卡交易数据缺乏可用性，出于保密原因不能公开共享。另一方面，作者没有做出足够的努力来提供他们的代码并使他们的结果可重现。\n本书所介绍的一些技术，例如处理类别不平衡、模型集成或概念漂移的技术，被广泛认为是信用卡欺诈检测系统设计的重要组成部分。我们还涵盖了我们认为值得更多关注的记录较少的主题。其中包括建模过程的特定设计方面，例如性能指标和验证策略的选择，以及有前景的预处理和学习策略，例如特征嵌入、主动学习和迁移学习。虽然本书侧重于支付卡欺诈，但我们相信本书中介绍的大多数技术和讨论对从事更广泛的欺诈检测主题的其他从业者有用。\n音乐分类：超越监督学习，走向现实世界的应用 https://music-classification.github.io/tutorial/landing-page.html\n音乐分类是一项音乐信息检索 (MIR) 任务，其目标是对音乐语义的计算理解。 对于给定的歌曲，分类器预测相关的音乐属性。 根据任务定义，分类任务几乎是无限的——从流派、情绪和乐器到更广泛的概念，包括音乐相似性和音乐偏好。 检索到的信息可以进一步用于许多应用，包括音乐推荐、策展、播放列表生成和语义搜索。\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/course_recommendation_about_social_science/","summary":"文化分析\u0026amp;Python https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html\n本书对应于康奈尔大学和华盛顿大学教授的本科课程《文化分析简介：数据、计算和文化》，主要介绍了如何使用 Python 来研究文化材料，例如歌词、短篇小说、报纸文章、推文、Reddit 帖子和电影剧本。 它还介绍了网络抓取、API、主题建模、命名实体识别 (NER)、网络分析和映射等计算方法。\n这些材料最初是为了支持“”，这是。\n 什么是文化分析？  文化分析是用计算方法研究文化。 文化是一个广义的术语，包括文学、历史、政治、艺术、音乐、社交媒体等等。 文化分析是数字人文和信息科学等领域不断发展的研究领域。\n 计算和推理思维:数据科学基础  Computational and Inferential Thinking: The Foundations of Data Science\n https://inferentialthinking.com/chapters/intro.html\n加州大学伯克利分校数据科学基础课程结合了三个观点：推理思维、计算思维和现实世界的相关性。 给定来自某些现实世界现象的数据，如何分析该数据以了解该现象？ 该课程教授计算机编程和统计推断方面的关键概念和技能，并结合对现实世界数据集的动手分析，包括经济数据、文档集、地理数据和社交网络。 它深入探讨了围绕数据分析的社会问题，例如隐私和设计。该课程是与伯克利数据科学部合作提供的。\n使用 PySAL 和 PyData 堆栈的地理数据科学¶ https://geographicdata.science/book/intro.html\n地理数据无处不在。总体而言，社会过程、物理环境和个人行为在其地理模式、结构和间隔方面表现出惊人的规律性。随着与这些系统相关的数据在范围、强度和深度上的增长，从位置等常见地理属性中提取有意义的见解变得更加重要，而且如何利用标准数据科学中不太常见的拓扑属性（例如关系）也变得更加重要。\n本书介绍了一种思考地理挑战的新方法， 如空间自相关(Spatial Autocorrelation)、点模式分析(Point Pattern Analysis)。使用地理分析和计算推理，它向读者展示了如何解开隐藏在数据中的新见解。本书围绕 Python 中可用的优秀数据科学环境构建，提供示例和工作分析供读者复制、适应、扩展和改进。\n 心理科学Python编程 https://lukas-snoek.com/introPy/index.html\n阿姆斯特丹大学为心理学硕士研究生开设的，是为期 4 周的“心理科学编程”课程的一部分，从两周的 R 编程开始，之后学生可以在剩下的两周内选择两个主题之一：高级 R 或 Python/PsychoPy（本课程） 。因此，本 Python/PsychoPy 课程假设学生熟悉基本的编程概念（例如条件、循环和函数）。 在本课程的第 1 周，我们将深入研究 Python 特定的主题（至少与 R 相比），例如面向对象编程和最重要的数据处理包（包括 pandas、numpy 和 matplotlib）。 在第 2 周，我们将讨论如何使用 PsychoPy 软件包通过其图形界面 (Builder) 和 Python 界面 (Coder) 创建实验。","title":"免费社科类Python编程课程列表"},{"content":"OCRmyPDF https://github.com/ocrmypdf/OCRmyPDF\nOCRmyPDF 为扫描的 PDF 文件添加了 OCR 文本层，允许搜索或复制粘贴它们。\n matplotx https://github.com/nschloe/matplotx\nMatplotlib扩展库，可以提供更多样式，简化样式设定\n download https://github.com/choldgraf/download\n在线文件下载模块, 默认含进度条\nfrom download import download path = download(url, path, progressbar=True) \nbirdseye https://github.com/alexmojaki/birdseye\nBirdeye 是一个 Python 调试器，它记录函数调用中表达式的值，并让您在函数退出后轻松查看它们。 例如：\n python-pinyin https://github.com/mozillazg/python-pinyin\n汉语转拼音的 Python 库，将汉字转为拼音,可以用于汉字注音、排序、检索。\n\u0026gt;\u0026gt;\u0026gt; from pypinyin import pinyin, lazy_pinyin, Style \u0026gt;\u0026gt;\u0026gt; pinyin(\u0026#39;中心\u0026#39;) [[\u0026#39;zhōng\u0026#39;], [\u0026#39;xīn\u0026#39;]] \u0026gt;\u0026gt;\u0026gt; pinyin(\u0026#39;中心\u0026#39;, heteronym=True) # 启用多音字模式 [[\u0026#39;zhōng\u0026#39;, \u0026#39;zhòng\u0026#39;], [\u0026#39;xīn\u0026#39;]] \u0026gt;\u0026gt;\u0026gt; pinyin(\u0026#39;中心\u0026#39;, style=Style.TONE3, heteronym=True) [[\u0026#39;zhong1\u0026#39;, \u0026#39;zhong4\u0026#39;], [\u0026#39;xin1\u0026#39;]] \ntextnets https://github.com/jboynyc/textnets\n利用网络做文本分析，可以参考这篇技术文 PNAS | 文本网络分析\u0026amp;文化桥梁Python代码实现\n尽管网络分析通常用于描述人与人之间的关系——尤其是在社会科学中——但它也可以应用于词之间的关系。例如，网络关系可以通过文档中单个单词的共现来创建，或者可以使用双模式网络投影在文档之间创建关系。\n基于网络的自动文本分析方法的优点是\n 像社会群体一样，可以通过三元闭包更准确地测量词组的含义——或者任何两个词或术语相互的含义的原则如果将它们放在第三个词的上下文中，可以更准确地理解； 文本网络可以应用于任何长度的文档，这与通常需要大量单词才能正常运行的主题模型不同。在简短的社交媒体文本变得普遍的时代，这是一个显着的优势。 最后，这种方法受益于社区检测跨学科文献的最新进展，可以说它提供了更准确的单词分组方法，这些方法受益于网络内观察到的聚类，而不是词袋模型。    whoogle-search whoogle-search是一款可以自己架设，能够爬取谷歌搜索结果、无广告、不追踪、保护隐私的搜索引擎工具。\nwhoogle-search的安装部署方式非常丰富而且简单，可以通过Docker、Heroku、pip、手动等方式进行安装配置。\n安装之后配置相应的ip和端口就可以启动whoogle-search服务。\n以pip安装配置为例。\n安装\npip install whoogle-search 启动服务\nwhoogle-search --host \u0026lt;your ip\u0026gt; --port \u0026lt;your port\u0026gt;   poetry https://github.com/python-poetry/poetry\n类似于pip，可以对python的项目进行包管理。\nfuturecoder https://github.com/alexmojaki/futurecoder\n交互式学习Python，供人们自学 Python 编程，尤其是完全的编程初学者。 它经过精心设计，可减少挫折感并指导用户，同时确保他们学习如何解决问题。 目标是让尽可能多的人学习编程。\n attrs https://github.com/python-attrs/attrs\n没有样板的 Python 类, 主要目标是帮助您编写简洁且正确的软件，且不会减慢您的代码速度。案例引自\n 作者：小明 链接：https://zhuanlan.zhihu.com/p/34963159 来源：知乎\n class Product(object): def __init__(self, id, author_id, category_id, brand_id, spu_id, title, item_id, n_comments): self.id = id self.author_id = author_id self.category_id = category_id self.brand_id = brand_id self.spu_id = spu_id self.title = title self.item_id = item_id self.n_comments = n_comments 如果用attrs，代码会更简洁\nimport attr @attr.s(hash=True) class Product(object): id = attr.ib() author_id = attr.ib() brand_id = attr.ib() spu_id = attr.ib() title = attr.ib(repr=False, cmp=False, hash=False) item_id = attr.ib(repr=False, cmp=False, hash=False) n_comments = attr.ib(repr=False, cmp=False, hash=False) \nbacktrader https://github.com/mementum/backtrader\n投资测量Python回测框架\nfrom datetime import datetime import backtrader as bt class SmaCross(bt.SignalStrategy): def __init__(self): sma1, sma2 = bt.ind.SMA(period=10), bt.ind.SMA(period=30) crossover = bt.ind.CrossOver(sma1, sma2) self.signal_add(bt.SIGNAL_LONG, crossover) cerebro = bt.Cerebro() cerebro.addstrategy(SmaCross) data0 = bt.feeds.YahooFinanceData(dataname=\u0026#39;MSFT\u0026#39;, fromdate=datetime(2011, 1, 1), todate=datetime(2012, 12, 31)) cerebro.adddata(data0) cerebro.run() cerebro.plot() \nautopep8 https://github.com/hhatto/autopep8\n一种自动格式化 Python 代码以符合 PEP 8 样式指南的工具。\n##了解课程\n 点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly08/","summary":"OCRmyPDF https://github.com/ocrmypdf/OCRmyPDF\nOCRmyPDF 为扫描的 PDF 文件添加了 OCR 文本层，允许搜索或复制粘贴它们。\n matplotx https://github.com/nschloe/matplotx\nMatplotlib扩展库，可以提供更多样式，简化样式设定\n download https://github.com/choldgraf/download\n在线文件下载模块, 默认含进度条\nfrom download import download path = download(url, path, progressbar=True) \nbirdseye https://github.com/alexmojaki/birdseye\nBirdeye 是一个 Python 调试器，它记录函数调用中表达式的值，并让您在函数退出后轻松查看它们。 例如：\n python-pinyin https://github.com/mozillazg/python-pinyin\n汉语转拼音的 Python 库，将汉字转为拼音,可以用于汉字注音、排序、检索。\n\u0026gt;\u0026gt;\u0026gt; from pypinyin import pinyin, lazy_pinyin, Style \u0026gt;\u0026gt;\u0026gt; pinyin(\u0026#39;中心\u0026#39;) [[\u0026#39;zhōng\u0026#39;], [\u0026#39;xīn\u0026#39;]] \u0026gt;\u0026gt;\u0026gt; pinyin(\u0026#39;中心\u0026#39;, heteronym=True) # 启用多音字模式 [[\u0026#39;zhōng\u0026#39;, \u0026#39;zhòng\u0026#39;], [\u0026#39;xīn\u0026#39;]] \u0026gt;\u0026gt;\u0026gt; pinyin(\u0026#39;中心\u0026#39;, style=Style.TONE3, heteronym=True) [[\u0026#39;zhong1\u0026#39;, \u0026#39;zhong4\u0026#39;], [\u0026#39;xin1\u0026#39;]] \ntextnets https://github.com/jboynyc/textnets\n利用网络做文本分析，可以参考这篇技术文 PNAS | 文本网络分析\u0026amp;文化桥梁Python代码实现","title":"TechWeekly-08| 每周有趣有用的技术分享"},{"content":"使用文本相似度可以识别变化的时间点，先配置环境\n代码下载 配置环境 !pip3 install scikit-learn==1.0 !pip3 install cntext==1.2 # 安装pyecharts可视化 !pip3 install pyecharts==1.6.2 !pip3 install pyecharts-javascripthon==0.0.6 !pip3 install pyecharts-jupyter-installer==0.0.3 !pip3 install pyecharts-snapshot==0.2.0 \n1. 查看数据 本次使用sotu数据集，收集了从1790年至2018年国情咨文文本，这是漂亮国大统领每年发表的演讲，用于描述国家过去的成就和未来面临的挑战。\nimport pandas as pd df = pd.read_csv(\u0026#39;sotu.csv\u0026#39;) #text2是text向下顺移1位的结果 df[\u0026#39;text2\u0026#39;]=df[\u0026#39;text\u0026#39;].shift(1) #剔除空字符 df.dropna(inplace=True) df.tail(10)   两段文本的相似度可以通过cos计算\nfrom cntext.similarity import similarity_score text1 = \u0026#39;Mr. Speaker, Mr. Vice President, members of\u0026#39; text2 = \u0026#39;Thank you very much. Mr. Speaker, Mr. Vice\u0026#39; similarity_score(text1, text2) Run\n{'Sim_Cosine': 0.4629100498862757, 'Sim_Jaccard': 0.3, 'Sim_MinEdit': 16, 'Sim_Simple': 0.9619883040935673}  2. 相似度可视化 如果把很多个相邻文本(有时间先后顺序)依次计算相似度，可以绘制出曲线，我们根据自己的领域知识，就可以看出变化的时间点。\nfrom cntext.similarity import similarity_score cosines = [] for idx, row in df.iterrows(): text1 = df.loc[idx, \u0026#39;text\u0026#39;] text2 = df.loc[idx, \u0026#39;text2\u0026#39;] simi = similarity_score(text1, text2)[\u0026#39;Sim_Cosine\u0026#39;] cosines.append(simi) cosines Run\n[0.42767330405703097, 0.39821498388325544, 0.410744931596176, 0.3844380358041578, 0.4116242706522565, 0.4169268094228332, 0.4249719376001671, .... 0.39065212923423315, 0.3763764307701755, 0.35307484669994105, 0.4119319787659037, 0.43053043053064594, 0.45219743197249296, 0.421723837550935, 0.427904362863808] 紧接着\nfrom pyecharts.charts import Line from pyecharts import options as opts from pyecharts.globals import CurrentConfig, NotebookType CurrentConfig.NOTEBOOK_TYPE = NotebookType.JUPYTER_NOTEBOOK line = Line() line.add_xaxis(xaxis_data=[str(y) for y in df[\u0026#39;year\u0026#39;].values]) line.add_yaxis(\u0026#34;本期与上期的相似度\u0026#34;, cosines, label_opts=opts.LabelOpts(is_show=False)) line.set_global_opts(title_opts=opts.TitleOpts(title=\u0026#34;1790年至2018年国情咨文文本相似度\u0026#34;)) line.load_javascript() line.render(\u0026#39;1790年至2018年国情咨文文本相似度可视化.html\u0026#39;) line.render_notebook()   注意，横坐标显示的是当年报告 与 前一年报告 的对比相似度\n3. 图形解读 相似度越低，说明本期与前期相比，文本变化较大，在本场景中可能是漂亮国在大幅度调整政策。\n 在图中，我们最熟悉的时期是一战和二战，这个阶段在图中较长时间处于低位，漂亮国zf的政策处于战时状态。 漂亮国立国初期，相似度连线也长时间处于低位，体现了新国家正在探索为政之道。 漂亮国每4年选ju一次，那么换届年份，相似度也会比较低。  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/text_sim/","summary":"使用文本相似度可以识别变化的时间点，先配置环境\n代码下载 配置环境 !pip3 install scikit-learn==1.0 !pip3 install cntext==1.2 # 安装pyecharts可视化 !pip3 install pyecharts==1.6.2 !pip3 install pyecharts-javascripthon==0.0.6 !pip3 install pyecharts-jupyter-installer==0.0.3 !pip3 install pyecharts-snapshot==0.2.0 \n1. 查看数据 本次使用sotu数据集，收集了从1790年至2018年国情咨文文本，这是漂亮国大统领每年发表的演讲，用于描述国家过去的成就和未来面临的挑战。\nimport pandas as pd df = pd.read_csv(\u0026#39;sotu.csv\u0026#39;) #text2是text向下顺移1位的结果 df[\u0026#39;text2\u0026#39;]=df[\u0026#39;text\u0026#39;].shift(1) #剔除空字符 df.dropna(inplace=True) df.tail(10)   两段文本的相似度可以通过cos计算\nfrom cntext.similarity import similarity_score text1 = \u0026#39;Mr. Speaker, Mr. Vice President, members of\u0026#39; text2 = \u0026#39;Thank you very much. Mr. Speaker, Mr. Vice\u0026#39; similarity_score(text1, text2) Run\n{'Sim_Cosine': 0.4629100498862757, 'Sim_Jaccard': 0.","title":"使用文本相似度可以识别变化的时间点"},{"content":"前段时间发现apachecn在github上翻译了一本和特征工程相关的书籍：《Feature Engineering for Machine Learning》，中文名为《面向机器学习的特征工程》。\nFeature_Engineering_for_Machine_Learning.pdf\n三、文本数据: 展开、过滤和分块  译者：@kkejili\n校对者：@HeYun\n 如果让你来设计一个算法来分析以下段落，你会怎么做？\nEmma knocked on the door. No answer. She knocked again and waited. There was a large maple tree next to the house. Emma looked up the tree and saw a giant raven perched at the treetop. Under the afternoon sun, the raven gleamed magnificently. Its beak was hard and pointed, its claws sharp and strong. It looked regal and imposing. It reigned the tree it stood on. The raven was looking straight at Emma with its beady black eyes. Emma felt slightly intimidated. She took a step back from the door and tentatively said, “hello?” 复制ErrorOK! 该段包含很多信息。我们知道它谈到了到一个名叫Emma的人和一只乌鸦。这里有一座房子和一棵树，艾玛正想进屋，却看到了乌鸦。这只华丽的乌鸦注意到艾玛，她有点害怕，但正在尝试交流。\n那么，这些信息的哪些部分是我们应该提取的显着特征？首先，提取主要角色艾玛和乌鸦的名字似乎是个好主意。接下来，注意房子，门和树的布置可能也很好。关于乌鸦的描述呢？Emma的行为呢，敲门，退后一步，打招呼呢？\n本章介绍文本特征工程的基础知识。我们从词袋（bags of words）开始，这是基于字数统计的最简单的文本功能。一个非常相关的变换是 tf-idf，它本质上是一种特征缩放技术。它将被我在（下一篇）章节进行全面讨论。本章首先讨论文本特征提取，然后讨论如何过滤和清洗这些特征。\nBag of X：把自然文本变成平面向量 无论是构建机器学习模型还是特征工程，其结果应该是通俗易懂的。简单的事情很容易尝试，可解释的特征和模型相比于复杂的更易于调试。简单和可解释的功能并不总是会得到最精确的模型。但从简单开始就是一个好主意，仅在绝对必要时我们可以增加其复杂性。\n对于文本数据，我们可以从称为 BOW 的字数统计开始。字数统计表中并没有特别费力来寻找\u0026quot;Emma\u0026quot;或乌鸦这样有趣的实体。但是这两个词在该段落中被重复提到，并且它们在这里的计数比诸如\u0026quot;hello\u0026quot;之类的随机词更高。对于此类简单的文档分类任务，字数统计通常比较适用。它也可用于信息检索，其目标是检索与输入文本相关的文档集。这两个任务都很好解释词级特征，因为某些特定词的存在可能是本文档主题内容的重要指标。\n词袋 在词袋特征中，文本文档被转换成向量。（向量只是 n 个数字的集合。）向量包含词汇表中每个单词可能出现的数目。 如果单词\u0026quot;aardvark\u0026quot;在文档中出现三次，则该特征向量在与该单词对应的位置上的计数为 3。 如果词汇表中的单词没有出现在文档中，则计数为零。 例如，“这是一只小狗，它是非常可爱”的句子具有如图所示的 BOW 表示\n 图 3-1 转换词成向量描述图\nBOW 将文本文档转换为平面向量。 它是“平面的”，因为它不包含任何原始的文本结构。 原文是一系列词语。但是词袋向量并没有序列；它只是记得每个单词在文本中出现多少次。 它不代表任何词层次结构的概念。 例如，“动物”的概念包括“狗”，“猫”，“乌鸦”等。但是在一个词袋表示中，这些词都是矢量的相同元素。\n 图 3-2 两个等效的词向量，向量中单词的排序不重要，只要它在数据集中的个数和文档中出现数量是一致的。\n重要的是特征空间中数据的几何形状。 在一个词袋矢量中，每个单词成为矢量的一个维度。如果词汇表中有 n 个单词，则文档将成为n维空间中的一个点。 很难想象二维或三维以外的任何物体的几何形状，所以我们必须使用我们的想象力。 图3-3显示了我们的例句在对应于“小狗”和“可爱”两个维度的特征空间中的样子。\n 图 3-3 特征空间中文本文档的图示\n 图 3-4 三维特征空间\n图 3-3 和图 3-4 描绘了特征空间中的数据向量。 坐标轴表示单个单词，它们是词袋表示下的特征，空间中的点表示数据点（文本文档）。 有时在数据空间中查看特征向量也是有益的。 特征向量包含每个数据点中特征的值。 轴表示单个数据点和点表示特征向量。 图 3-5 展示了一个例子。 通过对文本文档进行词袋特征化，一个特征是一个词，一个特征向量包含每个文档中这个词的计数。 这样，一个单词被表示为一个“一个词向量”。正如我们将在第 4 章中看到的那样，这些文档词向量来自词袋向量的转置矩阵。\n Bag-of-N-gram Bag-of-N-gram 或者 bag-of-ngram 是 BOW 的自然延伸。 n-gram 是 n 个有序的记号（token）。一个词基本上是一个 1-gram，也被称为一元模型。当它被标记后，计数机制可以将单个词进行计数，或将重叠序列计数为 n-gram。例如，\u0026quot;Emma knocked on the door\u0026quot;这句话会产生 n-gram，如\u0026quot;Emma knocked\u0026quot;，\u0026quot;knocked on\u0026quot;，\u0026quot;on the\u0026quot;，\u0026quot;the door\u0026quot;。 N-gram 保留了文本的更多原始序列结构，故 bag-of-ngram可以提供更多信息。但是，这是有代价的。理论上，用 k 个独特的词，可能有 k 个独立的 2-gram（也称为 bigram）。在实践中，并不是那么多，因为不是每个单词后都可以跟一个单词。尽管如此，通常有更多不同的 n-gram（n \u0026gt; 1）比单词更多。这意味着词袋会更大并且有稀疏的特征空间。这也意味着 n-gram 计算，存储和建模的成本会变高。n 越大，信息越丰富，成本越高。\n为了说明随着 n 增加 n-gram 的数量如何增加，我们来计算纽约时报文章数据集上的 n-gram。我们使用 Pandas 和 scikit-learn 中的CountVectorizer转换器来计算前 10,000 条评论的 n-gram。\n\u0026gt;\u0026gt;\u0026gt; import pandas \u0026gt;\u0026gt;\u0026gt; import json \u0026gt;\u0026gt;\u0026gt; from sklearn.feature_extraction.text import CountVectorizer # Load the first 10,000 reviews  \u0026gt;\u0026gt;\u0026gt; f = open(\u0026#39;data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json\u0026#39;) \u0026gt;\u0026gt;\u0026gt; js = [] \u0026gt;\u0026gt;\u0026gt; for i in range(10000): ... js.append(json.loads(f.readline())) \u0026gt;\u0026gt;\u0026gt; f.close() \u0026gt;\u0026gt;\u0026gt; review_df = pd.DataFrame(js) # Create feature transformers for unigram, bigram, and trigram.  # The default ignores single-character words, which is useful in practice because it trims  # uninformative words. But we explicitly include them in this example for illustration purposes.  \u0026gt;\u0026gt;\u0026gt; bow_converter = CountVectorizer(token_pattern=\u0026#39;(?u)\\\\b\\\\w+\\\\b\u0026#39;) \u0026gt;\u0026gt;\u0026gt; bigram_converter = CountVectorizer(ngram_range=(2,2), token_pattern=\u0026#39;(?u)\\\\b\\\\w+\\\\b\u0026#39;) \u0026gt;\u0026gt;\u0026gt; trigram_converter = CountVectorizer(ngram_range=(3,3), token_pattern=\u0026#39;(?u)\\\\b\\\\w+\\\\b\u0026#39;) # Fit the transformers and look at vocabulary size  \u0026gt;\u0026gt;\u0026gt; bow_converter.fit(review_df[\u0026#39;text\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; words = bow_converter.get_feature_names() \u0026gt;\u0026gt;\u0026gt; bigram_converter.fit(review_df[\u0026#39;text\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; bigram = bigram_converter.get_feature_names() \u0026gt;\u0026gt;\u0026gt; trigram_converter.fit(review_df[\u0026#39;text\u0026#39;]) \u0026gt;\u0026gt;\u0026gt; trigram = trigram_converter.get_feature_names() \u0026gt;\u0026gt;\u0026gt; print (len(words), len(bigram), len(trigram)) 26047 346301 847545 # Sneak a peek at the ngram themselves \u0026gt;\u0026gt;\u0026gt; words[:10] [\u0026#39;0\u0026#39;, \u0026#39;00\u0026#39;, \u0026#39;000\u0026#39;, \u0026#39;0002\u0026#39;, \u0026#39;00am\u0026#39;, \u0026#39;00ish\u0026#39;, \u0026#39;00pm\u0026#39;, \u0026#39;01\u0026#39;, \u0026#39;01am\u0026#39;, \u0026#39;02\u0026#39;] \u0026gt;\u0026gt;\u0026gt; bigram[-10:] [\u0026#39;zucchinis at\u0026#39;, \u0026#39;zucchinis took\u0026#39;, \u0026#39;zucchinis we\u0026#39;, \u0026#39;zuma over\u0026#39;, \u0026#39;zuppa di\u0026#39;, \u0026#39;zuppa toscana\u0026#39;, \u0026#39;zuppe di\u0026#39;, \u0026#39;zurich and\u0026#39;, \u0026#39;zz top\u0026#39;, \u0026#39;à la\u0026#39;] \u0026gt;\u0026gt;\u0026gt; trigram[:10] [\u0026#39;0 10 definitely\u0026#39;, \u0026#39;0 2 also\u0026#39;, \u0026#39;0 25 per\u0026#39;, \u0026#39;0 3 miles\u0026#39;, \u0026#39;0 30 a\u0026#39;, \u0026#39;0 30 everything\u0026#39;, \u0026#39;0 30 lb\u0026#39;, \u0026#39;0 35 tip\u0026#39;, \u0026#39;0 5 curry\u0026#39;, \u0026#39;0 5 pork\u0026#39;] 复制ErrorOK!   图3-6 Number of unique n-gram in the first 10,000 reviews of the Yelp dataset\n过滤清洗特征 我们如何清晰地将信号从噪声中分离出来？ 通过过滤，使用原始标记化和计数来生成简单词表或 n-gram 列表的技术变得更加可用。 短语检测，我们将在下面讨论，可以看作是一个特别的 bigram 过滤器。 以下是执行过滤的几种方法。\n停用词 分类和检索通常不需要对文本有深入的理解。 例如，在\u0026quot;Emma knocked on the door\u0026quot;一句中，\u0026quot;on\u0026quot;和\u0026quot;the\u0026quot;这两个词没有包含很多信息。 代词、冠词和介词大部分时间并没有显示出其价值。流行的 Python NLP 软件包 NLTK 包含许多语言的语言学家定义的停用词列表。 （您将需要安装 NLTK 并运行nltk.download()来获取所有的好东西。）各种停用词列表也可以在网上找到。 例如，这里有一些来自英语停用词的示例词\nSample words from the nltk stopword list a, about, above, am, an, been, didn’t, couldn’t, i’d, i’ll, itself, let’s, myself, our, they, through, when’s, whom, ... 复制ErrorOK! 请注意，该列表包含撇号，并且这些单词没有大写。 为了按原样使用它，标记化过程不得去掉撇号，并且这些词需要转换为小写。\n基于频率的过滤 停用词表是一种去除空洞特征常用词的方法。还有其他更统计的方法来理解“常用词”的概念。在搭配提取中，我们看到依赖于手动定义的方法，以及使用统计的方法。同样的想法也适用于文字过滤。我们也可以使用频率统计。\n高频词 频率统计对滤除语料库专用常用词以及通用停用词很有用。例如，纽约时报文章数据集中经常出现“纽约时报”和其中单个单词。“议院”这个词经常出现在加拿大议会辩论的Hansard语料库中的“众议院”一词中，这是一种用于统计机器翻译的流行数据集，因为它包含所有文档的英文和法文版本。这些词在普通语言中有意义，但不在语料库中。手动定义的停用词列表将捕获一般停用词，但不是语料库特定的停用词。\n表 3-1 列出了 Yelp 评论数据集中最常用的 40 个单词。在这里，频率被认为是它们出现在文件（评论）中的数量，而不是它们在文件中的数量。正如我们所看到的，该列表涵盖了许多停用词。它也包含一些惊喜。\u0026quot;s\u0026quot;和\u0026quot;t\u0026quot;在列表中，因为我们使用撇号作为标记化分隔符，并且诸如\u0026quot;Mary's\u0026quot;或\u0026quot;did not\u0026quot;之类的词被解析为\u0026quot;Mary s\u0026quot;和\u0026quot;didn t\u0026quot;。词\u0026quot;good\u0026quot;，\u0026quot;food\u0026quot;和\u0026quot;great\u0026quot;分别出现在三分之一的评论中。但我们可能希望保留它们，因为它们对于情感分析或业务分类非常有用。\n 最常用的单词最可以揭示问题，并突出显示通常有用的单词通常在该语料库中曾出现过多次。 例如，纽约时报语料库中最常见的词是“时代”。实际上，它有助于将基于频率的过滤与停用词列表结合起来。还有一个棘手的问题，即何处放置截止点。 不幸的是这里没有统一的答案。在大多数情况下截断还需手动确定，并且在数据集改变时可能需要重新检查。\n稀有词 根据任务的不同，可能还需要筛选出稀有词。对于统计模型而言，仅出现在一个或两个文档中的单词更像噪声而非有用信息。例如，假设任务是根据他们的 Yelp 评论对企业进行分类，并且单个评论包含\u0026quot;gobbledygook\u0026quot;这个词。基于这一个词，我们将如何说明这家企业是餐厅，美容院还是一间酒吧？即使我们知道在这种情况下的这种生意发生在酒吧，它也会对于其他包含\u0026quot;gobbledygook\u0026quot;这个词的评论来说，这可能是一个错误。\n不仅稀有词不可靠，而且还会产生计算开销。这套 160 万个 Yelp 评论包含 357,481 个独特单词（用空格和标点符号表示），其中 189,915 只出现在一次评论中，41,162 次出现在两次评论中。超过 60% 的词汇很少发生。这是一种所谓的重尾分布，在现实世界的数据中非常普遍。许多统计机器学习模型的训练时间随着特征数量线性地变化，并且一些模型是二次的或更差的。稀有词汇会产生大量的计算和存储成本，而不会带来额外的收益。\n根据字数统计，可以很容易地识别和修剪稀有词。或者，他们的计数可以汇总到一个特殊的垃圾箱中，可以作为附加功能。图3-7展示了一个短文档中的表示形式，该短文档包含一些常用单词和两个稀有词\u0026quot;gobbledygook\u0026quot;和\u0026quot;zylophant\u0026quot;。通常单词保留自己的计数，可以通过停用词列表或其他频率进一步过滤方法。这些难得的单词会失去他们的身份并被分组到垃圾桶功能中.\n 由于在计算整个语料库之前不会知道哪些词很少，因此需要收集垃圾桶功能作为后处理步骤。\n由于本书是关于特征工程的，因此我们将重点放在特征上。但稀有概念也适用于数据点。如果文本文档很短，那么它可能不包含有用的信息，并且在训练模型时不应使用该信息。\n应用此规则时必须谨慎。维基百科转储包含许多不完整的存根，可能安全过滤。另一方面，推文本身就很短，并且需要其他特征和建模技巧。\n词干解析（Stemming） 简单解析的一个问题是同一个单词的不同变体会被计算为单独的单词。例如，\u0026quot;flower\u0026quot;和\u0026quot;flowers\u0026quot;在技术上是不同的记号，\u0026quot;swimmer\u0026quot;，\u0026quot;swimming\u0026quot;和\u0026quot;swim\u0026quot;也是如此，尽管它们的含义非常接近。如果所有这些不同的变体都映射到同一个单词，那将会很好。\n词干解析是一项 NLP 任务，试图将单词切分为基本的语言词干形式。有不同的方法。有些基于语言规则，其他基于观察统计。被称为词形化的算法的一个子类将词性标注和语言规则结合起来。\nPorter stemmer 是英语中使用最广泛的免费词干工具。原来的程序是用 ANSI C 编写的，但是很多其他程序包已经封装它来提供对其他语言的访问。尽管其他语言的努力正在进行，但大多数词干工具专注于英语。\n以下是通过 NLTK Python 包运行 Porter stemmer 的示例。正如我们所看到的，它处理了大量的情况，包括将\u0026quot;sixties\u0026quot;和\u0026quot;sixty\u0026quot;转变为同一根\u0026quot;sixti\u0026quot;。但这并不完美。单词\u0026quot;goes\u0026quot;映射到\u0026quot;goe\u0026quot;，而\u0026quot;go\u0026quot;映射到它自己。\n\u0026gt;\u0026gt;\u0026gt; import nltk \u0026gt;\u0026gt;\u0026gt; stemmer = nltk.stem.porter.PorterStemmer() \u0026gt;\u0026gt;\u0026gt; stemmer.stem(\u0026#39;flowers\u0026#39;) u\u0026#39;lemon\u0026#39; \u0026gt;\u0026gt;\u0026gt; stemmer.stem(\u0026#39;zeroes\u0026#39;) u\u0026#39;zero\u0026#39; \u0026gt;\u0026gt;\u0026gt; stemmer.stem(\u0026#39;stemmer\u0026#39;) u\u0026#39;stem\u0026#39; \u0026gt;\u0026gt;\u0026gt; stemmer.stem(\u0026#39;sixties\u0026#39;) u\u0026#39;sixti\u0026#39; \u0026gt;\u0026gt;\u0026gt; stemmer.stem(\u0026#39;sixty\u0026#39;) u\u0026#39;sixty\u0026#39; \u0026gt;\u0026gt;\u0026gt; stemmer.stem(\u0026#39;goes\u0026#39;) u\u0026#39;goe\u0026#39; \u0026gt;\u0026gt;\u0026gt; stemmer.stem(\u0026#39;go\u0026#39;) u\u0026#39;go\u0026#39; 复制ErrorOK! 词干解析的确有一个计算成本。 最终收益是否大于成本取决于应用程序。\n含义的原子：从单词到 N-gram 到短语 词袋的概念很简单。但是，一台电脑怎么知道一个词是什么？文本文档以数字形式表示为一个字符串，基本上是一系列字符。也可能会遇到 JSON blob 或 HTML 页面形式的半结构化文本。但即使添加了标签和结构，基本单位仍然是一个字符串。如何将字符串转换为一系列的单词？这涉及解析和标记化的任务，我们将在下面讨论。\n解析和分词 当字符串包含的不仅仅是纯文本时，解析是必要的。例如，如果原始数据是网页，电子邮件或某种类型的日志，则它包含额外的结构。人们需要决定如何处理日志中的标记，页眉，页脚或无趣的部分。如果文档是网页，则解析器需要处理 URL。如果是电子邮件，则可能需要特殊字段，例如 From，To 和 Subject 需要被特别处理，否则，这些标题将作为最终计数中的普通单词统计，这可能没有用处。\n解析后，文档的纯文本部分可以通过标记。这将字符串（一系列字符）转换为一系列记号。然后可以将每个记号计为一个单词。分词器需要知道哪些字符表示一个记号已经结束，另一个正在开始。空格字符通常是好的分隔符，正如标点符号一样。如果文本包含推文，则不应将井号（#）用作分隔符（也称为分隔符）。\n有时，分析需要使用句子而不是整个文档。例如，n-gram 是一个句子的概括，不应超出句子范围。更复杂的文本特征化方法，如 word2vec 也适用于句子或段落。在这些情况下，需要首先将文档解析为句子，然后将每个句子进一步标记为单词。\n字符串对象 字符串对象有各种编码，如 ASCII 或 Unicode。纯英文文本可以用 ASCII 编码。 一般语言需要 Unicode。 如果文档包含非 ASCII 字符，则确保分词器可以处理该特定编码。否则，结果将不正确。\n短语检测的搭配提取 连续的记号能立即被转化成词表和 n-gram。但从语义上讲，我们更习惯于理解短语，而不是 n-gram。在计算自然语言处理中，有用短语的概念被称为搭配。用 Manning 和 Schütze（1999：141）的话来说：“搭配是一个由两个或两个以上单词组成的表达，它们对应于某种常规的说话方式。”\n搭配比其部分的总和更有意义。例如，\u0026quot;strong tea\u0026quot;具有超越\u0026quot;great physical strength\u0026quot;和\u0026quot;tea\u0026quot;的不同含义，因此被认为是搭配。另一方面，“可爱的小狗”这个短语恰恰意味着它的部分总和：“可爱”和“小狗”。因此，它不被视为搭配。\n搭配不一定是连续的序列。\u0026quot;Emma knocked on the door\u0026quot;一词被认为包含搭配\u0026quot;knock door\u0026quot;，因此不是每一个搭配都是一个 n-gram。相反，并不是每个 n-gram 都被认为是一个有意义的搭配。\n由于搭配不仅仅是其部分的总和，它们的含义也不能通过单个单词计数来充分表达。作为一种表现形式，词袋不足。袋子的 ngram 也是有问题的，因为它们捕获了太多无意义的序列（考虑\u0026quot;this is in the bag-of-ngram example\u0026quot;），而没有足够的有意义的序列。\n搭配作为功能很有用。但是，如何从文本中发现并提取它们呢？一种方法是预先定义它们。如果我们努力尝试，我们可能会找到各种语言的全面成语列表，我们可以通过文本查看任何匹配。这将是非常昂贵的，但它会工作。如果语料库是非常特定领域的并且包含深奥的术语，那么这可能是首选的方法。但是这个列表需要大量的手动管理，并且需要不断更新语料库。例如，分析推文，博客和文章可能不太现实。\n自从统计 NLP 过去二十年出现以来，人们越来越多地选择用于查找短语的统计方法。统计搭配提取方法不是建立固定的短语和惯用语言列表，而是依赖不断发展的数据来揭示当今流行的语言。\n基于频率的方法 一个简单的黑魔法是频繁发生的 n-gram。这种方法的问题是最常发生的，这种可能不是最有用的。 表 3-2 显示了整个 Yelp 评论数据集中最流行的 bigram（n=2）。 正如我们所知的，按文件计数排列的最常见的十大常见术语是非常通用的术语，并不包含太多含义。\n 用于搭配提取的假设检验 原始流行度计数（Raw popularity count）是一个比较粗糙的方法。我们必须找到更聪慧的统计数据才能够轻松挑选出有意义的短语。关键的想法是看两个单词是否经常出现在一起。回答这个问题的统计机制被称为假设检验。\n假设检验是将噪音数据归结为“是”或“否”的答案。它涉及将数据建模为从随机分布中抽取的样本。随机性意味着人们永远无法 100% 的确定答案；总会有异常的机会。所以答案附在概率上。例如，假设检验的结果可能是“这两个数据集来自同一分布，其概率为 95%”。对于假设检验的温和介绍，请参阅可汗学院关于假设检验和 p 值的教程。\n在搭配提取的背景下，多年来已经提出了许多假设检验。最成功的方法之一是基于似然比检验（Dunning，1993）。对于给定的一对单词，该方法测试两个假设观察的数据集。假设 1（原假设）表示，词语 1 独立于词语 2 出现。另一种说法是说，看到词语1对我们是否看到词语2没有影响。假设 2（备选假设）说，看到词 1 改变了看到单词 2 的可能性。我们采用备选假设来暗示这两个单词形成一个共同的短语。因此，短语检测（也称为搭配提取）的似然比检验提出了以下问题：给定文本语料库中观察到的单词出现更可能是从两个单词彼此独立出现的模型中生成的，或者模型中两个词的概率纠缠？\n这是有用的。让我们算一点。（数学非常精确和简洁地表达事物，但它确实需要与自然语言完全不同的分析器。）\n 似然函数L(Data; H)表示在单词对的独立模型或非独立模型下观察数据集中词频的概率。为了计算这个概率，我们必须对如何生成数据做出另一个假设。最简单的数据生成模型是二项模型，其中对于数据集中的每个单词，我们抛出一个硬币，并且如果硬币朝上出现，我们插入我们的特殊单词，否则插入其他单词。在此策略下，特殊词的出现次数遵循二项分布。二项分布完全由词的总数，词的出现次数和词首概率决定。\n似然比检验分析常用短语的算法收益如下。\n  计算所有单体词的出现概率：p(w)。\n  计算所有唯一双元的条件成对词发生概率：p(W2 × W1)\n  计算所有唯一的双对数似然比对数。\n  根据它们的似然比排序双字节。\n  以最小似然比值作为特征。\n  掌握似然比测试 关键在于测试比较的不是概率参数本身，而是在这些参数（以及假设的数据生成模型）下观察数据的概率。可能性是统计学习的关键原则之一。但是在你看到它的前几次，这绝对是一个令人困惑的问题。一旦你确定了逻辑，它就变得直观了。\n还有另一种基于点互信息的统计方法。但它对真实世界文本语料库中常见的罕见词很敏感。因此它不常用，我们不会在这里展示它。\n请注意，搭配抽取的所有统计方法，无论是使用原始频率，假设测试还是点对点互信息，都是通过过滤候选词组列表来进行操作的。生成这种清单的最简单和最便宜的方法是计算 n-gram。它可能产生不连续的序列，但是它们计算成本颇高。在实践中，即使是连续 n-gram，人们也很少超过 bi-gram 或 tri-gram，因为即使在过滤之后，它们的数量也很多。为了生成更长的短语，还有其他方法，如分块或与词性标注相结合。\n分块（Chunking）和词性标注（part-of-Speech Tagging） 分块比 n-gram 要复杂一点，因为它基于词性，基于规则的模型形成了记号序列。\n例如，我们可能最感兴趣的是在问题中找到所有名词短语，其中文本的实体，主题最为有趣。 为了找到这个，我们使用词性标记每个作品，然后检查该标记的邻域以查找词性分组或“块”。 定义单词到词类的模型通常是语言特定的。 几种开源 Python 库（如 NLTK，Spacy 和 TextBlob）具有多种语言模型。\n为了说明 Python 中的几个库如何使用词性标注非常简单地进行分块，我们再次使用 Yelp 评论数据集。 我们将使用 spacy 和 TextBlob 来评估词类以找到名词短语。\n\u0026gt;\u0026gt;\u0026gt; import pandas as pd \u0026gt;\u0026gt;\u0026gt; import json # Load the first 10 reviews  \u0026gt;\u0026gt;\u0026gt; f = open(\u0026#39;data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json\u0026#39;) \u0026gt;\u0026gt;\u0026gt; js = [] \u0026gt;\u0026gt;\u0026gt; for i in range(10): js.append(json.loads(f.readline())) \u0026gt;\u0026gt;\u0026gt; f.close() \u0026gt;\u0026gt;\u0026gt; review_df = pd.DataFrame(js) ## First we\u0026#39;ll walk through spaCy\u0026#39;s functions  \u0026gt;\u0026gt;\u0026gt; import spacy # preload the language model  \u0026gt;\u0026gt;\u0026gt; nlp = spacy.load(\u0026#39;en\u0026#39;) # We can create a Pandas Series of spaCy nlp variables  \u0026gt;\u0026gt;\u0026gt; doc_df = review_df[\u0026#39;text\u0026#39;].apply(nlp) # spaCy gives you fine grained parts of speech using: (.pos_)  # and coarse grained parts of speech using: (.tag_)  \u0026gt;\u0026gt;\u0026gt; for doc in doc_df[4]: print([doc.text, doc.pos_, doc.tag_]) Got VERB VBP a DET DT letter NOUN NN in ADP IN the DET DT mail NOUN NN last ADJ JJ week NOUN NN that ADJ WDT said VERB VBD Dr. PROPN NNP Goldberg PROPN NNP is VERB VBZ moving VERB VBG to ADP IN Arizona PROPN NNP to PART TO take VERB VB a DET DT new ADJ JJ position NOUN NN there ADV RB in ADP IN June PROPN NNP . PUNCT . SPACE SP He PRON PRP will VERB MD be VERB VB missed VERB VBN very ADV RB much ADV RB . PUNCT . SPACE SP I PRON PRP think VERB VBP finding VERB VBG a DET DT new ADJ JJ doctor NOUN NN in ADP IN NYC PROPN NNP that ADP IN you PRON PRP actually ADV RB like INTJ UH might VERB MD almost ADV RB be VERB VB as ADV RB awful ADJ JJ as ADP IN trying VERB VBG to PART TO find VERB VB a DET DT date NOUN NN ! PUNCT . # spaCy also does some basic noun chunking for us  \u0026gt;\u0026gt;\u0026gt; print([chunk for chunk in doc_df[4].noun_chunks]) [a letter, the mail, Dr. Goldberg, Arizona, a new position, June, He, I, a new doctor, NYC, you, a date] #####  ## We can do the same feature transformations using Textblob  \u0026gt;\u0026gt;\u0026gt; from textblob import TextBlob # The default tagger in TextBlob uses the PatternTagger, which is fine for our example.  # You can also specify the NLTK tagger, which works better for incomplete sentences.  \u0026gt;\u0026gt;\u0026gt; blob_df = review_df[\u0026#39;text\u0026#39;].apply(TextBlob) \u0026gt;\u0026gt;\u0026gt; blob_df[4].tags [(\u0026#39;Got\u0026#39;, \u0026#39;NNP\u0026#39;), (\u0026#39;a\u0026#39;, \u0026#39;DT\u0026#39;), (\u0026#39;letter\u0026#39;, \u0026#39;NN\u0026#39;), (\u0026#39;in\u0026#39;, \u0026#39;IN\u0026#39;), (\u0026#39;the\u0026#39;, \u0026#39;DT\u0026#39;), (\u0026#39;mail\u0026#39;, \u0026#39;NN\u0026#39;), (\u0026#39;last\u0026#39;, \u0026#39;JJ\u0026#39;), (\u0026#39;week\u0026#39;, \u0026#39;NN\u0026#39;), (\u0026#39;that\u0026#39;, \u0026#39;WDT\u0026#39;), (\u0026#39;said\u0026#39;, \u0026#39;VBD\u0026#39;), (\u0026#39;Dr.\u0026#39;, \u0026#39;NNP\u0026#39;), (\u0026#39;Goldberg\u0026#39;, \u0026#39;NNP\u0026#39;), (\u0026#39;is\u0026#39;, \u0026#39;VBZ\u0026#39;), (\u0026#39;moving\u0026#39;, \u0026#39;VBG\u0026#39;), (\u0026#39;to\u0026#39;, \u0026#39;TO\u0026#39;), (\u0026#39;Arizona\u0026#39;, \u0026#39;NNP\u0026#39;), (\u0026#39;to\u0026#39;, \u0026#39;TO\u0026#39;), (\u0026#39;take\u0026#39;, \u0026#39;VB\u0026#39;), (\u0026#39;a\u0026#39;, \u0026#39;DT\u0026#39;), (\u0026#39;new\u0026#39;, \u0026#39;JJ\u0026#39;), (\u0026#39;position\u0026#39;, \u0026#39;NN\u0026#39;), (\u0026#39;there\u0026#39;, \u0026#39;RB\u0026#39;), (\u0026#39;in\u0026#39;, \u0026#39;IN\u0026#39;), (\u0026#39;June\u0026#39;, \u0026#39;NNP\u0026#39;), (\u0026#39;He\u0026#39;, \u0026#39;PRP\u0026#39;), (\u0026#39;will\u0026#39;, \u0026#39;MD\u0026#39;), (\u0026#39;be\u0026#39;, \u0026#39;VB\u0026#39;), (\u0026#39;missed\u0026#39;, \u0026#39;VBN\u0026#39;), (\u0026#39;very\u0026#39;, \u0026#39;RB\u0026#39;), (\u0026#39;much\u0026#39;, \u0026#39;JJ\u0026#39;), (\u0026#39;I\u0026#39;, \u0026#39;PRP\u0026#39;), (\u0026#39;think\u0026#39;, \u0026#39;VBP\u0026#39;), (\u0026#39;finding\u0026#39;, \u0026#39;VBG\u0026#39;), (\u0026#39;a\u0026#39;, \u0026#39;DT\u0026#39;), (\u0026#39;new\u0026#39;, \u0026#39;JJ\u0026#39;), (\u0026#39;doctor\u0026#39;, \u0026#39;NN\u0026#39;), (\u0026#39;in\u0026#39;, \u0026#39;IN\u0026#39;), (\u0026#39;NYC\u0026#39;, \u0026#39;NNP\u0026#39;), (\u0026#39;that\u0026#39;, \u0026#39;IN\u0026#39;), (\u0026#39;you\u0026#39;, \u0026#39;PRP\u0026#39;), (\u0026#39;actually\u0026#39;, \u0026#39;RB\u0026#39;), (\u0026#39;like\u0026#39;, \u0026#39;IN\u0026#39;), (\u0026#39;might\u0026#39;, \u0026#39;MD\u0026#39;), (\u0026#39;almost\u0026#39;, \u0026#39;RB\u0026#39;), (\u0026#39;be\u0026#39;, \u0026#39;VB\u0026#39;), (\u0026#39;as\u0026#39;, \u0026#39;RB\u0026#39;), (\u0026#39;awful\u0026#39;, \u0026#39;JJ\u0026#39;), (\u0026#39;as\u0026#39;, \u0026#39;IN\u0026#39;), (\u0026#39;trying\u0026#39;, \u0026#39;VBG\u0026#39;), (\u0026#39;to\u0026#39;, \u0026#39;TO\u0026#39;), (\u0026#39;find\u0026#39;, \u0026#39;VB\u0026#39;), (\u0026#39;a\u0026#39;, \u0026#39;DT\u0026#39;), (\u0026#39;date\u0026#39;, \u0026#39;NN\u0026#39;)] \u0026gt;\u0026gt;\u0026gt; print([np for np in blob_df[4].noun_phrases]) [\u0026#39;got\u0026#39;, \u0026#39;goldberg\u0026#39;, \u0026#39;arizona\u0026#39;, \u0026#39;new position\u0026#39;, \u0026#39;june\u0026#39;, \u0026#39;new doctor\u0026#39;, \u0026#39;nyc\u0026#39; 复制ErrorOK! 你可以看到每个库找到的名词短语有些不同。spacy 包含英语中的常见单词，如\u0026quot;a\u0026quot;和\u0026quot;the\u0026quot;，而 TextBlob 则删除这些单词。这反映了规则引擎的差异，它驱使每个库都认为是“名词短语”。 你也可以写你的词性关系来定义你正在寻找的块。使用 Python 进行自然语言处理可以深入了解从头开始用 Python 进行分块。\n总结 词袋模型易于理解和计算，对分类和搜索任务很有用。但有时单个单词太简单，不足以将文本中的某些信息封装起来。为了解决这个问题，人们寄希望于比较长的序列。Bag-of-ngram 是 BOW 的自然概括，这个概念仍然容于理解，而且它的计算开销这就像 BOW 一样容易。\nBag of-ngram 生成更多不同的 ngram。它增加了特征存储成本，以及模型训练和预测阶段的计算成本。虽然数据点的数量保持不变，但特征空间的维度现在更大。因此数据密度更为稀疏。n 越高，存储和计算成本越高，数据越稀疏。由于这些原因，较长的 n-gram 并不总是会使模型精度的得到提高（或任何其他性能指标）。人们通常在n = 2或 3 时停止。较少的 n-gram 很少被使用。\n防止稀疏性和成本增加的一种方法是过滤 n-gram 并保留最有意义的短语。这是搭配抽取的目标。理论上，搭配（或短语）可以在文本中形成非连续的标记序列。然而，在实践中，寻找非连续词组的计算成本要高得多并且没有太多的收益。因此搭配抽取通常从一个候选人名单中开始，并利用统计方法对他们进行过滤。\n所有这些方法都将一系列文本标记转换为一组断开的计数。与一个序列相比，一个集合的结构要少得多；他们导致平面特征向量。\n在本章中，我们用简单的语言描述文本特征化技术。这些技术将一段充满丰富语义结构的自然语言文本转化为一个简单的平面向量。我们讨论一些常用的过滤技术来降低向量维度。我们还引入了 ngram 和搭配抽取作为方法，在平面向量中添加更多的结构。下一章将详细介绍另一种常见的文本特征化技巧，称为 tf-idf。随后的章节将讨论更多方法将结构添加回平面向量。\n参考文献 Dunning, Ted. 1993. “Accurate methods for the statistics of surprise and\ncoincidence.” ACM Journal of Computational Linguistics, special issue on using large corpora , 19:1 (61—74).\n“Hypothesis Testing and p-Values.” Khan Academy, accessed May 31,\n2016,https://www.khanacademy.org/math/probability/statistics-inferential/hypothesis-testing/v/hypothesis-testing-and-p-values.\nManning,Christopher D. and Hinrich Schütze. 1999. Foundations of StatisticalNatural Language Processing . Cambridge, Massachusettes: MIT Press.\nSometimes people call it the document “vector.” The vector extends from the original and ends at the specified point. For our purposes, “vector” and “point” are the same thing.\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/text_features_tutorial/","summary":"前段时间发现apachecn在github上翻译了一本和特征工程相关的书籍：《Feature Engineering for Machine Learning》，中文名为《面向机器学习的特征工程》。\nFeature_Engineering_for_Machine_Learning.pdf\n三、文本数据: 展开、过滤和分块  译者：@kkejili\n校对者：@HeYun\n 如果让你来设计一个算法来分析以下段落，你会怎么做？\nEmma knocked on the door. No answer. She knocked again and waited. There was a large maple tree next to the house. Emma looked up the tree and saw a giant raven perched at the treetop. Under the afternoon sun, the raven gleamed magnificently. Its beak was hard and pointed, its claws sharp and strong. It looked regal and imposing.","title":"文本数据: 展开、过滤和分块"},{"content":"代码下载 现在一提到文本分析，除了词频统计、情感分析，就属话题分析最火，主流技术路线是使用LDA话题模型进行主题分析。但是LDA适合文档区分度大，文本档数较大。如果不满足这两点，LDA虽然能跑出模型，但是跑出的topic无法解读，没有意义。今天分享一个技术文，在看技术文之前，将技术文的背景文献稍微整理翻译了下，方便大家更好的理解textnets的应用场景。\n网络分析通常用于描述人与人之间的关系——尤其是在社会科学中——但它也可以应用于词之间的关系。例如，网络关系可以通过文档中单个单词的共现来创建，或者可以使用双模式网络投影在文档之间创建关系。\n基于网络的自动文本分析方法的优点是\n  像社会群体一样，可以通过三元闭包更准确地测量词组的含义——或者任何两个词或术语相互的含义的原则如果将它们放在第三个词的上下文中，可以更准确地理解；\n  文本网络可以应用于任何长度的文档，这与通常需要大量单词才能正常运行的主题模型不同。在简短的社交媒体文本变得普遍的时代，这是一个显着的优势。\n  最后，这种方法受益于社区检测跨学科文献的最新进展，可以说它提供了更准确的单词分组方法，这些方法受益于网络内观察到的聚类，而不是词袋模型。\n  背景-文化桥梁 文化信息传递理论和公共审议和计算技术。\n Markowitz, D. M., \u0026amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).\n 由于每天光顾此类论坛的人数迅速增加，社交媒体为倡导组织塑造公共辩论提供了有力的机会。 然而，社会科学家还没有解释为什么一些议题发起者能成功发起大规模的广泛参与性(公开辩论/广泛对话)，而大多数其他组织却没做到。 本文使用自动文本分析发现，如果组织方建立文化桥梁，在很少能一起讨论的议题领域内产生可连接的对话主题信息，这类信息不仅能引起多个受众的共鸣，而且还能让这些受众彼此进行对话，那么组织方更有可能激发新社交媒体受众的评论。 在控制这些因素的情况下，建立实质性文化桥梁的组织， 其所发布信息， 比那些没有建立实质性文化桥梁的组织， 得到的评论数多 2.52 倍。\n 社交网络分析通常用于描述个人之间的友谊或其他关系，但它也可通过参与者的消息或想法的类型来描述参与者之间的关系（如下图) 是“文化网络”中的一个小区域。\n 每个节点描述一个参与议题公开对话的参与者 节点间的边代表那些在社交媒体倡导领域内讨论类似议题的人。  PNAS2016这篇使用“自闭症谱系障碍ASD疾病的治病原因大讨论”做的数据分享，图中 t1 处的一类用户认为ASD致病可能跟疫苗有关，而另一类人可能认为ASD可能与遗传因素的有关。从图形看，t1这类议题发起方式，没有产生广泛参与性。而t2和t3，文化网络中因为文化桥梁的存在，产生了广泛参与性。\n假设的文化网络，其中节点代表参与有关议题的对话参与者，而节点之间的边则描述了其消息内容的相似性。议题广泛参与性，除了话题发起者影响力、话题投放资源等因素影响，还有一个因素就是发起的话题是否吸引了受众。对于参与者而言，最有吸引力的话题需要满足“新颖，且熟悉”。\n用TF-IDF刻画文化网络中的“新且熟悉” 在文本分析中有一个文本特征提取技术tf-idf\n tf指词语在某文档中出现的次数；从词语的角度，该值越大越熟悉 idf逆文档数，即词语出现在多少个文档中；从词语的角度，该值越小越新颖   本教程将引导您完成使用文本网络分析和可视化数据所需的所有步骤。 在解决与使用文本网络相关的其他杂项问题之前，本教程首先介绍了一个独立的示例。\n安装 pip3 install textnets \n1. 查看数据 pnas2016这篇的数据没有开源，通过文本构建文化网络、发现文化桥梁。这里使用一个特别特别小的新闻数据，关于人类第一次登月。如果我们使用textnets，准备的数据需要有两个列\n 议题参与者，类比报刊 议题参与者发布的内容，如评论等  import pandas as pd df = pd.read_csv(\u0026#39;test.csv\u0026#39;) df | | Unnamed: 0 | headlines | |---:|:------------------|:--------------------------------------------------------------------------| | 0 | The Guardian | 3:56 am: Man Steps On to the Moon | | 1 | New York Times | Men Walk on Moon -- Astronauts Land on Plain, Collect Rocks, Plant Flag | | 2 | Boston Globe | Man Walks on Moon | | 3 | Houston Chronicle | Armstrong and Aldrich \u0026#34;Take One Small Step for Man\u0026#34; on the Moon | | 4 | Washington Post | The Eagle Has Landed -- Two Men Walk on the Moon | | 5 | Chicago Tribune | Giant Leap for Mankind -- Armstrong Takes 1st Step on Moon | | 6 | Los Angeles Times | Walk on Moon -- That\\\u0026#39;s One Small Step for Man, One Giant Leap for Mankind | \n2. 导入corpus 使用textnets库的将数据导入为其特有的语料格式。从下方可以看到textnets可能会用spacy，如果要配置英文en_core_web_sm或中文zh_core_web_sm, 请查看该文 https://t.hk.uy/aCmr\nimport textnets as tn import pandas as pd #设置随机种子，保证代码可重复性 tn.params[\u0026#34;seed\u0026#34;] = 42 corpus = tn.Corpus.from_csv(\u0026#39;test.csv\u0026#39;) corpus   3. 构建网络 需要注意的是corpus.tokenized()是textnets特有的分词方法，如果所处理的新闻是中文，需要提前分词去停用词整理为像英文数据格式，用空格间隔单词。\ntextnets提供了构建网络的方法\ntn.Textnet(data, min_docs, connected, doc_attrs)\n data DataFrame类型, 三列，自己可以运行 corpus.tokenized() 查看样式 min_docs 一个词语存在于至少多少个文档中，默认为2。一个词至少出现在两个doc中，才会让两个doc产生连接 connected 仅保留网络的最大连接组件（默认值：False） doc_attrs 文档节点的属性，字典的字典(双层嵌套字典)  t = tn.Textnet(corpus.tokenized(), min_docs=1) 使用所有默认参数， textnets 会帮我们删除英文停用词，词干化(合并同类词)，并删除标点符号、数字、URL 等。\n但这里我们将破例将 min_docs 设置为1（因为数据只有几句话几十个单词，这里破例设置为1，正常这里至少是2）\nt.plot(label_nodes=True, #标记节点名(单词、媒体) show_clusters=True) #绘制簇的边界   show_clusters 使用 Leiden社区检测算法(Leiden community detection algorithm)找到了分区成簇，它似乎识别了同一主题(登月)下不同词之间的远近(相似的词在一个簇中，不同的词处于不同的簇中)。\n你可能会疑惑：为什么网络图中的单词: moon会自己漂移？ 那是因为moon这个词在每个文档中只出现一次，所以每个文档moon的tf-idf得分为0。\n让我们再次可视化相同的事情，但这次根据节点的 BiRank（二部网络的中心性度量）缩放节点，根据权重缩放边缘。\nt.plot(label_nodes=True, show_clusters=True, scale_nodes_by=\u0026#34;birank\u0026#34;, scale_edges_by=\u0026#34;weight\u0026#34;)   我们还可以只可视化报刊网络，不显示词语。这里设置node_type=\u0026lsquo;doc\u0026rsquo;\n#node_type有两种值， doc、term papers = t.project(node_type=\u0026#34;doc\u0026#34;) papers.plot(label_nodes=True)   和之前的双向网络一样，我们可以看到Houston Chronicle、 Chicago Tribune、 Los Angeles Times更紧密地聚集在一起。\n接下来，词网络：\nwords = t.project(node_type=\u0026#34;term\u0026#34;) words.plot(label_nodes=True, show_clusters=True)   除了可视化之外，我们还可以使用社交网络指标分析我们的语料库。 例如，具有教高介数中心性betweenness centrality的文档可能将主题不同簇联系起来，起到文化桥梁的作用，从而刺激跨越符号鸿沟的交流(Bail,2016)。\npapers.top_betweenness() Los Angeles Times 7.0 Boston Globe 0.0 Chicago Tribune 0.0 Houston Chronicle 0.0 New York Times 0.0 The Guardian 0.0 Washington Post 0.0 dtype: float64  words.top_betweenness() walk 72.00 man 18.00 step 16.00 small 12.75 land 6.00 giant 6.00 leap 6.00 mankind 6.00 armstrong 3.25 plain 0.00 dtype: float64  这是因为New York Times在其标题中使用了“walk”一词，将“one small step”簇与“man on moon”簇联系起来。\n我们可以再次生成词网络图，这次根据节点的中介中心性缩放节点，并使用“骨干提取”从网络中修剪边缘：cite:pSerrano2009。\n我们还可以使用 color_clusters（而不是 show_clusters）根据节点的分区为节点着色。\n我们可以过滤节点标签，只标记那些中间中心性betweenness centrality分数高于中位数的节点。 这在高阶网络中特别有用，其中标记每个节点会导致视觉混乱。\nwords.plot(label_nodes=True, scale_nodes_by=\u0026#34;betweenness\u0026#34;, color_clusters=True, alpha=0.5, edge_width=[10*w for w in words.edges[\u0026#34;weight\u0026#34;]], edge_opacity=0.4, node_label_filter=lambda n: n.betweenness() \u0026gt; words.betweenness.median())   其他textnets案例资料 https://www.jboy.space/blog/enemies-foreign-and-partisan.html\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pnas_culture_bridges/","summary":"代码下载 现在一提到文本分析，除了词频统计、情感分析，就属话题分析最火，主流技术路线是使用LDA话题模型进行主题分析。但是LDA适合文档区分度大，文本档数较大。如果不满足这两点，LDA虽然能跑出模型，但是跑出的topic无法解读，没有意义。今天分享一个技术文，在看技术文之前，将技术文的背景文献稍微整理翻译了下，方便大家更好的理解textnets的应用场景。\n网络分析通常用于描述人与人之间的关系——尤其是在社会科学中——但它也可以应用于词之间的关系。例如，网络关系可以通过文档中单个单词的共现来创建，或者可以使用双模式网络投影在文档之间创建关系。\n基于网络的自动文本分析方法的优点是\n  像社会群体一样，可以通过三元闭包更准确地测量词组的含义——或者任何两个词或术语相互的含义的原则如果将它们放在第三个词的上下文中，可以更准确地理解；\n  文本网络可以应用于任何长度的文档，这与通常需要大量单词才能正常运行的主题模型不同。在简短的社交媒体文本变得普遍的时代，这是一个显着的优势。\n  最后，这种方法受益于社区检测跨学科文献的最新进展，可以说它提供了更准确的单词分组方法，这些方法受益于网络内观察到的聚类，而不是词袋模型。\n  背景-文化桥梁 文化信息传递理论和公共审议和计算技术。\n Markowitz, D. M., \u0026amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).\n 由于每天光顾此类论坛的人数迅速增加，社交媒体为倡导组织塑造公共辩论提供了有力的机会。 然而，社会科学家还没有解释为什么一些议题发起者能成功发起大规模的广泛参与性(公开辩论/广泛对话)，而大多数其他组织却没做到。 本文使用自动文本分析发现，如果组织方建立文化桥梁，在很少能一起讨论的议题领域内产生可连接的对话主题信息，这类信息不仅能引起多个受众的共鸣，而且还能让这些受众彼此进行对话，那么组织方更有可能激发新社交媒体受众的评论。 在控制这些因素的情况下，建立实质性文化桥梁的组织， 其所发布信息， 比那些没有建立实质性文化桥梁的组织， 得到的评论数多 2.52 倍。\n 社交网络分析通常用于描述个人之间的友谊或其他关系，但它也可通过参与者的消息或想法的类型来描述参与者之间的关系（如下图) 是“文化网络”中的一个小区域。\n 每个节点描述一个参与议题公开对话的参与者 节点间的边代表那些在社交媒体倡导领域内讨论类似议题的人。  PNAS2016这篇使用“自闭症谱系障碍ASD疾病的治病原因大讨论”做的数据分享，图中 t1 处的一类用户认为ASD致病可能跟疫苗有关，而另一类人可能认为ASD可能与遗传因素的有关。从图形看，t1这类议题发起方式，没有产生广泛参与性。而t2和t3，文化网络中因为文化桥梁的存在，产生了广泛参与性。\n假设的文化网络，其中节点代表参与有关议题的对话参与者，而节点之间的边则描述了其消息内容的相似性。议题广泛参与性，除了话题发起者影响力、话题投放资源等因素影响，还有一个因素就是发起的话题是否吸引了受众。对于参与者而言，最有吸引力的话题需要满足“新颖，且熟悉”。\n用TF-IDF刻画文化网络中的“新且熟悉” 在文本分析中有一个文本特征提取技术tf-idf\n tf指词语在某文档中出现的次数；从词语的角度，该值越大越熟悉 idf逆文档数，即词语出现在多少个文档中；从词语的角度，该值越小越新颖   本教程将引导您完成使用文本网络分析和可视化数据所需的所有步骤。 在解决与使用文本网络相关的其他杂项问题之前，本教程首先介绍了一个独立的示例。","title":"PNAS | 文本网络分析\u0026文化桥梁Python代码实现"},{"content":"textgenrnn是在Keras/Tensorflow基础上搭建的Python包，特性:\n 有现代的神经网络架构，使用注意力权重和嵌入Embedding来加速训练和提升模型质量 支持字符级别和单词级别的训练 可设置RNN尺寸、RNN层数、是否使用双向RNN 可支持对任意输入文本的训练，包括大文件 可以使用GPU训练，使用CPU生成文本 提供基于GPU的cuDNN，以加速模型训练 使用情景标签训练模型，更快的学习，产出更好的效果。  安装 pip3 install textgenrnn \n快速上手 from textgenrnn import textgenrnn textgen = textgenrnn() textgen.generate() Run\n[Spoiler] Anyone else find this post and their person that was a little more than I really like the Star Wars in the fire or health and posting a personal house of the 2016 Letter for the game in a report of my backyard. \n使用新文本训练新模型也很简单\ntextgen.train_from_file(\u0026#39;hacker_news_2000.txt\u0026#39;, num_epochs=1) textgen.generate() Run\nProject State Project Firefox \n生成3个论文标题按照疯狂程度的增加顺序（temperature越高，生成算法偏离学习概率分布的程度越大)\ntextgen.generate(3, temperature=1.0) Run\nWhy we got money “regular alter” Urburg to Firefox acquires Nelf Multi Shamn Kubernetes by Google’s Bern \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/textgenrnn/","summary":"textgenrnn是在Keras/Tensorflow基础上搭建的Python包，特性:\n 有现代的神经网络架构，使用注意力权重和嵌入Embedding来加速训练和提升模型质量 支持字符级别和单词级别的训练 可设置RNN尺寸、RNN层数、是否使用双向RNN 可支持对任意输入文本的训练，包括大文件 可以使用GPU训练，使用CPU生成文本 提供基于GPU的cuDNN，以加速模型训练 使用情景标签训练模型，更快的学习，产出更好的效果。  安装 pip3 install textgenrnn \n快速上手 from textgenrnn import textgenrnn textgen = textgenrnn() textgen.generate() Run\n[Spoiler] Anyone else find this post and their person that was a little more than I really like the Star Wars in the fire or health and posting a personal house of the 2016 Letter for the game in a report of my backyard.","title":"神经网络textgenrnn库生成文本"},{"content":"30天入门Python，需要你耐得住寂寞，又能静下心来敲代码，不要惧怕英文的Python教程，其实敲代码运行代码的过程与玩游戏无异。\n引言 Python 是一种用于通用编程的高级编程语言。它是一种开源的、解释性的、面向对象的编程语言。 Python 是由荷兰程序员 Guido van Rossum 创建的。 Python 编程语言的名称来源于英国的小品喜剧系列，Month Python\u0026rsquo;s Flying Circus。第一个版本于 1991 年 2 月 20 日发布。这个为期 30 天的 Python 挑战将帮助您逐步学习最新版本的 Python3。这些主题分为 30 天，每天包含几个主题，带有易于理解的解释、真实示例、许多动手练习和项目。\n这个挑战是为想要学习 Python 编程语言的初学者和专业人士设计的。完成挑战可能需要30到100天，积极参与电报群的人完成挑战的概率很高。\nWhy Python ? 它是一种非常接近人类语言的编程语言，因此易于学习和使用。 Python 被各种行业和公司（包括 Google）使用。 它已被用于开发 Web 应用程序、桌面应用程序、系统管理和机器学习库。 Python 是数据科学和机器学习社区中高度接受的语言。 我希望这足以说服你开始学习 Python。 Python 正在吞噬世界，而你在它吃掉你之前就应该杀了它。\n目录    # Day Topics     01 引言   02 变量,内置函数   03 四则运算   04 字符串   05 列表   06 元组   07 集合   08 字典   09 条件语句   10 循环语句   11 函数   12 魔窟   13 列表生成式   14 高级函数   15 类型错误   16 日期   17 异常处理语句   18 正则表达式   19 文件读写   20 包管理   21 类\u0026amp;对象   22 网络爬虫   23 虚拟环境   24 统计   25 Pandas   26 Web开发   27 MongoDB数据库   28 API接口   29 搭建API   30 总结    Welcome 恭喜决定参加 30 天的 Python 编程挑战。 在这个挑战中，您将学习成为 Python 程序员所需的一切以及整个编程概念。 在挑战结束时，您将获得 30DaysOfPython 编程挑战证书。\n如果您想积极参与挑战，可以加入30DaysOfPython 挑战(https://t.me/ThirtyDaysOfPython) 电报群。\n下载课件 https://github.com/Asabeneh/30-Days-Of-Python/archive/refs/heads/master.zip\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/30_days_of_python/","summary":"30天入门Python，需要你耐得住寂寞，又能静下心来敲代码，不要惧怕英文的Python教程，其实敲代码运行代码的过程与玩游戏无异。\n引言 Python 是一种用于通用编程的高级编程语言。它是一种开源的、解释性的、面向对象的编程语言。 Python 是由荷兰程序员 Guido van Rossum 创建的。 Python 编程语言的名称来源于英国的小品喜剧系列，Month Python\u0026rsquo;s Flying Circus。第一个版本于 1991 年 2 月 20 日发布。这个为期 30 天的 Python 挑战将帮助您逐步学习最新版本的 Python3。这些主题分为 30 天，每天包含几个主题，带有易于理解的解释、真实示例、许多动手练习和项目。\n这个挑战是为想要学习 Python 编程语言的初学者和专业人士设计的。完成挑战可能需要30到100天，积极参与电报群的人完成挑战的概率很高。\nWhy Python ? 它是一种非常接近人类语言的编程语言，因此易于学习和使用。 Python 被各种行业和公司（包括 Google）使用。 它已被用于开发 Web 应用程序、桌面应用程序、系统管理和机器学习库。 Python 是数据科学和机器学习社区中高度接受的语言。 我希望这足以说服你开始学习 Python。 Python 正在吞噬世界，而你在它吃掉你之前就应该杀了它。\n目录    # Day Topics     01 引言   02 变量,内置函数   03 四则运算   04 字符串   05 列表   06 元组   07 集合   08 字典   09 条件语句   10 循环语句   11 函数   12 魔窟   13 列表生成式   14 高级函数   15 类型错误   16 日期   17 异常处理语句   18 正则表达式   19 文件读写   20 包管理   21 类\u0026amp;对象   22 网络爬虫   23 虚拟环境   24 统计   25 Pandas   26 Web开发   27 MongoDB数据库   28 API接口   29 搭建API   30 总结    Welcome 恭喜决定参加 30 天的 Python 编程挑战。 在这个挑战中，您将学习成为 Python 程序员所需的一切以及整个编程概念。 在挑战结束时，您将获得 30DaysOfPython 编程挑战证书。","title":"30天Python编程学习挑战"},{"content":"[论文下载The predictive utility of word familiarity for online engagements and funding.pdf](The predictive utility of word familiarity for online engagements and funding.pdf)\n Markowitz, D. M., \u0026amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).\n 摘要 处理流畅性等元认知框架通常表明人们对简单和通用的语言的反应比复杂和技术语言更有利。与复杂的信息相比，人们更容易处理简单和非技术性的信息，因此会更多地与目标进行互动。在涵盖 12 个现场样本（总 n = 1,064,533）的两项研究中，我们通过展示人们在付出时间和注意力时更多地使用非技术语言（例如，简单的在线语言往往会获得更多社交信息）来建立并复制这种越简单越好的现象订婚）。然而，人们在捐款时会对复杂的语言做出反应（例如，慈善捐赠活动和赠款摘要中的复杂语言往往会收到更多的钱）。这一证据表明，人们根据时间或金钱目标以不同的方式使用复杂语言的启发式方法。这些结果强调语言是社会和心理过程的镜头，以及大规模测量文本模式的计算方法。\n processing fluency field studies automated text analysis common words jargon  ## 研究背景-复杂词汇的负面效应 术语（jargon），是复杂的、技术的、专业的语言，与日常语言相比，加工难度更大、更不流畅。许多关于加工流畅度（processing fluency）的研究都发现了使用术语的负面结果： 由于术语会给予人们不熟悉、加工困难的感觉，从而导致其较难理解。使用术语来描述手术过程的医生可能导致病人关于健康风险的错误估计；阅读了关于一项技术的复杂描述的人们（与阅读简单描述的人们相比）对该技术的理解更差并高估其风险。因此，不常用的、技术性的词汇通常不被看好，因为人们对其不熟悉而感觉较难加工，并给人们带来理解上的挑战。\n然而，对于复杂词汇的影响，以往研究基本基于实验室结果，效应的强度、健壮性、对真实行为的预测性等仍不清楚。此外，以往大多数关于加工流畅度（processing fluency）的研究都依赖于人们的主观判断，即通过询问被试对于简单或复杂文本的感受来判断效应的大小。该研究则弥补了这两点不足，将加工流畅度操作性定义为词法流畅度（lexical fluency，即所用的词汇为通用词汇还是复杂词汇），并考察复杂词汇对人们在真实世界中行为的影响。\n工具性启发法 工具性启发法（instrumentality heuristic）认为，如果一个感觉很困难的经历是有助于达到特定目标的，人们会给予这个经历更高的评价。由此，如果工具性目标被激活，那么加工流畅性低的复杂文本，反而可能会被给予更高的评价。对此，该研究同时考察了复杂词汇对于线上参与度（社会参与度）和资金筹集的影响。\n研究假设  假设一：没有工具性目标被激活时，人们更喜欢简单的语言，表现为更高的社会参与度 假设二：工具性目标被激活时，人们更喜欢复杂的语言，表现为更多的资金支持  实验结果支持这两个假设：通用词汇与更多的线上支持（高社会参与度）相关，复杂词汇则与更多的资金支持相关。\n数据 研究一的数据包括：   来自左倾（纽约时报）、右倾（福克斯新闻）、中立（美联社）的新闻媒体的推特\n  随机选择来自上述三个组织的的记者/名人的个人推特\n  共和党政治家和特朗普手下的推特\n  Reddit文章标题\n  科学论文（来自PLoS One）的标题和、摘要\n  TED演讲标题、内容\n  研究二的数据包括：  三个慈善平台   a) Kickstarter，主要是关于对创意项目的投资\n b) Indiegogo，主要是关于对创意项目和初创企业的投资\n c) GoFundMe，时要是关于生活事件的筹募（医疗、事故等）\nNIH基金申请书的摘要  数据分析 自动文本分析工具：研究使用自动文本分析工具LIWC（Linguistic Inquiry and Word Count）来对文本进行分析。LIWC词典是一个经过专家和统计分析认证的工具，其包含了6400个代表“非正式、非专业”的英语单词。研究者把通用词汇的比例操作性定义为文本中LIWC词典中词汇的比例。\n混合效应回归分析：使用混合效应回归分析的方法对数据进行分析。其中，回归模型中的控制变量主要有5类，分别是信息源（如新闻来源、演讲者、作者），时间（如年份、视频长度、发帖距今时间、发表时间），主题（如社会/政治等），金钱（如申请成功与否、货币类型）和投入程度（如出资人的数量、股份的数量）。\n数据转换：\n  研究一中，由于发表时间更长的信息更可能有更高的线上参与度，因此计算中所有参与度指标均除以了数据提取日期与发表日期之间的时间距离（数据提取-发表日期）。此外，对于考察的社会参与度指标，均进行了log转换。下文（表XX）中的点赞率、转发率等，均指代经过了上述转换后的点赞数、转发数等。\n  对研究一参与度相关指标求和时（如推特点赞率与转发率之和），对各指标标准化后再求和。\n  研究二中的因变量（各数据集中的所得资金数额）亦均进行了log转换。\n  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pnas_text_fluency/","summary":"[论文下载The predictive utility of word familiarity for online engagements and funding.pdf](The predictive utility of word familiarity for online engagements and funding.pdf)\n Markowitz, D. M., \u0026amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).\n 摘要 处理流畅性等元认知框架通常表明人们对简单和通用的语言的反应比复杂和技术语言更有利。与复杂的信息相比，人们更容易处理简单和非技术性的信息，因此会更多地与目标进行互动。在涵盖 12 个现场样本（总 n = 1,064,533）的两项研究中，我们通过展示人们在付出时间和注意力时更多地使用非技术语言（例如，简单的在线语言往往会获得更多社交信息）来建立并复制这种越简单越好的现象订婚）。然而，人们在捐款时会对复杂的语言做出反应（例如，慈善捐赠活动和赠款摘要中的复杂语言往往会收到更多的钱）。这一证据表明，人们根据时间或金钱目标以不同的方式使用复杂语言的启发式方法。这些结果强调语言是社会和心理过程的镜头，以及大规模测量文本模式的计算方法。\n processing fluency field studies automated text analysis common words jargon  ## 研究背景-复杂词汇的负面效应 术语（jargon），是复杂的、技术的、专业的语言，与日常语言相比，加工难度更大、更不流畅。许多关于加工流畅度（processing fluency）的研究都发现了使用术语的负面结果： 由于术语会给予人们不熟悉、加工困难的感觉，从而导致其较难理解。使用术语来描述手术过程的医生可能导致病人关于健康风险的错误估计；阅读了关于一项技术的复杂描述的人们（与阅读简单描述的人们相比）对该技术的理解更差并高估其风险。因此，不常用的、技术性的词汇通常不被看好，因为人们对其不熟悉而感觉较难加工，并给人们带来理解上的挑战。","title":"PNAS|词汇熟悉度对线上参与和资金筹集的预测性效用"},{"content":"PyWebIO https://github.com/pywebio/PyWebIO\n快速构建 Web 应用的 Python 工具。通过该项目你可在不写 HTML、CSS、JS 代码的前提下，仅用 Python 快速完成一个包含数据展示、表单的小型 Web 应用页面\n{{ \u0026lt; figure src=\u0026ldquo;img/PyWebIO.gif\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\npottery https://github.com/brainix/pottery\n以 Python 的方式操作 Redis 的库。忘记那些 Redis 命令吧，只要你知道如何使用 Python 字典，那么你就会用这个库操作 Redis\n\u0026gt;\u0026gt;\u0026gt; from pottery import RedisList \u0026gt;\u0026gt;\u0026gt; tel = RedisDict({\u0026#39;jack\u0026#39;: 4098, \u0026#39;sape\u0026#39;: 4139}, redis=redis, key=\u0026#39;tel\u0026#39;) \u0026gt;\u0026gt;\u0026gt; tel[\u0026#39;guido\u0026#39;] = 4127 \u0026gt;\u0026gt;\u0026gt; tel RedisDict{\u0026#39;jack\u0026#39;: 4098, \u0026#39;sape\u0026#39;: 4139, \u0026#39;guido\u0026#39;: 4127} \u0026gt;\u0026gt;\u0026gt; tel[\u0026#39;jack\u0026#39;] 4098 \u0026gt;\u0026gt;\u0026gt; squares = RedisList([1, 4, 9, 16, 25], redis=redis, key=\u0026#39;squares\u0026#39;) \u0026gt;\u0026gt;\u0026gt; squares RedisList[1, 4, 9, 16, 25] \u0026gt;\u0026gt;\u0026gt; squares[0] 1 \nspider-flow https://github.com/ssssssss-team/spider-flow\n新一代爬虫平台，以图形化方式定义爬虫流程，不写代码即可完成爬虫。\n{{ \u0026lt; figure src=\u0026ldquo;img/spiderflow.gif\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nPaddleSpeech https://github.com/PaddlePaddle/PaddleSpeech\n百度飞浆开源的语音工具包，可实现端到端语音同声翻译， 文本转声音等。\n{{ \u0026lt; figure src=\u0026ldquo;img/PaddleSpeech.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nPaddleGAN https://github.com/PaddlePaddle/PaddleGAN\n百度飞浆开源的GAN项目，图片的变脸卡通化、视频图片的修复。。。挺好玩的，感兴趣可以试试\n{{ \u0026lt; figure src=\u0026ldquo;img/PaddleGAN.jpeg\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nlatexcv https://github.com/jankapunkt/latexcv\n一组用 LaTeX 编写的简历和简历模板。\n{{ \u0026lt; figure src=\u0026ldquo;img/latexcv1.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\n{{ \u0026lt; figure src=\u0026ldquo;img/latexcv2.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nMockingBird https://github.com/babysor/MockingBird\nAI拟声: 5秒内克隆您的声音并生成任意语音内容 https://www.bilibili.com/video/BV17Q4y1B7mY/\n{{ \u0026lt; figure src=\u0026ldquo;img/MockingBird.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nTextNet https://github.com/jboynyc/textnets\ntextnet将文档集表示为文档和单词的网络,为文本分析与可视化提供了新的可能性。\n{{ \u0026lt; figure src=\u0026ldquo;img/textnet.svg\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nfinetuner https://github.com/jina-ai/finetuner\nFinetuner 可调整任何深度神经网络的权重，以便更好地嵌入搜索任务。\n  专为微调而设计\n  强大而直观：您只需要finetuner.fit()\n  与框架无关：承诺在 PyTorch、Tensorflow/Keras 和 PaddlePaddle 深度学习后端提供相同的 API 和用户体验。\n  Jina 集成：与 Jina 平滑集成，减少实验和生产之间的上下文切换成本。\n  {{ \u0026lt; figure src=\u0026ldquo;img/finetuner.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\ntaguette https://github.com/remram44/taguette\n免费开源的定性研究工具——MIRROR OF GITLAB REPOSITORY\n{{ \u0026lt; figure src=\u0026ldquo;img/taguette.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\n##covid19-daily-newsletter\nhttps://github.com/edoriggio/covid19-daily-newsletter\n每日电子邮件以跟踪 COVID-19 的传播, 该项目有价值的地方是我们可以借鉴搭建自己的newsletter平台。\n{{ \u0026lt; figure src=\u0026ldquo;img/newsletter1.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\n{{ \u0026lt; figure src=\u0026ldquo;img/newsletter2.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nvizzu https://github.com/vizzuhq/vizzu-lib\n一个 JS 的数据可视化动画库，功能很强，可以用来制作数据图表的动画。\n{{ \u0026lt; figure src=\u0026ldquo;img/vizzu.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nCrimeKgAssitant https://github.com/liuhuanyong/CrimeKgAssitant\n罪名法务智能项目,内容包括856项罪名知识图谱, 基于280万罪名训练库的罪名预测,基于20W法务问答对的13类问题分类与法律资讯问答功能.\n##了解课程\n 点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly07/","summary":"PyWebIO https://github.com/pywebio/PyWebIO\n快速构建 Web 应用的 Python 工具。通过该项目你可在不写 HTML、CSS、JS 代码的前提下，仅用 Python 快速完成一个包含数据展示、表单的小型 Web 应用页面\n{{ \u0026lt; figure src=\u0026ldquo;img/PyWebIO.gif\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\npottery https://github.com/brainix/pottery\n以 Python 的方式操作 Redis 的库。忘记那些 Redis 命令吧，只要你知道如何使用 Python 字典，那么你就会用这个库操作 Redis\n\u0026gt;\u0026gt;\u0026gt; from pottery import RedisList \u0026gt;\u0026gt;\u0026gt; tel = RedisDict({\u0026#39;jack\u0026#39;: 4098, \u0026#39;sape\u0026#39;: 4139}, redis=redis, key=\u0026#39;tel\u0026#39;) \u0026gt;\u0026gt;\u0026gt; tel[\u0026#39;guido\u0026#39;] = 4127 \u0026gt;\u0026gt;\u0026gt; tel RedisDict{\u0026#39;jack\u0026#39;: 4098, \u0026#39;sape\u0026#39;: 4139, \u0026#39;guido\u0026#39;: 4127} \u0026gt;\u0026gt;\u0026gt; tel[\u0026#39;jack\u0026#39;] 4098 \u0026gt;\u0026gt;\u0026gt; squares = RedisList([1, 4, 9, 16, 25], redis=redis, key=\u0026#39;squares\u0026#39;) \u0026gt;\u0026gt;\u0026gt; squares RedisList[1, 4, 9, 16, 25] \u0026gt;\u0026gt;\u0026gt; squares[0] 1","title":"TechWeekly-07| 每周有趣有用的技术分享"},{"content":"cntext更新至1.1 版本 本次更新了共现矩阵的计算函数。\n更新方法\npip3 install cntext --upgrade 或指定版本安装\npip3 install cntext==1.1 \nco_occurrence_matrix 词共现矩阵\nfrom cntext.dictionary import co_occurrence_matrix documents = [\u0026#34;I go to school every day by bus .\u0026#34;, \u0026#34;i go to theatre every night by bus\u0026#34;] co_occurrence_matrix(documents, window_size=2, lang=\u0026#39;english\u0026#39;)   documents2 = [\u0026#34;编程很好玩\u0026#34;, \u0026#34;Python是最好学的编程\u0026#34;] co_occurrence_matrix(documents2, window_size=2, lang=\u0026#39;chinese\u0026#39;)   广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/cntext_upgrade/","summary":"cntext更新至1.1 版本 本次更新了共现矩阵的计算函数。\n更新方法\npip3 install cntext --upgrade 或指定版本安装\npip3 install cntext==1.1 \nco_occurrence_matrix 词共现矩阵\nfrom cntext.dictionary import co_occurrence_matrix documents = [\u0026#34;I go to school every day by bus .\u0026#34;, \u0026#34;i go to theatre every night by bus\u0026#34;] co_occurrence_matrix(documents, window_size=2, lang=\u0026#39;english\u0026#39;)   documents2 = [\u0026#34;编程很好玩\u0026#34;, \u0026#34;Python是最好学的编程\u0026#34;] co_occurrence_matrix(documents2, window_size=2, lang=\u0026#39;chinese\u0026#39;)   广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"共词矩阵 | cntext更新至1.1"},{"content":"音素 音素是构成语音的基本声音，音节和单词建立在音节上。在与语音和语言处理相关的各种应用（例如文本到语音系统）中，将文本从其拼写形式转录为语音字母表是一项重要要求。\nPhonemizer 是一个精确寻址的 Python 包, 它将文本从其拼写表示转录为语音表示。 该包设计用户友好的，并公开了一个高级音素化函数， 支持大约100种不同的语言。phonemizer 使用的默认后端是 eSpeak （Dunn \u0026amp; Vitolins，2019 年），一种基于语言专业知识和手写转录规则的文本转语音软件。它将文本转录成国际音标，并支持一百多种语言。使用 MBROLA 声音（Tits \u0026amp; Vitolins，2019），eSpeak 后端可用于大约 35 种语言，以 SAMPA 计算机可读语音字母表转录文本。\n安装 安装phonemizer前需要配置espeak-ng，\n win https://github.com/espeak-ng/espeak-ng/releases下载对应的msi文件点击安装 mac 首先配置好homebrew，之后命令行brew install espeak  pip3 install phonemizer 音素化phonemize from phonemizer import phonemize\nphonemize(text, language=\u0026lsquo;en-us\u0026rsquo;, prepend_text=False, preserve_punctuation=False, with_stress=False, njobs=1)\n text 文本列表 language 语言。\u0026ldquo;en-us\u0026quot;美国英语， \u0026ldquo;zh\u0026quot;中文 prepend_text 输出结果保留输入的文本，默认False preserve_punctuation 输出结果保留标点符号，默认False with_stress 标记重读，默认False njobs 并行运算核数，默认使用cpu的1个核。  from phonemizer import phonemize texts = [\u0026#39;hello, my name is david\u0026#39;, \u0026#39;nice to meet you!\u0026#39;] # Do this: phonemized = phonemize(texts, language=\u0026#39;en-us\u0026#39;) phonemized Run\n['həloʊ maɪ neɪm ɪz deɪvɪd ', 'naɪs tə miːt juː ']  但上面的用法速度较慢， 更高效的写法应该为\nfrom phonemizer.backend import EspeakBackend backend = EspeakBackend(language=\u0026#39;en-us\u0026#39;) texts = [\u0026#39;hello, my name is david\u0026#39;, \u0026#39;nice to meet you!\u0026#39;] phonemized = backend.phonemize(texts) phonemized Run\n142 µs ± 851 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  给每个单词构造音素，输出结果为字典样式\nfrom phonemizer.backend import EspeakBackend from phonemizer.punctuation import Punctuation from phonemizer.separator import Separator backend = EspeakBackend(\u0026#39;en-us\u0026#39;) text = \u0026#39;It amused him to think that they were probably talking about him at that very moment\u0026#39; words = [word for word in text.lower().split(\u0026#39; \u0026#39;)] # 忽略词语边界，音素之间用空格间隔 separator = Separator(phone=\u0026#39; \u0026#39;, word=None) lexicon = {word: backend.phonemize([word], separator=separator, strip=True)[0] for word in words} lexicon Run\n{'it': 'ɪ t', 'amused': 'ɐ m j uː s d', 'him': 'h ɪ m', 'to': 't uː', 'think': 'θ ɪ ŋ k', 'that': 'ð æ t', 'they': 'ð eɪ', 'were': 'w ɜː', 'probably': 'p ɹ ɑː b ə b l i', 'talking': 't ɔː k ɪ ŋ', 'about': 'ɐ b aʊ t', 'at': 'æ t', 'very': 'v ɛ ɹ i', 'moment': 'm oʊ m ə n t'}  中文的音素化 text_zhs = [\u0026#39;你好我的名字是大卫\u0026#39;, \u0026#39;很高兴认识你\u0026#39;] phonemized_zhs = phonemize(text_zhs, language=\u0026#39;zh\u0026#39;) phonemized_zhs Run\n[\u0026#39;ni2 xɑu2 wo2 tə1 miɜŋ tsi̪5 s.i.5 tɑ5 wei5 \u0026#39;, \u0026#39;xə2n kɑu5 ɕi5ŋ ʐə5n s.i.1 ni2 \u0026#39;] \nfrom phonemizer.backend import EspeakBackend from phonemizer.punctuation import Punctuation from phonemizer.separator import Separator import re text = \u0026#39;想到他们可能在那个时候谈论他，他觉得好笑\u0026#39; words = re.findall(\u0026#39;[\\u4e00-\\u9fa5]\u0026#39;, text) backend = EspeakBackend(\u0026#39;zh\u0026#39;) separator = Separator(phone=\u0026#39; \u0026#39;, word=None) # 构建每个汉字一个对应的音素表达，输出结果为字典样式 lexicon = {word: backend.phonemize([word], separator=separator, strip=True)[0] for word in words} lexicon Run\n{'想': 'ɕ iɑ2 ŋ ', '到': 't ɑu5 ', '他': 'th ɑ5 ', '们': 'm ə1 n ', '可': 'kh o2 ', '能': 'n əɜ ŋ ', '在': 'ts ai5 ', '那': 'n ɑ5 ', '个': 'k o1 ', '时': 's. i.ɜ ', '候': 'x ou5 ', '谈': 'th aɜ n ', '论': 'l uə5 n ', '觉': 'tɕ yɛɜ ', '得': 't ə1 ', '好': 'x ɑu2 ', '笑': 'ɕ j ɑu5 '}  心理学相关概念 这个包用起来比较简单，但是想到一个场景，说不定可以多个角度去分析文本。\n经常看文本的时候，脑海里不自觉的读出声音，这种现象今天查了下叫做“听觉表象”，听觉表象产生于语言的视觉区和语言的运动区——角回和布洛卡区。通过听觉，听出相应的字的声音，我们就可以领会到这句话的意思。从常人的生长发育的过程，我们都是先牙牙学语，通过声音理解内容的含义，而后经过十数教育学会语言书面文字，掌握文字系统，此时我们的神经回路是“视觉(听觉)~记忆~理解”\n对一个东西的“知觉”在心理学上叫做这个东西的“知觉表征”，相当于是把这个东西转码为了一个，用于后续在心理上对这个东西进行加工。这个心理符号的编码与这个东西本身的特征以及最初的感觉通道有关——一个真正的苹果常会被以视觉的方式编码，成为一个视觉知觉表征，可以简单理解成这个苹果的图像；一个词“苹果”常会被以听觉的方式编码，成为一个AVL单元，可以简单地理解成把“苹果”这两个字和“ping’guo”这个读音打包在一起的一个文件。\n感觉代码被经过某些处理后储存在记忆当中，当有一天需要用的时候再被从记忆里提取出来。这个时候的提取，本身是一种建构，也就是与将感觉处理后存储起来的一个相反的过程——所以心理学上把我们所提取（建构）的这个代码就称为表象。表象和知觉是机能等价的（Neisser，1972），可以简单理解为表象就是是一种基于过去经验的知觉。这也解释了我们默读词汇的时候，这个AVL单元里“语音”的部分是哪里来的了——这是我们基于对自己声音的了解而建构的一种听觉表象。简单来说这个语音就是根据我们对自己声音的认识，来“想象”的读出来的声音。\n 知乎回答-心理学哈士奇\nConrad R (1963). Acoustic confusions and memory span for words. Nature, 197: 1029-1030.\nNeisser U (1972). Changing conception of imagery. In P W Sheehan (ED), The Function and Nature of Imagery. London: Achademic Press. 知乎回答-心理学哈士奇\n 引用格式 Bernard, M. and Titeux, H. (2021). Phonemizer: Text to phones transcription for multiple languages in python. Journal of Open Source Software, 6(68):3958.\n@article{Bernard2021, doi = {10.21105/joss.03958}, url = {https://doi.org/10.21105/joss.03958}, year = {2021}, publisher = {The Open Journal}, volume = {6}, number = {68}, pages = {3958}, author = {Mathieu Bernard and Hadrien Titeux}, title = {Phonemizer: Text to Phones Transcription for Multiple Languages in Python}, journal = {Journal of Open Source Software} } \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/phonemizer/","summary":"音素 音素是构成语音的基本声音，音节和单词建立在音节上。在与语音和语言处理相关的各种应用（例如文本到语音系统）中，将文本从其拼写形式转录为语音字母表是一项重要要求。\nPhonemizer 是一个精确寻址的 Python 包, 它将文本从其拼写表示转录为语音表示。 该包设计用户友好的，并公开了一个高级音素化函数， 支持大约100种不同的语言。phonemizer 使用的默认后端是 eSpeak （Dunn \u0026amp; Vitolins，2019 年），一种基于语言专业知识和手写转录规则的文本转语音软件。它将文本转录成国际音标，并支持一百多种语言。使用 MBROLA 声音（Tits \u0026amp; Vitolins，2019），eSpeak 后端可用于大约 35 种语言，以 SAMPA 计算机可读语音字母表转录文本。\n安装 安装phonemizer前需要配置espeak-ng，\n win https://github.com/espeak-ng/espeak-ng/releases下载对应的msi文件点击安装 mac 首先配置好homebrew，之后命令行brew install espeak  pip3 install phonemizer 音素化phonemize from phonemizer import phonemize\nphonemize(text, language=\u0026lsquo;en-us\u0026rsquo;, prepend_text=False, preserve_punctuation=False, with_stress=False, njobs=1)\n text 文本列表 language 语言。\u0026ldquo;en-us\u0026quot;美国英语， \u0026ldquo;zh\u0026quot;中文 prepend_text 输出结果保留输入的文本，默认False preserve_punctuation 输出结果保留标点符号，默认False with_stress 标记重读，默认False njobs 并行运算核数，默认使用cpu的1个核。  from phonemizer import phonemize texts = [\u0026#39;hello, my name is david\u0026#39;, \u0026#39;nice to meet you!","title":"Phonemizer音素化 Python文本语音表征包"},{"content":"  Omnizart 是一个 Python 库，是自动音乐转录的简化解决方案。该库收集了音乐与文化技术实验室(https://sites.google.com/view/mctl/home)的研究成果，分析和弦音乐并转录乐器的音符 、和弦 、drum events、帧级人声旋律、音符级人声旋律。\nOmnizart 提供构建基于深度学习的音乐转录生命周期的主要功能，涵盖从数据集下载、特征预处理、模型训练、转录和声音化。还提供了预先训练的检查点，以便立即使用转录。该论文可以从 Journal of Open Source Software (JOSS) 中找到。\n演示 Colab 使用 Colab notebook https://bit.ly/OmnizartColab几乎可以立即转录您最喜欢的歌曲！\n声音样本 原声\n\n和弦转录\n 鼓点转录\n 音符级人声转录\n 帧级语音转录\n Source files can be downloaded here. You can use Audacity to open the files.\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/omnizart/","summary":"  Omnizart 是一个 Python 库，是自动音乐转录的简化解决方案。该库收集了音乐与文化技术实验室(https://sites.google.com/view/mctl/home)的研究成果，分析和弦音乐并转录乐器的音符 、和弦 、drum events、帧级人声旋律、音符级人声旋律。\nOmnizart 提供构建基于深度学习的音乐转录生命周期的主要功能，涵盖从数据集下载、特征预处理、模型训练、转录和声音化。还提供了预先训练的检查点，以便立即使用转录。该论文可以从 Journal of Open Source Software (JOSS) 中找到。\n演示 Colab 使用 Colab notebook https://bit.ly/OmnizartColab几乎可以立即转录您最喜欢的歌曲！\n声音样本 原声\n\n和弦转录\n 鼓点转录\n 音符级人声转录\n 帧级语音转录\n Source files can be downloaded here. You can use Audacity to open the files.\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"OMNIZART: 音乐转录变得容易"},{"content":" author:cj1128\nGithub: https://github.com/cj1128\nBlog: https://cjting.me/\nsrc: https://cjting.me/2021/08/07/fourier-transform-and-audio-visualization/\n 印象中使用的第一个 PC 音乐播放器是「千千静听」，大概是 08 年左右。我还清楚地记得它自带了一首梁静茹的歌「Love is everything」，动听的旋律至今萦绕耳旁。\n 千千静听的左上角有一组随着音乐跳动的柱子，我想大家都习以为常了，很多播放器都有这功能。但是其实有没有想过，这是怎么实现的？\n要理解这个问题，我们首先要理解声音是什么。\n波 中学物理有介绍过，声音是由物体振动产生的，这个振动经由介质传导到我们的耳朵中引起耳膜振动从而让我们听见声音。\n我们可以用波形来描述振动，其中横轴是时间，纵轴是振动的位移，也就是离开原点的距离。\n振动的两个关键属性是频率和振幅，频率是指一秒钟振动多少次，对应于音高，频率越高声音越尖锐刺耳。\n振幅则表示最大的位移值，对应于音量，振幅越大声音越响。\n 上图是用 matplotlib 绘制的一个正弦波，可以看出在 0.01s 内振动了 2 次，因此频率是 200，振幅是 1。\nfrom matplotlib import pyplot as plt import numpy as np def sin_wave(hz): x = np.linspace(0, 0.01, 1000, endpoint=False) y = np.sin(x * hz * 2 * np.pi) plt.plot(x, y) plt.xlabel(\u0026#34;Time\u0026#34;) plt.axhline(y=0, color=\u0026#39;k\u0026#39;) plt.show() sin_wave(200) 这种最简单的波形对应的声音叫做 pure tone，顾名思义，非常的简单而且纯粹。\n下面是用 scipy 生成的 200hz 的声音，大家可以听听看。\n import scipy.io.wavfile as wav SAMPLE_RATE = 44100 DURATION = 5 def write_wav(hz): x = np.linspace(0, DURATION, SAMPLE_RATE * DURATION, endpoint=False) tone = np.sin(x * hz * 2 * np.pi) normalized_tone = np.int16((tone / tone.max()) * 32767) wav.write(f\u0026#34;{hz}hz.wav\u0026#34;, SAMPLE_RATE, normalized_tone) write_wav(200) 这是 800hz 的声音，可以明显感觉到，声音更加尖锐了。\n 现实世界中我们听到的声音不会是 pure tone，而是各种 pure tone 叠加的结果。\n下图中蓝色是 200hz，黄色是 800hz，而绿色则是它们叠加的结果，已经不再是一个 pure tone 了。\n 下面是绿色曲线听起来的效果。\n 总结一下：\n 我们可以用一个波形图来表示声音，其中 X 轴是时间，Y 轴是振动的位移 最简单的正弦波对应的声音叫做 pure tone 日常中我们听到的声音都是各种 pure tone 叠加的结果  Tip:\n如果想了解一点乐理，LIGHTNOTE 非常不错。\n它的网页做得很棒，通过互动的形式讲述基本的乐理知识，包括音阶、和弦，十二平均律等。\n采样 因为声音是一个随时间变化的连续函数，任意一段间隔内都有无穷多个值，而无穷多的数据是没办法存储在计算机中的。\n想要存储，我们就需要将它离散化变成离散序列，具体的方法就是采样，使用固定的间隔对函数进行求值。\n这是原始的声音。\n 这是采样后的结果。\n 通过采样，我们将一个无尽序列变成了一个有限序列，其中每一个值叫做样本，这样就可以方便地在计算机中存储了。\n采样的关键参数有两个，分别是采样频率和采样深度。\n采样频率是指每秒钟采样多少次，很明显，采样频率越高，样本越多，数据量越大，同时也更接近原始的声音。\n采样深度是指用多少比特去存储采样得到的值，使用的比特越多，还原得到的声音越细腻，和图片的色彩深度是一个道理。\n假设我们用 16bit 采样深度和 44100 的采样频率，那么一段一秒钟的声音就变成了一个 44100 大小的 int16 数组。\n得到采样值数组以后，接下来如何存储这个数组就是编码的范畴了。我们可以直接存，也可以采用某种算法压缩以后再存。各种各样的办法，对应着各种各样的音频格式，比如 MP3, AAC, WAV 等。\n这其中 AAC 和 MP3 格式是有损的，也就是存储以后再读出来，得到的样本和原始的会有一些区别，但这些区别很细微，可以忽略。有损的特点就是在基本不影响最终播放效果的情况下，大幅度减少文件体积。\n而 WAV 则是无损的，输入是什么，读出来就是什么，缺点自然是体积要大很多。\n我们可以通过 scipy 来读取 WAV 音频。\nimport scipy.io.wavfile as wav rate, all_samples = wav.read(\u0026#34;xxx.wav\u0026#34;) print(rate, len(all_samples), all_samples.dtype) # 44100 10639873 int16 # 上面分别是：采样率，样本总数，以及样本值的类型 # int16 说明每一个样本是 16bit 整数 print(all_samples[:20]) # [-41 -51 -49 -41 -28 -15 -20 -33 -32 -38 -54 -54 -44 -30 -8 10 11 2 # -14 -36] # 可以看到样本就是一组数字 可以看出，采样频率是 44100，一共有 10639873 个样本，使用 int16 进行存储，计算可知这首歌的时间是 241 秒。\n傅里叶变换 给定一个 200hz 的波形和一个 800hz 的波形，计算它们叠加以后的结果非常简单，没有任何难度。\n 但是，如果给到叠加以后的结果呢？\n 能看出上面的绿色曲线是 200hz+800hz 叠加得到的吗？完全看不出来，甚至，这样的逆推真的可行吗？还是说和永动机一样在原理上就是不可行的？\n答案是可行的，通过傅里叶变换，我们可以将一个复合波形拆解为构成它的简单波形。\n想想都觉得不可思议，这是怎么做到的？傅里叶又是怎么想到的？不得不说十八世纪的法国数学家傅里叶是一个天才，让人仰望。\nTip: 3Blue1Brown 有一个视频介绍傅里叶变换的原理，非常棒：形象的介绍：什么是傅里叶变换？\n接下来我们来看看怎样通过傅里叶变换来拆解复合波形。\n我们可以认为傅里叶变换是一个函数，它的输入为 N 个实数，代表采样值，输出为 N 个复数，代表不同频率的分量，这里我们忽略复数的实部和虚部，只关心它的模，也就是绝对值。\nTip: 这里关于傅里叶变换的说法并不完整，比如傅里叶变换的输入也可以是复数。但是对我们来说理解到这一层面就够了。如果想要深入研究，可以去学习数字信号处理相关的课程。\n我们使用 scipy 来实际进行一下傅里叶变换。\nfrom scipy.fft import fft, fftfreq # 这是即将进行变换的采样值: [0, 1, 2, 3, 4, 5, 6, 7] samples = np.arange(8) # 这是傅里叶变换的结果 y = fft(samples) for i in y: print(i) # 变换结果为 8 个复数 # (28-0j) # (-3.9999999999999996+9.65685424949238j) # (-4+4j) # (-4+1.6568542494923797j) # (-4-0j) # (-4-1.6568542494923797j) # (-4-4j) # (-3.9999999999999996-9.65685424949238j) # x 是上面每个结果对应的频率 # 这里 fftfreq 函数的第一个参数为样本数量 # 第二个参数为 采样率的 倒数，我们假定采样率为 8 x = fftfreq(len(samples), 1 / 8) print(x) # [ 0. 1. 2. 3. -4. -3. -2. -1.] # 这里意味着 y[0] 对应的频率是 0，y[1] 对应的频率是 1 # 负数频率是什么含义？我们可以不管，忽略 上面的代码我们构造了 8 个数字进行傅里叶变换，并假定这组样本是使用采样率 8 进行采样得到的。\n变换的结果是 8 个复数，这 8 个复数对应 8 个频率，我们可以看出，正频率和负频率对应的变换结果是一样的，也就是变换后的结果是对称的。\n上面得到的结果意义不大，因为输入没什么意义。现在我们使用傅里叶变换处理一下 200hz+800hz 叠加后的波形，看看是否能还原回 200hz 和 800hz。\nimport numpy as np from matplotlib import pyplot as plt from matplotlib.pyplot import figure from scipy.fft import fft, fftfreq figure(figsize=(14, 6), dpi=80) DURATION = 0.01 SAMPLE_RATE = 44100 def gen_sine_wave(freq): x = np.linspace(0, DURATION, int(DURATION * SAMPLE_RATE), endpoint=False) y = np.sin(x * freq * 2 * np.pi) return y hz200 = gen_sine_wave(200) hz800 = gen_sine_wave(800) # 叠加后的采样数据 # 这里每个样本不是 int16，而是 float，无关紧要 total = hz200 + hz800 y = fft(total) x = fftfreq(len(total), 1 / SAMPLE_RATE) # 此时横轴是频率，纵轴是该频率的分量 # 使用 np.abs 计算复数的模 plt.plot(x, np.abs(y)) plt.show() 图中我们可以看出，首先输出结果是左右对称的，我们忽略掉负频率部分，只关注正频率。\n其次，傅里叶变换告诉我们该输入信号由 2 个频率组成，程序运行时鼠标放上去可以看出是 200 和 800。也就是说，通过傅里叶变换，我们将复合波形拆解为了简单波形。\n或者说，通过傅里叶变换，我们将一个信号拆解为了一组不同频率的正弦波，将它从时域变换到了频域。信号还是这个信号，只不过换个角度去看它。\n总结一下：\n 傅里叶变换是一个函数，输入一串数字代表样本值，输出一串复数代表频率分量 每个数字具体代表的频率可以根据样本数量和采样频率计算得知 我们不关心复数的方向，我们只关心复数的模 傅里叶输出的结果是左右对称的，因此只有一半的信息有价值  因为输出结果是对称的，只有一半的信息有价值，因此有一个变体叫做 rfft，只返回一半的信息，计算速度可以更快一些。\nimport numpy as np from scipy.fft import rfft, rfftfreq SAMPLE_RATE = 8 samples = np.arange(8) # 此时 x 和 y 都是 5 个值 y = rfft(samples) x = rfftfreq(len(samples), 1 / SAMPLE_RATE) # y[0] 表示频率 0 的分量 # y[1] 表示频率 1 的分量 print(x) # [0. 1. 2. 3. 4.] for i in y: print(i) # 和之前 fft/fftfreq 的输出对比，可以发现结果是一样的，只不过去掉了对称的冗余信息 # (28+0j) # (-3.9999999999999996+9.65685424949238j) # (-4+4j) # (-4+1.6568542494923797j) # (-4+0j) \n音频可视化 到了这里所有实现音频可视化所需要的知识都备齐了。\n首先我们可以猜到，随音乐跳动的每个柱子，对应的是一个频率或一组频率，而柱子的高度则是频率的分量大小，这两个信息傅里叶变换都能给到。\n现在剩下的问题则是输入是什么？我们不可能把一首歌曲的所有样本作为输入进行傅里叶变换，如果这样做，我们只能得到一份频率数据。\n我们希望得到的频率数据随着音乐在变化，因此这里要选择一个窗口大小（FFT_SIZE），比如 2048。随着音乐的播放，我们每次都从当前播放位置选择 2048 个样本然后进行傅里叶变换。\n现在我们的初步音频可视化方案就确定了：\n 解析音频文件得到 allSamples 每次绘图时，从当前 sample 开始选择 FFT_SIZE 个 sample 对这些 sample 进行傅里叶变换 对变换得到的复数求模 把结果变换到 0 ~ 1 然后绘图  接下来我们用 Web 来实现一个简单的音频可视化工具。\n首先，解析音频文件得到 allSamples 我们可以使用 WebAudio 的 API。\n// 得到某个音频文件的二进制数据 const ab = fetch(\u0026#34;xxx.mp3\u0026#34;).then(res =\u0026gt; res.arrayBuffer()) // 新建 WebAudio context const audioCtx = new AudioContext() // 解析 arrayBuffer const audioBuffer = audioCtx.decodeAudioData(ab) console.log(audioBuffer) // AudioBuffer {length: 10639872, duration: 241.2669387755102, sampleRate: 44100, numberOfChannels: 2} // duration: 241.2669387755102 // length: 10639872 // numberOfChannels: 2 // sampleRate: 44100 // [[Prototype]]: AudioBuffer  // 一般来说音频都有多个声道，用于立体声播放 // 这里我们选第一个声道就可以了 const allSamples = audioBuffer.getChannelData(0) // allSamples 就是我们要的样本数组，每一个样本值是浮点数 console.log(allSamples.slice(0, 10)) // Float32Array(10) [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 接下来，我们使用 requestAnimationFrame 来进行绘制。\n每次绘制的时候，需要先获取当前的播放位置。audioBuffer 中有音频的总时间，通过 audioCtx.currentTime 我们可以得知当前的播放时间，两个相除便是播放位置。\n// 用户点击播放时 const onPlay = () =\u0026gt; { // 记录下播放的开始时间  const startTime = audioCtx.currentTime const draw = () =\u0026gt; { requestAnimationFrame(draw) // 目前播放了多少时间 (seconds)  const cur = audioCtx.currentTime - startTime // 播放的进度是多少  const per = cur / audioBuffer.duration const startIndex = Math.floor(allSamples.length * per) // 从 startIndex 开始切割 FFT_SIZE 个 sample 出来  // 此时要进行傅里叶变换的 samples 就准备好了  const samples = allSamples.slice(startIndex, startIndex + FFT_SIZE) } requestAnimationFrame(draw) } 接下来就是傅里叶变换了，这里我找了一下，JS 相关的库很少，不过验证下来 fft.js 是可用的。\nTip: 不得不说科学计算还是 Python 方便，numpy 和 scipy 真的太好用了。\n经过 FFT 以后，我们得到了 1025 个复数，对这些复数取模，得到 1025 个实数。\n// JS 没有复数，我们自行定义一个 class Complex { constructor(real, imag) { this.real = real this.imag = imag } abs() { return Math.sqrt(this.real * this.real + this.imag * this.imag) } } const rfft = (samples) =\u0026gt; { const f = new FFTJS(samples.length) const out = f.createComplexArray() const N = samples.length / 2 + 1 f.realTransform(out, samples) const value = [] for(let i = 0; i \u0026lt; N; i++) { value[i] = new Complex(out[2*i+0], out[2*i+1]) } return value } // 此时我们得到了一组实数 const y = rfft(samples).map(c =\u0026gt; c.abs()) Tip:\n其实这里还有一个过程，我们不会直接把 samples 进行 FFT 变换，而是会先应用一个 Window Function，这样做的目的如果感兴趣可以自行了解。\n对于我们来说，Window Function 就是一个 number[] -\u0026gt; number[] 的函数。\n接下来就是将这些数字变换到 0 ~ 1 区间，这一步比较简单，观察一下最大最小值做个线性变换就行了。\nconst result = y.map(v =\u0026gt; (v + 20) / 80) 有了一组 0 ~ 1 的数字以后，我们就可以使用 Canvas 进行绘图了。\nconst W = 800 // canvas width const H = 600 // canvas height  const draw = (spectrum) =\u0026gt; { ctx.clearRect(0, 0, W, H) const barWidth = W / spectrum.length for(let i = 0; i \u0026lt; spectrum.length; i++) { const v = spectrum[i] const x = i * barWidth const height = v * H const y = H - height ctx.fillRect(x, y, barWidth, height) } } 然后我们来看看结果，打开 audio-vis-demo，选择一个音频文件，取消勾选 “Smooth”，点击播放，然后我们会发现，柱子可以正常地绘制出来，但是它们”跳跃“得非常厉害，不够平稳。\n这是因为我们漏了一步，叫做 Time smoothing。\n具体的机理我并不理解，所以就不再说明了，对我们的目标来说，这部分当做黑盒处理就可以了。\n这里我想说明一下，在我看来，学习任何知识都是分层的，学到自己感兴趣的层级即可，对下一层有一个定性的认识，需要的时候可以再去细究。\n我们可以认为 Time smoothing 就是对当前的 result 和上一个 result 进行某种操作，然后输出一组值。\n下面是具体的算法，代码非常简单，但是效果非常明显。\nconst smoothConstantDown = 0.08 const smoothConstantUp = 0.8 if(lastY != null) { for(let i = 0; i \u0026lt; lastY.length; i++) { if(y[i] \u0026lt; lastY[i]) { lastY[i] = y[i] * smoothConstantDown + lastY[i] * (1 - smoothConstantDown) } else { lastY[i] = y[i] * smoothConstantUp + lastY[i] * (1 - smoothConstantUp) } } } else { lastY = y } 我们在 Demo 中勾选上 “Smooth” 启用这个算法，然后就会发现图像马上变得不再跳动了，整体结果已经和其他播放器很接近了。\n注意，这里我们是为了学习底层的步骤和细节，所以很多操作都自己来做。如果真的需要开发相关功能，使用 WebAudio 的 API 是更合理的选择。\n我在 audio-vis-demo 中也基于 WebAudio 的 API 进行了实现，主要是为了对比和参考。\n到这里，我们完整地实现了从一组数字到可视化图形的全过程，深切地感受到了数学的魅力。这里的编码一点也不难，难的，或者是让人赞叹的，是背后的数学，人类智慧皇冠上最灿烂的明珠。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/voice2pic/","summary":"author:cj1128\nGithub: https://github.com/cj1128\nBlog: https://cjting.me/\nsrc: https://cjting.me/2021/08/07/fourier-transform-and-audio-visualization/\n 印象中使用的第一个 PC 音乐播放器是「千千静听」，大概是 08 年左右。我还清楚地记得它自带了一首梁静茹的歌「Love is everything」，动听的旋律至今萦绕耳旁。\n 千千静听的左上角有一组随着音乐跳动的柱子，我想大家都习以为常了，很多播放器都有这功能。但是其实有没有想过，这是怎么实现的？\n要理解这个问题，我们首先要理解声音是什么。\n波 中学物理有介绍过，声音是由物体振动产生的，这个振动经由介质传导到我们的耳朵中引起耳膜振动从而让我们听见声音。\n我们可以用波形来描述振动，其中横轴是时间，纵轴是振动的位移，也就是离开原点的距离。\n振动的两个关键属性是频率和振幅，频率是指一秒钟振动多少次，对应于音高，频率越高声音越尖锐刺耳。\n振幅则表示最大的位移值，对应于音量，振幅越大声音越响。\n 上图是用 matplotlib 绘制的一个正弦波，可以看出在 0.01s 内振动了 2 次，因此频率是 200，振幅是 1。\nfrom matplotlib import pyplot as plt import numpy as np def sin_wave(hz): x = np.linspace(0, 0.01, 1000, endpoint=False) y = np.sin(x * hz * 2 * np.pi) plt.plot(x, y) plt.xlabel(\u0026#34;Time\u0026#34;) plt.axhline(y=0, color=\u0026#39;k\u0026#39;) plt.show() sin_wave(200) 这种最简单的波形对应的声音叫做 pure tone，顾名思义，非常的简单而且纯粹。","title":"音频可视化：采样、频率和傅里叶变换"},{"content":" Bollen, Johan, et al. \u0026ldquo;Historical language records reveal a surge of cognitive distortions in recent decades.\u0026rdquo; Proceedings of the National Academy of Sciences 118.30 (2021).\n 摘要 患有抑郁症的人容易出现适应不良的思维模式，即认知扭曲，他们以过于消极和不准确的方式思考自己、世界和未来。 这些扭曲与个人情绪、行为和语言的显着变化有关。 我们假设社会可以经历类似的集体心理变化，这些变化会反映在语言使用的历史记录中。我们调查了过去 125 年超 1400 万本书中认知扭曲（congnition disorder）的文本标记的流行情况，并观察到自 1980 年代以来它们的流行程度激增，达到超过大萧条和两次世界大战的水平。 这种模式似乎不是由词义、出版和写作标准或 Google 图书样本的变化驱动的。 我们的研究结果发现，通过语言分析最近的社会转向与认知扭曲和内化障碍相关。\n正文 抑郁症与独特且可识别的适应不良思维模式有关，称为认知扭曲，其中个人以不准确和过于消极的方式思考自己、未来和世界 (9-12)。例如，当个体用消极的、绝对主义的术语（例如，“I am a loser”）给自己贴上标签时，就会出现在抑郁症中看到的认知扭曲。他们可能会用二分法、极端的术语谈论未来事件（例如，“My meeting will be a complete disaster”）或对别人的心态做出毫无根据的假设（例如，“Everybody will think that I am a failure”）。\n认知扭曲的类型通常区分许多部分重叠的类型，例如“灾难化”、“二分推理”、“否定积极的”、“情感推理”、“算命”、“标记和错误标记”、“放大和最小化”、“心理过滤”、“读心术”、“过度概括”、“个性化”和“应该陈述”。\n**认知行为疗法 (cognitive-behavioral therapy，CBT) ** 是治疗抑郁症和其他内化障碍的黄金标准 (13)，其基础理论认为认知扭曲与内化障碍有关； 它们反映了环境压力下的负面情感和回避行为模式 (14, 15)。 语言与这种动态密切相关。 事实上，最近的研究表明，患有内化障碍的个体在他们的语言中表现出明显更高水平的认知扭曲 (16, 17)，以至于他们的患病率可能被用作抑郁症易感性的指标 (18, 19)。\n我们分析了过去 125 年中大量以英语、西班牙语和德语出版的超过 1400 万本书籍（谷歌图书）中的大量认知扭曲标记的流行情况。具体来说，我们正在研究由 CBT 专家、计算语言学家和双语母语人士组成的团队设计的数百个 1 到 5 个单词 (n-gram)、 标记的认知失真图式 (cognitive distortion schemata,CDS) 的纵向流行情况，以及由 CBT 专家小组外部验证，以捕捉 12 种认知扭曲的表达 (9)。 CDS n-gram 被设计为简短、明确和独立的语句，使用频率很高的术语表达特定认知扭曲类型的核心（图 1 和 SI 附录，表 S1-S3）。例如，3-gram 的“I am a”捕获了标签和错误标签失真，而不管其上下文或所涉及的精确标签（“女士”、“尊贵的人”、“失败者”等）。这些相同的 n-gram 在早期的研究中被证明显着更多。\nCDS流行度测量 CDS n-gram 显示在灰色框内的示例，周围是合理的上下文词，这些词可能会有所不同，而不会影响 n-gram 是否标记给定类型的认知扭曲的表达（例如，读心术Mindreading、情感推理Emotiona lReasoning、标记Labeling和错误标记Mislabeling） . CDS 是由 CBT 专家、语言学家和母语使用者组成的团队设计的，用于捕捉特定认知扭曲类型的表达，而不管其特定的词汇上下文。 对于英语（美国）、西班牙语和德语，专家团队分别定义了 241、435 和 296 个 n-gram 来标记 12 种常见的认知扭曲类型。 请注意，我们的流行度测量只计算 CDS n-gram 的出现，而不管上下文（“每个人都在思考”、“仍然感觉”和“我是一个”）。 按失真类型提供的所有 CDS n-gram 的完整列表在SI Appendix, Tables S1–S3.\n(A-C) 美国英语 (A)、西班牙语 (B) 和德语 (C) 从 1855 年到 2020 年 (125 y) 的 CDS n-gram 流行时间序列的中值 z 分数，其中添加了年份标记 对于重大历史事件。 在 20 世纪的大部分时间里，所有时间序列都显示出稳定或下降的水平，随后在过去的 30 年里认知扭曲急剧增加。\n美国英语从 1899 年到 1978 年呈下降趋势，在 1914 年和 1940 年（第一次世界大战和第二次世界大战）以及特别是 1968 年出现小高峰。随后是 CDS 流行率从 1978 年开始激增，并持续到 2019 年。\n对于西班牙语 我们发现从 1895 年到 1980 年代初期的稳定水平，在这一点上出现了一个趋势，即 CDS 患病率水平高于之前观察到的任何水平。\n德国表现出稳定的 CDS 流行水平，除了第一次世界大战和第二次世界大战前后和之后的强劲高峰，直到 2007 年突然激增。\nData 研究数据谷歌已经开源，开源下载哦\nhttps://storage.googleapis.com/books/ngrams/books/datasetsv3.html\nCDS ngram词表 该论文CDS ngram词表\n代码 ngram代码实现\nfrom nltk.util import ngrams from nltk.tokenize import word_tokenize sentence = \u0026#34;Historical language records reveal a surge of cognitive distortions in recent decades\u0026#34; words = word_tokenize(sentence) print(\u0026#39;分词结果: \u0026#39;, words) #2-gram two_grams = [\u0026#39; \u0026#39;.join(tw) for tw in ngrams(words, 2)] print(\u0026#39;2-gram处理结果: \u0026#39;, two_grams) #3-gram three_grams = [\u0026#39; \u0026#39;.join(tw) for tw in ngrams(words, 3)] print(\u0026#39;3-gram处理结果: \u0026#39;, three_grams) Run\n分词结果: [\u0026#39;Historical\u0026#39;, \u0026#39;language\u0026#39;, \u0026#39;records\u0026#39;, \u0026#39;reveal\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;surge\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;cognitive\u0026#39;, \u0026#39;distortions\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;recent\u0026#39;, \u0026#39;decades\u0026#39;] 2-gram处理结果: [\u0026#39;Historical language\u0026#39;, \u0026#39;language records\u0026#39;, \u0026#39;records reveal\u0026#39;, \u0026#39;reveal a\u0026#39;, \u0026#39;a surge\u0026#39;, \u0026#39;surge of\u0026#39;, \u0026#39;of cognitive\u0026#39;, \u0026#39;cognitive distortions\u0026#39;, \u0026#39;distortions in\u0026#39;, \u0026#39;in recent\u0026#39;, \u0026#39;recent decades\u0026#39;] 3-gram处理结果: [\u0026#39;Historical language records\u0026#39;, \u0026#39;language records reveal\u0026#39;, \u0026#39;records reveal a\u0026#39;, \u0026#39;reveal a surge\u0026#39;, \u0026#39;a surge of\u0026#39;, \u0026#39;surge of cognitive\u0026#39;, \u0026#39;of cognitive distortions\u0026#39;, \u0026#39;cognitive distortions in\u0026#39;, \u0026#39;distortions in recent\u0026#39;, \u0026#39;in recent decades\u0026#39;] 统计统计CDS-ngram与ngram频数，进而计算出CDS流行度。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pnas_historical_language/","summary":"Bollen, Johan, et al. \u0026ldquo;Historical language records reveal a surge of cognitive distortions in recent decades.\u0026rdquo; Proceedings of the National Academy of Sciences 118.30 (2021).\n 摘要 患有抑郁症的人容易出现适应不良的思维模式，即认知扭曲，他们以过于消极和不准确的方式思考自己、世界和未来。 这些扭曲与个人情绪、行为和语言的显着变化有关。 我们假设社会可以经历类似的集体心理变化，这些变化会反映在语言使用的历史记录中。我们调查了过去 125 年超 1400 万本书中认知扭曲（congnition disorder）的文本标记的流行情况，并观察到自 1980 年代以来它们的流行程度激增，达到超过大萧条和两次世界大战的水平。 这种模式似乎不是由词义、出版和写作标准或 Google 图书样本的变化驱动的。 我们的研究结果发现，通过语言分析最近的社会转向与认知扭曲和内化障碍相关。\n正文 抑郁症与独特且可识别的适应不良思维模式有关，称为认知扭曲，其中个人以不准确和过于消极的方式思考自己、未来和世界 (9-12)。例如，当个体用消极的、绝对主义的术语（例如，“I am a loser”）给自己贴上标签时，就会出现在抑郁症中看到的认知扭曲。他们可能会用二分法、极端的术语谈论未来事件（例如，“My meeting will be a complete disaster”）或对别人的心态做出毫无根据的假设（例如，“Everybody will think that I am a failure”）。\n认知扭曲的类型通常区分许多部分重叠的类型，例如“灾难化”、“二分推理”、“否定积极的”、“情感推理”、“算命”、“标记和错误标记”、“放大和最小化”、“心理过滤”、“读心术”、“过度概括”、“个性化”和“应该陈述”。\n**认知行为疗法 (cognitive-behavioral therapy，CBT) ** 是治疗抑郁症和其他内化障碍的黄金标准 (13)，其基础理论认为认知扭曲与内化障碍有关； 它们反映了环境压力下的负面情感和回避行为模式 (14, 15)。 语言与这种动态密切相关。 事实上，最近的研究表明，患有内化障碍的个体在他们的语言中表现出明显更高水平的认知扭曲 (16, 17)，以至于他们的患病率可能被用作抑郁症易感性的指标 (18, 19)。","title":"PNAS | 历史语言记录揭示了近几十年来认知扭曲的激增"},{"content":"RPA for Python 简单而强大的自动化套件！ 您可以使用它快速实现自动化操作网站、自动化处理桌面应用程序上的重复性耗时任务。\n安装 !pip3 install rpa \n快速上手 首次运行下方代码会，提示安装200M左右的配置文件，建议选择状况较好的网络环境。\nimport rpa as r r.init() #操控Chrome打开某个链接 r.url(\u0026#39;https://www.google.com\u0026#39;) #定位搜索框，输入decentralization，回车执行搜索 r.type(\u0026#39;//*[@name=\u0026#34;q\u0026#34;]\u0026#39;, \u0026#39;decentralization[enter]\u0026#39;) #定位结果页的统计信息。 print(r.read(\u0026#39;result-stats\u0026#39;)) #截屏 r.snap(\u0026#39;page\u0026#39;, \u0026#39;results.png\u0026#39;) r.close() 找到约 1,590,000,000 条结果 （用时 0.39 秒） True  需要注意的是 //*[@name=\u0026quot;q\u0026quot;] 与 result-stats 都是网页的定位表达式，建议大家稍微学一下html选择器 selector。 咱们爬虫课程中的pyquery部分讲的主要是selector，感兴趣可以付下一下。\n再试一下 这里我给大家操作一下，通过开发者工具Element面板选定并复制的方法构造selector。\nimport rpa as r r.init() r.url(\u0026#39;https://www.baidu.com\u0026#39;) r.type(\u0026#39;#kw\u0026#39;, \u0026#39;大邓和他的Python[enter]\u0026#39;) print(r.read(\u0026#39;//*[@id=\u0026#34;tsn_inner\u0026#34;]/div[2]\u0026#39;)) r.snap(\u0026#39;page\u0026#39;, \u0026#39;results.png\u0026#39;) r.close() 搜索工具百度为您找到相关结果约1,920,000个 True  \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/rpa/","summary":"RPA for Python 简单而强大的自动化套件！ 您可以使用它快速实现自动化操作网站、自动化处理桌面应用程序上的重复性耗时任务。\n安装 !pip3 install rpa \n快速上手 首次运行下方代码会，提示安装200M左右的配置文件，建议选择状况较好的网络环境。\nimport rpa as r r.init() #操控Chrome打开某个链接 r.url(\u0026#39;https://www.google.com\u0026#39;) #定位搜索框，输入decentralization，回车执行搜索 r.type(\u0026#39;//*[@name=\u0026#34;q\u0026#34;]\u0026#39;, \u0026#39;decentralization[enter]\u0026#39;) #定位结果页的统计信息。 print(r.read(\u0026#39;result-stats\u0026#39;)) #截屏 r.snap(\u0026#39;page\u0026#39;, \u0026#39;results.png\u0026#39;) r.close() 找到约 1,590,000,000 条结果 （用时 0.39 秒） True  需要注意的是 //*[@name=\u0026quot;q\u0026quot;] 与 result-stats 都是网页的定位表达式，建议大家稍微学一下html选择器 selector。 咱们爬虫课程中的pyquery部分讲的主要是selector，感兴趣可以付下一下。\n再试一下 这里我给大家操作一下，通过开发者工具Element面板选定并复制的方法构造selector。\nimport rpa as r r.init() r.url(\u0026#39;https://www.baidu.com\u0026#39;) r.type(\u0026#39;#kw\u0026#39;, \u0026#39;大邓和他的Python[enter]\u0026#39;) print(r.read(\u0026#39;//*[@id=\u0026#34;tsn_inner\u0026#34;]/div[2]\u0026#39;)) r.snap(\u0026#39;page\u0026#39;, \u0026#39;results.png\u0026#39;) r.close() 搜索工具百度为您找到相关结果约1,920,000个 True  \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"Python自动化利器RPA爬数据so easy"},{"content":"想知道什么时候网站有新的变动(新消息)\u0026hellip;\n 政府部门更新（更改通常只在他们的网站上） 地方政府有新闻了（变化往往只在他们的网站上） 当您不在他们的邮件列表中时，新软件发布、安全建议。 有变化的节日 房地产上市变化 来自政府网站的 COVID 相关新闻 检测和监控 JSON API 响应的变化 API 监控和警报  其实超级简单，一个命令即可！\n安装 pip3 install changedetection.io 假设项目文件夹存放于桌面，在桌面新建一个monitordetection\n#mac changedetection.io -d desktop/monitordetection -p 5000 #win changedetection.io -d Desktop/monitordetection -p 5000 然后访问 http://127.0.0.1:5000 ，您现在应该可以访问 UI。\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/changedetection/","summary":"想知道什么时候网站有新的变动(新消息)\u0026hellip;\n 政府部门更新（更改通常只在他们的网站上） 地方政府有新闻了（变化往往只在他们的网站上） 当您不在他们的邮件列表中时，新软件发布、安全建议。 有变化的节日 房地产上市变化 来自政府网站的 COVID 相关新闻 检测和监控 JSON API 响应的变化 API 监控和警报  其实超级简单，一个命令即可！\n安装 pip3 install changedetection.io 假设项目文件夹存放于桌面，在桌面新建一个monitordetection\n#mac changedetection.io -d desktop/monitordetection -p 5000 #win changedetection.io -d Desktop/monitordetection -p 5000 然后访问 http://127.0.0.1:5000 ，您现在应该可以访问 UI。\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"changedetection|自动追踪网站更新"},{"content":"rpa Python自动化操纵包\nhttps://github.com/tebelorg/RPA-Python\n{{ \u0026lt; figure src=\u0026ldquo;img/tagui_python.gif\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nCrunch https://github.com/chrissimpkins/Crunch 疯狂（缓慢但非常好）PNG 图像优化 {{ \u0026lt; figure src=\u0026ldquo;img/header-arrow.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} VS可视化 https://github.com/hiDaDeng/vs 将 google 的自动完成功能可视化为图表。 {{ \u0026lt; figure src=\u0026ldquo;img/huawei.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} {{ \u0026lt; figure src=\u0026ldquo;img/python.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} {{ \u0026lt; figure src=\u0026ldquo;img/digitalazition.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} jupyter-text2code https://github.com/deepklarity/jupyter-text2code 在jupyter notebook内，可将英语查询转换为相关的 Python 代码 {{ \u0026lt; figure src=\u0026ldquo;img/jupyter-text2code-demo.gif\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nautoscraper https://github.com/alirezamika/autoscraper\n一个用于 Python 的智能、自动、快速和轻量级的 Web 爬虫\n{{ \u0026lt; figure src=\u0026ldquo;img/autoscraper.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} newscatcher https://github.com/kotartemiy/newscatcher\n以编程方式从（几乎）任何网站收集规范化新闻。\n{{ \u0026lt; figure src=\u0026ldquo;img/newscatcher.gif\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} fusuma https://github.com/hiroppy/fusuma Fusuma 使用 Markdown 轻松制作幻灯片。 halo https://github.com/halo-dev/halo 一款优秀的开源博客发布应用。 {{ \u0026lt; figure src=\u0026ldquo;img/halo.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} 己思 https://dinorss.org/ 一个简单、高效、开源的RSS阅读器服务 {{ \u0026lt; figure src=\u0026ldquo;img/己思1.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} {{ \u0026lt; figure src=\u0026ldquo;img/己思2.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} ##了解课程\n 点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly06/","summary":"rpa Python自动化操纵包\nhttps://github.com/tebelorg/RPA-Python\n{{ \u0026lt; figure src=\u0026ldquo;img/tagui_python.gif\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nCrunch https://github.com/chrissimpkins/Crunch 疯狂（缓慢但非常好）PNG 图像优化 {{ \u0026lt; figure src=\u0026ldquo;img/header-arrow.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} VS可视化 https://github.com/hiDaDeng/vs 将 google 的自动完成功能可视化为图表。 {{ \u0026lt; figure src=\u0026ldquo;img/huawei.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} {{ \u0026lt; figure src=\u0026ldquo;img/python.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} {{ \u0026lt; figure src=\u0026ldquo;img/digitalazition.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} jupyter-text2code https://github.com/deepklarity/jupyter-text2code 在jupyter notebook内，可将英语查询转换为相关的 Python 代码 {{ \u0026lt; figure src=\u0026ldquo;img/jupyter-text2code-demo.gif\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nautoscraper https://github.com/alirezamika/autoscraper\n一个用于 Python 的智能、自动、快速和轻量级的 Web 爬虫\n{{ \u0026lt; figure src=\u0026ldquo;img/autoscraper.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} newscatcher https://github.com/kotartemiy/newscatcher\n以编程方式从（几乎）任何网站收集规范化新闻。","title":"TechWeekly-06 | 每周有趣有用的技术分享"},{"content":"Top2Vec 是一种用于主题建模和语义搜索的算法。**我个人从理解代码和使用代码难度来看， 对于Python小白，BERTopic更适合直接用预训练词向量，而Top2Vec更适合对小规模数据训练词向量后做主题建模。**它自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。训练 Top2Vec 模型后，您可以：\n 获取检测到的主题数。 获取话题。 获取主题大小。 获取分层主题。 按关键字搜索主题。 按主题搜索文档。 按关键字搜索文档。 找出相似的词。 查找类似的文档。 使用 RESTful-Top2Vec 公开模型 有关其工作原理的更多详细信息，请参阅论文。  亮点\n 自动查找主题数。 不需要停用词列表。 不需要词干/词形还原。 适用于短文本。 创建联合嵌入的主题、文档和词向量。 内置搜索功能。  它是如何工作的？\n该算法做出的假设是，许多语义相似的文档都表明了一个潜在的主题。\n第一步是创建文档和词向量的联合嵌入。一旦文档和单词被嵌入到一个向量空间中，算法的目标就是找到密集的文档集群，然后确定哪些单词将这些文档吸引到一起。每个密集区域是一个主题，将文档吸引到密集区域的词就是主题词。\n!pip3 install top2vec==1.0.27 0. 代码下载 click to download code\n\n1. 导入数据 使用某灾难数据集，这里是存在标注的标签，但是我们假设不用label的，仅作为评判Top2vec运行效果的标准。点击cnews.csv下载\nfrom IPython.display import display import cntext as ct from top2vec import Top2Vec import pandas as pd import jieba stopwords = ct.load_pkl_dict(\u0026#39;STOPWORDS.pkl\u0026#39;)[\u0026#39;STOPWORDS\u0026#39;][\u0026#39;chinese\u0026#39;] df = pd.read_csv(\u0026#39;cnews.csv\u0026#39;) df.head() df.label.value_counts() Run\n时政 120 科技 106 时尚 106 财经 105 家居 103 教育 97 娱乐 96 体育 95 房产 87 游戏 85 Name: label, dtype: int64  \n2. 清洗数据 一般而言，作中文文本分析，需要把中文分词构造成类西方语言(空格间隔词语的文本)风格。在此期间，顺便将停用词剔除。其实在用top2vec时，不剔除停用词影响也不大。\ndef clean_text(text): words = jieba.lcut(text) words = [w for w in words if w not in stopwords] return \u0026#39; \u0026#39;.join(words) df[\u0026#39;cleantext\u0026#39;] = df.text.apply(clean_text) df.head() \n3. 训练模型 Top2vec有一下四个常用参数\nTop2vec(documents, min_count, speed, workers)\n documents: 文档列表 min_count: 词语最少出现次数。低于min_count的词不加入模型中 speed: 训练速度，参数默认\u0026quot;learn\u0026quot;  \u0026ldquo;fast-learn\u0026rdquo; 速度最快，训练效果最差 \u0026ldquo;learn\u0026rdquo; 速度，训练效果中等 \u0026ldquo;deep-learn\u0026rdquo; 速度最慢，训练效果最佳   workers: 并行运行数，该值最大取值为电脑CPU的核数。  model = Top2Vec(documents=df[\u0026#39;cleantext\u0026#39;].to_list(), min_count=10, speed=\u0026#34;deep-learn\u0026#34;, workers=8) Run\n2021-12-14 20:21:10,318 - top2vec - INFO - Pre-processing documents for training 2021-12-14 20:21:10,871 - top2vec - INFO - Creating joint document/word embedding 2021-12-14 20:25:06,082 - top2vec - INFO - Creating lower dimension embedding of documents 2021-12-14 20:25:14,645 - top2vec - INFO - Finding dense areas of documents 2021-12-14 20:25:14,683 - top2vec - INFO - Finding topics  # 话题个数 model.get_num_topics() Run\n9  # 各话题数量 topic_sizes, topic_nums = model.get_topic_sizes() {\u0026#34;topic_sizes\u0026#34;:topic_sizes, \u0026#34;topic_ids\u0026#34;:topic_nums} Run\n{'topic_sizes': array([361, 116, 107, 99, 97, 93, 82, 25, 20]), 'topic_ids': array([0, 1, 2, 3, 4, 5, 6, 7, 8])}  \n4. get_topics 用pyecharts词云图显示话题信息， 为了简化代码，将该功能封装为函数\ndef gen_wordcloud(topic_words, word_scores, topic_id): \u0026#34;\u0026#34;\u0026#34; topic_words: 主题词列表 word_scores: 主题特征词的权重得分(词语表征主题的能力) topic_id: 主题id \u0026#34;\u0026#34;\u0026#34; import pyecharts.options as opts from pyecharts.charts import WordCloud from IPython.display import display data = [(word, str(weight)) for word, weight in zip(topic_words, word_scores)] wc = WordCloud() wc.add(series_name=\u0026#34;\u0026#34;, data_pair=data, word_size_range=[6, 88]) wc.set_global_opts( title_opts=opts.TitleOpts(title=\u0026#34;Topic_{topic_id}\u0026#34;.format(topic_id=topic_id), title_textstyle_opts=opts.TextStyleOpts(font_size=23)), tooltip_opts=opts.TooltipOpts(is_show=True)) display(wc.render_notebook()) topic_wordss, word_scoress, topic_ids = model.get_topics(11) for topic_words, word_scores, topic_id in zip(topic_wordss, word_scoress, topic_ids): gen_wordcloud(topic_words, word_scores, topic_id) \n5. get_documents_topics get_documents_topics(doc_ids, num_topics=1)\n doc_ids: 待查询文档id列表 num_topics: 返回某文档可能归属话题的个数  # 查第一条文档的 model.get_documents_topics(doc_ids=[0], num_topics=1) Run\n(array([0]), array([0.1384481], dtype=float32), array([[\u0026#39;政府\u0026#39;, \u0026#39;经济\u0026#39;, \u0026#39;政策\u0026#39;, \u0026#39;建设\u0026#39;, \u0026#39;中方\u0026#39;, \u0026#39;发展\u0026#39;, \u0026#39;促进\u0026#39;, \u0026#39;部门\u0026#39;, \u0026#39;留学\u0026#39;, \u0026#39;学生\u0026#39;, \u0026#39;会议\u0026#39;, \u0026#39;我要\u0026#39;, \u0026#39;事务\u0026#39;, \u0026#39;日电\u0026#39;, \u0026#39;房价\u0026#39;, \u0026#39;教育\u0026#39;, \u0026#39;国务院\u0026#39;, \u0026#39;温家宝\u0026#39;, \u0026#39;留学生\u0026#39;, \u0026#39;人数\u0026#39;, \u0026#39;移民\u0026#39;, \u0026#39;会见\u0026#39;, \u0026#39;推动\u0026#39;, \u0026#39;申请者\u0026#39;, \u0026#39;申请\u0026#39;, \u0026#39;官员\u0026#39;, \u0026#39;住房\u0026#39;, \u0026#39;房屋\u0026#39;, \u0026#39;加强\u0026#39;, \u0026#39;中国政府\u0026#39;, \u0026#39;购房\u0026#39;, \u0026#39;国家\u0026#39;, \u0026#39;支付\u0026#39;, \u0026#39;楼市\u0026#39;, \u0026#39;外交部\u0026#39;, \u0026#39;接收\u0026#39;, \u0026#39;两国\u0026#39;, \u0026#39;原则\u0026#39;, \u0026#39;各地\u0026#39;, \u0026#39;总理\u0026#39;, \u0026#39;战略\u0026#39;, \u0026#39;和平\u0026#39;, \u0026#39;框架\u0026#39;, \u0026#39;评论\u0026#39;, \u0026#39;有序\u0026#39;, \u0026#39;装修\u0026#39;, \u0026#39;中国\u0026#39;, \u0026#39;就业\u0026#39;, \u0026#39;友好\u0026#39;, \u0026#39;人力资源\u0026#39;]], dtype=\u0026#39;\u0026lt;U9\u0026#39;), array([[0.3623712 , 0.36037514, 0.35219163, 0.35109183, 0.3499857 , 0.34666985, 0.3426961 , 0.34161803, 0.34010434, 0.3382269 , 0.33710504, 0.336056 , 0.33598724, 0.33488944, 0.3303768 , 0.32483265, 0.324798 , 0.32201332, 0.3174801 , 0.3153757 , 0.3152491 , 0.31338856, 0.31334093, 0.31244045, 0.31202242, 0.30908576, 0.3086405 , 0.30838227, 0.30605763, 0.3053521 , 0.30474398, 0.30268514, 0.30253592, 0.30242488, 0.30227807, 0.3017046 , 0.30116442, 0.30062813, 0.2996228 , 0.29806197, 0.2972776 , 0.29709277, 0.29706252, 0.29584888, 0.29578486, 0.29524648, 0.2944737 , 0.2939484 , 0.29286712, 0.29246706]], dtype=float32)) \n6. search_topics 根据关键词搜索话题，查某词是否属于某话题，属于该主题的概率 search_topics(keywords, num_topics, keywords_neg=None)\n keywords: 关键词列表 num_topics: 返回话题个数，按照语义相似度从高到低排序 keywords_neg: 反义词列表  def gen_wordcloud2(query_word, topic_words, word_scores, topic_id, topic_probability): \u0026#34;\u0026#34;\u0026#34; query_word: 待查询词 topic_words: 主题词列表 word_scores: 主题特征词的权重得分(词语表征主题的能力) topic_id: 主题id topic_probability: 主题概率 \u0026#34;\u0026#34;\u0026#34; import pyecharts.options as opts from pyecharts.charts import WordCloud from IPython.display import display data = [(word, str(weight)) for word, weight in zip(topic_words, word_scores)] wc = WordCloud() wc.add(series_name=\u0026#34;\u0026#34;, data_pair=data, word_size_range=[6, 88]) title = \u0026#34;\u0026#34;\u0026#34;Word{query_word}\\nTopic_{topic_id}\\nProbability:{probability:.2f}\u0026#34;\u0026#34;\u0026#34;.format(query_word=query_word, topic_id=topic_id, probability=topic_probability) wc.set_global_opts( title_opts=opts.TitleOpts(title=title, title_textstyle_opts=opts.TextStyleOpts(font_size=18)), tooltip_opts=opts.TooltipOpts(is_show=True)) display(wc.render_notebook()) query_word = \u0026#34;电影\u0026#34; topic_wordss, word_scoress, topic_scores, topic_ids = model.search_topics(keywords=[query_word], num_topics=4) for topic_words, word_scores, topic_score, topic_id in zip(topic_wordss, word_scoress, topic_scores, topic_ids): if topic_score\u0026gt;0.5: gen_wordcloud2(query_word=query_word, topic_words=topic_words, word_scores=word_scores, topic_id=topic_id, topic_probability=topic_score) 7. query_topics 根据一段文本寻找最符合该文本的话题 query_topics(query, num_topics)\n query: 查询文本，注意是用空格间隔词语的文本 num_topics: 返回的话题数  返回话题特征词列表， 话题特征词权重， 话题概率， 话题id\nquerytext = \u0026#39;刘晓庆 55 岁 近日 颁奖礼 刘晓庆 一袭 宝蓝色 超低 胸 V 领 长裙 亮相 轻薄 蕾丝 奢华 皮草 艳丽 色彩 翠绿\u0026#39; topic_words, word_scores, topic_scores, topic_ids = model.query_topics(query=querytext, num_topics=2) print(\u0026#39;可能归属的话题有: \u0026#39;, topic_ids) print(\u0026#39;归属于该话题的概率\u0026#39;, topic_scores) Run\n可能归属的话题有: [1 4] 归属于该话题的概率 [0.32036728 0.1276904 ] \n8. search_documents_by_keywords 根据关键词，筛选文档\nsearch_documents_by_keywords(keywords, num_docs, keywords_neg=None, return_documents=True)\n#文档， 语义相关性， 文档id docs, scores, doc_ids = model.search_documents_by_keywords(keywords=[\u0026#39;搭配\u0026#39;], num_docs=3, keywords_neg=None, return_documents=True) for doc, score, doc_id in zip(docs, scores, doc_ids): print(f\u0026#34;Document: {doc_id}, Semantic similarity: {score}\u0026#34;) print(doc) print(\u0026#39;----------\u0026#39;) print() Run\nDocument: 106, Semantic similarity: 0.4943176805973053 白色 短裙 百变 休闲 感 要点 一定 敞开 衬衫 配合 牛仔裤 休闲 感 短裤 衬衫 短 敞开 显得 好好 穿 裤子 搭配 七分裤 遮住 臀部 长度 关键 尽量 选择 艳丽 颜色 带 出 青春 感 NO.3 白色 短裙 tips : 白色 短裙 + 粉色 上衣 这是 一套 减龄 百分百 搭配 白色 短裙 本来 清纯 粉色 上衣 搭配 更加 具有活力 tips : 白色 短裙 + 抹胸 + 外套 想要 性感 一点 就加 一件 抹胸 抹胸 胸前 构造 曲线 完美 再加 外套 保暖 得体 看似 简单 一款 搭配 其实 暗地里 偷偷地 修饰 身材 ---------- Document: 870, Semantic similarity: 0.4483542740345001 组图 看达人 演绎 豹纹 军装 风 导语 懂得 潮流 总是 知道 适合 今冬 流行 亮点 太 军装 豹纹 类似 民族风情 想要 知道 搭配 快 看看 时尚 达 穿 军绿色 宽松 款 大衣 不失 俏皮 味道 高腰 设计 短裙 有效 提升 腰线 衬托出 修长 美腿 豹纹 今年 冬季 抢眼 搭配 元素 加上 驼色 针织衫 灰色 围巾 暖 棕色 手 挎包 整体 色调 统一 迷人 棕色 蓝色 结合能 眼前一亮 简洁 款式 依然 突显 独特 品味 宽松 针织 外套 衬托出 优美 身形 搭配 同样 沉闷 黑色 包包 性感 丝袜 装扮 依然 透露 出 迷人 气息 立领 衬衫 加上 深黄 高腰 裤 摩登 感 十足 随意 披上 外套 更显 慵懒 个性 法式 风情 ---------- Document: 450, Semantic similarity: 0.4471719563007355 街 拍 爱 招摇过市 毛茸茸 ( 组图 ) 导语 皮草 每个 冬天 可能 丢弃 每个 需要 温暖 早些 相比 人造皮 草比 真皮 草 风头 更劲 时尚 环保 大牌 秀 场上 超模 一个个 穿着 人造皮 草 “ 招摇过市 ” 之后 街头 潮人 没有 理由 拒绝 外形 酷酷 这件 气场 皮草 单品 配合默契 摇滚 风 配饰 搭配 黑色 皮草 长 背心 更显 利落 酷酷 黑色 皮草 搭配 蓝色 衬衣 不同 感觉 加上 下半身 底裤 时髦 包包 颜色 提亮 整身 装扮 抹胸 式 皮草 特点 高贵典雅 适合 搭配 连衣裙 装饰 增添 时尚 美感 复古 圆点 连衣裙 搭配 宽松 棕色 皮草 衣 名媛 感觉 典雅 淑女 短款 黑色 皮草 搭配 贴身 仔裤 搭配 长靴 潇洒 帅气 茸茸 帽子 增添 不少 甜美 感 ---------- \n9. search_documents_by_topic 根据指定的topic_id， 显示该主题前num_docs个文档，显示的文档是根据概率从高到低降序显示\n#查看topic4的前5条文档 topic_id = 4 documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=topic_id, num_docs=5) for doc, score, doc_id in zip(documents, document_scores, document_ids): print(f\u0026#34;Document: {doc_id}, Semantic similarity: {score}\u0026#34;) print(\u0026#34;-----------\u0026#34;) print(doc) print(\u0026#34;-----------\u0026#34;) print() Run\nDocument: 905, Semantic similarity: 0.4941929578781128 ----------- 现场 阿联 第三节 未 亮相 奇才 连续 3 记 重扣 逆转 比分 新浪 体育讯 北京 时间 4 2 奇才 主场 迎战 联盟 垫底 骑士 奇才 新秀 后卫 约翰 - 沃尔因 一场 对阵 热火 比赛 斗殴 禁赛 一场 伤愈 复出 安德雷 - 布 莱切 回到 首发 阵容 奇才 本赛季 首场 客场 胜利 面对 骑士 取得 当时 奇才 115 - 110 击败 对手 上半场 骑士 命中率 达到 53.8% 奇才 仅 44% 骑士 希克 森 ( 16 ) 塞 申斯 ( 12 ) 得分 双 奇才 布 莱切 ( 22 ) 麦基 ( 12 ) 埃文斯 4 投 0 仅 抢下 1 篮板 易建联 上场 7 08 2 投 0 抢下 3 篮板 异地 再战 埃文斯 终于 得分 抢断 吉后 犯规 两罚 命中 吉 随即 突破 上篮 命中 回敬 球 杰 弗斯 三分 不进 吉 抢下 篮板 上篮 再进 一球 布 莱切 中投 命中 霍林斯 篮下 出手 不进 布 莱切 抢下 篮板 此后 克劳福德 连续 突破 先是 助攻 麦基 扣篮 盖掉 戴维斯 投篮 助攻 布 莱切 扣篮 戴维斯 运球 被断 布 莱切 传给 杰 弗斯 一记 暴扣 奇才 连续 3 次 颇具 气势 扣篮 连得 6 反超 骑士 1 骑士 请求 暂停 回到 比赛 吉 上篮 不进 麦基 低位 单打 不进 布 莱切 抢下 篮板 3 得手 骑士 进攻 24 违例 奇才 越战越勇 克劳福德 身体 失去 重心 情况 仍然 将球 投进 一个打 3 骑士 连续 吉 挺身而出 三分 命中 个人 已经 得到 10 此人 本赛季 短暂 效力 奇才 麦基 中投 不进 布 莱切 抢下 前场 篮板 将球 放进 麦基 防守 领到 犯规 希克 森两罚 命中 麦基 强攻 造成 霍林斯 犯规 两罚 一中 戴维斯 三分 不进 克劳福德 跑 投 命中 戴维斯 突分 霍林斯 暴扣 命中 回过头来 克劳福德 助攻 麦基扣 劲 爆 哈兰 高迪 中投 不进 克劳福德 投篮 偏出 布 莱切 3 报价 连续 抢 篮板 进攻 最后 犯规 两罚 一中 现在 已经 得到 32 18 篮板 布 莱切 底线 遭 报价 分球 埃文斯 三分 命中 霍林斯 篮下 重扣 奇才 请求 暂停 布 莱切 继续 得分 吉布森 上篮 命中 克劳福德 中投 不进 抢下 篮板 杰 弗斯 运球 突破 犯规 两罚 命中 易建联 节 没有 登场 第三节 比赛 结束 骑士 82 - 83 奇才 ( 草头 王 ) ----------- Document: 689, Semantic similarity: 0.4917592704296112 ----------- 直击 康大 内线 一柱擎天 13 优势 到手 胜利在望 新浪 体育讯 北京 时间 4 5 ( 休斯敦 时间 4 4 ) 消息 NCAA Final 4 总决赛 休斯敦 Reliant 球馆 举行 比赛 进入 最后 6 分钟 本场 表现 十分 亮眼 康涅狄格 内线 阿莱克斯 - 奥里 瓦基接 队友 直传 空切 篮下 扣篮 得分 打成 2 + 1 目前 已经 拿下 10 9 篮板 3 封盖 巴特勒 仍然 没 解决 进攻 端的 问题 下半场 23 投 仅仅 3 屡次 外线 空挡 出手 均 打铁 告终 仅仅 入账 8 目前 康大 已经 取得 13 优势 胜利在望 ( silencer ) ----------- Document: 425, Semantic similarity: 0.47035443782806396 ----------- 今日 数据 趣谈 魔兽 悲情 似 张大帅 基德 焕发 第二 春 新浪 体育讯 北京 时间 4 17 NBA 季后赛 正式 开打 进行 4 场 比赛 以下 今日 比赛 诞生 有趣 数据 今日 首场 季后赛 芝加哥 公牛 第四节 剩 4 分钟 仍以 88 - 98 落后 接下来 打出 16 - 1 攻击 波 主场 一举 逆转 印第安纳 步行者 取胜 继 2004 之后 NBA 季后赛 舞台 再次出现 终场 前 4 分钟 落后 两位数 最终 翻盘 成功 案例 2004 5 9 西部 决赛 明尼苏达 森林狼 萨克拉门托 国王 比赛 森林狼 同样 终场 前 4 分钟 仍以 78 - 88 落后 接下来 打出 16 - 1 ( 惊人 相似 ) 最终 94 - 89 逆转 取胜 今天 公牛 逆转 步行者 比赛 德里克 - 罗斯 砍 39 罚球 21 投 19 2008 洛杉矶 湖人 对阵 犹他 爵士 一场 季后赛 科比 - 布莱恩特 创下 单场 罚球 23 投 21 季后赛 纪录 罗斯 位居 全场 三分球 9 次 出手 竟无一 命中 季后赛 历史 此前 两次 类似 案例 2008 奥兰多 魔术 对阵 多伦多 猛龙 一场 比赛 拉沙德 - 刘易斯 三分球 9 投 0 一次 熟知 1994 总决赛 第七场 约翰 - 斯塔克 斯 三分 线外 11 投 0 纽约 尼克斯 负于 休斯敦 火箭 冠军 擦肩而过 今天 亚特兰大 老鹰 客场 战胜 奥兰多 魔术 比赛 老鹰 五名 球员 得分 低于 13 — — 乔 - 约翰逊 ( 25 16 投 9 ) 贾马尔 - 克劳福德 ( 23 14 投 7 ) 艾尔 - 霍福德 ( 16 14 投 7 ) 约什 - 史密斯 ( 15 12 投 6 ) 科克 - 辛里奇 ( 13 10 投 6 ) 该队 过去 199 场 季后赛 尚属 首次 老鹰队 史上 一次 出现 这种 盛况 1966 4 14 131 - 127 战胜 洛杉矶 湖人 比赛 当时 书写 纪录 五人 里奇 - 古尔林 克里夫 - 哈根 泽尔莫 - 比蒂 比尔 - 布里奇斯 乔 - 考 德维尔 今天 负于 老鹰 比赛 德怀特 - 霍华德 ( 46 ) 贾 米尔 - 尼尔森 ( 27 ) 砍 73 队友 总共 仅 拿下 20 魔术 最终 93 - 103 负于 更为 均衡 对手 NBA 历史 8 支 球队 一场 季后赛 比赛 有过 两名 球员 联手 砍 全队 至少 75% 得分 1 队 取胜 追溯到 1950 4 9 当年 总决赛 第一场 比赛 乔治 - 麦肯 得到 37 吉姆 - 波 拉德 得到 14 率领 明尼阿波利斯 湖人 68 - 66 战胜 锡 拉丘兹 民族 ( 费城 76 前身 ) 7 队则 败北 得到 46 霍华德 抢下 19 篮板 常规 时间 取得 1975 4 19 布法罗 勇敢者 ( 洛杉矶 快船 前身 ) 战胜 华盛顿 子弹 ( 华盛顿 奇才 前身 ) 一场 季后赛 效力 勇敢者 鲍勃 - 麦卡 杜 同样 没有 加时赛 情况 砍 50 21 篮板 威尔特 - 张伯伦 一场 季后赛 常规 时间 砍 46 19 篮板 球队 却输 ( 事实上 张大帅 生涯 3 场 比赛 取得 数据 竟 败北 ) 刚 谢幕 本赛季 常规赛 杰森 - 基德 仅 两场 比赛 得分 达到 20 + 1 20 对阵 湖人 比赛 砍 赛季 最高 21 今天 达拉斯 小牛 主场 战胜 波特兰 开拓者 比赛 砍 24 命中 6 记 三分球 一场 季后赛 比赛 砍 20 + 得分 刷新 常规赛 创下 赛季 新高 NBA 历史 壮举 球员 如今 38 岁 基德 年龄 最大 成为 NBA 历史 一场 季后赛 比赛 单场 命中 6 记 三分球 年龄 最大 球员 此前 纪录 雷吉 - 米勒 2002 创下 当时 36 岁 今天 小牛 战胜 开拓者 比赛 德克 - 诺维茨基 第四节 13 次 罚球 出手 命中 追平 迈克尔 - 乔丹 纪录 1990 - 91 赛季 季后赛 一场 公牛 底特律 活塞 比赛 乔丹 单节 命中 13 次 罚球 率队 105 - 97 取胜 最终 公牛 获得 赛季 总冠军 今天 迈阿密 热火 战胜 费城 76 比赛 克里斯 - 波什 得到 25 12 篮板 勒布朗 - 詹姆斯 得到 21 14 篮板 他俩 队友 参加 首场 季后赛 前 一个 赛季 各为其主 接下来 赛季 并肩作战 季后赛 首场 比赛 砍 得分 20 + 篮板 10 + 组合 波什 詹姆斯 之前 无先例 ( 魑魅 ) ----------- Document: 155, Semantic similarity: 0.45704954862594604 ----------- 现场 麦蒂 返场 销魂 跳投 两 连击 小拜 纳姆 单节 11 新浪 体育讯 北京 时间 4 6 华盛顿 奇才 主场 迎战 底特律 活塞 此前 球队 已经 客场 两连胜 若能 战胜 活塞 奇才 本赛季 首次 迎来 三连胜 异地 再战 埃文斯 中投 命中率 先 得分 拜纳姆 中投 不进 克劳福德 一人 带球 运 前场 对手 尚未 落位 情况 直接 出手 投篮 命中 这种 投篮 欠缺 考虑 根本 没有 战术 配合 全 个人 手感 遇到 防守 稍 一点 球队 沃尔 抢断 埃文斯 直接 暴扣 奇才 反超 4 活塞 请求 暂停 沃尔 报价 对手 拜纳姆 得到 机会 三分 出手 命中 布 莱切 上篮 得手 门罗 助攻 威尔 考克斯 扣篮 命中 埃文斯 三分 不进 拜纳姆 突破 上篮 命中 威尔 考克斯 拿布 莱切 没有 办法 运球 进攻 威尔 考克斯 只能 伸直 手臂 不断 滑步 被布 莱切 强投 命中 活塞 拜纳姆 发力 突破 上篮 命中 布 莱切 中投 不进 拜纳姆 卷土重来 造成 沃尔 犯规 两罚 命中 个人 已经 得到 11 门罗 抢断 布 莱切 普林斯 上篮 命中 活塞 反超 3 麦基 传球 失误 奇才 请求 暂停 威尔 考克斯 篮下 强打 奇才 反击 埃文斯 上篮 命中 普林斯 糟糕 状态 继续 中投 偏出 布 莱切 运球 单打 活塞 两名 内线 屡试不爽 造成 门罗 犯规 两罚 命中 汉密尔顿 中投 不进 威尔 考克斯 抢下 前场 篮板 直接 扣篮 命中 布 莱切 继续 发威 转身 摆脱 上篮 命中 拜纳姆 三分 偏出 球 砸 远 活塞 球员 退守 不及 克劳福德 轻松 上篮 命中 沃尔 中投 不进 拜纳姆 反击 遭 侵犯 两罚 命中 个人 单节 已经 得到 11 布 莱切 对手 包夹 中投 偏出 普林斯 跑 投 命中 活塞 反超 一分 克劳福德 中投 打铁 拜纳姆 没能 命中 三分 麦蒂 回到 赛场 塞拉芬 进攻 犯规 普林斯 中投 不进 门罗 补篮 命中 麦蒂断 球 直接 中投 命中 布 莱切 走步 麦蒂 假动作 点飞 克劳福德 投篮 再进 第三节 比赛 结束 活塞 81 - 78 奇才 ( 草头 王 ) ----------- Document: 254, Semantic similarity: 0.45255911350250244 ----------- 奇才 vs 步行者 前瞻 走出 客场 阴影 斗狠 东部 老八 新浪 体育讯 北京 时间 4 7 奇才队 客场 挑战 东区 第八 步行者 目前 奇才 客场 战绩 3 胜 35 负 最近 客场 两连胜 奇才队 背靠背 作战 今天 主场 107 - 105 险胜 活塞 球队 一举 拿到 赛季 最长 三连胜 实际上 这是 奇才队 2007 - 08 赛季 以后 球队 第一个 赛季 三连胜 这场 比赛 奇才 惊人 获得 35 次 罚球 沃尔一人 包办 16 次 全场 得到 26 12 次 助攻 6 篮板 4 次 抢断 布 莱切 无疑 三连胜 第一 功臣 连胜 期间 场均 得到 29 15.3 篮板 克劳福德 同样 火爆 异常 一位 前锋 首发 埃文斯 表现 低估 活塞 比赛 埃文斯 13 投 9 中射下 20 沃尔 拿到 职业生涯 首个 三连胜 “ 联盟 留下 标签 一名 菜鸟 证明 一部分 很多 想 站 球场上 一分钟 全力以赴 ” 奇才 三连胜 对手 名副其实 鱼腩 球队 无论如何 三连胜 这支 弱旅 一个 不小 激励 尤其 伤病 满营 情况 目前 球队 6 可能 赛季 结束 前 无法 归队 包括 得分王 尼克 - 杨 约什 - 霍华德 拉沙德 - 刘易斯 布克 恩戴 耶 卡 蒂尔 - 马丁 步行者 35 胜 43 负 暂居 东部 第八 目前 东部 前七 已经 锁定 季后赛 剩下 第八名 悬念 步行者 领先 第九位 山猫 2.5 个胜场 领先 10 位 雄鹿 3.5 个胜场 剩下 4 场 比赛 情况 悬念 并不大 明天 山猫 雄鹿 迎战 强敌 ( 魔术 热火 ) 步行者 机会 扩大 领先 场次 优势 球队 头号 得分手 格兰杰 状态 过去 5 场 比赛 得分 20 以下 最近 三场 三分球 12 投 3 汉斯 布鲁在 过去 6 场 比赛 陷入 挣扎 场均 9.3 5.7 篮板 ( 之前 11 场 比赛 贡献 20.6 7.8 篮板 ) 一场 比赛 步行者 12 输给 黄蜂队 主教练 沃格尔 称之为 “ 惨痛 失败 ” 本赛季 两队 战成 2 - 1 步行者 赢下 最近 两次 交锋 两场 比赛 奇才 命中率 均 低于 40% 总 失误 高达 41 次 预计 两队 首发 奇才 沃尔 克劳福德 埃文斯 布 莱切 麦基 步行者 科 里森 格兰杰 乔治 汉斯 布鲁 希伯特 ( 木瓜 丁 ) ----------- \ndocuments, document_scores, document_ids = model.search_documents_by_keywords(keywords=[\u0026#34;搭配\u0026#34;, \u0026#34;高跟鞋\u0026#34;], num_docs=5) for doc, score, doc_id in zip(documents, document_scores, document_ids): print(f\u0026#34;Document: {doc_id}, Semantic similarity: {score}\u0026#34;) print(\u0026#34;-----------\u0026#34;) print(doc) print(\u0026#34;-----------\u0026#34;) print() Document: 727, Semantic similarity: 0.5883481502532959 ----------- 组图 冷气 办公室 连衣裙 配小 坎肩 美国 设计师 Diane Von Furstenberg 曾经 感觉 女人 穿 连衣裙 女人 找到 一件 适合 dream dress 重要 无需 费神 搭配 单穿 连身 优雅 飞扬 裙摆 似乎 告诉 女 连衣裙 玩起 High Fashion 变脸 游戏 DKNY 绿色 连衣裙 新品 未 定价 H \u0026amp; M 黑色 外套 新品 未 定价 Agatha 配件 新品 未 定价 C . Banner 高跟鞋 新品 未 定价 低 V 领 连衣裙 秀出 属于 性感 更好 展现出 颈部 线条 搭配 修身 剪裁 西装 短款 皮手套 极具 欧美 明星 范儿 细 高跟鞋 更好 突出 双腿 长度 整体 显得 轻盈 不少 On \u0026amp; on 米色 连衣裙 新品 未 定价 Asobio 针织 外套 RMB 449 Kookai 金色 腰带 Jc ----------- Document: 435, Semantic similarity: 0.5440454483032227 ----------- 组图 秋冬 优雅 妖娆 女星 爱 裸 色系 导语 裸色 优雅 代名词 女星 近来 誓 裸色 进行 到底 无论是 徐若 ? 性感 乐基儿 气质 搭配 各色 礼服 赏心悦目 娇俏 款式 更是 大饱眼福 徐若 ? 飘逸 丝带 立刻 彰显 天王 嫂 贵妇 气质 袁咏仪 翻领 西装 气质 非凡 裸色 短款 紧身 西装 皮质 面料 彰显 个性 夹带 一点 蕾丝 装饰 女性 柔美 油然而生 搭配 碎花 蛋糕 裙 气质 非凡 ----------- Document: 870, Semantic similarity: 0.523485541343689 ----------- 组图 看达人 演绎 豹纹 军装 风 导语 懂得 潮流 总是 知道 适合 今冬 流行 亮点 太 军装 豹纹 类似 民族风情 想要 知道 搭配 快 看看 时尚 达 穿 军绿色 宽松 款 大衣 不失 俏皮 味道 高腰 设计 短裙 有效 提升 腰线 衬托出 修长 美腿 豹纹 今年 冬季 抢眼 搭配 元素 加上 驼色 针织衫 灰色 围巾 暖 棕色 手 挎包 整体 色调 统一 迷人 棕色 蓝色 结合能 眼前一亮 简洁 款式 依然 突显 ----------- Document: 522, Semantic similarity: 0.4756317138671875 ----------- 女星 争当 蓝色妖姬 \u0026amp; nbsp ; 英国 气质 女演员 瑞切尔 ・ 薇 兹 时尚 点评 英国 气质 女演员 瑞切尔 · 薇 兹 ( Rachel Weisz ) 美貌 非常 头脑 修身 印花 连衣裙 搭配 抢眼 棕红色 短 夹克 非常 好看 搭配 黑色 罗马 feel 高跟鞋 特别 有潮味 时尚 点评 身材 不算 瘦 女星 Lea Michele 搭配 起来 非常 特色 一味 地瘦 风格 满是 褶皱 裙子 非常 修身 亮眼 颜色 非常 ----------- Document: 707, Semantic similarity: 0.47334203124046326 ----------- 组图 黑丝 短裙 上阵 5 旬 女星 胜过 90 红星 导语 气温 越来越低 女星 不畏 严寒 纷纷 穿着 短裙 透视装 出席 活动 一番 比拼 不难 发现 气质 年轻 难得 厉害 一起 看看 刘晓庆 55 岁 近日 颁奖礼 刘晓庆 一袭 宝蓝色 超低 胸 V 领 长裙 亮相 轻薄 蕾丝 奢华 皮草 艳丽 色彩 翠绿 首饰 配上 短小 精炼 波波 头 瞬间 减龄 15 岁 张曼玉 46 岁 一向 气质 型 美女 著称 反倒 少 繁琐 修饰 刻意 打扮 超级 简单 Lanvin for H \u0026amp; M 斜肩 礼裙 搭配 一双 皮质 手套 -----------  \n10. get_topic_hierarchy 对话题进行分类，需要\n 先执行model.hierarchical_topic_reduction 再执行model.get_topic_hierarchy。  # 将话题分为2类 model.hierarchical_topic_reduction(num_topics=2) model.get_topic_hierarchy() Run\n[[7, 6, 1, 8, 5, 4, 3], [2, 0]]  \n11. similar_words 查找相似词， 该方法其实也可以用于扩充词典。\nsimilar_words(keywords, num_words, keywords_neg=None)\n keywords: 待查询关键词列表 num_words: 返回相似词个数 keywords_neg: 指定反义词列表  # 查找【增进】的最相似的10个词 model.similar_words(keywords=[\u0026#34;增进\u0026#34;], num_words=10, keywords_neg=None) Run\n(array(['两国关系', '两国', '温家宝', '王刚', '战略', '友好', '中欧', '政治', '会见', '人民'], dtype='\u0026lt;U4'), array([0.50498132, 0.49835259, 0.4636392 , 0.45802986, 0.45299921, 0.44836198, 0.43550295, 0.43471974, 0.43099192, 0.42711113]))  12. save 训练不易， 记得保存模型。\nmodel.save(\u0026#39;随便起个名字.pkl\u0026#39;) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/top2vec_tutorial/","summary":"Top2Vec 是一种用于主题建模和语义搜索的算法。**我个人从理解代码和使用代码难度来看， 对于Python小白，BERTopic更适合直接用预训练词向量，而Top2Vec更适合对小规模数据训练词向量后做主题建模。**它自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。训练 Top2Vec 模型后，您可以：\n 获取检测到的主题数。 获取话题。 获取主题大小。 获取分层主题。 按关键字搜索主题。 按主题搜索文档。 按关键字搜索文档。 找出相似的词。 查找类似的文档。 使用 RESTful-Top2Vec 公开模型 有关其工作原理的更多详细信息，请参阅论文。  亮点\n 自动查找主题数。 不需要停用词列表。 不需要词干/词形还原。 适用于短文本。 创建联合嵌入的主题、文档和词向量。 内置搜索功能。  它是如何工作的？\n该算法做出的假设是，许多语义相似的文档都表明了一个潜在的主题。\n第一步是创建文档和词向量的联合嵌入。一旦文档和单词被嵌入到一个向量空间中，算法的目标就是找到密集的文档集群，然后确定哪些单词将这些文档吸引到一起。每个密集区域是一个主题，将文档吸引到密集区域的词就是主题词。\n!pip3 install top2vec==1.0.27 0. 代码下载 click to download code\n\n1. 导入数据 使用某灾难数据集，这里是存在标注的标签，但是我们假设不用label的，仅作为评判Top2vec运行效果的标准。点击cnews.csv下载\nfrom IPython.display import display import cntext as ct from top2vec import Top2Vec import pandas as pd import jieba stopwords = ct.load_pkl_dict(\u0026#39;STOPWORDS.pkl\u0026#39;)[\u0026#39;STOPWORDS\u0026#39;][\u0026#39;chinese\u0026#39;] df = pd.read_csv(\u0026#39;cnews.csv\u0026#39;) df.head() df.","title":"Top2Vec|主题建模和语义搜索库"},{"content":" Src:https://github.com/ruanyf/weekly/blob/master/docs/issue-175.md\nAuthor: 阮一峰\n 上一期谈到，“特长程序员”比“全能程序员”吃香。\n网友发给我一个网址，国外有人已经谈过这个话题。\n 国外作者画了一张图，将世界上的程序员比喻成三种形状。\n（1）大饼型：拥有知识广度，但没有知识深度。\n（2）竹竿型：拥有知识深度，但没有知识广度。\n（3）T 型：介于前两者之间。\n那篇文章也认为，通常情况下，竹竿型程序员的出路最好，雇主愿意为知识深度买单。因为现代社会高度分工，越是大公司，岗位职责越明确，需要的是领域专家，而不是多面手。\n但是作者提到，有一种情况例外，知识广度有明显优势，那就是确定项目路线的时候。\n 请看上图，右上角有一个黑点，那是公司的目标。当前位置在左下角的原点，两点之间没有现成的道路，需要自己寻找实现路径。\n这时，知识广度就发挥作用了，了解的信息越广泛，知道的情况越多，就越可能具备洞察力，能够在多条路径里面，选出最合适的道路。知识面狭窄的人，由于不了解其它道路，不管遇到什么问题，可能都选择他会的那一条路，比如 Java。\n这就是全能程序员的优势，判断正确的仰角 θ，使得两点之间距离最短；特长程序员的优势是前进速度 r，可以在既定道路上做到快速前进。所以，知识的广度能告诉你什么是正确的方向，知识的深度则可以让你在该方向上快速前进。\n对于长期而艰巨的项目，走得快固然重要，但更重要的是走对方向。如果仰角 θ 不对，走得再快也没用，因为一开始就走错方向，后期必须停下来校正方向，甚至可能永远到达不了目标，白白浪费了生命。\n 因此，全能型的人才比较适合确定项目方向，担任团队领导。 乔布斯、马斯克就是这样的人，强在知识的广度，而不是知识的深度。他们既懂技术，又懂市场和管理，还了解人文（“我喜欢站在人文和技术的交叉点”），一旦掌握资源，就能带领团队，做出创新的产品。但是，如果让他们担任工程师，绝对是糟糕的工程师，会被开除。\n不过话说回来，现实中，谁会因为你知道的事情多，就让你担任团队领导呢？反而是因为你克服了技术难题，才有机会来领导团队。乔布斯和马斯克都是自己创业才出头的，恐怕不是偶然。\n总结一下，对于公司来说，如果有明确的技术方向，那么就需要聘请特长程序员，帮助加快开发速度。如果是刚刚诞生的创业公司，方向还在摸索之中，那么全能程序员也许更有价值。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/wide_vs_broden/","summary":" Src:https://github.com/ruanyf/weekly/blob/master/docs/issue-175.md\nAuthor: 阮一峰\n 上一期谈到，“特长程序员”比“全能程序员”吃香。\n网友发给我一个网址，国外有人已经谈过这个话题。\n 国外作者画了一张图，将世界上的程序员比喻成三种形状。\n（1）大饼型：拥有知识广度，但没有知识深度。\n（2）竹竿型：拥有知识深度，但没有知识广度。\n（3）T 型：介于前两者之间。\n那篇文章也认为，通常情况下，竹竿型程序员的出路最好，雇主愿意为知识深度买单。因为现代社会高度分工，越是大公司，岗位职责越明确，需要的是领域专家，而不是多面手。\n但是作者提到，有一种情况例外，知识广度有明显优势，那就是确定项目路线的时候。\n 请看上图，右上角有一个黑点，那是公司的目标。当前位置在左下角的原点，两点之间没有现成的道路，需要自己寻找实现路径。\n这时，知识广度就发挥作用了，了解的信息越广泛，知道的情况越多，就越可能具备洞察力，能够在多条路径里面，选出最合适的道路。知识面狭窄的人，由于不了解其它道路，不管遇到什么问题，可能都选择他会的那一条路，比如 Java。\n这就是全能程序员的优势，判断正确的仰角 θ，使得两点之间距离最短；特长程序员的优势是前进速度 r，可以在既定道路上做到快速前进。所以，知识的广度能告诉你什么是正确的方向，知识的深度则可以让你在该方向上快速前进。\n对于长期而艰巨的项目，走得快固然重要，但更重要的是走对方向。如果仰角 θ 不对，走得再快也没用，因为一开始就走错方向，后期必须停下来校正方向，甚至可能永远到达不了目标，白白浪费了生命。\n 因此，全能型的人才比较适合确定项目方向，担任团队领导。 乔布斯、马斯克就是这样的人，强在知识的广度，而不是知识的深度。他们既懂技术，又懂市场和管理，还了解人文（“我喜欢站在人文和技术的交叉点”），一旦掌握资源，就能带领团队，做出创新的产品。但是，如果让他们担任工程师，绝对是糟糕的工程师，会被开除。\n不过话说回来，现实中，谁会因为你知道的事情多，就让你担任团队领导呢？反而是因为你克服了技术难题，才有机会来领导团队。乔布斯和马斯克都是自己创业才出头的，恐怕不是偶然。\n总结一下，对于公司来说，如果有明确的技术方向，那么就需要聘请特长程序员，帮助加快开发速度。如果是刚刚诞生的创业公司，方向还在摸索之中，那么全能程序员也许更有价值。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"知识广度 vs 知识深度"},{"content":"经常有朋友咨询可否提供有偿服务，目前可接文本数据分析，常见的分析支持\n 词频统计 情感分析 LDA话题分析 文本分类 相似度分析 其他  为了提高沟通效率，需要您提供需求文档，需要说明两部分信息\n1、数据说明  是否已拥有数据 数据的文件格式(pdf、txt、excel类等) 数据量(文件数或多少M)  2、数据分析  列出需要计算的指标（列出计算方法） 分析结果的存储格式及字段样式(建议excel格式)  提示 我自己时间精力有限，服务肯定没有全职数据分析师做的好，更建议大家付费购买课程自学，技术上打铁还需自身硬。常见的数据分析都已形成课程，价格也不贵。\n 点击上方图片购买课程   点击进入详情页\n咨询方式 准备好需求文档后，可加微信18373154668（微信手机同号）\n 中午12:00-14:00 晚间22:00-24:00  ","permalink":"/blog/paid_for_service/","summary":"经常有朋友咨询可否提供有偿服务，目前可接文本数据分析，常见的分析支持\n 词频统计 情感分析 LDA话题分析 文本分类 相似度分析 其他  为了提高沟通效率，需要您提供需求文档，需要说明两部分信息\n1、数据说明  是否已拥有数据 数据的文件格式(pdf、txt、excel类等) 数据量(文件数或多少M)  2、数据分析  列出需要计算的指标（列出计算方法） 分析结果的存储格式及字段样式(建议excel格式)  提示 我自己时间精力有限，服务肯定没有全职数据分析师做的好，更建议大家付费购买课程自学，技术上打铁还需自身硬。常见的数据分析都已形成课程，价格也不贵。\n 点击上方图片购买课程   点击进入详情页\n咨询方式 准备好需求文档后，可加微信18373154668（微信手机同号）\n 中午12:00-14:00 晚间22:00-24:00  ","title":"有偿数据分析服务"},{"content":"爱企查 想搜集企业信息，可以使用爱企查网站，例如通过该网站，搜”华为“，，可以获得与关键词华为相关的很多企业名信息  设计网络爬虫步骤\n 使用开发者工具network面板审查网站的网址规律urls 对单个网址url尝试访问 确定网站是html或json类型 从网页中解析定位需要的数据。   使用pyquery解析html页面数据； 或使用json解析json页面数据  存储到csv 重复2-5  尝试访问第一页  经过开发者工具network，可以使用requests对其进行访问。\n需要注意的是，headers中需要加入Referer参数，该参数作用是告诉服务器\n 兄弟，我是经过Referer介绍的，不然我也不可能知道 https://aiqicha.baidu.com/s/advanceFilterAjax?q=%E5%8D%8E%E4%B8%BA\u0026amp;t=\u0026amp;p=1\u0026amp;s=10\u0026amp;o=0\u0026amp;f=%7B%7D 这个网址啊\n import requests from urllib.parse import quote query = \u0026#39;华为\u0026#39; url = \u0026#39;https://aiqicha.baidu.com/s/advanceFilterAjax?q={q}\u0026amp;t=\u0026amp;p=1\u0026amp;s=10\u0026amp;o=0\u0026amp;f=%7B%7D\u0026#39;.format(q=quote(query)) headers = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;https://aiqicha.baidu.com/s?q={q}\u0026amp;t=0\u0026#39;.format(q=quote(query))} resp = requests.get(url, headers=headers) resp \u0026lt;Response [200]\u0026gt;  解析网页数据   通过开发者工具可以看到，这个网站采用的json类型网页数据。\n  好在这类网页的解析定位比较简单。\nfor com in resp.json()[\u0026#39;data\u0026#39;][\u0026#39;resultList\u0026#39;][:3]: print(com) print() Run\n{\u0026#39;pid\u0026#39;: \u0026#39;31360200662522\u0026#39;, \u0026#39;entName\u0026#39;: \u0026#39;\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;技术有限公司\u0026#39;, \u0026#39;entType\u0026#39;: \u0026#39;有限责任公司(自然人投资或控股的法人独资)\u0026#39;, \u0026#39;validityFrom\u0026#39;: \u0026#39;1896-08-14\u0026#39;, \u0026#39;domicile\u0026#39;: \u0026#39;深圳市龙岗区坂田\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;总部办公楼\u0026#39;, \u0026#39;entLogo\u0026#39;: \u0026#39;https://zhengxin-pub.cdn.bcebos.com/logopic/67e739bf0e47768f4a1f542daf3f7f42_fullsize.jpg\u0026#39;, \u0026#39;openStatus\u0026#39;: \u0026#39;开业\u0026#39;, \u0026#39;legalPerson\u0026#39;: \u0026#39;赵明路\u0026#39;, \u0026#39;tags\u0026#39;: {\u0026#39;laTaxer\u0026#39;: \u0026#39;\u0026lt;span class=\u0026#34;zx-ent-tag laTaxer\u0026#34;\u0026gt;A级纳税人(2020)\u0026lt;/span\u0026gt;\u0026#39;}, \u0026#39;logoWord\u0026#39;: \u0026#39;华为技术\u0026#39;, \u0026#39;titleName\u0026#39;: \u0026#39;华为技术有限公司\u0026#39;, \u0026#39;titleLegal\u0026#39;: \u0026#39;赵明路\u0026#39;, \u0026#39;titleDomicile\u0026#39;: \u0026#39;深圳市龙岗区坂田华为总部办公楼\u0026#39;, \u0026#39;levelAtaxer\u0026#39;: [2020, 2018, 2019, 2016, 2017, 2014, 2015], \u0026#39;regCap\u0026#39;: \u0026#39;5,035,113.2万\u0026#39;, \u0026#39;scope\u0026#39;: \u0026#39;一般经营项目是:程控交换机、传输设备、数据通信设备、宽带多媒体设备、电源、无线通信设备、微电子产品、软件、系统集成工程、计算机及配套设备、终端设备及相关通信信息产品、数据中心机房基础设施及配套产品(含供配电、空调制冷设备、智能管理监控等)的开发、生产、销售、技术服务、工程安装、维修、咨询、代理、租赁;信息系统设计、集成、运行维护;集成电路设计、研发;统一通信及协作类产品,服务器及配套软硬件产品,存储设备及相关软件的研发、生产、销售;无线数据产品(不含限制项目)的研发、生产、销售;通信站点机房基础设施及通信配套设备(含通信站点、通信机房、通信电源、机柜、天线、通信线缆、配电、智能管理监控、锂电及储能系统等)的研发、生产、销售;能源科学技术研究及能源相关产品的研发、生产、销售;大数据产品、物联网及通信相关领域产品的研发、生产、销售;汽车零部件及智能系统的研发、生产、销售及服务;建筑工程;设计、制作、发布、代理各类广告;通信设备租赁(不含限制项目);培训服务;技术认证服务;信息咨询(不含限制项目);企业管理咨询(不含限制项目);进出口业务;国内商业、物资供销业业务(不含专营、专控、专卖商品);对外经济技术合作业务;房屋租赁业务(持许可经营证);以及其他法律法规不禁止的经营活动(依法须经批准的项目,经相关部门批准后方可开展经营活动)。,许可经营项目是:增值电信业务经营。\u0026#39;, \u0026#39;regNo\u0026#39;: \u0026#39;815503001822039217\u0026#39;, \u0026#39;hitReason\u0026#39;: [{\u0026#39;品牌项目\u0026#39;: \u0026#39;\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;\u0026#39;}, {\u0026#39;商标名称\u0026#39;: \u0026#39;\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;\u0026#39;}, {\u0026#39;企业名称\u0026#39;: \u0026#39;\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;技术有限公司\u0026#39;}, {\u0026#39;网站名称\u0026#39;: \u0026#39;\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;应用平台1\u0026#39;}, {\u0026#39;地址\u0026#39;: \u0026#39;深圳市龙岗区坂田\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;总部办公楼\u0026#39;}], \u0026#39;labels\u0026#39;: {\u0026#39;opening\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;开业\u0026#39;, \u0026#39;style\u0026#39;: \u0026#39;blue\u0026#39;, \u0026#39;fontColor\u0026#39;: \u0026#39;#1EA930\u0026#39;, \u0026#39;bgColor\u0026#39;: \u0026#39;#EBF6EC\u0026#39;}}, \u0026#39;personTitle\u0026#39;: \u0026#39;法定代表人\u0026#39;, \u0026#39;personId\u0026#39;: \u0026#39;a9f275934f59110096757b656ba41382\u0026#39;} {\u0026#39;pid\u0026#39;: \u0026#39;28610144220343\u0026#39;, \u0026#39;entName\u0026#39;: \u0026#39;\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;终端(深圳)有限公司\u0026#39;, \u0026#39;entType\u0026#39;: \u0026#39;有限责任公司(外商投资、非独资)\u0026#39;, \u0026#39;validityFrom\u0026#39;: \u0026#39;2003-12-25\u0026#39;, \u0026#39;domicile\u0026#39;: \u0026#39;深圳市龙岗区坂田\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;基地B区2号楼\u0026#39;, \u0026#39;entLogo\u0026#39;: \u0026#39;https://zhengxin-pub.cdn.bcebos.com/logopic/a638462a7a48ab79f84b7db2c0e64230_fullsize.jpg\u0026#39;, \u0026#39;openStatus\u0026#39;: \u0026#39;开业\u0026#39;, \u0026#39;legalPerson\u0026#39;: \u0026#39;赵明路\u0026#39;, \u0026#39;tags\u0026#39;: {\u0026#39;laTaxer\u0026#39;: \u0026#39;\u0026lt;span class=\u0026#34;zx-ent-tag laTaxer\u0026#34;\u0026gt;A级纳税人(2020)\u0026lt;/span\u0026gt;\u0026#39;}, \u0026#39;logoWord\u0026#39;: \u0026#39;华为终端\u0026#39;, \u0026#39;titleName\u0026#39;: \u0026#39;华为终端(深圳)有限公司\u0026#39;, \u0026#39;titleLegal\u0026#39;: \u0026#39;赵明路\u0026#39;, \u0026#39;titleDomicile\u0026#39;: \u0026#39;深圳市龙岗区坂田华为基地B区2号楼\u0026#39;, \u0026#39;levelAtaxer\u0026#39;: [2020, 2018, 2019, 2016, 2017, 2014, 2015], \u0026#39;regCap\u0026#39;: \u0026#39;1,598,080.8万\u0026#39;, \u0026#39;scope\u0026#39;: \u0026#39;一般经营项目是：开发、生产、销售通信电子产品及配套产品，并提供技术咨询和售后服务。进出口业务（不含分销)。，许可经营项目是：\u0026#39;, \u0026#39;regNo\u0026#39;: \u0026#39;815503006447640305\u0026#39;, \u0026#39;hitReason\u0026#39;: [{\u0026#39;企业名称\u0026#39;: \u0026#39;\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;终端(深圳)有限公司\u0026#39;}, {\u0026#39;网站名称\u0026#39;: \u0026#39;\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;HARMONYOS网站\u0026#39;}, {\u0026#39;地址\u0026#39;: \u0026#39;深圳市龙岗区坂田\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;基地B区2号楼\u0026#39;}], \u0026#39;labels\u0026#39;: {\u0026#39;opening\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;开业\u0026#39;, \u0026#39;style\u0026#39;: \u0026#39;blue\u0026#39;, \u0026#39;fontColor\u0026#39;: \u0026#39;#1EA930\u0026#39;, \u0026#39;bgColor\u0026#39;: \u0026#39;#EBF6EC\u0026#39;}}, \u0026#39;personTitle\u0026#39;: \u0026#39;法定代表人\u0026#39;, \u0026#39;personId\u0026#39;: \u0026#39;a9f275934f59110096757b656ba41382\u0026#39;} {\u0026#39;pid\u0026#39;: \u0026#39;30140456955334\u0026#39;, \u0026#39;entName\u0026#39;: \u0026#39;\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;终端有限公司\u0026#39;, \u0026#39;entType\u0026#39;: \u0026#39;有限责任公司(外商投资企业法人独资)\u0026#39;, \u0026#39;validityFrom\u0026#39;: \u0026#39;2012-11-23\u0026#39;, \u0026#39;domicile\u0026#39;: \u0026#39;广东省东莞市松山湖园区新城路2号\u0026#39;, \u0026#39;entLogo\u0026#39;: \u0026#39;https://zhengxin-pub.cdn.bcebos.com/logopic/cc662a5d573b793e9c5b84031350ced0_fullsize.jpg\u0026#39;, \u0026#39;openStatus\u0026#39;: \u0026#39;开业\u0026#39;, \u0026#39;legalPerson\u0026#39;: \u0026#39;赵明路\u0026#39;, \u0026#39;tags\u0026#39;: {\u0026#39;laTaxer\u0026#39;: \u0026#39;\u0026lt;span class=\u0026#34;zx-ent-tag laTaxer\u0026#34;\u0026gt;A级纳税人(2020)\u0026lt;/span\u0026gt;\u0026#39;}, \u0026#39;logoWord\u0026#39;: \u0026#39;华为终端\u0026#39;, \u0026#39;titleName\u0026#39;: \u0026#39;华为终端有限公司\u0026#39;, \u0026#39;titleLegal\u0026#39;: \u0026#39;赵明路\u0026#39;, \u0026#39;titleDomicile\u0026#39;: \u0026#39;广东省东莞市松山湖园区新城路2号\u0026#39;, \u0026#39;levelAtaxer\u0026#39;: [2020, 2018, 2019, 2016, 2017, 2014, 2015], \u0026#39;regCap\u0026#39;: \u0026#39;70,000.0万\u0026#39;, \u0026#39;scope\u0026#39;: \u0026#39;开发、生产、销售：通信及电子产品、计算机、卫星电视接收天线、高频头、数字卫星电视接收机及前述产品的配套产品，并提供技术咨询和售后服务；开发、生产、销售：医疗器械（第一类、第二类、第三类医疗器械），并提供技术咨询和售后服务；增值电信业务经营；佣金代理；货物或技术进出口（国家禁止或涉及行政审批的货物和技术进出口除外）。(依法须经批准的项目，经相关部门批准后方可开展经营活动)\u0026#39;, \u0026#39;regNo\u0026#39;: \u0026#39;815518000494355853\u0026#39;, \u0026#39;hitReason\u0026#39;: [{\u0026#39;企业名称\u0026#39;: \u0026#39;\u0026lt;em\u0026gt;华为\u0026lt;/em\u0026gt;终端有限公司\u0026#39;}], \u0026#39;labels\u0026#39;: {\u0026#39;opening\u0026#39;: {\u0026#39;text\u0026#39;: \u0026#39;开业\u0026#39;, \u0026#39;style\u0026#39;: \u0026#39;blue\u0026#39;, \u0026#39;fontColor\u0026#39;: \u0026#39;#1EA930\u0026#39;, \u0026#39;bgColor\u0026#39;: \u0026#39;#EBF6EC\u0026#39;}}, \u0026#39;personTitle\u0026#39;: \u0026#39;法定代表人\u0026#39;, \u0026#39;personId\u0026#39;: \u0026#39;a9f275934f59110096757b656ba41382\u0026#39;} \n完整爬虫 经过刚刚的几个步骤，我们现在只需要\nimport csv import requests import time query = \u0026#39;华为\u0026#39; max_pages = 10 #获取前10页的企业信息数据 #存储数据 csvf = open(\u0026#39;企业信息.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;titleName\u0026#39;, \u0026#39;titleDomicile\u0026#39;, \u0026#39;titleLegal\u0026#39;, \u0026#39;validityFrom\u0026#39;, \u0026#39;regCap\u0026#39;, \u0026#39;regNo\u0026#39;, \u0026#39;scope\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() #访问 for page in range(1, max_pages): url = \u0026#39;https://aiqicha.baidu.com/s/advanceFilterAjax?q={q}\u0026amp;t=\u0026amp;p={p}\u0026amp;s=10\u0026amp;o=0\u0026amp;f=%7B%7D\u0026#39;.format(q=quote(query), p=page) headers = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;https://aiqicha.baidu.com/s?q={q}\u0026amp;t=0\u0026#39;.format(q=quote(query))} resp = requests.get(url, headers=headers) #解析数据 for com in resp.json()[\u0026#39;data\u0026#39;][\u0026#39;resultList\u0026#39;]: data = dict() for fieldname in fieldnames: data[fieldname] = com[fieldname] #写入csv writer.writerow(data) csvf.close() \n运行结果 采集10页的爬虫运行结束后，尝试读取 企业信息.csv\nimport pandas as pd df= pd.read_csv(\u0026#39;企业信息.csv\u0026#39;) df.head()   广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/%E7%88%B1%E4%BC%81%E6%9F%A5/","summary":"爱企查 想搜集企业信息，可以使用爱企查网站，例如通过该网站，搜”华为“，，可以获得与关键词华为相关的很多企业名信息  设计网络爬虫步骤\n 使用开发者工具network面板审查网站的网址规律urls 对单个网址url尝试访问 确定网站是html或json类型 从网页中解析定位需要的数据。   使用pyquery解析html页面数据； 或使用json解析json页面数据  存储到csv 重复2-5  尝试访问第一页  经过开发者工具network，可以使用requests对其进行访问。\n需要注意的是，headers中需要加入Referer参数，该参数作用是告诉服务器\n 兄弟，我是经过Referer介绍的，不然我也不可能知道 https://aiqicha.baidu.com/s/advanceFilterAjax?q=%E5%8D%8E%E4%B8%BA\u0026amp;t=\u0026amp;p=1\u0026amp;s=10\u0026amp;o=0\u0026amp;f=%7B%7D 这个网址啊\n import requests from urllib.parse import quote query = \u0026#39;华为\u0026#39; url = \u0026#39;https://aiqicha.baidu.com/s/advanceFilterAjax?q={q}\u0026amp;t=\u0026amp;p=1\u0026amp;s=10\u0026amp;o=0\u0026amp;f=%7B%7D\u0026#39;.format(q=quote(query)) headers = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;https://aiqicha.baidu.com/s?q={q}\u0026amp;t=0\u0026#39;.format(q=quote(query))} resp = requests.get(url, headers=headers) resp \u0026lt;Response [200]\u0026gt;  解析网页数据   通过开发者工具可以看到，这个网站采用的json类型网页数据。\n  好在这类网页的解析定位比较简单。","title":"案例实战 | 企业信息数据采集"},{"content":"摘 要：\n在数字化生活背景下, 传统的管理变成或正在变成数据的管理, 传统的决策变成或正在变成基于数据分析的决策.从大数据的数据特征、问题特征和管理决策特征出发, 讨论管理决策研究和应用的范式转变.大数据驱动范式可以从外部嵌入、技术增强和使能创新三个角度来审视, 并体现出“数据驱动+模型驱动”的“关联+因果”含义.此外, 围绕大数据特征和重要研究方向, 阐述了全景式PAGE框架及其要素.\n关键词： 大数据 ； 管理决策 ； 研究范式 ； 全景式 PAGE 框架\n信息科技的飞速发展和深度融合开启了数字化生活的新篇章, 把人们带入了大数据 (big data) 时代.一方面, 随着各种感应探测技术、智能终端以及移动互联的广泛应用, 使得社会经济生活的方方面面以更细粒度的数据形式呈现, 进而整个社会的“像素”得到显著提升;另一方面, 社会“像素”的提升促进了数字“成像”的发展, 使得通过数据世界可以更清晰地描绘社会经济活动情境, 进而基于数据的商务分析 (business analytics, BA) 正在成为使能创新的核心竞争力.在此背景下, 传统的管理变成或正在变成数据的管理, 传统的决策变成或正在变成基于数据分析的决策.\n近年来, 大数据成为学界、政界和业界持续关注的热点.在学术界, 早在2008年和2011年, 《Nature》与《Science》杂志分别从互联网技术、互联网经济学、超级计算、环境科学以及生物医药等多方面讨论大数据的处理与应用45此后, 大数据在各个学科领域包括医学、经济学、管理学以及公共管理等领域得到了广泛的探讨与研究6789同时, 大数据也引起世界各国高度重视, 美国、欧盟、澳大利亚以及日本等国部署了一系列大数据相关战略和关键领域。在产业界, 国内外大批知名企业掀起了技术产业创新浪潮, 通过收购与合作构建和提升大数据技术与应用能力, 布局和开拓相关的业态和市场.\n我国政府对大数据高度重视并有一系列前瞻性洞见和部署.2015年十八届五中全会提出实施国家大数据战略 , 国务院发布《促进大数据发展行动纲要》 , 指出大数据是国家基础性战略资源, 旨在全面推进我国大数据发展和应用, 加快建设数据强国.2017年十九大报告进一步强调要推动互联网、大数据、人工智能和实体经济深度融合.通过国家需求、政策支持、产业结合以及企业研发等形式, 近些年来涌现出一大批重大规划和政产学研项目, 包括国家自然科学基金委员会 (NSFC) 于2015年9月启动的“大数据驱动的管理与决策研究”重大研究计划 (简称NSFC大数据重大研究计划, 参见附注) .\n大数据在给社会经济生活带来深刻变革的同时, 也对管理与决策研究带来一系列新的重要课题.从信息技术 (IT) 范畴来看, 可以从两个视角来认识大数据, 即大数据的“造”与“用”视角 (如图1所示) .这和产品的属性类似, 一方面, 人们关心产品是如何设计和制造出来的;另一方面, 人们关心产品是如何使用和有用的.大数据以IT的形式呈现, 通常可以概括为数据和系统 (包括算法、应用、平台等) .从造的视角出发, 涉及的主要问题包括大数据分析 (如画像、学习、推断等) 和大数据系统建设 (如体系、功能、集成等) .从用的视角出发, 涉及的主要问题包括大数据使用行为 (如采纳、影响、管理等) 和大数据使能创新 (如要素、价值、市场等) .\n 值得一提的是, 大数据相关的研究不仅需要对相关领域的理论与应用进行探索和创新, 也需要对许多惯常的认识视角和方法论范式进行审视和发展.同时, 我国学者和研究人员也面临着“严谨 (rigor) 与相关 (relevance) ” (学术规范与实践影响) 和“世界与中国” (国际视野与中国根基) 既分野又统一的挑战, 当然应对这些挑战也为创新机遇开拓了广袤的空间.\n大数据特征 概括说来, 大数据的特征可以从三个方面来描述:数据特征, 问题特征和管理决策特征, 分别刻画大数据具有的数据属性、大数据问题的特点、以及管理决策大数据问题的视角.\n1.1 大数据的数据特征 大数据作为数据, 具有体量大、多样性、 (价值) 密度低、速率高等属性特征 (即4V等特征）.第一, 数字化生活各要素的数据生成和交互加速了数据的海量积累, 使得数据规模剧增.体量大可以从超规模 (即超出传统规模) 和问题领域角度来理解, 因为规模是与问题领域相关, 而不是拘泥于统一量纲标准.例如, 市场营销领域的客户满意度调查的传统方式是问卷和访谈, 那么进一步考虑海量网上购物评论和社交媒体体验分享的用户生成内容 (user generated content, UGC) 就构成了一个大数据情境.第二, 数字化生活各要素的数据生成和交互丰富了数据类型, 使得数据多样性成为常态.多样性强调数据的多源异构和富媒体 (如文本、语音、图片、视频等) 特点.例如, 社交网络上的公众声音、智慧交通平台上的影像信息等均为富媒体形态且来源广泛.第三, 数字化生活各要素的数据生成和交互在加速海量积累的同时也减少了价值数据的占比, 使得价值发现的难度提升.价值密度低意味着数据挖掘和商务分析是大数据应用的关键.例如, 对于在线企业或服务平台来讲, 随着网络访问的增加和业务活动的扩展, 识别高价值的潜在用户变得相对困难, 也凸显出大数据分析的重要性.第四, 数字化生活各要素的数据生成和交互强化了流数据形态和即时性, 使得数据传输和交换速率显著升高.速率高对平滑流通和连续商务提出了更高要求.例如, 智能手机客户端应用软件 (Apps) 的使用需要在服务内容和效果方面 (包括相关内容的浏览、下载、上传、响应、展现等) 有良好的临场感和实时体验.\n1.2 大数据的问题特征 在各类研究和应用问题中, 有一类问题可以归为大数据问题.大数据问题应至少具有以下三个特点:粒度缩放、跨界关联和全局视图.首先, 粒度缩放是指问题要素的数据化, 并能够在不同粒度层级间进行缩放.这需要通过数据感知、连接和采集获得足够细的粒度性, 同时对于不同层级间的粒度转换具有分解和聚合能力.其次, 跨界关联是指问题的要素空间外拓.这需要扩展惯常的要素约束和领域视角, 强调“外部性”和“跨界”, 在问题要素空间中通过引入外部视角与传统视角联动, 将内部数据 (如个体自身、企业组织和行业等内部数据) 与外部数据 (如社会媒体内容等) 予以关联.最后, 全局视图是指问题定义与求解的全局性, 强调对相关情境的整体画像及其动态演化的把控和诠释.这需要基于数据分析和平台集成的全景式“成像”能力.\n在数字化生活的背景下, 具有粒度缩放、跨界关联和全局视图特点的应用问题不断涌现, 进而激发了大量创新并催生了许多新模式、新业态.例如, 在医疗健康领域, 传统疾病诊疗中的病人就医关系正在被扩展为融合院外检测、干预、康复数据的新型诊疗模式.其中, 不仅涉及传统意义上的生化、影像和诊疗等医院内部数据, 也涉及医院外病人和社区相关的体征、体验、社会关系、环境等外部数据.这里, 需要获取相关生化组织、疾病、人、社区、环境等微观宏观粒度信息;同时进行视角拓展和关联, 包括从科室内外到医院内外的跨界融合;进而, 可以在全局层面进行更为有效的诊疗决策和管理.此外, 近年来发展迅速的新型医疗健康服务平台, 通过整合社会和行业资源, 连接医生、公众、医院以及相关上下游企业提供信息咨询、诊疗链入、健康指导等服务产品, 形成了一类新业态并呈现显著的大数据问题特征.再如, 在新型商务领域, 共享单车体现了大数据问题的粒度缩放、跨界关联和全局视图特点.通过车载传感器、定位系统以及智能手机终端等设备获得调度和管理需要的“人—车—路”粒度信息;同时, 打通导航、支付、通讯、商铺以及餐饮等诸多业务功能, 实现跨界联动;进而, 企业和平台可以从全局出发形成整体画像, 并优化布局和运作以做出相应的管理决策.\n1.3 大数据管理决策特征 一般而言, 管理者在业务活动中通常有三个关注:发生了什么 (what) , 为什么发生 (why) 以及将发生什么 (will) .在大数据问题特征的情境下, 这三个关注可以从业务层面、数据层面和决策层面进行刻画, 进而形成管理决策大数据问题的特征框架 (如图2所示) .\n 首先, 对于发生了什么 (what) 的关注, 业务层面需要反映业务的状态, 即已经发生或者正在发生的事件和活动 (如市场份额、交易现状、KPI表现等) ;数据层面需要体现业务环节的数据粒度, 即现有的数据能否足够支撑管理者对不同粒度层级的业务状态进行了解和把握 (如感知、采集、解析、融合等) ;决策层面需要构建问题的全局视角, 即定期整合汇总以及随需要素展现 (如:按时统计报表、实时信息查询等) .\n接着, 对于为什么会发生 (why) 的关注, 业务层面需要反映业务及其要素之间的联系, 即业务特定状态的发生与哪些环节和要素有关联;数据层面需要体现不同业务数据路径的连接, 即不同粒度层级和跨界关联的业务数据是否有效融通, 并能够支持对数据的分析处理 (如多维、切分、回溯等) ;决策层面需要发现关联业务/要素之间的因果关系, 即厘清业务逻辑和状态转换机理.在此, 特别需要指出的是, 在很多情形下, 尤其在管理决策领域, 大数据需要既讲关联也讲因果.对于许多管理问题而言, 如果决策者对事件之间的因果关系没有准确的分析与判断, 则难以做出有效的决策, 当管理者面临重大决策时更是如此 (如投融资、进入新市场、业务转型、结构重组等) .\n进而, 对于将发生什么 (will) 的关注, 业务层面需要反映业务发展轨迹, 即勾勒出由决策或变化导致的业务走向;数据层面需要体现数据的动态演化情况, 即对于相关事件进行不确定性动态建模并能够支持智能学习和推断 (如模拟、预测、人工智能等) ;决策层面需要提升前瞻性和风险洞见, 即获得决策情境映现和趋势预判能力.\n大数据驱动范式 系统化管理理论的产生及其发展, 包括行为理论、决策理论、权变理论和战略管理等理论体系和管理模型的研究[19], 在提炼管理思想、诠释管理模式和指导管理实践方面发挥了重要作用.长期以来, 管理学研究一直以模型驱动范式为领域主流.在模型驱动范式下, 研究者基于观察抽象和理论推演建立概念模型和关联假设, 再借助解析手段 (例如运筹学和博弈论等分析工具) 对模型进行求解和优化, 或利用相关数据 (包括仿真数据、调研数据、观测数据、系统记录数据等) 对假设进行统计检验.此外, 建立在归纳逻辑基础上的扎根理论等研究范式, 传统上强调从文献概括、实地调研、深度访谈中进行定性推演形成理论和认识.\n但是, 在大数据背景下, 一些新的挑战正在涌现[20,21].这里, 以传统的行为模型或计量模型 (简称传统模型) 为例.第一, 传统模型基于观察抽象、理论推演以及经验提炼确定变量 (或构念) 组合, 以此构建变量关系和理论假设, 并通过数据实证进行模型检验.然而, 在大数据背景下, 常常需要检验大量的变量组合 (如指数级组合数) , 这就使得逐一构建传统模型并进行检验成为难以完成的任务.第二, 有些重要潜在影响因素和隐变量没有被意识到, 因而没有被考虑到传统模型的变量组合中, 这常常导致传统模型的假设与数据的适配性不强, 模型解释力不高.第三, 虽然知道有些影响因素和变量是重要的, 但是由于这些因素和变量在传统意义上不可测或不可获 (如文本、图像、语音等富媒体数据) , 难以容纳到传统模型变量组合中, 进而造成模型解释力不理想.第四, 当样本数据规模大幅增加时, 对一些变量的显著性检验有效性下降, 可能出现联系缺失或拟合过度等情形.\n面对上述挑战, 数据驱动范式的优势不断凸显.概括说来, 数据驱动范式的作用有两个:一是直接发现特定变量关系模式, 形成问题解决方案;二是与模型驱动范式进行补充扩展, 形成融合范式.值得指出的是, 数据驱动范式发现的一类重要关系模式是关联 (association) 及其扩展形式 (如关联规则、层次关联、数量关联、时态关联、类关联、模式关联等) , 并广泛应用到许多领域 (如搜索、推荐、模式识别等) [22].然而, 许多管理决策情形不仅需要关联也需要因果, 这在一定程度上催生了融合范式及其应用.例如, 首先利用数据驱动范式的关联挖掘方法发现变量间的关联, 以缩减变量空间和组合规模;进而利用模型驱动范式的行为方法辨识构念影响路径, 或计量方法解析变量间的因果关系.这是一个“数据驱动+模型驱动”思路, 体现“关联+因果”的诉求, 这对于管理决策尤为重要.这里, 与传统模型相比一个重要区别是, 此时的变量空间中可能存在着一些新颖且潜在的变量及其关联, 在进一步融合运用模型驱动方法构建变量关系时存在困难, 因为已有的理论知识和领域经验不能直接支持相关的建模逻辑和关系形式.这就需要在更深 (包括间接、潜隐) 层面上探寻新的变量影响机理和理论, 并在方法论上另辟新径 (如通过步进/层次/迭代的试错和启发建模方式) .\n特别地, 当数据具有4V等特征并且面对管理决策大数据问题时, 考虑数据驱动与模型驱动的结合、管理决策的关联因果特点、使能创新等元素的一类新型范式 (在此称作大数据驱动范式) 应运而生, 并在深入研究与应用过程中得到进一步发展完善.一般而言, 大数据驱动范式具有“数据驱动+模型驱动”的“关联+因果”性质.具体说来, 大数据驱动范式的框架可从三个角度来审视:外部嵌入、技术增强以及使能创新 (如图3所示) .前两个角度主要涉及方法论层面, 后一个角度主要涉及价值创造层面.\n 2.1 外部嵌入 外部嵌入指外部视角引入, 即将传统模型视角之外的一些重要变量 (包括构念、因素等) 引入到模型中.假设自变量集合为X'={x1, x2, …, xm, xm+1, …, xn}, 其中x1, x2, …, xm为传统建模变量, xm+1, …, xn为通过数据驱动方法新引入的变量 (多为富媒体形态) .如果没有变量引入 (n=m) , 传统模型的变量关系是Y=f (X) , X={x1, x2, …, xm}.在跨界关联情境下 (n\u0026gt;m) , 将形成新变量关系Y'=f' (X') .换句话说, Y=f (X) 可以是Y'=f' (X') 的特例;一般意义上讲, X'≠X, f'≠f, Y'≠Y.显然, 新变量关系的构建面临着深刻的挑战, 既有新变量空间的发现, 又有新视角的洞察, 也有新变量关系的辨识和新理论的生成.当然, 对于研究和应用来讲, 这些挑战同时也是创新的机遇.例如, 在金融领域, 可以考虑引入搜索平台上的股票关注数据变量以及社交媒体平台上的相关公共事件数据变量等, 以构建新型股价预测模型;在商务领域, 可以考虑引入购物平台上的评论数据变量以及朋友圈中的体验和口碑数据变量等, 以构建新型商品营销模型;在医疗健康领域, 可以考虑引入院外病友智能检测终端数据变量以及区域环境诱因数据变量等, 以构建新型呼吸疾病预防诊疗模型;在公共管理领域, 可以考虑引入社交平台上的受众意见数据变量以及相关领域联动影响数据变量等, 以构建新型公共政策模型.\n2.2 技术增强 对于传统模型来讲, 通过外部嵌入而引入的变量多为富媒体、潜隐性、不可测或不可获, 通常需要利用数据驱动方法和技术.可以说, 数据和技术意识及其能力是大数据背景下研究和应用的核心竞争力, 也是大数据驱动范式的关键要素.技术增强旨在提升这样的能力与要素水平.\n从大数据的“用”与“造”视角出发, 技术增强具有两方面含义.一方面, “用”的视角要求管理模型驱动的研究和应用能够增强对外部大数据的敏感性, 引入外部变量并构建其关系;同时, 能够增强对大数据分析技术的敏感性, 构建方法和工具的获取和使用能力.研究和应用创新通常体现在通过新型范式开发新的变量关系, 进而形成新的管理学模型和应用 (如面向管理问题的新型行为模型或计量模型) , 以获得更深入和更具解释力的管理决策洞见和策略.\n另一方面, “造”的视角要求数据驱动的研究和应用能够增强对于管理决策问题的敏感性, 构建面向管理决策问题的方法和技术.研究和应用创新通常体现在根据管理决策问题特点及其数据属性开发相关性质、测度和策略, 以获得新颖有效的算法和解决方案.值得指出的是, 这里许多算法 (特别是启发式算法和近似解法) 需要经过实验数据的验证以评估其效率和效果.\n多年来, 不管是“用”的视角还是“造”的视角在数据的使用标准上也经历了一个不断升级的过程, 从模拟数据到标杆数据, 再到相当规模的实际数据, 形成一个逐步丰富和叠加的验证实践.在大数据情境下, 实际数据的规模化得到了进一步强化.此外, 在算法比较中, 更关注算法带来的实用效果提升的显著性, 特别在涉及相关用户的场景中, 通常需要进行用户行为实验及其效果感知评测.\n在数据类型方面, 富媒体形态 (如文本、图像、音频、视频等) 成为主流.其中, 音频数据、视频数据具有时间连续性特点.由于计算机中通常采用编码、采样等方式表示富媒体数据, 因而数据变换成为大数据分析的重要内容.常用的数据变换方法包括文本处理的向量空间模型 (VSM) [23]、主题模型 (topic model) [24], 图像处理的尺度不变特征转换 (SIFT) [25], 音频处理的短时傅里叶变换 (STFT) [26], 视频处理的时空兴趣点检测 (STIP) [27]等方法.近年来, 随着大数据平台化运算能力的显著提升, 基于深度神经网络的相关方法进一步发展, 并在富媒体数据变换上展现出良好的应用效果和发展前景.例如, 用于文本数据的单词嵌入 (word embedding) [28], 用于图像数据的卷积神经网络 (CNN) [29]和胶囊神经网络 (capsnet) [30], 用于音视频等具有时间序列特征数据的循环神经网络 (RNN) [31]、长短时记忆神经网络 (LSTM) [32]等.其他较新的数据变换方法还包括多层感知机 (MLP) 、自学习编码器 (AE) 、受限制玻尔兹曼机 (RBM) 、深度语义相似模型 (DSSM) 、神经自回归分布估计 (NADE) 、生成对抗网络 (GAN) 等[33,34].\n2.3 使能创新 大数据驱动的一个重要含义是大数据使能 (enabling) .大数据能力主要包括大数据战略、大数据基础设施、大数据分析 (6) 方法与技术等.大数据使能是指大数据能力带动的价值创造.例如, 从研究和应用范式角度看, 外部嵌入是一种使能情形, Y'=f' (X') 中, 大数据能力通过自变量X'体现, 创造的价值通过因变量Y'体现, 使能转换方式通过f'体现.从研究和应用情境角度看, 企业的价值创造可以体现在其价值链的环节上, 既包括价值链的主环节及其活动, 也包括价值链的支持环节及其活动[35].在企业内外部大数据环境下, 企业使能创新是通过构建大数据能力, 带动新洞察、新模式、新机会的发现, 进而推动产品/服务创新和商业模式创新, 以实现企业的价值创造 (如图4所示) .\n 综上所述, 大数据驱动范式通过技术增强引入了新视角, 进而推动了新型变量关系、要素机理和理论模型构建, 并提升了大数据使能创新的价值创造.这对于应对新型商务形态的进一步发展机遇和挑战具有重要意义.简单说来, 新型商务可以通过两个阶段予以描述.第一个阶段称作数据商务 (digital business或data-centric business) , 即“数据化+商务分析 (BA) ”.此时通过细化数据粒度使得商务要素的“像素”显著提升, 并在此基础上进行商务分析, 针对不同管理场景和层次进行“成像”和决策.第二个阶段称作算法商务 (algorithmic business) , 即“商务分析+”.此时, 在已有的商务高像素基础上, 成像算法成为关注重点, 旨在获得面对新模式、新业态、新人群[3]的发展策略和竞争优势.这里, “商务分析+”包括BA算法创新和BA使能创新.\n近年来, 人工智能 (artificial intelligence, AI) 的研究和应用得到了快速发展, 并受到各界的广泛重视.人工智能自二十世纪50年代以来的发展起起伏伏[36], 虽然在相关思想、模型和方法等方面取得了许多重要进展和成果, 但是由于常常受限于数据基础以及计算能力的不足, 其学习、进化以及推理等方面的能力难以得到发挥, 应用效果也受到影响.直至进入大数据时代, 人工智能的许多成果得到了工程化和产品化实现, 开始在深度和广度上渗透到社会经济活动中, 并引发人们对于未来产业和人类生存的遐想和担忧.机器人和智能产品早期用于替代人类简单重复体力性工作, 现在则可以开始尝试用于替代不少复杂并具有智力的工作, 诸如围棋[37]、翻译[38]、绘画[39,40]、作曲[40]、作诗[41]、无人驾驶[42]、人脸识别[42]、意念控制[43,44]等等.人工智能在管理领域的应用也初见端倪, 比如财务机器人[45]、自动金融交易[42,45]、竞争智能[46]、客户服务[45,47]、人力资源管理[48]、市场营销[42,45]等等.毫无疑问, 人工智能将在新型商务中发挥着越来越重要的角色.另一方面, 伴随着从弱人工智能到强人工智能乃至超人工智能的进阶, 人们对于人工智能应用在隐私和伦理方面的担忧也在不断加重[49].此外, 人工智能理论和技术发展也面临众多挑战 (如“黑盒子”特点、学习机理、语义理解等) , 这些对于强调“关联+因果”的管理决策领域尤为重要.\n最后, 管理学是一门融合了“科学”与“艺术”的学科.在大数据背景下, “科学”层面的可测性、程式化和可重复性等要素正在越来越多地被数据和算法表达;而“艺术”层面的情感、心理以及认知等要素也开始被不断“量化”, 包括借助一些感知技术 (affective technologies) (如眼动、脑电技术等) .未来的管理学在探究组织内外“任务”与“人”有机结合的过程中, 数据驱动特征将愈加凸显, 相关范式转变也将进一步深化.\n全景式PAGE框架 全景式PAGE框架是融合大数据特征和重要研究方向的要素矩阵, 旨在刻画大数据驱动的“全景式”管理决策框架.全景式PAGE框架具有三个要件:大数据问题特征、PAGE内核、领域情境 (如图5所示) .大数据问题特征涵盖粒度缩放、跨界关联和全局视图, 并作为管理决策背景下的特征视角映射到研究内容方向上.PAGE内核是指四个研究方向, 即理论范式 (paradigm) 、分析技术 (analytics) 、资源治理 (governance) 以及使能创新 (enabling) .领域情境是指针对具体行业/领域 (如商务、金融、医疗健康和公共管理等) 进行集成升华.\n 围绕PAGE内核, 在大数据问题特征映射下可以形成一个4×3的要素矩阵.在理论范式 (P) 研究方向上, 重点关注管理决策范式转变机理与理论.传统的管理决策正在从以管理流程为主的线性范式逐渐向以数据为中心的新型扁平化互动范式转变, 管理决策中各参与方的角色和相关信息流向更趋于多元和交互.概括说来, 新型管理决策范式呈现出大数据驱动的全景式特点.进而, 由于全景式的多维交互动态性以及全要素参与特点, 在研究上需要采用新型的研究范式 (即大数据驱动范式) .具体说来, 在粒度缩放方面, 需要决策要素在宏观和微观层面可测可获;在跨界关联方面, 需要引入外部要素并形成内外要素互动;在全局视图方面, 需要多维整合并能够针对不同决策环境进行情境映现和评估.\n在分析技术 (A) 研究方向上, 重点关注管理决策问题导向的大数据分析方法和支撑技术.在粒度缩放方面, 需要数据的感知与采集, 并能够在不同维度和层次上进行分解与聚合;在跨界关联方面, 需要捕捉数据关系及其动态变化, 并能够进行针对多源异构的内外数据融合;在全局视图方面, 需要体系构建和平台计算能力, 并能够形成各类画像以及开展智能应用.\n在资源治理 (G) 研究方向上, 重点关注大数据资源治理机制设计与协同管理.在粒度缩放方面, 需要进行资源要素的数据化, 并明确数据标准和权属;在跨界关联方面, 需要刻画资源流通的契约关系, 并形成有效协调共享模式;在全局视图方面, 需要建立资源管理机制, 并制定组织的资源战略.\n在使能创新 (E) 研究方向上, 重点关注大数据使能的价值创造与模式创新.在粒度缩放方面, 需要提升业务价值环节的像素, 并把握业务状态;在跨界关联方面, 需要梳理业务逻辑和联系, 并辨识影响业务状态的因果关系;在全局视图方面, 需要提升大数据使能创新能力, 并促进组织发展与价值创造.\n围绕领域情境, 可以对PAGE相关研究和应用进行凝练、整合和升华.以NSFC大数据重大研究计划集成平台构建为例, 一般来讲, 集成平台由三个部件组成, 分别是平台体系、内置部件、整合部件.作为简化示例, 对于商务领域集成平台, 平台体系由一个商务管理决策相关的数据池, 以及相应的数据管理和应用管理平台系统 (包括模型、方法、工具库) 等组成;内置部件由针对特定行业 (如汽车) 和特定领域 (如营销) 的研究成果及示范系统组成;整合部件由商务领域内 (不限于内置部件领域) 其它相关项目成果在平台体系框架下经过提炼升华汇集而成.对于金融领域集成平台, 平台体系由一个金融监测预警服务平台, 以及相应的数据管理和应用管理平台系统 (包括模型、方法、工具库) 等组成;内置部件由针对特定行业 (如互联网金融) 和特定领域 (如征信评估、风险预警等) 的研究成果及示范系统组成;整合部件由金融领域内 (不限于内置部件领域) 其它相关项目成果在平台体系框架下经过提炼升华汇集而成.\n结束语 面向管理决策研究和应用的大数据驱动范式通过技术增强引入了新视角, 进而推动了新型变量关系、要素机理和理论模型构建, 并提升了大数据使能创新的价值创造.这对于应对新型商务形态的进一步机遇和挑战具有重要意义.此外, 全景式PAGE框架刻画了在粒度缩放、跨界关联和全局视图特征视角映射下的理论范式、分析技术、资源治理、使能创新等重要研究方向.\n附注:国家自然科学基金委员会“大数据驱动的管理与决策研究”重大研究计划是一个具有统一目标的项目集群, 旨在充分发挥管理、信息、数理、医学等多学科交叉合作研究的优势, 以全景式PAGE框架作为总体思路框架, 坚持“有限目标、稳定支持、集成升华、跨越发展”的原则, 围绕学科领域趋势、理论应用特点, 注重基础性、前瞻性和交叉性研究创新.自2015年底至2017年底, 此重大研究计划部署了包括培育项目、重点项目和集成项目等一系列项目.其后续的项目部署将在全景式PAGE框架下, 进一步突出凝练、整合与升华, 强调与总体思路框架内容的契合性和贡献度.\n本文素材部分来自国家自然科学基金委“大数据驱动的管理与决策研究”重大研究计划相关的系列研讨.由衷感谢不同学科领域专家学者 (包括NSFC大数据重大研究计划指导专家组、顾问专家组、管理工作组等专家学者) 的真知灼见和思想贡献!\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/management_challenge_in_big_data_era/","summary":"摘 要：\n在数字化生活背景下, 传统的管理变成或正在变成数据的管理, 传统的决策变成或正在变成基于数据分析的决策.从大数据的数据特征、问题特征和管理决策特征出发, 讨论管理决策研究和应用的范式转变.大数据驱动范式可以从外部嵌入、技术增强和使能创新三个角度来审视, 并体现出“数据驱动+模型驱动”的“关联+因果”含义.此外, 围绕大数据特征和重要研究方向, 阐述了全景式PAGE框架及其要素.\n关键词： 大数据 ； 管理决策 ； 研究范式 ； 全景式 PAGE 框架\n信息科技的飞速发展和深度融合开启了数字化生活的新篇章, 把人们带入了大数据 (big data) 时代.一方面, 随着各种感应探测技术、智能终端以及移动互联的广泛应用, 使得社会经济生活的方方面面以更细粒度的数据形式呈现, 进而整个社会的“像素”得到显著提升;另一方面, 社会“像素”的提升促进了数字“成像”的发展, 使得通过数据世界可以更清晰地描绘社会经济活动情境, 进而基于数据的商务分析 (business analytics, BA) 正在成为使能创新的核心竞争力.在此背景下, 传统的管理变成或正在变成数据的管理, 传统的决策变成或正在变成基于数据分析的决策.\n近年来, 大数据成为学界、政界和业界持续关注的热点.在学术界, 早在2008年和2011年, 《Nature》与《Science》杂志分别从互联网技术、互联网经济学、超级计算、环境科学以及生物医药等多方面讨论大数据的处理与应用45此后, 大数据在各个学科领域包括医学、经济学、管理学以及公共管理等领域得到了广泛的探讨与研究6789同时, 大数据也引起世界各国高度重视, 美国、欧盟、澳大利亚以及日本等国部署了一系列大数据相关战略和关键领域。在产业界, 国内外大批知名企业掀起了技术产业创新浪潮, 通过收购与合作构建和提升大数据技术与应用能力, 布局和开拓相关的业态和市场.\n我国政府对大数据高度重视并有一系列前瞻性洞见和部署.2015年十八届五中全会提出实施国家大数据战略 , 国务院发布《促进大数据发展行动纲要》 , 指出大数据是国家基础性战略资源, 旨在全面推进我国大数据发展和应用, 加快建设数据强国.2017年十九大报告进一步强调要推动互联网、大数据、人工智能和实体经济深度融合.通过国家需求、政策支持、产业结合以及企业研发等形式, 近些年来涌现出一大批重大规划和政产学研项目, 包括国家自然科学基金委员会 (NSFC) 于2015年9月启动的“大数据驱动的管理与决策研究”重大研究计划 (简称NSFC大数据重大研究计划, 参见附注) .\n大数据在给社会经济生活带来深刻变革的同时, 也对管理与决策研究带来一系列新的重要课题.从信息技术 (IT) 范畴来看, 可以从两个视角来认识大数据, 即大数据的“造”与“用”视角 (如图1所示) .这和产品的属性类似, 一方面, 人们关心产品是如何设计和制造出来的;另一方面, 人们关心产品是如何使用和有用的.","title":"转载 | 管理决策情境下大数据驱动的研究和应用挑战"},{"content":"plotnine https://github.com/has2k1/plotnine\nPython版的ggplot2可视化包\n{{ \u0026lt; figure src=\u0026ldquo;img/plotnine.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nGeoPy https://github.com/geopy/geopy\n{{ \u0026lt; figure src=\u0026ldquo;img/GeoPy.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\n地理数据计算包\nZettlr https://www.zettlr.com/\n科研助手/笔记软件：一款功能全面而出色的笔记/学术写作软件Zettlr\n{{ \u0026lt; figure src=\u0026ldquo;img/Zettlr.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nBeeware https://beeware.org/\n厉害了，BeeWare 帮助您编写跨平台本机 GUI 应用程序。\n{{ \u0026lt; figure src=\u0026ldquo;img/beeware.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nbig_screen https://github.com/hiDaDeng/big_screen 数据大屏生成工具，后端使用 flask。只要传一下数据，就可以制作出数据大屏，适合不了解前端的数据工作者。 {{ \u0026lt; figure src=\u0026ldquo;img/corp.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} screely https://www.screely.com/\nmimestream https://mimestream.com/\n前苹果邮件客户端工程师 Neil Jhaveri 推出的 Gmail macOS 原生客户端，外观简洁大方，十分契合原生风格，参考介绍文章。\n{{ \u0026lt; figure src=\u0026ldquo;img/mimestream.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\n##了解课程\n 点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly05/","summary":"plotnine https://github.com/has2k1/plotnine\nPython版的ggplot2可视化包\n{{ \u0026lt; figure src=\u0026ldquo;img/plotnine.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nGeoPy https://github.com/geopy/geopy\n{{ \u0026lt; figure src=\u0026ldquo;img/GeoPy.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\n地理数据计算包\nZettlr https://www.zettlr.com/\n科研助手/笔记软件：一款功能全面而出色的笔记/学术写作软件Zettlr\n{{ \u0026lt; figure src=\u0026ldquo;img/Zettlr.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nBeeware https://beeware.org/\n厉害了，BeeWare 帮助您编写跨平台本机 GUI 应用程序。\n{{ \u0026lt; figure src=\u0026ldquo;img/beeware.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nbig_screen https://github.com/hiDaDeng/big_screen 数据大屏生成工具，后端使用 flask。只要传一下数据，就可以制作出数据大屏，适合不了解前端的数据工作者。 {{ \u0026lt; figure src=\u0026ldquo;img/corp.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} screely https://www.screely.com/\nmimestream https://mimestream.com/\n前苹果邮件客户端工程师 Neil Jhaveri 推出的 Gmail macOS 原生客户端，外观简洁大方，十分契合原生风格，参考介绍文章。\n{{ \u0026lt; figure src=\u0026ldquo;img/mimestream.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\n##了解课程\n 点击上方图片购买课程   点击进入详情页","title":"TechWeekly-05 | 每周有趣有用的技术分享"},{"content":"DaDengAndHisPython博客改版上线了\n博客网站改版了，将之前黑暗风改为极简风，内容相对整洁，无广告，会不定期更新Python数据分析内容。\n博客有主页、推文列表栏、搜索栏、标签栏、联系页等。\n欢迎页  推文列表页  搜索页  标签页   点击左下角”阅读原文“进入博客网站\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/papermod/","summary":"DaDengAndHisPython博客改版上线了\n博客网站改版了，将之前黑暗风改为极简风，内容相对整洁，无广告，会不定期更新Python数据分析内容。\n博客有主页、推文列表栏、搜索栏、标签栏、联系页等。\n欢迎页  推文列表页  搜索页  标签页   点击左下角”阅读原文“进入博客网站\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"高质量的Newsletter汇总"},{"content":" 本文原载《法学家》2021年第6期。\n作者 | 周翔（法学博士，浙江大学光华法学院特聘副研究员）\n来源 |《法学家》2021年第6期“主题研讨二：跨学科法学研究的方法论检视”栏目。\n 因篇幅较长，已略去原文注释。\n目录  一、前大数据时代的法律实证研究 二、大数据技术运用的各个节点 三、大数据技术对于实证研究而言是一场接力 四、大数据技术对于规范研究而言是一种助力 结　语  　中国法学正在迎来“大数据”“人工智能”的研究热潮。“数字法学”“计算法学”等各类新词汇层出不穷，但研究者们却并不都是在同一内涵和外延下使用这些概念。因此，本文有必要在开篇之初先设置一套分类法，对既有的学术研究成果加以归类，从而明确本文在既有研究中的坐标位置。笔者将既有的相关研究分为如下四大类：\n第一类称作“学科论”，此类研究看待技术的视角最广，其目标是希望厘清法学+数字技术的最大学科边界；\n第二类称作“对象论”，是把“大数据”“人工智能”等视为法律规制和法学研究的对象，此类文章占了当前本领域研究成果中的大多数；\n第三类称作“工程论”，这类研究关注到数字技术可以被引入执法、司法等各个场景当中，赋能法治的各环节；\n第四类才是“方法论”，是从学术研究方法的视角看待大数据技术，探讨其能为学术活动提供哪些新契机。本文的研究侧重于第四类，亦即集中讨论大数据分析技术作为学术活动的工具，能够给法学研究提供何种新方法。\n　本文旨在回答大数据技术如何才能成为法学研究的方法，如何与法律实证研究、规范研究这两大传统的方法实现有效的互动。关于此，左卫民在《迈向大数据法律研究》一文（以下简称为“左文”）中较早地提出了“大数据技术如何作为法学研究方法”这一命题，在“方法论”层面为将大数据技术引入法学研究当中起到了重要的推动作用。同时，笔者认为，以下方面仍值得进一步探讨：\n第一，“左文”中提到“从研究范式看，大数据法律研究可能推动实证研究的跨越式发展，特别是机器学习方式的引入，会使法学研究从法教义学、社科法学和实证法律研究等范式转向数据科学式的法学研究”。“范式”一词在托马斯·库恩（Thomas Kuhn）那里，是指“一个成熟的科学共同体在某段时间内所认可的研究方法、问题领域和解题标准的源头活水”。形成一个范式，是任何一个学科在发展中达到成熟的标志。“数据科学式”的法学研究已经是一种成熟的范式了么？它与“左文”中提到的法教义学、社科法学等既有的法学研究范式之间又是什么样的关系？\n第二，“左文”中提到“需要将小数据社科研究中已普遍运用和相对成熟的数据分析方法……运用到大数据分析中”。社会科学中开展定量研究，是以统计学原理为根基的，这和大数据技术联系密切的机器学习方法之间有何差异？法学研究又能够吸取大数据技术中的哪些优势？\n第三，左文中还提到“一些大数据法律研究缺乏必要的问题意识，主要是描述式研究，沦为‘调查报告式’的数据展示”。这涉及的是大数据分析技术应用现状的问题。\n上述三个方面，围绕大数据技术对法学研究的主要贡献展开，清晰地定位了大数据技术在既有的法学研究方法体系中的地位。\n　上述延伸和思考，一方面是对话，另一方面是形成本文研究的路径。本文的基本立足点在于：大数据分析技术如果对法学研究有影响的话，那么主要是使得法学更加社会科学化、更重视实证的方法、更习惯从大数据中探索法律世界的规律。这些贡献决定了大数据技术在法学研究方法体系中的位置，其既是对以统计学为基础的法律实证研究的接力，更是对以法教义学、立法论研究为核心的传统规范研究的助力。在这一基本立场之下，本文首先对前大数据时代的实证研究方法、研究对象作一番回顾；接着结合笔者担任大数据分析师的经历，针对技术细节作梳理；然后在这些基础之上，就如何实现大数据技术、定量的实证研究、规范的法教义学研究三者间的互动提出一套初步的方案；最后，鉴于当前学界在相关概念上存在较多的混用现象，本文就此作一些观点上的澄清。\n一、前大数据时代的法律实证研究 　关于什么是实证研究，目前虽然尚无统一定论，但在“基于模型和数据的方法”这一点上则有比较明确的共识。包括法学在内的各个社会科学学科之所以都对定量方法感兴趣，是因为“定量的方法，乃一切科学进程的核心”。只要是跨越个案地探寻案件平均水平、共性特征、要素相关性的研究，都将被本文纳入前大数据时代法律实证研究的范畴。下文对此类法律实证研究的回顾和梳理，主要是从研究的方法、测量的工具、样本量的大小等三个维度展开。\n（一）以统计为主的研究方法 　从研究方法来看，既有的法律实证研究均奉统计学为同宗。通常认为，统计学的方法有描述性统计和相关性分析两大类。结合法学研究的特点，这里稍作更细致的划分。根据笔者的概括，以往的研究成果主要利用了如下三种方法。\n　方法一：多案例分析。对案例的运用，如果不是针对法条进行解释、对立法提出建议，那么在笔者看来即为一种实证研究的路径。例如陈杭平关于民事诉讼标的的研究，为案件类型化设定了一套分类标准，即诉讼标的的不同含义，纵轴根据不同学说见解区分为三个版本，横轴以诉讼标的的不同领域或场景为标准。从多个案例中挖掘某些规律性信息，是一种超越个案、试图通过案件类型化获得研究结论的方法尝试。\n　方法二：描述性统计。前述的多案例分析，还很难称得上是严格意义上的实证分析。左卫民的一系列文章有力地推动了法律实证研究向前发展，其主要采用的便是描述性统计方法。例如，他关于审判委员会的研究，统计了某地区的三级法院审判委员会委员的审判经验与学历背景，并将讨论的议题细化到宏观指导议题和个案议题，分别进行统计和分析；再比如，他另一份关于中国刑事法律援助的研究，通过调研和考察某省的三家法院，统计了各类型辩护的人数和占比，并由此回答“究竟应当在多大范围内推行并构建当代中国的法律援助制度”这一问题。\n　方法三：相关性分析。白建军等人的研究则在描述性统计之基础上，迈向了相关性分析这一相对复杂的层次。相关性研究也被称为推论统计，是将统计学手法与概率理论相融合，对“‘无法整体把握的大的对象’或‘还未发生而未来会发生的事情’进行推测”。白建军等人开始关注引起某一现象的原因，试图建立自变量和因变量之间统计学意义上的相关性，所采用的回归模型主要为多元线性回归、logistics回归等常见模型。例如，白建军迄今为止在中国知网上下载量最高的一篇论文，是通过相关系数、多元线性回归来研究犯罪率的社会成因；又如，李本森关于速裁程序的研究，则以诉讼效率、量刑均衡和诉讼权利作为其关心的因变量，采用的是多元线性回归模型。\n　在线性回归、logistics回归这两大常用的回归模型之基础上，法律实证研究方法也根据议题之需推陈出新。首先表现为统计方法趋于高级，例如白建军在其一贯的相关性分析之基础上，对无法观察的变量作了科学处理，将中国民众的刑法偏好这一因变量，拆解为犯罪圈大小、刑罚轻重、罪刑均衡程度等三个可通过问卷调查测量的因子，以打分取值的形式获得该变量的数值；其次是结果呈现方法上的创新，例如单勇关于盗窃罪的回归分析结果，用GIS作图的方法实现可视化，选取建筑物、停车场等10类空间因素为自变量，用于说明因变量和空间变量的地理联系；最后是体现在用于分析的软件工具之更新上，过去的法律实证研究以使用SPSS软件为多，而近来的研究很多提及使用了Stata、R等工具。当然，使用更高级的软件进行分析，其效果未必就一定更好，但上述变化至少标志着学者们在工具利用方面的水平提升，且有些回归模型是初阶工具所无法胜任的。\n（二）多元测量工具并存 　测量工具是指获得数据的方法。数据搜集在统计学中是重要的一环，“所有统计数据追踪其初始来源，都是来自调查或实验”。统计学上根据是否为直接获得第一手数据，区分直接来源和间接来源。法学实证研究多以一手的直接来源为主，主要的测量工具包括如下几种。\n　工具一：问卷调查。这种测量工具与传统的社会科学方法保持一致，通过设置问题、受调查者回答的方式收集受访者的信息。此种方法在获取受访者主观态度方面效果明显，为学界所常用。例如，程金华在研究过程中为了解检察人员针对检察人员分类改革的认识而发放问卷；胡铭关于司法公信力的研究，通过向社会公众和司法官分别发放问卷，比较和审视“对于影响司法公信力的要素的认知与评判”。\n　工具二：模拟实验。“实验大多是对自然现象而言的”，但在近年来的法学研究中也时常出现。司法裁判一般不具有可再现性，同一个案件在真实场景下只可能出现一次。模拟实验是一种对司法裁判过程的模拟再现，借此可发现一些影响裁判结果的变量。目前中国法学界的模拟实验主要是对一些经典案例裁判过程的复盘，以验证某些观点。例如李学尧等人关于案卷材料阅读流畅度与裁判尺度之关系的研究，通过问卷字体大小、是否斜体、是否加重、行间距以及案件数量的变化来操纵阅读流畅度的变化。\n　工具三：文本摘录。法律中的文本，其典型形态为裁判文书。在裁判文书大量公开上网后，有人认为数据法学的春天即将到来。的确，裁判文书是记录诉讼过程最终的、有法律效力的、体系最完整的文本。对裁判文书的利用，比如文姬关于信用卡诈骗罪的研究当中有很多维度的信息挖掘，包括审级、行为人出生年等16个变量。文本的种类近年来也出现不少创新，比如习超等人关于证券监管的研究采用的是对上市公司执法事件的披露信息。\n　工具四：实地/田野调查。倘若只是单纯采取个案式的访谈记录，则在方法论上一般将之归入定性研究的范畴。但如果是在田野调查中对多个样本进行观察或访谈，采取结构化的方式收集数据，最终对数据进行跨越个案的量化分析，那么也可以被视为实证研究的测量工具之一。此种方式在国内法学研究中不乏实例，比如一份关于当事人法律意识的研究，所主要利用的素材就是在某法院传达室对一百多位当事人进行访谈所收集的数据；再比如左卫民在研究基层法院的财政制度、法官的工作时间分配等问题时，课题组前往实地调研、观察记录收集数据资料。\n　除了采用上述工具之一，实证研究还可以多种测量工具结合、定量和定性方法混用。比如胡铭关于庭审实质化的研究就不仅利用了判决文书，且还通过观摩庭审直播并记录的方式收集数据。\n（三）万级以下的样本量 　实证研究的论文中约定俗成要报告样本量，而之所以特别指出研究所用的样本量大小，是由于样本量直接关系到根据小样本得出的结论能否推及至更大的范围，因此抽样是统计学中很重要的概念。建立一个好样本的关键，是尽量选择最符合总体的样本，如果样本具有代表性，那么表明样本与总体有十分相似的特性，进而可以通过样本预测出总体具有哪种规律。\n　法律实证研究中的样本量过去以百级、千级为主，比如文姬关于信用卡诈骗罪的研究所利用的裁判文书样本有2103份，习超等人对证券监管“旋转门”的研究则采用了7103个监管事件作为样本。样本量大小是个相对的概念，还要考虑“全体”的规模大小。当我们确定研究的问题后，从理论上讲“全体”的范围就固定了。若以裁判文书为测量工具，则有三个层次的案件范围，依次分别是客观真实发生的案件总数、裁判文书网上公开的案件数、用于实际研究的案件样本数。前大数据时代的法律实证研究，是在公开的裁判文书范围内选取一定的样本进行分析，距公开的案件“全体”和真实客观的案件“全体”相去甚远，正因如此，不少研究受到样本代表性不足的质疑。\n　除了抽样和样本的代表性问题外，前述提及的那些研究基本没有交待如何实现样本的数据结构化。根据笔者自身的数据分析经验，以传统方式处理样本耗时巨大。如果以阅读裁判文书并摘取的形式处理样本，那么一天工作8小时也只能阅读20—40份裁判文书，如此来算，处理千量级的裁判文书样本就得花费数月时间。如果再安排专人抽查数据录入的质量，那么工时还得另计。\n（四）留待提升的不足 　在研究方法上，相关性分析方法之后难有新的突破；在分析工具上，研究者虽试图推陈出新，但总体变化不大。以构建回归模型做研究为例，中国法学界目前用过的回归算法种类屈指可数。某些研究虽然其方法有一定的创新，比如采用决策树的方法，但又和机器学习的决策树算法相去较远。另一个问题在于分析软件，小样本时代没有使用分析软件的明显障碍，但在大样本时代则要考虑借助的分析工具是否恰当，能否高效运行。上述两个方面，大数据技术都可能给其带来变革。\n　测量工具上，以往较强依赖于社会资源的协调能力。中国法学界当前使用的测量工具中，问卷调查、模拟实验的应用较为普遍，而上述二法的共同局限在于严重依赖研究者的社会资源协调能力。很多研究并不避讳利用了作者的挂职身份、承担横向课题的机会、本省司法资源的便利条件等。就数据获取而言，即使只是选择几个投放点，也要付出很大的成本，且若没有较大经费支持则难以做到。上述列举的那些研究成果，因此往往是知名学者的作品。这也造就了一个怪圈：是先成名后做实证研究，还是因实证研究而成名？而在大数据时代，研究者将在一定程度上不再依靠外部资源的协调能力。\n　以往样本量太小，使得据其发现的规律的代表性不足。前文已经提及，样本量关乎结论的普遍性，统计分析的根本目标在于“推论”。样本量越小，对抽样的随机性要求就越高，而抽样始终是一个难题。造成抽样偏差的成因很复杂，比如抽样空间条目不齐全、抽样单位不正确等，无法穷举。如何克服抽样的难题？换个角度试想一下，研究的样本如果就是公开的“全体”，那么抽样的重要性将被极大淡化，而基于公开的全体案例作分析，在大数据技术的赋能下是能够实现的。\n二、大数据技术运用的各个节点 　从实证研究的过程来看，在选择议题、提出假设和设计变量等步骤中，数据的收集和分析是大数据技术最相关的两个环节。其中，数据的收集又包括语料的获取、语料转为数据和数据清洗等步骤。本文认为，大数据分析技术正是借助数据的收集和分析这两点，助力法律实证研究向更高阶段发展。从已有大数据分析的实践经验来看，可大致分为以下几个步骤。\n（一）语料的获取 　语料获取是应用大数据技术的第一个环节。凡是对立法活动、司法实践有所记录的载体，都可作为实证研究的原始语料。当然，文本仍是当前法律大数据分析主要的语料类型，大数据技术目前还比较难以有效处理图片、音视频等数据。所幸的是，法律文书本来就是记录法律活动最重要的、也是表达相对准确的语言形式。\n　当前的语料获取问题，应重点聚焦于如何便利地获取法律类文书。研究者作为个体要想获得供研究之用的文本，除逐一复制或下载外，还有两条路径值得重视：一是采取网络爬虫的方式，从数据源自动爬取，业内有句话叫作“可见即可得”，其意思是只要有该网站的访问权限，那么便可获得该数据，包括一般性的网页、API资源、文件资源和媒体资源；二是与拥有法律文书数据的公司进行合作，后者一般掌握较为完整的法律文书库。\n　网络上的其他数据资源也不可被忽视。在传统的法学研究中，我们便已看到许多研究者用到法律年鉴、地方志等信息，而此类信息如今已基本实现无纸化、网络化。我们可从以下几个渠道收集与自己研究有关的语料：一是国家及地方各公立机构的官方网站，比如图书馆、档案馆、财政局等行政事业单位；二是各行业的商业机构网站，比如上市公司财务报表的披露网站、各行业的商业情报网站等；三是一些人气活跃的社区论坛，比如在司法公信力、媒体和司法之关系等研究议题中，我们关心的案件舆情就在论坛社区中有丰富的表达。网络资源可有效弥补裁判文书这一测量工具的局限性，因为法治事件的真实场景变量复杂，法律文书只能反映其中的一小部分。\n（二）语料转为数据 　传统的实证研究是统计学思维，处理的是阿拉伯数字。这一点在大数据的语境下没有本质变化，即机器学习仍然难以根据文本直接构建模型，数据分析的对象仍是结构化数据。遗憾的是，法律领域的语料主要是自然语言，以数字形式呈现、直接可用的信息少之又少。因此，在获取与我们研究议题相关的文本语料后，还将面临如何将语料转为结构化数据的问题。前述提及的那些实证研究成果也用到文本，并主要采取人工摘录的方式进行处理，而大数据获取技术中的文本挖掘技术，通过计算机就可实现数据结构化。\n　将语料转为数据的过程，本质上是将自然语言转化为机器语言。处理自然语言的工具，大致可分为基于概率和基于规则两类。前者是通过人工标注一定的语料，再由机器模型识别剩余部分的语料，得到的是一个结果的分布概率；后者的典型代表是正则表达式，因其结果更为准确，故而成为当前适用广泛的提取方法。学术研究要求较高的准确性，因此基于规则的方法更为现实可取。正则表达式，在技术书中被定义为“一门袖珍编程语言的通用模式表示法，赋予使用者描述和分析文本的能力”，这里也可将其通俗地理解为高级版的关键词检索。正则表达式可将我们关心的、希望提取的某一要素，通过计算机能理解的方式表达出来。不过其具有的缺陷是，如果编写者未能预见同一意思下全部的汉语表达方式，那么该正则表达式也将无法识别出全部包含该意思的文书。\n　语言表达方式虽然具有多样性，但也并非无穷无尽，常见的文字表达类型是可以由正则表达式囊括的。实践中的通常做法为：先人工阅读一部分文书，枚举语言表述的类型→通过正则表达出每种类型→正则遍历文书，筛选出被命中的文书→再次阅读部分未经匹配的文书→优化正则表达式→再次遍历剩余未被命中的文书。多次循环后，正则表达式将会得到很大的改进，信息提取的准确性也会随之提高。数据的获取任务至此完成，这也是大数据技术相较于传统实证研究在技术上的巨大变革之处，即数据的获取不再高度依赖于外部资源的多寡，只要研究者掌握了一定的编程能力，那么就可以从最大的数据资源库即互联网中获取自己想要的各类数据。\n（三）数据清洗 　数据清洗面临两种情形，一种是从文本到数据的过程中存在信息的遗漏，另一种是有些文书信息虽然被提取了出来，但存在错别字或其他不当之处。处理信息残缺的方法，至少有以下几种：第一种是根据信息有残缺的文书编号，追溯至该份文书，人工阅读发现原因并修正提取的方法。这和上文提及的不断优化正则、扩大匹配的范围的做法很接近。第二种是统计学中处理残缺值的传统方法，比如用平均值替代、剔除该样本等。第三种是借助Excel表格中的工具、pandas等第三方库对数据逐一进行修正，通过人工的个别修正使数据回归正常。例如裁判文书中存在诸多错别字、语病等错误需要清洗，笔者曾遇到过某一罪名在裁判文书中，有十余种错误的文字表述、八种不同的“零”写法，这些均属于若无人工预判则机器便无法自动识别的情形。\n　数据清洗，主要面对的是如何处理自然语言中不同词汇的相同意思（同义问题），以及一个词汇在不同背景中有不同的意思（多义问题）。在数据清洗的实践中，可以发现存在如下几种规律：（1）词汇类型有限的数据项，需要清洗的脏数据比较少。例如提取裁判文书中的法院审级，一般文书落款中有“中级”“高级”“最高”等关键词，正则很容易匹配成功；而当鉴别机构的名称时，由于全国各地的命名方式不一，便会出现很多数据空缺需要填补的问题。（2）数据的清洗和人工的投入量基本成正比。无论是用人工标签+机器学习的方法，还是用正则表达式提取，都需要人工阅读并发现数据错误，添加惩罚项或修改正则来减少脏数据。（3）数据清洗要适可而止，因为数据清洗需要耗费大量的人工成本。一些简单且明显的错误，比如审判员人数提取为“2人”，能够及时返查并纠正，但人工清洗全部的脏数据是不可能的。现阶段在计算机还没有能力自查和纠错的情况下，学术共同体对待数据获取环节的准确性只能给予更多的包容。\n（四）数据分析 　若要从数据中产生规律性的知识，则还要依赖于数据分析的工具。以统计学思维看待数据分析的方法，主要有描述性分析和相关性分析两大类。\n　大数据时代的分析环节，仍有运用描述性统计的必要。大数据时代虽然样本量呈几何式增长，但试图把握司法实践之一般规律的需求并未改变。而描述性分析是最有利于把握案件整体情况、聚焦重点案件类型的方法。\n　关于相关性分析，大数据技术主要借助于机器学习，“根据训练数据是否拥有标记信息，学习任务可大致划分为两大类：‘监督学习’和‘无监督学习’”。有监督的机器学习，其建模方法为研究者提供了一种新思路，即把样本一分为二，区分训练集和测试集，用训练集拟合参数，用测试集评估数据模型的准确性。机器学习与统计学中的回归建模方法之间最大的一点差异，在于检验模型参数的可靠性上，机器学习采用交叉检验的方法，而统计学上则主要采用假设检验的方法，其典型者如t检验。无监督的机器学习事前不作标记，通过对无标记训练样本的学习，来揭示数据的内在性质及规律。以统计学视角来看，即事前不设置因变量。通过该项技术，可以从中探索我们所关心的研究议题，从而确定因变量。\n　用大数据的机器学习方法建模，最好采取Python语言编写程序。该语言可调用各类第三方库，statsmodels、scikit-learn等第三方库中已内置有大部分的常用算法，调用接口即可满足现有的研究需要。在大数据时代，获取的数据样本量将轻松突破万级，例如研究某些社会高度关注的案件的网络舆情，从微博、论坛中获取的评论数甚至可以很轻松地突破百万条。自己编写程序做大数据分析，在面对大样本时，能更好地满足个性化的研究需求。\n（五）前述流程的局限 　大数据技术并非没有局限性，它是一把双刃剑。“技术不是敌人，我们的敌人是寄居在技术里的浪漫又革命的‘解决问题兽’”。因此，人们要保持独立思考的能力，用批判性的眼光去接受、采用技术。在看待大数据技术在法学研究中的应用前景时，同样应重视可能存在的一些局限性。\n　第一个局限在于，大数据技术更难以关照到个案的细节之处。这也是左文中提到的大数据时代的一个特点，即样本量变大后，做不到人工查看每个样本。此为一个明显的缺陷。大数据分析所反映的只是数据间的相关性，但要解释此种相关性，还要依靠外部其他角度的素材。当回答为什么出现数据间存在显著相关性这一问题时，特别离不开对一些典型个案作具体的剖析。如前所述，大数据技术不再要求人工逐一阅读，便可将文本信息转为数据信息，但是小样本时代，逐一阅读案例，恰恰是发现有价值细节、启发研究灵感的历程。因此，大数据时代的法学研究，在用数据说话的同时，仍然少不了要深入到对典型个案的阅读中。\n　第二个局限在于，计算机技术的使用门槛较高，许多研究者面临着计算机技术有关知识匮乏的挑战。首先，在数据获取上，研究者最好能够掌握一些网络爬虫的技巧。从笔者的实战经验来看，爬取一般网站论坛上的数据相对容易，但爬取微博、微信公众号等数据就比较困难，这是因为后者设置了很多反爬虫的措施。再以法学研究常用的裁判文书为例，中国裁判文书网为确保正常访问，采取了一系列的加密措施，而这意味着研究者根本无法一劳永逸地解决数据获取的问题。其次，当前在研究成果发表时，法学期刊通常并不要求同步公开研究所依据的数据样本，也就是说，同行无法获知所采集的数据库详情、数据清洗的程度等。这是大数据法学研究早期阶段的特点。在大数据法学研究的成熟阶段，各研究者可能反复使用同一批大数据，并有一系列量化的模型衡量指标。\n　第三个局限在于，机器学习所用的部分算法，在变量参数和影响路径的可解释性上，不如那些简单的统计学算法。这部分是由于一些自身原理所造成的，比如机器学习中可能嵌套多层级函数，其目的是提高模型的拟合度。以神经网络的一般模型为例，有（d+l+1）*q+l个参数需确定，d、l、q分别代表输入、输出、隐层的神经元个数，神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权，即参数值。这还只是一个隐层的情形，“容量”越大的深度学习，参数就越复杂，对法学研究而言的可解释性也越弱。如果认为法律实证研究主要是社会科学意义上的追求现象间相关性的分析，那么越是过程复杂的机器学习算法，越不能透过模型发现变量间的关系。\n　综上，笔者对待大数据技术的整体态度是，获取更大规模、更多类型的数据，对外部资源的依赖程度降低，是其最主要的贡献点，同时也要警惕研究过程中脱离个案细节、技术门槛提高、复杂模型的可解释性弱等风险。有效化解上述风险的策略包括：在跨越技术门槛上，可考虑借鉴其他学科团队式研究的模式，吸纳技术人员参与，改变过去一些法学期刊所认为的合署论文便有“搭便车”嫌疑的前见；在克服脱离个案细节这一问题上，则可以多采取混合研究的方法，即定性的方法和定量的方法相结合，实证研究和规范研究相结合；在数据分析时的算法选择上，则应尽可能选择一些原理简单、可解释性强的算法。\n三、大数据技术对于实证研究而言是一场接力 　大数据技术对于实证研究而言有一种接力的价值，两者的共性大于差异。大数据技术主要应定位于加强实证研究的某些环节，但并不改变实证研究基本的方法论框架。本文认为，大数据技术的接力作用，主要体现为：（1）降低了数据获取的难度，作为本文第一节中介绍过的那五种测量工具之外单独的一种数据获取途径，以网络爬虫、文本挖掘为代表的大数据技术，在获取数据上具有时间成本和经济成本更为低廉的优势。（2）加强了描述性统计的能力，适合探索性的量化研究。若对实证研究作描述性分析和相关性分析的二分，则大数据技术更擅长概览式地描述研究对象。（3）拓展了可量化研究的议题，使得某些议题的论证更加充分和有说服力。\n（一）拓新数据获取的重要渠道 　任何定量研究均离不开信度可靠、效度可行的数据来源。对大样本的追求，在统计学上称为“一致性”，费希尔（Stanley Fischer）用数学公式说明了“你得到的数据越多，你计算出的统计量越有可能接近参数真值”。在本文前一节的大数据技术应用详解中，所提及的第一步便是大数据的获取技术，若能掌握大数据的获取技术，或者吸纳有相关技术能力的合作者参与，则将大大拓展数据获取的渠道，互联网将成为一种新的测量工具。本节结合一些国内外较新的关于应用大数据技术的文献，深入探讨互联网这一大数据的来源，以此说明大数据技术在获取数据时的优势。\n　首先，把互联网视为数据获取的来源时，主要是将互联网视为一个“知识库”。互联网沉淀了人类活动的大量数据，其中一些是与法律有关的行为数据，例如裁判文书是对司法过程和结果的一种记录，网友针对某一热点案件的留言是司法民意的表达。这些数据的特点是它们的产生最初并非为了供研究之用，故而只能提供有限的数据项，研究者需迁就网络数据可用的数据维度进行研究设计。利用裁判文书开展大数据分析就十分典型，裁判文书的数据维度并不是为研究所设计的，因此在选题时，就要充分考虑裁判文书中所体现的信息是否足够用于回答该问题、有无其他数据源可作补充。\n　其次，进一步拓宽数据获取的思路，还可以把互联网视为形成数据的“实验室”和“协作平台”。大数据技术可以把互联网作为提问数据和实验数据的来源，即前述实证研究测量工具中的实验方法、问卷和田野等方法也可以在互联网中大规模使用。通过网络发送调查问卷，在学术界目前已经有一些成功的研究案例。例如，在一项针对累犯成因机制的研究中，通过给刑满释放的研究对象发放智能手机，大数据采集平台每天向研究对象发送问卷收集数据，并与定位数据、短信数据等数据源相结合，分析再犯罪的成因机制。借助互联网还可以开展随机对照实验，例如一项在二手交易网络商城开展的实验，通过在商品详情描述中改变卖家手持商品的手背肤色、手臂是否有文身、出价和商品介绍的质量等变量，分析这些变量与商品成交价的相关性，从而实证分析商品交易中存在的种族歧视问题。甚至还可以设计一个研究任务分包的网站，将数据的收集、标注等任务进行拆解，让更多的人参与到结构化数据库的建设中来。例如在一份关于国外政党之竞选政策立场的研究文献中，研究者事先将政党宣言作出类型化的定义，然后在网站上分包，最终从1500名工人处收集到20万条的分类数据，而分类的结果经过事后的验证，与专家分类的结果高度吻合。此类对文本、图片的信息采集和分类工作，如果能够分拆为不需要大量专业训练且答案较明确的任务，那么通过互联网的方式，就可以实现低成本的大数据采集和处理。\n（二）提高实证研究的描述分析能力 　社会科学所立足的成熟的研究范式，目前仍是提出假设、并用定量的统计方法加以验证的过程，这一套研究的基本方法在引入大数据技术后，并没有发生实质性改变。在数据分析的描述性和相关性之二分法中，大数据分析技术主要加强的是描述性部分，相关性分析仍主要沿用实证研究中倚赖的统计学算法。要想用好大数据分析技术，还应注意区分商业利用和学术研究的不同侧重点，商业领域的分析技术，不一定都能直接迁移至法学研究中来。\n　首先，大数据分析技术主要提高了研究对象的整体描述能力。实证研究中的描述性分析，针对研究对象设计变量，统计平均值、方差等。除这些外，大数据分析技术还有其他可供选择的方法，比如通过词频的计算提炼文本的关键词、通过情感分析的技术反映某些文本的正负情感及强度、通过文本摘要的技术浓缩海量文本的内容，分析的结果可以用词云、动图等多样的可视化方法来呈现。这些技术与实证分析中的描述性分析非常接近，只是起到丰富描述分析工具箱的作用。\n　其次，现有以统计学为基础的相关性分析，所用的算法仍将保持主流地位。统计学中最基本的线性回归、对数回归等模型，仍是当前最为成熟、较适合社会科学研究使用的方法。这并非法律实证研究特定阶段的现象，例如对美国在政治学、社会学领域最权威的6本期刊于2001—2010年间发表的实证研究论文所采用的方法进行统计后发现，最小二乘法（OLS）和logit回归的方法之和占比最高，达到六成。作为大数据分析的主要技术，机器学习在吸收统计学的基本算法后，通过模型的嵌套演变出神经网络、深度学习等高级算法，同时损失了算法的可解释性。而可解释性的本质是输入变量（即自变量）的参数、影响输出变量（即因变量）的路径透明可见，机器学习中的很多算法，在这方面其实不如过去实证研究中常用的统计学算法。\n　最后，应清醒地认识到，某些大数据分析技术之所以难以引入到实证研究中，是因为大数据技术的发展动力来自商业市场的需求，其初衷不是为学术研究而开发的。因此，要区别大数据技术在工程领域和在学术领域的使用差别。工程领域要求大数据模型有较强的结果预测能力，不太重视输入变量与输出结果间发生联系的路径。这使得技术开发的着力点在于如何能够准确预测未来，例如市场中多款量刑辅助的产品提供给办案人员的，是一个案件未来可能判处的刑期结果，而不是提供充分的说理。而学术研究更关心法律现象背后的社会成因机制，希望揭示出现象背后的原因。上述二者虽有共同点（进行精准预测的前提，也要有一个基于历史案件的模型），但考核模型表现优劣的标准是极为不同的。知晓此种差异后，研究者才能对当前眼花缭乱的大数据分析技术有所甄别，优先选择那些具有较好可解释性的机器学习算法。\n（三）加强某些议题的论证力度 　数据源和样本量的扩大，分析能力的增强，使得某些研究议题有机会变换新的角度、充实更有力的论据、得出更有说服力的结论。大数据技术作为一种方法并不直接产生新议题，但是能够增强旧有议题的论证能力，为原先难以量化研究的重要议题开启新的篇章。本节选取“法治中国”这一研究议题，尝试构想一个引入大数据技术后的学术发展新空间。\n　“法治中国”在近年来备受关注，是我国法学研究中的一个重要议题。一般认为，“‘法治中国’的内涵比‘法治国家’更加丰富、更加深刻、更具中国特色”，关于“法治中国”的主体、客体、竞争力等，都是“法治中国”之科学含义研究中的重要子课题。“法治中国”同时是一个有待进一步发展的议题，自党的十八届四中全会提出该口号后，关于“法治中国”的核心价值和精神元素是什么、具体的模式如何这些问题，虽然已经有一些研究成果，但还要继续丰富其内涵。在一些知名法学家的带领下，亟待更多法学青年学者的跟进，特别是作为一个与我国法治实践紧密联系的议题，“法治中国”应首先从国家、社会的各个实践侧面做出事实的归纳。\n　大数据技术可以在归纳中国法治实践中发挥大作用。具体可勾连几个看似不相关但实则联系密切的议题。一是近年来强调“中国问题”的学术反思。2011年举办的“中国法学研究之转型”研讨会上，诸多学者曾呼吁法学研究范式应该转变，认为“当前对中国特有的问题关注不够，缺乏中国问题意识”。具体而言，中国问题是在中国的政治建构、区域发展的极不平衡、社会在转型期中的急剧变化、社会治理资源的多元化等背景下形成的。本文认为，地域间、时间跨度中的中国法治实践差异，可通过大数据的时间序列、地理坐标图等各种形式予以呈现，法学研究要逐渐习惯于用数字化的方法发现并解释中国法治实践中的问题。二是与大数据技术直接关联的“法治评估”，这是关于立法、执法、司法等各领域的评估，其最大特色在于将指标构建技术和统计方法作为工具。笔者认为法治评估的相关研究，主要不在于实现地区间法治状况的可比性（这的确是提出法治评估的原因之一），而是旨在强调各国治理结构的差别，总结不同国家间某问题的不同法治方案。在西方学界过往的法治评估中，“所有实行西方政治制度的国家的得分必定高”。法治中国的研究要想有说服力地破除上述迷局，既要讲道理，更要摆事实，特别是利用好大数据所呈现的事实。\n四、大数据技术对于规范研究而言是一种助力 　法学实证研究和传统的规范研究间如何衔接和对话，是一个困扰研究者和期刊编辑的共同问题。有期刊编辑抱怨说，很多看似眼花缭乱的定量研究，最终得出的结论却不那么新奇，其言外之意是不需如此费劲，读者也早已知道这样的结论。还有学者坦言，实证研究和传统的规范研究间缺少对话，存在“平行线”难题，法学实证研究存在“叫好不叫座”的现象，即便高质量的实证研究，其被引用率也不高。数字法学时代到来后，上述问题能否有所改善？笔者以为，与其勉为其难地与规范研究直接对话，不如以“提供给规范研究一定启示”的姿态，定位大数据技术的贡献。此种贡献主要是便于研究者更自主、低成本地了解法律实践的运行状况，它是一种助力的功能。\n（一）拓宽了解释论的问题边界 　法律规范之所以需要解释，其原因在于“制定法的真实含义不只是隐藏在法条文字中，而且隐藏在具体的生活事实中”，生活事实的不断变化，使得法条一直有予以解释的必要性。换言之，这是一种来自司法实践中法条适用的困难所延伸出来的需求。但是，法教义学的规范研究，其传统重镇在高校，科研人员的作业模式与司法实践间隔较远，真正熟悉办案一线的学者并不多。这使得他们在发现哪个法条的哪个关键词存在司法适用困难、故而具有研究必要性上颇费周折。而哪怕是具有司法实践经验的研究者，在这个问题上的表现往往也好不到哪里去，因为他们的经验只是来自直接或间接经办过的案件，是一种主观的、个案式的感受。司法大数据的引入，有望改变上述局面。开展规范研究的学者可通过多个地区的法律案件文书，把文本向数据转换、提取文本背后的有用信息，进而全面获得实践中的裁判观点。波斯纳（Richard A. Posner）对此有过恰当的评论，他认为“法律决定和教义全都由事实驱动，而不是由理论驱动”。\n　首先，法律大数据所挖掘的信息，为规范研究提供了问题意识，为解释设定了起点。规范之所以需要解释，是因为存在疑义。此种疑义并非凭空而来，而是在法律的具体适用中凸显。在过去，此种凸显主要依靠典型案例的被发现而引起学术界的重视。如今，大数据技术的兴起，缩短了该种疑义被发现的进程，并克服了主观选择案例的片面性。这是因为，研究者可不再依赖于司法机关筛选出的指导案例，或者主观随意地挑选案件，而是通过公开的裁判文书进行全样本的大数据分析，挖掘出研究者所关心的司法实践的某一侧面情况。在评价中国的法教义学之缺陷时，有学者指责在中国看不到“法学与司法之间的深入对话”，进而强调中国学者应当虚心、耐心和诚心地向中国法官学习。面对面交流自然是学习的方式之一，但其成本太高。更有效的方式是跨越个案地、基于海量样本地分析法官所写的裁判文书（裁判文书是法官裁判观点的浓缩精华）。\n　其次，大数据方法赋能后的实证研究，为研究者提供了法律概念的社会语境。解释的最终目标是达致“裁定之案件获得公平的处理”，这种公平处理首先要具体化为探寻某一规则的立法目的。目的解释在某些学者眼里是指“探求法律在今日法秩序的标准意义”。那么，今日法秩序的理想图景又从何获知？学术研究者、法律适用者面临探寻这一出处的难题。以往的学理解释，一般是从部门法的基本价值出发解释法条，例如刑法的解释总是要考虑罪刑法定、罪刑均衡、法益保护、保障人权等，又如诉讼法中强调程序参与、诉讼效率、纠纷解决等。在学理解释者看来，这些基本就是衡量解释是否恰当的主要标准。而在具体法律适用者（例如面临个案裁判需要的法官）那里，还可能有其他社会、经济甚至政治的因素要加以考量。大数据的实证分析技术，为解释这些“关键词”提供了上述维度的信息参考，例如可利用大数据分析某个条款在不同案件背景下的不同解释结论，这些背景包括年代、当地的社会经济背景、原被告双方的身份等。以往的实证研究当中并非没有此类尝试，但毫无疑问，在大样本中分析裁判观点的社会语境，所得出的结论将更具有普适性。\n　当然，法教义学同样也给大数据分析以有价值的课题，规范研究者可以将其感兴趣的问题传递给法律大数据的分析者。如此一来，“这些学科的研究对象和知识兴趣就受到教义学的影响了，或者，也会引发交叉学科的研究课题的产生”。\n（二）为立法论提供效果评估工具 　2011年3月，时任全国人大常委会委员长吴邦国在十一届全国人大四次会议第二次全体会议上宣布“中国特色社会主义法律体系已经形成”。在此之后，学界有过一种观点，亦即认为我们的学术研究将从立法中心主义转向司法中心主义。另一种更谨慎的观点则认为，就中国特色社会主义法律体系这一宏大工程而言，上述时间节点是一个终点，但更是一个起点。在此后的七八年里，现实更加验证的似乎是后一种观点，即立法并没有消退，经济发展、社会转型向法律制度提出了新的要求。实证研究亦表明，立法中心主义的研究氛围始终存在。既然立法论的研究从未消失，我们更应当重视此类研究推动的立法质量和效果。法律体系形成及其规模的持续扩大，并不表明法律体系已经完备或能够自动产生实效，更不意味着立法必然合乎社会需要。就立法进行事前和事后的评估，这不只是立法机关的工作职责，同时也是借此反思立法论研究的良好契机。\n　这里主要探讨立法评估的方法，重点考察大数据技术是否有助于提升立法评估的广度和精度。当前的立法评估方式包括征集公众意见、问卷调查、实地走访等。例如在一份对地方法规的评估中，其研究者主要是在政府机构的主导下，通过第三方评估机构，推动各部门和区县自查、设计和布置调查问卷、文献梳理、重点走访和调研等方式，来完成评估。评估的方法当前“主要运用的是定性分析方法，很少运用定量分析方法及运用影响分析方法”，而这从评估的精确性来讲是不够的。毕竟，现代国家的管理是“数目字”管理，在现代政府的协调性行政控制中，对这些“官方数据”的例行监测是不可或缺的。大数据技术在立法评估中有如下两方面可能的贡献。\n　首先，大数据技术有助于更好地收集来自社会各界的反馈。笔者在研究中访问了全国人大和多个地方人大的网站，发现它们目前都还停留于前大数据时代的意见收集模式。大数据时代很注重对信息的标签化收集和处理，产业界将此称为“打标签”。若能在信息收集环节按照大数据分析的需要进行改造，增加备选的、对立法评估有价值的“标签”供用户勾选，则将有助于提高所收集的信息之质量。而互联网的发展，为利益相关者尤其是公众参与立法评估提供了手段。\n　其次，在立法有关材料的文本清洗和分类中，大数据技术也将提供更多的工具。在各地的立法评估实践中，会面对大量的文字材料。根据某省立法部门的反映，他们缺乏的是针对各方面立法意见的信息汇总和分类的能力。立法机关当前仍然停留于通过传统的“人工看、人工做统计”的方式来获悉各方面的反馈。大数据技术中的词频统计、主题分析、情感分析等相关技术，可以对庞杂的立法建议作清洗、聚类，而这些立法意见的文本处理能力是可积累和可复用的，根据过往的立法意见所构建的筛选模型，例如征集到的立法反馈有哪些意见类型、主要针对立法的哪部分提出意见、意见提出者的身份等，通过机器学习，可以应用于今后对立法意见的高效筛选之中。\n　最后，就立法评估的时间节点而言，大数据技术更能发挥作用的应该是立法后的评估。立法前评估与立法后评估的区别在于，立法前评估主要评估立法的必要性、合法性、协调性和可操作性，而立法后评估则重在考察法律法规对经济、社会和环境的实际影响。影响评估和成本—收益分析是两种不同的方法。成本—收益法是一种法经济学的路径，该方法之所以在立法前评估中经常被使用，是因为在立法之前一切影响都是估计的，并无立法产生的实际影响可以测量。较之事前的估计，关于事后的立法影响，其有关信息显然更多，数据分析也将更有应用的空间。因此，大数据技术和法经济学的方法，在立法前、后的评估中将体现出不同的分工。\n　上述主要讨论立法部门引入大数据技术展开立法评估，此外，大数据技术还应赋能学者的立法论研究，为研究提供检验成效、提示风险的能力。在一些西方学者看来，实证研究的前提为认同法律乃是一种工具，且由此对它可以用一种实证性的方法来加以检验。又由于法律规范对于维持社会秩序具有极大的重要性，社会变革一般不允许像其他科学领域中那样被“视为一种迭代过程”，因此，“在公共事务领域，失败是一个典型的只能在私下里低声讨论的事情”。但是，对某一制度的变革方案之效果进行大数据分析，绝对是有意义的，哪怕实证分析的结论不完全公开、仅供特定人参阅。例如陈卫东等人的课题组将某些改革举措限定在局部区域进行自然实验时，其中就用到大量的统计数据，该研究若能增加数据的维度和样本的数量，则其论证的效果也许会更好。此种对法律制度立法效果的大数据评估，已经在学术界得到一定的认可，例如在一项对精神损害赔偿发生机制的研究中，其研究者就意识到实证研究可以大幅度提高立法预测个体行动的精确性。\n结　语 　揭开大数据技术的面纱，我们可以看到，作为法学研究的一种新方法，大数据技术增强了我们获取数据、分析数据的能力，使得在更大时空范围内研究法治实践的规律成为了可能。\n　笔者认为，“数字技术+法学”应区分不同的细分场景展开讨论，不同的法律场景具有不同的特点。例如，首先应区别工程和学术，在学术研究中引入大数据技术，模型设计有充裕的时间，过程的可解释性要求较高。其次应区别学科和学术，作为法学研究方法的大数据技术，只是学科论中的内容之一。有学者认为，“计算法学的研究方法中最主要、最具特点的方法还是本文所指的运用计算机科学智能化处理大量法律数据以解决法律问题的方法”。本文的见解与其相近，同时认为这套大数据的方法不只适用于计算法学，而是全面覆盖法学的各个二级学科。最后是研究中具体方法的细分，如果将法学研究的方法区分为规范研究和实证研究，那么大数据技术方法和法学研究的结合点主要是在实证研究上。有学者认为，“计算法学可归属为实证法学的基本范畴”，“计算法学通过兼收并蓄的统合吸纳了定性研究和定量研究各自的优长”。本文主张狭义地将大数据技术定位为是对定量研究产生的变革，这并不妨碍与定性研究的彼此互鉴。在我国法学界，实证研究将与规范研究长期并存、共同发展。若对此心存疑虑，则不妨回顾一下美国法学研究在20世纪60年代所谓的“跨学科”研究方法之转向，以及90年代对此的二次转向，还有我国法学界在2005年前后也出现了一次“中国法学向何处去”的热烈讨论，便可以发现规范研究和各种跨学科法学研究方法间存在着难舍难分、始终共存的关系。\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/big_data_method_in_law_research/","summary":"本文原载《法学家》2021年第6期。\n作者 | 周翔（法学博士，浙江大学光华法学院特聘副研究员）\n来源 |《法学家》2021年第6期“主题研讨二：跨学科法学研究的方法论检视”栏目。\n 因篇幅较长，已略去原文注释。\n目录  一、前大数据时代的法律实证研究 二、大数据技术运用的各个节点 三、大数据技术对于实证研究而言是一场接力 四、大数据技术对于规范研究而言是一种助力 结　语  　中国法学正在迎来“大数据”“人工智能”的研究热潮。“数字法学”“计算法学”等各类新词汇层出不穷，但研究者们却并不都是在同一内涵和外延下使用这些概念。因此，本文有必要在开篇之初先设置一套分类法，对既有的学术研究成果加以归类，从而明确本文在既有研究中的坐标位置。笔者将既有的相关研究分为如下四大类：\n第一类称作“学科论”，此类研究看待技术的视角最广，其目标是希望厘清法学+数字技术的最大学科边界；\n第二类称作“对象论”，是把“大数据”“人工智能”等视为法律规制和法学研究的对象，此类文章占了当前本领域研究成果中的大多数；\n第三类称作“工程论”，这类研究关注到数字技术可以被引入执法、司法等各个场景当中，赋能法治的各环节；\n第四类才是“方法论”，是从学术研究方法的视角看待大数据技术，探讨其能为学术活动提供哪些新契机。本文的研究侧重于第四类，亦即集中讨论大数据分析技术作为学术活动的工具，能够给法学研究提供何种新方法。\n　本文旨在回答大数据技术如何才能成为法学研究的方法，如何与法律实证研究、规范研究这两大传统的方法实现有效的互动。关于此，左卫民在《迈向大数据法律研究》一文（以下简称为“左文”）中较早地提出了“大数据技术如何作为法学研究方法”这一命题，在“方法论”层面为将大数据技术引入法学研究当中起到了重要的推动作用。同时，笔者认为，以下方面仍值得进一步探讨：\n第一，“左文”中提到“从研究范式看，大数据法律研究可能推动实证研究的跨越式发展，特别是机器学习方式的引入，会使法学研究从法教义学、社科法学和实证法律研究等范式转向数据科学式的法学研究”。“范式”一词在托马斯·库恩（Thomas Kuhn）那里，是指“一个成熟的科学共同体在某段时间内所认可的研究方法、问题领域和解题标准的源头活水”。形成一个范式，是任何一个学科在发展中达到成熟的标志。“数据科学式”的法学研究已经是一种成熟的范式了么？它与“左文”中提到的法教义学、社科法学等既有的法学研究范式之间又是什么样的关系？\n第二，“左文”中提到“需要将小数据社科研究中已普遍运用和相对成熟的数据分析方法……运用到大数据分析中”。社会科学中开展定量研究，是以统计学原理为根基的，这和大数据技术联系密切的机器学习方法之间有何差异？法学研究又能够吸取大数据技术中的哪些优势？\n第三，左文中还提到“一些大数据法律研究缺乏必要的问题意识，主要是描述式研究，沦为‘调查报告式’的数据展示”。这涉及的是大数据分析技术应用现状的问题。\n上述三个方面，围绕大数据技术对法学研究的主要贡献展开，清晰地定位了大数据技术在既有的法学研究方法体系中的地位。\n　上述延伸和思考，一方面是对话，另一方面是形成本文研究的路径。本文的基本立足点在于：大数据分析技术如果对法学研究有影响的话，那么主要是使得法学更加社会科学化、更重视实证的方法、更习惯从大数据中探索法律世界的规律。这些贡献决定了大数据技术在法学研究方法体系中的位置，其既是对以统计学为基础的法律实证研究的接力，更是对以法教义学、立法论研究为核心的传统规范研究的助力。在这一基本立场之下，本文首先对前大数据时代的实证研究方法、研究对象作一番回顾；接着结合笔者担任大数据分析师的经历，针对技术细节作梳理；然后在这些基础之上，就如何实现大数据技术、定量的实证研究、规范的法教义学研究三者间的互动提出一套初步的方案；最后，鉴于当前学界在相关概念上存在较多的混用现象，本文就此作一些观点上的澄清。\n一、前大数据时代的法律实证研究 　关于什么是实证研究，目前虽然尚无统一定论，但在“基于模型和数据的方法”这一点上则有比较明确的共识。包括法学在内的各个社会科学学科之所以都对定量方法感兴趣，是因为“定量的方法，乃一切科学进程的核心”。只要是跨越个案地探寻案件平均水平、共性特征、要素相关性的研究，都将被本文纳入前大数据时代法律实证研究的范畴。下文对此类法律实证研究的回顾和梳理，主要是从研究的方法、测量的工具、样本量的大小等三个维度展开。\n（一）以统计为主的研究方法 　从研究方法来看，既有的法律实证研究均奉统计学为同宗。通常认为，统计学的方法有描述性统计和相关性分析两大类。结合法学研究的特点，这里稍作更细致的划分。根据笔者的概括，以往的研究成果主要利用了如下三种方法。\n　方法一：多案例分析。对案例的运用，如果不是针对法条进行解释、对立法提出建议，那么在笔者看来即为一种实证研究的路径。例如陈杭平关于民事诉讼标的的研究，为案件类型化设定了一套分类标准，即诉讼标的的不同含义，纵轴根据不同学说见解区分为三个版本，横轴以诉讼标的的不同领域或场景为标准。从多个案例中挖掘某些规律性信息，是一种超越个案、试图通过案件类型化获得研究结论的方法尝试。\n　方法二：描述性统计。前述的多案例分析，还很难称得上是严格意义上的实证分析。左卫民的一系列文章有力地推动了法律实证研究向前发展，其主要采用的便是描述性统计方法。例如，他关于审判委员会的研究，统计了某地区的三级法院审判委员会委员的审判经验与学历背景，并将讨论的议题细化到宏观指导议题和个案议题，分别进行统计和分析；再比如，他另一份关于中国刑事法律援助的研究，通过调研和考察某省的三家法院，统计了各类型辩护的人数和占比，并由此回答“究竟应当在多大范围内推行并构建当代中国的法律援助制度”这一问题。\n　方法三：相关性分析。白建军等人的研究则在描述性统计之基础上，迈向了相关性分析这一相对复杂的层次。相关性研究也被称为推论统计，是将统计学手法与概率理论相融合，对“‘无法整体把握的大的对象’或‘还未发生而未来会发生的事情’进行推测”。白建军等人开始关注引起某一现象的原因，试图建立自变量和因变量之间统计学意义上的相关性，所采用的回归模型主要为多元线性回归、logistics回归等常见模型。例如，白建军迄今为止在中国知网上下载量最高的一篇论文，是通过相关系数、多元线性回归来研究犯罪率的社会成因；又如，李本森关于速裁程序的研究，则以诉讼效率、量刑均衡和诉讼权利作为其关心的因变量，采用的是多元线性回归模型。\n　在线性回归、logistics回归这两大常用的回归模型之基础上，法律实证研究方法也根据议题之需推陈出新。首先表现为统计方法趋于高级，例如白建军在其一贯的相关性分析之基础上，对无法观察的变量作了科学处理，将中国民众的刑法偏好这一因变量，拆解为犯罪圈大小、刑罚轻重、罪刑均衡程度等三个可通过问卷调查测量的因子，以打分取值的形式获得该变量的数值；其次是结果呈现方法上的创新，例如单勇关于盗窃罪的回归分析结果，用GIS作图的方法实现可视化，选取建筑物、停车场等10类空间因素为自变量，用于说明因变量和空间变量的地理联系；最后是体现在用于分析的软件工具之更新上，过去的法律实证研究以使用SPSS软件为多，而近来的研究很多提及使用了Stata、R等工具。当然，使用更高级的软件进行分析，其效果未必就一定更好，但上述变化至少标志着学者们在工具利用方面的水平提升，且有些回归模型是初阶工具所无法胜任的。\n（二）多元测量工具并存 　测量工具是指获得数据的方法。数据搜集在统计学中是重要的一环，“所有统计数据追踪其初始来源，都是来自调查或实验”。统计学上根据是否为直接获得第一手数据，区分直接来源和间接来源。法学实证研究多以一手的直接来源为主，主要的测量工具包括如下几种。\n　工具一：问卷调查。这种测量工具与传统的社会科学方法保持一致，通过设置问题、受调查者回答的方式收集受访者的信息。此种方法在获取受访者主观态度方面效果明显，为学界所常用。例如，程金华在研究过程中为了解检察人员针对检察人员分类改革的认识而发放问卷；胡铭关于司法公信力的研究，通过向社会公众和司法官分别发放问卷，比较和审视“对于影响司法公信力的要素的认知与评判”。\n　工具二：模拟实验。“实验大多是对自然现象而言的”，但在近年来的法学研究中也时常出现。司法裁判一般不具有可再现性，同一个案件在真实场景下只可能出现一次。模拟实验是一种对司法裁判过程的模拟再现，借此可发现一些影响裁判结果的变量。目前中国法学界的模拟实验主要是对一些经典案例裁判过程的复盘，以验证某些观点。例如李学尧等人关于案卷材料阅读流畅度与裁判尺度之关系的研究，通过问卷字体大小、是否斜体、是否加重、行间距以及案件数量的变化来操纵阅读流畅度的变化。\n　工具三：文本摘录。法律中的文本，其典型形态为裁判文书。在裁判文书大量公开上网后，有人认为数据法学的春天即将到来。的确，裁判文书是记录诉讼过程最终的、有法律效力的、体系最完整的文本。对裁判文书的利用，比如文姬关于信用卡诈骗罪的研究当中有很多维度的信息挖掘，包括审级、行为人出生年等16个变量。文本的种类近年来也出现不少创新，比如习超等人关于证券监管的研究采用的是对上市公司执法事件的披露信息。\n　工具四：实地/田野调查。倘若只是单纯采取个案式的访谈记录，则在方法论上一般将之归入定性研究的范畴。但如果是在田野调查中对多个样本进行观察或访谈，采取结构化的方式收集数据，最终对数据进行跨越个案的量化分析，那么也可以被视为实证研究的测量工具之一。此种方式在国内法学研究中不乏实例，比如一份关于当事人法律意识的研究，所主要利用的素材就是在某法院传达室对一百多位当事人进行访谈所收集的数据；再比如左卫民在研究基层法院的财政制度、法官的工作时间分配等问题时，课题组前往实地调研、观察记录收集数据资料。\n　除了采用上述工具之一，实证研究还可以多种测量工具结合、定量和定性方法混用。比如胡铭关于庭审实质化的研究就不仅利用了判决文书，且还通过观摩庭审直播并记录的方式收集数据。\n（三）万级以下的样本量 　实证研究的论文中约定俗成要报告样本量，而之所以特别指出研究所用的样本量大小，是由于样本量直接关系到根据小样本得出的结论能否推及至更大的范围，因此抽样是统计学中很重要的概念。建立一个好样本的关键，是尽量选择最符合总体的样本，如果样本具有代表性，那么表明样本与总体有十分相似的特性，进而可以通过样本预测出总体具有哪种规律。\n　法律实证研究中的样本量过去以百级、千级为主，比如文姬关于信用卡诈骗罪的研究所利用的裁判文书样本有2103份，习超等人对证券监管“旋转门”的研究则采用了7103个监管事件作为样本。样本量大小是个相对的概念，还要考虑“全体”的规模大小。当我们确定研究的问题后，从理论上讲“全体”的范围就固定了。若以裁判文书为测量工具，则有三个层次的案件范围，依次分别是客观真实发生的案件总数、裁判文书网上公开的案件数、用于实际研究的案件样本数。前大数据时代的法律实证研究，是在公开的裁判文书范围内选取一定的样本进行分析，距公开的案件“全体”和真实客观的案件“全体”相去甚远，正因如此，不少研究受到样本代表性不足的质疑。\n　除了抽样和样本的代表性问题外，前述提及的那些研究基本没有交待如何实现样本的数据结构化。根据笔者自身的数据分析经验，以传统方式处理样本耗时巨大。如果以阅读裁判文书并摘取的形式处理样本，那么一天工作8小时也只能阅读20—40份裁判文书，如此来算，处理千量级的裁判文书样本就得花费数月时间。如果再安排专人抽查数据录入的质量，那么工时还得另计。\n（四）留待提升的不足 　在研究方法上，相关性分析方法之后难有新的突破；在分析工具上，研究者虽试图推陈出新，但总体变化不大。以构建回归模型做研究为例，中国法学界目前用过的回归算法种类屈指可数。某些研究虽然其方法有一定的创新，比如采用决策树的方法，但又和机器学习的决策树算法相去较远。另一个问题在于分析软件，小样本时代没有使用分析软件的明显障碍，但在大样本时代则要考虑借助的分析工具是否恰当，能否高效运行。上述两个方面，大数据技术都可能给其带来变革。","title":"转载 | 周翔：作为法学研究方法的大数据技术"},{"content":"本文档不定期更新，碰到比较好的 Newsletter 就会加进来。\n同时欢迎自荐、推荐你觉得不错的 Newsletter，请点击这里提交 issue！\n写作 Newsletter 工具或者平台，排名不分先后。\n Substack Start a paid newsletter，国外比较成功的一个 Newsletter 平台 Getrevue An editorial newsletter tool for writers and publishers Hedwig 一个简单可靠的邮件 Newsletter 创作平台，邀请码 hedwigpub 知园 取自一个小众概念“数字花园” 竹白 支持多种订阅方式的 newsletter 平台，且支持微信订阅。 语雀 新一代云端知识库，也不支持邮件订阅 podletter podletter汇总  编程技术  阮一峰科技周刊 经济学博士，IT大佬，分享的内容精彩非常 大邓和他的Python 自荐，不定期更新Python及数据分析相关内容  互联网和科技  科技爱好者周刊 从 2018 年初开始发布，记录每周值得分享的科技内容，周五发布。个人觉得是中文科技 Newsletter 最好之一，最喜欢的板块是「本周话题」。 hackernewsletter 每周发布一份关于创业、技术、编程等方面的文章。目前已有 60000 多个用户订阅。 The Quibbler 这是一份关于互联网、科技领域的 Newsletter，每月一期，偶尔提前。 商业内观一份穿梭在人类用思维构建的商业世界里，却一心想去往 2500 年前轴心时代的Newsletter。 海上星光产品通讯 关注产品、设计、科技的整理和思考。 湾区日报 关注创业与技术，每天推送 3~5 篇优质英文文章。 产品沉思录 关注互联网产品的设计与运营等内容，每周精选四篇推荐，并附上独立的观点，以期为你拓展视野的边界。  成长和思考  The 3-2-1 Newsletter 超过 100万 用户订阅，3 个简短思考、2 个引用、 1 个思考问题 DAVID PERELL 超过 4万 用户订阅，Monday Musings 和 Friday Finds，有读者反馈，有趣而且是从未见过的内容。 事不过三 每周三推送，文章分为 3 个板块：认识自己，好好学习，好好生活。截止到 2021 年 10 月，已经推送了 46 期了。 类地行星 会有一个世界，和地球有一点像，也有一点不像。 小胖’s Daily Note 记录我日常发现的一些东西。 自说自话\t没有记录就没有发生，而记录本身就已是一种反抗。 知更鸟文档 一份关于「 Thought」，「 Tools」和「Creativity」的所见分享。 光明王 加入你的网上邻居，一起阅读新闻，一起奇思妙想，一起欢度时光。 熊言熊语 关注学习分享和知识科普的播客栏目，我们希望用声音记录改变与成长。聊学习工作、聊科研科普。 生活奇旅 探寻如何更好地生活。 声波微步 有关泛社会议题的观察、解读和生活方式分享。 反向连接 You only connect dots by looking backward. 独来读趣 read to the infinity and beyond. 世俗的理想主义青年 专注于探讨与输出Martech营销技术、泛商业领域以及流行文化的日常产品思考。 Futurow 未来派 爱捣鼓者的日常，软件、开源、运营、阅读，什么有趣捣鼓什么。  工具和效率  Ali Abdaal 主理人是一名医生，超过 12 万用户订阅，分享可行的生产力提升，实际的生活建议，高品质的见解。 地心引力 关注效率工具与生活方式，一起脱离重力束缚。 happy letter 每个工作日发送简短实用的效率方法，健康习惯，思维模式。订阅人数已超 2000。 Sustainable Curiosity 日间碎片化阅读的批注和观点的集散地，关注领域：互联网、商业纪实、效率工具和数字化管理提升。 潦草学者 分享日常思考。长期关注互联网商业，效率工具。  多才多艺 Miscellaneous  新闻实验室 主理人是方可成，作者是香港中文大学社会科学院新闻传播学院的一名助理教授，站在行业变动的前沿，把握媒体变革的脉搏。用学术眼光理解新闻传播现象背后的逻辑，并用平易的语言进行解释。 推播助栏 每月推荐7集类型多样的播客节目，和你一起听见这个世界。 Random Lab 投资行业从业，内容包括商业观察、品牌出海、读书笔记，以及奢侈品行业二三事。 唯理通讯 推荐精选自互联网的深度文本。希望这些内容传播富有思考的声音，并促进订阅者对社会议题的关注。 精选日报 一文了解天下事。 刻意体会 Time takes the time time takes. Steve说每周通讯 内容来自Steve和朋友们每周精选的优质内容。 Λ-Reading （兰布达阅读）是一份智识阅读通讯（Newsletter），每周 1-2 期。(包含免费和付费) 逆流Upstream 专注观察中美流媒体与创作者经济赛道。每周/隔周原创更新，解析1个创业公司、推荐2档优质节目、传播3个行业观点。 一份全面的RSS和Newsletter 和 中文Podcast  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/newsletter-list/","summary":"本文档不定期更新，碰到比较好的 Newsletter 就会加进来。\n同时欢迎自荐、推荐你觉得不错的 Newsletter，请点击这里提交 issue！\n写作 Newsletter 工具或者平台，排名不分先后。\n Substack Start a paid newsletter，国外比较成功的一个 Newsletter 平台 Getrevue An editorial newsletter tool for writers and publishers Hedwig 一个简单可靠的邮件 Newsletter 创作平台，邀请码 hedwigpub 知园 取自一个小众概念“数字花园” 竹白 支持多种订阅方式的 newsletter 平台，且支持微信订阅。 语雀 新一代云端知识库，也不支持邮件订阅 podletter podletter汇总  编程技术  阮一峰科技周刊 经济学博士，IT大佬，分享的内容精彩非常 大邓和他的Python 自荐，不定期更新Python及数据分析相关内容  互联网和科技  科技爱好者周刊 从 2018 年初开始发布，记录每周值得分享的科技内容，周五发布。个人觉得是中文科技 Newsletter 最好之一，最喜欢的板块是「本周话题」。 hackernewsletter 每周发布一份关于创业、技术、编程等方面的文章。目前已有 60000 多个用户订阅。 The Quibbler 这是一份关于互联网、科技领域的 Newsletter，每月一期，偶尔提前。 商业内观一份穿梭在人类用思维构建的商业世界里，却一心想去往 2500 年前轴心时代的Newsletter。 海上星光产品通讯 关注产品、设计、科技的整理和思考。 湾区日报 关注创业与技术，每天推送 3~5 篇优质英文文章。 产品沉思录 关注互联网产品的设计与运营等内容，每周精选四篇推荐，并附上独立的观点，以期为你拓展视野的边界。  成长和思考  The 3-2-1 Newsletter 超过 100万 用户订阅，3 个简短思考、2 个引用、 1 个思考问题 DAVID PERELL 超过 4万 用户订阅，Monday Musings 和 Friday Finds，有读者反馈，有趣而且是从未见过的内容。 事不过三 每周三推送，文章分为 3 个板块：认识自己，好好学习，好好生活。截止到 2021 年 10 月，已经推送了 46 期了。 类地行星 会有一个世界，和地球有一点像，也有一点不像。 小胖’s Daily Note 记录我日常发现的一些东西。 自说自话\t没有记录就没有发生，而记录本身就已是一种反抗。 知更鸟文档 一份关于「 Thought」，「 Tools」和「Creativity」的所见分享。 光明王 加入你的网上邻居，一起阅读新闻，一起奇思妙想，一起欢度时光。 熊言熊语 关注学习分享和知识科普的播客栏目，我们希望用声音记录改变与成长。聊学习工作、聊科研科普。 生活奇旅 探寻如何更好地生活。 声波微步 有关泛社会议题的观察、解读和生活方式分享。 反向连接 You only connect dots by looking backward.","title":"高质量的Newsletter汇总"},{"content":"领域知识图谱的数据集，当前还比较缺失，而作为构建难度最大的产业链图谱领域更为空白。产业链作为产业经济学中的一个概念，是各个产业部门之间基于一定的技术经济关联，并依据特定的逻辑关系和时空布局关系客观形成的链条式关联关系形态。从本质上来说，产业链的本质是用于描述一个具有某种内在联系的企业群结构，产业链中大量存在着上下游关系和相互价值的交换，上游环节向下游环节输送产品或服务，下游环节向上游环节反馈信息。\n作者已经先后发布两大领域的实体图谱数据： 1、情报领域【武器装备知识图谱】，地址：https://github.com/liuhuanyong/QAonMilitaryKG\n2、医疗领域【医疗知识图谱】，地址： https://github.com/liuhuanyong/QASystemOnMedicalKG\n当前，为了进一步推动产业发展，本文围绕金融领域，推出面向上市公司的产业链图谱。\n项目地址：\n 一、项目构成 产业链知识图谱包括A股上市公司、行业和产品共3类实体，包括上市公司所属行业关系、行业上级关系、产品上游原材料关系、产品下游产品关系、公司主营产品、产品小类共6大类。\n通过数据处理、抽取，最终建成图谱规模数十万，其中包括上市公司4,654家，行业511个，产品95,559条、上游材料56,824条，上级行业480条，下游产品390条，产品小类52,937条，所属行业3,946条。  二、项目构建 1、实体构建\n1）上市公司\n目前上市公司已经达到四千多家，是我国重要的公司代表与行业标杆，本图谱选取上市公司作为基础实体之一。通过交易所公开信息中，可以得到上市公司代码、全称、简称、注册地址、挂牌等多个信息。\n 2）行业分类\n行业是产业链图谱中另一个核心内容，也是承载产业、公司及产品的一个媒介，通过这一领携作用，可以生产出大量的行业指数、热点行业等指标。\n目前关于行业，已经陆续出现多个行业规范，代表性的有申万三级行业分类、国民经济行业分类等。中国上市公司所属行业的分类准则是依据营业收入等财务数据为主要分类标准和依据，所采用财务数据为经过会计事务所审计并已公开披露的合并报表数据。\n2021年6月，申万发布了2021版的行业分类规范，将1级行业从28个调整至31个、2级行业从104个调整至134个、3级行业从227个调整至346个，新增1级行业美容护理等，新增2级行业，并将上市公司进行了归属。本图谱选用申万行业作为基础数据。\n 3）业务产品 业务产品主要指公司主营范围、经营的产品，用于对一个公司的定位。可以从公司的经营范围、年报等文本中进行提取得到。\n 2、关系构建 1）公司所属行业 通过公开的上市公司行业分类表，可以得到上市公司所对应的行业分类数据。  2）行业上级关系 通过公开的行业三级分类情况，可以通过组合的形式得到行业之间的上级关系数据。  3）公司主营产品关系\n上市公司的经营产品数据可以从两个方面来获得，一个是从公司简介中的经营范围中结合制定的规则进行提取，另一个是从公司每年发布的半年报、年报中进行提取。这些报告中会有按经营业务、经营产品、经营地域等几个角度对公司的营收占比进行统计，也可以通过制定规则的方式进行提取。第二种方法中，由于已经有统计数据，所以我们可以根据占比数据大小，对主营产品这一关系进行赋值。\n 4）产品之间的上下游关系\n产品之间的上下游关系，是展示产品之间传导逻辑关系的一个重要方法，包括上游原材料以及下游产品两大类。我们可以多种来获取：\n一种是基于规则模式匹配的方式进行抽取，如抽取上游原材料这一关系可以由诸如\u0026quot;a是b的原料/原材料/主要构件/重要原材料/ 上游原料\u0026quot;的模式进行抽取\u0026quot;，而下游产品，则同理可以通过\u0026quot;A是B的下游成品/产品\u0026quot;等模式进行提取。\n另一种是基于序列标注的提取。还有一种是基于现有结构化知识图谱的提取，例如已经结构化好的百科知识三元组，可以通过设定谓词及其扩展进行过滤。\n 5）产品之间的小类关系\n对于一个产品而言，其是有大小层级分类的，在缺少大类产品名称的时候，可以通过计算小类产品来得到相应指标。与产品之间的上下游数据类似，可以通过启发式规则的方式进行提取，如“A是一种B”，也可以通过字符之间的组成成分进行匹配生成，如“螺纹钢”是“精细螺纹钢”的一个大类。\n 三、项目运行 1、data文件夹下包括了本项目的数据信息：\n1)company.json:公司实体数据\n2)industry.json:行业实体数据 3)product.json:产品实体数据 4)company_industry.json:公司-行业关系数据 5)industry_industry.json:行业-行业关系数据 6)product_product.json:产品-产品数据 7)company_product.json:公司-产品数据\n2、项目运行:\npython build_graph.py\n四、项目总结 产业链图谱是众多领域知识图谱中较为棘手的一种，本项目通过现有的数据，借助数据处理、结构化提取方式，设计、构建并形成了一个节点100,718，关系边169,153的十万级别产业链图谱。就产业链图谱的构建而言，我们需要至少从以上三个方面加以考虑：\n 其一，产业链的主观性与标准性。产业链的主观性较强，不同的人对产业链的构建、产业链节点、关系的类型，产业链的颗粒度问题都有不同的理解。不同的设定会直接导致不同的应用结果。正如我们所看到的，目前存在不同的行业标准，不同的网站、机构也将公司归为不同的行业。 其二，产业链的动态性和全面性。产业链需要具备足够大的复用性和扩展性，几千家上市公司实际上是冰山一角。国内有几千万家公司，而且不断会有新增，如何将新增的公司融入到这个额产业链中，也是一个很大挑战。此外，产业本身是动态的， 随着行业的发展，不断会有新的行业出现。如何捕捉这种行业的变化，使得整个图谱变得与时俱进，也是需要考量的点。 其三，产业链的定量推理特性。单纯定性的构建产业链知识图谱，如果没有足够的参数，仅有知识表达是无法进行推理的，推理要求知识图谱Schema具备节点间推理传导的必备参数，以及影响推理传导的其他关键参数。对于必备参数来说，从公司到产品必须有主营占比、市场占比、产能占比等数据，从产品到产品必须有成本占比和消耗占比等数据。  参考数据来源 1、申万行业：http://www.swsindex.com\n2、深交所: http://www.szse.cn\n3、上交所: http://www.sse.com.cn\nIf any question about the project or me ,see https://liuhuanyong.github.io/\n如有自然语言处理、知识图谱、事理图谱、社会计算、语言资源建设等问题或合作，可联系我： 1、我的github项目介绍：https://liuhuanyong.github.io\n2、我的csdn博客：https://blog.csdn.net/lhy2014\n3、about me:刘焕勇，lhy_in_blcu@126.com. 4、我的技术公众号:老刘说NLP\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/chain_knowledge_graph/","summary":"领域知识图谱的数据集，当前还比较缺失，而作为构建难度最大的产业链图谱领域更为空白。产业链作为产业经济学中的一个概念，是各个产业部门之间基于一定的技术经济关联，并依据特定的逻辑关系和时空布局关系客观形成的链条式关联关系形态。从本质上来说，产业链的本质是用于描述一个具有某种内在联系的企业群结构，产业链中大量存在着上下游关系和相互价值的交换，上游环节向下游环节输送产品或服务，下游环节向上游环节反馈信息。\n作者已经先后发布两大领域的实体图谱数据： 1、情报领域【武器装备知识图谱】，地址：https://github.com/liuhuanyong/QAonMilitaryKG\n2、医疗领域【医疗知识图谱】，地址： https://github.com/liuhuanyong/QASystemOnMedicalKG\n当前，为了进一步推动产业发展，本文围绕金融领域，推出面向上市公司的产业链图谱。\n项目地址：\n 一、项目构成 产业链知识图谱包括A股上市公司、行业和产品共3类实体，包括上市公司所属行业关系、行业上级关系、产品上游原材料关系、产品下游产品关系、公司主营产品、产品小类共6大类。\n通过数据处理、抽取，最终建成图谱规模数十万，其中包括上市公司4,654家，行业511个，产品95,559条、上游材料56,824条，上级行业480条，下游产品390条，产品小类52,937条，所属行业3,946条。  二、项目构建 1、实体构建\n1）上市公司\n目前上市公司已经达到四千多家，是我国重要的公司代表与行业标杆，本图谱选取上市公司作为基础实体之一。通过交易所公开信息中，可以得到上市公司代码、全称、简称、注册地址、挂牌等多个信息。\n 2）行业分类\n行业是产业链图谱中另一个核心内容，也是承载产业、公司及产品的一个媒介，通过这一领携作用，可以生产出大量的行业指数、热点行业等指标。\n目前关于行业，已经陆续出现多个行业规范，代表性的有申万三级行业分类、国民经济行业分类等。中国上市公司所属行业的分类准则是依据营业收入等财务数据为主要分类标准和依据，所采用财务数据为经过会计事务所审计并已公开披露的合并报表数据。\n2021年6月，申万发布了2021版的行业分类规范，将1级行业从28个调整至31个、2级行业从104个调整至134个、3级行业从227个调整至346个，新增1级行业美容护理等，新增2级行业，并将上市公司进行了归属。本图谱选用申万行业作为基础数据。\n 3）业务产品 业务产品主要指公司主营范围、经营的产品，用于对一个公司的定位。可以从公司的经营范围、年报等文本中进行提取得到。\n 2、关系构建 1）公司所属行业 通过公开的上市公司行业分类表，可以得到上市公司所对应的行业分类数据。  2）行业上级关系 通过公开的行业三级分类情况，可以通过组合的形式得到行业之间的上级关系数据。  3）公司主营产品关系\n上市公司的经营产品数据可以从两个方面来获得，一个是从公司简介中的经营范围中结合制定的规则进行提取，另一个是从公司每年发布的半年报、年报中进行提取。这些报告中会有按经营业务、经营产品、经营地域等几个角度对公司的营收占比进行统计，也可以通过制定规则的方式进行提取。第二种方法中，由于已经有统计数据，所以我们可以根据占比数据大小，对主营产品这一关系进行赋值。\n 4）产品之间的上下游关系\n产品之间的上下游关系，是展示产品之间传导逻辑关系的一个重要方法，包括上游原材料以及下游产品两大类。我们可以多种来获取：\n一种是基于规则模式匹配的方式进行抽取，如抽取上游原材料这一关系可以由诸如\u0026quot;a是b的原料/原材料/主要构件/重要原材料/ 上游原料\u0026quot;的模式进行抽取\u0026quot;，而下游产品，则同理可以通过\u0026quot;A是B的下游成品/产品\u0026quot;等模式进行提取。\n另一种是基于序列标注的提取。还有一种是基于现有结构化知识图谱的提取，例如已经结构化好的百科知识三元组，可以通过设定谓词及其扩展进行过滤。\n 5）产品之间的小类关系\n对于一个产品而言，其是有大小层级分类的，在缺少大类产品名称的时候，可以通过计算小类产品来得到相应指标。与产品之间的上下游数据类似，可以通过启发式规则的方式进行提取，如“A是一种B”，也可以通过字符之间的组成成分进行匹配生成，如“螺纹钢”是“精细螺纹钢”的一个大类。\n 三、项目运行 1、data文件夹下包括了本项目的数据信息：\n1)company.json:公司实体数据\n2)industry.json:行业实体数据 3)product.json:产品实体数据 4)company_industry.json:公司-行业关系数据 5)industry_industry.json:行业-行业关系数据 6)product_product.json:产品-产品数据 7)company_product.json:公司-产品数据\n2、项目运行:\npython build_graph.py\n四、项目总结 产业链图谱是众多领域知识图谱中较为棘手的一种，本项目通过现有的数据，借助数据处理、结构化提取方式，设计、构建并形成了一个节点100,718，关系边169,153的十万级别产业链图谱。就产业链图谱的构建而言，我们需要至少从以上三个方面加以考虑：\n 其一，产业链的主观性与标准性。产业链的主观性较强，不同的人对产业链的构建、产业链节点、关系的类型，产业链的颗粒度问题都有不同的理解。不同的设定会直接导致不同的应用结果。正如我们所看到的，目前存在不同的行业标准，不同的网站、机构也将公司归为不同的行业。 其二，产业链的动态性和全面性。产业链需要具备足够大的复用性和扩展性，几千家上市公司实际上是冰山一角。国内有几千万家公司，而且不断会有新增，如何将新增的公司融入到这个额产业链中，也是一个很大挑战。此外，产业本身是动态的， 随着行业的发展，不断会有新的行业出现。如何捕捉这种行业的变化，使得整个图谱变得与时俱进，也是需要考量的点。 其三，产业链的定量推理特性。单纯定性的构建产业链知识图谱，如果没有足够的参数，仅有知识表达是无法进行推理的，推理要求知识图谱Schema具备节点间推理传导的必备参数，以及影响推理传导的其他关键参数。对于必备参数来说，从公司到产品必须有主营占比、市场占比、产能占比等数据，从产品到产品必须有成本占比和消耗占比等数据。  参考数据来源 1、申万行业：http://www.swsindex.com\n2、深交所: http://www.szse.cn\n3、上交所: http://www.sse.com.cn","title":"中文金融领域知识图谱的数据集ChainKnowledgeGraph"},{"content":"ChineseSemanticKB ChineseSemanticKB,chinese semantic knowledge base, 面向中文处理的12类、百万规模的语义常用词典，包括34万抽象语义库、34万反义语义库、43万同义语义库等，可支持句子扩展、转写、事件抽象与泛化等多种应用场景。\n项目介绍 语义知识库是自然语言处理中十分重要的一个基础资源，与学术界追求算法模型不同，工业界的自然语言处理对于底层的词汇知识库、语义知识库等多种资源依赖度很高，具体体现在：\n1、具有落地场景的自然语言处理任务都是业务高度相关，一个业务需求刚进去，需要解决的是业务的词汇问题，无基础词库，无项目冷启动；\n2、规则和正则启动下的工业级应用，规则的扩展、泛化都需要底层的词汇网络做支撑；\n3、目前包括搜索、问答、舆情监控、事件分析等应用，与标签体系的运作关系密切，而这与先验的底层词汇库依赖性很强；\n4、自然语言场景越来越关注推理层面，即所谓的“认知”层面，认知背后的各种逻辑关系库，是驱动这一决策的根本途径；\n5、当前，面向中文开源词库的工作存在少量、分散的状态，无论从规模，还是质量，都需要进一步聚合；\n因此，我从过往的开源工作中进一步抽离和整理，形成了中文处理的12类、百万规模的语义常用词典，包括34万抽象语义库、34万反义语义库、43万同义语义库等，用于相关下游任务。\n项目放于dict当中，可直接下载，不建议二次建库共享，尊重开源。\n词库的类别    词库类型 词库规模 词库举例 词库应用     抽象关系库 346,048 座椅,抽象,家具 事件抽象与泛化，人民币贬值到货币贬值，再到美元贬值，可支持查询扩展、推荐等任务   反义关系库 34,380 开心@苦恼 可用于句子改写，开心改苦恼，支持数据增强，句子生成   同义关系库 424,826 开心@高兴 可用于查询扩展、数据增强，也可结合抽象关系库完成推荐等任务   简称关系库 136,081 北京大学@北大 可用于句子标准化、句子改写、实体消歧等任务   程度副词 222 极其,2.0 可用于情感强度计算，带情感色彩的句子生成   否定词 586 不,无,没有 可用于情感计算等任务   节日时间词 54 春节、五四节 可用于时间词识别等任务   量比词 7 占比、环比、同比 可用于金融领域指标类数据提取任务   数量介词 24 大约、达到、超过 可用于金融事件抽象或主干化的搭配词处理任务   停用词 3,861 ？、的、着 常规的文本特征提取等任务   修饰副词 222 所、有所 可结合程度副词完成情感强度计算等任务   情态词 77 肯定、应该、大概 可用于句子主观性计算、舆情与可信度计算    总结 1、本项目开源了一个目前可用于事件处理以及工业舆情的12类语义词库，总规模数目一百余万；\n2、本项目开源的34万抽象语义库、34万反义语义库、43万同义语义库，在作者的实际工作中【事件处理、事理抽取、事件推理】等有重要用途;\n3、中文常用语义常用词典，均来源于公开文本+人工整理+机器抽取形成，其中若有质量不高之处，可积极批评指正;\n4、中文开源事业还是要坚持做下去，尽可能地缩短自然语言处理学术界和工业界之间的鸿沟。\n If any question about the project or me ,see https://liuhuanyong.github.io/.\n如有自然语言处理、知识图谱、事理图谱、社会计算、语言资源建设等问题或合作，可联系我： 1、我的github项目介绍：https://liuhuanyong.github.io 2、我的csdn技术博客：https://blog.csdn.net/lhy2014 3、我的联系方式: 刘焕勇，中国科学院软件研究所，lhy_in_blcu@126.com. 4、我的共享知识库项目：刘焕勇，数据地平线，http://www.openkg.cn/organization/datahorizon.\n5、我的工业项目：刘焕勇，数据地平线，大规模实时事理学习系统：https://xueji.datahorizon.cn. 6、我的工业项目：刘焕勇，数据地平线，面向事件和语义的自然语言处理工具箱：https://nlp.datahorizon.cn\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/chinese_semantic_kb/","summary":"ChineseSemanticKB ChineseSemanticKB,chinese semantic knowledge base, 面向中文处理的12类、百万规模的语义常用词典，包括34万抽象语义库、34万反义语义库、43万同义语义库等，可支持句子扩展、转写、事件抽象与泛化等多种应用场景。\n项目介绍 语义知识库是自然语言处理中十分重要的一个基础资源，与学术界追求算法模型不同，工业界的自然语言处理对于底层的词汇知识库、语义知识库等多种资源依赖度很高，具体体现在：\n1、具有落地场景的自然语言处理任务都是业务高度相关，一个业务需求刚进去，需要解决的是业务的词汇问题，无基础词库，无项目冷启动；\n2、规则和正则启动下的工业级应用，规则的扩展、泛化都需要底层的词汇网络做支撑；\n3、目前包括搜索、问答、舆情监控、事件分析等应用，与标签体系的运作关系密切，而这与先验的底层词汇库依赖性很强；\n4、自然语言场景越来越关注推理层面，即所谓的“认知”层面，认知背后的各种逻辑关系库，是驱动这一决策的根本途径；\n5、当前，面向中文开源词库的工作存在少量、分散的状态，无论从规模，还是质量，都需要进一步聚合；\n因此，我从过往的开源工作中进一步抽离和整理，形成了中文处理的12类、百万规模的语义常用词典，包括34万抽象语义库、34万反义语义库、43万同义语义库等，用于相关下游任务。\n项目放于dict当中，可直接下载，不建议二次建库共享，尊重开源。\n词库的类别    词库类型 词库规模 词库举例 词库应用     抽象关系库 346,048 座椅,抽象,家具 事件抽象与泛化，人民币贬值到货币贬值，再到美元贬值，可支持查询扩展、推荐等任务   反义关系库 34,380 开心@苦恼 可用于句子改写，开心改苦恼，支持数据增强，句子生成   同义关系库 424,826 开心@高兴 可用于查询扩展、数据增强，也可结合抽象关系库完成推荐等任务   简称关系库 136,081 北京大学@北大 可用于句子标准化、句子改写、实体消歧等任务   程度副词 222 极其,2.0 可用于情感强度计算，带情感色彩的句子生成   否定词 586 不,无,没有 可用于情感计算等任务   节日时间词 54 春节、五四节 可用于时间词识别等任务   量比词 7 占比、环比、同比 可用于金融领域指标类数据提取任务   数量介词 24 大约、达到、超过 可用于金融事件抽象或主干化的搭配词处理任务   停用词 3,861 ？、的、着 常规的文本特征提取等任务   修饰副词 222 所、有所 可结合程度副词完成情感强度计算等任务   情态词 77 肯定、应该、大概 可用于句子主观性计算、舆情与可信度计算    总结 1、本项目开源了一个目前可用于事件处理以及工业舆情的12类语义词库，总规模数目一百余万；","title":"中文语义常用词典ChineseSemanticKB"},{"content":"ashares 代码非原创，是对项目Ashare的封装, 中国股市A股股票行情实时数据最简封装API接口, 包含日线,分时分钟线,全部格式成DataFrame格式数据,可用来研究，量化分析，证券股票程序化自动化交易系统 行情系统包括新浪腾讯双数据核心，自动故障切换，为量化研究者在数据获取方面极大地减轻工作量，更加专注于策略和模型的研究与实现。\n功能特点  双内核封装，新浪财经，腾讯股票的实时行情数据，包括任意历史日线，周线，月线，分钟线，小时线等，已经稳定运行数年 双内核一主一备，自动热备，自动切换，Ashare即使用来做量化实盘行情源也可以满足。 全部数据格式清理成DataFrame格式数据，让您非常方便的使用pandas来分析和处理 和其他行情库（tushare等）比的优点是什么？ \u0026ndash; 简单 轻量 便携 开源 Ashare把复杂的数据获取，拆分，整合逻辑全部封装成一个函数 get_price() 看完下面例子就会了 Ashare可以用在任何需要量化研究，量化分析的场合  安装 pip install akshares \n快速上手 from ashares import get_price, # 证券代码兼容多种格式 通达信，同花顺，聚宽 # sh000001 (000001.XSHG) sz399006 (399006.XSHE) sh600519 ( 600519.XSHG )  df=get_price(\u0026#39;sh000001\u0026#39;, frequency=\u0026#39;1d\u0026#39;, count=5) #默认获取今天往前5天的日线实时行情 print(\u0026#39;上证指数日线行情\\n\u0026#39;,df) df=get_price(\u0026#39;000001.XSHG\u0026#39;,frequency=\u0026#39;1d\u0026#39;,count=5,end_date=\u0026#39;2021-04-30\u0026#39;) #可以指定结束日期，获取历史行情 print(\u0026#39;上证指数历史行情\\n\u0026#39;,df) df=get_price(\u0026#39;000001.XSHG\u0026#39;,frequency=\u0026#39;1w\u0026#39;,count=5,end_date=\u0026#39;2018-06-15\u0026#39;) #支持\u0026#39;1d\u0026#39;日, \u0026#39;1w\u0026#39;周, \u0026#39;1M\u0026#39;月  print(\u0026#39;上证指数历史周线\\n\u0026#39;,df) df=get_price(\u0026#39;sh600519\u0026#39;,frequency=\u0026#39;15m\u0026#39;,count=5) #分钟线实时行情，可用\u0026#39;1m\u0026#39;,\u0026#39;5m\u0026#39;,\u0026#39;15m\u0026#39;,\u0026#39;30m\u0026#39;,\u0026#39;60m\u0026#39; print(\u0026#39;贵州茅台15分钟线\\n\u0026#39;,df) df=get_price(\u0026#39;600519.XSHG\u0026#39;,frequency=\u0026#39;60m\u0026#39;,count=6) #分钟线实时行情，可用\u0026#39;1m\u0026#39;,\u0026#39;5m\u0026#39;,\u0026#39;15m\u0026#39;,\u0026#39;30m\u0026#39;,\u0026#39;60m\u0026#39; print(\u0026#39;贵州茅台60分钟线\\n\u0026#39;,df) Run\n#上证指数日线行情---------------------------------------------------- open close high low volume 2021-06-07 3597.14 3599.54 3600.38 3581.90 303718677.0 2021-06-08 3598.75 3580.11 3621.52 3563.25 304491470.0 2021-06-09 3576.80 3591.40 3598.71 3572.64 298323296.0 2021-06-10 3587.53 3610.86 3624.34 3584.13 318174808.0 2021-06-11 3614.11 3589.75 3614.40 3587.15 360554970.0 #贵州茅台60分钟线---------------------------------------------------- open close high low volume 2021-06-10 14:00:00 2237.00 2224.16 2245.00 2222.00 4541.53 2021-06-10 15:00:00 2222.21 2238.48 2240.34 2222.21 4146.88 2021-06-11 10:30:00 2239.00 2220.00 2244.00 2197.86 12030.00 2021-06-11 11:30:00 2220.01 2210.18 2231.80 2200.18 4868.00 2021-06-11 14:00:00 2210.10 2223.35 2224.48 2206.01 4544.00 2021-06-11 15:00:00 2223.33 2178.81 2226.80 2178.81 12529.00 \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/ashares/","summary":"ashares 代码非原创，是对项目Ashare的封装, 中国股市A股股票行情实时数据最简封装API接口, 包含日线,分时分钟线,全部格式成DataFrame格式数据,可用来研究，量化分析，证券股票程序化自动化交易系统 行情系统包括新浪腾讯双数据核心，自动故障切换，为量化研究者在数据获取方面极大地减轻工作量，更加专注于策略和模型的研究与实现。\n功能特点  双内核封装，新浪财经，腾讯股票的实时行情数据，包括任意历史日线，周线，月线，分钟线，小时线等，已经稳定运行数年 双内核一主一备，自动热备，自动切换，Ashare即使用来做量化实盘行情源也可以满足。 全部数据格式清理成DataFrame格式数据，让您非常方便的使用pandas来分析和处理 和其他行情库（tushare等）比的优点是什么？ \u0026ndash; 简单 轻量 便携 开源 Ashare把复杂的数据获取，拆分，整合逻辑全部封装成一个函数 get_price() 看完下面例子就会了 Ashare可以用在任何需要量化研究，量化分析的场合  安装 pip install akshares \n快速上手 from ashares import get_price, # 证券代码兼容多种格式 通达信，同花顺，聚宽 # sh000001 (000001.XSHG) sz399006 (399006.XSHE) sh600519 ( 600519.XSHG )  df=get_price(\u0026#39;sh000001\u0026#39;, frequency=\u0026#39;1d\u0026#39;, count=5) #默认获取今天往前5天的日线实时行情 print(\u0026#39;上证指数日线行情\\n\u0026#39;,df) df=get_price(\u0026#39;000001.XSHG\u0026#39;,frequency=\u0026#39;1d\u0026#39;,count=5,end_date=\u0026#39;2021-04-30\u0026#39;) #可以指定结束日期，获取历史行情 print(\u0026#39;上证指数历史行情\\n\u0026#39;,df) df=get_price(\u0026#39;000001.XSHG\u0026#39;,frequency=\u0026#39;1w\u0026#39;,count=5,end_date=\u0026#39;2018-06-15\u0026#39;) #支持\u0026#39;1d\u0026#39;日, \u0026#39;1w\u0026#39;周, \u0026#39;1M\u0026#39;月  print(\u0026#39;上证指数历史周线\\n\u0026#39;,df) df=get_price(\u0026#39;sh600519\u0026#39;,frequency=\u0026#39;15m\u0026#39;,count=5) #分钟线实时行情，可用\u0026#39;1m\u0026#39;,\u0026#39;5m\u0026#39;,\u0026#39;15m\u0026#39;,\u0026#39;30m\u0026#39;,\u0026#39;60m\u0026#39; print(\u0026#39;贵州茅台15分钟线\\n\u0026#39;,df) df=get_price(\u0026#39;600519.XSHG\u0026#39;,frequency=\u0026#39;60m\u0026#39;,count=6) #分钟线实时行情，可用\u0026#39;1m\u0026#39;,\u0026#39;5m\u0026#39;,\u0026#39;15m\u0026#39;,\u0026#39;30m\u0026#39;,\u0026#39;60m\u0026#39; print(\u0026#39;贵州茅台60分钟线\\n\u0026#39;,df) Run\n#上证指数日线行情---------------------------------------------------- open close high low volume 2021-06-07 3597.","title":"ashares库|A股市场历史行情数据"},{"content":"大家看过了数值数据随时间变化的曲线，但是，却鲜有可视化的工具能很好地对类别特征随着时间变化进行绘制。本文就介绍一种很好的类别特征可视化工具PyCatFlow，非常的简单且实用。\nPyCatFlow PyCatFlow 是一个 Python 包，用于可视化类别数据的时间变化。它的灵感来自 Bernhard Rieder 的可视化工具 RankFlow，它允许将排名列表随着时间的推移进行可视化，例如，Google 或 YouTube 上查询的搜索结果的变化。在我看来，尽管它的用户界面极简,RankFlow 是一个非常有用的工具，而且我很难为其准备数据。事实证明，这些困难主要源于“滥用”RankFlow，或者用更积极的术语来说，它源于将 RankFlow 用于其设计目的以外的其他目的。\n背景：了解RankFlow RankFlow 允许比较排名列表（随着时间的推移）。 在最简单的形式中，它需要以每列代表一个排名列表的方式排列表格数据。\nTime 1,Time 2,Time 3,Time 4 Item 1,Item 1,Item 1,Item 2 Item 2,Item 4,Item 2,Item 1 Item 3,Item 2,Item 4,Item 4 Item 4,Item 3,Item 5,Item 5 每个排名列表都可以通过权重进行补充，从而为数据添加另一层信息。 例如，如果我们采用 YouTube 搜索结果数据，则可以将观看次数、赞成票数或赞成票反对率用作权重信息。 为简单起见，示例数据仅由排名列表组成，并在以下流程图中显示结果。\n 为了加快图表的后处理速度，我决定创建一个类似于 RankFlow 的可视化工具，它非常适合不包含显式排名信息但可能包含额外分类数据的时间数据。\nPyCatFlow快速上手  如果我们用两种不同颜色，灰色和绿色分别表示男生和女生，然后每一个格子表示一类电影，横轴是男生和女生随着时间推移看的不同的电影，这样我们就可以一目了然男生和女生的喜好的变化，真的是太直观了！ PyCatFlow大大方便了我们对于类别特征随着时间变化的分析。\n准备数据\nimport pandas as pd df = pd.read_csv(\u0026#34;data/sample_data_ChatterBot_Requirements.csv\u0026#34;, sep=\u0026#39;\\t\u0026#39;) df.head(5)     column items category column order     0 2015-09-08 fuzzywuzzy A_Requirements 1   1 2015-09-08 requests A_Requirements 1   2 2015-09-08 requests-oauthlib A_Requirements 1   3 2015-09-08 pymongo A_Requirements 1   4 2015-09-08 jsondatabase A_Requirements 1    import pycatflow as pcf # Load and parse data from file fname=\u0026#34;data/sample_data_ChatterBot_Requirements.csv\u0026#34; data = pcf.read_file(fname, columns=\u0026#34;column\u0026#34;, nodes=\u0026#34;items\u0026#34;, categories=\u0026#34;category\u0026#34;, column_order=\u0026#34;column order\u0026#34;) \u0026#39;\u0026#39;\u0026#39; visualize(data, spacing=50, node_size=10, width=None, height=None, minValue=1, maxValue=10, node_scaling=\u0026#34;linear\u0026#34;, connection_type=\u0026#34;semi-curved\u0026#34;, color_startEnd=True, color_categories=True, nodes_color=\u0026#34;gray\u0026#34;, start_node_color=\u0026#34;green\u0026#34;, end_node_color=\u0026#34;red\u0026#34;, palette=None, show_labels=True, label_text=\u0026#34;item\u0026#34;, label_font=\u0026#34;sans-serif\u0026#34;, label_color=\u0026#34;black\u0026#34;, label_size=5, label_shortening=\u0026#34;clip\u0026#34;, label_position=\u0026#34;nodes\u0026#34;, line_opacity=0.5, line_stroke_color=\u0026#34;white\u0026#34;, line_stroke_width=0.5, line_stroke_thick=0.5, legend=True, sort_by=\u0026#34;frequency\u0026#34;) \u0026#39;\u0026#39;\u0026#39; viz = pcf.visualize(data, spacing=20, width=800, maxValue=20, minValue=2) #save visualization to files viz.savePng(\u0026#39;sample_viz.png\u0026#39;) viz.saveSvg(\u0026#39;sample_viz.svg\u0026#39;) #show visualization viz   # Another option is to visualize the graph with curved connections. # The implementation of this connection type draws on https://github.com/bernorieder/RankFlow viz = pcf.visualize(data, spacing=20, width=800, maxValue=20, minValue=2, connection_type=\u0026#39;curved\u0026#39;) viz   # The third option are straight connections between nodes viz = pcf.visualize(data, spacing=20, width=800, maxValue=20, minValue=2, connection_type=\u0026#39;straight\u0026#39;) # show visualization viz   代码下载 https://github.com/bumatic/PyCatFlow\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pycatflow/","summary":"大家看过了数值数据随时间变化的曲线，但是，却鲜有可视化的工具能很好地对类别特征随着时间变化进行绘制。本文就介绍一种很好的类别特征可视化工具PyCatFlow，非常的简单且实用。\nPyCatFlow PyCatFlow 是一个 Python 包，用于可视化类别数据的时间变化。它的灵感来自 Bernhard Rieder 的可视化工具 RankFlow，它允许将排名列表随着时间的推移进行可视化，例如，Google 或 YouTube 上查询的搜索结果的变化。在我看来，尽管它的用户界面极简,RankFlow 是一个非常有用的工具，而且我很难为其准备数据。事实证明，这些困难主要源于“滥用”RankFlow，或者用更积极的术语来说，它源于将 RankFlow 用于其设计目的以外的其他目的。\n背景：了解RankFlow RankFlow 允许比较排名列表（随着时间的推移）。 在最简单的形式中，它需要以每列代表一个排名列表的方式排列表格数据。\nTime 1,Time 2,Time 3,Time 4 Item 1,Item 1,Item 1,Item 2 Item 2,Item 4,Item 2,Item 1 Item 3,Item 2,Item 4,Item 4 Item 4,Item 3,Item 5,Item 5 每个排名列表都可以通过权重进行补充，从而为数据添加另一层信息。 例如，如果我们采用 YouTube 搜索结果数据，则可以将观看次数、赞成票数或赞成票反对率用作权重信息。 为简单起见，示例数据仅由排名列表组成，并在以下流程图中显示结果。\n 为了加快图表的后处理速度，我决定创建一个类似于 RankFlow 的可视化工具，它非常适合不包含显式排名信息但可能包含额外分类数据的时间数据。\nPyCatFlow快速上手  如果我们用两种不同颜色，灰色和绿色分别表示男生和女生，然后每一个格子表示一类电影，横轴是男生和女生随着时间推移看的不同的电影，这样我们就可以一目了然男生和女生的喜好的变化，真的是太直观了！ PyCatFlow大大方便了我们对于类别特征随着时间变化的分析。\n准备数据\nimport pandas as pd df = pd.read_csv(\u0026#34;data/sample_data_ChatterBot_Requirements.csv\u0026#34;, sep=\u0026#39;\\t\u0026#39;) df.head(5)     column items category column order     0 2015-09-08 fuzzywuzzy A_Requirements 1   1 2015-09-08 requests A_Requirements 1   2 2015-09-08 requests-oauthlib A_Requirements 1   3 2015-09-08 pymongo A_Requirements 1   4 2015-09-08 jsondatabase A_Requirements 1    import pycatflow as pcf # Load and parse data from file fname=\u0026#34;data/sample_data_ChatterBot_Requirements.","title":"PyCatFlow库|类别数据时间序列可视化库"},{"content":"Backtrader是用于量化回测的python框架，作者是德国人Daniel Rodriguez。相比于zipline等量化回测平台，backtrader是一个易懂、易上手的量化投资框架，今天我们试着用Backtrader进行简单的均线买入卖出量化策略回溯，即5日均线上穿20日均线，则表示股票处于强势，买入。反之，处于弱势，卖出。\n下载 点击下载本文代码数据\n安装 pip3 install backtrader \n快速入门   买入：MA5上穿MA20， 即五日价格移动平均线(MA5)和二十日价格移动平均线(MA20)， 最近处于涨势\n  卖出：MA20下穿MA5， 即五日价格移动平均线(MA5)和二十日价格移动平均线(MA20)， 最近处于涨势\n  import datetime import backtrader as bt if __name__ == \u0026#34;__main__\u0026#34;: #初始化 cerebro = bt.Cerebro() #设定初始资金 cerebro.broker.setcash(100000.0) #策略执行前的资金 print(\u0026#39;启动资金: {}\u0026#39;.format(cerebro.broker.getvalue())) #策略执行 cerebro.run() #策略执行前的资金 print(\u0026#39;启动资金: {}\u0026#39;.format(cerebro.broker.getvalue())) 启动资金: 100000.0 启动资金: 100000.0  每次股票交易，证券经纪人会收取一定的佣金，如万三（每一万元交易收三元）即0.003\ncerebro.broker.setcommission(0.003) 交易会有最小的购买/卖出份额，一般一手100股\ncerebro.addsizer(bt.sizers.FixedSize, stake=100) \n加载数据   前复权：保持当前价格不变，将历史价格进行增减，从而使股价连续。 前复权用来看盘非常方便，能一眼看出股价的历史走势，叠加各种技术指标也比较顺畅，是各种行情软件默认的复权方式。 这种方法虽然很常见，但也有两个缺陷需要注意。\n 为了保证当前价格不变，每次股票除权除息，均需要重新调整历史价格，因此其历史价格是时变的。 这会导致在不同时点看到的历史前复权价可能出现差异。 对于有持续分红的公司来说，前复权价可能出现负值。    后复权 ：保证历史价格不变，在每次股票权益事件发生后，调整当前的股票价格。 后复权价格和真实股票价格可能差别较大，不适合用来看盘。 其优点在于，可以被看作投资者的长期财富增长曲线，反映投资者的真实收益率情况。\n  在量化投资研究中普遍采用后复权数据，使用 https://github.com/mpquant/Ashare 下载的股票数据\nBacktrader将数据集称作数据流Data Feeds, 默认数据集是yahoo的股票数据，通过以下代码即可加载:\n# 创建数据 data = bt.feeds.YahooFinanceCSVData( dataname=\u0026#39;sz000725.csv\u0026#39;, datetime=0, open=1, high=2, low=3, close=4, volume=5, dtformat=(\u0026#39;%Y-%m-%d\u0026#39;), fromdate = datetime.datetime(2014, 7, 11), todate = datetime.datetime(2021, 12, 1) ) \n添加指标 backtrader中内置了许多计算值表，比如移动平滑线、MACD、RSI等等， 我们这一篇文章仅需要移动平均线MA， 设置方法如下\nself.sma5 = bt.indicators.MovingAverageSimple(self.datas[0], period=5) self.sma20 = bt.indicators.MovingAverageSimple(self.datas[0], period=20) datas[0]是第一个数据集， period是指多少天的移动平均线，比如5，则返回MA5的相关数据。\n构建策略 使用backtrader构建策略是一件很简单的事情， 继承backtrader的策略类，并重写部分方法，就能实现策略。比如\n 重写属于我们自己的log函数 均线金叉死叉策略  class TestStrategy(bt.Strategy): \u0026#34;\u0026#34;\u0026#34; 继承并构建自己的策略 \u0026#34;\u0026#34;\u0026#34; def log(self, txt, dt=None, doprint=False): \u0026#34;日志函数，用于统一输出日志格式\u0026#34; if doprint: dt = dt or self.datas[0].datetime.date(0) print(\u0026#39;{}, {}\u0026#39;.format(dt.isoformat(), txt)) def __init__(self): # 初始化相关数据 self.dataclose = self.datas[0].close self.order = None self.buyprice = None self.buycomm = None # 移动平均线初始化 self.sma5 = bt.indicators.MovingAverageSimple(self.datas[0], period=5) self.sma20 = bt.indicators.MovingAverageSimple(self.datas[0], period=20) def notify_order(self, order): \u0026#34;\u0026#34;\u0026#34; 订单状态处理 Arguments: order {object}-- 订单状态 \u0026#34;\u0026#34;\u0026#34; if order.status in [order.Submitted, order.Accepted]: # 如订单已被处理，则不用做任何事情 return # 检查订单是否完成 if order.status in [order.Completed]: if order.isbuy(): self.buyprice = order.executed.price self.buycomm = order.executed.comm self.bar_executed = len(self) # 订单因为缺少资金之类的原因被拒绝执行 elif order.status in [order.Canceled, order.Margin, order.Rejected]: self.log(\u0026#39;Order Canceled/Margin/Rejected\u0026#39;) # 订单状态处理完成，设为空 self.order = None def notify_trade(self, trade): \u0026#34;\u0026#34;\u0026#34; 交易成果 Arguments: trade {object}-- 交易状态 \u0026#34;\u0026#34;\u0026#34; if not trade.isclosed: return # 显示交易的毛利率和净利润 self.log(\u0026#39;OPERATION PROFIT, GROSS {}, NET {}\u0026#39;.format(trade.pnl, trade.pnlcomm), doprint=True) def next(self): \u0026#39;\u0026#39;\u0026#39; 下一次执行 \u0026#39;\u0026#39;\u0026#39; # 记录收盘价 self.log(\u0026#39;Close, {}\u0026#39;.format(self.dataclose[0])) # 是否正在下单，如果是的话不能提交第二次订单 if self.order: return # 是否已经买入 if not self.position: # 还没买，如果 MA5 \u0026gt; MA10 说明涨势，买入 if self.sma5[0] \u0026gt; self.sma20[0]: self.order = self.buy() else: # 已经买了，如果 MA5 \u0026lt; MA10 ，说明跌势，卖出 if self.sma5[0] \u0026lt; self.sma20[0]: self.order = self.sell() #def stop(self): #self.log(u\u0026#39;(金叉死叉有用吗) Ending Value {}\u0026#39;.format(self.broker.getvalue()), doprint=True) \n策略回测 为了验证我们开头提到的策略，咱使用了 京东方sz000725 在2014年7月11日至今2021年12月3日的股票数据，将数据命名为sz000725.csv, 我们先用pandas审查下csv\nimport pandas as pd df = pd.read_csv(\u0026#39;data/sz000725.csv\u0026#39;) df.head() | | Unnamed: 0 | date | open | high | low | close | volume | |---:|-------------:|------------:|-------:|-------:|------:|--------:|------------:| | 0 | 0 | 2.01407e+07 | 2.17 | 2.2 | 2.16 | 2.19 | 7.49341e+07 | | 1 | 1 | 2.01407e+07 | 2.18 | 2.2 | 2.17 | 2.2 | 8.10931e+07 | | 2 | 2 | 2.01407e+07 | 2.19 | 2.21 | 2.18 | 2.2 | 8.19694e+07 | | 3 | 3 | 2.01407e+07 | 2.2 | 2.21 | 2.19 | 2.21 | 7.96481e+07 | | 4 | 4 | 2.01407e+07 | 2.2 | 2.21 | 2.19 | 2.21 | 8.75106e+07 | 在backtrader中，使用GenericCSVData函数来加载csv，需要注明日期始末、open/high/low/close/volume等字段在csv中的列数(第几列，从0开始，0表示第一列)\nimport backtrader as bt import datetime if __name__ == \u0026#34;__main__\u0026#34;: # 初始化模型 cerebro = bt.Cerebro() init_cash = 100000.0 fromdate = datetime.datetime(2014, 7, 11) todate = datetime.datetime(2021, 12, 3) #构建策略 strategy = cerebro.addstrategy(TestStrategy) #每次买100股 cerebro.addsizer(bt.sizers.FixedSize, stake=100) #加载数据到模型 data = bt.feeds.GenericCSVData( dataname=\u0026#39;data/sz000725.csv\u0026#39;, fromdate=fromdate, todate=todate, dtformat=\u0026#39;%Y%m%d\u0026#39;, datetime=1, open=2, high=3, low=4, close=5, volume=6 ) cerebro.adddata(data) # 设定初始资金和佣金 cerebro.broker.setcash(init_cash) cerebro.broker.setcommission(0.003) print(\u0026#39;会不会玩了个寂寞？\u0026#39;) #策略执行前的资金 print(\u0026#39;启动资金: {}\u0026#39;.format(cerebro.broker.getvalue())) #策略执行 cerebro.run() #策略结束时的资金 print(\u0026#39;策略结束时资金: {}\u0026#39;.format(cerebro.broker.getvalue())) duration_year = (todate-fromdate).days/360 end_value = cerebro.broker.getvalue() roi = pow(end_value/init_cash, 1/duration_year)-1 print(\u0026#39;策略年华收益率: {}%\u0026#39;.format(roi*100)) 会不会玩了个寂寞？ 启动资金: 100000.0 2014-08-27, OPERATION PROFIT, GROSS -3.000000000000025, NET -4.365000000000025 2014-10-28, OPERATION PROFIT, GROSS 10.999999999999988, NET 9.568999999999988 2014-11-24, OPERATION PROFIT, GROSS -4.0000000000000036, NET -5.584000000000003 2015-01-15, OPERATION PROFIT, GROSS 52.0, NET 50.242 2015-05-08, OPERATION PROFIT, GROSS 113.00000000000003, NET 110.82500000000003 2015-07-02, OPERATION PROFIT, GROSS 25.0, NET 22.075 2015-08-25, OPERATION PROFIT, GROSS -96.0, NET -98.076 2015-11-03, OPERATION PROFIT, GROSS -8.999999999999986, NET -10.760999999999985 2015-11-30, OPERATION PROFIT, GROSS -16.000000000000014, NET -17.812000000000015 2015-12-31, OPERATION PROFIT, GROSS -8.999999999999986, NET -10.820999999999986 2016-03-14, OPERATION PROFIT, GROSS -10.999999999999988, NET -12.514999999999988 2016-04-14, OPERATION PROFIT, GROSS 0.0, NET -1.548 2016-06-16, OPERATION PROFIT, GROSS -6.000000000000005, NET -7.404000000000005 2016-07-28, OPERATION PROFIT, GROSS 0.0, NET -1.404 2016-09-08, OPERATION PROFIT, GROSS 8.000000000000007, NET 6.566000000000007 2016-12-19, OPERATION PROFIT, GROSS 31.999999999999986, NET 30.421999999999986 2017-02-10, OPERATION PROFIT, GROSS 14.000000000000012, NET 12.110000000000012 2017-02-20, OPERATION PROFIT, GROSS -4.999999999999982, NET -6.940999999999982 2017-03-06, OPERATION PROFIT, GROSS -10.999999999999988, NET -12.976999999999988 2017-05-12, OPERATION PROFIT, GROSS 44.99999999999997, NET 42.86699999999997 2017-06-01, OPERATION PROFIT, GROSS -22.00000000000002, NET -24.38200000000002 2017-07-13, OPERATION PROFIT, GROSS -14.000000000000012, NET -16.406000000000013 2017-09-18, OPERATION PROFIT, GROSS -2.9999999999999805, NET -5.31899999999998 2017-11-27, OPERATION PROFIT, GROSS 153.00000000000003, NET 150.12900000000002 2018-01-08, OPERATION PROFIT, GROSS -8.000000000000007, NET -11.360000000000007 2018-02-01, OPERATION PROFIT, GROSS 2.000000000000046, NET -1.6419999999999533 2018-03-23, OPERATION PROFIT, GROSS -25.0, NET -28.303 2018-08-07, OPERATION PROFIT, GROSS -12.00000000000001, NET -14.124000000000011 2018-09-04, OPERATION PROFIT, GROSS -27.0, NET -29.115000000000002 2018-11-26, OPERATION PROFIT, GROSS -14.000000000000012, NET -15.668000000000012 2019-01-29, OPERATION PROFIT, GROSS -4.999999999999982, NET -6.598999999999982 2019-03-15, OPERATION PROFIT, GROSS 70.00000000000001, NET 67.90000000000002 2019-04-12, OPERATION PROFIT, GROSS -15.99999999999997, NET -18.35799999999997 2019-04-29, OPERATION PROFIT, GROSS -14.999999999999991, NET -17.34299999999999 2019-06-06, OPERATION PROFIT, GROSS -5.000000000000027, NET -7.043000000000027 2019-06-19, OPERATION PROFIT, GROSS 4.999999999999982, NET 2.9509999999999823 2019-08-07, OPERATION PROFIT, GROSS 36.999999999999964, NET 34.84299999999996 2019-08-30, OPERATION PROFIT, GROSS -10.999999999999988, NET -13.282999999999987 2019-09-27, OPERATION PROFIT, GROSS -41.99999999999995, NET -44.35799999999995 2020-02-04, OPERATION PROFIT, GROSS 30.00000000000003, NET 27.67200000000003 2020-03-06, OPERATION PROFIT, GROSS 9.999999999999964, NET 7.017999999999965 2020-05-28, OPERATION PROFIT, GROSS -25.0, NET -27.301000000000002 2020-07-21, OPERATION PROFIT, GROSS 54.999999999999986, NET 52.25499999999999 2020-09-14, OPERATION PROFIT, GROSS 52.999999999999936, NET 50.002999999999936 2020-10-19, OPERATION PROFIT, GROSS -5.999999999999961, NET -8.98199999999996 2020-12-10, OPERATION PROFIT, GROSS 2.000000000000046, NET -1.083999999999954 2021-02-02, OPERATION PROFIT, GROSS 66.00000000000001, NET 62.454000000000015 2021-03-03, OPERATION PROFIT, GROSS -13.000000000000078, NET -16.67500000000008 2021-03-11, OPERATION PROFIT, GROSS -33.999999999999986, NET -37.64199999999999 2021-05-12, OPERATION PROFIT, GROSS 31.00000000000005, NET 27.12700000000005 2021-07-06, OPERATION PROFIT, GROSS -33.00000000000001, NET -36.717000000000006 2021-07-26, OPERATION PROFIT, GROSS -34.999999999999964, NET -38.70499999999996 2021-09-17, OPERATION PROFIT, GROSS -21.999999999999975, NET -25.467999999999975 2021-11-22, OPERATION PROFIT, GROSS -7.000000000000028, NET -9.979000000000028 策略结束时资金: 100120.964 策略年华收益率: 0.016108152552885002%  策略可视化 from backtrader_plotting import Bokeh from backtrader_plotting.schemes import Tradimo b = Bokeh(style=\u0026#39;bar\u0026#39;, plot_mode=\u0026#39;single\u0026#39;, scheme=Tradimo()) cerebro.plot(b)    广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/backtrader_demo/","summary":"Backtrader是用于量化回测的python框架，作者是德国人Daniel Rodriguez。相比于zipline等量化回测平台，backtrader是一个易懂、易上手的量化投资框架，今天我们试着用Backtrader进行简单的均线买入卖出量化策略回溯，即5日均线上穿20日均线，则表示股票处于强势，买入。反之，处于弱势，卖出。\n下载 点击下载本文代码数据\n安装 pip3 install backtrader \n快速入门   买入：MA5上穿MA20， 即五日价格移动平均线(MA5)和二十日价格移动平均线(MA20)， 最近处于涨势\n  卖出：MA20下穿MA5， 即五日价格移动平均线(MA5)和二十日价格移动平均线(MA20)， 最近处于涨势\n  import datetime import backtrader as bt if __name__ == \u0026#34;__main__\u0026#34;: #初始化 cerebro = bt.Cerebro() #设定初始资金 cerebro.broker.setcash(100000.0) #策略执行前的资金 print(\u0026#39;启动资金: {}\u0026#39;.format(cerebro.broker.getvalue())) #策略执行 cerebro.run() #策略执行前的资金 print(\u0026#39;启动资金: {}\u0026#39;.format(cerebro.broker.getvalue())) 启动资金: 100000.0 启动资金: 100000.0  每次股票交易，证券经纪人会收取一定的佣金，如万三（每一万元交易收三元）即0.003\ncerebro.broker.setcommission(0.003) 交易会有最小的购买/卖出份额，一般一手100股\ncerebro.addsizer(bt.sizers.FixedSize, stake=100) \n加载数据   前复权：保持当前价格不变，将历史价格进行增减，从而使股价连续。 前复权用来看盘非常方便，能一眼看出股价的历史走势，叠加各种技术指标也比较顺畅，是各种行情软件默认的复权方式。 这种方法虽然很常见，但也有两个缺陷需要注意。\n 为了保证当前价格不变，每次股票除权除息，均需要重新调整历史价格，因此其历史价格是时变的。 这会导致在不同时点看到的历史前复权价可能出现差异。 对于有持续分红的公司来说，前复权价可能出现负值。    后复权 ：保证历史价格不变，在每次股票权益事件发生后，调整当前的股票价格。 后复权价格和真实股票价格可能差别较大，不适合用来看盘。 其优点在于，可以被看作投资者的长期财富增长曲线，反映投资者的真实收益率情况。","title":"backtrader库|简单均线买入卖出策略"},{"content":"Dataspell——一个专为数据科学家制作的IDE。 一方面，您拥有 Jupyter 以实现最大的交互性，另一方面拥有 PyCharm 作为专业环境。\n什么是DataSpell？ Dataspell 是专为数据科学家制作的 IDE（集成开发环境）。 它由 Jetbrains ( IntelliJ Idea 和 PyCharm 背后的一家公司)开发，支持所有主要操作系统平台，包括 ARM (M1) Mac。\nDataSpell亮点：\n 原生notebook体验——如果你使用过 Jupyter 会有宾至如归的感觉。 智能代码辅助 - 有点像 Jupyter 和 PyCharm 生了孩子。 Markdown 和 JavaScript — 根据需要设置notebook样式并嵌入数据可视化。 交互式外壳 - REPL 控制台 + 语法检查。 SQL 数据库支持 — 连接到数据库、直观地检查数据、导入/导出数据等。  打开dataspell，很像PyCharm\n 创建你的第一个notebook 该过程几乎与任何其他 Jetbrains 产品相同。 点击根目录，进入新建——Jupyter Notebook：\n 我将其命名为 first_notebook.ipynb。\n 样式是个人喜好的东西，但你不能说设计不是一流的。\n在创建笔记本时，您可能已经看到创建 R 脚本和 Markdown 文件的选项。 一旦 Python 正常运行，Dataspell 将同时支持 R 和 Julia（来源）。\n现在让我们探索基本的 Python 和数据科学库是如何工作的。 下图显示了 Numpy 数组和 Pandas DataFrame：\n 正如预期的那样。 我喜欢 Numpy 数组打印为 Pandas 系列的方式，但这只是个人喜好。 Matplotlib 呢？ 这是一个真正简单的散点图：\n 我更喜欢出版物可视化的白色背景，但在晚上工作时眼睛无法长时间工作，夜晚我喜欢黑色背景。 干得好，Dataspell。\n接下来，让我们探索每个 IDE 必须具备的功能——智能编码辅助。\n智能编码辅助 我不喜欢 JupyterLab 的是缺乏编码帮助。 确实有插件，但它们的帮助远不及专业的 IDE。 我不喜欢 IDE 的是缺乏交互性。 希望 Dataspell 将是两全其美的。\n让我们尝试导入 Numpy，看看会发生什么：\n 支持自动代码提示——它完美无缺； 也支持代码未正确编写时的警告/错误，例如函数add_integers 应该返回一个整数，但它故意返回一个浮点数：\n 支持markdown 首先，将鼠标悬停在单元格上方并单击添加 Markdown 单元格：\n 另一种方法是单击代码下拉列表并将单元格类型切换为 Markdown。\n下面是一些 Markdown 代码供您尝试：\n 这是运行单元格时的样子：\n 支持数据库 作为数据科学家，您几乎每天都必须建立和管理数据库连接。 当 SQL 写成字符串时，调试很长的 SQL 行并不容易。 Dataspell 有一个解决方案。以下是使用 sqlite3 包建立连接、创建数据库和表的方法：\n 您现在可以像往常一样插入数据：\n Dataspell 的特别之处在于无需离开 IDE 或编写不必要的查询即可浏览数据库的选项。 只需双击数据库，就会打开一个新的侧窗口。从那里，您可以轻松单击任何感兴趣的表格，在单独的选项卡中浏览它：\n \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/data_spell/","summary":"Dataspell——一个专为数据科学家制作的IDE。 一方面，您拥有 Jupyter 以实现最大的交互性，另一方面拥有 PyCharm 作为专业环境。\n什么是DataSpell？ Dataspell 是专为数据科学家制作的 IDE（集成开发环境）。 它由 Jetbrains ( IntelliJ Idea 和 PyCharm 背后的一家公司)开发，支持所有主要操作系统平台，包括 ARM (M1) Mac。\nDataSpell亮点：\n 原生notebook体验——如果你使用过 Jupyter 会有宾至如归的感觉。 智能代码辅助 - 有点像 Jupyter 和 PyCharm 生了孩子。 Markdown 和 JavaScript — 根据需要设置notebook样式并嵌入数据可视化。 交互式外壳 - REPL 控制台 + 语法检查。 SQL 数据库支持 — 连接到数据库、直观地检查数据、导入/导出数据等。  打开dataspell，很像PyCharm\n 创建你的第一个notebook 该过程几乎与任何其他 Jetbrains 产品相同。 点击根目录，进入新建——Jupyter Notebook：\n 我将其命名为 first_notebook.ipynb。\n 样式是个人喜好的东西，但你不能说设计不是一流的。\n在创建笔记本时，您可能已经看到创建 R 脚本和 Markdown 文件的选项。 一旦 Python 正常运行，Dataspell 将同时支持 R 和 Julia（来源）。","title":"DataSpell数据挖掘编辑器"},{"content":" 点击上方图片购买课程    参考自阮一峰技术周刊\nsrc:https://github.com/ruanyf/weekly\n freepn https://www.freepn.org/ 免费点对点的科学上网 {{ \u0026lt; figure src=\u0026ldquo;img/freepn.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} Sharkle 点击圆圈，随机打开一个有趣的网站。\n{{ \u0026lt; figure src=\u0026ldquo;img/Sharkle.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\n免费的 Python 书籍 https://github.com/pamoroso/free-python-books\n这个仓库收集网上的 Python 免费书籍（英文）。\nCharts.css https://chartscss.org/\n一个只使用 CSS 的图形库（不需要 JS），可以把\u0026lt;table\u0026gt;标签变成折线图、直方图或其他图形。\n{{ \u0026lt; figure src=\u0026ldquo;img/charts.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nfocalboard https://www.focalboard.com/\n桌面的看板软件，类似 Trello，可以用来管理待办事项。\n{{ \u0026lt; figure src=\u0026ldquo;img/focalboard.gif\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nbuzzing https://www.buzzing.cc/\n用中文浏览国外社交媒体里的热门讨论。母语快速导读， 感兴趣再进原文深度阅读\n数据显示：使用母语导读比直接浏览英文导读更容易快速找到感兴趣的内容，继而有效提升英文阅读量。\n{{ \u0026lt; figure src=\u0026ldquo;img/buzzing.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nray https://ray.so/\n可以生成漂亮的代码截图，颜色、阴影、背景都可以调整。\n{{ \u0026lt; figure src=\u0026ldquo;img/raycode.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nArchivy https://github.com/archivy/archivy/\n开源的知识库软件，可以自己本地架设，管理个人笔记。\nmultiavatar https://multiavatar.com/\n{{ \u0026lt; figure src=\u0026ldquo;img/multiavatar.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} 一个在线工具，可以根据输入的文字，生成用户头像图片（ avatar）。\nbest-resume-ever https://github.com/salomonelli/best-resume-ever\n基于 Web 的简历模板，可以生成网页简历，然后用浏览器打印成 PDF 文件。\n配置好该项目后，可以一键同时生成多种风格的简历 {{ \u0026lt; figure src=\u0026ldquo;img/best-resume-ever.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\npersonal-management-system https://github.com/Volmarg/personal-management-system\n一个 Web 开源应用，可以自己搭建服务，将个人信息（笔记、代码事项、密码、日程安排、联系人等等）放在一个地方管理。\n{{ \u0026lt; figure src=\u0026ldquo;img/personalmanagement.jpeg\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nLinkAce https://github.com/Kovah/LinkAce/ 自托管的网络书签应用。 {{ \u0026lt; figure src=\u0026ldquo;img/linkace.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\npap.er https://paper.meiyuan.in/ 一个 Mac 电脑的桌面应用，每天获取来自全球的新鲜精美壁纸。 {{ \u0026lt; figure src=\u0026ldquo;img/paper.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nSVG Favicon Maker https://formito.com/tools/favicon\n在线制作 SVG 格式的 Favicon {{ \u0026lt; figure src=\u0026ldquo;img/svgmaker.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\ncodedmails https://codedmails.com/\n提供邮件html模板下载 {{ \u0026lt; figure src=\u0026ldquo;img/codedmails.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\n##了解课程\n 点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly04/","summary":"点击上方图片购买课程    参考自阮一峰技术周刊\nsrc:https://github.com/ruanyf/weekly\n freepn https://www.freepn.org/ 免费点对点的科学上网 {{ \u0026lt; figure src=\u0026ldquo;img/freepn.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}} Sharkle 点击圆圈，随机打开一个有趣的网站。\n{{ \u0026lt; figure src=\u0026ldquo;img/Sharkle.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\n免费的 Python 书籍 https://github.com/pamoroso/free-python-books\n这个仓库收集网上的 Python 免费书籍（英文）。\nCharts.css https://chartscss.org/\n一个只使用 CSS 的图形库（不需要 JS），可以把\u0026lt;table\u0026gt;标签变成折线图、直方图或其他图形。\n{{ \u0026lt; figure src=\u0026ldquo;img/charts.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nfocalboard https://www.focalboard.com/\n桌面的看板软件，类似 Trello，可以用来管理待办事项。\n{{ \u0026lt; figure src=\u0026ldquo;img/focalboard.gif\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nbuzzing https://www.buzzing.cc/\n用中文浏览国外社交媒体里的热门讨论。母语快速导读， 感兴趣再进原文深度阅读\n数据显示：使用母语导读比直接浏览英文导读更容易快速找到感兴趣的内容，继而有效提升英文阅读量。\n{{ \u0026lt; figure src=\u0026ldquo;img/buzzing.png\u0026rdquo; width=\u0026ldquo;800\u0026rdquo; \u0026gt;}}\nray https://ray.so/\n可以生成漂亮的代码截图，颜色、阴影、背景都可以调整。\n{{ \u0026lt; figure src=\u0026ldquo;img/raycode.","title":"TechWeekly-04 | 每周有趣有用的技术分享"},{"content":"最近在google搜Python在经管中的内容，意外发现专著： 在会计研究中使用Python进行文本分析，内容特别新，专著中含有Python代码，也有会计领域文本分析的应用成果。跟 视频专栏课| Python网络爬虫文本分析 结合起来，特别适合会计领域python初学者，将文本分析应用于会计研究中。\n Vic Anand, Khrystyna Bochkay, Roman Chychyla and Andrew Leone (2020 isbn), “Using Python for Text Analysis in Accounting Research (forthcoming)”, Foundations and Trends ® in Accounting: Vol. xx, No. xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.\n http://dx.doi.org/10.1561/1400000062\n摘要 会计研究中文本数据的重要性显着增加。为了帮助研究人员理解和使用文本数据，本专著定义和描述了文本数据的常用度量，然后演示了使用 Python 编程语言收集和处理文本数据。该专著充满了示例代码，这些代码复现了最近研究论文中的文本分析任务。\n在专著的第一部分，我们提供了 Python 入门指南。我们首先描述 Anaconda，它是 Python 的一个发行版，它提供了文本分析所需的库及其安装。然后，我们介绍了 Jupyter notebook，这是一种改进研究工作流程并促进可复制研究的编程环境。接下来，我们将教授 Python 编程的基础知识，并演示使用 Pandas 包中的表格数据的基础知识。\n专著的第二部分侧重于会计研究中常用的具体文本分析方法和技术。我们首先介绍正则表达式，这是一种用于在文本中查找模式的复杂语言。然后我们将展示如何使用正则表达式从文本中提取特定部分。接下来，我们介绍将文本数据（非结构化数据）转换为表示感兴趣变量（结构化数据）的数值度量的想法。具体来说，我们介绍了基于字典的方法\n 测量文档情绪， 计算文本复杂度， 识别前瞻性句子和风险披露， 收集文本中的信息量，以及 计算不同文本片段的相似度。  对于这些任务中的每一个，我们引用相关论文并提供代码片段来实现这些论文中的相关指标。\n最后，专著的第三部分侧重于自动化文本数据的收集。我们介绍了网页抓取并提供了从 EDGAR 下载文件的代码。\n关键词 文本分析，数据收集，Python，自然语言处理\nUsing Python for Text Analysis in Accounting Research (forthcoming)目录 1. 引言 2. 在电脑中配置Python  2.1 Python包的作用 2.2 Anaconda软件版本 2.3 安装Anaconda 2.4 Anaconda的使用  3. Jupyter Notebook  3.1 案例 JupyterLab: Jupyter Notebook的开发版(最新版) 如何启动JupyterLab 在JupyterLab中写代码 Markdown标记语言与格式化文本代码块  4. Python编程语言简要介绍  4.1 基础知识 4.2 变量与数据类型 4.3 操作 4.4 print函数 4.5 控制流 4.6 函数 4.7 集合类型数据-list、tuple、dictionaries 4.8 处理字符串  5. 处理表数据： Pandas包  5.1 Pandas使用场景 5.2 导入import 声明 5.3 加载数据、导出数据 5.4 在pandas中查看数据 5.5 筛选数据 5.6 创建新列（字段） 5.7 删除列（字段）、列（字段）名重命名 5.8 对数据排序 5.9 合并数据  6 正则表达式介绍  6.1 查看文本中的模式 6.2 字符与字符集 6.3 Regex的定位与边界 6.4 模式匹配次数限定 6.5 分组 \u0026hellip;  7. 基于字典法 的文本分析  7.1 字典法文本分析的优势 7.2 理解字典 7.3 识别文本中的词语与句子 7.4 词干化、词形还原 7.5 词语权重 7.6 基于词典法的词频统计函数  8. 量化文本复杂度  8.1 理解文本复杂度 8.2 计算文本字符长度 8.3 使用Fog指数测量文本可读性 8.4 使用BOG指数测量文本可读性  9. 句子结构与分类  9.1 识别前瞻性陈述forward-looking sentences 9.2 使用字典法做文本分类 9.3 识别句子的主语与宾语 9.4 识别命名实体 9.5 词性标注与命名实体识别任务  10. 测量文本相似度  10.1 使用相似度比较文本 10.2 长文本使用cosine相似度计算相似度 10.3 短文本使用Levenshtein距离计算相似度 10.4 使用word2vec词嵌入计算语义相似度  11. 识别文本中的具体信息  11.1 文本识别与抽取 11.2 案例: 从10-k filing中提取出MD\u0026amp;A 11.3 案例: 从10-k html网页文件中提取处MD\u0026amp;A 11.4 从XBRL金融报告中抽取文本  12. 从网络中收集数据  12.1 在互联网中采集数据 12.2 证券交易委员会的EDGAR数据 12.3 网络爬虫 12.4 关于api接口  致谢 参考文献(部分)  Bentley, J. W., T. E. Christensen, K. H. Gee, and B. C. Whipple. 2018. “Disentangling managers’ and analysts’ non-GAAP reporting”. Journal of Accounting Research. 56(4): 1039–1081.\nBlankespoor, E. 2019. “The impact of information processing costs on ﬁrm disclosure choice: Evidence from the XBRL mandate”. Journal of Accounting Research. 57(4): 919–967.\nBochkay, K., R. Chychyla, and D. Nanda. 2019. “Dynamics of CEO disclosure style”. The Accounting Review. 94(4): 103–140.\nBochkay, K., J. Hales, and S. Chava. 2020. “Hyperbole or reality? Investor response to extreme language in earnings conference calls”. The Accounting Review. 95(2): 31–60.\nBochkay, K. and C. B. Levine. 2019. “Using MD\u0026amp;A to improve earnings forecasts”. Journal of Accounting, Auditing \u0026amp; Finance. 34(3): 458482.\nBonsall, S. B., A. J. Leone, B. P. Miller, and K. Rennekamp. 2017. “A plain English measure of ﬁnancial reporting readability”. Journal of Accounting and Economics. 63(2): 329–357.\nBozanic, Z., D. T. Roulstone, and A. Van Buskirk. 2018. “Management earnings forecasts and other forward-looking statements”. Journal of Accounting and Economics. 65(1): 1–20.\nChychyla, R., A. J. Leone, and M. Minutti-Meza. 2019. “Complexity of ﬁnancial reporting standards and accounting expertise”. Journal of Accounting and Economics. 67(1): 226–253.\nGow, I. D., D. F. Larcker, and A. A. Zakolyukina. 2019. “Non-answers during conference calls”. Chicago Booth Research Paper. (19-01). Guay, W., D. Samuels, and D. Taylor. 2016. “Guiding through the Fog:Financial statement complexity and voluntary disclosure”. Journal of Accounting and Economics. 62(2): 234–269.\nHeitmann, M., C. Siebert, J. Hartmann, and C. Schamp. 2020. “More Than a Feeling: Benchmarks for Sentiment Analysis Accuracy”. Working Paper, https://papers.ssrn.com/sol3/papers.cfm?abstract_ id=3489963.\n 本书下载 https://github.com/hiDaDeng/DaDengAndHisPython/blob/master/Using_Python_For_Text_Analysis_In_Accounting_Research.pdf\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/text_mining_in_accouting_research/","summary":"最近在google搜Python在经管中的内容，意外发现专著： 在会计研究中使用Python进行文本分析，内容特别新，专著中含有Python代码，也有会计领域文本分析的应用成果。跟 视频专栏课| Python网络爬虫文本分析 结合起来，特别适合会计领域python初学者，将文本分析应用于会计研究中。\n Vic Anand, Khrystyna Bochkay, Roman Chychyla and Andrew Leone (2020 isbn), “Using Python for Text Analysis in Accounting Research (forthcoming)”, Foundations and Trends ® in Accounting: Vol. xx, No. xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.\n http://dx.doi.org/10.1561/1400000062\n摘要 会计研究中文本数据的重要性显着增加。为了帮助研究人员理解和使用文本数据，本专著定义和描述了文本数据的常用度量，然后演示了使用 Python 编程语言收集和处理文本数据。该专著充满了示例代码，这些代码复现了最近研究论文中的文本分析任务。\n在专著的第一部分，我们提供了 Python 入门指南。我们首先描述 Anaconda，它是 Python 的一个发行版，它提供了文本分析所需的库及其安装。然后，我们介绍了 Jupyter notebook，这是一种改进研究工作流程并促进可复制研究的编程环境。接下来，我们将教授 Python 编程的基础知识，并演示使用 Pandas 包中的表格数据的基础知识。\n专著的第二部分侧重于会计研究中常用的具体文本分析方法和技术。我们首先介绍正则表达式，这是一种用于在文本中查找模式的复杂语言。然后我们将展示如何使用正则表达式从文本中提取特定部分。接下来，我们介绍将文本数据（非结构化数据）转换为表示感兴趣变量（结构化数据）的数值度量的想法。具体来说，我们介绍了基于字典的方法\n 测量文档情绪， 计算文本复杂度， 识别前瞻性句子和风险披露， 收集文本中的信息量，以及 计算不同文本片段的相似度。  对于这些任务中的每一个，我们引用相关论文并提供代码片段来实现这些论文中的相关指标。\n最后，专著的第三部分侧重于自动化文本数据的收集。我们介绍了网页抓取并提供了从 EDGAR 下载文件的代码。","title":"在会计研究中使用Python进行文本分析"},{"content":"多媒体文件的下载包括\n 图片 音频 视频 文件  代码非常简单，只要准备好多媒体文件链接url和存储路径file即可，代码如下\nimport requests def download(url, file): \u0026#34;\u0026#34;\u0026#34; 下载多媒体及文件 url： 多媒体文件链接（结尾有文件格式名） file: 存储文件的路径（结尾有文件格式名） \u0026#34;\u0026#34;\u0026#34; resp = requests.get(url) #获取到二进制数据 binarydata = resp.content #以二进制形式将数据流存入fname中 with open(file, \u0026#39;wb\u0026#39;) as f: f.write(binarydata) 案例数据 视频由于体积太大未能上传至我的博客服务器，我为大家准备了音频、pdf文件、图片文件三种数据类型。其实不论什么类型，只要是文件，均可使用上面的download函数下载。\npics = [\u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/about-page.jpg\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/about-us.jpg\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/AI_Knowledge_intro.png\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/banner-1.jpg\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/blog-post-1.jpg\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/blog-post-2.jpg\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/blog-post-3.jpg\u0026#39;] pdfs=[\u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/网络爬虫.pdf\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/相似度.pdf\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/中文可读性.pdf\u0026#39;] musics=[\u0026#39;https://hidadeng.github.io/blog/multimediaexamples/music/Elastic.mp3\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/music/HAPPYBEL.WAV\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/music/MU14.WAV\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/music/typing.wav\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/music/Water.mp3\u0026#39;] \n批量下载 以pdf为例\nimport requests pdfs=[\u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/网络爬虫.pdf\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/相似度.pdf\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/中文可读性.pdf\u0026#39;] def download(url, file): resp = requests.get(url) binarydata = resp.content with open(file, \u0026#39;wb\u0026#39;) as f: f.write(binarydata) for link in pdfs: #注意，代码运行前所在文件夹内新建一个data文件夹 filepath=\u0026#39;data/\u0026#39;+url.spit(\u0026#39;/\u0026#39;)[-1] download(url=link, file=filepath) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/multi_media_examples/","summary":"多媒体文件的下载包括\n 图片 音频 视频 文件  代码非常简单，只要准备好多媒体文件链接url和存储路径file即可，代码如下\nimport requests def download(url, file): \u0026#34;\u0026#34;\u0026#34; 下载多媒体及文件 url： 多媒体文件链接（结尾有文件格式名） file: 存储文件的路径（结尾有文件格式名） \u0026#34;\u0026#34;\u0026#34; resp = requests.get(url) #获取到二进制数据 binarydata = resp.content #以二进制形式将数据流存入fname中 with open(file, \u0026#39;wb\u0026#39;) as f: f.write(binarydata) 案例数据 视频由于体积太大未能上传至我的博客服务器，我为大家准备了音频、pdf文件、图片文件三种数据类型。其实不论什么类型，只要是文件，均可使用上面的download函数下载。\npics = [\u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/about-page.jpg\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/about-us.jpg\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/AI_Knowledge_intro.png\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/banner-1.jpg\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/blog-post-1.jpg\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/blog-post-2.jpg\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pic/blog-post-3.jpg\u0026#39;] pdfs=[\u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/网络爬虫.pdf\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/相似度.pdf\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/中文可读性.pdf\u0026#39;] musics=[\u0026#39;https://hidadeng.github.io/blog/multimediaexamples/music/Elastic.mp3\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/music/HAPPYBEL.WAV\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/music/MU14.WAV\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/music/typing.wav\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/music/Water.mp3\u0026#39;] \n批量下载 以pdf为例\nimport requests pdfs=[\u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/网络爬虫.pdf\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/相似度.pdf\u0026#39;, \u0026#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/中文可读性.pdf\u0026#39;] def download(url, file): resp = requests.get(url) binarydata = resp.content with open(file, \u0026#39;wb\u0026#39;) as f: f.","title":"使用Python采集多媒体文件数据"},{"content":"  official docs\n  github\n  pyjanitor是参照R语言janitor包语法，为Python量身定制的数据清洗包,即可清洗数据，又可让代码简洁干净。\n代码下载 click to download the code\n安装 !pip3 install pyjanitor \n为什么用janitor？ 数据预处理通常由一系列步骤组成，这些步骤涉及将原始数据转换为可理解/可用的格式。这一系列的步骤需要按照一定的顺序运行才能成功。我们以基础数据文件为起点，对其执行操作，例如删除空行/空行、用其他值替换它们、添加/重命名/删除数据列、过滤行等。更正式地说，这些步骤以及它们的关系和依赖关系通常被称为有向无环图 (DAG)。\npandas API 对 Python 数据科学生态系统非常宝贵，它实现了方法子集的方法链作为 API 的一部分。例如，重置索引 (.reset_index())、删除空值 (.dropna()) 等都是通过适当的 pd.DataFrame 方法调用来完成的。\n受 R 统计语言生态系统 dplyr 包的易用性和表达能力的启发，我们将 pyjanitor 开发为语法包，用于为 Pandas 用户表达数据处理 DAG。\n为了实现这一点，我们需要调用声明式的操作 替换为允许 逻辑顺序的方法链。让我们看看下面带注释的示例。首先，这里是数据清理路径的文字描述：\n 创建一个dataframe。 删除一列。 删除两个特定列中具有空值的行。 重命名另外两列。 添加一个新列。  让我们导入一些库并从本示例的一些示例数据开始：\n# Libraries import numpy as np import pandas as pd import janitor # Sample Data curated for this example company_sales = { \u0026#39;SalesMonth\u0026#39;: [\u0026#39;Jan\u0026#39;, \u0026#39;Feb\u0026#39;, \u0026#39;Mar\u0026#39;, \u0026#39;April\u0026#39;], \u0026#39;Company1\u0026#39;: [150.0, 200.0, 300.0, 400.0], \u0026#39;Company2\u0026#39;: [180.0, 250.0, np.nan, 500.0], \u0026#39;Company3\u0026#39;: [400.0, 500.0, 600.0, 675.0] } \n常见的Pandas实现方式 下面是传统的Pandas方式\n# The Pandas Way # 1. 创建一个dataframe df = pd.DataFrame(company_sales) # 2.删除一列。 Say \u0026#39;Company1\u0026#39; del df[\u0026#39;Company1\u0026#39;] # 3. 删除两个特定列中具有空值的行。 \u0026#39;Company2\u0026#39; and \u0026#39;Company3\u0026#39; df = df.dropna(subset=[\u0026#39;Company2\u0026#39;, \u0026#39;Company3\u0026#39;]) # 4. 重命名另外两列。 将\u0026#39;Company2\u0026#39; 改为 \u0026#39;Amazon\u0026#39;； 将 \u0026#39;Company3\u0026#39; 改为 \u0026#39;Facebook\u0026#39; df = df.rename({\u0026#39;Company2\u0026#39;: \u0026#39;Amazon\u0026#39;, \u0026#39;Company3\u0026#39;: \u0026#39;Facebook\u0026#39;}, axis=1) # 5. 添加一个新列 \u0026#39;Google\u0026#39; df[\u0026#39;Google\u0026#39;] = [450.0, 550.0, 800.0] df | | SalesMonth | Amazon | Facebook | Google | | ---: | :--------- | -----: | -------: | -----: | | 0 | Jan | 180 | 400 | 450 | | 1 | Feb | 250 | 500 | 550 | | 3 | April | 500 | 675 | 800 | \n稍微高级一点Pandas实现方式 稍微高级一点的用户可能会利用函数式 API：\ndf = ( pd.DataFrame(company_sales) .drop(columns=\u0026#34;Company1\u0026#34;) .dropna(subset=[\u0026#34;Company2\u0026#34;, \u0026#34;Company3\u0026#34;]) .rename(columns={\u0026#34;Company2\u0026#34;: \u0026#34;Amazon\u0026#34;, \u0026#34;Company3\u0026#34;:\u0026#34;Facebook\u0026#34;}) .assign(Google=[450.0, 550.0, 800.0]) ) df | | SalesMonth | Amazon | Facebook | Google | | ---: | :--------- | -----: | -------: | -----: | | 0 | Jan | 180 | 400 | 450 | | 1 | Feb | 250 | 500 | 550 | | 3 | April | 500 | 675 | 800 | \nPyJanitor实现方式 借助pyjanitor库，我们可以使用方法名链式代码\ndf = ( pd.DataFrame(company_sales) .remove_columns([\u0026#39;Company1\u0026#39;]) .dropna(subset=[\u0026#34;Company2\u0026#34;, \u0026#34;Company3\u0026#34;]) .rename_column(\u0026#34;Company2\u0026#34;, \u0026#34;Amazon\u0026#34;) .rename_column(\u0026#34;Company3\u0026#34;, \u0026#34;Facebook\u0026#34;) .add_column(\u0026#34;Google\u0026#34;, [450.0, 550.0, 800.0]) ) df | | SalesMonth | Amazon | Facebook | Google | | ---: | :--------- | -----: | -------: | -----: | | 0 | Jan | 180 | 400 | 450 | | 1 | Feb | 250 | 500 | 550 | | 3 | April | 500 | 675 | 800 | 因此，pyjanitor 的词源与“清洁度”有双重关系。 首先，它是关于使用方便的数据清理例程扩展 Pandas。 其次，它是关于为常见的 Pandas 例程提供更清晰、方法链接、基于动词的 API。\npyjanitor更多功能  清理列名（多索引是可能的！） 删除空行和列 识别重复条目 将列编码为分类 将数据拆分为特征和目标（用于机器学习） 添加、删除和重命名列 将多列合并为一列 日期转换（从 matlab、excel、unix）到 Python 日期时间格式 将具有分隔的分类值的单个列扩展为虚拟编码变量 基于分隔符连接和分离列 用于根据列上的查询过滤数据框的语法糖 金融、生物、化学、工程和 pyspark 的实验子模块  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pyjanitor_tutorial/","summary":"official docs\n  github\n  pyjanitor是参照R语言janitor包语法，为Python量身定制的数据清洗包,即可清洗数据，又可让代码简洁干净。\n代码下载 click to download the code\n安装 !pip3 install pyjanitor \n为什么用janitor？ 数据预处理通常由一系列步骤组成，这些步骤涉及将原始数据转换为可理解/可用的格式。这一系列的步骤需要按照一定的顺序运行才能成功。我们以基础数据文件为起点，对其执行操作，例如删除空行/空行、用其他值替换它们、添加/重命名/删除数据列、过滤行等。更正式地说，这些步骤以及它们的关系和依赖关系通常被称为有向无环图 (DAG)。\npandas API 对 Python 数据科学生态系统非常宝贵，它实现了方法子集的方法链作为 API 的一部分。例如，重置索引 (.reset_index())、删除空值 (.dropna()) 等都是通过适当的 pd.DataFrame 方法调用来完成的。\n受 R 统计语言生态系统 dplyr 包的易用性和表达能力的启发，我们将 pyjanitor 开发为语法包，用于为 Pandas 用户表达数据处理 DAG。\n为了实现这一点，我们需要调用声明式的操作 替换为允许 逻辑顺序的方法链。让我们看看下面带注释的示例。首先，这里是数据清理路径的文字描述：\n 创建一个dataframe。 删除一列。 删除两个特定列中具有空值的行。 重命名另外两列。 添加一个新列。  让我们导入一些库并从本示例的一些示例数据开始：\n# Libraries import numpy as np import pandas as pd import janitor # Sample Data curated for this example company_sales = { \u0026#39;SalesMonth\u0026#39;: [\u0026#39;Jan\u0026#39;, \u0026#39;Feb\u0026#39;, \u0026#39;Mar\u0026#39;, \u0026#39;April\u0026#39;], \u0026#39;Company1\u0026#39;: [150.","title":"pyjanitor数据分析清洁包"},{"content":"这是北京语言大学智能计算机辅助语言学习（ICALL）研究组维护的文本可读性阅读清单。\n   目录     1. 综述   2. 相关研究   2.1 中文可读性   2.2 其他语言可读性   3. 可读性分析工具   4 中文数据    1. 综述   Klare, G. R. (1974–1975). Assessing readability. Reading Research Quarterly.\n  吴思远, 蔡建永, 于东, 江新. 2018. 文本可读性的自动分析研究综述. 中文信息学报.\n  郭凯、金檀、陆小飞. 2018. 文本难度调控的研究与实践——从可读公式、多维特征到智能改编. 外语测试与教学.\n  2. Related Task 2.1 Research on Chinese Readability   Yao-Ting Sung, Tao Hsing Chang. 2016. CRIE: An automated analyzer for Chinese texts. Behavior Research Methods.\n  Yao-Ting Sung, Weic Lin, SB Dyson, Kuoen Chang. 2015. Leveling L2 Texts Through Readability: Combining Multilevel Linguistic Features with the CEFR. *The Modern Language Journal.\n  LAU Tak Pang. 2006. Chinese Readability Analysis and its Applications on the Internet. Master\u0026rsquo;s thesis, The Chinese University of Hong Kong.\n  Yu Qiaona. 2016.Defining and Assessing Chinese Syntactic Complexity via TC-Units. Doctor\u0026rsquo;s thesis, University of Hawaii at Manoa.\n  2.2 Research on Readability in Other Languages   Arthur C. Graesser, Danielle S. McNamara. 2004. Coh-Metrix: Analysis of text on cohesion and language. Behavior Research Methods, Instruments, \u0026amp; Computers.\n  Arthur C. Graesser, Danielle S. McNamara. 2011. Coh-Metrix: Providing multilevel analysis of text characteristic. Educational Researcher.\n  Xiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics.\n  Xiaofei Lu. 2013. A corpus-based comparison of syntactic complexity in NNS and NS university students’ writing. Automatic Treatment and Analysis of Learner Corpus Data\n  陆小飞, 许琪. 2016. 二语句法复杂度分析器及其在二语写作研究中的应用. 外语教学与研究\n  Xiaofei Lu. 2017. Automated measurement of syntactic complexity in corpus-based L2 writing research and implications for writing assessment. Language Testing. Language Testing\n  Jin, T., Lu, X., \u0026amp; Ni, J. (2020). Syntactic complexity in adapted teaching materials: Differences among grade levels and implications for benchmarking. The Modern Language Journal\n  Menglin Xia ,Ekaterina Kochmar ,Ted Briscoe. 2016. Text Readability Assessment for Second Language Learners. Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications\n  Xiaobin Chen, Detmar Meurers. 2016. CTAP: A Web-Based Tool Supporting Automatic Complexity Analysis. Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity.\n  Chen, X. 2018. Automatic Analysis of Linguistic Complexity and Its Application in Language Learning Research, PhD thesis in computational linguistics, Eberhard Karls Universität Tübingen.\n  Nadezda Okinina, Jennifer-Carmen Frey. CTAP for Italian: Integrating Components for the Analysis of Italian into a Multilingual Linguistic Complexity Analysis Tool.\n  Zarah Weiss, Z. 2017. Using Measures of Linguistic Complexity to Assess German L2 Proficiency in Learner Corpora under Consideration of Task-Effects. M.A. Thesis in Computational Linguistics.\n  Weiss Z, Meurers D. 2019. Broad Linguistic Modeling is Beneficial for German L2 Proficiency Assessment. Widening the Scope of Learner Corpus Research, Selected Papers from the Fourth Learner Corpus Research Conference.\n  S Tonelli, KT Manh, E Pianta. 2012. Making readability indices readable. Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations.\n  Lijun Feng. 2010. Automatic Readability Assessment. *Doctor\u0026rsquo;s thesis, City University of New York.\n  3. Readability Analysis Tools   Lu Xiaofei (2010). Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics. (Web-based L2 Syntactical Complexity Analyzer (L2SCA))\n  Yao-Ting Sung, Tao Hsing Chang. 2016. CRIE: An automated analyzer for Chinese texts. Behavior Research Methods. (文本可读性指标自动化分析系统(Chinese Readability Index Explorer, CRIE))\n  Arthur C. Graesser, Danielle S. McNamara . 2011. Coh-Metrix: Providing multilevel analysis of text characteristic. Educational Researcher. (中文文本自动化分析系统: Coh-Metrix)\n  Xiaobin Chen, Detmar Meurers. 2016. CTAP: A Web-Based Tool Supporting Automatic Complexity Analysis. Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC)). (CTAP)\n  金檀、陆小飞、郭凯、李百川. 2018. Eng-Editor: An online English text evaluation and adaptation system. 广州：语言数据网(languagedata.net/tester). ( 英语阅读分级指难针 )\n  4. Chinese Data Resources  汉语词法难度分级表 汉语句法难度分级表 国际汉语教师语法教学手册 国际汉语教师中级语法教学手册  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/text_readability/","summary":"这是北京语言大学智能计算机辅助语言学习（ICALL）研究组维护的文本可读性阅读清单。\n   目录     1. 综述   2. 相关研究   2.1 中文可读性   2.2 其他语言可读性   3. 可读性分析工具   4 中文数据    1. 综述   Klare, G. R. (1974–1975). Assessing readability. Reading Research Quarterly.\n  吴思远, 蔡建永, 于东, 江新. 2018. 文本可读性的自动分析研究综述. 中文信息学报.\n  郭凯、金檀、陆小飞. 2018. 文本难度调控的研究与实践——从可读公式、多维特征到智能改编. 外语测试与教学.\n  2. Related Task 2.1 Research on Chinese Readability   Yao-Ting Sung, Tao Hsing Chang.","title":"文本可读性研究及应用清单"},{"content":"Python 科学可视化领域是巨大的，由无数工具组成，从最通用和最广泛使用的工具到更专业和机密的工具。其中一些工具是基于社区的，而另一些则是由公司开发的。有些是专门为 Web 制作的，有些仅适用于桌面，有些处理 3D 和大数据，而有些则针对完美的 2D 渲染。\n在这个图景中，Matplotlib 有着非常特别的地方。\n 它是一个多功能且功能强大的库，可让您设计非常高质量的图形，适用于科学出版。 它还提供了一个简单直观的界面以及一个面向对象的架构，允许您调整图形中的任何内容。 最后，它可以用作常规图形库以设计非科学图形。  本书章节四个部分   第一部分 Matplotlib 库的基本原理。\n这包括回顾构成图形的不同部分、不同的坐标系、可用的比例和投影，我们还将介绍一些与排版和颜色相关的概念。\n  第二部分 图形涉及实践。\n在介绍了一些生成更好图形的简单规则之后，我们将继续解释 Matplotlib 默认值和样式系统，然后再深入研究图形布局组织。然后我们将探索可用的不同类型的情节，看看如何用不同的元素装饰一个人物。\n  第三部分 更高级的概念\n即 3D 图形、优化和动画。第四部分也是最后一部分是展示集合。\n  美图展览     案例代码  import numpy as np import matplotlib.pyplot as plt from matplotlib.textpath import TextPath from matplotlib.patches import PathPatch from matplotlib.collections import PolyCollection from matplotlib.font_manager import FontProperties from matplotlib import font_manager as fm, rcParams import matplotlib.pyplot as plt fig, ax = plt.subplots() #更改字体，支持中文。 prop = FontProperties(fname=\u0026#39;fonts/Alibaba-PuHuiTi-Bold.otf\u0026#39;, weight=100) red = np.array([233, 77, 85, 255]) / 255 darkred = np.array([130, 60, 71, 255]) / 255 fig = plt.figure(figsize=(14.8 / 2.54, 21 / 2.54)) ax = fig.add_axes([0, 0, 1, 1], aspect=1, xlim=[-10, 10], ylim=[-14.2, 14.2]) ax.axis(\u0026#34;off\u0026#34;) # Text path path = TextPath((0, 0), \u0026#34;MATPLOTLIB库\u0026#34;, size=2, prop=prop) # Text centering V = path.vertices xmin, xmax = V[:, 0].min(), V[:, 0].max() ymin, ymax = V[:, 1].min(), V[:, 1].max() V -= (xmin + xmax) / 2 + 1, (ymin + ymax) / 2 # Compute shadow by iterating over text path segments polys = [] for (point, code) in path.iter_segments(curves=False): if code == path.MOVETO: points = [point] elif code == path.LINETO: points.append(point) elif code == path.CLOSEPOLY: points.append(points[0]) points = np.array(points) for i in range(len(points) - 1): p0, p1 = points[i], points[i + 1] polys.append([p0, p1, p1 + (+20, -20), p0 + (+20, -20)]) # Display shadow collection = PolyCollection( polys, closed=True, linewidth=0.0, facecolor=darkred, zorder=-10 ) ax.add_collection(collection) # Display text patch = PathPatch(path, facecolor=\u0026#34;white\u0026#34;, edgecolor=\u0026#34;none\u0026#34;, zorder=10) ax.add_artist(patch) # Transparent gradient to fade out shadow I = np.zeros((200, 1, 4)) + red ax.imshow(I, extent=[-11, 11, -15, 15], zorder=-20, clip_on=False) I[:, 0, 3] = np.linspace(0, 1, len(I)) ax.imshow(I, extent=[-11, 11, -15, 15], zorder=0, clip_on=False) ax.text( 6.5, -1.75, \u0026#34;一个多功能的科学可视化库\u0026#34;, color=\u0026#34;white\u0026#34;, ha=\u0026#34;right\u0026#34;, va=\u0026#34;baseline\u0026#34;, size=10, #family=\u0026#34;Pacifico\u0026#34;, zorder=30, fontproperties=prop ) # Save and show result plt.savefig(\u0026#34;text-shadow.pdf\u0026#34;) plt.savefig(\u0026#34;text-shadow.png\u0026#34;, dpi=600) plt.show() \n电子书下载 您可以阅读 PDF（95Mo，首选站点）这本书，该书是开放访问的，托管在 HAL 上，HAL 是一个面向学术界的法国开放档案馆。最新版本也可以在 GitHub 上找到。本书的来源（包括代码示例）可在 github.com/rougier/scientific-visualization-book 上找到。\n代码下载 点击下载\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/sci_matplotlib/","summary":"Python 科学可视化领域是巨大的，由无数工具组成，从最通用和最广泛使用的工具到更专业和机密的工具。其中一些工具是基于社区的，而另一些则是由公司开发的。有些是专门为 Web 制作的，有些仅适用于桌面，有些处理 3D 和大数据，而有些则针对完美的 2D 渲染。\n在这个图景中，Matplotlib 有着非常特别的地方。\n 它是一个多功能且功能强大的库，可让您设计非常高质量的图形，适用于科学出版。 它还提供了一个简单直观的界面以及一个面向对象的架构，允许您调整图形中的任何内容。 最后，它可以用作常规图形库以设计非科学图形。  本书章节四个部分   第一部分 Matplotlib 库的基本原理。\n这包括回顾构成图形的不同部分、不同的坐标系、可用的比例和投影，我们还将介绍一些与排版和颜色相关的概念。\n  第二部分 图形涉及实践。\n在介绍了一些生成更好图形的简单规则之后，我们将继续解释 Matplotlib 默认值和样式系统，然后再深入研究图形布局组织。然后我们将探索可用的不同类型的情节，看看如何用不同的元素装饰一个人物。\n  第三部分 更高级的概念\n即 3D 图形、优化和动画。第四部分也是最后一部分是展示集合。\n  美图展览     案例代码  import numpy as np import matplotlib.pyplot as plt from matplotlib.textpath import TextPath from matplotlib.patches import PathPatch from matplotlib.collections import PolyCollection from matplotlib.font_manager import FontProperties from matplotlib import font_manager as fm, rcParams import matplotlib.","title":"科学绘图matplotlib"},{"content":" 参考自阮一峰技术周刊\nsrc:https://github.com/ruanyf/weekly\n 音乐可视化 https://cjting.me/2021/08/07/fourier-transform-and-audio-visualization/\n音频可视化：采样、频率和傅里叶变换，含Python代码。\n如果对乐理感兴趣，可以前往 https://www.lightnote.co/music-theory/pentatonic 学习\n风险评分卡 https://github.com/amphibian-dev/toad\nToad 是专为工业界模型开发设计的Python工具包，特别针对评分卡的开发。Toad 的功能覆盖了建模全流程，从 EDA、特征工程、特征筛选 到 模型验证和评分卡转化。Toad 的主要功能极大简化了建模中最重要最费时的流程，即特征筛选和分箱。\n Hora 用于搜索近似的文本或图形, 支持Java、 Python 和 JavaScript ，特点是搜素速度非常快。\nhttps://horasearch.com/\n Top2vec https://github.com/ddangelov/Top2Vec\nTop2Vec 是一种用于主题建模和语义搜索的算法。 它自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。 训练 Top2Vec 模型后，您可以：\n 获取检测到的主题数 获取话题 获取分层主题 按关键字搜索主题 按主题搜索文档。 按关键字搜索文档 找出相似的词 查找类似的文档 使用 RESTful-Top2Vec 公开模型  优点\n 自动查找主题数。 不需要停用词列表。 不需要词干/词形还原。 适用于短文本。 创建联合嵌入的主题、文档和词向量。 内置搜索功能。    短视频简历 《纽约时报》报道，越来越多的美国年轻人拍一段自己的短视频，放在 TikTok 上面求职，请求看到的人转发。\n这比 A4 纸的简历效果好多了，值得借鉴。大家完全可以在简历上印一个自己视频的二维码。\nhttps://medium.com/@ashaxshaxlow/job-hunters-have-you-posted-your-r%C3%A9sum%C3%A9-on-tiktok-7c97c0965b4f\n moviepy https://zulko.github.io/moviepy/gallery.html\n视频编辑的 Python 库，基于 ffmpeg, 可以方便地编写脚本，进行各种神剪辑。\n WiFi连接卡 打印一张带有 WiFi 详细信息的登录卡片，把它贴到冰箱上、放到你的钱包里\u0026hellip;\n您的 WiFi 信息永远不会发送到服务端。本网站不使用追踪、分析或指纹识别。\n 一纸简历 https://cv.devtool.tech/app\n支持模板与实时预览，可以导出为 PDF 文件\n ##了解课程\n 点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly03/","summary":"参考自阮一峰技术周刊\nsrc:https://github.com/ruanyf/weekly\n 音乐可视化 https://cjting.me/2021/08/07/fourier-transform-and-audio-visualization/\n音频可视化：采样、频率和傅里叶变换，含Python代码。\n如果对乐理感兴趣，可以前往 https://www.lightnote.co/music-theory/pentatonic 学习\n风险评分卡 https://github.com/amphibian-dev/toad\nToad 是专为工业界模型开发设计的Python工具包，特别针对评分卡的开发。Toad 的功能覆盖了建模全流程，从 EDA、特征工程、特征筛选 到 模型验证和评分卡转化。Toad 的主要功能极大简化了建模中最重要最费时的流程，即特征筛选和分箱。\n Hora 用于搜索近似的文本或图形, 支持Java、 Python 和 JavaScript ，特点是搜素速度非常快。\nhttps://horasearch.com/\n Top2vec https://github.com/ddangelov/Top2Vec\nTop2Vec 是一种用于主题建模和语义搜索的算法。 它自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。 训练 Top2Vec 模型后，您可以：\n 获取检测到的主题数 获取话题 获取分层主题 按关键字搜索主题 按主题搜索文档。 按关键字搜索文档 找出相似的词 查找类似的文档 使用 RESTful-Top2Vec 公开模型  优点\n 自动查找主题数。 不需要停用词列表。 不需要词干/词形还原。 适用于短文本。 创建联合嵌入的主题、文档和词向量。 内置搜索功能。    短视频简历 《纽约时报》报道，越来越多的美国年轻人拍一段自己的短视频，放在 TikTok 上面求职，请求看到的人转发。\n这比 A4 纸的简历效果好多了，值得借鉴。大家完全可以在简历上印一个自己视频的二维码。\nhttps://medium.com/@ashaxshaxlow/job-hunters-have-you-posted-your-r%C3%A9sum%C3%A9-on-tiktok-7c97c0965b4f\n moviepy https://zulko.","title":"TechWeekly-03 | 每周有趣有用的技术分享"},{"content":"案例文献 胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.\n摘要： 在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现，年报 MD\u0026amp;A 中披露的“短期视域” 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越 易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴**。**\n关键词： 管理者短视 长期投资 文本分析 机器学习\n变量测量论证 语言能够反映人的认知、偏好和个性（Webb et al.，1966），研究者可通过分析实验对象语言中使用的词语类型和词频来捕捉人的特质（Miller and Ross，1975；Pennebaker et al.，2003）。如一个人的语言中越强调“过去”、“ 曾经”等词汇，反映其越关注过去；一个人的语言中越强调“将来”、“ 可能”、“ 要去”等词汇，反映其越关注未来（Pennebaker et al.，2003）。基于此研究范式，本文结合已有的英文“短期视域”词集、MD\u0026amp;A 中文语料特点以及 Word2Vec 机器学习制定出能够反映管理者“短期视域”的中文词集，随后通过词典法构建出管理者的短视主义指标。\nMD\u0026amp;A 是管理者对报告期内企业经营状况的回顾以及对下一年度经营计划以及企业未来发展所面临的机遇、挑战和各种风险的阐述。已有利用 MD\u0026amp;A 等文本刻画管理者特质的研究成果在一定程度上证实了其可靠性（Li，2012；蒋艳辉、冯楚建，2014）。如\n Li（2012）利用美国上市公司 MD\u0026amp;A 文本来刻画管理者的 自我归因偏差。 蒋艳辉和冯楚建（2014）利用 MD\u0026amp;A 中“我们”、“ 我公司”、“ 我们公司”等词语出现的频率刻画管理者的自我指涉度，从而衡量管理层对公司的认知和努力程度。 同时，国外文献表明 CEO 对企业的经营决策起着绝对的主导作用，能够直接影响企业未来的发展方向和命运（Chandler，1962；Finkelstein and Hambrick，1996）。CEO 的特质如自恋程度、学历和任期等都会极大影响公司的信息披露特点（Marquez Illescas et al.，2019；Lewis et al.，2019），因此年报披露的文本信息更多地反映了 CEO 的意思。而在我国，上市公司的董事长更像发达国家的 CEO（姜付秀等，2009；陈传明、孙俊华，2008；李健等，2012）。  因此，我们从 MD\u0026amp;A 中捕获的管理者短视主义特质更多反映的是董事长的短视主义特质，本文的管理者指的是企业的董事长。\n指标构建过程 具体来讲**，管理者短视主义指标**的构建过程如下。\n 借鉴 Brochet 等（2015）的英文“短期视域”词集与 Li（2010）构建文本指标的思路，我们阅读了 500 份 MD\u0026amp;A 语料以获取中文文本信息的特点，制定出中文 MD\u0026amp;A 中有关“短期视域”的种子词集，包括直接和间接 两大类。直接短期视域大类包括：“ 天内”、“ 数月”、“ 年内”、“ 尽快”、“ 立刻”、“ 马上”；间接****短期视域大类包括：“ 契机”、“ 之际”、 “压力”、“ 考验”。 针对同一概念或者事物，表达者往往使用多个语义相似的词汇进行描述，因此需要对种子词集进行相似词扩充。本文采用 Word2Vec 中的 CBOW 模型（Continuous Bag-of-words Model）对中文年度财务报告语料进行训练。 我们通过邀请 3 名业界和学术界专家以及对比 MD\u0026amp;A 文本样例对指标词集进行核验，最终确定词集包含 43 个“短期视域”词汇（词集和语句示例详见《管理世界》网络发行版附录 2）。随后，本文基于词典法计算 “短期视域”词汇总词频占 MD\u0026amp;A 总词频的比例，乘以 100 后得到管理者短视主义指标。该指标值越大，表明管理者越短视。  技术分析  纯技术讨论，非论文内容\n 这篇管理世界的论文，主要难点有两个：\n  如何构建 短视主义词典(集) ？\n   根据对研究和数据的了解，人工摘选一些 短视主义词典(集)种子词；人工，不需要python编程 使用Word2Vec技术扩充 短视主义词典(集)；需要python编程    如何使用 短视主义词典(集) 计算 短视主义指标？\n   需要使用Python编程语言，根据 词典法 实现短视主义指标的计算。    python学习与实现 难点主要可在掌握 视频专栏课| Python网络爬虫与文本分析 后，结合以下两个技能点实现\n 扩充词集可以用到之前分享的wordexpansion库 https://github.com/DataPlusCommunity/wordexpansion 计算短视主义指标，即词典法可以用到cnsenti库 https://github.com/DataPlusCommunity/cnsenti  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/text_mining_in_2021_management_world/","summary":"案例文献 胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.\n摘要： 在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现，年报 MD\u0026amp;A 中披露的“短期视域” 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越 易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴**。**\n关键词： 管理者短视 长期投资 文本分析 机器学习\n变量测量论证 语言能够反映人的认知、偏好和个性（Webb et al.，1966），研究者可通过分析实验对象语言中使用的词语类型和词频来捕捉人的特质（Miller and Ross，1975；Pennebaker et al.，2003）。如一个人的语言中越强调“过去”、“ 曾经”等词汇，反映其越关注过去；一个人的语言中越强调“将来”、“ 可能”、“ 要去”等词汇，反映其越关注未来（Pennebaker et al.，2003）。基于此研究范式，本文结合已有的英文“短期视域”词集、MD\u0026amp;A 中文语料特点以及 Word2Vec 机器学习制定出能够反映管理者“短期视域”的中文词集，随后通过词典法构建出管理者的短视主义指标。\nMD\u0026amp;A 是管理者对报告期内企业经营状况的回顾以及对下一年度经营计划以及企业未来发展所面临的机遇、挑战和各种风险的阐述。已有利用 MD\u0026amp;A 等文本刻画管理者特质的研究成果在一定程度上证实了其可靠性（Li，2012；蒋艳辉、冯楚建，2014）。如\n Li（2012）利用美国上市公司 MD\u0026amp;A 文本来刻画管理者的 自我归因偏差。 蒋艳辉和冯楚建（2014）利用 MD\u0026amp;A 中“我们”、“ 我公司”、“ 我们公司”等词语出现的频率刻画管理者的自我指涉度，从而衡量管理层对公司的认知和努力程度。 同时，国外文献表明 CEO 对企业的经营决策起着绝对的主导作用，能够直接影响企业未来的发展方向和命运（Chandler，1962；Finkelstein and Hambrick，1996）。CEO 的特质如自恋程度、学历和任期等都会极大影响公司的信息披露特点（Marquez Illescas et al.，2019；Lewis et al.，2019），因此年报披露的文本信息更多地反映了 CEO 的意思。而在我国，上市公司的董事长更像发达国家的 CEO（姜付秀等，2009；陈传明、孙俊华，2008；李健等，2012）。  因此，我们从 MD\u0026amp;A 中捕获的管理者短视主义特质更多反映的是董事长的短视主义特质，本文的管理者指的是企业的董事长。\n指标构建过程 具体来讲**，管理者短视主义指标**的构建过程如下。\n 借鉴 Brochet 等（2015）的英文“短期视域”词集与 Li（2010）构建文本指标的思路，我们阅读了 500 份 MD\u0026amp;A 语料以获取中文文本信息的特点，制定出中文 MD\u0026amp;A 中有关“短期视域”的种子词集，包括直接和间接 两大类。直接短期视域大类包括：“ 天内”、“ 数月”、“ 年内”、“ 尽快”、“ 立刻”、“ 马上”；间接****短期视域大类包括：“ 契机”、“ 之际”、 “压力”、“ 考验”。 针对同一概念或者事物，表达者往往使用多个语义相似的词汇进行描述，因此需要对种子词集进行相似词扩充。本文采用 Word2Vec 中的 CBOW 模型（Continuous Bag-of-words Model）对中文年度财务报告语料进行训练。 我们通过邀请 3 名业界和学术界专家以及对比 MD\u0026amp;A 文本样例对指标词集进行核验，最终确定词集包含 43 个“短期视域”词汇（词集和语句示例详见《管理世界》网络发行版附录 2）。随后，本文基于词典法计算 “短期视域”词汇总词频占 MD\u0026amp;A 总词频的比例，乘以 100 后得到管理者短视主义指标。该指标值越大，表明管理者越短视。  技术分析  纯技术讨论，非论文内容","title":"管理世界 | 使用文本分析\u0026机器学习测量短视主义"},{"content":"\n 翻译自\nBerger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.\u0026rdquo; Journal of Marketing (2019): 0022242919873106.\n 论文作者们的报告视频已上传到B站(下图)，感兴趣的童鞋可以先收藏再收看\nJourmal of Marketing Webinar｜2019市场营销\n摘要 语言文字是营销场景中最常用的交互方式，比如在线评论、消费者服务热线、新闻发布、营销传播等活动都创造了有价值的文本数据。但营销研究者如何用好这些数据？本文回顾了文本分析相关研究，并详细介绍了如何用文本数据做市场研究。作者讨论了文本如何反映文本生产者， 文本信息如何影响信息接受者。\n接下来，本文讨论了文本如何预测并理解文本背后的信息，回顾了文本分析的方法和测量指标(metrics),提供了一整套的文本分析操作流程。最后，作者提到文本分析内部信度和外部效度问题，研究者如何解决。本文讨论营销各个领域可能存在的研究机会，虽然目前市场营销的研究问题大都是跨学科的，但是营销的各个子领域经常都是孤立，借助文本分析可能架构起连接营销各个子领域的桥梁。\n关键词  计算语义学coputational linguistics 机器学习machine learning 市场洞察marketing insight 跨学科interdisciplinary 自然语言处理natural language processing 文本分析text analysis 文本挖掘 text mining  无所不在文本 交流沟通是营销的重要组成部分，消费者、企业、消费者投资者、社会，不同水平或者统一水平都有信息交流与沟通。而信息交流的过程中往往会产生或者转化为文本数据。\n最简单的的文本数据世界模型是生产者与消费者。模型内生产者和接受者都可能是消费者、企业、投资者和社会。消费者书写在线评论，公司制作会计年报，文化生产者代表社会意义制作出书籍、影片和艺术品（Table 1）\n在此情形下，研究者可能选择文本如何反映或如何影响？\n How text reflects its producer？ How text impacts its receiver？  尤其是文本可以反映一定的信息，这些信息是可以帮助营销人员洞察市场规律，进而利用规律影响文本信息的接受者。\n文本反映生产者 首先，文本可以反映了个人的一些信息。例如“在社交媒体某推特上写着某人谈论着上周他们做了什么。”这句话有很多待挖掘的信息，比如他们这些人什么情况，是内向还是外向、神经质还是严肃认真、他们感觉如何、某时刻他们想了什么(Moon and Kamakura 2017)。总之，文本可以看作指纹或签名(Pennebaker 2011)。\n通过文本也可以用于理解领导人、机构或者文化精英。例如领导人用词表达会反映出其领导风格，对利益相关方的态度。透过广告、网站或者消费者服务商(consumer service agent)的言语，人们会了解公司的品牌个性(Opoku, Abratt, and Pitt 2006)，公司是如何看待消费者(Packard and Berger 2019a)，管理层对终端用户的定位(Molner, Prabhu, and Yadav 2019)。年报也会有未来业绩表现的有价值线索(Loughran and McDonald 2016)。\n除了单独分析个人或组织的言语，也可以对多个内容生产者合并起来进行更大层面的研究。透过人群或组织产生的文本，我们可以更好理解他们的本质。例如，分析微博，可以得出老年人和年轻人之间如何看待幸福(as excitement vs. peacefulness; Mogilner, Kamvar, and Aaker 2011)。消费者们在品牌社区的言语能更深的投射出消费者对品牌的态度(Homburg, Ehm, and Artz 2015)。\n而更宏大的层面，文本也能反映出文化差异。如美国人的表达相比东亚人具有更高的唤醒水平(Tsai 2007)，更喜欢用“我”而不是“我们”，也透露着崇尚个人主义，而不是集体主义。\n透过时间，研究者也可以监测美国国民情绪是否在911恐怖袭击前后发生变化(Cohn, Mehl, and Pennebaker 2004)。透过新闻报告、歌词等内容也可以帮助研究者了解社会态度和社会规范，分析有关对女性、少数族裔(Boghrati and Berger 2019; Garg et al. 2018)和特定产业态度的时代变迁(Humphreys 2010)。\n虽然文本分析并不容易，但企业和组织可以使用社交网络倾听民声。了解消费者是否喜欢新产品，消费者如何看待品牌，消费者最看重什么(Lee and Bradlow 2011; Netzer et al. 2012)。监管机构可以确定什么药物有不良部反映(Feldman et al. 2015; Netzer et al. 2012),公共卫生部门可以提前了解流感今年爆发最严重的地区(Alessa and Faezipour 2018),投资者可以预测股价涨跌 (Bollen, Mao, and Zeng 2011; Tirunillai and Tellis 2012).\n文本作用于消费者 文本不止可以反映生产者的信息，也可以知道文本如何影响消费者，消费者会有什么样的行为和选择。广告会塑造消费者的消费行为(Stewart and Furse 1986),报纸用语会改变消费者的态度(Humphreys and LaTour 2013), 消费者杂志会扭曲消费者产品分类感知(e.g., Rosa et al. 1999),电影剧本会影响观众的反应(Berger, Kim, and Meyer 2019; Eliashberg, Hui, and Zhang 2014; Reagan et al. 2016),等等。\n需要注意的是文本的反映reflects和影响impacts并不是非此即彼，往往会同时起作用，尽管如此，研究人员倾向于使用文本差异来研究它俩。\n当研究文本的reflects时，倾向于将reflects当作因变量，试图挖掘文本生产者的个性personality或属于什么社会团体。\n当研究文本的impacts时，倾向于将impacts看作自变量，检验文本是否以及如何导致消费者诸如购买、分享和卷入行为。在本框架中，文本信息潜藏着某些潜在的影响力，是被当作诱因，对后续或者其他主体有作用力的。\n文本内容也会被客观条件影响 文本内容还可以被客观条件所塑造，如\n 技术限制和社会文化基因(社会规范) 文本信息生产者与消费者之间的领域知识 先前客观历史  首先，不同题材因社会规范，表达内容和方式有所不同。例如观点陈述时，新闻不如报告来的客观(Ljung 2000).酒店评论卡和其他反馈主要被极端观点占据。在Snapchat和其他SNS平台的推文达多较短，且昙花一现；而自在线评论经常较长且可以回溯到多年以前。\n技术和物理也会改变文本表达。推特只能发少于280字符的推文。移动电话在键入方面受到限制，并且可能会影响人们在其上产生的文本（Melumad，Inman和Pham 2019; Ransbotham，Lurie和Liu 2019）。\n其次，信息生产者和消费者之间的关系会影响说什么，怎么说。当生产者和消费者彼此很熟悉，文本表达会更非正式(Goffman 1959)，导致第三方很难通过直接明确的信息了解生产者与消费者之间的对话的态度。\n这些因素对于解读文本信息至关重要，消费者给好朋友分享什么往往跟其他不同。企业可能会因为特定的冬季，其年报中可能会含有利好市场的信息。\n最后，历史可能也会影响文本的内容。在留言板上，以前的帖子可能会影响以后的帖子；如果有人在先前的帖子中提出了要点，则被访者很可能会在以后的帖子中提及该要点。如果转发的帖子含有自己的分析，其内容会偏离大多数的帖子。更广泛地说，＃metoo或#blacklivesmatter之类的媒体框架可能使某些概念或事实更容易被演讲者使用，因此即使看起来似乎无关，它们也更可能出现在文本中（McCombs\u0026amp;Shaw 1972; Xiong，Cho\u0026amp;Boatwright 2019）。\n使用文本预测与理解 文本除了reflects 和 impacts之外，还有predict和understanding。\n预测 某些文本研究出发点是做预测\n 什么消费者最喜欢贷款(Netzer, Lemaire, and Herzenstein 2019)? 什么电影会大火(Eliashberg et al. 2014)? 未来股市走向(Bollen, Mao, and Zeng 2011; Tirunillai and Tellis 2012)?  类似上面的研究，会使用很多文本特征来做机器学习和预测，研究人员不怎么关系任意的文本特质，他们更关心预测的表现。\n用文本做预测的主要难点是，文本数据可以生成成千上万的特征(相当于变量x1，x2\u0026hellip;xn)，而文本数据记录数甚至可能少于特征数。为了解决这个为题，使用新的特征分类方法，减少特征数量，又有可能存在拟合问题。\n理解 预测之外的研究主要是理解文本\n 消费者怎样表达会如何影响口碑(Packard and Berger 2017)? 为何某些推文会被挑中分享？ 歌曲为何变火？ 品牌如何让消费者忠诚？  理解的目标是理解为什么事情发生以及如何发生的。这类研究往往会用到心理学、社会学的方法，旨在理解文本的什么特征会导致什么后续结果，以及为什么产生这样的后果。\n用文本做理解的难点是找出观测数据背后的因果关系。相应的，该领域的工作可能会强调实验数据，以允许对关键的独立变量进行操作。另一个挑战是解释文本特征之间的关系。使用第二人称的歌曲往往较火(Packard and Berger 2019b),但是为什么使用第二人称会火，单纯的文本数据很难挖掘出来作用机制。\n在prediction领域，研究人员利用 文本的reflects方面 来预测 生产者的状态、特性、满意度、性格等。研究人员利用 *文本impacts方面 * 来预测 消费者的阅读、分享和购买行为。\n在understand领域，研究人员利用 文本的reflects方面 来理解为什么当人们压抑的时候会使用特殊人称。利用 *文本impacts方面 * 来理解为何带有情绪的文本会更容易被阅读和分享。\n粘合营销各领域 尽管有reflects vs impacts， prediction vs understanding之分，做文本分析需要整合多种技能·技术和不同营销领域的相关知识。\n就拿消费者行为学来说，在行为经济学大放异彩之前，假设情景操纵是存在争议的。实验可重复性问题，研究者开始寻找试图增强信度、效度的新工具。使用二手数据经常受限于只能做“是什么”的研究，不能做“为什么”的研究。但文本数据提供了做为什么的可能。例如在线评论可以用来理解为何某人购买了此商品的决策，尽管人们可能并不总是知道为什么要做某事，但他们的语言常常提供解释的痕迹（Pennebaker 2011），甚至超出了他们有意识地表达的范围。\n定量建模人员一直在寻找新的数据源和工具来解释和预测行为。非结构化数据提供了一组丰富的预测变量，这些预测变量通常可以随时大规模获得，并且可以与结构化度量一起作为因变量或自变量组合。通过产品评论，用户驱动的社交媒体活动以及公司驱动的营销活动，文本可以实时提供可以阐明消费者需求/偏好的数据。这提供了对传统营销研究工具的替代或补充。在许多情况下，文本可以追溯到个人，从而可以区分个人差异和动态。\n营销策略研究人员希望企业能实现其营销目标，并更好地理解影响组织成功的因素。文本分析提供了一种客观而系统的解决方案，以评估可能更有效的自然数据（例如，致股东的信，新闻稿，专利文本，营销信息，与分析师的电话会议）中可能的因素，如了解客户、合作伙伴和员工关系性质以及品牌情感强度(Kubbler，Colicev和Pauwels2017）使用词典和支持向量机方法来提取情绪并将其与消费者心态指标相关联。\n也有学者借鉴人口和社会学领域，使用定性和内容分析研究文本数据。消费者文化领域，研究者对字里行间的意义、规范和价值观更感兴趣。文本分析提供了事物变化或比较不同事物的量化指标。文本分析为营销学者解锁了非结构化数据的开锁姿势，提供了文本的定性与定量研究的新疆界。\n文本分析工具、方法和指标 给予前任做的文本数据驱动的洞察，有学者可能好奇如何开启文本研究之路。在本节会评述文本分析相关研究，包括\n 构念如何用文本数据构建 将提取的文本信息整合到后续建模和分析中所需的过程  本节目的是提供综合的入门指导，而是把可用的技术路线留给各位\n 讨论各种方法如何恰当的使用 各种方法在使用时应该注意什么  文本处理分析包括的步骤有\n 数据预处理 文本信息提取 常用的文本分析指标  数据预处理 文本数据是非结构化的脏数据。在任何常规数据分析之前，都要先将文本数据预先清洗处理，进而产生出类似excel表的干净的数据。常用的工具有R语言和Python语言，两种编程语言都有一套易用的数据预处理包。使用某些软件，如Linguistic Inquiry and Word Count (LIWC; Tausczik and Pennebaker 2010) 和WordStat (Peladeau 2016)之前，文本数据需要做少量的预处理。预处理可见Table 2和 Table 3 。\n1. 数据获取 巧妇难为无米之炊，做文本研究的第一步就是采集数据，文本存在于邮件、公司年报、在线评论之中，无所不在，浩瀚无比。可以用人工手动复制粘贴到excel之中，但是效率太低，我们可以使用python设计网络爬虫采集数据。常见访问库requests、数据解析库pyquery和BeautifulSoup、数据存储库csv。\n2. 分词 将文本分词(切词)，数据尺度从章节段落拆解成颗粒度更小的词语层面，方便进行分析。但是要注意，英文是用空格间隔词语，而中文没有空格，还要注意粒度分的不能太细，如“the U. S.”按照空格分词会分出“the”、“U.” 和“S.”，导致美国这个实体被切分消失。\n3. 清洗 网络爬虫在采集数据阶段，采集的并不是干净的文本数据，还有一些像HTML标签、图片、链接等字符，需要采集时清除掉。\n4. 剔除停止词 文本中有很多经常出现的无意义或者意义微乎其微的词，如\u0026quot;a\u0026quot;、the\u0026quot;、\u0026ldquo;is\u0026rdquo; 等。一般情况下，这些词是需要剔除的。但是当研究的是书写者的语言风格，这些无意义词语往往含有千丝万缕的写作习惯信息，所以此时不能剔除。(e.g., Packard, Moore, and McFerran 2018；Pennebaker 2011).\n5. 拼写 一般情况下，还需要将错误书写的词正确修改过来。但是当研究者对错误率感兴趣的时候，这时候就不要更正拼写问题。(e.g., Netzer, Lemaire, and Herzenstein 2019).\n6. 词干化 词干化是为了将相同或者相近意思的词合并为一个词，如“car” ` “cars” 统一识别为 “car,”\n文本信息提取 1. 命名实体抽取 这是文本分析最基础、最简单、最常用的部分。例如姓名、地址、品牌、产品属性、情绪、词性等等都可以看作一种实体信息。实体抽取可以用来\n 监测啥叫媒体讨论，商业竞争情报 也可用作机器学习中的特征（预测指标），预测是否是欺诈信息 构建更复杂的文本表达方式的度量指标，如情感、情绪、写作风格  这部分一般需要强大的编程语言，如Python和R；当然有些情况下不用编程，使用WordStat也能做实体抽取。大多数情况下实体抽取经常伴随着专业词典或词表的使用，如(概念、品牌、分类、地址等)。通用的词典包括LIWC(Pennebaker et al. 2015)， EL 2.0 (Rocklage, Rucker, and Nordgren 2018), Diction 5.0 或General Inquirer for psychological states and traits (Berger and Milkman [2012]; Ludwig et al. [2013]; Netzer, Lemaire, and Herzenstein [2019]).\n情感词典，如Hedonometer (Dodds et al. 2011), VADER (Hutto and Gilbert 2014), 和LIWC能计算出文本中含有的情感信息。情感分析经常使用词袋法（Bag of Words）计算文本中的情感。但是该方法不考虑词语在文本中的顺序，而顺序是能影响情感信息的。尽管词典法对构建构念和比较构念比较简单，但基于人工编码的机器学习方法(e.g.,Borah and Tellis 2016; Hartmann et al. 2018; Hennig-Thurau, Wiertz, and Feldhaus 2015)更适合做精准概念的度量(Hartmannetal.2019)，尤其是这个领域是不常见或者比较复杂。\n如果研究者想挖掘出实体之间的关系就用到word2vec或者词嵌入word embedding (Mikolov et al. 2013)，这两种方法都把每一个词分配一个长度固定的向量，我们知道向量可以在空间中比较，如cos余弦计算词语之间的相似度。\n2. 话题模型 实体抽取有两个大问题:\n 维度太高，经常能从文本数据中抽取出数千个实体 实体的解读与解释  话题模型更多的是对文本的解释，而非预测(e.g., Berger and Packard 2018; Tirunillai and Tellis 2014)。话题模型最常见的是LDA，某个词以一定的概率属于话题，文本以多种话题按照一定的概率分布。\nLDA是无监督学习，需要事先指定话题数，输出的结果是不同的类分布，需要研究者解读每一个话题到底是什么题材内容。话题区间范围一般建议结合统计分布和研究者经验确定话题数目。\n3. 关系抽取 关系抽取可以用实体共现性来捕捉(e.g., Boghrati and Berger 2019; Netzer et al. 2012; Toubia and Netzer 2017).但营销学者对诸如产品、属性和情感之间的关系感兴趣。例如，研究者对评论中是否提及某个产品属性的问题。Feldman et al. (2015) and Netzer et al. (2012) 提供了药物与不良反应之间的关系来识别问题药物。\n关系抽取用的实现大多思路不难，多是一些人工规则的设计，如产品“Ford”、属性“oid consumption”和问题“excessive”共现性来捕捉福特车耗油。然而这样的方法需要手写复杂的规则，现在变得慢慢不流行。\n更通用的方法是机器学习法，人工标注相关的数据，训练机器学习模型。这类实现方法需要大量的人工标注，一种可用的工具是Stanford Sentence and Grammatical Dependency Parser (http://nlp.stanford.edu:8080/parser/) 。该工具可以识别词语依存关系，如“the hotel was very nice,” ，“nice” 与 “hotel”相关联，说明这个hotel挺nice的。\n当然，也可以扩文本之间做比较，这里不过多赘述。\n文本分析指标 早起市场营销，如在线评论领域的文本分析指标多为\n 数量(e.g., Godes and Mayzlin 2004; Moe and Trusov2011) 效价，评论评分t (e.g., Godes and Silva 2012; Moe and Schweidel 2012; Ying, Feinberg and Wedel 2006)· 方差，如信息墒(e.g., Godes and Mayzlin 2004).  然而如今这些指标经常忽略了文本的丰富度。以下几种是更好用的指标\n1. count measure 使用相应的词典，统计实体出现次数，这样可以对不同实体进行比较(Berger and Milkman 2012; Borah and Tellis 2016; Pennebaker et al. 2015; Schweidel and Moe 2014; Tirunillai and Tellis 2014)。缺点是更长的文本通常含有更多的实体(的数量)，还有一个局限就是某些实体会比其他实体更多的出现，如“电脑”商品的在线评论中“电脑”出现次数会远多于其他词。\n2. 相似度 在某些情况下，研究者更对文档之间的相似度感兴趣(e.g., Ludwig et al. 2013).。两个广告之间的相似程度如何？两首歌的歌词相似程度多少？相似度的计算方法有cos余弦相似、jaccard相似 (e.g., Toubia and Netzer 2017)\n3. 可读性 同样的意思可以用不同的难度的词汇去表达，造成阅读的难易程度。可读性反映了作者的内容复杂度和读者的阅读难度。(e.g., Ghose and Ipeirotis 2011)。\n常见的可读性算法有Flesch–Kincaid和the simple measure of gobbledygook (SMOG)。可阅读性经常将得分设置到1-12分之间，在美国学校里阅读理解成绩水平得分就是1-12分。\n未来营销研究新机会 1. 借鉴融合 文本分析在营销界中可以起到促进各个子领域交叉授粉，避免同质化学术繁殖。品牌社群是最早被来自社会学背景的研究者发现和研究的(Mun˜iz and O’Guinn 2001)。随后，定性和定量范式研究者逐渐界定了概念、识别了社群中的地位和作用(e.g., Mathwick, Wiertz, and De Ruyter 2007)。文本分析可以让学者研究如何在更大尺度层面去量化社群中的消费者沟通行为。例如，社群中不同权利地位的人使用的语言是否存在差异，使用不同动态指标预测社群产出情况(e.g., Manchanda, Packard, and Pattabhitamaiah 2015)。研究人员也可以追踪到底哪类用户发明新用语，又是哪些人跟随使用这些新用语。研究可以检查人们是否随着时间的开始使用社群语言，并根据他们对群体语言的适应程度来预测哪些人可能会留下或离开(Danescu-Niculescu-Mizil et al. 2013; Srivastava and Goldberg 2017)。定量或机器学习的研究人员可能会发现社群中最常讨论的主题，以及这些主题如何随着社群的发展而动态变化。阐述性范式的研究人员可能会研究这些话语在概念上如何关联，以找到是哪些潜在社区准则促成成员留下。然后，营销战略领域的研究人员可能会使用或开发词典来将这些社区与公司绩效联系起来，并为公司提供有关如何保持不同品牌社区（或环境）成员参与度的指导。\n不同子领域的营销学者会使用不同的技能集，研究不同的文本传播类型。消费者与消费者(consumer-to-consumer)之间的沟通主要研究的是两者间的行为，而营销战略学者倾向于研究企业与消费者、企业与企业之间的沟通。不同营销子领域的学者间的合作，能帮助他们结合不同的文本数据源。\n它山之石可以攻玉，例如营销战略学者借鉴经济学领域的交易理论(代理理论)来研究企业间的关系，但现在营销战略相关发现可以用于研究消费者之间的沟通行为。\n2. 扩展文本领域研究 我们希望看到更多的消费者-企业间的沟通的研究(e.g., Packard and Berger 2019a; Packard, Moore, and McFerran 2018)，这些沟通经常都是非约束非的，这其中蕴涵着有价值的关系数据，可以有很多应用价值。\n而在企业间沟通方面，大多数侧重于沟通(Communication)的角色(e.g., Palmatier, Dant, and Grewal 2007)。然而在文本数据上，在词语层面上，有相关研究很少。例如很少有研究销售人员与消费者之间的信息交换类型。\n类似的，在会计金融领域有很多人采用年报作为数据源(for a review, see Loughran and McDonald [2016])，但营销学者很少注意到公司与投资者之间的存在的研究机会。大多数学者只是用来研究如何预测公司股价或者开发新的公司市值估值模型。鉴于最近有兴趣将营销相关活动与公司估值联系起来（例如McCarthy和Fader 2018），这可能是一个需要进一步追求的领域。公司的所有沟通，包括年度报告等必需的文件，或广告和销售互动等任意形式的沟通，都可以用做观测变量，例如市场定位，营销能力，营销领导风格，甚至公司的品牌个性。\n在消费者、企业、社会之间也存在着大量的研究机会。有关企业文化(规范)的数据，例如新闻媒体和政府报告，可能有助于阐明影响市场的力量。例如，要了解Uber这样的公司如何抵抗市场变化，可以研究市政厅会议的笔录和其他听取并回答市民意见的政府文件。诸如#metoo和#blacklivesmatter之类的社会运动形式的外来冲击影响了营销传播和品牌形象。未来研究的一种潜在途径是采用文化品牌化方法（Holt，2016年），研究不同公众如何定义，塑造和倡导市场中的特定含义。公司及其品牌并不是凭空存在的，它们独立于其经营所在的社会。但是，在市场营销方面的有限研究已经考虑了如何使用文本在社会层面上得出公司的意图和行为。例如，学者们展示了诸如locavores（这类人只食用当地产的食品；Thompson和Coskuner-Balli，2007年），时尚达人（Scaraboto和Fischer，2012年）以及博主（McQuarrie，Miller和Phillips，2012年），这几类人群塑造了市场。通过文本分析，可以衡量和更好地理解这些社会群体的意图对市场的影响。\n未来研究的另一个机会是使用文本数据来研究文化和文化成功。跨学科研究了文化传播，艺术变革和创新传播等主题，目的是理解某些产品为何成功而其他产品却失败的原因(Bass 1969; Boyd and Richerson 1986; Cavalli-Sforza and Feldman 1981; Rogers 1995; Salganik, Dodds, and Watts 2006; Simonton 1980). While success may be random (Bielby and Bielby 1994; Hirsch 1972),可能的原因是没把握好消费者的口味偏好 (Berger and Heath 2005)。\n通过在大范围更快速度地量化书籍、电影或其他文化物品，研究人员可以测量具体的叙事是否更具吸引力，更具情感波动性的电影是否更成功，使用某些语言特征的歌曲是否更有可能登上广告牌榜首 ，以及唤起特定情感的书籍是否售出更多。尽管没有像社交媒体数据那样广泛可用，但最近越来越多的文化项目数据可用。诸如Google Books语料库（Akpinar\u0026amp;Berger 2015），歌曲歌词网站或电影脚本数据库等数据集可提供大量信息。此类数据可以使叙事结构分析，以识别\u0026quot;基本情节\u0026quot;'(Reagan et al 2016; Van Laer et al2019）。\n3. 用文本测量关键构念 在个体层面上，情感和满意度可能是最常用的测量变量(e.g., Bu¨schken and Allenby, 2016; Homburg, Ehm, and Artz 2015; Herhausen et al. 2019; Ma, Baohung, and Kekre 2015; Schweidel and Moe 2014)其他从文本数据中提取的测量变量包括语言的真实性authenticity和情绪性emotion(e.g., Mogilner, Kamvar, and Aaker 2011; Van Laer et al. 2019)。也有心理学测量变量，如性格类型presonality type和建构水平construal level(Kern et al. 2016; Snefjella and Kuperman 2015),这都是潜在的可以借鉴应用到消费者话语研究的。\n未来个体层面的研究会考虑社会认同和社会参与度， 研究人员目前对消费者已经可以测量情绪的积极或消极，但他们才刚刚开始探索重点（Rocklage\u0026amp;Fazio 2015），信任，承诺和其他模式属性。为此，利用语用学的语言理论并研究语义学上的阶段性可能是有用的（Villarroel et al2017）。一旦开展了此类工作，我们建议研究人员仔细验证建议的方法，以按照上述方法测量此类构念。\n在公司层面，已在公司生产的文本（例如年度报告和新闻稿）中确定了一些构念。诸如市场定位、广告目标、未来定位、欺骗意图、公司重点和创新定位均已使用此材料进行了测量和验证（详见Web Appendix Table 1)。未来企业层面的营销研究需要重新界定和丰富战略定位的测量(创新定位、市场驱动vs市场驱动定位)。组织文化、结构和能力由于难于测量，可以从企业、雇员和外部利益相关者的文本数据来测量(see Molner, Prabhu, and Yadav [2019])。类似的，企业领导层的思维和管理风格可以从他们怎么说来侦测(see Yadav, Prabhu, and Chandy [2007])。公司的绩效指标可以通过之前的公司相关文本数据进行预测(e.g., Herhausen et al. 2019)。从这个角度看，我们有很多使用数据的新机会。例如，从企业内部员工的相关信息(LinkedIn 和 Glassdoor)可以测量基于员工的品牌价值。最后，企业语言的更多微妙属性，如冲突、歧义、开放性都可以为管理学增加新发现。再比如，使用一些非正式文本数据，如员工邮件记录、销售通话记录或消费者服务中心通话记录。\n营销工作较少在社会或文化层面上衡量结构，但这种工作趋向于集中于公司如何适应现有意义和规范的文化结构。例如，制度逻辑和合法性是通过分析媒体文本来衡量的，Berger等人的品牌公众崛起也增加了文化中对品牌的讨论（Arvidsson and Caliandro 2016）。在文化层面，营销研究可能会继续关注企业如何适应文化环境，但也可能会关注文化环境如何影响消费者。例如，对文化不确定性，风险，敌意和变化的测量可以理解文化对消费者和企业影响。通过文本衡量开放性和多样性也是适时探索的主题，并且可能会促进测量方面的创新，例如侧重于语言多样性。通过文本分析，也可以更好地理解重要的文化论述，例如围绕债务和信用的语言。与性别和种族有关的语言的测量可能有助于探索多样性和包容性，从而使公司和消费者对来自不同作家的文本做出反应。\n机遇与挑战 本节是从技术角度出发探讨文本分析方法的新机遇与挑战。\n1. 机遇 虽然我们的讨论集中于文本内容，但文本只是非结构化数据的一个示例，而音频，视频和图像则是其他示例。社交媒体帖子通常将文字与图片或视频结合在一起。平面广告通常会在精心构造的视觉效果上覆盖文字。尽管电视广告可能不会在屏幕上包含文本，但它可能具有音频轨道，其中包含与视频同步进行的文本。\n直到最近，文本数据一直受到最多关注，这主要是由于存在提取有意义特征的工具。也就是说，诸如Praat（Boersma 2001）之类的工具允许研究人员从音频中提取信息（Van Zant和Berger 2019）。音频数据相对于文本数据的优势之一是，它以音调和语音标记的形式提供了丰富的内容，可以添加到所表达的实际单词中（Xiao，Kim和Ding 2013）。这使研究人员不仅可以研究说的内容，还可以研究说的方式，检查音调，语气以及其他声音或副语言特征如何塑造行为。\n同样，最近的研究开发了分析图像的方法（Liu，Xuan等人2018），既可以表征图像的内容，也可以识别图像中的特征。文本和图像组合的影响的研究很少（例如Hartmann等人2019）。例如，可以根据图像的颜色来描述图像。在印刷广告的上下文中，当与特定调色板的图像结合使用时，文本内容的说服力可能会降低，而其他调色板可能会增强文本的说服力。与简单的图像结合使用，文本的重要性可能会非常明显。但是，当文本与复杂的图像配对时，观看者可能会主要关注图像，从而减少了文本的影响。在这种情况下，作为广告精美图片一部分的法律披露可能不会引起受众的注意。\n当文本加到视频中时，其扮演的角色也引发了类似的问题。研究已经提出了表征视频内容的方法（例如Liu等人2018）。除了包含视频脚本之外，文本还可能在视觉上出现。除了在其中显示文本的音频上下文之外，其影响可能还取决于同时显示的视觉效果。也可能是其在视频中相对于视频开头的位置可能会降低其效果。例如，由于多种原因，在视频中稍后说出的情感性文字内容可能缺乏说服力（例如，观众在讲出文字时可能已经不再注意了）。或者，与音频配对的视觉效果可能对观众更具吸引力，或者视频的先前内容可能耗尽了观众的注意力资源。正如我们对图像和视频的讨论所暗示的那样，文本只是营销传播的一个组成部分。未来的研究必须调查其与其他特征的相互作用，不仅包括其出现的内容，还包括其出现的时间（Kanuri，Chen和Sridhar 2018），以及在哪种媒体上。\n2. 挑战 尽管机会众多，但文本数据也带来了各种挑战。首先是面临可解释性的挑战。在某些方面，文本分析似乎提供了衡量行为过程的更客观的方法。例如，一个人可以计算第一人称“ I”和第二人称“ you”。第一人称在文本中越多，说明这个人更关心自己 （Berger 2014），这种量化词语数量的方法提供看起来更像很客观像真理的东西。但是，尽管该过程的一部分肯定是更客观的（例如，不同类型的代词的数量），但此类度量与基础过程（即，关于口碑传播者的说法）之间的联系仍然需要一定程度的解释。其他潜在的行为方式甚至更难以计数。例如，虽然某些词（例如“love”）通常是积极的，但它们的积极性可能在很大程度上取决于特质个体差异和上下文。\n更普遍地，在理解文本信息出现的上下文中存在挑战和机遇。例如，餐厅评论可能包含很多否定词，但这是否意味着该人更讨厌食物，服务或餐厅？包含更多第二人称代词（“ you”）的歌曲可能会更成功（Packard and Berger 2019b），但要了解原因，了解歌词是否使用“ you”作为句子的主语或宾语是有帮助的。上下文提供了含义，而且越多的人不仅了解正在使用的单词，而且还了解如何使用它们，则越容易获得新知识新洞察。基于词典工具特别容易对使用场景变化特别敏感，建议尽可能使用针对特定研究环境创建的词典（例如，Loughran和McDonald [2016]开发的财务情感工具）。\n数据隐私挑战是一个重大问题。研究通常使用从网站上抓取的在线产品评论和销售排名数据（Wang，Mai和Chiang 2013）或从社交媒体平台上抓取的消费者的活动数据（Godes和Mayzlin 2004；Tirunillai和Tellis 2012）。尽管这种方法很普遍，但是法律问题已经开始出现。LinkedIn未能成功阻止一家初创公司抓取用户公共资料中发布的数据（Rodriguez，2017）。虽然根据法律可能允许收集公共数据，但它可能与那些拥有研究人员感兴趣的数据的平台的服务条款相冲突。\n随着从数字化文本和其他形式的数字化内容（例如图像，视频）中提取见解的兴趣日益浓厚，研究人员应确保他们已获得进行工作的适当权限。不这样做可能导致开展此类项目变得更加困难。一种潜在的解决方案是创建一个学术数据集，例如Yelp提供的数据集（https://www.yelp.com/dataset），该数据集可能包含过时或经过清理的数据，以确保不会产生 公司的运营或用户隐私风险。\n对数字化文本以及其他用户创建的内容的收集和分析，也引发了有关用户对隐私的期望的问题。随着欧盟《通用数据保护条例》的发布以及有关Cambridge Analytica从Facebook收集用户数据的能力的启示，研究人员必须注意其工作的潜在滥用。我们还应考虑超出用户生成内容的预期用途的程度。例如，尽管用户可能会理解，Facebook采取的行动可能会导致他们针对与其互动的品牌进行专门的广告宣传，但他们可能无法预期其Facebook和Instagram活动的全部内容都将被用于构建其他品牌可能使用的心理特征。了解消费者关于其在线行为及其提供的文字的隐私偏好可以为从业者和研究人员提供重要的指导。未来研究的另一个亮点是可以提高营销的精确度，同时最大限度地减少对隐私的侵犯（Provost et al 2009）。\n总结 沟通是营销的重要方面，涵盖组织与合作伙伴之间，企业与消费者之间以及消费者之间的沟通。文本数据包含这些交流的详细信息，并且通过自动文本分析，研究人员已准备好将这种原始材料转换成有价值的见解。文本数据使用方面的许多最新进展是在营销之外的领域开发的。当我们展望未来和营销人员的角色时，这些最新进展应作为示例。营销人员在消费者，公司和组织之间的接口上处于有利位置，可以利用和改进工具来提取文本信息，以解决当今企业和社会所面临的一些关键问题，例如错误的信息滥用。营销提供了一种宝贵的观点，对这次对话至关重要，但这只有通过更广阔的视野，打破理论和方法论的孤岛，并与其他学科合作，我们的研究才能吸引尽可能多的受众来影响公众话语。我们希望这个框架能够鼓励人们对界定营销的界限进行反思，并为未来的突破性见解开辟道路。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/text_mining_in_marketing_research/","summary":"翻译自\nBerger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.\u0026rdquo; Journal of Marketing (2019): 0022242919873106.\n 论文作者们的报告视频已上传到B站(下图)，感兴趣的童鞋可以先收藏再收看\nJourmal of Marketing Webinar｜2019市场营销\n摘要 语言文字是营销场景中最常用的交互方式，比如在线评论、消费者服务热线、新闻发布、营销传播等活动都创造了有价值的文本数据。但营销研究者如何用好这些数据？本文回顾了文本分析相关研究，并详细介绍了如何用文本数据做市场研究。作者讨论了文本如何反映文本生产者， 文本信息如何影响信息接受者。\n接下来，本文讨论了文本如何预测并理解文本背后的信息，回顾了文本分析的方法和测量指标(metrics),提供了一整套的文本分析操作流程。最后，作者提到文本分析内部信度和外部效度问题，研究者如何解决。本文讨论营销各个领域可能存在的研究机会，虽然目前市场营销的研究问题大都是跨学科的，但是营销的各个子领域经常都是孤立，借助文本分析可能架构起连接营销各个子领域的桥梁。\n关键词  计算语义学coputational linguistics 机器学习machine learning 市场洞察marketing insight 跨学科interdisciplinary 自然语言处理natural language processing 文本分析text analysis 文本挖掘 text mining  无所不在文本 交流沟通是营销的重要组成部分，消费者、企业、消费者投资者、社会，不同水平或者统一水平都有信息交流与沟通。而信息交流的过程中往往会产生或者转化为文本数据。\n最简单的的文本数据世界模型是生产者与消费者。模型内生产者和接受者都可能是消费者、企业、投资者和社会。消费者书写在线评论，公司制作会计年报，文化生产者代表社会意义制作出书籍、影片和艺术品（Table 1）\n在此情形下，研究者可能选择文本如何反映或如何影响？\n How text reflects its producer？ How text impacts its receiver？  尤其是文本可以反映一定的信息，这些信息是可以帮助营销人员洞察市场规律，进而利用规律影响文本信息的接受者。","title":"文本分析在市场营销研究中的应用"},{"content":"在大数据的今天，通过互联网超文本链接，无数的个人、团体、公司、政府等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。社会、管理、经济、营销、金融等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是三份文档翻译汇总而来，我觉得讲的挺明白的，其中加入了我的一点点理解和扩充。\n一、文本产生及其作用方式  How text reflects its producer？ How text impacts its receiver？  graph LR Text_Producer --\u0026gt; Text Text --\u0026gt; Text_Receiver Text_Receiver --\u0026gt;Text Text --\u0026gt; Text_Producer 文本信息的==生产者producer== 与 ==消费者receiver==，涵盖 ==个人、公司(组织)、国家(社会)==三个层面。\ngraph LR Consumers --\u0026gt; Firms Consumers --\u0026gt; Investors Consumers --\u0026gt; Society Firms --\u0026gt; Consumers Firms --\u0026gt; Investors Investors --\u0026gt; Firms Investors --\u0026gt; Society Firms --\u0026gt; Society Society --\u0026gt; Investors Society --\u0026gt; Consumers  需要注意的是文本的==反映Reflects==和==影响Impacts==并不是非此即彼，往往会同时起作用。\n    \u0026mdash; 研究目的 自变量 因变量 因变量     Reflects 文本可以反映producer的一些信息，帮助研究者理解producer。\n例如试图挖掘producer的个性personality或属于什么社会团体。 了解公司的品牌个性；\n年报也会有未来业绩表现的有价值线索；\n消费者们在品牌社区的言语能更深的投射出消费者对品牌的态度；\n而更宏大的层面，文本也能反映出文化差异。\n了解消费者是否喜欢新产品，消费者如何看待品牌，消费者最看重什么 文本 文本   Affects 知道文本如何影响receiver，receiver会有什么样的行为和选择。 检验文本是否以及如何导致消费者诸如购买、分享和卷入行为。\n广告会塑造消费者的消费行为\n消费者杂志会扭曲消费者产品分类感知\n电影剧本会影响观众的反应 文本消费者 文本消费者    \n二、如何使用文本数据    \u0026mdash; Reflects Affects 目的 应用 难点     Predict 预测 producer的状态、特性、性格等 预测 receiver阅读、分享和购买行为 研究人员不怎么关系任意的文本特质，他们更关心预测的表现。 什么消费者最喜欢贷款;\n什么电影会大火;\n未来股市走向;\n 文本数据可以生成成千上万的特征(相当于变量x1，x2\u0026hellip;xn)，而文本数据记录数甚至可能少于特征数。\n为了解决这个为题，使用新的特征分类方法，减少特征数量，又有可能存在拟合问题。   Understanding 为什么当人们压抑的时候会使用特殊人称。 来理解为何带有情绪的文本会更容易被阅读和分享 理解为什么事情发生以及如何发生的这类研究往往会用到心理学、社会学的方法，旨在理解文本的什么特征会导致什么后续结果，以及为什么产生这样的后果。 消费者怎样表达会如何影响口碑;\n为何某些推文会被挑中分享？\n歌曲为何变火？\n品牌如何让消费者忠诚？ 找出观测数据背后的因果关系。相应的，该领域的工作可能会强调实验数据，以允许对关键的独立变量进行操作。\n另一个挑战是解释文本特征之间的关系。    \n三、文本信息的指标 粗略的分，文本信息可以分为定性与定量两种类型\n   定性/量 分析方法 优点 缺点     定性（text as text） 质性（扎根） 依靠研究者领域知识，可以对少量的数据做出深刻洞见。 难以应对大规模数据；\n编码过程并不能保证唯一；   定量 textual data(text as data) 明显的文本特征，如词频、可阅读性 标准如一;\n适合大规模文本挖掘；\n纷繁复杂中涌现出潜在规律 需要破坏文本的结构，丧失了部分信息量    早先的营销领域，如在线评论文本分析指标多为\n 数量，如文本长度(e.g., Godes and Mayzlin 2004; Moe and Trusov2011) **情感得分(效价，评论评分) **(e.g., Godes and Silva 2012; Moe and Schweidel 2012; Ying, Feinberg and Wedel 2006)· 方差，如信息墒(e.g., Godes and Mayzlin 2004).  然而如今这些指标经常忽略了文本的丰富度。以下几种是更好用的指标\n   指标 功能 补充     实体词词频 使用相应的实体词典，统计实体出现次数，这样可以对不同实体进行比较 更长的文本通常含有更多的实体(的数量)；\n还有一个局限就是某些实体会比其他实体更多的出现，如“电脑”商品的在线评论中“电脑”出现次数会远多于其他词。   相似度 文档之间的相似度感兴趣。\n如两个广告之间的相似程度如何？\n两首歌的歌词相似程度多少？ 相似度的计算方法有\ncos余弦相似\njaccard相似   可读性 同样的意思可以用不同的难度的词汇去表达，造成阅读的难易程度。可读性反映了作者的内容复杂度和读者的阅读难度。 常见的可读性算法有Flesch–Kincaid和the simple measure of gobbledygook (SMOG)。\n可阅读性经常将得分设置到1-12分之间，在美国学校里阅读理解成绩水平得分就是1-12分。    \n四、文本分析步骤    序号 步骤 解释 中文 英文     1 读取数据 数据一般存储于不同的文件夹不同文件内，需要将其导入到计算机     2 分词 导入到计算的文本是字符串数据，需要整理为更好用的列表 例如“我爱你中国”分词后\n得到[\u0026ldquo;我\u0026rdquo;, \u0026ldquo;爱\u0026rdquo;, \u0026ldquo;你\u0026rdquo;, \u0026ldquo;中国\u0026rdquo;] \u0026ldquo;I love China\u0026quot;分为\n[\u0026ldquo;I\u0026rdquo;, \u0026ldquo;love\u0026rdquo;, \u0026ldquo;China\u0026rdquo;]   3 剔除符号和无意义的停止词 为了降低计算机运行时间，对分析结果影响较小的字符，诸如符号和无意义的词语需要剔除掉 如“的”，“她”， ”呢”， “了” \u0026ldquo;is\u0026rdquo; , \u0026ldquo;a\u0026rdquo;, \u0026ldquo;the\u0026rdquo;   4 字母变小写，词干化 同义词归并，同主体词归并 “中铁”，“中国铁建”，“中铁集团”都可以归并为“中铁” 先变为小写，这样“I”和“i”都归并为“i”；\n“was”，“are”，“is”都归并为“be”   5 构建文档词频矩阵 使用一定的编码方式，即用某种方式表示文本。常见的有词袋法、tf-idf；\n可以使用scikit-learn构建文档词频矩阵，但中英文略有区别，需要注意 “我爱你中国”需要先整理为“我 爱 你 中国” “I love China”    \n五、文本分析技术对比 从左向右，自动化程度越来越高，人工介入的越来越少\n   技术 描述 优点 缺点 常被应用(领域) 软件     主题分析Thematic analysis 需要有经验的人员基于自身经验和李俊杰，对研究的数据进行挖掘。编码过程为迭代进行 使用参与者自己的话语或者构念来挖掘数据，对少量文本理解的更深入 属于时间、劳动密集型任务，不适合大规模数据。\n由于不同的编码人员有不同的经历和偏好，编码过程的标准不可靠 社会学、管理学 Nvivo；   内容分析/基于字典方法 统计文本中词语/词组的出现频率 允许对研究的数据进行定量分析 采用的词典应尽量与研究问题适应，词典适配性问题突出 管理学 LIWC、Nvivo、DICTION；   词袋法（Bag of words） 将文本字符串转为计算机能理解的数字化向量 编码标准稳定简单，具有统计学特性，扩展性强 编码过程忽略词语的先后顺序 管理学 Python的scikit-learn、gensim、nltk等；R   监督学习(Supervise models),如SVM、Bayes、Logistic Regression 研究者要知道输入数据X和标签y；需要核实的模型需要X和y之间的关系和规律 允许事先定义编码规则(如选择词袋法还是tfidf)；逻辑简单 需要高质量的标注数据(工作量大)；you与特征词太多，训练的模型很容易过拟合。 计算机学、政治学、管理学 Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）   无监督学习(Kmeans、 LDA话题模型) 使用聚类、话题分析，让计算机自动对数据进行分组 在没有人工标注的情况下，加速了数据的“标注”或“分类” “标注”是机器按照数字特征进行的分组，需要研究者解读才可以赋予“标准“意义；训练过程需要大量的调参 计算机学、政治学、管留学 Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）   自然语言处理 按照人类对语言的理解进行建模，考虑词语顺序 计算机自动化；可分析语义 大多数模型是人类无法解读的黑箱；\n虽然代码编程量小，但训练代码耗时巨大 计算科学；市场营销；心理学 pytorch、tensorflow    据被压缩成词组频数，定性的文本数据转化为定量的频数。本课程中会涉及到的内容\n Thematic Analysis 定性 Content Analysis Dictionary Bag of words 词袋法 Supervised ，监督学习 文本分类问题 Unsupervised，如非监督LDA话题模型 Natural language processing  \n应用案例 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究 摘要：众筹融资效果决定着众筹平台的兴衰。 众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。 而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。 不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。\n首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。\n然后，==借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。==\n最后，建立语言说服风格对项目筹资影响的计量模型，并 对 ==Kickstarter 平台上的 128345 个项目进行实证分析==。 总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。\n关键词：众筹 融资 语言风格 说服性 投资意愿\nCopycats vs. Original Mobile Apps 摘要: 尽管移动应用程序市场的增长为移动应用程序开发人员创新提供了巨大的市场机会和经济诱因，但它也不可避免地刺激了模仿者开发盗版软件。原始应用的从业人员和开发人员声称，模仿者窃取了原始应用的想法和潜在需求，并呼吁应用平台对此类模仿者采取行动。令人惊讶的是，很少有严格的研究来分析模仿者是否以及如何影响原始应用的需求。\n==进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原始或模仿者并检测两种模仿者的方法：欺骗性和非欺骗性。==\n根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的==5,141个开发人员的10,100个动作游戏应用程序==样本中，模仿应用程序对原始应用程序需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原件的需求。\n结果表明，从总体上讲，模仿者对原始移动应用程序需求的影响在统计上是微不足道的。==我们的研究通过提供一种识别模仿者的方法==，并提供模仿者对原始应用需求的影响的证据，为越来越多的移动应用消费文献做出了贡献。\nLAZY PRICES 摘要: 使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对报告进行积极更改时，这种行为蕴含着公司未来运营的重要信号。\n财务报告的语言和结构的变化也对公司的未来收益产生重大影响：做空\u0026quot;变化\u0026quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。\n研究发现，对10-K的变化可以预测未来的收益、获利能力、未来的新闻公告，甚至未来的公司破产。同时，不做任何变化的公司将获得显著的异常收益。与资产价格典型的反应不足研究不同，我们发现没有任何与这些变化相关的公告效应–仅在后来通过新闻，事件或收益披露信息时才产生回报–暗示投资者并未注意到整个公众领域的这些变化。\n 纽约时报在2010年4月23日发了一条FDA将有对输液泵(infusion pumps)更严格对审批管理规定的新闻，新闻中提到了Baxter公司。新闻公布当天，Baxter股价大跌。\n10天后的（2010年5月4日），Baxter宣布召回问题的输液泵产品，股价当天再次大跌。\n 相关文献  [1]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.\u0026rdquo; Journal of Marketing (2019): 0022242919873106.\n[2]Kenneth Benoit. July 16, 2019. “Text as Data: An Overview” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. Handbook of Research Methods in Political Science and International Relations. Thousand Oaks: Sage.\n[3]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. \u0026ldquo;A review of best practice recommendations for text analysis in R (and a user-friendly app).\u0026rdquo; Journal of Business and Psychology 33, no. 4 (2018): 445-459.\n[4]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.管理世界.2016;5:81-98.\n[5]Wang, Quan, Beibei Li, and Param Vir Singh. \u0026ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.\u0026rdquo; Information Systems Research 29, no. 2 (2018): 273-291.\n[6]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. \u0026ldquo;Lazy prices.\u0026rdquo; The Journal of Finance 75, no. 3 (2020): 1371-1415.\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/review_about_the_application_of_text_mining_in_management_science/","summary":"在大数据的今天，通过互联网超文本链接，无数的个人、团体、公司、政府等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。社会、管理、经济、营销、金融等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是三份文档翻译汇总而来，我觉得讲的挺明白的，其中加入了我的一点点理解和扩充。\n一、文本产生及其作用方式  How text reflects its producer？ How text impacts its receiver？  graph LR Text_Producer --\u0026gt; Text Text --\u0026gt; Text_Receiver Text_Receiver --\u0026gt;Text Text --\u0026gt; Text_Producer 文本信息的==生产者producer== 与 ==消费者receiver==，涵盖 ==个人、公司(组织)、国家(社会)==三个层面。\ngraph LR Consumers --\u0026gt; Firms Consumers --\u0026gt; Investors Consumers --\u0026gt; Society Firms --\u0026gt; Consumers Firms --\u0026gt; Investors Investors --\u0026gt; Firms Investors --\u0026gt; Society Firms --\u0026gt; Society Society --\u0026gt; Investors Society --\u0026gt; Consumers  需要注意的是文本的==反映Reflects==和==影响Impacts==并不是非此即彼，往往会同时起作用。\n    \u0026mdash; 研究目的 自变量 因变量 因变量     Reflects 文本可以反映producer的一些信息，帮助研究者理解producer。","title":"文本分析在经管领域中的应用概述"},{"content":"rpy2简介 代码下载 点击下载文本\nPython和R是一对数据科学两大语言，在互联互通的路上，我感觉R更加的积极。还记得之前 Python中调用R的库-rpy2， 在Python使用R语言语法还是有些不自然。在R中调用Python体验十分棒，一起跟我看看吧。\nreticulate包是可以让R语言非常流畅自然联通Python的关键。\nRmarkdown预备知识\nRmarkdown很像jupyter notbeook和markdown的结合。\n 代码块 markdon文本  代码块 在Rmarkdown中的代码块的开始都是以三引号、{}和语言名为标志，以三引号结尾。\nlibrary(ggplot2) ggplot(mpg, aes(x=displ, y=cty))+ geom_point() library(ggplot2) ggplot(mpg, aes(x=displ, y=cty))+ geom_point()   后面设置好reticulate包后，就可以在Rmarkdown中使用python代码块，\nimport pandas as pd df = pd.read_csv(\u0026#39;test.csv\u0026#39;) df.head() \nmarkdown文本 文本部分全部支持markdown语法，这里不做赘述。\n安装rpy install.packages(\u0026#34;reticulate\u0026#34;) 环境设置 当我们在R代码块中设置好Python环境，就可以在Rmarkdown中跑Python代码。\n查询Python 首先我们需要知道Python安装路径，可以在命令行中逐行执行下面代码\npython3 import sys sys.executable 我的mac电脑的Python安装路径为\n/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 \n指定Python 执行下方的代码(路径改为自己的Python安装路径)\nlibrary(reticulate) ## Warning: package \u0026#39;reticulate\u0026#39; was built under R version 3.6.2 use_python(\u0026#39;/Library/Frameworks/Python.framework/Versions/3.7/bin/python3\u0026#39;) 执行代码后，我们就可以愉快的使用Python代码\n运行Python代码 在Rmarkdown中写Python代码块\n作图 import numpy as np import matplotlib.pyplot as plt # 计算正弦曲线上点的 x 和 y 坐标 x = np.arange(0, 3 * np.pi, 0.1) y = np.sin(x) plt.title(\u0026#34;sine wave form\u0026#34;) # 使用 matplotlib 来绘制点 plt.plot(x, y) plt.show()   读取csv import pandas as pd df = pd.read_csv(\u0026#34;test.csv\u0026#34;, encoding=\u0026#34;gbk\u0026#34;) df.head() ## birthday name text age gender height weight ## 0 1985/10/08 Alice 我很开心，每天都这么快乐，我很幸福 35 female 175 55 ## 1 95.07.07 Mary 我很难过 25 female 165 50 ## 2 01-11-10 Mike 唉，真难受 19 male 180 75 ## 3 90/2/8 Smith 无所谓开心还是难过 30 male 175 70 ## 4 93-1-5 Henry 每天赚一万，真爽！ 27 male 185 80 返回的df是Python对象，我们可以看到表格不好看，这是R中的Python对象。\n在R代码块中调用Python变量(对象) 刚刚讲的都是在Rmakdown中运行Python代码块，并不是在R代码块中运行Python代码或者调用Python变量。\npy$python_variable_name R代码块中调用Python方法\npy$python_variable_name\n py相当于Python中的对象 $ 相当于Python中的点 python_variable_name 是Python代码块中的变量名  比如在上文中Python的变量df，在R中调用\npy$df   现在调用Python对象df时，R会默认将其转为R对象，所以内容一样，样式似乎变好看了。\nR代码块中导入Python库 使用os库的listdir函数查询当前项目文件夹内的文件列表\nimport os os.listdir() ## [\u0026#39;reticulate学习.md\u0026#39;, \u0026#39;test.csv\u0026#39;, \u0026#39;test.py\u0026#39;, \u0026#39;reticulate学习.html\u0026#39;, \u0026#39;reticulate学习_files\u0026#39;, \u0026#39;reticulate.pdf\u0026#39;, \u0026#39;reticulate学习.Rmd\u0026#39;, \u0026#39;data.py\u0026#39;] 在R代码块中实现上方的Python功能，如下，很简单\nlibrary(reticulate) #导入库 os \u0026lt;- import(\u0026#34;os\u0026#34;) #os库的listdir函数 os$listdir() ## [1] \u0026#34;reticulate学习.md\u0026#34; \u0026#34;test.csv\u0026#34; \u0026#34;test.py\u0026#34; ## [4] \u0026#34;reticulate学习.html\u0026#34; \u0026#34;reticulate学习_files\u0026#34; \u0026#34;reticulate.pdf\u0026#34; ## [7] \u0026#34;reticulate学习.Rmd\u0026#34; \u0026#34;data.py\u0026#34; 可以发现\n import(\u0026quot;os)代替了import os $代替了. \u0026lt;- 代替了 =  再熟悉一下\nlibrary(reticulate) pd \u0026lt;- import(\u0026#34;pandas\u0026#34;) df2 \u0026lt;- pd$read_csv(\u0026#34;test.csv\u0026#34;, encoding=\u0026#34;gbk\u0026#34;) df2   需要注意的是，在R代码块中执行Python代码时，默认会将Python对象转为R对象。\nsource_python() 使用reticulate包中的source_python(\u0026lsquo;py文件路径\u0026rsquo;)可以导入py文件中的变量，这样就可以在R代码块中使用外部变量。例如我在data.py中准备A和B两个字符串\nA = \u0026#39;我是张三,\u0026#39; B = \u0026#39;来自河北\u0026#39; 在R代码块中运行data.py\nlibrary(reticulate) source_python(\u0026#34;data.py\u0026#34;) print(A) ## [1] \u0026#34;我是张三,\u0026#34; print(B) ## [1] \u0026#34;来自河北\u0026#34; paste0(A, B) ## [1] \u0026#34;我是张三,来自河北\u0026#34; py_run_file() 在R代码块中运行项目文件夹中的test.py文件\nlibrary(reticulate) py_run_file(\u0026#34;test.py\u0026#34;) 数据类型对比    R Python Examples     Single-element vector Scalar 1, 1L, TRUE, \u0026quot;foo\u0026quot;   Multi-element vector List c(1.0, 2.0, 3.0), c(1L, 2L, 3L)   List of multiple types Tuple list(1L, TRUE, \u0026quot;foo\u0026quot;)   Named list Dict list(a = 1L, b = 2.0), dict(x = x_data)   Matrix/Array NumPy ndarray matrix(c(1,2,3,4), nrow = 2, ncol = 2)   Data Frame Pandas DataFrame data.frame(x = c(1,2,3), y = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;))   Function Python function function(x) x + 1   NULL, TRUE, FALSE None, True, False NULL, TRUE, FALSE    广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/call_python_code_in_rmarkdown/","summary":"rpy2简介 代码下载 点击下载文本\nPython和R是一对数据科学两大语言，在互联互通的路上，我感觉R更加的积极。还记得之前 Python中调用R的库-rpy2， 在Python使用R语言语法还是有些不自然。在R中调用Python体验十分棒，一起跟我看看吧。\nreticulate包是可以让R语言非常流畅自然联通Python的关键。\nRmarkdown预备知识\nRmarkdown很像jupyter notbeook和markdown的结合。\n 代码块 markdon文本  代码块 在Rmarkdown中的代码块的开始都是以三引号、{}和语言名为标志，以三引号结尾。\nlibrary(ggplot2) ggplot(mpg, aes(x=displ, y=cty))+ geom_point() library(ggplot2) ggplot(mpg, aes(x=displ, y=cty))+ geom_point()   后面设置好reticulate包后，就可以在Rmarkdown中使用python代码块，\nimport pandas as pd df = pd.read_csv(\u0026#39;test.csv\u0026#39;) df.head() \nmarkdown文本 文本部分全部支持markdown语法，这里不做赘述。\n安装rpy install.packages(\u0026#34;reticulate\u0026#34;) 环境设置 当我们在R代码块中设置好Python环境，就可以在Rmarkdown中跑Python代码。\n查询Python 首先我们需要知道Python安装路径，可以在命令行中逐行执行下面代码\npython3 import sys sys.executable 我的mac电脑的Python安装路径为\n/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 \n指定Python 执行下方的代码(路径改为自己的Python安装路径)\nlibrary(reticulate) ## Warning: package \u0026#39;reticulate\u0026#39; was built under R version 3.6.2 use_python(\u0026#39;/Library/Frameworks/Python.framework/Versions/3.7/bin/python3\u0026#39;) 执行代码后，我们就可以愉快的使用Python代码\n运行Python代码 在Rmarkdown中写Python代码块","title":"在Rmarkdown中调用Python代码"},{"content":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/SciencePlot科研绘图.zip\n安装 !pip3 install SciencePlots Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/ Collecting SciencePlots Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c2/44/7b5c0ecd6f2862671a076425546f86ac540bc48c1a618a82d6faa3b26f58/SciencePlots-1.0.9.tar.gz (10 kB) Installing build dependencies ... \u001b[?25l/  tips:\nSciencePlots库需要电脑安装LaTex，其中\n MacOS电脑安装MacTex https://www.tug.org/mactex/ Windows电脑安装MikTex https://miktex.org/  初始化绘图样式 在SciencePlots库中科研绘图样式都是用的science\nimport matplotlib.pyplot as plt plt.style.use(\u0026#39;science\u0026#39;) 当然你也可以同时设置多个样式\nplt.style.use([\u0026#39;science\u0026#39;, \u0026#39;ieee\u0026#39;]) 在上面的代码中， ieee 会覆盖掉 science 中的某些参数（列宽、字号等）， 以达到符合 IEEE论文的绘图要求\n如果要临时使用某种绘图样式，科研使用如下语法\n#注意，此处是语法示例， #如要运行， 请提前准备好x和y的数据 with plt.style.context([\u0026#39;science\u0026#39;, \u0026#39;ieee\u0026#39;]): plt.figure() plt.plot(x, y) plt.show() 案例 定义函数曲线， 准备数据\nimport numpy as np import matplotlib.pyplot as plt def function(x, p): return x ** (2 * p + 1) / (1 + x ** (2 * p)) pparam = dict(xlabel=\u0026#39;Voltage (mV)\u0026#39;, ylabel=\u0026#39;Current ($\\mu$A)\u0026#39;) x = np.linspace(0.75, 1.25, 201) science样式 with plt.style.context([\u0026#39;science\u0026#39;]): fig, ax = plt.subplots() for p in [10, 15, 20, 30, 50, 100]: ax.plot(x, function(x, p), label=p) ax.legend(title=\u0026#39;Order\u0026#39;) ax.autoscale(tight=True) ax.set(**pparam) fig.savefig(\u0026#39;figures/fig1.pdf\u0026#39;) fig.savefig(\u0026#39;figures/fig1.jpg\u0026#39;, dpi=300)   science+ieee样式 针对IEEE论文准备的science+ieee样式\nwith plt.style.context([\u0026#39;science\u0026#39;, \u0026#39;ieee\u0026#39;]): fig, ax = plt.subplots() for p in [10, 20, 40, 100]: ax.plot(x, function(x, p), label=p) ax.legend(title=\u0026#39;Order\u0026#39;) ax.autoscale(tight=True) ax.set(**pparam) # Note: $\\mu$ doesn\u0026#39;t work with Times font (used by ieee style) ax.set_ylabel(r\u0026#39;Current (\\textmu A)\u0026#39;) fig.savefig(\u0026#39;figures/fig2a.pdf\u0026#39;) fig.savefig(\u0026#39;figures/fig2a.jpg\u0026#39;, dpi=300)   science+scatter样式 IEEE 要求图形以黑白打印时必须可读。 ieee 样式还可以将图形宽度设置为适合IEEE论文的一列。\nwith plt.style.context([\u0026#39;science\u0026#39;, \u0026#39;scatter\u0026#39;]): fig, ax = plt.subplots(figsize=(4, 4)) ax.plot([-2, 2], [-2, 2], \u0026#39;k--\u0026#39;) ax.fill_between([-2, 2], [-2.2, 1.8], [-1.8, 2.2], color=\u0026#39;dodgerblue\u0026#39;, alpha=0.2, lw=0) for i in range(7): x1 = np.random.normal(0, 0.5, 10) y1 = x1 + np.random.normal(0, 0.2, 10) ax.plot(x1, y1, label=r\u0026#34;$^\\#${}\u0026#34;.format(i+1)) ax.legend(title=\u0026#39;Sample\u0026#39;, loc=2) xlbl = r\u0026#34;$\\log_{10}\\left(\\frac{L_\\mathrm{IR}}{\\mathrm{L}_\\odot}\\right)$\u0026#34; ylbl = r\u0026#34;$\\log_{10}\\left(\\frac{L_\\mathrm{6.2}}{\\mathrm{L}_\\odot}\\right)$\u0026#34; ax.set_xlabel(xlbl) ax.set_ylabel(ylbl) ax.set_xlim([-2, 2]) ax.set_ylim([-2, 2]) fig.savefig(\u0026#39;figures/fig3.pdf\u0026#39;) fig.savefig(\u0026#39;figures/fig3.jpg\u0026#39;, dpi=300)   dark_background +science+high-vis 您还可以将这些样式与Matplotlib随附的其他样式结合使用。 例如，dark_background +science+high-vis样式：\nwith plt.style.context([\u0026#39;dark_background\u0026#39;, \u0026#39;science\u0026#39;, \u0026#39;high-vis\u0026#39;]): fig, ax = plt.subplots() for p in [10, 15, 20, 30, 50, 100]: ax.plot(x, function(x, p), label=p) ax.legend(title=\u0026#39;Order\u0026#39;) ax.autoscale(tight=True) ax.set(**pparam) fig.savefig(\u0026#39;figures/fig5.pdf\u0026#39;) fig.savefig(\u0026#39;figures/fig5.jpg\u0026#39;, dpi=300)   广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/scienceplots/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/SciencePlot科研绘图.zip\n安装 !pip3 install SciencePlots Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/ Collecting SciencePlots Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c2/44/7b5c0ecd6f2862671a076425546f86ac540bc48c1a618a82d6faa3b26f58/SciencePlots-1.0.9.tar.gz (10 kB) Installing build dependencies ... \u001b[?25l/  tips:\nSciencePlots库需要电脑安装LaTex，其中\n MacOS电脑安装MacTex https://www.tug.org/mactex/ Windows电脑安装MikTex https://miktex.org/  初始化绘图样式 在SciencePlots库中科研绘图样式都是用的science\nimport matplotlib.pyplot as plt plt.style.use(\u0026#39;science\u0026#39;) 当然你也可以同时设置多个样式\nplt.style.use([\u0026#39;science\u0026#39;, \u0026#39;ieee\u0026#39;]) 在上面的代码中， ieee 会覆盖掉 science 中的某些参数（列宽、字号等）， 以达到符合 IEEE论文的绘图要求\n如果要临时使用某种绘图样式，科研使用如下语法\n#注意，此处是语法示例， #如要运行， 请提前准备好x和y的数据 with plt.style.context([\u0026#39;science\u0026#39;, \u0026#39;ieee\u0026#39;]): plt.figure() plt.plot(x, y) plt.show() 案例 定义函数曲线， 准备数据\nimport numpy as np import matplotlib.pyplot as plt def function(x, p): return x ** (2 * p + 1) / (1 + x ** (2 * p)) pparam = dict(xlabel=\u0026#39;Voltage (mV)\u0026#39;, ylabel=\u0026#39;Current ($\\mu$A)\u0026#39;) x = np.","title":"科研绘图SciencePlots库"},{"content":"代码下载 点击跳转到下载链接页面\nR语言的ggplot2绘图能力超强，python虽有matplotlib，但是语法臃肿，使用复杂，入门极难，seaborn的出现稍微改善了matplotlib代码量问题，但是定制化程度依然需要借助matplotlib，使用难度依然很大。\n好消息是python中有一个plotnine包，可以实现绝大多数ggplot2的绘图功能，两者语法十分相似，R和Python的语法转换成本大大降低。\n plotnine文档 https://plotnine.readthedocs.io/en/latest/ R语言ggplot2文档 https://ggplot2.tidyverse.org/reference/index.html  安装 !pip3 install plotnine 准备数据 from plotnine.data import mpg #dataframe mpg.head()     manufacturer model displ year cyl trans drv cty hwy fl class     0 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact   1 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact   2 audi a4 2 2008 4 manual(m6) f 20 31 p compact   3 audi a4 2 2008 4 auto(av) f 21 30 p compact   4 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact    快速作图qplot 我们先直接看最简单好用的快速作图函数qplot(x, y, data)\n 横坐标displ 纵坐标cty 数据mpg  from plotnine import qplot qplot(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;, data=mpg)   ggplot图层 qplot是快速作图函数，如果想让图更好看，进行私人订制，那么我们需要进行图层设计\n首先设置ggplot图层（相当于买了一个高级画布），\n 数据mpg 横坐标x轴为displ 纵坐标y轴cty  在plotnine中，变量所对应的数据均可通过字段名调用\nfrom plotnine import ggplot, geom_point, aes ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg)   图层叠加 我们可以看到，已经绘制出一个空的ggplot图层，x轴为displ，y轴为cty。\n接下来我们给这个图层上加上数据对应的散点，使用geom_point()直接追加在ggplot图层之上即可。\n(ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg) + geom_point() )   color 在上图中，散点是没有区分每辆车的气缸数cyl。\n在geom_point()中，我们可以按照气缸数cyl分门别类，按照颜色显示出来\n(ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg) + geom_point(aes(color=\u0026#39;cyl\u0026#39;)) )   上图挺好看的，有时候需要绘制的字段是离散型数值，但是上色后可能不够明显，需要声明该字段为离散型。这时候用factor()来告诉plotnine，这个字段是离散型数值\n(ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg) + geom_point(aes(color=\u0026#39;factor(cyl)\u0026#39;)) )   size 有时候为了增加可视化显示的维度数，还可以考虑加入点的大小size\n(ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg) + geom_point(aes(size=\u0026#39;hwy\u0026#39;)) )   梯度色 如果你想自己设置颜色的梯度，可以通过scale_color_gradient设置\nfrom plotnine import scale_color_gradient (ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg) + geom_point(aes(color=\u0026#39;hwy\u0026#39;)) + scale_color_gradient(low=\u0026#39;blue\u0026#39;, high=\u0026#39;red\u0026#39;) )   条形图 plotnine中可绘制的图有很多，刚刚已经讲了散点图，接下来我们看看plotnine中的条形图。\n首先准备一下数据\nimport pandas as pd df = pd.DataFrame({ \u0026#39;variable\u0026#39;: [\u0026#39;gender\u0026#39;, \u0026#39;gender\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;income\u0026#39;, \u0026#39;income\u0026#39;, \u0026#39;income\u0026#39;, \u0026#39;income\u0026#39;], \u0026#39;category\u0026#39;: [\u0026#39;Female\u0026#39;, \u0026#39;Male\u0026#39;, \u0026#39;1-24\u0026#39;, \u0026#39;25-54\u0026#39;, \u0026#39;55+\u0026#39;, \u0026#39;Lo\u0026#39;, \u0026#39;Lo-Med\u0026#39;, \u0026#39;Med\u0026#39;, \u0026#39;High\u0026#39;], \u0026#39;value\u0026#39;: [60, 40, 50, 30, 20, 10, 25, 25, 40], }) df[\u0026#39;variable\u0026#39;] = pd.Categorical(df[\u0026#39;variable\u0026#39;], categories=[\u0026#39;gender\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;income\u0026#39;]) df[\u0026#39;category\u0026#39;] = pd.Categorical(df[\u0026#39;category\u0026#39;], categories=df[\u0026#39;category\u0026#39;]) df     variable category value     0 gender Female 60   1 gender Male 40   2 age 1-24 50   3 age 25-54 30   4 age 55+ 20   5 income Lo 10   6 income Lo-Med 25   7 income Med 25   8 income High 40    from plotnine import ggplot, aes, geom_text, position_dodge, geom_point #调整文本位置 dodge_text = position_dodge(width=0.9) # new (ggplot(df, aes(x=\u0026#39;variable\u0026#39;, y=\u0026#39;value\u0026#39;, fill=\u0026#39;category\u0026#39;)) #类别填充颜色 + geom_col(position=\u0026#39;dodge\u0026#39;, show_legend=False) # modified + geom_text(aes(y=-.5, label=\u0026#39;category\u0026#39;), # new position=dodge_text, color=\u0026#39;gray\u0026#39;, #文本颜色 size=8, #字号 angle=30, #文本的角度 va=\u0026#39;top\u0026#39;) + lims(y=(-5, 60)) # new )   from plotnine.data import economics_long economics_long.head()     date variable value value01     0 1967-07-01 00:00:00 pce 507.4 0   1 1967-08-01 00:00:00 pce 510.5 0.000266001   2 1967-09-01 00:00:00 pce 516.3 0.00076368   3 1967-10-01 00:00:00 pce 512.9 0.000471937   4 1967-11-01 00:00:00 pce 518.1 0.000918132    from plotnine import ggplot, aes, geom_line (ggplot(economics_long, aes(x=\u0026#39;date\u0026#39;, y=\u0026#39;value01\u0026#39;, color=\u0026#39;variable\u0026#39;)) + geom_line() )   plotnine目前已经支持绝大多数ggplot2，但是文档方面没有ggplot2全，所以学习plotnine时可以参考ggplot2。\n plotnine文档 https://plotnine.readthedocs.io/en/latest/ R语言ggplot2文档 https://ggplot2.tidyverse.org/reference/index.html  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/plotnine/","summary":"代码下载 点击跳转到下载链接页面\nR语言的ggplot2绘图能力超强，python虽有matplotlib，但是语法臃肿，使用复杂，入门极难，seaborn的出现稍微改善了matplotlib代码量问题，但是定制化程度依然需要借助matplotlib，使用难度依然很大。\n好消息是python中有一个plotnine包，可以实现绝大多数ggplot2的绘图功能，两者语法十分相似，R和Python的语法转换成本大大降低。\n plotnine文档 https://plotnine.readthedocs.io/en/latest/ R语言ggplot2文档 https://ggplot2.tidyverse.org/reference/index.html  安装 !pip3 install plotnine 准备数据 from plotnine.data import mpg #dataframe mpg.head()     manufacturer model displ year cyl trans drv cty hwy fl class     0 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact   1 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact   2 audi a4 2 2008 4 manual(m6) f 20 31 p compact   3 audi a4 2 2008 4 auto(av) f 21 30 p compact   4 audi a4 2.","title":"plotnine绘图 | python的ggplot2语法绘图包"},{"content":"一、文本的作用 文本涉及两个主体，即文本生产者和文本消费者：\n 文本生产者: 生成文本的主体；传递生产者想表达的内容，可能也会潜在蕴含着生产者的一些特质属性 文本消费者: 阅读文本的主体；消费者阅读这段文本时，文本又对消费者认知活动产生影响。  在大数据的今天，通过互联网超文本链接，无数的个人、团体、公司、政府等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。社会、管理、经济、营销、金融等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是从政治学和经管领域的两份文档翻译来，我觉得讲的挺明白的，其中加入了我的一些理解和扩充。\n\n二、 理解文本  text as text 原始的文本，定性的文本 textual data(text as data) 量化后的文本数据，可定量  2.1 text as text  text as text 原始的文本，定性的文本\n 文本的重点是传递着某种东西，从某种意义上说，所有形式的文本都包含可以被视为数据形式的信息。因此，文本总是以某种方式提供信息（即使我们不了解如何操作）。但是，言语活动的主要目标不是记录信息，而是进行交流：传达思想，指令，查询等。我们可以记录下来并将其视为数据，但是将我们的想法或思想表达为单词和句子的目的主要是交流，而不是将我们的想法或思想记录为数据形式。大多数数据是这样的：它表征的活动与数据本身完全不同。\n例如，在经济学中，可能是我们想要刻画的经济交易（使用价值媒介交换商品或服务），而数据是以某种聚合形式对这些交易进行抽象，这有助于我们理解交易的意义。通过就抽象的相关特征达成共识，我们可以记录并分析人类活动，例如制造业，服务业或农业。从通信行为中提取文本数据特征的过程遵循相同的过程，但有一个主要区别：由于原始文本可以直接通过记录的语言与我们交谈，因此文本首先不需要进行处理或抽象化待分析。但是，我在这里的论点是，特征抽象的过程是将文本视为数据而不是直接将其视为文本的方法的独特之处。\n具有讽刺意味的是，只有当我们破坏了直接理解文本的能力时，才有可能利用文本的数据获取洞察力。为了使它作为数据有用，我们必须消除原始文本的结构，将文本转换为结构化的表格数据。定量分析是理解非语言数据的起点；另一方面，非结构的文本变成丑陋表格数据的过程，出于统计分析或机器学习目的，我们经常质疑这一过程丢失了什么信息。\n机器是愚蠢的，但是将文本视为数据意味着让愚蠢的机器处理并可能分析我们的文本。关键是，为了将文本作为数据 而不是文本仅仅是文本，我们必须破坏原始文本的直接可解释性，但目的是从其样式化特征中进行更系统，更大规模的推断。我们应该坚定不移地认识到这一过程，但也不要因此而寝食不安，因为将文本作为数据进行分析的重点永远不是解释数据而是挖掘其深层次的模式。数据挖掘是一个破坏性的过程-正如采挖矿山资源-为了开采其宝贵资源，开发过程不可避免会破坏地表形态和环境。\n2.2 Latent versus manifest characteristics from textual data  textual data(text as data) 量化后的文本数据，可定量的数据。所以小标题我翻译为“量化后的文本数据隐藏的信息vs直观可见的信息”，\n 在政治学领域，我们通常最感兴趣的不是文本本身，而是文本透漏给我们有关作者的一些隐藏特性。在政治（以及心理学）研究中，我们有关政治和社会行为者的一些重要理论，很多时候直接观察行为活动很难观察到其内在的品质。\n例如，意识形态是研究政治竞争和政治偏好的基础，但是我们没有直接的衡量工具来记录个人或政党有关“社会和道德自由政策与保守政策”的相对偏好。其他偏好，包括支持或反对特定政策，如1846年废除了英国的《玉米法》（Schonhardt-Bailey，2003年）；在关于《莱肯公约》的辩论中支持或反对进一步的欧洲一体化（Benoit等，2005）；再比如支持或反对不信任运动（Laver和Benoit，2002年）。\n这些偏好是作为政治行为者的内部状态而存在的，无论这些行为者是立法者，政党，代表还是候选人，都无法直接观察。非言语行为指标也可用于推断这些信息，但事实表明，政治行为者所说的话比其他行为形式更为真诚。\n因此，文本数据（Textual data）可能包含有关取向和信念的重要信息，对于这些取向和信念，非语言形式的行为可能会充当不良指标。长期以来，心理学领域也一直将言语行为作为可观察到的潜在兴趣状态的暗示，例如人格特质（例如Tausczik和Pennebaker，2010年）。缺少增强的询问技术或头脑阅读技术来识别政治和社会行为者的偏好，信念，意图，偏见或个性，下一个最佳选择是根据其说话或书写的内容来收集和分析数据。关注的对象不是文本包含的内容，而是其内容作为有关潜在特征的数据所揭示的内容，这些潜在特征为其提供了可观察的含义。最后一句话比较难理解，可以理解为万事万物有联系，通过联系思维来挖掘文本中的信息。\n文本数据(Textual data)还可能具有较为明显的特征，例如，政治传播的许多领域都与文本所指出的潜在特征无关，而与文本本身所包含的传播形式和性质有关。举一个经典的例子，在一个著名的政治局委员对斯大林诞辰70周年之际的文章的研究中，莱特斯，伯努特和加索夫（1951）能够衡量各团体在共产主义意识形态方面的差异。在这一政治事件中，这些信息不仅预示了潜在的方向，而且还预示了在可预见的斯大林死后事件中有关领导权斗争的某种政治动作。这些信息本身是重要的，这些信息只能从每个政治局委员撰写的公开文章中搜集而来，它们必须充分了解将在党和苏联苏维埃新闻，并由其他政权参与者解释为信号。再举一个例子，如果我们对一个政治演说家是使用民粹主义还是种族主义语言感兴趣，那么该语言将直接以民粹主义或种族主义术语或参考形式出现在文本中，而要紧的是它们是否被使用。与其说这些术语代表什么，不如说是什么。例如Jagers和Walgrave（2007）在研究比利时政党的政党政治广播时，发现极右翼政党Vlaams Blok所使用的民粹词语远比其他比利时政党丰富的多。\n在实践中，从文本可观察到的明显特征与潜在特征之间的特征的有时候这两个概念区分的并不明显。举例来说，文体风格可以用一些明显的特征词对文本进行量化，体现出作者的一些写作偏好。例如，在使用适用于政治文本的可读性度量改编的研究中，我们可能会对政治成熟度的潜在水平感兴趣，这可以用来衡量说话者的意图或说话者的特征，这一点从观察到的文本样本中可以看出。或者，我们可能会对它们在可读性上的明显差异感兴趣，这是传播媒介更直接指标。例如，在对英国议会历史演讲的研究中，Spirling（2016）将19世纪末期向简单语言的转变归因于广播扩展特许经营的民主化效应。Benoit，Munger和Spirling（2019）使用类似的措施，比较了同一位总统当天在同一天发表的美国总统国情咨文演讲的样本，但其口头和书面形式均表明口头形式使用的语言较为简单。前一项研究可能对语言的易用性感兴趣，该语言的易用性是政治代表制更潜在的特征的指标，而后一项分析可能更侧重于交付媒介的明显后果。对于许多使用文本数据的研究设计而言，区别更多是研究目标的问题，而不是结构化和分析文本数据的某些内在方式。\n2.3 文本分析的步骤 完整的文本分析步骤包括:\n 读取数据 分词(中文必须有这一步，由于英文是空格间隔的语言，英文有时候不需要分词） 剔除符号和无意义的停止词 字母变小写，词干化 使用一定的编码方式构建文档词频矩阵     序号 步骤 解释 中文 英文     1 读取数据 数据一般存储于不同的文件夹不同文件内，需要将其导入到计算机     2 分词 导入到计算的文本是字符串数据，需要整理为更好用的列表 例如“我爱你中国”分词后\n得到[\u0026ldquo;我\u0026rdquo;, \u0026ldquo;爱\u0026rdquo;, \u0026ldquo;你\u0026rdquo;, \u0026ldquo;中国\u0026rdquo;] \u0026ldquo;I love China\u0026quot;分为\n[\u0026ldquo;I\u0026rdquo;, \u0026ldquo;love\u0026rdquo;, \u0026ldquo;China\u0026rdquo;]   3 剔除符号和无意义的停止词 为了降低计算机运行时间，对分析结果影响较小的字符，诸如符号和无意义的词语需要剔除掉 如“的”，“她”， ”呢”， “了” \u0026ldquo;is\u0026rdquo; , \u0026ldquo;a\u0026rdquo;, \u0026ldquo;the\u0026rdquo;   4 字母变小写，词干化 同义词归并，同主体词归并 “中铁”，“中国铁建”，“中铁集团”都可以归并为“中铁” 先变为小写，这样“I”和“i”都归并为“i”；\n“was”，“are”，“is”都归并为“be”   5 构建文档词频矩阵 使用一定的编码方式，即用某种方式表示文本。常见的有词袋法、tf-idf；\n可以使用scikit-learn构建文档词频矩阵，但中英文略有区别，需要注意 “我爱你中国”需要先整理为“我 爱 你 中国” “I love China”    \n三、常见的文本分析技术有  主题分析(Thematic analysis) 内容分析(content analysis) 基于词典的方法(dictionary analysis) 文本向量化(Bag-of-words) 监督学习如SVM、Bayes和Regression 无监督学习，如LDA话题模型 自然语言处理  上述文本分析技术，按照人与机器参与程度，绘制在下图。一般来说，越向右，文本分析技术的自动化程度越高，需要注意的是自动化越高，并不代表人的工作量就越少。\n3.1 主题分析Thematic Analysis 主题分析(Thematic analysis)是一种专家方法，一般与扎根理论方法相结合(Baumer, Mimno, Guha, Quan, \u0026amp; Gay, 2017)。扎根理论与主题分析的理念是基于专家自身经验和对世界的理解，做出对数据的见解，从而构建新理论。主题分析常见于组织科学和传播学(Gioia, Corley, \u0026amp; Hamilton, 2013; Strauss \u0026amp; Corbin, 1998)。\n主题分析涉及一个反复迭代的过程，在此过程中，研究人员将开发出一系列源自文本的代码和类别。除非要精炼理论，否则一般在分析开始之前尚不知道类别。在这种情况下，数据分析需要对文献和数据进行不断的比较。\n 研究人员从参与者自己的语言开始（称为“一阶编码”或“开放式编码”；Gioia等人，2013；Strauss＆Corbin，1998） 然后将相似的代码归为一类（称为“二阶代码”或“主轴编码”；Strauss＆Corbin，1998）。  诸如NVivo和ATLAS.ti之类的计算机软件可以帮助简化上述过程，但文本的分类通常依赖于人类编码衍生的类别的操作定义，计算机自动化的程度依旧很低，分析的数据量通常不大。而且编码过程对编码者的要求严格，通常是对该领域有较深理解的人才适合做此类工作。\n3.2 内容分析/基于词典的方法法 内容分析 和 其他基于字典的方法 通常是通过对特定文本中 单词/词组 的频率计数进行的（Reinard，2008；Short，Broberg，Cogliser＆Brigham，2010）。因为按照这种方法，文本数据被压缩成词组频数，定性的文本数据转化为定量的频数，索引可用于回答更多以定量为导向的研究问题（McKenny等，2016；Reinard，2008）。\n比如进行文本情感分析，我们可以用很简单的思路。即统计文本中正面词出现的总数和负面词出现的总数，得出文本的情感值。而在此分析过程中，我们需要事先拥有一个正面词词典和负面词词典。\n是否有成熟的领域词典、或者构建领域词典，这需要研究者对研究问题和研究的数据有一定的领域知识，工作量也会因是否有词典而不同。一般有现成的成熟的词典，计算机自动化程度高，人工工作量低。\n与主题分析类似，计算机软件可以协助内容分析过程。像DICTION这样的程序会使用 分类字典 自动对文本评分（即，根据单词或n-gram而非操作定义确定主题）。可以与主题分析类似地使用其他程序，例如NVivo或ATLAS.ti，在主题分析中，通过软件的帮助手动进行编码和分类，以组织数据。\n3.3 词袋法Bag-of-words 文本数据是非结构化的定性数据，计算机并不能直接使用。我们需要按照计算机容易理解的方式去组织数据，类似于上图的第一步骤,四段英文文本被组织成一个文档特征矩阵（document-feature-matrix），矩阵中\n 每一行代表一个英文文档 每一个列代表一个特征词  3.3.1 词袋法 vs 主题分析中的编码者 为了理解词袋法，可以类比主题分析 中的编码者。我们可以将词袋法看做是一个死板的，不知变通的人，脑子很简单，只知道统计特征词在每个文档中出现的词频。那么据此我们就知道词袋法和人的优缺点。\n对于词袋法，优点是规则标准统一，缺点是不知变通，牺牲了文本中很多的信息量。强调编码过程的高标准，牺牲了分析的深度。\n对于研究者参与 主题分析 这样的编码过程，优点是研究者有很强的领域知识和强大的洞察力，可以灵活洞察规律，缺点是每个研究者都具有特殊的经历和偏好，编码标准不统一。用研究者编码的过程，强调编码的深度和质量，牺牲了编码分析过程的标准性。\n3.3.2 词袋法的用途 词袋法编码是计算科学领域对文本数据的简化和压缩的方法，后续可以据此进行监督学习和无监督学习。\n3.4 监督学习 在有监督的方法中，研究人员事先知道ta正在寻找什么（罗伯茨等，2014）。比如要判断论文的作者身份这个问题，研究人员为程序提供输入（在这种情况下为文本）和输出（例如，文本作者的身份），然后系统创建一种算法来映射两者之间的联系（Janasik， Honkela和Bruun，2009年）。Mosteller and Wallace（1963）通过使用简单的贝叶斯单词概率来预测12篇有争议的联邦主义者论文（詹姆斯·麦迪逊或亚历山大·汉密尔顿）的作者身份。如今，朴素贝叶斯（Bayes）和支持向量机（SVM）等技术是用于文本分析的流行的监督算法（Manning，Prabhakar和Hinrich，2008年）。\n3.5 无监督学习 无监督算法，如主题分析（Janasik等，2009）可识别数据中的单词簇和主题。但是，与主题分析不同，主题建模使用高度自动化的方法来确定重要主题，分析过程所需的时间和领域知识相对较少。尽管人类的洞察力仍然对帮助解释出现的主题很重要，主题建模适合分析大规模文本数据（Kobayashi1，Mol，Berkers，Kismihok和Den Hartog，2017）。主题建模利用了主题分析（即人类洞察力、解释力）和机器学习（即快速分析大量文本）的优势。\n3.6 自然语言处理 最后，自然语言处理(Natural Language Processing)通常是文本分析中自动化程度最高的形式（有关综述，请参阅Manning等人，2008）。这种方法模拟了人类如何理解和处理语言（Chowdhury，2003；Collobert等，2011；Joshi，1991）。例如，NLP技术可以标记句子中单词的词性（例如，名词，形容词等），将文档从一种语言翻译成另一种语言，甚至使用句子的上下文来阐明词语的词义（Buntine＆Jakulin，2004年）。\n因此，与词袋法不同，NLP认为单词顺序很重要。当使用训练集时，使用深度学习和多模式（即结合文本和图像）等尖端技术进行情感分析是NLP的一种流行形式（Kouloumpis，Wilson和Moore，2011）。这种特殊的分析将文本的总体态度，情感或观点分类为肯定，否定或中立。\n与主题分析形成鲜明对比的是，自然语言处理是一个完全计算机自动化的过程，因此几乎不需要人类的理解和或解释（Quinn等人，2010）。此外，相对于需要人工编码（例如，主题分析）的技术，NLP的执行速度非常快，并且比其他方法更具系统性。例如，计算机科学，信息科学，语言学和心理学的研究人员利用NLP作为文本分析工具（Chowdhury，2003年）。\n大邓提醒一下，自然语言处理属于人工智能范畴，人工智能技术没有那么神，我们应该将其理解为“人工”+“智能”可能更妥当一些，即数据准备阶段用大量的人工时对数据进行标注，产生训练数据集合。之后借助于计算机的“智能”学习数据集中的规律，因此人工智能脱离了人工标注数据的喂养，只能做很简单的事情，更像是人工智障。\n3.7 不同文本分析技术汇总对比    技术 描述 优点 缺点 常被应用(领域) 软件     主题分析Thematic analysis 需要有经验的人员基于自身经验和李俊杰，对研究的数据进行挖掘。编码过程为迭代进行 使用参与者自己的话语或者构念来挖掘数据，对少量文本理解的更深入 属于时间、劳动密集型任务，不适合大规模数据。\n由于不同的编码人员有不同的经历和偏好，编码过程的标准不可靠 社会学、管理学 Nvivo；   内容分析/基于字典方法 统计文本中词语/词组的出现频率 允许对研究的数据进行定量分析 采用的词典应尽量与研究问题适应，词典适配性问题突出 管理学 LIWC、Nvivo、DICTION；   词袋法（Bag of words） 将文本字符串转为计算机能理解的数字化向量 编码标准稳定简单，具有统计学特性，扩展性强 编码过程忽略词语的先后顺序 管理学 Python的scikit-learn、gensim、nltk等；R   监督学习(Supervise models),如SVM、Bayes、Logistic Regression 研究者要知道输入数据X和标签y；需要核实的模型需要X和y之间的关系和规律 允许事先定义编码规则(如选择词袋法还是tfidf)；逻辑简单 需要高质量的标注数据(工作量大)；you与特征词太多，训练的模型很容易过拟合。 计算机学、政治学、管理学 Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）   无监督学习(Kmeans、 LDA话题模型) 使用聚类、话题分析，让计算机自动对数据进行分组 在没有人工标注的情况下，加速了数据的“标注”或“分类” “标注”是机器按照数字特征进行的分组，需要研究者解读才可以赋予“标准“意义；训练过程需要大量的调参 计算机学、政治学、管留学 Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）   自然语言处理 按照人类对语言的理解进行建模，考虑词语顺序 计算机自动化；可分析语义 大多数模型是人类无法解读的黑箱；\n虽然代码编程量小，但训练代码耗时巨大 计算科学；市场营销；心理学 pytorch、tensorflow    3.8 Python能做哪些？ 计算机能做的文本分析，Python都能做到，包括\n 基于词典的分析法；如基于词典法的情感计算 词袋法；可以进行文本相似度计算 有监督机器学习；如基于机器学习的情感分析；文本分类 无监督机器学习；lda话题模型对文本进行话题分析 自然语言处理；考虑词语顺序的LSTM  除了自然语言处理部分，四种方法在我的《Python网络爬虫与文本数据分析》视频课程中都有相关的讲解和实战代码\n\n相关文献  [1]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.\u0026rdquo; Journal of Marketing (2019): 0022242919873106.\n  [2]Kenneth Benoit. July 16, 2019. “[Text as Data: An Overview](https://kenbenoit.net/pdfs/28 Benoit Text as Data draft 2.pdf).” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. Handbook of Research Methods in Political Science and International Relations. Thousand Oaks: Sage.\n  [3]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. \u0026ldquo;A review of best practice recommendations for text analysis in R (and a user-friendly app).\u0026rdquo; Journal of Business and Psychology 33, no. 4 (2018): 445-459.\n \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/read_this_you_will_know_what_is_text_mining/","summary":"一、文本的作用 文本涉及两个主体，即文本生产者和文本消费者：\n 文本生产者: 生成文本的主体；传递生产者想表达的内容，可能也会潜在蕴含着生产者的一些特质属性 文本消费者: 阅读文本的主体；消费者阅读这段文本时，文本又对消费者认知活动产生影响。  在大数据的今天，通过互联网超文本链接，无数的个人、团体、公司、政府等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。社会、管理、经济、营销、金融等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是从政治学和经管领域的两份文档翻译来，我觉得讲的挺明白的，其中加入了我的一些理解和扩充。\n\n二、 理解文本  text as text 原始的文本，定性的文本 textual data(text as data) 量化后的文本数据，可定量  2.1 text as text  text as text 原始的文本，定性的文本\n 文本的重点是传递着某种东西，从某种意义上说，所有形式的文本都包含可以被视为数据形式的信息。因此，文本总是以某种方式提供信息（即使我们不了解如何操作）。但是，言语活动的主要目标不是记录信息，而是进行交流：传达思想，指令，查询等。我们可以记录下来并将其视为数据，但是将我们的想法或思想表达为单词和句子的目的主要是交流，而不是将我们的想法或思想记录为数据形式。大多数数据是这样的：它表征的活动与数据本身完全不同。\n例如，在经济学中，可能是我们想要刻画的经济交易（使用价值媒介交换商品或服务），而数据是以某种聚合形式对这些交易进行抽象，这有助于我们理解交易的意义。通过就抽象的相关特征达成共识，我们可以记录并分析人类活动，例如制造业，服务业或农业。从通信行为中提取文本数据特征的过程遵循相同的过程，但有一个主要区别：由于原始文本可以直接通过记录的语言与我们交谈，因此文本首先不需要进行处理或抽象化待分析。但是，我在这里的论点是，特征抽象的过程是将文本视为数据而不是直接将其视为文本的方法的独特之处。\n具有讽刺意味的是，只有当我们破坏了直接理解文本的能力时，才有可能利用文本的数据获取洞察力。为了使它作为数据有用，我们必须消除原始文本的结构，将文本转换为结构化的表格数据。定量分析是理解非语言数据的起点；另一方面，非结构的文本变成丑陋表格数据的过程，出于统计分析或机器学习目的，我们经常质疑这一过程丢失了什么信息。\n机器是愚蠢的，但是将文本视为数据意味着让愚蠢的机器处理并可能分析我们的文本。关键是，为了将文本作为数据 而不是文本仅仅是文本，我们必须破坏原始文本的直接可解释性，但目的是从其样式化特征中进行更系统，更大规模的推断。我们应该坚定不移地认识到这一过程，但也不要因此而寝食不安，因为将文本作为数据进行分析的重点永远不是解释数据而是挖掘其深层次的模式。数据挖掘是一个破坏性的过程-正如采挖矿山资源-为了开采其宝贵资源，开发过程不可避免会破坏地表形态和环境。\n2.2 Latent versus manifest characteristics from textual data  textual data(text as data) 量化后的文本数据，可定量的数据。所以小标题我翻译为“量化后的文本数据隐藏的信息vs直观可见的信息”，\n 在政治学领域，我们通常最感兴趣的不是文本本身，而是文本透漏给我们有关作者的一些隐藏特性。在政治（以及心理学）研究中，我们有关政治和社会行为者的一些重要理论，很多时候直接观察行为活动很难观察到其内在的品质。\n例如，意识形态是研究政治竞争和政治偏好的基础，但是我们没有直接的衡量工具来记录个人或政党有关“社会和道德自由政策与保守政策”的相对偏好。其他偏好，包括支持或反对特定政策，如1846年废除了英国的《玉米法》（Schonhardt-Bailey，2003年）；在关于《莱肯公约》的辩论中支持或反对进一步的欧洲一体化（Benoit等，2005）；再比如支持或反对不信任运动（Laver和Benoit，2002年）。\n这些偏好是作为政治行为者的内部状态而存在的，无论这些行为者是立法者，政党，代表还是候选人，都无法直接观察。非言语行为指标也可用于推断这些信息，但事实表明，政治行为者所说的话比其他行为形式更为真诚。\n因此，文本数据（Textual data）可能包含有关取向和信念的重要信息，对于这些取向和信念，非语言形式的行为可能会充当不良指标。长期以来，心理学领域也一直将言语行为作为可观察到的潜在兴趣状态的暗示，例如人格特质（例如Tausczik和Pennebaker，2010年）。缺少增强的询问技术或头脑阅读技术来识别政治和社会行为者的偏好，信念，意图，偏见或个性，下一个最佳选择是根据其说话或书写的内容来收集和分析数据。关注的对象不是文本包含的内容，而是其内容作为有关潜在特征的数据所揭示的内容，这些潜在特征为其提供了可观察的含义。最后一句话比较难理解，可以理解为万事万物有联系，通过联系思维来挖掘文本中的信息。\n文本数据(Textual data)还可能具有较为明显的特征，例如，政治传播的许多领域都与文本所指出的潜在特征无关，而与文本本身所包含的传播形式和性质有关。举一个经典的例子，在一个著名的政治局委员对斯大林诞辰70周年之际的文章的研究中，莱特斯，伯努特和加索夫（1951）能够衡量各团体在共产主义意识形态方面的差异。在这一政治事件中，这些信息不仅预示了潜在的方向，而且还预示了在可预见的斯大林死后事件中有关领导权斗争的某种政治动作。这些信息本身是重要的，这些信息只能从每个政治局委员撰写的公开文章中搜集而来，它们必须充分了解将在党和苏联苏维埃新闻，并由其他政权参与者解释为信号。再举一个例子，如果我们对一个政治演说家是使用民粹主义还是种族主义语言感兴趣，那么该语言将直接以民粹主义或种族主义术语或参考形式出现在文本中，而要紧的是它们是否被使用。与其说这些术语代表什么，不如说是什么。例如Jagers和Walgrave（2007）在研究比利时政党的政党政治广播时，发现极右翼政党Vlaams Blok所使用的民粹词语远比其他比利时政党丰富的多。\n在实践中，从文本可观察到的明显特征与潜在特征之间的特征的有时候这两个概念区分的并不明显。举例来说，文体风格可以用一些明显的特征词对文本进行量化，体现出作者的一些写作偏好。例如，在使用适用于政治文本的可读性度量改编的研究中，我们可能会对政治成熟度的潜在水平感兴趣，这可以用来衡量说话者的意图或说话者的特征，这一点从观察到的文本样本中可以看出。或者，我们可能会对它们在可读性上的明显差异感兴趣，这是传播媒介更直接指标。例如，在对英国议会历史演讲的研究中，Spirling（2016）将19世纪末期向简单语言的转变归因于广播扩展特许经营的民主化效应。Benoit，Munger和Spirling（2019）使用类似的措施，比较了同一位总统当天在同一天发表的美国总统国情咨文演讲的样本，但其口头和书面形式均表明口头形式使用的语言较为简单。前一项研究可能对语言的易用性感兴趣，该语言的易用性是政治代表制更潜在的特征的指标，而后一项分析可能更侧重于交付媒介的明显后果。对于许多使用文本数据的研究设计而言，区别更多是研究目标的问题，而不是结构化和分析文本数据的某些内在方式。\n2.3 文本分析的步骤 完整的文本分析步骤包括:\n 读取数据 分词(中文必须有这一步，由于英文是空格间隔的语言，英文有时候不需要分词） 剔除符号和无意义的停止词 字母变小写，词干化 使用一定的编码方式构建文档词频矩阵     序号 步骤 解释 中文 英文     1 读取数据 数据一般存储于不同的文件夹不同文件内，需要将其导入到计算机     2 分词 导入到计算的文本是字符串数据，需要整理为更好用的列表 例如“我爱你中国”分词后","title":"读完本文你就了解什么是文本分析"},{"content":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/数据分析plydata库学习.zip\nplydata是一个提供数据处理语法的Python库，借鉴了R语言dplyr，tidyr和forcats等包中的管道操作符。\nplydata使用 \u0026gt;\u0026gt;运算符 作为管道符号，或者使用ply（data，* verbs）函数代替 \u0026gt;\u0026gt;， 目前仅支持对pandas.DataFrame数据进行操作。\n安装 !pip3 install plydata 快速上手 import pandas as pd from plydata import define, query, if_else, ply df = pd.DataFrame({ \u0026#39;x\u0026#39;: [0, 1, 2, 3], \u0026#39;y\u0026#39;: [\u0026#39;zero\u0026#39;, \u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]}) df     x y     0 0 zero   1 1 one   2 2 two   3 3 three    define define函数名很简单，定义变量或者操作变量。\ndefine(data, *args,**kwargs)\n data 待操作的dataframe数据 args、kwargs 操作过程及结果。  比如我们想在df中新建一个z列，z列的值完全复制自x列。可以使用代码 define(df, z=\u0026lsquo;x\u0026rsquo;)\ndefine(df, z=\u0026#39;x\u0026#39;)     x y z     0 0 zero 0   1 1 one 1   2 2 two 2   3 3 three 3    注意: df中有x列，所以这里的使用的\u0026rsquo;x',而不是x。\n\u0026raquo;管道符 刚刚的问题可以使用管道符实现与define(df, z=\u0026lsquo;x\u0026rsquo;)相同的功能。\n#等同于df[\u0026#39;z\u0026#39;]=df[\u0026#39;x\u0026#39;] #等同于define(df, z=\u0026#39;x\u0026#39;) df \u0026gt;\u0026gt; define(z=\u0026#39;x\u0026#39;)     x y z     0 0 zero 0   1 1 one 1   2 2 two 2   3 3 three 3    如果有多个环节，可以用括号包裹住，环节与环节用\u0026gt;\u0026gt;和换行前后衔接。\n比如我们有多个操作，每一步操作如下\n m=2x n=m*m q=m+n  (df \u0026gt;\u0026gt; define(m=\u0026#39;2*x\u0026#39;) \u0026gt;\u0026gt; define(n=\u0026#39;m*m\u0026#39;) \u0026gt;\u0026gt; define(q=\u0026#39;m+n\u0026#39;) )     x y m n q     0 0 zero 0 0 0   1 1 one 2 4 6   2 2 two 4 16 20   3 3 three 6 36 42    上面所有的plydata相关操作不会修改原始数据df\ndf     x y     0 0 zero   1 1 one   2 2 two   3 3 three    if_else 在df中新建z列，z的值满足\n 当x大于1，z为1 当x小于等于1， z为0  使用if_else(predicate, true_value, false_value)\n predicate 逻辑判断条件字符串 true_value 满足逻辑条件返回的值 false_value 不满足逻辑条件返回的值  #等同于define(df, z=if_else(\u0026#39;x\u0026gt;1\u0026#39;, 1, 0)) df \u0026gt;\u0026gt; define(z=if_else(\u0026#39;x\u0026gt;1\u0026#39;, 1, 0))     x y z     0 0 zero 0   1 1 one 0   2 2 two 1   3 3 three 1    query query(data, expr)\n data 待查询的dataframe数据 expr 查询条件字符串  (df \u0026gt;\u0026gt; define(z=if_else(\u0026#39;x\u0026gt;1\u0026#39;, 1, 0)) \u0026gt;\u0026gt; query(\u0026#39;z==1\u0026#39;) )     x y z     2 2 two 1   3 3 three 1    ply() ply功能等同于管道符\u0026raquo;， 刚刚上面的代码\n(df \u0026gt;\u0026gt; define(z=if_else(\u0026#39;x\u0026gt;1\u0026#39;, 1, 0)) \u0026gt;\u0026gt; query(\u0026#39;z==1\u0026#39;) ) 可以用ply\nply(df, define(z=if_else(\u0026#39;x \u0026gt; 1\u0026#39;, 1, 0)), query(\u0026#39;z == 1\u0026#39;) )     x y z     2 2 two 1   3 3 three 1    plydata与plotnine 在R语言中，用ggplot2作图经常会用到管道符。而在Python中，plydata提供管道符，可以与作图库plotnine结合使用。\nfrom plotnine import ggplot, geom_line, aes from plydata import define, if_else import numpy as np df = pd.DataFrame({\u0026#39;x\u0026#39;: np.linspace(0, 2*np.pi, 500)}) (df \u0026gt;\u0026gt; define(y=\u0026#39;np.sin(x)\u0026#39;) \u0026gt;\u0026gt; define(sign=if_else(\u0026#39;y\u0026gt;=0\u0026#39;, \u0026#39;\u0026#34;pos\u0026#34;\u0026#39;, \u0026#39;\u0026#34;neg\u0026#34;\u0026#39;)) \u0026gt;\u0026gt; (ggplot(aes(x=\u0026#39;x\u0026#39;, y=\u0026#39;y\u0026#39;, color=\u0026#39;sign\u0026#39;))+ geom_line(size=1.5)) )   广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/plydata/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/数据分析plydata库学习.zip\nplydata是一个提供数据处理语法的Python库，借鉴了R语言dplyr，tidyr和forcats等包中的管道操作符。\nplydata使用 \u0026gt;\u0026gt;运算符 作为管道符号，或者使用ply（data，* verbs）函数代替 \u0026gt;\u0026gt;， 目前仅支持对pandas.DataFrame数据进行操作。\n安装 !pip3 install plydata 快速上手 import pandas as pd from plydata import define, query, if_else, ply df = pd.DataFrame({ \u0026#39;x\u0026#39;: [0, 1, 2, 3], \u0026#39;y\u0026#39;: [\u0026#39;zero\u0026#39;, \u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]}) df     x y     0 0 zero   1 1 one   2 2 two   3 3 three    define define函数名很简单，定义变量或者操作变量。","title":"数据分析plydata库"},{"content":"70G年报pdf数据集 数据下载说明 所有pdf均来自上海证券交易所官网，使用shreport库进行的下载。\n报告信息汇总文件  summary.xlsx内字段\n company 上市公司企业名 code 股票代码 type 报告类型 year 报告年份 date 报告发布日期 pdf 报告pdf文件下载链接  import pandas as pd from pathlib import Path #报告汇总文件summary.xlsx df = pd.read_excel(\u0026#39;summary.xlsx\u0026#39;) df.head() 一共有报告71126份\nlen(df) 71149 一共有上市公司1486家\nlen(df[\u0026#39;company\u0026#39;].unique()) 1486 summary文件夹 summary文件夹内是每家公司的报告披露情况\ndf1 = pd.read_excel(\u0026#39;summary/600000.xlsx\u0026#39;) df1.head() 浦发银行一共有75份定期报告\nlen(df1) 75 reports文件夹 reports文件夹存放着以各各公司股票代码命名的文件夹\n文件夹内是该公司所有定期报告\n读取pdf报告 可使用pdfdocx库读取pdf,\npdfdocx文档链接 https://github.com/thunderhit/pdfdocx\nfrom pdfdocx import read_pdf p_text = read_pdf(\u0026#39;reports/600000/600000_2012_1.pdf\u0026#39;) p_text Run\n上海浦东发展银行股份有限公司 \\n\\n2012 年第一季度报告 \\n\\n \\n\\n \\n\\n§1 重要提示 \\n\\n1.1 公司董事会、监事会及其董事、监事、高级管理人员保证本报告所载资料不存在任何虚假记载、\\n\\n误导性陈述或者重大遗漏，并对其内容的真实性、准确性和完整性承担个别及连带责任。\\n\\n1.2 公司于 2012 年 4 月 26 日以通讯表决的方式召开第四届董事会第二十六次会议审议通过本报告，\\n\\n1.4 公司董事长、行长吉晓辉、财务总监刘信义及财务机构负责人傅能声明：保证本季度报告中财务\\n\\n公司全体董事出席董事会会议并行使表决权。\\n\\n1.3 公司第一季度财务报告未经审计。\\n\\n报告的真实、完整。\\n\\n \\n§2 公司基本情况 \\n\\n2.1 主要会计数据及财务指标 \\n\\n本报告期末 \\n\\n上年度期末 \\n\\n币种:人民币 \\n\\n本报告期末比上年\\n度期末增减(%) \\n\\n总资产(千元) \\n\\n归属于上市公司股东的所有者权益(千元) \\n\\n2,804,646,567\\n\\n157,055,724\\n\\n2,684,693,689 \\n148,891,235 \\n\\n归属于上市公司股东的每股净资产(元) \\n\\n8.420\\n\\n7.982 \\n\\n4.47 \\n5.48 \\n5.49 \\n\\n经营活动产生的现金流量净额(千元) \\n\\n每股经营活动产生的现金流\\n\\n \\n\\n \\n \\n母公司现金流量表 \\n \\n2012 年 1—3 月 \\n \\n编制单位: 上海浦东发展银行股份有限公司.... 70G数据下载 链接:https://pan.baidu.com/s/14PI6MbxunFQ3fZOfR33zkw 密码:osoi\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/70g_china_market_anunal_report_datasets/","summary":"70G年报pdf数据集 数据下载说明 所有pdf均来自上海证券交易所官网，使用shreport库进行的下载。\n报告信息汇总文件  summary.xlsx内字段\n company 上市公司企业名 code 股票代码 type 报告类型 year 报告年份 date 报告发布日期 pdf 报告pdf文件下载链接  import pandas as pd from pathlib import Path #报告汇总文件summary.xlsx df = pd.read_excel(\u0026#39;summary.xlsx\u0026#39;) df.head() 一共有报告71126份\nlen(df) 71149 一共有上市公司1486家\nlen(df[\u0026#39;company\u0026#39;].unique()) 1486 summary文件夹 summary文件夹内是每家公司的报告披露情况\ndf1 = pd.read_excel(\u0026#39;summary/600000.xlsx\u0026#39;) df1.head() 浦发银行一共有75份定期报告\nlen(df1) 75 reports文件夹 reports文件夹存放着以各各公司股票代码命名的文件夹\n文件夹内是该公司所有定期报告\n读取pdf报告 可使用pdfdocx库读取pdf,\npdfdocx文档链接 https://github.com/thunderhit/pdfdocx\nfrom pdfdocx import read_pdf p_text = read_pdf(\u0026#39;reports/600000/600000_2012_1.pdf\u0026#39;) p_text Run\n上海浦东发展银行股份有限公司 \\n\\n2012 年第一季度报告 \\n\\n \\n\\n \\n\\n§1 重要提示 \\n\\n1.","title":"70G上交所年报数据集"},{"content":"代码下载 点击下载\n在数据分析中，Python和R各有千秋，虽然Python或R都能在数据分析打通关，从采集、清洗(预处理)、分析、可视化，但是在不同的环节，不同的语言易用程度不同。Python胜在干脏活累活，诸如数据采集、数据清洗、机器学习等；而R语言胜在统计分析、可视化等。所以，如果你正好Python和R都入门了，可以考虑两者结合。那么就会遇到今天的问题-如何在jupyter中使用R语言代码？\nrpy2包 rpy2包首先是Python包，ta衔接了Python和R，通过rpy2可以运行R语言相关代码、函数、包。\n在Jupyter notebook中主要有两种情况\n 单元格中以Python为主，可以插入R的代码字符串 单个的单元格要么只有R代码，要么只有Python代码  遇到这类问题，各位的电脑要确保\n 电脑已经安装了Python和R 已安装rpy2包  安装rpy2包\n!pip3 install rpy2 import rpy2.robjects as robjects from rpy2.robjects import pandas2ri #R代码运行会尽量以DataFrame显示 pandas2ri.activate() #运行R代码 robjects.r(\u0026#39;R代码字符串\u0026#39;) 运行R代码 rpy2.robjects.r(\u0026#39;R代码字符串\u0026#39;) rpy2.robjects.r()函数会识别 R代码字符串, 并将其执行。\nimport rpy2.robjects as robjects from rpy2.robjects import pandas2ri import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) #直接声明，data frame强制转为DataFrame pandas2ri.activate() #R代码 r_code = \u0026#34;\u0026#34;\u0026#34; library(dplyr) text \u0026lt;- c(\u0026#34;Because I could not stop for Death -\u0026#34;, \u0026#34;He kindly stopped for me -\u0026#34;, \u0026#34;The Carriage held but just Ourselves -\u0026#34;, \u0026#34;and Immortality\u0026#34;) text_df \u0026lt;- tibble(docid=1:4, text=text) text_df \u0026#34;\u0026#34;\u0026#34; #运行R代码 robjects.r(r_code)     docid text     1 1 Because I could not stop for Death -   2 2 He kindly stopped for me -   3 3 The Carriage held but just Ourselves -   4 4 and Immortality     准备一个R代码r_code，该代码会生成R语言的tibble数据类型(R语言中的一种data frame)。 robjects.r(r_code) 运行R代码(字符串) 在本例中，使用pandas2ri.activate()强制声明，在Python中将变量text_df以pandas.DataFrame显示  调用R数据集 rpy2.robjects.r.data(\u0026#39;R的数据集名\u0026#39;) 调用R的数据集\nimport rpy2.robjects as robjects  robjects.r.data(\u0026lsquo;R数据集名\u0026rsquo;) 准备R数据集，此时Python并没有显示该数据集，可以理解为准备数据 robjects.r['R数据集名'] 导入R的数据集  import rpy2.robjects as robjects from rpy2.robjects import pandas2ri pandas2ri.activate() #准备iris robjects.r.data(\u0026#39;iris\u0026#39;) #导入iris iris = robjects.r[\u0026#39;iris\u0026#39;] iris.head()     Sepal.Length Sepal.Width Petal.Length Petal.Width Species     1 5.1 3.5 1.4 0.2 setosa   2 4.9 3 1.4 0.2 setosa   3 4.7 3.2 1.3 0.2 setosa   4 4.6 3.1 1.5 0.2 setosa   5 5 3.6 1.4 0.2 setosa    调用R语言包 rpy2.robjects.packages.importr(\u0026#39;R包名\u0026#39;) R语言中的readr包有read_csv()函数，可以读取csv文件。\nfrom rpy2.robjects.packages import importr from rpy2.robjects import pandas2ri pandas2ri.activate() #导入R语言中的readr包 readr = importr(\u0026#34;readr\u0026#34;) #使用readr包中的read_csv()函数 mtcars = readr.read_csv(\u0026#34;mtcars.csv\u0026#34;) mtcars.head()     car mpg cyl disp hp drat wt qsec vs am gear carb     1 Mazda RX4 21 6 160 110 3.9 2.62 16.46 0 1 4 4   2 Mazda RX4 Wag 21 6 160 110 3.9 2.875 17.02 0 1 4 4   3 Datsun 710 22.8 4 108 93 3.85 2.32 18.61 1 1 4 1   4 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1   5 Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.02 0 0 3 2    Cell只运行R代码 上面的几个章节中，每个cell中代码均为Python和R的混用，如果在Cell中只运行R代码，\n 可以先单独在一个cell中运行 %load_ext rpy2.ipython, 在另外一个cell中使用%%R声明本cell中使用的是R代码。  %load_ext rpy2.ipython %%R library(ggplot2) ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species))+ geom_point()   %%R -h 550 -w 800 #设置宽、高 library(ggplot2) ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species))+ geom_point()   了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/call_r_language_code_in_jupyter/","summary":"代码下载 点击下载\n在数据分析中，Python和R各有千秋，虽然Python或R都能在数据分析打通关，从采集、清洗(预处理)、分析、可视化，但是在不同的环节，不同的语言易用程度不同。Python胜在干脏活累活，诸如数据采集、数据清洗、机器学习等；而R语言胜在统计分析、可视化等。所以，如果你正好Python和R都入门了，可以考虑两者结合。那么就会遇到今天的问题-如何在jupyter中使用R语言代码？\nrpy2包 rpy2包首先是Python包，ta衔接了Python和R，通过rpy2可以运行R语言相关代码、函数、包。\n在Jupyter notebook中主要有两种情况\n 单元格中以Python为主，可以插入R的代码字符串 单个的单元格要么只有R代码，要么只有Python代码  遇到这类问题，各位的电脑要确保\n 电脑已经安装了Python和R 已安装rpy2包  安装rpy2包\n!pip3 install rpy2 import rpy2.robjects as robjects from rpy2.robjects import pandas2ri #R代码运行会尽量以DataFrame显示 pandas2ri.activate() #运行R代码 robjects.r(\u0026#39;R代码字符串\u0026#39;) 运行R代码 rpy2.robjects.r(\u0026#39;R代码字符串\u0026#39;) rpy2.robjects.r()函数会识别 R代码字符串, 并将其执行。\nimport rpy2.robjects as robjects from rpy2.robjects import pandas2ri import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) #直接声明，data frame强制转为DataFrame pandas2ri.activate() #R代码 r_code = \u0026#34;\u0026#34;\u0026#34; library(dplyr) text \u0026lt;- c(\u0026#34;Because I could not stop for Death -\u0026#34;, \u0026#34;He kindly stopped for me -\u0026#34;, \u0026#34;The Carriage held but just Ourselves -\u0026#34;, \u0026#34;and Immortality\u0026#34;) text_df \u0026lt;- tibble(docid=1:4, text=text) text_df \u0026#34;\u0026#34;\u0026#34; #运行R代码 robjects.","title":"rpy2包 | 在Jupyter中调用R语言的代码"},{"content":"数据集下载 链接:https://pan.baidu.com/s/1rUfj7NYYAnM3NuFWCHyPpA 密码:ux8z\n在昨天发的文章里提到了yelp数据集，官网显示\u0026quot;Yelp数据集是我们用于个人，教育和学术目的的业务，评论和用户数据的子集。 以JSON文件形式提供，可用于在学习如何制作移动应用程序的同时，向学生介绍数据库，学习NLP或提供示例生产数据。\u0026quot;\nyelp官网显示，这个数据集亮点如下：\n 668+w条评论 19+w个商业机构 20w张图片 10个都市区域 字段包括:营业时间、是否泊车、可用性和环境  在kaggle上也能看到使用这个数据集的案例，说不定有你需要的分析方法。\n我已经从yelp官网下载了数据, 参照kaggle的很多例子，咱们也在自己电脑上跑跑简单的分析\nbusiness数据读取 kaggle中的数据是csv文件，咱这里是json，略有不同，但读取都可以用pandas读取\nimport pandas as pd #一开始用注释掉的代码，有bug，经过百度找到lines=True解决方案 #business = pd.read_json(\u0026#39;yelp_dataset/business.json\u0026#39;) business = pd.read_json(\u0026#39;yelp_dataset/business.json\u0026#39;, lines=True) business.head() 评分分布 import seaborn as sns sns.color_palette() [(0.12156862745098039, 0.4666666666666667, 0.7058823529411765), (1.0, 0.4980392156862745, 0.054901960784313725), (0.17254901960784313, 0.6274509803921569, 0.17254901960784313), (0.8392156862745098, 0.15294117647058825, 0.1568627450980392), (0.5803921568627451, 0.403921568627451, 0.7411764705882353), (0.5490196078431373, 0.33725490196078434, 0.29411764705882354), (0.8901960784313725, 0.4666666666666667, 0.7607843137254902), (0.4980392156862745, 0.4980392156862745, 0.4980392156862745), (0.7372549019607844, 0.7411764705882353, 0.13333333333333333), (0.09019607843137255, 0.7450980392156863, 0.8117647058823529)]  import matplotlib.pyplot as plt import seaborn as sns colors = sns.color_palette() rating = business[\u0026#39;stars\u0026#39;].value_counts() rating.sort_index(inplace=True) rating.plot(kind=\u0026#39;bar\u0026#39;, figsize=(10, 5), color=colors[:9], rot=0) #字体倾斜角度 plt.title(\u0026#39;Rating Distribution of Yelp\u0026#39;,fontweight=\u0026#39;bold\u0026#39;) plt.show() 行业统计 查看行业店家数量分布\nimport numpy as np business[\u0026#39;categories\u0026#39;] = business[\u0026#39;categories\u0026#39;].apply(lambda x: x if x else \u0026#39; \u0026#39;) category_str = \u0026#39;,\u0026#39;.join(business[\u0026#39;categories\u0026#39;]) category_list = category_str.split(\u0026#39;,\u0026#39;) category_df = pd.DataFrame(category_list, columns=[\u0026#39;category\u0026#39;]) top15_category = category_df[\u0026#39;category\u0026#39;].value_counts()[:15] top15_category top15_category.plot(kind=\u0026#39;bar\u0026#39;, color=colors[:20], figsize=(20, 10), rot=30, fontsize=20) plt.title(\u0026#39;Top 20 Category in Yelp\u0026#39;, fontsize=25, fontweight=\u0026#39;bold\u0026#39;) plt.show() 城市分布 显示yelp中Top20城\ncitys = business[\u0026#39;city\u0026#39;].value_counts()[:20] citys.sort_values(ascending=True, inplace=True)#降序，原地修改原始数据 citys.plot(kind=\u0026#39;barh\u0026#39;, #水平条形图 figsize=(10, 15), fontsize=20, color=colors[:20]) plt.title(\u0026#39;Top 20 city in the Yelp\u0026#39;, fontsize=20, fontweight=\u0026#39;bold\u0026#39;) plt.show() 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/yelpdataset_10g/","summary":"数据集下载 链接:https://pan.baidu.com/s/1rUfj7NYYAnM3NuFWCHyPpA 密码:ux8z\n在昨天发的文章里提到了yelp数据集，官网显示\u0026quot;Yelp数据集是我们用于个人，教育和学术目的的业务，评论和用户数据的子集。 以JSON文件形式提供，可用于在学习如何制作移动应用程序的同时，向学生介绍数据库，学习NLP或提供示例生产数据。\u0026quot;\nyelp官网显示，这个数据集亮点如下：\n 668+w条评论 19+w个商业机构 20w张图片 10个都市区域 字段包括:营业时间、是否泊车、可用性和环境  在kaggle上也能看到使用这个数据集的案例，说不定有你需要的分析方法。\n我已经从yelp官网下载了数据, 参照kaggle的很多例子，咱们也在自己电脑上跑跑简单的分析\nbusiness数据读取 kaggle中的数据是csv文件，咱这里是json，略有不同，但读取都可以用pandas读取\nimport pandas as pd #一开始用注释掉的代码，有bug，经过百度找到lines=True解决方案 #business = pd.read_json(\u0026#39;yelp_dataset/business.json\u0026#39;) business = pd.read_json(\u0026#39;yelp_dataset/business.json\u0026#39;, lines=True) business.head() 评分分布 import seaborn as sns sns.color_palette() [(0.12156862745098039, 0.4666666666666667, 0.7058823529411765), (1.0, 0.4980392156862745, 0.054901960784313725), (0.17254901960784313, 0.6274509803921569, 0.17254901960784313), (0.8392156862745098, 0.15294117647058825, 0.1568627450980392), (0.5803921568627451, 0.403921568627451, 0.7411764705882353), (0.5490196078431373, 0.33725490196078434, 0.29411764705882354), (0.8901960784313725, 0.4666666666666667, 0.7607843137254902), (0.4980392156862745, 0.4980392156862745, 0.4980392156862745), (0.7372549019607844, 0.7411764705882353, 0.13333333333333333), (0.09019607843137255, 0.7450980392156863, 0.8117647058823529)]  import matplotlib.pyplot as plt import seaborn as sns colors = sns.","title":"YelpDaset | 酒店管理类数据集10+G"},{"content":"哈尔滨        了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/beautiful_harbin_scene/","summary":"哈尔滨        了解课程  点击上方图片购买课程   点击进入详情页","title":"哈尔滨的美景"},{"content":"代码下载 点击下载代码\npandas_bokeh pandas_bokeh可以使得dataframe直接调用bokeh底层代码。通过使用pandas_bokeh，可以在notebook或者html中显示，语法相比于bokeh更简洁易用。\n安装 !pip3 install pandas_bokeh 快速上手 对fruits.csv做一个条形图\nimport pandas as pd df = pd.read_excel(\u0026#39;fruits.xlsx\u0026#39;) df     fruits 2015 2016 2017     0 苹果 2 5 3   1 梨 1 3 2   2 香蕉 4 3 4   3 草莓 3 2 4   4 樱桃 2 4 5   5 橘子 4 6 3    import pandas as pd import pandas_bokeh import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) #忽略某些不影响程序的提示 #在notebook中能显示可视化结果 pandas_bokeh.output_notebook() #将fruits列设置为行索引 df = pd.read_excel(\u0026#39;fruits.xlsx\u0026#39;) df.plot_bokeh(kind=\u0026#39;bar\u0026#39;, x = \u0026#39;fruits\u0026#39;, #将fruits列选做x轴 y = [\u0026#39;2015\u0026#39;, \u0026#39;2016\u0026#39;, \u0026#39;2017\u0026#39;], #将年份选做y轴 ylabel=\u0026#39;水果价格(元/斤)\u0026#39;, title=\u0026#39;水果\u0026#39;, show_figure=True) #显示   上面的例子已经可以看到该库的简洁优美之处，现在我们多学点\npandas_bokeh输出设定  pandas_bokeh.output_notebook() 在notebook中能显示可视化结果 pandas_bokeh.output_file(filename) 将结果输出到html文件中  支持可视化图种类  line bar point scatter histogram area pie map  以bar为例，调用可视化接口时，有以下两种使用方法\n df.plot_bokeh.line(\u0026hellip;) df.plot_bokeh(kind=\u0026lsquo;line\u0026rsquo;)  import numpy as np df = pd.read_excel(\u0026#39;fake_stocks.xlsx\u0026#39;) df.plot_bokeh(kind=\u0026#34;line\u0026#34;, x=\u0026#39;日期\u0026#39;, #将excel中的日期列当做x轴 y=[\u0026#39;Google\u0026#39;, \u0026#39;Apple\u0026#39;]) #将\u0026#39;Google\u0026#39;, \u0026#39;Apple\u0026#39;两列作为y轴   高级参数 df.plot_bokeh(kind, x, y, figsize, title, xlim, ylim, xlabel, ylabel, logx, logy, xticks, yticks, color, colormap, hovertool, zooming, panning, **kwargs)  kind: 支持的图种类\u0026quot;line\u0026quot;, \u0026ldquo;point\u0026rdquo;, \u0026ldquo;scatter\u0026rdquo;, \u0026ldquo;bar\u0026rdquo; ,\u0026ldquo;histogram\u0026quot;等 x: 选中数据某列名作为x轴。如果x不传入参数，会默认使用df的索引作为x轴 y: 将数据中的某列或某些列指定为y轴 figsize: 图的尺寸,如figsize=(600, 350) title: 图的标题 xlim/ylim: 设置图的x轴和y轴的范围 xlabel/ylabel: 设置x轴和y轴的名字 logx/logy: 布尔型值，对x和y的数据是否进行log变换 xticks/yticks: 显性定义横纵坐标刻度 color: 对图中使用同一的颜色，如果想定义多种颜色，请使用colormap参数 colormap: 可以对图中的不同对象设置颜色， 传入的是颜色字符串列表。 hovertool: 默认True，鼠标放在图上会悬浮显示具体信息。 zooming: 布尔值，默认True支持缩放 panning: 布尔值，默认True支持平移 kwargs**: 更多参数设定请看官方文档  文档  pandas_bokeh文档地址https://github.com/PatrikHlobil/Pandas-Bokeh Bokeh官方文档地址https://docs.bokeh.org/en/latest/  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pandas_bokeh_vis/","summary":"代码下载 点击下载代码\npandas_bokeh pandas_bokeh可以使得dataframe直接调用bokeh底层代码。通过使用pandas_bokeh，可以在notebook或者html中显示，语法相比于bokeh更简洁易用。\n安装 !pip3 install pandas_bokeh 快速上手 对fruits.csv做一个条形图\nimport pandas as pd df = pd.read_excel(\u0026#39;fruits.xlsx\u0026#39;) df     fruits 2015 2016 2017     0 苹果 2 5 3   1 梨 1 3 2   2 香蕉 4 3 4   3 草莓 3 2 4   4 樱桃 2 4 5   5 橘子 4 6 3    import pandas as pd import pandas_bokeh import warnings warnings.","title":"使用pandas_bokeh做可视化"},{"content":" 本文提供了一条通往实际“被动”收入和财务自由的现实途径\nSrc https://themakingofamillionaire.com/passive-income-is-a-lie-but-scalable-income-is-real-f0483af14483\nAuthor: Ben Le Fort\n 如今，个人理财中最大的流行语之一是“被动收入”。 不乏网络营销人员想要向您推销的想法是，如果您只是听他们的（并购买他们的“课程”），您也可以以最少的努力开始创收。\n但事情是这样的； 被动收入的想法是一堆废话。这并不意味着您无法开展数字业务并永远改变您的财务生活，因为您绝对可以而且应该这样做。\n在本文中，我将解释\n 为什么被动收入是一个神话？ 可扩展收入如何运作的示例 从财务角度，多久才能退休，且只靠利息过活？ 增加可扩展的收入帮助我节省了第一笔 10 万美元 我总是记得“为什么”我这样做  为什么被动收入是一个神话？ 正如我过去所写，博主和 YouTuber 已经开始将任何数字业务称为“被动收入机会”。\n 联属网络营销。 销售数码产品/软件。 销售信息产品，如课程和书籍。 在线销售实物商品。 在 YouTube 频道、播客或博客上展示广告。  通常，当您听到有人提到“被动收入”时，他们真正指的是可扩展的商业收入。 可扩展的收入意味着不能保证您赚到一毛钱，但您的收入潜力没有限制。\n可扩展收入如何运作的示例 我的业务之一就是销售在线课程，这是可扩展收入的一个完美例子。\n 如果没有人买我的课程，我就赚不到钱。 既然是数字产品，卖额外的课程也不用花我额外的钱。 这与销售实物的人形成鲜明对比。 如果我卖电脑，我会为我卖的每台电脑支付额外的费用。 在经济学中，这被称为“边际生产成本”。 销售数字产品，尤其是像课程这样的信息产品意味着我的边际生产成本几乎为零。 这意味着如果我出售价值 0 美元或 100 万美元的课程，我的成本将是相同的。  这就是拥有可扩展收入的含义，也就是大多数人所说的“被动收入”。\n诚然，每增加一美元可扩展收入，您就需要减少工作量； 这不是说你可以抬起脚停止工作。 被动收入是您无需做任何工作就可以赚取的钱。 如果我停止从事我的业务，收入很快就会枯竭。被动收入的唯一真正来源来自投资收入。\n 股票分红。 从债券中赚取的利息。 从房地产收取的租金。  除非你继承了很多钱，否则你产生大量被动收入的唯一方法是通过工作、入不敷出以及投资差额来产生主动收入。\n不要相信任何告诉你不同的人。 那些告诉你不工作也能赚钱的人要么是想卖给你东西，要么是妄想，要么两者兼而有之。\n主动收入可以来自朝九晚五的工作或收入可扩展的企业。 或者，就我而言，两者兼而有之。\n**致富的简单途径在于保持开支不变，增加收入，并投资差价。**过去，我写过关于我们可以利用“两个杠杆”来提高储蓄率的文章。\n 对已有用的财富，尽可能节省节俭。 赚更多的钱。  **学会节俭但轻松幸福地生活，是迈向财务独立的关键一步。 **一旦你养成了良好的消费习惯，积累财富的最快途径就是不断寻找增加收入的方法，同时保持生活成本稳定并投资差额。\n 如果你能靠 25,000 美元生活并赚 50,000 美元，那意味着你有 25,000 美元可以投资。 如果你能靠 25,000 美元生活并赚 75,000 美元，那意味着你有 50,000 美元可以投资。 如果你能靠 25,000 美元生活并赚 125,000 美元，那意味着你有 100,000 美元可以投资。  如果你能找到一种每年投资收益数倍于你的年支出的方法，你的个人财富很快就会像火箭一样起飞。这是一个简单的概念，但在现实生活中要困难得多。除非您准备实行极端节俭，否则您不太可能每年通过朝九晚五的工作获得的薪水节省 2-4 倍的年度开支。当所有的税收和减免都从你的薪水中扣除，你减去你需要支付的住房、交通、食品和其他基本生活费用时，月底很可能已经所剩无几。\n这就是为什么创造收入可扩展的副业对我来说是一个改变游戏规则的人。\n从财务角度，多久才能退休，且只靠利息过活？ 不考虑退休金，只考虑工作多久，个人的财富利息收入能hold住个人年开支。需要确定以下四个指标\n 个人现金积蓄 money_pool 个人年收入（平均） avg_income 个人年支出（平均） avg_budget 投资年华收益率(平均) rate_of_return  def work_n_year_to_retire(money_pool, avg_income, avg_budget, rate_of_return): \u0026#34;\u0026#34;\u0026#34; money_pool: 现有储蓄金额(元) avg_income: 平均年收入(元) avg_budget: 年支出 rate_of_return: 年化收益率 \u0026#34;\u0026#34;\u0026#34; n = 0 while True: if money_pool*rate_of_return\u0026gt;=avg_budget: break else: money_pool = (money_pool + avg_income - avg_budget)*(1+rate_of_return) n = n + 1 return n, money_pool work_n_year_to_retire(money_pool=0, avg_income=200000, avg_budget=50000, rate_of_return=0.03) Run\n收入固定，改变节俭程度(consumption_ratio)\n 假设现有存款0元， 年收入20w， 年支出13w, 年华收益3%， 需要工作35年 积累435w的财富年利息收入才能支撑个人支出 假设现有存款0元， 年收入20w， 年支出10w, 年华收益3%， 需要工作23年 积累334w的财富年利息收入才能支撑个人支出 假设现有存款0元， 年收入20w， 年支出7w, 年华收益3%， 需要工作15年 积累249w的财富年利息收入才能支撑个人支出 假设现有存款0元， 年收入20w， 年支出5w, 年华收益3%， 需要工作10年 积累177w的财富年利息收入才能支撑个人支出  节俭程度固定，改变年收入\n 假设现有存款0元，年支出5w, 年收入6w， 年华收益3%， 需要工作60年 积累167w的财富年利息收入才能支撑个人支出 假设现有存款0元，年支出5w, 年收入10w， 年华收益3%， 需要工作23年 积累167w的财富年利息收入才能支撑个人支出 假设现有存款0元，年支出5w, 年收入15w， 年华收益3%， 需要工作14年 积累176w的财富年利息收入才能支撑个人支出 假设现有存款0元，年支出5w, 年收入20w， 年华收益3%， 需要工作10年 积累177w的财富年利息收入才能支撑个人支出  增加可扩展的收入帮助我节省了第一笔 10 万美元 两年前，我把写作当成副业。 在我写作的第一个月，我赚了 15 美元。 我一直在写作和建立观众，慢慢地，写作的收入开始增加。 2020 年初，我创建了一个在线课程以进一步通过我的写作获利，并且最近开始专注于建立一个 YouTube 频道。\n总而言之，我的兼职收入在每月 2,500 美元至 4,000 美元之间或每年在 30,000 美元至 54,000 美元之间。 每一分钱都投资于股市。 我可以将我的副业赚到的每一分钱都投资到股票市场，因为我可以用日常工作的收入来支付我所有的生活费用，然后还有一部分。 我的副业收入帮助我在我的投资账户中存入了超过 100,000 美元，比我想象的要早几年。\n我总是记得“为什么”我这样做 让我说清楚； 我所做的基本上是做两份工作。 我的大部分晚上和周末都花在增加我的副业收入上。这并不容易，但对我来说，这是值得的。 去年，我当了父亲，积累财富的概念有了全新的意义。 我的重点不仅是为我的余生提供财务保障，而且还为我儿子的整个生活和他可能拥有的任何孩子提供财务保障。\n我希望我的儿子和我家人的后代不要为钱而紧张。 金钱能买到的最宝贵的东西就是自由； 可以自由地做自己喜欢的事情。 许多人从事他们并不真正喜欢的工作，原因有两个。\n 他们需要一份工作来支付账单。 没有人愿意给他们“梦寐以求的工作”。  结果，他们没有自由来做自己喜欢的事情。通过致力于学习如何建立一个我喜欢的副业（我确实喜欢我的副业）并获得可扩展的收入，我可以通过两种方式为我的儿子提供这种自由。\n 积累足够的财务财富来维持我们家庭的生活开支。 这意味着他不需要接受一份他不喜欢支付账单的工作。 学习我可以传给我儿子的技能，这样他就可以学习如何通过做他热爱的工作来谋生。 这意味着，如果其他人不愿意为他提供他梦寐以求的工作，他将拥有出去为自己创造的工具。  与实现这一愿景的好处相比，我在接下来的几年中需要做出的牺牲微不足道。\n简而言之，我的“为什么”比我需要预先支付的成本更有力。\n可扩展的收入加上强大的“为什么”可以改变游戏规则。 但是不要误会，它没有任何被动。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/python_tell_you_what_is_finance_freedom/","summary":"本文提供了一条通往实际“被动”收入和财务自由的现实途径\nSrc https://themakingofamillionaire.com/passive-income-is-a-lie-but-scalable-income-is-real-f0483af14483\nAuthor: Ben Le Fort\n 如今，个人理财中最大的流行语之一是“被动收入”。 不乏网络营销人员想要向您推销的想法是，如果您只是听他们的（并购买他们的“课程”），您也可以以最少的努力开始创收。\n但事情是这样的； 被动收入的想法是一堆废话。这并不意味着您无法开展数字业务并永远改变您的财务生活，因为您绝对可以而且应该这样做。\n在本文中，我将解释\n 为什么被动收入是一个神话？ 可扩展收入如何运作的示例 从财务角度，多久才能退休，且只靠利息过活？ 增加可扩展的收入帮助我节省了第一笔 10 万美元 我总是记得“为什么”我这样做  为什么被动收入是一个神话？ 正如我过去所写，博主和 YouTuber 已经开始将任何数字业务称为“被动收入机会”。\n 联属网络营销。 销售数码产品/软件。 销售信息产品，如课程和书籍。 在线销售实物商品。 在 YouTube 频道、播客或博客上展示广告。  通常，当您听到有人提到“被动收入”时，他们真正指的是可扩展的商业收入。 可扩展的收入意味着不能保证您赚到一毛钱，但您的收入潜力没有限制。\n可扩展收入如何运作的示例 我的业务之一就是销售在线课程，这是可扩展收入的一个完美例子。\n 如果没有人买我的课程，我就赚不到钱。 既然是数字产品，卖额外的课程也不用花我额外的钱。 这与销售实物的人形成鲜明对比。 如果我卖电脑，我会为我卖的每台电脑支付额外的费用。 在经济学中，这被称为“边际生产成本”。 销售数字产品，尤其是像课程这样的信息产品意味着我的边际生产成本几乎为零。 这意味着如果我出售价值 0 美元或 100 万美元的课程，我的成本将是相同的。  这就是拥有可扩展收入的含义，也就是大多数人所说的“被动收入”。\n诚然，每增加一美元可扩展收入，您就需要减少工作量； 这不是说你可以抬起脚停止工作。 被动收入是您无需做任何工作就可以赚取的钱。 如果我停止从事我的业务，收入很快就会枯竭。被动收入的唯一真正来源来自投资收入。\n 股票分红。 从债券中赚取的利息。 从房地产收取的租金。  除非你继承了很多钱，否则你产生大量被动收入的唯一方法是通过工作、入不敷出以及投资差额来产生主动收入。\n不要相信任何告诉你不同的人。 那些告诉你不工作也能赚钱的人要么是想卖给你东西，要么是妄想，要么两者兼而有之。\n主动收入可以来自朝九晚五的工作或收入可扩展的企业。 或者，就我而言，两者兼而有之。\n**致富的简单途径在于保持开支不变，增加收入，并投资差价。**过去，我写过关于我们可以利用“两个杠杆”来提高储蓄率的文章。\n 对已有用的财富，尽可能节省节俭。 赚更多的钱。  **学会节俭但轻松幸福地生活，是迈向财务独立的关键一步。 **一旦你养成了良好的消费习惯，积累财富的最快途径就是不断寻找增加收入的方法，同时保持生活成本稳定并投资差额。","title":"Python告诉你“被动收入”的财务自由是谎言，但可扩展收入的财务自由是真的"},{"content":" 整理自阮一峰技术周刊\nsrc:https://github.com/ruanyf/weekly\n Feeddd 一个免费服务，提供微信公众号的 RSS 文件，用户可以自己创建订阅源。\n MDvideo 一个桌面软件，自动将 Markdown 文档转成一段视频。 文档里面的视频、音频、图片网址，都会抓取后插入视频，还可以根据文字生成人工语音的旁白朗读。\n Read Aloud https://github.com/ken107/read-aloud\n开源的浏览器朗读插件，可以朗读整个网页或选定文本，支持40多种语言，包括中文。\n 元宇宙 元宇宙相关资源，该仓库收集元宇宙相关资源\nhttps://github.com/shadowcz007/awesome-metaverse\n为网站申请 ISSN 号码 作者介绍怎么为自己的个人网站，申请了一个 ISSN（国际标准期刊号），便于被学术期刊引用。\nhttps://shkspr.mobi/blog/2021/09/how-to-add-issn-metadata-to-a-web-page/\nlists.sr.ht 免费的邮件列表服务，可以用来架设自己的邮件列表。\n https://lists.sr.ht/\n“一证通查”电话卡服务 https://getsimnum.caict.ac.cn/\n工信部推出，可查看个人名下登记了多少张电话卡。\n Github支持脚注  youtube视频下载 是对python视频下载youtube-dl 包进行GUI封装，实现界面下载操作，支持多个视频网站的视频下载。\n BeMyEars https://www.engineerdraft.com/bemyears/\n 公司作息表 民间众包项目，用户通过腾讯文档的表格，提交自己所在部门的上下班作息时间，可以用来了解不同公司的加班情况。\nhttps://github.com/Robin970822/WorkingTime\n  平台提交/发布的数据信息未经团队允许，任何人不得盗用并使用。数据禁止用于任何其他目的，包括但不限于商业用途、学术研究、非盈利性的报告、新闻报道等。如有第三方违规使用数据，WorkingTime 团队保留追诉权力！\n 邮件列表服务 https://github.com/knadh/listmonk\n开源的邮件列表管理器，内置管理界面，在服务器或者本地电脑结合python自动化邮件群发即可开展邮件列表服务\n 商务部国别指南 http://fec.mofcom.gov.cn/article/gbdqzn/\n商务部网站提供全世界各国的《国别指南》，PDF 文件免费下载，详细介绍各国基本情况，内容非常实用。\n ##了解课程\n 点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly02/","summary":"整理自阮一峰技术周刊\nsrc:https://github.com/ruanyf/weekly\n Feeddd 一个免费服务，提供微信公众号的 RSS 文件，用户可以自己创建订阅源。\n MDvideo 一个桌面软件，自动将 Markdown 文档转成一段视频。 文档里面的视频、音频、图片网址，都会抓取后插入视频，还可以根据文字生成人工语音的旁白朗读。\n Read Aloud https://github.com/ken107/read-aloud\n开源的浏览器朗读插件，可以朗读整个网页或选定文本，支持40多种语言，包括中文。\n 元宇宙 元宇宙相关资源，该仓库收集元宇宙相关资源\nhttps://github.com/shadowcz007/awesome-metaverse\n为网站申请 ISSN 号码 作者介绍怎么为自己的个人网站，申请了一个 ISSN（国际标准期刊号），便于被学术期刊引用。\nhttps://shkspr.mobi/blog/2021/09/how-to-add-issn-metadata-to-a-web-page/\nlists.sr.ht 免费的邮件列表服务，可以用来架设自己的邮件列表。\n https://lists.sr.ht/\n“一证通查”电话卡服务 https://getsimnum.caict.ac.cn/\n工信部推出，可查看个人名下登记了多少张电话卡。\n Github支持脚注  youtube视频下载 是对python视频下载youtube-dl 包进行GUI封装，实现界面下载操作，支持多个视频网站的视频下载。\n BeMyEars https://www.engineerdraft.com/bemyears/\n 公司作息表 民间众包项目，用户通过腾讯文档的表格，提交自己所在部门的上下班作息时间，可以用来了解不同公司的加班情况。\nhttps://github.com/Robin970822/WorkingTime\n  平台提交/发布的数据信息未经团队允许，任何人不得盗用并使用。数据禁止用于任何其他目的，包括但不限于商业用途、学术研究、非盈利性的报告、新闻报道等。如有第三方违规使用数据，WorkingTime 团队保留追诉权力！\n 邮件列表服务 https://github.com/knadh/listmonk\n开源的邮件列表管理器，内置管理界面，在服务器或者本地电脑结合python自动化邮件群发即可开展邮件列表服务\n 商务部国别指南 http://fec.mofcom.gov.cn/article/gbdqzn/\n商务部网站提供全世界各国的《国别指南》，PDF 文件免费下载，详细介绍各国基本情况，内容非常实用。\n ##了解课程\n 点击上方图片购买课程   点击进入详情页","title":"TechWeekly-02 | 每周有趣有用的技术分享"},{"content":"           代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/人工智能与图数据库技术.pdf\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/ai_and_graph_database_technology/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/人工智能与图数据库技术.pdf\n了解课程  点击上方图片购买课程   点击进入详情页","title":"人工智能与图数据库技术"},{"content":" src: https://mp.weixin.qq.com/s/7D2d7BIaFAhPrxtDOWGgLA\nauthor: 俊欣\n公众号: 关于数据分析与可视化\n 数据预处理过程当中, 大致会遇到\n 加载数据 处理缺失值如何处理 处理离散型数据该如何处理 数据的标准化 将数据集划分成训练集与测试集 去掉重复值  加载数据 我们导入必要的库并且加载数据\nimport pandas as pd df = pd.read_csv(\u0026#34;data.csv\u0026#34;) 在进行数据分析前，可以查看一下数据的总体情况，从宏观上了解数据\ndata.head() #显示前五行数据 data.tail() #显示末尾五行数据 data.info() #查看各字段的信息 data.shape #查看数据集有几行几列,data.shape[0]是行数,data.shape[1]是列数 data.describe() #查看数据的大体情况，均值，最值，分位数值... data.columns.tolist() #得到列名的list \n处理缺失值 缺失值一直都是数据预处理当中比较常见的一个问题，而在处理类似的问题的时候，方式方法也是多种多样的，我们一一来介绍，\ndata = [[\u0026#39;小明\u0026#39;,25,55],[\u0026#39;小红\u0026#39;,28,60],[\u0026#39;小王\u0026#39;,26]] df = pd.DataFrame(data=data,columns=[\u0026#39;Name\u0026#39;,\u0026#39;Age\u0026#39;,\u0026#39;Weight\u0026#39;]) Name Age Weight 0 小明 25 55.0 1 小红 28 60.0 2 小王 26 NaN 针对上面的数据集，我们通过pandas中的方法看一下缺失值的情况\ndf.isnull() Name Age Weight 0 False False False 1 False False False 2 False False True 另外我们也可以这么来做，检测每一列空值的数量\ndf.isnull().sum() Name 0 Age 0 Weight 1 dtype: int64 而在面对缺失值的时候，我们一方面可以将其去除\ndf.dropna() Name Age Weight 0 小明 25 55.0 1 小红 28 60.0 当然我们也可以对缺失值进行填充，例如用平均值来填充\ndf.fillna(df.mean()) Name Age Weight 0 小明 25 55.0 1 小红 28 60.0 2 小王 26 57.5 除了pandas当中的方法之外，我们也可以使用sklearn库当中的一些函数方法，例如\nfrom sklearn.impute import SimpleImputer imputer = SimpleImputer(missing_values=np.nan, strategy=\u0026#39;mean\u0026#39;) imputer = imputer.fit(df[[\u0026#39;Weight\u0026#39;]]) df[\u0026#39;Weight\u0026#39;] = imputer.transform(df[[\u0026#39;Weight\u0026#39;]]) 最后返回的结果也和上面的fillna()方法返回的结果一致，我们用平均值来代码空值，那么同样道理我们也可以用中位数、众数等统计值来进行替换，这里就不做多说\n处理离散型数据 另外当数据集当中出现离散型数据的时候，我们也要进行相应的处理，毕竟在后面的建模过程当中，机器学习的模型需要的是连续型的数据。 离散型数据也分为两种，一种是有序的离散变量，就比方说是衣服的尺码，有M码的、也有L码的、也还有与之更大的尺码，另外一种则是无序的，例如衣服的颜色，颜色之间没有大小之分，因此在编码的时候也应该另外处理。\ndf_cat = pd.DataFrame(data = [[\u0026#39;green\u0026#39;,\u0026#39;M\u0026#39;,10.1,\u0026#39;class1\u0026#39;], [\u0026#39;blue\u0026#39;,\u0026#39;L\u0026#39;,20.1,\u0026#39;class2\u0026#39;], [\u0026#39;white\u0026#39;,\u0026#39;M\u0026#39;,30.1,\u0026#39;class1\u0026#39;]], ) df_cat.columns = [\u0026#39;color\u0026#39;,\u0026#39;size\u0026#39;,\u0026#39;price\u0026#39;,\u0026#39;classlabel\u0026#39;] color size price classlabel 0 green M 10.1 class1 1 blue L 20.1 class2 2 white M 30.1 class1 对于有序的离散型变量，我们可以使用map()函数\nsize_mapping = {\u0026#39;M\u0026#39;:1,\u0026#39;L\u0026#39;:2} df_cat[\u0026#39;size\u0026#39;] = df_cat[\u0026#39;size\u0026#39;].map(size_mapping) df_cat[\u0026#39;size\u0026#39;] 0 1 1 2 2 1 Name: size, dtype: int64 另外我们也可以使用sklearn库中的LabelEncoder()方法来处理\nfrom sklearn.preprocessing import LabelEncoder class_le = LabelEncoder() df_cat[\u0026#39;size\u0026#39;] = class_le.fit_transform(df_cat[\u0026#39;size\u0026#39;].values) 而对于无序的离散型变量，我们可以采用独热编码，例如对color这一列进行编码过之后会有color_green、color_blue以及color_white三个特征，特征值为0或者1\npd.get_dummies(df_cat[\u0026#39;color\u0026#39;], prefix = \u0026#34;color\u0026#34;) color_blue color_green color_white 0 0 1 0 1 1 0 0 2 0 0 1 然后我们将此并入到源数据当中去\ndf_cat[[\u0026#34;size\u0026#34;, \u0026#34;price\u0026#34;]].join(dummies) size price color_blue color_green color_white 0 1 10.1 0 1 0 1 2 20.1 1 0 0 2 1 30.1 0 0 1 但是考虑到后面搭建模型的时候，变量与变量之间应该保持独立，而不应该是存在依赖的关系，对于color这一列中存在三种颜色，分别是blue、green以及white，当前两类取值都为0的时候，color只可能是white 所以将get_dummies()方法中的drop_first默认值为False改为True dummies = pd.get_dummies(df_cat[\u0026#39;color\u0026#39;], prefix = \u0026#34;color\u0026#34;, drop_first=True) df_cat[[\u0026#34;size\u0026#34;, \u0026#34;price\u0026#34;]].join(dummies) \n数据的标准化 由于不同的变量，它们往往存在不同的单位以及不同的取值范围，有时候取值范围的差异较大会对机器学习的模型带来很多不必要的麻烦。因此为了最后预测结果的可靠性，我们需要对数据进行标准化，对数据按比例进行缩放，使之落入一个小的特定区间。而标准化算法有\n z-score 标准化  这种方法根据原始数据的均值和标准差进行数据的标准化，经过处理的数据符合正态分布，即均值为0，标准差为1 ，当然sklearn库当中的代码则是\nfrom sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test)  线性归一化  from sklearn.preprocessing import MinMaxScaler() min_max_scaler = MinMaxScaler() X_train_minmax = min_max_scaler.fit_transform(X_train) print(X_train_minmax) 训练集中的数据经过转化，取值范围都集中在[0,1]之间\n MaxAbsScaler()方法  MaxAbsScaler()方法和上述的线性归一化效果相类似，训练集中的数据经过转化，取值范围在[-1,1]之间\nmax_abs_scaler = preprocessing.MaxAbsScaler() X_train_maxabs = max_abs_scaler.fit_transform(X_train) X_test_maxabs = max_abs_scaler.transform(X_test)  RobustScaler()方法  要是当数据集当中存在很多的极值的时候，利用平均值和标准差来进行数据的标准化效果可能并不理想，毕竟极值会影响到平均值和标准差的计算，这个时候我们就需要用到RobustScaler()方法，\nfrom sklearn.preprocessing import RobustScaler transformer = RobustScaler().fit(X) transformer.transform(X) \n将数据集划分成训练集和测试集 在建模之前，我们需要将数据集分成训练集和测试集，我们在训练集上面建立模型，训练与优化模型，然后再将模型放到测试集上面，评估一下模型的性能以及优化的效果，在sklearn库中也有相对应的方法\nfrom sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state= 1) 上面的变量y具体指的是被预测的因变量，而x则是在预测中使用的自变量\n去除重复值 在pandas当中也有对应的方法来去除掉重复值\ndf.drop_duplicates() \u0026lt;br\u0026gt; ## 代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/4000字归纳总结PandasSklearn数据预处理.ipynb \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pre_process_in_pandas/","summary":"src: https://mp.weixin.qq.com/s/7D2d7BIaFAhPrxtDOWGgLA\nauthor: 俊欣\n公众号: 关于数据分析与可视化\n 数据预处理过程当中, 大致会遇到\n 加载数据 处理缺失值如何处理 处理离散型数据该如何处理 数据的标准化 将数据集划分成训练集与测试集 去掉重复值  加载数据 我们导入必要的库并且加载数据\nimport pandas as pd df = pd.read_csv(\u0026#34;data.csv\u0026#34;) 在进行数据分析前，可以查看一下数据的总体情况，从宏观上了解数据\ndata.head() #显示前五行数据 data.tail() #显示末尾五行数据 data.info() #查看各字段的信息 data.shape #查看数据集有几行几列,data.shape[0]是行数,data.shape[1]是列数 data.describe() #查看数据的大体情况，均值，最值，分位数值... data.columns.tolist() #得到列名的list \n处理缺失值 缺失值一直都是数据预处理当中比较常见的一个问题，而在处理类似的问题的时候，方式方法也是多种多样的，我们一一来介绍，\ndata = [[\u0026#39;小明\u0026#39;,25,55],[\u0026#39;小红\u0026#39;,28,60],[\u0026#39;小王\u0026#39;,26]] df = pd.DataFrame(data=data,columns=[\u0026#39;Name\u0026#39;,\u0026#39;Age\u0026#39;,\u0026#39;Weight\u0026#39;]) Name Age Weight 0 小明 25 55.0 1 小红 28 60.0 2 小王 26 NaN 针对上面的数据集，我们通过pandas中的方法看一下缺失值的情况\ndf.isnull() Name Age Weight 0 False False False 1 False False False 2 False False True 另外我们也可以这么来做，检测每一列空值的数量","title":"使用Pandas做数据预处理"},{"content":" src: https://mp.weixin.qq.com/s/MmfEtyKaMqNn_Ik1oJtitQ\nauthor: 俊欣\n公众号: 关于数据分析与可视化\n 代码下载 点击Pandas本文代码下载\n今天我们来谈论一下pandas库当中文本数据的操作，希望大家再看完本篇文章之后会有不少的收获，我们大致会讲\n 创建一个包含文本数据的DataFrame 常用处理文本数据的方法的总结 正则表达式与DataFrame内部方法的结合  创建文本内容的数据 我们先来创建一个包含文本数据的DataFrame，来供我们后面使用\nimport pandas as pd df = pd.DataFrame({ \u0026#34;姓\u0026#34;: [\u0026#34;李\u0026#34;,\u0026#34;王\u0026#34;,\u0026#34;戴\u0026#34;, \u0026#34;李\u0026#34;, \u0026#34;张\u0026#34;], \u0026#34;名\u0026#34;: [\u0026#34;华\u0026#34;,\u0026#34;硕\u0026#34;,\u0026#34;建业\u0026#34;, \u0026#34;四\u0026#34;, \u0026#34;三\u0026#34;], \u0026#34;户籍地址\u0026#34;: [\u0026#34; 浙江省·宁波市 \u0026#34;, \u0026#34; 浙江省·杭州市 \u0026#34;, \u0026#34; 浙江省·丽水市 \u0026#34;, \u0026#34; 浙江省·衢州市 \u0026#34;, \u0026#34; 浙江省·湖州市 \u0026#34;], \u0026#34;微信ID\u0026#34;: [\u0026#34;Tomoplplplut1248781\u0026#34;, \u0026#34;Smopopo857\u0026#34;, \u0026#34;Adahuhuifhhjfj\u0026#34;, \u0026#34;Tull1945121\u0026#34;, \u0026#34;ZPWERERTFD599557\u0026#34;], \u0026#34;邮箱地址\u0026#34;: [\u0026#34;tom02159@163.com\u0026#34;, \u0026#34;smitt7821@163.com\u0026#34;, \u0026#34;adams623@163.com\u0026#34;, \u0026#34;tull0305@163.com\u0026#34;, \u0026#34;five7532@163.com\u0026#34;] }) df | | 姓 | 名 | 户籍地址 | 微信ID | 邮箱地址 | |---:|:-----|:-----|:--------------|:--------------------|:------------------| | 0 | 李 | 华 | 浙江省·宁波市 | Tomoplplplut1248781 | tom02159@163.com | | 1 | 王 | 硕 | 浙江省·杭州市 | Smopopo857 | smitt7821@163.com | | 2 | 戴 | 建业 | 浙江省·丽水市 | Adahuhuifhhjfj | adams623@163.com | | 3 | 李 | 四 | 浙江省·衢州市 | Tull1945121 | tull0305@163.com | | 4 | 张 | 三 | 浙江省·湖州市 | ZPWERERTFD599557 | five7532@163.com | \n常用处理文本数据的方法总结 Python当中用来处理字符串数据的方法有很多，基本上都可以在DataFrame内部拿来使用，例如lower()方法和upper()方法，给字母大小写\ndf[\u0026#34;微信ID\u0026#34;].str.lower() 0 tomoplplplut1248781 1 smopopo857 2 adahuhuifhhjfj 3 tull1945121 4 zpwerertfd599557 Name: 微信ID, dtype: object df[\u0026#34;微信ID\u0026#34;].str.len() 0 19 1 10 2 14 3 11 4 16 Name: 微信ID, dtype: int64 当然我们看到户籍地址这一列中的数据有很多的空格\ndf[\u0026#34;户籍地址\u0026#34;] 0 浙江省·宁波市 1 浙江省·杭州市 2 浙江省·丽水市 3 浙江省·衢州市 4 浙江省·湖州市 Name: 户籍地址, dtype: object 我们可以使用处理字符串时的strip()方法\ndf[\u0026#34;户籍地址\u0026#34;].str.strip() 0 浙江省·宁波市 1 浙江省·杭州市 2 浙江省·丽水市 3 浙江省·衢州市 4 浙江省·湖州市 Name: 户籍地址, dtype: object 与之相类似的还有lstrip()方法以及rstrip()方法，这里就不做赘述。\n在字符串的处理过程当中，startswith()方法和endswith()方法也是用的非常的频繁，例如我们想要挑选出户籍地址是“宁波市”的数据，\ndf[\u0026#34;户籍地址\u0026#34;] = df[\u0026#34;户籍地址\u0026#34;].str.strip() df[df[\u0026#34;户籍地址\u0026#34;].str.endswith(\u0026#34;宁波市\u0026#34;)] 姓 名 户籍地址 微信ID 邮箱地址 0 李 华 浙江省·宁波市 Tomoplplplut1248781 tom02159@163.com 另外我们还可以使用replace()方法来实现当中的字符串的替换\ndf[\u0026#34;户籍地址\u0026#34;].str.replace(\u0026#34;·\u0026#34;, \u0026#34;--\u0026#34;) 0 浙江省--宁波市 1 浙江省--杭州市 2 浙江省--丽水市 3 浙江省--衢州市 4 浙江省--湖州市 Name: 户籍地址, dtype: object 那既然用到了replace()方法，那么split()方法也可以尝试一下\ndf[\u0026#34;户籍地址\u0026#34;].str.split(\u0026#34;·\u0026#34;) 0 [浙江省, 宁波市] 1 [浙江省, 杭州市] 2 [浙江省, 丽水市] 3 [浙江省, 衢州市] 4 [浙江省, 湖州市] Name: 户籍地址, dtype: object 在经过spilit()方法的切割过之后就变成了列表的形式，然后可以通过get()方法或者[]来获取里面的元素，例如\ndf[\u0026#34;户籍地址\u0026#34;].str.split(\u0026#34;·\u0026#34;).str.get(0) 或者\ndf[\u0026#34;户籍地址\u0026#34;].str.split(\u0026#34;·\u0026#34;).str[0] 0 浙江省 1 浙江省 2 浙江省 3 浙江省 4 浙江省 Name: 户籍地址, dtype: object 那么获取列表当中的第二个元素也是同样的道理，当然我们也可以在split()方法当中添加expand=True这个参数，来将上面列表形式的数据转化成DataFrame格式\ndf[\u0026#34;户籍地址\u0026#34;].str.split(\u0026#34;·\u0026#34;, expand=True) 0 1 0 浙江省 宁波市 1 浙江省 杭州市 2 浙江省 丽水市 3 浙江省 衢州市 4 浙江省 湖州市 同样地，我们可以在后面添加[]来获取我们想要的元素\ndf[\u0026#34;户籍地址\u0026#34;].str.split(\u0026#34;·\u0026#34;, expand=True)[1] 0 宁波市 1 杭州市 2 丽水市 3 衢州市 4 湖州市 Name: 1, dtype: object \n正则表达式与DataFrame内部方法的结合 假如我们想要提取文本数据内部的一部分数据，可以结合正则表达式来使用，例如我们想要提取“微信ID”这一列当中的字母和数字，并且将两者分开来\ntwo_groups = \u0026#34;([a-zA-Z]+)([0-9]+)\u0026#34; df[\u0026#34;微信ID\u0026#34;].str.extract(two_groups, expand=True) 0 1 0 Tomoplplplut 1248781 1 Smopopo 857 2 NaN NaN 3 Tull 1945121 4 ZPWERERTFD 599557 当然了，如果想是要提取文本数据中的部分数据，可以直接在str方法后面添加索引，例如\ndf[\u0026#34;邮箱地址\u0026#34;].str[-8:] 0 @163.com 1 @163.com 2 @163.com 3 @163.com 4 @163.com Name: 邮箱地址, dtype: object 当然，从另外一个角度讲，正则表达式也可以帮助我们确认文本数据是否符合某种规律，\ntwo_groups = \u0026#34;([a-zA-Z]+)([0-9]+)\u0026#34; df[\u0026#34;微信ID\u0026#34;].str.match(two_groups) 0 True 1 True 2 False 3 True 4 True Name: 微信ID, dtype: bool 当中有一个为False，不满足字母+数字的规律，我们再进一步，将满足条件的数据提取出来\ndf[df[\u0026#34;微信ID\u0026#34;].str.match(two_groups)] 姓 名 户籍地址 微信ID 邮箱地址 0 李 华 浙江省·宁波市 Tomoplplplut1248781 tom02159@163.com 1 王 硕 浙江省·杭州市 Smopopo857 smitt7821@163.com 3 李 四 浙江省·衢州市 Tull1945121 tull0305@163.com 4 张 三 浙江省·湖州市 ZPWERERTFD599557 five7532@163.com 针对文本数据而言，contains()方法也能够派上用场，例如下面的数据\n姓 名 户籍地址 微信ID 邮箱地址 0 李 华 浙江省·宁波市 Tomoplplplut1248781 tom02159@163.com 1 王 硕 浙江省·杭州市 Smopopo857 smitt7821@163.com 2 戴 建业 浙江省·丽水市 Adahuhuifhhjfj adams623@163.com 3 李 四 浙江省·衢州市 Tull1945121 tull0305@163.com 4 张 三 浙江省·湖州市 ZPWERERTFD599557 five7532@163.com 5 黄 五 浙江省·宁波市 hunhunhu45652 1erdcvf127@16.com 我们用contains()来提取出户籍地址为“宁波市”的内容，可以这么做\ndf[df[\u0026#34;户籍地址\u0026#34;].str.contains(\u0026#34;宁波市\u0026#34;)] 姓 名 户籍地址 微信ID 邮箱地址 0 李 华 浙江省·宁波市 Tomoplplplut1248781 tom02159@163.com 5 黄 五 浙江省·宁波市 hunhunhu45652 1erdcvf127@16.com 暂时就这些了，下一篇原创的文章安排在周天，非技术方面的，期待一下？\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/text_analysis_in_pandas/","summary":"src: https://mp.weixin.qq.com/s/MmfEtyKaMqNn_Ik1oJtitQ\nauthor: 俊欣\n公众号: 关于数据分析与可视化\n 代码下载 点击Pandas本文代码下载\n今天我们来谈论一下pandas库当中文本数据的操作，希望大家再看完本篇文章之后会有不少的收获，我们大致会讲\n 创建一个包含文本数据的DataFrame 常用处理文本数据的方法的总结 正则表达式与DataFrame内部方法的结合  创建文本内容的数据 我们先来创建一个包含文本数据的DataFrame，来供我们后面使用\nimport pandas as pd df = pd.DataFrame({ \u0026#34;姓\u0026#34;: [\u0026#34;李\u0026#34;,\u0026#34;王\u0026#34;,\u0026#34;戴\u0026#34;, \u0026#34;李\u0026#34;, \u0026#34;张\u0026#34;], \u0026#34;名\u0026#34;: [\u0026#34;华\u0026#34;,\u0026#34;硕\u0026#34;,\u0026#34;建业\u0026#34;, \u0026#34;四\u0026#34;, \u0026#34;三\u0026#34;], \u0026#34;户籍地址\u0026#34;: [\u0026#34; 浙江省·宁波市 \u0026#34;, \u0026#34; 浙江省·杭州市 \u0026#34;, \u0026#34; 浙江省·丽水市 \u0026#34;, \u0026#34; 浙江省·衢州市 \u0026#34;, \u0026#34; 浙江省·湖州市 \u0026#34;], \u0026#34;微信ID\u0026#34;: [\u0026#34;Tomoplplplut1248781\u0026#34;, \u0026#34;Smopopo857\u0026#34;, \u0026#34;Adahuhuifhhjfj\u0026#34;, \u0026#34;Tull1945121\u0026#34;, \u0026#34;ZPWERERTFD599557\u0026#34;], \u0026#34;邮箱地址\u0026#34;: [\u0026#34;tom02159@163.com\u0026#34;, \u0026#34;smitt7821@163.com\u0026#34;, \u0026#34;adams623@163.com\u0026#34;, \u0026#34;tull0305@163.com\u0026#34;, \u0026#34;five7532@163.com\u0026#34;] }) df | | 姓 | 名 | 户籍地址 | 微信ID | 邮箱地址 | |---:|:-----|:-----|:--------------|:--------------------|:------------------| | 0 | 李 | 华 | 浙江省·宁波市 | Tomoplplplut1248781 | tom02159@163.","title":"使用Pandas处理文本数据"},{"content":"今天分享几个不为人知的pandas函数，大家可能平时看到的不多，但是使用起来倒是非常的方便，也能够帮助我们数据分析人员大幅度地提高工作效率，同时也希望大家看完之后能够有所收获\nitems()方法 pandas当中的items()方法可以用来遍历数据集当中的每一列，同时返回列名以及每一列当中的内容，通过以元组的形式，示例如下\nimport pandas as pd df = pd.DataFrame({\u0026#39;species\u0026#39;: [\u0026#39;bear\u0026#39;, \u0026#39;bear\u0026#39;, \u0026#39;marsupial\u0026#39;], \u0026#39;population\u0026#39;: [1864, 22000, 80000]}, index=[\u0026#39;panda\u0026#39;, \u0026#39;polar\u0026#39;, \u0026#39;koala\u0026#39;]) df | | species | population | |:------|:----------|-------------:| | panda | bear | 1864 | | polar | bear | 22000 | | koala | marsupial | 80000 | \nfor label, content in df.items(): print(f\u0026#39;label: {label}\u0026#39;) print(f\u0026#39;content: {content}\u0026#39;, sep=\u0026#39;\\n\u0026#39;) print(\u0026#34;=\u0026#34; * 50) label: species content: panda bear polar bear koala marsupial Name: species, dtype: object ================================================== label: population content: panda 1864 polar 22000 koala 80000 Name: population, dtype: int64 ==================================================  相继的打印出了‘species’和‘population’这两列的列名和相应的内容\niterrows()方法 而对于iterrows()方法而言，其功能则是遍历数据集当中的每一行，返回每一行的索引以及带有列名的每一行的内容，示例如下\nfor label, content in df.iterrows(): print(f\u0026#39;label: {label}\u0026#39;) print(f\u0026#39;content: {content}\u0026#39;, sep=\u0026#39;\\n\u0026#39;) print(\u0026#34;=\u0026#34; * 50) label: panda content: species bear population 1864 Name: panda, dtype: object ================================================== label: polar content: species bear population 22000 Name: polar, dtype: object ================================================== label: koala content: species marsupial population 80000 Name: koala, dtype: object ==================================================  insert()方法 insert()方法主要是用于在数据集当中的特定位置处插入数据。在DataFrame数据集当中，列的索引也是从0开始的\n示例如下\n#在第二列插入size字段，内容如下 df.insert(1, \u0026#34;size\u0026#34;, [2000, 3000, 4000]) df | | species | size | population | |:------|:----------|-------:|-------------:| | panda | bear | 2000 | 1864 | | polar | bear | 3000 | 22000 | | koala | marsupial | 4000 | 80000 | \nassign()方法 assign()方法可以用来在数据集当中添加新的列，示例如下\ndf = df.assign(size_1=lambda x: x.population * 9 / 5 + 32) df | | species | size | population | size_1 | |:------|:----------|-------:|-------------:|---------:| | panda | bear | 2000 | 1864 | 3387.2 | | polar | bear | 3000 | 22000 | 39632 | | koala | marsupial | 4000 | 80000 | 144032 | 从上面的例子中可以看出，我们通过一个lambda匿名函数，在数据集当中添加一个新的列，命名为size_1，当然我们也可以通过assign()方法来创建不止一个列\ndf = df.assign(size_1 = lambda x: x.population * 9 / 5 + 32, size_2 = lambda x: x.population * 8 / 5 + 10) df | | species | size | population | size_1 | size_2 | |:------|:----------|-------:|-------------:|---------:|---------:| | panda | bear | 2000 | 1864 | 3387.2 | 2992.4 | | polar | bear | 3000 | 22000 | 39632 | 35210 | | koala | marsupial | 4000 | 80000 | 144032 | 128010 | \neval()方法 eval()方法主要是用来执行用字符串来表示的运算过程的，例如\ndf.eval(\u0026#34;size_3 = size_1 + size_2\u0026#34;) | | species | size | population | size_1 | size_2 | size_3 | |:------|:----------|-------:|-------------:|---------:|---------:|---------:| | panda | bear | 2000 | 1864 | 3387.2 | 2992.4 | 6379.6 | | polar | bear | 3000 | 22000 | 39632 | 35210 | 74842 | | koala | marsupial | 4000 | 80000 | 144032 | 128010 | 272042 | 当然我们也可以同时对执行多个运算过程\ndf = df.eval(\u0026#39;\u0026#39;\u0026#39; size_3 = size_1 + size_2 size_4 = size_1 - size_2 \u0026#39;\u0026#39;\u0026#39;) df | | species | size | population | size_1 | size_2 | size_3 | size_4 | |:------|:----------|-------:|-------------:|---------:|---------:|---------:|---------:| | panda | bear | 2000 | 1864 | 3387.2 | 2992.4 | 6379.6 | 394.8 | | polar | bear | 3000 | 22000 | 39632 | 35210 | 74842 | 4422 | | koala | marsupial | 4000 | 80000 | 144032 | 128010 | 272042 | 16022 | \npop()方法 pop()方法主要是用来删除掉数据集中特定的某一列数据\ndf.pop(\u0026#34;size_3\u0026#34;) 而原先的数据集当中就没有这个‘size_3’这一例的数据了\ntruncate()方法 truncate()方法主要是根据行索引来筛选指定行的数据的，示例如下\ndf = pd.DataFrame({\u0026#39;A\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;], \u0026#39;B\u0026#39;: [\u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;j\u0026#39;], \u0026#39;C\u0026#39;: [\u0026#39;k\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;n\u0026#39;, \u0026#39;o\u0026#39;]}, index=[1, 2, 3, 4, 5]) df | | A | B | C | |---:|:----|:----|:----| | 1 | a | f | k | | 2 | b | g | l | | 3 | c | h | m | | 4 | d | i | n | | 5 | e | j | o | 我们使用truncate()方法来做一下尝试\ndf.truncate(before=2, after=4) | | A | B | C | |---:|:----|:----|:----| | 2 | b | g | l | | 3 | c | h | m | | 4 | d | i | n | 我们看到参数before和after存在于truncate()方法中，目的就是把行索引2之前和行索引4之后的数据排除在外，筛选出剩余的数据\ncount()方法 count()方法主要是用来计算某一列当中非空值的个数，示例如下\nimport numpy as np df = pd.DataFrame({\u0026#34;Name\u0026#34;: [\u0026#34;John\u0026#34;, \u0026#34;Myla\u0026#34;, \u0026#34;Lewis\u0026#34;, \u0026#34;John\u0026#34;, \u0026#34;John\u0026#34;], \u0026#34;Age\u0026#34;: [24., np.nan, 25, 33, 26], \u0026#34;Single\u0026#34;: [True, True, np.nan, True, False]}) df | | Name | Age | Single | |---:|:-------|------:|---------:| | 0 | John | 24 | 1 | | 1 | Myla | nan | 1 | | 2 | Lewis | 25 | nan | | 3 | John | 33 | 1 | | 4 | John | 26 | 0 | 我们使用count()方法来计算一下数据集当中非空值的个数\ndf.count() Name 5 Age 4 Single 4 dtype: int64  add_prefix()方法/add_suffix()方法 add_prefix()方法和add_suffix()方法分别会给列名以及行索引添加后缀和前缀，对于Series()数据集而言，前缀与后缀是添加在行索引处，而对于DataFrame()数据集而言，前缀与后缀是添加在列索引处，示例如下\ns = pd.Series([1, 2, 3, 4]) s 0 1 1 2 2 3 3 4 dtype: int64  我们使用add_prefix()方法与add_suffix()方法在Series()数据集上\ns.add_prefix(\u0026#39;row_\u0026#39;) row_0 1 row_1 2 row_2 3 row_3 4 dtype: int64  又例如\ns.add_suffix(\u0026#39;_row\u0026#39;) 0_row 1 1_row 2 2_row 3 3_row 4 dtype: int64  而对于DataFrame()形式数据集而言，add_prefix()方法以及add_suffix()方法是将前缀与后缀添加在列索引处的\ndf = pd.DataFrame({\u0026#39;A\u0026#39;: [1, 2, 3, 4], \u0026#39;B\u0026#39;: [3, 4, 5, 6]}) df | | A | B | |---:|----:|----:| | 0 | 1 | 3 | | 1 | 2 | 4 | | 2 | 3 | 5 | | 3 | 4 | 6 | \ndf.add_prefix(\u0026#34;column_\u0026#34;) | | column_A | column_B | |---:|----:|----:| | 0 | 1 | 3 | | 1 | 2 | 4 | | 2 | 3 | 5 | | 3 | 4 | 6 | \ndf.add_suffix(\u0026#34;_column\u0026#34;) | | A_column | B_column | |---:|----:|----:| | 0 | 1 | 3 | | 1 | 2 | 4 | | 2 | 3 | 5 | | 3 | 4 | 6 | \nclip()方法 clip()方法主要是通过设置阈值来改变数据集当中的数值，当数值超过阈值的时候，就做出相应的调整\ndata = {\u0026#39;col_0\u0026#39;: [9, -3, 0, -1, 5], \u0026#39;col_1\u0026#39;: [-2, -7, 6, 8, -5]} df = pd.DataFrame(data) df | | col_0 | col_1 | |---:|--------:|--------:| | 0 | 9 | -2 | | 1 | -3 | -7 | | 2 | 0 | 6 | | 3 | -1 | 8 | | 4 | 5 | -5 | \ndf.clip(lower = -4, upper = 4) | | col_0 | col_1 | |---:|--------:|--------:| | 0 | 4 | -2 | | 1 | -3 | -4 | | 2 | 0 | 4 | | 3 | -1 | 4 | | 4 | 4 | -4 | 我们看到参数lower和upper分别代表阈值的上限与下限，数据集当中超过上限与下限的值会被替代。\nfilter()方法 pandas当中的filter()方法是用来筛选出特定范围的数据的，示例如下\ndf = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12])), index=[\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;], columns=[\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]) df | | one | two | three | |:---|------:|------:|--------:| | A | 1 | 2 | 3 | | B | 4 | 5 | 6 | | C | 7 | 8 | 9 | | D | 10 | 11 | 12 | 我们使用filter()方法来筛选数据\ndf.filter(items=[\u0026#39;one\u0026#39;, \u0026#39;three\u0026#39;]) | | one | three | |:---|------:|--------:| | A | 1 | 3 | | B | 4 | 6 | | C | 7 | 9 | | D | 10 | 12 | 我们还可以使用正则表达式来筛选数据\n#字段名e结尾的列 df.filter(regex=\u0026#39;e$\u0026#39;, axis=1) | | one | three | |:---|------:|--------:| | A | 1 | 3 | | B | 4 | 6 | | C | 7 | 9 | | D | 10 | 12 | 当然通过参数axis来调整筛选行方向或者是列方向的数据\ndf.filter(like=\u0026#39;B\u0026#39;, axis=0) | | one | two | three | |:---|------:|------:|--------:| | B | 4 | 5 | 6 | \nfirst()方法 当数据集当中的行索引是日期的时候，可以通过该方法来筛选前面几行的数据\nindex_1 = pd.date_range(\u0026#39;2021-11-11\u0026#39;, periods=5, freq=\u0026#39;2D\u0026#39;) ts = pd.DataFrame({\u0026#39;A\u0026#39;: [1, 2, 3, 4, 5]}, index=index_1) ts | | A | |:--------------------|----:| | 2021-11-11 00:00:00 | 1 | | 2021-11-13 00:00:00 | 2 | | 2021-11-15 00:00:00 | 3 | | 2021-11-17 00:00:00 | 4 | | 2021-11-19 00:00:00 | 5 | \n我们使用first()方法来进行一些操作，例如筛选出前面3天的数据\nts.first(\u0026#39;3D\u0026#39;) | | A | |:--------------------|----:| | 2021-11-11 00:00:00 | 1 | | 2021-11-13 00:00:00 | 2 | \nisin()方法 isin()方法主要是用来确认数据集当中的数值是否被包含在给定的列表当中\ndf = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12])), index=[\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;], columns=[\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]) df.isin([3, 5, 12]) | | one | two | three | |:---|------:|------:|--------:| | A | 0 | 0 | 1 | | B | 0 | 1 | 0 | | C | 0 | 0 | 0 | | D | 0 | 0 | 1 | 若是数值被包含在列表当中了，也就是3、5、12当中，返回的是True，否则就返回False\ndf.plot.area()方法 下面我们来讲一下如何在Pandas当中通过一行代码来绘制图表，将所有的列都通过面积图的方式来绘制\ndf = pd.DataFrame({ \u0026#39;sales\u0026#39;: [30, 20, 38, 95, 106, 65], \u0026#39;signups\u0026#39;: [7, 9, 6, 12, 18, 13], \u0026#39;visits\u0026#39;: [20, 42, 28, 62, 81, 50], }, index=pd.date_range(start=\u0026#39;2021/01/01\u0026#39;, end=\u0026#39;2021/07/01\u0026#39;, freq=\u0026#39;M\u0026#39;)) df | | sales | signups | visits | |:--------------------|--------:|----------:|---------:| | 2021-01-31 00:00:00 | 30 | 7 | 20 | | 2021-02-28 00:00:00 | 20 | 9 | 42 | | 2021-03-31 00:00:00 | 38 | 6 | 28 | | 2021-04-30 00:00:00 | 95 | 12 | 62 | | 2021-05-31 00:00:00 | 106 | 18 | 81 | | 2021-06-30 00:00:00 | 65 | 13 | 50 | \nax = df.plot.area(figsize = (10, 5))   df.plot.bar()方法 下面我们看一下如何通过一行代码来绘制柱状图\ndf = pd.DataFrame({\u0026#39;label\u0026#39;:[\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;], \u0026#39;values\u0026#39;:[10, 30, 50, 70]}) df.to_markdown() | | label | values | |---:|:--------|---------:| | 0 | A | 10 | | 1 | B | 30 | | 2 | C | 50 | | 3 | D | 70 | \nax = df.plot.bar(x=\u0026#39;label\u0026#39;, y=\u0026#39;values\u0026#39;, rot=-15)   当然我们也可以根据不同的类别来绘制柱状图\nage = [0.1, 17.5, 40, 48, 52, 69, 88] weight = [2, 8, 70, 1.5, 25, 12, 28] index = [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;G\u0026#39;] df = pd.DataFrame({\u0026#39;age\u0026#39;: age, \u0026#39;weight\u0026#39;: weight}, index=index) ax = df.plot.bar(rot=0)   ax = df.plot.barh()   df.plot.box()方法 我们来看一下箱型图的具体的绘制，通过pandas一行代码来实现\ndata = np.random.randn(25, 3) df = pd.DataFrame(data, columns=list(\u0026#39;ABC\u0026#39;)) df | | A | B | C | |---:|----------:|-----------:|-----------:| | 0 | -1.59132 | 1.47926 | 1.16891 | | 1 | -0.649097 | 0.0501129 | -0.81485 | | 2 | 1.31677 | 1.00035 | 0.0662983 | | 3 | -1.04484 | 0.445727 | 0.0140137 | | 4 | 0.217317 | -0.692044 | -0.758549 | | 5 | -0.566574 | -0.159038 | 0.781744 | | 6 | -0.903068 | 1.50697 | 1.2605 | | 7 | 1.38627 | -0.0539971 | -0.0619803 | | 8 | -1.50639 | -0.187649 | 1.00115 | | 9 | -1.84435 | -1.37282 | 0.905218 | | 10 | -0.400618 | 0.503642 | 1.20152 | | 11 | -0.164643 | 1.58712 | -0.0475168 | | 12 | 1.99742 | -0.928291 | 0.502433 | | 13 | -1.25877 | 2.0764 | -0.840626 | | 14 | -0.293273 | -2.09935 | -0.152372 | | 15 | -0.686338 | 0.798964 | 1.4551 | | 16 | 0.407321 | 0.601732 | 0.456718 | | 17 | 0.594827 | -1.8498 | 1.22704 | | 18 | -0.345245 | -1.27973 | -0.0971918 | | 19 | 1.91415 | 0.656389 | -1.15816 | | 20 | 0.149819 | 1.10037 | -0.0785313 | | 21 | -0.311471 | -1.10781 | 0.707936 | | 22 | 0.614726 | -0.142359 | -1.23091 | | 23 | 1.46869 | 1.27063 | 0.797499 | | 24 | -1.02252 | 0.819603 | -0.220382 | \nax = df.plot.box()   df.plot.pie()方法 接下来是饼图的绘制\ndf = pd.DataFrame({\u0026#39;mass\u0026#39;: [1.33, 4.87 , 5.97], \u0026#39;radius\u0026#39;: [2439.7, 6051.8, 6378.1]}, index=[\u0026#39;Mercury\u0026#39;, \u0026#39;Venus\u0026#39;, \u0026#39;Earth\u0026#39;]) plot = df.plot.pie(y=\u0026#39;mass\u0026#39;, figsize=(8, 8))   除此之外，还有折线图、直方图、散点图等等，步骤与方式都与上述的技巧有异曲同工之妙，大家感兴趣的可以自己另外去尝试。\n代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/4000字详细说明_推荐20个好用到爆的Pandas函数方法.ipynb\n src: https://mp.weixin.qq.com/s/PEhYrNbZyI9Prl3OwrBcAw\nauthor: 俊欣 公众号: 关于数据分析与可视化\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析https://hidadeng.github.io/blog/management_python_course/)  ","permalink":"/blog/pandas_20_most_usefull_functions/","summary":"今天分享几个不为人知的pandas函数，大家可能平时看到的不多，但是使用起来倒是非常的方便，也能够帮助我们数据分析人员大幅度地提高工作效率，同时也希望大家看完之后能够有所收获\nitems()方法 pandas当中的items()方法可以用来遍历数据集当中的每一列，同时返回列名以及每一列当中的内容，通过以元组的形式，示例如下\nimport pandas as pd df = pd.DataFrame({\u0026#39;species\u0026#39;: [\u0026#39;bear\u0026#39;, \u0026#39;bear\u0026#39;, \u0026#39;marsupial\u0026#39;], \u0026#39;population\u0026#39;: [1864, 22000, 80000]}, index=[\u0026#39;panda\u0026#39;, \u0026#39;polar\u0026#39;, \u0026#39;koala\u0026#39;]) df | | species | population | |:------|:----------|-------------:| | panda | bear | 1864 | | polar | bear | 22000 | | koala | marsupial | 80000 | \nfor label, content in df.items(): print(f\u0026#39;label: {label}\u0026#39;) print(f\u0026#39;content: {content}\u0026#39;, sep=\u0026#39;\\n\u0026#39;) print(\u0026#34;=\u0026#34; * 50) label: species content: panda bear polar bear koala marsupial Name: species, dtype: object ================================================== label: population content: panda 1864 polar 22000 koala 80000 Name: population, dtype: int64 ==================================================  相继的打印出了‘species’和‘population’这两列的列名和相应的内容","title":"推荐20个好用到爆的Pandas函数方法"},{"content":" src: https://mp.weixin.qq.com/s/j71IPWmT57g3VTajGhgN7Q author: 俊欣 公众号: 关于数据分析与可视化\n 本篇我们继续前面pandas系列教程的探讨，今天小编会介绍pandas库当中一些非常基础的方法与函数，希望大家看了之后会有所收获。\n准备需要的数据集 我们先准备生成一些随机数，作为后面需要用到的数据集\nimport pandas as pd import numpy as np index = pd.date_range(\u0026#34;1/1/2000\u0026#34;, periods=8) series = pd.Series(np.random.randn(5), index=[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;]) df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) \nHead and tail head()和tail()方法是用来查看数据集当中的前几行和末尾几行的，默认是查看5行，当然读者朋友也可以自行设定行数\nseries2 = pd.Series(np.random.randn(100)) series2.head() 0 0.578276 1 0.643313 2 -0.336030 3 -0.422468 4 -0.493812 dtype: float64  # 同理 series2.tail() 95 1.307962 96 1.165135 97 0.717692 98 0.605668 99 0.264990 dtype: float64  数据的统计分析 在pandas当中用describe()方法来对表格中的数据做一个概括性的统计分析，例如\nseries2.describe() count 100.000000 mean 0.106277 std 1.027541 min -2.554005 25% -0.510912 50% 0.028765 75% 0.795444 max 2.512260 dtype: float64  当然，我们也可以设置好输出的分位\nseries2.describe(percentiles=[0.05, 0.25, 0.75, 0.95]) count 100.000000 mean 0.106277 std 1.027541 min -2.554005 5% -1.450067 25% -0.510912 50% 0.028765 75% 0.795444 95% 1.757926 max 2.512260 dtype: float64  对于离散型的数据来说，describe()方法给出的结果则会简洁很多\ns = pd.Series([\u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;a\u0026#34;]) s.describe() count 10 unique 4 top a freq 5 dtype: object  要是表格中既包含了离散型数据，也包含了连续型的数据，默认的话，describe()是会针对连续型数据进行统计分析\ndf2 = pd.DataFrame({\u0026#34;a\u0026#34;: [\u0026#34;Yes\u0026#34;, \u0026#34;Yes\u0026#34;, \u0026#34;No\u0026#34;, \u0026#34;No\u0026#34;], \u0026#34;b\u0026#34;: np.random.randn(4)}) df2.describe() | | b | |:------|---------:| | count | 4 | | mean | 0.967026 | | std | 0.859657 | | min | 0.204027 | | 25% | 0.233797 | | 50% | 0.947075 | | 75% | 1.6803 | | max | 1.76993 | 当然我们也可以指定让其强制统计分析离散型数据或者连续型数据\ndf2.describe(include=[\u0026#34;object\u0026#34;]) | | a | |:-------|:----| | count | 4 | | unique | 2 | | top | Yes | | freq | 2 | 同理，我们也可以指定连续型的数据进行统计分析\ndf2.describe(include=[\u0026#34;number\u0026#34;]) | | b | |:------|---------:| | count | 4 | | mean | 0.967026 | | std | 0.859657 | | min | 0.204027 | | 25% | 0.233797 | | 50% | 0.947075 | | 75% | 1.6803 | | max | 1.76993 | 如果我们都要去做统计分析，可以这么来执行\ndf2.describe(include=\u0026#34;all\u0026#34;) | | a | b | |:-------|:----|-----------:| | count | 4 | 4 | | unique | 2 | nan | | top | Yes | nan | | freq | 2 | nan | | mean | nan | 0.967026 | | std | nan | 0.859657 | | min | nan | 0.204027 | | 25% | nan | 0.233797 | | 50% | nan | 0.947075 | | 75% | nan | 1.6803 | | max | nan | 1.76993 | \n最大/最小值的位置 idxmin()和idxmax()方法是用来查找表格当中最大/最小值的位置，返回的是值的索引\ns1 = pd.Series(np.random.randn(5)) s1 0 2.244266 1 1.398258 2 -1.827026 3 -0.058691 4 0.275471 dtype: float64  s1.idxmin(), s1.idxmax() (2, 0)  用在DataFrame上面的话，如下\ndf1 = pd.DataFrame(np.random.randn(5, 3), columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) df1.idxmin(axis=0) | | 0 | |:---|----:| | A | 2 | | B | 3 | | C | 2 | 同理，我们将axis参数改成1\ndf1.idxmin(axis=1) | | 0 | |---:|:----| | 0 | B | | 1 | B | | 2 | A | | 3 | B | | 4 | B | \nvalue_counts()方法 pandas当中的value_counts()方法主要用于数据表的计数以及排序，用来查看表格当中，指定列有多少个不同的数据值并且计算不同值在该列当中出现的次数，先来看一个简单的例子\ndf = pd.DataFrame({\u0026#39;城市\u0026#39;: [\u0026#39;北京\u0026#39;, \u0026#39;广州\u0026#39;, \u0026#39;上海\u0026#39;, \u0026#39;上海\u0026#39;, \u0026#39;杭州\u0026#39;, \u0026#39;成都\u0026#39;, \u0026#39;香港\u0026#39;, \u0026#39;南京\u0026#39;, \u0026#39;北京\u0026#39;, \u0026#39;北京\u0026#39;], \u0026#39;收入\u0026#39;: [10000, 10000, 5500, 5500, 4000, 50000, 8000, 5000, 5200, 5600], \u0026#39;年龄\u0026#39;: [50, 43, 34, 40, 25, 25, 45, 32, 25, 25]}) df[\u0026#34;城市\u0026#34;].value_counts() 北京 3 上海 2 广州 1 杭州 1 南京 1 香港 1 成都 1 Name: 城市, dtype: int64  可以看到北京出现了3次，上海出现了2次，并且默认采用的是降序来排列的，下面我们来看一下用升序的方式来排列一下收入这一列\ndf[\u0026#34;收入\u0026#34;].value_counts(ascending=True) 5600 1 5000 1 8000 1 5200 1 50000 1 4000 1 10000 2 5500 2 Name: 收入, dtype: int64  同时里面也还可以利用参数normalize=True，来计算不同值的计数占比\ndf[\u0026#39;年龄\u0026#39;].value_counts(ascending=True, normalize=True) 32 0.1 34 0.1 50 0.1 40 0.1 43 0.1 45 0.1 25 0.4 Name: 年龄, dtype: float64  数据分组 我们可以使用cut()方法以及qcut()方法来对表格中的连续型数据分组，首先我们看一下cut()方法，假设下面这组数据代表的是小组每个成员的年龄\nages = np.array([2,3,10,40,36,45,58,62,85,89,95,18,20,25,35,32]) pd.cut(ages, 5) [(1.907, 20.6], (1.907, 20.6], (1.907, 20.6], (39.2, 57.8], (20.6, 39.2], ..., (1.907, 20.6], (1.907, 20.6], (20.6, 39.2], (20.6, 39.2], (20.6, 39.2]] Length: 16 Categories (5, interval[float64]): [(1.907, 20.6] \u0026lt; (20.6, 39.2] \u0026lt; (39.2, 57.8] \u0026lt; (57.8, 76.4] \u0026lt; (76.4, 95.0]]  由上可以看到用cut()方法将数据平分成了5个区间，且区间两边都有扩展以包含最大值和最小值，当然我们也可以给每一个区间加上标记\npd.cut(ages, 5, labels=[u\u0026#34;婴儿\u0026#34;,u\u0026#34;少年\u0026#34;,u\u0026#34;青年\u0026#34;,u\u0026#34;中年\u0026#34;,u\u0026#34;老年\u0026#34;]) ['婴儿', '婴儿', '婴儿', '青年', '少年', ..., '婴儿', '婴儿', '少年', '少年', '少年'] Length: 16 Categories (5, object): ['婴儿' \u0026lt; '少年' \u0026lt; '青年' \u0026lt; '中年' \u0026lt; '老年']  而对于qcut()方法来说，我们可以指定区间来进行分组，例如\npd.qcut(ages, [0,0.5,1], labels=[\u0026#39;小朋友\u0026#39;,\u0026#39;大孩子\u0026#39;]) ['小朋友', '小朋友', '小朋友', '大孩子', '大孩子', ..., '小朋友', '小朋友', '小朋友', '小朋友', '小朋友'] Length: 16 Categories (2, object): ['小朋友' \u0026lt; '大孩子']  这里将年龄这组数据分成两部分[0, 0.5, 1]，一组是标上标记小朋友，另一组是大孩子，不过通常情况下，我们用的cut()方法比较多\n\n引用函数 要是在表格当中引用其他的方法，或者是自建的函数，可以使用通过pandas当中的以下这几个方法\n pipe() apply()和applymap() agg()和transform()  pipe()方法 首先我们来看pipe()这个方法，我们可以将自己定义好的函数，以链路的形式一个接着一个传给我们要处理的数据集上\ndef extract_city_name(df): df[\u0026#34;state_name\u0026#34;] = df[\u0026#34;state_and_code\u0026#34;].str.split(\u0026#34;,\u0026#34;).str.get(0) return df def add_country_name(df, country_name=None): df[\u0026#34;state_and_country\u0026#34;] = df[\u0026#34;state_name\u0026#34;] + country_name return df 然后我们用pip()这个方法来将上面我们定义的函数串联起来\ndf_p = pd.DataFrame({\u0026#34;city_and_code\u0026#34;: [\u0026#34;Arizona, AZ\u0026#34;]}) df_p = pd.DataFrame({\u0026#34;state_and_code\u0026#34;: [\u0026#34;Arizona, AZ\u0026#34;]}) df_p.pipe(extract_city_name).pipe(add_country_name, country_name=\u0026#34;_USA\u0026#34;) | | state_and_code | state_name | state_and_country | |---:|:-----------------|:-------------|:--------------------| | 0 | Arizona, AZ | Arizona | Arizona_USA | \napply()方法和applymap()方法 apply()方法可以对表格中的数据按照行或者是列方向进行处理，默认是按照列方向，如下\ndf.apply(np.mean) A -0.101751 B -0.360288 C -0.637433 dtype: float64 当然，我们也可以通过axis参数来进行调节\ndf.apply(np.mean, axis = 1) 0 -0.803675 1 -0.179640 2 -1.200973 3 0.156888 4 0.381631 5 0.049274 6 1.174923 7 0.612591 dtype: float64 除此之外，我们也可以直接调用匿名函数lambda的形式\ndf.apply(lambda x: x.max() - x.min()) A 1.922863 B 2.874672 C 1.943930 dtype: float64 也可以调用自己定义的函数方法\ndf = pd.DataFrame(np.random.randn(5, 3), columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) def normalize(x): return (x - x.mean()) / x.std() 我们用上apply()方法\ndf.apply(normalize) A B C 0 1.149795 0.390263 -0.813770 1 0.805843 -0.532374 0.859627 2 0.047824 -0.085334 -0.067179 3 -0.903319 -1.215023 1.149538 4 -1.100144 1.442467 -1.128216 apply()方法作用于数据集当中的每个行或者是列，而applymap()方法则是对数据集当中的所有元素都进行处理\ndf = pd.DataFrame({\u0026#39;key1\u0026#39; : [\u0026#39;a\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;d\u0026#39;], \u0026#39;key2\u0026#39; : [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;one\u0026#39;], \u0026#39;data1\u0026#39; : np.arange(1, 6), \u0026#39;data2\u0026#39; : np.arange(10,15)}) df key1 key2 data1 data2 0 a one 1 10 1 c two 2 11 2 b three 3 12 3 b four 4 13 4 d five 5 14 我们来自定义一个函数\ndef add_A(x): return \u0026#34;A\u0026#34; + str(x) df.applymap(add_A) key1 key2 data1 data2 0 Aa Aone A1 A10 1 Ac Atwo A2 A11 2 Ab Athree A3 A12 3 Ab Afour A4 A13 4 Ad Afive A5 A14 我们然后也可以通过lambda()自定义函数方法，然后来去除掉这个A\ndf.applymap(add_A).applymap(lambda x: x.split(\u0026#34;A\u0026#34;)[1]) key1 key2 data1 data2 0 a one 1 10 1 c two 2 11 2 b three 3 12 3 b four 4 13 4 d five 5 14 \nagg()方法和transform()方法 agg()方法本意上是聚合函数，我们可以将用于统计分析的一系列方法都放置其中，并且放置多个\ndf = pd.DataFrame(np.random.randn(5, 3), columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) df.agg(np.sum) A 2.042573 B 2.189269 C -1.066976 dtype: float64  当然，当中的np.sum部分也可以用字符串来表示，例如\ndf.agg(\u0026#34;sum\u0026#34;) A 2.042573 B 2.189269 C -1.066976 dtype: float64  我们尝试在当中放置多个统计分析的函数方法\ndf.agg([\u0026#34;sum\u0026#34;, \u0026#34;mean\u0026#34;, \u0026#34;median\u0026#34;]) 当然我们也可以和lambda匿名函数混合着搭配\ndf.agg([\u0026#34;sum\u0026#34;, lambda x: x.mean()]) A B C sum -0.066486 -1.288341 -1.236244 \u0026lt;lambda\u0026gt; -0.013297 -0.257668 -0.247249 或者和自己定义的函数方法混合着用\ndef my_mean(x): return x.mean() df.agg([\u0026#34;sum\u0026#34;, my_mean]) A B C sum -4.850201 -1.544773 0.429007 my_mean -0.970040 -0.308955 0.085801 与此同时，我们在agg()方法中添加字典，实现不同的列使用不同的函数方法\ndf.agg({\u0026#34;A\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;mean\u0026#34;}) A -0.801753 B 0.097550 dtype: float64 \n索引和列名的重命名 针对索引和列名的重命名，我们可以通过pandas当中的rename()方法来实现，例如我们有这样一个数据集\ndf1 = pd.DataFrame(np.random.randn(5, 3), columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;], index = [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;]) df1 A B C a 0.343690 0.869984 -1.929814 b 1.025613 0.470155 -0.242463 c -0.400908 -0.362684 0.226857 d -1.339706 -0.302005 -1.784452 e -0.957026 -0.813600 0.215098 我们可以这样来操作\ndf1.rename(columns={\u0026#34;A\u0026#34;: \u0026#34;one\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;two\u0026#34;, \u0026#34;C\u0026#34;: \u0026#34;three\u0026#34;}, index={\u0026#34;a\u0026#34;: \u0026#34;apple\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;banana\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;cat\u0026#34;}) one two three apple 0.383813 0.588964 -0.162386 banana -0.462068 -2.938896 0.935492 cat -0.059807 -1.987281 0.095432 d -0.085230 2.013733 -1.324039 e -0.678352 0.306776 0.808697 当然我们可以拆开来，单独对行或者是列进行重命名，对列的重命名可以这么来做\ndf1.rename({\u0026#34;A\u0026#34;: \u0026#34;one\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;two\u0026#34;, \u0026#34;C\u0026#34;: \u0026#34;three\u0026#34;}, axis = \u0026#34;columns\u0026#34;) one two three a -0.997108 -1.383011 0.474298 b 1.009910 0.286303 1.120783 c 1.130700 -0.566922 1.841451 d -0.350438 -0.171079 -0.079804 e 0.988050 -0.524604 0.653306 \n排序 在pandas当中，我们可以针对数据集当中的值来进行排序\ndf1 = pd.DataFrame( {\u0026#34;one\u0026#34;: [2, 1, 1, 1], \u0026#34;two\u0026#34;: [1, 3, 2, 4], \u0026#34;three\u0026#34;: [5, 4, 3, 2]} ) df1 one two three 0 2 1 5 1 1 3 4 2 1 2 3 3 1 4 2 我们按照“three”这一列当中的数值来进行排序\ndf1.sort_values(by = \u0026#34;three\u0026#34;) one two three 3 1 4 2 2 1 2 3 1 1 3 4 0 2 1 5 我们也可以依照多列进行排序\ndf1.sort_values(by = [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;]) one two three 2 1 2 3 1 1 3 4 3 1 4 2 0 2 1 5 在“one”这一列相等的时候，比较“two”这一列数值的大小，在排序的过程当中，默认采用的都是升序，我们可以改成降序来进行编排\ndf1.sort_values(\u0026#34;two\u0026#34;, ascending=False) one two three 3 1 4 2 1 1 3 4 2 1 2 3 0 2 1 5 \n数据类型的转换 最后涉及到的是数据类型的转换，在这之前，我们先得知道如何来查看数据的类型，pandas当中有相应的方法可以处理\ndf2 = pd.DataFrame( { \u0026#34;A\u0026#34;: pd.Series(np.random.randn(5), dtype=\u0026#34;float16\u0026#34;), \u0026#34;B\u0026#34;: pd.Series(np.random.randn(5)), \u0026#34;C\u0026#34;: pd.Series(np.array(np.random.randn(5), dtype=\u0026#34;uint8\u0026#34;)), } ) df2 A B C 0 -0.498779 -0.501512 0 1 -0.055817 -0.528227 254 2 -0.914551 0.763298 1 3 -0.916016 1.366833 0 4 1.993164 1.834457 0 我们通过dtypes属性来查看数据的类型\nA float16 B float64 C uint8 dtype: object 而通过astype()方法来实现数据类型的转换\ndf2[\u0026#34;B\u0026#34;].astype(\u0026#34;int64\u0026#34;) 0 0 1 0 2 0 3 2 4 1 Name: B, dtype: int64 \n根据数据类型来筛选 与此同时，我们也可以根据相对应的数据类型来进行筛选，运用pandas当中的select_dtypes方法，我们先来创建一个数据集包含了各种数据类型的\ndf = pd.DataFrame( { \u0026#34;string_1\u0026#34;: list(\u0026#34;abcde\u0026#34;), \u0026#34;int64_1\u0026#34;: list(range(1, 6)), \u0026#34;uint8_1\u0026#34;: np.arange(3, 8).astype(\u0026#34;u1\u0026#34;), \u0026#34;float64_1\u0026#34;: np.arange(4.0, 9.0), \u0026#34;bool1\u0026#34;: [True, False, True, True, False], \u0026#34;bool2\u0026#34;: [False, True, False, False, True], \u0026#34;dates_1\u0026#34;: pd.date_range(\u0026#34;now\u0026#34;, periods=5), \u0026#34;category_1\u0026#34;: pd.Series(list(\u0026#34;ABCDE\u0026#34;)).astype(\u0026#34;category\u0026#34;), } ) df string_1 int64_1 uint8_1 ... bool2 dates_1 category_1 0 a 1 3 ... False 2021-11-10 10:43:05.957685 A 1 b 2 4 ... True 2021-11-11 10:43:05.957685 B 2 c 3 5 ... False 2021-11-12 10:43:05.957685 C 3 d 4 6 ... False 2021-11-13 10:43:05.957685 D 4 e 5 7 ... True 2021-11-14 10:43:05.957685 E 我们先来查看一下各个列的数据类型\ndf.dtypes string_1 object int64_1 int64 uint8_1 uint8 float64_1 float64 bool1 bool bool2 bool dates_1 datetime64[ns] category_1 category dtype: object 我们筛选类型为布尔值的数据\ndf.select_dtypes(include=[bool]) bool1 bool2 0 True False 1 False True 2 True False 3 True False 4 False True 筛选出数据类型为整型的数据\ndf.select_dtypes(include=[\u0026#39;int64\u0026#39;]) int64_1 0 1 1 2 2 3 3 4 4 5 \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/pandas_must_know_50_examples/","summary":"src: https://mp.weixin.qq.com/s/j71IPWmT57g3VTajGhgN7Q author: 俊欣 公众号: 关于数据分析与可视化\n 本篇我们继续前面pandas系列教程的探讨，今天小编会介绍pandas库当中一些非常基础的方法与函数，希望大家看了之后会有所收获。\n准备需要的数据集 我们先准备生成一些随机数，作为后面需要用到的数据集\nimport pandas as pd import numpy as np index = pd.date_range(\u0026#34;1/1/2000\u0026#34;, periods=8) series = pd.Series(np.random.randn(5), index=[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;]) df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) \nHead and tail head()和tail()方法是用来查看数据集当中的前几行和末尾几行的，默认是查看5行，当然读者朋友也可以自行设定行数\nseries2 = pd.Series(np.random.randn(100)) series2.head() 0 0.578276 1 0.643313 2 -0.336030 3 -0.422468 4 -0.493812 dtype: float64  # 同理 series2.tail() 95 1.307962 96 1.165135 97 0.717692 98 0.","title":"推荐|pandas必知必会50例"},{"content":"spacy 产业级自然语言处理python包 https://spacy.io/\n点击下载\n特性  支持64+语言 针对19门语言的64流水线pipeline处理函数 多任务预训练transformers，如BERT 预训练词向量 支持命名实体识别 支持 POS词性标注 支持 句法依存 支持 文本分类 支持 词干化 内置可视化  spacy安装 pip install spacy==3.2.0 \n模型下载安装 sm小型/ md中型/ lg大型\n  中文模型3.2.0版\n zh_core_web_sm https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.2.0/zh_core_web_sm-3.2.0-py3-none-any.whl zh_core_web_md https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.2.0/zh_core_web_md-3.2.0-py3-none-any.whl zh_core_web_lg https://github.com/explosion/spacy-models/releases/download/zh_core_web_lg-3.2.0/zh_core_web_lg-3.2.0-py3-none-any.whl    英文模型3.2.0版\n en_core_web_sm https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl en_core_web_md https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl en_core_web_lg https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl    注意： 模型大小的区别主要体现在词向量维度数的差距，模型越大， 词向量的维度越多。\n以版本3.2.0的en_core_web_sm为例，点击对应链接，下载至桌面。\n打开命令行， 依次执行\ncd Desktop pip3 install en_core_web_sm-3.2.0-py3-none-any.whl 即可安装完成。\nDoc类型  doc.lang_ doc的语言 doc.text doc的文本 doc.ents 文本中的实体词  import spacy #使用中文zh_core_web_sm模型 zh_nlp = spacy.load(\u0026#34;zh_core_web_sm\u0026#34;) test1 = \u0026#39;国家管网微信公众号11月13日消息，11月12日，国家管网集团首期绿色超短期融资券在全国银行间市场成功发行。此次债券发行是国家管网集团成立后首次在公开市场亮相，由工商银行独家承销，发行金额60亿元、期限270天，为本年度单笔最大金额绿色超短期融资券，募集资金将全部用于储气库等绿色低碳天然气储运基础设施建设；发行票面利率2.41%，认购总规模达2.53倍，低于资本市场同期可比产品利率超10个BP，反映了市场对绿色基础设施建设项目的青睐。\u0026#39; doc1 = zh_nlp(test1) doc1 国家管网微信公众号11月13日消息，11月12日，国家管网集团首期绿色超短期融资券在全国银行间市场成功发行。此次债券发行是国家管网集团成立后首次在公开市场亮相，由工商银行独家承销，发行金额60亿元、期限270天，为本年度单笔最大金额绿色超短期融资券，募集资金将全部用于储气库等绿色低碳天然气储运基础设施建设；发行票面利率2.41%，认购总规模达2.53倍，低于资本市场同期可比产品利率超10个BP，反映了市场对绿色基础设施建设项目的青睐。  doc1.lang_ 'zh'  doc1.text '国家管网微信公众号11月13日消息，11月12日，国家管网集团首期绿色超短期融资券在全国银行间市场成功发行。此次债券发行是国家管网集团成立后首次在公开市场亮相，由工商银行独家承销，发行金额60亿元、期限270天，为本年度单笔最大金额绿色超短期融资券，募集资金将全部用于储气库等绿色低碳天然气储运基础设施建设；发行票面利率2.41%，认购总规模达2.53倍，低于资本市场同期可比产品利率超10个BP，反映了市场对绿色基础设施建设项目的青睐。'  doc1.vector array([-1.81135774e-01, 2.31929451e-01, 1.45746097e-01, 6.82696044e-01, -8.44623148e-03, -2.21295916e-02, 4.06811416e-01, -4.60287899e-01, -5.73987663e-01, -1.33687481e-01, -5.34314513e-01, -6.64901555e-01, -3.94947737e-01, 6.35875063e-03, -2.03339502e-01, 5.78875951e-02, -3.34325433e-01, -3.77648622e-01, 2.43863747e-01, -5.56892566e-02, -7.30801523e-01, -2.41785884e-01, -4.50579911e-01, -3.13598923e-02, 9.07084942e-02, -8.06667805e-01, 7.28501499e-01, -8.59559357e-01, -4.44110222e-02, 9.64611948e-01, -2.57230818e-01, 1.09481342e-01, -3.73580456e-01, -8.51007993e-04, 5.30374162e-02, -5.51876485e-01, -4.82654065e-01, 2.68822908e-01, -4.20012563e-01, 4.33068752e-01, -5.14427841e-01, 5.53584039e-01, -2.00293139e-02, 9.45062563e-02, 1.04523234e-01, 1.34134221e+00, -5.23905218e-01, 1.31230903e+00, 3.28943968e-01, 3.39987069e-01, 8.26785386e-01, 5.35273492e-01, -4.27510649e-01, -1.02807179e-01, -1.91500232e-01, 2.63696283e-01, 6.33961499e-01, -5.65908328e-02, -1.94336250e-01, -5.89190602e-01, 2.22078279e-01, 3.41992415e-02, 5.37312031e-01, 2.77926654e-01, -3.00608397e-01, -6.42910838e-01, -1.33188680e-01, 2.82793492e-01, 6.25911206e-02, 2.08833948e-01, 2.69211121e-02, 1.65822819e-01, -4.32190485e-02, -6.67634964e-01, 6.50937319e-01, -2.43003711e-01, 9.57057327e-02, -3.56370257e-03, -1.13566548e-01, -1.65319979e-01, 7.40000159e-02, 3.65676880e-01, -2.21356809e-01, 2.03256473e-01, 2.26293072e-01, 3.11525285e-01, 3.37869138e-01, -3.12896192e-01, 5.31899095e-01, -1.86223835e-01, -6.03411011e-02, 4.97923464e-01, 3.10418844e-01, -2.48594299e-01, -3.67455184e-01, -4.46804255e-01], dtype=float32)  #doc1中的实体词 doc1.ents (11月13日, 11月12日, 国家管网集团, 全国银行, 国家管网集团, 工商银行, 60亿元, 270天, 2, 2, 53, 超10)  #doc1中的实体词类别 [ent.label_ for ent in doc1.ents] ['DATE', 'DATE', 'ORG', 'ORG', 'ORG', 'ORG', 'MONEY', 'DATE', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL']  Token类型  token.text 文本 token.pos_ 词性  for token in doc1: print(token.text, \u0026#39; \u0026#39;, token.pos_) 国家 NOUN 管网 NOUN 微信 ADJ 公众号 NOUN 11月 NOUN 13日 NOUN 消息 NOUN ， PUNCT 11月 NOUN 12日 NOUN ， PUNCT 国家 NOUN 管网 NOUN 集团 NOUN 首期 ADV 绿色 VERB 超短 NOUN 期融 NOUN 资券 VERB 在 ADP 全国 ADJ 银行 NOUN 间 PART 市场 NOUN 成功 ADV 发行 VERB 。 PUNCT 此次 DET 债券 NOUN 发行 VERB 是 VERB 国家 NOUN 管网 NOUN 集团 NOUN 成立 VERB 后 PART 首次 ADV 在 ADP 公开 ADJ 市场 NOUN 亮相 VERB ， PUNCT 由 ADP 工商 NOUN 银行 NOUN 独家 ADV 承销 VERB ， PUNCT 发行 NOUN 金额 NOUN 60亿 NUM 元 NUM 、 PUNCT 期限 NOUN 270 NUM 天 NUM ， PUNCT 为 ADP 本 DET 年度 NOUN 单笔 NOUN 最 ADV 大 ADJ 金额 NOUN 绿色 ADJ 超短 NOUN 期融 NOUN 资券 NOUN ， PUNCT 募集 NOUN 资金 NOUN 将 ADV 全部 ADV 用于 VERB 储气库 NOUN 等 PART 绿色 ADJ 低碳 VERB 天然气 NOUN 储运 NOUN 基础 NOUN 设施 NOUN 建设 NOUN ； PUNCT 发行 VERB 票面 ADJ 利率 NOUN 2 NUM . PUNCT 41% NOUN ， PUNCT 认购 NOUN 总 ADJ 规模 NOUN 达 VERB 2 NUM . PUNCT 53 NUM 倍 NUM ， PUNCT 低于 VERB 资本 NOUN 市场 NOUN 同期 NOUN 可比 ADV 产品 NOUN 利率 NOUN 超10 VERB 个 NUM BP NOUN ， PUNCT 反映 VERB 了 PART 市场 NOUN 对 ADP 绿色 ADJ 基础 NOUN 设施 NOUN 建设 NOUN 项目 NOUN 的 PART 青睐 NOUN 。 PUNCT  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/spacy_industry_application/","summary":"spacy 产业级自然语言处理python包 https://spacy.io/\n点击下载\n特性  支持64+语言 针对19门语言的64流水线pipeline处理函数 多任务预训练transformers，如BERT 预训练词向量 支持命名实体识别 支持 POS词性标注 支持 句法依存 支持 文本分类 支持 词干化 内置可视化  spacy安装 pip install spacy==3.2.0 \n模型下载安装 sm小型/ md中型/ lg大型\n  中文模型3.2.0版\n zh_core_web_sm https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.2.0/zh_core_web_sm-3.2.0-py3-none-any.whl zh_core_web_md https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.2.0/zh_core_web_md-3.2.0-py3-none-any.whl zh_core_web_lg https://github.com/explosion/spacy-models/releases/download/zh_core_web_lg-3.2.0/zh_core_web_lg-3.2.0-py3-none-any.whl    英文模型3.2.0版\n en_core_web_sm https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl en_core_web_md https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl en_core_web_lg https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl    注意： 模型大小的区别主要体现在词向量维度数的差距，模型越大， 词向量的维度越多。\n以版本3.2.0的en_core_web_sm为例，点击对应链接，下载至桌面。\n打开命令行， 依次执行\ncd Desktop pip3 install en_core_web_sm-3.2.0-py3-none-any.whl 即可安装完成。\nDoc类型  doc.lang_ doc的语言 doc.","title":"spacy产业级自然语言处理包"},{"content":"1. 酒精消费 此可视化显示了 2001 年至 2018 年人均（人均）饮酒量最高的国家。有趣的是，这段时间的最高国家主要由东非和欧洲国家组成。\n如果您想了解更多关于全球酒精消费的信息，请查看牛津关于全球酒精消费的报告\nhttps://ourworldindata.org/alcohol-consumptio\n以防万一您感兴趣，这种类型的数据可视化称为条形图竞赛。 我相信你已经在 YouTube 和 Reddit 上看到了很多这样的内容。 如果您想自己构建一个，这里有一个教程，您可以查看\nhttps://towardsdatascience.com/step-by-step-tutorial-create-a-bar-chart-race-animation-da7d5fcd7079\n2. 健康\u0026amp;财富的221年 通常少即是多，但这是我最喜欢的可视化，因为它以清晰的方式传达了如此多的信息，而且非常积极！\n此可视化显示了每个国家的财富（人均 GDP）和健康（平均预期寿命）如何随时间变化。 圆圈的大小代表每个国家的人口，颜色代表每个国家所属的大陆。\n看到我们作为一个物种走了多远真是太神奇了，是吧？\nhttps://www.reddit.com/r/dataisbeautiful/comments/lmlrks/oc_our_health_and_wealth_over_221_years/\n3. 地球光纤电缆网络 3D 地图 这个由 Tyler Morgan 创建的可视化是世界光缆网络的 3D 地图。 该网络用于传输电话信号、互联网通信和电视信号。\n真正看到我们在全球范围内的相互联系是非常疯狂的，不是吗？\n这是使用 rayrender 和 geojsonsf 包在 R 中创建的。 如果您想查看完整代码，可以在此处查看。\nhttps://gist.github.com/tylermorganwall/b222fcebcac3de56a6e144d73d166322\n4. 美国Covid病例增长 我不是特别喜欢花哨的数据可视化，因为它们通常不像简单的图形（如折线图）那样有效地传达信息。 但是，因为这个动画非常独特（而且有点令人不安），我觉得有必要将它添加到前 10 个可视化效果中。\n虽然没有轴可以告诉我们绝对数字，但它是一个简洁的可视化，向我们展示了与 2020 年年初相比，COVID 病例数的增长速度。\n这是使用 d3 创建的，完整代码可以在这里找到。\nhttps://observablehq.com/@bagami/the-us-covid-syringe\n5. 美国 COVID 等值线图 此可视化告诉我们，从 2020 年 2 月开始到 2021 年 10 月，美国的 COVID 病例是如何增长的。有趣的是，您可以清楚地看到这一时期 COVID 的“波浪”。\n这种类型的可视化被称为等值线图，它在比较不同地区（州、国家、大陆等）的特定变量随着时间的推移非常强大。\n如果你想学习如何用 Python 构建一个，我在这里写了一份创建 Choropleth 地图的分步指南。\nhttps://towardsdatascience.com/visualizing-the-coronavirus-pandemic-with-choropleth-maps-7f30fccaecf5\n6. 所有精神疾病的地图 此可视化显示了 DSM-5 中的每一种精神障碍，它代表精神障碍诊断和统计手册。 它是美国精神障碍的标准分类。 有 20 多个类别和数百种疾病，一旦您开始了解它，这种可视化可能会花费比您想象的更多的时间。\nhttps://www.reddit.com/r/dataisbeautiful/comments/kugn7e/oc_every_mental_disorder_diagnosis_in_the_dsm5/\nhttps://webcache.googleusercontent.com/search?q=cache:LY74prf8a0gJ:https://www.psychiatry.org/File%2520Library/Psychiatrists/Practice/DSM/APA_DSM-5-Contents.pdf+\u0026amp;cd=1\u0026amp;hl=en\u0026amp;ct=clnk\u0026amp;gl=us\u0026amp;client=safari\n7. 我们的塑料去哪儿了？ 该动画展示了塑料的生命周期，以及其中大部分的结束位置。 可悲的是，很明显，我们没有像我们应该回收的那样回收尽可能多的塑料，惊人的 60% 最终被填埋或进入海洋。\n这种类型的可视化是一个动画桑基图，它类似于树图，因为它将数据分解为几个子组并按比例表示值。\n如果您想学习如何在 Python 中构建 Sankey 图，请查看此链接。\nhttps://towardsdatascience.com/sankey-diagram-basics-with-pythons-plotly-7a13d557401a\n8. 近60年来，Top 100 艺术家 最后，这个可视化显示了 Billboard 1960 年到 2020 年的前 100 位艺术家。我个人喜欢这个图表，因为它提供了很多信息：你可以看到顶级艺术家是谁，他们什么时候最流行，以及他们有多少歌曲 称霸排行榜！\n你认识多少艺术家？\n 点击上方图片购买课程   了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/8_most_popular_vis/","summary":"1. 酒精消费 此可视化显示了 2001 年至 2018 年人均（人均）饮酒量最高的国家。有趣的是，这段时间的最高国家主要由东非和欧洲国家组成。\n如果您想了解更多关于全球酒精消费的信息，请查看牛津关于全球酒精消费的报告\nhttps://ourworldindata.org/alcohol-consumptio\n以防万一您感兴趣，这种类型的数据可视化称为条形图竞赛。 我相信你已经在 YouTube 和 Reddit 上看到了很多这样的内容。 如果您想自己构建一个，这里有一个教程，您可以查看\nhttps://towardsdatascience.com/step-by-step-tutorial-create-a-bar-chart-race-animation-da7d5fcd7079\n2. 健康\u0026amp;财富的221年 通常少即是多，但这是我最喜欢的可视化，因为它以清晰的方式传达了如此多的信息，而且非常积极！\n此可视化显示了每个国家的财富（人均 GDP）和健康（平均预期寿命）如何随时间变化。 圆圈的大小代表每个国家的人口，颜色代表每个国家所属的大陆。\n看到我们作为一个物种走了多远真是太神奇了，是吧？\nhttps://www.reddit.com/r/dataisbeautiful/comments/lmlrks/oc_our_health_and_wealth_over_221_years/\n3. 地球光纤电缆网络 3D 地图 这个由 Tyler Morgan 创建的可视化是世界光缆网络的 3D 地图。 该网络用于传输电话信号、互联网通信和电视信号。\n真正看到我们在全球范围内的相互联系是非常疯狂的，不是吗？\n这是使用 rayrender 和 geojsonsf 包在 R 中创建的。 如果您想查看完整代码，可以在此处查看。\nhttps://gist.github.com/tylermorganwall/b222fcebcac3de56a6e144d73d166322\n4. 美国Covid病例增长 我不是特别喜欢花哨的数据可视化，因为它们通常不像简单的图形（如折线图）那样有效地传达信息。 但是，因为这个动画非常独特（而且有点令人不安），我觉得有必要将它添加到前 10 个可视化效果中。\n虽然没有轴可以告诉我们绝对数字，但它是一个简洁的可视化，向我们展示了与 2020 年年初相比，COVID 病例数的增长速度。\n这是使用 d3 创建的，完整代码可以在这里找到。\nhttps://observablehq.com/@bagami/the-us-covid-syringe\n5. 美国 COVID 等值线图 此可视化告诉我们，从 2020 年 2 月开始到 2021 年 10 月，美国的 COVID 病例是如何增长的。有趣的是，您可以清楚地看到这一时期 COVID 的“波浪”。","title":"2021最流行的8张可视化图"},{"content":" 整理自阮一峰技术周刊\nsrc:https://github.com/ruanyf/weekly\n Netlify Drap 拖拽本地html文件夹至该网站，可快速实现建站\nhttps://app.netlify.com/drop\n 类似的还有fleek https://fleek.co/\nroapi 浏览器查看csv数据\nhttps://github.com/roapi/roapi\n OCRmyPDF 为扫描的 PDF 文件添加了 OCR 文本层，允许对其进行搜索\nhttps://github.com/ocrmypdf/OCRmyPDF\n CodePng 把代码转为美观的截图\nhttps://www.codepng.app/\n  https://github.com/zail0/codePng\ngiscus 为个人博客添加评论区\nhttps://github.com/giscus/giscus\nYouglish 通过搜索查找出现某单词的视频片段，可导出对应音频\nhttps://youglish.com/\n Agora Flat 开源教室 前后端完全开源 快速搭建简约美观的互动教室\n 杰文斯悖论 1865年，英国经济学家威廉·斯坦利·杰文斯（William Stanley Jevons）观察到，提高煤炭的使用效率，会导致煤炭消费量的增加。\n他因而提出，节省资源的技术，只会带来资源使用量的增加。 由于这与人们的直觉相反，所以称为杰文斯悖论。\n杰文斯悖论有时被认为意味着，节约能源的努力是徒劳的。提高化石能源的使用效率，反而会加快化石能源的耗尽速度。\n这说明，可持续能源政策不能只依赖节能技术的进步，而必须配合其他类型的政府干预措施，限制能源的使用。\n Paul Graham博客文集 经典文章值得反复阅读\nhttps://github.com/evmn/Paul-Graham\n ##了解课程\n 点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/techweekly01/","summary":"整理自阮一峰技术周刊\nsrc:https://github.com/ruanyf/weekly\n Netlify Drap 拖拽本地html文件夹至该网站，可快速实现建站\nhttps://app.netlify.com/drop\n 类似的还有fleek https://fleek.co/\nroapi 浏览器查看csv数据\nhttps://github.com/roapi/roapi\n OCRmyPDF 为扫描的 PDF 文件添加了 OCR 文本层，允许对其进行搜索\nhttps://github.com/ocrmypdf/OCRmyPDF\n CodePng 把代码转为美观的截图\nhttps://www.codepng.app/\n  https://github.com/zail0/codePng\ngiscus 为个人博客添加评论区\nhttps://github.com/giscus/giscus\nYouglish 通过搜索查找出现某单词的视频片段，可导出对应音频\nhttps://youglish.com/\n Agora Flat 开源教室 前后端完全开源 快速搭建简约美观的互动教室\n 杰文斯悖论 1865年，英国经济学家威廉·斯坦利·杰文斯（William Stanley Jevons）观察到，提高煤炭的使用效率，会导致煤炭消费量的增加。\n他因而提出，节省资源的技术，只会带来资源使用量的增加。 由于这与人们的直觉相反，所以称为杰文斯悖论。\n杰文斯悖论有时被认为意味着，节约能源的努力是徒劳的。提高化石能源的使用效率，反而会加快化石能源的耗尽速度。\n这说明，可持续能源政策不能只依赖节能技术的进步，而必须配合其他类型的政府干预措施，限制能源的使用。\n Paul Graham博客文集 经典文章值得反复阅读\nhttps://github.com/evmn/Paul-Graham\n ##了解课程\n 点击上方图片购买课程   点击进入详情页","title":"TechWeekly-01 | 每周有趣有用的技术分享"},{"content":"cntext 中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等\n github地址 https://github.com/hidadeng/cntext pypi地址 https://pypi.org/project/cntext/ 视频课-Python网络爬虫与文本数据分析  功能模块含\n cntext stats 文本统计,可读性等 dictionary 构建词表(典) sentiment 情感分析 similarity 文本相似度 visualization 可视化，如词云图  代码下载 https://github.com/hidadeng/cntext/tree/main/examples\n安装 pip install cntext==0.9 \n一、cntext 查看cntext基本信息\nimport cntext help(cntext) Run\nHelp on package cntext: NAME cntext PACKAGE CONTENTS description (package) dictionary (package) sentiment (package) similarity (package) visualization (package) DATA ADV_words = [\u0026#39;都\u0026#39;, \u0026#39;全\u0026#39;, \u0026#39;单\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;光\u0026#39;, \u0026#39;尽\u0026#39;, \u0026#39;净\u0026#39;, \u0026#39;仅\u0026#39;, \u0026#39;就\u0026#39;, \u0026#39;只\u0026#39;, \u0026#39;一共\u0026#39;, \u0026#39;... CONJ_words = [\u0026#39;乃\u0026#39;, \u0026#39;乍\u0026#39;, \u0026#39;与\u0026#39;, \u0026#39;无\u0026#39;, \u0026#39;且\u0026#39;, \u0026#39;丕\u0026#39;, \u0026#39;为\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;其\u0026#39;, \u0026#39;况\u0026#39;, \u0026#39;厥\u0026#39;, \u0026#39;... DUTIR_Ais = {\u0026#39;sigh\u0026#39;, \u0026#39;一命呜呼\u0026#39;, \u0026#39;一场春梦\u0026#39;, \u0026#39;一场空\u0026#39;, \u0026#39;一头跌在菜刀上－切肤之痛\u0026#39;, \u0026#39;一念之差\u0026#39;, ..... DUTIR_Haos = {\u0026#39;1兒巴经\u0026#39;, \u0026#39;3x\u0026#39;, \u0026#39;8错\u0026#39;, \u0026#39;BUCUO\u0026#39;, \u0026#39;Cool毙\u0026#39;, \u0026#39;NB\u0026#39;, ...} DUTIR_Jings = {\u0026#39;848\u0026#39;, \u0026#39;FT\u0026#39;, \u0026#39;_god\u0026#39;, \u0026#39;yun\u0026#39;, \u0026#39;一个骰子掷七点－出乎意料\u0026#39;, \u0026#39;一举成名\u0026#39;, ...... DUTIR_Jus = {\u0026#39;一则以喜，一则以惧\u0026#39;, \u0026#39;一发千钧\u0026#39;, \u0026#39;一年被蛇咬，三年怕草索\u0026#39;, \u0026#39;一座皆惊\u0026#39;, \u0026#39;一脸横肉\u0026#39;, \u0026#39;一蛇两头... DUTIR_Les = {\u0026#39;:)\u0026#39;, \u0026#39;CC\u0026#39;, \u0026#39;Happy\u0026#39;, \u0026#39;LOL\u0026#39;, \u0026#39;_so\u0026#39;, \u0026#39;haha\u0026#39;, ...} DUTIR_Nus = {\u0026#39;2气斗狠\u0026#39;, \u0026#39;MD\u0026#39;, \u0026#39;TNND\u0026#39;, \u0026#39;gun\u0026#39;, \u0026#39;kao\u0026#39;, \u0026#39;一刀两断\u0026#39;, ...} DUTIR_Wus = {\u0026#39;B4\u0026#39;, \u0026#39;BD\u0026#39;, \u0026#39;BS\u0026#39;, \u0026#39;HC\u0026#39;, \u0026#39;HJ\u0026#39;, \u0026#39;JJWW\u0026#39;, ...} HOWNET_deny = {\u0026#39;不\u0026#39;, \u0026#39;不可\u0026#39;, \u0026#39;不是\u0026#39;, \u0026#39;不能\u0026#39;, \u0026#39;不要\u0026#39;, \u0026#39;休\u0026#39;, ...} HOWNET_extreme = {\u0026#39;万\u0026#39;, \u0026#39;万万\u0026#39;, \u0026#39;万分\u0026#39;, \u0026#39;万般\u0026#39;, \u0026#39;不亦乐乎\u0026#39;, \u0026#39;不可开交\u0026#39;, ...} HOWNET_ish = {\u0026#39;一些\u0026#39;, \u0026#39;一点\u0026#39;, \u0026#39;一点儿\u0026#39;, \u0026#39;不丁点儿\u0026#39;, \u0026#39;不大\u0026#39;, \u0026#39;不怎么\u0026#39;, ...} HOWNET_more = {\u0026#39;多\u0026#39;, \u0026#39;大不了\u0026#39;, \u0026#39;如斯\u0026#39;, \u0026#39;尤甚\u0026#39;, \u0026#39;强\u0026#39;, \u0026#39;愈\u0026#39;, ...} HOWNET_neg = {\u0026#39;一下子爆发\u0026#39;, \u0026#39;一下子爆发的一连串\u0026#39;, \u0026#39;一不小心\u0026#39;, \u0026#39;一个屁\u0026#39;, \u0026#39;一仍旧贯\u0026#39;, \u0026#39;一偏\u0026#39;, ...} HOWNET_pos = {\u0026#39;\u0026#39;, \u0026#39;一专多能\u0026#39;, \u0026#39;一丝不差\u0026#39;, \u0026#39;一丝不苟\u0026#39;, \u0026#39;一个心眼儿\u0026#39;, \u0026#39;一五一十\u0026#39;, ...} HOWNET_very = {\u0026#39;不为过\u0026#39;, \u0026#39;不少\u0026#39;, \u0026#39;不胜\u0026#39;, \u0026#39;不过\u0026#39;, \u0026#39;何啻\u0026#39;, \u0026#39;何止\u0026#39;, ...} STOPWORDS_en = {\u0026#39;a\u0026#39;, \u0026#39;about\u0026#39;, \u0026#39;above\u0026#39;, \u0026#39;across\u0026#39;, \u0026#39;after\u0026#39;, \u0026#39;afterwards\u0026#39;... STOPWORDS_zh = {\u0026#39;、\u0026#39;, \u0026#39;。\u0026#39;, \u0026#39;〈\u0026#39;, \u0026#39;〉\u0026#39;, \u0026#39;《\u0026#39;, \u0026#39;》\u0026#39;, ...} FORMAL_pos_words = [\u0026#39;100强\u0026#39;, \u0026#39;3A级\u0026#39;, \u0026#39;50强\u0026#39;, \u0026#39;AAA级\u0026#39;, \u0026#39;AAA企业\u0026#39;, \u0026#39;爱戴\u0026#39;,..] FORMAL_neg_words = [\u0026#39;安于现状\u0026#39;, \u0026#39;暗藏\u0026#39;, \u0026#39;暗淡\u0026#39;, \u0026#39;暗黑\u0026#39;, \u0026#39;暗流\u0026#39;, ..] UNFORMAL_pos_words = [\u0026#39;爱心\u0026#39;,\u0026#39;安定\u0026#39;,\u0026#39;安全\u0026#39;,\u0026#39;安然无恙\u0026#39;,\u0026#39;安泰\u0026#39;,\u0026#39;霸主\u0026#39;,...] UNFORMAL_neg_words = [\u0026#39;哀鸿遍野\u0026#39;,\u0026#39;肮脏\u0026#39;,\u0026#39;罢免\u0026#39;,\u0026#39;白痴\u0026#39;,\u0026#39;败笔\u0026#39;,\u0026#39;败诉\u0026#39;,\u0026#39;半信半疑\u0026#39;..] FILE /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/cntext/__init__.py \nfrom cntext import dict_info dict_info() Run\n【大连理工大学情感本体库】 七大情绪分类，依次是哀、恶、好、惊、惧、乐、怒；对应的情绪词表依次： DUTIR_Ais = {\u0026#34;泣血捶膺\u0026#34;, \u0026#34;望断白云\u0026#34;, \u0026#34;日暮途穷\u0026#34;, \u0026#34;身微力薄\u0026#34;...} DUTIR_Wus = {\u0026#34;饰非遂过\u0026#34;, \u0026#34;恶语\u0026#34;, \u0026#34;毁害\u0026#34;, \u0026#34;恶籍盈指\u0026#34;, \u0026#34;脾气爆躁\u0026#34;, \u0026#34;淫贱\u0026#34;, \u0026#34;凌乱\u0026#34;...} DUTIR_Haos = {\u0026#34;打破砂锅璺到底\u0026#34;, \u0026#34;多彩\u0026#34;, \u0026#34;披沙拣金\u0026#34;, \u0026#34;见机行事\u0026#34;, \u0026#34;精神饱满\u0026#34;...} DUTIR_Jings = {\u0026#34;骇人视听\u0026#34;, \u0026#34;拍案惊奇\u0026#34;, \u0026#34;悬念\u0026#34;, \u0026#34;无翼而飞\u0026#34;, \u0026#34;原来\u0026#34;, \u0026#34;冷门\u0026#34;...} DUTIR_Jus ={\u0026#34;山摇地动\u0026#34;, \u0026#34;月黑风高\u0026#34;, \u0026#34;流血\u0026#34;, \u0026#34;老鼠偷猫饭－心惊肉跳\u0026#34;, \u0026#34;一发千钧\u0026#34;...} DUTIR_Les ={\u0026#34;含哺鼓腹\u0026#34;, \u0026#34;欢呼鼓舞\u0026#34;, \u0026#34;莺歌蝶舞\u0026#34;, \u0026#34;将伯之助\u0026#34;, \u0026#34;逸兴横飞\u0026#34;, \u0026#34;舒畅\u0026#34;...} DUTIR_Nus = {\u0026#34;怨气满腹\u0026#34;, \u0026#34;面有愠色\u0026#34;, \u0026#34;愤愤\u0026#34;, \u0026#34;直眉瞪眼\u0026#34;, \u0026#34;负气斗狠\u0026#34;, \u0026#34;挑眼\u0026#34;...} 【知网Hownet词典】 含正负形容词、否定词、副词等词表，对应的词表依次: HOWNET_deny = {\u0026#34;不\u0026#34;, \u0026#34;不是\u0026#34;, \u0026#34;不能\u0026#34;, \u0026#34;不可\u0026#34;...} HOWNET_extreme = {\u0026#34;百分之百\u0026#34;, \u0026#34;倍加\u0026#34;, \u0026#34;备至\u0026#34;, \u0026#34;不得了\u0026#34;...} HOWNET_ish = {\u0026#34;点点滴滴\u0026#34;, \u0026#34;多多少少\u0026#34;, \u0026#34;怪\u0026#34;, \u0026#34;好生\u0026#34;, \u0026#34;还\u0026#34;, \u0026#34;或多或少\u0026#34;...} HOWNET_more = {\u0026#34;大不了\u0026#34;, \u0026#34;多\u0026#34;, \u0026#34;更\u0026#34;, \u0026#34;比较\u0026#34;, \u0026#34;更加\u0026#34;, \u0026#34;更进一步\u0026#34;, \u0026#34;更为\u0026#34;, \u0026#34;还\u0026#34;, \u0026#34;还要\u0026#34;...} HOWNET_neg = {\u0026#34;压坏\u0026#34;, \u0026#34;鲁莽的\u0026#34;, \u0026#34;被控犯罪\u0026#34;, \u0026#34;银根紧\u0026#34;, \u0026#34;警惕的\u0026#34;, \u0026#34;残缺\u0026#34;, \u0026#34;致污物\u0026#34;, \u0026#34;柔弱\u0026#34;...} HOWNET_pos = {\u0026#34;无误\u0026#34;, \u0026#34;感激不尽\u0026#34;, \u0026#34;受大众欢迎\u0026#34;, \u0026#34;敬礼\u0026#34;, \u0026#34;文雅\u0026#34;, \u0026#34;一尘不染\u0026#34;, \u0026#34;高精度\u0026#34;, \u0026#34;兴盛\u0026#34;...} HOWNET_very = {\u0026#34;不为过\u0026#34;, \u0026#34;超\u0026#34;, \u0026#34;超额\u0026#34;, \u0026#34;超外差\u0026#34;, \u0026#34;超微结构\u0026#34;, \u0026#34;超物质\u0026#34;, \u0026#34;出头\u0026#34;...} 【停用词表】 中英文停用词表，依次 STOPWORDS_zh = {\u0026#34;经\u0026#34;, \u0026#34;得\u0026#34;, \u0026#34;则甚\u0026#34;, \u0026#34;跟\u0026#34;, \u0026#34;好\u0026#34;, \u0026#34;具体地说\u0026#34;...} STOPWORDS_en = {\u0026#39;a\u0026#39;, \u0026#39;about\u0026#39;, \u0026#39;above\u0026#39;, \u0026#39;across\u0026#39;, \u0026#39;after\u0026#39;...} 【中文副词/连词】 副词ADV、连词CONJ ADV_words = [\u0026#39;都\u0026#39;, \u0026#39;全\u0026#39;, \u0026#39;单\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;光\u0026#39;...} CONJ_words = [\u0026#39;乃\u0026#39;, \u0026#39;乍\u0026#39;, \u0026#39;与\u0026#39;, \u0026#39;无\u0026#39;, \u0026#39;且\u0026#39;...} 【金融情绪词典】 姚加权,冯绪,王赞钧,纪荣嵘,张维.语调、情绪及市场影响:基于金融情绪词典[J].管理科学学报,2021,24(05):26-46. #正式-肯定情绪词典 FORMAL_pos_words = [\u0026#39;100强\u0026#39;, \u0026#39;3A级\u0026#39;, \u0026#39;50强\u0026#39;, \u0026#39;AAA级\u0026#39;, \u0026#39;AAA企业\u0026#39;, \u0026#39;爱戴\u0026#39;,...] #正式-否定情绪词典 FORMAL_neg_words = [\u0026#39;安于现状\u0026#39;, \u0026#39;暗藏\u0026#39;, \u0026#39;暗淡\u0026#39;, \u0026#39;暗黑\u0026#39;, \u0026#39;暗流\u0026#39;, ...] #非正式-肯定情绪词典 UNFORMAL_pos_words = [\u0026#39;爱心\u0026#39;,\u0026#39;安定\u0026#39;,\u0026#39;安全\u0026#39;,\u0026#39;安然无恙\u0026#39;,\u0026#39;安泰\u0026#39;,\u0026#39;霸主\u0026#39;,...] #非正式-否定情绪词典 UNFORMAL_neg_words = [\u0026#39;哀鸿遍野\u0026#39;,\u0026#39;肮脏\u0026#39;,\u0026#39;罢免\u0026#39;,\u0026#39;白痴\u0026#39;,\u0026#39;败笔\u0026#39;,\u0026#39;败诉\u0026#39;,\u0026#39;半信半疑\u0026#39;...] \n查看词表\nfrom cntext import CONJ_words, ADV_words #获取连词词表 CONJ_words Run\n[\u0026#39;乃\u0026#39;, \u0026#39;乍\u0026#39;, \u0026#39;与\u0026#39;, \u0026#39;无\u0026#39;, \u0026#39;且\u0026#39;, \u0026#39;丕\u0026#39;, \u0026#39;为\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;其\u0026#39;, \u0026#39;况\u0026#39;, \u0026#39;厥\u0026#39;, \u0026#39;则\u0026#39;, \u0026#39;那\u0026#39;, \u0026#39;兼\u0026#39;, ... ] \n二、stats 目前含\n term_freq 词频统计函数，返回Counter类型 readability 中文可读性  from cntext.stats import term_freq, readability text = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更\u0026#39; term_freq(text) Counter({\u0026#39;看待\u0026#39;: 1, \u0026#39;网文\u0026#39;: 1, \u0026#39;作者\u0026#39;: 1, \u0026#39;黑客\u0026#39;: 1, \u0026#39;大佬\u0026#39;: 1, \u0026#39;盗号\u0026#39;: 1, \u0026#39;改文因\u0026#39;: 1, \u0026#39;万分\u0026#39;: 1, \u0026#39;惭愧\u0026#39;: 1, \u0026#39;停\u0026#39;: 1}) \n**中文可读性 ** 算法参考自\n 徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.\n  readability1 \u0026mdash;每个分句中的平均字数 readability2 \u0026mdash;每个句子中副词和连词所占的比例 readability3 \u0026mdash;参考Fog Index， readability3=(readability1+readability2)×0.5  以上三个指标越大，都说明文本的复杂程度越高，可读性越差。\nreadability(text) {\u0026#39;readability1\u0026#39;: 27.0, \u0026#39;readability2\u0026#39;: 0.17647058823529413, \u0026#39;readability3\u0026#39;: 13.588235294117647} \n三、dictionary 本模块用于构建词表(典),含\n SoPmi 共现法扩充词表(典) W2VModels 词向量word2vec扩充词表(典)  3.1 SoPmi 共现法 from cntext.dictionary import SoPmi import os sopmier = SoPmi(cwd=os.getcwd(), input_txt_file=\u0026#39;data/sopmi_corpus.txt\u0026#39;, #原始数据，您的语料 seedword_txt_file=\u0026#39;data/sopmi_seed_words.txt\u0026#39;, #人工标注的初始种子词 ) sopmier.sopmi() Run\nstep 1/4:...seg corpus ... Loading model cost 0.678 seconds. Prefix dict has been built successfully. step 1/4 finished:...cost 60.78995203971863... step 2/4:...collect cowords ... step 2/4 finished:...cost 0.6169600486755371... step 3/4:...compute sopmi ... step 1/4 finished:...cost 0.26422882080078125... step 4/4:...save candiwords ... finished! cost 61.8965539932251 \n3.2 W2VModels 词向量 from cntext.dictionary import W2VModels import os #初始化模型 model = W2VModels(cwd=os.getcwd()) #语料数据 w2v_corpus.txt model.train(input_txt_file=\u0026#39;data/w2v_corpus.txt\u0026#39;) #根据种子词，筛选出没类词最相近的前100个词 model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/integrity.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/innovation.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/quality.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/respect.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/teamwork.txt\u0026#39;, topn=100) Run\n数据预处理开始....... 预处理结束........... Word2Vec模型训练开始...... 已将模型存入 /Users/Desktop/cntext/test/output/w2v_candi_words/w2v.model 准备寻找每个seed在语料中所有的相似候选词 初步搜寻到 572 个相似的候选词 计算每个候选词 与 integrity 的相似度， 选出相似度最高的前 100 个候选词 已完成 【integrity 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/integrity.txt， 耗时 46 秒 准备寻找每个seed在语料中所有的相似候选词 初步搜寻到 516 个相似的候选词 计算每个候选词 与 innovation 的相似度， 选出相似度最高的前 100 个候选词 已完成 【innovation 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/innovation.txt， 耗时 46 秒 准备寻找每个seed在语料中所有的相似候选词 初步搜寻到 234 个相似的候选词 计算每个候选词 与 quality 的相似度， 选出相似度最高的前 100 个候选词 已完成 【quality 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/quality.txt， 耗时 46 秒 准备寻找每个seed在语料中所有的相似候选词 初步搜寻到 243 个相似的候选词 计算每个候选词 与 respect 的相似度， 选出相似度最高的前 100 个候选词 已完成 【respect 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/respect.txt， 耗时 46 秒 准备寻找每个seed在语料中所有的相似候选词 初步搜寻到 319 个相似的候选词 计算每个候选词 与 teamwork 的相似度， 选出相似度最高的前 100 个候选词 已完成 【teamwork 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/teamwork.txt， 耗时 46 秒 \n四、 sentiment  senti_by_hownet 使用知网Hownet词典对文本进行情感分析 senti_by_dutir 使用大连理工大学情感本体库dutir对文本进行情绪分析 senti_by_diydict 使用自定义词典 对文本进行情感分析  4.1 senti_by_hownet(text, adj_adv=False) 使用知网Hownet词典进行(中)文本数据的情感分析，统计正、负情感信息出现次数(得分)\n text: 待分析的中文文本数据 adj_adv: 是否考虑副词（否定词、程度词）对情绪形容词的反转和情感强度修饰作用，默认False。默认False只统计情感形容词出现个数；  from cntext.sentiment import senti_by_hownet text = \u0026#39;今天股票大涨，心情倍爽，非常开心啊。\u0026#39; senti_by_dutir(text) Run\n{\u0026#39;word_num\u0026#39;: 12, \u0026#39;sentence_num\u0026#39;: 2, \u0026#39;stopword_num\u0026#39;: 4, \u0026#39;好_num\u0026#39;: 0, \u0026#39;乐_num\u0026#39;: 1, \u0026#39;哀_num\u0026#39;: 0, \u0026#39;怒_num\u0026#39;: 0, \u0026#39;惧_num\u0026#39;: 0, \u0026#39;恶_num\u0026#39;: 0, \u0026#39;惊_num\u0026#39;: 0} \n考虑副词（否定词、程度词）对情绪形容词的反转和情感强度修饰作用\nsenti_by_hownet(text, adj_adv=True) Run\n{\u0026#39;sentence_num\u0026#39;: 1, \u0026#39;word_num\u0026#39;: 12, \u0026#39;stopword_num\u0026#39;: 3, \u0026#39;pos_score\u0026#39;: 13.0, \u0026#39;neg_score\u0026#39;: 0.0} \n4.2 senti_by_dutir(text) 使用大连理工大学情感本体库对文本进行情绪分析，统计各情绪词语出现次数。\nfrom cntext.sentiment import senti_by_dutir text = \u0026#39;今天股票大涨，心情倍爽，非常开心啊。\u0026#39; senti_by_dutir(text) Run\n{\u0026#39;word_num\u0026#39;: 12, \u0026#39;sentence_num\u0026#39;: 2, \u0026#39;stopword_num\u0026#39;: 4, \u0026#39;好_num\u0026#39;: 0, \u0026#39;乐_num\u0026#39;: 1, \u0026#39;哀_num\u0026#39;: 0, \u0026#39;怒_num\u0026#39;: 0, \u0026#39;惧_num\u0026#39;: 0, \u0026#39;恶_num\u0026#39;: 0, \u0026#39;惊_num\u0026#39;: 0}  情绪分析使用的大连理工大学情感本体库，如发表论文，请注意用户许可协议\n如果用户使用该资源发表论文或取得科研成果，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。\n参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”\n \n4.3 senti_by_diy(text) 使用diy词典进行情感分析，计算各个情绪词出现次数，未考虑强度副词、否定词对情感的复杂影响，\n text: 待分析中文文本 sentiwords: 情感词字典； {\u0026lsquo;category1\u0026rsquo;: \u0026lsquo;category1 词语列表\u0026rsquo;, \u0026lsquo;category2\u0026rsquo;: \u0026lsquo;category2词语列表\u0026rsquo;, \u0026lsquo;category3\u0026rsquo;: \u0026lsquo;category3词语列表\u0026rsquo;, \u0026hellip; }  sentiwords = {\u0026#39;pos\u0026#39;: [\u0026#39;开心\u0026#39;, \u0026#39;愉快\u0026#39;, \u0026#39;倍爽\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;难过\u0026#39;, \u0026#39;悲伤\u0026#39;], \u0026#39;adv\u0026#39;: [\u0026#39;倍\u0026#39;]} text = \u0026#39;今天股票大涨，心情倍爽，非常开心啊。\u0026#39; senti_by_diydict(text, sentiwords) Run\n{\u0026#39;pos_num\u0026#39;: 1, \u0026#39;neg_num\u0026#39;: 0, \u0026#39;adv_num\u0026#39;: 1, \u0026#39;stopword_num\u0026#39;: 4, \u0026#39;sentence_num\u0026#39;: 2, \u0026#39;word_num\u0026#39;: 12} \n4.4 注意 返回结果: num表示词语出现次数； score是考虑副词、否定词对情感的修饰，结果不是词频，是情感类别的得分。\n\n五、similarity 使用cosine、jaccard、miniedit等计算两文本的相似度，算法实现参考自\n Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.\n from cntext.similarity import similarity_score text1 = \u0026#39;编程真好玩编程真好玩\u0026#39; text2 = \u0026#39;游戏真好玩编程真好玩\u0026#39; similarity_score(text1, text2) Run\n{\u0026#39;Sim_Cosine\u0026#39;: 0.816496580927726, \u0026#39;Sim_Jaccard\u0026#39;: 0.6666666666666666, \u0026#39;Sim_MinEdit\u0026#39;: 1, \u0026#39;Sim_Simple\u0026#39;: 0.9183673469387755} \n六、visualization 文本信息可视化，含wordcloud、wordshiftor\n wordcloud 词云图 wordshiftor 两文本词移图  6.1 wordcloud(text, title, html_path)  text: 中文文本字符串数据 title: 词云图标题 html_path: 词云图html文件存储路径  from cntext.visualization import wordcloud text1 = \u0026#34;\u0026#34;\u0026#34;在信息化时代，各种各样的数据被广泛采集和利用，有些数据看似无关紧要甚至好像是公开的，但同样关乎国家安全。11月1日是《反间谍法》颁布实施七周年。近年来，国家安全机关按照《反间谍法》《数据安全法》有关规定，依法履行数据安全监管职责，在全国范围内开展涉外数据专项执法行动，发现一些境外数据公司长期、大量、实时搜集我境内船舶数据，数据安全领域的“商业间谍”魅影重重。 2020年6月，国家安全机关在反间谍专项行动中发现，有境外数据公司通过网络在境内私下招募“数据贡献员”。广东省湛江市国家安全局据此开展调查，在麻斜军港附近发现有可疑的无线电设备在持续搜集湛江港口舰船数据，并通过互联网实时传往境外。在临近海港的一个居民楼里，国家安全机关工作人员最终锁定了位置。 一套简易的无线电设备是AIS陆基基站，用来接收AIS系统发射的船舶数据。AIS系统是船舶身份自动识别系统，国际海事组织要求300总吨以上船舶必须强制安装。船只在航行过程中，通过AIS系统向其他船只和主管部门发送船只航向、航速、目的港等信息，用于航行避让、交通导航、轨迹回溯等功能。国家安全机关查获的设备虽然看上去简陋，功能却十分强大。 国家安全机关进一步调查发现，这个基站的来历并不简单。2016年，湛江市的无线电爱好者郑某偶然收到一封境外某海事数据公司发来的邀请邮件。 作为资深的无线电爱好者，能免费领取价值几千元的设备还能获取更多的船舶信息，郑某当然心动。而且，这个基站的架设也非常容易，只要简单组装连上家里的网络，自己的任务就算完成。郑某马上浏览了这家公司申请无线电设备的页面，并按对方要求填写了信息。 \u0026#34;\u0026#34;\u0026#34; wordcloud(text=text1, title=\u0026#39;词云图测试\u0026#39;, html_path=\u0026#39;output/词云图测试.html\u0026#39;) Run\n 6.2 wordshiftor(text1, text2, title, top_n, matplotlib_family)  text1: 文本数据1；字符串 text2: 文本数据2；字符串 title: 词移图标题 top_n: 显示最常用的前n词； 默认值15 matplotlib_family matplotlib中文字体，默认\u0026quot;Arial Unicode MS\u0026quot;；如绘图字体乱码请，请参考下面提示  text1 = \u0026#34;\u0026#34;\u0026#34;在信息化时代，各种各样的数据被广泛采集和利用，有些数据看似无关紧要甚至好像是公开的，但同样关乎国家安全。11月1日是《反间谍法》颁布实施七周年。近年来，国家安全机关按照《反间谍法》《数据安全法》有关规定，依法履行数据安全监管职责，在全国范围内开展涉外数据专项执法行动，发现一些境外数据公司长期、大量、实时搜集我境内船舶数据，数据安全领域的“商业间谍”魅影重重。 2020年6月，国家安全机关在反间谍专项行动中发现，有境外数据公司通过网络在境内私下招募“数据贡献员”。广东省湛江市国家安全局据此开展调查，在麻斜军港附近发现有可疑的无线电设备在持续搜集湛江港口舰船数据，并通过互联网实时传往境外。在临近海港的一个居民楼里，国家安全机关工作人员最终锁定了位置。 一套简易的无线电设备是AIS陆基基站，用来接收AIS系统发射的船舶数据。AIS系统是船舶身份自动识别系统，国际海事组织要求300总吨以上船舶必须强制安装。船只在航行过程中，通过AIS系统向其他船只和主管部门发送船只航向、航速、目的港等信息，用于航行避让、交通导航、轨迹回溯等功能。国家安全机关查获的设备虽然看上去简陋，功能却十分强大。 国家安全机关进一步调查发现，这个基站的来历并不简单。2016年，湛江市的无线电爱好者郑某偶然收到一封境外某海事数据公司发来的邀请邮件。 作为资深的无线电爱好者，能免费领取价值几千元的设备还能获取更多的船舶信息，郑某当然心动。而且，这个基站的架设也非常容易，只要简单组装连上家里的网络，自己的任务就算完成。郑某马上浏览了这家公司申请无线电设备的页面，并按对方要求填写了信息。 \u0026#34;\u0026#34;\u0026#34; text2 = \u0026#34;\u0026#34;\u0026#34; 通知强调，各地商务主管部门要紧紧围绕保供稳价工作目标，压实“菜篮子”市长负责制，细化工作措施；强化横向协作与纵向联动，加强与有关部门的工作协调，形成工作合力；建立完善省际间和本地区联保联供机制，健全有关工作方案，根据形势及时开展跨区域调运；加强市场运行监测，每日跟踪蔬菜、肉类等重点生活必需品供求和价格变化情况，及时预测，及早预警。 通知要求，各地支持鼓励大型农产品流通企业与蔬菜、粮油、畜禽养殖等农产品生产基地建立紧密合作关系，签订长期供销协议；耐储蔬菜要提前采购，锁定货源，做好本地菜与客菜之间，北菜与南菜之间、设施菜与露天菜之间的梯次轮换和衔接供应；健全完备本地肉类储备规模及管理制度；北方省份要按时完成本年度冬春蔬菜储备计划，南方省份要根据自身情况建立完善蔬菜储备；及时投放肉类、蔬菜等生活必需品储备，补充市场供应。 \u0026#34;\u0026#34;\u0026#34; from cntext.visualization import wordshiftor wordshiftor(text1=text1, text2=text2, title=\u0026#39;两文本对比\u0026#39;) Run\n 6.3 textpic(title=\u0026lsquo;PYTHON测试\u0026rsquo;, subtitle=\u0026lsquo;使用Python生成图片\u0026rsquo;, font=\u0026lsquo;Alibaba-PuHuiTi-Bold.otf\u0026rsquo;, titlesize=1.8, subsize=14)  title: 主标题 subtitle: 副标题 font: 本地中文字体路径 titlesize: 主标题字体大小 subsize: 副标题字体大小  textpic(title=\u0026#39;PYTHON测试\u0026#39;, subtitle=\u0026#39;使用Python生成图片\u0026#39;, font=\u0026#39;data/Alibaba-PuHuiTi-Bold.otf\u0026#39;, titlesize=1.8, subsize=14)   注意\n 设置参数matplotlib_family，需要先运行下面代码获取本机字体列表 from matplotlib.font_manager import FontManager mpl_fonts = set(f.name for f in FontManager().ttflist) print(mpl_fonts)\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/cntext_v_1/","summary":"cntext 中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等\n github地址 https://github.com/hidadeng/cntext pypi地址 https://pypi.org/project/cntext/ 视频课-Python网络爬虫与文本数据分析  功能模块含\n cntext stats 文本统计,可读性等 dictionary 构建词表(典) sentiment 情感分析 similarity 文本相似度 visualization 可视化，如词云图  代码下载 https://github.com/hidadeng/cntext/tree/main/examples\n安装 pip install cntext==0.9 \n一、cntext 查看cntext基本信息\nimport cntext help(cntext) Run\nHelp on package cntext: NAME cntext PACKAGE CONTENTS description (package) dictionary (package) sentiment (package) similarity (package) visualization (package) DATA ADV_words = [\u0026#39;都\u0026#39;, \u0026#39;全\u0026#39;, \u0026#39;单\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;光\u0026#39;, \u0026#39;尽\u0026#39;, \u0026#39;净\u0026#39;, \u0026#39;仅\u0026#39;, \u0026#39;就\u0026#39;, \u0026#39;只\u0026#39;, \u0026#39;一共\u0026#39;, \u0026#39;... CONJ_words = [\u0026#39;乃\u0026#39;, \u0026#39;乍\u0026#39;, \u0026#39;与\u0026#39;, \u0026#39;无\u0026#39;, \u0026#39;且\u0026#39;, \u0026#39;丕\u0026#39;, \u0026#39;为\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;其\u0026#39;, \u0026#39;况\u0026#39;, \u0026#39;厥\u0026#39;, \u0026#39;.","title":"cntext中文文本分析库 | 值得收藏"},{"content":"Huggingface（抱抱脸）总部位于纽约，是一家专注于自然语言处理、人工智能和分布式系统的创业公司。他们所提供的聊天机器人技术一直颇受欢迎，但更出名的是他们在NLP开源社区上的贡献。\nHuggingface一直致力于自然语言处理NLP技术的平民化(democratize)，希望每个人都能用上最先进(SOTA, state-of-the-art)的NLP技术，而非困窘于训练资源的匮乏。\nHugging Face所有模型的地址\nhttps://huggingface.co/models\n你可以在这里下载所需要的模型，也可以上传你微调之后用于特定task的模型。\nHugging Face使用文档的地址\nhttps://huggingface.co/transformers/master/index.html\n\n英汉互译 from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline zh2en_model = AutoModelForSeq2SeqLM.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-zh-en\u0026#39;) zh2en_tokenizer = AutoTokenizer.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-zh-en\u0026#39;) zh2en_translation = pipeline(\u0026#39;translation_zh_to_en\u0026#39;, model=zh2en_model, tokenizer=zh2en_tokenizer) zh2en_translation(\u0026#39;Python是一门非常强大的编程语言!\u0026#39;) [{'translation_text': 'Python is a very powerful programming language!'}]  en2zh_model = AutoModelForSeq2SeqLM.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-en-zh\u0026#39;) en2zh_tokenizer = AutoTokenizer.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-en-zh\u0026#39;) en2zh_translation = pipeline(\u0026#39;translation_en_to_zh\u0026#39;, model=en2zh_model, tokenizer=en2zh_tokenizer) en2zh_translation(\u0026#39;Python is a very powerful programming language!\u0026#39;) [{'translation_text': 'Python是一个非常强大的编程语言!'}]  \n文本分类 模型 uer/roberta-base-finetuned-chinanews-chinese是使用5个中文文本分类数据集训练得到\n 京东full、京东binary和大众点评数据集包含不同情感极性的用户评论数据。 凤凰网 和 China Daily 包含不同主题类的新闻文本数据  from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline model = AutoModelForSequenceClassification.from_pretrained(\u0026#39;uer/roberta-base-finetuned-chinanews-chinese\u0026#39;) tokenizer = AutoTokenizer.from_pretrained(\u0026#39;uer/roberta-base-finetuned-chinanews-chinese\u0026#39;) text_classification = pipeline(\u0026#39;sentiment-analysis\u0026#39;, model=model, tokenizer=tokenizer) test_text = \u0026#34;上证指数大涨2%\u0026#34; text_classification(test_text, return_all_scores=True) [[{'label': 'mainland China politics', 'score': 0.0002807585697155446}, {'label': 'Hong Kong - Macau politics', 'score': 0.00015504546172451228}, {'label': 'International news', 'score': 6.818029214628041e-05}, {'label': 'financial news', 'score': 0.9991051554679871}, {'label': 'culture', 'score': 0.00011297615128569305}, {'label': 'entertainment', 'score': 0.00012184812658233568}, {'label': 'sports', 'score': 0.0001558474759804085}]]  test_text = \u0026#34;Python是一门强大的编程语言\u0026#34; text_classification(test_text, return_all_scores=True) [[{'label': 'mainland China politics', 'score': 0.02050291746854782}, {'label': 'Hong Kong - Macau politics', 'score': 0.0030984438490122557}, {'label': 'International news', 'score': 0.005687597207725048}, {'label': 'financial news', 'score': 0.03360358253121376}, {'label': 'culture', 'score': 0.913349986076355}, {'label': 'entertainment', 'score': 0.010810119099915028}, {'label': 'sports', 'score': 0.012947351671755314}]]  \n代码下载 https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211108HuggingFace学习\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/huggingface_test/","summary":"Huggingface（抱抱脸）总部位于纽约，是一家专注于自然语言处理、人工智能和分布式系统的创业公司。他们所提供的聊天机器人技术一直颇受欢迎，但更出名的是他们在NLP开源社区上的贡献。\nHuggingface一直致力于自然语言处理NLP技术的平民化(democratize)，希望每个人都能用上最先进(SOTA, state-of-the-art)的NLP技术，而非困窘于训练资源的匮乏。\nHugging Face所有模型的地址\nhttps://huggingface.co/models\n你可以在这里下载所需要的模型，也可以上传你微调之后用于特定task的模型。\nHugging Face使用文档的地址\nhttps://huggingface.co/transformers/master/index.html\n\n英汉互译 from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline zh2en_model = AutoModelForSeq2SeqLM.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-zh-en\u0026#39;) zh2en_tokenizer = AutoTokenizer.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-zh-en\u0026#39;) zh2en_translation = pipeline(\u0026#39;translation_zh_to_en\u0026#39;, model=zh2en_model, tokenizer=zh2en_tokenizer) zh2en_translation(\u0026#39;Python是一门非常强大的编程语言!\u0026#39;) [{'translation_text': 'Python is a very powerful programming language!'}]  en2zh_model = AutoModelForSeq2SeqLM.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-en-zh\u0026#39;) en2zh_tokenizer = AutoTokenizer.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-en-zh\u0026#39;) en2zh_translation = pipeline(\u0026#39;translation_en_to_zh\u0026#39;, model=en2zh_model, tokenizer=en2zh_tokenizer) en2zh_translation(\u0026#39;Python is a very powerful programming language!\u0026#39;) [{'translation_text': 'Python是一个非常强大的编程语言!'}]  \n文本分类 模型 uer/roberta-base-finetuned-chinanews-chinese是使用5个中文文本分类数据集训练得到\n 京东full、京东binary和大众点评数据集包含不同情感极性的用户评论数据。 凤凰网 和 China Daily 包含不同主题类的新闻文本数据  from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline model = AutoModelForSequenceClassification.","title":"Hugging Face | 自然语言处理平台"},{"content":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211031使用matplotlib绘制卡通化图表\n数据可视化是讲故事的好方法，从中可以轻松地吸收信息并识别数据中的模式。我们的一位学生决定使用 Matplotlib 在 Python 中创建数据可视化，以了解 Netflix 上可用的不同类型的内容。本文将重点介绍使用 Matplotlib 以一种有趣的方式进行数据可视化。\n在 Netflix 上看完精彩的节目或电影后，您有没有想过 Netflix 为您提供了如此多的惊人内容？当然，我并不孤单，不是吗？一个想法会导致另一个想法，在不知不觉中，您已经下定决心进行探索性数据分析，以了解更多关于最受欢迎的演员是谁以及哪个国家/地区更喜欢哪种类型的信息。\n使用 Python 制作常规的条形图和饼图，虽然它们在传达结果方面做得很好，但我想为这个项目添加一些有趣的元素。\n我最近了解到你可以在 Python 最流行的数据可视化库 Matplotlib 中创建类似 xkcd 的绘图，并决定我应该在这个项目中整理我所有的 Matplotlib 可视化，只是为了让事情变得更有趣。\n一起来看看数据怎么说吧！\n导入数据 7787部电影/TV节目的信息\nimport pandas as pd df = pd.read_csv(\u0026#34;netflix_titles.csv\u0026#34;) df.head()   Netflix历年发展里程碑 描述一下 Netflix 多年来演变的时间表。\nimport matplotlib.pyplot as plt import numpy as np plt.rcParams[\u0026#39;figure.dpi\u0026#39;] = 200 # these go on the numbers below tl_dates = [ \u0026#34;1997\\nFounded\u0026#34;, \u0026#34;1998\\nMail Service\u0026#34;, \u0026#34;2003\\nGoes Public\u0026#34;, \u0026#34;2007\\nStreaming service\u0026#34;, \u0026#34;2016\\nGoes Global\u0026#34;, \u0026#34;2021\\nNetflix \u0026amp; Chill\u0026#34; ] tl_x = [1, 2, 4, 5.3, 8, 9] # the numbers go on these tl_sub_x = [1.5,3,5,6.5,7] tl_sub_times = [ \u0026#34;1998\u0026#34;,\u0026#34;2000\u0026#34;,\u0026#34;2006\u0026#34;,\u0026#34;2010\u0026#34;,\u0026#34;2012\u0026#34; ] tl_text = [ \u0026#34;Netflix.com launched\u0026#34;, \u0026#34;Starts\\nPersonal\\nRecommendations\u0026#34;,\u0026#34;Billionth DVD Delivery\u0026#34;,\u0026#34;Canadian\\nLaunch\u0026#34;,\u0026#34;UK Launch\u0026#34;] with plt.xkcd(): # Set figure \u0026amp; Axes fig, ax = plt.subplots(figsize=(15, 4), constrained_layout=True) ax.set_ylim(-2, 1.75) ax.set_xlim(0, 10) # Timeline : line ax.axhline(0, xmin=0.1, xmax=0.9, c=\u0026#39;deeppink\u0026#39;, zorder=1) # Timeline : Date Points ax.scatter(tl_x, np.zeros(len(tl_x)), s=120, c=\u0026#39;palevioletred\u0026#39;, zorder=2) ax.scatter(tl_x, np.zeros(len(tl_x)), s=30, c=\u0026#39;darkmagenta\u0026#39;, zorder=3) # Timeline : Time Points ax.scatter(tl_sub_x, np.zeros(len(tl_sub_x)), s=50, c=\u0026#39;darkmagenta\u0026#39;,zorder=4) # Date Text for x, date in zip(tl_x, tl_dates): ax.text(x, -0.55, date, ha=\u0026#39;center\u0026#39;, fontfamily=\u0026#39;serif\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, color=\u0026#39;royalblue\u0026#39;,fontsize=12) # Stemplot : vertical line levels = np.zeros(len(tl_sub_x)) levels[::2] = 0.3 levels[1::2] = -0.3 markerline, stemline, baseline = ax.stem(tl_sub_x, levels, use_line_collection=True) plt.setp(baseline, zorder=0) plt.setp(markerline, marker=\u0026#39;,\u0026#39;, color=\u0026#39;darkmagenta\u0026#39;) plt.setp(stemline, color=\u0026#39;darkmagenta\u0026#39;) # Text for idx, x, time, txt in zip(range(1, len(tl_sub_x)+1), tl_sub_x, tl_sub_times, tl_text): ax.text(x, 1.3*(idx%2)-0.5, time, ha=\u0026#39;center\u0026#39;, fontfamily=\u0026#39;serif\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, color=\u0026#39;royalblue\u0026#39;, fontsize=11) ax.text(x, 1.3*(idx%2)-0.6, txt, va=\u0026#39;top\u0026#39;, ha=\u0026#39;center\u0026#39;, fontfamily=\u0026#39;serif\u0026#39;,color=\u0026#39;royalblue\u0026#39;) # Spine for spine in [\u0026#34;left\u0026#34;, \u0026#34;top\u0026#34;, \u0026#34;right\u0026#34;, \u0026#34;bottom\u0026#34;]: ax.spines[spine].set_visible(False) # Ticks ax.set_xticks([]) ax.set_yticks([]) # Title ax.set_title(\u0026#34;Netflix through the years\u0026#34;, fontweight=\u0026#34;bold\u0026#34;, fontfamily=\u0026#39;serif\u0026#39;, fontsize=16, color=\u0026#39;royalblue\u0026#39;) ax.text(2.4,1.57,\u0026#34;From DVD rentals to a global audience of over 150m people - is it time for Netflix to Chill?\u0026#34;, fontfamily=\u0026#39;serif\u0026#39;, fontsize=12, color=\u0026#39;mediumblue\u0026#39;) plt.show()   电影 vs 电视综艺 接下来，我决定看一下电影与电视节目的比例。\ncol = \u0026#34;type\u0026#34; grouped = df[col].value_counts().reset_index() grouped = grouped.rename(columns = {col : \u0026#34;count\u0026#34;, \u0026#34;index\u0026#34; : col}) with plt.xkcd(): explode = (0, 0.1) # only \u0026#34;explode\u0026#34; the 2nd slice (i.e. \u0026#39;TV Show\u0026#39;) fig1, ax1 = plt.subplots(figsize=(5, 5), dpi=100) ax1.pie(grouped[\u0026#34;count\u0026#34;], explode=explode, labels=grouped[\u0026#34;type\u0026#34;], autopct=\u0026#39;%1.1f%%\u0026#39;, shadow=True, startangle=90) ax1.axis(\u0026#39;equal\u0026#39;) # Equal aspect ratio ensures that pie is drawn as a circle. plt.show()   内容最多的国家 from collections import Counter col = \u0026#34;country\u0026#34; categories = \u0026#34;, \u0026#34;.join(df[col].fillna(\u0026#34;\u0026#34;)).split(\u0026#34;, \u0026#34;) counter_list = Counter(categories).most_common(25) counter_list = [_ for _ in counter_list if _[0] != \u0026#34;\u0026#34;] labels = [_[0] for _ in counter_list] values = [_[1] for _ in counter_list] with plt.xkcd(): fig, ax = plt.subplots(figsize=(10, 10), dpi=100) y_pos = np.arange(len(labels)) ax.barh(y_pos, values, align=\u0026#39;center\u0026#39;) ax.set_yticks(y_pos) ax.set_yticklabels(labels) ax.invert_yaxis() # labels read top-to-bottom ax.set_xlabel(\u0026#39;Content\u0026#39;) ax.set_title(\u0026#39;Countries with most content\u0026#39;) plt.show()   最流行的导演 from collections import Counter from matplotlib.pyplot import figure import math colours = [\u0026#34;orangered\u0026#34;, \u0026#34;mediumseagreen\u0026#34;, \u0026#34;darkturquoise\u0026#34;, \u0026#34;mediumpurple\u0026#34;, \u0026#34;deeppink\u0026#34;, \u0026#34;indianred\u0026#34;] countries_list = [\u0026#34;United States\u0026#34;, \u0026#34;India\u0026#34;, \u0026#34;United Kingdom\u0026#34;, \u0026#34;Japan\u0026#34;, \u0026#34;France\u0026#34;, \u0026#34;Canada\u0026#34;] col = \u0026#34;director\u0026#34; with plt.xkcd(): figure(num=None, figsize=(20, 8)) x=1 for country in countries_list: country_df = df[df[\u0026#34;country\u0026#34;]==country] categories = \u0026#34;, \u0026#34;.join(country_df[col].fillna(\u0026#34;\u0026#34;)).split(\u0026#34;, \u0026#34;) counter_list = Counter(categories).most_common(6) counter_list = [_ for _ in counter_list if _[0] != \u0026#34;\u0026#34;] labels = [_[0] for _ in counter_list][::-1] values = [_[1] for _ in counter_list][::-1] if max(values)\u0026lt;10: values_int = range(0, math.ceil(max(values))+1) else: values_int = range(0, math.ceil(max(values))+1, 2) plt.subplot(2, 3, x) plt.barh(labels,values, color = colours[x-1]) plt.xticks(values_int) plt.title(country) x+=1 plt.suptitle(\u0026#39;Popular Directors with the most content\u0026#39;) plt.tight_layout() plt.show()   Netflix 专注于什么样的内容？ 我还想浏览评级栏并比较 Netflix 为儿童、青少年和成人制作的内容量——以及这些年来他们的重点是否从一个群体转移到另一个群体。\n为此，我首先查看了 DataFrame 中的独特评级：\ndf[\u0026#34;date_added\u0026#34;] = pd.to_datetime(df[\u0026#39;date_added\u0026#39;]) df[\u0026#39;year_added\u0026#39;] = df[\u0026#39;date_added\u0026#39;].dt.year.astype(\u0026#39;Int64\u0026#39;) ratings_list = [\u0026#39;TV-MA\u0026#39;, \u0026#39;R\u0026#39;, \u0026#39;PG-13\u0026#39;, \u0026#39;TV-14\u0026#39;, \u0026#39;TV-PG\u0026#39;, \u0026#39;TV-G\u0026#39;, \u0026#39;TV-Y\u0026#39;, \u0026#39;TV-Y7\u0026#39;, \u0026#39;PG\u0026#39;, \u0026#39;G\u0026#39;, \u0026#39;NC-17\u0026#39;, \u0026#39;TV-Y7-FV\u0026#39;] ratings_group_list = [\u0026#39;Little Kids\u0026#39;, \u0026#39;Older Kids\u0026#39;, \u0026#39;Teens\u0026#39;, \u0026#39;Mature\u0026#39;] ratings_dict={ \u0026#39;TV-G\u0026#39;: \u0026#39;Little Kids\u0026#39;, \u0026#39;TV-Y\u0026#39;: \u0026#39;Little Kids\u0026#39;, \u0026#39;G\u0026#39;: \u0026#39;Little Kids\u0026#39;, \u0026#39;TV-PG\u0026#39;: \u0026#39;Older Kids\u0026#39;, \u0026#39;TV-Y7\u0026#39;: \u0026#39;Older Kids\u0026#39;, \u0026#39;PG\u0026#39;: \u0026#39;Older Kids\u0026#39;, \u0026#39;TV-Y7-FV\u0026#39;: \u0026#39;Older Kids\u0026#39;, \u0026#39;PG-13\u0026#39;: \u0026#39;Teens\u0026#39;, \u0026#39;TV-14\u0026#39;: \u0026#39;Teens\u0026#39;, \u0026#39;TV-MA\u0026#39;: \u0026#39;Mature\u0026#39;, \u0026#39;R\u0026#39;: \u0026#39;Mature\u0026#39;, \u0026#39;NC-17\u0026#39;: \u0026#39;Mature\u0026#39; } for rating_val, rating_group in ratings_dict.items(): df.loc[df.rating == rating_val, \u0026#34;rating\u0026#34;] = rating_group df[\u0026#39;rating_val\u0026#39;]=1 x=0 labels=[\u0026#39;kinda\\nless\u0026#39;, \u0026#39;not so\\nbad\u0026#39;, \u0026#39;holyshit\\nthat\\\u0026#39;s too\\nmany\u0026#39;] with plt.xkcd(): for r in ratings_group_list: grouped = df[df[\u0026#39;rating\u0026#39;]==r] year_df = grouped.groupby([\u0026#39;year_added\u0026#39;]).sum() year_df.reset_index(level=0, inplace=True) plt.plot(year_df[\u0026#39;year_added\u0026#39;], year_df[\u0026#39;rating_val\u0026#39;], color=colours[x], marker=\u0026#39;o\u0026#39;) values_int = range(2008, math.ceil(max(year_df[\u0026#39;year_added\u0026#39;]))+1, 2) plt.yticks([200, 600, 1000], labels) plt.xticks(values_int) plt.title(\u0026#39;Count of shows and movies that Netflix\\nhas been producing for different audiences\u0026#39;, fontsize=12) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize=14) plt.ylabel(\u0026#39;Content Count\u0026#39;, fontsize=14) x+=1 plt.legend(ratings_group_list) plt.tight_layout() plt.show()   广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/cute_matplotlib/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211031使用matplotlib绘制卡通化图表\n数据可视化是讲故事的好方法，从中可以轻松地吸收信息并识别数据中的模式。我们的一位学生决定使用 Matplotlib 在 Python 中创建数据可视化，以了解 Netflix 上可用的不同类型的内容。本文将重点介绍使用 Matplotlib 以一种有趣的方式进行数据可视化。\n在 Netflix 上看完精彩的节目或电影后，您有没有想过 Netflix 为您提供了如此多的惊人内容？当然，我并不孤单，不是吗？一个想法会导致另一个想法，在不知不觉中，您已经下定决心进行探索性数据分析，以了解更多关于最受欢迎的演员是谁以及哪个国家/地区更喜欢哪种类型的信息。\n使用 Python 制作常规的条形图和饼图，虽然它们在传达结果方面做得很好，但我想为这个项目添加一些有趣的元素。\n我最近了解到你可以在 Python 最流行的数据可视化库 Matplotlib 中创建类似 xkcd 的绘图，并决定我应该在这个项目中整理我所有的 Matplotlib 可视化，只是为了让事情变得更有趣。\n一起来看看数据怎么说吧！\n导入数据 7787部电影/TV节目的信息\nimport pandas as pd df = pd.read_csv(\u0026#34;netflix_titles.csv\u0026#34;) df.head()   Netflix历年发展里程碑 描述一下 Netflix 多年来演变的时间表。\nimport matplotlib.pyplot as plt import numpy as np plt.rcParams[\u0026#39;figure.dpi\u0026#39;] = 200 # these go on the numbers below tl_dates = [ \u0026#34;1997\\nFounded\u0026#34;, \u0026#34;1998\\nMail Service\u0026#34;, \u0026#34;2003\\nGoes Public\u0026#34;, \u0026#34;2007\\nStreaming service\u0026#34;, \u0026#34;2016\\nGoes Global\u0026#34;, \u0026#34;2021\\nNetflix \u0026amp; Chill\u0026#34; ] tl_x = [1, 2, 4, 5.","title":"使用matplotlib绘制超可爱超萌化的图表"},{"content":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211031如何在DataFrame中使用If-Else条件语句创建新列\n有时我们希望根据 DataFrame 其他列(字段) 的值向 DataFrame 添加一列。\n虽然这听起来很简单，但如果我们尝试使用 if-else 条件来完成它可能会变得有点复杂。 值得庆幸的是，使用 numpy 两个函数np.where()、np.select() 就能实现这一需求。\n导入数据 我们有一个包含 4,000 多条 Dataquest 推文的数据集,字段包括：\n date twitter发送的日期 time 推文发送时间 tweet 推文内容 mentions 谁提到了该推文 photos 图片链接 replies_count 推文回复数 retweets_count 推文再转发数 likes_count 推文获得的点赞数  import pandas as pd import numpy as np df = pd.read_csv(\u0026#39;tweets.csv\u0026#39;) df.head()   \n问题 我们看到数据集中的photos字段是图片链接\n 如果某条记录有信息，会显示图片链接列表 如果不含图片，该字段对应的数据是空列表  我们只想查看带有图片的推文是否获得更多交互，因此我们实际上并不需要图片 URL。 让我们尝试创建一个名为 has_image 的新列，该列将包含布尔值\n 如果推文包含图像，则为 True，否则为 False。  那么我们如何创建一个has_image字段？\n\nnp.where() np.where(condition, value if condition is true, value if condition is false)\n为此，我们将使用 numpy 的内置 where() 函数。 这个函数依次接受三个参数：我们要测试的条件，如果条件为真则分配给新列的值，如果条件为假则分配给新列的值。 它看起来像这样：\ndf[\u0026#39;has_image\u0026#39;] = np.where(df[\u0026#39;photos\u0026#39;]!=\u0026#39;[]\u0026#39;, True, False) df.head()   np.select() 这种方法很好用，但如果我们新建列的值不止True、False呢？\n例如我们把likes_count 进行分类，不同取值范围定义为不同的类别\n tier_4 少于2个赞 tier_3 3-9 个赞 tier_2 10-15 个赞 tier_1 16+ 个赞  为此，我们可以使用名为 np.select() 的函数。我们将给它两个参数：一个我们的条件列表，以及一个我们想要分配给新列中每一行的值的相关列表。\n这意味着顺序很重要：如果满足条件列表中的第一个条件，则值列表中的第一个值将分配给该行的新列。如果满足第二个条件，则将分配第二个值，依此类推。\n让我们来看看它在 Python 代码中的表现：\n# create a list of our conditions conditions = [ (df[\u0026#39;likes_count\u0026#39;] \u0026lt;= 2), (df[\u0026#39;likes_count\u0026#39;] \u0026gt; 2) \u0026amp; (df[\u0026#39;likes_count\u0026#39;] \u0026lt;= 9), (df[\u0026#39;likes_count\u0026#39;] \u0026gt; 9) \u0026amp; (df[\u0026#39;likes_count\u0026#39;] \u0026lt;= 15), (df[\u0026#39;likes_count\u0026#39;] \u0026gt; 15) ] # create a list of the values we want to assign for each condition values = [\u0026#39;tier_4\u0026#39;, \u0026#39;tier_3\u0026#39;, \u0026#39;tier_2\u0026#39;, \u0026#39;tier_1\u0026#39;] # create a new column and use np.select to assign values to it using our lists as arguments df[\u0026#39;tier\u0026#39;] = np.select(conditions, values) # display updated DataFrame df.head()   广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/how_to_use_ifelse_in_pandas/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211031如何在DataFrame中使用If-Else条件语句创建新列\n有时我们希望根据 DataFrame 其他列(字段) 的值向 DataFrame 添加一列。\n虽然这听起来很简单，但如果我们尝试使用 if-else 条件来完成它可能会变得有点复杂。 值得庆幸的是，使用 numpy 两个函数np.where()、np.select() 就能实现这一需求。\n导入数据 我们有一个包含 4,000 多条 Dataquest 推文的数据集,字段包括：\n date twitter发送的日期 time 推文发送时间 tweet 推文内容 mentions 谁提到了该推文 photos 图片链接 replies_count 推文回复数 retweets_count 推文再转发数 likes_count 推文获得的点赞数  import pandas as pd import numpy as np df = pd.read_csv(\u0026#39;tweets.csv\u0026#39;) df.head()   \n问题 我们看到数据集中的photos字段是图片链接\n 如果某条记录有信息，会显示图片链接列表 如果不含图片，该字段对应的数据是空列表  我们只想查看带有图片的推文是否获得更多交互，因此我们实际上并不需要图片 URL。 让我们尝试创建一个名为 has_image 的新列，该列将包含布尔值\n 如果推文包含图像，则为 True，否则为 False。  那么我们如何创建一个has_image字段？","title":"如何在DataFrame中使用If-Else条件语句创建新列"},{"content":"Pipe map和filter是处理iterable数据最好用的函数，但却让代码看起来很乱，使代码可读性大大降低。\narr = [1, 2, 3, 4, 5] #对arr筛选偶数，并对偶数乘以2 list(map(lambda x: x*2, filter(lambda x:x%2==0, arr))) [4, 8]  刚刚的iterable的例子，其实可以使用pipe库中的 | 来应用多种方法。\nfrom pipe import select, where arr = [1, 2, 3, 4, 5] list(arr |where(lambda x:x%2==0) |select(lambda x:x*2)) [4, 8]  pipe是什么？ pipe是python中的管道操作库，可以使数据分析多个步骤(函数）像管道(流水线)一样上下衔接，共同完成一个数据分析任务。\n我喜欢pipe是因为它让iterable代码变得干净整洁，可读性大大增强。后面我会通过几个案例让大家快速掌握pipe库。首先先安装pipe\n!pip3 install pipe \n点击下载本文代码\nwhere 对iterable中的数据进行筛选操作\nfrom pipe import where arr = [1, 2, 3, 4, 5] #把偶数筛选出来 list(arr | where(lambda x: x%2==0)) [2, 4]  select 对iterable中的数据进行某种操作\nfrom pipe import select arr = [1, 2, 3, 4, 5] #对arr中的每个数 乘以2 list(arr | select(lambda x: x*2)) [2, 4, 6, 8, 10]  现在你可能会有疑问： 为何在Python已拥有map和filter情况下， 还用pipe库中的 select和 where呢？\n因为可以使用管道在一个方法后面加入另一个方法， 加不止1次!!\nfrom pipe import select, where arr = [1, 2, 3, 4, 5] list(arr | where(lambda x: x%2==0) #筛选arr中的偶数 | select(lambda x: x*2) #对偶数乘以2 ) [4, 8]  非折叠iterable chain 对于嵌套结构的iterable数据，最难任务之一就是将其展平。\nfrom pipe import chain nested = [[1,2,[3]], [4, 5]] list((nested | chain)) [1, 2, [3], 4, 5]  即时经过上述操作， 依然不是完全展开。 为了处理深度嵌套数据， 可以使用traverse方法。\ntraverse 遍历traverse方法可以用递归的方式展开 嵌套对象。\nfrom pipe import traverse nested = [[1,2,[3]], [4, 5]] list((nested | traverse)) [1, 2, 3, 4, 5]  现在我们从抽取字典values中的列表，并将其展平\nfrom pipe import traverse, select fruits = [ {\u0026#34;name\u0026#34;: \u0026#34;apple\u0026#34;, \u0026#34;price\u0026#34;: [2, 5]}, {\u0026#34;name\u0026#34;: \u0026#34;orange\u0026#34;, \u0026#34;price\u0026#34;: 4}, {\u0026#34;name\u0026#34;: \u0026#34;grape\u0026#34;, \u0026#34;price\u0026#34;: 5} ] list(fruits | select(lambda fruit: fruit[\u0026#34;price\u0026#34;]) | traverse) [2, 5, 4, 5]  groupby 有时候，需要对列表中的数据进行分组，这可能用到groupby方法。\nfrom pipe import select, groupby list( (1, 2, 3, 4, 5, 6, 7, 8, 9) | groupby(lambda x: \u0026#34;偶数\u0026#34; if x%2==0 else \u0026#34;奇数\u0026#34;) | select(lambda x: {x[0]: list(x[1])}) ) [{'偶数': [2, 4, 6, 8]}, {'奇数': [1, 3, 5, 7, 9]}]  在上面的代码中， 我们使用groupby将数字分为奇数组和偶数组。groupby方法输出的结果如下\n[(\u0026#39;偶数\u0026#39;, \u0026lt;itertools._grouper at 0x10bd54550\u0026gt;), (\u0026#39;奇数\u0026#39;, \u0026lt;itertools._grouper at 0x10bd4d350\u0026gt;)] 接下来，使用select将元素为元组的列表转化为字典，其中\n 元组中第1位置做字典的关键词 元组中第2位置做字典的值  [{\u0026#39;偶数\u0026#39;: [2, 4, 6, 8]}, {\u0026#39;奇数\u0026#39;: [1, 3, 5, 7, 9]}] Cool！为了range值大于2， 我们在select内增加where条件操作\nfrom pipe import select, groupby list( (1, 2, 3, 4, 5, 6, 7, 8, 9) | groupby(lambda x: \u0026#34;偶数\u0026#34; if x%2==0 else \u0026#34;奇数\u0026#34;) | select(lambda x: {x[0]: list(x[1] | where(lambda x: x\u0026gt;2) ) } ) ) [{'偶数': [4, 6, 8]}, {'奇数': [3, 5, 7, 9]}]  dedup 使用Key对list数据进行去重\nfrom pipe import dedup arr = [1, 2, 2, 3, 4, 5, 6, 6, 7, 9, 3, 3, 1] list(arr | dedup) [1, 2, 3, 4, 5, 6, 7, 9]  这看起来没啥新意，毕竟python内置的set函数即可实现刚刚的需求。然而，dedup通过key获得列表中的唯一元素。\n例如，获得小于5的唯一元素， 且另一个元素大于或等于5\nfrom pipe import dedup arr = [1, 2, 2, 3, 4, 5, 6, 6, 7, 9, 3, 3, 1] list(arr | dedup(lambda key: key\u0026lt;5)) [1, 5]  from pipe import traverse, select data = [ {\u0026#34;name\u0026#34;: \u0026#34;apple\u0026#34;, \u0026#34;count\u0026#34;: 2}, {\u0026#34;name\u0026#34;: \u0026#34;orange\u0026#34;, \u0026#34;count\u0026#34;: 4}, {\u0026#34;name\u0026#34;: \u0026#34;grape\u0026#34;, \u0026#34;count\u0026#34;: None}, {\u0026#34;name\u0026#34;: \u0026#34;orange\u0026#34;, \u0026#34;count\u0026#34;: 7} ] list( data | dedup(key=lambda fruit: fruit[\u0026#34;name\u0026#34;]) | select(lambda fruit: fruit[\u0026#34;count\u0026#34;]) | where(lambda count: isinstance(count, int)) ) [2, 4]  了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/pipe_make_python_code_more_clean/","summary":"Pipe map和filter是处理iterable数据最好用的函数，但却让代码看起来很乱，使代码可读性大大降低。\narr = [1, 2, 3, 4, 5] #对arr筛选偶数，并对偶数乘以2 list(map(lambda x: x*2, filter(lambda x:x%2==0, arr))) [4, 8]  刚刚的iterable的例子，其实可以使用pipe库中的 | 来应用多种方法。\nfrom pipe import select, where arr = [1, 2, 3, 4, 5] list(arr |where(lambda x:x%2==0) |select(lambda x:x*2)) [4, 8]  pipe是什么？ pipe是python中的管道操作库，可以使数据分析多个步骤(函数）像管道(流水线)一样上下衔接，共同完成一个数据分析任务。\n我喜欢pipe是因为它让iterable代码变得干净整洁，可读性大大增强。后面我会通过几个案例让大家快速掌握pipe库。首先先安装pipe\n!pip3 install pipe \n点击下载本文代码\nwhere 对iterable中的数据进行筛选操作\nfrom pipe import where arr = [1, 2, 3, 4, 5] #把偶数筛选出来 list(arr | where(lambda x: x%2==0)) [2, 4]  select 对iterable中的数据进行某种操作","title":"让Python代码更简洁的pipe包"},{"content":"如果大家之前了解selenium库，那么antoma不用过多介绍，您就能知道ta是做浏览器自动化的。automa通过点击连接卡片实现浏览器的自动化运行。\n没有做不到，只有想不到。从自动填写表单、执行重复性任务、截取屏幕截图到抓取网站数据，您想使用此扩展程序做什么取决于您。您甚至可以安排自动化执行的时间。下面我们看一下开发者制作的操作视频\n 从视频中，大家可以看到，工作流可执行表单填写、屏幕截图、网站数据抓取等各种重复性工作。如果大家感兴趣，可以试着用一下automa。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/automa_rpa/","summary":"如果大家之前了解selenium库，那么antoma不用过多介绍，您就能知道ta是做浏览器自动化的。automa通过点击连接卡片实现浏览器的自动化运行。\n没有做不到，只有想不到。从自动填写表单、执行重复性任务、截取屏幕截图到抓取网站数据，您想使用此扩展程序做什么取决于您。您甚至可以安排自动化执行的时间。下面我们看一下开发者制作的操作视频\n 从视频中，大家可以看到，工作流可执行表单填写、屏幕截图、网站数据抓取等各种重复性工作。如果大家感兴趣，可以试着用一下automa。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"automa插件|无编程基础也可自动化办公"},{"content":"尽管已经有很多方法可用于关键字生成（例如，Rake、YAKE!、TF-IDF 等），但我想创建一个非常基本但功能强大的方法来提取关键字和关键短语。这就是 KeyBERT 的用武之地！它使用 BERT 嵌入 和 简单余弦相似度 来查找文档中与文档本身最相似的短语。\nKeyBERT步骤\n 首先使用 BERT 提取文档嵌入以获得文档级向量表示。 随后，为 N-gram 词/短语提取词向量。 然后，我们使用余弦相似度来找到与文档最相似的单词/短语。 最后可以将最相似的词识别为最能描述整个文档的词。  代码下载 click to download the code\n安装 !pip3 install keybert==0.5.0 \n初始化模型 KeyBERT库需要安装配置spacy语言模型\n具体参考公众号：大邓和他的Python 2021-10-29 的推文 查看spacy配置方法\n初始化模型\nfrom keybert import KeyBERT import spacy import jieba zh_model = spacy.load(\u0026#34;zh_core_web_sm\u0026#34;) bertModel = KeyBERT(model=zh_model) \n准备数据 中文测试数据需要先分词，而后构造成类英文的语言结构(用空格间隔的文本)\n# 测试数据 doc = \u0026#34;\u0026#34;\u0026#34;时值10月25日抗美援朝纪念日，《长津湖》片方发布了“纪念中国人民志愿军抗美援朝出国作战71周年特别短片”，再次向伟大的志愿军致敬！ 电影《长津湖》全情全景地还原了71年前抗美援朝战场上那场史诗战役，志愿军奋不顾身的英勇精神令观众感叹：“岁月峥嵘英雄不灭，丹心铁骨军魂永存！”影片上映以来票房屡创新高，目前突破53亿元，暂列中国影史票房总榜第三名。 值得一提的是，这部影片的很多主创或有军人的血脉，或有当兵的经历，或者家人是军人。提起这些他们也充满自豪，影片总监制黄建新称：“当兵以后会有一种特别能坚持的劲儿。”饰演雷公的胡军透露：“我父亲曾经参加过抗美援朝，还得了一个三等功。”影片历史顾问王树增表示：“我当了五十多年的兵，我的老部队就是上甘岭上下来的，那些老兵都是我的偶像。” “身先士卒卫华夏家国，血战无畏护山河无恙。”片中饰演七连连长伍千里的吴京感叹：“要永远记住这些先烈们，他们给我们带来今天的和平。感谢他们的付出，才让我们有今天的幸福生活。”饰演新兵伍万里的易烊千玺表示：“战争的残酷、碾压式的伤害，其实我们现在的年轻人几乎很难能体会到，希望大家看完电影后能明白，是那些先辈们的牺牲奉献，换来了我们的现在。” 影片对战争群像的恢弘呈现，对个体命运的深切关怀，令许多观众无法控制自己的眼泪，观众称：“当看到影片中的惊险战斗场面，看到英雄们壮怀激烈的拼杀，为国捐躯的英勇无畏和无悔付出，我明白了为什么说今天的幸福生活来之不易。”（记者 王金跃） \u0026#34;\u0026#34;\u0026#34; doc = \u0026#39; \u0026#39;.join(jieba.lcut(doc)) # 关键词提取 keywords = bertModel.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10) keywords [('铁骨', 0.5028), ('纪念日', 0.495), ('丹心', 0.4894), ('战役', 0.4869), ('影史', 0.473), ('父亲', 0.4576), ('票房', 0.4571), ('偶像', 0.4497), ('精神', 0.4436), ('家国', 0.4373)]  常用参数 bertModel.extract_keywords(docs, keyphrase_ngram_range, stop_words, top_n)\n docs 文档字符串（空格间隔词语的字符串） keyphrase_ngram_range 设置ngram，默认(1, 1) stop_words 停用词列表 top_n 显示前n个关键词，默认5 highlight 可视化标亮关键词，默认False use_maxsum: 默认False;是否使用Max Sum Similarity作为关键词提取标准， use_mmr: 默认False;是否使用Maximal Marginal Relevance (MMR) 作为关键词提取标准 diversity 如果use_mmr=True，可以设置该参数。参数取值范围从0到1  对于keyphrase_ngram_range参数，\n (1, 1) 只单个词， 如\u0026quot;抗美援朝\u0026quot;, \u0026ldquo;纪念日\u0026quot;是孤立的两个词 (2, 2) 考虑词组， 如出现有意义的词组 \u0026ldquo;抗美援朝 纪念日\u0026rdquo; (1, 2) 同时考虑以上两者情况  # 关键词提取 keywords = bertModel.extract_keywords(doc, keyphrase_ngram_range=(2, 2), stop_words=None, diversity=0.7, top_n=10) keywords [('影片 总监制', 0.5412), ('丹心 铁骨', 0.5339), ('抗美援朝 纪念日', 0.5295), ('长津湖 片方', 0.5252), ('志愿军 致敬', 0.5207), ('老兵 偶像', 0.5192), ('票房 创新', 0.5108), ('军人 血脉', 0.5084), ('家国 血战', 0.4946), ('家人 军人', 0.4885)]  #可视化 keywords = bertModel.extract_keywords(doc, keyphrase_ngram_range=(2, 2), stop_words=None, highlight=True, top_n=10) # 关键词提取 keywords = bertModel.extract_keywords(doc, keyphrase_ngram_range=(2, 2), stop_words=None, use_mmr=True, diversity=0.05, top_n=10) keywords [('影片 总监制', 0.5412), ('长津湖 片方', 0.5252), ('抗美援朝 纪念日', 0.5295), ('丹心 铁骨', 0.5339), ('志愿军 致敬', 0.5207), ('老兵 偶像', 0.5192), ('票房 创新', 0.5108), ('军人 血脉', 0.5084), ('家国 血战', 0.4946), ('家人 军人', 0.4885)]  英文KeyBERT 同样需要配置spacy，参考公众号：大邓和他的Python 2021-10-29 的推文 查看spacy配置方法\nfrom keybert import KeyBERT import spacy en_model = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) doc = \u0026#34;\u0026#34;\u0026#34; Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \u0026#39;reasonable\u0026#39; way (see inductive bias). \u0026#34;\u0026#34;\u0026#34; kw_model = KeyBERT() keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2)) keywords Run\n[('supervised learning', 0.6779), ('supervised', 0.6676), ('signal supervised', 0.6152), ('examples supervised', 0.6112), ('labeled training', 0.6013)]  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/keybert_tutorial/","summary":"尽管已经有很多方法可用于关键字生成（例如，Rake、YAKE!、TF-IDF 等），但我想创建一个非常基本但功能强大的方法来提取关键字和关键短语。这就是 KeyBERT 的用武之地！它使用 BERT 嵌入 和 简单余弦相似度 来查找文档中与文档本身最相似的短语。\nKeyBERT步骤\n 首先使用 BERT 提取文档嵌入以获得文档级向量表示。 随后，为 N-gram 词/短语提取词向量。 然后，我们使用余弦相似度来找到与文档最相似的单词/短语。 最后可以将最相似的词识别为最能描述整个文档的词。  代码下载 click to download the code\n安装 !pip3 install keybert==0.5.0 \n初始化模型 KeyBERT库需要安装配置spacy语言模型\n具体参考公众号：大邓和他的Python 2021-10-29 的推文 查看spacy配置方法\n初始化模型\nfrom keybert import KeyBERT import spacy import jieba zh_model = spacy.load(\u0026#34;zh_core_web_sm\u0026#34;) bertModel = KeyBERT(model=zh_model) \n准备数据 中文测试数据需要先分词，而后构造成类英文的语言结构(用空格间隔的文本)\n# 测试数据 doc = \u0026#34;\u0026#34;\u0026#34;时值10月25日抗美援朝纪念日，《长津湖》片方发布了“纪念中国人民志愿军抗美援朝出国作战71周年特别短片”，再次向伟大的志愿军致敬！ 电影《长津湖》全情全景地还原了71年前抗美援朝战场上那场史诗战役，志愿军奋不顾身的英勇精神令观众感叹：“岁月峥嵘英雄不灭，丹心铁骨军魂永存！”影片上映以来票房屡创新高，目前突破53亿元，暂列中国影史票房总榜第三名。 值得一提的是，这部影片的很多主创或有军人的血脉，或有当兵的经历，或者家人是军人。提起这些他们也充满自豪，影片总监制黄建新称：“当兵以后会有一种特别能坚持的劲儿。”饰演雷公的胡军透露：“我父亲曾经参加过抗美援朝，还得了一个三等功。”影片历史顾问王树增表示：“我当了五十多年的兵，我的老部队就是上甘岭上下来的，那些老兵都是我的偶像。” “身先士卒卫华夏家国，血战无畏护山河无恙。”片中饰演七连连长伍千里的吴京感叹：“要永远记住这些先烈们，他们给我们带来今天的和平。感谢他们的付出，才让我们有今天的幸福生活。”饰演新兵伍万里的易烊千玺表示：“战争的残酷、碾压式的伤害，其实我们现在的年轻人几乎很难能体会到，希望大家看完电影后能明白，是那些先辈们的牺牲奉献，换来了我们的现在。” 影片对战争群像的恢弘呈现，对个体命运的深切关怀，令许多观众无法控制自己的眼泪，观众称：“当看到影片中的惊险战斗场面，看到英雄们壮怀激烈的拼杀，为国捐躯的英勇无畏和无悔付出，我明白了为什么说今天的幸福生活来之不易。”（记者 王金跃） \u0026#34;\u0026#34;\u0026#34; doc = \u0026#39; \u0026#39;.join(jieba.lcut(doc)) # 关键词提取 keywords = bertModel.","title":"KeyBERT | 关键词发现"},{"content":"BERT是自然语言处理领域最新的词向量技术，而BERTopic 是基于BERT词向量进行主题建模技术，它利用 Transformer 和 c-TF-IDF 来创建密集的集群，允许轻松解释主题，同时在主题描述中保留重要词。\nBERTopic亮点\n 支持引导式Guided 支持（半）监督式 支持动态主题。 支持可视化  安装 !pip3 install bertopic==0.10.0 !pip3 install cntext==1.6.5 \n准备数据 这里使用的新闻数据集， 共2000条。 新闻类别涵 '娱乐', '教育', '游戏', '财经', '时政', '时尚', '科技', '体育', '家居', '房产' 这里假设大家不知道有10类新闻题材， 构建模型的时候不会用到label字段的数据。\nimport pandas as pd df = pd.read_csv(\u0026#39;cnews.csv\u0026#39;) df.head() # 新闻题材 print(df.label.unique()) #记录数 print(len(df)) Run\n['娱乐' '教育' '游戏' '财经' '时政' '时尚' '科技' '体育' '家居' '房产'] 2000  # 各类题材的新闻记录数 df.label.value_counts() Run\n时政 120 科技 106 时尚 106 财经 105 家居 103 教育 97 娱乐 96 体育 95 房产 87 游戏 85 \n这里定义了一个清洗数据函数clean_text，需要注意BERTopic需要先将中文分词改造成类似英文文本格式（用空格间隔词语）\nimport re import jieba import cntext as ct stopwords = ct.load_pkl_dict(\u0026#39;STOPWORDS.pkl\u0026#39;)[\u0026#39;STOPWORDS\u0026#39;][\u0026#39;chinese\u0026#39;] def clean_text(text): words = jieba.lcut(text) words = [w for w in words if w not in stopwords] return \u0026#39; \u0026#39;.join(words) test = \u0026#34;云南永善县级地震已致人伤间民房受损中新网月日电据云南昭通市防震减灾局官方网站消息截至日时云南昭通永善县级地震已造成人受伤其中重伤人轻伤人已全部送医院救治民房受损户间倒塌户间个乡镇所学校不同程度受损目前被损毁电力交通通讯设施已全部抢通修复当地已调拨帐篷顶紧急转移万人月日时分云南昭通永善县发生里氏级地震震源深度公里当地震感强烈此外成都等四川多地也有明显震感\u0026#34; clean_text(test) Run\n\u0026#39;云南 永善县 级 地震 已致 伤间 民房 受损 中新网 日电 云南 昭通市 防震 减灾 局 官方网站 消息 日时 云南 昭通 永善县 级 地震 造成 受伤 重伤 轻伤 送 医院 救治 民房 受损 户间 倒塌 户间 乡镇 学校 不同 程度 受损 目前 损毁 电力 交通 通讯 设施 抢通 修复 调拨 帐篷 顶 紧急 转移 万人 时分 云南 昭通 永善县 发生 里氏 级 地震 震源 深度 公里 震感 强烈 成都 四川 多地 明显 震感\u0026#39; 对2000条数据进行clean_text，得到的结果存储到content字段中。\n我的macbook内存16G, 运行时间10s\ndf[\u0026#39;content\u0026#39;] = df[\u0026#39;text\u0026#39;].apply(clean_text) df.head() \n训练Topic模型 文本分析步骤包括构建特征工程和训练，在本文中，直接使用开源的预训练中文词向量，省去了特征模型的学习时间。\n选取的与训练模型均为word2vec格式，这样方便我们使用gensim将其导入。\n   模型名 数据 预训练模型资源地址     sgns.zhihu.words.bz2 知乎 链接: https://pan.baidu.com/s/1BDxP28KL_23Odj9NWZGe-Q 提取码: n1qq   sgns.wiki.words.bz2 中文维基百科 链接: https://pan.baidu.com/s/1B1sxHmPeIPJYiCuP1zrmMw 提取码: hofj   sgns.financial.words.bz2 金融 链接: https://pan.baidu.com/s/1L_hmGjZMY2ExBn9Vfc_eRg 提取码: hhn6   sgns.renmin.words.bz2 人民日报 链接: https://pan.baidu.com/s/1VQIDrwZH3Y3Lpy4-smPutw 提取码: 3b53   sgns.sougou.words.bz2 搜狗新闻 链接: https://pan.baidu.com/s/15nCaeB41mwK0ZVLrukXpFQ 提取码: 04en     Note:\n除了表格外的资源，还可以使用spacy现有的预训练模型。\n 本文案例cnews.csv是新闻类数据，这里最好选择使用同样为新闻题材的文本训练出的模型，这样BERTopic效果会更精准一些。sgns.sougou.words.bz2是使用搜狗新闻数据训练的语言模型。\nfrom gensim.models import KeyedVectors chinese_sougou_news_models = KeyedVectors.load_word2vec_format(\u0026#39;sgns.sogou.word.bz2\u0026#39;, unicode_errors=\u0026#39;ignore\u0026#39;) chinese_sougou_news_models Run\n\u0026lt;gensim.models.keyedvectors.KeyedVectors at 0x7f93e5b8cc10\u0026gt; \nfrom bertopic import BERTopic topic_model = BERTopic(language=\u0026#34;chinese (simplified)\u0026#34;, embedding_model=chinese_sougou_news_models, calculate_probabilities=True, verbose=True) docs = df[\u0026#39;content\u0026#39;].tolist() #2000条进行fit_transform需要1min topics, probs = topic_model.fit_transform(docs) 100%|██████████| 2000/2000 [01:31\u0026lt;00:00, 21.91it/s] 2021-10-28 12:11:25,583 - BERTopic - Transformed documents to Embeddings 2021-10-28 12:11:34,582 - BERTopic - Reduced dimensionality with UMAP 2021-10-28 12:11:34,718 - BERTopic - Clustered UMAP embeddings with HDBSCAN CPU times: user 1min 50s, sys: 7.7 s, total: 1min 57s Wall time: 1min 43s  \n主题模型方法  topic_model.get_topic_info 查看各主题信息 topic_model.find_topics(term, top_n=5) 查找term最有可能所属话题 topic_model.get_topic(0) 查看Topic 0的特征词 topic_model.visualize_topics() 话题间距离的可视化 topic_model.visualize_distribution(probs[0]) 查看某条文本的主题分布 topic_model.visualize_hierarchy(top_n_topics=20) 主题层次聚类可视化 topic_model.visualize_barchart(topics=[1]) 显示主题1的词条形图 topic_model.visualize_heatmap(n_clusters=10) 主题相似度热力图 topic_model.visualize_term_rank() 可视化词语 topic_model.save() 保存主题模型 topic_model.reduce_topics() 压缩主题个数(合并相近的主题)  .get_topic_info() 查看BERTopic基于cnews.csv数据， 跑出的各主题\ntopic_model.get_topic_info() .find_topics(term) 查看与词语【投资】最相关的主题，返回候选的最相思的5个主题id\n# similar_topics, similarity = topic_model.find_topics(\u0026#34;投资\u0026#34;, top_n=5) similar_topics Run\n[3, 9, 8, 10, 4]  .get_topic() 查看id为3的主题信息（主题词及权重）\ntopic_model.get_topic(3) Run\n[(\u0026#39;基金\u0026#39;, 0.15109221307919193), (\u0026#39;投资\u0026#39;, 0.042856192509064), (\u0026#39;公司\u0026#39;, 0.039785278320496976), (\u0026#39;市场\u0026#39;, 0.037072163603417835), (\u0026#39;股票\u0026#39;, 0.03230913401086524), (\u0026#39;型基金\u0026#39;, 0.02721898070238429), (\u0026#39;收益\u0026#39;, 0.025435672141638468), (\u0026#39;投资者\u0026#39;, 0.024633503649868493), (\u0026#39;经理\u0026#39;, 0.02458550023931051), (\u0026#39;发行\u0026#39;, 0.022672639068067168)] \n.visualize_topics() 可视化主题间距离\nvisualize_topics1 = topic_model.visualize_topics() #可视化结果保存至html中，可以动态显示信息 visualize_topics1.write_html(\u0026#39;visualize_topics.html\u0026#39;) visualize_topics1 点击查看visualize_topics1.html\n.visualize_distribution() 显示第一条新闻的主题概率分布\nfirst_new_topic_probs = topic_model.visualize_distribution(probs[0]) first_new_topic_probs.write_html(\u0026#39;first_new_topic_probs.html\u0026#39;) first_new_topic_probs 点击查看first_new_topic_probs.html\n为了理解主题的潜在层次结构，我们可以使用 scipy.cluster.hierarchy 创建聚类并可视化它们之间的关系。 这有助于合并相似主题，达到降低主题模型主题数量nr_topics。\n.visualize_hierarchy(top_n_topics) 话题层次聚类可视化，模型跑出12个主题，这里就按12进行分层聚类\ntopic_model.visualize_hierarchy(top_n_topics=12) .visualize_barchart(topics) 显示topics的词条形图\ntopic_model.visualize_barchart(topics=[1]) .visualize_heatmap(n_clusters) 话题相似热力图。BERTopic可将主题以embeddings形式（向量）表示， 因此我们可以应用余弦相似度来创建相似度矩阵。 每两两主题可进行余弦计算，最终结果将是一个矩阵，显示主题间的相似程度。\ntopic_similar_heatmap = topic_model.visualize_heatmap(n_clusters=11) topic_similar_heatmap.write_html(\u0026#39;topic_similar_heatmap.html\u0026#39;) topic_similar_heatmap 点击查看topic_similar_heatmap.html\n通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。 从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。 此外，可以轻松地将主题表示相互比较。\n.visualize_term_rank() 通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。\n从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。\n此外，可以轻松地将主题表示相互比较。\nterm_score_decline = topic_model.visualize_term_rank() term_score_decline.write_html(\u0026#39;term_score_decline.html\u0026#39;) term_score_decline 点击查看term_score_decline.html\n.update_topics() 更新主题模型。当您训练了一个模型并查看了代表它们的主题和单词时，您可能对表示不满意。 也许您忘记删除停用词，或者您想尝试不同的 n_gram_range。 我们可以使用函数 update_topics 使用 c-TF-IDF 的新参数更新主题表示。\n使用.update_topics()更新，\ntopic_model.update_topics(df.content.tolist(), topics, n_gram_range=(1, 3)) topic_model得到了更新，\nsimilar_topics, similarity = topic_model.find_topics(\u0026#34;手机\u0026#34;, top_n=5) similar_topics Run\n[2, 7, 4, 1, 5]  查看话题2的信息\ntopic_model.get_topic(2) Run\n[(\u0026#39;功能\u0026#39;, 0.022132351014298786), (\u0026#39;采用\u0026#39;, 0.02136925357979149), (\u0026#39;像素\u0026#39;, 0.020797285140907094), (\u0026#39;拍摄\u0026#39;, 0.017850841110848677), (\u0026#39;机身\u0026#39;, 0.015056931248982912), (\u0026#39;英寸\u0026#39;, 0.014624438184138326), (\u0026#39;佳能\u0026#39;, 0.012857768505732597), (\u0026#39;支持\u0026#39;, 0.012600856600766349), (\u0026#39;光学\u0026#39;, 0.012462085658291079), (\u0026#39;相机\u0026#39;, 0.011832978982454568)] 模型保存\n# Save model #model.save(\u0026#34;my_model\u0026#34;) # Load model #my_model = BERTopic.load(\u0026#34;my_model\u0026#34;) \n.reduce_topics() 压缩主题数\nnew_topics, new_probs = topic_model.reduce_topics(docs, topics, probs, nr_topics=10) Run\n2021-10-28 12:28:01,976 - BERTopic - Reduced number of topics from 20 to 11  代码数据 click to download\n总结 本文使用中文文本数据展示BERTopic部分功能，如果对英文数据感兴趣，可以前往 https://github.com/MaartenGr/BERTopic 深入学习。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/bertopic_tutorial/","summary":"BERT是自然语言处理领域最新的词向量技术，而BERTopic 是基于BERT词向量进行主题建模技术，它利用 Transformer 和 c-TF-IDF 来创建密集的集群，允许轻松解释主题，同时在主题描述中保留重要词。\nBERTopic亮点\n 支持引导式Guided 支持（半）监督式 支持动态主题。 支持可视化  安装 !pip3 install bertopic==0.10.0 !pip3 install cntext==1.6.5 \n准备数据 这里使用的新闻数据集， 共2000条。 新闻类别涵 '娱乐', '教育', '游戏', '财经', '时政', '时尚', '科技', '体育', '家居', '房产' 这里假设大家不知道有10类新闻题材， 构建模型的时候不会用到label字段的数据。\nimport pandas as pd df = pd.read_csv(\u0026#39;cnews.csv\u0026#39;) df.head() # 新闻题材 print(df.label.unique()) #记录数 print(len(df)) Run\n['娱乐' '教育' '游戏' '财经' '时政' '时尚' '科技' '体育' '家居' '房产'] 2000  # 各类题材的新闻记录数 df.label.value_counts() Run\n时政 120 科技 106 时尚 106 财经 105 家居 103 教育 97 娱乐 96 体育 95 房产 87 游戏 85","title":"BERTopic库 | 使用预训练模型做话题建模"},{"content":"以往对比两个文本数据差异，比较简单的技术实现方法是生成两文个词云图，但是词云图无法直观显示词语层面的权重。\nShifterator包提供了构建词移图的功能，垂直条形图可以量化哪些词会导致两个文本之间的成对差异以及它们如何起作用。 通过允许您查看单词使用方式的变化，单词转换可帮助您对情绪、熵和分歧进行分析，这些分析从根本上来说更具可解释性。\nShifterator亮点：\n 提供可解释的工具，用于将文本作为数据处理并映射出两个文本相似性或差异性 实现常见的文本比较度量，包括相对频率、香农熵、Tsallis熵、Kullback-Leibler散度和 Jensen-Shannon 散度。 基于字典的情绪分析方法计算的加权平均值。 在研究初期可用于诊断数据、感知测量误差。  计算社会科学家、数字人文主义者和其他文本分析从业者都可以使用 Shifterator 从文本数据构建可靠、稳健和可解释的故事。\n安装 !pip3 install shifterator==0.2.2 \n导入数据 准备的外卖csv数据，含label和review两个字段。\n其中label是好评差评的标注，\n 0为差评， 1为好评  import pandas as pd reviews_df = pd.read_csv(\u0026#34;data/WaiMai8k.csv\u0026#34;, encoding=\u0026#39;utf-8\u0026#39;) reviews_df.head()   有个疑问，外卖好差评中的用词有什么差异(区别/特点)？\n准备两组文本数据 shifterator需要两组文本数据，格式为长度相同的词频统计字典。\n按照label类别，将数据整理为两个文本数据。在准备的过程中，我们需要做一些清洗操作\n 清除非中文字符，如网址、邮箱、标点符号 清除信息量比较低的停用词  import collections import jieba import re texts_neg = reviews_df[reviews_df[\u0026#39;label\u0026#39;]==0][\u0026#39;review\u0026#39;].tolist() texts_pos = reviews_df[reviews_df[\u0026#39;label\u0026#39;]==1][\u0026#39;review\u0026#39;].tolist() def clean_text(docs): \u0026#34;\u0026#34;\u0026#34;清洗文本中的非中文字符、停用词，返回词频统计结果 docs : 待处理的文档列表 \u0026#34;\u0026#34;\u0026#34; stop_words = open(\u0026#39;data/stopwords.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read().split(\u0026#39;\\n\u0026#39;) text = \u0026#34;\u0026#34;.join(docs) text = \u0026#34;\u0026#34;.join(re.findall(\u0026#34;[\\u4e00-\\u9fa5]+\u0026#34;, text)) words = jieba.lcut(text) words = [w for w in words if w not in stop_words] wordfreq_dict = collections.Counter(words) return wordfreq_dict clean_texts_neg = clean_text(texts_neg) clean_texts_pos = clean_text(texts_pos) \n无聊的传统绘图 使用条形图、词云图绘制，为了缩小代码量，这里只绘制差评数据。需要注意的是matplotlib不显示中文，这里需要先使用下面三行代码获取电脑中自带的中文字体列表mpl_fonts，\nfrom matplotlib.font_manager import FontManager mpl_fonts = set(f.name for f in FontManager().ttflist) print(mpl_fonts) 经过运行，我的电脑mpl_fonts里有Arial Unicode MS ，后面用matplotlib显示中文的地方，我都使用该字体。\n#准备DataFrame数据 common_neg = pd.DataFrame(clean_texts_neg.most_common(15), columns=[\u0026#39;words\u0026#39;, \u0026#39;count\u0026#39;]) common_pos = pd.DataFrame(clean_texts_pos.most_common(15), columns=[\u0026#39;words\u0026#39;, \u0026#39;count\u0026#39;]) from matplotlib import pyplot as plt import seaborn as sns import matplotlib sns.set(font_scale=1.5) sns.set_style(\u0026#34;whitegrid\u0026#34;) #为了显示中文 matplotlib.rc(\u0026#34;font\u0026#34;, family=\u0026#39;Arial Unicode MS\u0026#39;) fig, ax = plt.subplots(figsize=(16, 8)) #绘制水平条形图 common_neg.sort_values(by=\u0026#39;count\u0026#39;).plot.barh(x=\u0026#39;words\u0026#39;, y=\u0026#39;count\u0026#39;, ax=ax, color=\u0026#34;red\u0026#34;) ax.set_title(\u0026#34;外卖差评常见词\u0026#34;) plt.show()   绘制词云图，这里使用的pyecharts包。由于该包作者更新强度比较大，为了保证日后本教程仍可正常运行，这里提供当前我使用的pyecharts相关的版本，大家可以运行下面代码保证运行出正确结果\n!pip3 install pyecharts==1.6.2 !pip3 install pyecharts-javascripthon==0.0.6 !pip3 install pyecharts-jupyter-installer==0.0.3 !pip3 install pyecharts-snapshot==0.2.0 import pyecharts.options as opts from pyecharts.charts import WordCloud from pyecharts.globals import CurrentConfig, NotebookType CurrentConfig.NOTEBOOK_TYPE = NotebookType.JUPYTER_NOTEBOOK wordfreqs = [(w, str(f)) for w,f in dict(clean_texts_neg).items()] wc = WordCloud() wc.add(series_name=\u0026#34;\u0026#34;, data_pair=wordfreqs, word_size_range=[20, 100]) wc.set_global_opts(title_opts=opts.TitleOpts(title=\u0026#34;外卖差评词云图\u0026#34;, title_textstyle_opts=opts.TextStyleOpts(font_size=23)), tooltip_opts=opts.TooltipOpts(is_show=True)) wc.load_javascript() wc.render_notebook()   使用Shifterator绘制词移图 终于要用到 Shifterator 包了！ 我们可以使用这个包根据频率和情绪（或其他值）比较负面和正面的外卖评论，这里我只计算了频率作为权重\n熵移图Entropy shift 第一幅图是entropy shift graph\n具体信息请查看文档 https://github.com/ryanjgallagher/shifterator\nfrom shifterator import EntropyShift import matplotlib matplotlib.rc(\u0026#34;font\u0026#34;, family=\u0026#39;Arial Unicode MS\u0026#39;) entropy_shift = EntropyShift(type2freq_1=clean_texts_neg, type2freq_2=clean_texts_pos, base=2) entropy_shift.get_shift_graph(title=\u0026#39;外卖差评 vs 外卖好评\u0026#39;)   看起来最能决定外卖差评的用语是配送时间，其次才是口味。\n最能决定外卖好评的似乎是口味，其次才是配送时间。\n通过Shifterator我们能够看出不同词在不同文本中的作用程度。需要注意的是，我们只使用了最高的前15词频，所以显示的词有些少\n总结 希望本文能对你的研究有帮助，代码下载地址\nhttps://github.com/hidadeng/DaDengAndHisPython/tree/master/20211027shifterator学习\n代码撰写调试不易，希望帮忙转载\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/shifterator_text_vis/","summary":"以往对比两个文本数据差异，比较简单的技术实现方法是生成两文个词云图，但是词云图无法直观显示词语层面的权重。\nShifterator包提供了构建词移图的功能，垂直条形图可以量化哪些词会导致两个文本之间的成对差异以及它们如何起作用。 通过允许您查看单词使用方式的变化，单词转换可帮助您对情绪、熵和分歧进行分析，这些分析从根本上来说更具可解释性。\nShifterator亮点：\n 提供可解释的工具，用于将文本作为数据处理并映射出两个文本相似性或差异性 实现常见的文本比较度量，包括相对频率、香农熵、Tsallis熵、Kullback-Leibler散度和 Jensen-Shannon 散度。 基于字典的情绪分析方法计算的加权平均值。 在研究初期可用于诊断数据、感知测量误差。  计算社会科学家、数字人文主义者和其他文本分析从业者都可以使用 Shifterator 从文本数据构建可靠、稳健和可解释的故事。\n安装 !pip3 install shifterator==0.2.2 \n导入数据 准备的外卖csv数据，含label和review两个字段。\n其中label是好评差评的标注，\n 0为差评， 1为好评  import pandas as pd reviews_df = pd.read_csv(\u0026#34;data/WaiMai8k.csv\u0026#34;, encoding=\u0026#39;utf-8\u0026#39;) reviews_df.head()   有个疑问，外卖好差评中的用词有什么差异(区别/特点)？\n准备两组文本数据 shifterator需要两组文本数据，格式为长度相同的词频统计字典。\n按照label类别，将数据整理为两个文本数据。在准备的过程中，我们需要做一些清洗操作\n 清除非中文字符，如网址、邮箱、标点符号 清除信息量比较低的停用词  import collections import jieba import re texts_neg = reviews_df[reviews_df[\u0026#39;label\u0026#39;]==0][\u0026#39;review\u0026#39;].tolist() texts_pos = reviews_df[reviews_df[\u0026#39;label\u0026#39;]==1][\u0026#39;review\u0026#39;].tolist() def clean_text(docs): \u0026#34;\u0026#34;\u0026#34;清洗文本中的非中文字符、停用词，返回词频统计结果 docs : 待处理的文档列表 \u0026#34;\u0026#34;\u0026#34; stop_words = open(\u0026#39;data/stopwords.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read().split(\u0026#39;\\n\u0026#39;) text = \u0026#34;\u0026#34;.","title":"Shifterator库 | 词移图分辨两文本用词风格差异"},{"content":"代码下载 点击此处下载代码\n 原文链接 https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-24207127cad7\n  想象一下，你正试图训练一个机器学习模型来预测广告是否被特定的人点击。在收到关于某人的一些信息后，模型预测某人会不会点击广告。\n 但是为什么模型会输出这样的预测结果呢？ 每个特征对预测的贡献有多大？ 如果您能看到一个图表，显示每个特征对预测的贡献程度，如下所示，不是很好吗？\n Shapley值就能起到特征权重测度的作用。\nShapley值是什么？ Shapley值是博弈论中使用的一种方法，它涉及公平地将收益和成本分配给在联盟中工作的行动者。 由于每个行动者对联盟的贡献是不同的，Shapley值保证每个行动者根据贡献的多少获得公平的份额。\n 小案例 Shapley值被广泛地应用于求解群体中每个工人(特征)的贡献问题。要理解Shapley值的作用，让我们想象一下贵公司刚刚做了A/B测试，他们在测试广告策略的不同组合。\n每个策略在特定月份的收入是：\n 无广告：150美元 社交媒体：300美元 谷歌广告：200美元 电子邮件营销：350美元 社交媒体和谷歌广告：320美元 社交媒体和电子邮件营销：400美元 谷歌广告和电子邮件营销：350美元 电子邮件营销，谷歌广告和社交媒体：450美元   使用三则广告与不使用广告的收入相差300美元，每则广告对这一差异有多大的贡献?\n 我们可以通过计算每一类广告的Shapley值来计算谷歌广告对公司收入的总贡献入手，通过公式可以计算出Google广告的总贡献：\n 让我们找到Google广告的边际贡献及其权重。\n寻找谷歌广告的边际贡献 第一，我们将发现谷歌广告对以下群体的边际贡献：\n 无广告 谷歌广告+社交媒体 谷歌广告+电子邮件营销 谷歌广告+电子邮件营销+社交媒体   Google广告 对 无广告 的边际贡献是：\n 谷歌广告 对 谷歌广告\u0026amp;社交媒体组合 的边际贡献是：\n 谷歌广告 对 谷歌广告\u0026amp;电子邮件营销组合 的边际贡献是：\n 谷歌广告 对 谷歌广告、电子邮件营销和社交媒体组合 的边际贡献是：\n 发现权重 为了发现权重，我们将把不同广告策略的组合组织成如下多个层次，每个层次对应于每个组合中广告策略的数量。\n然后根据每个层次的边数分配权重，我们看到了这一点：\n 第一级包含3条边，因此每个边的权重为1/3 第二级包含6条边，因此每条边的权重将为1/6 第三级包含3条边，因此每条边的权重将为1/3   发现Google广告的总贡献 根据前面的权重和边际贡献，我们已经可以找到Google广告的总贡献!\n  酷!所以谷歌广告在使用3种广告策略与不使用广告的总收入差异中贡献了36.67美元。36.67是Google广告的Shapey值。\n 重复以上步骤，对于另外两种广告策略，我们可以看出：\n  电子邮件营销贡献151.67美元\n  社交媒体贡献116.67美元\n  谷歌广告贡献36.67美元\n   他们共同出资300美元，用于使用3种不同类型的广告与不使用广告的区别!挺酷的，不是吗? 既然我们理解了Shapley值，那么让我们看看如何使用它来解释机器学习模型。\nSHAP-在Python中解释机器学习模型 SHAP是一个Python库，它使用Shapley值来解释任何机器学习模型的输出。\n安装SHAP\n!pip3 install shap 训练模型 为了理解SHAP工作原理，我们使用Kaggle平台内的advertising广告数据集。\nimport pandas as pd df = pd.read_csv(\u0026#34;advertising.csv\u0026#34;) df.head()   我们将建立一个机器学习模型, 该模型根据用户个人特质信息来预测其是否点击广告。\n我们使用Patsy将DataFrame转换为一组特征和一组目标值：\nfrom patsy import dmatrices from sklearn.model_selection import train_test_split y, X = dmatrices( \u0026#34;clicked_on_ad ~ daily_time_spent_on_site + age + area_income + daily_internet_usage + male -1\u0026#34;, data=df, ) X_frame = pd.DataFrame(data=X, columns=X.design_info.column_names) 把数据分为测试集和训练接\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7) 接下来使用XGBoost训练模型，并做预测\nimport xgboost model = xgboost.XGBClassifier().fit(X_train, y_train) y_predicted = model.predict(X_test) 为了查看模型表现，我们使用F1得分\nfrom sklearn.metrics import f1_score f1 = f1_score(y_test, y_predicted) f1 0.9619047619047619  太好了!\n解释该模型 该模型很好地预测了用户是否点击广告。但它是如何得出这样的预测的? 每个特征对最终预测与平均预测的差异贡献了多少?\n注意，这个问题与我们在文章开头论述的问题非常相似。\n因此，寻找每个特征的Shapley值可以帮助我们确定它们的贡献。得到特征i的重要性的步骤与之前类似，其中i是特征的索引：\n 获取所有不包含特征i的子集 找出特征i对这些子集中每个子集的边际贡献 聚合所有边际贡献来计算特征i的贡献  若要使用SHAP查找Shapley值，只需将训练好的模型插入shap.Explainer\nimport shap explainer = shap.Explainer(model) shap_values = explainer(X_frame) ntree_limit is deprecated, use `iteration_range` or model slicing instead.  SHAP瀑布图 可视化第一个预测的解释：\n#第一条记录是未点击 shap.plots.waterfall(shap_values[0])   啊哈!现在我们知道每个特征对第一次预测的贡献。对上图的解释：\n  蓝色条显示某一特定特征在多大程度上降低了预测的值。 红条显示了一个特定的特征在多大程度上增加了预测值。 负值意味着该人点击广告的概率小于0.5  我们应该期望总贡献等于预测与均值预测的差值。我们来验证一下：\n 酷!他们是平等的。\n可视化第二个预测的解释：\n#第二条记录也是未点击 shap.plots.waterfall(shap_values[1])   SHAP摘要图 我们可以使用SHAP摘要图，而不是查看每个单独的实例，来可视化这些特性对多个实例的整体影响：\nshap.summary_plot(shap_values, X)   SHAP摘要图告诉我们数据集上最重要的特征及其影响范围。\n从上面的情节中，我们可以对模型的预测获得一些有趣的见解：\n 用户的 daily_internet_usage 对该用户是否点击广告的影响最大。 随着daily_time_spent_on_site的增加，用户点击广告的可能性降低。 随着area_income的增加，用户点击广告的可能性降低。 随着age的增长，用户更容易点击广告。 如果用户是male，则该用户点击广告的可能性较小。  SHAP条形图 我们还可以使用SHAP条形图得到全局特征重要性图。\nshap.plots.bar(shap_values)   很酷!\n结论 恭喜你!您刚刚了解了Shapey值以及如何使用它来解释一个机器学习模型。希望本文将提供您使用Python来解释自己的机器学习模型的基本知识。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/shap_ml_explanation/","summary":"代码下载 点击此处下载代码\n 原文链接 https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-24207127cad7\n  想象一下，你正试图训练一个机器学习模型来预测广告是否被特定的人点击。在收到关于某人的一些信息后，模型预测某人会不会点击广告。\n 但是为什么模型会输出这样的预测结果呢？ 每个特征对预测的贡献有多大？ 如果您能看到一个图表，显示每个特征对预测的贡献程度，如下所示，不是很好吗？\n Shapley值就能起到特征权重测度的作用。\nShapley值是什么？ Shapley值是博弈论中使用的一种方法，它涉及公平地将收益和成本分配给在联盟中工作的行动者。 由于每个行动者对联盟的贡献是不同的，Shapley值保证每个行动者根据贡献的多少获得公平的份额。\n 小案例 Shapley值被广泛地应用于求解群体中每个工人(特征)的贡献问题。要理解Shapley值的作用，让我们想象一下贵公司刚刚做了A/B测试，他们在测试广告策略的不同组合。\n每个策略在特定月份的收入是：\n 无广告：150美元 社交媒体：300美元 谷歌广告：200美元 电子邮件营销：350美元 社交媒体和谷歌广告：320美元 社交媒体和电子邮件营销：400美元 谷歌广告和电子邮件营销：350美元 电子邮件营销，谷歌广告和社交媒体：450美元   使用三则广告与不使用广告的收入相差300美元，每则广告对这一差异有多大的贡献?\n 我们可以通过计算每一类广告的Shapley值来计算谷歌广告对公司收入的总贡献入手，通过公式可以计算出Google广告的总贡献：\n 让我们找到Google广告的边际贡献及其权重。\n寻找谷歌广告的边际贡献 第一，我们将发现谷歌广告对以下群体的边际贡献：\n 无广告 谷歌广告+社交媒体 谷歌广告+电子邮件营销 谷歌广告+电子邮件营销+社交媒体   Google广告 对 无广告 的边际贡献是：\n 谷歌广告 对 谷歌广告\u0026amp;社交媒体组合 的边际贡献是：\n 谷歌广告 对 谷歌广告\u0026amp;电子邮件营销组合 的边际贡献是：\n 谷歌广告 对 谷歌广告、电子邮件营销和社交媒体组合 的边际贡献是：\n 发现权重 为了发现权重，我们将把不同广告策略的组合组织成如下多个层次，每个层次对应于每个组合中广告策略的数量。\n然后根据每个层次的边数分配权重，我们看到了这一点：\n 第一级包含3条边，因此每个边的权重为1/3 第二级包含6条边，因此每条边的权重将为1/6 第三级包含3条边，因此每条边的权重将为1/3   发现Google广告的总贡献 根据前面的权重和边际贡献，我们已经可以找到Google广告的总贡献!","title":"SHAP机器学习模型解释库"},{"content":"\n 作者 bot_developer\n搬运自\n https://www.kaggle.com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests https://github.com/bot-developer3/Scraping-Tools-Benzinga.   背景  获得高质量（历史）股市新闻数据既困难又昂贵； 订阅历史新闻数据提供商服务可能需要花费数千美元。  \n数据集 采集了6000只股票2009-2020年间的4万条新闻文章\n数据链接:https://pan.baidu.com/s/1rMo4Ek2bxvVLmeyxskVCAg 密码:paen\nraw_analyst_ratings.csv 直接抓取分析师评级数据，有1034位分析师对6204只股票进行了股票分析，分析记录累积1407328条, 字段包括：索引、标题、URL、文章作者（出版商总是benzinga）、出版时间戳、股票代码。\n请注意，此 CSV 文件中的所有日期均不包含精确的时分秒信息。 如果您打算使用此文件进行回测（analyst_ratings_processed.csv 更好），请假设文章是在第二天而不是当前文章中显示的日期发布的。\nraw_partner_headlines.csv 直接抓取原始新闻标题，共有1845559条记录, 字段包括：索引、标题、URL、出版商（不是 benzinga）、日期、股票行情。\nanalyst_ratings_processed.csv 处理过的分析师评级数据， 共有1400469条记录， 字段包括：文章标题，日期，股票\n时区为 UTC-4。 这与 raw_analys_theadlines 之间的区别在于，它具有精确到分钟的日期，而 raw_analys_tratings 只是没有小时或分钟的那一天。\n\n注意  数据爬自benzinga.com，新闻内容版权归Benzinga所有。  \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/daily_financial_news_for_6000_stocks/","summary":"\n 作者 bot_developer\n搬运自\n https://www.kaggle.com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests https://github.com/bot-developer3/Scraping-Tools-Benzinga.   背景  获得高质量（历史）股市新闻数据既困难又昂贵； 订阅历史新闻数据提供商服务可能需要花费数千美元。  \n数据集 采集了6000只股票2009-2020年间的4万条新闻文章\n数据链接:https://pan.baidu.com/s/1rMo4Ek2bxvVLmeyxskVCAg 密码:paen\nraw_analyst_ratings.csv 直接抓取分析师评级数据，有1034位分析师对6204只股票进行了股票分析，分析记录累积1407328条, 字段包括：索引、标题、URL、文章作者（出版商总是benzinga）、出版时间戳、股票代码。\n请注意，此 CSV 文件中的所有日期均不包含精确的时分秒信息。 如果您打算使用此文件进行回测（analyst_ratings_processed.csv 更好），请假设文章是在第二天而不是当前文章中显示的日期发布的。\nraw_partner_headlines.csv 直接抓取原始新闻标题，共有1845559条记录, 字段包括：索引、标题、URL、出版商（不是 benzinga）、日期、股票行情。\nanalyst_ratings_processed.csv 处理过的分析师评级数据， 共有1400469条记录， 字段包括：文章标题，日期，股票\n时区为 UTC-4。 这与 raw_analys_theadlines 之间的区别在于，它具有精确到分钟的日期，而 raw_analys_tratings 只是没有小时或分钟的那一天。\n\n注意  数据爬自benzinga.com，新闻内容版权归Benzinga所有。  \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","title":"DataShare | 6000+个股票的每日财经新闻"},{"content":"1. 简介 label-studio 假设我们想使用机器学习做文本分析，一般都需要先对数据进行标注，才能训练出效果比较好的监督机器学习模型。\nlabel-studio是多媒体数据标注工具，可以很方便的进行标注和导出。\nLabel Studio 是一款开源数据标注工具，用于标注和探索多种类型的数据。 您可以使用多种数据格式执行的标记任务。\n您还可以将 Label Studio 与机器学习模型集成，以提供标签（预标签）的预测，或执行持续的主动学习。\n官方文档 https://labelstud.io/\n操作步骤  安装Label Studio 启动Label Studio 创建Label Studio账号 项目默认配置 导入数据 标注数据 结束标记，导出标注数据  安装 命令行中执行\npip install label-studio==1.1.0 2 快速上手 在桌面创建自动生成一个名为Project的项目文件夹。\n Win命令行执行  label-studio --data-dir Desktop/Project  Mac命令行执行  label-studio --data-dir desktop/Project \n执行上方代码大概10s左右，会在浏览器弹出如下界面\n 注册好账号密码，点击Create Project\n 项目描述填写好，点击按钮**Data Import **，\n 这里我们要做文本分析，导入csv\n  设置标注模式，点击按钮Labeling Setup,选择Natural Language Process、TEXT Classification。就考研进行pos、neg、neo三个类别的文本标注。\n 注意label-studio提供了diy，考研根据自己需要点击Code设定标注类别名称、增减类别。大家感兴趣的可以深入研究。\n 点击Save 按钮，开始准备标注数据啦\n数据界面，勾选全部数据，点击蓝色按钮Label All Tasks\n 开始标注，勾选你认为合适的标签，点击右侧Submit\n 导出标注数据,先点击右侧Export按钮，选择导出格式，最后点击底部Export按钮执行导出。\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/label_studio_test/","summary":"1. 简介 label-studio 假设我们想使用机器学习做文本分析，一般都需要先对数据进行标注，才能训练出效果比较好的监督机器学习模型。\nlabel-studio是多媒体数据标注工具，可以很方便的进行标注和导出。\nLabel Studio 是一款开源数据标注工具，用于标注和探索多种类型的数据。 您可以使用多种数据格式执行的标记任务。\n您还可以将 Label Studio 与机器学习模型集成，以提供标签（预标签）的预测，或执行持续的主动学习。\n官方文档 https://labelstud.io/\n操作步骤  安装Label Studio 启动Label Studio 创建Label Studio账号 项目默认配置 导入数据 标注数据 结束标记，导出标注数据  安装 命令行中执行\npip install label-studio==1.1.0 2 快速上手 在桌面创建自动生成一个名为Project的项目文件夹。\n Win命令行执行  label-studio --data-dir Desktop/Project  Mac命令行执行  label-studio --data-dir desktop/Project \n执行上方代码大概10s左右，会在浏览器弹出如下界面\n 注册好账号密码，点击Create Project\n 项目描述填写好，点击按钮**Data Import **，\n 这里我们要做文本分析，导入csv\n  设置标注模式，点击按钮Labeling Setup,选择Natural Language Process、TEXT Classification。就考研进行pos、neg、neo三个类别的文本标注。\n 注意label-studio提供了diy，考研根据自己需要点击Code设定标注类别名称、增减类别。大家感兴趣的可以深入研究。\n 点击Save 按钮，开始准备标注数据啦","title":"Label-Studio|多媒体数据标注工具"},{"content":"情感分析  无权重。直接计算文本中正、负情感词出现的次数 有权重。tf-idf， tf是词频，idf是权重。  Tfidf法 scikit库除了CountVectorizer类，还有TfidfVectorizer类。TF-IDF这个定义相信大家应该已经耳熟能详了：\n     TF 词语出现越多，这个词越有信息量 IDF 词语越少的出现在文本中，词语越有信息量。  原始数据 import pandas as pd corpus = [\u0026#34;hello, i am glad to meet you\u0026#34;, \u0026#34;it is wonderful\u0026#34;, \u0026#34;i hate you\u0026#34;, \u0026#34;i am sad\u0026#34;] df1 = pd.DataFrame(corpus, columns=[\u0026#39;Text\u0026#39;]) df1   构造tfidf from sklearn.feature_extraction.text import TfidfVectorizer def createDTM(corpus): \u0026#34;\u0026#34;\u0026#34;构建文档词语矩阵\u0026#34;\u0026#34;\u0026#34; vectorize = TfidfVectorizer() #注意fit_transform相当于fit之后又transform。 dtm = vectorize.fit_transform(corpus) #vectorize.fit(corpus) #dtm = vectorize.transform(corpus)  #打印dtm return pd.DataFrame(dtm.toarray(), columns=vectorize.get_feature_names()) df2 = createDTM(df[\u0026#39;text\u0026#39;]) df2   合并df1和df2 df = pd.concat([df1, df2], axis=1) df   #积极词典 pos_words = [\u0026#39;glad\u0026#39;, \u0026#39;hello\u0026#39;, \u0026#39;wonderful\u0026#39;] #消极词典 neg_words = [\u0026#39;sad\u0026#39;, \u0026#39;hate\u0026#39;] #积极词典 df[pos_words] df[pos_words].sum(axis=1) Run\n0 0.873439 1 0.577350 2 0.000000 3 0.000000 dtype: float64  df[\u0026#39;Pos\u0026#39;] = df[pos_words].sum(axis=1) df   df[\u0026#39;Neg\u0026#39;] = df[neg_words].sum(axis=1) df   输出 df.to_csv(\u0026#39;output/tfidf有权重的情感分析.csv\u0026#39;) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/weighted_tfidf_sentiment_analysis/","summary":"情感分析  无权重。直接计算文本中正、负情感词出现的次数 有权重。tf-idf， tf是词频，idf是权重。  Tfidf法 scikit库除了CountVectorizer类，还有TfidfVectorizer类。TF-IDF这个定义相信大家应该已经耳熟能详了：\n     TF 词语出现越多，这个词越有信息量 IDF 词语越少的出现在文本中，词语越有信息量。  原始数据 import pandas as pd corpus = [\u0026#34;hello, i am glad to meet you\u0026#34;, \u0026#34;it is wonderful\u0026#34;, \u0026#34;i hate you\u0026#34;, \u0026#34;i am sad\u0026#34;] df1 = pd.DataFrame(corpus, columns=[\u0026#39;Text\u0026#39;]) df1   构造tfidf from sklearn.feature_extraction.text import TfidfVectorizer def createDTM(corpus): \u0026#34;\u0026#34;\u0026#34;构建文档词语矩阵\u0026#34;\u0026#34;\u0026#34; vectorize = TfidfVectorizer() #注意fit_transform相当于fit之后又transform。 dtm = vectorize.fit_transform(corpus) #vectorize.fit(corpus) #dtm = vectorize.transform(corpus)  #打印dtm return pd.","title":"tfidf有权重的情感分析"},{"content":"可以使用cnsenti库中的自定义方法，计算年报或财经类社交媒体的文本情绪。\n 姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报，2021. 24(5), 26-46.\n 该论文开发了中文的金融情感词典，已有的中文金融情感词典有以下不足：\n 大多采用形容情绪词，对于金融场景适用性差 将LM英文词典本土化，制作中文金融情绪词典 词典构建方法多为人工  该论文开发中文情绪词典，从年报和社交媒体两个数据源出发，借助数据挖掘和深度学习算法，构建了正式用语 和 非正式用于两大类情感词典。\n标注思路 一般构建词典要么用多个词典融合，要么人工标准训练。该论文采用了一定的技巧，不需要人工标注即可实现近乎人工标注的效果。\n正式词典标注思路 正式用语情感词典，通过年报公布后3个交易日累积正负收益率为标准，将年报标记为正负面情绪两类。\n非正式词典标注思路 使用所有中国上市公司在雪球论坛和东方财富股吧内相关帖子，共8130万条。\n在网络股票论坛，用户发表自己的意见时，经常带有表情符号，从而使得帖子带有明显的情绪指标。 这种含有特殊指标的帖子，省去了人工标注文本情绪的工作。\n具体构建词典的步骤，大家可以阅读论文原文。论文已经公开了中文情感词典，我已将其整理为4个txt文件\n formal_pos.txt 正式用语正面情绪词典 formal_neg.txt 正式用语负面情绪词典 unformal_pos.txt 非正式用语正面情绪词典 unformal_neg.txt 非正式用语负面情绪词典  中文金融词典使用方法 cnsenti实现了自定义词典功能，导入不同的txt词典文件，即可实现不同方面的情绪词统计。\n年报正式用语词典  dict/formal_pos.txt 正式用语正面情绪词典 dict/formal_neg.txt 正式用语负面情绪词典  from cnsenti import Sentiment senti = Sentiment(pos=\u0026#39;dict/formal_pos.txt\u0026#39;, #正面词典txt文件相对路径 neg=\u0026#39;dict/formal_neg.txt\u0026#39;, #负面词典txt文件相对路径 merge=False, #是否将cnsenti自带词典和用户导入的自定义词典融合 encoding=\u0026#39;utf-8\u0026#39;) #两txt均为utf-8编码 test_text = \u0026#39;这家公司是行业的引领者，是中流砥柱。今年的业绩非常好。\u0026#39; result = senti.sentiment_count(test_text) print(\u0026#39;sentiment_count\u0026#39;,result) Run\nsentiment_count {\u0026#39;words\u0026#39;: 16, \u0026#39;sentences\u0026#39;: 2, \u0026#39;pos\u0026#39;: 3, \u0026#39;neg\u0026#39;: 0} \n财经社交媒体非正式用语词典  dict/unformal_pos.txt 非正式用语正面情绪词典 dict/unformal_neg.txt 非正式用语负面情绪词典  from cnsenti import Sentiment senti = Sentiment(pos=\u0026#39;dict/unformal_pos.txt\u0026#39;, #正面词典txt文件相对路径 neg=\u0026#39;dict/unformal_neg.txt\u0026#39;, #负面词典txt文件相对路径 merge=False, #融合cnsenti自带词典和用户导入的自定义词典 encoding=\u0026#39;utf-8\u0026#39;) #两txt均为utf-8编码 test_text = \u0026#39;这个股票前期走势承压，现在阴跌，散户只能割肉离场，这股票真垃圾\u0026#39; result = senti.sentiment_count(test_text) print(\u0026#39;sentiment_count\u0026#39;,result) Run\nsentiment_count {\u0026#39;words\u0026#39;: 18, \u0026#39;sentences\u0026#39;: 1, \u0026#39;pos\u0026#39;: 0, \u0026#39;neg\u0026#39;: 2} \n说明 读者如需使用本项目词典，请引用如下参考文献：\n 姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报，2021. 24(5), 26-46.\n 广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/chinese_financial_dictionary/","summary":"可以使用cnsenti库中的自定义方法，计算年报或财经类社交媒体的文本情绪。\n 姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报，2021. 24(5), 26-46.\n 该论文开发了中文的金融情感词典，已有的中文金融情感词典有以下不足：\n 大多采用形容情绪词，对于金融场景适用性差 将LM英文词典本土化，制作中文金融情绪词典 词典构建方法多为人工  该论文开发中文情绪词典，从年报和社交媒体两个数据源出发，借助数据挖掘和深度学习算法，构建了正式用语 和 非正式用于两大类情感词典。\n标注思路 一般构建词典要么用多个词典融合，要么人工标准训练。该论文采用了一定的技巧，不需要人工标注即可实现近乎人工标注的效果。\n正式词典标注思路 正式用语情感词典，通过年报公布后3个交易日累积正负收益率为标准，将年报标记为正负面情绪两类。\n非正式词典标注思路 使用所有中国上市公司在雪球论坛和东方财富股吧内相关帖子，共8130万条。\n在网络股票论坛，用户发表自己的意见时，经常带有表情符号，从而使得帖子带有明显的情绪指标。 这种含有特殊指标的帖子，省去了人工标注文本情绪的工作。\n具体构建词典的步骤，大家可以阅读论文原文。论文已经公开了中文情感词典，我已将其整理为4个txt文件\n formal_pos.txt 正式用语正面情绪词典 formal_neg.txt 正式用语负面情绪词典 unformal_pos.txt 非正式用语正面情绪词典 unformal_neg.txt 非正式用语负面情绪词典  中文金融词典使用方法 cnsenti实现了自定义词典功能，导入不同的txt词典文件，即可实现不同方面的情绪词统计。\n年报正式用语词典  dict/formal_pos.txt 正式用语正面情绪词典 dict/formal_neg.txt 正式用语负面情绪词典  from cnsenti import Sentiment senti = Sentiment(pos=\u0026#39;dict/formal_pos.txt\u0026#39;, #正面词典txt文件相对路径 neg=\u0026#39;dict/formal_neg.txt\u0026#39;, #负面词典txt文件相对路径 merge=False, #是否将cnsenti自带词典和用户导入的自定义词典融合 encoding=\u0026#39;utf-8\u0026#39;) #两txt均为utf-8编码 test_text = \u0026#39;这家公司是行业的引领者，是中流砥柱。今年的业绩非常好。\u0026#39; result = senti.sentiment_count(test_text) print(\u0026#39;sentiment_count\u0026#39;,result) Run\nsentiment_count {\u0026#39;words\u0026#39;: 16, \u0026#39;sentences\u0026#39;: 2, \u0026#39;pos\u0026#39;: 3, \u0026#39;neg\u0026#39;: 0}","title":"中文金融情感词典"},{"content":"Clumper可以用来处理嵌套样式的json数据结构。\n代码下载 Getting Started 安装 !pip3 install clumper Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple Collecting clumper Downloading https://pypi.tuna.tsinghua.edu.cn/packages/70/62/0731ab9b48c91132aff487217980dcb147ffc0922a278adc05986f6a8d4b/clumper-0.2.13-py2.py3-none-any.whl (21 kB) Installing collected packages: clumper Successfully installed clumper-0.2.13 \u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.1.2 is available. You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m  为了展示Clumper如何工作，我准备了pokemon.json， 由列表组成(该列表由多个字典组成)，下面是pokemon.json部分内容\nimport json with open(\u0026#39;pokemon.json\u0026#39;) as jsonf: pokemon = json.loads(jsonf.read()) pokemon[:2] [{'name': 'Bulbasaur', 'type': ['Grass', 'Poison'], 'total': 318, 'hp': 45, 'attack': 49}, {'name': 'Ivysaur', 'type': ['Grass', 'Poison'], 'total': 405, 'hp': 60, 'attack': 62}]  我们准备的pokemon.json列表中大概有800个字典，数量级刚刚好，不会因为太大导致电脑无法运行数据分析，也不会太小导致手动操作性价比更高。\nExample 基本操作 from clumper import Clumper list_of_dicts = [ {\u0026#39;a\u0026#39;: 7, \u0026#39;b\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 4}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6} ] (Clumper(list_of_dicts) .mutate(c=lambda d: d[\u0026#39;a\u0026#39;]+d[\u0026#39;b\u0026#39;]) .sort(lambda d: d[\u0026#39;c\u0026#39;]) .collect() ) [{'a': 2, 'b': 4, 'c': 6}, {'a': 7, 'b': 2, 'c': 9}, {'a': 3, 'b': 6, 'c': 9}]  代码解析 Step1\n首先使用mutate方法，该方法可以在每条记录中生成新变量。\n 结算结果仍为Clumper类\nStep2\n接下来对mutate之后的数据进行排序\n 得到的结果仍为Clumper类。\n从上面的小代码案例中，可以看到整套流程像是一个流水线车间，每一行就是一个生成环节，生产环节之间使用.连接起来。\nfrom clumper import Clumper (Clumper(pokemon) .keep(lambda d: len(d[\u0026#39;type\u0026#39;])==1) #保留type长度为1的字典 .mutate(type=lambda d: d[\u0026#39;type\u0026#39;][0], #type值从列表变为字符串 ratio=lambda d: d[\u0026#39;attack\u0026#39;]/d[\u0026#39;hp\u0026#39;]) #新建ratio .select(\u0026#39;name\u0026#39;, \u0026#39;type\u0026#39;, \u0026#39;ratio\u0026#39;) #字典最后只保留name， type， ratio三个字段 .sort(lambda d: d[\u0026#39;ratio\u0026#39;], reverse=True) #按照ratio降序排列 .head(5) #只保留前5个 .collect() #转成列表显示 ) \nCommon Verbs Keep keep函数可以从原始数据中抽取符合指定条件的子集。  from clumper import Clumper list_dicts = [{\u0026#39;a\u0026#39;: 1}, {\u0026#39;a\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 3}, {\u0026#39;a\u0026#39;: 4}] (Clumper(list_dicts) .keep(lambda d: d[\u0026#39;a\u0026#39;] \u0026gt;= 3) .collect() #试一试去掉.collect()后的效果 ) [{'a': 3}, {'a': 4}]  可以实现缺失值处理，以不同的方式实现pandas的.dropna()的功能。\nfrom clumper import Clumper data = [ {\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 4}, {\u0026#34;a\u0026#34;: 2, \u0026#34;b\u0026#34;: 3}, {\u0026#34;a\u0026#34;: 3, \u0026#34;b\u0026#34;: 2}, {\u0026#34;a\u0026#34;: 4}, ] #只保留含有b的字段 (Clumper(data) .keep(lambda d: \u0026#39;b\u0026#39; in d.keys()) .collect() ) [{'a': 1, 'b': 4}, {'a': 2, 'b': 3}, {'a': 3, 'b': 2}]  Mutate mutate可以在每条记录中，创建新字段、改写旧字段。  from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 3, \u0026#39;c\u0026#39;:4}, {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 6}] #新建了c和s字段 (Clumper(list_dicts) .mutate(c=lambda d: d[\u0026#39;a\u0026#39;] + d[\u0026#39;b\u0026#39;], s=lambda d: d[\u0026#39;a\u0026#39;] + d[\u0026#39;b\u0026#39;] + d[\u0026#39;c\u0026#39;]) .collect() ) [{'a': 1, 'b': 2, 'c': 3, 's': 6}, {'a': 2, 'b': 3, 'c': 5, 's': 10}, {'a': 1, 'b': 6, 'c': 7, 's': 14}]  Sort sort可以实现排序  from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 3}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 1}] (Clumper(list_dicts) .sort(lambda d: d[\u0026#39;b\u0026#39;]) #默认升序 .collect() ) [{'a': 2, 'b': 1}, {'a': 1, 'b': 2}, {'a': 3, 'b': 3}]  Select select挑选每条记录中的某个(些)字段  from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 3, \u0026#39;c\u0026#39;:4}, {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 6}] (Clumper(list_dicts) .select(\u0026#39;a\u0026#39;) .collect() ) [{'a': 1}, {'a': 2}, {'a': 1}]  Drop 剔除某个（些）字段。  from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 3, \u0026#39;c\u0026#39;:4}, {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 6}] (Clumper(list_dicts) .drop(\u0026#39;c\u0026#39;) .collect() ) [{'a': 1, 'b': 2}, {'a': 2, 'b': 3}, {'a': 1, 'b': 6}]  GroupBy 根据某个（些）字段对数据集进行分组，得到不同Group类的集合。一般与.agg()方法联合使用。  from clumper import Clumper grade_dicts = [ {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 98, \u0026#39;name\u0026#39;: \u0026#39;张三\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 88, \u0026#39;name\u0026#39;: \u0026#39;王五\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 99, \u0026#39;name\u0026#39;: \u0026#39;赵六\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 58, \u0026#39;name\u0026#39;: \u0026#39;李四\u0026#39;}] (Clumper(grade_dicts) .group_by(\u0026#34;gender\u0026#34;) .groups==(\u0026#39;gender\u0026#39;, ) ) True  Ungroup GroupBy的反操作  from clumper import Clumper grade_dicts = [ {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 98, \u0026#39;name\u0026#39;: \u0026#39;张三\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 88, \u0026#39;name\u0026#39;: \u0026#39;王五\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 99, \u0026#39;name\u0026#39;: \u0026#39;赵六\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 58, \u0026#39;name\u0026#39;: \u0026#39;李四\u0026#39;}] (Clumper(grade_dicts) .group_by(\u0026#34;gender\u0026#34;) .ungroup().groups == tuple() ) True  \nAbout Groups Agg 聚合描述性统计方法\nagg如下图，可以理解成三个步骤，即group-\u0026gt;split-\u0026gt;summary  常用的描述性统计函数有： mean、count、unqiue、n_unique、sum、min和max\n求学生的平均成绩、最优和最差成绩\nfrom clumper import Clumper grade_dicts = [{\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 98, \u0026#39;name\u0026#39;: \u0026#39;张三\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 88, \u0026#39;name\u0026#39;: \u0026#39;王五\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 99, \u0026#39;name\u0026#39;: \u0026#39;赵六\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 58, \u0026#39;name\u0026#39;: \u0026#39;李四\u0026#39;}] (Clumper(grade_dicts) .agg(mean_grade=(\u0026#39;grade\u0026#39;, \u0026#39;mean\u0026#39;), max_grade=(\u0026#39;grade\u0026#39;, \u0026#39;max\u0026#39;), min_grade=(\u0026#39;grade\u0026#39;, \u0026#39;min\u0026#39;)) .collect() ) [{'mean_grade': 85.75, 'max_grade': 99, 'min_grade': 58}]  求男生和女生各自的平均成绩、最优和最差成绩\nfrom clumper import Clumper grade_dicts = [{\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 98, \u0026#39;name\u0026#39;: \u0026#39;张三\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 88, \u0026#39;name\u0026#39;: \u0026#39;王五\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 99, \u0026#39;name\u0026#39;: \u0026#39;赵六\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 58, \u0026#39;name\u0026#39;: \u0026#39;李四\u0026#39;}] (Clumper(grade_dicts) .group_by(\u0026#39;gender\u0026#39;) .agg(mean_grade=(\u0026#39;grade\u0026#39;, \u0026#39;mean\u0026#39;), max_grade=(\u0026#39;grade\u0026#39;, \u0026#39;max\u0026#39;), min_grade=(\u0026#39;grade\u0026#39;, \u0026#39;min\u0026#39;)) .collect()) [{'gender': '男', 'mean_grade': 78, 'max_grade': 98, 'min_grade': 58}, {'gender': '女', 'mean_grade': 93.5, 'max_grade': 99, 'min_grade': 88}]  Collect 一般Clumper函数返回的结果显示为Clumper类，是看不到具体内容的。\ncollect作用主要是展开显示。  剔除重复 剔除重复内容  from clumper import Clumper data = [{\u0026#34;a\u0026#34;: 1}, {\u0026#34;a\u0026#34;: 2}, {\u0026#34;a\u0026#34;: 2}] (Clumper(data) .drop_duplicates() .collect() ) [{'a': 1}, {'a': 2}]   ### 什么是Group？ from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 9, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;} ] (Clumper(list_dicts) .group_by(\u0026#39;grp\u0026#39;) ) \u0026lt;Clumper groups=('grp',) len=5 @0x103cb0290\u0026gt;  当前的group以grp作为关键词  现在经过 .group_by('grp')操作后，说明你对每个grp组感兴趣。具体一点，一个组是{'grp': 'a'}, 另一个组是{'grp': 'b'}.\nAgg without groups  from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 9, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;} ] (Clumper(list_dicts) .agg(s=(\u0026#39;a\u0026#39;, \u0026#39;sum\u0026#39;), m=(\u0026#39;a\u0026#39;, \u0026#39;mean\u0026#39;)) .collect()) [{'s': 29, 'm': 5.8}]  with groups 分别计算组grp=a、组grp=b的sum和mean  from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 9, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;} ] (Clumper(list_dicts) .group_by(\u0026#39;grp\u0026#39;) .agg(s=(\u0026#39;a\u0026#39;, \u0026#39;sum\u0026#39;), m=(\u0026#39;a\u0026#39;, \u0026#39;mean\u0026#39;)) .collect()) [{'grp': 'a', 's': 18, 'm': 6}, {'grp': 'b', 's': 11, 'm': 5.5}]  agg内置的统计函数名 内置的统计函数，可直接通过字符串调用\n{ \u0026#34;mean\u0026#34;: mean, \u0026#34;count\u0026#34;: lambda d: len(d), \u0026#34;unique\u0026#34;: lambda d: list(set(d)), \u0026#34;n_unique\u0026#34;: lambda d: len(set(d)), \u0026#34;sum\u0026#34;: sum, \u0026#34;min\u0026#34;: min, \u0026#34;max\u0026#34;: max, \u0026#34;median\u0026#34;: median, \u0026#34;var\u0026#34;: variance, \u0026#34;std\u0026#34;: stdev, \u0026#34;values\u0026#34;: lambda d: d, \u0026#34;first\u0026#34;: lambda d: d[0], \u0026#34;last\u0026#34;: lambda d: d[-1], } Transform .transform()与.agg()类似。主要的区别是transform处理过程中，记录数和字段数不会出现压缩。\nwithout groups  from clumper import Clumper data = [{\u0026#34;a\u0026#34;: 6, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;}, {\u0026#34;a\u0026#34;: 2, \u0026#34;grp\u0026#34;: \u0026#34;b\u0026#34;}, {\u0026#34;a\u0026#34;: 7, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;}, {\u0026#34;a\u0026#34;: 9, \u0026#34;grp\u0026#34;: \u0026#34;b\u0026#34;}, {\u0026#34;a\u0026#34;: 5, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;}] (Clumper(data) .transform(s=(\u0026#34;a\u0026#34;, \u0026#34;sum\u0026#34;), u=(\u0026#34;a\u0026#34;, \u0026#34;unique\u0026#34;)) .collect()) [{'a': 6, 'grp': 'a', 's': 29, 'u': [2, 5, 6, 7, 9]}, {'a': 2, 'grp': 'b', 's': 29, 'u': [2, 5, 6, 7, 9]}, {'a': 7, 'grp': 'a', 's': 29, 'u': [2, 5, 6, 7, 9]}, {'a': 9, 'grp': 'b', 's': 29, 'u': [2, 5, 6, 7, 9]}, {'a': 5, 'grp': 'a', 's': 29, 'u': [2, 5, 6, 7, 9]}]  with groups  from clumper import Clumper data = [ {\u0026#34;a\u0026#34;: 6, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;}, {\u0026#34;a\u0026#34;: 2, \u0026#34;grp\u0026#34;: \u0026#34;b\u0026#34;}, {\u0026#34;a\u0026#34;: 7, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;}, {\u0026#34;a\u0026#34;: 9, \u0026#34;grp\u0026#34;: \u0026#34;b\u0026#34;}, {\u0026#34;a\u0026#34;: 5, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;} ] (Clumper(data) .group_by(\u0026#34;grp\u0026#34;) .transform(s=(\u0026#34;a\u0026#34;, \u0026#34;sum\u0026#34;), u=(\u0026#34;a\u0026#34;, \u0026#34;unique\u0026#34;)) .collect() ) [{'a': 6, 'grp': 'a', 's': 18, 'u': [5, 6, 7]}, {'a': 7, 'grp': 'a', 's': 18, 'u': [5, 6, 7]}, {'a': 5, 'grp': 'a', 's': 18, 'u': [5, 6, 7]}, {'a': 2, 'grp': 'b', 's': 11, 'u': [9, 2]}, {'a': 9, 'grp': 'b', 's': 11, 'u': [9, 2]}]  Mutate clumper库中的row_number可以给每条记录显示索引位置（第几个）。\nwithout groups  from clumper import Clumper from clumper.sequence import row_number list_dicts = [ {\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 4, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;} ] (Clumper(list_dicts) .mutate(index=row_number()) .collect() ) [{'a': 6, 'grp': 'a', 'index': 1}, {'a': 2, 'grp': 'b', 'index': 2}, {'a': 7, 'grp': 'a', 'index': 3}, {'a': 4, 'grp': 'b', 'index': 4}, {'a': 5, 'grp': 'a', 'index': 5}]  with groups  from clumper import Clumper from clumper.sequence import row_number list_dicts = [ {\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 4, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;} ] (Clumper(list_dicts) .group_by(\u0026#39;grp\u0026#39;) .mutate(index=row_number()) .collect() ) [{'a': 6, 'grp': 'a', 'index': 1}, {'a': 7, 'grp': 'a', 'index': 2}, {'a': 5, 'grp': 'a', 'index': 3}, {'a': 2, 'grp': 'b', 'index': 1}, {'a': 4, 'grp': 'b', 'index': 2}]  Sort 排序, 默认升序\nwithout groups  from clumper import Clumper list_dicts = [{\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 9, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}] (Clumper(list_dicts) #根据字段a进行排序  .sort(key=lambda d: d[\u0026#39;a\u0026#39;]) .collect()) [{'a': 2, 'grp': 'b'}, {'a': 5, 'grp': 'a'}, {'a': 6, 'grp': 'a'}, {'a': 7, 'grp': 'a'}, {'a': 9, 'grp': 'b'}]  with groups  from clumper import Clumper list_dicts = [{\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 9, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}] (Clumper(list_dicts) .group_by(\u0026#39;grp\u0026#39;) .sort(key=lambda d: d[\u0026#39;a\u0026#39;]) .collect()) [{'a': 5, 'grp': 'a'}, {'a': 6, 'grp': 'a'}, {'a': 7, 'grp': 'a'}, {'a': 2, 'grp': 'b'}, {'a': 9, 'grp': 'b'}]  Ungroup 最后，如果你已经进行完了分组计算，想再次整合起来，取消分组状态，可以使用.ungroup()\n\nMerge Verbs  如果想将多个记录整理到一个记录中，有很多种实现方法。\nConcat 如果想垂直方向将多个记录堆叠，可以使用concat  from clumper import Clumper c1 = Clumper([{\u0026#34;a\u0026#34;: 1}]) c2 = Clumper([{\u0026#34;a\u0026#34;: 2}]) c3 = Clumper([{\u0026#34;a\u0026#34;: 3}]) c1.concat(c2).collect() [{'a': 1}, {'a': 2}]  #等同于c1.concat(c2).concat(c3).collect() c1.concat(c2, c3).collect() [{'a': 1}, {'a': 2}, {'a': 3}]  Joins Joins类似于数学里的交集、并集的，大致有以下四种，  left join 左连接，以左为主，表示以table1为主，关联上table2的数据，结果显示table1的所有数据，然后table2显示的是和table1有交集部分的数据。  from clumper import Clumper left = Clumper([ {\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 4}, {\u0026#34;a\u0026#34;: 2, \u0026#34;b\u0026#34;: 6}, {\u0026#34;a\u0026#34;: 3, \u0026#34;b\u0026#34;: 8}, ]) right = Clumper([ {\u0026#34;c\u0026#34;: 9, \u0026#34;b\u0026#34;: 4}, {\u0026#34;c\u0026#34;: 8, \u0026#34;b\u0026#34;: 5}, {\u0026#34;c\u0026#34;: 7, \u0026#34;b\u0026#34;: 6}, ]) #根据b进行左右两表的合并 result = left.inner_join(right, mapping={\u0026#34;b\u0026#34;: \u0026#34;b\u0026#34;}) result.collect() [{'a': 1, 'b': 4, 'c': 9}, {'a': 2, 'b': 6, 'c': 7}]  inner join 内连接， 交集\n from clumper import Clumper left = Clumper([ {\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;:4}, {\u0026#34;a\u0026#34;: 2, \u0026#34;b\u0026#34;:6}, {\u0026#34;a\u0026#34;: 3, \u0026#34;b\u0026#34;:8}, ]) right = Clumper([ {\u0026#34;c\u0026#34;: 9, \u0026#34;b\u0026#34;:4}, {\u0026#34;c\u0026#34;: 8, \u0026#34;b\u0026#34;:5}, {\u0026#34;c\u0026#34;: 7, \u0026#34;b\u0026#34;:6}, ]) result = left.inner_join(right, mapping={\u0026#34;b\u0026#34;: \u0026#34;b\u0026#34;}) result.collect() [{'a': 1, 'b': 4, 'c': 9}, {'a': 2, 'b': 6, 'c': 7}]   \nNested Data 由于嵌套数据序列确实具有各种形状和大小，因此该库提供了各种方法来帮助您将数据重塑为不同的格式。 本文档将演示这些方法的工作原理。\nExplode 炸裂（展开）  from clumper import Clumper data = [{\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: [80, 99], \u0026#39;name\u0026#39;:[\u0026#39;张三\u0026#39;, \u0026#39;李四\u0026#39;]}] (Clumper(data) .explode(\u0026#39;name\u0026#39;, \u0026#39;grade\u0026#39;) .collect() ) [{'gender': '男', 'grade': 80, 'name': '张三'}, {'gender': '男', 'grade': 99, 'name': '张三'}, {'gender': '男', 'grade': 80, 'name': '李四'}, {'gender': '男', 'grade': 99, 'name': '李四'}]  from clumper import Clumper data = [{\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: [80, 99], \u0026#39;name\u0026#39;:[\u0026#39;张三\u0026#39;, \u0026#39;李四\u0026#39;]}] #.explode(\u0026#39;name\u0026#39;, \u0026#39;grade\u0026#39;)略微有些区别 #请查看两者运行结果 (Clumper(data) .explode(item=\u0026#39;name\u0026#39;, val=\u0026#39;grade\u0026#39;) .collect() ) [{'gender': '男', 'item': '张三', 'val': 80}, {'gender': '男', 'item': '张三', 'val': 99}, {'gender': '男', 'item': '李四', 'val': 80}, {'gender': '男', 'item': '李四', 'val': 99}]  Unpack 与explode类似  from clumper import Clumper list_dicts = { \u0026#39;a\u0026#39;: 1, \u0026#39;rows\u0026#39;: [{\u0026#39;b\u0026#39;: 2, \u0026#39;c\u0026#39;: 3}, {\u0026#39;b\u0026#39;: 3}, {\u0026#39;b\u0026#39;: 4}] } (Clumper(list_dicts) .unpack(\u0026#39;rows\u0026#39;) .collect() ) [{'a': 1, 'b': 2, 'c': 3}, {'a': 1, 'b': 3}, {'a': 1, 'b': 4}]  Flatten keys  from clumper import Clumper data = { \u0026#39;feature_1\u0026#39;: {\u0026#39;propery_1\u0026#39;: 1, \u0026#39;property_2\u0026#39;: 2}, \u0026#39;feature_2\u0026#39;: {\u0026#39;propery_1\u0026#39;: 3, \u0026#39;property_2\u0026#39;: 4}, \u0026#39;feature_3\u0026#39;: {\u0026#39;propery_1\u0026#39;: 5, \u0026#39;property_2\u0026#39;: 6}, } (Clumper(data, listify=False) .flatten_keys() .collect() ) [{'propery_1': 1, 'property_2': 2, 'key': 'feature_1'}, {'propery_1': 3, 'property_2': 4, 'key': 'feature_2'}, {'propery_1': 5, 'property_2': 6, 'key': 'feature_3'}]  \nSummary Methods Clumper支持常用的统计性方法，诸如mean、max、min等\nmean  from clumper import Clumper list_of_dicts = [ {\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7} ] Clumper(list_of_dicts).mean(\u0026#34;a\u0026#34;) 3.5  Clumper(list_of_dicts).mean(\u0026#34;b\u0026#34;) 6.666666666666667  count 统计记录数  from clumper import Clumper list_of_dicts = [ {\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7} ] #含有a的一共有多少条记录 Clumper(list_of_dicts).count(\u0026#34;a\u0026#34;) 4  Clumper(list_of_dicts).count(\u0026#34;b\u0026#34;) 3  unique 汇总某字段不重样的值的种类，如[a, b, a, a]，经过unique后，返回[a, b]  from clumper import Clumper list_of_dicts = [{\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}] Clumper(list_of_dicts).unique(\u0026#34;a\u0026#34;) [2, 3, 7]  Clumper(list_of_dicts).unique(\u0026#34;b\u0026#34;) [6, 7]  n_unique 统计某字段对应的值一种有多少种  from clumper import Clumper list_of_dicts = [ {\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7} ] Clumper(list_of_dicts).n_unique(\u0026#34;a\u0026#34;) 3  Clumper(list_of_dicts).n_unique(\u0026#34;b\u0026#34;) 2  min  from clumper import Clumper list_of_dicts = [{\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}] Clumper(list_of_dicts).min(\u0026#34;a\u0026#34;) 2  Clumper(list_of_dicts).min(\u0026#34;b\u0026#34;) 6  max  from clumper import Clumper list_of_dicts = [{\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}] Clumper(list_of_dicts).max(\u0026#34;a\u0026#34;) 7  Clumper(list_of_dicts).max(\u0026#34;b\u0026#34;) 7  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/clumper_pipeline/","summary":"Clumper可以用来处理嵌套样式的json数据结构。\n代码下载 Getting Started 安装 !pip3 install clumper Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple Collecting clumper Downloading https://pypi.tuna.tsinghua.edu.cn/packages/70/62/0731ab9b48c91132aff487217980dcb147ffc0922a278adc05986f6a8d4b/clumper-0.2.13-py2.py3-none-any.whl (21 kB) Installing collected packages: clumper Successfully installed clumper-0.2.13 \u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.1.2 is available. You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m  为了展示Clumper如何工作，我准备了pokemon.json， 由列表组成(该列表由多个字典组成)，下面是pokemon.json部分内容\nimport json with open(\u0026#39;pokemon.json\u0026#39;) as jsonf: pokemon = json.loads(jsonf.read()) pokemon[:2] [{'name': 'Bulbasaur', 'type': ['Grass', 'Poison'], 'total': 318, 'hp': 45, 'attack': 49}, {'name': 'Ivysaur', 'type': ['Grass', 'Poison'], 'total': 405, 'hp': 60, 'attack': 62}]  我们准备的pokemon.","title":"Clumper库|dplyr样式的Python包"},{"content":"Typora简介 Typora是一个所见即所得的Markdown格式文本编辑器，支持Windows、macOS和GNU/Linux操作系统，拼写检查、自定义CSS样式、数学公式渲染（通过MathJax）等特性。\n如果你还不知道Typora，请访问Typora — a markdown editor, markdown reader.\n主题安装方法  下载本主题的压缩文件Latest release 打开Typora, 点击菜单栏的偏好设置-外观-打开主题文件夹 将解压后的文件复制到主题文件夹下(压缩包包含mlike文件夹、 mlike.css mlike-light.css、mlike-dark.css) 重新启动Typora，点击菜单栏的主题-Mlike Light或者Mlike Dark  具体的安装方法可查看 Install Theme (typora.io)\nTypora Themes 下面15个主题是大邓最喜欢的主题\n Autumnus Adark Drake FluentLight Jamstatic LessLight LessLightPrint Mo NewPrint OptAutumnus OrangeHeart PixII Torillic Vue Whitey  1.Autumnus  2.Adark  3.Drake  4.FluentLight   5.Jamstatic  6.LessLight  7.LessLightPrint  8.Mo  9.NewPrint  10.OptAutumnus  11.OrangeHeart  12.PixII  13.Torillic  14.Vue  15.Whitey  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/my-favorite-typora-themes/","summary":"Typora简介 Typora是一个所见即所得的Markdown格式文本编辑器，支持Windows、macOS和GNU/Linux操作系统，拼写检查、自定义CSS样式、数学公式渲染（通过MathJax）等特性。\n如果你还不知道Typora，请访问Typora — a markdown editor, markdown reader.\n主题安装方法  下载本主题的压缩文件Latest release 打开Typora, 点击菜单栏的偏好设置-外观-打开主题文件夹 将解压后的文件复制到主题文件夹下(压缩包包含mlike文件夹、 mlike.css mlike-light.css、mlike-dark.css) 重新启动Typora，点击菜单栏的主题-Mlike Light或者Mlike Dark  具体的安装方法可查看 Install Theme (typora.io)\nTypora Themes 下面15个主题是大邓最喜欢的主题\n Autumnus Adark Drake FluentLight Jamstatic LessLight LessLightPrint Mo NewPrint OptAutumnus OrangeHeart PixII Torillic Vue Whitey  1.Autumnus  2.Adark  3.Drake  4.FluentLight   5.Jamstatic  6.LessLight  7.LessLightPrint  8.Mo  9.NewPrint  10.OptAutumnus  11.OrangeHeart  12.PixII  13.","title":"我最喜欢的15个Typora主题"},{"content":"代码下载 点击此处下载代码\n本文B站视频 https://www.bilibili.com/video/BV1AE411r7ph\n一、知识准备  python语法基本知识 https://www.bilibili.com/video/BV1eb411h7sP/ python网络爬虫 https://www.bilibili.com/video/BV1AE411r7ph/  二、网址规律分析 2.1 上交所  上交所多为GET请求方法，伪码\nimport requests url = \u0026#39;上交所网址规律\u0026#39; headers = \u0026#39;你的浏览器useragent(带referer)\u0026#39; cookies = \u0026#39;你的cookies\u0026#39; resp = requests.get(url, headers=headers, cookies=cookies) \n2.2 深交所   深交所多为POST请求方法，伪码\nimport requests url = \u0026#39;深交所网址规律\u0026#39; headers = \u0026#39;你的浏览器useragent(带referer)\u0026#39; cookies = \u0026#39;你的cookies\u0026#39; param = \u0026#39;form data构造的字典，补全网址规律\u0026#39; resp = requests.get(url, headers=headers, cookies=cookies, data=param) \n三、定位pdf相关数据 访问得到的结果均为json数据，解析定位方法可使用python的字典方法。\n  四、存储数据 几千个pdf数据量很容易达到1000+M，如果长时间自动下载容易失败。\n建议先获取所有公司相关信息，存储到csv中。\n后续再单独使用pandas读取，逐一下载pdf。\n注意，这里推荐使用csv新的语法\nwith open(\u0026#39;你的csv文件路径\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) as csvf: #csv文件内的字段名 fieldnames = [\u0026#39;title\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;link\u0026#39;, \u0026#39;content\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() #访问 url = \u0026#39;网址\u0026#39; resp = requests.get(url,....) #定位 for company in resp.json()[\u0026#39;data\u0026#39;]: #解析数据 row = dict() row[\u0026#39;title\u0026#39;] = \u0026#39;采集到的标题\u0026#39; row[\u0026#39;date\u0026#39;] = \u0026#39;采集到的日期\u0026#39; row[\u0026#39;link\u0026#39;] = \u0026#39;采集到的pdf链接\u0026#39; row[\u0026#39;content\u0026#39;] = \u0026#39;采集到的内容\u0026#39; #写入csv writer.writerow(row) \n五、批量下载pdf 以深交所为例，已经采集到深圳交易所.csv，现在下载只需要执行\n## 下载 import requests import pandas as pd def download(link, fpath): \u0026#34;\u0026#34;\u0026#34; 下载多媒体及文件 link： 多媒体文件链接（结尾有文件格式名） fpath: 存储文件的路径（结尾有文件格式名） \u0026#34;\u0026#34;\u0026#34; resp = requests.get(link) #获取到二进制数据 binarydata = resp.content #以二进制形式将数据流存入fname中 with open(fpath, \u0026#39;wb\u0026#39;) as f: f.write(binarydata) df = pd.read_csv(\u0026#39;深圳交易所.csv\u0026#39;) for title, link in zip(df[\u0026#39;title\u0026#39;], df[\u0026#39;link\u0026#39;]): fpath = \u0026#39;深圳/{title}.PDF\u0026#39;.format(title=title) download(link, fpath) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/stock_exchange_prospectus/","summary":"代码下载 点击此处下载代码\n本文B站视频 https://www.bilibili.com/video/BV1AE411r7ph\n一、知识准备  python语法基本知识 https://www.bilibili.com/video/BV1eb411h7sP/ python网络爬虫 https://www.bilibili.com/video/BV1AE411r7ph/  二、网址规律分析 2.1 上交所  上交所多为GET请求方法，伪码\nimport requests url = \u0026#39;上交所网址规律\u0026#39; headers = \u0026#39;你的浏览器useragent(带referer)\u0026#39; cookies = \u0026#39;你的cookies\u0026#39; resp = requests.get(url, headers=headers, cookies=cookies) \n2.2 深交所   深交所多为POST请求方法，伪码\nimport requests url = \u0026#39;深交所网址规律\u0026#39; headers = \u0026#39;你的浏览器useragent(带referer)\u0026#39; cookies = \u0026#39;你的cookies\u0026#39; param = \u0026#39;form data构造的字典，补全网址规律\u0026#39; resp = requests.get(url, headers=headers, cookies=cookies, data=param) \n三、定位pdf相关数据 访问得到的结果均为json数据，解析定位方法可使用python的字典方法。\n  四、存储数据 几千个pdf数据量很容易达到1000+M，如果长时间自动下载容易失败。\n建议先获取所有公司相关信息，存储到csv中。\n后续再单独使用pandas读取，逐一下载pdf。\n注意，这里推荐使用csv新的语法\nwith open(\u0026#39;你的csv文件路径\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) as csvf: #csv文件内的字段名 fieldnames = [\u0026#39;title\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;link\u0026#39;, \u0026#39;content\u0026#39;] writer = csv.","title":"深交所上交所pdf批量下载"},{"content":"代码下载 点击此处下载代码\n问题 如何将数据中，同一股票代码同一年的某个字段加总成一条？\n我想把某公司同一年的数据var加总到一起\n思路 可以通过pandas库实现这个需求\n 获取公司股票代码列表 获取某公司年份列表 对某个公司同年的var进行加总 (var代指一个字段或变量) for循环对所有的公司重复2-3操作  准备数据 import numpy as np import pandas as pd #强制股票代码转为str类型 df = pd.read_excel(\u0026#39;data.xlsx\u0026#39;, converters={\u0026#34;code\u0026#34;: str}) df.head()    .dataframe tbody tr th:only-of-type { vertical-align: middle; } 实验代码 1. 获取公司股票代码列表 codes = df.code.unique() codes array(['000001', '000002', '000004', '000005', '000006'], dtype=object)  2. 获取某公司年份列表 以000001为例\nyears = set(df[df[\u0026#39;code\u0026#39;]==\u0026#39;000001\u0026#39;][\u0026#39;year\u0026#39;].values) years {2000, 2002, 2007, 2008, 2010, 2013, 2019}  3. 对某个公司同年的baladded进行加总 以000001公司2000年为例\nndf = df[df[\u0026#39;code\u0026#39;]==\u0026#39;000001\u0026#39;] ndf.head()   ndf[ndf[\u0026#39;year\u0026#39;]==2000]   ndf[ndf[\u0026#39;year\u0026#39;]==2000][\u0026#39;baladded\u0026#39;] 0 -65856130.0 1 -65856130.0 Name: baladded, dtype: float64  ndf[ndf[\u0026#39;year\u0026#39;]==2000][\u0026#39;baladded\u0026#39;].sum() -131712260.0  for循环对所有的公司重复2-3操作 汇总代码\nresults = [] codes = df.code.unique() for code in codes: years = set(df[df[\u0026#39;code\u0026#39;]==code][\u0026#39;year\u0026#39;].values) for year in years: ndf = df[df[\u0026#39;code\u0026#39;]==code] baladded_sum = ndf[ndf[\u0026#39;year\u0026#39;]==year][\u0026#39;baladded\u0026#39;].sum() data = (code, year, baladded_sum) results.append(data) result_df = pd.DataFrame(results, columns=[\u0026#39;code\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;baladded_sum\u0026#39;]) result_df   保存结果\nresult_df.to_excel(\u0026#39;result.xlsx\u0026#39;, index=False) \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/pandas_example_company_analysis/","summary":"代码下载 点击此处下载代码\n问题 如何将数据中，同一股票代码同一年的某个字段加总成一条？\n我想把某公司同一年的数据var加总到一起\n思路 可以通过pandas库实现这个需求\n 获取公司股票代码列表 获取某公司年份列表 对某个公司同年的var进行加总 (var代指一个字段或变量) for循环对所有的公司重复2-3操作  准备数据 import numpy as np import pandas as pd #强制股票代码转为str类型 df = pd.read_excel(\u0026#39;data.xlsx\u0026#39;, converters={\u0026#34;code\u0026#34;: str}) df.head()    .dataframe tbody tr th:only-of-type { vertical-align: middle; } 实验代码 1. 获取公司股票代码列表 codes = df.code.unique() codes array(['000001', '000002', '000004', '000005', '000006'], dtype=object)  2. 获取某公司年份列表 以000001为例\nyears = set(df[df[\u0026#39;code\u0026#39;]==\u0026#39;000001\u0026#39;][\u0026#39;year\u0026#39;].values) years {2000, 2002, 2007, 2008, 2010, 2013, 2019}  3.","title":"Pandas小案例 | 对某公司同年的某指标批量汇总"},{"content":"在B站看到一位博主用Hugo制作个人博客的视频，感觉挺简单的，真的十几分钟就能看到云端出现自己的博客，当然了想让自己的博客更美观更炫酷，精雕细琢会花很多功夫。现在大家看到的效果，大邓用了一整天的时间，一点点修饰改动出来的。\n 1. 安装Hugo 这里以Mac为例，安装Hugo，命令行输入\nbrew install hugo \n2. 新建Hugo项目 切换到桌面(我喜欢把项目放到桌面), 命令行执行\ncd desktop 新建一个叫做MyBlog的hugo项目文件夹，命令行执行\nhugo new site MyBlog 现在可以在桌面看到一个MyBlog文件夹，接下来切换工作目录到MyBlog\ncd MyBlog 记者目前我们的命令行处于MyBlog的根目录 , 接下来下载网站主题\n\n3. Academic主题下载 Hugo有很多主题，我选择的 https://themes.gohugo.io/academic/，\n在命令行逐行执行下方命令\ncd themes git clone https://github.com/gcushen/hugo-academic.git 我们可以在 MyBlog/themes 看到多了一个 hugo-academic文件夹，把hugo-academic改为academic ，现在网站已经建立好了\n这里切换回项目根目录MyBlog\ncd .. 命令行执行pwd，检查一下目录\npwd 得到\n/Users/电脑用户名/desktop/MyBlog \n4. 启动本地博客 现在我们以academic主题为例，启动博客\n命令行执行\nhugo server -t academic --buildDrafts 补充: t的意思是主题\n执行后，在命令行中会提示我们\nhttp://localhost:1313/ 在浏览器中复制粘贴上方的链接，我们的Blog毛坯房搭建好了~\n5. 在本地新建一篇文章 依旧是MyBlog根目录，命令行执行\nhugo new post/first-article.md 在MyBlog/content内新生成了一个post文件夹，并且post内有了一个first-article.md文件。\n接下来就是在first-article.md内用markdown方式写内容即可。\n我们测试一下现在的网站,继续回到MyBlog根目录，命令行执行\nhugo server -t academic --buildDrafts 在浏览器中我们可以看到有First Ariticle的文章。\n6. 将本地博客部署到服务器 在github新建一个仓库，仓库名命名方式\n\u0026lt;你的github用户名\u0026gt;.github.io 比如我的github账号名是thunderhit，那么仓库名为\nthunderhit.github.io 在MyBlog根目录，命令行执行\nhugo --theme=academic --baseUrl=\u0026#39;/\u0026#39; --buildDrafts 注意: 主题academic, 网站地址 https://hidadeng.github.io/ ，你们根据自己需要改成自己的仓库名\n现在我们在MyBlog中多了一个public文件夹，其中有我们新建的文章内容。\n绑定public与github仓库\n命令行切换到public目录，初始化git\ncd public git init git add . git commit -m \u0026#39;我的hugo博客第一次提交\u0026#39; 把public与远程github仓库关联\n依次执行（大家的github地址略微不同，需要改动一下)\ngit remote add origin git@github.com:hidadeng/hidadeng.github.io.git git push -u origin master 命令行上传完毕后，在浏览器网址栏打开链接 https://hidadeng.github.io/\n就可以看到我们自己的博客了~\n\n更多 如果大家想学仔细学Hugo，推荐大家看B站Up主：ianianying的视频\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/hugo_website_test/","summary":"在B站看到一位博主用Hugo制作个人博客的视频，感觉挺简单的，真的十几分钟就能看到云端出现自己的博客，当然了想让自己的博客更美观更炫酷，精雕细琢会花很多功夫。现在大家看到的效果，大邓用了一整天的时间，一点点修饰改动出来的。\n 1. 安装Hugo 这里以Mac为例，安装Hugo，命令行输入\nbrew install hugo \n2. 新建Hugo项目 切换到桌面(我喜欢把项目放到桌面), 命令行执行\ncd desktop 新建一个叫做MyBlog的hugo项目文件夹，命令行执行\nhugo new site MyBlog 现在可以在桌面看到一个MyBlog文件夹，接下来切换工作目录到MyBlog\ncd MyBlog 记者目前我们的命令行处于MyBlog的根目录 , 接下来下载网站主题\n\n3. Academic主题下载 Hugo有很多主题，我选择的 https://themes.gohugo.io/academic/，\n在命令行逐行执行下方命令\ncd themes git clone https://github.com/gcushen/hugo-academic.git 我们可以在 MyBlog/themes 看到多了一个 hugo-academic文件夹，把hugo-academic改为academic ，现在网站已经建立好了\n这里切换回项目根目录MyBlog\ncd .. 命令行执行pwd，检查一下目录\npwd 得到\n/Users/电脑用户名/desktop/MyBlog \n4. 启动本地博客 现在我们以academic主题为例，启动博客\n命令行执行\nhugo server -t academic --buildDrafts 补充: t的意思是主题\n执行后，在命令行中会提示我们\nhttp://localhost:1313/ 在浏览器中复制粘贴上方的链接，我们的Blog毛坯房搭建好了~\n5. 在本地新建一篇文章 依旧是MyBlog根目录，命令行执行\nhugo new post/first-article.md 在MyBlog/content内新生成了一个post文件夹，并且post内有了一个first-article.","title":"使用Hugo框架建立个人网站"},{"content":"参考GreatDanton 项目，丢弃庞杂丑陋的浏览器收藏夹， 打造简洁科研浏览器首页。\n每个人都可以自定义自己的浏览器首页，替换默认浏览器首页/起始页。 该项目可在任何现代浏览器上运行，只需将index.html设置为主页并添加自己的链接 到index.html\n点击查看效果 \n一、功能  搜索(google) 日历(倒计时) 待办事项 支持DIY自己的首页  二、截图 各位可根据自身科研或者工作需要，更改成自己的标签名，替换为自己需要的网站网址\n   日历, 在index.html中可以设置自己认为最最重要的日子\n\u0026lt;script\u0026gt; // 显示日期时钟 showClock(); //在日历上显示倒计时 countDown({\u0026#34;y\u0026#34;: 2022, \u0026#34;m\u0026#34;: 6, \u0026#34;d\u0026#34;: 1 }, \u0026#34;Message when your countdown ends\u0026#34;); \u0026lt;/script\u0026gt;   待办事项\n 三、添加链接 网站链接需要直接加到index.html的 \u0026lt;div class=\u0026quot;slides-container\u0026quot;标签内，以学术生活如下\n\u0026lt;div class=\u0026#34;slide\u0026#34; name=\u0026#34;科研\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;part\u0026#34;\u0026gt; \u0026lt;h1\u0026gt; 数据库 \u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;links\u0026#34;\u0026gt; \u0026lt;a href=\u0026#39;https://scholar.google.com/\u0026#39;\u0026gt;Google Scholar\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://webofknowledge.com/\u0026#39;\u0026gt; Web of Science \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://search.ebscohost.com/\u0026#39;\u0026gt; EBSCO \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://scholar.cnki.net/\u0026#39;\u0026gt; CNKI Scholar \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://www.ssrn.com/index.cfm/en/\u0026#39;\u0026gt;SSRN\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://www.proquest.com/site/pqdd_unavailable.shtml\u0026#39;\u0026gt; ProQuest \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://pubsonline.informs.org/\u0026#39;\u0026gt; Informs \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://www.emerald.com/insight/\u0026#39;\u0026gt; Emerald Insight \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://link.springer.com/\u0026#39;\u0026gt; Springer \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;http://www.cnki.net/\u0026#39;\u0026gt; 知网 \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;   四、使用方法  点击时钟，弹出日期框和倒计时信息。 搜索框支持!command搜索语法. 例如    搜索框命令 目标网站 例子 作用     !scholar google scholar !scholar python 在谷歌学术中搜python相关信息   !book 豆瓣读书 !book python 在豆瓣读书中搜python相关书籍信息   !movie 豆瓣电影 !movie 悬崖之上 在豆瓣电影中搜悬崖之上相关电影信息   !zhihu 知乎 !zhihu python 在知乎网站搜python相关信息   !youtube youtube !youtube python 在youtube搜索python相关视频   !taobao 淘宝 !taobao python 在淘宝搜python相关商品服务   !jd 京东 !jd python 在京东搜python相关商品服务   !bilibili B站 !bilibili python 在B站搜索python相关视频   !github github !github python 在github上搜python相关仓库代码等信息   !mail qq邮箱 !mail 默认打开qq邮箱      五、项目代码结构 ├── components │ ├── calendar.js │ ├── clock.js │ ├── countdown.js │ ├── notes.js │ ├── search-box.js │ └── slides.js ├── css │ ├── main_min.css │ └── main.scss ├── index.html  Components文件夹: 含有浏览器主页所需的所有组件js文件。  calendar.js -\u0026gt; 日历 clock.js -\u0026gt; 时钟 countdown.js -\u0026gt; 倒计时 notes.js -\u0026gt;待办事项todo list search-box.js -\u0026gt; 主页搜索框 slides.js -\u0026gt; 页面滑动功能   index.html -\u0026gt; 浏览器主页入口  六、代码获取  直接下载 github更多代码  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/brower_startpage/","summary":"参考GreatDanton 项目，丢弃庞杂丑陋的浏览器收藏夹， 打造简洁科研浏览器首页。\n每个人都可以自定义自己的浏览器首页，替换默认浏览器首页/起始页。 该项目可在任何现代浏览器上运行，只需将index.html设置为主页并添加自己的链接 到index.html\n点击查看效果 \n一、功能  搜索(google) 日历(倒计时) 待办事项 支持DIY自己的首页  二、截图 各位可根据自身科研或者工作需要，更改成自己的标签名，替换为自己需要的网站网址\n   日历, 在index.html中可以设置自己认为最最重要的日子\n\u0026lt;script\u0026gt; // 显示日期时钟 showClock(); //在日历上显示倒计时 countDown({\u0026#34;y\u0026#34;: 2022, \u0026#34;m\u0026#34;: 6, \u0026#34;d\u0026#34;: 1 }, \u0026#34;Message when your countdown ends\u0026#34;); \u0026lt;/script\u0026gt;   待办事项\n 三、添加链接 网站链接需要直接加到index.html的 \u0026lt;div class=\u0026quot;slides-container\u0026quot;标签内，以学术生活如下\n\u0026lt;div class=\u0026#34;slide\u0026#34; name=\u0026#34;科研\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;part\u0026#34;\u0026gt; \u0026lt;h1\u0026gt; 数据库 \u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;links\u0026#34;\u0026gt; \u0026lt;a href=\u0026#39;https://scholar.google.com/\u0026#39;\u0026gt;Google Scholar\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://webofknowledge.com/\u0026#39;\u0026gt; Web of Science \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://search.ebscohost.com/\u0026#39;\u0026gt; EBSCO \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://scholar.","title":"Hi Research 定义自己的科研首页"},{"content":"Jaal是基于Dash和Visdcc构建的可交互的Python社交网络库。由于底层使用了dash，所以我们可以认为jaal更像是一个仪表盘。基于此，jaal也提供了多种处理网络数据的可选项，例如搜索、过滤、给节点（边）上色等。所有的操作，两三行代码即可搞定。\n!pip3 install jaal \n一、快速上手 本文准备了《权利的游戏》的节点nodes.csv和边edges.csv数据， 可以使用jaal的plot()函数绘制《权利的游戏》关系网络图。 必须有的字段，这里加粗了 nodes.csv数据含\n from: 节点 to: 节点 weight: 边权重 strenth:  edges.csv数据\n id 节点id，姓名 gender 节点的性别   import pandas as pd from jaal import Jaal edge_df = pd.read_csv(\u0026#39;edges.csv\u0026#39;) node_df = pd.read_csv(\u0026#39;nodes.csv\u0026#39;) Jaal(edge_df, node_df).plot() 运行代码后，会生成一个本地服务链接，例如 http://127.0.0.1:8050/ ， 点击链接，浏览器就能看到\n 二、Jaal功能  设置面板 Jaal运行产生的浏览器界面左侧会有一个设置面板，可以对数据进行搜索、筛选、上色。 搜索 可以高亮搜索到的节点 过滤 支持pandas的query语法 上色 基于类别，对节点、边进行上色。能最多支持20个类别，即节点、边数据允许有20种属性  三、 案例 3.1 搜索 第一个选项是搜索，我们可以在其中搜索图中的特定节点。 它支持在节点标签上逐字符搜索。 以下是我们尝试搜索“ Arya”的示例\n 3.2 过滤 接下来，我们进行过滤。 Jaal支持在节点和边要素上都进行过滤的选项。 为此，我们提供了单独的文本区域。 下面我们可以看到节点和边缘过滤查询的实时效果。\n 3.3 染色 最后，我们可能希望查看任何功能的整体分布，而不是进行过滤。 目前，Jaal通过提供根据任何分类特征为节点或边缘着色的选项来解决此问题。 我们可以在下面看到一个真实的例子。\n 四、 代码获取  直接下载 github更多代码  \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/jaal_social_network_visualization/","summary":"Jaal是基于Dash和Visdcc构建的可交互的Python社交网络库。由于底层使用了dash，所以我们可以认为jaal更像是一个仪表盘。基于此，jaal也提供了多种处理网络数据的可选项，例如搜索、过滤、给节点（边）上色等。所有的操作，两三行代码即可搞定。\n!pip3 install jaal \n一、快速上手 本文准备了《权利的游戏》的节点nodes.csv和边edges.csv数据， 可以使用jaal的plot()函数绘制《权利的游戏》关系网络图。 必须有的字段，这里加粗了 nodes.csv数据含\n from: 节点 to: 节点 weight: 边权重 strenth:  edges.csv数据\n id 节点id，姓名 gender 节点的性别   import pandas as pd from jaal import Jaal edge_df = pd.read_csv(\u0026#39;edges.csv\u0026#39;) node_df = pd.read_csv(\u0026#39;nodes.csv\u0026#39;) Jaal(edge_df, node_df).plot() 运行代码后，会生成一个本地服务链接，例如 http://127.0.0.1:8050/ ， 点击链接，浏览器就能看到\n 二、Jaal功能  设置面板 Jaal运行产生的浏览器界面左侧会有一个设置面板，可以对数据进行搜索、筛选、上色。 搜索 可以高亮搜索到的节点 过滤 支持pandas的query语法 上色 基于类别，对节点、边进行上色。能最多支持20个类别，即节点、边数据允许有20种属性  三、 案例 3.1 搜索 第一个选项是搜索，我们可以在其中搜索图中的特定节点。 它支持在节点标签上逐字符搜索。 以下是我们尝试搜索“ Arya”的示例\n 3.2 过滤 接下来，我们进行过滤。 Jaal支持在节点和边要素上都进行过滤的选项。 为此，我们提供了单独的文本区域。 下面我们可以看到节点和边缘过滤查询的实时效果。","title":"Jaal库~轻松绘制动态社交网络关系图"},{"content":"代码获取  直接下载 github更多代码  Kaggle是个很棒的地方,对于数据科学家和机器学习工程师来说，这是一个知识的金矿。可以在同一地点找到由本领域专家带来的高质量，高效，可重现，很棒的代码。自推出以来，它已经举办了164场比赛。这些比赛吸引了来自世界各地的专家和专家加入该平台。结果，每场比赛以及Kaggle提供的大量开源数据集都有许多高质量的笔记本和脚本。\n在数据科学之旅的开始，我将去Kaggle查找数据集以练习我的技能。每当我查看其他内核时，我都会对代码的复杂性感到不知所措，然后马上回避。\n但是现在，我发现自己花费了大量时间阅读其他笔记本并提交竞赛文件。有时候，有些东西值得您度过整个周末。有时，我会发现简单但致命的有效代码技巧和最佳实践，这些技巧和最佳实践只能通过观察其他专家来学习。\n在整个系列中，您会发现我在典型的数据科学工作流程中可能有用的任何内容，包括与通用库相关的代码快捷方式，Kaggle的顶级行业专家遵循的最佳实践等，这些都是我在学习过程中学到的。\n1. 只绘制相关系数矩阵的下三角部分 好的相关矩阵可以说明数据集中目标变量之间的相关性\nimport pandas as pd df = pd.read_csv(\u0026#39;data/melbourne_housing_raw.csv\u0026#39;) df.head()   import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(16, 12)) cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\u0026#34;light\u0026#34;, as_cmap=True) sns.heatmap(df.corr(), center=0, annot=True, fmt=\u0026#39;.2f\u0026#39;, square=True, cmap=cmap) plt.show()   但上图中，数据集中存在大量的特征，导致相似矩阵过于庞大，让人看起来不知所措。\n相关矩阵大部分沿主对角线对称，因此它们包含重复数据。 同样，对角线本身也没有用。 让我们看看如何只绘制有用的一半：\nimport numpy as np # 计算相关系数 matrix = df.corr() # 创建遮罩（为了只显示下三角） mask = np.triu(np.ones_like(matrix, dtype=bool)) # 定制调色板 cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\u0026#34;light\u0026#34;, as_cmap=True) # 设定图片尺寸 plt.figure(figsize=(16, 12)) # 绘制相似矩阵热力图 sns.heatmap(matrix, mask=mask, center=0, annot=True, fmt=\u0026#39;.2f\u0026#39;, square=True, cmap=cmap) plt.show()   由此产生的可视化图更容易解释并且没有视觉干扰干扰。\n 首先，我们使用DataFrame的.corr方法构建相关矩阵。 然后，我们使用dtype设置为bool的np.ones_like函数来创建一个True矩阵，其形状与DataFrame相同：  np.ones((5, 5)) array([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]])  np.ones_like(np.ones((4, 4)), dtype=bool) array([[ True, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True]])  将布尔方阵传递给Numpy的.triu函数，该函数将返回一个二维布尔蒙版，其中包含矩阵下三角的False值。\nnp.triu(np.ones_like(np.ones((4, 4)), dtype=bool)) array([[ True, True, True, True], [False, True, True, True], [False, False, True, True], [False, False, False, True]])  然后，我们可以将其传递给Seaborn的heatmap函数，以根据此蒙版对矩阵进行可视化\nns.heatmap(matrix, mask=mask, center=0, annot=True, fmt=\u0026#39;.2f\u0026#39;, square=True, cmap=cmap) \n2. value_counts考虑缺失值 使用value_counts时，可将dropna设置为False来查看任何列中缺失值的比例。通过确定缺失值的比例，可以决定是否丢弃含有缺失值的记录。\n#字段CouncilArea的数据分布情况 df.CouncilArea.value_counts(dropna=False, normalize=True).head() Boroondara City Council 0.105431 Darebin City Council 0.081791 Moreland City Council 0.060877 Glen Eira City Council 0.057549 Melbourne City Council 0.056000 Name: CouncilArea, dtype: float64  但是，如果要查看所有列中缺失值的比例，则value_counts不是最佳选择。 相反，您可以执行以下操作：\n  首先，通过将缺失值的数量除以DataFrame的长度来找到比例。\n  然后，您可以过滤掉0％的列，即i。 e。 只选择缺少值的列。\n  missing_props = df.isna().sum() / len(df) missing_props[missing_props \u0026gt; 0].sort_values(ascending=False) BuildingArea 0.605761 YearBuilt 0.553863 Landsize 0.338813 Car 0.250394 Bathroom 0.235993 Bedroom2 0.235735 Longtitude 0.228821 Lattitude 0.228821 Price 0.218321 Propertycount 0.000086 Regionname 0.000086 CouncilArea 0.000086 Postcode 0.000029 Distance 0.000029 dtype: float64  3. 使用Pandas的Styler 我们中的许多人从未意识到pandas的巨大潜力。pandas的一个被低估且经常被忽视的功能是其对DataFrames进行样式设置的能力。 使用pandas DataFrames的.style属性，可以将条件设计和样式应用于它们。\n作为第一个示例，让我们看看如何根据每个单元格的值来更改背景颜色：\ndiamonds = pd.read_csv(\u0026#39;data/diamonds.csv\u0026#39;) diamonds.head()   pd.crosstab(diamonds.cut, diamonds.clarity).style.background_gradient(cmap=\u0026#39;rocket_r\u0026#39;)   几乎没有使用Seaborn的热图功能的热图。 在这里，我们使用pd.crosstab对钻石切割(cut)和净度(clarity)的每种组合进行计数。\n将.style.background_gradient与调色板配合使用，您可以轻松地发现哪种组合出现得最多。 仅从上面的DataFrame中，我们可以看到大多数钻石都是“ VS2”净度类型。\n我们甚至可以通过在交叉表中找到每种钻石切割cut和净度clarity组合的平均价格来进一步做到这一点：\npd.crosstab(diamonds.cut, diamonds.clarity, aggfunc=np.mean, values=diamonds.price).style.background_gradient(cmap=\u0026#39;rocket_r\u0026#39;)   通过将.format方法与格式字符串{：.2f}链接起来，我们指定了2个浮点数的精度。\nagg_prices = pd.crosstab(diamonds.cut, diamonds.clarity, aggfunc=np.mean, values=diamonds.price).style.background_gradient(cmap=\u0026#39;rocket_r\u0026#39;) agg_prices.format(\u0026#39;{:.2f}\u0026#39;)   4. matplotlib默认全局设置 在进行探索性数据分析时，您可能想对所有绘图应用自定义调色板，对刻度标签使用更大的字体，更改图例的位置，使用固定的图形大小等。\n对绘图自定义参数的更改是一项非常无聊，重复且耗时的任务。 幸运的是，您可以使用Matplotlib的rcParams为绘图设置全局配置。\nrcParams只是一个普通的Python字典，其中包含Matplotlib的默认设置：\nfrom matplotlib import rcParams import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) rcParams RcParams({'_internal.classic_mode': False, 'agg.path.chunksize': 0, 'animation.avconv_args': [], 'animation.avconv_path': 'avconv', 'animation.bitrate': -1, 'animation.codec': 'h264', 'animation.convert_args': [], 'animation.convert_path': 'convert', 'animation.embed_limit': 20.0, 'animation.ffmpeg_args': [], 'animation.ffmpeg_path': 'ffmpeg', 'animation.frame_format': 'png', 'animation.html': 'none', 'animation.html_args': [], 'animation.writer': 'ffmpeg', 'axes.autolimit_mode': 'data', 'axes.axisbelow': 'line', 'axes.edgecolor': 'black', 'axes.facecolor': 'white', 'axes.formatter.limits': [-7, 7], 'axes.formatter.min_exponent': 0, 'axes.formatter.offset_threshold': 4, 'axes.formatter.use_locale': False, 'axes.formatter.use_mathtext': False, 'axes.formatter.useoffset': True, 'axes.grid': False, 'axes.grid.axis': 'both', 'axes.grid.which': 'major', 'axes.labelcolor': 'black', 'axes.labelpad': 4.0, 'axes.labelsize': 'medium', 'axes.labelweight': 'normal', 'axes.linewidth': 0.8, 'axes.prop_cycle': cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']), 'axes.spines.bottom': True, 'axes.spines.left': True, 'axes.spines.right': True, 'axes.spines.top': True, 'axes.titlepad': 6.0, 'axes.titlesize': 'large', 'axes.titleweight': 'normal', 'axes.unicode_minus': True, 'axes.xmargin': 0.05, 'axes.ymargin': 0.05, 'axes3d.grid': True, 'backend': 'module://ipykernel.pylab.backend_inline', 'backend_fallback': True, 'boxplot.bootstrap': None, 'boxplot.boxprops.color': 'black', 'boxplot.boxprops.linestyle': '-', 'boxplot.boxprops.linewidth': 1.0, 'boxplot.capprops.color': 'black', 'boxplot.capprops.linestyle': '-', 'boxplot.capprops.linewidth': 1.0, 'boxplot.flierprops.color': 'black', 'boxplot.flierprops.linestyle': 'none', 'boxplot.flierprops.linewidth': 1.0, 'boxplot.flierprops.marker': 'o', 'boxplot.flierprops.markeredgecolor': 'black', 'boxplot.flierprops.markeredgewidth': 1.0, 'boxplot.flierprops.markerfacecolor': 'none', 'boxplot.flierprops.markersize': 6.0, 'boxplot.meanline': False, 'boxplot.meanprops.color': 'C2', 'boxplot.meanprops.linestyle': '--', 'boxplot.meanprops.linewidth': 1.0, 'boxplot.meanprops.marker': '^', 'boxplot.meanprops.markeredgecolor': 'C2', 'boxplot.meanprops.markerfacecolor': 'C2', 'boxplot.meanprops.markersize': 6.0, 'boxplot.medianprops.color': 'C1', 'boxplot.medianprops.linestyle': '-', 'boxplot.medianprops.linewidth': 1.0, 'boxplot.notch': False, 'boxplot.patchartist': False, 'boxplot.showbox': True, 'boxplot.showcaps': True, 'boxplot.showfliers': True, 'boxplot.showmeans': False, 'boxplot.vertical': True, 'boxplot.whiskerprops.color': 'black', 'boxplot.whiskerprops.linestyle': '-', 'boxplot.whiskerprops.linewidth': 1.0, 'boxplot.whiskers': 1.5, 'contour.corner_mask': True, 'contour.negative_linestyle': 'dashed', 'datapath': '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/mpl-data', 'date.autoformatter.day': '%Y-%m-%d', 'date.autoformatter.hour': '%m-%d %H', 'date.autoformatter.microsecond': '%M:%S.%f', 'date.autoformatter.minute': '%d %H:%M', 'date.autoformatter.month': '%Y-%m', 'date.autoformatter.second': '%H:%M:%S', 'date.autoformatter.year': '%Y', 'docstring.hardcopy': False, 'errorbar.capsize': 0.0, 'examples.directory': '', 'figure.autolayout': False, 'figure.constrained_layout.h_pad': 0.04167, 'figure.constrained_layout.hspace': 0.02, 'figure.constrained_layout.use': False, 'figure.constrained_layout.w_pad': 0.04167, 'figure.constrained_layout.wspace': 0.02, 'figure.dpi': 72.0, 'figure.edgecolor': (1, 1, 1, 0), 'figure.facecolor': (1, 1, 1, 0), 'figure.figsize': [6.0, 4.0], 'figure.frameon': True, 'figure.max_open_warning': 20, 'figure.subplot.bottom': 0.125, 'figure.subplot.hspace': 0.2, 'figure.subplot.left': 0.125, 'figure.subplot.right': 0.9, 'figure.subplot.top': 0.88, 'figure.subplot.wspace': 0.2, 'figure.titlesize': 'large', 'figure.titleweight': 'normal', 'font.cursive': ['Apple Chancery', 'Textile', 'Zapf Chancery', 'Sand', 'Script MT', 'Felipa', 'cursive'], 'font.family': ['sans-serif'], 'font.fantasy': ['Comic Sans MS', 'Chicago', 'Charcoal', 'Impact', 'Western', 'Humor Sans', 'xkcd', 'fantasy'], 'font.monospace': ['DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Computer Modern Typewriter', 'Andale Mono', 'Nimbus Mono L', 'Courier New', 'Courier', 'Fixed', 'Terminal', 'monospace'], 'font.sans-serif': ['DejaVu Sans', 'Bitstream Vera Sans', 'Computer Modern Sans Serif', 'Lucida Grande', 'Verdana', 'Geneva', 'Lucid', 'Arial', 'Helvetica', 'Avant Garde', 'sans-serif'], 'font.serif': ['DejaVu Serif', 'Bitstream Vera Serif', 'Computer Modern Roman', 'New Century Schoolbook', 'Century Schoolbook L', 'Utopia', 'ITC Bookman', 'Bookman', 'Nimbus Roman No9 L', 'Times New Roman', 'Times', 'Palatino', 'Charter', 'serif'], 'font.size': 10.0, 'font.stretch': 'normal', 'font.style': 'normal', 'font.variant': 'normal', 'font.weight': 'normal', 'grid.alpha': 1.0, 'grid.color': '#b0b0b0', 'grid.linestyle': '-', 'grid.linewidth': 0.8, 'hatch.color': 'black', 'hatch.linewidth': 1.0, 'hist.bins': 10, 'image.aspect': 'equal', 'image.cmap': 'viridis', 'image.composite_image': True, 'image.interpolation': 'nearest', 'image.lut': 256, 'image.origin': 'upper', 'image.resample': True, 'interactive': True, 'keymap.all_axes': ['a'], 'keymap.back': ['left', 'c', 'backspace', 'MouseButton.BACK'], 'keymap.copy': ['ctrl+c', 'cmd+c'], 'keymap.forward': ['right', 'v', 'MouseButton.FORWARD'], 'keymap.fullscreen': ['f', 'ctrl+f'], 'keymap.grid': ['g'], 'keymap.grid_minor': ['G'], 'keymap.help': ['f1'], 'keymap.home': ['h', 'r', 'home'], 'keymap.pan': ['p'], 'keymap.quit': ['ctrl+w', 'cmd+w', 'q'], 'keymap.quit_all': ['W', 'cmd+W', 'Q'], 'keymap.save': ['s', 'ctrl+s'], 'keymap.xscale': ['k', 'L'], 'keymap.yscale': ['l'], 'keymap.zoom': ['o'], 'legend.borderaxespad': 0.5, 'legend.borderpad': 0.4, 'legend.columnspacing': 2.0, 'legend.edgecolor': '0.8', 'legend.facecolor': 'inherit', 'legend.fancybox': True, 'legend.fontsize': 'medium', 'legend.framealpha': 0.8, 'legend.frameon': True, 'legend.handleheight': 0.7, 'legend.handlelength': 2.0, 'legend.handletextpad': 0.8, 'legend.labelspacing': 0.5, 'legend.loc': 'best', 'legend.markerscale': 1.0, 'legend.numpoints': 1, 'legend.scatterpoints': 1, 'legend.shadow': False, 'legend.title_fontsize': None, 'lines.antialiased': True, 'lines.color': 'C0', 'lines.dash_capstyle': 'butt', 'lines.dash_joinstyle': 'round', 'lines.dashdot_pattern': [6.4, 1.6, 1.0, 1.6], 'lines.dashed_pattern': [3.7, 1.6], 'lines.dotted_pattern': [1.0, 1.65], 'lines.linestyle': '-', 'lines.linewidth': 1.5, 'lines.marker': 'None', 'lines.markeredgecolor': 'auto', 'lines.markeredgewidth': 1.0, 'lines.markerfacecolor': 'auto', 'lines.markersize': 6.0, 'lines.scale_dashes': True, 'lines.solid_capstyle': 'projecting', 'lines.solid_joinstyle': 'round', 'markers.fillstyle': 'full', 'mathtext.bf': 'sans:bold', 'mathtext.cal': 'cursive', 'mathtext.default': 'it', 'mathtext.fallback_to_cm': True, 'mathtext.fontset': 'dejavusans', 'mathtext.it': 'sans:italic', 'mathtext.rm': 'sans', 'mathtext.sf': 'sans', 'mathtext.tt': 'monospace', 'patch.antialiased': True, 'patch.edgecolor': 'black', 'patch.facecolor': 'C0', 'patch.force_edgecolor': False, 'patch.linewidth': 1.0, 'path.effects': [], 'path.simplify': True, 'path.simplify_threshold': 0.1111111111111111, 'path.sketch': None, 'path.snap': True, 'pdf.compression': 6, 'pdf.fonttype': 3, 'pdf.inheritcolor': False, 'pdf.use14corefonts': False, 'pgf.preamble': '', 'pgf.rcfonts': True, 'pgf.texsystem': 'xelatex', 'polaraxes.grid': True, 'ps.distiller.res': 6000, 'ps.fonttype': 3, 'ps.papersize': 'letter', 'ps.useafm': False, 'ps.usedistiller': False, 'savefig.bbox': None, 'savefig.directory': '~', 'savefig.dpi': 'figure', 'savefig.edgecolor': 'white', 'savefig.facecolor': 'white', 'savefig.format': 'png', 'savefig.frameon': True, 'savefig.jpeg_quality': 95, 'savefig.orientation': 'portrait', 'savefig.pad_inches': 0.1, 'savefig.transparent': False, 'scatter.edgecolors': 'face', 'scatter.marker': 'o', 'svg.fonttype': 'path', 'svg.hashsalt': None, 'svg.image_inline': True, 'text.antialiased': True, 'text.color': 'black', 'text.hinting': 'auto', 'text.hinting_factor': 8, 'text.latex.preamble': '', 'text.latex.preview': False, 'text.latex.unicode': True, 'text.usetex': False, 'timezone': 'UTC', 'tk.window_focus': False, 'toolbar': 'toolbar2', 'verbose.fileo': 'sys.stdout', 'verbose.level': 'silent', 'webagg.address': '127.0.0.1', 'webagg.open_in_browser': True, 'webagg.port': 8988, 'webagg.port_retries': 50, 'xtick.alignment': 'center', 'xtick.bottom': True, 'xtick.color': 'black', 'xtick.direction': 'out', 'xtick.labelbottom': True, 'xtick.labelsize': 'medium', 'xtick.labeltop': False, 'xtick.major.bottom': True, 'xtick.major.pad': 3.5, 'xtick.major.size': 3.5, 'xtick.major.top': True, 'xtick.major.width': 0.8, 'xtick.minor.bottom': True, 'xtick.minor.pad': 3.4, 'xtick.minor.size': 2.0, 'xtick.minor.top': True, 'xtick.minor.visible': False, 'xtick.minor.width': 0.6, 'xtick.top': False, 'ytick.alignment': 'center_baseline', 'ytick.color': 'black', 'ytick.direction': 'out', 'ytick.labelleft': True, 'ytick.labelright': False, 'ytick.labelsize': 'medium', 'ytick.left': True, 'ytick.major.left': True, 'ytick.major.pad': 3.5, 'ytick.major.right': True, 'ytick.major.size': 3.5, 'ytick.major.width': 0.8, 'ytick.minor.left': True, 'ytick.minor.pad': 3.4, 'ytick.minor.right': True, 'ytick.minor.size': 2.0, 'ytick.minor.visible': False, 'ytick.minor.width': 0.6, 'ytick.right': False})  您可以调整每个图的任意参数设置，一般的图像设置如固定图形大小，刻度标签字体大小以及其他一些参数。\n通过这种设置，可以减少很多重复的代码量\n# 去掉顶部和右侧的线条Remove top and right spines rcParams[\u0026#39;axes.spines.top\u0026#39;] = False rcParams[\u0026#39;axes.spines.right\u0026#39;] = False # 设置图的尺寸Set fixed figure size rcParams[\u0026#39;figure.figsize\u0026#39;] = [12, 9] # 设置图片像素清晰度 Set dots per inch to 300, very high quality images rcParams[\u0026#39;figure.dpi\u0026#39;] = 300 # 设置自动调整布局Enable autolayout rcParams[\u0026#39;figure.autolayout\u0026#39;] = True # 设置全局字号Set global fontsize rcParams[\u0026#39;font.style\u0026#39;] = 16 # 刻度字号Fontsize of ticklabels rcParams[\u0026#39;xtick.labelsize\u0026#39;] = 10 rcParams[\u0026#39;ytick.labelsize\u0026#39;] = 10 \n5. Pandas全局设置 就像Matplotlib一样，pandas具有可以使用的全局设置。 当然，它们大多数与显示选项有关。\n get_option() - 获取pandas单个选项 set_option() — 设置pandas单个选项 reset_option() — 重置pandas选项值  我更喜欢显示所有的列，lets go\npd.set_option(\u0026#39;display.max_columns\u0026#39;, None) df.head()   \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/kaggle_best_practice_of_data_mining/","summary":"代码获取  直接下载 github更多代码  Kaggle是个很棒的地方,对于数据科学家和机器学习工程师来说，这是一个知识的金矿。可以在同一地点找到由本领域专家带来的高质量，高效，可重现，很棒的代码。自推出以来，它已经举办了164场比赛。这些比赛吸引了来自世界各地的专家和专家加入该平台。结果，每场比赛以及Kaggle提供的大量开源数据集都有许多高质量的笔记本和脚本。\n在数据科学之旅的开始，我将去Kaggle查找数据集以练习我的技能。每当我查看其他内核时，我都会对代码的复杂性感到不知所措，然后马上回避。\n但是现在，我发现自己花费了大量时间阅读其他笔记本并提交竞赛文件。有时候，有些东西值得您度过整个周末。有时，我会发现简单但致命的有效代码技巧和最佳实践，这些技巧和最佳实践只能通过观察其他专家来学习。\n在整个系列中，您会发现我在典型的数据科学工作流程中可能有用的任何内容，包括与通用库相关的代码快捷方式，Kaggle的顶级行业专家遵循的最佳实践等，这些都是我在学习过程中学到的。\n1. 只绘制相关系数矩阵的下三角部分 好的相关矩阵可以说明数据集中目标变量之间的相关性\nimport pandas as pd df = pd.read_csv(\u0026#39;data/melbourne_housing_raw.csv\u0026#39;) df.head()   import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(16, 12)) cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\u0026#34;light\u0026#34;, as_cmap=True) sns.heatmap(df.corr(), center=0, annot=True, fmt=\u0026#39;.2f\u0026#39;, square=True, cmap=cmap) plt.show()   但上图中，数据集中存在大量的特征，导致相似矩阵过于庞大，让人看起来不知所措。\n相关矩阵大部分沿主对角线对称，因此它们包含重复数据。 同样，对角线本身也没有用。 让我们看看如何只绘制有用的一半：\nimport numpy as np # 计算相关系数 matrix = df.corr() # 创建遮罩（为了只显示下三角） mask = np.triu(np.ones_like(matrix, dtype=bool)) # 定制调色板 cmap = sns.","title":"Kaggle数据挖掘最佳实践"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n学Python一段时间后，都会听到一句“Life is short, so we learn Python! ”，恭喜你选择Python这门强大而有趣的语言。\n输出数字a的绝对数 在学习代码之前，我给大家看一段话\n There is such a number a, if a is greater than or equal to 0, we will print a; if a is less than 0, we will print -a\n 相信大家一看就明白了这是求某数的绝对值方法的英文描述。下面我们用精炼的Python语言表示\na = -50 if a \u0026gt;= 0: print(a) else: print(-a) 50  Python号称是最说人话的编程语言，以最接近人类理解的方式构建代码。\nPython与英语对比 从上面的例子中，我们已经知道了Python和英语一样都是一种语言，学习语言就需要学习基本的知识点，包括背单词和了解语法。\n   英语 Python 例如     单词 数据类型 列表、字符串、字典等   语法 逻辑语句 if条件判读语句、for循环语句等    每天积累一点点 本部分非必须，仅仅为了展示python也可以作图\n#mac #!pip3 install matplotlib #win !pip install matplotlib Looking in indexes: https://mirrors.aliyun.com/pypi/simple/ Requirement already satisfied: matplotlib in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (3.2.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u0026gt;=2.0.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (2.4.7) Requirement already satisfied: cycler\u0026gt;=0.10 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (0.10.0) Requirement already satisfied: numpy\u0026gt;=1.11 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (1.18.2) Requirement already satisfied: kiwisolver\u0026gt;=1.0.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (1.2.0) Requirement already satisfied: python-dateutil\u0026gt;=2.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (2.8.1) Requirement already satisfied: six in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from cycler\u0026gt;=0.10-\u0026gt;matplotlib) (1.14.0)  import matplotlib.pyplot as plt import math %matplotlib inline ability = 1 scale = 1.01 records = [] days = range(1, 365) for day in days: ability = ability*scale records.append(ability) plt.plot(days, records) plt.title(\u0026#39;Be better everyday!\u0026#39;) Text(0.5, 1.0, 'Be better everyday!')   Python是面向对象的编程语言 面向对象是最难理解的部分，这里大家只需要记住\n 类型和对象是紧密绑定的，说对象就是在说类型。 不同的类型有不同的功能，都是为了更高效的实现人类需求或者数据分析需求     类型 实例(对象) 实例(对象) 方法     猪 村东头老王家的猪 把猪把粮食变成肉 猪.产肉   牛 村东头老张家的耕牛 把粮食变成畜力 牛.耕地   列表 hobbies = ['跑步', '乒乓球'， '篮球'，'篮球'] 统计某群体爱好的分布, 查看各爱好的人数 hobbies.count('篮球')   字符串 str1 = \u0026quot;Hello，World!\u0026quot; 将文本内容由World更改为Python str1.replace('World', 'Python')   字典 grade = {'David':98, 'Mary':88,...} 方便数据检索 grade.get('David')   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;    Python中的数据类型 不同的数据类型适合处理不同的数据，有不同的应用场景。\n   数据类型 例子     数字 a = 5   字符串 my_str = \u0026quot;abcdefg\u0026quot;   列表 my_list = [1, 2, 3]   元组 my_tuple = (1, 2, 3)   字典 my_dict = {'David': 25, 'Mark':30}   空值 None    了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-01-python-is-a-language/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n学Python一段时间后，都会听到一句“Life is short, so we learn Python! ”，恭喜你选择Python这门强大而有趣的语言。\n输出数字a的绝对数 在学习代码之前，我给大家看一段话\n There is such a number a, if a is greater than or equal to 0, we will print a; if a is less than 0, we will print -a\n 相信大家一看就明白了这是求某数的绝对值方法的英文描述。下面我们用精炼的Python语言表示\na = -50 if a \u0026gt;= 0: print(a) else: print(-a) 50  Python号称是最说人话的编程语言，以最接近人类理解的方式构建代码。\nPython与英语对比 从上面的例子中，我们已经知道了Python和英语一样都是一种语言，学习语言就需要学习基本的知识点，包括背单词和了解语法。\n   英语 Python 例如     单词 数据类型 列表、字符串、字典等   语法 逻辑语句 if条件判读语句、for循环语句等    每天积累一点点 本部分非必须，仅仅为了展示python也可以作图","title":"Python是一种语言"},{"content":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\nWin环境配置 注意：\n Anaconda和Python都是python。一定要记住二选其一，不要都安装，不然在学习python第三方包安装的时，你会遇到一些麻烦。 如果之前没安装过两个软件，建议从头到尾按照我的视频进行电脑配置。  Typora软件下载 链接:https://pan.baidu.com/s/17yNfZbkNbnYsBko4eai0Ig 密码:ceve\n Jupyter使用方法  一、Python安装 anaconda官网 https://www.anaconda.com/\n注意  慢一点按照视频操作 勾选 Add Anaconda 3 to the system PATH environment variable 勾选 Register Anaconda 3 as the system 3.9  \n二、pip配置 pip是python的命令行安装工具，可以帮我们安装第三方库。\n2.1 更改pip镜像 为了保证安装的速度和成功率，命令行执行\npip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ 2.2 使用方法 pip install packagename 2.3 第三方库安装方法   pip安装法\n 单个库的安装，命令行执行  pip3 install 库的名字   多个库的安装， 命令行执行  pip3 install -r requirements.txt      pypi本地安装\n  在https://pypi.org/ 搜库，点进去\n  找Download files，下载whl或压缩文件到桌面。例如文件名 xxx.whl\n  命令行依次执行\n   cd desktop\n  pip3 install xxx.whl\n      github本地安装（如github项目中存在setup.py文件，可以安装使用）\n  下载github项目至桌面，解压\n  命令依次执行 - cd desktop - python3 setup.py install\n  \n三、Jupyter notebook 3.1 安装 命令行执行\npip install jupyter 3.2 调用 命令行执行\njupyter notebook 3.3 常用快捷键    jupyter内快捷键 功能     ESC+A（ESC+B） 当前单元格上(下)新建一个新的Cell   D+D 删除当前单元格   Shift+Enter 执行单元格内的Python代码   ESC+M 单元格由代码模式转为标记模式    个人建议： Markdown语法特别好用，强烈建议学习，顺便安装一个Typora软件。\n\n四、Tips 环境配置太难，而且有时候电脑还会出现一些视频里出现不了的问题。这时不妨在淘宝搜python环境配置，寻找一对一远程协助\n 点击上方图片购买课程   了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-02-win-settings/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\nWin环境配置 注意：\n Anaconda和Python都是python。一定要记住二选其一，不要都安装，不然在学习python第三方包安装的时，你会遇到一些麻烦。 如果之前没安装过两个软件，建议从头到尾按照我的视频进行电脑配置。  Typora软件下载 链接:https://pan.baidu.com/s/17yNfZbkNbnYsBko4eai0Ig 密码:ceve\n Jupyter使用方法  一、Python安装 anaconda官网 https://www.anaconda.com/\n注意  慢一点按照视频操作 勾选 Add Anaconda 3 to the system PATH environment variable 勾选 Register Anaconda 3 as the system 3.9  \n二、pip配置 pip是python的命令行安装工具，可以帮我们安装第三方库。\n2.1 更改pip镜像 为了保证安装的速度和成功率，命令行执行\npip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ 2.2 使用方法 pip install packagename 2.3 第三方库安装方法   pip安装法\n 单个库的安装，命令行执行  pip3 install 库的名字   多个库的安装， 命令行执行  pip3 install -r requirements.","title":"Win电脑Python环境配置"},{"content":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\nMac环境配置 注意：\n Anaconda和Python都是python。一定要记住二选其一，不要都安装，不然在学习python第三方包安装的时，你会遇到一些麻烦。 如果之前没安装过两个软件，建议从头到尾按照我的视频进行电脑配置。  Typora软件下载 链接:https://pan.baidu.com/s/17yNfZbkNbnYsBko4eai0Ig 密码:ceve\n Jupyter使用方法  一、Python安装 anaconda官网 https://www.anaconda.com/\nmac自带python2，为了与python2区别，凡是在命令行中使用pip和python，我们都要加上3。\n安装成功的标准是命令行可以调用python3\n$ python3 \n命令行打开的方法 command+空格 启动 聚焦搜索Spotlight，再输入terminal\n\n二、pip3设置 pip3是python的命令行安装工具，可以帮我们安装第三方库。\n2.1 更改pip3镜像 为了保证安装的速度和成功率，命令行执行\npip3 config set global.index-url https://mirrors.aliyun.com/pypi/simple/ 2.2 使用方法 pip3 install packagename 2.3 第三方库安装方法   pip安装法\n 单个库的安装，命令行执行  pip3 install 库的名字   多个库的安装， 命令行执行  pip3 install -r requirements.txt      pypi本地安装\n  在https://pypi.org/ 搜库，点进去\n  找Download files，下载whl或压缩文件到桌面。例如文件名 xxx.whl\n  命令行依次执行\n   cd desktop\n  pip3 install xxx.whl\n      github本地安装（如github项目中存在setup.py文件，可以安装使用）\n  下载github项目至桌面，解压\n  命令依次执行 - cd desktop - python3 setup.py install\n  \n三、Jupyter notebook 3.1 安装 命令行执行\npip3 install jupyter 3.2 调用 命令行执行\njupyter notebook 3.3 常用快捷键    jupyter内快捷键 功能     ESC+A（ESC+B） 当前单元格上(下)新建一个新的Cell   D+D 删除当前单元格   Shift+Enter 执行单元格内的Python代码   ESC+M 单元格由代码模式转为Markdown标记模式    推荐： Markdown语法特别好用，强烈建议学习，顺便安装一个Typora软件。\n\n四、Tips 环境配置太难，而且有时候电脑还会出现一些视频里出现不了的问题。这时不妨在淘宝搜python环境配置，寻找一对一远程协助\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-03-mac-settings/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\nMac环境配置 注意：\n Anaconda和Python都是python。一定要记住二选其一，不要都安装，不然在学习python第三方包安装的时，你会遇到一些麻烦。 如果之前没安装过两个软件，建议从头到尾按照我的视频进行电脑配置。  Typora软件下载 链接:https://pan.baidu.com/s/17yNfZbkNbnYsBko4eai0Ig 密码:ceve\n Jupyter使用方法  一、Python安装 anaconda官网 https://www.anaconda.com/\nmac自带python2，为了与python2区别，凡是在命令行中使用pip和python，我们都要加上3。\n安装成功的标准是命令行可以调用python3\n$ python3 \n命令行打开的方法 command+空格 启动 聚焦搜索Spotlight，再输入terminal\n\n二、pip3设置 pip3是python的命令行安装工具，可以帮我们安装第三方库。\n2.1 更改pip3镜像 为了保证安装的速度和成功率，命令行执行\npip3 config set global.index-url https://mirrors.aliyun.com/pypi/simple/ 2.2 使用方法 pip3 install packagename 2.3 第三方库安装方法   pip安装法\n 单个库的安装，命令行执行  pip3 install 库的名字   多个库的安装， 命令行执行  pip3 install -r requirements.txt      pypi本地安装\n  在https://pypi.","title":"Mac电脑Python环境配置"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n网络爬虫与文本分析实际上主要是对字符串做的处理，一定要熟悉字符串哦\n\n字符串string 定义 字符串是以 单引号 或 双引号 或 三引号 括起来的任意文本，如\n \u0026lsquo;abc\u0026rsquo; \u0026ldquo;abc\u0026rdquo; \u0026lsquo;\u0026lsquo;\u0026lsquo;abc\u0026rsquo;\u0026rsquo;\u0026rsquo; \u0026ldquo;\u0026ldquo;\u0026ldquo;abc\u0026rdquo;\u0026rdquo;\u0026rdquo;  a = \u0026#39;abc\u0026#39; a 'abc'  \u0026#34;abc\u0026#34; 'abc'  \u0026#39;\u0026#39;\u0026#39;abc\u0026#39;\u0026#39;\u0026#39; 'abc'  \u0026#34;\u0026#34;\u0026#34;abc\u0026#34;\u0026#34;\u0026#34; 'abc'  print(\u0026#39;abc\u0026#39;) print(\u0026#34;abc\u0026#34;) print(\u0026#39;\u0026#39;\u0026#39;abc\u0026#39;\u0026#39;\u0026#39;) print(\u0026#34;\u0026#34;\u0026#34;abc\u0026#34;\u0026#34;\u0026#34;) abc abc abc abc  print(\u0026#39;abc\u0026#39;) print(\u0026#39;efg\u0026#39;) abc efg  注意:\n 这里所说的引号都是英文引号 '' 或 \u0026quot;\u0026quot; 本身不是字符串的一部分，因此，字符串 'abc' 只有a，b，c这3个字符。 notebook中只显示最后一个，所以这里用了两个print   **Tips: **这里所说的引号都是英文引号\n'' 或 \u0026quot;\u0026quot; 本身不是字符串的一部分，因此，字符串 'abc' 只有a，b，c这3个字符。\nnotebook中只显示最后一个，所以这里用了两个print\n \n拼接+ 按顺序拼接\na = \u0026#39;P\u0026#39; b = \u0026#39;ython\u0026#39; print(a+b) print(b+a) Python ythonP  a = \u0026#39;P\u0026#39; print(a+b) print(b+a) \n切片 切片就像切糕，把自己想要的那块切下来\n name = \u0026#39;My Name is Mike\u0026#39; name[0] 'M'  name[-15] 'M'  name[3:6] 'Nam'  name[-12:-9] 'Nam'  name[0:2] 'My'  name[:2] 'My'  name[5:] 'me is Mike'  name[-7:] 'is Mike'  \n切片总结    切片表达式 解读     从左往右 索引值从0开始，0表示\u0026rsquo;第一个'   从右向左 -1表示倒数第一个，-2表示倒数第二个   a:b 选取列表索引位置为a，a+1...b-2, b-1的值   a: 选取列表中a之后的所有元素(含a)   :b 选取列表中b之前的所有元素(不含b)    字符串常用方法 再次强调，数据类(型)与猪牛羊不同的动物类型一样，都有满足人类需要的特殊本领(方法)。方法可以理解为数据类(型)一种特殊的本性、属性、特性\n   字符串常用方法 功能     str.lower() 变小写   str.upper() 变大写   str.split(sep) 使用sep将字符串分割，默认sep为空格   str.replace(old, new) 将str中的old替换为new   str.format() 向str中填充内容    words = \u0026#39;Python is poweful!\u0026#39; words.lower() 'python is poweful!'  words.upper() 'PYTHON IS POWEFUL!'  words 'Python is poweful!'  words.split(\u0026#39; \u0026#39;) ['Python', 'is', 'poweful!']  words.replace(\u0026#39;Python\u0026#39;, \u0026#39;Python programing language\u0026#39;) 'Python programing language is poweful!'  需要发送每个员工的工资组成详情。\n\u0026#34;张三,你这个月的工资是2310元；以下是你的工资详情。。。。\u0026#34; \u0026#34;李四,你这个月的工资是3456元；以下是你的工资详情。。。。\u0026#34; \u0026#34;王五,你这个月的工资是2431元；以下是你的工资详情。。。。\u0026#34; 如何自动化自动化填充?\ntemplate = \u0026#39;{name},你这个月的工资是{salary}元；以下是你的工资详情\u0026#39; print(template.format(name=\u0026#39;张三\u0026#39;, salary=\u0026#39;2310\u0026#39;)) print(template.format(name=\u0026#39;李四\u0026#39;, salary=\u0026#39;3456\u0026#39;)) print(template.format(name=\u0026#39;王五\u0026#39;, salary=\u0026#39;2431\u0026#39;)) 张三,你这个月的工资是2310元；以下是你的工资详情 李四,你这个月的工资是3456元；以下是你的工资详情 王五,你这个月的工资是2431元；以下是你的工资详情  \n转义符\\ 如果字符串内部既包含 单引号 又包含 双引号， 会发生什么？\nprint(\u0026#39;I\u0026#39;m \u0026#34;OK\u0026#34;!\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-28-af5dc904b170\u0026gt;\u0026quot;, line 1 print('I'm \u0026quot;OK\u0026quot;!') ^ SyntaxError: invalid syntax  # 可以用 转义字符\\ 来标识，比如： print(\u0026#39;I\\\u0026#39;m \\\u0026#34;OK\\\u0026#34;!\u0026#39;) I'm \u0026quot;OK\u0026quot;!  常见的转义符还有\n \\n 换行 \\t 制表符 字符\\ 本身也要转义，所以 \\\\ 表示的字符就是 \\  可以试着自己运行下面代码，看看下面打印结果是?：\nprint(\u0026#39;Hello\\tWorld!\u0026#39;) print(\u0026#39;转义\\n换行!\u0026#39;) print(\u0026#39;反斜杠线\\\\\u0026#39;) \nprint(\u0026#39;Hello\\tWorld!\u0026#39;) print(\u0026#39;Hello World!\u0026#39;) Hello\tWorld! Hello World!  print(\u0026#39;转义\\n换行!\u0026#39;) 转义 换行!  print(\u0026#39;反斜杠线\\\\\u0026#39;) 反斜杠线\\  \nr 如果字符串里面有很多字符都需要转义，就需要加很多\\,\n为了简化，Python还允许用r''表示''内部的字符串默认不转义,例如\nprint(\u0026#39;\\\\\\t\\\\\u0026#39;) print(r\u0026#39;\\\\\\t\\\\\u0026#39;) print(\u0026#39;hello world!\u0026#39;) \nprint(\u0026#39;\\\\\\t\\\\\u0026#39;) \\\t\\  print(r\u0026#39;\\\\\\t\\\\\u0026#39;) \\\\\\t\\\\  \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-04-string/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n网络爬虫与文本分析实际上主要是对字符串做的处理，一定要熟悉字符串哦\n\n字符串string 定义 字符串是以 单引号 或 双引号 或 三引号 括起来的任意文本，如\n \u0026lsquo;abc\u0026rsquo; \u0026ldquo;abc\u0026rdquo; \u0026lsquo;\u0026lsquo;\u0026lsquo;abc\u0026rsquo;\u0026rsquo;\u0026rsquo; \u0026ldquo;\u0026ldquo;\u0026ldquo;abc\u0026rdquo;\u0026rdquo;\u0026rdquo;  a = \u0026#39;abc\u0026#39; a 'abc'  \u0026#34;abc\u0026#34; 'abc'  \u0026#39;\u0026#39;\u0026#39;abc\u0026#39;\u0026#39;\u0026#39; 'abc'  \u0026#34;\u0026#34;\u0026#34;abc\u0026#34;\u0026#34;\u0026#34; 'abc'  print(\u0026#39;abc\u0026#39;) print(\u0026#34;abc\u0026#34;) print(\u0026#39;\u0026#39;\u0026#39;abc\u0026#39;\u0026#39;\u0026#39;) print(\u0026#34;\u0026#34;\u0026#34;abc\u0026#34;\u0026#34;\u0026#34;) abc abc abc abc  print(\u0026#39;abc\u0026#39;) print(\u0026#39;efg\u0026#39;) abc efg  注意:\n 这里所说的引号都是英文引号 '' 或 \u0026quot;\u0026quot; 本身不是字符串的一部分，因此，字符串 'abc' 只有a，b，c这3个字符。 notebook中只显示最后一个，所以这里用了两个print   **Tips: **这里所说的引号都是英文引号\n'' 或 \u0026quot;\u0026quot; 本身不是字符串的一部分，因此，字符串 'abc' 只有a，b，c这3个字符。","title":"数据类型-字符串"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n 当你不知道用什么数据类型的时候，一定要记得list，大多数的脏活累活ta都帮你搞定\n \n列表list 定义 list是一种有序的集合,内部可以由任何数据类型的组成的\n现在有5位员工的汇总信息，\n   id 姓名 年龄 性别 爱好     1 David 25 Male Basketball, Pingpang, Hiking   2 Mary 23 Female Reading, Movies   3 Henry 23 Male Diving, Hiking   4 Swift 21 Male Football, Music   5 Lenard 26 Male Stay at Home    现在我们需要用一种格式去组织5位员工的信息，以列表为例\nnames = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] ages = [25, 23, 23, 21, 26] hobbies = [[\u0026#39;Basketball\u0026#39;, \u0026#39;Pingpang\u0026#39;, \u0026#39;Hiking\u0026#39;], [\u0026#39;Reading\u0026#39;, \u0026#39;Movies\u0026#39;], [\u0026#39;Diving\u0026#39;, \u0026#39;Hiking\u0026#39;], [\u0026#39;Football\u0026#39;, \u0026#39;Music\u0026#39;], [\u0026#39;Stay at Home\u0026#39;]] print(type(names)) print(type(ages)) print(type(hobbies)) print(names) print(ages) print(hobbies) \u0026lt;class 'list'\u0026gt; \u0026lt;class 'list'\u0026gt; \u0026lt;class 'list'\u0026gt; ['David', 'Mary', 'Henry', 'Swift', 'Lenard'] [25, 23, 23, 21, 26] [['Basketball', 'Pingpang', 'Hiking'], ['Reading', 'Movies'], ['Diving', 'Hiking'], ['Football', 'Music'], ['Stay at Home']]  **多想一下： **组织数据的方式有很多种，list也可以用不同的方式去组织，大家可以自己想一想。\n\n拼接 a1 = [\u0026#39;Michael\u0026#39;, \u0026#39;Bob\u0026#39;] a2 = [\u0026#39;David\u0026#39;, \u0026#39;Lee\u0026#39;] print(a1 + a2) print(a2 + a1) ['Michael', 'Bob', 'David', 'Lee'] ['David', 'Lee', 'Michael', 'Bob']  \n拆包 注意： 等号两边长度相同\nname, age = [\u0026#39;David\u0026#39;, 25] print(name) print(age) David 25  \n切片 列表的切片与字符串类似\n   id 姓名 年龄 性别 爱好 正索引 倒索引     1 David 25 Male Basketball, Pingpang, Hiking 0 -5   2 Mary 23 Female Reading, Movies 1 -4   3 Henry 23 Male Diving, Hiking 2 -3   4 Swift 21 Male Football, Music 3 -2   5 Lenard 26 Male Stay at Home 4 -1    names = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] names ['David', 'Mary', 'Henry', 'Swift', 'Lenard']  print(names[2]) print(names[-3]) Henry Henry  print(names[0:3]) print(names[-5:-2]) ['David', 'Mary', 'Henry'] ['David', 'Mary', 'Henry']  print(names[2:]) print(names[-3:]) ['Henry', 'Swift', 'Lenard'] ['Henry', 'Swift', 'Lenard']  print(names[:2]) print(names[:-3]) ['David', 'Mary'] ['David', 'Mary']  列表常用方法    常用方法 功能     list.append(a) 向list中添加元素a   list.extend(lst) 向list中添加列表lst   list.count(a) 统计list中a的个数    names = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] print(names) names.append(\u0026#39;Mary\u0026#39;) print(names) ['David', 'Mary', 'Henry', 'Swift', 'Lenard'] ['David', 'Mary', 'Henry', 'Swift', 'Lenard', 'Mary']  names = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] print(names) names.extend([\u0026#39;Mary\u0026#39;]) print(names) ['David', 'Mary', 'Henry', 'Swift', 'Lenard'] ['David', 'Mary', 'Henry', 'Swift', 'Lenard', 'Mary']  ages = [25, 23, 23, 21, 26] print(ages.count(23)) 2  one = [\u0026#39;David\u0026#39;] print(one[-1]) print(one[0]) David David  \n元组tuple 形似列表，也有\n 元组拼接 切片 拆包  name_list = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] name_tuple = (\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;) print(type(name_list)) print(type(name_tuple)) print(name_list) print(name_tuple) \u0026lt;class 'list'\u0026gt; \u0026lt;class 'tuple'\u0026gt; ['David', 'Mary', 'Henry', 'Swift', 'Lenard'] ('David', 'Mary', 'Henry', 'Swift', 'Lenard')  print(name_list==name_tuple) False  \n集合 names2 = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;David\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;David\u0026#39;] print(set(names2)) {'Swift', 'Mary', 'David'}  集合的方法  setdata.add(ele) 向setdata中加入ele  name_set = set(names2) print(name_set) name_set.add(\u0026#39;William\u0026#39;) print(name_set) {'Swift', 'Mary', 'David'} {'William', 'Swift', 'Mary', 'David'}  name_set[2] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-20-98cb669cc173\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 name_set[2] TypeError: 'set' object is not subscriptable  注意： 集合不能切片\n\n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-05-list-tuple-set/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n 当你不知道用什么数据类型的时候，一定要记得list，大多数的脏活累活ta都帮你搞定\n \n列表list 定义 list是一种有序的集合,内部可以由任何数据类型的组成的\n现在有5位员工的汇总信息，\n   id 姓名 年龄 性别 爱好     1 David 25 Male Basketball, Pingpang, Hiking   2 Mary 23 Female Reading, Movies   3 Henry 23 Male Diving, Hiking   4 Swift 21 Male Football, Music   5 Lenard 26 Male Stay at Home    现在我们需要用一种格式去组织5位员工的信息，以列表为例\nnames = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] ages = [25, 23, 23, 21, 26] hobbies = [[\u0026#39;Basketball\u0026#39;, \u0026#39;Pingpang\u0026#39;, \u0026#39;Hiking\u0026#39;], [\u0026#39;Reading\u0026#39;, \u0026#39;Movies\u0026#39;], [\u0026#39;Diving\u0026#39;, \u0026#39;Hiking\u0026#39;], [\u0026#39;Football\u0026#39;, \u0026#39;Music\u0026#39;], [\u0026#39;Stay at Home\u0026#39;]] print(type(names)) print(type(ages)) print(type(hobbies)) print(names) print(ages) print(hobbies) \u0026lt;class 'list'\u0026gt; \u0026lt;class 'list'\u0026gt; \u0026lt;class 'list'\u0026gt; ['David', 'Mary', 'Henry', 'Swift', 'Lenard'] [25, 23, 23, 21, 26] [['Basketball', 'Pingpang', 'Hiking'], ['Reading', 'Movies'], ['Diving', 'Hiking'], ['Football', 'Music'], ['Stay at Home']]  **多想一下： **组织数据的方式有很多种，list也可以用不同的方式去组织，大家可以自己想一想。","title":"数据类型-列表元组集合"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n 最有层次感的数据类型，特别干净整洁。在写爬虫时，我们最希望遇到的数据类型就是ta\n 下图就是最简单的字典的样式，键值对 key-value-pairs\n \n字典 字典是有key，有value的 key-value-pair类型数据（键-值对）\n   id 姓名 年龄 性别 爱好     1 David 25 Male Basketball, Pingpang, Hiking   2 Mary 23 Female Reading, Movies   3 Henry 23 Male Diving, Hiking    将上面的员工信息以关键词name、age、hobbies 组织成字典数据\n空字典 david = dict() print(david) {}  填充 david[\u0026#39;age\u0026#39;] = 23 print(david) {'age': 23}  david[\u0026#39;hobbies\u0026#39;] = [\u0026#39;Basketball\u0026#39;, \u0026#39;Pingpang\u0026#39;, \u0026#39;Hiking\u0026#39;] print(david) {'age': 23, 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}  david[\u0026#39;gender\u0026#39;] = \u0026#39;Male\u0026#39; print(david) {'age': 23, 'hobbies': ['Basketball', 'Pingpang', 'Hiking'], 'gender': 'Male'}  \n更新 david[\u0026#39;age\u0026#39;] = 25 print(david) {'age': 25, 'hobbies': ['Basketball', 'Pingpang', 'Hiking'], 'gender': 'Male'}  完整的信息 infos = {\u0026#39;David\u0026#39;:{\u0026#39;age\u0026#39;:25, \u0026#39;gender\u0026#39;:\u0026#39;Male\u0026#39;, \u0026#39;hobbies\u0026#39;:[\u0026#39;Basketball\u0026#39;, \u0026#39;Pingpang\u0026#39;, \u0026#39;Hiking\u0026#39;]}, \u0026#39;Mary\u0026#39;:{\u0026#39;age\u0026#39;:23, \u0026#39;gender\u0026#39;: \u0026#39;Female\u0026#39;, \u0026#39;hobbies\u0026#39;: [\u0026#39;Reading\u0026#39;, \u0026#39;Movies\u0026#39;]}, \u0026#39;Henry\u0026#39;:{\u0026#39;age\u0026#39;:23, \u0026#39;gender\u0026#39;: \u0026#39;Male\u0026#39;, \u0026#39;hobbies\u0026#39;: [\u0026#39;Diving\u0026#39;, \u0026#39;Hiking\u0026#39;]} } print(infos) {'David': {'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}, 'Mary': {'age': 23, 'gender': 'Female', 'hobbies': ['Reading', 'Movies']}, 'Henry': {'age': 23, 'gender': 'Male', 'hobbies': ['Diving', 'Hiking']}}  字典的方法    方法 效果     dictdata.items() 返回dictdata所有item   dictdata.keys() 返回dictdata的所有关键词   dictdata.values() 返回dictdata的所有值   dictdata.get(keystr) 获取关键词keystr对应的值   dictdata[keystr] 获取关键词keystr对应的值    infos.items() dict_items([('David', {'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}), ('Mary', {'age': 23, 'gender': 'Female', 'hobbies': ['Reading', 'Movies']}), ('Henry', {'age': 23, 'gender': 'Male', 'hobbies': ['Diving', 'Hiking']})])  #把infos.items()转化为列表 list(infos.items())[0] ('David', {'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']})  infos.keys() dict_keys(['David', 'Mary', 'Henry'])  infos.values() dict_values([{'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}, {'age': 23, 'gender': 'Female', 'hobbies': ['Reading', 'Movies']}, {'age': 23, 'gender': 'Male', 'hobbies': ['Diving', 'Hiking']}])  print(infos[\u0026#39;David\u0026#39;]) print(infos.get(\u0026#39;David\u0026#39;)) {'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']} {'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}  注意： 两种功能等同，但是get获取方法更加安全稳定。\n例如\nprint(infos[\u0026#39;Will\u0026#39;]) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) \u0026lt;ipython-input-16-f7c283c8ad8e\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 print(infos['Will']) KeyError: 'Will'  print(infos.get(\u0026#39;Will\u0026#39;)) None  了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-06-dict/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n 最有层次感的数据类型，特别干净整洁。在写爬虫时，我们最希望遇到的数据类型就是ta\n 下图就是最简单的字典的样式，键值对 key-value-pairs\n \n字典 字典是有key，有value的 key-value-pair类型数据（键-值对）\n   id 姓名 年龄 性别 爱好     1 David 25 Male Basketball, Pingpang, Hiking   2 Mary 23 Female Reading, Movies   3 Henry 23 Male Diving, Hiking    将上面的员工信息以关键词name、age、hobbies 组织成字典数据\n空字典 david = dict() print(david) {}  填充 david[\u0026#39;age\u0026#39;] = 23 print(david) {'age': 23}  david[\u0026#39;hobbies\u0026#39;] = [\u0026#39;Basketball\u0026#39;, \u0026#39;Pingpang\u0026#39;, \u0026#39;Hiking\u0026#39;] print(david) {'age': 23, 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}  david[\u0026#39;gender\u0026#39;] = \u0026#39;Male\u0026#39; print(david) {'age': 23, 'hobbies': ['Basketball', 'Pingpang', 'Hiking'], 'gender': 'Male'}","title":"数据类型-字典"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n布尔值Boolean 用于逻辑判断，一般与if结合使用。\na = True print(a) True  True True  False False  其他产生布尔值的方式\n 布尔运算 比较运算 成员运算  布尔运算 中学数学课里的且或非\n   运算符号 功能 例子 等于     x and y 且 True and False False   x or y 或 True or False True   not x 非 not True False    x = True and False print(x) False  x = True or False print(x) True  x = not True print(x) False  x = not False print(x) True  比较运算 注意： =和==的区别，=用来把某个值传给某个变量(赋值操作)，==用来判断两个值(变量)是否相等(判断操作)\n   比较运算符号 功能 例子 等于     == 相等 5==3 False   != 不等于 5!=3 True   \u0026gt; 大于 5\u0026gt;3 True   \u0026lt; 小于 5\u0026lt;3 False   \u0026lt;= 小于等于 5\u0026lt;=3 False   \u0026gt;= 小于 5\u0026gt;=3 True    a = 5 b = 3 x = a\u0026lt;b print(x) print(type(x)) False \u0026lt;class 'bool'\u0026gt;  print(5==5) True  print(5!=5) False  **注意:**比较符两侧必须为同样的数据类型\na = \u0026#39;5\u0026#39; b = 5 print(a\u0026gt;b) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-15-861d7a553a1d\u0026gt; in \u0026lt;module\u0026gt; 2 b = 5 3 ----\u0026gt; 4 print(a\u0026gt;b) TypeError: '\u0026gt;' not supported between instances of 'str' and 'int'  a = \u0026#39;5\u0026#39; b = \u0026#39;5\u0026#39; print(a==b) True  \n成员运算符in 用来判断某个值是否在集合中（这个集合可以使列表、元组、字符串等)\n   案例 结果     4 in [1,2,4] True   4 not in [1,2,4] False   3 in [1,2,4] False   3 not in [1,2,4] True    x = 4 in [1,2,4] x True  y = 4 not in [1,2,4] y False  \nNone 特殊的空值，类似于C语言中的Null。\nNone \u0026#39;\u0026#39; \n''  [] []  dict() {}  type(None) NoneType  \n##了解课程\n 点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-07-boolean-none/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n布尔值Boolean 用于逻辑判断，一般与if结合使用。\na = True print(a) True  True True  False False  其他产生布尔值的方式\n 布尔运算 比较运算 成员运算  布尔运算 中学数学课里的且或非\n   运算符号 功能 例子 等于     x and y 且 True and False False   x or y 或 True or False True   not x 非 not True False    x = True and False print(x) False  x = True or False print(x) True  x = not True print(x) False  x = not False print(x) True  比较运算 注意： =和==的区别，=用来把某个值传给某个变量(赋值操作)，==用来判断两个值(变量)是否相等(判断操作)","title":"数据类型-布尔值\u0026None"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\nif条件判断  condition为布尔值、布尔运算、成员运算符 通常我们理解的condition为布尔值\n#Tab condition = True if condition: print(\u0026#39;condition为True\u0026#39;) else: print(\u0026#39;condition为False\u0026#39;) condition为True  age = 17 if age\u0026gt;=18: print(\u0026#39;你是成年人了\u0026#39;) else: print(\u0026#39;你还是个孩子\u0026#39;) 你还是个孩子  age = 20 if age\u0026gt;=55: print(\u0026#39;老年人\u0026#39;) elif 35\u0026lt;=age\u0026lt;55: print(\u0026#39;中年\u0026#39;) elif 18\u0026lt;=age\u0026lt;35: print(\u0026#39;青年\u0026#39;) elif 0\u0026lt;=age\u0026lt;18: print(\u0026#39;儿童\u0026#39;) 青年  \n特殊的conditon  各种空值(空字符串、空列表等)作用等同于False 各种非空值，作用等同于True  a = None if a: print(\u0026#39;a是非空数据\u0026#39;) else: print(\u0026#39;a是空数据\u0026#39;) a是空数据  \nfor循环  重复做某件事 迭代出数据中的内容(元素)   上面这个图可以解读为 我们想对iterable这个集合中的每一个item:\n做点事(对item做操作)  重复做某事 问题1 计算1+2+3+\u0026hellip;+97+98+99+100=?\n1 + 2 = 3 3 + 3 = 6 6 + 4 = 10 10 + 5 = 15\nresult = 0 #int for i in range(1, 101): result = result + i print(result) 5050  迭代出数据中的内容 从某种“集合”（这个“集合”可以使list、set、tuple等），只要“集合”内部有多个成员就可以使用for循环迭代出内部的成员\nnames = [\u0026#39;David\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Mary\u0026#39;] for name in names: print(name) David Henry Mary  name = \u0026#39;David\u0026#39; for s in name: print(s) D a v i d  infos = {\u0026#39;David\u0026#39;:{\u0026#39;age\u0026#39;:25, \u0026#39;gender\u0026#39;:\u0026#39;Male\u0026#39;}, \u0026#39;Mary\u0026#39;:{\u0026#39;age\u0026#39;:23, \u0026#39;gender\u0026#39;: \u0026#39;Female\u0026#39;}, \u0026#39;Henry\u0026#39;:{\u0026#39;age\u0026#39;:23, \u0026#39;gender\u0026#39;: \u0026#39;Male\u0026#39;} } for item in infos.items(): print(item) ('David', {'age': 25, 'gender': 'Male'}) ('Mary', {'age': 23, 'gender': 'Female'}) ('Henry', {'age': 23, 'gender': 'Male'})  for name, info in infos.items(): print(name, info) David Mary Henry  \ntry-except 遇到无关紧要的bug，不会停下来，让程序有一定的容错能力。通俗点就是此处不留爷，自有留爷处，凡事别钻牛角尖。\nfor x in [1,2,0,2,1]: print(10/x) 10.0 5.0 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) \u0026lt;ipython-input-19-83bea9c92c0e\u0026gt; in \u0026lt;module\u0026gt; 1 for x in [1,2,0,2,1]: ----\u0026gt; 2 print(10/x) ZeroDivisionError: division by zero  for x in [1,2,0,2,1]: try: print(10/x) except: print(\u0026#39;0除错误\u0026#39;) #pass 10.0 5.0 0除错误 5.0 10.0  \n练习1 假设现在某人的能力为1， 每天比前一天进步0.01， 一年后他的能力是多少？\nability = 1 scale = 1.01 records = [] for i in range(365): ability = ability * 1.01 records.append(ability) print(records) [1.01, 1.0201, 1.030301, 1.04060401, 1.0510100501, 1.061520150601, 1.0721353521070098, 1.08285670562808, 1.0936852726843609, 1.1046221254112045, 1.1156683466653166, ...................36.30913774096189, 36.672229118371504, 37.03895140955522, 37.40934092365077, 37.783434332887275]  import matplotlib.pyplot as plt import math %matplotlib inline ability = 1 scale = 1.02 records = [] days = range(1, 365) for day in days: ability = ability*scale records.append(ability) plt.plot(days, records) plt.title(\u0026#39;Be better everyday!\u0026#39;) Text(0.5, 1.0, 'Be better everyday!')   \n安装包的方法  命令行执行 pip install packagename jupyter notebook的Cell中执行!pip install packagename 如果是mac，pip写成pip3  !pip install matplotlib Looking in indexes: https://mirrors.aliyun.com/pypi/simple/ Requirement already satisfied: matplotlib in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (3.2.1) Requirement already satisfied: kiwisolver\u0026gt;=1.0.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (1.2.0) Requirement already satisfied: python-dateutil\u0026gt;=2.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (2.8.1) Requirement already satisfied: cycler\u0026gt;=0.10 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (0.10.0) Requirement already satisfied: numpy\u0026gt;=1.11 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (1.18.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u0026gt;=2.0.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (2.4.7) Requirement already satisfied: six\u0026gt;=1.5 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from python-dateutil\u0026gt;=2.1-\u0026gt;matplotlib) (1.14.0)  练习2 打印九九乘法表格\n 用到的知识点:\n for循环 字符串format方法 print函数(涉及到end参数)  for row in range(1, 10): #print(row) for col in range(1, row+1): formula = \u0026#39;{col}*{row}={res}\u0026#39;.format(col=col, row=row, res=col*row) print(formula, end=\u0026#39;\\t\u0026#39;) print(\u0026#39;\u0026#39;) 1*1=1\t1*2=2\t2*2=4\t1*3=3\t2*3=6\t3*3=9\t1*4=4\t2*4=8\t3*4=12\t4*4=16\t1*5=5\t2*5=10\t3*5=15\t4*5=20\t5*5=25\t1*6=6\t2*6=12\t3*6=18\t4*6=24\t5*6=30\t6*6=36\t1*7=7\t2*7=14\t3*7=21\t4*7=28\t5*7=35\t6*7=42\t7*7=49\t1*8=8\t2*8=16\t3*8=24\t4*8=32\t5*8=40\t6*8=48\t7*8=56\t8*8=64\t1*9=9\t2*9=18\t3*9=27\t4*9=36\t5*9=45\t6*9=54\t7*9=63\t8*9=72\t9*9=81\t help(print) Help on built-in function print in module builtins: print(...) print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream.  for row in range(1, 10): for col in range(1, row+1): formula = \u0026#39;{col}*{row}={res}\u0026#39; content = formula.format(col=col, row=row, res=col*row) print(content, end=\u0026#39;\\t\u0026#39;) print() 1*1=1\t1*2=2\t2*2=4\t1*3=3\t2*3=6\t3*3=9\t1*4=4\t2*4=8\t3*4=12\t4*4=16\t1*5=5\t2*5=10\t3*5=15\t4*5=20\t5*5=25\t1*6=6\t2*6=12\t3*6=18\t4*6=24\t5*6=30\t6*6=36\t1*7=7\t2*7=14\t3*7=21\t4*7=28\t5*7=35\t6*7=42\t7*7=49\t1*8=8\t2*8=16\t3*8=24\t4*8=32\t5*8=40\t6*8=48\t7*8=56\t8*8=64\t1*9=9\t2*9=18\t3*9=27\t4*9=36\t5*9=45\t6*9=54\t7*9=63\t8*9=72\t9*9=81\t \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-08-logical-if-else-try-except/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\nif条件判断  condition为布尔值、布尔运算、成员运算符 通常我们理解的condition为布尔值\n#Tab condition = True if condition: print(\u0026#39;condition为True\u0026#39;) else: print(\u0026#39;condition为False\u0026#39;) condition为True  age = 17 if age\u0026gt;=18: print(\u0026#39;你是成年人了\u0026#39;) else: print(\u0026#39;你还是个孩子\u0026#39;) 你还是个孩子  age = 20 if age\u0026gt;=55: print(\u0026#39;老年人\u0026#39;) elif 35\u0026lt;=age\u0026lt;55: print(\u0026#39;中年\u0026#39;) elif 18\u0026lt;=age\u0026lt;35: print(\u0026#39;青年\u0026#39;) elif 0\u0026lt;=age\u0026lt;18: print(\u0026#39;儿童\u0026#39;) 青年  \n特殊的conditon  各种空值(空字符串、空列表等)作用等同于False 各种非空值，作用等同于True  a = None if a: print(\u0026#39;a是非空数据\u0026#39;) else: print(\u0026#39;a是空数据\u0026#39;) a是空数据  \nfor循环  重复做某件事 迭代出数据中的内容(元素)   上面这个图可以解读为 我们想对iterable这个集合中的每一个item:","title":"逻辑语句(if\u0026for\u0026tryexcept)"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n列表推导式唯一的用处就是增强代码的可阅读性，初次接触可能比较难理解，但是大家一定要理解，文本分析中经常会用到ta。\n\n问题1 用列表表示集合X $X= {x| x \\in [1,2,3,4,5,6,7,8,9,10]}$\nX = [1,2,3,4,5,6,7,8,9,10] X [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  X = list(range(1, 10)) X [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n问题2 表示集合Y $Y = {x^2| x \\in [1,2,3,4,5,6,7,8,9,10]}$\nY = [] for x in X: Y.append(x*x) Y [1, 4, 9, 16, 25, 36, 49, 64, 81]  \n列表推导式 实现步骤\n 先复制X 再对X中的元素x进行操作  #先复制X [x for x in X] [1, 2, 3, 4, 5, 6, 7, 8, 9]  #再对x进行操作 [x*x for x in X] [1, 4, 9, 16, 25, 36, 49, 64, 81]  理解列表推导式  带条件的列表推导式 $C= {x^2 | (x \\in X) \\cap (x\u0026gt;5)}$\n#复制X [x for x in X] [1, 2, 3, 4, 5, 6, 7, 8, 9]  #X中的要大于5 #[x for x in X if x\u0026gt;5] [x for x in X if x\u0026gt;5] [6, 7, 8, 9]  #对满足条件的x进行操作 #[x*x for x in X if x\u0026gt;5] [x*x for x in X if x\u0026gt;5] [36, 49, 64, 81]  \n问题3 全部小写 words = [\u0026#39;Life\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;short\u0026#39;, \u0026#39;so\u0026#39;, \u0026#39;we\u0026#39;, \u0026#39;use\u0026#39;, \u0026#39;Python\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;easy\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;learn\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;easy\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;use\u0026#39;] words ['Life', 'is', 'short', 'so', 'we', 'use', 'Python', 'python', 'is', 'easy', 'to', 'learn', 'and', 'easy', 'to', 'use']  lower_words = [w.lower() for w in words] lower_words ['life', 'is', 'short', 'so', 'we', 'use', 'python', 'python', 'is', 'easy', 'to', 'learn', 'and', 'easy', 'to', 'use']  \n问题4 计算lower_words列表数据的单词词频 wordset = set(lower_words) [(w, lower_words.count(w)) for w in wordset] [('so', 1), ('and', 1), ('learn', 1), ('use', 2), ('to', 2), ('we', 1), ('easy', 2), ('python', 2), ('is', 2), ('short', 1), ('life', 1)]  #1 生产词语集合 wordset = set(lower_words) print(wordset) {'so', 'and', 'learn', 'use', 'to', 'we', 'easy', 'python', 'is', 'short', 'life'}  #2. wordset复制wordset自己 [w for w in wordset] #3. 对wordset中每个词语w进行一些操作 [lower_words.count(w) for w in wordset] [1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1]  #3. 词频和词语一起显示 [(w,lower_words.count(w)) for w in wordset] [('so', 1), ('and', 1), ('learn', 1), ('use', 2), ('to', 2), ('we', 1), ('easy', 2), ('python', 2), ('is', 2), ('short', 1), ('life', 1)]  \n问题5 d = {\u0026#39;x\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;y\u0026#39;: \u0026#39;B\u0026#39;, \u0026#39;z\u0026#39;: \u0026#39;C\u0026#39; } 如何将d中的 键值对 拼接,输出为\n['xA', 'yB', 'zC]\nd = {\u0026#39;x\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;y\u0026#39;: \u0026#39;B\u0026#39;, \u0026#39;z\u0026#39;: \u0026#39;C\u0026#39; } d.items() dict_items([('x', 'A'), ('y', 'B'), ('z', 'C')])  #1 自己复制d.items()自己 [i for i in d.items()] [('x', 'A'), ('y', 'B'), ('z', 'C')]  #2 对任何一个元素都要进行字符串的拼接操作 [i[0]+i[1] for i in d.items()] ['xA', 'yB', 'zC']  \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-09-list-comprehension/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n列表推导式唯一的用处就是增强代码的可阅读性，初次接触可能比较难理解，但是大家一定要理解，文本分析中经常会用到ta。\n\n问题1 用列表表示集合X $X= {x| x \\in [1,2,3,4,5,6,7,8,9,10]}$\nX = [1,2,3,4,5,6,7,8,9,10] X [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  X = list(range(1, 10)) X [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n问题2 表示集合Y $Y = {x^2| x \\in [1,2,3,4,5,6,7,8,9,10]}$\nY = [] for x in X: Y.append(x*x) Y [1, 4, 9, 16, 25, 36, 49, 64, 81]","title":"高级语法-列表推导式"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n流水线每个环节都有质量要求，只有符合质量规范，才能流通到下一个环节。这样环环相扣，设计好后经过试运营就可以投产了。Python编程也一样，编程序其实也是设计流水线，而函数就是流水线上的一个个的环节。\n\n函数 可重复使用的代码块\n def函数常见参数定义\u0026amp;调用方式\n 位置参数 关键词参数 默认参数  \n位置参数 def hello1(name, age): return \u0026#39;我是{0},今年{1}\u0026#39;.format(name, age) hello1(\u0026#39;张三\u0026#39;, 25) '我是张三,今年25'  hello1(25,\u0026#39;张三\u0026#39;) '我是25,今年张三'  \n关键词参数 def hello2(name, age): return \u0026#39;我是{0},今年{1}\u0026#39;.format(name, age) hello2(name=\u0026#39;张三\u0026#39;, age=25) '我是张三,今年25'  hello2(age=25, name=\u0026#39;张三\u0026#39;) '我是张三,今年25'  \n默认参数 def hello3(name, age, gender=\u0026#39;男\u0026#39;): return \u0026#39;我是{0},今年{1}, 性别{2}\u0026#39;.format(name, age, gender) hello3(\u0026#39;David\u0026#39;, 25) '我是David,今年25, 性别男'  hello3(\u0026#39;David\u0026#39;, 25, gender=\u0026#39;male\u0026#39;) '我是David,今年25, 性别male'  了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-10-understand-function/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n流水线每个环节都有质量要求，只有符合质量规范，才能流通到下一个环节。这样环环相扣，设计好后经过试运营就可以投产了。Python编程也一样，编程序其实也是设计流水线，而函数就是流水线上的一个个的环节。\n\n函数 可重复使用的代码块\n def函数常见参数定义\u0026amp;调用方式\n 位置参数 关键词参数 默认参数  \n位置参数 def hello1(name, age): return \u0026#39;我是{0},今年{1}\u0026#39;.format(name, age) hello1(\u0026#39;张三\u0026#39;, 25) '我是张三,今年25'  hello1(25,\u0026#39;张三\u0026#39;) '我是25,今年张三'  \n关键词参数 def hello2(name, age): return \u0026#39;我是{0},今年{1}\u0026#39;.format(name, age) hello2(name=\u0026#39;张三\u0026#39;, age=25) '我是张三,今年25'  hello2(age=25, name=\u0026#39;张三\u0026#39;) '我是张三,今年25'  \n默认参数 def hello3(name, age, gender=\u0026#39;男\u0026#39;): return \u0026#39;我是{0},今年{1}, 性别{2}\u0026#39;.format(name, age, gender) hello3(\u0026#39;David\u0026#39;, 25) '我是David,今年25, 性别男'  hello3(\u0026#39;David\u0026#39;, 25, gender=\u0026#39;male\u0026#39;) '我是David,今年25, 性别male'  了解课程  点击上方图片购买课程   点击进入详情页","title":"高级语法-理解函数"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n有三大类内置函数\n 数学相关函数 类型转化函数 功能函数  函数名加粗的是都是重点\n\n数学相关    函数 功能 例子 运行结果     abs(a) 对a取绝对值 abs(-1) 1   max(lst)、min(lst) 寻找lst中的最大、最小值 max([3, 2, 9]) 9   sum(lst) 对lst内所有数字求和 sum([3, 2, 9]) 14   sorted(lst， reverse) 对lst排序； 参数reverse为布尔值控制升降序 sorted([3, 2, 9]) [2, 3, 9]   range(start, end, step) 生成以步长step，生成从start到end的数列,默认step=1，结果取不到end list(range(1,5)) [1, 2, 3,4]    #取绝对值 abs(-1) 1  #取最大 max([3, 2, 9]) 9  #取最小 min([3, 2, 9]) 2  #求和 sum([3, 2, 9]) 14  #排序 sorted([3,2,9]) [2, 3, 9]  #排序(方向调整) sorted([3,2,9], reverse=True) [9, 3, 2]  #生成序列 list(range(1, 10)) [1, 2, 3, 4, 5, 6, 7, 8, 9]  list(range(1, 10, 2)) [1, 3, 5, 7, 9]  \n类型转换    函数 功能 例子 运行结果     int(string) 将字符串数改为整数型 int(\u0026lsquo;9\u0026rsquo;) 9   float(int/str) 将int或str改为浮点型 float(9)、float(\u0026lsquo;9\u0026rsquo;) 9.0   list(iterable) 将可迭代对象为列表。这里的iterable可以为字符串，可以是列表 list(range(1,5)) [1,2,3,4]    enumerate(lst) 返回带有索引值的序列seq,需要list(seq)处理后才能看到seq list(enumerate([\u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;, \u0026lsquo;c\u0026rsquo;])) [(0,\u0026lsquo;a\u0026rsquo;), (1, \u0026lsquo;b\u0026rsquo;), (2, \u0026lsquo;c\u0026rsquo;)]   tuple(lst) 将lst变为tuple tuple([1,2,3]) (1,2,3)   set(lst) 将lst变为集合 set([1,4,4,4,3]) {1,3,4}    a = 9 b = 9 a+b 18  #变转化为整数 int(\u0026#39;9\u0026#39;) 9  #转化为小数 float(\u0026#39;9\u0026#39;) 9.0  float(9) 9.0  #转化为列表 list(range(1, 5)) [1, 2, 3, 4]  #给列表中每个元素分配一个索引值 names = [\u0026#39;张三\u0026#39;, \u0026#39;李四\u0026#39;, \u0026#39;王五\u0026#39;] list(enumerate(names)) [(0, '张三'), (1, '李四'), (2, '王五')]  \n功能函数    函数 功能 例子 运行结果     eval(expression) 执行一个字符串表达式 eval(\u0026lsquo;1+1\u0026rsquo;) 2   zip(lst1,lst2\u0026hellip;) 将lst1,lst2\u0026hellip;合并,返回zip对象。需要list处理一下zip对象 list(zip([1,2,3],[4,5,6])) [(1, 4), (2, 5), (3, 6)]   type(x) 查看X的类型 type(\u0026lsquo;2\u0026rsquo;) \u0026lt;class \u0026lsquo;str\u0026rsquo;\u0026gt;   help(x) 查看X的相关信息 help([1, 2]) Help on list object..   map(func, lst) 对lst中的每一个个体都进行func操作 list(map(sum, [[1,1], [1,2]])) [2, 3]   print(value, end='\\n') 打印value print(\u0026lsquo;abc\u0026rsquo;) abc   open(file， encoding) 打开file文件， encoding是file的文件编码      \neval() eval(str_expression)\nstr_expression 是字符串表达式，可以是变量、函数等\na = 9 b = 9 c = \u0026#39;a+b\u0026#39; print(a+b) print(c) print(eval(c)) 18 a+b 18  eval(\u0026#39;a+b\u0026#39;) 18  d = \u0026#39;hello world\u0026#39; print(\u0026#39;d\u0026#39;) print(eval(\u0026#39;d\u0026#39;)) d hello world  def hello(): print(\u0026#39;hello python\u0026#39;) print(\u0026#39;hello()\u0026#39;) hello()  eval(\u0026#39;hello()\u0026#39;) hello python  \nzip(lst1, lst2,lst3\u0026hellip;) 将lst1， lst2， lst3按照顺序进行合并\nnames = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Unique\u0026#39;] sexs = [\u0026#39;male\u0026#39;, \u0026#39;femal\u0026#39;, \u0026#39;male\u0026#39;, \u0026#39;male\u0026#39;] ages = [25, 22, 30, 40] list(zip(names, sexs, ages)) [('David', 'male', 25), ('Mary', 'femal', 22), ('Henry', 'male', 30), ('Unique', 'male', 40)]  \ntype/help 查看数据类型、查看感兴趣对象的介绍\na = [1,3,5] type(a) list  help(a) Help on list object: class list(object) | list(iterable=(), /) | | Built-in mutable sequence. | | If no argument is given, the constructor creates a new empty list. | The argument must be an iterable if specified. | | Methods defined here: | ......... | append(self, object, /) | Append object to the end of the list. | | | count(self, value, /) | Return number of occurrences of value. | | extend(self, iterable, /) | Extend list by appending elements from the iterable. |  type(print) builtin_function_or_method  help(print) Help on built-in function print in module builtins: print(...) print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream.  map(func, lst)映射运算 将func运算映射到lst上每个元素\nlst = [[1,1], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2]] res = map(sum, lst) list(res) [2, 3, 3, 3, 3, 3, 3]  \nprint(value, end='\\n') 打印value，默认使用换行结束\nhelp(print) Help on built-in function print in module builtins: print(...) print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream.  print(\u0026#39;hello world!\u0026#39;) print(\u0026#39;hello python!\u0026#39;) hello world! hello python!  print(\u0026#39;hello world!\u0026#39;, end=\u0026#39;\\t\u0026#39;) print(\u0026#39;hello python!\u0026#39;) hello world!\thello python!  \nopen(file, mode=\u0026lsquo;r\u0026rsquo;, encoding=None)\n file 文件路径 mode 操作方式们，最常用的是r和a+。r读取， a+是追加写入 encoding 编码方式 ，常见的文件编码方式主要是utf-8和gbk  读取返回io对象\nio对象有read()方法\n 相对路径\ndata\n  绝对路径\nC:Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\n 建议大家都要用相对路径\n# 读取数据 open(\u0026#39;data/test.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() '章节设计\\n\\n第一部分 环境配置\\n第二部分 快速入门python\\n第三部分 网络爬虫\\n第四部分 简单的文本分析\\n第五部分 进阶文本分析'  # 新建文件/在已有的文件内插入内容 f = open(\u0026#39;data/test2.txt\u0026#39;, mode=\u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) f.write(\u0026#39;我在学python，现在是下午五点\u0026#39;) f.close() \n# 新建文件/在已有的文件内插入内容 f = open(\u0026#39;data/test2.txt\u0026#39;, mode=\u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) f.write(\u0026#39;\\nLife is short, so to learn Python\u0026#39;) f.close() \n# 新建文件/在已有的文件内插入内容 f = open(\u0026#39;data/test2.txt\u0026#39;, mode=\u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) f.write(\u0026#39;\\nLife is short, so to learn Python\u0026#39;) f.write(\u0026#39;\\nLife is short, so to learn music\u0026#39;) f.write(\u0026#39;\\nLife is short, so to learn english\u0026#39;) f.close() \n重点函数  sorted(lst， ascending) range(start, end, step) enumerate(lst) eval(expression) zip(lst1, lst2..) map(func, lst) print(x) open(file, mode, encoding)  了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-11-built-function/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n有三大类内置函数\n 数学相关函数 类型转化函数 功能函数  函数名加粗的是都是重点\n\n数学相关    函数 功能 例子 运行结果     abs(a) 对a取绝对值 abs(-1) 1   max(lst)、min(lst) 寻找lst中的最大、最小值 max([3, 2, 9]) 9   sum(lst) 对lst内所有数字求和 sum([3, 2, 9]) 14   sorted(lst， reverse) 对lst排序； 参数reverse为布尔值控制升降序 sorted([3, 2, 9]) [2, 3, 9]   range(start, end, step) 生成以步长step，生成从start到end的数列,默认step=1，结果取不到end list(range(1,5)) [1, 2, 3,4]    #取绝对值 abs(-1) 1  #取最大 max([3, 2, 9]) 9  #取最小 min([3, 2, 9]) 2  #求和 sum([3, 2, 9]) 14  #排序 sorted([3,2,9]) [2, 3, 9]  #排序(方向调整) sorted([3,2,9], reverse=True) [9, 3, 2]  #生成序列 list(range(1, 10)) [1, 2, 3, 4, 5, 6, 7, 8, 9]  list(range(1, 10, 2)) [1, 3, 5, 7, 9]","title":"常用内置函数"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n路径是可以让程序知道待操作的文件在哪里，python中有os和pathlib两个内置的路径库，我们就讲这个名字一看就懂的路径库pathlib。\n\n绝对vs相对\n 相对路径 'img' 绝对路径 'C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/img'  **注意：**当移动文件夹位置或者将代码分享给朋友使用时，你的代码再次运行就会出错。为了避免这个问题，强烈建议用相对路径\nimport pathlib #当前代码所在的文件夹的相对路径 pathlib.Path() WindowsPath('.')  from pathlib import Path #当前代码所在的文件夹的相对路径 Path() WindowsPath('.')  \npathlib.Path()属性方法 **提醒：**下表加粗的都是常用的方法，其他了解即可\n   方法 功能     cwd() 获取代码所在的当前工作路径   joinpath(\u0026hellip;grandpadir, fatherdir, \u0026hellip;file) 生成路径   iterdir() 返回某路径下的文件(夹)目录   glob(pattern) 返回符合pattern的所有文件的文件路径   is_file() 判断某路径是否为文件，返回布尔值   is_dir() 判断某路径是否为文件夹，返回布尔值   exists() 判断某路径是否存在，返回布尔值   mkdir(parents=True, exist_ok=True) 创建某路径对应的文件夹    \ncwd() 例：获取当前代码所在文件夹的绝对路径\nPath().cwd() WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门')  \njoinpath(\u0026hellip;grandpadir, fatherdir, \u0026hellip;file) 把\u0026hellip;grandpadir, fatherdir, \u0026hellip;file加入到某路径中\n例：获得data文件夹的路径\nPath().cwd().joinpath(\u0026#39;data\u0026#39;) WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data')  例：获得data/test.txt文件的路径\nPath().cwd().joinpath(\u0026#39;data\u0026#39;, \u0026#39;test.txt\u0026#39;) WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test.txt')  \niterdir() 返回某路径下的文件(夹)目录\n例：获得02-Python语法入门文件夹里的所有文件(夹)路径\nlist(Path().cwd().iterdir()) [WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/.ipynb_checkpoints'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/01-Python跟英语一样是一门语言.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/02-数据类型之字符串.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/03-数据类型之列表元组集合.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/04-数据类型之字典.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/05-数据类型之布尔值\u0026amp;None.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/06-逻辑语句(if\u0026amp;for\u0026amp;tryexcept).ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/07-列表推导式.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/08-理解函数.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/09-常用内置函数.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/09-常用函数.md'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/10-内置库之文件路径pathlib库.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/11-内置库之csv文件库.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/12. 内置库之正则表达式re库.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/13-初学python常出错误汇总.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/img')]  \nglob(pattern) 查找某路径内满足pattern的所有文件路径 。\npattern='*.*' 匹配任意格式任意名字的文件\npattern='*.txt' 匹配出所有的txt文件\n例：获得data文件夹内的所有的文件路径\nlist(Path().cwd().joinpath(\u0026#39;data\u0026#39;).glob(\u0026#39;*.*\u0026#39;)) [WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test.txt'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test2.csv'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test2.txt'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/twitter_sentiment.csv')]  例：获得data文件夹内的所有的txt额路径\nlist(Path().cwd().joinpath(\u0026#39;data\u0026#39;).glob(\u0026#39;*.txt\u0026#39;)) [WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test.txt'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test2.txt')]  例：获得data/reports内的pdf路径\ndirs = Path().cwd().joinpath(\u0026#39;data\u0026#39;, \u0026#39;reports\u0026#39;).iterdir() dirs = list(dirs) for dir in dirs: files = dir.glob(\u0026#39;*.*\u0026#39;) for file in files: print(file) C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600000\\600000_20010901_1.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600004\\600004_2006_n.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600004\\600004_2006_z.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600007\\600007_2001_n.pdf ....... C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600007\\600007_2002_1.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\603937\\603937_2018_z.pdf ...... C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\603937\\603937_2019_3.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\603937\\603937_2019_z.pdf  例**：获得data/reports内的 含有\u0026quot;_n\u0026quot; 额pdf路径\ndirs = Path().cwd().joinpath(\u0026#39;data\u0026#39;, \u0026#39;reports\u0026#39;).iterdir() dirs = list(dirs) for dir in dirs: files = dir.glob(\u0026#39;*_n.pdf\u0026#39;) for file in files: print(file) C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600000\\600000_2006_n.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600000\\600000_2008_n.pdf ........ C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\601872\\601872_2014_n.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\601872\\601872_2015_n.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\601872\\601872_2016_n.pdf  \nis_file() 判断某路径是否为一个文件。返回布尔值：\n True 真实存在的文件路径 False 不真实存在或者文件夹路径  例 \u0026lsquo;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test.txt\u0026rsquo;是文件路径？\nfpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test.txt\u0026#39;) fpath.is_file() True  例 \u0026lsquo;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test222.txt\u0026rsquo;是文件路径？\nfpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test222.txt\u0026#39;) fpath.is_file() False  fpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data\u0026#39;) fpath.is_file() False  \nis_dir() 判断某路径是否为一个文件夹。返回布尔值，True、False\n例： \u0026lsquo;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data\u0026rsquo; 是 文件夹路径？\nfpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data\u0026#39;) fpath.is_dir() True  \nexists() 判断某路径是否存在。返回布尔值，True、False\n例： \u0026lsquo;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data\u0026rsquo; 是否存在？\nfpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data\u0026#39;) fpath.exists() True  fpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/datasss\u0026#39;) fpath.exists() False  mkdir(parents=True, exist_ok=True) 创建某路径\npath = Path().cwd().joinpath(\u0026#39;data\u0026#39;, \u0026#39;stocks\u0026#39;, \u0026#39;800000\u0026#39;) path.mkdir(parents=True, exist_ok=True) \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-12-pathlib/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n路径是可以让程序知道待操作的文件在哪里，python中有os和pathlib两个内置的路径库，我们就讲这个名字一看就懂的路径库pathlib。\n\n绝对vs相对\n 相对路径 'img' 绝对路径 'C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/img'  **注意：**当移动文件夹位置或者将代码分享给朋友使用时，你的代码再次运行就会出错。为了避免这个问题，强烈建议用相对路径\nimport pathlib #当前代码所在的文件夹的相对路径 pathlib.Path() WindowsPath('.')  from pathlib import Path #当前代码所在的文件夹的相对路径 Path() WindowsPath('.')  \npathlib.Path()属性方法 **提醒：**下表加粗的都是常用的方法，其他了解即可\n   方法 功能     cwd() 获取代码所在的当前工作路径   joinpath(\u0026hellip;grandpadir, fatherdir, \u0026hellip;file) 生成路径   iterdir() 返回某路径下的文件(夹)目录   glob(pattern) 返回符合pattern的所有文件的文件路径   is_file() 判断某路径是否为文件，返回布尔值   is_dir() 判断某路径是否为文件夹，返回布尔值   exists() 判断某路径是否存在，返回布尔值   mkdir(parents=True, exist_ok=True) 创建某路径对应的文件夹","title":"内置库-文件路径pathlib库"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n在编程中一般不适用excel，而是用一种很像excel的csv来存储数据。而且Excel软件可以打开csv的。\n一、csv存储数据代码步骤 **说明:**代码看不懂没关系，能背过最好。背不过也没关系，能理解代码功能，而且亲自上手调试过，调试正常无误的代码可以加入你的代码笔记本中，然后以后需要的时候复制粘贴修改参数即可\n1. 1 新建一个csv文件 import csv path = \u0026#39;data/test.csv\u0026#39; csvf = open(path, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) 1.2 定义字段名，并初始化csv文件为writer fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() 1.3 将待存储数据整理为字典格式 test_data = {\u0026#39;name\u0026#39;: \u0026#39;David\u0026#39;, \u0026#39;age\u0026#39;: 25} 1.4 用writer往csv中存储数据 writer.writerow(test_data) 1.5 最后记得关闭csv文件 csvf.close() \nimport csv csvf = open(\u0026#39;data/test1.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() csvf.close() \nimport csv csvf = open(\u0026#39;data/test2.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() test_data = {\u0026#39;name\u0026#39;: \u0026#39;David\u0026#39;, \u0026#39;age\u0026#39;: 25} writer.writerow(test_data) csvf.close() \n二、很多数据的存储 如果很多数据存储时，就可以用之前学到的for循环。\ndatas = [{\u0026#39;name\u0026#39;: \u0026#39;David\u0026#39;, \u0026#39;age\u0026#39;: 25}, {\u0026#39;name\u0026#39;: \u0026#39;Mary\u0026#39;, \u0026#39;age\u0026#39;: 30}, {\u0026#39;name\u0026#39;: \u0026#39;Henry\u0026#39;, \u0026#39;age\u0026#39;: 35}] datas [{'name': 'David', 'age': 25}, {'name': 'Mary', 'age': 30}, {'name': 'Henry', 'age': 35}]  import csv csvf = open(\u0026#39;data/test2.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() for data in datas: writer.writerow(data) csvf.close() \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-13-csv/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n在编程中一般不适用excel，而是用一种很像excel的csv来存储数据。而且Excel软件可以打开csv的。\n一、csv存储数据代码步骤 **说明:**代码看不懂没关系，能背过最好。背不过也没关系，能理解代码功能，而且亲自上手调试过，调试正常无误的代码可以加入你的代码笔记本中，然后以后需要的时候复制粘贴修改参数即可\n1. 1 新建一个csv文件 import csv path = \u0026#39;data/test.csv\u0026#39; csvf = open(path, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) 1.2 定义字段名，并初始化csv文件为writer fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() 1.3 将待存储数据整理为字典格式 test_data = {\u0026#39;name\u0026#39;: \u0026#39;David\u0026#39;, \u0026#39;age\u0026#39;: 25} 1.4 用writer往csv中存储数据 writer.writerow(test_data) 1.5 最后记得关闭csv文件 csvf.close() \nimport csv csvf = open(\u0026#39;data/test1.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() csvf.close() \nimport csv csvf = open(\u0026#39;data/test2.","title":"内置库-数据存储csv库"},{"content":" Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n正则表达式主要用于数据清洗，比如从脏乱差的文本中抽取出自己需要的信息。常见于爬虫和文本分析。\n一、正则表达式中的符号 按照符号的功能，我将其分为三类，一般情况下表达式都是由这三种符号组成的。\n1.1 正则字符 预警，听不懂看不懂，都不要紧的。不要绞尽脑汁，本节后面会柳暗花明的。\n   正则符号 描述 匹配自己时     \\ 转义字符。例如， \u0026lsquo;n\u0026rsquo; 匹配字符 \u0026lsquo;n\u0026rsquo;。 '\\n'   ( ) 标记一个子表达式的开始和结束位置。 \\( \\)   . 匹配除换行符 \\n 之外的任何单字符。 \\.    | |左右两侧均可参与匹配    \\d 匹配字符串中的单个数字    a-zA-Z 匹配全部英文字符    0-9 匹配全部数字    \\s 匹配字符串中的\\n,\\t,空格    [] 中括号内任意正则符号均可参与匹配 \\[ \\]   ^ 当在方括号表达式中使用，^对其后的正则表达式进行了反义表达。 \\^    1.2 限定字符 提前预警，听不懂不要绞尽脑汁，本节后面会柳暗花明的\n   正则符号 描述 匹配自己时     * 匹配前面的子表达式零次或多次。 \\*   ? 匹配前面的子表达式零次或一次 \\?   + 匹配前面的子表达式一次或多次。 \\+   {m} n 是一个非负整数。匹配确定的 m 次。    {m,} m 是一个非负整数。至少匹配m 次。    {m, n} m 和 n 均为非负整数，其中m \u0026lt;= n。最少匹配 m 次且最多匹配 n 次。     1.3 定位字符 预警，听不懂不要绞尽脑汁，本节后面会柳暗花明的\n   正则符号 描述 匹配自己时     ^ 匹配输入字符串的开始位置。 \\^   $ 匹配输入字符串的结尾位置 \\$   \\b 匹配一个单词边界，即字与空格间的位置    \\B 非单词边界匹配     \n二、re库常用方法 至暗时刻已过，光明即将到来\n   re库常用函数 作用     re.findall(pattern, string) 根据pattern返回匹配结果（列表）    |re.split(pattern, string) |使用pattern分割string，返回列表 |re.sub(pattern, repl, string)|使用repl替换string中的pattern|\n\n三、只需要掌握 万能的百度谷歌+你的尝试，比什么都强大\n 搜索引擎检索到自己需要的正则表达式 最简单最好用表达式(.*?) 在正则表达式测试网站验证自己的正则表达式  3.1 检索找到自己需要的正则表达式 比如我只需要中文，其余字符统统不要。\n我会在百度搜中文正则表达式\n发现很多网页中网友提到````，于是\n[\\u4e00-\\u9fa5]\nimport re pattern = \u0026#39;[\\u4e00-\\u9fa5]+\u0026#39; string = \u0026#34;\u0026#34;\u0026#34;Python是一门面向对象的编程语言，诞生于1991年。\\ 目前以广泛应用在网站开发、游戏软件开发、数据采集、机器学习等多个领域。\\ 一般情况下Python是Java的20%，所以说人生苦短，我用Python。\u0026#34;\u0026#34;\u0026#34; chinese_words = re.findall(pattern, string) chinese_text = \u0026#39;\u0026#39;.join(chinese_words) chinese_text '是一门面向对象的编程语言诞生于年目前以广泛应用在网站开发游戏软件开发数据采集机器学习等多个领域一般情况下是的所以说人生苦短我用'  3.2 最简单最好用表达式(.*?) (.*?)特别好用，ta的暗号及使用口诀一定要背过\npattern设计步骤：\n正则符号组成正则表达式，用于匹配需要的字符。\n 找到重复的一致的规律 复制粘贴到pattern中 扣掉想要的数据 替换为(.*?) 或者相应的正则符号表达式*  比如现在需要快速挖掘出intros中的姓名、籍贯和年龄\nimport re pattern = \u0026#39;我叫(.*?)，来自(.*?)，今年(.*?)岁。\u0026#39; intros = [\u0026#39;我叫张三，来自山东，今年25岁。\u0026#39;, \u0026#39;我叫李四，来自河北，今年28岁。\u0026#39;, \u0026#39;我叫王五，来自河南，今年24岁。\u0026#39;] for intro in intros: info = re.findall(pattern, intro) print(info) [('张三', '山东', '25')] [('李四', '河北', '28')] [('王五', '河南', '24')]  特别需要注意的是pattern中的(.*?)左右两侧必须有字符，否则匹配失败。\n import re pattern = \u0026#39;(.*?)，来自(.*?)，今年(.*?)\u0026#39; intros = [\u0026#39;我叫张三，来自山东，今年25岁。\u0026#39;, \u0026#39;我叫李四，来自河北，今年28岁。\u0026#39;, \u0026#39;我叫王五，来自河南，今年24岁。\u0026#39;] for intro in intros: info = re.findall(pattern, intro) print(info) [('我叫张三', '山东', '')] [('我叫李四', '河北', '')] [('我叫王五', '河南', '')]  由于\n'(.*?)，来自(.*?)，今年(.*?)'\n中最左侧和最右侧的(.*?)没有被其他字符左右包裹，导致匹配姓名和年龄失败。\nimport re pattern = \u0026#39;叫(.*?)，来自(.*?)，今年(.*?)岁\u0026#39; intros = [\u0026#39;我叫张三，来自山东，今年25岁。\u0026#39;, \u0026#39;我叫李四，来自河北，今年28岁。\u0026#39;, \u0026#39;我叫王五，来自河南，今年24岁。\u0026#39;] for intro in intros: info = re.findall(pattern, intro) print(info) [('张三', '山东', '25')] [('李四', '河北', '28')] [('王五', '河南', '24')]  \n四、案例 4.1 找出文本中出现的年份 import re pattern = \u0026#39;\\d{4}\u0026#39; string = \u0026#34;\u0026#34;\u0026#34;Python是一门面向对象的编程语言，诞生于1991年。\\ 目前以广泛应用在网站开发、游戏软件开发、数据采集、机器学习等多个领域。\\ 一般情况下Python是Java的20%，所以说人生苦短，我用Python。\u0026#34;\u0026#34;\u0026#34; re.findall(pattern, string) ['1991']  4.2 re.split(pattern, string) 断句\npattern = \u0026#39;；|。\u0026#39; string = \u0026#34;\u0026#34;\u0026#34;Python是一门面向对象的编程语言，诞生于1991年；\\ 目前以广泛应用在网站开发、游戏软件开发、数据采集、机器学习等多个领域。\\ 一般情况下Python是Java的20%，所以说人生苦短，我用Python。\u0026#34;\u0026#34;\u0026#34; res = re.split(pattern, string) res = [r for r in res if r] res ['Python是一门面向对象的编程语言，诞生于1991年', '目前以广泛应用在网站开发、游戏软件开发、数据采集、机器学习等多个领域', '一般情况下Python是Java的20%，所以说人生苦短，我用Python']  4.3 re.sub(pattern, repl, string) 将数字替换为NUM\npattern = \u0026#39;\\d+\u0026#39; repl = \u0026#39;NUM\u0026#39; string = \u0026#34;\u0026#34;\u0026#34;Python是一门面向对象的编程语言，诞生于1991年。\\ 一般情况下Python是Java的20%，所以说人生苦短，我用Python。\u0026#34;\u0026#34;\u0026#34; re.sub(pattern, repl, string) 'Python是一门面向对象的编程语言，诞生于NUM年。一般情况下Python是Java的NUM%，所以说人生苦短，我用Python。'  4.4 . 统一表达 将指代同一个主体的不同表达词语统一为同一个词\ntext = \u0026#39;中国铁路工程集团有限公司成立于1950年3月，总部位于北京。目前中国中铁已经发展成中国和亚洲最大的多功能综合型建设集团。\u0026#39; pattern = \u0026#39;中国铁路工程集团有限公司|中国中铁\u0026#39; repl = \u0026#39;中铁\u0026#39; re.sub(pattern, repl, text) '中铁成立于1950年3月，总部位于北京。目前中铁已经发展成中国和亚洲最大的多功能综合型建设集团。'  text = \u0026#39;中国铁路工程集团有限公司成立于1950年3月，总部位于北京。目前中国中铁已经发展成中国和亚洲最大的多功能综合型建设集团。\u0026#39; pattern = \u0026#39;[中国铁路工程集团有限公司|中国中铁]+\u0026#39; repl = \u0026#39;中铁\u0026#39; re.sub(pattern, repl, text) '中铁成立于1950年3月，总部位于北京。目前中铁已经发展成中铁和亚洲最大的多功能综合型建设中铁。'  4.5 分割文本数据的章节 一二三四五六七八九十零百\ntext = \u0026#34;\u0026#34;\u0026#34; 第一篇 Python简介 第二篇 Python入门语法 第三篇 Python网络爬虫 第四篇 文本数据编码 第五篇 数据分析 第六篇 可视化\u0026#34;\u0026#34;\u0026#34; pattern = \u0026#39;第[一二三四五六七八九十零百]+篇\u0026#39; res = re.split(pattern, text) res = [r.replace(\u0026#39; \u0026#39;, \u0026#39;\u0026#39;) for r in res if \u0026#39; \u0026#39;!=r] res ['Python简介', 'Python入门语法', 'Python网络爬虫', '文本数据编码', '数据分析', '可视化']  4.6 抽取出数字 比如日期数据\ntext = \u0026#39;中国铁路工程集团有限公司成立于1950年3月，总部位于北京。目前中国中铁已经发展成中国和亚洲最大的多功能综合型建设集团。\u0026#39; pattern = \u0026#39;\\d+\u0026#39; \u0026#39;-\u0026#39;.join(re.findall(pattern, text)) '1950-3'  \n了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-14-regex/","summary":"Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n正则表达式主要用于数据清洗，比如从脏乱差的文本中抽取出自己需要的信息。常见于爬虫和文本分析。\n一、正则表达式中的符号 按照符号的功能，我将其分为三类，一般情况下表达式都是由这三种符号组成的。\n1.1 正则字符 预警，听不懂看不懂，都不要紧的。不要绞尽脑汁，本节后面会柳暗花明的。\n   正则符号 描述 匹配自己时     \\ 转义字符。例如， \u0026lsquo;n\u0026rsquo; 匹配字符 \u0026lsquo;n\u0026rsquo;。 '\\n'   ( ) 标记一个子表达式的开始和结束位置。 \\( \\)   . 匹配除换行符 \\n 之外的任何单字符。 \\.    | |左右两侧均可参与匹配    \\d 匹配字符串中的单个数字    a-zA-Z 匹配全部英文字符    0-9 匹配全部数字    \\s 匹配字符串中的\\n,\\t,空格    [] 中括号内任意正则符号均可参与匹配 \\[ \\]   ^ 当在方括号表达式中使用，^对其后的正则表达式进行了反义表达。 \\^    1.","title":"内置库-正则表达式re库"},{"content":" 14种常见错误 Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n学习编程就是在遇到错误、认识错误、解决错误的过程。遇到错误，大家要发挥主观能动性，用自己的英文阅读能力去先读一下英文报错提示，一般情况下错误提示会告诉你是什么类型的错误，错误出在哪一行。\n再结合百度谷歌，80%以上的问题都能解决。现在我们了解一下常见的问题都有哪些，如何克服这些问题。\n1. 忘记写冒号 在 if、elif、else、for、while、def语句后面忘记添加 :\nage = 42 if age == 42 print(\u0026#39;Hello!\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-10-1f5acea116cf\u0026gt;\u0026quot;, line 3 if age == 42 ^ SyntaxError: invalid syntax  age = 42 if age == 42: print(\u0026#39;Hello!\u0026#39;) Hello!  \n2. 误用 = = 是赋值操作，而判断两个值是否相等是 ==\ngender = \u0026#39;男\u0026#39; if gender = \u0026#39;男\u0026#39;: print(\u0026#39;Man\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-12-c3ceea5a9004\u0026gt;\u0026quot;, line 3 if gender = '男': ^ SyntaxError: invalid syntax  gender = \u0026#39;男\u0026#39; if gender == \u0026#39;男\u0026#39;: print(\u0026#39;Man\u0026#39;) Man  \n3. 错误的缩进 Python用缩进区分代码块，常见的错误用法：\nprint(\u0026#39;Hello!\u0026#39;) print(\u0026#39;Howdy!\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-14-784bdb6e1df5\u0026gt;\u0026quot;, line 2 print('Howdy!') ^ IndentationError: unexpected indent  print(\u0026#39;Hello!\u0026#39;) print(\u0026#39;Howdy!\u0026#39;) Hello! Howdy!  num = 25 if num == 25: print(\u0026#39;Hello!\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-16-8e4debcdf119\u0026gt;\u0026quot;, line 3 print('Hello!') ^ IndentationError: expected an indented block  num = 25 if num == 25: print(\u0026#39;Hello!\u0026#39;) Hello!  \n4. 变量没有定义 if c in [\u0026#39;New York\u0026#39;, \u0026#39;Bei Jing\u0026#39;, \u0026#39;Tokyo\u0026#39;]: print(\u0026#39;This is a mega city\u0026#39;) --------------------------------------------------------------------------- NameError Traceback (most recent call last) \u0026lt;ipython-input-21-d91d0b36da73\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 if c in ['New York', 'Bei Jing', 'Tokyo']: 2 print('This is a mega c') NameError: name 'c' is not defined  city =\u0026#39;New York\u0026#39; if city in [\u0026#39;New York\u0026#39;, \u0026#39;Bei Jing\u0026#39;, \u0026#39;Tokyo\u0026#39;]: print(\u0026#39;This is a mega city\u0026#39;) This is a mega city  \n5. 中英文输入法导致的错误  英文冒号 英文括号 英文逗号 英文单双引号  if 5\u0026gt;3： print(\u0026#39;5比3大\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-23-47f8b985b82d\u0026gt;\u0026quot;, line 1 if 5\u0026gt;3： ^ SyntaxError: invalid character in identifier  if 5\u0026gt;3: print(\u0026#39;5比3大\u0026#39;) 5比3大  spam = [1, 2， 3]  File \u0026quot;\u0026lt;ipython-input-26-a003060d051a\u0026gt;\u0026quot;, line 1 spam = [1, 2， 3] ^ SyntaxError: invalid character in identifier  spam = [1, 2, 3] spam [1, 2, 3]  if 5\u0026gt;3: print(\u0026#39;5比3大’)  File \u0026quot;\u0026lt;ipython-input-30-ac2e4eb87092\u0026gt;\u0026quot;, line 2 print('5比3大’) ^ SyntaxError: EOL while scanning string literal  if 5\u0026gt;3: print(\u0026#39;5比3大\u0026#39;) 5比3大  \n6. 不同数据类型的拼接 同种数据类型 字符串/列表/元组 支持拼接\n字典/集合不支持拼接\n\u0026#39;I have \u0026#39; + 12 + \u0026#39; eggs.\u0026#39; --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-32-20c7c89a2ec6\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 'I have ' + 12 + ' eggs.' TypeError: can only concatenate str (not \u0026quot;int\u0026quot;) to str  \u0026#39;I have {}eggs.\u0026#39;.format(12) 'I have 12 eggs.'  [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]+\u0026#39;def\u0026#39; --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-35-0e8919333d6b\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 ['a', 'b', 'c']+'def' TypeError: can only concatenate list (not \u0026quot;str\u0026quot;) to list  (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;)+[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-36-90742621216d\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 ('a', 'b', 'c')+['a', 'b', 'c'] TypeError: can only concatenate tuple (not \u0026quot;list\u0026quot;) to tuple  set([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;])+set([\u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;]) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-37-ddf5fb1e6c8c\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 set(['a', 'b', 'c'])+set(['d', 'e']) TypeError: unsupported operand type(s) for +: 'set' and 'set'  grades1 = {\u0026#39;Mary\u0026#39;:99, \u0026#39;Henry\u0026#39;:77} grades2 = {\u0026#39;David\u0026#39;:88, \u0026#39;Unique\u0026#39;:89} grades1+grades2 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-38-1b1456844331\u0026gt; in \u0026lt;module\u0026gt; 2 grades2 = {'David':88, 'Unique':89} 3 ----\u0026gt; 4 grades1+grades2 TypeError: unsupported operand type(s) for +: 'dict' and 'dict'  \n7. 索引位置问题 spam = [\u0026#39;cat\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;mouse\u0026#39;] print(spam[5]) --------------------------------------------------------------------------- IndexError Traceback (most recent call last) \u0026lt;ipython-input-41-e0a79346266d\u0026gt; in \u0026lt;module\u0026gt; 1 spam = ['cat', 'dog', 'mouse'] ----\u0026gt; 2 print(spam[5]) IndexError: list index out of range  \n8. 使用字典中不存在的键 在字典对象中访问 key 可以使用 []，\n但是如果该 key 不存在，就会导致：KeyError: \u0026lsquo;zebra\u0026rsquo;\nspam = {\u0026#39;cat\u0026#39;: \u0026#39;Zophie\u0026#39;, \u0026#39;dog\u0026#39;: \u0026#39;Basil\u0026#39;, \u0026#39;mouse\u0026#39;: \u0026#39;Whiskers\u0026#39;} print(spam[\u0026#39;zebra\u0026#39;]) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) \u0026lt;ipython-input-42-92c9b44ff034\u0026gt; in \u0026lt;module\u0026gt; 3 'mouse': 'Whiskers'} 4 ----\u0026gt; 5 print(spam['zebra']) KeyError: 'zebra'  为了避免这种情况，可以使用 get 方法\nspam = {\u0026#39;cat\u0026#39;: \u0026#39;Zophie\u0026#39;, \u0026#39;dog\u0026#39;: \u0026#39;Basil\u0026#39;, \u0026#39;mouse\u0026#39;: \u0026#39;Whiskers\u0026#39;} print(spam.get(\u0026#39;zebra\u0026#39;)) None  key 不存在时，get 默认返回 None\n9. 忘了括号 当函数中传入的是函数或者方法时，容易漏写括号\nspam = {\u0026#39;cat\u0026#39;: \u0026#39;Zophie\u0026#39;, \u0026#39;dog\u0026#39;: \u0026#39;Basil\u0026#39;, \u0026#39;mouse\u0026#39;: \u0026#39;Whiskers\u0026#39;} print(spam.get(\u0026#39;zebra\u0026#39;) #end of funtion  File \u0026quot;\u0026lt;ipython-input-44-d105cc86097c\u0026gt;\u0026quot;, line 5 print(spam.get('zebra') #end of funtion ^ SyntaxError: unexpected EOF while parsing  spam = {\u0026#39;cat\u0026#39;: \u0026#39;Zophie\u0026#39;, \u0026#39;dog\u0026#39;: \u0026#39;Basil\u0026#39;, \u0026#39;mouse\u0026#39;: \u0026#39;Whiskers\u0026#39;} print(spam.get(\u0026#39;zebra\u0026#39;)) None  \n10. 漏传参数 def diyadd(x, y, z): return x+y+z diyadd(1, 2) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-46-7184f3f906ca\u0026gt; in \u0026lt;module\u0026gt; 2 return x+y+z 3 ----\u0026gt; 4 diyadd(1, 2) TypeError: diyadd() missing 1 required positional argument: 'z'  diyadd(1, 2, 4) 7  \n11. 缺失依赖库 电脑中没有相关的库\nimport packagename --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) \u0026lt;ipython-input-48-6d7d6f569116\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 import packagename ModuleNotFoundError: No module named 'packagename'  !pip install packagename \n12. 使用了python中的关键词 如try、except、def、class、object、None、True、False等\ntry = 5 print(try)  File \u0026quot;\u0026lt;ipython-input-49-508e87fe2ff3\u0026gt;\u0026quot;, line 1 try = 5 ^ SyntaxError: invalid syntax  a = 5 print(a) 5  def = 6 print(def)  File \u0026quot;\u0026lt;ipython-input-51-c797890e9b85\u0026gt;\u0026quot;, line 1 def = 6 ^ SyntaxError: invalid syntax  d = 6 print(d) 6  13. 文件编码问题 import pandas as pd df = pd.read_csv(\u0026#39;data/twitter_sentiment.csv\u0026#39;) df.head() UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 7-8: invalid continuation byte UnicodeDecodeError Traceback (most recent call last) \u0026lt;ipython-input-53-f7ee81cff3e5\u0026gt; in \u0026lt;module\u0026gt; 1 import pandas as pd 2 ----\u0026gt; 3 df = pd.read_csv('data/twitter_sentiment.csv') 4 df.head() pandas\\_libs\\parsers.pyx in pandas._libs.parsers._string_box_utf8() UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 7-8: invalid continuation byte  import pandas as pd #gbk/utf-8只能解决大部分编码问题，但不能解决全部问题 df = pd.read_csv(\u0026#39;data/twitter_sentiment.csv\u0026#39;, encoding=\u0026#39;gbk\u0026#39;) df.head() --------------------------------------------------------------------------- UnicodeDecodeError Traceback (most recent call last) \u0026lt;ipython-input-55-6aa161f42239\u0026gt; in \u0026lt;module\u0026gt; 2 3 #gbk/utf-8只能解决大部分编码问题，但不能解决全部问题 ----\u0026gt; 4 df = pd.read_csv('data/twitter_sentiment.csv', encoding='gbk') 5 df.head() c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\io\\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision) 674 ) 675 -\u0026gt; 1891 self._reader = parsers.TextReader(src, **kwds) 1892 self.unnamed_cols = self._reader.unnamed_cols 1893 pandas\\_libs\\parsers.pyx in pandas._libs.parsers.TextReader.__cinit__() pandas\\_libs\\parsers.pyx in pandas._libs.parsers.TextReader._get_header() pandas\\_libs\\parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows() pandas\\_libs\\parsers.pyx in pandas._libs.parsers.raise_parser_error() UnicodeDecodeError: 'gbk' codec can't decode byte 0xbd in position 10717: illegal multibyte sequence  上面的程序会提示编码错误， 尝试encoding编码参数传入utf-8、gbk，也没有解决问题。\n那怎么找到正确的编码参数呢？ import chardet #读取为二进制数据 binary_data = open(\u0026#39;data/twitter_sentiment.csv\u0026#39;, \u0026#39;rb\u0026#39;).read() #传给chardet.detect，稍等片刻 chardet.detect(binary_data) {'encoding': 'Windows-1252', 'confidence': 0.7291192008535122, 'language': ''}  import pandas as pd df = pd.read_csv(\u0026#39;data/twitter_sentiment.csv\u0026#39;, encoding=\u0026#39;Windows-1252\u0026#39;) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ItemID Sentiment SentimentText     0 1 0 is so sad for my APL frie...   1 2 0 I missed the New Moon trail...   2 3 1 omg its already 7:30 :O   3 4 0 .. Omgaga. Im sooo im gunna CRy. I'...   4 5 0 i think mi bf is cheating on me!!! ...     \n14. 路径字符串写法  Mac\u0026amp;Win 推荐使用 / 写法 如果使用\\ 写法，安全起见，请换成\\\\ （Mac不支持\\\\ ）  \\n \\t \\d open(\u0026#39;data/test.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() '章节设计\\n\\n第一部分 环境配置\\n第二部分 快速入门python\\n第三部分 网络爬虫\\n第四部分 简单的文本分析\\n第五部分 进阶文本分析'  open(\u0026#39;data\\test.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() --------------------------------------------------------------------------- OSError Traceback (most recent call last) \u0026lt;ipython-input-59-d855ed58b500\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 open('data\\test.txt', encoding='utf-8').read() OSError: [Errno 22] Invalid argument: 'data\\test.txt'  open(\u0026#39;data\\\\test.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() '章节设计\\n\\n第一部分 环境配置\\n第二部分 快速入门python\\n第三部分 网络爬虫\\n第四部分 简单的文本分析\\n第五部分 进阶文本分析'  open(\u0026#39;data\\Test.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() '章节设计\\n\\n第一部分 环境配置\\n第二部分 快速入门python\\n第三部分 网络爬虫\\n第四部分 简单的文本分析\\n第五部分 进阶文本分析'  了解课程  点击上方图片购买课程   点击进入详情页\n","permalink":"/blog/basic-15-common-errors/","summary":"14种常见错误 Python语法入门-课件下载 链接:https://pan.baidu.com/s/1K2fFbHuvfxIOWNrFwddm3Q 密码:zj8z\n学习编程就是在遇到错误、认识错误、解决错误的过程。遇到错误，大家要发挥主观能动性，用自己的英文阅读能力去先读一下英文报错提示，一般情况下错误提示会告诉你是什么类型的错误，错误出在哪一行。\n再结合百度谷歌，80%以上的问题都能解决。现在我们了解一下常见的问题都有哪些，如何克服这些问题。\n1. 忘记写冒号 在 if、elif、else、for、while、def语句后面忘记添加 :\nage = 42 if age == 42 print(\u0026#39;Hello!\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-10-1f5acea116cf\u0026gt;\u0026quot;, line 3 if age == 42 ^ SyntaxError: invalid syntax  age = 42 if age == 42: print(\u0026#39;Hello!\u0026#39;) Hello!  \n2. 误用 = = 是赋值操作，而判断两个值是否相等是 ==\ngender = \u0026#39;男\u0026#39; if gender = \u0026#39;男\u0026#39;: print(\u0026#39;Man\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-12-c3ceea5a9004\u0026gt;\u0026quot;, line 3 if gender = '男': ^ SyntaxError: invalid syntax  gender = \u0026#39;男\u0026#39; if gender == \u0026#39;男\u0026#39;: print(\u0026#39;Man\u0026#39;) Man","title":"python常见错误及解决办法"},{"content":"python虽然与R一样都可以做数据分析，但是在计量方面较为薄弱，python更像是干脏活，清洗数据用的。现在慢慢的python也有一些在计量的包，比如causalinference，这个包可以做因果推断分析。\n下载 click to download the code\n安装 !pip3 install causalinference \n数据导入 import pandas as pd df = pd.read_csv(\u0026#39;data.csv\u0026#39;) df Run\n    y istreatment x1 x2 x3     0 4.63639 1 -0.355052 0.441348 0.908629   1 -1.96549 0 -0.81926 -0.712998 0.0375631   2 0.581781 0 1.39134 -0.0172917 -0.804188   3 -2.06729 0 -0.831021 0.49786 0.349555   4 9.54683 1 1.68232 0.608986 0.937725    数据描述\n x1，x2，x3 协变量(控制变量) y 因变量 istreatment 处置变量D，标注每条数据隶属于treatment或control组。1为treatment， 0为control。  from causalinference import CausalModel Y = df[\u0026#39;y\u0026#39;].values D = df[\u0026#39;istreatment\u0026#39;].values X = df[[\u0026#39;x1\u0026#39;, \u0026#39;x2\u0026#39;, \u0026#39;x3\u0026#39;]].values #CausalModel参数依次为Y， D， X。其中Y为因变量 causal = CausalModel(Y, D, X) causal Run\n\u0026lt;causalinference.causal.CausalModel at 0x7fd3ad0edee0\u0026gt;  描述性统计分析 print(causal.summary_stats) Run\nSummary Statistics Controls (N_c=2509) Treated (N_t=2491) Variable Mean S.d. Mean S.d. Raw-diff -------------------------------------------------------------------------------- Y -1.012 1.742 4.978 3.068 5.989 Controls (N_c=2509) Treated (N_t=2491) Variable Mean S.d. Mean S.d. Nor-diff -------------------------------------------------------------------------------- X0 -0.343 0.940 0.336 0.961 0.714 X1 -0.347 0.936 0.345 0.958 0.730 X2 -0.313 0.940 0.306 0.963 0.650  causal.summary_stats含有的指标字段名\ncausal.summary_stats.keys() Run\ndict_keys(['N', 'K', 'N_c', 'N_t', 'Y_c_mean', 'Y_t_mean', 'Y_c_sd', 'Y_t_sd', 'rdiff', 'X_c_mean', 'X_t_mean', 'X_c_sd', 'X_t_sd', 'ndiff'])  使用OLS估计处置效应 估计处置效应最简单的方法是使用OLS方法，\nCausalModel.est_via_ols(adj)\n该方法有一个参数adj\n adj=0 模型未使用X(协变量） adj=1 模型使用了D(是否为处置组)和X（协变量）。 adj=2 模型使用了D(是否为处置组)、X（协变量）、D与X的交互 adj默认为2  causal.est_via_ols(adj=2) print(causal.estimates) Run\nTreatment Effect Estimates: OLS Est. S.e. z P\u0026gt;|z| [95% Conf. int.] -------------------------------------------------------------------------------- ATE 3.017 0.034 88.740 0.000 2.950 3.083 ATC 2.031 0.040 51.183 0.000 1.953 2.108 ATT 4.010 0.039 103.964 0.000 3.934 4.086  参数解读\n ATE 平均处置效应(average treatment eﬀect) ATC 控制组的平均处置效应(average treatment eﬀect for the controls) ATT 处置组的平均处置效应(average treatment eﬀect for the treated)  你们再试试adj设置为0和1分别运行出什么结果\n倾向得分估计 我们估计处置效应时，很希望处置组和控制组很类似。比如研究受教育水平对个人收入的影响，其他变量如家庭背景、年龄、地区等协变量存在差异，我们希望控制组和处置组的之间的协变量平衡性尽可能的好，这样两个组就会很像，当对这两个组的受教育水平进行操作时，两个组的收入差异可以认为是受教育水平带来的。\n让两个组很像，这里就用到倾向得分估计。\ncausal.est_propensity_s() print(causal.propensity) Run\nEstimated Parameters of Propensity Score Coef. S.e. z P\u0026gt;|z| [95% Conf. int.] -------------------------------------------------------------------------------- Intercept 0.005 0.035 0.145 0.885 -0.063 0.073 X1 0.999 0.041 24.495 0.000 0.919 1.079 X0 1.000 0.041 24.543 0.000 0.920 1.080 X2 0.933 0.040 23.181 0.000 0.855 1.012  分层方法估计处置效应 倾向得分估计，让两个组尽量相似，但实际上这个相似值范围有点大。比如假设受教育水平对个人收入的影响，身高、体重等颜值信息（协变量）其实对收入也是有影响的，那么就应该对人群进行分层，不同颜值水平下受教育水平对个人收入的影响。\n分层方法估计CausalModel.stratify_s 自动选择协变量\ncausal.stratify_s() print(causal.strata) Run\nStratification Summary Propensity Score Sample Size Ave. Propensity Outcome Stratum Min. Max. Controls Treated Controls Treated Raw-diff -------------------------------------------------------------------------------- 1 0.001 0.043 153 5 0.024 0.029 -0.049 2 0.043 0.069 148 8 0.056 0.059 0.142 3 0.070 0.118 283 29 0.093 0.092 0.953 4 0.119 0.178 268 45 0.147 0.147 1.154 5 0.178 0.240 247 65 0.208 0.210 1.728 6 0.240 0.361 451 174 0.299 0.300 2.093 7 0.361 0.427 196 117 0.393 0.395 2.406 8 0.427 0.499 153 159 0.465 0.464 2.868 9 0.499 0.532 82 75 0.515 0.515 2.973 10 0.532 0.568 65 91 0.551 0.553 3.259 11 0.568 0.630 114 198 0.600 0.601 3.456 12 0.630 0.758 180 445 0.693 0.696 3.918 13 0.758 0.818 77 236 0.787 0.789 4.503 14 0.818 0.876 57 255 0.845 0.849 4.937 15 0.876 0.933 23 289 0.904 0.904 5.171 16 0.933 0.998 12 300 0.957 0.963 6.822  \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/causal_inference/","summary":"python虽然与R一样都可以做数据分析，但是在计量方面较为薄弱，python更像是干脏活，清洗数据用的。现在慢慢的python也有一些在计量的包，比如causalinference，这个包可以做因果推断分析。\n下载 click to download the code\n安装 !pip3 install causalinference \n数据导入 import pandas as pd df = pd.read_csv(\u0026#39;data.csv\u0026#39;) df Run\n    y istreatment x1 x2 x3     0 4.63639 1 -0.355052 0.441348 0.908629   1 -1.96549 0 -0.81926 -0.712998 0.0375631   2 0.581781 0 1.39134 -0.0172917 -0.804188   3 -2.06729 0 -0.831021 0.49786 0.349555   4 9.54683 1 1.","title":"causalinference库 | 使用Python做因果推断"},{"content":"文献 Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. The Journal of Finance, 75(3), pp.1371-1415.\n摘要 使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对报告进行积极更改时，这种行为蕴含着公司未来运营的重要信号。\n财务报告的语言和结构的变化也对公司的未来收益产生重大影响：做空\u0026quot;变化\u0026quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。\n研究发现，对10-K的变化可以预测未来的收益、获利能力、未来的新闻公告，甚至未来的公司破产。同时，不做任何变化的公司将获得显著的异常收益。与资产价格典型的反应不足研究不同，我们发现没有任何与这些变化相关的公告效应–仅在后来通过新闻，事件或收益披露信息时才产生回报–暗示投资者并未注意到整个公众领域的这些变化。\n研究背景 之前的研究认为，尽管投资者一次对包含重大变化的财务报表的发布作出了迅时反应，但随着时间的流逝，这种公告作用是会减弱的(Brown and Tucker, 2011 and Feldman et al., 2010)。这表示10-K报告会随着时间推移，信息价值大打折扣。尽管我们复现了这个事实，即与常规文件的变更没有重大的公告效应，但我们认为，前人的研究忽略了更重要部分(如MD\u0026amp;A)对对资产价格的影响。\n确切的说，并不是报告的披露效应的信息价值变低了，而是投资者越来越难以发现报告中微妙的信息变化， 比如因为报告变得越来越冗杂。投资者只有看到某些新闻后，才会逐渐意识到之前公司报告内容变化的的真正价值。\n例如Baxter公司\n 纽约时报在 2010年4月23日 发了一条FDA将有对输液泵(infusion pumps)更严格对审批管理规定的新闻，新闻中提到了Baxter公司。新闻公布当天，Baxter股价大跌。 10天后的（2010年5月4日），Baxter宣布召回问题的输液泵产品，股价当天再次大跌。  两次负面新闻导致Baxter股价大跌超过20%，最有意思的是Baxter公司一个多月前（2010年2月23日）10-k报告中 提到 了与这两条新闻类似的 线索。\n截图中写着 Baxter的产品COLLEGUE未来可能面脸额外的处罚，而且相关销售面临着FDA、OIG、DOI和FTC越来越严格的审批，面临的执法强度也越来越大。\n因纽约时报发布的消息，股价大跌。但是大跌之前Baxter的10-k报告中似乎提示未来公司可能面临的风险，但是投资者怎么没有注意到这个重要线索呢？\n数据获取与分析方法 这篇文章用到了很多 文本数据挖掘 方法，如\n 数据采集(报告下载和信息监测) 正则表达式（数据分割与抽取） 文本相似度(计算报告变化程度)  我大致说下这几部分技术在这篇论文中的应用。\n1. 数据采集 这篇论文研究者认为，只有投资者意识到本期报告和上一期报告做对比，才能发现报告变化，进而对股价有影响。所以当有新公告公布后，投资者是否下载本期报告的同时顺带着下载上一期报告，下载量又是多少。\n下载量可以从Freedom of Information Act下载，\n可以拿到的信息包括:\n 报告文件 报告下载时间 报告下载的IP地址(可以通过这个ip来当作投资者的id)  2. 正则表达式 一个公司报告文件会有不同部分，我们需要将不同的部分分别识别出来。这里用到正则表达式，可以进行快速的数据清洗和数据抽取。\n3. 文本相似度 文本转为向量后就可以进行相似度计算,\n这里使用我开发的cntext包，可以实现cosine和jaccard相似度的计算。\nimport cntext as ct A = \u0026#39;We expect demand to increase.\u0026#39; B = \u0026#39;We expect worldwide demand to increase.\u0026#39; C = \u0026#39;We expect weakness in sales\u0026#39; print(ct.cosine_sim(A, C)) print(ct.jaccard_sim(A, B)) Run\n0.40 0.83 如果对Baxter公司多个年度对报告进行相似度计算，绘制成图就会发现2010年与前后变化很大。相似度越低，说明公司报告前后变化很大，应该引起投资者注意，如果能注意到就会避免纽约时报导致到股价暴跌。如下图\n\n案例实现 由于没有完全一样的数据，这里使用政府工作报告数据类比，使用cosine相似度画出趋势线条。\n使用相似性识别变化的时间点\n准备数据 政府工作报告 http://www.gov.cn/guowuyuan/zfgzbg.htm\nprc_reports.xlsx 链接:https://pan.baidu.com/s/1sVU3mkEcP7Z3_hbG5AVNUA 密码:zjrq\n将下载好后的 prc_reports.xlsx 文件放置于 .ipynb文件 所在的文件夹内。\nimport pandas as pd df = pd.read_excel(\u0026#39;prc_reports.xlsx\u0026#39;) df.dropna(inplace=True) df.head(10) Run\n计算相似度 运行时间大概30s， 运算结果是列表数据 cosines\nimport cntext as ct cosines = [] #row Series for idx, row in df.iterrows(): text1 = df.loc[idx, \u0026#39;report\u0026#39;] text2 = df.loc[idx, \u0026#39;report2\u0026#39;] simi = ct.cosine_sim(text1, text2) cosines.append(simi) cosines Run\n[0.44\u0026#39;, \u0026#39;0.39\u0026#39;, \u0026#39;0.35\u0026#39;, ... \u0026#39;0.62\u0026#39;, \u0026#39;0.61\u0026#39;, \u0026#39;0.60\u0026#39;] 绘制柱状图 from pyecharts.charts import Bar from pyecharts import options as opts from pyecharts.globals import CurrentConfig, NotebookType CurrentConfig.NOTEBOOK_TYPE = NotebookType.JUPYTER_NOTEBOOK bar = Bar() bar.add_xaxis(xaxis_data=[str(y) for y in df[\u0026#39;year\u0026#39;].values]) bar.add_yaxis(\u0026#34;相似度\u0026#34;, cosines, label_opts=opts.LabelOpts(is_show=False)) bar.set_global_opts(title_opts=opts.TitleOpts(title=\u0026#34;政府工作报告相似度可视化\u0026#34;)) bar.load_javascript() bar.render(\u0026#39;政府工作报告相似度可视化1.html\u0026#39;) bar.render_notebook() Run\n解读 从图中可以看到除1959年异常外，其他方面能挖掘出很多信息。从相似度整体趋势，\n1959-1992 第一阶段， 1992-至今 第二阶段\n1992年附近，第一次确立社会主义市场经济制度。之后的岁月里一直围绕着经济建设高速发展。\n同时也可以看出在第一阶段前期相似度异常的低，可以理解为新中国初建，百废待兴，对于建设者而言，组着和管理这个国家的政府也在学习如何建设新中国。而90年代后，相似度越来越高，体现了政府越来越熟悉如何治理国家，如何搞经济建设。\n\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2019-12-08-lazy-prices/","summary":"文献 Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. The Journal of Finance, 75(3), pp.1371-1415.\n摘要 使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对报告进行积极更改时，这种行为蕴含着公司未来运营的重要信号。\n财务报告的语言和结构的变化也对公司的未来收益产生重大影响：做空\u0026quot;变化\u0026quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。\n研究发现，对10-K的变化可以预测未来的收益、获利能力、未来的新闻公告，甚至未来的公司破产。同时，不做任何变化的公司将获得显著的异常收益。与资产价格典型的反应不足研究不同，我们发现没有任何与这些变化相关的公告效应–仅在后来通过新闻，事件或收益披露信息时才产生回报–暗示投资者并未注意到整个公众领域的这些变化。\n研究背景 之前的研究认为，尽管投资者一次对包含重大变化的财务报表的发布作出了迅时反应，但随着时间的流逝，这种公告作用是会减弱的(Brown and Tucker, 2011 and Feldman et al., 2010)。这表示10-K报告会随着时间推移，信息价值大打折扣。尽管我们复现了这个事实，即与常规文件的变更没有重大的公告效应，但我们认为，前人的研究忽略了更重要部分(如MD\u0026amp;A)对对资产价格的影响。\n确切的说，并不是报告的披露效应的信息价值变低了，而是投资者越来越难以发现报告中微妙的信息变化， 比如因为报告变得越来越冗杂。投资者只有看到某些新闻后，才会逐渐意识到之前公司报告内容变化的的真正价值。\n例如Baxter公司\n 纽约时报在 2010年4月23日 发了一条FDA将有对输液泵(infusion pumps)更严格对审批管理规定的新闻，新闻中提到了Baxter公司。新闻公布当天，Baxter股价大跌。 10天后的（2010年5月4日），Baxter宣布召回问题的输液泵产品，股价当天再次大跌。  两次负面新闻导致Baxter股价大跌超过20%，最有意思的是Baxter公司一个多月前（2010年2月23日）10-k报告中 提到 了与这两条新闻类似的 线索。\n截图中写着 Baxter的产品COLLEGUE未来可能面脸额外的处罚，而且相关销售面临着FDA、OIG、DOI和FTC越来越严格的审批，面临的执法强度也越来越大。\n因纽约时报发布的消息，股价大跌。但是大跌之前Baxter的10-k报告中似乎提示未来公司可能面临的风险，但是投资者怎么没有注意到这个重要线索呢？\n数据获取与分析方法 这篇文章用到了很多 文本数据挖掘 方法，如\n 数据采集(报告下载和信息监测) 正则表达式（数据分割与抽取） 文本相似度(计算报告变化程度)  我大致说下这几部分技术在这篇论文中的应用。\n1. 数据采集 这篇论文研究者认为，只有投资者意识到本期报告和上一期报告做对比，才能发现报告变化，进而对股价有影响。所以当有新公告公布后，投资者是否下载本期报告的同时顺带着下载上一期报告，下载量又是多少。\n下载量可以从Freedom of Information Act下载，\n可以拿到的信息包括:\n 报告文件 报告下载时间 报告下载的IP地址(可以通过这个ip来当作投资者的id)  2.","title":"文本相似 | Lazy Prices公司年报内容变动预示重大风险"},{"content":"cnsentiDemo 这是使用streamlit库将中文情感分析[cnsenti 部署到网络世界，可在线提供简单的中文文本的情绪及情感计算。\nstreamlit库(https://docs.streamlit.io/en/stable/)， 是目前简单易用的数据可视化web框架，比flask和django少了很多的扩展性，但是容易学习上手，适合初学者把玩。\n Demo \n网站 现在技术有限，该网站大致内容分为三部分\n 准备数据 数据分析  情感分析 词云图   谢谢支持  \n本地使用 本网站的cnsentiDemo项目文件夹的文件有\n- main.py - cnsenti_example.csv - 大邓和他的Python.png - requirements.txt - 其他文件 将cnsentiDemo项目下载，在电脑本地离线使用cnsenti的方法\n 下载解压到桌面desktop 命令行, 执行 cd desktop/cnsentiDemo 命令行，执行 pip3 install -r requirements.txt 命令行, 执行 streamlit run main.py 根据命令行的提示，复制粘贴网址到桌面。我这里是 **http://localhost:8501** 浏览器打开效果就会与视频等同  上述过程中，Mac和Win会有一些缺点导致无法使用，需要根据命令行提示解决各自系统的小问题，例如\n Win需要使用64位的Python Mac可能需要安装Xcode-install 其他可能的问题  \nWeb部署方法 如果想将自己的streamlit项目部署成网站，可以使用Heroku和github帮助你完成人生第一个小网站。操作方法：\n 将写好的streamlit项目上传至github自有仓库 Heroku注册账号 点击Heroku网页右上角New， 选择Create new app 绑定github，连接github里的streamlit项目 部署  部署方法也可参考 Youtube视频\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/cnsenti_streamlit/","summary":"cnsentiDemo 这是使用streamlit库将中文情感分析[cnsenti 部署到网络世界，可在线提供简单的中文文本的情绪及情感计算。\nstreamlit库(https://docs.streamlit.io/en/stable/)， 是目前简单易用的数据可视化web框架，比flask和django少了很多的扩展性，但是容易学习上手，适合初学者把玩。\n Demo \n网站 现在技术有限，该网站大致内容分为三部分\n 准备数据 数据分析  情感分析 词云图   谢谢支持  \n本地使用 本网站的cnsentiDemo项目文件夹的文件有\n- main.py - cnsenti_example.csv - 大邓和他的Python.png - requirements.txt - 其他文件 将cnsentiDemo项目下载，在电脑本地离线使用cnsenti的方法\n 下载解压到桌面desktop 命令行, 执行 cd desktop/cnsentiDemo 命令行，执行 pip3 install -r requirements.txt 命令行, 执行 streamlit run main.py 根据命令行的提示，复制粘贴网址到桌面。我这里是 **http://localhost:8501** 浏览器打开效果就会与视频等同  上述过程中，Mac和Win会有一些缺点导致无法使用，需要根据命令行提示解决各自系统的小问题，例如\n Win需要使用64位的Python Mac可能需要安装Xcode-install 其他可能的问题  \nWeb部署方法 如果想将自己的streamlit项目部署成网站，可以使用Heroku和github帮助你完成人生第一个小网站。操作方法：\n 将写好的streamlit项目上传至github自有仓库 Heroku注册账号 点击Heroku网页右上角New， 选择Create new app 绑定github，连接github里的streamlit项目 部署  部署方法也可参考 Youtube视频","title":"当cnsenti遇上streamlit"},{"content":"昨天我从PyData2018发现一个视频，讲如何在数据缺失的情况下挖掘出用户和产品的特征向量, 用于产品推荐系统。\n Daniel Pyrathon - A practical guide to Singular Value Decomposition in Python PyCon2018\n   如果视频无法观看，可以前往腾讯视频\n一、预备知识 1.1 协同过滤 日常生活中，像亚马逊、淘宝、京东、今日头条等各大互联网公司会无时不刻的收集我们的网络用户行为数据，并根据积累的历史行为数据对我们推送推荐内容或者推荐商品。这就是我们不曾感受到存在的推荐算法所起到的作用，这之中比较常见的实现方式是协同过滤（Collaberative Filtering）。数据设计到用户、产品及产品评价三种信息，数据类似于下图 1.2 相似的人更容易做相似的事 协同过滤的核心想法是相似的人往往会做相似的事情。比如，A 和 B 是两个崇尚科技的人（相似信息源于大量的观影数据），而 B 喜欢 看科幻片 ，那么我们猜测 A 也喜欢 科幻片。 1.3 问题提出 上面我们展示的用户电影可视化图，实际上就是推荐算法中经常用到的用户-评价矩阵,\n 那么我们如何对矩阵进行计算，才能获取相似性信息？ 有了相似性信息我们又如何去利用相似性信息去做产品推荐？ 我们知道两个向量通过余弦相似计算就可以得出两个向量的近似程度，那么这些向量我们又该如何从用户-评价矩阵提取呢？  1.4 奇异值分解SVD 这就用到奇异值分解（Singular Value Decompositon），简称SVD。具体怎么提取不是我们本文的重点，Python都帮我们实现了，我们只需要稍微了解下SVD，就直接上手用。\n比如我们现在有了用户-评价矩阵 给定一个矩阵，我们都可以分解得到两种矩阵，一种是用户信息矩阵，一种是评价信息（产品）矩阵。这两种矩阵在本例中使用了n_features = 2，即对于用户向量或者产品评价向量长度均为2，实际上也可以为其他数字（比如3，4。。） 那么User1对于蓝色电影的喜欢程度是可以通过向量计算得出3.52 1.5 用户相似性 如下图，在二维坐标中我们可以看出不同用户间的相似度。 \n二、项目实战 我们将使用Python的surprise库，对MovieLens数据集构建一个简单的协同过滤推荐系统。\n安装方法:\npip3 install scikit-surprise 如果你的anaconda自带jupyter notebook。那么你可能需要使用下面的安装方法\nconda install -c conda-forge scikit-surprise 从安装名我们发现其余scikit的特殊关系，所以熟悉scikit的同学看本文会比较轻松。\n代码下载 点击下载\n## 2.1 准备数据 MovieLens数据集含有1000个用户的100000个观影评分记录。其中我们只需要使用该数据集中的u.data文件，该文件以行存储，每一行包括``userID itemID rating timestamp``,且各个字段之间以``\\t``间隔。部分数据如下 [\u0026#39;196\\t242\\t3\\t881250949\\n\u0026#39;, \u0026#39;186\\t302\\t3\\t891717742\\n\u0026#39;, \u0026#39;22\\t377\\t1\\t878887116\\n\u0026#39;, \u0026#39;244\\t51\\t2\\t880606923\\n\u0026#39;, \u0026#39;166\\t346\\t1\\t886397596\\n\u0026#39;] \n2.2 切割数据 在surprise库中我们可以创建读取器Reader的格式。在本例中，我们使用\\t将每行数据分隔后分配给\nuser item rating timestamp\n定义好Reader格式后，我们使用Dataset对象对数据进行读取操作。\nfrom surprise import Reader, Dataset #定义数据格式 reader = Reader(line_format=\u0026#39;user item rating timestamp\u0026#39;, sep=\u0026#39;\\t\u0026#39;) #使用reader格式从u.data文件中读取数据 data = Dataset.load_from_file(\u0026#39;u.data\u0026#39;, reader=reader) \n2.3 交叉检验 surprise提供了交叉验证（crossvalidation）的接口，crossvalidation是啥？\n我们先看图解释下\n一份数据平均的分成5份，如果4份做训练集，1份做测试集。那么当我们训练模型的时候有1/5的数据我们的模型是无法学习的，这就浪费了20%。\n但是我们又不能拿把所有的数据经过一次训练，再拿其中训练过的数据去做预测。因为这样会导致准确率a非常高，但放到实践中这个模型的预测准确率实际上是低于a的。\n所以就有了crossvalidation交叉检验。我们一份数据训练5次，每次完整的数据分成4份训练1份测试。这样就解决了上面遇到的问题。如下图\n#n_folds=5是指数据分成5份，做5次训练预测 data.split(n_folds=5) \n2.4 最优化Optimization 训练怎么达到最优，那就要有Optimization，也就是要有一个可供参考的标准。\n训练的方式与其他机器学习方法类似，要使得一种算法试图优化其预测值尽可能接近真实值。在协作过滤应用中，我们的算法将尝试预测某个用户-电影组合的评级，并将该预测值与真实值进行比较。 使用经典误差测量如均方根误差（Root mean squared error，RMSE）和平均绝对误差（Mean absolute error，MAE）来测量预测值和真实值之间的差异。\n在surprise库中，我们有广泛的算法可供选择，并为每种算法（SVD，NMF，KNN）提供多种参数选择。 就我们的例子而言，我们将使用SVD算法。 优化目标measures采用RMSE', 'MAE\nfrom surprise import SVD, evaluate #相当于scikit的机器学习算法的初始化 svd = SVD() #相当于scikit中的score，模型评估 evaluate(svd, data, measures=[\u0026#39;RMSE\u0026#39;, \u0026#39;MAE\u0026#39;]) Run\nEvaluating RMSE, MAE of algorithm SVD. ------------ Fold 1 RMSE: 0.9324 MAE: 0.7346 ------------ Fold 2 RMSE: 0.9422 MAE: 0.7423 ------------ Fold 3 RMSE: 0.9367 MAE: 0.7398 ------------ Fold 4 RMSE: 0.9310 MAE: 0.7323 ------------ Fold 5 RMSE: 0.9393 MAE: 0.7422 ------------ ------------ Mean RMSE: 0.9363 Mean MAE : 0.7382 ------------ ------------ CaseInsensitiveDefaultDict(list, {'mae': [0.734621556055766, 0.7422621194493935, 0.7398192302116903, 0.7323079165231016, 0.7422361108902022], 'rmse': [0.9324301825022976, 0.9421845177536299, 0.9366580726086371, 0.9310376368987473, 0.9392636694333337]})  从上面运行结果看，optimizer选用RMSE后，5次训练的平均准确率高达93.63%。\n2.5 预测 最后我们还是很想看看训练出模型，其预测能力到底结果怎么样？\n这次我们就做交叉验证了，省事点直接全部丢给SVD去训练\nfrom surprise import SVD from surprise import Reader, Dataset #读取数据 reader = Reader(line_format=\u0026#39;user item rating timestamp\u0026#39;, sep=\u0026#39;\\t\u0026#39;) data = Dataset.load_from_file(\u0026#39;u.data\u0026#39;, reader=reader) data = data.build_full_trainset() #初始化svd模型,用data训练模型 svd =SVD() svd.fit(data) Run\n\u0026lt;surprise.prediction_algorithms.matrix_factorization.SVD at 0x10ab7d7f0\u0026gt; 上面的代码\ndata = data.build_full_trainset()\n这一行本来我没有写，但是当我注释掉这一行。出现下面的错误，\nDatasetAutoFolds\u0026#39; object has no attribute \u0026#39;global_mean\u0026#39; on python surprise 最后在stackoverflow中找到解决办法，需要将data转化为surprise能够用的trainset类。\nhttps://stackoverflow.com/questions/49263964/datasetautofolds-object-has-no-attribute-global-mean-on-python-surprise 下面继续我们的预测，userid为196，itemid为302， 其真实评分为4。\nuserid = str(196) itemid = str(302) actual_rating = 4 print(svd.predict(userid, 302, 4)) user: 196 item: 302 r_ui = 4.00 est = 3.41 {'was_impossible': False}  预测值为3.41， 真实值为4。还是相对靠谱的。\n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/svd_in_recommendation_system/","summary":"昨天我从PyData2018发现一个视频，讲如何在数据缺失的情况下挖掘出用户和产品的特征向量, 用于产品推荐系统。\n Daniel Pyrathon - A practical guide to Singular Value Decomposition in Python PyCon2018\n   如果视频无法观看，可以前往腾讯视频\n一、预备知识 1.1 协同过滤 日常生活中，像亚马逊、淘宝、京东、今日头条等各大互联网公司会无时不刻的收集我们的网络用户行为数据，并根据积累的历史行为数据对我们推送推荐内容或者推荐商品。这就是我们不曾感受到存在的推荐算法所起到的作用，这之中比较常见的实现方式是协同过滤（Collaberative Filtering）。数据设计到用户、产品及产品评价三种信息，数据类似于下图 1.2 相似的人更容易做相似的事 协同过滤的核心想法是相似的人往往会做相似的事情。比如，A 和 B 是两个崇尚科技的人（相似信息源于大量的观影数据），而 B 喜欢 看科幻片 ，那么我们猜测 A 也喜欢 科幻片。 1.3 问题提出 上面我们展示的用户电影可视化图，实际上就是推荐算法中经常用到的用户-评价矩阵,\n 那么我们如何对矩阵进行计算，才能获取相似性信息？ 有了相似性信息我们又如何去利用相似性信息去做产品推荐？ 我们知道两个向量通过余弦相似计算就可以得出两个向量的近似程度，那么这些向量我们又该如何从用户-评价矩阵提取呢？  1.4 奇异值分解SVD 这就用到奇异值分解（Singular Value Decompositon），简称SVD。具体怎么提取不是我们本文的重点，Python都帮我们实现了，我们只需要稍微了解下SVD，就直接上手用。\n比如我们现在有了用户-评价矩阵 给定一个矩阵，我们都可以分解得到两种矩阵，一种是用户信息矩阵，一种是评价信息（产品）矩阵。这两种矩阵在本例中使用了n_features = 2，即对于用户向量或者产品评价向量长度均为2，实际上也可以为其他数字（比如3，4。。） 那么User1对于蓝色电影的喜欢程度是可以通过向量计算得出3.52 1.5 用户相似性 如下图，在二维坐标中我们可以看出不同用户间的相似度。 \n二、项目实战 我们将使用Python的surprise库，对MovieLens数据集构建一个简单的协同过滤推荐系统。\n安装方法:\npip3 install scikit-surprise 如果你的anaconda自带jupyter notebook。那么你可能需要使用下面的安装方法\nconda install -c conda-forge scikit-surprise 从安装名我们发现其余scikit的特殊关系，所以熟悉scikit的同学看本文会比较轻松。","title":"推荐系统与协同过滤、奇异值分解"},{"content":"代码下载 click to download\n实验目的 如果您以前从未使用过树状图，那么使用树状图是查看多维数据如何聚集在一起的好方法。 在这本笔记本中，我将简单探索通过层次分析，借助树状图将其可视化。\n层次分析 层次分析是聚类分析的一种，scipy有这方面的封装包。\nlinkage函数从字面意思是链接，层次分析就是不断链接的过程，最终从n条数据，经过不断链接，最终聚合成一类，算法就此停止。\ndendrogram是用来绘制树形图的函数。\n实验数据 grain_variety是标签，其他列为多种属性的值（特征）。\nfrom scipy.cluster.hierarchy import linkage, dendrogram import matplotlib.pyplot as plt import pandas as pd seeds_df = pd.read_csv(\u0026#39;seeds-less-rows.csv\u0026#39;) seeds_df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  area perimeter compactness length width asymmetry_coefficient groove_length grain_variety     0 14.88 14.57 0.8811 5.554 3.333 1.018 4.956 Kama wheat   1 14.69 14.49 0.8799 5.563 3.259 3.586 5.219 Kama wheat   2 14.03 14.16 0.8796 5.438 3.201 1.717 5.001 Kama wheat   3 19.31 16.59 0.8815 6.341 3.810 3.477 6.238 Rosa wheat   4 17.99 15.86 0.8992 5.890 3.694 2.068 5.837 Rosa wheat     #移除文本数据列 varieties = list(seeds_df.pop(\u0026#39;grain_variety\u0026#39;)) varieties ['Kama wheat', 'Kama wheat', 'Kama wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Rosa wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat', 'Canadian wheat']  samples = seeds_df.values print(samples) print(\u0026#39;samples的维度\u0026#39;,samples.shape) [[14.88 14.57 0.8811 5.554 3.333 1.018 4.956 ] [14.69 14.49 0.8799 5.563 3.259 3.586 5.219 ] [14.03 14.16 0.8796 5.438 3.201 1.717 5.001 ] [19.31 16.59 0.8815 6.341 3.81 3.477 6.238 ] [17.99 15.86 0.8992 5.89 3.694 2.068 5.837 ] [18.85 16.17 0.9056 6.152 3.806 2.843 6.2 ] [19.38 16.72 0.8716 6.303 3.791 3.678 5.965 ] [17.36 15.76 0.8785 6.145 3.574 3.526 5.971 ] [13.32 13.94 0.8613 5.541 3.073 7.035 5.44 ] [11.43 13.13 0.8335 5.176 2.719 2.221 5.132 ] [11.26 13.01 0.8355 5.186 2.71 5.335 5.092 ] [12.46 13.41 0.8706 5.236 3.017 4.987 5.147 ] [11.81 13.45 0.8198 5.413 2.716 4.898 5.352 ] [11.23 12.88 0.8511 5.14 2.795 4.325 5.003 ]] samples的维度 (14, 7)  使用linkage函数对samples进行层次聚类 X = linkage(y, method=\u0026#39;single\u0026#39;, metric=\u0026#39;euclidean\u0026#39;) sacipy中y是距离矩阵，我对此只是傻傻的理解成特征矩阵。 (m*n) m行代表m条记录,n代表n个特征\n返回结果X是(m-1)*4的矩阵。 具体含义请看下面的案例\nmergings = linkage(samples) #我们发现mergings比samples少一行 print(\u0026#39;sample维度\u0026#39;,samples.shape) print(\u0026#39;mergings维度\u0026#39;,mergings.shape) sample维度 (14, 7) mergings维度 (13, 4)  #层次分析可视化，leaf的字体不旋转，大小为10。 #这里我们不显示每一条数据的具体名字标签（varieties），默认以数字标签显示 dendrogram(mergings,leaf_rotation=0,leaf_font_size=10) plt.show() #在图中显示的数字是最细粒度的叶子，相当于每个样本数据点。 mergings array([[ 3. , 6. , 0.37233454, 2. ], [11. , 12. , 0.77366442, 2. ], [10. , 15. , 0.89804259, 3. ], [ 5. , 14. , 0.90978998, 3. ], [13. , 16. , 1.02732924, 4. ], [ 0. , 2. , 1.18832161, 2. ], [ 4. , 17. , 1.28425969, 4. ], [ 7. , 20. , 1.62187345, 5. ], [ 1. , 19. , 2.02587613, 3. ], [ 9. , 18. , 2.13385537, 5. ], [ 8. , 23. , 2.323123 , 6. ], [22. , 24. , 2.87625877, 9. ], [21. , 25. , 3.12231564, 14. ]])  层次分析图从上到下看，依次是枝和叶。\n第一列和第二列代表类标签，包含叶子和枝子。\n第三列代表叶叶（或叶枝，枝枝）之间的距离\n第四列代表该层次类中含有的样本数（记录数）\nX = linkage(y, method=\u0026#39;single\u0026#39;, metric=\u0026#39;euclidean\u0026#39;) method是指计算类间距离的方法,比较常用的有3种:\n(1)single:最近邻,把类与类间距离最近的作为类间距\n(2)average:平均距离,类与类间所有pairs距离的平均\n(3)complete:最远邻,把类与类间距离最远的作为类间距\n我们写曾侧分析法函数，看看不同的method从图中有什么区别\ndef hierarchy_analysis(samples,method=\u0026#39;single\u0026#39;): mergings = linkage(samples, method=method) dendrogram(mergings, labels=varieties, leaf_rotation=45, leaf_font_size=10) plt.show() #single hierarchy_analysis(samples,method=\u0026#39;single\u0026#39;) #average hierarchy_analysis(samples,method=\u0026#39;average\u0026#39;) #complete hierarchy_analysis(samples,method=\u0026#39;complete\u0026#39;) 由于数据量比较少，complete和average方法做出来的图完全一样。 \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/hierarchy_dendrogram_tutorial/","summary":"代码下载 click to download\n实验目的 如果您以前从未使用过树状图，那么使用树状图是查看多维数据如何聚集在一起的好方法。 在这本笔记本中，我将简单探索通过层次分析，借助树状图将其可视化。\n层次分析 层次分析是聚类分析的一种，scipy有这方面的封装包。\nlinkage函数从字面意思是链接，层次分析就是不断链接的过程，最终从n条数据，经过不断链接，最终聚合成一类，算法就此停止。\ndendrogram是用来绘制树形图的函数。\n实验数据 grain_variety是标签，其他列为多种属性的值（特征）。\nfrom scipy.cluster.hierarchy import linkage, dendrogram import matplotlib.pyplot as plt import pandas as pd seeds_df = pd.read_csv(\u0026#39;seeds-less-rows.csv\u0026#39;) seeds_df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  area perimeter compactness length width asymmetry_coefficient groove_length grain_variety     0 14.","title":"使用scipy实现层次聚类分析"},{"content":"title: 1.5G数据集 | 200万条Indiegogo众筹项目信息 author: 大邓 date: \u0026#39;2022-10-16\u0026#39; slug: [] categories: [] tags: - 文本分析 - 数据集 cover: image: images/blog/indiegogo-dataset.jpeg description: 1.57G indiegogo-dataset.jpeg keywords: - Python - 文本分析 - 经济管理 - 众筹网站 - indiegogo draft: no type: post Indiegogo Indiegogo成立于2008年，全球最大的科创新品首发和众筹平台， 是美国最早的众筹平台之一。\n\n参考论文 该数据集研究价值，可用于研究市场营销、创新创业、信息管理等， 部分使用众筹数据集作为研究对象的论文。\n [1]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.管理世界.2016;5:81-98. [2]Dai, Hengchen and Dennis J. Zhang. “Prosocial Goal Pursuit in Crowdfunding: Evidence from Kickstarter.” Journal of Marketing Research 56 (2019): 498 - 517. [3]Gafni, H., Marom, D.M., Robb, A.M., \u0026amp; Sade, O. (2020). Gender Dynamics in Crowdfunding (Kickstarter): Evidence on Entrepreneurs, Backers, and Taste-Based Discrimination*. Review of Finance. [4]Jensen, Lasse Skovgaard and Ali Gürcan Özkil. “Identifying challenges in crowdfunded product development: a review of Kickstarter projects.” Design Science 4 (2018): n. pag.\n \nIndiegogo数据 2016年4月写好的Indiegogo爬虫，每月执行一次, 最新的数据 可以前往https://webrobots.io/indiegogo-dataset/\n\n‘原始’数据 Web Robot网上公开的的Indiegogo原始数据几十个 csv文件,\n\n整理 将上图的zip全部合并为一个 Indiegogo_dataset.csv , 该文件 1.57G 。\nimport pandas as pd import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) dff = pd.read_csv(\u0026#39;Indiegogo_Dataset/Indiegogo_dataset.csv\u0026#39;, on_bad_lines=\u0026#39;skip\u0026#39;) dff.head() Run\n数据集的字段有\ndf.columns Run\nIndex([\u0026#39;bullet_point\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;category_url\u0026#39;, #项目类目及url \u0026#39;clickthrough_url\u0026#39;, #进入当前项目经由的某url \u0026#39;close_date\u0026#39;, #项目截止日期 \u0026#39;currency\u0026#39;, #货币 \u0026#39;funds_raised_amount\u0026#39;, #当前已筹集的资金 \u0026#39;funds_raised_percent\u0026#39;, #筹集资金进度(当前筹资/项目目标金额) \u0026#39;image_url\u0026#39;, #图片url \u0026#39;is_indemand\u0026#39;, \u0026#39;is_pre_launch\u0026#39;, #是否为预演 \u0026#39;offered_by\u0026#39;, #项目发起人 \u0026#39;open_date\u0026#39;, #项目开始日期 \u0026#39;perk_goal_percentage\u0026#39;, \u0026#39;perks_claimed\u0026#39;, \u0026#39;price_offered\u0026#39;, #众筹价 \u0026#39;price_retail\u0026#39;, #零售价 \u0026#39;product_stage\u0026#39;, #产品阶段 \u0026#39;project_id\u0026#39;, #项目id \u0026#39;project_type\u0026#39;, #项目类型 \u0026#39;source_url\u0026#39;, #项目url \u0026#39;tagline\u0026#39;, \u0026#39;tags\u0026#39;, #标签 \u0026#39;title\u0026#39; ], #项目标题 dtype=\u0026#39;object\u0026#39;) \n数据获取  原始数据  https://webrobots.io/indiegogo-dataset/   整理的1.57G csv,  链接: https://pan.baidu.com/s/1j3PtV4GbFsyhjmr0NLbnKg 提取码: vfyc    \n广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-12-08-indiegogo-dataset/","summary":"title: 1.5G数据集 | 200万条Indiegogo众筹项目信息 author: 大邓 date: \u0026#39;2022-10-16\u0026#39; slug: [] categories: [] tags: - 文本分析 - 数据集 cover: image: images/blog/indiegogo-dataset.jpeg description: 1.57G indiegogo-dataset.jpeg keywords: - Python - 文本分析 - 经济管理 - 众筹网站 - indiegogo draft: no type: post Indiegogo Indiegogo成立于2008年，全球最大的科创新品首发和众筹平台， 是美国最早的众筹平台之一。\n\n参考论文 该数据集研究价值，可用于研究市场营销、创新创业、信息管理等， 部分使用众筹数据集作为研究对象的论文。\n [1]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.管理世界.2016;5:81-98. [2]Dai, Hengchen and Dennis J. Zhang. “Prosocial Goal Pursuit in Crowdfunding: Evidence from Kickstarter.” Journal of Marketing Research 56 (2019): 498 - 517.","title":""},{"content":"title: 1850万条 | 世界地图POI兴趣点数据集 author: 大邓 date: \u0026#39;2022-12-10\u0026#39; slug: [] categories: [] tags: - 数据集 cover: image: images/blog/OpenStreetMap.jpeg description: 1850万条世界地图POI兴趣点数据集，可用于GIS、区域经济等领域的研究 keywords: - Python - POI - 经济管理 - 地理信息系统 draft: no type: post 世界地图POI兴趣点数据集 POI数据集包含全球超过 1850 万个 POI， 数据按国家或地区组织分别以 CSV 文存档中， 数据集每月更新一次。\n数据价值 POI数据集含 区域位置、商业地点、营业时间，运营主体，网站等信息， 可用于GIS、区域经济等领域的研究。 文末有数据集获取方式 , 数据集中包含的字段有\n- ID OpenStreetMap ID - NAME 地名、国际名称 - CATEGORY、SUBCATEGORY POI类目/子类目 - LAT、LON 经度、纬度 - SRID 基于OSM标签的POI分类（14类167子类） - WKT WGS84中的geometry (WKT)； - IMAGE 链接到照片/图像； - OPENING_HOURS 营业时间 - WIKIPEDIA 链接到维基百科文章； - LAST_UPDATE 上次更新日期， - OPERATOR 运营商 - ALTERNATIVE_NAME 备用名称 - INTERNATIONAL_NAME 国际名称（通常为英文或音译为拉丁字符）； - STREET、HOUSENUMBER 地址（街道、门牌号） - POSTCODE、CITY、COUNTRY 地址（邮编、城市、国家）； - DESCRIPTION 完整描述（如果在 OSM 中列出）； - PHONE、FAX、WEBSITE、EMAIL 联系人（电话号码、传真号码、网站、邮箱）； - OTHER_TAGS 而其余标记值列在“OTHER_TAGS”列下。 \n数据质量对比 OpenStreetMap（简称OSM，中文是公开地图）是一个网上地图协作计划，目标是创造一个内容自由且能让所有人编辑的世界地图。OSM的数据有两种来源\n 广大用户的贡献（众包），包括利用 GPS 设备自行测绘和根据卫星影像地图（Bing/Yahoo!/Landsat等）绘制两种， 少数政府部门的测绘机构及商业公司根据相应授权提供。  而Google的数据则主要依靠专业测绘商采购（在中国主要是 AutoNavi/高德），以自己采集（街景）、政府部门提供（主要是NASA的Landsat影像）和用户贡献（Google Map Maker）作为补充。据此不难看出，OSM数据的优势主要体现在更新及时，而Google则胜在较强的专业性和准确性。至于数据的覆盖面，这要看OSM贡献者数量和Google财力与测绘商能力的对比。当OSM贡献者的数量和参与热情达到一定水平，其数据的数量和质量完全不逊于Google（请看OSM上德国地图）。维基百科战胜大英百科全书即是侧证。\n导入数据 以中国数据为例\nimport pandas as pd import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) df = pd.read_csv(\u0026#39;china-pois.osm.csv\u0026#39;, sep=\u0026#39;|\u0026#39;) df.head() #poi数据量 len(df) 911246  #poi数据集的字段 df.columns Index(['ID', 'NAME', 'CATEGORY', 'SUBCATEGORY', 'LON', 'LAT', 'SRID', 'WKT', 'CITY', 'IMAGE', 'EMAIL', 'COUNTRY', 'OPENING_HOURS', 'WIKIPEDIA', 'OPERATOR', 'DESCRIPTION', 'LAST_UPDATE', 'ALTERNATIVE_NAME', 'POSTCODE', 'INTERNATIONAL_NAME', 'WEBSITE', 'PHONE', 'NAME_EN', 'STREET', 'HOUSENUMBER', 'FAX', 'OTHER_TAGS'], dtype='object')  #poi类型分布 df.CATEGORY.value_counts() SETTLEMENTS 397769 TRANSPORT 198462 EDUCATION 56087 LANDUSE 50161 TOURISM 47618 SHOP 42939 EAT/DRINK 28386 PUBLICSERVICE 22905 AUTOMOTIVE 14809 ACCOMMODATION 13092 BUSINESS 12573 HEALTH 10747 RELIGIOUS 8039 SPORT 7659 Name: CATEGORY, dtype: int64  #经纬度范围 print(\u0026#39;经度(东)\u0026#39;, df.LON.max()) print(\u0026#39;经度(西)\u0026#39;, df.LON.min()) print(\u0026#39;纬度(北)\u0026#39;, df.LAT.max()) print(\u0026#39;纬度(南)\u0026#39;, df.LAT.min()) 经度(东) 135.08528800000002 经度(西) 72.2818637 纬度(北) 53.56513885988782 纬度(南) 15.1251016  字段解析  ID OpenStreetMap ID NAME 地名、国际名称 CATEGORY、SUBCATEGORY POI类目/子类目 LAT、LON 经度、纬度 SRID 基于OSM标签的POI分类（14类167子类） WKT WGS84中的geometry (WKT)； IMAGE 链接到照片/图像； OPENING_HOURS 营业时间 WIKIPEDIA 链接到维基百科文章； LAST_UPDATE 上次更新日期， OPERATOR 运营商 ALTERNATIVE_NAME 备用名称 INTERNATIONAL_NAME 国际名称（通常为英文或音译为拉丁字符）； STREET、HOUSENUMBER 地址（街道、门牌号） POSTCODE、CITY、COUNTRY 地址（邮编、城市、国家）； DESCRIPTION 完整描述（如果在 OSM 中列出）； PHONE、FAX、WEBSITE、EMAIL 联系人（电话号码、传真号码、网站、邮箱）； OTHER_TAGS 而其余标记值列在“OTHER_TAGS”列下。  下载地址 数据集下载地址\nhttp://download.slipo.eu/results/osm-to-csv/poi/\n参考资料  http://slipo.eu/?p=1551 OpenStreetMap百度词条 https://www.zhihu.com/question/19993564/answer/14428059 http://download.slipo.eu/results/osm-to-csv/poi/  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2022-12-10-1850w-poi-dataset/","summary":"title: 1850万条 | 世界地图POI兴趣点数据集 author: 大邓 date: \u0026#39;2022-12-10\u0026#39; slug: [] categories: [] tags: - 数据集 cover: image: images/blog/OpenStreetMap.jpeg description: 1850万条世界地图POI兴趣点数据集，可用于GIS、区域经济等领域的研究 keywords: - Python - POI - 经济管理 - 地理信息系统 draft: no type: post 世界地图POI兴趣点数据集 POI数据集包含全球超过 1850 万个 POI， 数据按国家或地区组织分别以 CSV 文存档中， 数据集每月更新一次。\n数据价值 POI数据集含 区域位置、商业地点、营业时间，运营主体，网站等信息， 可用于GIS、区域经济等领域的研究。 文末有数据集获取方式 , 数据集中包含的字段有\n- ID OpenStreetMap ID - NAME 地名、国际名称 - CATEGORY、SUBCATEGORY POI类目/子类目 - LAT、LON 经度、纬度 - SRID 基于OSM标签的POI分类（14类167子类） - WKT WGS84中的geometry (WKT)； - IMAGE 链接到照片/图像； - OPENING_HOURS 营业时间 - WIKIPEDIA 链接到维基百科文章； - LAST_UPDATE 上次更新日期， - OPERATOR 运营商 - ALTERNATIVE_NAME 备用名称 - INTERNATIONAL_NAME 国际名称（通常为英文或音译为拉丁字符）； - STREET、HOUSENUMBER 地址（街道、门牌号） - POSTCODE、CITY、COUNTRY 地址（邮编、城市、国家）； - DESCRIPTION 完整描述（如果在 OSM 中列出）； - PHONE、FAX、WEBSITE、EMAIL 联系人（电话号码、传真号码、网站、邮箱）； - OTHER_TAGS 而其余标记值列在“OTHER_TAGS”列下。","title":""},{"content":"title: 金融研究 | 央行货币政策文本相似度计算与可视化 author: \u0026#34;大邓\u0026#34; date: \u0026#39;2023-01-10\u0026#39; tags: - 文本分析 - 经济管理 cover: image: images/blog/simi_finance_cover_paper.png description: 本文利用金融情感词典和文本分析技术,分析中国人民银行货币政策执行报告的**文本情绪、文本相似度和文本可读性**等多维文本信息,刻画央行货币政策执行报告的文本特征,探究货币政策报告的文本信息与宏观经济和股票市场的关系。**实证研究发现,货币政策报告的文本情绪的改善会引起显著为正的股票市场价格反应, 报告文本相似度的增加会引起股票市场波动性的显著降低, 报告可读性对公布后股票市场的波动性影响不显著**。货币政策报告文本情绪还与诸多宏观经济指标显著相关。进一步研究发现,引起股票市场显著反应的是报告文本情绪中反映货币政策指引的部分,而反映宏观经济历史状态的部分对股票市场的影响不显著。本文从文本大数据分析角度证明了我国央行沟通的有效性,对国内央行沟通相关研究形成了有益补充。 keywords: - 文本相似度 - 金融研究 - 央行货币政策 draft: no type: post 姜富伟,胡逸驰,黄楠.央行货币政策报告文本信息、宏观经济与股票市场[J].金融研究,2021,(06):95-113.\n摘要:本文利用金融情感词典和文本分析技术,分析中国人民银行货币政策执行报告的文本情绪、文本相似度和文本可读性等多维文本信息,刻画央行货币政策执行报告的文本特征,探究货币政策报告的文本信息与宏观经济和股票市场的关系。实证研究发现,货币政策报告的文本情绪的改善会引起显著为正的股票市场价格反应, 报告文本相似度的增加会引起股票市场波动性的显著降低, 报告可读性对公布后股票市场的波动性影响不显著。货币政策报告文本情绪还与诸多宏观经济指标显著相关。进一步研究发现,引起股票市场显著反应的是报告文本情绪中反映货币政策指引的部分,而反映宏观经济历史状态的部分对股票市场的影响不显著。本文从文本大数据分析角度证明了我国央行沟通的有效性,对国内央行沟通相关研究形成了有益补充。\n文文相似度很好用，下图是该论文中绘制的2001-2018年间的货币政策报告文本相似度。 前后相邻两个季度的货币政策文本相似度越高，说明政策相似性高，政策连贯性强(变化小)。如果相似度较低，则政策变动的风险较大，政策连贯性差(变化大)。\n复现相似度 本文只实现文本相似度的度量、文本相似度趋势的可视化。\n 准备数据 相似度计算 可视化  1. 准备数据 首先先手动从 中国人民银行 下载货币政策报告。\n下图是我下载好的报告\n之后将其整理到csv中\nimport os import csv from pdfdocx import read_pdf with open(\u0026#39;pbc_reports.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) as csvf: #年份、季度、报告文本 fieldnames = [\u0026#39;year\u0026#39;, \u0026#39;q\u0026#39;, \u0026#39;text\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() pdfs = [\u0026#39;data/{}\u0026#39;.format(f) for f in os.listdir(\u0026#39;data\u0026#39;)] for pf in pdfs: data = { \u0026#39;year\u0026#39;: pf.split(\u0026#39;/\u0026#39;)[-1][:4], \u0026#39;q\u0026#39;: pf.split(\u0026#39;/\u0026#39;)[-1][5], \u0026#39;text\u0026#39;: read_pdf(\u0026#39;data/2013-3.pdf\u0026#39;), } writer.writerow(data) \n2. 读取数据 下载pdf时，遗漏了货币政策报告日期数据，将 pbc_reports.csv 修改为 pbc_reports.xlsx ，增加了 date 字段。\nimport pandas as pd import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) df = pd.read_excel(\u0026#39;pbc_reports.xlsx\u0026#39;) df.head() Run\n#让每一行含有前后两个季度的报告文本 df[\u0026#39;text2\u0026#39;] = df[\u0026#39;text\u0026#39;].shift(1) df = df[1:] df.head() Run\n3. 计算相似度 水平(行)方向，计算每一行中的 text 与 text2 两者的文本相似度。\nimport cntext as ct def cosine_similarity(row): #row 为 pd.Series 类型数据，类似于字段 try: sim = ct.cosine_sim(row[\u0026#39;text\u0026#39;], row[\u0026#39;text2\u0026#39;]) return float(sim) except: #异常标记为1 return 1 #计算结果存储到 similarity 字段中 df[\u0026#39;similarity\u0026#39;] = df.apply(lambda row: cosine_similarity(row), axis=1) df.head() Run\n4. 绘制折线图 这里为了方便，使用 pandas_bokeh 库。 注意: 绘图不限于Python，各位也可以用excel、R。\n参数:\n kind 图表类型，折线图line x 横轴字段 y 纵轴字段 xlabel 横轴标签 ylabel 纵轴标签 title 图标题  import pandas_bokeh pandas_bokeh.output_notebook() #选择折线图line # df.plot_bokeh(kind = \u0026#39;line\u0026#39;, x = \u0026#39;date\u0026#39;, y = \u0026#39;similarity\u0026#39;, title = \u0026#39;2001~2022央行货币政策相似度趋势\u0026#39;, xlabel = \u0026#39;报告发布日期\u0026#39;, ylabel = \u0026#39;相似度\u0026#39;) Run\n刚刚生成的图没有经过移动平滑处理，所以锯齿比较多。论文中使用三季度移动平均线处理了 similarity ，我在此将其命名为 ma3_similarity\nimport pandas_bokeh pandas_bokeh.output_notebook() #三季度移动平均线 df[\u0026#39;ma3_similarity\u0026#39;] = df[\u0026#39;similarity\u0026#39;].rolling(window=3, center=True, min_periods=1).mean() df.plot_bokeh(kind = \u0026#39;line\u0026#39;, x = \u0026#39;date\u0026#39;, y = \u0026#39;ma3_similarity\u0026#39;, title = \u0026#39;2001~2022央行货币政策相似度趋势\u0026#39;, xlabel = \u0026#39;报告发布日期\u0026#39;, ylabel = \u0026#39;相似度\u0026#39;) Run\n基本复刻论文原图相似度的变化趋势\n代码下载  代码及视频讲解已经添加至 支持开票 | Python实证指标构建与文本分析 中，感兴趣的同学欢迎订阅该系列课，涵盖Python语法入门、数据采集、文本分析、机器学习等。 未订阅 支持开票 | Python实证指标构建与文本分析 的朋友们，可转发本文集赞30+， 加微信 372335839 ， 备注「姓名-学校-专业-央行相似度」，获取本文数据及代码。  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/2023-01-10-similarity-of-cental-bank-monetary-policy/","summary":"title: 金融研究 | 央行货币政策文本相似度计算与可视化 author: \u0026#34;大邓\u0026#34; date: \u0026#39;2023-01-10\u0026#39; tags: - 文本分析 - 经济管理 cover: image: images/blog/simi_finance_cover_paper.png description: 本文利用金融情感词典和文本分析技术,分析中国人民银行货币政策执行报告的**文本情绪、文本相似度和文本可读性**等多维文本信息,刻画央行货币政策执行报告的文本特征,探究货币政策报告的文本信息与宏观经济和股票市场的关系。**实证研究发现,货币政策报告的文本情绪的改善会引起显著为正的股票市场价格反应, 报告文本相似度的增加会引起股票市场波动性的显著降低, 报告可读性对公布后股票市场的波动性影响不显著**。货币政策报告文本情绪还与诸多宏观经济指标显著相关。进一步研究发现,引起股票市场显著反应的是报告文本情绪中反映货币政策指引的部分,而反映宏观经济历史状态的部分对股票市场的影响不显著。本文从文本大数据分析角度证明了我国央行沟通的有效性,对国内央行沟通相关研究形成了有益补充。 keywords: - 文本相似度 - 金融研究 - 央行货币政策 draft: no type: post 姜富伟,胡逸驰,黄楠.央行货币政策报告文本信息、宏观经济与股票市场[J].金融研究,2021,(06):95-113.\n摘要:本文利用金融情感词典和文本分析技术,分析中国人民银行货币政策执行报告的文本情绪、文本相似度和文本可读性等多维文本信息,刻画央行货币政策执行报告的文本特征,探究货币政策报告的文本信息与宏观经济和股票市场的关系。实证研究发现,货币政策报告的文本情绪的改善会引起显著为正的股票市场价格反应, 报告文本相似度的增加会引起股票市场波动性的显著降低, 报告可读性对公布后股票市场的波动性影响不显著。货币政策报告文本情绪还与诸多宏观经济指标显著相关。进一步研究发现,引起股票市场显著反应的是报告文本情绪中反映货币政策指引的部分,而反映宏观经济历史状态的部分对股票市场的影响不显著。本文从文本大数据分析角度证明了我国央行沟通的有效性,对国内央行沟通相关研究形成了有益补充。\n文文相似度很好用，下图是该论文中绘制的2001-2018年间的货币政策报告文本相似度。 前后相邻两个季度的货币政策文本相似度越高，说明政策相似性高，政策连贯性强(变化小)。如果相似度较低，则政策变动的风险较大，政策连贯性差(变化大)。\n复现相似度 本文只实现文本相似度的度量、文本相似度趋势的可视化。\n 准备数据 相似度计算 可视化  1. 准备数据 首先先手动从 中国人民银行 下载货币政策报告。\n下图是我下载好的报告\n之后将其整理到csv中\nimport os import csv from pdfdocx import read_pdf with open(\u0026#39;pbc_reports.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) as csvf: #年份、季度、报告文本 fieldnames = [\u0026#39;year\u0026#39;, \u0026#39;q\u0026#39;, \u0026#39;text\u0026#39;] writer = csv.","title":""},{"content":"A+期刊    Areas journal      Acounting Journal of Accounting Research (UTD 24) Accounting Review(UTD24) Journal of Accounting and Economics(UTD24)    Finance Journal of Finance(UTD24) Journal of Financial Economics(UTD24) Review of Financial Studies(UTD 24)    Information Systems Information Systems Research(UTD24) MISQuarterly(UTD24)    Marketing Journal of consumer research(UTD24) Journal of Marketing(UTD24) Journal of Marketing Research(UTD24) Marketing Science(UTD24)     \n国际自然基金委认定的30本 A   管理科学学报（英文版：Journal of Management Science and Engineering (JMSE) ）\n  系统工程理论与实践\n  管理世界\n  数量经济技术经济研究\n  中国软科学\n  金融研究\n  中国管理科学\n  系统工程学报\n  会计研究\n  系统管理学报\n  管理评论\n  管理工程学报\n  南开管理评论\n  科研管理\n  情报学报\n  公共管理学报\n  管理科学\n  预测\n  运筹与管理\n  科学学研究\n  中国工业经济\n  农业经济问题\n  B  管理学报 工业工程与管理 系统工程 科学学与科学技术管理 研究与发展管理 中国人口、资源与环境 数理统计与管理 中国农村经济  \nUTD24 A1期刊 （Business Week, FT 列表期刊中未列入我院A+列表的期刊）\n1 Harvard Business Review\n2 Journal of Business Ethics\n3 Academy of Management Perspectives (AMP)\n4 Accounting, Organisations and Society (Elsevier)\n5 California Management Review (UC Berkeley)\n6 Contemporary Accounting Research (Wiley)\n7 Entrepreneurship Theory and Practice (Baylor University, Wiley)\n8 Human Resource Management (Wiley)\n9 Journal of Applied Psychology (American Psychological Association)\n10 Journal of Business Venturing (Elsevier)\n11 Journal of Consumer Psychology (Elsevier)\n12 Journal of Financial and Quantitative Analysis (Cambridge University Press)\n13 Journal of Management Studies (Wiley)\n14 Journal of the American Statistical Association (American Statistical Association)\n15 Organization Studies (SAGE)\n16 Organizational Behaviour and Human Decision Processes (Academic Press)\n17 Quarterly Journal of Economics (MIT)\n18 Rand Journal of Economics (The Rand Corporation, Wiley)\n19 Review of Accounting Studies (Springer)\n20 Sloan Management Review (MIT)\n\nA2期刊 （澳大利亚管理学院院长联合会管理学期刊列表期刊的A*类期刊，除去我院A+与A1类期刊）\n1 Academy of Management Annals\n2 Academy of Management Learning and Education\n3 Accident Analysis and Prevention\n4 Accounting, Organizations and Society\n5 ACM Transactions on Computer-Human Interaction\n6 Advances in Experimental Social Psychology\n7 American Economic Journal: Applied Economics\n8 American Economic Journal: Economic Policy\n9 American Economic Journal: Macroeconomics\n10 American Economic Journal: Microeconomics\n11 American Journal of Agricultural Economics\n12 American Journal of International Law\n13 American Journal of Political Science\n14 American Journal of Public Health\n15 American Journal of Sociology\n16 American Political Science Review\n17 American Psychologist\n18 American Sociological Review\n19 Annals of Applied Probability\n20 Annals of Applied Statistics\n21 Annals of Probability\n22 Annals of Statistics\n23 Annals of Tourism Research\n24 Annual Review of Psychology\n25 Annual Review of Sociology\n26 Auditing: A Journal of Practice and Theory\n27 Automation in Construction\n28 Biometrics\n29 Biometrika\n30 Biostatistics\n31 Boston University Law Review\n32 British Journal of Industrial Relations: an international journal of employment relations\n33 British Tax Review\n34 Cambridge Law Journal\n35 Canadian Tax Journal\n36 Columbia Law Review\n37 Commercial Law Journal\n38 Cornell Law Review\n39 Decision Sciences\n40 Decision Support Systems\n41 Econometric Theory\n42 Economic Theory\n43 Energy Economics\n44 Entrepreneurship: Theory and Practice\n45 Environment and Planning A\n46 Environment and Planning B: planning and design\n47 European Economic Review\n48 European Journal of Information Systems\n49 European Journal of Marketing\n50 European Journal of Operational Research\n51 Experimental Economics\n52 Federal Law Review\n53 Foreign Affairs\n54 Foreign Policy (Washington)\n55 Games and Economic Behavior\n56 Gender and Society\n57 Global Environmental Change\n58 Harvard Law Review\n59 Health Economics\n60 Human Relations\n61 Industrial and Labor Relations Review\n62 Industrial Marketing Management\n63 Industrial Relations: a journal of economy and society\n64 Information and Management IF score 3.890\n65 Information and Organization\n66 Information Systems Journal\n67 International Economic Review\n68 International Journal of Hospitality Management\n69 International Journal of Production Economics\n70 International Journal of Research in Marketing\n71 Journal of Applied Econometrics\n72 Journal of Banking and Finance\n73 Journal of Business and Economic Statistics\n74 Journal of Computational and Graphical Statistics\n75 Journal of Conflict Resolution: research on war and peace between and within nations\n76 Journal of Construction Engineering and Management\n77 Journal of Contract Law\n78 Journal of Corporate Finance\n79 Journal of Development Economics\n80 Journal of Econometrics\n81 Journal of Economic Behavior and Organization\n82 Journal of Economic Dynamics and Control\n83 Journal of Economic Growth\n84 Journal of Economic Literature\n85 Journal of Economic Perspectives\n86 Journal of Economic Theory\n87 Journal of Environmental Economics and Management\n88 Journal of Experimental Psychology: general\n89 Journal of Experimental Psychology: human perception and performance\n90 Journal of Experimental Psychology: learning, memory, and cognition\n91 Journal of Financial Intermediation\n92 Journal of Financial Markets\n93 Journal of Health Economics\n94 Journal of Human Resources: education, manpower and welfare economics\n95 Journal of Information Technology\n96 Journal of International Economics\n97 Journal of Labor Economics\n98 Journal of Management\n99 Journal of Management Information Systems\n100 Journal of Monetary Economics\n101 Journal of Money, Credit and Banking\n102 Journal of Organizational Behavior\n103 Journal of Personality and Social Psychology\n104 Journal of Product Innovation Management\n105 Journal of Public Economics\n106 Journal of Retailing\n107 Journal of Service Research\n108 Journal of Sport Management\n109 Journal of Strategic Information Systems\n110 Journal of Sustainable Tourism\n111 Journal of the Academy of Marketing Science 初审26天，接收522天\n112 Journal of the American Society for Information Science and Technology\n113 Journal of the Association for Information Systems\n114 Journal of the European Economic Association\n115 Journal of the Royal Statistical Society Series B: Statistical Methodology\n116 Journal of Travel Research\n117 Journal of Urban Economics\n118 Journal of Vocational Behavior\n119 Law Quarterly Review\n120 Management Accounting Research\n121 Omega\n122Organizational Research Methods\n123 Oxford University Commonwealth Law Journal\n124 Personality and Social Psychology Bulletin\n125 Personality and Social Psychology Review\n126 Personnel Psychology: a journal of applied research\n127Probability Theory and Related Fields\n128 Psychological Bulletin\n129 Psychological Review\n130 Psychological Science\n131 Quantitative Economics\n132 Regional Studies\n133 Research Policy\n134 Review of Asset Pricing Studies\n135 Review of Corporate Finance Studies\n136 Review of Economic Dynamics\n137 Review of Finance\n138 Sociology\n139 The Accounting Review\n140 The Economic Journal\n141 The European Accounting Review\n142 The Journal of Business (Chicago)\n143 The Journal of Economic History\n144 The Journal of Law and Economics\n145 The Leadership Quarterly\n146 The Modern Law Review\n147 The Quarterly Journal of Economics\n148 The Review of Economic Studies\n149 The Review of Economics and Statistics\n150 The Review of Financial Studies\n151 The Yale Law Journal\n152 Theoretical Economics\n153 Torts Law Journal\n154 Tourism Management\n155 Transportation Research Part A: Policy and Practice\n156 Transportation Research Part B: Methodological\n157 Transportation Research Part E: Logistics and Transportation Review\n158 University of Chicago Law Review\n159 Urban Studies: an international journal for research in urban studies\n\n非分类列表期刊 International Journal of Information Management\nJournal of Interactive Marketing\nComputers in Human Behavior\nInformation Systems\nBehaviour \u0026amp; Information Technology\nEI检索\nCPCI-S(ISTP)检索\nINFORMS Journal On Computing\n心理科学进展\n","permalink":"/list/","summary":"A+期刊    Areas journal      Acounting Journal of Accounting Research (UTD 24) Accounting Review(UTD24) Journal of Accounting and Economics(UTD24)    Finance Journal of Finance(UTD24) Journal of Financial Economics(UTD24) Review of Financial Studies(UTD 24)    Information Systems Information Systems Research(UTD24) MISQuarterly(UTD24)    Marketing Journal of consumer research(UTD24) Journal of Marketing(UTD24) Journal of Marketing Research(UTD24) Marketing Science(UTD24)","title":""},{"content":"前几天刚刚分享了，\n大数据时代下社会科学研究方法的拓展—基于词嵌入技术的文本分析的应用\n人类在留下语言、文字的过程中，也留下了自己的偏见、态度等主观认知信息（偏见、态度）。词嵌入做为一种词向量模型，可以隐含上下文的情景信息，态度及偏见很容易保留在词向量的某些维度中。通过词向量距离的测算，就可以间接测得不同群体 对 某概念(组织、群体、品牌、地域等)的态度偏见。\n下面整理了几篇 集智俱乐部 分享过词嵌入解读文章， 部分含视频讲解。文章末尾还有更多词嵌入的最新文献，感兴趣的同学也可以收藏。\nTips pnas的数据挖掘的论文，大多都含有数据及代码。这里有几个python库，可以可视化刻板印象\n  whatlies库|可视化词向量\n  parallax\n  WordBias\n  偏见 文化中的几何：词嵌入如何捕捉文化社会学的微妙关系  Kozlowski, A.C., Taddy, M. and Evans, J.A., 2019. The geometry of culture: Analyzing the meanings of class through word embeddings. American Sociological Review, 84(5), pp.905-949.\n 来自芝加哥大学和亚马逊的研究者，针对海量文本资料，将所有词向量分解为性别，阶级和种族三个维度，并通过将不同词向量在这三个维度上的投影来给出该词的性别、阶级和种族属性。本文是对这项工作的解读。\n点击查看详细解读\n故事的形态可预期其成功  Toubia, O., Berger, J. and Eliashberg, J., 2021. How quantifying the shape of stories predicts their success. Proceedings of the National Academy of Sciences, 118(26).\n 通过NLP，分析了电影、电视剧及科研论文的叙事模式，与其成功之间的关系。发现不同类型的文章，由于大众的认知偏好，促成其成功的叙事模式是不同。作为计算社会学的一部分，该研究通过量化分析，确认了面对不同的叙事模式，存在普遍的认知偏好。\n点击查看详细解读\n童话里都是骗人的？用词向量解析故事中的性别偏见   Xu H, Zhang Z, Wu L, Wang C_J. The Cinderella Complex: Word Embeddings Quantify Gender Stereotypes in Movies and Books. Available from https://arxiv.org/abs/1811.04599. 2019.06. Caliskan A, Bryson JJ, Narayanan A. Semantics derived automatically from language corpora contain human-like biases. Science. 2017;356: 183–186. Garg N, Schiebinger L, Jurafsky D, Zou J. Word embeddings quantify 100 years of gender and ethnic stereotypes . Proceedings of the National Academy of Sciences. 2018. pp. E3635–E3644. doi:10.1073/pnas.1720347115 Dowling C. The Cinderella Complex: Women’s Hidden Fear of Independence. 1982.   “男人是女人通往幸福的道路”——这种偏见是如何通过一个精心设计的故事创造出来的？灰姑娘式的叙事结构形成并强化了\u0026quot;灰姑娘情结\u0026quot;，即女性对独立的恐惧和被他人照顾的无意识欲望。\u0026ldquo;灰姑娘情结\u0026quot;在不同时期和不同文化中广泛存在，这提醒研究我们有必要通过教育、政策和其他方面创造新的叙述方式来与之作斗争。\n研究者提出了计算机化的框架分析，通过描绘故事的形状来测量性别刻板印象。词嵌入技术提供了一个强大的替代情感词典的方法，首先，研究团队构建一个“高兴——不高兴”的情感轴，然后计算余弦相似性来得到每一个词的情感得分。\n点击查看详细解读\n词向量带你洞悉美国性别与种族歧视的100年历史演变 性别歧视、种族歧视都是存在了上百年的社会现象，这些现象在不同历史时期有怎样的发展变化呢？发表在PNAS这篇论文中，研究者用词向量的方法研究大量文本数据，挖掘出美国近一百年文化刻板印象的演化。\n Garg, N., Schiebinger, L., Jurafsky, D. and Zou, J., 2018. Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16), pp.E3635-E3644.\n 详细解读请看 https://mp.weixin.qq.com/s/VroknX42MBdckptv4tELJg\n利用向量表征挖掘知识的创造和组织 词向量是自然语言处理中的一项基础性技术，通过词语之间的共同出现网络，可以在低维空间表征词汇间的语义相关性。4月23日发表在 Science Advences 的论文，通过论文引用网络，结合神经网络为不同的学科的科研期刊构建了连续的向量化嵌入表征，从中可以了解新知是如何被创造和组织的。\n Peng, H., Ke, Q., Budak, C., Romero, D.M. and Ahn, Y.Y., 2021. Neural embeddings of scholarly periodicals reveal complex disciplinary organizations. Science Advances, 7(17), p.eabb9004.\n 点击查看详细解读\n量化在线平台中的社会组织和政治两极分化 大量选择志同道合的人可能会分裂和极化网络社会，特别是在党派差异方面。 通过利用大规模的聚合行为模式来量化在线社区在社会维度上的定位。应用 14 年来在 Reddit 上 10,000 个社区中发表的 51 亿条评论，我们衡量了宏观社区结构在年龄、性别和美国政治党派方面的组织方式。\n检查政治内容，我们发现 Reddit 在 2016 年美国总统大选前后经历了一次重大的两极分化事件。然而，与传统观念相反，个人层面的两极分化是罕见的。 2016 年的系统级转变主要是由新用户的到来推动的。 Reddit 上的政治两极分化与平台上的先前活动无关，而是在时间上与外部事件保持一致。\n研究还观察到明显的意识形态不对称，2016 年两极分化的急剧增加完全归因于右翼活动的变化。这种方法广泛适用于在线互动的研究，我们的研究结果对在线平台的设计、理解在线行为的社会背景以及量化在线两极分化的动态和机制具有重要意义。\n Waller, I. and Anderson, A., 2021. Quantifying social organization and political polarization in online platforms. Nature, 600(7888), pp.264-268. 点击查看详细解读\n 更多文献  Arseniev-Koehler, A., Cochran, S.D., Mays, V.M., Chang, K.W. and Foster, J.G., 2022. Integrating topic modeling and word embedding to characterize violent deaths. Proceedings of the National Academy of Sciences, 119(10), p.e2108801119. Bollen, J., Ten Thij, M., Breithaupt, F., Barron, A.T., Rutter, L.A., Lorenzo-Luaces, L. and Scheffer, M., 2021. Historical language records reveal a surge of cognitive distortions in recent decades. Proceedings of the National Academy of Sciences, 118(30). Kim, L., Smith, D.S., Hofstra, B. and McFarland, D.A., 2022. Gendered knowledge in fields and academic careers. Research Policy, 51(1), p.104411. Lawson, M.A., Martin, A.E., Huda, I. and Matz, S.C., 2022. Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language. Proceedings of the National Academy of Sciences, 119(9), p.e2026443119. Brady, W.J., McLoughlin, K., Doan, T.N. and Crockett, M.J., 2021. How social learning amplifies moral outrage expression in online social networks. Science Advances, 7(33), p.eabe5641. Bailey, A.H., Williams, A. and Cimpian, A., 2022. Based on billions of words on the internet, people= men. Science Advances, 8(13), p.eabm2463. Lewis, M. and Lupyan, G., 2020. Gender stereotypes are reflected in the distributional structure of 25 languages. Nature human behaviour, 4(10), pp.1021-1028. Schramowski, P., Turan, C., Andersen, N., Rothkopf, C.A. and Kersting, K., 2022. Large pre-trained language models contain human-like biases of what is right and wrong to do. Nature Machine Intelligence, 4(3), pp.258-268. Costa-jussà, M.R., 2019. An analysis of gender bias studies in natural language processing. Nature Machine Intelligence, 1(11), pp.495-496. Rodman, E., 2020. A timely intervention: Tracking the changing meanings of political concepts with word vectors. Political Analysis, 28(1), pp.87-111. Bhatia, S., 2017. Associative judgment and vector space semantics. Psychological review, 124(1), p.1. Kurdi, B., Mann, T.C., Charlesworth, T.E. and Banaji, M.R., 2019. The relationship between implicit intergroup attitudes and beliefs. Proceedings of the National Academy of Sciences, 116(13), pp.5862-5871. Charlesworth, T.E., Yang, V., Mann, T.C., Kurdi, B. and Banaji, M.R., 2021. Gender stereotypes in natural language: Word embeddings show robust consistency across child and adult language corpora of more than 65 million words. Psychological Science, 32(2), pp.218-240. Bhatia, S., 2019. Predicting risk perception: New insights from data science. Management Science, 65(8), pp.3800-3823. Rheault, L. and Cochrane, C., 2020. Word embeddings for the analysis of ideological placement in parliamentary corpora. Political Analysis, 28(1), pp.112-133. Yang, K., Lau, R.Y. and Abbasi, A., 2022. Getting Personal: A Deep Learning Artifact for Text-Based Measurement of Personality. Information Systems Research. Rodman, E., 2020. A timely intervention: Tracking the changing meanings of political concepts with word vectors. Political Analysis, 28(1), pp.87-111. Margulis, E.H., Wong, P.C., Turnbull, C., Kubit, B.M. and McAuley, J.D., 2022. Narratives imagined in response to instrumental music reveal culture-bounded intersubjectivity. Proceedings of the National Academy of Sciences, 119(4). Thompson, B., Roberts, S.G. and Lupyan, G., 2020. Cultural influences on word meanings revealed through large-scale semantic alignment. Nature Human Behaviour, 4(10), pp.1029-1038.  广而告之  长期征稿 长期招募小伙伴 付费视频课 | Python实证指标构建与文本分析  ","permalink":"/blog/embeddingsandattitude/","summary":"前几天刚刚分享了，\n大数据时代下社会科学研究方法的拓展—基于词嵌入技术的文本分析的应用\n人类在留下语言、文字的过程中，也留下了自己的偏见、态度等主观认知信息（偏见、态度）。词嵌入做为一种词向量模型，可以隐含上下文的情景信息，态度及偏见很容易保留在词向量的某些维度中。通过词向量距离的测算，就可以间接测得不同群体 对 某概念(组织、群体、品牌、地域等)的态度偏见。\n下面整理了几篇 集智俱乐部 分享过词嵌入解读文章， 部分含视频讲解。文章末尾还有更多词嵌入的最新文献，感兴趣的同学也可以收藏。\nTips pnas的数据挖掘的论文，大多都含有数据及代码。这里有几个python库，可以可视化刻板印象\n  whatlies库|可视化词向量\n  parallax\n  WordBias\n  偏见 文化中的几何：词嵌入如何捕捉文化社会学的微妙关系  Kozlowski, A.C., Taddy, M. and Evans, J.A., 2019. The geometry of culture: Analyzing the meanings of class through word embeddings. American Sociological Review, 84(5), pp.905-949.\n 来自芝加哥大学和亚马逊的研究者，针对海量文本资料，将所有词向量分解为性别，阶级和种族三个维度，并通过将不同词向量在这三个维度上的投影来给出该词的性别、阶级和种族属性。本文是对这项工作的解读。\n点击查看详细解读\n故事的形态可预期其成功  Toubia, O., Berger, J. and Eliashberg, J., 2021. How quantifying the shape of stories predicts their success.","title":"词嵌入测量不同群体对某概念的态度(偏见)"}]