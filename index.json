[{"content":"    最近在google搜Python在经管中的内容，意外发现专著： 在会计研究中使用Python进行文本分析，内容特别新，专著中含有Python代码，也有会计领域文本分析的应用成果。跟 视频专栏课| Python网络爬虫文本分析 结合起来，特别适合会计领域python初学者，将文本分析应用于会计研究中。\n Vic Anand, Khrystyna Bochkay, Roman Chychyla and Andrew Leone (2020 isbn), “Using Python for Text Analysis in Accounting Research (forthcoming)”, Foundations and Trends ® in Accounting: Vol. xx, No. xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.\n http://dx.doi.org/10.1561/1400000062\n摘要 会计研究中文本数据的重要性显着增加。为了帮助研究人员理解和使用文本数据，本专著定义和描述了文本数据的常用度量，然后演示了使用 Python 编程语言收集和处理文本数据。该专著充满了示例代码，这些代码复现了最近研究论文中的文本分析任务。\n在专著的第一部分，我们提供了 Python 入门指南。我们首先描述 Anaconda，它是 Python 的一个发行版，它提供了文本分析所需的库及其安装。然后，我们介绍了 Jupyter notebook，这是一种改进研究工作流程并促进可复制研究的编程环境。接下来，我们将教授 Python 编程的基础知识，并演示使用 Pandas 包中的表格数据的基础知识。\n专著的第二部分侧重于会计研究中常用的具体文本分析方法和技术。我们首先介绍正则表达式，这是一种用于在文本中查找模式的复杂语言。然后我们将展示如何使用正则表达式从文本中提取特定部分。接下来，我们介绍将文本数据（非结构化数据）转换为表示感兴趣变量（结构化数据）的数值度量的想法。具体来说，我们介绍了基于字典的方法\n 测量文档情绪， 计算文本复杂度， 识别前瞻性句子和风险披露， 收集文本中的信息量，以及 计算不同文本片段的相似度。  对于这些任务中的每一个，我们引用相关论文并提供代码片段来实现这些论文中的相关指标。\n最后，专著的第三部分侧重于自动化文本数据的收集。我们介绍了网页抓取并提供了从 EDGAR 下载文件的代码。\n关键词 文本分析，数据收集，Python，自然语言处理\nUsing Python for Text Analysis in Accounting Research (forthcoming)目录 1. 引言 2. 在电脑中配置Python  2.1 Python包的作用 2.2 Anaconda软件版本 2.3 安装Anaconda 2.4 Anaconda的使用  3. Jupyter Notebook  3.1 案例 JupyterLab: Jupyter Notebook的开发版(最新版) 如何启动JupyterLab 在JupyterLab中写代码 Markdown标记语言与格式化文本代码块  4. Python编程语言简要介绍  4.1 基础知识 4.2 变量与数据类型 4.3 操作 4.4 print函数 4.5 控制流 4.6 函数 4.7 集合类型数据-list、tuple、dictionaries 4.8 处理字符串  5. 处理表数据： Pandas包  5.1 Pandas使用场景 5.2 导入import 声明 5.3 加载数据、导出数据 5.4 在pandas中查看数据 5.5 筛选数据 5.6 创建新列（字段） 5.7 删除列（字段）、列（字段）名重命名 5.8 对数据排序 5.9 合并数据  6 正则表达式介绍  6.1 查看文本中的模式 6.2 字符与字符集 6.3 Regex的定位与边界 6.4 模式匹配次数限定 6.5 分组 \u0026hellip;  7. 基于字典法 的文本分析  7.1 字典法文本分析的优势 7.2 理解字典 7.3 识别文本中的词语与句子 7.4 词干化、词形还原 7.5 词语权重 7.6 基于词典法的词频统计函数  8. 量化文本复杂度  8.1 理解文本复杂度 8.2 计算文本字符长度 8.3 使用Fog指数测量文本可读性 8.4 使用BOG指数测量文本可读性  9. 句子结构与分类  9.1 识别前瞻性陈述forward-looking sentences 9.2 使用字典法做文本分类 9.3 识别句子的主语与宾语 9.4 识别命名实体 9.5 词性标注与命名实体识别任务  10. 测量文本相似度  10.1 使用相似度比较文本 10.2 长文本使用cosine相似度计算相似度 10.3 短文本使用Levenshtein距离计算相似度 10.4 使用word2vec词嵌入计算语义相似度  11. 识别文本中的具体信息  11.1 文本识别与抽取 11.2 案例: 从10-k filing中提取出MD\u0026amp;A 11.3 案例: 从10-k html网页文件中提取处MD\u0026amp;A 11.4 从XBRL金融报告中抽取文本  12. 从网络中收集数据  12.1 在互联网中采集数据 12.2 证券交易委员会的EDGAR数据 12.3 网络爬虫 12.4 关于api接口  致谢 参考文献(部分)  Bentley, J. W., T. E. Christensen, K. H. Gee, and B. C. Whipple. 2018. “Disentangling managers’ and analysts’ non-GAAP reporting”. Journal of Accounting Research. 56(4): 1039–1081.\nBlankespoor, E. 2019. “The impact of information processing costs on ﬁrm disclosure choice: Evidence from the XBRL mandate”. Journal of Accounting Research. 57(4): 919–967.\nBochkay, K., R. Chychyla, and D. Nanda. 2019. “Dynamics of CEO disclosure style”. The Accounting Review. 94(4): 103–140.\nBochkay, K., J. Hales, and S. Chava. 2020. “Hyperbole or reality? Investor response to extreme language in earnings conference calls”. The Accounting Review. 95(2): 31–60.\nBochkay, K. and C. B. Levine. 2019. “Using MD\u0026amp;A to improve earnings forecasts”. Journal of Accounting, Auditing \u0026amp; Finance. 34(3): 458482.\nBonsall, S. B., A. J. Leone, B. P. Miller, and K. Rennekamp. 2017. “A plain English measure of ﬁnancial reporting readability”. Journal of Accounting and Economics. 63(2): 329–357.\nBozanic, Z., D. T. Roulstone, and A. Van Buskirk. 2018. “Management earnings forecasts and other forward-looking statements”. Journal of Accounting and Economics. 65(1): 1–20.\nChychyla, R., A. J. Leone, and M. Minutti-Meza. 2019. “Complexity of ﬁnancial reporting standards and accounting expertise”. Journal of Accounting and Economics. 67(1): 226–253.\nGow, I. D., D. F. Larcker, and A. A. Zakolyukina. 2019. “Non-answers during conference calls”. Chicago Booth Research Paper. (19-01). Guay, W., D. Samuels, and D. Taylor. 2016. “Guiding through the Fog:Financial statement complexity and voluntary disclosure”. Journal of Accounting and Economics. 62(2): 234–269.\nHeitmann, M., C. Siebert, J. Hartmann, and C. Schamp. 2020. “More Than a Feeling: Benchmarks for Sentiment Analysis Accuracy”. Working Paper, https://papers.ssrn.com/sol3/papers.cfm?abstract_ id=3489963.\n 本书下载 https://github.com/hiDaDeng/DaDengAndHisPython/blob/master/Using_Python_For_Text_Analysis_In_Accounting_Research.pdf\n","permalink":"/blog/%E5%9C%A8%E4%BC%9A%E8%AE%A1%E7%A0%94%E7%A9%B6%E4%B8%AD%E4%BD%BF%E7%94%A8python%E8%BF%9B%E8%A1%8C%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/","summary":"最近在google搜Python在经管中的内容，意外发现专著： 在会计研究中使用Python进行文本分析，内容特别新，专著中含有Python代码，也有会计领域文本分析的应用成果。跟 视频专栏课| Python网络爬虫文本分析 结合起来，特别适合会计领域python初学者，将文本分析应用于会计研究中。\n Vic Anand, Khrystyna Bochkay, Roman Chychyla and Andrew Leone (2020 isbn), “Using Python for Text Analysis in Accounting Research (forthcoming)”, Foundations and Trends ® in Accounting: Vol. xx, No. xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.\n http://dx.doi.org/10.1561/1400000062\n摘要 会计研究中文本数据的重要性显着增加。为了帮助研究人员理解和使用文本数据，本专著定义和描述了文本数据的常用度量，然后演示了使用 Python 编程语言收集和处理文本数据。该专著充满了示例代码，这些代码复现了最近研究论文中的文本分析任务。\n在专著的第一部分，我们提供了 Python 入门指南。我们首先描述 Anaconda，它是 Python 的一个发行版，它提供了文本分析所需的库及其安装。然后，我们介绍了 Jupyter notebook，这是一种改进研究工作流程并促进可复制研究的编程环境。接下来，我们将教授 Python 编程的基础知识，并演示使用 Pandas 包中的表格数据的基础知识。\n专著的第二部分侧重于会计研究中常用的具体文本分析方法和技术。我们首先介绍正则表达式，这是一种用于在文本中查找模式的复杂语言。然后我们将展示如何使用正则表达式从文本中提取特定部分。接下来，我们介绍将文本数据（非结构化数据）转换为表示感兴趣变量（结构化数据）的数值度量的想法。具体来说，我们介绍了基于字典的方法\n 测量文档情绪， 计算文本复杂度， 识别前瞻性句子和风险披露， 收集文本中的信息量，以及 计算不同文本片段的相似度。  对于这些任务中的每一个，我们引用相关论文并提供代码片段来实现这些论文中的相关指标。","title":"在会计研究中使用Python进行文本分析"},{"content":"多媒体文件的下载包括\n 图片 音频 视频 文件  代码非常简单，只要准备好多媒体文件链接url和存储路径file即可，代码如下\nimport requests def download(url, file): \u0026#34;\u0026#34;\u0026#34; 下载多媒体及文件 url： 多媒体文件链接（结尾有文件格式名） file: 存储文件的路径（结尾有文件格式名） \u0026#34;\u0026#34;\u0026#34; resp = requests.get(url) #获取到二进制数据 binarydata = resp.content #以二进制形式将数据流存入fname中 with open(file, \u0026#39;wb\u0026#39;) as f: f.write(binarydata) 案例 视频由于体积太大未能上传至我的博客服务器，我为大家准备了音频、pdf文件、图片文件三种数据类型。其实不论什么类型，只要是文件，均可使用上面的download函数下载。\npics = [\u0026#39;\u0026#39;] pdfs=[\u0026#39;\u0026#39;] musics=[\u0026#39;\u0026#39;] ","permalink":"/blog/multimediaexamples/","summary":"多媒体文件的下载包括\n 图片 音频 视频 文件  代码非常简单，只要准备好多媒体文件链接url和存储路径file即可，代码如下\nimport requests def download(url, file): \u0026#34;\u0026#34;\u0026#34; 下载多媒体及文件 url： 多媒体文件链接（结尾有文件格式名） file: 存储文件的路径（结尾有文件格式名） \u0026#34;\u0026#34;\u0026#34; resp = requests.get(url) #获取到二进制数据 binarydata = resp.content #以二进制形式将数据流存入fname中 with open(file, \u0026#39;wb\u0026#39;) as f: f.write(binarydata) 案例 视频由于体积太大未能上传至我的博客服务器，我为大家准备了音频、pdf文件、图片文件三种数据类型。其实不论什么类型，只要是文件，均可使用上面的download函数下载。\npics = [\u0026#39;\u0026#39;] pdfs=[\u0026#39;\u0026#39;] musics=[\u0026#39;\u0026#39;] ","title":"使用Python采集多媒体文件数据"},{"content":"pyjanitor是参照R语言janitor包语法，为Python量身定制的数据清洗包,即可清洗数据，又可让代码简洁干净。\n安装 !pip3 install pyjanitor \n为什么用janitor？ 数据预处理通常由一系列步骤组成，这些步骤涉及将原始数据转换为可理解/可用的格式。这一系列的步骤需要按照一定的顺序运行才能成功。我们以基础数据文件为起点，对其执行操作，例如删除空行/空行、用其他值替换它们、添加/重命名/删除数据列、过滤行等。更正式地说，这些步骤以及它们的关系和依赖关系通常被称为有向无环图 (DAG)。\npandas API 对 Python 数据科学生态系统非常宝贵，它实现了方法子集的方法链作为 API 的一部分。例如，重置索引 (.reset_index())、删除空值 (.dropna()) 等都是通过适当的 pd.DataFrame 方法调用来完成的。\n受 R 统计语言生态系统 dplyr 包的易用性和表达能力的启发，我们将 pyjanitor 开发为语法包，用于为 Pandas 用户表达数据处理 DAG。\n为了实现这一点，我们需要调用声明式的操作 替换为允许 逻辑顺序的方法链。让我们看看下面带注释的示例。首先，这里是数据清理路径的文字描述：\n 创建一个dataframe。 删除一列。 删除两个特定列中具有空值的行。 重命名另外两列。 添加一个新列。  让我们导入一些库并从本示例的一些示例数据开始：\n# Libraries import numpy as np import pandas as pd import janitor # Sample Data curated for this example company_sales = { \u0026#39;SalesMonth\u0026#39;: [\u0026#39;Jan\u0026#39;, \u0026#39;Feb\u0026#39;, \u0026#39;Mar\u0026#39;, \u0026#39;April\u0026#39;], \u0026#39;Company1\u0026#39;: [150.0, 200.0, 300.0, 400.0], \u0026#39;Company2\u0026#39;: [180.0, 250.0, np.nan, 500.0], \u0026#39;Company3\u0026#39;: [400.0, 500.0, 600.0, 675.0] } \n常见的Pandas实现方式 下面是传统的Pandas方式\n# The Pandas Way # 1. 创建一个dataframe df = pd.DataFrame(company_sales) # 2.删除一列。 Say \u0026#39;Company1\u0026#39; del df[\u0026#39;Company1\u0026#39;] # 3. 删除两个特定列中具有空值的行。 \u0026#39;Company2\u0026#39; and \u0026#39;Company3\u0026#39; df = df.dropna(subset=[\u0026#39;Company2\u0026#39;, \u0026#39;Company3\u0026#39;]) # 4. 重命名另外两列。 将\u0026#39;Company2\u0026#39; 改为 \u0026#39;Amazon\u0026#39;； 将 \u0026#39;Company3\u0026#39; 改为 \u0026#39;Facebook\u0026#39; df = df.rename({\u0026#39;Company2\u0026#39;: \u0026#39;Amazon\u0026#39;, \u0026#39;Company3\u0026#39;: \u0026#39;Facebook\u0026#39;}, axis=1) # 5. 添加一个新列 \u0026#39;Google\u0026#39; df[\u0026#39;Google\u0026#39;] = [450.0, 550.0, 800.0] df | | SalesMonth | Amazon | Facebook | Google | | ---: | :--------- | -----: | -------: | -----: | | 0 | Jan | 180 | 400 | 450 | | 1 | Feb | 250 | 500 | 550 | | 3 | April | 500 | 675 | 800 | \n稍微高级一点Pandas实现方式 稍微高级一点的用户可能会利用函数式 API：\ndf = ( pd.DataFrame(company_sales) .drop(columns=\u0026#34;Company1\u0026#34;) .dropna(subset=[\u0026#34;Company2\u0026#34;, \u0026#34;Company3\u0026#34;]) .rename(columns={\u0026#34;Company2\u0026#34;: \u0026#34;Amazon\u0026#34;, \u0026#34;Company3\u0026#34;:\u0026#34;Facebook\u0026#34;}) .assign(Google=[450.0, 550.0, 800.0]) ) df | | SalesMonth | Amazon | Facebook | Google | | ---: | :--------- | -----: | -------: | -----: | | 0 | Jan | 180 | 400 | 450 | | 1 | Feb | 250 | 500 | 550 | | 3 | April | 500 | 675 | 800 | \nPyJanitor实现方式 借助pyjanitor库，我们可以使用方法名链式代码\ndf = ( pd.DataFrame(company_sales) .remove_columns([\u0026#39;Company1\u0026#39;]) .dropna(subset=[\u0026#34;Company2\u0026#34;, \u0026#34;Company3\u0026#34;]) .rename_column(\u0026#34;Company2\u0026#34;, \u0026#34;Amazon\u0026#34;) .rename_column(\u0026#34;Company3\u0026#34;, \u0026#34;Facebook\u0026#34;) .add_column(\u0026#34;Google\u0026#34;, [450.0, 550.0, 800.0]) ) df | | SalesMonth | Amazon | Facebook | Google | | ---: | :--------- | -----: | -------: | -----: | | 0 | Jan | 180 | 400 | 450 | | 1 | Feb | 250 | 500 | 550 | | 3 | April | 500 | 675 | 800 | 因此，pyjanitor 的词源与“清洁度”有双重关系。 首先，它是关于使用方便的数据清理例程扩展 Pandas。 其次，它是关于为常见的 Pandas 例程提供更清晰、方法链接、基于动词的 API。\npyjanitor更多功能  清理列名（多索引是可能的！） 删除空行和列 识别重复条目 将列编码为分类 将数据拆分为特征和目标（用于机器学习） 添加、删除和重命名列 将多列合并为一列 日期转换（从 matlab、excel、unix）到 Python 日期时间格式 将具有分隔的分类值的单个列扩展为虚拟编码变量 基于分隔符连接和分离列 用于根据列上的查询过滤数据框的语法糖 金融、生物、化学、工程和 pyspark 的实验子模块  代码下载 https://github.com/hiDaDeng/DaDengAndHisPython/blob/master/20211125pyjanitor学习.ipynb\n","permalink":"/blog/pyjanitor%E5%AD%A6%E4%B9%A0/","summary":"pyjanitor是参照R语言janitor包语法，为Python量身定制的数据清洗包,即可清洗数据，又可让代码简洁干净。\n安装 !pip3 install pyjanitor \n为什么用janitor？ 数据预处理通常由一系列步骤组成，这些步骤涉及将原始数据转换为可理解/可用的格式。这一系列的步骤需要按照一定的顺序运行才能成功。我们以基础数据文件为起点，对其执行操作，例如删除空行/空行、用其他值替换它们、添加/重命名/删除数据列、过滤行等。更正式地说，这些步骤以及它们的关系和依赖关系通常被称为有向无环图 (DAG)。\npandas API 对 Python 数据科学生态系统非常宝贵，它实现了方法子集的方法链作为 API 的一部分。例如，重置索引 (.reset_index())、删除空值 (.dropna()) 等都是通过适当的 pd.DataFrame 方法调用来完成的。\n受 R 统计语言生态系统 dplyr 包的易用性和表达能力的启发，我们将 pyjanitor 开发为语法包，用于为 Pandas 用户表达数据处理 DAG。\n为了实现这一点，我们需要调用声明式的操作 替换为允许 逻辑顺序的方法链。让我们看看下面带注释的示例。首先，这里是数据清理路径的文字描述：\n 创建一个dataframe。 删除一列。 删除两个特定列中具有空值的行。 重命名另外两列。 添加一个新列。  让我们导入一些库并从本示例的一些示例数据开始：\n# Libraries import numpy as np import pandas as pd import janitor # Sample Data curated for this example company_sales = { \u0026#39;SalesMonth\u0026#39;: [\u0026#39;Jan\u0026#39;, \u0026#39;Feb\u0026#39;, \u0026#39;Mar\u0026#39;, \u0026#39;April\u0026#39;], \u0026#39;Company1\u0026#39;: [150.","title":"pyjanitor数据分析清洁包"},{"content":"这是北京语言大学智能计算机辅助语言学习（ICALL）研究组维护的文本可读性阅读清单。\n   目录     1. 综述   2. 相关研究   2.1 中文可读性   2.2 其他语言可读性   3. 可读性分析工具   4 中文数据    1. 综述   Klare, G. R. (1974–1975). Assessing readability. Reading Research Quarterly.\n  吴思远, 蔡建永, 于东, 江新. 2018. 文本可读性的自动分析研究综述. 中文信息学报.\n  郭凯、金檀、陆小飞. 2018. 文本难度调控的研究与实践——从可读公式、多维特征到智能改编. 外语测试与教学.\n  2. Related Task 2.1 Research on Chinese Readability   Yao-Ting Sung, Tao Hsing Chang. 2016. CRIE: An automated analyzer for Chinese texts. Behavior Research Methods.\n  Yao-Ting Sung, Weic Lin, SB Dyson, Kuoen Chang. 2015. Leveling L2 Texts Through Readability: Combining Multilevel Linguistic Features with the CEFR. *The Modern Language Journal.\n  LAU Tak Pang. 2006. Chinese Readability Analysis and its Applications on the Internet. Master\u0026rsquo;s thesis, The Chinese University of Hong Kong.\n  Yu Qiaona. 2016.Defining and Assessing Chinese Syntactic Complexity via TC-Units. Doctor\u0026rsquo;s thesis, University of Hawaii at Manoa.\n  2.2 Research on Readability in Other Languages   Arthur C. Graesser, Danielle S. McNamara. 2004. Coh-Metrix: Analysis of text on cohesion and language. Behavior Research Methods, Instruments, \u0026amp; Computers.\n  Arthur C. Graesser, Danielle S. McNamara. 2011. Coh-Metrix: Providing multilevel analysis of text characteristic. Educational Researcher.\n  Xiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics.\n  Xiaofei Lu. 2013. A corpus-based comparison of syntactic complexity in NNS and NS university students’ writing. Automatic Treatment and Analysis of Learner Corpus Data\n  陆小飞, 许琪. 2016. 二语句法复杂度分析器及其在二语写作研究中的应用. 外语教学与研究\n  Xiaofei Lu. 2017. Automated measurement of syntactic complexity in corpus-based L2 writing research and implications for writing assessment. Language Testing. Language Testing\n  Jin, T., Lu, X., \u0026amp; Ni, J. (2020). Syntactic complexity in adapted teaching materials: Differences among grade levels and implications for benchmarking. The Modern Language Journal\n  Menglin Xia ,Ekaterina Kochmar ,Ted Briscoe. 2016. Text Readability Assessment for Second Language Learners. Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications\n  Xiaobin Chen, Detmar Meurers. 2016. CTAP: A Web-Based Tool Supporting Automatic Complexity Analysis. Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity.\n  Chen, X. 2018. Automatic Analysis of Linguistic Complexity and Its Application in Language Learning Research, PhD thesis in computational linguistics, Eberhard Karls Universität Tübingen.\n  Nadezda Okinina, Jennifer-Carmen Frey. CTAP for Italian: Integrating Components for the Analysis of Italian into a Multilingual Linguistic Complexity Analysis Tool.\n  Zarah Weiss, Z. 2017. Using Measures of Linguistic Complexity to Assess German L2 Proficiency in Learner Corpora under Consideration of Task-Effects. M.A. Thesis in Computational Linguistics.\n  Weiss Z, Meurers D. 2019. Broad Linguistic Modeling is Beneficial for German L2 Proficiency Assessment. Widening the Scope of Learner Corpus Research, Selected Papers from the Fourth Learner Corpus Research Conference.\n  S Tonelli, KT Manh, E Pianta. 2012. Making readability indices readable. Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations.\n  Lijun Feng. 2010. Automatic Readability Assessment. *Doctor\u0026rsquo;s thesis, City University of New York.\n  3. Readability Analysis Tools   Lu Xiaofei (2010). Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics. (Web-based L2 Syntactical Complexity Analyzer (L2SCA))\n  Yao-Ting Sung, Tao Hsing Chang. 2016. CRIE: An automated analyzer for Chinese texts. Behavior Research Methods. (文本可读性指标自动化分析系统(Chinese Readability Index Explorer, CRIE))\n  Arthur C. Graesser, Danielle S. McNamara . 2011. Coh-Metrix: Providing multilevel analysis of text characteristic. Educational Researcher. (中文文本自动化分析系统: Coh-Metrix)\n  Xiaobin Chen, Detmar Meurers. 2016. CTAP: A Web-Based Tool Supporting Automatic Complexity Analysis. Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC)). (CTAP)\n  金檀、陆小飞、郭凯、李百川. 2018. Eng-Editor: An online English text evaluation and adaptation system. 广州：语言数据网(languagedata.net/tester). ( 英语阅读分级指难针 )\n  4. Chinese Data Resources  汉语词法难度分级表 汉语句法难度分级表 国际汉语教师语法教学手册 国际汉语教师中级语法教学手册  ","permalink":"/blog/%E6%96%87%E6%9C%AC%E5%8F%AF%E8%AF%BB%E6%80%A7textreadability/","summary":"这是北京语言大学智能计算机辅助语言学习（ICALL）研究组维护的文本可读性阅读清单。\n   目录     1. 综述   2. 相关研究   2.1 中文可读性   2.2 其他语言可读性   3. 可读性分析工具   4 中文数据    1. 综述   Klare, G. R. (1974–1975). Assessing readability. Reading Research Quarterly.\n  吴思远, 蔡建永, 于东, 江新. 2018. 文本可读性的自动分析研究综述. 中文信息学报.\n  郭凯、金檀、陆小飞. 2018. 文本难度调控的研究与实践——从可读公式、多维特征到智能改编. 外语测试与教学.\n  2. Related Task 2.1 Research on Chinese Readability   Yao-Ting Sung, Tao Hsing Chang.","title":"文本可读性研究及应用清单"},{"content":"Python 科学可视化领域是巨大的，由无数工具组成，从最通用和最广泛使用的工具到更专业和机密的工具。其中一些工具是基于社区的，而另一些则是由公司开发的。有些是专门为 Web 制作的，有些仅适用于桌面，有些处理 3D 和大数据，而有些则针对完美的 2D 渲染。\n在这个图景中，Matplotlib 有着非常特别的地方。\n 它是一个多功能且功能强大的库，可让您设计非常高质量的图形，适用于科学出版。 它还提供了一个简单直观的界面以及一个面向对象的架构，允许您调整图形中的任何内容。 最后，它可以用作常规图形库以设计非科学图形。  本书章节四个部分   第一部分 Matplotlib 库的基本原理。\n这包括回顾构成图形的不同部分、不同的坐标系、可用的比例和投影，我们还将介绍一些与排版和颜色相关的概念。\n  第二部分 图形涉及实践。\n在介绍了一些生成更好图形的简单规则之后，我们将继续解释 Matplotlib 默认值和样式系统，然后再深入研究图形布局组织。然后我们将探索可用的不同类型的情节，看看如何用不同的元素装饰一个人物。\n  第三部分 更高级的概念\n即 3D 图形、优化和动画。第四部分也是最后一部分是展示集合。\n  美图展览       案例代码   import numpy as np import matplotlib.pyplot as plt from matplotlib.textpath import TextPath from matplotlib.patches import PathPatch from matplotlib.collections import PolyCollection from matplotlib.font_manager import FontProperties from matplotlib import font_manager as fm, rcParams import matplotlib.pyplot as plt fig, ax = plt.subplots() #更改字体，支持中文。 prop = FontProperties(fname=\u0026#39;fonts/Alibaba-PuHuiTi-Bold.otf\u0026#39;, weight=100) red = np.array([233, 77, 85, 255]) / 255 darkred = np.array([130, 60, 71, 255]) / 255 fig = plt.figure(figsize=(14.8 / 2.54, 21 / 2.54)) ax = fig.add_axes([0, 0, 1, 1], aspect=1, xlim=[-10, 10], ylim=[-14.2, 14.2]) ax.axis(\u0026#34;off\u0026#34;) # Text path path = TextPath((0, 0), \u0026#34;MATPLOTLIB库\u0026#34;, size=2, prop=prop) # Text centering V = path.vertices xmin, xmax = V[:, 0].min(), V[:, 0].max() ymin, ymax = V[:, 1].min(), V[:, 1].max() V -= (xmin + xmax) / 2 + 1, (ymin + ymax) / 2 # Compute shadow by iterating over text path segments polys = [] for (point, code) in path.iter_segments(curves=False): if code == path.MOVETO: points = [point] elif code == path.LINETO: points.append(point) elif code == path.CLOSEPOLY: points.append(points[0]) points = np.array(points) for i in range(len(points) - 1): p0, p1 = points[i], points[i + 1] polys.append([p0, p1, p1 + (+20, -20), p0 + (+20, -20)]) # Display shadow collection = PolyCollection( polys, closed=True, linewidth=0.0, facecolor=darkred, zorder=-10 ) ax.add_collection(collection) # Display text patch = PathPatch(path, facecolor=\u0026#34;white\u0026#34;, edgecolor=\u0026#34;none\u0026#34;, zorder=10) ax.add_artist(patch) # Transparent gradient to fade out shadow I = np.zeros((200, 1, 4)) + red ax.imshow(I, extent=[-11, 11, -15, 15], zorder=-20, clip_on=False) I[:, 0, 3] = np.linspace(0, 1, len(I)) ax.imshow(I, extent=[-11, 11, -15, 15], zorder=0, clip_on=False) ax.text( 6.5, -1.75, \u0026#34;一个多功能的科学可视化库\u0026#34;, color=\u0026#34;white\u0026#34;, ha=\u0026#34;right\u0026#34;, va=\u0026#34;baseline\u0026#34;, size=10, #family=\u0026#34;Pacifico\u0026#34;, zorder=30, fontproperties=prop ) # Save and show result plt.savefig(\u0026#34;text-shadow.pdf\u0026#34;) plt.savefig(\u0026#34;text-shadow.png\u0026#34;, dpi=600) plt.show() \n电子书下载 您可以阅读 PDF（95Mo，首选站点）这本书，该书是开放访问的，托管在 HAL 上，HAL 是一个面向学术界的法国开放档案馆。最新版本也可以在 GitHub 上找到。本书的来源（包括代码示例）可在 github.com/rougier/scientific-visualization-book 上找到。\n代码下载 https://github.com/hiDaDeng/DaDengAndHisPython/tree/master/matplotlib绘图支持中文\n","permalink":"/blog/%E7%A7%91%E5%AD%A6%E7%BB%98%E5%9B%BEmatplotlib/","summary":"Python 科学可视化领域是巨大的，由无数工具组成，从最通用和最广泛使用的工具到更专业和机密的工具。其中一些工具是基于社区的，而另一些则是由公司开发的。有些是专门为 Web 制作的，有些仅适用于桌面，有些处理 3D 和大数据，而有些则针对完美的 2D 渲染。\n在这个图景中，Matplotlib 有着非常特别的地方。\n 它是一个多功能且功能强大的库，可让您设计非常高质量的图形，适用于科学出版。 它还提供了一个简单直观的界面以及一个面向对象的架构，允许您调整图形中的任何内容。 最后，它可以用作常规图形库以设计非科学图形。  本书章节四个部分   第一部分 Matplotlib 库的基本原理。\n这包括回顾构成图形的不同部分、不同的坐标系、可用的比例和投影，我们还将介绍一些与排版和颜色相关的概念。\n  第二部分 图形涉及实践。\n在介绍了一些生成更好图形的简单规则之后，我们将继续解释 Matplotlib 默认值和样式系统，然后再深入研究图形布局组织。然后我们将探索可用的不同类型的情节，看看如何用不同的元素装饰一个人物。\n  第三部分 更高级的概念\n即 3D 图形、优化和动画。第四部分也是最后一部分是展示集合。\n  美图展览       案例代码   import numpy as np import matplotlib.pyplot as plt from matplotlib.textpath import TextPath from matplotlib.patches import PathPatch from matplotlib.collections import PolyCollection from matplotlib.font_manager import FontProperties from matplotlib import font_manager as fm, rcParams import matplotlib.","title":"科学绘图matplotlib"},{"content":"案例文献 胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.\n摘要： 在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现，年报 MD\u0026amp;A 中披露的“短期视域” 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越 易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴**。**\n关键词： 管理者短视 长期投资 文本分析 机器学习\n变量测量论证 语言能够反映人的认知、偏好和个性（Webb et al.，1966），研究者可通过分析实验对象语言中使用的词语类型和词频来捕捉人的特质（Miller and Ross，1975；Pennebaker et al.，2003）。如一个人的语言中越强调“过去”、“ 曾经”等词汇，反映其越关注过去；一个人的语言中越强调“将来”、“ 可能”、“ 要去”等词汇，反映其越关注未来（Pennebaker et al.，2003）。基于此研究范式，本文结合已有的英文“短期视域”词集、MD\u0026amp;A 中文语料特点以及 Word2Vec 机器学习制定出能够反映管理者“短期视域”的中文词集，随后通过词典法构建出管理者的短视主义指标。\nMD\u0026amp;A 是管理者对报告期内企业经营状况的回顾以及对下一年度经营计划以及企业未来发展所面临的机遇、挑战和各种风险的阐述。已有利用 MD\u0026amp;A 等文本刻画管理者特质的研究成果在一定程度上证实了其可靠性（Li，2012；蒋艳辉、冯楚建，2014）。如\n Li（2012）利用美国上市公司 MD\u0026amp;A 文本来刻画管理者的 自我归因偏差。 蒋艳辉和冯楚建（2014）利用 MD\u0026amp;A 中“我们”、“ 我公司”、“ 我们公司”等词语出现的频率刻画管理者的自我指涉度，从而衡量管理层对公司的认知和努力程度。 同时，国外文献表明 CEO 对企业的经营决策起着绝对的主导作用，能够直接影响企业未来的发展方向和命运（Chandler，1962；Finkelstein and Hambrick，1996）。CEO 的特质如自恋程度、学历和任期等都会极大影响公司的信息披露特点（Marquez Illescas et al.，2019；Lewis et al.，2019），因此年报披露的文本信息更多地反映了 CEO 的意思。而在我国，上市公司的董事长更像发达国家的 CEO（姜付秀等，2009；陈传明、孙俊华，2008；李健等，2012）。  因此，我们从 MD\u0026amp;A 中捕获的管理者短视主义特质更多反映的是董事长的短视主义特质，本文的管理者指的是企业的董事长。\n指标构建过程 具体来讲**，管理者短视主义指标**的构建过程如下。\n 借鉴 Brochet 等（2015）的英文“短期视域”词集与 Li（2010）构建文本指标的思路，我们阅读了 500 份 MD\u0026amp;A 语料以获取中文文本信息的特点，制定出中文 MD\u0026amp;A 中有关“短期视域”的种子词集，包括直接和间接 两大类。直接短期视域大类包括：“ 天内”、“ 数月”、“ 年内”、“ 尽快”、“ 立刻”、“ 马上”；间接****短期视域大类包括：“ 契机”、“ 之际”、 “压力”、“ 考验”。 针对同一概念或者事物，表达者往往使用多个语义相似的词汇进行描述，因此需要对种子词集进行相似词扩充。本文采用 Word2Vec 中的 CBOW 模型（Continuous Bag-of-words Model）对中文年度财务报告语料进行训练。 我们通过邀请 3 名业界和学术界专家以及对比 MD\u0026amp;A 文本样例对指标词集进行核验，最终确定词集包含 43 个“短期视域”词汇（词集和语句示例详见《管理世界》网络发行版附录 2）。随后，本文基于词典法计算 “短期视域”词汇总词频占 MD\u0026amp;A 总词频的比例，乘以 100 后得到管理者短视主义指标。该指标值越大，表明管理者越短视。  技术分析  纯技术讨论，非论文内容\n 这篇管理世界的论文，主要难点有两个：\n  如何构建 短视主义词典(集) ？\n   根据对研究和数据的了解，人工摘选一些 短视主义词典(集)种子词；人工，不需要python编程 使用Word2Vec技术扩充 短视主义词典(集)；需要python编程    如何使用 短视主义词典(集) 计算 短视主义指标？\n   需要使用Python编程语言，根据 词典法 实现短视主义指标的计算。    python学习与实现 难点主要可在掌握 视频专栏课| Python网络爬虫与文本分析 后，结合以下两个技能点实现\n 扩充词集可以用到之前分享的wordexpansion库 https://github.com/DataPlusCommunity/wordexpansion 计算短视主义指标，即词典法可以用到cnsenti库 https://github.com/DataPlusCommunity/cnsenti  ","permalink":"/blog/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%E5%9C%A82021%E7%AE%A1%E7%90%86%E4%B8%96%E7%95%8C%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/","summary":"案例文献 胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.\n摘要： 在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现，年报 MD\u0026amp;A 中披露的“短期视域” 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越 易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴**。**\n关键词： 管理者短视 长期投资 文本分析 机器学习\n变量测量论证 语言能够反映人的认知、偏好和个性（Webb et al.，1966），研究者可通过分析实验对象语言中使用的词语类型和词频来捕捉人的特质（Miller and Ross，1975；Pennebaker et al.，2003）。如一个人的语言中越强调“过去”、“ 曾经”等词汇，反映其越关注过去；一个人的语言中越强调“将来”、“ 可能”、“ 要去”等词汇，反映其越关注未来（Pennebaker et al.，2003）。基于此研究范式，本文结合已有的英文“短期视域”词集、MD\u0026amp;A 中文语料特点以及 Word2Vec 机器学习制定出能够反映管理者“短期视域”的中文词集，随后通过词典法构建出管理者的短视主义指标。\nMD\u0026amp;A 是管理者对报告期内企业经营状况的回顾以及对下一年度经营计划以及企业未来发展所面临的机遇、挑战和各种风险的阐述。已有利用 MD\u0026amp;A 等文本刻画管理者特质的研究成果在一定程度上证实了其可靠性（Li，2012；蒋艳辉、冯楚建，2014）。如\n Li（2012）利用美国上市公司 MD\u0026amp;A 文本来刻画管理者的 自我归因偏差。 蒋艳辉和冯楚建（2014）利用 MD\u0026amp;A 中“我们”、“ 我公司”、“ 我们公司”等词语出现的频率刻画管理者的自我指涉度，从而衡量管理层对公司的认知和努力程度。 同时，国外文献表明 CEO 对企业的经营决策起着绝对的主导作用，能够直接影响企业未来的发展方向和命运（Chandler，1962；Finkelstein and Hambrick，1996）。CEO 的特质如自恋程度、学历和任期等都会极大影响公司的信息披露特点（Marquez Illescas et al.，2019；Lewis et al.，2019），因此年报披露的文本信息更多地反映了 CEO 的意思。而在我国，上市公司的董事长更像发达国家的 CEO（姜付秀等，2009；陈传明、孙俊华，2008；李健等，2012）。  因此，我们从 MD\u0026amp;A 中捕获的管理者短视主义特质更多反映的是董事长的短视主义特质，本文的管理者指的是企业的董事长。\n指标构建过程 具体来讲**，管理者短视主义指标**的构建过程如下。\n 借鉴 Brochet 等（2015）的英文“短期视域”词集与 Li（2010）构建文本指标的思路，我们阅读了 500 份 MD\u0026amp;A 语料以获取中文文本信息的特点，制定出中文 MD\u0026amp;A 中有关“短期视域”的种子词集，包括直接和间接 两大类。直接短期视域大类包括：“ 天内”、“ 数月”、“ 年内”、“ 尽快”、“ 立刻”、“ 马上”；间接****短期视域大类包括：“ 契机”、“ 之际”、 “压力”、“ 考验”。 针对同一概念或者事物，表达者往往使用多个语义相似的词汇进行描述，因此需要对种子词集进行相似词扩充。本文采用 Word2Vec 中的 CBOW 模型（Continuous Bag-of-words Model）对中文年度财务报告语料进行训练。 我们通过邀请 3 名业界和学术界专家以及对比 MD\u0026amp;A 文本样例对指标词集进行核验，最终确定词集包含 43 个“短期视域”词汇（词集和语句示例详见《管理世界》网络发行版附录 2）。随后，本文基于词典法计算 “短期视域”词汇总词频占 MD\u0026amp;A 总词频的比例，乘以 100 后得到管理者短视主义指标。该指标值越大，表明管理者越短视。  技术分析  纯技术讨论，非论文内容","title":"文本分析方法在2021管理世界中的应用"},{"content":" 翻译自\nBerger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.\u0026rdquo; Journal of Marketing (2019): 0022242919873106.\n 论文作者们的报告视频已上传到B站(下图)，感兴趣的童鞋可以先收藏再收看\nJourmal of Marketing Webinar｜2019市场营销\n摘要 语言文字是营销场景中最常用的交互方式，比如在线评论、消费者服务热线、新闻发布、营销传播等活动都创造了有价值的文本数据。但营销研究者如何用好这些数据？本文回顾了文本分析相关研究，并详细介绍了如何用文本数据做市场研究。作者讨论了文本如何反映文本生产者， 文本信息如何影响信息接受者。\n接下来，本文讨论了文本如何预测并理解文本背后的信息，回顾了文本分析的方法和测量指标(metrics),提供了一整套的文本分析操作流程。最后，作者提到文本分析内部信度和外部效度问题，研究者如何解决。本文讨论营销各个领域可能存在的研究机会，虽然目前市场营销的研究问题大都是跨学科的，但是营销的各个子领域经常都是孤立，借助文本分析可能架构起连接营销各个子领域的桥梁。\n关键词  计算语义学coputational linguistics 机器学习machine learning 市场洞察marketing insight 跨学科interdisciplinary 自然语言处理natural language processing 文本分析text analysis 文本挖掘 text mining  无所不在文本 之前的研究认为，尽管投资者一次对包含重大变化的财务报\n交流沟通是营销的重要组成部分，消费者、企业、消费者投资者、社会，不同水平或者统一水平都有信息交流与沟通。而信息交流的过程中往往会产生或者转化为文本数据。\n最简单的的文本数据世界模型是生产者与消费者。模型内生产者和接受者都可能是消费者、企业、投资者和社会。消费者书写在线评论，公司制作会计年报，文化生产者代表社会意义制作出书籍、影片和艺术品（Table 1）\n在此情形下，研究者可能选择文本如何反映或如何影响？\n How text reflects its producer？ How text impacts its receiver？  尤其是文本可以反映一定的信息，这些信息是可以帮助营销人员洞察市场规律，进而利用规律影响文本信息的接受者。\n  文本反映生产者 首先，文本可以反映了个人的一些信息。例如“在社交媒体某推特上写着某人谈论着上周他们做了什么。”这句话有很多待挖掘的信息，比如他们这些人什么情况，是内向还是外向、神经质还是严肃认真、他们感觉如何、某时刻他们想了什么(Moon and Kamakura 2017)。总之，文本可以看作指纹或签名(Pennebaker 2011)。\n通过文本也可以用于理解领导人、机构或者文化精英。例如领导人用词表达会反映出其领导风格，对利益相关方的态度。透过广告、网站或者消费者服务商(consumer service agent)的言语，人们会了解公司的品牌个性(Opoku, Abratt, and Pitt 2006)，公司是如何看待消费者(Packard and Berger 2019a)，管理层对终端用户的定位(Molner, Prabhu, and Yadav 2019)。年报也会有未来业绩表现的有价值线索(Loughran and McDonald 2016)。\n除了单独分析个人或组织的言语，也可以对多个内容生产者合并起来进行更大层面的研究。透过人群或组织产生的文本，我们可以更好理解他们的本质。例如，分析微博，可以得出老年人和年轻人之间如何看待幸福(as excitement vs. peacefulness; Mogilner, Kamvar, and Aaker 2011)。消费者们在品牌社区的言语能更深的投射出消费者对品牌的态度(Homburg, Ehm, and Artz 2015)。\n而更宏大的层面，文本也能反映出文化差异。如美国人的表达相比东亚人具有更高的唤醒水平(Tsai 2007)，更喜欢用“我”而不是“我们”，也透露着崇尚个人主义，而不是集体主义。\n透过时间，研究者也可以监测美国国民情绪是否在911恐怖袭击前后发生变化(Cohn, Mehl, and Pennebaker 2004)。透过新闻报告、歌词等内容也可以帮助研究者了解社会态度和社会规范，分析有关对女性、少数族裔(Boghrati and Berger 2019; Garg et al. 2018)和特定产业态度的时代变迁(Humphreys 2010)。\n虽然文本分析并不容易，但企业和组织可以使用社交网络倾听民声。了解消费者是否喜欢新产品，消费者如何看待品牌，消费者最看重什么(Lee and Bradlow 2011; Netzer et al. 2012)。监管机构可以确定什么药物有不良部反映(Feldman et al. 2015; Netzer et al. 2012),公共卫生部门可以提前了解流感今年爆发最严重的地区(Alessa and Faezipour 2018),投资者可以预测股价涨跌 (Bollen, Mao, and Zeng 2011; Tirunillai and Tellis 2012).\n文本作用于消费者 文本不止可以反映生产者的信息，也可以知道文本如何影响消费者，消费者会有什么样的行为和选择。广告会塑造消费者的消费行为(Stewart and Furse 1986),报纸用语会改变消费者的态度(Humphreys and LaTour 2013), 消费者杂志会扭曲消费者产品分类感知(e.g., Rosa et al. 1999),电影剧本会影响观众的反应(Berger, Kim, and Meyer 2019; Eliashberg, Hui, and Zhang 2014; Reagan et al. 2016),等等。\n需要注意的是文本的反映reflects和影响impacts并不是非此即彼，往往会同时起作用，尽管如此，研究人员倾向于使用文本差异来研究它俩。\n当研究文本的reflects时，倾向于将reflects当作因变量，试图挖掘文本生产者的个性personality或属于什么社会团体。\n当研究文本的impacts时，倾向于将impacts看作自变量，检验文本是否以及如何导致消费者诸如购买、分享和卷入行为。在本框架中，文本信息潜藏着某些潜在的影响力，是被当作诱因，对后续或者其他主体有作用力的。\n文本内容也会被客观条件影响 文本内容还可以被客观条件所塑造，如\n 技术限制和社会文化基因(社会规范) 文本信息生产者与消费者之间的领域知识 先前客观历史  首先，不同题材因社会规范，表达内容和方式有所不同。例如观点陈述时，新闻不如报告来的客观(Ljung 2000).酒店评论卡和其他反馈主要被极端观点占据。在Snapchat和其他SNS平台的推文达多较短，且昙花一现；而自在线评论经常较长且可以回溯到多年以前。\n技术和物理也会改变文本表达。推特只能发少于280字符的推文。移动电话在键入方面受到限制，并且可能会影响人们在其上产生的文本（Melumad，Inman和Pham 2019; Ransbotham，Lurie和Liu 2019）。\n其次，信息生产者和消费者之间的关系会影响说什么，怎么说。当生产者和消费者彼此很熟悉，文本表达会更非正式(Goffman 1959)，导致第三方很难通过直接明确的信息了解生产者与消费者之间的对话的态度。\n这些因素对于解读文本信息至关重要，消费者给好朋友分享什么往往跟其他不同。企业可能会因为特定的冬季，其年报中可能会含有利好市场的信息。\n最后，历史可能也会影响文本的内容。在留言板上，以前的帖子可能会影响以后的帖子；如果有人在先前的帖子中提出了要点，则被访者很可能会在以后的帖子中提及该要点。如果转发的帖子含有自己的分析，其内容会偏离大多数的帖子。更广泛地说，＃metoo或#blacklivesmatter之类的媒体框架可能使某些概念或事实更容易被演讲者使用，因此即使看起来似乎无关，它们也更可能出现在文本中（McCombs\u0026amp;Shaw 1972; Xiong，Cho\u0026amp;Boatwright 2019）。\n使用文本预测与理解 文本除了reflects 和 impacts之外，还有predict和understanding。\n## 预测 某些文本研究出发点是做预测\n 什么消费者最喜欢贷款(Netzer, Lemaire, and Herzenstein 2019)? 什么电影会大火(Eliashberg et al. 2014)? 未来股市走向(Bollen, Mao, and Zeng 2011; Tirunillai and Tellis 2012)?  类似上面的研究，会使用很多文本特征来做机器学习和预测，研究人员不怎么关系任意的文本特质，他们更关心预测的表现。\n用文本做预测的主要难点是，文本数据可以生成成千上万的特征(相当于变量x1，x2\u0026hellip;xn)，而文本数据记录数甚至可能少于特征数。为了解决这个为题，使用新的特征分类方法，减少特征数量，又有可能存在拟合问题。\n## 理解 预测之外的研究主要是理解文本\n 消费者怎样表达会如何影响口碑(Packard and Berger 2017)? 为何某些推文会被挑中分享？ 歌曲为何变火？ 品牌如何让消费者忠诚？  理解的目标是理解为什么事情发生以及如何发生的。这类研究往往会用到心理学、社会学的方法，旨在理解文本的什么特征会导致什么后续结果，以及为什么产生这样的后果。\n用文本做理解的难点是找出观测数据背后的因果关系。相应的，该领域的工作可能会强调实验数据，以允许对关键的独立变量进行操作。另一个挑战是解释文本特征之间的关系。使用第二人称的歌曲往往较火(Packard and Berger 2019b),但是为什么使用第二人称会火，单纯的文本数据很难挖掘出来作用机制。\n在prediction领域，研究人员利用 文本的reflects方面 来预测 生产者的状态、特性、满意度、性格等。研究人员利用 *文本impacts方面 * 来预测 消费者的阅读、分享和购买行为。\n在understand领域，研究人员利用 文本的reflects方面 来理解为什么当人们压抑的时候会使用特殊人称。利用 *文本impacts方面 * 来理解为何带有情绪的文本会更容易被阅读和分享。\n粘合营销各领域 尽管有reflects vs impacts， prediction vs understanding之分，做文本分析需要整合多种技能·技术和不同营销领域的相关知识。\n就拿消费者行为学来说，在行为经济学大放异彩之前，假设情景操纵是存在争议的。实验可重复性问题，研究者开始寻找试图增强信度、效度的新工具。使用二手数据经常受限于只能做“是什么”的研究，不能做“为什么”的研究。但文本数据提供了做为什么的可能。例如在线评论可以用来理解为何某人购买了此商品的决策，尽管人们可能并不总是知道为什么要做某事，但他们的语言常常提供解释的痕迹（Pennebaker 2011），甚至超出了他们有意识地表达的范围。\n定量建模人员一直在寻找新的数据源和工具来解释和预测行为。非结构化数据提供了一组丰富的预测变量，这些预测变量通常可以随时大规模获得，并且可以与结构化度量一起作为因变量或自变量组合。通过产品评论，用户驱动的社交媒体活动以及公司驱动的营销活动，文本可以实时提供可以阐明消费者需求/偏好的数据。这提供了对传统营销研究工具的替代或补充。在许多情况下，文本可以追溯到个人，从而可以区分个人差异和动态。\n营销策略研究人员希望企业能实现其营销目标，并更好地理解影响组织成功的因素。文本分析提供了一种客观而系统的解决方案，以评估可能更有效的自然数据（例如，致股东的信，新闻稿，专利文本，营销信息，与分析师的电话会议）中可能的因素，如了解客户、合作伙伴和员工关系性质以及品牌情感强度(Kubbler，Colicev和Pauwels2017）使用词典和支持向量机方法来提取情绪并将其与消费者心态指标相关联。\n也有学者借鉴人口和社会学领域，使用定性和内容分析研究文本数据。消费者文化领域，研究者对字里行间的意义、规范和价值观更感兴趣。文本分析提供了事物变化或比较不同事物的量化指标。文本分析为营销学者解锁了非结构化数据的开锁姿势，提供了文本的定性与定量研究的新疆界。\n文本分析工具、方法和指标 给予前任做的文本数据驱动的洞察，有学者可能好奇如何开启文本研究之路。在本节会评述文本分析相关研究，包括\n 构念如何用文本数据构建 将提取的文本信息整合到后续建模和分析中所需的过程  本节目的是提供综合的入门指导，而是把可用的技术路线留给各位\n 讨论各种方法如何恰当的使用 各种方法在使用时应该注意什么  文本处理分析包括的步骤有\n 数据预处理 文本信息提取 常用的文本分析指标  数据预处理 文本数据是非结构化的脏数据。在任何常规数据分析之前，都要先将文本数据预先清洗处理，进而产生出类似excel表的干净的数据。常用的工具有R语言和Python语言，两种编程语言都有一套易用的数据预处理包。使用某些软件，如Linguistic Inquiry and Word Count (LIWC; Tausczik and Pennebaker 2010) 和WordStat (Peladeau 2016)之前，文本数据需要做少量的预处理。预处理可见Table 2和 Table 3 。\n    1. 数据获取 巧妇难为无米之炊，做文本研究的第一步就是采集数据，文本存在于邮件、公司年报、在线评论之中，无所不在，浩瀚无比。可以用人工手动复制粘贴到excel之中，但是效率太低，我们可以使用python设计网络爬虫采集数据。常见访问库requests、数据解析库pyquery和BeautifulSoup、数据存储库csv。\n2. 分词 将文本分词(切词)，数据尺度从章节段落拆解成颗粒度更小的词语层面，方便进行分析。但是要注意，英文是用空格间隔词语，而中文没有空格，还要注意粒度分的不能太细，如“the U. S.”按照空格分词会分出“the”、“U.” 和“S.”，导致美国这个实体被切分消失。\n3. 清洗 网络爬虫在采集数据阶段，采集的并不是干净的文本数据，还有一些像HTML标签、图片、链接等字符，需要采集时清除掉。\n4. 剔除停止词 文本中有很多经常出现的无意义或者意义微乎其微的词，如\u0026quot;a\u0026quot;、the\u0026quot;、\u0026ldquo;is\u0026rdquo; 等。一般情况下，这些词是需要剔除的。但是当研究的是书写者的语言风格，这些无意义词语往往含有千丝万缕的写作习惯信息，所以此时不能剔除。(e.g., Packard, Moore, and McFerran 2018；Pennebaker 2011).\n5. 拼写 一般情况下，还需要将错误书写的词正确修改过来。但是当研究者对错误率感兴趣的时候，这时候就不要更正拼写问题。(e.g., Netzer, Lemaire, and Herzenstein 2019).\n6. 词干化 词干化是为了将相同或者相近意思的词合并为一个词，如“car” ` “cars” 统一识别为 “car,”\n文本信息提取   1. 命名实体抽取 这是文本分析最基础、最简单、最常用的部分。例如姓名、地址、品牌、产品属性、情绪、词性等等都可以看作一种实体信息。实体抽取可以用来\n 监测啥叫媒体讨论，商业竞争情报 也可用作机器学习中的特征（预测指标），预测是否是欺诈信息 构建更复杂的文本表达方式的度量指标，如情感、情绪、写作风格  这部分一般需要强大的编程语言，如Python和R；当然有些情况下不用编程，使用WordStat也能做实体抽取。大多数情况下实体抽取经常伴随着专业词典或词表的使用，如(概念、品牌、分类、地址等)。通用的词典包括LIWC(Pennebaker et al. 2015)， EL 2.0 (Rocklage, Rucker, and Nordgren 2018), Diction 5.0 或General Inquirer for psychological states and traits (Berger and Milkman [2012]; Ludwig et al. [2013]; Netzer, Lemaire, and Herzenstein [2019]).\n情感词典，如Hedonometer (Dodds et al. 2011), VADER (Hutto and Gilbert 2014), 和LIWC能计算出文本中含有的情感信息。情感分析经常使用词袋法（Bag of Words）计算文本中的情感。但是该方法不考虑词语在文本中的顺序，而顺序是能影响情感信息的。尽管词典法对构建构念和比较构念比较简单，但基于人工编码的机器学习方法(e.g.,Borah and Tellis 2016; Hartmann et al. 2018; Hennig-Thurau, Wiertz, and Feldhaus 2015)更适合做精准概念的度量(Hartmannetal.2019)，尤其是这个领域是不常见或者比较复杂。\n如果研究者想挖掘出实体之间的关系就用到word2vec或者词嵌入word embedding (Mikolov et al. 2013)，这两种方法都把每一个词分配一个长度固定的向量，我们知道向量可以在空间中比较，如cos余弦计算词语之间的相似度。\n2. 话题模型 实体抽取有两个大问题:\n 维度太高，经常能从文本数据中抽取出数千个实体 实体的解读与解释  话题模型更多的是对文本的解释，而非预测(e.g., Berger and Packard 2018; Tirunillai and Tellis 2014)。话题模型最常见的是LDA，某个词以一定的概率属于话题，文本以多种话题按照一定的概率分布。\nLDA是无监督学习，需要事先指定话题数，输出的结果是不同的类分布，需要研究者解读每一个话题到底是什么题材内容。话题区间范围一般建议结合统计分布和研究者经验确定话题数目。\n3. 关系抽取 关系抽取可以用实体共现性来捕捉(e.g., Boghrati and Berger 2019; Netzer et al. 2012; Toubia and Netzer 2017).但营销学者对诸如产品、属性和情感之间的关系感兴趣。例如，研究者对评论中是否提及某个产品属性的问题。Feldman et al. (2015) and Netzer et al. (2012) 提供了药物与不良反应之间的关系来识别问题药物。\n关系抽取用的实现大多思路不难，多是一些人工规则的设计，如产品“Ford”、属性“oid consumption”和问题“excessive”共现性来捕捉福特车耗油。然而这样的方法需要手写复杂的规则，现在变得慢慢不流行。\n更通用的方法是机器学习法，人工标注相关的数据，训练机器学习模型。这类实现方法需要大量的人工标注，一种可用的工具是Stanford Sentence and Grammatical Dependency Parser (http://nlp.stanford.edu:8080/parser/) 。该工具可以识别词语依存关系，如“the hotel was very nice,” ，“nice” 与 “hotel”相关联，说明这个hotel挺nice的。\n当然，也可以扩文本之间做比较，这里不过多赘述。\n文本分析指标 早起市场营销，如在线评论领域的文本分析指标多为\n 数量(e.g., Godes and Mayzlin 2004; Moe and Trusov2011) 效价，评论评分t (e.g., Godes and Silva 2012; Moe and Schweidel 2012; Ying, Feinberg and Wedel 2006)· 方差，如信息墒(e.g., Godes and Mayzlin 2004).  然而如今这些指标经常忽略了文本的丰富度。以下几种是更好用的指标\n1. count measure 使用相应的词典，统计实体出现次数，这样可以对不同实体进行比较(Berger and Milkman 2012; Borah and Tellis 2016; Pennebaker et al. 2015; Schweidel and Moe 2014; Tirunillai and Tellis 2014)。缺点是更长的文本通常含有更多的实体(的数量)，还有一个局限就是某些实体会比其他实体更多的出现，如“电脑”商品的在线评论中“电脑”出现次数会远多于其他词。\n2. 相似度 在某些情况下，研究者更对文档之间的相似度感兴趣(e.g., Ludwig et al. 2013).。两个广告之间的相似程度如何？两首歌的歌词相似程度多少？相似度的计算方法有cos余弦相似、jaccard相似 (e.g., Toubia and Netzer 2017)\n3. 可读性 同样的意思可以用不同的难度的词汇去表达，造成阅读的难易程度。可读性反映了作者的内容复杂度和读者的阅读难度。(e.g., Ghose and Ipeirotis 2011)。\n常见的可读性算法有Flesch–Kincaid和the simple measure of gobbledygook (SMOG)。可阅读性经常将得分设置到1-12分之间，在美国学校里阅读理解成绩水平得分就是1-12分。\n未来营销研究新机会 1. 借鉴融合 文本分析在营销界中可以起到促进各个子领域交叉授粉，避免同质化学术繁殖。品牌社群是最早被来自社会学背景的研究者发现和研究的(Mun˜iz and O’Guinn 2001)。随后，定性和定量范式研究者逐渐界定了概念、识别了社群中的地位和作用(e.g., Mathwick, Wiertz, and De Ruyter 2007)。文本分析可以让学者研究如何在更大尺度层面去量化社群中的消费者沟通行为。例如，社群中不同权利地位的人使用的语言是否存在差异，使用不同动态指标预测社群产出情况(e.g., Manchanda, Packard, and Pattabhitamaiah 2015)。研究人员也可以追踪到底哪类用户发明新用语，又是哪些人跟随使用这些新用语。研究可以检查人们是否随着时间的开始使用社群语言，并根据他们对群体语言的适应程度来预测哪些人可能会留下或离开(Danescu-Niculescu-Mizil et al. 2013; Srivastava and Goldberg 2017)。定量或机器学习的研究人员可能会发现社群中最常讨论的主题，以及这些主题如何随着社群的发展而动态变化。阐述性范式的研究人员可能会研究这些话语在概念上如何关联，以找到是哪些潜在社区准则促成成员留下。然后，营销战略领域的研究人员可能会使用或开发词典来将这些社区与公司绩效联系起来，并为公司提供有关如何保持不同品牌社区（或环境）成员参与度的指导。\n不同子领域的营销学者会使用不同的技能集，研究不同的文本传播类型。消费者与消费者(consumer-to-consumer)之间的沟通主要研究的是两者间的行为，而营销战略学者倾向于研究企业与消费者、企业与企业之间的沟通。不同营销子领域的学者间的合作，能帮助他们结合不同的文本数据源。\n它山之石可以攻玉，例如营销战略学者借鉴经济学领域的交易理论(代理理论)来研究企业间的关系，但现在营销战略相关发现可以用于研究消费者之间的沟通行为。\n2. 扩展文本领域研究 我们希望看到更多的消费者-企业间的沟通的研究(e.g., Packard and Berger 2019a; Packard, Moore, and McFerran 2018)，这些沟通经常都是非约束非的，这其中蕴涵着有价值的关系数据，可以有很多应用价值。\n而在企业间沟通方面，大多数侧重于沟通(Communication)的角色(e.g., Palmatier, Dant, and Grewal 2007)。然而在文本数据上，在词语层面上，有相关研究很少。例如很少有研究销售人员与消费者之间的信息交换类型。\n类似的，在会计金融领域有很多人采用年报作为数据源(for a review, see Loughran and McDonald [2016])，但营销学者很少注意到公司与投资者之间的存在的研究机会。大多数学者只是用来研究如何预测公司股价或者开发新的公司市值估值模型。鉴于最近有兴趣将营销相关活动与公司估值联系起来（例如McCarthy和Fader 2018），这可能是一个需要进一步追求的领域。公司的所有沟通，包括年度报告等必需的文件，或广告和销售互动等任意形式的沟通，都可以用做观测变量，例如市场定位，营销能力，营销领导风格，甚至公司的品牌个性。\n在消费者、企业、社会之间也存在着大量的研究机会。有关企业文化(规范)的数据，例如新闻媒体和政府报告，可能有助于阐明影响市场的力量。例如，要了解Uber这样的公司如何抵抗市场变化，可以研究市政厅会议的笔录和其他听取并回答市民意见的政府文件。诸如#metoo和#blacklivesmatter之类的社会运动形式的外来冲击影响了营销传播和品牌形象。未来研究的一种潜在途径是采用文化品牌化方法（Holt，2016年），研究不同公众如何定义，塑造和倡导市场中的特定含义。公司及其品牌并不是凭空存在的，它们独立于其经营所在的社会。但是，在市场营销方面的有限研究已经考虑了如何使用文本在社会层面上得出公司的意图和行为。例如，学者们展示了诸如locavores（这类人只食用当地产的食品；Thompson和Coskuner-Balli，2007年），时尚达人（Scaraboto和Fischer，2012年）以及博主（McQuarrie，Miller和Phillips，2012年），这几类人群塑造了市场。通过文本分析，可以衡量和更好地理解这些社会群体的意图对市场的影响。\n未来研究的另一个机会是使用文本数据来研究文化和文化成功。跨学科研究了文化传播，艺术变革和创新传播等主题，目的是理解某些产品为何成功而其他产品却失败的原因(Bass 1969; Boyd and Richerson 1986; Cavalli-Sforza and Feldman 1981; Rogers 1995; Salganik, Dodds, and Watts 2006; Simonton 1980). While success may be random (Bielby and Bielby 1994; Hirsch 1972),可能的原因是没把握好消费者的口味偏好 (Berger and Heath 2005)。\n通过在大范围更快速度地量化书籍、电影或其他文化物品，研究人员可以测量具体的叙事是否更具吸引力，更具情感波动性的电影是否更成功，使用某些语言特征的歌曲是否更有可能登上广告牌榜首 ，以及唤起特定情感的书籍是否售出更多。尽管没有像社交媒体数据那样广泛可用，但最近越来越多的文化项目数据可用。诸如Google Books语料库（Akpinar\u0026amp;Berger 2015），歌曲歌词网站或电影脚本数据库等数据集可提供大量信息。此类数据可以使叙事结构分析，以识别\u0026quot;基本情节\u0026quot;'(Reagan et al 2016; Van Laer et al2019）。\n3. 用文本测量关键构念 在个体层面上，情感和满意度可能是最常用的测量变量(e.g., Bu¨schken and Allenby, 2016; Homburg, Ehm, and Artz 2015; Herhausen et al. 2019; Ma, Baohung, and Kekre 2015; Schweidel and Moe 2014)其他从文本数据中提取的测量变量包括语言的真实性authenticity和情绪性emotion(e.g., Mogilner, Kamvar, and Aaker 2011; Van Laer et al. 2019)。也有心理学测量变量，如性格类型presonality type和建构水平construal level(Kern et al. 2016; Snefjella and Kuperman 2015),这都是潜在的可以借鉴应用到消费者话语研究的。\n未来个体层面的研究会考虑社会认同和社会参与度， 研究人员目前对消费者已经可以测量情绪的积极或消极，但他们才刚刚开始探索重点（Rocklage\u0026amp;Fazio 2015），信任，承诺和其他模式属性。为此，利用语用学的语言理论并研究语义学上的阶段性可能是有用的（Villarroel et al2017）。一旦开展了此类工作，我们建议研究人员仔细验证建议的方法，以按照上述方法测量此类构念。\n在公司层面，已在公司生产的文本（例如年度报告和新闻稿）中确定了一些构念。诸如市场定位、广告目标、未来定位、欺骗意图、公司重点和创新定位均已使用此材料进行了测量和验证（详见Web Appendix Table 1)。未来企业层面的营销研究需要重新界定和丰富战略定位的测量(创新定位、市场驱动vs市场驱动定位)。组织文化、结构和能力由于难于测量，可以从企业、雇员和外部利益相关者的文本数据来测量(see Molner, Prabhu, and Yadav [2019])。类似的，企业领导层的思维和管理风格可以从他们怎么说来侦测(see Yadav, Prabhu, and Chandy [2007])。公司的绩效指标可以通过之前的公司相关文本数据进行预测(e.g., Herhausen et al. 2019)。从这个角度看，我们有很多使用数据的新机会。例如，从企业内部员工的相关信息(LinkedIn 和 Glassdoor)可以测量基于员工的品牌价值。最后，企业语言的更多微妙属性，如冲突、歧义、开放性都可以为管理学增加新发现。再比如，使用一些非正式文本数据，如员工邮件记录、销售通话记录或消费者服务中心通话记录。\n营销工作较少在社会或文化层面上衡量结构，但这种工作趋向于集中于公司如何适应现有意义和规范的文化结构。例如，制度逻辑和合法性是通过分析媒体文本来衡量的，Berger等人的品牌公众崛起也增加了文化中对品牌的讨论（Arvidsson and Caliandro 2016）。在文化层面，营销研究可能会继续关注企业如何适应文化环境，但也可能会关注文化环境如何影响消费者。例如，对文化不确定性，风险，敌意和变化的测量可以理解文化对消费者和企业影响。通过文本衡量开放性和多样性也是适时探索的主题，并且可能会促进测量方面的创新，例如侧重于语言多样性。通过文本分析，也可以更好地理解重要的文化论述，例如围绕债务和信用的语言。与性别和种族有关的语言的测量可能有助于探索多样性和包容性，从而使公司和消费者对来自不同作家的文本做出反应。\n机遇与挑战 本节是从技术角度出发探讨文本分析方法的新机遇与挑战。\n1. 机遇 虽然我们的讨论集中于文本内容，但文本只是非结构化数据的一个示例，而音频，视频和图像则是其他示例。社交媒体帖子通常将文字与图片或视频结合在一起。平面广告通常会在精心构造的视觉效果上覆盖文字。尽管电视广告可能不会在屏幕上包含文本，但它可能具有音频轨道，其中包含与视频同步进行的文本。\n直到最近，文本数据一直受到最多关注，这主要是由于存在提取有意义特征的工具。也就是说，诸如Praat（Boersma 2001）之类的工具允许研究人员从音频中提取信息（Van Zant和Berger 2019）。音频数据相对于文本数据的优势之一是，它以音调和语音标记的形式提供了丰富的内容，可以添加到所表达的实际单词中（Xiao，Kim和Ding 2013）。这使研究人员不仅可以研究说的内容，还可以研究说的方式，检查音调，语气以及其他声音或副语言特征如何塑造行为。\n同样，最近的研究开发了分析图像的方法（Liu，Xuan等人2018），既可以表征图像的内容，也可以识别图像中的特征。文本和图像组合的影响的研究很少（例如Hartmann等人2019）。例如，可以根据图像的颜色来描述图像。在印刷广告的上下文中，当与特定调色板的图像结合使用时，文本内容的说服力可能会降低，而其他调色板可能会增强文本的说服力。与简单的图像结合使用，文本的重要性可能会非常明显。但是，当文本与复杂的图像配对时，观看者可能会主要关注图像，从而减少了文本的影响。在这种情况下，作为广告精美图片一部分的法律披露可能不会引起受众的注意。\n当文本加到视频中时，其扮演的角色也引发了类似的问题。研究已经提出了表征视频内容的方法（例如Liu等人2018）。除了包含视频脚本之外，文本还可能在视觉上出现。除了在其中显示文本的音频上下文之外，其影响可能还取决于同时显示的视觉效果。也可能是其在视频中相对于视频开头的位置可能会降低其效果。例如，由于多种原因，在视频中稍后说出的情感性文字内容可能缺乏说服力（例如，观众在讲出文字时可能已经不再注意了）。或者，与音频配对的视觉效果可能对观众更具吸引力，或者视频的先前内容可能耗尽了观众的注意力资源。正如我们对图像和视频的讨论所暗示的那样，文本只是营销传播的一个组成部分。未来的研究必须调查其与其他特征的相互作用，不仅包括其出现的内容，还包括其出现的时间（Kanuri，Chen和Sridhar 2018），以及在哪种媒体上。\n2. 挑战 尽管机会众多，但文本数据也带来了各种挑战。首先是面临可解释性的挑战。在某些方面，文本分析似乎提供了衡量行为过程的更客观的方法。例如，一个人可以计算第一人称“ I”和第二人称“ you”。第一人称在文本中越多，说明这个人更关心自己 （Berger 2014），这种量化词语数量的方法提供看起来更像很客观像真理的东西。但是，尽管该过程的一部分肯定是更客观的（例如，不同类型的代词的数量），但此类度量与基础过程（即，关于口碑传播者的说法）之间的联系仍然需要一定程度的解释。其他潜在的行为方式甚至更难以计数。例如，虽然某些词（例如“love”）通常是积极的，但它们的积极性可能在很大程度上取决于特质个体差异和上下文。\n更普遍地，在理解文本信息出现的上下文中存在挑战和机遇。例如，餐厅评论可能包含很多否定词，但这是否意味着该人更讨厌食物，服务或餐厅？包含更多第二人称代词（“ you”）的歌曲可能会更成功（Packard and Berger 2019b），但要了解原因，了解歌词是否使用“ you”作为句子的主语或宾语是有帮助的。上下文提供了含义，而且越多的人不仅了解正在使用的单词，而且还了解如何使用它们，则越容易获得新知识新洞察。基于词典工具特别容易对使用场景变化特别敏感，建议尽可能使用针对特定研究环境创建的词典（例如，Loughran和McDonald [2016]开发的财务情感工具）。\n数据隐私挑战是一个重大问题。研究通常使用从网站上抓取的在线产品评论和销售排名数据（Wang，Mai和Chiang 2013）或从社交媒体平台上抓取的消费者的活动数据（Godes和Mayzlin 2004；Tirunillai和Tellis 2012）。尽管这种方法很普遍，但是法律问题已经开始出现。LinkedIn未能成功阻止一家初创公司抓取用户公共资料中发布的数据（Rodriguez，2017）。虽然根据法律可能允许收集公共数据，但它可能与那些拥有研究人员感兴趣的数据的平台的服务条款相冲突。\n随着从数字化文本和其他形式的数字化内容（例如图像，视频）中提取见解的兴趣日益浓厚，研究人员应确保他们已获得进行工作的适当权限。不这样做可能导致开展此类项目变得更加困难。一种潜在的解决方案是创建一个学术数据集，例如Yelp提供的数据集（https://www.yelp.com/dataset），该数据集可能包含过时或经过清理的数据，以确保不会产生 公司的运营或用户隐私风险。\n对数字化文本以及其他用户创建的内容的收集和分析，也引发了有关用户对隐私的期望的问题。随着欧盟《通用数据保护条例》的发布以及有关Cambridge Analytica从Facebook收集用户数据的能力的启示，研究人员必须注意其工作的潜在滥用。我们还应考虑超出用户生成内容的预期用途的程度。例如，尽管用户可能会理解，Facebook采取的行动可能会导致他们针对与其互动的品牌进行专门的广告宣传，但他们可能无法预期其Facebook和Instagram活动的全部内容都将被用于构建其他品牌可能使用的心理特征。了解消费者关于其在线行为及其提供的文字的隐私偏好可以为从业者和研究人员提供重要的指导。未来研究的另一个亮点是可以提高营销的精确度，同时最大限度地减少对隐私的侵犯（Provost et al 2009）。\n总结 沟通是营销的重要方面，涵盖组织与合作伙伴之间，企业与消费者之间以及消费者之间的沟通。文本数据包含这些交流的详细信息，并且通过自动文本分析，研究人员已准备好将这种原始材料转换成有价值的见解。文本数据使用方面的许多最新进展是在营销之外的领域开发的。当我们展望未来和营销人员的角色时，这些最新进展应作为示例。营销人员在消费者，公司和组织之间的接口上处于有利位置，可以利用和改进工具来提取文本信息，以解决当今企业和社会所面临的一些关键问题，例如错误的信息滥用。营销提供了一种宝贵的观点，对这次对话至关重要，但这只有通过更广阔的视野，打破理论和方法论的孤岛，并与其他学科合作，我们的研究才能吸引尽可能多的受众来影响公众话语。我们希望这个框架能够鼓励人们对界定营销的界限进行反思，并为未来的突破性见解开辟道路。\n","permalink":"/blog/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90%E5%9C%A8%E5%B8%82%E5%9C%BA%E8%90%A5%E9%94%80%E7%A0%94%E7%A9%B6%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/","summary":"翻译自\nBerger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.\u0026rdquo; Journal of Marketing (2019): 0022242919873106.\n 论文作者们的报告视频已上传到B站(下图)，感兴趣的童鞋可以先收藏再收看\nJourmal of Marketing Webinar｜2019市场营销\n摘要 语言文字是营销场景中最常用的交互方式，比如在线评论、消费者服务热线、新闻发布、营销传播等活动都创造了有价值的文本数据。但营销研究者如何用好这些数据？本文回顾了文本分析相关研究，并详细介绍了如何用文本数据做市场研究。作者讨论了文本如何反映文本生产者， 文本信息如何影响信息接受者。\n接下来，本文讨论了文本如何预测并理解文本背后的信息，回顾了文本分析的方法和测量指标(metrics),提供了一整套的文本分析操作流程。最后，作者提到文本分析内部信度和外部效度问题，研究者如何解决。本文讨论营销各个领域可能存在的研究机会，虽然目前市场营销的研究问题大都是跨学科的，但是营销的各个子领域经常都是孤立，借助文本分析可能架构起连接营销各个子领域的桥梁。\n关键词  计算语义学coputational linguistics 机器学习machine learning 市场洞察marketing insight 跨学科interdisciplinary 自然语言处理natural language processing 文本分析text analysis 文本挖掘 text mining  无所不在文本 之前的研究认为，尽管投资者一次对包含重大变化的财务报\n交流沟通是营销的重要组成部分，消费者、企业、消费者投资者、社会，不同水平或者统一水平都有信息交流与沟通。而信息交流的过程中往往会产生或者转化为文本数据。\n最简单的的文本数据世界模型是生产者与消费者。模型内生产者和接受者都可能是消费者、企业、投资者和社会。消费者书写在线评论，公司制作会计年报，文化生产者代表社会意义制作出书籍、影片和艺术品（Table 1）\n在此情形下，研究者可能选择文本如何反映或如何影响？\n How text reflects its producer？ How text impacts its receiver？  尤其是文本可以反映一定的信息，这些信息是可以帮助营销人员洞察市场规律，进而利用规律影响文本信息的接受者。","title":"文本分析在市场营销研究中的应用"},{"content":"在大数据的今天，通过互联网超文本链接，无数的个人、团体、公司、政府等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。社会、管理、经济、营销、金融等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是三份文档翻译汇总而来，我觉得讲的挺明白的，其中加入了我的一点点理解和扩充。\n一、文本产生及其作用方式  How text reflects its producer？ How text impacts its receiver？  graph LR Text_Producer --\u0026gt; Text Text --\u0026gt; Text_Receiver Text_Receiver --\u0026gt;Text Text --\u0026gt; Text_Producer 文本信息的==生产者producer== 与 ==消费者receiver==，涵盖 ==个人、公司(组织)、国家(社会)==三个层面。\ngraph LR Consumers --\u0026gt; Firms Consumers --\u0026gt; Investors Consumers --\u0026gt; Society Firms --\u0026gt; Consumers Firms --\u0026gt; Investors Investors --\u0026gt; Firms Investors --\u0026gt; Society Firms --\u0026gt; Society Society --\u0026gt; Investors Society --\u0026gt; Consumers  需要注意的是文本的==反映Reflects==和==影响Impacts==并不是非此即彼，往往会同时起作用。\n    \u0026mdash; 研究目的 自变量 因变量 因变量     Reflects 文本可以反映producer的一些信息，帮助研究者理解producer。\n例如试图挖掘producer的个性personality或属于什么社会团体。 了解公司的品牌个性；\n年报也会有未来业绩表现的有价值线索；\n消费者们在品牌社区的言语能更深的投射出消费者对品牌的态度；\n而更宏大的层面，文本也能反映出文化差异。\n了解消费者是否喜欢新产品，消费者如何看待品牌，消费者最看重什么 文本 文本   Affects 知道文本如何影响receiver，receiver会有什么样的行为和选择。 检验文本是否以及如何导致消费者诸如购买、分享和卷入行为。\n广告会塑造消费者的消费行为\n消费者杂志会扭曲消费者产品分类感知\n电影剧本会影响观众的反应 文本消费者 文本消费者    \n二、如何使用文本数据    \u0026mdash; Reflects Affects 目的 应用 难点     Predict 预测 producer的状态、特性、性格等 预测 receiver阅读、分享和购买行为 研究人员不怎么关系任意的文本特质，他们更关心预测的表现。 什么消费者最喜欢贷款;\n什么电影会大火;\n未来股市走向;\n 文本数据可以生成成千上万的特征(相当于变量x1，x2\u0026hellip;xn)，而文本数据记录数甚至可能少于特征数。\n为了解决这个为题，使用新的特征分类方法，减少特征数量，又有可能存在拟合问题。   Understanding 为什么当人们压抑的时候会使用特殊人称。 来理解为何带有情绪的文本会更容易被阅读和分享 理解为什么事情发生以及如何发生的这类研究往往会用到心理学、社会学的方法，旨在理解文本的什么特征会导致什么后续结果，以及为什么产生这样的后果。 消费者怎样表达会如何影响口碑;\n为何某些推文会被挑中分享？\n歌曲为何变火？\n品牌如何让消费者忠诚？ 找出观测数据背后的因果关系。相应的，该领域的工作可能会强调实验数据，以允许对关键的独立变量进行操作。\n另一个挑战是解释文本特征之间的关系。    \n三、文本信息的指标 粗略的分，文本信息可以分为定性与定量两种类型\n   定性/量 分析方法 优点 缺点     定性（text as text） 质性（扎根） 依靠研究者领域知识，可以对少量的数据做出深刻洞见。 难以应对大规模数据；\n编码过程并不能保证唯一；   定量 textual data(text as data) 明显的文本特征，如词频、可阅读性 标准如一;\n适合大规模文本挖掘；\n纷繁复杂中涌现出潜在规律 需要破坏文本的结构，丧失了部分信息量    早先的营销领域，如在线评论文本分析指标多为\n 数量，如文本长度(e.g., Godes and Mayzlin 2004; Moe and Trusov2011) **情感得分(效价，评论评分) **(e.g., Godes and Silva 2012; Moe and Schweidel 2012; Ying, Feinberg and Wedel 2006)· 方差，如信息墒(e.g., Godes and Mayzlin 2004).  然而如今这些指标经常忽略了文本的丰富度。以下几种是更好用的指标\n   指标 功能 补充     实体词词频 使用相应的实体词典，统计实体出现次数，这样可以对不同实体进行比较 更长的文本通常含有更多的实体(的数量)；\n还有一个局限就是某些实体会比其他实体更多的出现，如“电脑”商品的在线评论中“电脑”出现次数会远多于其他词。   相似度 文档之间的相似度感兴趣。\n如两个广告之间的相似程度如何？\n两首歌的歌词相似程度多少？ 相似度的计算方法有\ncos余弦相似\njaccard相似   可读性 同样的意思可以用不同的难度的词汇去表达，造成阅读的难易程度。可读性反映了作者的内容复杂度和读者的阅读难度。 常见的可读性算法有Flesch–Kincaid和the simple measure of gobbledygook (SMOG)。\n可阅读性经常将得分设置到1-12分之间，在美国学校里阅读理解成绩水平得分就是1-12分。    \n四、文本分析步骤    序号 步骤 解释 中文 英文     1 读取数据 数据一般存储于不同的文件夹不同文件内，需要将其导入到计算机     2 分词 导入到计算的文本是字符串数据，需要整理为更好用的列表 例如“我爱你中国”分词后\n得到[\u0026ldquo;我\u0026rdquo;, \u0026ldquo;爱\u0026rdquo;, \u0026ldquo;你\u0026rdquo;, \u0026ldquo;中国\u0026rdquo;] \u0026ldquo;I love China\u0026quot;分为\n[\u0026ldquo;I\u0026rdquo;, \u0026ldquo;love\u0026rdquo;, \u0026ldquo;China\u0026rdquo;]   3 剔除符号和无意义的停止词 为了降低计算机运行时间，对分析结果影响较小的字符，诸如符号和无意义的词语需要剔除掉 如“的”，“她”， ”呢”， “了” \u0026ldquo;is\u0026rdquo; , \u0026ldquo;a\u0026rdquo;, \u0026ldquo;the\u0026rdquo;   4 字母变小写，词干化 同义词归并，同主体词归并 “中铁”，“中国铁建”，“中铁集团”都可以归并为“中铁” 先变为小写，这样“I”和“i”都归并为“i”；\n“was”，“are”，“is”都归并为“be”   5 构建文档词频矩阵 使用一定的编码方式，即用某种方式表示文本。常见的有词袋法、tf-idf；\n可以使用scikit-learn构建文档词频矩阵，但中英文略有区别，需要注意 “我爱你中国”需要先整理为“我 爱 你 中国” “I love China”    \n五、文本分析技术对比 从左向右，自动化程度越来越高，人工介入的越来越少\n   技术 描述 优点 缺点 常被应用(领域) 软件     主题分析Thematic analysis 需要有经验的人员基于自身经验和李俊杰，对研究的数据进行挖掘。编码过程为迭代进行 使用参与者自己的话语或者构念来挖掘数据，对少量文本理解的更深入 属于时间、劳动密集型任务，不适合大规模数据。\n由于不同的编码人员有不同的经历和偏好，编码过程的标准不可靠 社会学、管理学 Nvivo；   内容分析/基于字典方法 统计文本中词语/词组的出现频率 允许对研究的数据进行定量分析 采用的词典应尽量与研究问题适应，词典适配性问题突出 管理学 LIWC、Nvivo、DICTION；   词袋法（Bag of words） 将文本字符串转为计算机能理解的数字化向量 编码标准稳定简单，具有统计学特性，扩展性强 编码过程忽略词语的先后顺序 管理学 Python的scikit-learn、gensim、nltk等；R   监督学习(Supervise models),如SVM、Bayes、Logistic Regression 研究者要知道输入数据X和标签y；需要核实的模型需要X和y之间的关系和规律 允许事先定义编码规则(如选择词袋法还是tfidf)；逻辑简单 需要高质量的标注数据(工作量大)；you与特征词太多，训练的模型很容易过拟合。 计算机学、政治学、管理学 Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）   无监督学习(Kmeans、 LDA话题模型) 使用聚类、话题分析，让计算机自动对数据进行分组 在没有人工标注的情况下，加速了数据的“标注”或“分类” “标注”是机器按照数字特征进行的分组，需要研究者解读才可以赋予“标准“意义；训练过程需要大量的调参 计算机学、政治学、管留学 Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）   自然语言处理 按照人类对语言的理解进行建模，考虑词语顺序 计算机自动化；可分析语义 大多数模型是人类无法解读的黑箱；\n虽然代码编程量小，但训练代码耗时巨大 计算科学；市场营销；心理学 pytorch、tensorflow    据被压缩成词组频数，定性的文本数据转化为定量的频数。本课程中会涉及到的内容\n Thematic Analysis 定性 Content Analysis Dictionary Bag of words 词袋法 Supervised ，监督学习 文本分类问题 Unsupervised，如非监督LDA话题模型 Natural language processing  \n应用案例 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究 摘要：众筹融资效果决定着众筹平台的兴衰。 众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。 而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。 不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。\n首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。\n然后，==借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。==\n最后，建立语言说服风格对项目筹资影响的计量模型，并 对 ==Kickstarter 平台上的 128345 个项目进行实证分析==。 总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。\n关键词：众筹 融资 语言风格 说服性 投资意愿\nCopycats vs. Original Mobile Apps 摘要: 尽管移动应用程序市场的增长为移动应用程序开发人员创新提供了巨大的市场机会和经济诱因，但它也不可避免地刺激了模仿者开发盗版软件。原始应用的从业人员和开发人员声称，模仿者窃取了原始应用的想法和潜在需求，并呼吁应用平台对此类模仿者采取行动。令人惊讶的是，很少有严格的研究来分析模仿者是否以及如何影响原始应用的需求。\n==进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原始或模仿者并检测两种模仿者的方法：欺骗性和非欺骗性。==\n根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的==5,141个开发人员的10,100个动作游戏应用程序==样本中，模仿应用程序对原始应用程序需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原件的需求。\n结果表明，从总体上讲，模仿者对原始移动应用程序需求的影响在统计上是微不足道的。==我们的研究通过提供一种识别模仿者的方法==，并提供模仿者对原始应用需求的影响的证据，为越来越多的移动应用消费文献做出了贡献。\nLAZY PRICES 摘要: 使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对报告进行积极更改时，这种行为蕴含着公司未来运营的重要信号。\n财务报告的语言和结构的变化也对公司的未来收益产生重大影响：做空\u0026quot;变化\u0026quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。\n研究发现，对10-K的变化可以预测未来的收益、获利能力、未来的新闻公告，甚至未来的公司破产。同时，不做任何变化的公司将获得显著的异常收益。与资产价格典型的反应不足研究不同，我们发现没有任何与这些变化相关的公告效应–仅在后来通过新闻，事件或收益披露信息时才产生回报–暗示投资者并未注意到整个公众领域的这些变化。\n 纽约时报在2010年4月23日发了一条FDA将有对输液泵(infusion pumps)更严格对审批管理规定的新闻，新闻中提到了Baxter公司。新闻公布当天，Baxter股价大跌。\n10天后的（2010年5月4日），Baxter宣布召回问题的输液泵产品，股价当天再次大跌。\n 相关文献  [1]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.\u0026rdquo; Journal of Marketing (2019): 0022242919873106.\n[2]Kenneth Benoit. July 16, 2019. “[Text as Data: An Overview](https://kenbenoit.net/pdfs/28 Benoit Text as Data draft 2.pdf).” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. Handbook of Research Methods in Political Science and International Relations. Thousand Oaks: Sage.\n[3]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. \u0026ldquo;A review of best practice recommendations for text analysis in R (and a user-friendly app).\u0026rdquo; Journal of Business and Psychology 33, no. 4 (2018): 445-459.\n[4]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.管理世界.2016;5:81-98.\n[5]Wang, Quan, Beibei Li, and Param Vir Singh. \u0026ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.\u0026rdquo; Information Systems Research 29, no. 2 (2018): 273-291.\n[6]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. \u0026ldquo;Lazy prices.\u0026rdquo; The Journal of Finance 75, no. 3 (2020): 1371-1415.\n ","permalink":"/blog/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90%E5%9C%A8%E7%BB%8F%E7%AE%A1%E9%A2%86%E5%9F%9F%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E6%A6%82%E8%BF%B0/","summary":"在大数据的今天，通过互联网超文本链接，无数的个人、团体、公司、政府等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。社会、管理、经济、营销、金融等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是三份文档翻译汇总而来，我觉得讲的挺明白的，其中加入了我的一点点理解和扩充。\n一、文本产生及其作用方式  How text reflects its producer？ How text impacts its receiver？  graph LR Text_Producer --\u0026gt; Text Text --\u0026gt; Text_Receiver Text_Receiver --\u0026gt;Text Text --\u0026gt; Text_Producer 文本信息的==生产者producer== 与 ==消费者receiver==，涵盖 ==个人、公司(组织)、国家(社会)==三个层面。\ngraph LR Consumers --\u0026gt; Firms Consumers --\u0026gt; Investors Consumers --\u0026gt; Society Firms --\u0026gt; Consumers Firms --\u0026gt; Investors Investors --\u0026gt; Firms Investors --\u0026gt; Society Firms --\u0026gt; Society Society --\u0026gt; Investors Society --\u0026gt; Consumers  需要注意的是文本的==反映Reflects==和==影响Impacts==并不是非此即彼，往往会同时起作用。\n    \u0026mdash; 研究目的 自变量 因变量 因变量     Reflects 文本可以反映producer的一些信息，帮助研究者理解producer。","title":"文本分析在经管领域中的应用概述"},{"content":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/在Rmarkdown中调用Python代码.zip\nPython和R是一对数据科学两大语言，在互联互通的路上，我感觉R更加的积极。还记得之前 Python中调用R的库-rpy2， 在Python使用R语言语法还是有些不自然。在R中调用Python体验十分棒，一起跟我看看吧。\nreticulate包是可以让R语言非常流畅自然联通Python的关键。\nRmarkdown预备知识\nRmarkdown很像jupyter notbeook和markdown的结合。\n 代码块 markdon文本  代码块 在Rmarkdown中的代码块的开始都是以三引号、{}和语言名为标志，以三引号结尾。\nlibrary(ggplot2) ggplot(mpg, aes(x=displ, y=cty))+ geom_point() library(ggplot2) ggplot(mpg, aes(x=displ, y=cty))+ geom_point()   后面设置好reticulate包后，就可以在Rmarkdown中使用python代码块，\nimport pandas as pd df = pd.read_csv(\u0026#39;test.csv\u0026#39;) df.head() markdown文本 文本部分全部支持markdown语法，这里不做赘述。\n安装 install.packages(\u0026#34;reticulate\u0026#34;) 环境设置 当我们在R代码块中设置好Python环境，就可以在Rmarkdown中跑Python代码。\n查询Python 首先我们需要知道Python安装路径，可以在命令行中逐行执行下面代码\npython3 import sys sys.executable 我的mac电脑的Python安装路径为\n/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 指定Python 执行下方的代码(路径改为自己的Python安装路径)\nlibrary(reticulate) ## Warning: package \u0026#39;reticulate\u0026#39; was built under R version 3.6.2 use_python(\u0026#39;/Library/Frameworks/Python.framework/Versions/3.7/bin/python3\u0026#39;) 执行代码后，我们就可以愉快的使用Python代码\n运行Python代码 在Rmarkdown中写Python代码块\n作图 import numpy as np import matplotlib.pyplot as plt # 计算正弦曲线上点的 x 和 y 坐标 x = np.arange(0, 3 * np.pi, 0.1) y = np.sin(x) plt.title(\u0026#34;sine wave form\u0026#34;) # 使用 matplotlib 来绘制点 plt.plot(x, y) plt.show()   读取csv import pandas as pd df = pd.read_csv(\u0026#34;test.csv\u0026#34;, encoding=\u0026#34;gbk\u0026#34;) df.head() ## birthday name text age gender height weight ## 0 1985/10/08 Alice 我很开心，每天都这么快乐，我很幸福 35 female 175 55 ## 1 95.07.07 Mary 我很难过 25 female 165 50 ## 2 01-11-10 Mike 唉，真难受 19 male 180 75 ## 3 90/2/8 Smith 无所谓开心还是难过 30 male 175 70 ## 4 93-1-5 Henry 每天赚一万，真爽！ 27 male 185 80 返回的df是Python对象，我们可以看到表格不好看，这是R中的Python对象。\n在R代码块中调用Python变量(对象) 刚刚讲的都是在Rmakdown中运行Python代码块，并不是在R代码块中运行Python代码或者调用Python变量。\npy$python_variable_name R代码块中调用Python方法\npy$python_variable_name\n py相当于Python中的对象 $ 相当于Python中的点 python_variable_name 是Python代码块中的变量名  比如在上文中Python的变量df，在R中调用\npy$df   现在调用Python对象df时，R会默认将其转为R对象，所以内容一样，样式似乎变好看了。\nR代码块中导入Python库 使用os库的listdir函数查询当前项目文件夹内的文件列表\nimport os os.listdir() ## [\u0026#39;reticulate学习.md\u0026#39;, \u0026#39;test.csv\u0026#39;, \u0026#39;test.py\u0026#39;, \u0026#39;reticulate学习.html\u0026#39;, \u0026#39;reticulate学习_files\u0026#39;, \u0026#39;reticulate.pdf\u0026#39;, \u0026#39;reticulate学习.Rmd\u0026#39;, \u0026#39;data.py\u0026#39;] 在R代码块中实现上方的Python功能，如下，很简单\nlibrary(reticulate) #导入库 os \u0026lt;- import(\u0026#34;os\u0026#34;) #os库的listdir函数 os$listdir() ## [1] \u0026#34;reticulate学习.md\u0026#34; \u0026#34;test.csv\u0026#34; \u0026#34;test.py\u0026#34; ## [4] \u0026#34;reticulate学习.html\u0026#34; \u0026#34;reticulate学习_files\u0026#34; \u0026#34;reticulate.pdf\u0026#34; ## [7] \u0026#34;reticulate学习.Rmd\u0026#34; \u0026#34;data.py\u0026#34; 可以发现\n import(\u0026quot;os)代替了import os $代替了. \u0026lt;- 代替了 =  再熟悉一下\nlibrary(reticulate) pd \u0026lt;- import(\u0026#34;pandas\u0026#34;) df2 \u0026lt;- pd$read_csv(\u0026#34;test.csv\u0026#34;, encoding=\u0026#34;gbk\u0026#34;) df2   需要注意的是，在R代码块中执行Python代码时，默认会将Python对象转为R对象。\nsource_python() 使用reticulate包中的source_python(\u0026lsquo;py文件路径\u0026rsquo;)可以导入py文件中的变量，这样就可以在R代码块中使用外部变量。例如我在data.py中准备A和B两个字符串\nA = \u0026#39;我是张三,\u0026#39; B = \u0026#39;来自河北\u0026#39; 在R代码块中运行data.py\nlibrary(reticulate) source_python(\u0026#34;data.py\u0026#34;) print(A) ## [1] \u0026#34;我是张三,\u0026#34; print(B) ## [1] \u0026#34;来自河北\u0026#34; paste0(A, B) ## [1] \u0026#34;我是张三,来自河北\u0026#34; py_run_file() 在R代码块中运行项目文件夹中的test.py文件\nlibrary(reticulate) py_run_file(\u0026#34;test.py\u0026#34;) 数据类型对比    R Python Examples     Single-element vector Scalar 1, 1L, TRUE, \u0026quot;foo\u0026quot;   Multi-element vector List c(1.0, 2.0, 3.0), c(1L, 2L, 3L)   List of multiple types Tuple list(1L, TRUE, \u0026quot;foo\u0026quot;)   Named list Dict list(a = 1L, b = 2.0), dict(x = x_data)   Matrix/Array NumPy ndarray matrix(c(1,2,3,4), nrow = 2, ncol = 2)   Data Frame Pandas DataFrame data.frame(x = c(1,2,3), y = c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;))   Function Python function function(x) x + 1   NULL, TRUE, FALSE None, True, False NULL, TRUE, FALSE    ","permalink":"/blog/%E5%9C%A8rmarkdown%E4%B8%AD%E8%B0%83%E7%94%A8python%E4%BB%A3%E7%A0%81/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/在Rmarkdown中调用Python代码.zip\nPython和R是一对数据科学两大语言，在互联互通的路上，我感觉R更加的积极。还记得之前 Python中调用R的库-rpy2， 在Python使用R语言语法还是有些不自然。在R中调用Python体验十分棒，一起跟我看看吧。\nreticulate包是可以让R语言非常流畅自然联通Python的关键。\nRmarkdown预备知识\nRmarkdown很像jupyter notbeook和markdown的结合。\n 代码块 markdon文本  代码块 在Rmarkdown中的代码块的开始都是以三引号、{}和语言名为标志，以三引号结尾。\nlibrary(ggplot2) ggplot(mpg, aes(x=displ, y=cty))+ geom_point() library(ggplot2) ggplot(mpg, aes(x=displ, y=cty))+ geom_point()   后面设置好reticulate包后，就可以在Rmarkdown中使用python代码块，\nimport pandas as pd df = pd.read_csv(\u0026#39;test.csv\u0026#39;) df.head() markdown文本 文本部分全部支持markdown语法，这里不做赘述。\n安装 install.packages(\u0026#34;reticulate\u0026#34;) 环境设置 当我们在R代码块中设置好Python环境，就可以在Rmarkdown中跑Python代码。\n查询Python 首先我们需要知道Python安装路径，可以在命令行中逐行执行下面代码\npython3 import sys sys.executable 我的mac电脑的Python安装路径为\n/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 指定Python 执行下方的代码(路径改为自己的Python安装路径)\nlibrary(reticulate) ## Warning: package \u0026#39;reticulate\u0026#39; was built under R version 3.6.2 use_python(\u0026#39;/Library/Frameworks/Python.framework/Versions/3.7/bin/python3\u0026#39;) 执行代码后，我们就可以愉快的使用Python代码\n运行Python代码 在Rmarkdown中写Python代码块\n作图 import numpy as np import matplotlib.","title":"在Rmarkdown中调用Python代码"},{"content":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/SciencePlot科研绘图.zip\n安装 !pip3 install SciencePlots Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/ Collecting SciencePlots Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c2/44/7b5c0ecd6f2862671a076425546f86ac540bc48c1a618a82d6faa3b26f58/SciencePlots-1.0.9.tar.gz (10 kB) Installing build dependencies ... \u001b[?25l/  tips:\nSciencePlots库需要电脑安装LaTex，其中\n MacOS电脑安装MacTex https://www.tug.org/mactex/ Windows电脑安装MikTex https://miktex.org/  初始化绘图样式 在SciencePlots库中科研绘图样式都是用的science\nimport matplotlib.pyplot as plt plt.style.use(\u0026#39;science\u0026#39;) 当然你也可以同时设置多个样式\nplt.style.use([\u0026#39;science\u0026#39;, \u0026#39;ieee\u0026#39;]) 在上面的代码中， ieee 会覆盖掉 science 中的某些参数（列宽、字号等）， 以达到符合 IEEE论文的绘图要求\n如果要临时使用某种绘图样式，科研使用如下语法\n#注意，此处是语法示例， #如要运行， 请提前准备好x和y的数据 with plt.style.context([\u0026#39;science\u0026#39;, \u0026#39;ieee\u0026#39;]): plt.figure() plt.plot(x, y) plt.show() 案例 定义函数曲线， 准备数据\nimport numpy as np import matplotlib.pyplot as plt def function(x, p): return x ** (2 * p + 1) / (1 + x ** (2 * p)) pparam = dict(xlabel=\u0026#39;Voltage (mV)\u0026#39;, ylabel=\u0026#39;Current ($\\mu$A)\u0026#39;) x = np.linspace(0.75, 1.25, 201) science样式 with plt.style.context([\u0026#39;science\u0026#39;]): fig, ax = plt.subplots() for p in [10, 15, 20, 30, 50, 100]: ax.plot(x, function(x, p), label=p) ax.legend(title=\u0026#39;Order\u0026#39;) ax.autoscale(tight=True) ax.set(**pparam) fig.savefig(\u0026#39;figures/fig1.pdf\u0026#39;) fig.savefig(\u0026#39;figures/fig1.jpg\u0026#39;, dpi=300)   science+ieee样式 针对IEEE论文准备的science+ieee样式\nwith plt.style.context([\u0026#39;science\u0026#39;, \u0026#39;ieee\u0026#39;]): fig, ax = plt.subplots() for p in [10, 20, 40, 100]: ax.plot(x, function(x, p), label=p) ax.legend(title=\u0026#39;Order\u0026#39;) ax.autoscale(tight=True) ax.set(**pparam) # Note: $\\mu$ doesn\u0026#39;t work with Times font (used by ieee style) ax.set_ylabel(r\u0026#39;Current (\\textmu A)\u0026#39;) fig.savefig(\u0026#39;figures/fig2a.pdf\u0026#39;) fig.savefig(\u0026#39;figures/fig2a.jpg\u0026#39;, dpi=300)   science+scatter样式 IEEE 要求图形以黑白打印时必须可读。 ieee 样式还可以将图形宽度设置为适合IEEE论文的一列。\nwith plt.style.context([\u0026#39;science\u0026#39;, \u0026#39;scatter\u0026#39;]): fig, ax = plt.subplots(figsize=(4, 4)) ax.plot([-2, 2], [-2, 2], \u0026#39;k--\u0026#39;) ax.fill_between([-2, 2], [-2.2, 1.8], [-1.8, 2.2], color=\u0026#39;dodgerblue\u0026#39;, alpha=0.2, lw=0) for i in range(7): x1 = np.random.normal(0, 0.5, 10) y1 = x1 + np.random.normal(0, 0.2, 10) ax.plot(x1, y1, label=r\u0026#34;$^\\#${}\u0026#34;.format(i+1)) ax.legend(title=\u0026#39;Sample\u0026#39;, loc=2) xlbl = r\u0026#34;$\\log_{10}\\left(\\frac{L_\\mathrm{IR}}{\\mathrm{L}_\\odot}\\right)$\u0026#34; ylbl = r\u0026#34;$\\log_{10}\\left(\\frac{L_\\mathrm{6.2}}{\\mathrm{L}_\\odot}\\right)$\u0026#34; ax.set_xlabel(xlbl) ax.set_ylabel(ylbl) ax.set_xlim([-2, 2]) ax.set_ylim([-2, 2]) fig.savefig(\u0026#39;figures/fig3.pdf\u0026#39;) fig.savefig(\u0026#39;figures/fig3.jpg\u0026#39;, dpi=300)   dark_background +science+high-vis 您还可以将这些样式与Matplotlib随附的其他样式结合使用。 例如，dark_background +science+high-vis样式：\nwith plt.style.context([\u0026#39;dark_background\u0026#39;, \u0026#39;science\u0026#39;, \u0026#39;high-vis\u0026#39;]): fig, ax = plt.subplots() for p in [10, 15, 20, 30, 50, 100]: ax.plot(x, function(x, p), label=p) ax.legend(title=\u0026#39;Order\u0026#39;) ax.autoscale(tight=True) ax.set(**pparam) fig.savefig(\u0026#39;figures/fig5.pdf\u0026#39;) fig.savefig(\u0026#39;figures/fig5.jpg\u0026#39;, dpi=300)   ","permalink":"/blog/%E7%A7%91%E7%A0%94%E7%BB%98%E5%9B%BEscienceplots/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/SciencePlot科研绘图.zip\n安装 !pip3 install SciencePlots Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/ Collecting SciencePlots Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c2/44/7b5c0ecd6f2862671a076425546f86ac540bc48c1a618a82d6faa3b26f58/SciencePlots-1.0.9.tar.gz (10 kB) Installing build dependencies ... \u001b[?25l/  tips:\nSciencePlots库需要电脑安装LaTex，其中\n MacOS电脑安装MacTex https://www.tug.org/mactex/ Windows电脑安装MikTex https://miktex.org/  初始化绘图样式 在SciencePlots库中科研绘图样式都是用的science\nimport matplotlib.pyplot as plt plt.style.use(\u0026#39;science\u0026#39;) 当然你也可以同时设置多个样式\nplt.style.use([\u0026#39;science\u0026#39;, \u0026#39;ieee\u0026#39;]) 在上面的代码中， ieee 会覆盖掉 science 中的某些参数（列宽、字号等）， 以达到符合 IEEE论文的绘图要求\n如果要临时使用某种绘图样式，科研使用如下语法\n#注意，此处是语法示例， #如要运行， 请提前准备好x和y的数据 with plt.style.context([\u0026#39;science\u0026#39;, \u0026#39;ieee\u0026#39;]): plt.figure() plt.plot(x, y) plt.show() 案例 定义函数曲线， 准备数据\nimport numpy as np import matplotlib.pyplot as plt def function(x, p): return x ** (2 * p + 1) / (1 + x ** (2 * p)) pparam = dict(xlabel=\u0026#39;Voltage (mV)\u0026#39;, ylabel=\u0026#39;Current ($\\mu$A)\u0026#39;) x = np.","title":"科研绘图SciencePlots库"},{"content":"代码下载 点击跳转到下载链接页面\nR语言的ggplot2绘图能力超强，python虽有matplotlib，但是语法臃肿，使用复杂，入门极难，seaborn的出现稍微改善了matplotlib代码量问题，但是定制化程度依然需要借助matplotlib，使用难度依然很大。\n好消息是python中有一个plotnine包，可以实现绝大多数ggplot2的绘图功能，两者语法十分相似，R和Python的语法转换成本大大降低。\n plotnine文档 https://plotnine.readthedocs.io/en/latest/ R语言ggplot2文档 https://ggplot2.tidyverse.org/reference/index.html  安装 !pip3 install plotnine 准备数据 from plotnine.data import mpg #dataframe mpg.head()     manufacturer model displ year cyl trans drv cty hwy fl class     0 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact   1 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact   2 audi a4 2 2008 4 manual(m6) f 20 31 p compact   3 audi a4 2 2008 4 auto(av) f 21 30 p compact   4 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact    快速作图qplot 我们先直接看最简单好用的快速作图函数qplot(x, y, data)\n 横坐标displ 纵坐标cty 数据mpg  from plotnine import qplot qplot(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;, data=mpg)   ggplot图层 qplot是快速作图函数，如果想让图更好看，进行私人订制，那么我们需要进行图层设计\n首先设置ggplot图层（相当于买了一个高级画布），\n 数据mpg 横坐标x轴为displ 纵坐标y轴cty  在plotnine中，变量所对应的数据均可通过字段名调用\nfrom plotnine import ggplot, geom_point, aes ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg)   图层叠加 我们可以看到，已经绘制出一个空的ggplot图层，x轴为displ，y轴为cty。\n接下来我们给这个图层上加上数据对应的散点，使用geom_point()直接追加在ggplot图层之上即可。\n(ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg) + geom_point() )   color 在上图中，散点是没有区分每辆车的气缸数cyl。\n在geom_point()中，我们可以按照气缸数cyl分门别类，按照颜色显示出来\n(ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg) + geom_point(aes(color=\u0026#39;cyl\u0026#39;)) )   上图挺好看的，有时候需要绘制的字段是离散型数值，但是上色后可能不够明显，需要声明该字段为离散型。这时候用factor()来告诉plotnine，这个字段是离散型数值\n(ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg) + geom_point(aes(color=\u0026#39;factor(cyl)\u0026#39;)) )   size 有时候为了增加可视化显示的维度数，还可以考虑加入点的大小size\n(ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg) + geom_point(aes(size=\u0026#39;hwy\u0026#39;)) )   梯度色 如果你想自己设置颜色的梯度，可以通过scale_color_gradient设置\nfrom plotnine import scale_color_gradient (ggplot(aes(x=\u0026#39;displ\u0026#39;, y=\u0026#39;cty\u0026#39;), mpg) + geom_point(aes(color=\u0026#39;hwy\u0026#39;)) + scale_color_gradient(low=\u0026#39;blue\u0026#39;, high=\u0026#39;red\u0026#39;) )   条形图 plotnine中可绘制的图有很多，刚刚已经讲了散点图，接下来我们看看plotnine中的条形图。\n首先准备一下数据\nimport pandas as pd df = pd.DataFrame({ \u0026#39;variable\u0026#39;: [\u0026#39;gender\u0026#39;, \u0026#39;gender\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;income\u0026#39;, \u0026#39;income\u0026#39;, \u0026#39;income\u0026#39;, \u0026#39;income\u0026#39;], \u0026#39;category\u0026#39;: [\u0026#39;Female\u0026#39;, \u0026#39;Male\u0026#39;, \u0026#39;1-24\u0026#39;, \u0026#39;25-54\u0026#39;, \u0026#39;55+\u0026#39;, \u0026#39;Lo\u0026#39;, \u0026#39;Lo-Med\u0026#39;, \u0026#39;Med\u0026#39;, \u0026#39;High\u0026#39;], \u0026#39;value\u0026#39;: [60, 40, 50, 30, 20, 10, 25, 25, 40], }) df[\u0026#39;variable\u0026#39;] = pd.Categorical(df[\u0026#39;variable\u0026#39;], categories=[\u0026#39;gender\u0026#39;, \u0026#39;age\u0026#39;, \u0026#39;income\u0026#39;]) df[\u0026#39;category\u0026#39;] = pd.Categorical(df[\u0026#39;category\u0026#39;], categories=df[\u0026#39;category\u0026#39;]) df     variable category value     0 gender Female 60   1 gender Male 40   2 age 1-24 50   3 age 25-54 30   4 age 55+ 20   5 income Lo 10   6 income Lo-Med 25   7 income Med 25   8 income High 40    from plotnine import ggplot, aes, geom_text, position_dodge, geom_point #调整文本位置 dodge_text = position_dodge(width=0.9) # new (ggplot(df, aes(x=\u0026#39;variable\u0026#39;, y=\u0026#39;value\u0026#39;, fill=\u0026#39;category\u0026#39;)) #类别填充颜色 + geom_col(position=\u0026#39;dodge\u0026#39;, show_legend=False) # modified + geom_text(aes(y=-.5, label=\u0026#39;category\u0026#39;), # new position=dodge_text, color=\u0026#39;gray\u0026#39;, #文本颜色 size=8, #字号 angle=30, #文本的角度 va=\u0026#39;top\u0026#39;) + lims(y=(-5, 60)) # new )   from plotnine.data import economics_long economics_long.head()     date variable value value01     0 1967-07-01 00:00:00 pce 507.4 0   1 1967-08-01 00:00:00 pce 510.5 0.000266001   2 1967-09-01 00:00:00 pce 516.3 0.00076368   3 1967-10-01 00:00:00 pce 512.9 0.000471937   4 1967-11-01 00:00:00 pce 518.1 0.000918132    from plotnine import ggplot, aes, geom_line (ggplot(economics_long, aes(x=\u0026#39;date\u0026#39;, y=\u0026#39;value01\u0026#39;, color=\u0026#39;variable\u0026#39;)) + geom_line() )   plotnine目前已经支持绝大多数ggplot2，但是文档方面没有ggplot2全，所以学习plotnine时可以参考ggplot2。\n plotnine文档 https://plotnine.readthedocs.io/en/latest/ R语言ggplot2文档 https://ggplot2.tidyverse.org/reference/index.html  ","permalink":"/blog/plotnine%E5%8C%85%E5%AD%A6%E4%B9%A0/","summary":"代码下载 点击跳转到下载链接页面\nR语言的ggplot2绘图能力超强，python虽有matplotlib，但是语法臃肿，使用复杂，入门极难，seaborn的出现稍微改善了matplotlib代码量问题，但是定制化程度依然需要借助matplotlib，使用难度依然很大。\n好消息是python中有一个plotnine包，可以实现绝大多数ggplot2的绘图功能，两者语法十分相似，R和Python的语法转换成本大大降低。\n plotnine文档 https://plotnine.readthedocs.io/en/latest/ R语言ggplot2文档 https://ggplot2.tidyverse.org/reference/index.html  安装 !pip3 install plotnine 准备数据 from plotnine.data import mpg #dataframe mpg.head()     manufacturer model displ year cyl trans drv cty hwy fl class     0 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact   1 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact   2 audi a4 2 2008 4 manual(m6) f 20 31 p compact   3 audi a4 2 2008 4 auto(av) f 21 30 p compact   4 audi a4 2.","title":"plotnine绘图 | python的ggplot2语法绘图包"},{"content":"一、文本的作用 文本涉及两个主体，即文本生产者和文本消费者：\n 文本生产者: 生成文本的主体；传递生产者想表达的内容，可能也会潜在蕴含着生产者的一些特质属性 文本消费者: 阅读文本的主体；消费者阅读这段文本时，文本又对消费者认知活动产生影响。  在大数据的今天，通过互联网超文本链接，无数的个人、团体、公司、政府等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。社会、管理、经济、营销、金融等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是从政治学和经管领域的两份文档翻译来，我觉得讲的挺明白的，其中加入了我的一些理解和扩充。\n二、 理解文本  text as text 原始的文本，定性的文本 textual data(text as data) 量化后的文本数据，可定量  2.1 text as text  text as text 原始的文本，定性的文本\n 文本的重点是传递着某种东西，从某种意义上说，所有形式的文本都包含可以被视为数据形式的信息。因此，文本总是以某种方式提供信息（即使我们不了解如何操作）。但是，言语活动的主要目标不是记录信息，而是进行交流：传达思想，指令，查询等。我们可以记录下来并将其视为数据，但是将我们的想法或思想表达为单词和句子的目的主要是交流，而不是将我们的想法或思想记录为数据形式。大多数数据是这样的：它表征的活动与数据本身完全不同。\n例如，在经济学中，可能是我们想要刻画的经济交易（使用价值媒介交换商品或服务），而数据是以某种聚合形式对这些交易进行抽象，这有助于我们理解交易的意义。通过就抽象的相关特征达成共识，我们可以记录并分析人类活动，例如制造业，服务业或农业。从通信行为中提取文本数据特征的过程遵循相同的过程，但有一个主要区别：由于原始文本可以直接通过记录的语言与我们交谈，因此文本首先不需要进行处理或抽象化待分析。但是，我在这里的论点是，特征抽象的过程是将文本视为数据而不是直接将其视为文本的方法的独特之处。\n具有讽刺意味的是，只有当我们破坏了直接理解文本的能力时，才有可能利用文本的数据获取洞察力。为了使它作为数据有用，我们必须消除原始文本的结构，将文本转换为结构化的表格数据。定量分析是理解非语言数据的起点；另一方面，非结构的文本变成丑陋表格数据的过程，出于统计分析或机器学习目的，我们经常质疑这一过程丢失了什么信息。\n机器是愚蠢的，但是将文本视为数据意味着让愚蠢的机器处理并可能分析我们的文本。关键是，为了将文本作为数据 而不是文本仅仅是文本，我们必须破坏原始文本的直接可解释性，但目的是从其样式化特征中进行更系统，更大规模的推断。我们应该坚定不移地认识到这一过程，但也不要因此而寝食不安，因为将文本作为数据进行分析的重点永远不是解释数据而是挖掘其深层次的模式。数据挖掘是一个破坏性的过程-随便问问哪个矿山-为了开采其宝贵资源，开采矿产资源不可避免会破坏地表形态和环境。\n2.2 Latent versus manifest characteristics from textual data  textual data(text as data) 量化后的文本数据，可定量的数据。所以小标题我翻译为“量化后的文本数据隐藏的信息vs直观可见的信息”，\n 在政治学领域，我们通常最感兴趣的不是文本本身，而是文本透漏给我们有关作者的一些隐藏特性。在政治（以及心理学）研究中，我们有关政治和社会行为者的一些重要理论，很多时候直接观察行为活动很难观察到其内在的品质。\n例如，意识形态是研究政治竞争和政治偏好的基础，但是我们没有直接的衡量工具来记录个人或政党有关“社会和道德自由政策与保守政策”的相对偏好。其他偏好，包括支持或反对特定政策，如1846年废除了英国的《玉米法》（Schonhardt-Bailey，2003年）；在关于《莱肯公约》的辩论中支持或反对进一步的欧洲一体化（Benoit等，2005）；再比如支持或反对不信任运动（Laver和Benoit，2002年）。\n这些偏好是作为政治行为者的内部状态而存在的，无论这些行为者是立法者，政党，代表还是候选人，都无法直接观察。非言语行为指标也可用于推断这些信息，但事实表明，政治行为者所说的话比其他行为形式更为真诚。\n因此，文本数据（Textual data）可能包含有关取向和信念的重要信息，对于这些取向和信念，非语言形式的行为可能会充当不良指标。长期以来，心理学领域也一直将言语行为作为可观察到的潜在兴趣状态的暗示，例如人格特质（例如Tausczik和Pennebaker，2010年）。缺少增强的询问技术或头脑阅读技术来识别政治和社会行为者的偏好，信念，意图，偏见或个性，下一个最佳选择是根据其说话或书写的内容来收集和分析数据。关注的对象不是文本包含的内容，而是其内容作为有关潜在特征的数据所揭示的内容，这些潜在特征为其提供了可观察的含义。最后一句话比较难理解，可以理解为万事万物有联系，通过联系思维来挖掘文本中的信息。\n文本数据(Textual data)还可能具有较为明显的特征，例如，政治传播的许多领域都与文本所指出的潜在特征无关，而与文本本身所包含的传播形式和性质有关。举一个经典的例子，在一个著名的政治局委员对斯大林诞辰70周年之际的文章的研究中，莱特斯，伯努特和加索夫（1951）能够衡量各团体在共产主义意识形态方面的差异。在这一政治事件中，这些信息不仅预示了潜在的方向，而且还预示了在可预见的斯大林死后事件中有关领导权斗争的某种政治动作。这些信息本身是重要的，这些信息只能从每个政治局委员撰写的公开文章中搜集而来，它们必须充分了解将在党和苏联苏维埃新闻，并由其他政权参与者解释为信号。再举一个例子，如果我们对一个政治演说家是使用民粹主义还是种族主义语言感兴趣，那么该语言将直接以民粹主义或种族主义术语或参考形式出现在文本中，而要紧的是它们是否被使用。与其说这些术语代表什么，不如说是什么。例如Jagers和Walgrave（2007）在研究比利时政党的政党政治广播时，发现极右翼政党Vlaams Blok所使用的民粹词语远比其他比利时政党丰富的多。\n在实践中，从文本可观察到的明显特征与潜在特征之间的特征的有时候这两个概念区分的并不明显。举例来说，文体风格可以用一些明显的特征词对文本进行量化，体现出作者的一些写作偏好。例如，在使用适用于政治文本的可读性度量改编的研究中，我们可能会对政治成熟度的潜在水平感兴趣，这可以用来衡量说话者的意图或说话者的特征，这一点从观察到的文本样本中可以看出。或者，我们可能会对它们在可读性上的明显差异感兴趣，这是传播媒介更直接指标。例如，在对英国议会历史演讲的研究中，Spirling（2016）将19世纪末期向简单语言的转变归因于广播扩展特许经营的民主化效应。Benoit，Munger和Spirling（2019）使用类似的措施，比较了同一位总统当天在同一天发表的美国总统国情咨文演讲的样本，但其口头和书面形式均表明口头形式使用的语言较为简单。前一项研究可能对语言的易用性感兴趣，该语言的易用性是政治代表制更潜在的特征的指标，而后一项分析可能更侧重于交付媒介的明显后果。对于许多使用文本数据的研究设计而言，区别更多是研究目标的问题，而不是结构化和分析文本数据的某些内在方式。\n2.3 文本分析的步骤   完整的文本分析步骤包括:\n 读取数据 分词(中文必须有这一步，由于英文是空格间隔的语言，英文有时候不需要分词） 剔除符号和无意义的停止词 字母变小写，词干化 使用一定的编码方式构建文档词频矩阵     序号 步骤 解释 中文 英文     1 读取数据 数据一般存储于不同的文件夹不同文件内，需要将其导入到计算机     2 分词 导入到计算的文本是字符串数据，需要整理为更好用的列表 例如“我爱你中国”分词后\n得到[\u0026ldquo;我\u0026rdquo;, \u0026ldquo;爱\u0026rdquo;, \u0026ldquo;你\u0026rdquo;, \u0026ldquo;中国\u0026rdquo;] \u0026ldquo;I love China\u0026quot;分为\n[\u0026ldquo;I\u0026rdquo;, \u0026ldquo;love\u0026rdquo;, \u0026ldquo;China\u0026rdquo;]   3 剔除符号和无意义的停止词 为了降低计算机运行时间，对分析结果影响较小的字符，诸如符号和无意义的词语需要剔除掉 如“的”，“她”， ”呢”， “了” \u0026ldquo;is\u0026rdquo; , \u0026ldquo;a\u0026rdquo;, \u0026ldquo;the\u0026rdquo;   4 字母变小写，词干化 同义词归并，同主体词归并 “中铁”，“中国铁建”，“中铁集团”都可以归并为“中铁” 先变为小写，这样“I”和“i”都归并为“i”；\n“was”，“are”，“is”都归并为“be”   5 构建文档词频矩阵 使用一定的编码方式，即用某种方式表示文本。常见的有词袋法、tf-idf；\n可以使用scikit-learn构建文档词频矩阵，但中英文略有区别，需要注意 “我爱你中国”需要先整理为“我 爱 你 中国” “I love China”    三、常见的文本分析技术有  主题分析(Thematic analysis) 内容分析(content analysis) 基于词典的方法(dictionary analysis) 文本向量化(Bag-of-words) 监督学习如SVM、Bayes和Regression 无监督学习，如LDA话题模型 自然语言处理  上述文本分析技术，按照人与机器参与程度，绘制在下图。一般来说，越向右，文本分析技术的自动化程度越高，需要注意的是自动化越高，并不代表人的工作量就越少。\n   3.1 主题分析Thematic Analysis 主题分析(Thematic analysis)是一种专家方法，一般与扎根理论方法相结合(Baumer, Mimno, Guha, Quan, \u0026amp; Gay, 2017)。扎根理论与主题分析的理念是基于专家自身经验和对世界的理解，做出对数据的见解，从而构建新理论。主题分析常见于组织科学和传播学(Gioia, Corley, \u0026amp; Hamilton, 2013; Strauss \u0026amp; Corbin, 1998)。\n主题分析涉及一个反复迭代的过程，在此过程中，研究人员将开发出一系列源自文本的代码和类别。除非要精炼理论，否则一般在分析开始之前尚不知道类别。在这种情况下，数据分析需要对文献和数据进行不断的比较。\n 研究人员从参与者自己的语言开始（称为“一阶编码”或“开放式编码”；Gioia等人，2013；Strauss＆Corbin，1998） 然后将相似的代码归为一类（称为“二阶代码”或“主轴编码”；Strauss＆Corbin，1998）。  诸如NVivo和ATLAS.ti之类的计算机软件可以帮助简化上述过程，但文本的分类通常依赖于人类编码衍生的类别的操作定义，计算机自动化的程度依旧很低，分析的数据量通常不大。而且编码过程对编码者的要求严格，通常是对该领域有较深理解的人才适合做此类工作。\n3.2 内容分析/基于词典的方法法 内容分析 和 其他基于字典的方法 通常是通过对特定文本中 单词/词组 的频率计数进行的（Reinard，2008；Short，Broberg，Cogliser＆Brigham，2010）。因为按照这种方法，文本数据被压缩成词组频数，定性的文本数据转化为定量的频数，索引可用于回答更多以定量为导向的研究问题（McKenny等，2016；Reinard，2008）。\n比如进行文本情感分析，我们可以用很简单的思路。即统计文本中正面词出现的总数和负面词出现的总数，得出文本的情感值。而在此分析过程中，我们需要事先拥有一个正面词词典和负面词词典。\n是否有成熟的领域词典、或者构建领域词典，这需要研究者对研究问题和研究的数据有一定的领域知识，工作量也会因是否有词典而不同。一般有现成的成熟的词典，计算机自动化程度高，人工工作量低。\n与主题分析类似，计算机软件可以协助内容分析过程。像DICTION这样的程序会使用 分类字典 自动对文本评分（即，根据单词或n-gram而非操作定义确定主题）。可以与主题分析类似地使用其他程序，例如NVivo或ATLAS.ti，在主题分析中，通过软件的帮助手动进行编码和分类，以组织数据。\n3.3 词袋法Bag-of-words   文本数据是非结构化的定性数据，计算机并不能直接使用。我们需要按照计算机容易理解的方式去组织数据，类似于上图的第一步骤,四段英文文本被组织成一个文档特征矩阵（document-feature-matrix），矩阵中\n 每一行代表一个英文文档 每一个列代表一个特征词  3.3.1 词袋法 vs 主题分析中的编码者 为了理解词袋法，可以类比主题分析 中的编码者。我们可以将词袋法看做是一个死板的，不知变通的人，脑子很简单，只知道统计特征词在每个文档中出现的词频。那么据此我们就知道词袋法和人的优缺点。\n对于词袋法，优点是规则标准统一，缺点是不知变通，牺牲了文本中很多的信息量。强调编码过程的高标准，牺牲了分析的深度。\n对于研究者参与 主题分析 这样的编码过程，优点是研究者有很强的领域知识和强大的洞察力，可以灵活洞察规律，缺点是每个研究者都具有特殊的经历和偏好，编码标准不统一。用研究者编码的过程，强调编码的深度和质量，牺牲了编码分析过程的标准性。\n3.3.2 词袋法的用途 词袋法编码是计算科学领域对文本数据的简化和压缩的方法，后续可以据此进行监督学习和无监督学习。\n3.4 监督学习 在有监督的方法中，研究人员事先知道ta正在寻找什么（罗伯茨等，2014）。比如要判断论文的作者身份这个问题，研究人员为程序提供输入（在这种情况下为文本）和输出（例如，文本作者的身份），然后系统创建一种算法来映射两者之间的联系（Janasik， Honkela和Bruun，2009年）。Mosteller and Wallace（1963）通过使用简单的贝叶斯单词概率来预测12篇有争议的联邦主义者论文（詹姆斯·麦迪逊或亚历山大·汉密尔顿）的作者身份。如今，朴素贝叶斯（Bayes）和支持向量机（SVM）等技术是用于文本分析的流行的监督算法（Manning，Prabhakar和Hinrich，2008年）。\n3.5 无监督学习 无监督算法，如主题分析（Janasik等，2009）可识别数据中的单词簇和主题。但是，与主题分析不同，主题建模使用高度自动化的方法来确定重要主题，分析过程所需的时间和领域知识相对较少。尽管人类的洞察力仍然对帮助解释出现的主题很重要，主题建模适合分析大规模文本数据（Kobayashi1，Mol，Berkers，Kismihok和Den Hartog，2017）。主题建模利用了主题分析（即人类洞察力、解释力）和机器学习（即快速分析大量文本）的优势。\n3.6 自然语言处理 最后，自然语言处理(Natural Language Processing)通常是文本分析中自动化程度最高的形式（有关综述，请参阅Manning等人，2008）。这种方法模拟了人类如何理解和处理语言（Chowdhury，2003；Collobert等，2011；Joshi，1991）。例如，NLP技术可以标记句子中单词的词性（例如，名词，形容词等），将文档从一种语言翻译成另一种语言，甚至使用句子的上下文来阐明词语的词义（Buntine＆Jakulin，2004年）。\n因此，与词袋法不同，NLP认为单词顺序很重要。当使用训练集时，使用深度学习和多模式（即结合文本和图像）等尖端技术进行情感分析是NLP的一种流行形式（Kouloumpis，Wilson和Moore，2011）。这种特殊的分析将文本的总体态度，情感或观点分类为肯定，否定或中立。\n与主题分析形成鲜明对比的是，自然语言处理是一个完全计算机自动化的过程，因此几乎不需要人类的理解和或解释（Quinn等人，2010）。此外，相对于需要人工编码（例如，主题分析）的技术，NLP的执行速度非常快，并且比其他方法更具系统性。例如，计算机科学，信息科学，语言学和心理学的研究人员利用NLP作为文本分析工具（Chowdhury，2003年）。\n大邓提醒一下，自然语言处理属于人工智能范畴，人工智能技术没有那么神，我们应该将其理解为“人工”+“智能”可能更妥当一些，即数据准备阶段用大量的人工时对数据进行标注，产生训练数据集合。之后借助于计算机的“智能”学习数据集中的规律，因此人工智能脱离了人工标注数据的喂养，只能做很简单的事情，更像是人工智障。\n3.7 不同文本分析技术汇总对比     技术 描述 优点 缺点 常被应用(领域) 软件     主题分析Thematic analysis 需要有经验的人员基于自身经验和李俊杰，对研究的数据进行挖掘。编码过程为迭代进行 使用参与者自己的话语或者构念来挖掘数据，对少量文本理解的更深入 属于时间、劳动密集型任务，不适合大规模数据。\n由于不同的编码人员有不同的经历和偏好，编码过程的标准不可靠 社会学、管理学 Nvivo；   内容分析/基于字典方法 统计文本中词语/词组的出现频率 允许对研究的数据进行定量分析 采用的词典应尽量与研究问题适应，词典适配性问题突出 管理学 LIWC、Nvivo、DICTION；   词袋法（Bag of words） 将文本字符串转为计算机能理解的数字化向量 编码标准稳定简单，具有统计学特性，扩展性强 编码过程忽略词语的先后顺序 管理学 Python的scikit-learn、gensim、nltk等；R   监督学习(Supervise models),如SVM、Bayes、Logistic Regression 研究者要知道输入数据X和标签y；需要核实的模型需要X和y之间的关系和规律 允许事先定义编码规则(如选择词袋法还是tfidf)；逻辑简单 需要高质量的标注数据(工作量大)；you与特征词太多，训练的模型很容易过拟合。 计算机学、政治学、管理学 Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）   无监督学习(Kmeans、 LDA话题模型) 使用聚类、话题分析，让计算机自动对数据进行分组 在没有人工标注的情况下，加速了数据的“标注”或“分类” “标注”是机器按照数字特征进行的分组，需要研究者解读才可以赋予“标准“意义；训练过程需要大量的调参 计算机学、政治学、管留学 Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）   自然语言处理 按照人类对语言的理解进行建模，考虑词语顺序 计算机自动化；可分析语义 大多数模型是人类无法解读的黑箱；\n虽然代码编程量小，但训练代码耗时巨大 计算科学；市场营销；心理学 pytorch、tensorflow    3.8 Python能做哪些？ 计算机能做的文本分析，Python都能做到，包括\n 基于词典的分析法；如基于词典法的情感计算 词袋法；可以进行文本相似度计算 有监督机器学习；如基于机器学习的情感分析；文本分类 无监督机器学习；lda话题模型对文本进行话题分析 自然语言处理；考虑词语顺序的LSTM  除了自然语言处理部分，四种方法在我的《Python网络爬虫与文本数据分析》视频课程中都有相关的讲解和实战代码\n相关文献  [1]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. \u0026ldquo;Uniting the tribes: Using text for marketing insight.\u0026rdquo; Journal of Marketing (2019): 0022242919873106.\n  [2]Kenneth Benoit. July 16, 2019. “[Text as Data: An Overview](https://kenbenoit.net/pdfs/28 Benoit Text as Data draft 2.pdf).” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. Handbook of Research Methods in Political Science and International Relations. Thousand Oaks: Sage.\n  [3]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. \u0026ldquo;A review of best practice recommendations for text analysis in R (and a user-friendly app).\u0026rdquo; Journal of Business and Psychology 33, no. 4 (2018): 445-459.\n ","permalink":"/blog/%E8%AF%BB%E5%AE%8C%E6%9C%AC%E6%96%87%E4%BD%A0%E5%B0%B1%E4%BA%86%E8%A7%A3%E4%BB%80%E4%B9%88%E6%98%AF%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/","summary":"一、文本的作用 文本涉及两个主体，即文本生产者和文本消费者：\n 文本生产者: 生成文本的主体；传递生产者想表达的内容，可能也会潜在蕴含着生产者的一些特质属性 文本消费者: 阅读文本的主体；消费者阅读这段文本时，文本又对消费者认知活动产生影响。  在大数据的今天，通过互联网超文本链接，无数的个人、团体、公司、政府等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。社会、管理、经济、营销、金融等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是从政治学和经管领域的两份文档翻译来，我觉得讲的挺明白的，其中加入了我的一些理解和扩充。\n二、 理解文本  text as text 原始的文本，定性的文本 textual data(text as data) 量化后的文本数据，可定量  2.1 text as text  text as text 原始的文本，定性的文本\n 文本的重点是传递着某种东西，从某种意义上说，所有形式的文本都包含可以被视为数据形式的信息。因此，文本总是以某种方式提供信息（即使我们不了解如何操作）。但是，言语活动的主要目标不是记录信息，而是进行交流：传达思想，指令，查询等。我们可以记录下来并将其视为数据，但是将我们的想法或思想表达为单词和句子的目的主要是交流，而不是将我们的想法或思想记录为数据形式。大多数数据是这样的：它表征的活动与数据本身完全不同。\n例如，在经济学中，可能是我们想要刻画的经济交易（使用价值媒介交换商品或服务），而数据是以某种聚合形式对这些交易进行抽象，这有助于我们理解交易的意义。通过就抽象的相关特征达成共识，我们可以记录并分析人类活动，例如制造业，服务业或农业。从通信行为中提取文本数据特征的过程遵循相同的过程，但有一个主要区别：由于原始文本可以直接通过记录的语言与我们交谈，因此文本首先不需要进行处理或抽象化待分析。但是，我在这里的论点是，特征抽象的过程是将文本视为数据而不是直接将其视为文本的方法的独特之处。\n具有讽刺意味的是，只有当我们破坏了直接理解文本的能力时，才有可能利用文本的数据获取洞察力。为了使它作为数据有用，我们必须消除原始文本的结构，将文本转换为结构化的表格数据。定量分析是理解非语言数据的起点；另一方面，非结构的文本变成丑陋表格数据的过程，出于统计分析或机器学习目的，我们经常质疑这一过程丢失了什么信息。\n机器是愚蠢的，但是将文本视为数据意味着让愚蠢的机器处理并可能分析我们的文本。关键是，为了将文本作为数据 而不是文本仅仅是文本，我们必须破坏原始文本的直接可解释性，但目的是从其样式化特征中进行更系统，更大规模的推断。我们应该坚定不移地认识到这一过程，但也不要因此而寝食不安，因为将文本作为数据进行分析的重点永远不是解释数据而是挖掘其深层次的模式。数据挖掘是一个破坏性的过程-随便问问哪个矿山-为了开采其宝贵资源，开采矿产资源不可避免会破坏地表形态和环境。\n2.2 Latent versus manifest characteristics from textual data  textual data(text as data) 量化后的文本数据，可定量的数据。所以小标题我翻译为“量化后的文本数据隐藏的信息vs直观可见的信息”，\n 在政治学领域，我们通常最感兴趣的不是文本本身，而是文本透漏给我们有关作者的一些隐藏特性。在政治（以及心理学）研究中，我们有关政治和社会行为者的一些重要理论，很多时候直接观察行为活动很难观察到其内在的品质。\n例如，意识形态是研究政治竞争和政治偏好的基础，但是我们没有直接的衡量工具来记录个人或政党有关“社会和道德自由政策与保守政策”的相对偏好。其他偏好，包括支持或反对特定政策，如1846年废除了英国的《玉米法》（Schonhardt-Bailey，2003年）；在关于《莱肯公约》的辩论中支持或反对进一步的欧洲一体化（Benoit等，2005）；再比如支持或反对不信任运动（Laver和Benoit，2002年）。\n这些偏好是作为政治行为者的内部状态而存在的，无论这些行为者是立法者，政党，代表还是候选人，都无法直接观察。非言语行为指标也可用于推断这些信息，但事实表明，政治行为者所说的话比其他行为形式更为真诚。\n因此，文本数据（Textual data）可能包含有关取向和信念的重要信息，对于这些取向和信念，非语言形式的行为可能会充当不良指标。长期以来，心理学领域也一直将言语行为作为可观察到的潜在兴趣状态的暗示，例如人格特质（例如Tausczik和Pennebaker，2010年）。缺少增强的询问技术或头脑阅读技术来识别政治和社会行为者的偏好，信念，意图，偏见或个性，下一个最佳选择是根据其说话或书写的内容来收集和分析数据。关注的对象不是文本包含的内容，而是其内容作为有关潜在特征的数据所揭示的内容，这些潜在特征为其提供了可观察的含义。最后一句话比较难理解，可以理解为万事万物有联系，通过联系思维来挖掘文本中的信息。\n文本数据(Textual data)还可能具有较为明显的特征，例如，政治传播的许多领域都与文本所指出的潜在特征无关，而与文本本身所包含的传播形式和性质有关。举一个经典的例子，在一个著名的政治局委员对斯大林诞辰70周年之际的文章的研究中，莱特斯，伯努特和加索夫（1951）能够衡量各团体在共产主义意识形态方面的差异。在这一政治事件中，这些信息不仅预示了潜在的方向，而且还预示了在可预见的斯大林死后事件中有关领导权斗争的某种政治动作。这些信息本身是重要的，这些信息只能从每个政治局委员撰写的公开文章中搜集而来，它们必须充分了解将在党和苏联苏维埃新闻，并由其他政权参与者解释为信号。再举一个例子，如果我们对一个政治演说家是使用民粹主义还是种族主义语言感兴趣，那么该语言将直接以民粹主义或种族主义术语或参考形式出现在文本中，而要紧的是它们是否被使用。与其说这些术语代表什么，不如说是什么。例如Jagers和Walgrave（2007）在研究比利时政党的政党政治广播时，发现极右翼政党Vlaams Blok所使用的民粹词语远比其他比利时政党丰富的多。\n在实践中，从文本可观察到的明显特征与潜在特征之间的特征的有时候这两个概念区分的并不明显。举例来说，文体风格可以用一些明显的特征词对文本进行量化，体现出作者的一些写作偏好。例如，在使用适用于政治文本的可读性度量改编的研究中，我们可能会对政治成熟度的潜在水平感兴趣，这可以用来衡量说话者的意图或说话者的特征，这一点从观察到的文本样本中可以看出。或者，我们可能会对它们在可读性上的明显差异感兴趣，这是传播媒介更直接指标。例如，在对英国议会历史演讲的研究中，Spirling（2016）将19世纪末期向简单语言的转变归因于广播扩展特许经营的民主化效应。Benoit，Munger和Spirling（2019）使用类似的措施，比较了同一位总统当天在同一天发表的美国总统国情咨文演讲的样本，但其口头和书面形式均表明口头形式使用的语言较为简单。前一项研究可能对语言的易用性感兴趣，该语言的易用性是政治代表制更潜在的特征的指标，而后一项分析可能更侧重于交付媒介的明显后果。对于许多使用文本数据的研究设计而言，区别更多是研究目标的问题，而不是结构化和分析文本数据的某些内在方式。\n2.3 文本分析的步骤   完整的文本分析步骤包括:\n 读取数据 分词(中文必须有这一步，由于英文是空格间隔的语言，英文有时候不需要分词） 剔除符号和无意义的停止词 字母变小写，词干化 使用一定的编码方式构建文档词频矩阵     序号 步骤 解释 中文 英文     1 读取数据 数据一般存储于不同的文件夹不同文件内，需要将其导入到计算机     2 分词 导入到计算的文本是字符串数据，需要整理为更好用的列表 例如“我爱你中国”分词后","title":"读完本文你就了解什么是文本分析"},{"content":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/数据分析plydata库学习.zip\nplydata是一个提供数据处理语法的Python库，借鉴了R语言dplyr，tidyr和forcats等包中的管道操作符。\nplydata使用 \u0026gt;\u0026gt;运算符 作为管道符号，或者使用ply（data，* verbs）函数代替 \u0026gt;\u0026gt;， 目前仅支持对pandas.DataFrame数据进行操作。\n安装 !pip3 install plydata 快速上手 import pandas as pd from plydata import define, query, if_else, ply df = pd.DataFrame({ \u0026#39;x\u0026#39;: [0, 1, 2, 3], \u0026#39;y\u0026#39;: [\u0026#39;zero\u0026#39;, \u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]}) df     x y     0 0 zero   1 1 one   2 2 two   3 3 three    define define函数名很简单，定义变量或者操作变量。\ndefine(data, *args,**kwargs)\n data 待操作的dataframe数据 args、kwargs 操作过程及结果。  比如我们想在df中新建一个z列，z列的值完全复制自x列。可以使用代码 define(df, z=\u0026lsquo;x\u0026rsquo;)\ndefine(df, z=\u0026#39;x\u0026#39;)     x y z     0 0 zero 0   1 1 one 1   2 2 two 2   3 3 three 3    注意: df中有x列，所以这里的使用的\u0026rsquo;x',而不是x。\n\u0026raquo;管道符 刚刚的问题可以使用管道符实现与define(df, z=\u0026lsquo;x\u0026rsquo;)相同的功能。\n#等同于df[\u0026#39;z\u0026#39;]=df[\u0026#39;x\u0026#39;] #等同于define(df, z=\u0026#39;x\u0026#39;) df \u0026gt;\u0026gt; define(z=\u0026#39;x\u0026#39;)     x y z     0 0 zero 0   1 1 one 1   2 2 two 2   3 3 three 3    如果有多个环节，可以用括号包裹住，环节与环节用\u0026gt;\u0026gt;和换行前后衔接。\n比如我们有多个操作，每一步操作如下\n m=2x n=m*m q=m+n  (df \u0026gt;\u0026gt; define(m=\u0026#39;2*x\u0026#39;) \u0026gt;\u0026gt; define(n=\u0026#39;m*m\u0026#39;) \u0026gt;\u0026gt; define(q=\u0026#39;m+n\u0026#39;) )     x y m n q     0 0 zero 0 0 0   1 1 one 2 4 6   2 2 two 4 16 20   3 3 three 6 36 42    上面所有的plydata相关操作不会修改原始数据df\ndf     x y     0 0 zero   1 1 one   2 2 two   3 3 three    if_else 在df中新建z列，z的值满足\n 当x大于1，z为1 当x小于等于1， z为0  使用if_else(predicate, true_value, false_value)\n predicate 逻辑判断条件字符串 true_value 满足逻辑条件返回的值 false_value 不满足逻辑条件返回的值  #等同于define(df, z=if_else(\u0026#39;x\u0026gt;1\u0026#39;, 1, 0)) df \u0026gt;\u0026gt; define(z=if_else(\u0026#39;x\u0026gt;1\u0026#39;, 1, 0))     x y z     0 0 zero 0   1 1 one 0   2 2 two 1   3 3 three 1    query query(data, expr)\n data 待查询的dataframe数据 expr 查询条件字符串  (df \u0026gt;\u0026gt; define(z=if_else(\u0026#39;x\u0026gt;1\u0026#39;, 1, 0)) \u0026gt;\u0026gt; query(\u0026#39;z==1\u0026#39;) )     x y z     2 2 two 1   3 3 three 1    ply() ply功能等同于管道符\u0026raquo;， 刚刚上面的代码\n(df \u0026gt;\u0026gt; define(z=if_else(\u0026#39;x\u0026gt;1\u0026#39;, 1, 0)) \u0026gt;\u0026gt; query(\u0026#39;z==1\u0026#39;) ) 可以用ply\nply(df, define(z=if_else(\u0026#39;x \u0026gt; 1\u0026#39;, 1, 0)), query(\u0026#39;z == 1\u0026#39;) )     x y z     2 2 two 1   3 3 three 1    plydata与plotnine 在R语言中，用ggplot2作图经常会用到管道符。而在Python中，plydata提供管道符，可以与作图库plotnine结合使用。\nfrom plotnine import ggplot, geom_line, aes from plydata import define, if_else import numpy as np df = pd.DataFrame({\u0026#39;x\u0026#39;: np.linspace(0, 2*np.pi, 500)}) (df \u0026gt;\u0026gt; define(y=\u0026#39;np.sin(x)\u0026#39;) \u0026gt;\u0026gt; define(sign=if_else(\u0026#39;y\u0026gt;=0\u0026#39;, \u0026#39;\u0026#34;pos\u0026#34;\u0026#39;, \u0026#39;\u0026#34;neg\u0026#34;\u0026#39;)) \u0026gt;\u0026gt; (ggplot(aes(x=\u0026#39;x\u0026#39;, y=\u0026#39;y\u0026#39;, color=\u0026#39;sign\u0026#39;))+ geom_line(size=1.5)) )   ","permalink":"/blog/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90plydata%E5%BA%93/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/数据分析plydata库学习.zip\nplydata是一个提供数据处理语法的Python库，借鉴了R语言dplyr，tidyr和forcats等包中的管道操作符。\nplydata使用 \u0026gt;\u0026gt;运算符 作为管道符号，或者使用ply（data，* verbs）函数代替 \u0026gt;\u0026gt;， 目前仅支持对pandas.DataFrame数据进行操作。\n安装 !pip3 install plydata 快速上手 import pandas as pd from plydata import define, query, if_else, ply df = pd.DataFrame({ \u0026#39;x\u0026#39;: [0, 1, 2, 3], \u0026#39;y\u0026#39;: [\u0026#39;zero\u0026#39;, \u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]}) df     x y     0 0 zero   1 1 one   2 2 two   3 3 three    define define函数名很简单，定义变量或者操作变量。","title":"数据分析plydata库"},{"content":"70G年报pdf数据集   数据下载说明 所有pdf均来自上海证券交易所官网，使用shreport库进行的下载。\n  报告信息汇总文件    summary.xlsx内字段\n company 上市公司企业名 code 股票代码 type 报告类型 year 报告年份 date 报告发布日期 pdf 报告pdf文件下载链接  import pandas as pd from pathlib import Path #报告汇总文件summary.xlsx df = pd.read_excel(\u0026#39;summary.xlsx\u0026#39;) df.head()   一共有报告71126份\nlen(df) 71149 一共有上市公司1486家\nlen(df[\u0026#39;company\u0026#39;].unique()) 1486 summary文件夹 summary文件夹内是每家公司的报告披露情况\n  df1 = pd.read_excel(\u0026#39;summary/600000.xlsx\u0026#39;) df1.head()   浦发银行一共有75份定期报告\nlen(df1) 75 reports文件夹 reports文件夹存放着以各各公司股票代码命名的文件夹\n文件夹内是该公司所有定期报告\n  读取pdf报告 可使用pdfdocx库读取pdf,\npdfdocx文档链接 https://github.com/thunderhit/pdfdocx\nfrom pdfdocx import read_pdf p_text = read_pdf(\u0026#39;reports/600000/600000_2012_1.pdf\u0026#39;) p_text Run\n上海浦东发展银行股份有限公司 \\n\\n2012 年第一季度报告 \\n\\n \\n\\n \\n\\n§1 重要提示 \\n\\n1.1 公司董事会、监事会及其董事、监事、高级管理人员保证本报告所载资料不存在任何虚假记载、\\n\\n误导性陈述或者重大遗漏，并对其内容的真实性、准确性和完整性承担个别及连带责任。\\n\\n1.2 公司于 2012 年 4 月 26 日以通讯表决的方式召开第四届董事会第二十六次会议审议通过本报告，\\n\\n1.4 公司董事长、行长吉晓辉、财务总监刘信义及财务机构负责人傅能声明：保证本季度报告中财务\\n\\n公司全体董事出席董事会会议并行使表决权。\\n\\n1.3 公司第一季度财务报告未经审计。\\n\\n报告的真实、完整。\\n\\n \\n§2 公司基本情况 \\n\\n2.1 主要会计数据及财务指标 \\n\\n本报告期末 \\n\\n上年度期末 \\n\\n币种:人民币 \\n\\n本报告期末比上年\\n度期末增减(%) \\n\\n总资产(千元) \\n\\n归属于上市公司股东的所有者权益(千元) \\n\\n2,804,646,567\\n\\n157,055,724\\n\\n2,684,693,689 \\n148,891,235 \\n\\n归属于上市公司股东的每股净资产(元) \\n\\n8.420\\n\\n7.982 \\n\\n4.47 \\n5.48 \\n5.49 \\n\\n经营活动产生的现金流量净额(千元) \\n\\n每股经营活动产生的现金流\\n\\n \\n\\n \\n \\n母公司现金流量表 \\n \\n2012 年 1—3 月 \\n \\n编制单位: 上海浦东发展银行股份有限公司.... 70G数据下载 链接:https://pan.baidu.com/s/14PI6MbxunFQ3fZOfR33zkw 密码:osoi\n","permalink":"/blog/70g%E5%B9%B4%E6%8A%A5pdf%E6%95%B0%E6%8D%AE%E9%9B%86/","summary":"70G年报pdf数据集   数据下载说明 所有pdf均来自上海证券交易所官网，使用shreport库进行的下载。\n  报告信息汇总文件    summary.xlsx内字段\n company 上市公司企业名 code 股票代码 type 报告类型 year 报告年份 date 报告发布日期 pdf 报告pdf文件下载链接  import pandas as pd from pathlib import Path #报告汇总文件summary.xlsx df = pd.read_excel(\u0026#39;summary.xlsx\u0026#39;) df.head()   一共有报告71126份\nlen(df) 71149 一共有上市公司1486家\nlen(df[\u0026#39;company\u0026#39;].unique()) 1486 summary文件夹 summary文件夹内是每家公司的报告披露情况\n  df1 = pd.read_excel(\u0026#39;summary/600000.xlsx\u0026#39;) df1.head()   浦发银行一共有75份定期报告\nlen(df1) 75 reports文件夹 reports文件夹存放着以各各公司股票代码命名的文件夹\n文件夹内是该公司所有定期报告\n  读取pdf报告 可使用pdfdocx库读取pdf,\npdfdocx文档链接 https://github.com/thunderhit/pdfdocx\nfrom pdfdocx import read_pdf p_text = read_pdf(\u0026#39;reports/600000/600000_2012_1.","title":"70g年报pdf数据集"},{"content":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/在jupyter中调用R语言代码.zip\n在数据分析中，Python和R各有千秋，虽然Python或R都能在数据分析打通关，从采集、清洗(预处理)、分析、可视化，但是在不同的环节，不同的语言易用程度不同。Python胜在干脏活累活，诸如数据采集、数据清洗、机器学习等；而R语言胜在统计分析、可视化等。所以，如果你正好Python和R都入门了，可以考虑两者结合。那么就会遇到今天的问题-如何在jupyter中使用R语言代码？\nrpy2包 rpy2包首先是Python包，ta衔接了Python和R，通过rpy2可以运行R语言相关代码、函数、包。\n在Jupyter notebook中主要有两种情况\n 单元格中以Python为主，可以插入R的代码字符串 单个的单元格要么只有R代码，要么只有Python代码  遇到这类问题，各位的电脑要确保\n 电脑已经安装了Python和R 已安装rpy2包  安装rpy2包\n!pip3 install rpy2 import rpy2.robjects as robjects from rpy2.robjects import pandas2ri #R代码运行会尽量以DataFrame显示 pandas2ri.activate() #运行R代码 robjects.r(\u0026#39;R代码字符串\u0026#39;) 运行R代码 rpy2.robjects.r(\u0026#39;R代码字符串\u0026#39;) rpy2.robjects.r()函数会识别 R代码字符串, 并将其执行。\nimport rpy2.robjects as robjects from rpy2.robjects import pandas2ri import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) #直接声明，data frame强制转为DataFrame pandas2ri.activate() #R代码 r_code = \u0026#34;\u0026#34;\u0026#34; library(dplyr) text \u0026lt;- c(\u0026#34;Because I could not stop for Death -\u0026#34;, \u0026#34;He kindly stopped for me -\u0026#34;, \u0026#34;The Carriage held but just Ourselves -\u0026#34;, \u0026#34;and Immortality\u0026#34;) text_df \u0026lt;- tibble(docid=1:4, text=text) text_df \u0026#34;\u0026#34;\u0026#34; #运行R代码 robjects.r(r_code)     docid text     1 1 Because I could not stop for Death -   2 2 He kindly stopped for me -   3 3 The Carriage held but just Ourselves -   4 4 and Immortality     准备一个R代码r_code，该代码会生成R语言的tibble数据类型(R语言中的一种data frame)。 robjects.r(r_code) 运行R代码(字符串) 在本例中，使用pandas2ri.activate()强制声明，在Python中将变量text_df以pandas.DataFrame显示  调用R数据集 rpy2.robjects.r.data(\u0026#39;R的数据集名\u0026#39;) 调用R的数据集\nimport rpy2.robjects as robjects  robjects.r.data(\u0026lsquo;R数据集名\u0026rsquo;) 准备R数据集，此时Python并没有显示该数据集，可以理解为准备数据 robjects.r['R数据集名'] 导入R的数据集  import rpy2.robjects as robjects from rpy2.robjects import pandas2ri pandas2ri.activate() #准备iris robjects.r.data(\u0026#39;iris\u0026#39;) #导入iris iris = robjects.r[\u0026#39;iris\u0026#39;] iris.head()     Sepal.Length Sepal.Width Petal.Length Petal.Width Species     1 5.1 3.5 1.4 0.2 setosa   2 4.9 3 1.4 0.2 setosa   3 4.7 3.2 1.3 0.2 setosa   4 4.6 3.1 1.5 0.2 setosa   5 5 3.6 1.4 0.2 setosa    调用R语言包 rpy2.robjects.packages.importr(\u0026#39;R包名\u0026#39;) R语言中的readr包有read_csv()函数，可以读取csv文件。\nfrom rpy2.robjects.packages import importr from rpy2.robjects import pandas2ri pandas2ri.activate() #导入R语言中的readr包 readr = importr(\u0026#34;readr\u0026#34;) #使用readr包中的read_csv()函数 mtcars = readr.read_csv(\u0026#34;mtcars.csv\u0026#34;) mtcars.head()     car mpg cyl disp hp drat wt qsec vs am gear carb     1 Mazda RX4 21 6 160 110 3.9 2.62 16.46 0 1 4 4   2 Mazda RX4 Wag 21 6 160 110 3.9 2.875 17.02 0 1 4 4   3 Datsun 710 22.8 4 108 93 3.85 2.32 18.61 1 1 4 1   4 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1   5 Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.02 0 0 3 2    Cell只运行R代码 上面的几个章节中，每个cell中代码均为Python和R的混用，如果在Cell中只运行R代码，\n 可以先单独在一个cell中运行 %load_ext rpy2.ipython, 在另外一个cell中使用%%R声明本cell中使用的是R代码。  %load_ext rpy2.ipython %%R library(ggplot2) ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species))+ geom_point()   %%R -h 550 -w 800 #设置宽、高 library(ggplot2) ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species))+ geom_point()   ","permalink":"/blog/%E5%9C%A8jupyter%E4%B8%AD%E8%B0%83%E7%94%A8r%E8%AF%AD%E8%A8%80%E7%9A%84%E4%BB%A3%E7%A0%81/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/在jupyter中调用R语言代码.zip\n在数据分析中，Python和R各有千秋，虽然Python或R都能在数据分析打通关，从采集、清洗(预处理)、分析、可视化，但是在不同的环节，不同的语言易用程度不同。Python胜在干脏活累活，诸如数据采集、数据清洗、机器学习等；而R语言胜在统计分析、可视化等。所以，如果你正好Python和R都入门了，可以考虑两者结合。那么就会遇到今天的问题-如何在jupyter中使用R语言代码？\nrpy2包 rpy2包首先是Python包，ta衔接了Python和R，通过rpy2可以运行R语言相关代码、函数、包。\n在Jupyter notebook中主要有两种情况\n 单元格中以Python为主，可以插入R的代码字符串 单个的单元格要么只有R代码，要么只有Python代码  遇到这类问题，各位的电脑要确保\n 电脑已经安装了Python和R 已安装rpy2包  安装rpy2包\n!pip3 install rpy2 import rpy2.robjects as robjects from rpy2.robjects import pandas2ri #R代码运行会尽量以DataFrame显示 pandas2ri.activate() #运行R代码 robjects.r(\u0026#39;R代码字符串\u0026#39;) 运行R代码 rpy2.robjects.r(\u0026#39;R代码字符串\u0026#39;) rpy2.robjects.r()函数会识别 R代码字符串, 并将其执行。\nimport rpy2.robjects as robjects from rpy2.robjects import pandas2ri import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) #直接声明，data frame强制转为DataFrame pandas2ri.activate() #R代码 r_code = \u0026#34;\u0026#34;\u0026#34; library(dplyr) text \u0026lt;- c(\u0026#34;Because I could not stop for Death -\u0026#34;, \u0026#34;He kindly stopped for me -\u0026#34;, \u0026#34;The Carriage held but just Ourselves -\u0026#34;, \u0026#34;and Immortality\u0026#34;) text_df \u0026lt;- tibble(docid=1:4, text=text) text_df \u0026#34;\u0026#34;\u0026#34; #运行R代码 robjects.","title":"rpy2包 | 在Jupyter中调用R语言的代码"},{"content":"数据集下载 链接:https://pan.baidu.com/s/1rUfj7NYYAnM3NuFWCHyPpA 密码:ux8z\n在昨天发的文章里提到了yelp数据集，官网显示\u0026quot;Yelp数据集是我们用于个人，教育和学术目的的业务，评论和用户数据的子集。 以JSON文件形式提供，可用于在学习如何制作移动应用程序的同时，向学生介绍数据库，学习NLP或提供示例生产数据。\u0026quot;\nyelp官网显示，这个数据集亮点如下：\n 668+w条评论 19+w个商业机构 20w张图片 10个都市区域 字段包括:营业时间、是否泊车、可用性和环境    在kaggle上也能看到使用这个数据集的案例，说不定有你需要的分析方法。\n  我已经从yelp官网下载了数据, 参照kaggle的很多例子，咱们也在自己电脑上跑跑简单的分析\nbusiness数据读取 kaggle中的数据是csv文件，咱这里是json，略有不同，但读取都可以用pandas读取\nimport pandas as pd #一开始用注释掉的代码，有bug，经过百度找到lines=True解决方案 #business = pd.read_json(\u0026#39;yelp_dataset/business.json\u0026#39;) business = pd.read_json(\u0026#39;yelp_dataset/business.json\u0026#39;, lines=True) business.head()   评分分布 import seaborn as sns sns.color_palette() [(0.12156862745098039, 0.4666666666666667, 0.7058823529411765), (1.0, 0.4980392156862745, 0.054901960784313725), (0.17254901960784313, 0.6274509803921569, 0.17254901960784313), (0.8392156862745098, 0.15294117647058825, 0.1568627450980392), (0.5803921568627451, 0.403921568627451, 0.7411764705882353), (0.5490196078431373, 0.33725490196078434, 0.29411764705882354), (0.8901960784313725, 0.4666666666666667, 0.7607843137254902), (0.4980392156862745, 0.4980392156862745, 0.4980392156862745), (0.7372549019607844, 0.7411764705882353, 0.13333333333333333), (0.09019607843137255, 0.7450980392156863, 0.8117647058823529)]  import matplotlib.pyplot as plt import seaborn as sns colors = sns.color_palette() rating = business[\u0026#39;stars\u0026#39;].value_counts() rating.sort_index(inplace=True) rating.plot(kind=\u0026#39;bar\u0026#39;, figsize=(10, 5), color=colors[:9], rot=0) #字体倾斜角度 plt.title(\u0026#39;Rating Distribution of Yelp\u0026#39;,fontweight=\u0026#39;bold\u0026#39;) plt.show()   行业统计 查看行业店家数量分布\nimport numpy as np business[\u0026#39;categories\u0026#39;] = business[\u0026#39;categories\u0026#39;].apply(lambda x: x if x else \u0026#39; \u0026#39;) category_str = \u0026#39;,\u0026#39;.join(business[\u0026#39;categories\u0026#39;]) category_list = category_str.split(\u0026#39;,\u0026#39;) category_df = pd.DataFrame(category_list, columns=[\u0026#39;category\u0026#39;]) top15_category = category_df[\u0026#39;category\u0026#39;].value_counts()[:15] top15_category top15_category.plot(kind=\u0026#39;bar\u0026#39;, color=colors[:20], figsize=(20, 10), rot=30, fontsize=20) plt.title(\u0026#39;Top 20 Category in Yelp\u0026#39;, fontsize=25, fontweight=\u0026#39;bold\u0026#39;) plt.show()   城市分布 显示yelp中Top20城\ncitys = business[\u0026#39;city\u0026#39;].value_counts()[:20] citys.sort_values(ascending=True, inplace=True)#降序，原地修改原始数据 citys.plot(kind=\u0026#39;barh\u0026#39;, #水平条形图 figsize=(10, 15), fontsize=20, color=colors[:20]) plt.title(\u0026#39;Top 20 city in the Yelp\u0026#39;, fontsize=20, fontweight=\u0026#39;bold\u0026#39;) plt.show()   ","permalink":"/blog/yelpdataset%E7%BD%91%E7%AB%99%E6%95%B0%E6%8D%AE%E9%9B%8610g/","summary":"数据集下载 链接:https://pan.baidu.com/s/1rUfj7NYYAnM3NuFWCHyPpA 密码:ux8z\n在昨天发的文章里提到了yelp数据集，官网显示\u0026quot;Yelp数据集是我们用于个人，教育和学术目的的业务，评论和用户数据的子集。 以JSON文件形式提供，可用于在学习如何制作移动应用程序的同时，向学生介绍数据库，学习NLP或提供示例生产数据。\u0026quot;\nyelp官网显示，这个数据集亮点如下：\n 668+w条评论 19+w个商业机构 20w张图片 10个都市区域 字段包括:营业时间、是否泊车、可用性和环境    在kaggle上也能看到使用这个数据集的案例，说不定有你需要的分析方法。\n  我已经从yelp官网下载了数据, 参照kaggle的很多例子，咱们也在自己电脑上跑跑简单的分析\nbusiness数据读取 kaggle中的数据是csv文件，咱这里是json，略有不同，但读取都可以用pandas读取\nimport pandas as pd #一开始用注释掉的代码，有bug，经过百度找到lines=True解决方案 #business = pd.read_json(\u0026#39;yelp_dataset/business.json\u0026#39;) business = pd.read_json(\u0026#39;yelp_dataset/business.json\u0026#39;, lines=True) business.head()   评分分布 import seaborn as sns sns.color_palette() [(0.12156862745098039, 0.4666666666666667, 0.7058823529411765), (1.0, 0.4980392156862745, 0.054901960784313725), (0.17254901960784313, 0.6274509803921569, 0.17254901960784313), (0.8392156862745098, 0.15294117647058825, 0.1568627450980392), (0.5803921568627451, 0.403921568627451, 0.7411764705882353), (0.5490196078431373, 0.33725490196078434, 0.29411764705882354), (0.8901960784313725, 0.4666666666666667, 0.7607843137254902), (0.4980392156862745, 0.4980392156862745, 0.4980392156862745), (0.7372549019607844, 0.7411764705882353, 0.13333333333333333), (0.","title":"YelpDaset | 酒店管理类数据集10+G"},{"content":"哈尔滨               ","permalink":"/blog/%E5%93%88%E5%B0%94%E6%BB%A8%E7%BE%8E%E6%99%AF/","summary":"哈尔滨               ","title":"哈尔滨的美景"},{"content":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/数据可视化pandas_bokeh.zip\npandas_bokeh pandas_bokeh可以使得dataframe直接调用bokeh底层代码。通过使用pandas_bokeh，可以在notebook或者html中显示，语法相比于bokeh更简洁易用。\n安装 !pip3 install pandas_bokeh 快速上手 对fruits.csv做一个条形图\nimport pandas as pd df = pd.read_excel(\u0026#39;fruits.xlsx\u0026#39;) df     fruits 2015 2016 2017     0 苹果 2 5 3   1 梨 1 3 2   2 香蕉 4 3 4   3 草莓 3 2 4   4 樱桃 2 4 5   5 橘子 4 6 3    import pandas as pd import pandas_bokeh import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) #忽略某些不影响程序的提示 #在notebook中能显示可视化结果 pandas_bokeh.output_notebook() #将fruits列设置为行索引 df = pd.read_excel(\u0026#39;fruits.xlsx\u0026#39;) df.plot_bokeh(kind=\u0026#39;bar\u0026#39;, x = \u0026#39;fruits\u0026#39;, #将fruits列选做x轴 y = [\u0026#39;2015\u0026#39;, \u0026#39;2016\u0026#39;, \u0026#39;2017\u0026#39;], #将年份选做y轴 ylabel=\u0026#39;水果价格(元/斤)\u0026#39;, title=\u0026#39;水果\u0026#39;, show_figure=True) #显示   上面的例子已经可以看到该库的简洁优美之处，现在我们多学点\npandas_bokeh输出设定  pandas_bokeh.output_notebook() 在notebook中能显示可视化结果 pandas_bokeh.output_file(filename) 将结果输出到html文件中  支持可视化图种类  line bar point scatter histogram area pie map  以bar为例，调用可视化接口时，有以下两种使用方法\n df.plot_bokeh.line(\u0026hellip;) df.plot_bokeh(kind=\u0026lsquo;line\u0026rsquo;)  import numpy as np df = pd.read_excel(\u0026#39;fake_stocks.xlsx\u0026#39;) df.plot_bokeh(kind=\u0026#34;line\u0026#34;, x=\u0026#39;日期\u0026#39;, #将excel中的日期列当做x轴 y=[\u0026#39;Google\u0026#39;, \u0026#39;Apple\u0026#39;]) #将\u0026#39;Google\u0026#39;, \u0026#39;Apple\u0026#39;两列作为y轴   高级参数 df.plot_bokeh(kind, x, y, figsize, title, xlim, ylim, xlabel, ylabel, logx, logy, xticks, yticks, color, colormap, hovertool, zooming, panning, **kwargs)  kind: 支持的图种类\u0026quot;line\u0026quot;, \u0026ldquo;point\u0026rdquo;, \u0026ldquo;scatter\u0026rdquo;, \u0026ldquo;bar\u0026rdquo; ,\u0026ldquo;histogram\u0026quot;等 x: 选中数据某列名作为x轴。如果x不传入参数，会默认使用df的索引作为x轴 y: 将数据中的某列或某些列指定为y轴 figsize: 图的尺寸,如figsize=(600, 350) title: 图的标题 xlim/ylim: 设置图的x轴和y轴的范围 xlabel/ylabel: 设置x轴和y轴的名字 logx/logy: 布尔型值，对x和y的数据是否进行log变换 xticks/yticks: 显性定义横纵坐标刻度 color: 对图中使用同一的颜色，如果想定义多种颜色，请使用colormap参数 colormap: 可以对图中的不同对象设置颜色， 传入的是颜色字符串列表。 hovertool: 默认True，鼠标放在图上会悬浮显示具体信息。 zooming: 布尔值，默认True支持缩放 panning: 布尔值，默认True支持平移 kwargs**: 更多参数设定请看官方文档  文档  pandas_bokeh文档地址https://github.com/PatrikHlobil/Pandas-Bokeh Bokeh官方文档地址https://docs.bokeh.org/en/latest/  ","permalink":"/blog/%E4%BD%BF%E7%94%A8pandas_bokeh%E5%81%9A%E5%8F%AF%E8%A7%86%E5%8C%96/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/数据可视化pandas_bokeh.zip\npandas_bokeh pandas_bokeh可以使得dataframe直接调用bokeh底层代码。通过使用pandas_bokeh，可以在notebook或者html中显示，语法相比于bokeh更简洁易用。\n安装 !pip3 install pandas_bokeh 快速上手 对fruits.csv做一个条形图\nimport pandas as pd df = pd.read_excel(\u0026#39;fruits.xlsx\u0026#39;) df     fruits 2015 2016 2017     0 苹果 2 5 3   1 梨 1 3 2   2 香蕉 4 3 4   3 草莓 3 2 4   4 樱桃 2 4 5   5 橘子 4 6 3    import pandas as pd import pandas_bokeh import warnings warnings.","title":"使用pandas_bokeh做可视化"},{"content":"\n 本文提供了一条通往实际“被动”收入和财务自由的现实途径\nSrc https://themakingofamillionaire.com/passive-income-is-a-lie-but-scalable-income-is-real-f0483af14483\nAuthor: Ben Le Fort\n 如今，个人理财中最大的流行语之一是“被动收入”。 不乏网络营销人员想要向您推销的想法是，如果您只是听他们的（并购买他们的“课程”），您也可以以最少的努力开始创收。\n但事情是这样的； 被动收入的想法是一堆废话。这并不意味着您无法开展数字业务并永远改变您的财务生活，因为您绝对可以而且应该这样做。\n在本文中，我将解释\n 为什么被动收入是一个神话？ 可扩展收入如何运作的示例 从财务角度，多久才能退休，且只靠利息过活？ 增加可扩展的收入帮助我节省了第一笔 10 万美元 我总是记得“为什么”我这样做  为什么被动收入是一个神话？ 正如我过去所写，博主和 YouTuber 已经开始将任何数字业务称为“被动收入机会”。\n 联属网络营销。 销售数码产品/软件。 销售信息产品，如课程和书籍。 在线销售实物商品。 在 YouTube 频道、播客或博客上展示广告。  通常，当您听到有人提到“被动收入”时，他们真正指的是可扩展的商业收入。 可扩展的收入意味着不能保证您赚到一毛钱，但您的收入潜力没有限制。\n可扩展收入如何运作的示例 我的业务之一就是销售在线课程，这是可扩展收入的一个完美例子。\n 如果没有人买我的课程，我就赚不到钱。 既然是数字产品，卖额外的课程也不用花我额外的钱。 这与销售实物的人形成鲜明对比。 如果我卖电脑，我会为我卖的每台电脑支付额外的费用。 在经济学中，这被称为“边际生产成本”。 销售数字产品，尤其是像课程这样的信息产品意味着我的边际生产成本几乎为零。 这意味着如果我出售价值 0 美元或 100 万美元的课程，我的成本将是相同的。  这就是拥有可扩展收入的含义，也就是大多数人所说的“被动收入”。\n诚然，每增加一美元可扩展收入，您就需要减少工作量； 这不是说你可以抬起脚停止工作。 被动收入是您无需做任何工作就可以赚取的钱。 如果我停止从事我的业务，收入很快就会枯竭。被动收入的唯一真正来源来自投资收入。\n 股票分红。 从债券中赚取的利息。 从房地产收取的租金。  除非你继承了很多钱，否则你产生大量被动收入的唯一方法是通过工作、入不敷出以及投资差额来产生主动收入。\n不要相信任何告诉你不同的人。 那些告诉你不工作也能赚钱的人要么是想卖给你东西，要么是妄想，要么两者兼而有之。\n主动收入可以来自朝九晚五的工作或收入可扩展的企业。 或者，就我而言，两者兼而有之。\n**致富的简单途径在于保持开支不变，增加收入，并投资差价。**过去，我写过关于我们可以利用“两个杠杆”来提高储蓄率的文章。\n 对已有用的财富，尽可能节省节俭。 赚更多的钱。  **学会节俭但轻松幸福地生活，是迈向财务独立的关键一步。 **一旦你养成了良好的消费习惯，积累财富的最快途径就是不断寻找增加收入的方法，同时保持生活成本稳定并投资差额。\n 如果你能靠 25,000 美元生活并赚 50,000 美元，那意味着你有 25,000 美元可以投资。 如果你能靠 25,000 美元生活并赚 75,000 美元，那意味着你有 50,000 美元可以投资。 如果你能靠 25,000 美元生活并赚 125,000 美元，那意味着你有 100,000 美元可以投资。  如果你能找到一种每年投资收益数倍于你的年支出的方法，你的个人财富很快就会像火箭一样起飞。这是一个简单的概念，但在现实生活中要困难得多。除非您准备实行极端节俭，否则您不太可能每年通过朝九晚五的工作获得的薪水节省 2-4 倍的年度开支。当所有的税收和减免都从你的薪水中扣除，你减去你需要支付的住房、交通、食品和其他基本生活费用时，月底很可能已经所剩无几。\n这就是为什么创造收入可扩展的副业对我来说是一个改变游戏规则的人。\n从财务角度，多久才能退休，且只靠利息过活？ 不考虑退休金，只考虑工作多久，个人的财富利息收入能hold住个人年开支。需要确定以下四个指标\n 个人现金积蓄 money_pool 个人年收入（平均） avg_income 个人年支出（平均） avg_budget 投资年华收益率(平均) rate_of_return  def work_n_year_to_retire(money_pool, avg_income, avg_budget, rate_of_return): \u0026#34;\u0026#34;\u0026#34; money_pool: 现有储蓄金额(元) avg_income: 平均年收入(元) avg_budget: 年支出 rate_of_return: 年化收益率 \u0026#34;\u0026#34;\u0026#34; n = 0 while True: if money_pool*rate_of_return\u0026gt;=avg_budget: break else: money_pool = (money_pool + avg_income - avg_budget)*(1+rate_of_return) n = n + 1 return n, money_pool work_n_year_to_retire(money_pool=0, avg_income=200000, avg_budget=50000, rate_of_return=0.03) Run\n收入固定，改变节俭程度(consumption_ratio)\n 假设现有存款0元， 年收入20w， 年支出13w, 年华收益3%， 需要工作35年 积累435w的财富年利息收入才能支撑个人支出 假设现有存款0元， 年收入20w， 年支出10w, 年华收益3%， 需要工作23年 积累334w的财富年利息收入才能支撑个人支出 假设现有存款0元， 年收入20w， 年支出7w, 年华收益3%， 需要工作15年 积累249w的财富年利息收入才能支撑个人支出 假设现有存款0元， 年收入20w， 年支出5w, 年华收益3%， 需要工作10年 积累177w的财富年利息收入才能支撑个人支出  节俭程度固定，改变年收入\n 假设现有存款0元，年支出5w, 年收入6w， 年华收益3%， 需要工作60年 积累167w的财富年利息收入才能支撑个人支出 假设现有存款0元，年支出5w, 年收入10w， 年华收益3%， 需要工作23年 积累167w的财富年利息收入才能支撑个人支出 假设现有存款0元，年支出5w, 年收入15w， 年华收益3%， 需要工作14年 积累176w的财富年利息收入才能支撑个人支出 假设现有存款0元，年支出5w, 年收入20w， 年华收益3%， 需要工作10年 积累177w的财富年利息收入才能支撑个人支出  增加可扩展的收入帮助我节省了第一笔 10 万美元 两年前，我把写作当成副业。 在我写作的第一个月，我赚了 15 美元。 我一直在写作和建立观众，慢慢地，写作的收入开始增加。 2020 年初，我创建了一个在线课程以进一步通过我的写作获利，并且最近开始专注于建立一个 YouTube 频道。\n总而言之，我的兼职收入在每月 2,500 美元至 4,000 美元之间或每年在 30,000 美元至 54,000 美元之间。 每一分钱都投资于股市。 我可以将我的副业赚到的每一分钱都投资到股票市场，因为我可以用日常工作的收入来支付我所有的生活费用，然后还有一部分。 我的副业收入帮助我在我的投资账户中存入了超过 100,000 美元，比我想象的要早几年。\n我总是记得“为什么”我这样做 让我说清楚； 我所做的基本上是做两份工作。 我的大部分晚上和周末都花在增加我的副业收入上。这并不容易，但对我来说，这是值得的。 去年，我当了父亲，积累财富的概念有了全新的意义。 我的重点不仅是为我的余生提供财务保障，而且还为我儿子的整个生活和他可能拥有的任何孩子提供财务保障。\n我希望我的儿子和我家人的后代不要为钱而紧张。 金钱能买到的最宝贵的东西就是自由； 可以自由地做自己喜欢的事情。 许多人从事他们并不真正喜欢的工作，原因有两个。\n 他们需要一份工作来支付账单。 没有人愿意给他们“梦寐以求的工作”。  结果，他们没有自由来做自己喜欢的事情。通过致力于学习如何建立一个我喜欢的副业（我确实喜欢我的副业）并获得可扩展的收入，我可以通过两种方式为我的儿子提供这种自由。\n 积累足够的财务财富来维持我们家庭的生活开支。 这意味着他不需要接受一份他不喜欢支付账单的工作。 学习我可以传给我儿子的技能，这样他就可以学习如何通过做他热爱的工作来谋生。 这意味着，如果其他人不愿意为他提供他梦寐以求的工作，他将拥有出去为自己创造的工具。  与实现这一愿景的好处相比，我在接下来的几年中需要做出的牺牲微不足道。\n简而言之，我的“为什么”比我需要预先支付的成本更有力。\n可扩展的收入加上强大的“为什么”可以改变游戏规则。 但是不要误会，它没有任何被动。\n","permalink":"/blog/python%E5%91%8A%E8%AF%89%E4%BD%A0%E8%A2%AB%E5%8A%A8%E6%94%B6%E5%85%A5%E7%9A%84%E8%B4%A2%E5%8A%A1%E8%87%AA%E7%94%B1%E6%98%AF%E8%B0%8E%E8%A8%80%E4%BD%86%E5%8F%AF%E6%89%A9%E5%B1%95%E6%94%B6%E5%85%A5%E7%9A%84%E8%B4%A2%E5%8A%A1%E8%87%AA%E7%94%B1%E6%98%AF%E7%9C%9F%E7%9A%84/","summary":"本文提供了一条通往实际“被动”收入和财务自由的现实途径\nSrc https://themakingofamillionaire.com/passive-income-is-a-lie-but-scalable-income-is-real-f0483af14483\nAuthor: Ben Le Fort\n 如今，个人理财中最大的流行语之一是“被动收入”。 不乏网络营销人员想要向您推销的想法是，如果您只是听他们的（并购买他们的“课程”），您也可以以最少的努力开始创收。\n但事情是这样的； 被动收入的想法是一堆废话。这并不意味着您无法开展数字业务并永远改变您的财务生活，因为您绝对可以而且应该这样做。\n在本文中，我将解释\n 为什么被动收入是一个神话？ 可扩展收入如何运作的示例 从财务角度，多久才能退休，且只靠利息过活？ 增加可扩展的收入帮助我节省了第一笔 10 万美元 我总是记得“为什么”我这样做  为什么被动收入是一个神话？ 正如我过去所写，博主和 YouTuber 已经开始将任何数字业务称为“被动收入机会”。\n 联属网络营销。 销售数码产品/软件。 销售信息产品，如课程和书籍。 在线销售实物商品。 在 YouTube 频道、播客或博客上展示广告。  通常，当您听到有人提到“被动收入”时，他们真正指的是可扩展的商业收入。 可扩展的收入意味着不能保证您赚到一毛钱，但您的收入潜力没有限制。\n可扩展收入如何运作的示例 我的业务之一就是销售在线课程，这是可扩展收入的一个完美例子。\n 如果没有人买我的课程，我就赚不到钱。 既然是数字产品，卖额外的课程也不用花我额外的钱。 这与销售实物的人形成鲜明对比。 如果我卖电脑，我会为我卖的每台电脑支付额外的费用。 在经济学中，这被称为“边际生产成本”。 销售数字产品，尤其是像课程这样的信息产品意味着我的边际生产成本几乎为零。 这意味着如果我出售价值 0 美元或 100 万美元的课程，我的成本将是相同的。  这就是拥有可扩展收入的含义，也就是大多数人所说的“被动收入”。\n诚然，每增加一美元可扩展收入，您就需要减少工作量； 这不是说你可以抬起脚停止工作。 被动收入是您无需做任何工作就可以赚取的钱。 如果我停止从事我的业务，收入很快就会枯竭。被动收入的唯一真正来源来自投资收入。\n 股票分红。 从债券中赚取的利息。 从房地产收取的租金。  除非你继承了很多钱，否则你产生大量被动收入的唯一方法是通过工作、入不敷出以及投资差额来产生主动收入。\n不要相信任何告诉你不同的人。 那些告诉你不工作也能赚钱的人要么是想卖给你东西，要么是妄想，要么两者兼而有之。\n主动收入可以来自朝九晚五的工作或收入可扩展的企业。 或者，就我而言，两者兼而有之。\n**致富的简单途径在于保持开支不变，增加收入，并投资差价。**过去，我写过关于我们可以利用“两个杠杆”来提高储蓄率的文章。","title":"Python告诉你“被动收入”的财务自由是谎言，但可扩展收入的财务自由是真的"},{"content":"                    代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/人工智能与图数据库技术.pdf\n","permalink":"/blog/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8A%80%E6%9C%AF/","summary":"代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/人工智能与图数据库技术.pdf","title":"人工智能与图数据库技术"},{"content":"\n数据预处理过程当中, 大致会遇到\n 加载数据 处理缺失值如何处理 处理离散型数据该如何处理 数据的标准化 将数据集划分成训练集与测试集 去掉重复值  加载数据 我们导入必要的库并且加载数据\nimport pandas as pd df = pd.read_csv(\u0026#34;data.csv\u0026#34;) 在进行数据分析前，可以查看一下数据的总体情况，从宏观上了解数据\ndata.head() #显示前五行数据 data.tail() #显示末尾五行数据 data.info() #查看各字段的信息 data.shape #查看数据集有几行几列,data.shape[0]是行数,data.shape[1]是列数 data.describe() #查看数据的大体情况，均值，最值，分位数值... data.columns.tolist() #得到列名的list \n处理缺失值 缺失值一直都是数据预处理当中比较常见的一个问题，而在处理类似的问题的时候，方式方法也是多种多样的，我们一一来介绍，\ndata = [[\u0026#39;小明\u0026#39;,25,55],[\u0026#39;小红\u0026#39;,28,60],[\u0026#39;小王\u0026#39;,26]] df = pd.DataFrame(data=data,columns=[\u0026#39;Name\u0026#39;,\u0026#39;Age\u0026#39;,\u0026#39;Weight\u0026#39;]) Name Age Weight 0 小明 25 55.0 1 小红 28 60.0 2 小王 26 NaN 针对上面的数据集，我们通过pandas中的方法看一下缺失值的情况\ndf.isnull() Name Age Weight 0 False False False 1 False False False 2 False False True 另外我们也可以这么来做，检测每一列空值的数量\ndf.isnull().sum() Name 0 Age 0 Weight 1 dtype: int64 而在面对缺失值的时候，我们一方面可以将其去除\ndf.dropna() Name Age Weight 0 小明 25 55.0 1 小红 28 60.0 当然我们也可以对缺失值进行填充，例如用平均值来填充\ndf.fillna(df.mean()) Name Age Weight 0 小明 25 55.0 1 小红 28 60.0 2 小王 26 57.5 除了pandas当中的方法之外，我们也可以使用sklearn库当中的一些函数方法，例如\nfrom sklearn.impute import SimpleImputer imputer = SimpleImputer(missing_values=np.nan, strategy=\u0026#39;mean\u0026#39;) imputer = imputer.fit(df[[\u0026#39;Weight\u0026#39;]]) df[\u0026#39;Weight\u0026#39;] = imputer.transform(df[[\u0026#39;Weight\u0026#39;]]) 最后返回的结果也和上面的fillna()方法返回的结果一致，我们用平均值来代码空值，那么同样道理我们也可以用中位数、众数等统计值来进行替换，这里就不做多说\n处理离散型数据 另外当数据集当中出现离散型数据的时候，我们也要进行相应的处理，毕竟在后面的建模过程当中，机器学习的模型需要的是连续型的数据。 离散型数据也分为两种，一种是有序的离散变量，就比方说是衣服的尺码，有M码的、也有L码的、也还有与之更大的尺码，另外一种则是无序的，例如衣服的颜色，颜色之间没有大小之分，因此在编码的时候也应该另外处理。\ndf_cat = pd.DataFrame(data = [[\u0026#39;green\u0026#39;,\u0026#39;M\u0026#39;,10.1,\u0026#39;class1\u0026#39;], [\u0026#39;blue\u0026#39;,\u0026#39;L\u0026#39;,20.1,\u0026#39;class2\u0026#39;], [\u0026#39;white\u0026#39;,\u0026#39;M\u0026#39;,30.1,\u0026#39;class1\u0026#39;]], ) df_cat.columns = [\u0026#39;color\u0026#39;,\u0026#39;size\u0026#39;,\u0026#39;price\u0026#39;,\u0026#39;classlabel\u0026#39;] color size price classlabel 0 green M 10.1 class1 1 blue L 20.1 class2 2 white M 30.1 class1 对于有序的离散型变量，我们可以使用map()函数\nsize_mapping = {\u0026#39;M\u0026#39;:1,\u0026#39;L\u0026#39;:2} df_cat[\u0026#39;size\u0026#39;] = df_cat[\u0026#39;size\u0026#39;].map(size_mapping) df_cat[\u0026#39;size\u0026#39;] 0 1 1 2 2 1 Name: size, dtype: int64 另外我们也可以使用sklearn库中的LabelEncoder()方法来处理\nfrom sklearn.preprocessing import LabelEncoder class_le = LabelEncoder() df_cat[\u0026#39;size\u0026#39;] = class_le.fit_transform(df_cat[\u0026#39;size\u0026#39;].values) 而对于无序的离散型变量，我们可以采用独热编码，例如对color这一列进行编码过之后会有color_green、color_blue以及color_white三个特征，特征值为0或者1\npd.get_dummies(df_cat[\u0026#39;color\u0026#39;], prefix = \u0026#34;color\u0026#34;) color_blue color_green color_white 0 0 1 0 1 1 0 0 2 0 0 1 然后我们将此并入到源数据当中去\ndf_cat[[\u0026#34;size\u0026#34;, \u0026#34;price\u0026#34;]].join(dummies) size price color_blue color_green color_white 0 1 10.1 0 1 0 1 2 20.1 1 0 0 2 1 30.1 0 0 1 但是考虑到后面搭建模型的时候，变量与变量之间应该保持独立，而不应该是存在依赖的关系，对于color这一列中存在三种颜色，分别是blue、green以及white，当前两类取值都为0的时候，color只可能是white 所以将get_dummies()方法中的drop_first默认值为False改为True dummies = pd.get_dummies(df_cat[\u0026#39;color\u0026#39;], prefix = \u0026#34;color\u0026#34;, drop_first=True) df_cat[[\u0026#34;size\u0026#34;, \u0026#34;price\u0026#34;]].join(dummies) \n数据的标准化 由于不同的变量，它们往往存在不同的单位以及不同的取值范围，有时候取值范围的差异较大会对机器学习的模型带来很多不必要的麻烦。因此为了最后预测结果的可靠性，我们需要对数据进行标准化，对数据按比例进行缩放，使之落入一个小的特定区间。而标准化算法有\n z-score 标准化  这种方法根据原始数据的均值和标准差进行数据的标准化，经过处理的数据符合正态分布，即均值为0，标准差为1 ，当然sklearn库当中的代码则是\nfrom sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test)  线性归一化  from sklearn.preprocessing import MinMaxScaler() min_max_scaler = MinMaxScaler() X_train_minmax = min_max_scaler.fit_transform(X_train) print(X_train_minmax) 训练集中的数据经过转化，取值范围都集中在[0,1]之间\n MaxAbsScaler()方法  MaxAbsScaler()方法和上述的线性归一化效果相类似，训练集中的数据经过转化，取值范围在[-1,1]之间\nmax_abs_scaler = preprocessing.MaxAbsScaler() X_train_maxabs = max_abs_scaler.fit_transform(X_train) X_test_maxabs = max_abs_scaler.transform(X_test)  RobustScaler()方法  要是当数据集当中存在很多的极值的时候，利用平均值和标准差来进行数据的标准化效果可能并不理想，毕竟极值会影响到平均值和标准差的计算，这个时候我们就需要用到RobustScaler()方法，\nfrom sklearn.preprocessing import RobustScaler transformer = RobustScaler().fit(X) transformer.transform(X) \n将数据集划分成训练集和测试集 在建模之前，我们需要将数据集分成训练集和测试集，我们在训练集上面建立模型，训练与优化模型，然后再将模型放到测试集上面，评估一下模型的性能以及优化的效果，在sklearn库中也有相对应的方法\nfrom sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state= 1) 上面的变量y具体指的是被预测的因变量，而x则是在预测中使用的自变量\n去除重复值 在pandas当中也有对应的方法来去除掉重复值\ndf.drop_duplicates() \u0026lt;br\u0026gt; ## 代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/4000字归纳总结PandasSklearn数据预处理.ipynb 出处  src: https://mp.weixin.qq.com/s/7D2d7BIaFAhPrxtDOWGgLA\nauthor: 俊欣\n公众号: 关于数据分析与可视化\n 千聊Python课程 https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596\n","permalink":"/blog/%E4%BD%BF%E7%94%A8pandas%E5%81%9A%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/","summary":"数据预处理过程当中, 大致会遇到\n 加载数据 处理缺失值如何处理 处理离散型数据该如何处理 数据的标准化 将数据集划分成训练集与测试集 去掉重复值  加载数据 我们导入必要的库并且加载数据\nimport pandas as pd df = pd.read_csv(\u0026#34;data.csv\u0026#34;) 在进行数据分析前，可以查看一下数据的总体情况，从宏观上了解数据\ndata.head() #显示前五行数据 data.tail() #显示末尾五行数据 data.info() #查看各字段的信息 data.shape #查看数据集有几行几列,data.shape[0]是行数,data.shape[1]是列数 data.describe() #查看数据的大体情况，均值，最值，分位数值... data.columns.tolist() #得到列名的list \n处理缺失值 缺失值一直都是数据预处理当中比较常见的一个问题，而在处理类似的问题的时候，方式方法也是多种多样的，我们一一来介绍，\ndata = [[\u0026#39;小明\u0026#39;,25,55],[\u0026#39;小红\u0026#39;,28,60],[\u0026#39;小王\u0026#39;,26]] df = pd.DataFrame(data=data,columns=[\u0026#39;Name\u0026#39;,\u0026#39;Age\u0026#39;,\u0026#39;Weight\u0026#39;]) Name Age Weight 0 小明 25 55.0 1 小红 28 60.0 2 小王 26 NaN 针对上面的数据集，我们通过pandas中的方法看一下缺失值的情况\ndf.isnull() Name Age Weight 0 False False False 1 False False False 2 False False True 另外我们也可以这么来做，检测每一列空值的数量","title":"使用Pandas做数据预处理"},{"content":"\n src: https://mp.weixin.qq.com/s/MmfEtyKaMqNn_Ik1oJtitQ\nauthor: 俊欣\n公众号: 关于数据分析与可视化\n 千聊Python课程 https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596\n今天我们来谈论一下pandas库当中文本数据的操作，希望大家再看完本篇文章之后会有不少的收获，我们大致会讲\n 创建一个包含文本数据的DataFrame 常用处理文本数据的方法的总结 正则表达式与DataFrame内部方法的结合  创建文本内容的数据 我们先来创建一个包含文本数据的DataFrame，来供我们后面使用\nimport pandas as pd df = pd.DataFrame({ \u0026#34;姓\u0026#34;: [\u0026#34;李\u0026#34;,\u0026#34;王\u0026#34;,\u0026#34;戴\u0026#34;, \u0026#34;李\u0026#34;, \u0026#34;张\u0026#34;], \u0026#34;名\u0026#34;: [\u0026#34;华\u0026#34;,\u0026#34;硕\u0026#34;,\u0026#34;建业\u0026#34;, \u0026#34;四\u0026#34;, \u0026#34;三\u0026#34;], \u0026#34;户籍地址\u0026#34;: [\u0026#34; 浙江省·宁波市 \u0026#34;, \u0026#34; 浙江省·杭州市 \u0026#34;, \u0026#34; 浙江省·丽水市 \u0026#34;, \u0026#34; 浙江省·衢州市 \u0026#34;, \u0026#34; 浙江省·湖州市 \u0026#34;], \u0026#34;微信ID\u0026#34;: [\u0026#34;Tomoplplplut1248781\u0026#34;, \u0026#34;Smopopo857\u0026#34;, \u0026#34;Adahuhuifhhjfj\u0026#34;, \u0026#34;Tull1945121\u0026#34;, \u0026#34;ZPWERERTFD599557\u0026#34;], \u0026#34;邮箱地址\u0026#34;: [\u0026#34;tom02159@163.com\u0026#34;, \u0026#34;smitt7821@163.com\u0026#34;, \u0026#34;adams623@163.com\u0026#34;, \u0026#34;tull0305@163.com\u0026#34;, \u0026#34;five7532@163.com\u0026#34;] }) df | | 姓 | 名 | 户籍地址 | 微信ID | 邮箱地址 | |---:|:-----|:-----|:--------------|:--------------------|:------------------| | 0 | 李 | 华 | 浙江省·宁波市 | Tomoplplplut1248781 | tom02159@163.com | | 1 | 王 | 硕 | 浙江省·杭州市 | Smopopo857 | smitt7821@163.com | | 2 | 戴 | 建业 | 浙江省·丽水市 | Adahuhuifhhjfj | adams623@163.com | | 3 | 李 | 四 | 浙江省·衢州市 | Tull1945121 | tull0305@163.com | | 4 | 张 | 三 | 浙江省·湖州市 | ZPWERERTFD599557 | five7532@163.com | \n常用处理文本数据的方法总结 Python当中用来处理字符串数据的方法有很多，基本上都可以在DataFrame内部拿来使用，例如lower()方法和upper()方法，给字母大小写\ndf[\u0026#34;微信ID\u0026#34;].str.lower() 0 tomoplplplut1248781 1 smopopo857 2 adahuhuifhhjfj 3 tull1945121 4 zpwerertfd599557 Name: 微信ID, dtype: object df[\u0026#34;微信ID\u0026#34;].str.len() 0 19 1 10 2 14 3 11 4 16 Name: 微信ID, dtype: int64 当然我们看到户籍地址这一列中的数据有很多的空格\ndf[\u0026#34;户籍地址\u0026#34;] 0 浙江省·宁波市 1 浙江省·杭州市 2 浙江省·丽水市 3 浙江省·衢州市 4 浙江省·湖州市 Name: 户籍地址, dtype: object 我们可以使用处理字符串时的strip()方法\ndf[\u0026#34;户籍地址\u0026#34;].str.strip() 0 浙江省·宁波市 1 浙江省·杭州市 2 浙江省·丽水市 3 浙江省·衢州市 4 浙江省·湖州市 Name: 户籍地址, dtype: object 与之相类似的还有lstrip()方法以及rstrip()方法，这里就不做赘述。\n在字符串的处理过程当中，startswith()方法和endswith()方法也是用的非常的频繁，例如我们想要挑选出户籍地址是“宁波市”的数据，\ndf[\u0026#34;户籍地址\u0026#34;] = df[\u0026#34;户籍地址\u0026#34;].str.strip() df[df[\u0026#34;户籍地址\u0026#34;].str.endswith(\u0026#34;宁波市\u0026#34;)] 姓 名 户籍地址 微信ID 邮箱地址 0 李 华 浙江省·宁波市 Tomoplplplut1248781 tom02159@163.com 另外我们还可以使用replace()方法来实现当中的字符串的替换\ndf[\u0026#34;户籍地址\u0026#34;].str.replace(\u0026#34;·\u0026#34;, \u0026#34;--\u0026#34;) 0 浙江省--宁波市 1 浙江省--杭州市 2 浙江省--丽水市 3 浙江省--衢州市 4 浙江省--湖州市 Name: 户籍地址, dtype: object 那既然用到了replace()方法，那么split()方法也可以尝试一下\ndf[\u0026#34;户籍地址\u0026#34;].str.split(\u0026#34;·\u0026#34;) 0 [浙江省, 宁波市] 1 [浙江省, 杭州市] 2 [浙江省, 丽水市] 3 [浙江省, 衢州市] 4 [浙江省, 湖州市] Name: 户籍地址, dtype: object 在经过spilit()方法的切割过之后就变成了列表的形式，然后可以通过get()方法或者[]来获取里面的元素，例如\ndf[\u0026#34;户籍地址\u0026#34;].str.split(\u0026#34;·\u0026#34;).str.get(0) 或者\ndf[\u0026#34;户籍地址\u0026#34;].str.split(\u0026#34;·\u0026#34;).str[0] 0 浙江省 1 浙江省 2 浙江省 3 浙江省 4 浙江省 Name: 户籍地址, dtype: object 那么获取列表当中的第二个元素也是同样的道理，当然我们也可以在split()方法当中添加expand=True这个参数，来将上面列表形式的数据转化成DataFrame格式\ndf[\u0026#34;户籍地址\u0026#34;].str.split(\u0026#34;·\u0026#34;, expand=True) 0 1 0 浙江省 宁波市 1 浙江省 杭州市 2 浙江省 丽水市 3 浙江省 衢州市 4 浙江省 湖州市 同样地，我们可以在后面添加[]来获取我们想要的元素\ndf[\u0026#34;户籍地址\u0026#34;].str.split(\u0026#34;·\u0026#34;, expand=True)[1] 0 宁波市 1 杭州市 2 丽水市 3 衢州市 4 湖州市 Name: 1, dtype: object \n正则表达式与DataFrame内部方法的结合 假如我们想要提取文本数据内部的一部分数据，可以结合正则表达式来使用，例如我们想要提取“微信ID”这一列当中的字母和数字，并且将两者分开来\ntwo_groups = \u0026#34;([a-zA-Z]+)([0-9]+)\u0026#34; df[\u0026#34;微信ID\u0026#34;].str.extract(two_groups, expand=True) 0 1 0 Tomoplplplut 1248781 1 Smopopo 857 2 NaN NaN 3 Tull 1945121 4 ZPWERERTFD 599557 当然了，如果想是要提取文本数据中的部分数据，可以直接在str方法后面添加索引，例如\ndf[\u0026#34;邮箱地址\u0026#34;].str[-8:] 0 @163.com 1 @163.com 2 @163.com 3 @163.com 4 @163.com Name: 邮箱地址, dtype: object 当然，从另外一个角度讲，正则表达式也可以帮助我们确认文本数据是否符合某种规律，\ntwo_groups = \u0026#34;([a-zA-Z]+)([0-9]+)\u0026#34; df[\u0026#34;微信ID\u0026#34;].str.match(two_groups) 0 True 1 True 2 False 3 True 4 True Name: 微信ID, dtype: bool 当中有一个为False，不满足字母+数字的规律，我们再进一步，将满足条件的数据提取出来\ndf[df[\u0026#34;微信ID\u0026#34;].str.match(two_groups)] 姓 名 户籍地址 微信ID 邮箱地址 0 李 华 浙江省·宁波市 Tomoplplplut1248781 tom02159@163.com 1 王 硕 浙江省·杭州市 Smopopo857 smitt7821@163.com 3 李 四 浙江省·衢州市 Tull1945121 tull0305@163.com 4 张 三 浙江省·湖州市 ZPWERERTFD599557 five7532@163.com 针对文本数据而言，contains()方法也能够派上用场，例如下面的数据\n姓 名 户籍地址 微信ID 邮箱地址 0 李 华 浙江省·宁波市 Tomoplplplut1248781 tom02159@163.com 1 王 硕 浙江省·杭州市 Smopopo857 smitt7821@163.com 2 戴 建业 浙江省·丽水市 Adahuhuifhhjfj adams623@163.com 3 李 四 浙江省·衢州市 Tull1945121 tull0305@163.com 4 张 三 浙江省·湖州市 ZPWERERTFD599557 five7532@163.com 5 黄 五 浙江省·宁波市 hunhunhu45652 1erdcvf127@16.com 我们用contains()来提取出户籍地址为“宁波市”的内容，可以这么做\ndf[df[\u0026#34;户籍地址\u0026#34;].str.contains(\u0026#34;宁波市\u0026#34;)] 姓 名 户籍地址 微信ID 邮箱地址 0 李 华 浙江省·宁波市 Tomoplplplut1248781 tom02159@163.com 5 黄 五 浙江省·宁波市 hunhunhu45652 1erdcvf127@16.com 暂时就这些了，下一篇原创的文章安排在周天，非技术方面的，期待一下？\n代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/文本数据处理Pandas篇.ipynb\n","permalink":"/blog/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86pandas%E7%AF%87/","summary":"src: https://mp.weixin.qq.com/s/MmfEtyKaMqNn_Ik1oJtitQ\nauthor: 俊欣\n公众号: 关于数据分析与可视化\n 千聊Python课程 https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596\n今天我们来谈论一下pandas库当中文本数据的操作，希望大家再看完本篇文章之后会有不少的收获，我们大致会讲\n 创建一个包含文本数据的DataFrame 常用处理文本数据的方法的总结 正则表达式与DataFrame内部方法的结合  创建文本内容的数据 我们先来创建一个包含文本数据的DataFrame，来供我们后面使用\nimport pandas as pd df = pd.DataFrame({ \u0026#34;姓\u0026#34;: [\u0026#34;李\u0026#34;,\u0026#34;王\u0026#34;,\u0026#34;戴\u0026#34;, \u0026#34;李\u0026#34;, \u0026#34;张\u0026#34;], \u0026#34;名\u0026#34;: [\u0026#34;华\u0026#34;,\u0026#34;硕\u0026#34;,\u0026#34;建业\u0026#34;, \u0026#34;四\u0026#34;, \u0026#34;三\u0026#34;], \u0026#34;户籍地址\u0026#34;: [\u0026#34; 浙江省·宁波市 \u0026#34;, \u0026#34; 浙江省·杭州市 \u0026#34;, \u0026#34; 浙江省·丽水市 \u0026#34;, \u0026#34; 浙江省·衢州市 \u0026#34;, \u0026#34; 浙江省·湖州市 \u0026#34;], \u0026#34;微信ID\u0026#34;: [\u0026#34;Tomoplplplut1248781\u0026#34;, \u0026#34;Smopopo857\u0026#34;, \u0026#34;Adahuhuifhhjfj\u0026#34;, \u0026#34;Tull1945121\u0026#34;, \u0026#34;ZPWERERTFD599557\u0026#34;], \u0026#34;邮箱地址\u0026#34;: [\u0026#34;tom02159@163.com\u0026#34;, \u0026#34;smitt7821@163.com\u0026#34;, \u0026#34;adams623@163.com\u0026#34;, \u0026#34;tull0305@163.com\u0026#34;, \u0026#34;five7532@163.com\u0026#34;] }) df | | 姓 | 名 | 户籍地址 | 微信ID | 邮箱地址 | |---:|:-----|:-----|:--------------|:--------------------|:------------------| | 0 | 李 | 华 | 浙江省·宁波市 | Tomoplplplut1248781 | tom02159@163.","title":"使用Pandas处理文本数据"},{"content":"今天分享几个不为人知的pandas函数，大家可能平时看到的不多，但是使用起来倒是非常的方便，也能够帮助我们数据分析人员大幅度地提高工作效率，同时也希望大家看完之后能够有所收获\nitems()方法 pandas当中的items()方法可以用来遍历数据集当中的每一列，同时返回列名以及每一列当中的内容，通过以元组的形式，示例如下\nimport pandas as pd df = pd.DataFrame({\u0026#39;species\u0026#39;: [\u0026#39;bear\u0026#39;, \u0026#39;bear\u0026#39;, \u0026#39;marsupial\u0026#39;], \u0026#39;population\u0026#39;: [1864, 22000, 80000]}, index=[\u0026#39;panda\u0026#39;, \u0026#39;polar\u0026#39;, \u0026#39;koala\u0026#39;]) df | | species | population | |:------|:----------|-------------:| | panda | bear | 1864 | | polar | bear | 22000 | | koala | marsupial | 80000 | \nfor label, content in df.items(): print(f\u0026#39;label: {label}\u0026#39;) print(f\u0026#39;content: {content}\u0026#39;, sep=\u0026#39;\\n\u0026#39;) print(\u0026#34;=\u0026#34; * 50) label: species content: panda bear polar bear koala marsupial Name: species, dtype: object ================================================== label: population content: panda 1864 polar 22000 koala 80000 Name: population, dtype: int64 ==================================================  相继的打印出了‘species’和‘population’这两列的列名和相应的内容\niterrows()方法 而对于iterrows()方法而言，其功能则是遍历数据集当中的每一行，返回每一行的索引以及带有列名的每一行的内容，示例如下\nfor label, content in df.iterrows(): print(f\u0026#39;label: {label}\u0026#39;) print(f\u0026#39;content: {content}\u0026#39;, sep=\u0026#39;\\n\u0026#39;) print(\u0026#34;=\u0026#34; * 50) label: panda content: species bear population 1864 Name: panda, dtype: object ================================================== label: polar content: species bear population 22000 Name: polar, dtype: object ================================================== label: koala content: species marsupial population 80000 Name: koala, dtype: object ==================================================  insert()方法 insert()方法主要是用于在数据集当中的特定位置处插入数据。在DataFrame数据集当中，列的索引也是从0开始的\n示例如下\n#在第二列插入size字段，内容如下 df.insert(1, \u0026#34;size\u0026#34;, [2000, 3000, 4000]) df | | species | size | population | |:------|:----------|-------:|-------------:| | panda | bear | 2000 | 1864 | | polar | bear | 3000 | 22000 | | koala | marsupial | 4000 | 80000 | \nassign()方法 assign()方法可以用来在数据集当中添加新的列，示例如下\ndf = df.assign(size_1=lambda x: x.population * 9 / 5 + 32) df | | species | size | population | size_1 | |:------|:----------|-------:|-------------:|---------:| | panda | bear | 2000 | 1864 | 3387.2 | | polar | bear | 3000 | 22000 | 39632 | | koala | marsupial | 4000 | 80000 | 144032 | 从上面的例子中可以看出，我们通过一个lambda匿名函数，在数据集当中添加一个新的列，命名为size_1，当然我们也可以通过assign()方法来创建不止一个列\ndf = df.assign(size_1 = lambda x: x.population * 9 / 5 + 32, size_2 = lambda x: x.population * 8 / 5 + 10) df | | species | size | population | size_1 | size_2 | |:------|:----------|-------:|-------------:|---------:|---------:| | panda | bear | 2000 | 1864 | 3387.2 | 2992.4 | | polar | bear | 3000 | 22000 | 39632 | 35210 | | koala | marsupial | 4000 | 80000 | 144032 | 128010 | \neval()方法 eval()方法主要是用来执行用字符串来表示的运算过程的，例如\ndf.eval(\u0026#34;size_3 = size_1 + size_2\u0026#34;) | | species | size | population | size_1 | size_2 | size_3 | |:------|:----------|-------:|-------------:|---------:|---------:|---------:| | panda | bear | 2000 | 1864 | 3387.2 | 2992.4 | 6379.6 | | polar | bear | 3000 | 22000 | 39632 | 35210 | 74842 | | koala | marsupial | 4000 | 80000 | 144032 | 128010 | 272042 | 当然我们也可以同时对执行多个运算过程\ndf = df.eval(\u0026#39;\u0026#39;\u0026#39; size_3 = size_1 + size_2 size_4 = size_1 - size_2 \u0026#39;\u0026#39;\u0026#39;) df | | species | size | population | size_1 | size_2 | size_3 | size_4 | |:------|:----------|-------:|-------------:|---------:|---------:|---------:|---------:| | panda | bear | 2000 | 1864 | 3387.2 | 2992.4 | 6379.6 | 394.8 | | polar | bear | 3000 | 22000 | 39632 | 35210 | 74842 | 4422 | | koala | marsupial | 4000 | 80000 | 144032 | 128010 | 272042 | 16022 | \npop()方法 pop()方法主要是用来删除掉数据集中特定的某一列数据\ndf.pop(\u0026#34;size_3\u0026#34;) 而原先的数据集当中就没有这个‘size_3’这一例的数据了\ntruncate()方法 truncate()方法主要是根据行索引来筛选指定行的数据的，示例如下\ndf = pd.DataFrame({\u0026#39;A\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;], \u0026#39;B\u0026#39;: [\u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;j\u0026#39;], \u0026#39;C\u0026#39;: [\u0026#39;k\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;n\u0026#39;, \u0026#39;o\u0026#39;]}, index=[1, 2, 3, 4, 5]) df | | A | B | C | |---:|:----|:----|:----| | 1 | a | f | k | | 2 | b | g | l | | 3 | c | h | m | | 4 | d | i | n | | 5 | e | j | o | 我们使用truncate()方法来做一下尝试\ndf.truncate(before=2, after=4) | | A | B | C | |---:|:----|:----|:----| | 2 | b | g | l | | 3 | c | h | m | | 4 | d | i | n | 我们看到参数before和after存在于truncate()方法中，目的就是把行索引2之前和行索引4之后的数据排除在外，筛选出剩余的数据\ncount()方法 count()方法主要是用来计算某一列当中非空值的个数，示例如下\nimport numpy as np df = pd.DataFrame({\u0026#34;Name\u0026#34;: [\u0026#34;John\u0026#34;, \u0026#34;Myla\u0026#34;, \u0026#34;Lewis\u0026#34;, \u0026#34;John\u0026#34;, \u0026#34;John\u0026#34;], \u0026#34;Age\u0026#34;: [24., np.nan, 25, 33, 26], \u0026#34;Single\u0026#34;: [True, True, np.nan, True, False]}) df | | Name | Age | Single | |---:|:-------|------:|---------:| | 0 | John | 24 | 1 | | 1 | Myla | nan | 1 | | 2 | Lewis | 25 | nan | | 3 | John | 33 | 1 | | 4 | John | 26 | 0 | 我们使用count()方法来计算一下数据集当中非空值的个数\ndf.count() Name 5 Age 4 Single 4 dtype: int64  add_prefix()方法/add_suffix()方法 add_prefix()方法和add_suffix()方法分别会给列名以及行索引添加后缀和前缀，对于Series()数据集而言，前缀与后缀是添加在行索引处，而对于DataFrame()数据集而言，前缀与后缀是添加在列索引处，示例如下\ns = pd.Series([1, 2, 3, 4]) s 0 1 1 2 2 3 3 4 dtype: int64  我们使用add_prefix()方法与add_suffix()方法在Series()数据集上\ns.add_prefix(\u0026#39;row_\u0026#39;) row_0 1 row_1 2 row_2 3 row_3 4 dtype: int64  又例如\ns.add_suffix(\u0026#39;_row\u0026#39;) 0_row 1 1_row 2 2_row 3 3_row 4 dtype: int64  而对于DataFrame()形式数据集而言，add_prefix()方法以及add_suffix()方法是将前缀与后缀添加在列索引处的\ndf = pd.DataFrame({\u0026#39;A\u0026#39;: [1, 2, 3, 4], \u0026#39;B\u0026#39;: [3, 4, 5, 6]}) df | | A | B | |---:|----:|----:| | 0 | 1 | 3 | | 1 | 2 | 4 | | 2 | 3 | 5 | | 3 | 4 | 6 | \ndf.add_prefix(\u0026#34;column_\u0026#34;) | | column_A | column_B | |---:|----:|----:| | 0 | 1 | 3 | | 1 | 2 | 4 | | 2 | 3 | 5 | | 3 | 4 | 6 | \ndf.add_suffix(\u0026#34;_column\u0026#34;) | | A_column | B_column | |---:|----:|----:| | 0 | 1 | 3 | | 1 | 2 | 4 | | 2 | 3 | 5 | | 3 | 4 | 6 | \nclip()方法 clip()方法主要是通过设置阈值来改变数据集当中的数值，当数值超过阈值的时候，就做出相应的调整\ndata = {\u0026#39;col_0\u0026#39;: [9, -3, 0, -1, 5], \u0026#39;col_1\u0026#39;: [-2, -7, 6, 8, -5]} df = pd.DataFrame(data) df | | col_0 | col_1 | |---:|--------:|--------:| | 0 | 9 | -2 | | 1 | -3 | -7 | | 2 | 0 | 6 | | 3 | -1 | 8 | | 4 | 5 | -5 | \ndf.clip(lower = -4, upper = 4) | | col_0 | col_1 | |---:|--------:|--------:| | 0 | 4 | -2 | | 1 | -3 | -4 | | 2 | 0 | 4 | | 3 | -1 | 4 | | 4 | 4 | -4 | 我们看到参数lower和upper分别代表阈值的上限与下限，数据集当中超过上限与下限的值会被替代。\nfilter()方法 pandas当中的filter()方法是用来筛选出特定范围的数据的，示例如下\ndf = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12])), index=[\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;], columns=[\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]) df | | one | two | three | |:---|------:|------:|--------:| | A | 1 | 2 | 3 | | B | 4 | 5 | 6 | | C | 7 | 8 | 9 | | D | 10 | 11 | 12 | 我们使用filter()方法来筛选数据\ndf.filter(items=[\u0026#39;one\u0026#39;, \u0026#39;three\u0026#39;]) | | one | three | |:---|------:|--------:| | A | 1 | 3 | | B | 4 | 6 | | C | 7 | 9 | | D | 10 | 12 | 我们还可以使用正则表达式来筛选数据\n#字段名e结尾的列 df.filter(regex=\u0026#39;e$\u0026#39;, axis=1) | | one | three | |:---|------:|--------:| | A | 1 | 3 | | B | 4 | 6 | | C | 7 | 9 | | D | 10 | 12 | 当然通过参数axis来调整筛选行方向或者是列方向的数据\ndf.filter(like=\u0026#39;B\u0026#39;, axis=0) | | one | two | three | |:---|------:|------:|--------:| | B | 4 | 5 | 6 | \nfirst()方法 当数据集当中的行索引是日期的时候，可以通过该方法来筛选前面几行的数据\nindex_1 = pd.date_range(\u0026#39;2021-11-11\u0026#39;, periods=5, freq=\u0026#39;2D\u0026#39;) ts = pd.DataFrame({\u0026#39;A\u0026#39;: [1, 2, 3, 4, 5]}, index=index_1) ts | | A | |:--------------------|----:| | 2021-11-11 00:00:00 | 1 | | 2021-11-13 00:00:00 | 2 | | 2021-11-15 00:00:00 | 3 | | 2021-11-17 00:00:00 | 4 | | 2021-11-19 00:00:00 | 5 | \n我们使用first()方法来进行一些操作，例如筛选出前面3天的数据\nts.first(\u0026#39;3D\u0026#39;) | | A | |:--------------------|----:| | 2021-11-11 00:00:00 | 1 | | 2021-11-13 00:00:00 | 2 | \nisin()方法 isin()方法主要是用来确认数据集当中的数值是否被包含在给定的列表当中\ndf = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12])), index=[\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;], columns=[\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]) df.isin([3, 5, 12]) | | one | two | three | |:---|------:|------:|--------:| | A | 0 | 0 | 1 | | B | 0 | 1 | 0 | | C | 0 | 0 | 0 | | D | 0 | 0 | 1 | 若是数值被包含在列表当中了，也就是3、5、12当中，返回的是True，否则就返回False\ndf.plot.area()方法 下面我们来讲一下如何在Pandas当中通过一行代码来绘制图表，将所有的列都通过面积图的方式来绘制\ndf = pd.DataFrame({ \u0026#39;sales\u0026#39;: [30, 20, 38, 95, 106, 65], \u0026#39;signups\u0026#39;: [7, 9, 6, 12, 18, 13], \u0026#39;visits\u0026#39;: [20, 42, 28, 62, 81, 50], }, index=pd.date_range(start=\u0026#39;2021/01/01\u0026#39;, end=\u0026#39;2021/07/01\u0026#39;, freq=\u0026#39;M\u0026#39;)) df | | sales | signups | visits | |:--------------------|--------:|----------:|---------:| | 2021-01-31 00:00:00 | 30 | 7 | 20 | | 2021-02-28 00:00:00 | 20 | 9 | 42 | | 2021-03-31 00:00:00 | 38 | 6 | 28 | | 2021-04-30 00:00:00 | 95 | 12 | 62 | | 2021-05-31 00:00:00 | 106 | 18 | 81 | | 2021-06-30 00:00:00 | 65 | 13 | 50 | \nax = df.plot.area(figsize = (10, 5))   df.plot.bar()方法 下面我们看一下如何通过一行代码来绘制柱状图\ndf = pd.DataFrame({\u0026#39;label\u0026#39;:[\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;], \u0026#39;values\u0026#39;:[10, 30, 50, 70]}) df.to_markdown() | | label | values | |---:|:--------|---------:| | 0 | A | 10 | | 1 | B | 30 | | 2 | C | 50 | | 3 | D | 70 | \nax = df.plot.bar(x=\u0026#39;label\u0026#39;, y=\u0026#39;values\u0026#39;, rot=-15)   当然我们也可以根据不同的类别来绘制柱状图\nage = [0.1, 17.5, 40, 48, 52, 69, 88] weight = [2, 8, 70, 1.5, 25, 12, 28] index = [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;G\u0026#39;] df = pd.DataFrame({\u0026#39;age\u0026#39;: age, \u0026#39;weight\u0026#39;: weight}, index=index) ax = df.plot.bar(rot=0)   ax = df.plot.barh()   df.plot.box()方法 我们来看一下箱型图的具体的绘制，通过pandas一行代码来实现\ndata = np.random.randn(25, 3) df = pd.DataFrame(data, columns=list(\u0026#39;ABC\u0026#39;)) df | | A | B | C | |---:|----------:|-----------:|-----------:| | 0 | -1.59132 | 1.47926 | 1.16891 | | 1 | -0.649097 | 0.0501129 | -0.81485 | | 2 | 1.31677 | 1.00035 | 0.0662983 | | 3 | -1.04484 | 0.445727 | 0.0140137 | | 4 | 0.217317 | -0.692044 | -0.758549 | | 5 | -0.566574 | -0.159038 | 0.781744 | | 6 | -0.903068 | 1.50697 | 1.2605 | | 7 | 1.38627 | -0.0539971 | -0.0619803 | | 8 | -1.50639 | -0.187649 | 1.00115 | | 9 | -1.84435 | -1.37282 | 0.905218 | | 10 | -0.400618 | 0.503642 | 1.20152 | | 11 | -0.164643 | 1.58712 | -0.0475168 | | 12 | 1.99742 | -0.928291 | 0.502433 | | 13 | -1.25877 | 2.0764 | -0.840626 | | 14 | -0.293273 | -2.09935 | -0.152372 | | 15 | -0.686338 | 0.798964 | 1.4551 | | 16 | 0.407321 | 0.601732 | 0.456718 | | 17 | 0.594827 | -1.8498 | 1.22704 | | 18 | -0.345245 | -1.27973 | -0.0971918 | | 19 | 1.91415 | 0.656389 | -1.15816 | | 20 | 0.149819 | 1.10037 | -0.0785313 | | 21 | -0.311471 | -1.10781 | 0.707936 | | 22 | 0.614726 | -0.142359 | -1.23091 | | 23 | 1.46869 | 1.27063 | 0.797499 | | 24 | -1.02252 | 0.819603 | -0.220382 | \nax = df.plot.box()   df.plot.pie()方法 接下来是饼图的绘制\ndf = pd.DataFrame({\u0026#39;mass\u0026#39;: [1.33, 4.87 , 5.97], \u0026#39;radius\u0026#39;: [2439.7, 6051.8, 6378.1]}, index=[\u0026#39;Mercury\u0026#39;, \u0026#39;Venus\u0026#39;, \u0026#39;Earth\u0026#39;]) plot = df.plot.pie(y=\u0026#39;mass\u0026#39;, figsize=(8, 8))   除此之外，还有折线图、直方图、散点图等等，步骤与方式都与上述的技巧有异曲同工之妙，大家感兴趣的可以自己另外去尝试。\n代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/4000字详细说明_推荐20个好用到爆的Pandas函数方法.ipynb\n src: https://mp.weixin.qq.com/s/PEhYrNbZyI9Prl3OwrBcAw\nauthor: 俊欣 公众号: 关于数据分析与可视化\n ","permalink":"/blog/4000%E5%AD%97%E8%AF%A6%E7%BB%86%E8%AF%B4%E6%98%8E_%E6%8E%A8%E8%8D%9020%E4%B8%AA%E5%A5%BD%E7%94%A8%E5%88%B0%E7%88%86%E7%9A%84pandas%E5%87%BD%E6%95%B0%E6%96%B9%E6%B3%95/","summary":"今天分享几个不为人知的pandas函数，大家可能平时看到的不多，但是使用起来倒是非常的方便，也能够帮助我们数据分析人员大幅度地提高工作效率，同时也希望大家看完之后能够有所收获\nitems()方法 pandas当中的items()方法可以用来遍历数据集当中的每一列，同时返回列名以及每一列当中的内容，通过以元组的形式，示例如下\nimport pandas as pd df = pd.DataFrame({\u0026#39;species\u0026#39;: [\u0026#39;bear\u0026#39;, \u0026#39;bear\u0026#39;, \u0026#39;marsupial\u0026#39;], \u0026#39;population\u0026#39;: [1864, 22000, 80000]}, index=[\u0026#39;panda\u0026#39;, \u0026#39;polar\u0026#39;, \u0026#39;koala\u0026#39;]) df | | species | population | |:------|:----------|-------------:| | panda | bear | 1864 | | polar | bear | 22000 | | koala | marsupial | 80000 | \nfor label, content in df.items(): print(f\u0026#39;label: {label}\u0026#39;) print(f\u0026#39;content: {content}\u0026#39;, sep=\u0026#39;\\n\u0026#39;) print(\u0026#34;=\u0026#34; * 50) label: species content: panda bear polar bear koala marsupial Name: species, dtype: object ================================================== label: population content: panda 1864 polar 22000 koala 80000 Name: population, dtype: int64 ==================================================  相继的打印出了‘species’和‘population’这两列的列名和相应的内容","title":"推荐20个好用到爆的Pandas函数方法"},{"content":"\n src: https://mp.weixin.qq.com/s/j71IPWmT57g3VTajGhgN7Q author: 俊欣 公众号: 关于数据分析与可视化\n 千聊Python课程 https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596\n本篇我们继续前面pandas系列教程的探讨，今天小编会介绍pandas库当中一些非常基础的方法与函数，希望大家看了之后会有所收获。\n准备需要的数据集 我们先准备生成一些随机数，作为后面需要用到的数据集\nimport pandas as pd import numpy as np index = pd.date_range(\u0026#34;1/1/2000\u0026#34;, periods=8) series = pd.Series(np.random.randn(5), index=[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;]) df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) \nHead and tail head()和tail()方法是用来查看数据集当中的前几行和末尾几行的，默认是查看5行，当然读者朋友也可以自行设定行数\nseries2 = pd.Series(np.random.randn(100)) series2.head() 0 0.578276 1 0.643313 2 -0.336030 3 -0.422468 4 -0.493812 dtype: float64  # 同理 series2.tail() 95 1.307962 96 1.165135 97 0.717692 98 0.605668 99 0.264990 dtype: float64  数据的统计分析 在pandas当中用describe()方法来对表格中的数据做一个概括性的统计分析，例如\nseries2.describe() count 100.000000 mean 0.106277 std 1.027541 min -2.554005 25% -0.510912 50% 0.028765 75% 0.795444 max 2.512260 dtype: float64  当然，我们也可以设置好输出的分位\nseries2.describe(percentiles=[0.05, 0.25, 0.75, 0.95]) count 100.000000 mean 0.106277 std 1.027541 min -2.554005 5% -1.450067 25% -0.510912 50% 0.028765 75% 0.795444 95% 1.757926 max 2.512260 dtype: float64  对于离散型的数据来说，describe()方法给出的结果则会简洁很多\ns = pd.Series([\u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;a\u0026#34;]) s.describe() count 10 unique 4 top a freq 5 dtype: object  要是表格中既包含了离散型数据，也包含了连续型的数据，默认的话，describe()是会针对连续型数据进行统计分析\ndf2 = pd.DataFrame({\u0026#34;a\u0026#34;: [\u0026#34;Yes\u0026#34;, \u0026#34;Yes\u0026#34;, \u0026#34;No\u0026#34;, \u0026#34;No\u0026#34;], \u0026#34;b\u0026#34;: np.random.randn(4)}) df2.describe() | | b | |:------|---------:| | count | 4 | | mean | 0.967026 | | std | 0.859657 | | min | 0.204027 | | 25% | 0.233797 | | 50% | 0.947075 | | 75% | 1.6803 | | max | 1.76993 | 当然我们也可以指定让其强制统计分析离散型数据或者连续型数据\ndf2.describe(include=[\u0026#34;object\u0026#34;]) | | a | |:-------|:----| | count | 4 | | unique | 2 | | top | Yes | | freq | 2 | 同理，我们也可以指定连续型的数据进行统计分析\ndf2.describe(include=[\u0026#34;number\u0026#34;]) | | b | |:------|---------:| | count | 4 | | mean | 0.967026 | | std | 0.859657 | | min | 0.204027 | | 25% | 0.233797 | | 50% | 0.947075 | | 75% | 1.6803 | | max | 1.76993 | 如果我们都要去做统计分析，可以这么来执行\ndf2.describe(include=\u0026#34;all\u0026#34;) | | a | b | |:-------|:----|-----------:| | count | 4 | 4 | | unique | 2 | nan | | top | Yes | nan | | freq | 2 | nan | | mean | nan | 0.967026 | | std | nan | 0.859657 | | min | nan | 0.204027 | | 25% | nan | 0.233797 | | 50% | nan | 0.947075 | | 75% | nan | 1.6803 | | max | nan | 1.76993 | \n最大/最小值的位置 idxmin()和idxmax()方法是用来查找表格当中最大/最小值的位置，返回的是值的索引\ns1 = pd.Series(np.random.randn(5)) s1 0 2.244266 1 1.398258 2 -1.827026 3 -0.058691 4 0.275471 dtype: float64  s1.idxmin(), s1.idxmax() (2, 0)  用在DataFrame上面的话，如下\ndf1 = pd.DataFrame(np.random.randn(5, 3), columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) df1.idxmin(axis=0) | | 0 | |:---|----:| | A | 2 | | B | 3 | | C | 2 | 同理，我们将axis参数改成1\ndf1.idxmin(axis=1) | | 0 | |---:|:----| | 0 | B | | 1 | B | | 2 | A | | 3 | B | | 4 | B | \nvalue_counts()方法 pandas当中的value_counts()方法主要用于数据表的计数以及排序，用来查看表格当中，指定列有多少个不同的数据值并且计算不同值在该列当中出现的次数，先来看一个简单的例子\ndf = pd.DataFrame({\u0026#39;城市\u0026#39;: [\u0026#39;北京\u0026#39;, \u0026#39;广州\u0026#39;, \u0026#39;上海\u0026#39;, \u0026#39;上海\u0026#39;, \u0026#39;杭州\u0026#39;, \u0026#39;成都\u0026#39;, \u0026#39;香港\u0026#39;, \u0026#39;南京\u0026#39;, \u0026#39;北京\u0026#39;, \u0026#39;北京\u0026#39;], \u0026#39;收入\u0026#39;: [10000, 10000, 5500, 5500, 4000, 50000, 8000, 5000, 5200, 5600], \u0026#39;年龄\u0026#39;: [50, 43, 34, 40, 25, 25, 45, 32, 25, 25]}) df[\u0026#34;城市\u0026#34;].value_counts() 北京 3 上海 2 广州 1 杭州 1 南京 1 香港 1 成都 1 Name: 城市, dtype: int64  可以看到北京出现了3次，上海出现了2次，并且默认采用的是降序来排列的，下面我们来看一下用升序的方式来排列一下收入这一列\ndf[\u0026#34;收入\u0026#34;].value_counts(ascending=True) 5600 1 5000 1 8000 1 5200 1 50000 1 4000 1 10000 2 5500 2 Name: 收入, dtype: int64  同时里面也还可以利用参数normalize=True，来计算不同值的计数占比\ndf[\u0026#39;年龄\u0026#39;].value_counts(ascending=True, normalize=True) 32 0.1 34 0.1 50 0.1 40 0.1 43 0.1 45 0.1 25 0.4 Name: 年龄, dtype: float64  数据分组 我们可以使用cut()方法以及qcut()方法来对表格中的连续型数据分组，首先我们看一下cut()方法，假设下面这组数据代表的是小组每个成员的年龄\nages = np.array([2,3,10,40,36,45,58,62,85,89,95,18,20,25,35,32]) pd.cut(ages, 5) [(1.907, 20.6], (1.907, 20.6], (1.907, 20.6], (39.2, 57.8], (20.6, 39.2], ..., (1.907, 20.6], (1.907, 20.6], (20.6, 39.2], (20.6, 39.2], (20.6, 39.2]] Length: 16 Categories (5, interval[float64]): [(1.907, 20.6] \u0026lt; (20.6, 39.2] \u0026lt; (39.2, 57.8] \u0026lt; (57.8, 76.4] \u0026lt; (76.4, 95.0]]  由上可以看到用cut()方法将数据平分成了5个区间，且区间两边都有扩展以包含最大值和最小值，当然我们也可以给每一个区间加上标记\npd.cut(ages, 5, labels=[u\u0026#34;婴儿\u0026#34;,u\u0026#34;少年\u0026#34;,u\u0026#34;青年\u0026#34;,u\u0026#34;中年\u0026#34;,u\u0026#34;老年\u0026#34;]) ['婴儿', '婴儿', '婴儿', '青年', '少年', ..., '婴儿', '婴儿', '少年', '少年', '少年'] Length: 16 Categories (5, object): ['婴儿' \u0026lt; '少年' \u0026lt; '青年' \u0026lt; '中年' \u0026lt; '老年']  而对于qcut()方法来说，我们可以指定区间来进行分组，例如\npd.qcut(ages, [0,0.5,1], labels=[\u0026#39;小朋友\u0026#39;,\u0026#39;大孩子\u0026#39;]) ['小朋友', '小朋友', '小朋友', '大孩子', '大孩子', ..., '小朋友', '小朋友', '小朋友', '小朋友', '小朋友'] Length: 16 Categories (2, object): ['小朋友' \u0026lt; '大孩子']  这里将年龄这组数据分成两部分[0, 0.5, 1]，一组是标上标记小朋友，另一组是大孩子，不过通常情况下，我们用的cut()方法比较多\n\n引用函数 要是在表格当中引用其他的方法，或者是自建的函数，可以使用通过pandas当中的以下这几个方法\n pipe() apply()和applymap() agg()和transform()  pipe()方法 首先我们来看pipe()这个方法，我们可以将自己定义好的函数，以链路的形式一个接着一个传给我们要处理的数据集上\ndef extract_city_name(df): df[\u0026#34;state_name\u0026#34;] = df[\u0026#34;state_and_code\u0026#34;].str.split(\u0026#34;,\u0026#34;).str.get(0) return df def add_country_name(df, country_name=None): df[\u0026#34;state_and_country\u0026#34;] = df[\u0026#34;state_name\u0026#34;] + country_name return df 然后我们用pip()这个方法来将上面我们定义的函数串联起来\ndf_p = pd.DataFrame({\u0026#34;city_and_code\u0026#34;: [\u0026#34;Arizona, AZ\u0026#34;]}) df_p = pd.DataFrame({\u0026#34;state_and_code\u0026#34;: [\u0026#34;Arizona, AZ\u0026#34;]}) df_p.pipe(extract_city_name).pipe(add_country_name, country_name=\u0026#34;_USA\u0026#34;) | | state_and_code | state_name | state_and_country | |---:|:-----------------|:-------------|:--------------------| | 0 | Arizona, AZ | Arizona | Arizona_USA | \napply()方法和applymap()方法 apply()方法可以对表格中的数据按照行或者是列方向进行处理，默认是按照列方向，如下\ndf.apply(np.mean) A -0.101751 B -0.360288 C -0.637433 dtype: float64 当然，我们也可以通过axis参数来进行调节\ndf.apply(np.mean, axis = 1) 0 -0.803675 1 -0.179640 2 -1.200973 3 0.156888 4 0.381631 5 0.049274 6 1.174923 7 0.612591 dtype: float64 除此之外，我们也可以直接调用匿名函数lambda的形式\ndf.apply(lambda x: x.max() - x.min()) A 1.922863 B 2.874672 C 1.943930 dtype: float64 也可以调用自己定义的函数方法\ndf = pd.DataFrame(np.random.randn(5, 3), columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) def normalize(x): return (x - x.mean()) / x.std() 我们用上apply()方法\ndf.apply(normalize) A B C 0 1.149795 0.390263 -0.813770 1 0.805843 -0.532374 0.859627 2 0.047824 -0.085334 -0.067179 3 -0.903319 -1.215023 1.149538 4 -1.100144 1.442467 -1.128216 apply()方法作用于数据集当中的每个行或者是列，而applymap()方法则是对数据集当中的所有元素都进行处理\ndf = pd.DataFrame({\u0026#39;key1\u0026#39; : [\u0026#39;a\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;d\u0026#39;], \u0026#39;key2\u0026#39; : [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;one\u0026#39;], \u0026#39;data1\u0026#39; : np.arange(1, 6), \u0026#39;data2\u0026#39; : np.arange(10,15)}) df key1 key2 data1 data2 0 a one 1 10 1 c two 2 11 2 b three 3 12 3 b four 4 13 4 d five 5 14 我们来自定义一个函数\ndef add_A(x): return \u0026#34;A\u0026#34; + str(x) df.applymap(add_A) key1 key2 data1 data2 0 Aa Aone A1 A10 1 Ac Atwo A2 A11 2 Ab Athree A3 A12 3 Ab Afour A4 A13 4 Ad Afive A5 A14 我们然后也可以通过lambda()自定义函数方法，然后来去除掉这个A\ndf.applymap(add_A).applymap(lambda x: x.split(\u0026#34;A\u0026#34;)[1]) key1 key2 data1 data2 0 a one 1 10 1 c two 2 11 2 b three 3 12 3 b four 4 13 4 d five 5 14 \nagg()方法和transform()方法 agg()方法本意上是聚合函数，我们可以将用于统计分析的一系列方法都放置其中，并且放置多个\ndf = pd.DataFrame(np.random.randn(5, 3), columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) df.agg(np.sum) A 2.042573 B 2.189269 C -1.066976 dtype: float64  当然，当中的np.sum部分也可以用字符串来表示，例如\ndf.agg(\u0026#34;sum\u0026#34;) A 2.042573 B 2.189269 C -1.066976 dtype: float64  我们尝试在当中放置多个统计分析的函数方法\ndf.agg([\u0026#34;sum\u0026#34;, \u0026#34;mean\u0026#34;, \u0026#34;median\u0026#34;]) 当然我们也可以和lambda匿名函数混合着搭配\ndf.agg([\u0026#34;sum\u0026#34;, lambda x: x.mean()]) A B C sum -0.066486 -1.288341 -1.236244 \u0026lt;lambda\u0026gt; -0.013297 -0.257668 -0.247249 或者和自己定义的函数方法混合着用\ndef my_mean(x): return x.mean() df.agg([\u0026#34;sum\u0026#34;, my_mean]) A B C sum -4.850201 -1.544773 0.429007 my_mean -0.970040 -0.308955 0.085801 与此同时，我们在agg()方法中添加字典，实现不同的列使用不同的函数方法\ndf.agg({\u0026#34;A\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;mean\u0026#34;}) A -0.801753 B 0.097550 dtype: float64 \n索引和列名的重命名 针对索引和列名的重命名，我们可以通过pandas当中的rename()方法来实现，例如我们有这样一个数据集\ndf1 = pd.DataFrame(np.random.randn(5, 3), columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;], index = [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;]) df1 A B C a 0.343690 0.869984 -1.929814 b 1.025613 0.470155 -0.242463 c -0.400908 -0.362684 0.226857 d -1.339706 -0.302005 -1.784452 e -0.957026 -0.813600 0.215098 我们可以这样来操作\ndf1.rename(columns={\u0026#34;A\u0026#34;: \u0026#34;one\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;two\u0026#34;, \u0026#34;C\u0026#34;: \u0026#34;three\u0026#34;}, index={\u0026#34;a\u0026#34;: \u0026#34;apple\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;banana\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;cat\u0026#34;}) one two three apple 0.383813 0.588964 -0.162386 banana -0.462068 -2.938896 0.935492 cat -0.059807 -1.987281 0.095432 d -0.085230 2.013733 -1.324039 e -0.678352 0.306776 0.808697 当然我们可以拆开来，单独对行或者是列进行重命名，对列的重命名可以这么来做\ndf1.rename({\u0026#34;A\u0026#34;: \u0026#34;one\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;two\u0026#34;, \u0026#34;C\u0026#34;: \u0026#34;three\u0026#34;}, axis = \u0026#34;columns\u0026#34;) one two three a -0.997108 -1.383011 0.474298 b 1.009910 0.286303 1.120783 c 1.130700 -0.566922 1.841451 d -0.350438 -0.171079 -0.079804 e 0.988050 -0.524604 0.653306 \n排序 在pandas当中，我们可以针对数据集当中的值来进行排序\ndf1 = pd.DataFrame( {\u0026#34;one\u0026#34;: [2, 1, 1, 1], \u0026#34;two\u0026#34;: [1, 3, 2, 4], \u0026#34;three\u0026#34;: [5, 4, 3, 2]} ) df1 one two three 0 2 1 5 1 1 3 4 2 1 2 3 3 1 4 2 我们按照“three”这一列当中的数值来进行排序\ndf1.sort_values(by = \u0026#34;three\u0026#34;) one two three 3 1 4 2 2 1 2 3 1 1 3 4 0 2 1 5 我们也可以依照多列进行排序\ndf1.sort_values(by = [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;]) one two three 2 1 2 3 1 1 3 4 3 1 4 2 0 2 1 5 在“one”这一列相等的时候，比较“two”这一列数值的大小，在排序的过程当中，默认采用的都是升序，我们可以改成降序来进行编排\ndf1.sort_values(\u0026#34;two\u0026#34;, ascending=False) one two three 3 1 4 2 1 1 3 4 2 1 2 3 0 2 1 5 \n数据类型的转换 最后涉及到的是数据类型的转换，在这之前，我们先得知道如何来查看数据的类型，pandas当中有相应的方法可以处理\ndf2 = pd.DataFrame( { \u0026#34;A\u0026#34;: pd.Series(np.random.randn(5), dtype=\u0026#34;float16\u0026#34;), \u0026#34;B\u0026#34;: pd.Series(np.random.randn(5)), \u0026#34;C\u0026#34;: pd.Series(np.array(np.random.randn(5), dtype=\u0026#34;uint8\u0026#34;)), } ) df2 A B C 0 -0.498779 -0.501512 0 1 -0.055817 -0.528227 254 2 -0.914551 0.763298 1 3 -0.916016 1.366833 0 4 1.993164 1.834457 0 我们通过dtypes属性来查看数据的类型\nA float16 B float64 C uint8 dtype: object 而通过astype()方法来实现数据类型的转换\ndf2[\u0026#34;B\u0026#34;].astype(\u0026#34;int64\u0026#34;) 0 0 1 0 2 0 3 2 4 1 Name: B, dtype: int64 \n根据数据类型来筛选 与此同时，我们也可以根据相对应的数据类型来进行筛选，运用pandas当中的select_dtypes方法，我们先来创建一个数据集包含了各种数据类型的\ndf = pd.DataFrame( { \u0026#34;string_1\u0026#34;: list(\u0026#34;abcde\u0026#34;), \u0026#34;int64_1\u0026#34;: list(range(1, 6)), \u0026#34;uint8_1\u0026#34;: np.arange(3, 8).astype(\u0026#34;u1\u0026#34;), \u0026#34;float64_1\u0026#34;: np.arange(4.0, 9.0), \u0026#34;bool1\u0026#34;: [True, False, True, True, False], \u0026#34;bool2\u0026#34;: [False, True, False, False, True], \u0026#34;dates_1\u0026#34;: pd.date_range(\u0026#34;now\u0026#34;, periods=5), \u0026#34;category_1\u0026#34;: pd.Series(list(\u0026#34;ABCDE\u0026#34;)).astype(\u0026#34;category\u0026#34;), } ) df string_1 int64_1 uint8_1 ... bool2 dates_1 category_1 0 a 1 3 ... False 2021-11-10 10:43:05.957685 A 1 b 2 4 ... True 2021-11-11 10:43:05.957685 B 2 c 3 5 ... False 2021-11-12 10:43:05.957685 C 3 d 4 6 ... False 2021-11-13 10:43:05.957685 D 4 e 5 7 ... True 2021-11-14 10:43:05.957685 E 我们先来查看一下各个列的数据类型\ndf.dtypes string_1 object int64_1 int64 uint8_1 uint8 float64_1 float64 bool1 bool bool2 bool dates_1 datetime64[ns] category_1 category dtype: object 我们筛选类型为布尔值的数据\ndf.select_dtypes(include=[bool]) bool1 bool2 0 True False 1 False True 2 True False 3 True False 4 False True 筛选出数据类型为整型的数据\ndf.select_dtypes(include=[\u0026#39;int64\u0026#39;]) int64_1 0 1 1 2 2 3 3 4 4 5 \n代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/pandas必知必会50例.ipynb\n","permalink":"/blog/pandas%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A50%E4%BE%8B/","summary":"src: https://mp.weixin.qq.com/s/j71IPWmT57g3VTajGhgN7Q author: 俊欣 公众号: 关于数据分析与可视化\n 千聊Python课程 https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596\n本篇我们继续前面pandas系列教程的探讨，今天小编会介绍pandas库当中一些非常基础的方法与函数，希望大家看了之后会有所收获。\n准备需要的数据集 我们先准备生成一些随机数，作为后面需要用到的数据集\nimport pandas as pd import numpy as np index = pd.date_range(\u0026#34;1/1/2000\u0026#34;, periods=8) series = pd.Series(np.random.randn(5), index=[\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;]) df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) \nHead and tail head()和tail()方法是用来查看数据集当中的前几行和末尾几行的，默认是查看5行，当然读者朋友也可以自行设定行数\nseries2 = pd.Series(np.random.randn(100)) series2.head() 0 0.578276 1 0.643313 2 -0.336030 3 -0.422468 4 -0.493812 dtype: float64  # 同理 series2.tail() 95 1.307962 96 1.","title":"推荐|pandas必知必会50例"},{"content":"spacy 产业级自然语言处理python包 https://spacy.io/\n特性  支持64+语言 针对19门语言的64流水线pipeline处理函数 多任务预训练transformers，如BERT 预训练词向量 支持命名实体识别 支持 POS词性标注 支持 句法依存 支持 文本分类 支持 词干化 内置可视化  spacy安装 pip install spacy==3.2.0 \n模型下载安装 sm小型/ md中型/ lg大型\n  中文模型3.2.0版\n zh_core_web_sm https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.2.0/zh_core_web_sm-3.2.0-py3-none-any.whl zh_core_web_md https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.2.0/zh_core_web_md-3.2.0-py3-none-any.whl zh_core_web_lg https://github.com/explosion/spacy-models/releases/download/zh_core_web_lg-3.2.0/zh_core_web_lg-3.2.0-py3-none-any.whl    英文模型3.2.0版\n en_core_web_sm https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl en_core_web_md https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl en_core_web_lg https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl    注意： 模型大小的区别主要体现在词向量维度数的差距，模型越大， 词向量的维度越多。\nDoc类型  doc.lang_ doc的语言 doc.text doc的文本 doc.ents 文本中的实体词  import spacy #使用中文zh_core_web_sm模型 zh_nlp = spacy.load(\u0026#34;zh_core_web_sm\u0026#34;) test1 = \u0026#39;国家管网微信公众号11月13日消息，11月12日，国家管网集团首期绿色超短期融资券在全国银行间市场成功发行。此次债券发行是国家管网集团成立后首次在公开市场亮相，由工商银行独家承销，发行金额60亿元、期限270天，为本年度单笔最大金额绿色超短期融资券，募集资金将全部用于储气库等绿色低碳天然气储运基础设施建设；发行票面利率2.41%，认购总规模达2.53倍，低于资本市场同期可比产品利率超10个BP，反映了市场对绿色基础设施建设项目的青睐。\u0026#39; doc1 = zh_nlp(test1) doc1 国家管网微信公众号11月13日消息，11月12日，国家管网集团首期绿色超短期融资券在全国银行间市场成功发行。此次债券发行是国家管网集团成立后首次在公开市场亮相，由工商银行独家承销，发行金额60亿元、期限270天，为本年度单笔最大金额绿色超短期融资券，募集资金将全部用于储气库等绿色低碳天然气储运基础设施建设；发行票面利率2.41%，认购总规模达2.53倍，低于资本市场同期可比产品利率超10个BP，反映了市场对绿色基础设施建设项目的青睐。  doc1.lang_ 'zh'  doc1.text '国家管网微信公众号11月13日消息，11月12日，国家管网集团首期绿色超短期融资券在全国银行间市场成功发行。此次债券发行是国家管网集团成立后首次在公开市场亮相，由工商银行独家承销，发行金额60亿元、期限270天，为本年度单笔最大金额绿色超短期融资券，募集资金将全部用于储气库等绿色低碳天然气储运基础设施建设；发行票面利率2.41%，认购总规模达2.53倍，低于资本市场同期可比产品利率超10个BP，反映了市场对绿色基础设施建设项目的青睐。'  doc1.vector array([-1.81135774e-01, 2.31929451e-01, 1.45746097e-01, 6.82696044e-01, -8.44623148e-03, -2.21295916e-02, 4.06811416e-01, -4.60287899e-01, -5.73987663e-01, -1.33687481e-01, -5.34314513e-01, -6.64901555e-01, -3.94947737e-01, 6.35875063e-03, -2.03339502e-01, 5.78875951e-02, -3.34325433e-01, -3.77648622e-01, 2.43863747e-01, -5.56892566e-02, -7.30801523e-01, -2.41785884e-01, -4.50579911e-01, -3.13598923e-02, 9.07084942e-02, -8.06667805e-01, 7.28501499e-01, -8.59559357e-01, -4.44110222e-02, 9.64611948e-01, -2.57230818e-01, 1.09481342e-01, -3.73580456e-01, -8.51007993e-04, 5.30374162e-02, -5.51876485e-01, -4.82654065e-01, 2.68822908e-01, -4.20012563e-01, 4.33068752e-01, -5.14427841e-01, 5.53584039e-01, -2.00293139e-02, 9.45062563e-02, 1.04523234e-01, 1.34134221e+00, -5.23905218e-01, 1.31230903e+00, 3.28943968e-01, 3.39987069e-01, 8.26785386e-01, 5.35273492e-01, -4.27510649e-01, -1.02807179e-01, -1.91500232e-01, 2.63696283e-01, 6.33961499e-01, -5.65908328e-02, -1.94336250e-01, -5.89190602e-01, 2.22078279e-01, 3.41992415e-02, 5.37312031e-01, 2.77926654e-01, -3.00608397e-01, -6.42910838e-01, -1.33188680e-01, 2.82793492e-01, 6.25911206e-02, 2.08833948e-01, 2.69211121e-02, 1.65822819e-01, -4.32190485e-02, -6.67634964e-01, 6.50937319e-01, -2.43003711e-01, 9.57057327e-02, -3.56370257e-03, -1.13566548e-01, -1.65319979e-01, 7.40000159e-02, 3.65676880e-01, -2.21356809e-01, 2.03256473e-01, 2.26293072e-01, 3.11525285e-01, 3.37869138e-01, -3.12896192e-01, 5.31899095e-01, -1.86223835e-01, -6.03411011e-02, 4.97923464e-01, 3.10418844e-01, -2.48594299e-01, -3.67455184e-01, -4.46804255e-01], dtype=float32)  #doc1中的实体词 doc1.ents (11月13日, 11月12日, 国家管网集团, 全国银行, 国家管网集团, 工商银行, 60亿元, 270天, 2, 2, 53, 超10)  #doc1中的实体词类别 [ent.label_ for ent in doc1.ents] ['DATE', 'DATE', 'ORG', 'ORG', 'ORG', 'ORG', 'MONEY', 'DATE', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'CARDINAL']  Token类型  token.text 文本 token.pos_ 词性  for token in doc1: print(token.text, \u0026#39; \u0026#39;, token.pos_) 国家 NOUN 管网 NOUN 微信 ADJ 公众号 NOUN 11月 NOUN 13日 NOUN 消息 NOUN ， PUNCT 11月 NOUN 12日 NOUN ， PUNCT 国家 NOUN 管网 NOUN 集团 NOUN 首期 ADV 绿色 VERB 超短 NOUN 期融 NOUN 资券 VERB 在 ADP 全国 ADJ 银行 NOUN 间 PART 市场 NOUN 成功 ADV 发行 VERB 。 PUNCT 此次 DET 债券 NOUN 发行 VERB 是 VERB 国家 NOUN 管网 NOUN 集团 NOUN 成立 VERB 后 PART 首次 ADV 在 ADP 公开 ADJ 市场 NOUN 亮相 VERB ， PUNCT 由 ADP 工商 NOUN 银行 NOUN 独家 ADV 承销 VERB ， PUNCT 发行 NOUN 金额 NOUN 60亿 NUM 元 NUM 、 PUNCT 期限 NOUN 270 NUM 天 NUM ， PUNCT 为 ADP 本 DET 年度 NOUN 单笔 NOUN 最 ADV 大 ADJ 金额 NOUN 绿色 ADJ 超短 NOUN 期融 NOUN 资券 NOUN ， PUNCT 募集 NOUN 资金 NOUN 将 ADV 全部 ADV 用于 VERB 储气库 NOUN 等 PART 绿色 ADJ 低碳 VERB 天然气 NOUN 储运 NOUN 基础 NOUN 设施 NOUN 建设 NOUN ； PUNCT 发行 VERB 票面 ADJ 利率 NOUN 2 NUM . PUNCT 41% NOUN ， PUNCT 认购 NOUN 总 ADJ 规模 NOUN 达 VERB 2 NUM . PUNCT 53 NUM 倍 NUM ， PUNCT 低于 VERB 资本 NOUN 市场 NOUN 同期 NOUN 可比 ADV 产品 NOUN 利率 NOUN 超10 VERB 个 NUM BP NOUN ， PUNCT 反映 VERB 了 PART 市场 NOUN 对 ADP 绿色 ADJ 基础 NOUN 设施 NOUN 建设 NOUN 项目 NOUN 的 PART 青睐 NOUN 。 PUNCT  代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/20211115spacy产业级自然语言处理包.ipynb\n","permalink":"/blog/spacy%E4%BA%A7%E4%B8%9A%E7%BA%A7%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%8C%85/","summary":"spacy 产业级自然语言处理python包 https://spacy.io/\n特性  支持64+语言 针对19门语言的64流水线pipeline处理函数 多任务预训练transformers，如BERT 预训练词向量 支持命名实体识别 支持 POS词性标注 支持 句法依存 支持 文本分类 支持 词干化 内置可视化  spacy安装 pip install spacy==3.2.0 \n模型下载安装 sm小型/ md中型/ lg大型\n  中文模型3.2.0版\n zh_core_web_sm https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.2.0/zh_core_web_sm-3.2.0-py3-none-any.whl zh_core_web_md https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.2.0/zh_core_web_md-3.2.0-py3-none-any.whl zh_core_web_lg https://github.com/explosion/spacy-models/releases/download/zh_core_web_lg-3.2.0/zh_core_web_lg-3.2.0-py3-none-any.whl    英文模型3.2.0版\n en_core_web_sm https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl en_core_web_md https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl en_core_web_lg https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl    注意： 模型大小的区别主要体现在词向量维度数的差距，模型越大， 词向量的维度越多。\nDoc类型  doc.lang_ doc的语言 doc.text doc的文本 doc.ents 文本中的实体词  import spacy #使用中文zh_core_web_sm模型 zh_nlp = spacy.","title":"spacy产业级自然语言处理包"},{"content":"1. 酒精消费 此可视化显示了 2001 年至 2018 年人均（人均）饮酒量最高的国家。有趣的是，这段时间的最高国家主要由东非和欧洲国家组成。\n如果您想了解更多关于全球酒精消费的信息，请查看牛津关于全球酒精消费的报告\nhttps://ourworldindata.org/alcohol-consumptio\n以防万一您感兴趣，这种类型的数据可视化称为条形图竞赛。 我相信你已经在 YouTube 和 Reddit 上看到了很多这样的内容。 如果您想自己构建一个，这里有一个教程，您可以查看\nhttps://towardsdatascience.com/step-by-step-tutorial-create-a-bar-chart-race-animation-da7d5fcd7079\n2. 健康\u0026amp;财富的221年 通常少即是多，但这是我最喜欢的可视化，因为它以清晰的方式传达了如此多的信息，而且非常积极！\n此可视化显示了每个国家的财富（人均 GDP）和健康（平均预期寿命）如何随时间变化。 圆圈的大小代表每个国家的人口，颜色代表每个国家所属的大陆。\n看到我们作为一个物种走了多远真是太神奇了，是吧？\nhttps://www.reddit.com/r/dataisbeautiful/comments/lmlrks/oc_our_health_and_wealth_over_221_years/\n3. 地球光纤电缆网络 3D 地图 这个由 Tyler Morgan 创建的可视化是世界光缆网络的 3D 地图。 该网络用于传输电话信号、互联网通信和电视信号。\n真正看到我们在全球范围内的相互联系是非常疯狂的，不是吗？\n这是使用 rayrender 和 geojsonsf 包在 R 中创建的。 如果您想查看完整代码，可以在此处查看。\nhttps://gist.github.com/tylermorganwall/b222fcebcac3de56a6e144d73d166322\n4. 美国Covid病例增长 我不是特别喜欢花哨的数据可视化，因为它们通常不像简单的图形（如折线图）那样有效地传达信息。 但是，因为这个动画非常独特（而且有点令人不安），我觉得有必要将它添加到前 10 个可视化效果中。\n虽然没有轴可以告诉我们绝对数字，但它是一个简洁的可视化，向我们展示了与 2020 年年初相比，COVID 病例数的增长速度。\n这是使用 d3 创建的，完整代码可以在这里找到。\nhttps://observablehq.com/@bagami/the-us-covid-syringe\n5. 美国 COVID 等值线图 此可视化告诉我们，从 2020 年 2 月开始到 2021 年 10 月，美国的 COVID 病例是如何增长的。有趣的是，您可以清楚地看到这一时期 COVID 的“波浪”。\n这种类型的可视化被称为等值线图，它在比较不同地区（州、国家、大陆等）的特定变量随着时间的推移非常强大。\n如果你想学习如何用 Python 构建一个，我在这里写了一份创建 Choropleth 地图的分步指南。\nhttps://towardsdatascience.com/visualizing-the-coronavirus-pandemic-with-choropleth-maps-7f30fccaecf5\n6. 所有精神疾病的地图 此可视化显示了 DSM-5 中的每一种精神障碍，它代表精神障碍诊断和统计手册。 它是美国精神障碍的标准分类。 有 20 多个类别和数百种疾病，一旦您开始了解它，这种可视化可能会花费比您想象的更多的时间。\nhttps://www.reddit.com/r/dataisbeautiful/comments/kugn7e/oc_every_mental_disorder_diagnosis_in_the_dsm5/\nhttps://webcache.googleusercontent.com/search?q=cache:LY74prf8a0gJ:https://www.psychiatry.org/File%2520Library/Psychiatrists/Practice/DSM/APA_DSM-5-Contents.pdf+\u0026amp;cd=1\u0026amp;hl=en\u0026amp;ct=clnk\u0026amp;gl=us\u0026amp;client=safari\n7. 我们的塑料去哪儿了？ 该动画展示了塑料的生命周期，以及其中大部分的结束位置。 可悲的是，很明显，我们没有像我们应该回收的那样回收尽可能多的塑料，惊人的 60% 最终被填埋或进入海洋。\n这种类型的可视化是一个动画桑基图，它类似于树图，因为它将数据分解为几个子组并按比例表示值。\n如果您想学习如何在 Python 中构建 Sankey 图，请查看此链接。\nhttps://towardsdatascience.com/sankey-diagram-basics-with-pythons-plotly-7a13d557401a\n8. 近60年来，Top 100 艺术家 最后，这个可视化显示了 Billboard 1960 年到 2020 年的前 100 位艺术家。我个人喜欢这个图表，因为它提供了很多信息：你可以看到顶级艺术家是谁，他们什么时候最流行，以及他们有多少歌曲 称霸排行榜！\n你认识多少艺术家？\n","permalink":"/blog/2021%E6%9C%80%E6%B5%81%E8%A1%8C%E7%9A%848%E5%BC%A0%E5%8F%AF%E8%A7%86%E5%8C%96%E5%9B%BE/","summary":"1. 酒精消费 此可视化显示了 2001 年至 2018 年人均（人均）饮酒量最高的国家。有趣的是，这段时间的最高国家主要由东非和欧洲国家组成。\n如果您想了解更多关于全球酒精消费的信息，请查看牛津关于全球酒精消费的报告\nhttps://ourworldindata.org/alcohol-consumptio\n以防万一您感兴趣，这种类型的数据可视化称为条形图竞赛。 我相信你已经在 YouTube 和 Reddit 上看到了很多这样的内容。 如果您想自己构建一个，这里有一个教程，您可以查看\nhttps://towardsdatascience.com/step-by-step-tutorial-create-a-bar-chart-race-animation-da7d5fcd7079\n2. 健康\u0026amp;财富的221年 通常少即是多，但这是我最喜欢的可视化，因为它以清晰的方式传达了如此多的信息，而且非常积极！\n此可视化显示了每个国家的财富（人均 GDP）和健康（平均预期寿命）如何随时间变化。 圆圈的大小代表每个国家的人口，颜色代表每个国家所属的大陆。\n看到我们作为一个物种走了多远真是太神奇了，是吧？\nhttps://www.reddit.com/r/dataisbeautiful/comments/lmlrks/oc_our_health_and_wealth_over_221_years/\n3. 地球光纤电缆网络 3D 地图 这个由 Tyler Morgan 创建的可视化是世界光缆网络的 3D 地图。 该网络用于传输电话信号、互联网通信和电视信号。\n真正看到我们在全球范围内的相互联系是非常疯狂的，不是吗？\n这是使用 rayrender 和 geojsonsf 包在 R 中创建的。 如果您想查看完整代码，可以在此处查看。\nhttps://gist.github.com/tylermorganwall/b222fcebcac3de56a6e144d73d166322\n4. 美国Covid病例增长 我不是特别喜欢花哨的数据可视化，因为它们通常不像简单的图形（如折线图）那样有效地传达信息。 但是，因为这个动画非常独特（而且有点令人不安），我觉得有必要将它添加到前 10 个可视化效果中。\n虽然没有轴可以告诉我们绝对数字，但它是一个简洁的可视化，向我们展示了与 2020 年年初相比，COVID 病例数的增长速度。\n这是使用 d3 创建的，完整代码可以在这里找到。\nhttps://observablehq.com/@bagami/the-us-covid-syringe\n5. 美国 COVID 等值线图 此可视化告诉我们，从 2020 年 2 月开始到 2021 年 10 月，美国的 COVID 病例是如何增长的。有趣的是，您可以清楚地看到这一时期 COVID 的“波浪”。","title":"2021最流行的8张可视化图"},{"content":"cntext 中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等\n github地址 https://github.com/hidadeng/cntext pypi地址 https://pypi.org/project/cntext/ 视频课-Python网络爬虫与文本数据分析  功能模块含\n cntext stats 文本统计,可读性等 dictionary 构建词表(典) sentiment 情感分析 similarity 文本相似度 visualization 可视化，如词云图  安装 pip install cntext==0.9 \n一、cntext 查看cntext基本信息\nimport cntext help(cntext) Run\nHelp on package cntext: NAME cntext PACKAGE CONTENTS description (package) dictionary (package) sentiment (package) similarity (package) visualization (package) DATA ADV_words = [\u0026#39;都\u0026#39;, \u0026#39;全\u0026#39;, \u0026#39;单\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;光\u0026#39;, \u0026#39;尽\u0026#39;, \u0026#39;净\u0026#39;, \u0026#39;仅\u0026#39;, \u0026#39;就\u0026#39;, \u0026#39;只\u0026#39;, \u0026#39;一共\u0026#39;, \u0026#39;... CONJ_words = [\u0026#39;乃\u0026#39;, \u0026#39;乍\u0026#39;, \u0026#39;与\u0026#39;, \u0026#39;无\u0026#39;, \u0026#39;且\u0026#39;, \u0026#39;丕\u0026#39;, \u0026#39;为\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;其\u0026#39;, \u0026#39;况\u0026#39;, \u0026#39;厥\u0026#39;, \u0026#39;... DUTIR_Ais = {\u0026#39;sigh\u0026#39;, \u0026#39;一命呜呼\u0026#39;, \u0026#39;一场春梦\u0026#39;, \u0026#39;一场空\u0026#39;, \u0026#39;一头跌在菜刀上－切肤之痛\u0026#39;, \u0026#39;一念之差\u0026#39;, ..... DUTIR_Haos = {\u0026#39;1兒巴经\u0026#39;, \u0026#39;3x\u0026#39;, \u0026#39;8错\u0026#39;, \u0026#39;BUCUO\u0026#39;, \u0026#39;Cool毙\u0026#39;, \u0026#39;NB\u0026#39;, ...} DUTIR_Jings = {\u0026#39;848\u0026#39;, \u0026#39;FT\u0026#39;, \u0026#39;_god\u0026#39;, \u0026#39;yun\u0026#39;, \u0026#39;一个骰子掷七点－出乎意料\u0026#39;, \u0026#39;一举成名\u0026#39;, ...... DUTIR_Jus = {\u0026#39;一则以喜，一则以惧\u0026#39;, \u0026#39;一发千钧\u0026#39;, \u0026#39;一年被蛇咬，三年怕草索\u0026#39;, \u0026#39;一座皆惊\u0026#39;, \u0026#39;一脸横肉\u0026#39;, \u0026#39;一蛇两头... DUTIR_Les = {\u0026#39;:)\u0026#39;, \u0026#39;CC\u0026#39;, \u0026#39;Happy\u0026#39;, \u0026#39;LOL\u0026#39;, \u0026#39;_so\u0026#39;, \u0026#39;haha\u0026#39;, ...} DUTIR_Nus = {\u0026#39;2气斗狠\u0026#39;, \u0026#39;MD\u0026#39;, \u0026#39;TNND\u0026#39;, \u0026#39;gun\u0026#39;, \u0026#39;kao\u0026#39;, \u0026#39;一刀两断\u0026#39;, ...} DUTIR_Wus = {\u0026#39;B4\u0026#39;, \u0026#39;BD\u0026#39;, \u0026#39;BS\u0026#39;, \u0026#39;HC\u0026#39;, \u0026#39;HJ\u0026#39;, \u0026#39;JJWW\u0026#39;, ...} HOWNET_deny = {\u0026#39;不\u0026#39;, \u0026#39;不可\u0026#39;, \u0026#39;不是\u0026#39;, \u0026#39;不能\u0026#39;, \u0026#39;不要\u0026#39;, \u0026#39;休\u0026#39;, ...} HOWNET_extreme = {\u0026#39;万\u0026#39;, \u0026#39;万万\u0026#39;, \u0026#39;万分\u0026#39;, \u0026#39;万般\u0026#39;, \u0026#39;不亦乐乎\u0026#39;, \u0026#39;不可开交\u0026#39;, ...} HOWNET_ish = {\u0026#39;一些\u0026#39;, \u0026#39;一点\u0026#39;, \u0026#39;一点儿\u0026#39;, \u0026#39;不丁点儿\u0026#39;, \u0026#39;不大\u0026#39;, \u0026#39;不怎么\u0026#39;, ...} HOWNET_more = {\u0026#39;多\u0026#39;, \u0026#39;大不了\u0026#39;, \u0026#39;如斯\u0026#39;, \u0026#39;尤甚\u0026#39;, \u0026#39;强\u0026#39;, \u0026#39;愈\u0026#39;, ...} HOWNET_neg = {\u0026#39;一下子爆发\u0026#39;, \u0026#39;一下子爆发的一连串\u0026#39;, \u0026#39;一不小心\u0026#39;, \u0026#39;一个屁\u0026#39;, \u0026#39;一仍旧贯\u0026#39;, \u0026#39;一偏\u0026#39;, ...} HOWNET_pos = {\u0026#39;\u0026#39;, \u0026#39;一专多能\u0026#39;, \u0026#39;一丝不差\u0026#39;, \u0026#39;一丝不苟\u0026#39;, \u0026#39;一个心眼儿\u0026#39;, \u0026#39;一五一十\u0026#39;, ...} HOWNET_very = {\u0026#39;不为过\u0026#39;, \u0026#39;不少\u0026#39;, \u0026#39;不胜\u0026#39;, \u0026#39;不过\u0026#39;, \u0026#39;何啻\u0026#39;, \u0026#39;何止\u0026#39;, ...} STOPWORDS_en = {\u0026#39;a\u0026#39;, \u0026#39;about\u0026#39;, \u0026#39;above\u0026#39;, \u0026#39;across\u0026#39;, \u0026#39;after\u0026#39;, \u0026#39;afterwards\u0026#39;... STOPWORDS_zh = {\u0026#39;、\u0026#39;, \u0026#39;。\u0026#39;, \u0026#39;〈\u0026#39;, \u0026#39;〉\u0026#39;, \u0026#39;《\u0026#39;, \u0026#39;》\u0026#39;, ...} FILE /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/cntext/__init__.py \nfrom cntext import dict_info dict_info() Run\n【大连理工大学情感本体库】 七大情绪分类，依次是哀、恶、好、惊、惧、乐、怒；对应的情绪词表依次： DUTIR_Ais = {\u0026#34;泣血捶膺\u0026#34;, \u0026#34;望断白云\u0026#34;, \u0026#34;日暮途穷\u0026#34;, \u0026#34;身微力薄\u0026#34;...} DUTIR_Wus = {\u0026#34;饰非遂过\u0026#34;, \u0026#34;恶语\u0026#34;, \u0026#34;毁害\u0026#34;, \u0026#34;恶籍盈指\u0026#34;, \u0026#34;脾气爆躁\u0026#34;, \u0026#34;淫贱\u0026#34;, \u0026#34;凌乱\u0026#34;...} DUTIR_Haos = {\u0026#34;打破砂锅璺到底\u0026#34;, \u0026#34;多彩\u0026#34;, \u0026#34;披沙拣金\u0026#34;, \u0026#34;见机行事\u0026#34;, \u0026#34;精神饱满\u0026#34;...} DUTIR_Jings = {\u0026#34;骇人视听\u0026#34;, \u0026#34;拍案惊奇\u0026#34;, \u0026#34;悬念\u0026#34;, \u0026#34;无翼而飞\u0026#34;, \u0026#34;原来\u0026#34;, \u0026#34;冷门\u0026#34;...} DUTIR_Jus ={\u0026#34;山摇地动\u0026#34;, \u0026#34;月黑风高\u0026#34;, \u0026#34;流血\u0026#34;, \u0026#34;老鼠偷猫饭－心惊肉跳\u0026#34;, \u0026#34;一发千钧\u0026#34;...} DUTIR_Les ={\u0026#34;含哺鼓腹\u0026#34;, \u0026#34;欢呼鼓舞\u0026#34;, \u0026#34;莺歌蝶舞\u0026#34;, \u0026#34;将伯之助\u0026#34;, \u0026#34;逸兴横飞\u0026#34;, \u0026#34;舒畅\u0026#34;...} DUTIR_Nus = {\u0026#34;怨气满腹\u0026#34;, \u0026#34;面有愠色\u0026#34;, \u0026#34;愤愤\u0026#34;, \u0026#34;直眉瞪眼\u0026#34;, \u0026#34;负气斗狠\u0026#34;, \u0026#34;挑眼\u0026#34;...} 【知网Hownet词典】 含正负形容词、否定词、副词等词表，对应的词表依次: HOWNET_deny = {\u0026#34;不\u0026#34;, \u0026#34;不是\u0026#34;, \u0026#34;不能\u0026#34;, \u0026#34;不可\u0026#34;...} HOWNET_extreme = {\u0026#34;百分之百\u0026#34;, \u0026#34;倍加\u0026#34;, \u0026#34;备至\u0026#34;, \u0026#34;不得了\u0026#34;...} HOWNET_ish = {\u0026#34;点点滴滴\u0026#34;, \u0026#34;多多少少\u0026#34;, \u0026#34;怪\u0026#34;, \u0026#34;好生\u0026#34;, \u0026#34;还\u0026#34;, \u0026#34;或多或少\u0026#34;...} HOWNET_more = {\u0026#34;大不了\u0026#34;, \u0026#34;多\u0026#34;, \u0026#34;更\u0026#34;, \u0026#34;比较\u0026#34;, \u0026#34;更加\u0026#34;, \u0026#34;更进一步\u0026#34;, \u0026#34;更为\u0026#34;, \u0026#34;还\u0026#34;, \u0026#34;还要\u0026#34;...} HOWNET_neg = {\u0026#34;压坏\u0026#34;, \u0026#34;鲁莽的\u0026#34;, \u0026#34;被控犯罪\u0026#34;, \u0026#34;银根紧\u0026#34;, \u0026#34;警惕的\u0026#34;, \u0026#34;残缺\u0026#34;, \u0026#34;致污物\u0026#34;, \u0026#34;柔弱\u0026#34;...} HOWNET_pos = {\u0026#34;无误\u0026#34;, \u0026#34;感激不尽\u0026#34;, \u0026#34;受大众欢迎\u0026#34;, \u0026#34;敬礼\u0026#34;, \u0026#34;文雅\u0026#34;, \u0026#34;一尘不染\u0026#34;, \u0026#34;高精度\u0026#34;, \u0026#34;兴盛\u0026#34;...} HOWNET_very = {\u0026#34;不为过\u0026#34;, \u0026#34;超\u0026#34;, \u0026#34;超额\u0026#34;, \u0026#34;超外差\u0026#34;, \u0026#34;超微结构\u0026#34;, \u0026#34;超物质\u0026#34;, \u0026#34;出头\u0026#34;...} 【停用词表】 中英文停用词表，依次 STOPWORDS_zh = {\u0026#34;经\u0026#34;, \u0026#34;得\u0026#34;, \u0026#34;则甚\u0026#34;, \u0026#34;跟\u0026#34;, \u0026#34;好\u0026#34;, \u0026#34;具体地说\u0026#34;...} STOPWORDS_en = {\u0026#39;a\u0026#39;, \u0026#39;about\u0026#39;, \u0026#39;above\u0026#39;, \u0026#39;across\u0026#39;, \u0026#39;after\u0026#39;...} 【中文副词/连词】 副词ADV、连词CONJ ADV_words = [\u0026#39;都\u0026#39;, \u0026#39;全\u0026#39;, \u0026#39;单\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;光\u0026#39;...} CONJ_words = [\u0026#39;乃\u0026#39;, \u0026#39;乍\u0026#39;, \u0026#39;与\u0026#39;, \u0026#39;无\u0026#39;, \u0026#39;且\u0026#39;...} \n查看词表\nfrom cntext import CONJ_words, ADV_words #获取连词词表 CONJ_words Run\n[\u0026#39;乃\u0026#39;, \u0026#39;乍\u0026#39;, \u0026#39;与\u0026#39;, \u0026#39;无\u0026#39;, \u0026#39;且\u0026#39;, \u0026#39;丕\u0026#39;, \u0026#39;为\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;其\u0026#39;, \u0026#39;况\u0026#39;, \u0026#39;厥\u0026#39;, \u0026#39;则\u0026#39;, \u0026#39;那\u0026#39;, \u0026#39;兼\u0026#39;, ... ] \n二、stats 目前含\n term_freq 词频统计函数，返回Counter类型 readability 中文可读性  from cntext.stats import term_freq, readability text = \u0026#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更\u0026#39; term_freq(text) Counter({\u0026#39;看待\u0026#39;: 1, \u0026#39;网文\u0026#39;: 1, \u0026#39;作者\u0026#39;: 1, \u0026#39;黑客\u0026#39;: 1, \u0026#39;大佬\u0026#39;: 1, \u0026#39;盗号\u0026#39;: 1, \u0026#39;改文因\u0026#39;: 1, \u0026#39;万分\u0026#39;: 1, \u0026#39;惭愧\u0026#39;: 1, \u0026#39;停\u0026#39;: 1}) \n**中文可读性 ** 算法参考自\n 徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.\n  readability1 \u0026mdash;每个分句中的平均字数 readability2 \u0026mdash;每个句子中副词和连词所占的比例 readability3 \u0026mdash;参考Fog Index， readability3=(readability1+readability2)×0.5  以上三个指标越大，都说明文本的复杂程度越高，可读性越差。\nreadability(text) {\u0026#39;readability1\u0026#39;: 27.0, \u0026#39;readability2\u0026#39;: 0.17647058823529413, \u0026#39;readability3\u0026#39;: 13.588235294117647} \n三、dictionary 本模块用于构建词表(典),含\n SoPmi 共现法扩充词表(典) W2VModels 词向量word2vec扩充词表(典)  3.1 SoPmi 共现法 from cntext.dictionary import SoPmi import os sopmier = SoPmi(cwd=os.getcwd(), input_txt_file=\u0026#39;data/sopmi_corpus.txt\u0026#39;, #原始数据，您的语料 seedword_txt_file=\u0026#39;data/sopmi_seed_words.txt\u0026#39;, #人工标注的初始种子词 ) sopmier.sopmi() Run\nstep 1/4:...seg corpus ... Loading model cost 0.678 seconds. Prefix dict has been built successfully. step 1/4 finished:...cost 60.78995203971863... step 2/4:...collect cowords ... step 2/4 finished:...cost 0.6169600486755371... step 3/4:...compute sopmi ... step 1/4 finished:...cost 0.26422882080078125... step 4/4:...save candiwords ... finished! cost 61.8965539932251 \n3.2 W2VModels 词向量 from cntext.dictionary import W2VModels import os #初始化模型 model = W2VModels(cwd=os.getcwd()) #语料数据 w2v_corpus.txt model.train(input_txt_file=\u0026#39;data/w2v_corpus.txt\u0026#39;) #根据种子词，筛选出没类词最相近的前100个词 model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/integrity.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/innovation.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/quality.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/respect.txt\u0026#39;, topn=100) model.find(seedword_txt_file=\u0026#39;data/w2v_seeds/teamwork.txt\u0026#39;, topn=100) Run\n数据预处理开始....... 预处理结束........... Word2Vec模型训练开始...... 已将模型存入 /Users/Desktop/cntext/test/output/w2v_candi_words/w2v.model 准备寻找每个seed在语料中所有的相似候选词 初步搜寻到 572 个相似的候选词 计算每个候选词 与 integrity 的相似度， 选出相似度最高的前 100 个候选词 已完成 【integrity 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/integrity.txt， 耗时 46 秒 准备寻找每个seed在语料中所有的相似候选词 初步搜寻到 516 个相似的候选词 计算每个候选词 与 innovation 的相似度， 选出相似度最高的前 100 个候选词 已完成 【innovation 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/innovation.txt， 耗时 46 秒 准备寻找每个seed在语料中所有的相似候选词 初步搜寻到 234 个相似的候选词 计算每个候选词 与 quality 的相似度， 选出相似度最高的前 100 个候选词 已完成 【quality 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/quality.txt， 耗时 46 秒 准备寻找每个seed在语料中所有的相似候选词 初步搜寻到 243 个相似的候选词 计算每个候选词 与 respect 的相似度， 选出相似度最高的前 100 个候选词 已完成 【respect 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/respect.txt， 耗时 46 秒 准备寻找每个seed在语料中所有的相似候选词 初步搜寻到 319 个相似的候选词 计算每个候选词 与 teamwork 的相似度， 选出相似度最高的前 100 个候选词 已完成 【teamwork 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/teamwork.txt， 耗时 46 秒 \n四、 sentiment  senti_by_hownet 使用知网Hownet词典对文本进行情感分析 senti_by_dutir 使用大连理工大学情感本体库dutir对文本进行情绪分析 senti_by_diydict 使用自定义词典 对文本进行情感分析  4.1 senti_by_hownet(text, adj_adv=False) 使用知网Hownet词典进行(中)文本数据的情感分析，统计正、负情感信息出现次数(得分)\n text: 待分析的中文文本数据 adj_adv: 是否考虑副词（否定词、程度词）对情绪形容词的反转和情感强度修饰作用，默认False。默认False只统计情感形容词出现个数；  from cntext.sentiment import senti_by_hownet text = \u0026#39;今天股票大涨，心情倍爽，非常开心啊。\u0026#39; senti_by_dutir(text) Run\n{\u0026#39;word_num\u0026#39;: 12, \u0026#39;sentence_num\u0026#39;: 2, \u0026#39;stopword_num\u0026#39;: 4, \u0026#39;好_num\u0026#39;: 0, \u0026#39;乐_num\u0026#39;: 1, \u0026#39;哀_num\u0026#39;: 0, \u0026#39;怒_num\u0026#39;: 0, \u0026#39;惧_num\u0026#39;: 0, \u0026#39;恶_num\u0026#39;: 0, \u0026#39;惊_num\u0026#39;: 0} \n考虑副词（否定词、程度词）对情绪形容词的反转和情感强度修饰作用\nsenti_by_hownet(text, adj_adv=True) Run\n{\u0026#39;sentence_num\u0026#39;: 1, \u0026#39;word_num\u0026#39;: 12, \u0026#39;stopword_num\u0026#39;: 3, \u0026#39;pos_score\u0026#39;: 13.0, \u0026#39;neg_score\u0026#39;: 0.0} \n4.2 senti_by_dutir(text) 使用大连理工大学情感本体库对文本进行情绪分析，统计各情绪词语出现次数。\nfrom cntext.sentiment import senti_by_dutir text = \u0026#39;今天股票大涨，心情倍爽，非常开心啊。\u0026#39; senti_by_dutir(text) Run\n{\u0026#39;word_num\u0026#39;: 12, \u0026#39;sentence_num\u0026#39;: 2, \u0026#39;stopword_num\u0026#39;: 4, \u0026#39;好_num\u0026#39;: 0, \u0026#39;乐_num\u0026#39;: 1, \u0026#39;哀_num\u0026#39;: 0, \u0026#39;怒_num\u0026#39;: 0, \u0026#39;惧_num\u0026#39;: 0, \u0026#39;恶_num\u0026#39;: 0, \u0026#39;惊_num\u0026#39;: 0}  情绪分析使用的大连理工大学情感本体库，如发表论文，请注意用户许可协议\n如果用户使用该资源发表论文或取得科研成果，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。\n参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”\n \n4.3 senti_by_diy(text) 使用diy词典进行情感分析，计算各个情绪词出现次数，未考虑强度副词、否定词对情感的复杂影响，\n text: 待分析中文文本 sentiwords: 情感词字典； {\u0026lsquo;category1\u0026rsquo;: \u0026lsquo;category1 词语列表\u0026rsquo;, \u0026lsquo;category2\u0026rsquo;: \u0026lsquo;category2词语列表\u0026rsquo;, \u0026lsquo;category3\u0026rsquo;: \u0026lsquo;category3词语列表\u0026rsquo;, \u0026hellip; }  sentiwords = {\u0026#39;pos\u0026#39;: [\u0026#39;开心\u0026#39;, \u0026#39;愉快\u0026#39;, \u0026#39;倍爽\u0026#39;], \u0026#39;neg\u0026#39;: [\u0026#39;难过\u0026#39;, \u0026#39;悲伤\u0026#39;], \u0026#39;adv\u0026#39;: [\u0026#39;倍\u0026#39;]} text = \u0026#39;今天股票大涨，心情倍爽，非常开心啊。\u0026#39; senti_by_diydict(text, sentiwords) Run\n{\u0026#39;pos_num\u0026#39;: 1, \u0026#39;neg_num\u0026#39;: 0, \u0026#39;adv_num\u0026#39;: 1, \u0026#39;stopword_num\u0026#39;: 4, \u0026#39;sentence_num\u0026#39;: 2, \u0026#39;word_num\u0026#39;: 12} \n4.4 注意 返回结果: num表示词语出现次数； score是考虑副词、否定词对情感的修饰，结果不是词频，是情感类别的得分。\n\n五、similarity 使用cosine、jaccard、miniedit等计算两文本的相似度，算法实现参考自\n Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.\n from cntext.similarity import similarity_score text1 = \u0026#39;编程真好玩编程真好玩\u0026#39; text2 = \u0026#39;游戏真好玩编程真好玩\u0026#39; similarity_score(text1, text2) Run\n{\u0026#39;Sim_Cosine\u0026#39;: 0.816496580927726, \u0026#39;Sim_Jaccard\u0026#39;: 0.6666666666666666, \u0026#39;Sim_MinEdit\u0026#39;: 1, \u0026#39;Sim_Simple\u0026#39;: 0.9183673469387755} \n六、visualization 文本信息可视化，含wordcloud、wordshiftor\n wordcloud 词云图 wordshiftor 两文本词移图  6.1 wordcloud(text, title, html_path)  text: 中文文本字符串数据 title: 词云图标题 html_path: 词云图html文件存储路径  from cntext.visualization import wordcloud text1 = \u0026#34;\u0026#34;\u0026#34;在信息化时代，各种各样的数据被广泛采集和利用，有些数据看似无关紧要甚至好像是公开的，但同样关乎国家安全。11月1日是《反间谍法》颁布实施七周年。近年来，国家安全机关按照《反间谍法》《数据安全法》有关规定，依法履行数据安全监管职责，在全国范围内开展涉外数据专项执法行动，发现一些境外数据公司长期、大量、实时搜集我境内船舶数据，数据安全领域的“商业间谍”魅影重重。 2020年6月，国家安全机关在反间谍专项行动中发现，有境外数据公司通过网络在境内私下招募“数据贡献员”。广东省湛江市国家安全局据此开展调查，在麻斜军港附近发现有可疑的无线电设备在持续搜集湛江港口舰船数据，并通过互联网实时传往境外。在临近海港的一个居民楼里，国家安全机关工作人员最终锁定了位置。 一套简易的无线电设备是AIS陆基基站，用来接收AIS系统发射的船舶数据。AIS系统是船舶身份自动识别系统，国际海事组织要求300总吨以上船舶必须强制安装。船只在航行过程中，通过AIS系统向其他船只和主管部门发送船只航向、航速、目的港等信息，用于航行避让、交通导航、轨迹回溯等功能。国家安全机关查获的设备虽然看上去简陋，功能却十分强大。 国家安全机关进一步调查发现，这个基站的来历并不简单。2016年，湛江市的无线电爱好者郑某偶然收到一封境外某海事数据公司发来的邀请邮件。 作为资深的无线电爱好者，能免费领取价值几千元的设备还能获取更多的船舶信息，郑某当然心动。而且，这个基站的架设也非常容易，只要简单组装连上家里的网络，自己的任务就算完成。郑某马上浏览了这家公司申请无线电设备的页面，并按对方要求填写了信息。 \u0026#34;\u0026#34;\u0026#34; wordcloud(text=text1, title=\u0026#39;词云图测试\u0026#39;, html_path=\u0026#39;output/词云图测试.html\u0026#39;) Run\n  6.2 wordshiftor(text1, text2, title, top_n, matplotlib_family)  text1: 文本数据1；字符串 text2: 文本数据2；字符串 title: 词移图标题 top_n: 显示最常用的前n词； 默认值15 matplotlib_family matplotlib中文字体，默认\u0026quot;Arial Unicode MS\u0026quot;；如绘图字体乱码请，请参考下面提示  text1 = \u0026#34;\u0026#34;\u0026#34;在信息化时代，各种各样的数据被广泛采集和利用，有些数据看似无关紧要甚至好像是公开的，但同样关乎国家安全。11月1日是《反间谍法》颁布实施七周年。近年来，国家安全机关按照《反间谍法》《数据安全法》有关规定，依法履行数据安全监管职责，在全国范围内开展涉外数据专项执法行动，发现一些境外数据公司长期、大量、实时搜集我境内船舶数据，数据安全领域的“商业间谍”魅影重重。 2020年6月，国家安全机关在反间谍专项行动中发现，有境外数据公司通过网络在境内私下招募“数据贡献员”。广东省湛江市国家安全局据此开展调查，在麻斜军港附近发现有可疑的无线电设备在持续搜集湛江港口舰船数据，并通过互联网实时传往境外。在临近海港的一个居民楼里，国家安全机关工作人员最终锁定了位置。 一套简易的无线电设备是AIS陆基基站，用来接收AIS系统发射的船舶数据。AIS系统是船舶身份自动识别系统，国际海事组织要求300总吨以上船舶必须强制安装。船只在航行过程中，通过AIS系统向其他船只和主管部门发送船只航向、航速、目的港等信息，用于航行避让、交通导航、轨迹回溯等功能。国家安全机关查获的设备虽然看上去简陋，功能却十分强大。 国家安全机关进一步调查发现，这个基站的来历并不简单。2016年，湛江市的无线电爱好者郑某偶然收到一封境外某海事数据公司发来的邀请邮件。 作为资深的无线电爱好者，能免费领取价值几千元的设备还能获取更多的船舶信息，郑某当然心动。而且，这个基站的架设也非常容易，只要简单组装连上家里的网络，自己的任务就算完成。郑某马上浏览了这家公司申请无线电设备的页面，并按对方要求填写了信息。 \u0026#34;\u0026#34;\u0026#34; text2 = \u0026#34;\u0026#34;\u0026#34; 通知强调，各地商务主管部门要紧紧围绕保供稳价工作目标，压实“菜篮子”市长负责制，细化工作措施；强化横向协作与纵向联动，加强与有关部门的工作协调，形成工作合力；建立完善省际间和本地区联保联供机制，健全有关工作方案，根据形势及时开展跨区域调运；加强市场运行监测，每日跟踪蔬菜、肉类等重点生活必需品供求和价格变化情况，及时预测，及早预警。 通知要求，各地支持鼓励大型农产品流通企业与蔬菜、粮油、畜禽养殖等农产品生产基地建立紧密合作关系，签订长期供销协议；耐储蔬菜要提前采购，锁定货源，做好本地菜与客菜之间，北菜与南菜之间、设施菜与露天菜之间的梯次轮换和衔接供应；健全完备本地肉类储备规模及管理制度；北方省份要按时完成本年度冬春蔬菜储备计划，南方省份要根据自身情况建立完善蔬菜储备；及时投放肉类、蔬菜等生活必需品储备，补充市场供应。 \u0026#34;\u0026#34;\u0026#34; from cntext.visualization import wordshiftor wordshiftor(text1=text1, text2=text2, title=\u0026#39;两文本对比\u0026#39;) Run\n  注意\n 设置参数matplotlib_family，需要先运行下面代码获取本机字体列表 from matplotlib.font_manager import FontManager mpl_fonts = set(f.name for f in FontManager().ttflist) print(mpl_fonts)\n \n代码下载 https://github.com/hidadeng/cntext/tree/main/examples\n","permalink":"/blog/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90cntext/","summary":"cntext 中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等\n github地址 https://github.com/hidadeng/cntext pypi地址 https://pypi.org/project/cntext/ 视频课-Python网络爬虫与文本数据分析  功能模块含\n cntext stats 文本统计,可读性等 dictionary 构建词表(典) sentiment 情感分析 similarity 文本相似度 visualization 可视化，如词云图  安装 pip install cntext==0.9 \n一、cntext 查看cntext基本信息\nimport cntext help(cntext) Run\nHelp on package cntext: NAME cntext PACKAGE CONTENTS description (package) dictionary (package) sentiment (package) similarity (package) visualization (package) DATA ADV_words = [\u0026#39;都\u0026#39;, \u0026#39;全\u0026#39;, \u0026#39;单\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;光\u0026#39;, \u0026#39;尽\u0026#39;, \u0026#39;净\u0026#39;, \u0026#39;仅\u0026#39;, \u0026#39;就\u0026#39;, \u0026#39;只\u0026#39;, \u0026#39;一共\u0026#39;, \u0026#39;... CONJ_words = [\u0026#39;乃\u0026#39;, \u0026#39;乍\u0026#39;, \u0026#39;与\u0026#39;, \u0026#39;无\u0026#39;, \u0026#39;且\u0026#39;, \u0026#39;丕\u0026#39;, \u0026#39;为\u0026#39;, \u0026#39;共\u0026#39;, \u0026#39;其\u0026#39;, \u0026#39;况\u0026#39;, \u0026#39;厥\u0026#39;, \u0026#39;.","title":"cntext中文文本分析库 |  值得收藏"},{"content":"Huggingface（抱抱脸）总部位于纽约，是一家专注于自然语言处理、人工智能和分布式系统的创业公司。他们所提供的聊天机器人技术一直颇受欢迎，但更出名的是他们在NLP开源社区上的贡献。\nHuggingface一直致力于自然语言处理NLP技术的平民化(democratize)，希望每个人都能用上最先进(SOTA, state-of-the-art)的NLP技术，而非困窘于训练资源的匮乏。\nHugging Face所有模型的地址\nhttps://huggingface.co/models\n你可以在这里下载所需要的模型，也可以上传你微调之后用于特定task的模型。\nHugging Face使用文档的地址\nhttps://huggingface.co/transformers/master/index.html\n\n英汉互译 from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline zh2en_model = AutoModelForSeq2SeqLM.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-zh-en\u0026#39;) zh2en_tokenizer = AutoTokenizer.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-zh-en\u0026#39;) zh2en_translation = pipeline(\u0026#39;translation_zh_to_en\u0026#39;, model=zh2en_model, tokenizer=zh2en_tokenizer) zh2en_translation(\u0026#39;Python是一门非常强大的编程语言!\u0026#39;) [{'translation_text': 'Python is a very powerful programming language!'}]  en2zh_model = AutoModelForSeq2SeqLM.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-en-zh\u0026#39;) en2zh_tokenizer = AutoTokenizer.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-en-zh\u0026#39;) en2zh_translation = pipeline(\u0026#39;translation_en_to_zh\u0026#39;, model=en2zh_model, tokenizer=en2zh_tokenizer) en2zh_translation(\u0026#39;Python is a very powerful programming language!\u0026#39;) [{'translation_text': 'Python是一个非常强大的编程语言!'}]  \n文本分类 模型 uer/roberta-base-finetuned-chinanews-chinese是使用5个中文文本分类数据集训练得到\n 京东full、京东binary和大众点评数据集包含不同情感极性的用户评论数据。 凤凰网 和 China Daily 包含不同主题类的新闻文本数据  from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline model = AutoModelForSequenceClassification.from_pretrained(\u0026#39;uer/roberta-base-finetuned-chinanews-chinese\u0026#39;) tokenizer = AutoTokenizer.from_pretrained(\u0026#39;uer/roberta-base-finetuned-chinanews-chinese\u0026#39;) text_classification = pipeline(\u0026#39;sentiment-analysis\u0026#39;, model=model, tokenizer=tokenizer) test_text = \u0026#34;上证指数大涨2%\u0026#34; text_classification(test_text, return_all_scores=True) [[{'label': 'mainland China politics', 'score': 0.0002807585697155446}, {'label': 'Hong Kong - Macau politics', 'score': 0.00015504546172451228}, {'label': 'International news', 'score': 6.818029214628041e-05}, {'label': 'financial news', 'score': 0.9991051554679871}, {'label': 'culture', 'score': 0.00011297615128569305}, {'label': 'entertainment', 'score': 0.00012184812658233568}, {'label': 'sports', 'score': 0.0001558474759804085}]]  test_text = \u0026#34;Python是一门强大的编程语言\u0026#34; text_classification(test_text, return_all_scores=True) [[{'label': 'mainland China politics', 'score': 0.02050291746854782}, {'label': 'Hong Kong - Macau politics', 'score': 0.0030984438490122557}, {'label': 'International news', 'score': 0.005687597207725048}, {'label': 'financial news', 'score': 0.03360358253121376}, {'label': 'culture', 'score': 0.913349986076355}, {'label': 'entertainment', 'score': 0.010810119099915028}, {'label': 'sports', 'score': 0.012947351671755314}]]  \n代码下载 https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211108HuggingFace学习\n","permalink":"/blog/huggingface%E5%AD%A6%E4%B9%A0/","summary":"Huggingface（抱抱脸）总部位于纽约，是一家专注于自然语言处理、人工智能和分布式系统的创业公司。他们所提供的聊天机器人技术一直颇受欢迎，但更出名的是他们在NLP开源社区上的贡献。\nHuggingface一直致力于自然语言处理NLP技术的平民化(democratize)，希望每个人都能用上最先进(SOTA, state-of-the-art)的NLP技术，而非困窘于训练资源的匮乏。\nHugging Face所有模型的地址\nhttps://huggingface.co/models\n你可以在这里下载所需要的模型，也可以上传你微调之后用于特定task的模型。\nHugging Face使用文档的地址\nhttps://huggingface.co/transformers/master/index.html\n\n英汉互译 from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline zh2en_model = AutoModelForSeq2SeqLM.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-zh-en\u0026#39;) zh2en_tokenizer = AutoTokenizer.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-zh-en\u0026#39;) zh2en_translation = pipeline(\u0026#39;translation_zh_to_en\u0026#39;, model=zh2en_model, tokenizer=zh2en_tokenizer) zh2en_translation(\u0026#39;Python是一门非常强大的编程语言!\u0026#39;) [{'translation_text': 'Python is a very powerful programming language!'}]  en2zh_model = AutoModelForSeq2SeqLM.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-en-zh\u0026#39;) en2zh_tokenizer = AutoTokenizer.from_pretrained(\u0026#39;Helsinki-NLP/opus-mt-en-zh\u0026#39;) en2zh_translation = pipeline(\u0026#39;translation_en_to_zh\u0026#39;, model=en2zh_model, tokenizer=en2zh_tokenizer) en2zh_translation(\u0026#39;Python is a very powerful programming language!\u0026#39;) [{'translation_text': 'Python是一个非常强大的编程语言!'}]  \n文本分类 模型 uer/roberta-base-finetuned-chinanews-chinese是使用5个中文文本分类数据集训练得到\n 京东full、京东binary和大众点评数据集包含不同情感极性的用户评论数据。 凤凰网 和 China Daily 包含不同主题类的新闻文本数据  from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline model = AutoModelForSequenceClassification.","title":"Hugging Face | 自然语言处理平台"},{"content":"数据可视化是讲故事的好方法，从中可以轻松地吸收信息并识别数据中的模式。我们的一位学生决定使用 Matplotlib 在 Python 中创建数据可视化，以了解 Netflix 上可用的不同类型的内容。本文将重点介绍使用 Matplotlib 以一种有趣的方式进行数据可视化。\n在 Netflix 上看完精彩的节目或电影后，您有没有想过 Netflix 为您提供了如此多的惊人内容？当然，我并不孤单，不是吗？一个想法会导致另一个想法，在不知不觉中，您已经下定决心进行探索性数据分析，以了解更多关于最受欢迎的演员是谁以及哪个国家/地区更喜欢哪种类型的信息。\n使用 Python 制作常规的条形图和饼图，虽然它们在传达结果方面做得很好，但我想为这个项目添加一些有趣的元素。\n我最近了解到你可以在 Python 最流行的数据可视化库 Matplotlib 中创建类似 xkcd 的绘图，并决定我应该在这个项目中整理我所有的 Matplotlib 可视化，只是为了让事情变得更有趣。\n一起来看看数据怎么说吧！\n导入数据 7787部电影/TV节目的信息\nimport pandas as pd df = pd.read_csv(\u0026#34;netflix_titles.csv\u0026#34;) df.head()   Netflix历年发展里程碑 描述一下 Netflix 多年来演变的时间表。\nimport matplotlib.pyplot as plt import numpy as np plt.rcParams[\u0026#39;figure.dpi\u0026#39;] = 200 # these go on the numbers below tl_dates = [ \u0026#34;1997\\nFounded\u0026#34;, \u0026#34;1998\\nMail Service\u0026#34;, \u0026#34;2003\\nGoes Public\u0026#34;, \u0026#34;2007\\nStreaming service\u0026#34;, \u0026#34;2016\\nGoes Global\u0026#34;, \u0026#34;2021\\nNetflix \u0026amp; Chill\u0026#34; ] tl_x = [1, 2, 4, 5.3, 8, 9] # the numbers go on these tl_sub_x = [1.5,3,5,6.5,7] tl_sub_times = [ \u0026#34;1998\u0026#34;,\u0026#34;2000\u0026#34;,\u0026#34;2006\u0026#34;,\u0026#34;2010\u0026#34;,\u0026#34;2012\u0026#34; ] tl_text = [ \u0026#34;Netflix.com launched\u0026#34;, \u0026#34;Starts\\nPersonal\\nRecommendations\u0026#34;,\u0026#34;Billionth DVD Delivery\u0026#34;,\u0026#34;Canadian\\nLaunch\u0026#34;,\u0026#34;UK Launch\u0026#34;] with plt.xkcd(): # Set figure \u0026amp; Axes fig, ax = plt.subplots(figsize=(15, 4), constrained_layout=True) ax.set_ylim(-2, 1.75) ax.set_xlim(0, 10) # Timeline : line ax.axhline(0, xmin=0.1, xmax=0.9, c=\u0026#39;deeppink\u0026#39;, zorder=1) # Timeline : Date Points ax.scatter(tl_x, np.zeros(len(tl_x)), s=120, c=\u0026#39;palevioletred\u0026#39;, zorder=2) ax.scatter(tl_x, np.zeros(len(tl_x)), s=30, c=\u0026#39;darkmagenta\u0026#39;, zorder=3) # Timeline : Time Points ax.scatter(tl_sub_x, np.zeros(len(tl_sub_x)), s=50, c=\u0026#39;darkmagenta\u0026#39;,zorder=4) # Date Text for x, date in zip(tl_x, tl_dates): ax.text(x, -0.55, date, ha=\u0026#39;center\u0026#39;, fontfamily=\u0026#39;serif\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, color=\u0026#39;royalblue\u0026#39;,fontsize=12) # Stemplot : vertical line levels = np.zeros(len(tl_sub_x)) levels[::2] = 0.3 levels[1::2] = -0.3 markerline, stemline, baseline = ax.stem(tl_sub_x, levels, use_line_collection=True) plt.setp(baseline, zorder=0) plt.setp(markerline, marker=\u0026#39;,\u0026#39;, color=\u0026#39;darkmagenta\u0026#39;) plt.setp(stemline, color=\u0026#39;darkmagenta\u0026#39;) # Text for idx, x, time, txt in zip(range(1, len(tl_sub_x)+1), tl_sub_x, tl_sub_times, tl_text): ax.text(x, 1.3*(idx%2)-0.5, time, ha=\u0026#39;center\u0026#39;, fontfamily=\u0026#39;serif\u0026#39;, fontweight=\u0026#39;bold\u0026#39;, color=\u0026#39;royalblue\u0026#39;, fontsize=11) ax.text(x, 1.3*(idx%2)-0.6, txt, va=\u0026#39;top\u0026#39;, ha=\u0026#39;center\u0026#39;, fontfamily=\u0026#39;serif\u0026#39;,color=\u0026#39;royalblue\u0026#39;) # Spine for spine in [\u0026#34;left\u0026#34;, \u0026#34;top\u0026#34;, \u0026#34;right\u0026#34;, \u0026#34;bottom\u0026#34;]: ax.spines[spine].set_visible(False) # Ticks ax.set_xticks([]) ax.set_yticks([]) # Title ax.set_title(\u0026#34;Netflix through the years\u0026#34;, fontweight=\u0026#34;bold\u0026#34;, fontfamily=\u0026#39;serif\u0026#39;, fontsize=16, color=\u0026#39;royalblue\u0026#39;) ax.text(2.4,1.57,\u0026#34;From DVD rentals to a global audience of over 150m people - is it time for Netflix to Chill?\u0026#34;, fontfamily=\u0026#39;serif\u0026#39;, fontsize=12, color=\u0026#39;mediumblue\u0026#39;) plt.show()   电影 vs 电视综艺 接下来，我决定看一下电影与电视节目的比例。\ncol = \u0026#34;type\u0026#34; grouped = df[col].value_counts().reset_index() grouped = grouped.rename(columns = {col : \u0026#34;count\u0026#34;, \u0026#34;index\u0026#34; : col}) with plt.xkcd(): explode = (0, 0.1) # only \u0026#34;explode\u0026#34; the 2nd slice (i.e. \u0026#39;TV Show\u0026#39;) fig1, ax1 = plt.subplots(figsize=(5, 5), dpi=100) ax1.pie(grouped[\u0026#34;count\u0026#34;], explode=explode, labels=grouped[\u0026#34;type\u0026#34;], autopct=\u0026#39;%1.1f%%\u0026#39;, shadow=True, startangle=90) ax1.axis(\u0026#39;equal\u0026#39;) # Equal aspect ratio ensures that pie is drawn as a circle. plt.show()   内容最多的国家 from collections import Counter col = \u0026#34;country\u0026#34; categories = \u0026#34;, \u0026#34;.join(df[col].fillna(\u0026#34;\u0026#34;)).split(\u0026#34;, \u0026#34;) counter_list = Counter(categories).most_common(25) counter_list = [_ for _ in counter_list if _[0] != \u0026#34;\u0026#34;] labels = [_[0] for _ in counter_list] values = [_[1] for _ in counter_list] with plt.xkcd(): fig, ax = plt.subplots(figsize=(10, 10), dpi=100) y_pos = np.arange(len(labels)) ax.barh(y_pos, values, align=\u0026#39;center\u0026#39;) ax.set_yticks(y_pos) ax.set_yticklabels(labels) ax.invert_yaxis() # labels read top-to-bottom ax.set_xlabel(\u0026#39;Content\u0026#39;) ax.set_title(\u0026#39;Countries with most content\u0026#39;) plt.show()   最流行的导演 from collections import Counter from matplotlib.pyplot import figure import math colours = [\u0026#34;orangered\u0026#34;, \u0026#34;mediumseagreen\u0026#34;, \u0026#34;darkturquoise\u0026#34;, \u0026#34;mediumpurple\u0026#34;, \u0026#34;deeppink\u0026#34;, \u0026#34;indianred\u0026#34;] countries_list = [\u0026#34;United States\u0026#34;, \u0026#34;India\u0026#34;, \u0026#34;United Kingdom\u0026#34;, \u0026#34;Japan\u0026#34;, \u0026#34;France\u0026#34;, \u0026#34;Canada\u0026#34;] col = \u0026#34;director\u0026#34; with plt.xkcd(): figure(num=None, figsize=(20, 8)) x=1 for country in countries_list: country_df = df[df[\u0026#34;country\u0026#34;]==country] categories = \u0026#34;, \u0026#34;.join(country_df[col].fillna(\u0026#34;\u0026#34;)).split(\u0026#34;, \u0026#34;) counter_list = Counter(categories).most_common(6) counter_list = [_ for _ in counter_list if _[0] != \u0026#34;\u0026#34;] labels = [_[0] for _ in counter_list][::-1] values = [_[1] for _ in counter_list][::-1] if max(values)\u0026lt;10: values_int = range(0, math.ceil(max(values))+1) else: values_int = range(0, math.ceil(max(values))+1, 2) plt.subplot(2, 3, x) plt.barh(labels,values, color = colours[x-1]) plt.xticks(values_int) plt.title(country) x+=1 plt.suptitle(\u0026#39;Popular Directors with the most content\u0026#39;) plt.tight_layout() plt.show()   Netflix 专注于什么样的内容？ 我还想浏览评级栏并比较 Netflix 为儿童、青少年和成人制作的内容量——以及这些年来他们的重点是否从一个群体转移到另一个群体。\n为此，我首先查看了 DataFrame 中的独特评级：\ndf[\u0026#34;date_added\u0026#34;] = pd.to_datetime(df[\u0026#39;date_added\u0026#39;]) df[\u0026#39;year_added\u0026#39;] = df[\u0026#39;date_added\u0026#39;].dt.year.astype(\u0026#39;Int64\u0026#39;) ratings_list = [\u0026#39;TV-MA\u0026#39;, \u0026#39;R\u0026#39;, \u0026#39;PG-13\u0026#39;, \u0026#39;TV-14\u0026#39;, \u0026#39;TV-PG\u0026#39;, \u0026#39;TV-G\u0026#39;, \u0026#39;TV-Y\u0026#39;, \u0026#39;TV-Y7\u0026#39;, \u0026#39;PG\u0026#39;, \u0026#39;G\u0026#39;, \u0026#39;NC-17\u0026#39;, \u0026#39;TV-Y7-FV\u0026#39;] ratings_group_list = [\u0026#39;Little Kids\u0026#39;, \u0026#39;Older Kids\u0026#39;, \u0026#39;Teens\u0026#39;, \u0026#39;Mature\u0026#39;] ratings_dict={ \u0026#39;TV-G\u0026#39;: \u0026#39;Little Kids\u0026#39;, \u0026#39;TV-Y\u0026#39;: \u0026#39;Little Kids\u0026#39;, \u0026#39;G\u0026#39;: \u0026#39;Little Kids\u0026#39;, \u0026#39;TV-PG\u0026#39;: \u0026#39;Older Kids\u0026#39;, \u0026#39;TV-Y7\u0026#39;: \u0026#39;Older Kids\u0026#39;, \u0026#39;PG\u0026#39;: \u0026#39;Older Kids\u0026#39;, \u0026#39;TV-Y7-FV\u0026#39;: \u0026#39;Older Kids\u0026#39;, \u0026#39;PG-13\u0026#39;: \u0026#39;Teens\u0026#39;, \u0026#39;TV-14\u0026#39;: \u0026#39;Teens\u0026#39;, \u0026#39;TV-MA\u0026#39;: \u0026#39;Mature\u0026#39;, \u0026#39;R\u0026#39;: \u0026#39;Mature\u0026#39;, \u0026#39;NC-17\u0026#39;: \u0026#39;Mature\u0026#39; } for rating_val, rating_group in ratings_dict.items(): df.loc[df.rating == rating_val, \u0026#34;rating\u0026#34;] = rating_group df[\u0026#39;rating_val\u0026#39;]=1 x=0 labels=[\u0026#39;kinda\\nless\u0026#39;, \u0026#39;not so\\nbad\u0026#39;, \u0026#39;holyshit\\nthat\\\u0026#39;s too\\nmany\u0026#39;] with plt.xkcd(): for r in ratings_group_list: grouped = df[df[\u0026#39;rating\u0026#39;]==r] year_df = grouped.groupby([\u0026#39;year_added\u0026#39;]).sum() year_df.reset_index(level=0, inplace=True) plt.plot(year_df[\u0026#39;year_added\u0026#39;], year_df[\u0026#39;rating_val\u0026#39;], color=colours[x], marker=\u0026#39;o\u0026#39;) values_int = range(2008, math.ceil(max(year_df[\u0026#39;year_added\u0026#39;]))+1, 2) plt.yticks([200, 600, 1000], labels) plt.xticks(values_int) plt.title(\u0026#39;Count of shows and movies that Netflix\\nhas been producing for different audiences\u0026#39;, fontsize=12) plt.xlabel(\u0026#39;Year\u0026#39;, fontsize=14) plt.ylabel(\u0026#39;Content Count\u0026#39;, fontsize=14) x+=1 plt.legend(ratings_group_list) plt.tight_layout() plt.show()   代码下载 https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211031使用matplotlib绘制卡通化图表\n","permalink":"/blog/%E4%BD%BF%E7%94%A8matplotlib%E7%BB%98%E5%88%B6%E8%B6%85%E5%8F%AF%E7%88%B1%E8%B6%85%E8%90%8C%E5%8C%96%E7%9A%84%E5%9B%BE%E8%A1%A8/","summary":"数据可视化是讲故事的好方法，从中可以轻松地吸收信息并识别数据中的模式。我们的一位学生决定使用 Matplotlib 在 Python 中创建数据可视化，以了解 Netflix 上可用的不同类型的内容。本文将重点介绍使用 Matplotlib 以一种有趣的方式进行数据可视化。\n在 Netflix 上看完精彩的节目或电影后，您有没有想过 Netflix 为您提供了如此多的惊人内容？当然，我并不孤单，不是吗？一个想法会导致另一个想法，在不知不觉中，您已经下定决心进行探索性数据分析，以了解更多关于最受欢迎的演员是谁以及哪个国家/地区更喜欢哪种类型的信息。\n使用 Python 制作常规的条形图和饼图，虽然它们在传达结果方面做得很好，但我想为这个项目添加一些有趣的元素。\n我最近了解到你可以在 Python 最流行的数据可视化库 Matplotlib 中创建类似 xkcd 的绘图，并决定我应该在这个项目中整理我所有的 Matplotlib 可视化，只是为了让事情变得更有趣。\n一起来看看数据怎么说吧！\n导入数据 7787部电影/TV节目的信息\nimport pandas as pd df = pd.read_csv(\u0026#34;netflix_titles.csv\u0026#34;) df.head()   Netflix历年发展里程碑 描述一下 Netflix 多年来演变的时间表。\nimport matplotlib.pyplot as plt import numpy as np plt.rcParams[\u0026#39;figure.dpi\u0026#39;] = 200 # these go on the numbers below tl_dates = [ \u0026#34;1997\\nFounded\u0026#34;, \u0026#34;1998\\nMail Service\u0026#34;, \u0026#34;2003\\nGoes Public\u0026#34;, \u0026#34;2007\\nStreaming service\u0026#34;, \u0026#34;2016\\nGoes Global\u0026#34;, \u0026#34;2021\\nNetflix \u0026amp; Chill\u0026#34; ] tl_x = [1, 2, 4, 5.","title":"使用matplotlib绘制超可爱超萌化的图表"},{"content":"有时我们希望根据 DataFrame 其他列(字段) 的值向 DataFrame 添加一列。\n虽然这听起来很简单，但如果我们尝试使用 if-else 条件来完成它可能会变得有点复杂。 值得庆幸的是，使用 numpy 两个函数np.where()、np.select() 就能实现这一需求。\n导入数据 我们有一个包含 4,000 多条 Dataquest 推文的数据集,字段包括：\n date twitter发送的日期 time 推文发送时间 tweet 推文内容 mentions 谁提到了该推文 photos 图片链接 replies_count 推文回复数 retweets_count 推文再转发数 likes_count 推文获得的点赞数  import pandas as pd import numpy as np df = pd.read_csv(\u0026#39;tweets.csv\u0026#39;) df.head()   \n问题 我们看到数据集中的photos字段是图片链接\n 如果某条记录有信息，会显示图片链接列表 如果不含图片，该字段对应的数据是空列表  我们只想查看带有图片的推文是否获得更多交互，因此我们实际上并不需要图片 URL。 让我们尝试创建一个名为 has_image 的新列，该列将包含布尔值\n 如果推文包含图像，则为 True，否则为 False。  那么我们如何创建一个has_image字段？\n\nnp.where() np.where(condition, value if condition is true, value if condition is false)\n为此，我们将使用 numpy 的内置 where() 函数。 这个函数依次接受三个参数：我们要测试的条件，如果条件为真则分配给新列的值，如果条件为假则分配给新列的值。 它看起来像这样：\ndf[\u0026#39;has_image\u0026#39;] = np.where(df[\u0026#39;photos\u0026#39;]!=\u0026#39;[]\u0026#39;, True, False) df.head()   np.select() 这种方法很好用，但如果我们新建列的值不止True、False呢？\n例如我们把likes_count 进行分类，不同取值范围定义为不同的类别\n tier_4 少于2个赞 tier_3 3-9 个赞 tier_2 10-15 个赞 tier_1 16+ 个赞  为此，我们可以使用名为 np.select() 的函数。我们将给它两个参数：一个我们的条件列表，以及一个我们想要分配给新列中每一行的值的相关列表。\n这意味着顺序很重要：如果满足条件列表中的第一个条件，则值列表中的第一个值将分配给该行的新列。如果满足第二个条件，则将分配第二个值，依此类推。\n让我们来看看它在 Python 代码中的表现：\n# create a list of our conditions conditions = [ (df[\u0026#39;likes_count\u0026#39;] \u0026lt;= 2), (df[\u0026#39;likes_count\u0026#39;] \u0026gt; 2) \u0026amp; (df[\u0026#39;likes_count\u0026#39;] \u0026lt;= 9), (df[\u0026#39;likes_count\u0026#39;] \u0026gt; 9) \u0026amp; (df[\u0026#39;likes_count\u0026#39;] \u0026lt;= 15), (df[\u0026#39;likes_count\u0026#39;] \u0026gt; 15) ] # create a list of the values we want to assign for each condition values = [\u0026#39;tier_4\u0026#39;, \u0026#39;tier_3\u0026#39;, \u0026#39;tier_2\u0026#39;, \u0026#39;tier_1\u0026#39;] # create a new column and use np.select to assign values to it using our lists as arguments df[\u0026#39;tier\u0026#39;] = np.select(conditions, values) # display updated DataFrame df.head()   \n代码下载 https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211031如何在DataFrame中使用If-Else条件语句创建新列\n","permalink":"/blog/%E5%A6%82%E4%BD%95%E5%9C%A8dataframe%E4%B8%AD%E4%BD%BF%E7%94%A8if-else%E6%9D%A1%E4%BB%B6%E8%AF%AD%E5%8F%A5%E5%88%9B%E5%BB%BA%E6%96%B0%E5%88%97/","summary":"有时我们希望根据 DataFrame 其他列(字段) 的值向 DataFrame 添加一列。\n虽然这听起来很简单，但如果我们尝试使用 if-else 条件来完成它可能会变得有点复杂。 值得庆幸的是，使用 numpy 两个函数np.where()、np.select() 就能实现这一需求。\n导入数据 我们有一个包含 4,000 多条 Dataquest 推文的数据集,字段包括：\n date twitter发送的日期 time 推文发送时间 tweet 推文内容 mentions 谁提到了该推文 photos 图片链接 replies_count 推文回复数 retweets_count 推文再转发数 likes_count 推文获得的点赞数  import pandas as pd import numpy as np df = pd.read_csv(\u0026#39;tweets.csv\u0026#39;) df.head()   \n问题 我们看到数据集中的photos字段是图片链接\n 如果某条记录有信息，会显示图片链接列表 如果不含图片，该字段对应的数据是空列表  我们只想查看带有图片的推文是否获得更多交互，因此我们实际上并不需要图片 URL。 让我们尝试创建一个名为 has_image 的新列，该列将包含布尔值\n 如果推文包含图像，则为 True，否则为 False。  那么我们如何创建一个has_image字段？","title":"如何在DataFrame中使用If-Else条件语句创建新列"},{"content":"动机 map和filter是处理iterable数据最好用的函数，但却让代码看起来很乱，使代码可读性大大降低。\narr = [1, 2, 3, 4, 5] #对arr筛选偶数，并对偶数乘以2 list(map(lambda x: x*2, filter(lambda x:x%2==0, arr))) [4, 8]  刚刚的iterable的例子，其实可以使用pipe库中的 | 来应用多种方法。\nfrom pipe import select, where arr = [1, 2, 3, 4, 5] list(arr |where(lambda x:x%2==0) |select(lambda x:x*2)) [4, 8]  pipe是什么？ pipe是python中的管道操作库，可以使数据分析多个步骤(函数）像管道(流水线)一样上下衔接，共同完成一个数据分析任务。\n我喜欢pipe是因为它让iterable代码变得干净整洁，可读性大大增强。后面我会通过几个案例让大家快速掌握pipe库。首先先安装pipe\n!pip3 install pipe where 对iterable中的数据进行筛选操作\nfrom pipe import where arr = [1, 2, 3, 4, 5] #把偶数筛选出来 list(arr | where(lambda x: x%2==0)) [2, 4]  select 对iterable中的数据进行某种操作\nfrom pipe import select arr = [1, 2, 3, 4, 5] #对arr中的每个数 乘以2 list(arr | select(lambda x: x*2)) [2, 4, 6, 8, 10]  现在你可能会有疑问： 为何在Python已拥有map和filter情况下， 还用pipe库中的 select和 where呢？\n因为可以使用管道在一个方法后面加入另一个方法， 加不止1次!!\nfrom pipe import select, where arr = [1, 2, 3, 4, 5] list(arr | where(lambda x: x%2==0) #筛选arr中的偶数 | select(lambda x: x*2) #对偶数乘以2 ) [4, 8]  非折叠iterable chain 对于嵌套结构的iterable数据，最难任务之一就是将其展平。\nfrom pipe import chain nested = [[1,2,[3]], [4, 5]] list((nested | chain)) [1, 2, [3], 4, 5]  即时经过上述操作， 依然不是完全展开。 为了处理深度嵌套数据， 可以使用traverse方法。\ntraverse 遍历traverse方法可以用递归的方式展开 嵌套对象。\nfrom pipe import traverse nested = [[1,2,[3]], [4, 5]] list((nested | traverse)) [1, 2, 3, 4, 5]  现在我们从抽取字典values中的列表，并将其展平\nfrom pipe import traverse, select fruits = [ {\u0026#34;name\u0026#34;: \u0026#34;apple\u0026#34;, \u0026#34;price\u0026#34;: [2, 5]}, {\u0026#34;name\u0026#34;: \u0026#34;orange\u0026#34;, \u0026#34;price\u0026#34;: 4}, {\u0026#34;name\u0026#34;: \u0026#34;grape\u0026#34;, \u0026#34;price\u0026#34;: 5} ] list(fruits | select(lambda fruit: fruit[\u0026#34;price\u0026#34;]) | traverse) [2, 5, 4, 5]  groupby 有时候，需要对列表中的数据进行分组，这可能用到groupby方法。\nfrom pipe import select, groupby list( (1, 2, 3, 4, 5, 6, 7, 8, 9) | groupby(lambda x: \u0026#34;偶数\u0026#34; if x%2==0 else \u0026#34;奇数\u0026#34;) | select(lambda x: {x[0]: list(x[1])}) ) [{'偶数': [2, 4, 6, 8]}, {'奇数': [1, 3, 5, 7, 9]}]  在上面的代码中， 我们使用groupby将数字分为奇数组和偶数组。groupby方法输出的结果如下\n[(\u0026#39;偶数\u0026#39;, \u0026lt;itertools._grouper at 0x10bd54550\u0026gt;), (\u0026#39;奇数\u0026#39;, \u0026lt;itertools._grouper at 0x10bd4d350\u0026gt;)] 接下来，使用select将元素为元组的列表转化为字典，其中\n 元组中第1位置做字典的关键词 元组中第2位置做字典的值  [{\u0026#39;偶数\u0026#39;: [2, 4, 6, 8]}, {\u0026#39;奇数\u0026#39;: [1, 3, 5, 7, 9]}] Cool！为了range值大于2， 我们在select内增加where条件操作\nfrom pipe import select, groupby list( (1, 2, 3, 4, 5, 6, 7, 8, 9) | groupby(lambda x: \u0026#34;偶数\u0026#34; if x%2==0 else \u0026#34;奇数\u0026#34;) | select(lambda x: {x[0]: list(x[1] | where(lambda x: x\u0026gt;2) ) } ) ) [{'偶数': [4, 6, 8]}, {'奇数': [3, 5, 7, 9]}]  dedup 使用Key对list数据进行去重\nfrom pipe import dedup arr = [1, 2, 2, 3, 4, 5, 6, 6, 7, 9, 3, 3, 1] list(arr | dedup) [1, 2, 3, 4, 5, 6, 7, 9]  这看起来没啥新意，毕竟python内置的set函数即可实现刚刚的需求。然而，dedup通过key获得列表中的唯一元素。\n例如，获得小于5的唯一元素， 且另一个元素大于或等于5\nfrom pipe import dedup arr = [1, 2, 2, 3, 4, 5, 6, 6, 7, 9, 3, 3, 1] list(arr | dedup(lambda key: key\u0026lt;5)) [1, 5]  from pipe import traverse, select data = [ {\u0026#34;name\u0026#34;: \u0026#34;apple\u0026#34;, \u0026#34;count\u0026#34;: 2}, {\u0026#34;name\u0026#34;: \u0026#34;orange\u0026#34;, \u0026#34;count\u0026#34;: 4}, {\u0026#34;name\u0026#34;: \u0026#34;grape\u0026#34;, \u0026#34;count\u0026#34;: None}, {\u0026#34;name\u0026#34;: \u0026#34;orange\u0026#34;, \u0026#34;count\u0026#34;: 7} ] list( data | dedup(key=lambda fruit: fruit[\u0026#34;name\u0026#34;]) | select(lambda fruit: fruit[\u0026#34;count\u0026#34;]) | where(lambda count: isinstance(count, int)) ) [2, 4]  代码下载 https://github.com/hidadeng/DaDengAndHisPython/blob/master/20211115使用pipe撰写干净的Python代码.ipynb\n","permalink":"/blog/%E4%BD%BF%E7%94%A8pipe%E6%92%B0%E5%86%99%E5%B9%B2%E5%87%80%E7%9A%84python%E4%BB%A3%E7%A0%81/","summary":"动机 map和filter是处理iterable数据最好用的函数，但却让代码看起来很乱，使代码可读性大大降低。\narr = [1, 2, 3, 4, 5] #对arr筛选偶数，并对偶数乘以2 list(map(lambda x: x*2, filter(lambda x:x%2==0, arr))) [4, 8]  刚刚的iterable的例子，其实可以使用pipe库中的 | 来应用多种方法。\nfrom pipe import select, where arr = [1, 2, 3, 4, 5] list(arr |where(lambda x:x%2==0) |select(lambda x:x*2)) [4, 8]  pipe是什么？ pipe是python中的管道操作库，可以使数据分析多个步骤(函数）像管道(流水线)一样上下衔接，共同完成一个数据分析任务。\n我喜欢pipe是因为它让iterable代码变得干净整洁，可读性大大增强。后面我会通过几个案例让大家快速掌握pipe库。首先先安装pipe\n!pip3 install pipe where 对iterable中的数据进行筛选操作\nfrom pipe import where arr = [1, 2, 3, 4, 5] #把偶数筛选出来 list(arr | where(lambda x: x%2==0)) [2, 4]  select 对iterable中的数据进行某种操作","title":"让Python代码更简洁的pipe包"},{"content":"尽管已经有很多方法可用于关键字生成（例如，Rake、YAKE!、TF-IDF 等），但我想创建一个非常基本但功能强大的方法来提取关键字和关键短语。这就是 KeyBERT 的用武之地！它使用 BERT 嵌入 和 简单余弦相似度 来查找文档中与文档本身最相似的短语。\nKeyBERT步骤\n 首先使用 BERT 提取文档嵌入以获得文档级向量表示。 随后，为 N-gram 词/短语提取词向量。 然后，我们使用余弦相似度来找到与文档最相似的单词/短语。 最后可以将最相似的词识别为最能描述整个文档的词。  安装 !pip3 install keybert==0.5.0 \n初始化模型 KeyBERT库需要安装配置spacy语言模型\n具体参考公众号：大邓和他的Python 2021-10-29 的推文 查看spacy配置方法\n初始化模型\nfrom keybert import KeyBERT import spacy import jieba zh_model = spacy.load(\u0026#34;zh_core_web_sm\u0026#34;) bertModel = KeyBERT(model=zh_model) \n准备数据 中文测试数据需要先分词，而后构造成类英文的语言结构(用空格间隔的文本)\n# 测试数据 doc = \u0026#34;\u0026#34;\u0026#34;时值10月25日抗美援朝纪念日，《长津湖》片方发布了“纪念中国人民志愿军抗美援朝出国作战71周年特别短片”，再次向伟大的志愿军致敬！ 电影《长津湖》全情全景地还原了71年前抗美援朝战场上那场史诗战役，志愿军奋不顾身的英勇精神令观众感叹：“岁月峥嵘英雄不灭，丹心铁骨军魂永存！”影片上映以来票房屡创新高，目前突破53亿元，暂列中国影史票房总榜第三名。 值得一提的是，这部影片的很多主创或有军人的血脉，或有当兵的经历，或者家人是军人。提起这些他们也充满自豪，影片总监制黄建新称：“当兵以后会有一种特别能坚持的劲儿。”饰演雷公的胡军透露：“我父亲曾经参加过抗美援朝，还得了一个三等功。”影片历史顾问王树增表示：“我当了五十多年的兵，我的老部队就是上甘岭上下来的，那些老兵都是我的偶像。” “身先士卒卫华夏家国，血战无畏护山河无恙。”片中饰演七连连长伍千里的吴京感叹：“要永远记住这些先烈们，他们给我们带来今天的和平。感谢他们的付出，才让我们有今天的幸福生活。”饰演新兵伍万里的易烊千玺表示：“战争的残酷、碾压式的伤害，其实我们现在的年轻人几乎很难能体会到，希望大家看完电影后能明白，是那些先辈们的牺牲奉献，换来了我们的现在。” 影片对战争群像的恢弘呈现，对个体命运的深切关怀，令许多观众无法控制自己的眼泪，观众称：“当看到影片中的惊险战斗场面，看到英雄们壮怀激烈的拼杀，为国捐躯的英勇无畏和无悔付出，我明白了为什么说今天的幸福生活来之不易。”（记者 王金跃） \u0026#34;\u0026#34;\u0026#34; doc = \u0026#39; \u0026#39;.join(jieba.lcut(doc)) # 关键词提取 keywords = bertModel.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10) keywords [('铁骨', 0.5028), ('纪念日', 0.495), ('丹心', 0.4894), ('战役', 0.4869), ('影史', 0.473), ('父亲', 0.4576), ('票房', 0.4571), ('偶像', 0.4497), ('精神', 0.4436), ('家国', 0.4373)]  常用extract_keywords参数 bertModel.extract_keywords(docs, keyphrase_ngram_range, stop_words, top_n)\n docs 文档字符串（空格间隔词语的字符串） keyphrase_ngram_range 设置ngram，默认(1, 1) stop_words 停用词列表 top_n 显示前n个关键词，默认5 highlight 可视化标亮关键词，默认False use_maxsum: 默认False;是否使用Max Sum Similarity作为关键词提取标准， use_mmr: 默认False;是否使用Maximal Marginal Relevance (MMR) 作为关键词提取标准 diversity 如果use_mmr=True，可以设置该参数。参数取值范围从0到1  对于keyphrase_ngram_range参数，\n (1, 1) 只单个词， 如\u0026quot;抗美援朝\u0026quot;, \u0026ldquo;纪念日\u0026quot;是孤立的两个词 (2, 2) 考虑词组， 如出现有意义的词组 \u0026ldquo;抗美援朝 纪念日\u0026rdquo; (1, 2) 同时考虑以上两者情况  # 关键词提取 keywords = bertModel.extract_keywords(doc, keyphrase_ngram_range=(2, 2), stop_words=None, diversity=0.7, top_n=10) keywords [('影片 总监制', 0.5412), ('丹心 铁骨', 0.5339), ('抗美援朝 纪念日', 0.5295), ('长津湖 片方', 0.5252), ('志愿军 致敬', 0.5207), ('老兵 偶像', 0.5192), ('票房 创新', 0.5108), ('军人 血脉', 0.5084), ('家国 血战', 0.4946), ('家人 军人', 0.4885)]  #可视化 keywords = bertModel.extract_keywords(doc, keyphrase_ngram_range=(2, 2), stop_words=None, highlight=True, top_n=10)   # 关键词提取 keywords = bertModel.extract_keywords(doc, keyphrase_ngram_range=(2, 2), stop_words=None, use_mmr=True, diversity=0.05, top_n=10) keywords [('影片 总监制', 0.5412), ('长津湖 片方', 0.5252), ('抗美援朝 纪念日', 0.5295), ('丹心 铁骨', 0.5339), ('志愿军 致敬', 0.5207), ('老兵 偶像', 0.5192), ('票房 创新', 0.5108), ('军人 血脉', 0.5084), ('家国 血战', 0.4946), ('家人 军人', 0.4885)]  英文KeyBERT 同样需要配置spacy，参考公众号：大邓和他的Python 2021-10-29 的推文 查看spacy配置方法\nfrom keybert import KeyBERT import spacy en_model = spacy.load(\u0026#34;en_core_web_sm\u0026#34;) doc = \u0026#34;\u0026#34;\u0026#34; Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \u0026#39;reasonable\u0026#39; way (see inductive bias). \u0026#34;\u0026#34;\u0026#34; kw_model = KeyBERT() keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2)) keywords Run\n[('supervised learning', 0.6779), ('supervised', 0.6676), ('signal supervised', 0.6152), ('examples supervised', 0.6112), ('labeled training', 0.6013)]  代码下载 https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211030KeyBERT关键词提取\n","permalink":"/blog/keybert%E5%85%B3%E9%94%AE%E8%AF%8D%E5%8F%91%E7%8E%B0/","summary":"尽管已经有很多方法可用于关键字生成（例如，Rake、YAKE!、TF-IDF 等），但我想创建一个非常基本但功能强大的方法来提取关键字和关键短语。这就是 KeyBERT 的用武之地！它使用 BERT 嵌入 和 简单余弦相似度 来查找文档中与文档本身最相似的短语。\nKeyBERT步骤\n 首先使用 BERT 提取文档嵌入以获得文档级向量表示。 随后，为 N-gram 词/短语提取词向量。 然后，我们使用余弦相似度来找到与文档最相似的单词/短语。 最后可以将最相似的词识别为最能描述整个文档的词。  安装 !pip3 install keybert==0.5.0 \n初始化模型 KeyBERT库需要安装配置spacy语言模型\n具体参考公众号：大邓和他的Python 2021-10-29 的推文 查看spacy配置方法\n初始化模型\nfrom keybert import KeyBERT import spacy import jieba zh_model = spacy.load(\u0026#34;zh_core_web_sm\u0026#34;) bertModel = KeyBERT(model=zh_model) \n准备数据 中文测试数据需要先分词，而后构造成类英文的语言结构(用空格间隔的文本)\n# 测试数据 doc = \u0026#34;\u0026#34;\u0026#34;时值10月25日抗美援朝纪念日，《长津湖》片方发布了“纪念中国人民志愿军抗美援朝出国作战71周年特别短片”，再次向伟大的志愿军致敬！ 电影《长津湖》全情全景地还原了71年前抗美援朝战场上那场史诗战役，志愿军奋不顾身的英勇精神令观众感叹：“岁月峥嵘英雄不灭，丹心铁骨军魂永存！”影片上映以来票房屡创新高，目前突破53亿元，暂列中国影史票房总榜第三名。 值得一提的是，这部影片的很多主创或有军人的血脉，或有当兵的经历，或者家人是军人。提起这些他们也充满自豪，影片总监制黄建新称：“当兵以后会有一种特别能坚持的劲儿。”饰演雷公的胡军透露：“我父亲曾经参加过抗美援朝，还得了一个三等功。”影片历史顾问王树增表示：“我当了五十多年的兵，我的老部队就是上甘岭上下来的，那些老兵都是我的偶像。” “身先士卒卫华夏家国，血战无畏护山河无恙。”片中饰演七连连长伍千里的吴京感叹：“要永远记住这些先烈们，他们给我们带来今天的和平。感谢他们的付出，才让我们有今天的幸福生活。”饰演新兵伍万里的易烊千玺表示：“战争的残酷、碾压式的伤害，其实我们现在的年轻人几乎很难能体会到，希望大家看完电影后能明白，是那些先辈们的牺牲奉献，换来了我们的现在。” 影片对战争群像的恢弘呈现，对个体命运的深切关怀，令许多观众无法控制自己的眼泪，观众称：“当看到影片中的惊险战斗场面，看到英雄们壮怀激烈的拼杀，为国捐躯的英勇无畏和无悔付出，我明白了为什么说今天的幸福生活来之不易。”（记者 王金跃） \u0026#34;\u0026#34;\u0026#34; doc = \u0026#39; \u0026#39;.join(jieba.lcut(doc)) # 关键词提取 keywords = bertModel.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10) keywords [('铁骨', 0.","title":"KeyBERT | 关键词发现"},{"content":"BERTopic 是一种主题建模技术，它利用 Transformer 和 c-TF-IDF 来创建密集的集群，允许轻松解释主题，同时在主题描述中保留重要词。\nBERTopic亮点\n 支持引导式Guided 支持（半）监督式 支持动态主题。 支持可视化  安装 !pip3 install bertopic==0.9.3 准备数据 这里使用的新闻数据集， 共2000条。 新闻类别涵 '娱乐', '教育', '游戏', '财经', '时政', '时尚', '科技', '体育', '家居', '房产' 这里假设大家不知道有10类新闻题材， 构建模型的时候不会用到label字段的数据。\nimport pandas as pd df = pd.read_csv(\u0026#39;data/cnews.csv\u0026#39;) df.head()   # 新闻题材 print(df.label.unique()) #记录数 print(len(df)) ['娱乐' '教育' '游戏' '财经' '时政' '时尚' '科技' '体育' '家居' '房产'] 2000  这里定义了一个清洗数据函数clean_text，需要注意BERTopic需要先将中文分词改造成类似英文文本格式（用空格间隔词语）\nimport re import jieba stoptext = open(\u0026#39;data/stopwords.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() stopwords = stoptext.split(\u0026#39;\\n\u0026#39;) def clean_text(text): words = jieba.lcut(text) words = [w for w in words if w not in stopwords] return \u0026#39; \u0026#39;.join(words) test = \u0026#34;云南永善县级地震已致人伤间民房受损中新网月日电据云南昭通市防震减灾局官方网站消息截至日时云南昭通永善县级地震已造成人受伤其中重伤人轻伤人已全部送医院救治民房受损户间倒塌户间个乡镇所学校不同程度受损目前被损毁电力交通通讯设施已全部抢通修复当地已调拨帐篷顶紧急转移万人月日时分云南昭通永善县发生里氏级地震震源深度公里当地震感强烈此外成都等四川多地也有明显震感\u0026#34; clean_text(test) '云南 永善县 级 地震 已致 伤间 民房 受损 中新网 月 日电 云南 昭通市 防震 减灾 局 官方网站 消息 截至 日时 云南 昭通 永善县 级 地震 受伤 重伤 轻伤 送 医院 救治 民房 受损 户间 倒塌 户间 乡镇 学校 程度 受损 损毁 电力 交通 通讯 设施 抢通 修复 当地 调拨 帐篷 紧急 转移 万人 月 日 时分 云南 昭通 永善县 发生 里氏 级 地震 震源 深度 公里 当地 震感 成都 四川 多地 震感'  对2000条数据进行clean_text，得到的结果存储到content字段中。\n我的macbook内存16G, 运行时间10s\ndf[\u0026#39;content\u0026#39;] = df[\u0026#39;text\u0026#39;].apply(clean_text) df.head()   训练Topic模型 文本分析步骤包括构建特征工程和训练，在本文中，直接使用spacy的中文词向量，省去了特征模型的学习时间。\n但这里需要\n 安装spacy 下载\u0026amp;安装zh_core_web_sm中文词向量模型  具体配置方法请看【公众号: 大邓和他的Python】2021年10月29日 的推文 建议收藏 | nltk和spacy配置方法\nfrom bertopic import BERTopic import spacy zh_model = spacy.load(\u0026#34;zh_core_web_sm\u0026#34;) topic_model = BERTopic(language=\u0026#34;chinese (simplified)\u0026#34;, embedding_model=zh_model, calculate_probabilities=True, verbose=True) docs = df[\u0026#39;content\u0026#39;].tolist() #2000条进行fit_transform需要1min topics, probs = topic_model.fit_transform(docs) 100%|██████████| 2000/2000 [01:31\u0026lt;00:00, 21.91it/s] 2021-10-28 12:11:25,583 - BERTopic - Transformed documents to Embeddings 2021-10-28 12:11:34,582 - BERTopic - Reduced dimensionality with UMAP 2021-10-28 12:11:34,718 - BERTopic - Clustered UMAP embeddings with HDBSCAN CPU times: user 1min 50s, sys: 7.7 s, total: 1min 57s Wall time: 1min 43s  \n主题模型方法  topic_model.get_topic_info 查看各主题信息 topic_model.find_topics(term, top_n=5) 查找term最有可能所属话题 topic_model.get_topic(0) 查看Topic 0的特征词 topic_model.visualize_topics() 话题间距离的可视化 topic_model.visualize_distribution(probs[0]) 查看某条文本的主题分布 topic_model.visualize_hierarchy(top_n_topics=20) 主题层次聚类可视化 topic_model.visualize_barchart(top_n_topics=6) 主题词条形图可视化 topic_model.visualize_heatmap(n_clusters=10) 主题相似度热力图 topic_model.visualize_term_rank() 可视化词语 topic_model.save() 保存主题模型  topic_model.get_topic_info()   similar_topics, similarity = topic_model.find_topics(\u0026#34;美国\u0026#34;, top_n=5) similar_topics [0, 3, 1, 2, -1]  topic_model.get_topic(0) [('中国', 0.017740927481291097), ('美国', 0.009187523853389844), ('国际', 0.007387509919710244), ('北京', 0.006355315208051378), ('台湾', 0.004591519972746738), ('上海', 0.00398117373168178), ('电影', 0.003959013801339396), ('文化', 0.003635760343311582), ('主持人 韩悦', 0.003598325963444241), ('全球', 0.0034710750361063997)]  topic_model.visualize_topics()   visualize_topics 显示第一条新闻的主题概率分布\ntopic_model.visualize_distribution(probs[0])   为了理解主题的潜在层次结构，我们可以使用 scipy.cluster.hierarchy 创建聚类并可视化它们之间的关系。 这有助于合并相似主题，达到降低主题模型主题数量nr_topics。\ntopic_model.visualize_hierarchy(top_n_topics=20)   topic_model.visualize_barchart(top_n_topics=6, width = 1000, height= 800)   BERTopic可将主题以embeddings形式（向量）表示， 因此我们可以应用余弦相似度来创建相似度矩阵。 每两两主题可进行余弦计算，最终结果将是一个矩阵，显示主题间的相似程度。\ntopic_model.visualize_heatmap(n_clusters=10, width=1000, height=1000)   通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。 从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。 此外，可以轻松地将主题表示相互比较。\ntopic_model.visualize_term_rank()   更新主题模型 当您训练了一个模型并查看了代表它们的主题和单词时，您可能对表示不满意。 也许您忘记删除停用词，或者您想尝试不同的 n_gram_range。 我们可以使用函数 update_topics 使用 c-TF-IDF 的新参数更新主题表示。\n经过更新，topic_model得到了更新，\ntopic_model.update_topics(df.content.tolist(), topics, n_gram_range=(1, 3)) similar_topics, similarity = topic_model.find_topics(\u0026#34;儿童\u0026#34;, top_n=5) similar_topics [11, 4, 7, -1, 2]  topic_model.get_topic(11) [('学生', 0.015023352605066086), ('儿童', 0.010260062682561771), ('投资', 0.00907917809925075), ('投资 移民', 0.008539711279754461), ('海外', 0.007267950590362874), ('学校', 0.006227402241189809), ('奖学金', 0.005431476690391167), ('留学人员', 0.00520544712332708), ('教师', 0.004945988616826368), ('联邦', 0.00465078395869278)]  # Save model #model.save(\u0026#34;my_model\u0026#34;) # Load model #my_model = BERTopic.load(\u0026#34;my_model\u0026#34;) \n压缩主题数 new_topics, new_probs = topic_model.reduce_topics(docs, topics, probs, nr_topics=10) 2021-10-28 12:28:01,976 - BERTopic - Reduced number of topics from 20 to 11  下载代码数据 https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211029BERTopic主题模型\n总结 本文使用中文文本数据展示BERTopic部分功能，如果对英文数据感兴趣，可以前往 https://github.com/MaartenGr/BERTopic 深入学习。\n","permalink":"/blog/bertopic%E4%B8%BB%E9%A2%98%E5%BB%BA%E6%A8%A1/","summary":"BERTopic 是一种主题建模技术，它利用 Transformer 和 c-TF-IDF 来创建密集的集群，允许轻松解释主题，同时在主题描述中保留重要词。\nBERTopic亮点\n 支持引导式Guided 支持（半）监督式 支持动态主题。 支持可视化  安装 !pip3 install bertopic==0.9.3 准备数据 这里使用的新闻数据集， 共2000条。 新闻类别涵 '娱乐', '教育', '游戏', '财经', '时政', '时尚', '科技', '体育', '家居', '房产' 这里假设大家不知道有10类新闻题材， 构建模型的时候不会用到label字段的数据。\nimport pandas as pd df = pd.read_csv(\u0026#39;data/cnews.csv\u0026#39;) df.head()   # 新闻题材 print(df.label.unique()) #记录数 print(len(df)) ['娱乐' '教育' '游戏' '财经' '时政' '时尚' '科技' '体育' '家居' '房产'] 2000  这里定义了一个清洗数据函数clean_text，需要注意BERTopic需要先将中文分词改造成类似英文文本格式（用空格间隔词语）\nimport re import jieba stoptext = open(\u0026#39;data/stopwords.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() stopwords = stoptext.","title":"BERTopic 主题建模库 | 建议收藏  "},{"content":"以往对比两个文本数据差异，比较简单的技术实现方法是生成两文个词云图，但是词云图无法直观显示词语层面的权重。\nShifterator包提供了构建词移图的功能，垂直条形图可以量化哪些词会导致两个文本之间的成对差异以及它们如何起作用。 通过允许您查看单词使用方式的变化，单词转换可帮助您对情绪、熵和分歧进行分析，这些分析从根本上来说更具可解释性。\nShifterator亮点：\n 提供可解释的工具，用于将文本作为数据处理并映射出两个文本相似性或差异性 实现常见的文本比较度量，包括相对频率、香农熵、Tsallis熵、Kullback-Leibler散度和 Jensen-Shannon 散度。 基于字典的情绪分析方法计算的加权平均值。 在研究初期可用于诊断数据、感知测量误差。  计算社会科学家、数字人文主义者和其他文本分析从业者都可以使用 Shifterator 从文本数据构建可靠、稳健和可解释的故事。\n安装 !pip3 install shifterator==0.2.2 \n导入数据 准备的外卖csv数据，含label和review两个字段。\n其中label是好评差评的标注，\n 0为差评， 1为好评  import pandas as pd reviews_df = pd.read_csv(\u0026#34;data/WaiMai8k.csv\u0026#34;, encoding=\u0026#39;utf-8\u0026#39;) reviews_df.head()   有个疑问，外卖好差评中的用词有什么差异(区别/特点)？\n准备两组文本数据 shifterator需要两组文本数据，格式为长度相同的词频统计字典。\n按照label类别，将数据整理为两个文本数据。在准备的过程中，我们需要做一些清洗操作\n 清除非中文字符，如网址、邮箱、标点符号 清除信息量比较低的停用词  import collections import jieba import re texts_neg = reviews_df[reviews_df[\u0026#39;label\u0026#39;]==0][\u0026#39;review\u0026#39;].tolist() texts_pos = reviews_df[reviews_df[\u0026#39;label\u0026#39;]==1][\u0026#39;review\u0026#39;].tolist() def clean_text(docs): \u0026#34;\u0026#34;\u0026#34;清洗文本中的非中文字符、停用词，返回词频统计结果 docs : 待处理的文档列表 \u0026#34;\u0026#34;\u0026#34; stop_words = open(\u0026#39;data/stopwords.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read().split(\u0026#39;\\n\u0026#39;) text = \u0026#34;\u0026#34;.join(docs) text = \u0026#34;\u0026#34;.join(re.findall(\u0026#34;[\\u4e00-\\u9fa5]+\u0026#34;, text)) words = jieba.lcut(text) words = [w for w in words if w not in stop_words] wordfreq_dict = collections.Counter(words) return wordfreq_dict clean_texts_neg = clean_text(texts_neg) clean_texts_pos = clean_text(texts_pos) \n无聊的传统绘图 使用条形图、词云图绘制，为了缩小代码量，这里只绘制差评数据。需要注意的是matplotlib不显示中文，这里需要先使用下面三行代码获取电脑中自带的中文字体列表mpl_fonts，\nfrom matplotlib.font_manager import FontManager mpl_fonts = set(f.name for f in FontManager().ttflist) print(mpl_fonts) 经过运行，我的电脑mpl_fonts里有Arial Unicode MS ，后面用matplotlib显示中文的地方，我都使用该字体。\n#准备DataFrame数据 common_neg = pd.DataFrame(clean_texts_neg.most_common(15), columns=[\u0026#39;words\u0026#39;, \u0026#39;count\u0026#39;]) common_pos = pd.DataFrame(clean_texts_pos.most_common(15), columns=[\u0026#39;words\u0026#39;, \u0026#39;count\u0026#39;]) from matplotlib import pyplot as plt import seaborn as sns import matplotlib sns.set(font_scale=1.5) sns.set_style(\u0026#34;whitegrid\u0026#34;) #为了显示中文 matplotlib.rc(\u0026#34;font\u0026#34;, family=\u0026#39;Arial Unicode MS\u0026#39;) fig, ax = plt.subplots(figsize=(16, 8)) #绘制水平条形图 common_neg.sort_values(by=\u0026#39;count\u0026#39;).plot.barh(x=\u0026#39;words\u0026#39;, y=\u0026#39;count\u0026#39;, ax=ax, color=\u0026#34;red\u0026#34;) ax.set_title(\u0026#34;外卖差评常见词\u0026#34;) plt.show()   绘制词云图，这里使用的pyecharts包。由于该包作者更新强度比较大，为了保证日后本教程仍可正常运行，这里提供当前我使用的pyecharts相关的版本，大家可以运行下面代码保证运行出正确结果\n!pip3 install pyecharts==1.6.2 !pip3 install pyecharts-javascripthon==0.0.6 !pip3 install pyecharts-jupyter-installer==0.0.3 !pip3 install pyecharts-snapshot==0.2.0 import pyecharts.options as opts from pyecharts.charts import WordCloud from pyecharts.globals import CurrentConfig, NotebookType CurrentConfig.NOTEBOOK_TYPE = NotebookType.JUPYTER_NOTEBOOK wordfreqs = [(w, str(f)) for w,f in dict(clean_texts_neg).items()] wc = WordCloud() wc.add(series_name=\u0026#34;\u0026#34;, data_pair=wordfreqs, word_size_range=[20, 100]) wc.set_global_opts(title_opts=opts.TitleOpts(title=\u0026#34;外卖差评词云图\u0026#34;, title_textstyle_opts=opts.TextStyleOpts(font_size=23)), tooltip_opts=opts.TooltipOpts(is_show=True)) wc.load_javascript() wc.render_notebook()   使用Shifterator绘制词移图 终于要用到 Shifterator 包了！ 我们可以使用这个包根据频率和情绪（或其他值）比较负面和正面的外卖评论，这里我只计算了频率作为权重\n熵移图Entropy shift 第一幅图是entropy shift graph\n具体信息请查看文档 https://github.com/ryanjgallagher/shifterator\nfrom shifterator import EntropyShift import matplotlib matplotlib.rc(\u0026#34;font\u0026#34;, family=\u0026#39;Arial Unicode MS\u0026#39;) entropy_shift = EntropyShift(type2freq_1=clean_texts_neg, type2freq_2=clean_texts_pos, base=2) entropy_shift.get_shift_graph(title=\u0026#39;外卖差评 vs 外卖好评\u0026#39;)   看起来最能决定外卖差评的用语是配送时间，其次才是口味。\n最能决定外卖好评的似乎是口味，其次才是配送时间。\n通过Shifterator我们能够看出不同词在不同文本中的作用程度。需要注意的是，我们只使用了最高的前15词频，所以显示的词有些少\n总结 希望本文能对你的研究有帮助，代码下载地址\nhttps://github.com/hidadeng/DaDengAndHisPython/tree/master/20211027shifterator学习\n代码撰写调试不易，希望帮忙转载\n  ","permalink":"/blog/shifterator%E5%AD%A6%E4%B9%A0md/","summary":"以往对比两个文本数据差异，比较简单的技术实现方法是生成两文个词云图，但是词云图无法直观显示词语层面的权重。\nShifterator包提供了构建词移图的功能，垂直条形图可以量化哪些词会导致两个文本之间的成对差异以及它们如何起作用。 通过允许您查看单词使用方式的变化，单词转换可帮助您对情绪、熵和分歧进行分析，这些分析从根本上来说更具可解释性。\nShifterator亮点：\n 提供可解释的工具，用于将文本作为数据处理并映射出两个文本相似性或差异性 实现常见的文本比较度量，包括相对频率、香农熵、Tsallis熵、Kullback-Leibler散度和 Jensen-Shannon 散度。 基于字典的情绪分析方法计算的加权平均值。 在研究初期可用于诊断数据、感知测量误差。  计算社会科学家、数字人文主义者和其他文本分析从业者都可以使用 Shifterator 从文本数据构建可靠、稳健和可解释的故事。\n安装 !pip3 install shifterator==0.2.2 \n导入数据 准备的外卖csv数据，含label和review两个字段。\n其中label是好评差评的标注，\n 0为差评， 1为好评  import pandas as pd reviews_df = pd.read_csv(\u0026#34;data/WaiMai8k.csv\u0026#34;, encoding=\u0026#39;utf-8\u0026#39;) reviews_df.head()   有个疑问，外卖好差评中的用词有什么差异(区别/特点)？\n准备两组文本数据 shifterator需要两组文本数据，格式为长度相同的词频统计字典。\n按照label类别，将数据整理为两个文本数据。在准备的过程中，我们需要做一些清洗操作\n 清除非中文字符，如网址、邮箱、标点符号 清除信息量比较低的停用词  import collections import jieba import re texts_neg = reviews_df[reviews_df[\u0026#39;label\u0026#39;]==0][\u0026#39;review\u0026#39;].tolist() texts_pos = reviews_df[reviews_df[\u0026#39;label\u0026#39;]==1][\u0026#39;review\u0026#39;].tolist() def clean_text(docs): \u0026#34;\u0026#34;\u0026#34;清洗文本中的非中文字符、停用词，返回词频统计结果 docs : 待处理的文档列表 \u0026#34;\u0026#34;\u0026#34; stop_words = open(\u0026#39;data/stopwords.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read().split(\u0026#39;\\n\u0026#39;) text = \u0026#34;\u0026#34;.","title":"Shifterator库 | 词移图分辨两文本用词风格差异"},{"content":"代码下载 点击此处下载代码\n 原文链接 https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-24207127cad7\n  想象一下，你正试图训练一个机器学习模型来预测广告是否被特定的人点击。在收到关于某人的一些信息后，模型预测某人会不会点击广告。\n  但是为什么模型会输出这样的预测结果呢？ 每个特征对预测的贡献有多大？ 如果您能看到一个图表，显示每个特征对预测的贡献程度，如下所示，不是很好吗？\n  Shapley值就能起到特征权重测度的作用。\nShapley值是什么？ Shapley值是博弈论中使用的一种方法，它涉及公平地将收益和成本分配给在联盟中工作的行动者。 由于每个行动者对联盟的贡献是不同的，Shapley值保证每个行动者根据贡献的多少获得公平的份额。\n  小案例 Shapley值被广泛地应用于求解群体中每个工人(特征)的贡献问题。要理解Shapley值的作用，让我们想象一下贵公司刚刚做了A/B测试，他们在测试广告策略的不同组合。\n每个策略在特定月份的收入是：\n 无广告：150美元 社交媒体：300美元 谷歌广告：200美元 电子邮件营销：350美元 社交媒体和谷歌广告：320美元 社交媒体和电子邮件营销：400美元 谷歌广告和电子邮件营销：350美元 电子邮件营销，谷歌广告和社交媒体：450美元    使用三则广告与不使用广告的收入相差300美元，每则广告对这一差异有多大的贡献?\n  我们可以通过计算每一类广告的Shapley值来计算谷歌广告对公司收入的总贡献入手，通过公式可以计算出Google广告的总贡献：\n  让我们找到Google广告的边际贡献及其权重。\n寻找谷歌广告的边际贡献 第一，我们将发现谷歌广告对以下群体的边际贡献：\n 无广告 谷歌广告+社交媒体 谷歌广告+电子邮件营销 谷歌广告+电子邮件营销+社交媒体    Google广告 对 无广告 的边际贡献是：\n  谷歌广告 对 谷歌广告\u0026amp;社交媒体组合 的边际贡献是：\n  谷歌广告 对 谷歌广告\u0026amp;电子邮件营销组合 的边际贡献是：\n  谷歌广告 对 谷歌广告、电子邮件营销和社交媒体组合 的边际贡献是：\n  发现权重 为了发现权重，我们将把不同广告策略的组合组织成如下多个层次，每个层次对应于每个组合中广告策略的数量。\n然后根据每个层次的边数分配权重，我们看到了这一点：\n 第一级包含3条边，因此每个边的权重为1/3 第二级包含6条边，因此每条边的权重将为1/6 第三级包含3条边，因此每条边的权重将为1/3    发现Google广告的总贡献 根据前面的权重和边际贡献，我们已经可以找到Google广告的总贡献!\n    酷!所以谷歌广告在使用3种广告策略与不使用广告的总收入差异中贡献了36.67美元。36.67是Google广告的Shapey值。\n  重复以上步骤，对于另外两种广告策略，我们可以看出：\n  电子邮件营销贡献151.67美元\n  社交媒体贡献116.67美元\n  谷歌广告贡献36.67美元\n    他们共同出资300美元，用于使用3种不同类型的广告与不使用广告的区别!挺酷的，不是吗? 既然我们理解了Shapley值，那么让我们看看如何使用它来解释机器学习模型。\nSHAP-在Python中解释机器学习模型 SHAP是一个Python库，它使用Shapley值来解释任何机器学习模型的输出。\n安装SHAP\n!pip3 install shap 训练模型 为了理解SHAP工作原理，我们使用Kaggle平台内的advertising广告数据集。\nimport pandas as pd df = pd.read_csv(\u0026#34;advertising.csv\u0026#34;) df.head()   我们将建立一个机器学习模型, 该模型根据用户个人特质信息来预测其是否点击广告。\n我们使用Patsy将DataFrame转换为一组特征和一组目标值：\nfrom patsy import dmatrices from sklearn.model_selection import train_test_split y, X = dmatrices( \u0026#34;clicked_on_ad ~ daily_time_spent_on_site + age + area_income + daily_internet_usage + male -1\u0026#34;, data=df, ) X_frame = pd.DataFrame(data=X, columns=X.design_info.column_names) 把数据分为测试集和训练接\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7) 接下来使用XGBoost训练模型，并做预测\nimport xgboost model = xgboost.XGBClassifier().fit(X_train, y_train) y_predicted = model.predict(X_test) 为了查看模型表现，我们使用F1得分\nfrom sklearn.metrics import f1_score f1 = f1_score(y_test, y_predicted) f1 0.9619047619047619  太好了!\n解释该模型 该模型很好地预测了用户是否点击广告。但它是如何得出这样的预测的? 每个特征对最终预测与平均预测的差异贡献了多少?\n注意，这个问题与我们在文章开头论述的问题非常相似。\n因此，寻找每个特征的Shapley值可以帮助我们确定它们的贡献。得到特征i的重要性的步骤与之前类似，其中i是特征的索引：\n 获取所有不包含特征i的子集 找出特征i对这些子集中每个子集的边际贡献 聚合所有边际贡献来计算特征i的贡献  若要使用SHAP查找Shapley值，只需将训练好的模型插入shap.Explainer\nimport shap explainer = shap.Explainer(model) shap_values = explainer(X_frame) ntree_limit is deprecated, use `iteration_range` or model slicing instead.  SHAP瀑布图 可视化第一个预测的解释：\n#第一条记录是未点击 shap.plots.waterfall(shap_values[0])   啊哈!现在我们知道每个特征对第一次预测的贡献。对上图的解释：\n   蓝色条显示某一特定特征在多大程度上降低了预测的值。 红条显示了一个特定的特征在多大程度上增加了预测值。 负值意味着该人点击广告的概率小于0.5  我们应该期望总贡献等于预测与均值预测的差值。我们来验证一下：\n  酷!他们是平等的。\n可视化第二个预测的解释：\n#第二条记录也是未点击 shap.plots.waterfall(shap_values[1])   SHAP摘要图 我们可以使用SHAP摘要图，而不是查看每个单独的实例，来可视化这些特性对多个实例的整体影响：\nshap.summary_plot(shap_values, X)   SHAP摘要图告诉我们数据集上最重要的特征及其影响范围。\n从上面的情节中，我们可以对模型的预测获得一些有趣的见解：\n 用户的 daily_internet_usage 对该用户是否点击广告的影响最大。 随着daily_time_spent_on_site的增加，用户点击广告的可能性降低。 随着area_income的增加，用户点击广告的可能性降低。 随着age的增长，用户更容易点击广告。 如果用户是male，则该用户点击广告的可能性较小。  SHAP条形图 我们还可以使用SHAP条形图得到全局特征重要性图。\nshap.plots.bar(shap_values)   很酷!\n结论 恭喜你!您刚刚了解了Shapey值以及如何使用它来解释一个机器学习模型。希望本文将提供您使用Python来解释自己的机器学习模型的基本知识。\n","permalink":"/blog/shap%E5%AD%A6%E4%B9%A0/","summary":"代码下载 点击此处下载代码\n 原文链接 https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-24207127cad7\n  想象一下，你正试图训练一个机器学习模型来预测广告是否被特定的人点击。在收到关于某人的一些信息后，模型预测某人会不会点击广告。\n  但是为什么模型会输出这样的预测结果呢？ 每个特征对预测的贡献有多大？ 如果您能看到一个图表，显示每个特征对预测的贡献程度，如下所示，不是很好吗？\n  Shapley值就能起到特征权重测度的作用。\nShapley值是什么？ Shapley值是博弈论中使用的一种方法，它涉及公平地将收益和成本分配给在联盟中工作的行动者。 由于每个行动者对联盟的贡献是不同的，Shapley值保证每个行动者根据贡献的多少获得公平的份额。\n  小案例 Shapley值被广泛地应用于求解群体中每个工人(特征)的贡献问题。要理解Shapley值的作用，让我们想象一下贵公司刚刚做了A/B测试，他们在测试广告策略的不同组合。\n每个策略在特定月份的收入是：\n 无广告：150美元 社交媒体：300美元 谷歌广告：200美元 电子邮件营销：350美元 社交媒体和谷歌广告：320美元 社交媒体和电子邮件营销：400美元 谷歌广告和电子邮件营销：350美元 电子邮件营销，谷歌广告和社交媒体：450美元    使用三则广告与不使用广告的收入相差300美元，每则广告对这一差异有多大的贡献?\n  我们可以通过计算每一类广告的Shapley值来计算谷歌广告对公司收入的总贡献入手，通过公式可以计算出Google广告的总贡献：\n  让我们找到Google广告的边际贡献及其权重。\n寻找谷歌广告的边际贡献 第一，我们将发现谷歌广告对以下群体的边际贡献：\n 无广告 谷歌广告+社交媒体 谷歌广告+电子邮件营销 谷歌广告+电子邮件营销+社交媒体    Google广告 对 无广告 的边际贡献是：\n  谷歌广告 对 谷歌广告\u0026amp;社交媒体组合 的边际贡献是：\n  谷歌广告 对 谷歌广告\u0026amp;电子邮件营销组合 的边际贡献是：\n  谷歌广告 对 谷歌广告、电子邮件营销和社交媒体组合 的边际贡献是：","title":"SHAP机器学习模型解释库"},{"content":"SmartScraper: 简单、自动、快捷的Python网络爬虫  Note: The origin developer of SmartScraper is Alireza Mika， I only change a little code of AutoScraper.\n SmartScraper使页面数据抓取变得容易，不再需要学习诸如pyquery、beautifulsoup等定位包，我们只需要提供的url和数据给ta学习网页定位规律即可。\n一、安装 pip install smartscraper \n二、快速上手 2.1 获取相似结果 例如 我们想从 豆瓣读书-小说 页面获得20本书的书名和出版信息\n P1 https://book.douban.com/tag/小说?start=0\u0026amp;type=T P2 https://book.douban.com/tag/小说?start=20\u0026amp;type=T    我们使用P1链接训练书名、出版信息这两个字段\nfrom smartscraper import SmartScraper # 待训练的网页链接 url = \u0026#39;https://book.douban.com/tag/小说?start=0\u0026amp;type=T\u0026#39; #定义 想要的字段 wanted_dict = {\u0026#34;title\u0026#34;:[\u0026#34;活着\u0026#34;], \u0026#34;pub\u0026#34;: [\u0026#34;余华 / 作家出版社 / 2012-8-1 / 20.00元\u0026#34;] } # 训练/在url对应的页面中寻找wanted_dict规律 scraper = SmartScraper() results = scraper.build(url, wanted_dict=wanted_dict) print(results) 运行代码，采集到的results如下\n{\u0026#39;title\u0026#39;: [\u0026#39;活着\u0026#39;, \u0026#39;房思琪的初恋乐园\u0026#39;, \u0026#39;白夜行\u0026#39;, \u0026#39;索拉里斯星\u0026#39;, \u0026#39;鄙视\u0026#39;, ...], \u0026#39;pub\u0026#39;: [\u0026#39;余华 / 作家出版社 / 2012-8-1 / 20.00元\u0026#39;, \u0026#39;林奕含 / 北京联合出版公司 / 2018-2 / 45.00元\u0026#39;, \u0026#39;[日] 东野圭吾 / 刘姿君 / 南海出版公司 / 2013-1-1 / CNY 39.50\u0026#39;, \u0026#39;[波] 斯坦尼斯瓦夫·莱姆 / 靖振忠 / 译林出版社 / 2021-8 / 49.00元\u0026#39;, \u0026#39;[意] 阿尔贝托·莫拉维亚 / 沈萼梅、刘锡荣 / 江苏凤凰文艺出版社 / 2021-7 / 62.00\u0026#39;, ...] } 使用刚刚训练的scraper尝试从 P2链接 获取书名和出版信息\nscraper.get_result_similar(\u0026#39;https://book.douban.com/tag/小说?start=20\u0026amp;type=T\u0026#39;) \n2.2 保存模型 训练的smartscraper模型可以保存，后续直接调用\nscraper.save(\u0026#39;douban_Book.pkl\u0026#39;) 模型导入代码\nscraper.load(\u0026#39;douban_Book.pkl\u0026#39;) \n","permalink":"/blog/smartscraper/","summary":"SmartScraper: 简单、自动、快捷的Python网络爬虫  Note: The origin developer of SmartScraper is Alireza Mika， I only change a little code of AutoScraper.\n SmartScraper使页面数据抓取变得容易，不再需要学习诸如pyquery、beautifulsoup等定位包，我们只需要提供的url和数据给ta学习网页定位规律即可。\n一、安装 pip install smartscraper \n二、快速上手 2.1 获取相似结果 例如 我们想从 豆瓣读书-小说 页面获得20本书的书名和出版信息\n P1 https://book.douban.com/tag/小说?start=0\u0026amp;type=T P2 https://book.douban.com/tag/小说?start=20\u0026amp;type=T    我们使用P1链接训练书名、出版信息这两个字段\nfrom smartscraper import SmartScraper # 待训练的网页链接 url = \u0026#39;https://book.douban.com/tag/小说?start=0\u0026amp;type=T\u0026#39; #定义 想要的字段 wanted_dict = {\u0026#34;title\u0026#34;:[\u0026#34;活着\u0026#34;], \u0026#34;pub\u0026#34;: [\u0026#34;余华 / 作家出版社 / 2012-8-1 / 20.00元\u0026#34;] } # 训练/在url对应的页面中寻找wanted_dict规律 scraper = SmartScraper() results = scraper.","title":"SmartScraper | 简单、自动、快捷的Python网络爬虫"},{"content":"\n 作者 bot_developer\n搬运自\n https://www.kaggle.com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests https://github.com/bot-developer3/Scraping-Tools-Benzinga.   背景  获得高质量（历史）股市新闻数据既困难又昂贵； 订阅历史新闻数据提供商服务可能需要花费数千美元。  \n数据集 采集了6000只股票2009-2020年间的4万条新闻文章\nraw_analyst_ratings.csv 直接抓取分析师评级数据，有1034位分析师对6204只股票进行了股票分析，分析记录累积1407328条, 字段包括：索引、标题、URL、文章作者（出版商总是benzinga）、出版时间戳、股票代码。\n请注意，此 CSV 文件中的所有日期均不包含精确的时分秒信息。 如果您打算使用此文件进行回测（analyst_ratings_processed.csv 更好），请假设文章是在第二天而不是当前文章中显示的日期发布的。\nraw_partner_headlines.csv 直接抓取原始新闻标题，共有1845559条记录, 字段包括：索引、标题、URL、出版商（不是 benzinga）、日期、股票行情。\nanalyst_ratings_processed.csv 处理过的分析师评级数据， 共有1400469条记录， 字段包括：文章标题，日期，股票\n时区为 UTC-4。 这与 raw_analys_theadlines 之间的区别在于，它具有精确到分钟的日期，而 raw_analys_tratings 只是没有小时或分钟的那一天。\n\n注意  数据爬自benzinga.com，新闻内容版权归Benzinga所有。  数据下载 链接:https://pan.baidu.com/s/1rMo4Ek2bxvVLmeyxskVCAg 密码:paen\n广告 对新闻类文本的分析与处理，如果这类数据大到一定程度，需要\n 结合python、pandas导入数据，进行文本清洗 结合已有相关情感词典，进行情感分析 使用词典或向量法扩充领域词典，再进行情感分析 也可使用机器学习，进行LDA话题分析，或文本分类甚至是机器学习。  以上技术储备在我的《Python网络爬虫与文本分析》 课中均有讲授，感兴趣的童鞋可以点击链接了解详情\n","permalink":"/blog/daily_financial_news_for_6000_stocks/","summary":"作者 bot_developer\n搬运自\n https://www.kaggle.com/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests https://github.com/bot-developer3/Scraping-Tools-Benzinga.   背景  获得高质量（历史）股市新闻数据既困难又昂贵； 订阅历史新闻数据提供商服务可能需要花费数千美元。  \n数据集 采集了6000只股票2009-2020年间的4万条新闻文章\nraw_analyst_ratings.csv 直接抓取分析师评级数据，有1034位分析师对6204只股票进行了股票分析，分析记录累积1407328条, 字段包括：索引、标题、URL、文章作者（出版商总是benzinga）、出版时间戳、股票代码。\n请注意，此 CSV 文件中的所有日期均不包含精确的时分秒信息。 如果您打算使用此文件进行回测（analyst_ratings_processed.csv 更好），请假设文章是在第二天而不是当前文章中显示的日期发布的。\nraw_partner_headlines.csv 直接抓取原始新闻标题，共有1845559条记录, 字段包括：索引、标题、URL、出版商（不是 benzinga）、日期、股票行情。\nanalyst_ratings_processed.csv 处理过的分析师评级数据， 共有1400469条记录， 字段包括：文章标题，日期，股票\n时区为 UTC-4。 这与 raw_analys_theadlines 之间的区别在于，它具有精确到分钟的日期，而 raw_analys_tratings 只是没有小时或分钟的那一天。\n\n注意  数据爬自benzinga.com，新闻内容版权归Benzinga所有。  数据下载 链接:https://pan.baidu.com/s/1rMo4Ek2bxvVLmeyxskVCAg 密码:paen\n广告 对新闻类文本的分析与处理，如果这类数据大到一定程度，需要\n 结合python、pandas导入数据，进行文本清洗 结合已有相关情感词典，进行情感分析 使用词典或向量法扩充领域词典，再进行情感分析 也可使用机器学习，进行LDA话题分析，或文本分类甚至是机器学习。  以上技术储备在我的《Python网络爬虫与文本分析》 课中均有讲授，感兴趣的童鞋可以点击链接了解详情","title":"DataShare | 6000+个股票的每日财经新闻"},{"content":"label-studio 假设我们想使用机器学习做文本分析，一般都需要先对数据进行标注，才能训练出效果比较好的监督机器学习模型。\nlabel-studio是多媒体数据标注工具，可以很方便的进行标注和导出。\nLabel Studio 是一款开源数据标注工具，用于标注和探索多种类型的数据。 您可以使用多种数据格式执行的标记任务。\n您还可以将 Label Studio 与机器学习模型集成，以提供标签（预标签）的预测，或执行持续的主动学习。\n官方文档 https://labelstud.io/\n操作步骤  安装Label Studio 启动Label Studio 创建Label Studio账号 项目默认配置 导入数据 标注数据 结束标记，导出标注数据  安装 命令行中执行\npip install label-studio==1.1.0 2 快速上手 在桌面创建自动生成一个名为Project的项目文件夹。\n Win命令行执行  label-studio --data-dir Desktop/Project  Mac命令行执行  label-studio --data-dir desktop/Project \n执行上方代码大概10s左右，会在浏览器弹出如下界面\n  注册好账号密码，点击Create Project\n  项目描述填写好，点击按钮**Data Import **，\n  这里我们要做文本分析，导入csv\n    设置标注模式，点击按钮Labeling Setup,选择Natural Language Process、TEXT Classification。就考研进行pos、neg、neo三个类别的文本标注。\n  注意label-studio提供了diy，考研根据自己需要点击Code设定标注类别名称、增减类别。大家感兴趣的可以深入研究。\n  点击Save 按钮，开始准备标注数据啦\n数据界面，勾选全部数据，点击蓝色按钮Label All Tasks\n  开始标注，勾选你认为合适的标签，点击右侧Submit\n  导出标注数据,先点击右侧Export按钮，选择导出格式，最后点击底部Export按钮执行导出。\n  ","permalink":"/blog/label-studio%E5%AD%A6%E4%B9%A0/","summary":"label-studio 假设我们想使用机器学习做文本分析，一般都需要先对数据进行标注，才能训练出效果比较好的监督机器学习模型。\nlabel-studio是多媒体数据标注工具，可以很方便的进行标注和导出。\nLabel Studio 是一款开源数据标注工具，用于标注和探索多种类型的数据。 您可以使用多种数据格式执行的标记任务。\n您还可以将 Label Studio 与机器学习模型集成，以提供标签（预标签）的预测，或执行持续的主动学习。\n官方文档 https://labelstud.io/\n操作步骤  安装Label Studio 启动Label Studio 创建Label Studio账号 项目默认配置 导入数据 标注数据 结束标记，导出标注数据  安装 命令行中执行\npip install label-studio==1.1.0 2 快速上手 在桌面创建自动生成一个名为Project的项目文件夹。\n Win命令行执行  label-studio --data-dir Desktop/Project  Mac命令行执行  label-studio --data-dir desktop/Project \n执行上方代码大概10s左右，会在浏览器弹出如下界面\n  注册好账号密码，点击Create Project\n  项目描述填写好，点击按钮**Data Import **，\n  这里我们要做文本分析，导入csv\n    设置标注模式，点击按钮Labeling Setup,选择Natural Language Process、TEXT Classification。就考研进行pos、neg、neo三个类别的文本标注。\n  注意label-studio提供了diy，考研根据自己需要点击Code设定标注类别名称、增减类别。大家感兴趣的可以深入研究。","title":"Label-Studio|多媒体数据标注工具"},{"content":"\n情感分析  无权重。直接计算文本中正、负情感词出现的次数 有权重。tf-idf， tf是词频，idf是权重。  Tfidf法 scikit库除了CountVectorizer类，还有TfidfVectorizer类。TF-IDF这个定义相信大家应该已经耳熟能详了：\n       TF 词语出现越多，这个词越有信息量 IDF 词语越少的出现在文本中，词语越有信息量。  原始数据 import pandas as pd corpus = [\u0026#34;hello, i am glad to meet you\u0026#34;, \u0026#34;it is wonderful\u0026#34;, \u0026#34;i hate you\u0026#34;, \u0026#34;i am sad\u0026#34;] df1 = pd.DataFrame(corpus, columns=[\u0026#39;Text\u0026#39;]) df1   构造tfidf from sklearn.feature_extraction.text import TfidfVectorizer def createDTM(corpus): \u0026#34;\u0026#34;\u0026#34;构建文档词语矩阵\u0026#34;\u0026#34;\u0026#34; vectorize = TfidfVectorizer() #注意fit_transform相当于fit之后又transform。 dtm = vectorize.fit_transform(corpus) #vectorize.fit(corpus) #dtm = vectorize.transform(corpus)  #打印dtm return pd.DataFrame(dtm.toarray(), columns=vectorize.get_feature_names()) df2 = createDTM(df[\u0026#39;text\u0026#39;]) df2   合并df1和df2 df = pd.concat([df1, df2], axis=1) df   #积极词典 pos_words = [\u0026#39;glad\u0026#39;, \u0026#39;hello\u0026#39;, \u0026#39;wonderful\u0026#39;] #消极词典 neg_words = [\u0026#39;sad\u0026#39;, \u0026#39;hate\u0026#39;] #积极词典 df[pos_words] df[pos_words].sum(axis=1) Run\n0 0.873439 1 0.577350 2 0.000000 3 0.000000 dtype: float64  df[\u0026#39;Pos\u0026#39;] = df[pos_words].sum(axis=1) df   df[\u0026#39;Neg\u0026#39;] = df[neg_words].sum(axis=1) df   输出 df.to_csv(\u0026#39;output/tfidf有权重的情感分析.csv\u0026#39;) ","permalink":"/blog/tfidf%E6%9C%89%E6%9D%83%E9%87%8D%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/","summary":"情感分析  无权重。直接计算文本中正、负情感词出现的次数 有权重。tf-idf， tf是词频，idf是权重。  Tfidf法 scikit库除了CountVectorizer类，还有TfidfVectorizer类。TF-IDF这个定义相信大家应该已经耳熟能详了：\n       TF 词语出现越多，这个词越有信息量 IDF 词语越少的出现在文本中，词语越有信息量。  原始数据 import pandas as pd corpus = [\u0026#34;hello, i am glad to meet you\u0026#34;, \u0026#34;it is wonderful\u0026#34;, \u0026#34;i hate you\u0026#34;, \u0026#34;i am sad\u0026#34;] df1 = pd.DataFrame(corpus, columns=[\u0026#39;Text\u0026#39;]) df1   构造tfidf from sklearn.feature_extraction.text import TfidfVectorizer def createDTM(corpus): \u0026#34;\u0026#34;\u0026#34;构建文档词语矩阵\u0026#34;\u0026#34;\u0026#34; vectorize = TfidfVectorizer() #注意fit_transform相当于fit之后又transform。 dtm = vectorize.fit_transform(corpus) #vectorize.fit(corpus) #dtm = vectorize.transform(corpus)  #打印dtm return pd.","title":"tfidf有权重的情感分析"},{"content":"可以使用cnsenti库中的自定义方法，计算年报或财经类社交媒体的文本情绪。\n 姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报，2021. 24(5), 26-46.\n 该论文开发了中文的金融情感词典，已有的中文金融情感词典有以下不足：\n 大多采用形容情绪词，对于金融场景适用性差 将LM英文词典本土化，制作中文金融情绪词典 词典构建方法多为人工  该论文开发中文情绪词典，从年报和社交媒体两个数据源出发，借助数据挖掘和深度学习算法，构建了正式用语 和 非正式用于两大类情感词典。\n标注思路 一般构建词典要么用多个词典融合，要么人工标准训练。该论文采用了一定的技巧，不需要人工标注即可实现近乎人工标注的效果。\n正式词典标注思路 正式用语情感词典，通过年报公布后3个交易日累积正负收益率为标准，将年报标记为正负面情绪两类。\n非正式词典标注思路 使用所有中国上市公司在雪球论坛和东方财富股吧内相关帖子，共8130万条。\n在网络股票论坛，用户发表自己的意见时，经常带有表情符号，从而使得帖子带有明显的情绪指标。 这种含有特殊指标的帖子，省去了人工标注文本情绪的工作。\n具体构建词典的步骤，大家可以阅读论文原文。论文已经公开了中文情感词典，我已将其整理为4个txt文件\n formal_pos.txt 正式用语正面情绪词典 formal_neg.txt 正式用语负面情绪词典 unformal_pos.txt 非正式用语正面情绪词典 unformal_neg.txt 非正式用语负面情绪词典  中文金融词典使用方法 cnsenti实现了自定义词典功能，导入不同的txt词典文件，即可实现不同方面的情绪词统计。\n年报正式用语词典  dict/formal_pos.txt 正式用语正面情绪词典 dict/formal_neg.txt 正式用语负面情绪词典  from cnsenti import Sentiment senti = Sentiment(pos=\u0026#39;dict/formal_pos.txt\u0026#39;, #正面词典txt文件相对路径 neg=\u0026#39;dict/formal_neg.txt\u0026#39;, #负面词典txt文件相对路径 merge=False, #是否将cnsenti自带词典和用户导入的自定义词典融合 encoding=\u0026#39;utf-8\u0026#39;) #两txt均为utf-8编码 test_text = \u0026#39;这家公司是行业的引领者，是中流砥柱。今年的业绩非常好。\u0026#39; result = senti.sentiment_count(test_text) print(\u0026#39;sentiment_count\u0026#39;,result) Run\nsentiment_count {\u0026#39;words\u0026#39;: 16, \u0026#39;sentences\u0026#39;: 2, \u0026#39;pos\u0026#39;: 3, \u0026#39;neg\u0026#39;: 0} \n财经社交媒体非正式用语词典  dict/unformal_pos.txt 非正式用语正面情绪词典 dict/unformal_neg.txt 非正式用语负面情绪词典  from cnsenti import Sentiment senti = Sentiment(pos=\u0026#39;dict/unformal_pos.txt\u0026#39;, #正面词典txt文件相对路径 neg=\u0026#39;dict/unformal_neg.txt\u0026#39;, #负面词典txt文件相对路径 merge=False, #融合cnsenti自带词典和用户导入的自定义词典 encoding=\u0026#39;utf-8\u0026#39;) #两txt均为utf-8编码 test_text = \u0026#39;这个股票前期走势承压，现在阴跌，散户只能割肉离场，这股票真垃圾\u0026#39; result = senti.sentiment_count(test_text) print(\u0026#39;sentiment_count\u0026#39;,result) Run\nsentiment_count {\u0026#39;words\u0026#39;: 18, \u0026#39;sentences\u0026#39;: 1, \u0026#39;pos\u0026#39;: 0, \u0026#39;neg\u0026#39;: 2} \n说明 读者如需使用本项目词典，请引用如下参考文献：\n 姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报，2021. 24(5), 26-46.\n 另外，Python暑期工作坊现在正在报名，内容涵盖Python语法、数据采集(网络爬虫)、文本数据清洗（文本分析）、机器学习等。20号开始直播。感兴趣的可以关注\n","permalink":"/blog/%E4%B8%AD%E6%96%87%E9%87%91%E8%9E%8D%E8%AF%8D%E5%85%B8/","summary":"可以使用cnsenti库中的自定义方法，计算年报或财经类社交媒体的文本情绪。\n 姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报，2021. 24(5), 26-46.\n 该论文开发了中文的金融情感词典，已有的中文金融情感词典有以下不足：\n 大多采用形容情绪词，对于金融场景适用性差 将LM英文词典本土化，制作中文金融情绪词典 词典构建方法多为人工  该论文开发中文情绪词典，从年报和社交媒体两个数据源出发，借助数据挖掘和深度学习算法，构建了正式用语 和 非正式用于两大类情感词典。\n标注思路 一般构建词典要么用多个词典融合，要么人工标准训练。该论文采用了一定的技巧，不需要人工标注即可实现近乎人工标注的效果。\n正式词典标注思路 正式用语情感词典，通过年报公布后3个交易日累积正负收益率为标准，将年报标记为正负面情绪两类。\n非正式词典标注思路 使用所有中国上市公司在雪球论坛和东方财富股吧内相关帖子，共8130万条。\n在网络股票论坛，用户发表自己的意见时，经常带有表情符号，从而使得帖子带有明显的情绪指标。 这种含有特殊指标的帖子，省去了人工标注文本情绪的工作。\n具体构建词典的步骤，大家可以阅读论文原文。论文已经公开了中文情感词典，我已将其整理为4个txt文件\n formal_pos.txt 正式用语正面情绪词典 formal_neg.txt 正式用语负面情绪词典 unformal_pos.txt 非正式用语正面情绪词典 unformal_neg.txt 非正式用语负面情绪词典  中文金融词典使用方法 cnsenti实现了自定义词典功能，导入不同的txt词典文件，即可实现不同方面的情绪词统计。\n年报正式用语词典  dict/formal_pos.txt 正式用语正面情绪词典 dict/formal_neg.txt 正式用语负面情绪词典  from cnsenti import Sentiment senti = Sentiment(pos=\u0026#39;dict/formal_pos.txt\u0026#39;, #正面词典txt文件相对路径 neg=\u0026#39;dict/formal_neg.txt\u0026#39;, #负面词典txt文件相对路径 merge=False, #是否将cnsenti自带词典和用户导入的自定义词典融合 encoding=\u0026#39;utf-8\u0026#39;) #两txt均为utf-8编码 test_text = \u0026#39;这家公司是行业的引领者，是中流砥柱。今年的业绩非常好。\u0026#39; result = senti.sentiment_count(test_text) print(\u0026#39;sentiment_count\u0026#39;,result) Run\nsentiment_count {\u0026#39;words\u0026#39;: 16, \u0026#39;sentences\u0026#39;: 2, \u0026#39;pos\u0026#39;: 3, \u0026#39;neg\u0026#39;: 0}","title":"中文金融情感词典"},{"content":"本文代码下载\n 链接:https://pan.baidu.com/s/1vJohEJ0pc6t4PBK04PiZbg 密码:t7a6\n whatlies 可以与spacy语言模型结合，可视化词向量。安装zh_core_web_md、en_core_web_md和whatlies。具体文档可以查看https://github.com/RasaHQ/whatlies\n!pip3 install zh_core_web_md-3.0.0-py3-none-any.whl !pip3 install en_core_web_md-3.0.0-py3-none-any.whl !pip3 install whatlies 快速上手 spacy模型中的词向量均为几十上百维度的词向量，通过压缩映射至二维空间后，横坐标man，纵坐标woman，就可以将词语的性别倾向可视化出来。\n例如woman词更接近纵轴，man更接近横轴。 nurse、queen一般更多的是女性从业者，因此更接近y轴。 king国王多为男性，所以更接近x轴。\n至于动物，女性喜欢养猫，男性喜欢养狗，所以也能体现出词语的性别倾向。\nfrom whatlies import EmbeddingSet from whatlies.language import SpacyLanguage lang = SpacyLanguage(\u0026#34;en_core_web_md\u0026#34;) words = [\u0026#34;cat\u0026#34;, \u0026#34;dog\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;kitten\u0026#34;, \u0026#34;man\u0026#34;, \u0026#34;woman\u0026#34;, \u0026#34;king\u0026#34;, \u0026#34;queen\u0026#34;, \u0026#34;doctor\u0026#34;, \u0026#34;nurse\u0026#34;] emb = EmbeddingSet(*[lang[w] for w in words]) emb.plot_interactive(x_axis=emb[\u0026#34;man\u0026#34;], y_axis=emb[\u0026#34;woman\u0026#34;])   whatlies也可以对中文进行操作。\nfrom whatlies import EmbeddingSet from whatlies.language import SpacyLanguage zh_lang = SpacyLanguage(\u0026#34;zh_core_web_md\u0026#34;) zh_words = [\u0026#34;猫\u0026#34;, \u0026#34;狗\u0026#34;, \u0026#34;鱼\u0026#34;, \u0026#34;鲤鱼\u0026#34;, \u0026#34;男人\u0026#34;, \u0026#34;女人\u0026#34;, \u0026#34;国王\u0026#34;, \u0026#34;王后\u0026#34;, \u0026#34;医生\u0026#34;, \u0026#34;护士\u0026#34;] zh_emb = EmbeddingSet(*[zh_lang[w] for w in zh_words]) zh_emb.plot_interactive(x_axis=zh_emb[\u0026#34;男人\u0026#34;], y_axis=zh_emb[\u0026#34;女人\u0026#34;])   ","permalink":"/blog/whatlies/","summary":"本文代码下载\n 链接:https://pan.baidu.com/s/1vJohEJ0pc6t4PBK04PiZbg 密码:t7a6\n whatlies 可以与spacy语言模型结合，可视化词向量。安装zh_core_web_md、en_core_web_md和whatlies。具体文档可以查看https://github.com/RasaHQ/whatlies\n!pip3 install zh_core_web_md-3.0.0-py3-none-any.whl !pip3 install en_core_web_md-3.0.0-py3-none-any.whl !pip3 install whatlies 快速上手 spacy模型中的词向量均为几十上百维度的词向量，通过压缩映射至二维空间后，横坐标man，纵坐标woman，就可以将词语的性别倾向可视化出来。\n例如woman词更接近纵轴，man更接近横轴。 nurse、queen一般更多的是女性从业者，因此更接近y轴。 king国王多为男性，所以更接近x轴。\n至于动物，女性喜欢养猫，男性喜欢养狗，所以也能体现出词语的性别倾向。\nfrom whatlies import EmbeddingSet from whatlies.language import SpacyLanguage lang = SpacyLanguage(\u0026#34;en_core_web_md\u0026#34;) words = [\u0026#34;cat\u0026#34;, \u0026#34;dog\u0026#34;, \u0026#34;fish\u0026#34;, \u0026#34;kitten\u0026#34;, \u0026#34;man\u0026#34;, \u0026#34;woman\u0026#34;, \u0026#34;king\u0026#34;, \u0026#34;queen\u0026#34;, \u0026#34;doctor\u0026#34;, \u0026#34;nurse\u0026#34;] emb = EmbeddingSet(*[lang[w] for w in words]) emb.plot_interactive(x_axis=emb[\u0026#34;man\u0026#34;], y_axis=emb[\u0026#34;woman\u0026#34;])   whatlies也可以对中文进行操作。\nfrom whatlies import EmbeddingSet from whatlies.language import SpacyLanguage zh_lang = SpacyLanguage(\u0026#34;zh_core_web_md\u0026#34;) zh_words = [\u0026#34;猫\u0026#34;, \u0026#34;狗\u0026#34;, \u0026#34;鱼\u0026#34;, \u0026#34;鲤鱼\u0026#34;, \u0026#34;男人\u0026#34;, \u0026#34;女人\u0026#34;, \u0026#34;国王\u0026#34;, \u0026#34;王后\u0026#34;, \u0026#34;医生\u0026#34;, \u0026#34;护士\u0026#34;] zh_emb = EmbeddingSet(*[zh_lang[w] for w in zh_words]) zh_emb.","title":"whatlies库|可视化词向量"},{"content":"\nClumper可以用来处理嵌套样式的json数据结构。\n本文代码下载\nGetting Started 安装 !pip3 install clumper Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple Collecting clumper Downloading https://pypi.tuna.tsinghua.edu.cn/packages/70/62/0731ab9b48c91132aff487217980dcb147ffc0922a278adc05986f6a8d4b/clumper-0.2.13-py2.py3-none-any.whl (21 kB) Installing collected packages: clumper Successfully installed clumper-0.2.13 \u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.1.2 is available. You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m  为了展示Clumper如何工作，我准备了pokemon.json， 由列表组成(该列表由多个字典组成)，下面是pokemon.json部分内容\nimport json with open(\u0026#39;pokemon.json\u0026#39;) as jsonf: pokemon = json.loads(jsonf.read()) pokemon[:2] [{'name': 'Bulbasaur', 'type': ['Grass', 'Poison'], 'total': 318, 'hp': 45, 'attack': 49}, {'name': 'Ivysaur', 'type': ['Grass', 'Poison'], 'total': 405, 'hp': 60, 'attack': 62}]  我们准备的pokemon.json列表中大概有800个字典，数量级刚刚好，不会因为太大导致电脑无法运行数据分析，也不会太小导致手动操作性价比更高。\nExample 基本操作 from clumper import Clumper list_of_dicts = [ {\u0026#39;a\u0026#39;: 7, \u0026#39;b\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 4}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6} ] (Clumper(list_of_dicts) .mutate(c=lambda d: d[\u0026#39;a\u0026#39;]+d[\u0026#39;b\u0026#39;]) .sort(lambda d: d[\u0026#39;c\u0026#39;]) .collect() ) [{'a': 2, 'b': 4, 'c': 6}, {'a': 7, 'b': 2, 'c': 9}, {'a': 3, 'b': 6, 'c': 9}]  代码解析 Step1\n首先使用mutate方法，该方法可以在每条记录中生成新变量。\n  结算结果仍为Clumper类\nStep2\n接下来对mutate之后的数据进行排序\n  得到的结果仍为Clumper类。\n从上面的小代码案例中，可以看到整套流程像是一个流水线车间，每一行就是一个生成环节，生产环节之间使用.连接起来。\nfrom clumper import Clumper (Clumper(pokemon) .keep(lambda d: len(d[\u0026#39;type\u0026#39;])==1) #保留type长度为1的字典 .mutate(type=lambda d: d[\u0026#39;type\u0026#39;][0], #type值从列表变为字符串 ratio=lambda d: d[\u0026#39;attack\u0026#39;]/d[\u0026#39;hp\u0026#39;]) #新建ratio .select(\u0026#39;name\u0026#39;, \u0026#39;type\u0026#39;, \u0026#39;ratio\u0026#39;) #字典最后只保留name， type， ratio三个字段 .sort(lambda d: d[\u0026#39;ratio\u0026#39;], reverse=True) #按照ratio降序排列 .head(5) #只保留前5个 .collect() #转成列表显示 ) \nCommon Verbs Keep keep函数可以从原始数据中抽取符合指定条件的子集。   from clumper import Clumper list_dicts = [{\u0026#39;a\u0026#39;: 1}, {\u0026#39;a\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 3}, {\u0026#39;a\u0026#39;: 4}] (Clumper(list_dicts) .keep(lambda d: d[\u0026#39;a\u0026#39;] \u0026gt;= 3) .collect() #试一试去掉.collect()后的效果 ) [{'a': 3}, {'a': 4}]  可以实现缺失值处理，以不同的方式实现pandas的.dropna()的功能。\nfrom clumper import Clumper data = [ {\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 4}, {\u0026#34;a\u0026#34;: 2, \u0026#34;b\u0026#34;: 3}, {\u0026#34;a\u0026#34;: 3, \u0026#34;b\u0026#34;: 2}, {\u0026#34;a\u0026#34;: 4}, ] #只保留含有b的字段 (Clumper(data) .keep(lambda d: \u0026#39;b\u0026#39; in d.keys()) .collect() ) [{'a': 1, 'b': 4}, {'a': 2, 'b': 3}, {'a': 3, 'b': 2}]  Mutate mutate可以在每条记录中，创建新字段、改写旧字段。   from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 3, \u0026#39;c\u0026#39;:4}, {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 6}] #新建了c和s字段 (Clumper(list_dicts) .mutate(c=lambda d: d[\u0026#39;a\u0026#39;] + d[\u0026#39;b\u0026#39;], s=lambda d: d[\u0026#39;a\u0026#39;] + d[\u0026#39;b\u0026#39;] + d[\u0026#39;c\u0026#39;]) .collect() ) [{'a': 1, 'b': 2, 'c': 3, 's': 6}, {'a': 2, 'b': 3, 'c': 5, 's': 10}, {'a': 1, 'b': 6, 'c': 7, 's': 14}]  Sort sort可以实现排序   from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 3}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 1}] (Clumper(list_dicts) .sort(lambda d: d[\u0026#39;b\u0026#39;]) #默认升序 .collect() ) [{'a': 2, 'b': 1}, {'a': 1, 'b': 2}, {'a': 3, 'b': 3}]  Select select挑选每条记录中的某个(些)字段   from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 3, \u0026#39;c\u0026#39;:4}, {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 6}] (Clumper(list_dicts) .select(\u0026#39;a\u0026#39;) .collect() ) [{'a': 1}, {'a': 2}, {'a': 1}]  Drop 剔除某个（些）字段。   from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 3, \u0026#39;c\u0026#39;:4}, {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 6}] (Clumper(list_dicts) .drop(\u0026#39;c\u0026#39;) .collect() ) [{'a': 1, 'b': 2}, {'a': 2, 'b': 3}, {'a': 1, 'b': 6}]  GroupBy 根据某个（些）字段对数据集进行分组，得到不同Group类的集合。一般与.agg()方法联合使用。   from clumper import Clumper grade_dicts = [ {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 98, \u0026#39;name\u0026#39;: \u0026#39;张三\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 88, \u0026#39;name\u0026#39;: \u0026#39;王五\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 99, \u0026#39;name\u0026#39;: \u0026#39;赵六\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 58, \u0026#39;name\u0026#39;: \u0026#39;李四\u0026#39;}] (Clumper(grade_dicts) .group_by(\u0026#34;gender\u0026#34;) .groups==(\u0026#39;gender\u0026#39;, ) ) True  Ungroup GroupBy的反操作   from clumper import Clumper grade_dicts = [ {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 98, \u0026#39;name\u0026#39;: \u0026#39;张三\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 88, \u0026#39;name\u0026#39;: \u0026#39;王五\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 99, \u0026#39;name\u0026#39;: \u0026#39;赵六\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 58, \u0026#39;name\u0026#39;: \u0026#39;李四\u0026#39;}] (Clumper(grade_dicts) .group_by(\u0026#34;gender\u0026#34;) .ungroup().groups == tuple() ) True  \nAbout Groups Agg 聚合描述性统计方法\nagg如下图，可以理解成三个步骤，即group-\u0026gt;split-\u0026gt;summary   常用的描述性统计函数有： mean、count、unqiue、n_unique、sum、min和max\n求学生的平均成绩、最优和最差成绩\nfrom clumper import Clumper grade_dicts = [{\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 98, \u0026#39;name\u0026#39;: \u0026#39;张三\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 88, \u0026#39;name\u0026#39;: \u0026#39;王五\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 99, \u0026#39;name\u0026#39;: \u0026#39;赵六\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 58, \u0026#39;name\u0026#39;: \u0026#39;李四\u0026#39;}] (Clumper(grade_dicts) .agg(mean_grade=(\u0026#39;grade\u0026#39;, \u0026#39;mean\u0026#39;), max_grade=(\u0026#39;grade\u0026#39;, \u0026#39;max\u0026#39;), min_grade=(\u0026#39;grade\u0026#39;, \u0026#39;min\u0026#39;)) .collect() ) [{'mean_grade': 85.75, 'max_grade': 99, 'min_grade': 58}]  求男生和女生各自的平均成绩、最优和最差成绩\nfrom clumper import Clumper grade_dicts = [{\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 98, \u0026#39;name\u0026#39;: \u0026#39;张三\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 88, \u0026#39;name\u0026#39;: \u0026#39;王五\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;女\u0026#39;, \u0026#39;grade\u0026#39;: 99, \u0026#39;name\u0026#39;: \u0026#39;赵六\u0026#39;}, {\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: 58, \u0026#39;name\u0026#39;: \u0026#39;李四\u0026#39;}] (Clumper(grade_dicts) .group_by(\u0026#39;gender\u0026#39;) .agg(mean_grade=(\u0026#39;grade\u0026#39;, \u0026#39;mean\u0026#39;), max_grade=(\u0026#39;grade\u0026#39;, \u0026#39;max\u0026#39;), min_grade=(\u0026#39;grade\u0026#39;, \u0026#39;min\u0026#39;)) .collect()) [{'gender': '男', 'mean_grade': 78, 'max_grade': 98, 'min_grade': 58}, {'gender': '女', 'mean_grade': 93.5, 'max_grade': 99, 'min_grade': 88}]  Collect 一般Clumper函数返回的结果显示为Clumper类，是看不到具体内容的。\ncollect作用主要是展开显示。   剔除重复 剔除重复内容   from clumper import Clumper data = [{\u0026#34;a\u0026#34;: 1}, {\u0026#34;a\u0026#34;: 2}, {\u0026#34;a\u0026#34;: 2}] (Clumper(data) .drop_duplicates() .collect() ) [{'a': 1}, {'a': 2}]   ### 什么是Group？ from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 9, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;} ] (Clumper(list_dicts) .group_by(\u0026#39;grp\u0026#39;) ) \u0026lt;Clumper groups=('grp',) len=5 @0x103cb0290\u0026gt;  当前的group以grp作为关键词   现在经过 .group_by('grp')操作后，说明你对每个grp组感兴趣。具体一点，一个组是{'grp': 'a'}, 另一个组是{'grp': 'b'}.\nAgg without groups   from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 9, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;} ] (Clumper(list_dicts) .agg(s=(\u0026#39;a\u0026#39;, \u0026#39;sum\u0026#39;), m=(\u0026#39;a\u0026#39;, \u0026#39;mean\u0026#39;)) .collect()) [{'s': 29, 'm': 5.8}]  with groups 分别计算组grp=a、组grp=b的sum和mean   from clumper import Clumper list_dicts = [ {\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 9, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;} ] (Clumper(list_dicts) .group_by(\u0026#39;grp\u0026#39;) .agg(s=(\u0026#39;a\u0026#39;, \u0026#39;sum\u0026#39;), m=(\u0026#39;a\u0026#39;, \u0026#39;mean\u0026#39;)) .collect()) [{'grp': 'a', 's': 18, 'm': 6}, {'grp': 'b', 's': 11, 'm': 5.5}]  agg内置的统计函数名 内置的统计函数，可直接通过字符串调用\n{ \u0026#34;mean\u0026#34;: mean, \u0026#34;count\u0026#34;: lambda d: len(d), \u0026#34;unique\u0026#34;: lambda d: list(set(d)), \u0026#34;n_unique\u0026#34;: lambda d: len(set(d)), \u0026#34;sum\u0026#34;: sum, \u0026#34;min\u0026#34;: min, \u0026#34;max\u0026#34;: max, \u0026#34;median\u0026#34;: median, \u0026#34;var\u0026#34;: variance, \u0026#34;std\u0026#34;: stdev, \u0026#34;values\u0026#34;: lambda d: d, \u0026#34;first\u0026#34;: lambda d: d[0], \u0026#34;last\u0026#34;: lambda d: d[-1], } Transform .transform()与.agg()类似。主要的区别是transform处理过程中，记录数和字段数不会出现压缩。\nwithout groups   from clumper import Clumper data = [{\u0026#34;a\u0026#34;: 6, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;}, {\u0026#34;a\u0026#34;: 2, \u0026#34;grp\u0026#34;: \u0026#34;b\u0026#34;}, {\u0026#34;a\u0026#34;: 7, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;}, {\u0026#34;a\u0026#34;: 9, \u0026#34;grp\u0026#34;: \u0026#34;b\u0026#34;}, {\u0026#34;a\u0026#34;: 5, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;}] (Clumper(data) .transform(s=(\u0026#34;a\u0026#34;, \u0026#34;sum\u0026#34;), u=(\u0026#34;a\u0026#34;, \u0026#34;unique\u0026#34;)) .collect()) [{'a': 6, 'grp': 'a', 's': 29, 'u': [2, 5, 6, 7, 9]}, {'a': 2, 'grp': 'b', 's': 29, 'u': [2, 5, 6, 7, 9]}, {'a': 7, 'grp': 'a', 's': 29, 'u': [2, 5, 6, 7, 9]}, {'a': 9, 'grp': 'b', 's': 29, 'u': [2, 5, 6, 7, 9]}, {'a': 5, 'grp': 'a', 's': 29, 'u': [2, 5, 6, 7, 9]}]  with groups   from clumper import Clumper data = [ {\u0026#34;a\u0026#34;: 6, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;}, {\u0026#34;a\u0026#34;: 2, \u0026#34;grp\u0026#34;: \u0026#34;b\u0026#34;}, {\u0026#34;a\u0026#34;: 7, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;}, {\u0026#34;a\u0026#34;: 9, \u0026#34;grp\u0026#34;: \u0026#34;b\u0026#34;}, {\u0026#34;a\u0026#34;: 5, \u0026#34;grp\u0026#34;: \u0026#34;a\u0026#34;} ] (Clumper(data) .group_by(\u0026#34;grp\u0026#34;) .transform(s=(\u0026#34;a\u0026#34;, \u0026#34;sum\u0026#34;), u=(\u0026#34;a\u0026#34;, \u0026#34;unique\u0026#34;)) .collect() ) [{'a': 6, 'grp': 'a', 's': 18, 'u': [5, 6, 7]}, {'a': 7, 'grp': 'a', 's': 18, 'u': [5, 6, 7]}, {'a': 5, 'grp': 'a', 's': 18, 'u': [5, 6, 7]}, {'a': 2, 'grp': 'b', 's': 11, 'u': [9, 2]}, {'a': 9, 'grp': 'b', 's': 11, 'u': [9, 2]}]  Mutate clumper库中的row_number可以给每条记录显示索引位置（第几个）。\nwithout groups   from clumper import Clumper from clumper.sequence import row_number list_dicts = [ {\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 4, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;} ] (Clumper(list_dicts) .mutate(index=row_number()) .collect() ) [{'a': 6, 'grp': 'a', 'index': 1}, {'a': 2, 'grp': 'b', 'index': 2}, {'a': 7, 'grp': 'a', 'index': 3}, {'a': 4, 'grp': 'b', 'index': 4}, {'a': 5, 'grp': 'a', 'index': 5}]  with groups   from clumper import Clumper from clumper.sequence import row_number list_dicts = [ {\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 4, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;} ] (Clumper(list_dicts) .group_by(\u0026#39;grp\u0026#39;) .mutate(index=row_number()) .collect() ) [{'a': 6, 'grp': 'a', 'index': 1}, {'a': 7, 'grp': 'a', 'index': 2}, {'a': 5, 'grp': 'a', 'index': 3}, {'a': 2, 'grp': 'b', 'index': 1}, {'a': 4, 'grp': 'b', 'index': 2}]  Sort 排序, 默认升序\nwithout groups   from clumper import Clumper list_dicts = [{\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 9, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}] (Clumper(list_dicts) #根据字段a进行排序  .sort(key=lambda d: d[\u0026#39;a\u0026#39;]) .collect()) [{'a': 2, 'grp': 'b'}, {'a': 5, 'grp': 'a'}, {'a': 6, 'grp': 'a'}, {'a': 7, 'grp': 'a'}, {'a': 9, 'grp': 'b'}]  with groups   from clumper import Clumper list_dicts = [{\u0026#39;a\u0026#39;: 6, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 2, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 7, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}, {\u0026#39;a\u0026#39;: 9, \u0026#39;grp\u0026#39;: \u0026#39;b\u0026#39;}, {\u0026#39;a\u0026#39;: 5, \u0026#39;grp\u0026#39;: \u0026#39;a\u0026#39;}] (Clumper(list_dicts) .group_by(\u0026#39;grp\u0026#39;) .sort(key=lambda d: d[\u0026#39;a\u0026#39;]) .collect()) [{'a': 5, 'grp': 'a'}, {'a': 6, 'grp': 'a'}, {'a': 7, 'grp': 'a'}, {'a': 2, 'grp': 'b'}, {'a': 9, 'grp': 'b'}]  Ungroup 最后，如果你已经进行完了分组计算，想再次整合起来，取消分组状态，可以使用.ungroup()\n\nMerge Verbs  如果想将多个记录整理到一个记录中，有很多种实现方法。\nConcat 如果想垂直方向将多个记录堆叠，可以使用concat   from clumper import Clumper c1 = Clumper([{\u0026#34;a\u0026#34;: 1}]) c2 = Clumper([{\u0026#34;a\u0026#34;: 2}]) c3 = Clumper([{\u0026#34;a\u0026#34;: 3}]) c1.concat(c2).collect() [{'a': 1}, {'a': 2}]  #等同于c1.concat(c2).concat(c3).collect() c1.concat(c2, c3).collect() [{'a': 1}, {'a': 2}, {'a': 3}]  Joins Joins类似于数学里的交集、并集的，大致有以下四种，   left join 左连接，以左为主，表示以table1为主，关联上table2的数据，结果显示table1的所有数据，然后table2显示的是和table1有交集部分的数据。   from clumper import Clumper left = Clumper([ {\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 4}, {\u0026#34;a\u0026#34;: 2, \u0026#34;b\u0026#34;: 6}, {\u0026#34;a\u0026#34;: 3, \u0026#34;b\u0026#34;: 8}, ]) right = Clumper([ {\u0026#34;c\u0026#34;: 9, \u0026#34;b\u0026#34;: 4}, {\u0026#34;c\u0026#34;: 8, \u0026#34;b\u0026#34;: 5}, {\u0026#34;c\u0026#34;: 7, \u0026#34;b\u0026#34;: 6}, ]) #根据b进行左右两表的合并 result = left.inner_join(right, mapping={\u0026#34;b\u0026#34;: \u0026#34;b\u0026#34;}) result.collect() [{'a': 1, 'b': 4, 'c': 9}, {'a': 2, 'b': 6, 'c': 7}]  inner join 内连接， 交集\n  from clumper import Clumper left = Clumper([ {\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;:4}, {\u0026#34;a\u0026#34;: 2, \u0026#34;b\u0026#34;:6}, {\u0026#34;a\u0026#34;: 3, \u0026#34;b\u0026#34;:8}, ]) right = Clumper([ {\u0026#34;c\u0026#34;: 9, \u0026#34;b\u0026#34;:4}, {\u0026#34;c\u0026#34;: 8, \u0026#34;b\u0026#34;:5}, {\u0026#34;c\u0026#34;: 7, \u0026#34;b\u0026#34;:6}, ]) result = left.inner_join(right, mapping={\u0026#34;b\u0026#34;: \u0026#34;b\u0026#34;}) result.collect() [{'a': 1, 'b': 4, 'c': 9}, {'a': 2, 'b': 6, 'c': 7}]   \nNested Data 由于嵌套数据序列确实具有各种形状和大小，因此该库提供了各种方法来帮助您将数据重塑为不同的格式。 本文档将演示这些方法的工作原理。\nExplode 炸裂（展开）   from clumper import Clumper data = [{\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: [80, 99], \u0026#39;name\u0026#39;:[\u0026#39;张三\u0026#39;, \u0026#39;李四\u0026#39;]}] (Clumper(data) .explode(\u0026#39;name\u0026#39;, \u0026#39;grade\u0026#39;) .collect() ) [{'gender': '男', 'grade': 80, 'name': '张三'}, {'gender': '男', 'grade': 99, 'name': '张三'}, {'gender': '男', 'grade': 80, 'name': '李四'}, {'gender': '男', 'grade': 99, 'name': '李四'}]  from clumper import Clumper data = [{\u0026#39;gender\u0026#39;: \u0026#39;男\u0026#39;, \u0026#39;grade\u0026#39;: [80, 99], \u0026#39;name\u0026#39;:[\u0026#39;张三\u0026#39;, \u0026#39;李四\u0026#39;]}] #.explode(\u0026#39;name\u0026#39;, \u0026#39;grade\u0026#39;)略微有些区别 #请查看两者运行结果 (Clumper(data) .explode(item=\u0026#39;name\u0026#39;, val=\u0026#39;grade\u0026#39;) .collect() ) [{'gender': '男', 'item': '张三', 'val': 80}, {'gender': '男', 'item': '张三', 'val': 99}, {'gender': '男', 'item': '李四', 'val': 80}, {'gender': '男', 'item': '李四', 'val': 99}]  Unpack 与explode类似   from clumper import Clumper list_dicts = { \u0026#39;a\u0026#39;: 1, \u0026#39;rows\u0026#39;: [{\u0026#39;b\u0026#39;: 2, \u0026#39;c\u0026#39;: 3}, {\u0026#39;b\u0026#39;: 3}, {\u0026#39;b\u0026#39;: 4}] } (Clumper(list_dicts) .unpack(\u0026#39;rows\u0026#39;) .collect() ) [{'a': 1, 'b': 2, 'c': 3}, {'a': 1, 'b': 3}, {'a': 1, 'b': 4}]  Flatten keys   from clumper import Clumper data = { \u0026#39;feature_1\u0026#39;: {\u0026#39;propery_1\u0026#39;: 1, \u0026#39;property_2\u0026#39;: 2}, \u0026#39;feature_2\u0026#39;: {\u0026#39;propery_1\u0026#39;: 3, \u0026#39;property_2\u0026#39;: 4}, \u0026#39;feature_3\u0026#39;: {\u0026#39;propery_1\u0026#39;: 5, \u0026#39;property_2\u0026#39;: 6}, } (Clumper(data, listify=False) .flatten_keys() .collect() ) [{'propery_1': 1, 'property_2': 2, 'key': 'feature_1'}, {'propery_1': 3, 'property_2': 4, 'key': 'feature_2'}, {'propery_1': 5, 'property_2': 6, 'key': 'feature_3'}]  \nSummary Methods Clumper支持常用的统计性方法，诸如mean、max、min等\nmean   from clumper import Clumper list_of_dicts = [ {\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7} ] Clumper(list_of_dicts).mean(\u0026#34;a\u0026#34;) 3.5  Clumper(list_of_dicts).mean(\u0026#34;b\u0026#34;) 6.666666666666667  count 统计记录数   from clumper import Clumper list_of_dicts = [ {\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7} ] #含有a的一共有多少条记录 Clumper(list_of_dicts).count(\u0026#34;a\u0026#34;) 4  Clumper(list_of_dicts).count(\u0026#34;b\u0026#34;) 3  unique 汇总某字段不重样的值的种类，如[a, b, a, a]，经过unique后，返回[a, b]   from clumper import Clumper list_of_dicts = [{\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}] Clumper(list_of_dicts).unique(\u0026#34;a\u0026#34;) [2, 3, 7]  Clumper(list_of_dicts).unique(\u0026#34;b\u0026#34;) [6, 7]  n_unique 统计某字段对应的值一种有多少种   from clumper import Clumper list_of_dicts = [ {\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7} ] Clumper(list_of_dicts).n_unique(\u0026#34;a\u0026#34;) 3  Clumper(list_of_dicts).n_unique(\u0026#34;b\u0026#34;) 2  min   from clumper import Clumper list_of_dicts = [{\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}] Clumper(list_of_dicts).min(\u0026#34;a\u0026#34;) 2  Clumper(list_of_dicts).min(\u0026#34;b\u0026#34;) 6  max   from clumper import Clumper list_of_dicts = [{\u0026#39;a\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}, {\u0026#39;a\u0026#39;: 3, \u0026#39;b\u0026#39;: 6}, {\u0026#39;a\u0026#39;: 2, \u0026#39;b\u0026#39;: 7}] Clumper(list_of_dicts).max(\u0026#34;a\u0026#34;) 7  Clumper(list_of_dicts).max(\u0026#34;b\u0026#34;) 7  ","permalink":"/blog/clumper/","summary":"Clumper可以用来处理嵌套样式的json数据结构。\n本文代码下载\nGetting Started 安装 !pip3 install clumper Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple Collecting clumper Downloading https://pypi.tuna.tsinghua.edu.cn/packages/70/62/0731ab9b48c91132aff487217980dcb147ffc0922a278adc05986f6a8d4b/clumper-0.2.13-py2.py3-none-any.whl (21 kB) Installing collected packages: clumper Successfully installed clumper-0.2.13 \u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.1.2 is available. You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m  为了展示Clumper如何工作，我准备了pokemon.json， 由列表组成(该列表由多个字典组成)，下面是pokemon.json部分内容\nimport json with open(\u0026#39;pokemon.json\u0026#39;) as jsonf: pokemon = json.loads(jsonf.read()) pokemon[:2] [{'name': 'Bulbasaur', 'type': ['Grass', 'Poison'], 'total': 318, 'hp': 45, 'attack': 49}, {'name': 'Ivysaur', 'type': ['Grass', 'Poison'], 'total': 405, 'hp': 60, 'attack': 62}]  我们准备的pokemon.","title":"Clumper库|dplyr样式的Python包"},{"content":"\nTypora简介 Typora是一个所见即所得的Markdown格式文本编辑器，支持Windows、macOS和GNU/Linux操作系统，拼写检查、自定义CSS样式、数学公式渲染（通过MathJax）等特性。\n如果你还不知道Typora，请访问Typora — a markdown editor, markdown reader.\n主题安装方法  下载本主题的压缩文件Latest release 打开Typora, 点击菜单栏的偏好设置-外观-打开主题文件夹 将解压后的文件复制到主题文件夹下(压缩包包含mlike文件夹、 mlike.css mlike-light.css、mlike-dark.css) 重新启动Typora，点击菜单栏的主题-Mlike Light或者Mlike Dark  具体的安装方法可查看 Install Theme (typora.io)\nTypora Themes 下面15个主题是大邓最喜欢的主题\n Autumnus Adark Drake FluentLight Jamstatic LessLight LessLightPrint Mo NewPrint OptAutumnus OrangeHeart PixII Torillic Vue Whitey  1.Autumnus   2.Adark   3.Drake   4.FluentLight     5.Jamstatic   6.LessLight   7.LessLightPrint   8.Mo   9.NewPrint   10.OptAutumnus   11.OrangeHeart   12.PixII   13.Torillic   14.Vue   15.Whitey   ","permalink":"/blog/my-favorite-typora-themes/","summary":"Typora简介 Typora是一个所见即所得的Markdown格式文本编辑器，支持Windows、macOS和GNU/Linux操作系统，拼写检查、自定义CSS样式、数学公式渲染（通过MathJax）等特性。\n如果你还不知道Typora，请访问Typora — a markdown editor, markdown reader.\n主题安装方法  下载本主题的压缩文件Latest release 打开Typora, 点击菜单栏的偏好设置-外观-打开主题文件夹 将解压后的文件复制到主题文件夹下(压缩包包含mlike文件夹、 mlike.css mlike-light.css、mlike-dark.css) 重新启动Typora，点击菜单栏的主题-Mlike Light或者Mlike Dark  具体的安装方法可查看 Install Theme (typora.io)\nTypora Themes 下面15个主题是大邓最喜欢的主题\n Autumnus Adark Drake FluentLight Jamstatic LessLight LessLightPrint Mo NewPrint OptAutumnus OrangeHeart PixII Torillic Vue Whitey  1.Autumnus   2.Adark   3.Drake   4.FluentLight     5.Jamstatic   6.LessLight   7.LessLightPrint   8.","title":"我最喜欢的15个Typora主题"},{"content":"代码下载 点击此处下载代码\n本文B站视频 https://www.bilibili.com/video/BV1AE411r7ph\n一、知识准备  python语法基本知识 https://www.bilibili.com/video/BV1eb411h7sP/ python网络爬虫 https://www.bilibili.com/video/BV1AE411r7ph/  二、网址规律分析 2.1 上交所   上交所多为GET请求方法，伪码\nimport requests url = \u0026#39;上交所网址规律\u0026#39; headers = \u0026#39;你的浏览器useragent(带referer)\u0026#39; cookies = \u0026#39;你的cookies\u0026#39; resp = requests.get(url, headers=headers, cookies=cookies) \n2.2 深交所     深交所多为POST请求方法，伪码\nimport requests url = \u0026#39;深交所网址规律\u0026#39; headers = \u0026#39;你的浏览器useragent(带referer)\u0026#39; cookies = \u0026#39;你的cookies\u0026#39; param = \u0026#39;form data构造的字典，补全网址规律\u0026#39; resp = requests.get(url, headers=headers, cookies=cookies, data=param) \n三、定位pdf相关数据 访问得到的结果均为json数据，解析定位方法可使用python的字典方法。\n    四、存储数据 几千个pdf数据量很容易达到1000+M，如果长时间自动下载容易失败。\n建议先获取所有公司相关信息，存储到csv中。\n后续再单独使用pandas读取，逐一下载pdf。\n注意，这里推荐使用csv新的语法\nwith open(\u0026#39;你的csv文件路径\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) as csvf: #csv文件内的字段名 fieldnames = [\u0026#39;title\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;link\u0026#39;, \u0026#39;content\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() #访问 url = \u0026#39;网址\u0026#39; resp = requests.get(url,....) #定位 for company in resp.json()[\u0026#39;data\u0026#39;]: #解析数据 row = dict() row[\u0026#39;title\u0026#39;] = \u0026#39;采集到的标题\u0026#39; row[\u0026#39;date\u0026#39;] = \u0026#39;采集到的日期\u0026#39; row[\u0026#39;link\u0026#39;] = \u0026#39;采集到的pdf链接\u0026#39; row[\u0026#39;content\u0026#39;] = \u0026#39;采集到的内容\u0026#39; #写入csv writer.writerow(row) \n五、批量下载pdf 以深交所为例，已经采集到深圳交易所.csv，现在下载只需要执行\n## 下载 import requests import pandas as pd def download(link, fpath): \u0026#34;\u0026#34;\u0026#34; 下载多媒体及文件 link： 多媒体文件链接（结尾有文件格式名） fpath: 存储文件的路径（结尾有文件格式名） \u0026#34;\u0026#34;\u0026#34; resp = requests.get(link) #获取到二进制数据 binarydata = resp.content #以二进制形式将数据流存入fname中 with open(fpath, \u0026#39;wb\u0026#39;) as f: f.write(binarydata) df = pd.read_csv(\u0026#39;深圳交易所.csv\u0026#39;) for title, link in zip(df[\u0026#39;title\u0026#39;], df[\u0026#39;link\u0026#39;]): fpath = \u0026#39;深圳/{title}.PDF\u0026#39;.format(title=title) download(link, fpath) ","permalink":"/blog/%E6%8B%9B%E8%82%A1%E8%AF%B4%E6%98%8E-%E8%AF%81%E5%88%B8%E4%BA%A4%E6%98%93%E6%89%80/","summary":"代码下载 点击此处下载代码\n本文B站视频 https://www.bilibili.com/video/BV1AE411r7ph\n一、知识准备  python语法基本知识 https://www.bilibili.com/video/BV1eb411h7sP/ python网络爬虫 https://www.bilibili.com/video/BV1AE411r7ph/  二、网址规律分析 2.1 上交所   上交所多为GET请求方法，伪码\nimport requests url = \u0026#39;上交所网址规律\u0026#39; headers = \u0026#39;你的浏览器useragent(带referer)\u0026#39; cookies = \u0026#39;你的cookies\u0026#39; resp = requests.get(url, headers=headers, cookies=cookies) \n2.2 深交所     深交所多为POST请求方法，伪码\nimport requests url = \u0026#39;深交所网址规律\u0026#39; headers = \u0026#39;你的浏览器useragent(带referer)\u0026#39; cookies = \u0026#39;你的cookies\u0026#39; param = \u0026#39;form data构造的字典，补全网址规律\u0026#39; resp = requests.get(url, headers=headers, cookies=cookies, data=param) \n三、定位pdf相关数据 访问得到的结果均为json数据，解析定位方法可使用python的字典方法。\n    四、存储数据 几千个pdf数据量很容易达到1000+M，如果长时间自动下载容易失败。","title":"深交所上交所pdf批量下载"},{"content":"代码下载 点击此处下载代码\n问题 如何将数据中，同一股票代码同一年的某个字段加总成一条？\n我想把某公司同一年的数据var加总到一起\n思路 可以通过pandas库实现这个需求\n 获取公司股票代码列表 获取某公司年份列表 对某个公司同年的var进行加总 (var代指一个字段或变量) for循环对所有的公司重复2-3操作  准备数据 import numpy as np import pandas as pd #强制股票代码转为str类型 df = pd.read_excel(\u0026#39;data.xlsx\u0026#39;, converters={\u0026#34;code\u0026#34;: str}) df.head()    .dataframe tbody tr th:only-of-type { vertical-align: middle; } 代码 1. 获取公司股票代码列表 codes = df.code.unique() codes array(['000001', '000002', '000004', '000005', '000006'], dtype=object)  2. 获取某公司年份列表 以000001为例\nyears = set(df[df[\u0026#39;code\u0026#39;]==\u0026#39;000001\u0026#39;][\u0026#39;year\u0026#39;].values) years {2000, 2002, 2007, 2008, 2010, 2013, 2019}  3. 对某个公司同年的baladded进行加总 以000001公司2000年为例\nndf = df[df[\u0026#39;code\u0026#39;]==\u0026#39;000001\u0026#39;] ndf.head()   ndf[ndf[\u0026#39;year\u0026#39;]==2000]   ndf[ndf[\u0026#39;year\u0026#39;]==2000][\u0026#39;baladded\u0026#39;] 0 -65856130.0 1 -65856130.0 Name: baladded, dtype: float64  ndf[ndf[\u0026#39;year\u0026#39;]==2000][\u0026#39;baladded\u0026#39;].sum() -131712260.0  for循环对所有的公司重复2-3操作 汇总代码\nresults = [] codes = df.code.unique() for code in codes: years = set(df[df[\u0026#39;code\u0026#39;]==code][\u0026#39;year\u0026#39;].values) for year in years: ndf = df[df[\u0026#39;code\u0026#39;]==code] baladded_sum = ndf[ndf[\u0026#39;year\u0026#39;]==year][\u0026#39;baladded\u0026#39;].sum() data = (code, year, baladded_sum) results.append(data) result_df = pd.DataFrame(results, columns=[\u0026#39;code\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;baladded_sum\u0026#39;]) result_df   保存结果\nresult_df.to_excel(\u0026#39;result.xlsx\u0026#39;, index=False) ","permalink":"/blog/pandas%E6%A1%88%E4%BE%8B%E5%85%AC%E5%8F%B8%E5%88%86%E6%9E%90/","summary":"代码下载 点击此处下载代码\n问题 如何将数据中，同一股票代码同一年的某个字段加总成一条？\n我想把某公司同一年的数据var加总到一起\n思路 可以通过pandas库实现这个需求\n 获取公司股票代码列表 获取某公司年份列表 对某个公司同年的var进行加总 (var代指一个字段或变量) for循环对所有的公司重复2-3操作  准备数据 import numpy as np import pandas as pd #强制股票代码转为str类型 df = pd.read_excel(\u0026#39;data.xlsx\u0026#39;, converters={\u0026#34;code\u0026#34;: str}) df.head()    .dataframe tbody tr th:only-of-type { vertical-align: middle; } 代码 1. 获取公司股票代码列表 codes = df.code.unique() codes array(['000001', '000002', '000004', '000005', '000006'], dtype=object)  2. 获取某公司年份列表 以000001为例\nyears = set(df[df[\u0026#39;code\u0026#39;]==\u0026#39;000001\u0026#39;][\u0026#39;year\u0026#39;].values) years {2000, 2002, 2007, 2008, 2010, 2013, 2019}  3.","title":"Pandas小案例 | 对某公司同年的某指标批量汇总"},{"content":"在B站看到一位博主用Hugo制作个人博客的视频，感觉挺简单的，真的十几分钟就能看到云端出现自己的博客，当然了想让自己的博客更美观更炫酷，精雕细琢会花很多功夫。现在大家看到的效果，大邓用了一整天的时间，一点点修饰改动出来的。\n  1. 安装Hugo 这里以Mac为例，安装Hugo，命令行输入\nbrew install hugo \n2. 新建Hugo项目 切换到桌面(我喜欢把项目放到桌面), 命令行执行\ncd desktop 新建一个叫做MyBlog的hugo项目文件夹，命令行执行\nhugo new site MyBlog 现在可以在桌面看到一个MyBlog文件夹，接下来切换工作目录到MyBlog\ncd MyBlog 记者目前我们的命令行处于MyBlog的根目录 , 接下来下载网站主题\n\n3. Academic主题下载 Hugo有很多主题，我选择的 https://themes.gohugo.io/academic/，\n在命令行逐行执行下方命令\ncd themes git clone https://github.com/gcushen/hugo-academic.git 我们可以在 MyBlog/themes 看到多了一个 hugo-academic文件夹，把hugo-academic改为academic ，现在网站已经建立好了\n这里切换回项目根目录MyBlog\ncd .. 命令行执行pwd，检查一下目录\npwd 得到\n/Users/电脑用户名/desktop/MyBlog \n4. 启动本地博客 现在我们以academic主题为例，启动博客\n命令行执行\nhugo server -t academic --buildDrafts 补充: t的意思是主题\n执行后，在命令行中会提示我们\nhttp://localhost:1313/ 在浏览器中复制粘贴上方的链接，我们的Blog毛坯房搭建好了~\n5. 在本地新建一篇文章 依旧是MyBlog根目录，命令行执行\nhugo new post/first-article.md 在MyBlog/content内新生成了一个post文件夹，并且post内有了一个first-article.md文件。\n接下来就是在first-article.md内用markdown方式写内容即可。\n我们测试一下现在的网站,继续回到MyBlog根目录，命令行执行\nhugo server -t academic --buildDrafts 在浏览器中我们可以看到有First Ariticle的文章。\n6. 将本地博客部署到服务器 在github新建一个仓库，仓库名命名方式\n\u0026lt;你的github用户名\u0026gt;.github.io 比如我的github账号名是thunderhit，那么仓库名为\nthunderhit.github.io 在MyBlog根目录，命令行执行\nhugo --theme=academic --baseUrl=\u0026#39;/\u0026#39; --buildDrafts 注意: 主题academic, 网站地址 https://hidadeng.github.io/ ，你们根据自己需要改成自己的仓库名\n现在我们在MyBlog中多了一个public文件夹，其中有我们新建的文章内容。\n绑定public与github仓库\n命令行切换到public目录，初始化git\ncd public git init git add . git commit -m \u0026#39;我的hugo博客第一次提交\u0026#39; 把public与远程github仓库关联\n依次执行（大家的github地址略微不同，需要改动一下)\ngit remote add origin git@github.com:hidadeng/hidadeng.github.io.git git push -u origin master 命令行上传完毕后，在浏览器网址栏打开链接 https://hidadeng.github.io/\n就可以看到我们自己的博客了~\n\n更多 如果大家想学仔细学Hugo，推荐大家看B站Up主：ianianying的视频\n\n","permalink":"/blog/hugo%E5%BB%BA%E7%AB%99/","summary":"在B站看到一位博主用Hugo制作个人博客的视频，感觉挺简单的，真的十几分钟就能看到云端出现自己的博客，当然了想让自己的博客更美观更炫酷，精雕细琢会花很多功夫。现在大家看到的效果，大邓用了一整天的时间，一点点修饰改动出来的。\n  1. 安装Hugo 这里以Mac为例，安装Hugo，命令行输入\nbrew install hugo \n2. 新建Hugo项目 切换到桌面(我喜欢把项目放到桌面), 命令行执行\ncd desktop 新建一个叫做MyBlog的hugo项目文件夹，命令行执行\nhugo new site MyBlog 现在可以在桌面看到一个MyBlog文件夹，接下来切换工作目录到MyBlog\ncd MyBlog 记者目前我们的命令行处于MyBlog的根目录 , 接下来下载网站主题\n\n3. Academic主题下载 Hugo有很多主题，我选择的 https://themes.gohugo.io/academic/，\n在命令行逐行执行下方命令\ncd themes git clone https://github.com/gcushen/hugo-academic.git 我们可以在 MyBlog/themes 看到多了一个 hugo-academic文件夹，把hugo-academic改为academic ，现在网站已经建立好了\n这里切换回项目根目录MyBlog\ncd .. 命令行执行pwd，检查一下目录\npwd 得到\n/Users/电脑用户名/desktop/MyBlog \n4. 启动本地博客 现在我们以academic主题为例，启动博客\n命令行执行\nhugo server -t academic --buildDrafts 补充: t的意思是主题\n执行后，在命令行中会提示我们\nhttp://localhost:1313/ 在浏览器中复制粘贴上方的链接，我们的Blog毛坯房搭建好了~\n5. 在本地新建一篇文章 依旧是MyBlog根目录，命令行执行\nhugo new post/first-article.","title":"使用Hugo框架建立个人网站"},{"content":"参考GreatDanton 项目，丢弃庞杂丑陋的浏览器收藏夹， 打造简洁科研浏览器首页。\n每个人都可以自定义自己的浏览器首页，替换默认浏览器首页/起始页。 该项目可在任何现代浏览器上运行，只需将index.html设置为主页并添加自己的链接 到index.html\n点击查看效果 \n一、功能  搜索(google) 日历(倒计时) 待办事项 支持DIY自己的首页  二、截图 各位可根据自身科研或者工作需要，更改成自己的标签名，替换为自己需要的网站网址\n      日历, 在index.html中可以设置自己认为最最重要的日子\n\u0026lt;script\u0026gt; // 显示日期时钟 showClock(); //在日历上显示倒计时 countDown({\u0026#34;y\u0026#34;: 2022, \u0026#34;m\u0026#34;: 6, \u0026#34;d\u0026#34;: 1 }, \u0026#34;Message when your countdown ends\u0026#34;); \u0026lt;/script\u0026gt;   待办事项\n  三、添加链接 网站链接需要直接加到index.html的 \u0026lt;div class=\u0026quot;slides-container\u0026quot;标签内，以学术生活如下\n\u0026lt;div class=\u0026#34;slide\u0026#34; name=\u0026#34;科研\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;part\u0026#34;\u0026gt; \u0026lt;h1\u0026gt; 数据库 \u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;links\u0026#34;\u0026gt; \u0026lt;a href=\u0026#39;https://scholar.google.com/\u0026#39;\u0026gt;Google Scholar\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://webofknowledge.com/\u0026#39;\u0026gt; Web of Science \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://search.ebscohost.com/\u0026#39;\u0026gt; EBSCO \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://scholar.cnki.net/\u0026#39;\u0026gt; CNKI Scholar \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://www.ssrn.com/index.cfm/en/\u0026#39;\u0026gt;SSRN\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://www.proquest.com/site/pqdd_unavailable.shtml\u0026#39;\u0026gt; ProQuest \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://pubsonline.informs.org/\u0026#39;\u0026gt; Informs \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://www.emerald.com/insight/\u0026#39;\u0026gt; Emerald Insight \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://link.springer.com/\u0026#39;\u0026gt; Springer \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;http://www.cnki.net/\u0026#39;\u0026gt; 知网 \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;   四、使用方法  点击时钟，弹出日期框和倒计时信息。 搜索框支持!command搜索语法. 例如    搜索框命令 目标网站 例子 作用     !scholar google scholar !scholar python 在谷歌学术中搜python相关信息   !book 豆瓣读书 !book python 在豆瓣读书中搜python相关书籍信息   !movie 豆瓣电影 !movie 悬崖之上 在豆瓣电影中搜悬崖之上相关电影信息   !zhihu 知乎 !zhihu python 在知乎网站搜python相关信息   !youtube youtube !youtube python 在youtube搜索python相关视频   !taobao 淘宝 !taobao python 在淘宝搜python相关商品服务   !jd 京东 !jd python 在京东搜python相关商品服务   !bilibili B站 !bilibili python 在B站搜索python相关视频   !github github !github python 在github上搜python相关仓库代码等信息   !mail qq邮箱 !mail 默认打开qq邮箱      五、项目代码结构 ├── components │ ├── calendar.js │ ├── clock.js │ ├── countdown.js │ ├── notes.js │ ├── search-box.js │ └── slides.js ├── css │ ├── main_min.css │ └── main.scss ├── index.html  Components文件夹: 含有浏览器主页所需的所有组件js文件。  calendar.js -\u0026gt; 日历 clock.js -\u0026gt; 时钟 countdown.js -\u0026gt; 倒计时 notes.js -\u0026gt;待办事项todo list search-box.js -\u0026gt; 主页搜索框 slides.js -\u0026gt; 页面滑动功能   index.html -\u0026gt; 浏览器主页入口  六、代码获取  直接下载 github更多代码  ","permalink":"/blog/%E6%B5%8F%E8%A7%88%E5%99%A8%E9%A6%96%E9%A1%B5/","summary":"参考GreatDanton 项目，丢弃庞杂丑陋的浏览器收藏夹， 打造简洁科研浏览器首页。\n每个人都可以自定义自己的浏览器首页，替换默认浏览器首页/起始页。 该项目可在任何现代浏览器上运行，只需将index.html设置为主页并添加自己的链接 到index.html\n点击查看效果 \n一、功能  搜索(google) 日历(倒计时) 待办事项 支持DIY自己的首页  二、截图 各位可根据自身科研或者工作需要，更改成自己的标签名，替换为自己需要的网站网址\n      日历, 在index.html中可以设置自己认为最最重要的日子\n\u0026lt;script\u0026gt; // 显示日期时钟 showClock(); //在日历上显示倒计时 countDown({\u0026#34;y\u0026#34;: 2022, \u0026#34;m\u0026#34;: 6, \u0026#34;d\u0026#34;: 1 }, \u0026#34;Message when your countdown ends\u0026#34;); \u0026lt;/script\u0026gt;   待办事项\n  三、添加链接 网站链接需要直接加到index.html的 \u0026lt;div class=\u0026quot;slides-container\u0026quot;标签内，以学术生活如下\n\u0026lt;div class=\u0026#34;slide\u0026#34; name=\u0026#34;科研\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;part\u0026#34;\u0026gt; \u0026lt;h1\u0026gt; 数据库 \u0026lt;/h1\u0026gt; \u0026lt;div class=\u0026#34;links\u0026#34;\u0026gt; \u0026lt;a href=\u0026#39;https://scholar.google.com/\u0026#39;\u0026gt;Google Scholar\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://webofknowledge.com/\u0026#39;\u0026gt; Web of Science \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#39;https://search.","title":"Hi Research 定义自己的科研首页"},{"content":"Jaal是基于Dash和Visdcc构建的可交互的Python社交网络库。由于底层使用了dash，所以我们可以认为jaal更像是一个仪表盘。基于此，jaal也提供了多种处理网络数据的可选项，例如搜索、过滤、给节点（边）上色等。所有的操作，两三行代码即可搞定。\n!pip3 install jaal 一、快速上手 本文准备了《权利的游戏》的节点nodes.csv和边edges.csv数据， 可以使用jaal的plot()函数绘制《权利的游戏》关系网络图。 必须有的字段，这里加粗了 nodes.csv数据含\n from: 节点 to: 节点 weight: 边权重 strenth:  edges.csv数据\n id 节点id，姓名 gender 节点的性别    import pandas as pd from jaal import Jaal edge_df = pd.read_csv(\u0026#39;edges.csv\u0026#39;) node_df = pd.read_csv(\u0026#39;nodes.csv\u0026#39;) Jaal(edge_df, node_df).plot() 运行代码后，会生成一个本地服务链接，例如 http://127.0.0.1:8050/ ， 点击链接，浏览器就能看到\n  二、Jaal功能  设置面板 Jaal运行产生的浏览器界面左侧会有一个设置面板，可以对数据进行搜索、筛选、上色。 搜索 可以高亮搜索到的节点 过滤 支持pandas的query语法 上色 基于类别，对节点、边进行上色。能最多支持20个类别，即节点、边数据允许有20种属性  三、 案例 3.1 搜索 第一个选项是搜索，我们可以在其中搜索图中的特定节点。 它支持在节点标签上逐字符搜索。 以下是我们尝试搜索“ Arya”的示例\n  3.2 过滤 接下来，我们进行过滤。 Jaal支持在节点和边要素上都进行过滤的选项。 为此，我们提供了单独的文本区域。 下面我们可以看到节点和边缘过滤查询的实时效果。\n  3.3 染色 最后，我们可能希望查看任何功能的整体分布，而不是进行过滤。 目前，Jaal通过提供根据任何分类特征为节点或边缘着色的选项来解决此问题。 我们可以在下面看到一个真实的例子。\n  四、 代码获取  直接下载 github更多代码  ","permalink":"/blog/jaal%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/","summary":"Jaal是基于Dash和Visdcc构建的可交互的Python社交网络库。由于底层使用了dash，所以我们可以认为jaal更像是一个仪表盘。基于此，jaal也提供了多种处理网络数据的可选项，例如搜索、过滤、给节点（边）上色等。所有的操作，两三行代码即可搞定。\n!pip3 install jaal 一、快速上手 本文准备了《权利的游戏》的节点nodes.csv和边edges.csv数据， 可以使用jaal的plot()函数绘制《权利的游戏》关系网络图。 必须有的字段，这里加粗了 nodes.csv数据含\n from: 节点 to: 节点 weight: 边权重 strenth:  edges.csv数据\n id 节点id，姓名 gender 节点的性别    import pandas as pd from jaal import Jaal edge_df = pd.read_csv(\u0026#39;edges.csv\u0026#39;) node_df = pd.read_csv(\u0026#39;nodes.csv\u0026#39;) Jaal(edge_df, node_df).plot() 运行代码后，会生成一个本地服务链接，例如 http://127.0.0.1:8050/ ， 点击链接，浏览器就能看到\n  二、Jaal功能  设置面板 Jaal运行产生的浏览器界面左侧会有一个设置面板，可以对数据进行搜索、筛选、上色。 搜索 可以高亮搜索到的节点 过滤 支持pandas的query语法 上色 基于类别，对节点、边进行上色。能最多支持20个类别，即节点、边数据允许有20种属性  三、 案例 3.1 搜索 第一个选项是搜索，我们可以在其中搜索图中的特定节点。 它支持在节点标签上逐字符搜索。 以下是我们尝试搜索“ Arya”的示例\n  3.","title":"Jaal库~轻松绘制动态社交网络关系图"},{"content":"Kaggle是个很棒的地方,对于数据科学家和机器学习工程师来说，这是一个知识的金矿。可以在同一地点找到由本领域专家带来的高质量，高效，可重现，很棒的代码。自推出以来，它已经举办了164场比赛。这些比赛吸引了来自世界各地的专家和专家加入该平台。结果，每场比赛以及Kaggle提供的大量开源数据集都有许多高质量的笔记本和脚本。\n在数据科学之旅的开始，我将去Kaggle查找数据集以练习我的技能。每当我查看其他内核时，我都会对代码的复杂性感到不知所措，然后马上回避。\n但是现在，我发现自己花费了大量时间阅读其他笔记本并提交竞赛文件。有时候，有些东西值得您度过整个周末。有时，我会发现简单但致命的有效代码技巧和最佳实践，这些技巧和最佳实践只能通过观察其他专家来学习。\n在整个系列中，您会发现我在典型的数据科学工作流程中可能有用的任何内容，包括与通用库相关的代码快捷方式，Kaggle的顶级行业专家遵循的最佳实践等，这些都是我在学习过程中学到的。\n1. 只绘制相关系数矩阵的下三角部分 好的相关矩阵可以说明数据集中目标变量之间的相关性\nimport pandas as pd df = pd.read_csv(\u0026#39;data/melbourne_housing_raw.csv\u0026#39;) df.head()   import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(16, 12)) cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\u0026#34;light\u0026#34;, as_cmap=True) sns.heatmap(df.corr(), center=0, annot=True, fmt=\u0026#39;.2f\u0026#39;, square=True, cmap=cmap) plt.show()   但上图中，数据集中存在大量的特征，导致相似矩阵过于庞大，让人看起来不知所措。\n相关矩阵大部分沿主对角线对称，因此它们包含重复数据。 同样，对角线本身也没有用。 让我们看看如何只绘制有用的一半：\nimport numpy as np # 计算相关系数 matrix = df.corr() # 创建遮罩（为了只显示下三角） mask = np.triu(np.ones_like(matrix, dtype=bool)) # 定制调色板 cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\u0026#34;light\u0026#34;, as_cmap=True) # 设定图片尺寸 plt.figure(figsize=(16, 12)) # 绘制相似矩阵热力图 sns.heatmap(matrix, mask=mask, center=0, annot=True, fmt=\u0026#39;.2f\u0026#39;, square=True, cmap=cmap) plt.show()   由此产生的可视化图更容易解释并且没有视觉干扰干扰。\n 首先，我们使用DataFrame的.corr方法构建相关矩阵。 然后，我们使用dtype设置为bool的np.ones_like函数来创建一个True矩阵，其形状与DataFrame相同：  np.ones((5, 5)) array([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]])  np.ones_like(np.ones((4, 4)), dtype=bool) array([[ True, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True]])  将布尔方阵传递给Numpy的.triu函数，该函数将返回一个二维布尔蒙版，其中包含矩阵下三角的False值。\nnp.triu(np.ones_like(np.ones((4, 4)), dtype=bool)) array([[ True, True, True, True], [False, True, True, True], [False, False, True, True], [False, False, False, True]])  然后，我们可以将其传递给Seaborn的heatmap函数，以根据此蒙版对矩阵进行可视化\nns.heatmap(matrix, mask=mask, center=0, annot=True, fmt=\u0026#39;.2f\u0026#39;, square=True, cmap=cmap) \n2. value_counts考虑缺失值 使用value_counts时，可将dropna设置为False来查看任何列中缺失值的比例。通过确定缺失值的比例，可以决定是否丢弃含有缺失值的记录。\n#字段CouncilArea的数据分布情况 df.CouncilArea.value_counts(dropna=False, normalize=True).head() Boroondara City Council 0.105431 Darebin City Council 0.081791 Moreland City Council 0.060877 Glen Eira City Council 0.057549 Melbourne City Council 0.056000 Name: CouncilArea, dtype: float64  但是，如果要查看所有列中缺失值的比例，则value_counts不是最佳选择。 相反，您可以执行以下操作：\n  首先，通过将缺失值的数量除以DataFrame的长度来找到比例。\n  然后，您可以过滤掉0％的列，即i。 e。 只选择缺少值的列。\n  missing_props = df.isna().sum() / len(df) missing_props[missing_props \u0026gt; 0].sort_values(ascending=False) BuildingArea 0.605761 YearBuilt 0.553863 Landsize 0.338813 Car 0.250394 Bathroom 0.235993 Bedroom2 0.235735 Longtitude 0.228821 Lattitude 0.228821 Price 0.218321 Propertycount 0.000086 Regionname 0.000086 CouncilArea 0.000086 Postcode 0.000029 Distance 0.000029 dtype: float64  3. 使用Pandas的Styler 我们中的许多人从未意识到pandas的巨大潜力。pandas的一个被低估且经常被忽视的功能是其对DataFrames进行样式设置的能力。 使用pandas DataFrames的.style属性，可以将条件设计和样式应用于它们。\n作为第一个示例，让我们看看如何根据每个单元格的值来更改背景颜色：\ndiamonds = pd.read_csv(\u0026#39;data/diamonds.csv\u0026#39;) diamonds.head()   pd.crosstab(diamonds.cut, diamonds.clarity).style.background_gradient(cmap=\u0026#39;rocket_r\u0026#39;)   几乎没有使用Seaborn的热图功能的热图。 在这里，我们使用pd.crosstab对钻石切割(cut)和净度(clarity)的每种组合进行计数。\n将.style.background_gradient与调色板配合使用，您可以轻松地发现哪种组合出现得最多。 仅从上面的DataFrame中，我们可以看到大多数钻石都是“ VS2”净度类型。\n我们甚至可以通过在交叉表中找到每种钻石切割cut和净度clarity组合的平均价格来进一步做到这一点：\npd.crosstab(diamonds.cut, diamonds.clarity, aggfunc=np.mean, values=diamonds.price).style.background_gradient(cmap=\u0026#39;rocket_r\u0026#39;)   通过将.format方法与格式字符串{：.2f}链接起来，我们指定了2个浮点数的精度。\nagg_prices = pd.crosstab(diamonds.cut, diamonds.clarity, aggfunc=np.mean, values=diamonds.price).style.background_gradient(cmap=\u0026#39;rocket_r\u0026#39;) agg_prices.format(\u0026#39;{:.2f}\u0026#39;)   4. matplotlib默认全局设置 在进行探索性数据分析时，您可能想对所有绘图应用自定义调色板，对刻度标签使用更大的字体，更改图例的位置，使用固定的图形大小等。\n对绘图自定义参数的更改是一项非常无聊，重复且耗时的任务。 幸运的是，您可以使用Matplotlib的rcParams为绘图设置全局配置。\nrcParams只是一个普通的Python字典，其中包含Matplotlib的默认设置：\nfrom matplotlib import rcParams import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) rcParams RcParams({'_internal.classic_mode': False, 'agg.path.chunksize': 0, 'animation.avconv_args': [], 'animation.avconv_path': 'avconv', 'animation.bitrate': -1, 'animation.codec': 'h264', 'animation.convert_args': [], 'animation.convert_path': 'convert', 'animation.embed_limit': 20.0, 'animation.ffmpeg_args': [], 'animation.ffmpeg_path': 'ffmpeg', 'animation.frame_format': 'png', 'animation.html': 'none', 'animation.html_args': [], 'animation.writer': 'ffmpeg', 'axes.autolimit_mode': 'data', 'axes.axisbelow': 'line', 'axes.edgecolor': 'black', 'axes.facecolor': 'white', 'axes.formatter.limits': [-7, 7], 'axes.formatter.min_exponent': 0, 'axes.formatter.offset_threshold': 4, 'axes.formatter.use_locale': False, 'axes.formatter.use_mathtext': False, 'axes.formatter.useoffset': True, 'axes.grid': False, 'axes.grid.axis': 'both', 'axes.grid.which': 'major', 'axes.labelcolor': 'black', 'axes.labelpad': 4.0, 'axes.labelsize': 'medium', 'axes.labelweight': 'normal', 'axes.linewidth': 0.8, 'axes.prop_cycle': cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']), 'axes.spines.bottom': True, 'axes.spines.left': True, 'axes.spines.right': True, 'axes.spines.top': True, 'axes.titlepad': 6.0, 'axes.titlesize': 'large', 'axes.titleweight': 'normal', 'axes.unicode_minus': True, 'axes.xmargin': 0.05, 'axes.ymargin': 0.05, 'axes3d.grid': True, 'backend': 'module://ipykernel.pylab.backend_inline', 'backend_fallback': True, 'boxplot.bootstrap': None, 'boxplot.boxprops.color': 'black', 'boxplot.boxprops.linestyle': '-', 'boxplot.boxprops.linewidth': 1.0, 'boxplot.capprops.color': 'black', 'boxplot.capprops.linestyle': '-', 'boxplot.capprops.linewidth': 1.0, 'boxplot.flierprops.color': 'black', 'boxplot.flierprops.linestyle': 'none', 'boxplot.flierprops.linewidth': 1.0, 'boxplot.flierprops.marker': 'o', 'boxplot.flierprops.markeredgecolor': 'black', 'boxplot.flierprops.markeredgewidth': 1.0, 'boxplot.flierprops.markerfacecolor': 'none', 'boxplot.flierprops.markersize': 6.0, 'boxplot.meanline': False, 'boxplot.meanprops.color': 'C2', 'boxplot.meanprops.linestyle': '--', 'boxplot.meanprops.linewidth': 1.0, 'boxplot.meanprops.marker': '^', 'boxplot.meanprops.markeredgecolor': 'C2', 'boxplot.meanprops.markerfacecolor': 'C2', 'boxplot.meanprops.markersize': 6.0, 'boxplot.medianprops.color': 'C1', 'boxplot.medianprops.linestyle': '-', 'boxplot.medianprops.linewidth': 1.0, 'boxplot.notch': False, 'boxplot.patchartist': False, 'boxplot.showbox': True, 'boxplot.showcaps': True, 'boxplot.showfliers': True, 'boxplot.showmeans': False, 'boxplot.vertical': True, 'boxplot.whiskerprops.color': 'black', 'boxplot.whiskerprops.linestyle': '-', 'boxplot.whiskerprops.linewidth': 1.0, 'boxplot.whiskers': 1.5, 'contour.corner_mask': True, 'contour.negative_linestyle': 'dashed', 'datapath': '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/matplotlib/mpl-data', 'date.autoformatter.day': '%Y-%m-%d', 'date.autoformatter.hour': '%m-%d %H', 'date.autoformatter.microsecond': '%M:%S.%f', 'date.autoformatter.minute': '%d %H:%M', 'date.autoformatter.month': '%Y-%m', 'date.autoformatter.second': '%H:%M:%S', 'date.autoformatter.year': '%Y', 'docstring.hardcopy': False, 'errorbar.capsize': 0.0, 'examples.directory': '', 'figure.autolayout': False, 'figure.constrained_layout.h_pad': 0.04167, 'figure.constrained_layout.hspace': 0.02, 'figure.constrained_layout.use': False, 'figure.constrained_layout.w_pad': 0.04167, 'figure.constrained_layout.wspace': 0.02, 'figure.dpi': 72.0, 'figure.edgecolor': (1, 1, 1, 0), 'figure.facecolor': (1, 1, 1, 0), 'figure.figsize': [6.0, 4.0], 'figure.frameon': True, 'figure.max_open_warning': 20, 'figure.subplot.bottom': 0.125, 'figure.subplot.hspace': 0.2, 'figure.subplot.left': 0.125, 'figure.subplot.right': 0.9, 'figure.subplot.top': 0.88, 'figure.subplot.wspace': 0.2, 'figure.titlesize': 'large', 'figure.titleweight': 'normal', 'font.cursive': ['Apple Chancery', 'Textile', 'Zapf Chancery', 'Sand', 'Script MT', 'Felipa', 'cursive'], 'font.family': ['sans-serif'], 'font.fantasy': ['Comic Sans MS', 'Chicago', 'Charcoal', 'Impact', 'Western', 'Humor Sans', 'xkcd', 'fantasy'], 'font.monospace': ['DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Computer Modern Typewriter', 'Andale Mono', 'Nimbus Mono L', 'Courier New', 'Courier', 'Fixed', 'Terminal', 'monospace'], 'font.sans-serif': ['DejaVu Sans', 'Bitstream Vera Sans', 'Computer Modern Sans Serif', 'Lucida Grande', 'Verdana', 'Geneva', 'Lucid', 'Arial', 'Helvetica', 'Avant Garde', 'sans-serif'], 'font.serif': ['DejaVu Serif', 'Bitstream Vera Serif', 'Computer Modern Roman', 'New Century Schoolbook', 'Century Schoolbook L', 'Utopia', 'ITC Bookman', 'Bookman', 'Nimbus Roman No9 L', 'Times New Roman', 'Times', 'Palatino', 'Charter', 'serif'], 'font.size': 10.0, 'font.stretch': 'normal', 'font.style': 'normal', 'font.variant': 'normal', 'font.weight': 'normal', 'grid.alpha': 1.0, 'grid.color': '#b0b0b0', 'grid.linestyle': '-', 'grid.linewidth': 0.8, 'hatch.color': 'black', 'hatch.linewidth': 1.0, 'hist.bins': 10, 'image.aspect': 'equal', 'image.cmap': 'viridis', 'image.composite_image': True, 'image.interpolation': 'nearest', 'image.lut': 256, 'image.origin': 'upper', 'image.resample': True, 'interactive': True, 'keymap.all_axes': ['a'], 'keymap.back': ['left', 'c', 'backspace', 'MouseButton.BACK'], 'keymap.copy': ['ctrl+c', 'cmd+c'], 'keymap.forward': ['right', 'v', 'MouseButton.FORWARD'], 'keymap.fullscreen': ['f', 'ctrl+f'], 'keymap.grid': ['g'], 'keymap.grid_minor': ['G'], 'keymap.help': ['f1'], 'keymap.home': ['h', 'r', 'home'], 'keymap.pan': ['p'], 'keymap.quit': ['ctrl+w', 'cmd+w', 'q'], 'keymap.quit_all': ['W', 'cmd+W', 'Q'], 'keymap.save': ['s', 'ctrl+s'], 'keymap.xscale': ['k', 'L'], 'keymap.yscale': ['l'], 'keymap.zoom': ['o'], 'legend.borderaxespad': 0.5, 'legend.borderpad': 0.4, 'legend.columnspacing': 2.0, 'legend.edgecolor': '0.8', 'legend.facecolor': 'inherit', 'legend.fancybox': True, 'legend.fontsize': 'medium', 'legend.framealpha': 0.8, 'legend.frameon': True, 'legend.handleheight': 0.7, 'legend.handlelength': 2.0, 'legend.handletextpad': 0.8, 'legend.labelspacing': 0.5, 'legend.loc': 'best', 'legend.markerscale': 1.0, 'legend.numpoints': 1, 'legend.scatterpoints': 1, 'legend.shadow': False, 'legend.title_fontsize': None, 'lines.antialiased': True, 'lines.color': 'C0', 'lines.dash_capstyle': 'butt', 'lines.dash_joinstyle': 'round', 'lines.dashdot_pattern': [6.4, 1.6, 1.0, 1.6], 'lines.dashed_pattern': [3.7, 1.6], 'lines.dotted_pattern': [1.0, 1.65], 'lines.linestyle': '-', 'lines.linewidth': 1.5, 'lines.marker': 'None', 'lines.markeredgecolor': 'auto', 'lines.markeredgewidth': 1.0, 'lines.markerfacecolor': 'auto', 'lines.markersize': 6.0, 'lines.scale_dashes': True, 'lines.solid_capstyle': 'projecting', 'lines.solid_joinstyle': 'round', 'markers.fillstyle': 'full', 'mathtext.bf': 'sans:bold', 'mathtext.cal': 'cursive', 'mathtext.default': 'it', 'mathtext.fallback_to_cm': True, 'mathtext.fontset': 'dejavusans', 'mathtext.it': 'sans:italic', 'mathtext.rm': 'sans', 'mathtext.sf': 'sans', 'mathtext.tt': 'monospace', 'patch.antialiased': True, 'patch.edgecolor': 'black', 'patch.facecolor': 'C0', 'patch.force_edgecolor': False, 'patch.linewidth': 1.0, 'path.effects': [], 'path.simplify': True, 'path.simplify_threshold': 0.1111111111111111, 'path.sketch': None, 'path.snap': True, 'pdf.compression': 6, 'pdf.fonttype': 3, 'pdf.inheritcolor': False, 'pdf.use14corefonts': False, 'pgf.preamble': '', 'pgf.rcfonts': True, 'pgf.texsystem': 'xelatex', 'polaraxes.grid': True, 'ps.distiller.res': 6000, 'ps.fonttype': 3, 'ps.papersize': 'letter', 'ps.useafm': False, 'ps.usedistiller': False, 'savefig.bbox': None, 'savefig.directory': '~', 'savefig.dpi': 'figure', 'savefig.edgecolor': 'white', 'savefig.facecolor': 'white', 'savefig.format': 'png', 'savefig.frameon': True, 'savefig.jpeg_quality': 95, 'savefig.orientation': 'portrait', 'savefig.pad_inches': 0.1, 'savefig.transparent': False, 'scatter.edgecolors': 'face', 'scatter.marker': 'o', 'svg.fonttype': 'path', 'svg.hashsalt': None, 'svg.image_inline': True, 'text.antialiased': True, 'text.color': 'black', 'text.hinting': 'auto', 'text.hinting_factor': 8, 'text.latex.preamble': '', 'text.latex.preview': False, 'text.latex.unicode': True, 'text.usetex': False, 'timezone': 'UTC', 'tk.window_focus': False, 'toolbar': 'toolbar2', 'verbose.fileo': 'sys.stdout', 'verbose.level': 'silent', 'webagg.address': '127.0.0.1', 'webagg.open_in_browser': True, 'webagg.port': 8988, 'webagg.port_retries': 50, 'xtick.alignment': 'center', 'xtick.bottom': True, 'xtick.color': 'black', 'xtick.direction': 'out', 'xtick.labelbottom': True, 'xtick.labelsize': 'medium', 'xtick.labeltop': False, 'xtick.major.bottom': True, 'xtick.major.pad': 3.5, 'xtick.major.size': 3.5, 'xtick.major.top': True, 'xtick.major.width': 0.8, 'xtick.minor.bottom': True, 'xtick.minor.pad': 3.4, 'xtick.minor.size': 2.0, 'xtick.minor.top': True, 'xtick.minor.visible': False, 'xtick.minor.width': 0.6, 'xtick.top': False, 'ytick.alignment': 'center_baseline', 'ytick.color': 'black', 'ytick.direction': 'out', 'ytick.labelleft': True, 'ytick.labelright': False, 'ytick.labelsize': 'medium', 'ytick.left': True, 'ytick.major.left': True, 'ytick.major.pad': 3.5, 'ytick.major.right': True, 'ytick.major.size': 3.5, 'ytick.major.width': 0.8, 'ytick.minor.left': True, 'ytick.minor.pad': 3.4, 'ytick.minor.right': True, 'ytick.minor.size': 2.0, 'ytick.minor.visible': False, 'ytick.minor.width': 0.6, 'ytick.right': False})  您可以调整每个图的任意参数设置，一般的图像设置如固定图形大小，刻度标签字体大小以及其他一些参数。\n通过这种设置，可以减少很多重复的代码量\n# 去掉顶部和右侧的线条Remove top and right spines rcParams[\u0026#39;axes.spines.top\u0026#39;] = False rcParams[\u0026#39;axes.spines.right\u0026#39;] = False # 设置图的尺寸Set fixed figure size rcParams[\u0026#39;figure.figsize\u0026#39;] = [12, 9] # 设置图片像素清晰度 Set dots per inch to 300, very high quality images rcParams[\u0026#39;figure.dpi\u0026#39;] = 300 # 设置自动调整布局Enable autolayout rcParams[\u0026#39;figure.autolayout\u0026#39;] = True # 设置全局字号Set global fontsize rcParams[\u0026#39;font.style\u0026#39;] = 16 # 刻度字号Fontsize of ticklabels rcParams[\u0026#39;xtick.labelsize\u0026#39;] = 10 rcParams[\u0026#39;ytick.labelsize\u0026#39;] = 10 \n5. Pandas全局设置 就像Matplotlib一样，pandas具有可以使用的全局设置。 当然，它们大多数与显示选项有关。\n get_option() - 获取pandas单个选项 set_option() — 设置pandas单个选项 reset_option() — 重置pandas选项值  我更喜欢显示所有的列，lets go\npd.set_option(\u0026#39;display.max_columns\u0026#39;, None) df.head()   6. 代码获取  直接下载 github更多代码  ","permalink":"/blog/kaggle%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","summary":"Kaggle是个很棒的地方,对于数据科学家和机器学习工程师来说，这是一个知识的金矿。可以在同一地点找到由本领域专家带来的高质量，高效，可重现，很棒的代码。自推出以来，它已经举办了164场比赛。这些比赛吸引了来自世界各地的专家和专家加入该平台。结果，每场比赛以及Kaggle提供的大量开源数据集都有许多高质量的笔记本和脚本。\n在数据科学之旅的开始，我将去Kaggle查找数据集以练习我的技能。每当我查看其他内核时，我都会对代码的复杂性感到不知所措，然后马上回避。\n但是现在，我发现自己花费了大量时间阅读其他笔记本并提交竞赛文件。有时候，有些东西值得您度过整个周末。有时，我会发现简单但致命的有效代码技巧和最佳实践，这些技巧和最佳实践只能通过观察其他专家来学习。\n在整个系列中，您会发现我在典型的数据科学工作流程中可能有用的任何内容，包括与通用库相关的代码快捷方式，Kaggle的顶级行业专家遵循的最佳实践等，这些都是我在学习过程中学到的。\n1. 只绘制相关系数矩阵的下三角部分 好的相关矩阵可以说明数据集中目标变量之间的相关性\nimport pandas as pd df = pd.read_csv(\u0026#39;data/melbourne_housing_raw.csv\u0026#39;) df.head()   import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(16, 12)) cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\u0026#34;light\u0026#34;, as_cmap=True) sns.heatmap(df.corr(), center=0, annot=True, fmt=\u0026#39;.2f\u0026#39;, square=True, cmap=cmap) plt.show()   但上图中，数据集中存在大量的特征，导致相似矩阵过于庞大，让人看起来不知所措。\n相关矩阵大部分沿主对角线对称，因此它们包含重复数据。 同样，对角线本身也没有用。 让我们看看如何只绘制有用的一半：\nimport numpy as np # 计算相关系数 matrix = df.corr() # 创建遮罩（为了只显示下三角） mask = np.triu(np.ones_like(matrix, dtype=bool)) # 定制调色板 cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\u0026#34;light\u0026#34;, as_cmap=True) # 设定图片尺寸 plt.","title":"Kaggle数据挖掘最佳实践"},{"content":"学习编程就是在遇到错误、认识错误、解决错误的过程。遇到错误，大家要发挥主观能动性，用自己的英文阅读能力去先读一下英文报错提示，一般情况下错误提示会告诉你是什么类型的错误，错误出在哪一行。\n再结合百度谷歌，80%以上的问题都能解决。现在我们了解一下常见的问题都有哪些，如何克服这些问题。\n1. 忘记写冒号 在 if、elif、else、for、while、def语句后面忘记添加 :\nage = 42 if age == 42 print(\u0026#39;Hello!\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-10-1f5acea116cf\u0026gt;\u0026quot;, line 3 if age == 42 ^ SyntaxError: invalid syntax  age = 42 if age == 42: print(\u0026#39;Hello!\u0026#39;) Hello!  \n2. 误用 = = 是赋值操作，而判断两个值是否相等是 ==\ngender = \u0026#39;男\u0026#39; if gender = \u0026#39;男\u0026#39;: print(\u0026#39;Man\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-12-c3ceea5a9004\u0026gt;\u0026quot;, line 3 if gender = '男': ^ SyntaxError: invalid syntax  gender = \u0026#39;男\u0026#39; if gender == \u0026#39;男\u0026#39;: print(\u0026#39;Man\u0026#39;) Man  \n3. 错误的缩进 Python用缩进区分代码块，常见的错误用法：\nprint(\u0026#39;Hello!\u0026#39;) print(\u0026#39;Howdy!\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-14-784bdb6e1df5\u0026gt;\u0026quot;, line 2 print('Howdy!') ^ IndentationError: unexpected indent  print(\u0026#39;Hello!\u0026#39;) print(\u0026#39;Howdy!\u0026#39;) Hello! Howdy!  num = 25 if num == 25: print(\u0026#39;Hello!\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-16-8e4debcdf119\u0026gt;\u0026quot;, line 3 print('Hello!') ^ IndentationError: expected an indented block  num = 25 if num == 25: print(\u0026#39;Hello!\u0026#39;) Hello!  \n4. 变量没有定义 if c in [\u0026#39;New York\u0026#39;, \u0026#39;Bei Jing\u0026#39;, \u0026#39;Tokyo\u0026#39;]: print(\u0026#39;This is a mega city\u0026#39;) --------------------------------------------------------------------------- NameError Traceback (most recent call last) \u0026lt;ipython-input-21-d91d0b36da73\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 if c in ['New York', 'Bei Jing', 'Tokyo']: 2 print('This is a mega c') NameError: name 'c' is not defined  city =\u0026#39;New York\u0026#39; if city in [\u0026#39;New York\u0026#39;, \u0026#39;Bei Jing\u0026#39;, \u0026#39;Tokyo\u0026#39;]: print(\u0026#39;This is a mega city\u0026#39;) This is a mega city  \n5. 中英文输入法导致的错误  英文冒号 英文括号 英文逗号 英文单双引号  if 5\u0026gt;3： print(\u0026#39;5比3大\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-23-47f8b985b82d\u0026gt;\u0026quot;, line 1 if 5\u0026gt;3： ^ SyntaxError: invalid character in identifier  if 5\u0026gt;3: print(\u0026#39;5比3大\u0026#39;) 5比3大  spam = [1, 2， 3]  File \u0026quot;\u0026lt;ipython-input-26-a003060d051a\u0026gt;\u0026quot;, line 1 spam = [1, 2， 3] ^ SyntaxError: invalid character in identifier  spam = [1, 2, 3] spam [1, 2, 3]  if 5\u0026gt;3: print(\u0026#39;5比3大’)  File \u0026quot;\u0026lt;ipython-input-30-ac2e4eb87092\u0026gt;\u0026quot;, line 2 print('5比3大’) ^ SyntaxError: EOL while scanning string literal  if 5\u0026gt;3: print(\u0026#39;5比3大\u0026#39;) 5比3大  \n6. 不同数据类型的拼接 同种数据类型 字符串/列表/元组 支持拼接\n字典/集合不支持拼接\n\u0026#39;I have \u0026#39; + 12 + \u0026#39; eggs.\u0026#39; --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-32-20c7c89a2ec6\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 'I have ' + 12 + ' eggs.' TypeError: can only concatenate str (not \u0026quot;int\u0026quot;) to str  \u0026#39;I have {}eggs.\u0026#39;.format(12) 'I have 12 eggs.'  [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]+\u0026#39;def\u0026#39; --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-35-0e8919333d6b\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 ['a', 'b', 'c']+'def' TypeError: can only concatenate list (not \u0026quot;str\u0026quot;) to list  (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;)+[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-36-90742621216d\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 ('a', 'b', 'c')+['a', 'b', 'c'] TypeError: can only concatenate tuple (not \u0026quot;list\u0026quot;) to tuple  set([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;])+set([\u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;]) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-37-ddf5fb1e6c8c\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 set(['a', 'b', 'c'])+set(['d', 'e']) TypeError: unsupported operand type(s) for +: 'set' and 'set'  grades1 = {\u0026#39;Mary\u0026#39;:99, \u0026#39;Henry\u0026#39;:77} grades2 = {\u0026#39;David\u0026#39;:88, \u0026#39;Unique\u0026#39;:89} grades1+grades2 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-38-1b1456844331\u0026gt; in \u0026lt;module\u0026gt; 2 grades2 = {'David':88, 'Unique':89} 3 ----\u0026gt; 4 grades1+grades2 TypeError: unsupported operand type(s) for +: 'dict' and 'dict'  \n7. 索引位置问题 spam = [\u0026#39;cat\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;mouse\u0026#39;] print(spam[5]) --------------------------------------------------------------------------- IndexError Traceback (most recent call last) \u0026lt;ipython-input-41-e0a79346266d\u0026gt; in \u0026lt;module\u0026gt; 1 spam = ['cat', 'dog', 'mouse'] ----\u0026gt; 2 print(spam[5]) IndexError: list index out of range  \n8. 使用字典中不存在的键 在字典对象中访问 key 可以使用 []，\n但是如果该 key 不存在，就会导致：KeyError: \u0026lsquo;zebra\u0026rsquo;\nspam = {\u0026#39;cat\u0026#39;: \u0026#39;Zophie\u0026#39;, \u0026#39;dog\u0026#39;: \u0026#39;Basil\u0026#39;, \u0026#39;mouse\u0026#39;: \u0026#39;Whiskers\u0026#39;} print(spam[\u0026#39;zebra\u0026#39;]) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) \u0026lt;ipython-input-42-92c9b44ff034\u0026gt; in \u0026lt;module\u0026gt; 3 'mouse': 'Whiskers'} 4 ----\u0026gt; 5 print(spam['zebra']) KeyError: 'zebra'  为了避免这种情况，可以使用 get 方法\nspam = {\u0026#39;cat\u0026#39;: \u0026#39;Zophie\u0026#39;, \u0026#39;dog\u0026#39;: \u0026#39;Basil\u0026#39;, \u0026#39;mouse\u0026#39;: \u0026#39;Whiskers\u0026#39;} print(spam.get(\u0026#39;zebra\u0026#39;)) None  key 不存在时，get 默认返回 None\n9. 忘了括号 当函数中传入的是函数或者方法时，容易漏写括号\nspam = {\u0026#39;cat\u0026#39;: \u0026#39;Zophie\u0026#39;, \u0026#39;dog\u0026#39;: \u0026#39;Basil\u0026#39;, \u0026#39;mouse\u0026#39;: \u0026#39;Whiskers\u0026#39;} print(spam.get(\u0026#39;zebra\u0026#39;) #end of funtion  File \u0026quot;\u0026lt;ipython-input-44-d105cc86097c\u0026gt;\u0026quot;, line 5 print(spam.get('zebra') #end of funtion ^ SyntaxError: unexpected EOF while parsing  spam = {\u0026#39;cat\u0026#39;: \u0026#39;Zophie\u0026#39;, \u0026#39;dog\u0026#39;: \u0026#39;Basil\u0026#39;, \u0026#39;mouse\u0026#39;: \u0026#39;Whiskers\u0026#39;} print(spam.get(\u0026#39;zebra\u0026#39;)) None  \n10. 漏传参数 def diyadd(x, y, z): return x+y+z diyadd(1, 2) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-46-7184f3f906ca\u0026gt; in \u0026lt;module\u0026gt; 2 return x+y+z 3 ----\u0026gt; 4 diyadd(1, 2) TypeError: diyadd() missing 1 required positional argument: 'z'  diyadd(1, 2, 4) 7  \n11. 缺失依赖库 电脑中没有相关的库\nimport packagename --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) \u0026lt;ipython-input-48-6d7d6f569116\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 import packagename ModuleNotFoundError: No module named 'packagename'  !pip install packagename \n12. 使用了python中的关键词 如try、except、def、class、object、None、True、False等\ntry = 5 print(try)  File \u0026quot;\u0026lt;ipython-input-49-508e87fe2ff3\u0026gt;\u0026quot;, line 1 try = 5 ^ SyntaxError: invalid syntax  a = 5 print(a) 5  def = 6 print(def)  File \u0026quot;\u0026lt;ipython-input-51-c797890e9b85\u0026gt;\u0026quot;, line 1 def = 6 ^ SyntaxError: invalid syntax  d = 6 print(d) 6  13. 文件编码问题 import pandas as pd df = pd.read_csv(\u0026#39;data/twitter_sentiment.csv\u0026#39;) df.head() UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 7-8: invalid continuation byte UnicodeDecodeError Traceback (most recent call last) \u0026lt;ipython-input-53-f7ee81cff3e5\u0026gt; in \u0026lt;module\u0026gt; 1 import pandas as pd 2 ----\u0026gt; 3 df = pd.read_csv('data/twitter_sentiment.csv') 4 df.head() pandas\\_libs\\parsers.pyx in pandas._libs.parsers._string_box_utf8() UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 7-8: invalid continuation byte  import pandas as pd #gbk/utf-8只能解决大部分编码问题，但不能解决全部问题 df = pd.read_csv(\u0026#39;data/twitter_sentiment.csv\u0026#39;, encoding=\u0026#39;gbk\u0026#39;) df.head() --------------------------------------------------------------------------- UnicodeDecodeError Traceback (most recent call last) \u0026lt;ipython-input-55-6aa161f42239\u0026gt; in \u0026lt;module\u0026gt; 2 3 #gbk/utf-8只能解决大部分编码问题，但不能解决全部问题 ----\u0026gt; 4 df = pd.read_csv('data/twitter_sentiment.csv', encoding='gbk') 5 df.head() c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pandas\\io\\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision) 674 ) 675 -\u0026gt; 1891 self._reader = parsers.TextReader(src, **kwds) 1892 self.unnamed_cols = self._reader.unnamed_cols 1893 pandas\\_libs\\parsers.pyx in pandas._libs.parsers.TextReader.__cinit__() pandas\\_libs\\parsers.pyx in pandas._libs.parsers.TextReader._get_header() pandas\\_libs\\parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows() pandas\\_libs\\parsers.pyx in pandas._libs.parsers.raise_parser_error() UnicodeDecodeError: 'gbk' codec can't decode byte 0xbd in position 10717: illegal multibyte sequence  上面的程序会提示编码错误， 尝试encoding编码参数传入utf-8、gbk，也没有解决问题。\n那怎么找到正确的编码参数呢？ import chardet #读取为二进制数据 binary_data = open(\u0026#39;data/twitter_sentiment.csv\u0026#39;, \u0026#39;rb\u0026#39;).read() #传给chardet.detect，稍等片刻 chardet.detect(binary_data) {'encoding': 'Windows-1252', 'confidence': 0.7291192008535122, 'language': ''}  import pandas as pd df = pd.read_csv(\u0026#39;data/twitter_sentiment.csv\u0026#39;, encoding=\u0026#39;Windows-1252\u0026#39;) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ItemID Sentiment SentimentText     0 1 0 is so sad for my APL frie...   1 2 0 I missed the New Moon trail...   2 3 1 omg its already 7:30 :O   3 4 0 .. Omgaga. Im sooo im gunna CRy. I'...   4 5 0 i think mi bf is cheating on me!!! ...     \n14. 路径字符串写法  Mac\u0026amp;Win 推荐使用 / 写法 如果使用\\ 写法，安全起见，请换成\\\\ （Mac不支持\\\\ ）  \\n \\t \\d open(\u0026#39;data/test.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() '章节设计\\n\\n第一部分 环境配置\\n第二部分 快速入门python\\n第三部分 网络爬虫\\n第四部分 简单的文本分析\\n第五部分 进阶文本分析'  open(\u0026#39;data\\test.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() --------------------------------------------------------------------------- OSError Traceback (most recent call last) \u0026lt;ipython-input-59-d855ed58b500\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 open('data\\test.txt', encoding='utf-8').read() OSError: [Errno 22] Invalid argument: 'data\\test.txt'  open(\u0026#39;data\\\\test.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() '章节设计\\n\\n第一部分 环境配置\\n第二部分 快速入门python\\n第三部分 网络爬虫\\n第四部分 简单的文本分析\\n第五部分 进阶文本分析'  open(\u0026#39;data\\Test.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() '章节设计\\n\\n第一部分 环境配置\\n第二部分 快速入门python\\n第三部分 网络爬虫\\n第四部分 简单的文本分析\\n第五部分 进阶文本分析'  \n","permalink":"/blog/%E5%B8%B8%E5%87%BA%E9%94%99%E8%AF%AF%E6%B1%87%E6%80%BB/","summary":"学习编程就是在遇到错误、认识错误、解决错误的过程。遇到错误，大家要发挥主观能动性，用自己的英文阅读能力去先读一下英文报错提示，一般情况下错误提示会告诉你是什么类型的错误，错误出在哪一行。\n再结合百度谷歌，80%以上的问题都能解决。现在我们了解一下常见的问题都有哪些，如何克服这些问题。\n1. 忘记写冒号 在 if、elif、else、for、while、def语句后面忘记添加 :\nage = 42 if age == 42 print(\u0026#39;Hello!\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-10-1f5acea116cf\u0026gt;\u0026quot;, line 3 if age == 42 ^ SyntaxError: invalid syntax  age = 42 if age == 42: print(\u0026#39;Hello!\u0026#39;) Hello!  \n2. 误用 = = 是赋值操作，而判断两个值是否相等是 ==\ngender = \u0026#39;男\u0026#39; if gender = \u0026#39;男\u0026#39;: print(\u0026#39;Man\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-12-c3ceea5a9004\u0026gt;\u0026quot;, line 3 if gender = '男': ^ SyntaxError: invalid syntax  gender = \u0026#39;男\u0026#39; if gender == \u0026#39;男\u0026#39;: print(\u0026#39;Man\u0026#39;) Man","title":"python常见错误及解决办法"},{"content":"正则表达式主要用于数据清洗，比如从脏乱差的文本中抽取出自己需要的信息。常见于爬虫和文本分析。\n一、正则表达式中的符号 按照符号的功能，我将其分为三类，一般情况下表达式都是由这三种符号组成的。\n1.1 正则字符 预警，听不懂看不懂，都不要紧的。不要绞尽脑汁，本节后面会柳暗花明的。\n   正则符号 描述 匹配自己时     \\ 转义字符。例如， \u0026lsquo;n\u0026rsquo; 匹配字符 \u0026lsquo;n\u0026rsquo;。 '\\n'   ( ) 标记一个子表达式的开始和结束位置。 \\( \\)   . 匹配除换行符 \\n 之外的任何单字符。 \\.    | |左右两侧均可参与匹配    \\d 匹配字符串中的单个数字    a-zA-Z 匹配全部英文字符    0-9 匹配全部数字    \\s 匹配字符串中的\\n,\\t,空格    [] 中括号内任意正则符号均可参与匹配 \\[ \\]   ^ 当在方括号表达式中使用，^对其后的正则表达式进行了反义表达。 \\^    1.2 限定字符 提前预警，听不懂不要绞尽脑汁，本节后面会柳暗花明的\n   正则符号 描述 匹配自己时     * 匹配前面的子表达式零次或多次。 \\*   ? 匹配前面的子表达式零次或一次 \\?   + 匹配前面的子表达式一次或多次。 \\+   {m} n 是一个非负整数。匹配确定的 m 次。    {m,} m 是一个非负整数。至少匹配m 次。    {m, n} m 和 n 均为非负整数，其中m \u0026lt;= n。最少匹配 m 次且最多匹配 n 次。     1.3 定位字符 预警，听不懂不要绞尽脑汁，本节后面会柳暗花明的\n   正则符号 描述 匹配自己时     ^ 匹配输入字符串的开始位置。 \\^   $ 匹配输入字符串的结尾位置 \\$   \\b 匹配一个单词边界，即字与空格间的位置    \\B 非单词边界匹配     \n二、re库常用方法 至暗时刻已过，光明即将到来\n   re库常用函数 作用     re.findall(pattern, string) 根据pattern返回匹配结果（列表）    |re.split(pattern, string) |使用pattern分割string，返回列表 |re.sub(pattern, repl, string)|使用repl替换string中的pattern|\n\n三、只需要掌握 万能的百度谷歌+你的尝试，比什么都强大\n 搜索引擎检索到自己需要的正则表达式 最简单最好用表达式(.*?) 在正则表达式测试网站验证自己的正则表达式  3.1 检索找到自己需要的正则表达式 比如我只需要中文，其余字符统统不要。\n我会在百度搜中文正则表达式\n发现很多网页中网友提到````，于是\n[\\u4e00-\\u9fa5]\nimport re pattern = \u0026#39;[\\u4e00-\\u9fa5]+\u0026#39; string = \u0026#34;\u0026#34;\u0026#34;Python是一门面向对象的编程语言，诞生于1991年。\\ 目前以广泛应用在网站开发、游戏软件开发、数据采集、机器学习等多个领域。\\ 一般情况下Python是Java的20%，所以说人生苦短，我用Python。\u0026#34;\u0026#34;\u0026#34; chinese_words = re.findall(pattern, string) chinese_text = \u0026#39;\u0026#39;.join(chinese_words) chinese_text '是一门面向对象的编程语言诞生于年目前以广泛应用在网站开发游戏软件开发数据采集机器学习等多个领域一般情况下是的所以说人生苦短我用'  3.2 最简单最好用表达式(.*?) (.*?)特别好用，ta的暗号及使用口诀一定要背过\npattern设计步骤：\n正则符号组成正则表达式，用于匹配需要的字符。\n 找到重复的一致的规律 复制粘贴到pattern中 扣掉想要的数据 替换为(.*?) 或者相应的正则符号表达式*  比如现在需要快速挖掘出intros中的姓名、籍贯和年龄\nimport re pattern = \u0026#39;我叫(.*?)，来自(.*?)，今年(.*?)岁。\u0026#39; intros = [\u0026#39;我叫张三，来自山东，今年25岁。\u0026#39;, \u0026#39;我叫李四，来自河北，今年28岁。\u0026#39;, \u0026#39;我叫王五，来自河南，今年24岁。\u0026#39;] for intro in intros: info = re.findall(pattern, intro) print(info) [('张三', '山东', '25')] [('李四', '河北', '28')] [('王五', '河南', '24')]  特别需要注意的是pattern中的(.*?)左右两侧必须有字符，否则匹配失败。\n import re pattern = \u0026#39;(.*?)，来自(.*?)，今年(.*?)\u0026#39; intros = [\u0026#39;我叫张三，来自山东，今年25岁。\u0026#39;, \u0026#39;我叫李四，来自河北，今年28岁。\u0026#39;, \u0026#39;我叫王五，来自河南，今年24岁。\u0026#39;] for intro in intros: info = re.findall(pattern, intro) print(info) [('我叫张三', '山东', '')] [('我叫李四', '河北', '')] [('我叫王五', '河南', '')]  由于\n'(.*?)，来自(.*?)，今年(.*?)'\n中最左侧和最右侧的(.*?)没有被其他字符左右包裹，导致匹配姓名和年龄失败。\nimport re pattern = \u0026#39;叫(.*?)，来自(.*?)，今年(.*?)岁\u0026#39; intros = [\u0026#39;我叫张三，来自山东，今年25岁。\u0026#39;, \u0026#39;我叫李四，来自河北，今年28岁。\u0026#39;, \u0026#39;我叫王五，来自河南，今年24岁。\u0026#39;] for intro in intros: info = re.findall(pattern, intro) print(info) [('张三', '山东', '25')] [('李四', '河北', '28')] [('王五', '河南', '24')]  \n四、案例 4.1 找出文本中出现的年份 import re pattern = \u0026#39;\\d{4}\u0026#39; string = \u0026#34;\u0026#34;\u0026#34;Python是一门面向对象的编程语言，诞生于1991年。\\ 目前以广泛应用在网站开发、游戏软件开发、数据采集、机器学习等多个领域。\\ 一般情况下Python是Java的20%，所以说人生苦短，我用Python。\u0026#34;\u0026#34;\u0026#34; re.findall(pattern, string) ['1991']  4.2 re.split(pattern, string) 断句\npattern = \u0026#39;；|。\u0026#39; string = \u0026#34;\u0026#34;\u0026#34;Python是一门面向对象的编程语言，诞生于1991年；\\ 目前以广泛应用在网站开发、游戏软件开发、数据采集、机器学习等多个领域。\\ 一般情况下Python是Java的20%，所以说人生苦短，我用Python。\u0026#34;\u0026#34;\u0026#34; res = re.split(pattern, string) res = [r for r in res if r] res ['Python是一门面向对象的编程语言，诞生于1991年', '目前以广泛应用在网站开发、游戏软件开发、数据采集、机器学习等多个领域', '一般情况下Python是Java的20%，所以说人生苦短，我用Python']  4.3 re.sub(pattern, repl, string) 将数字替换为NUM\npattern = \u0026#39;\\d+\u0026#39; repl = \u0026#39;NUM\u0026#39; string = \u0026#34;\u0026#34;\u0026#34;Python是一门面向对象的编程语言，诞生于1991年。\\ 一般情况下Python是Java的20%，所以说人生苦短，我用Python。\u0026#34;\u0026#34;\u0026#34; re.sub(pattern, repl, string) 'Python是一门面向对象的编程语言，诞生于NUM年。一般情况下Python是Java的NUM%，所以说人生苦短，我用Python。'  4.4 . 统一表达 将指代同一个主体的不同表达词语统一为同一个词\ntext = \u0026#39;中国铁路工程集团有限公司成立于1950年3月，总部位于北京。目前中国中铁已经发展成中国和亚洲最大的多功能综合型建设集团。\u0026#39; pattern = \u0026#39;中国铁路工程集团有限公司|中国中铁\u0026#39; repl = \u0026#39;中铁\u0026#39; re.sub(pattern, repl, text) '中铁成立于1950年3月，总部位于北京。目前中铁已经发展成中国和亚洲最大的多功能综合型建设集团。'  text = \u0026#39;中国铁路工程集团有限公司成立于1950年3月，总部位于北京。目前中国中铁已经发展成中国和亚洲最大的多功能综合型建设集团。\u0026#39; pattern = \u0026#39;[中国铁路工程集团有限公司|中国中铁]+\u0026#39; repl = \u0026#39;中铁\u0026#39; re.sub(pattern, repl, text) '中铁成立于1950年3月，总部位于北京。目前中铁已经发展成中铁和亚洲最大的多功能综合型建设中铁。'  4.5 分割文本数据的章节 一二三四五六七八九十零百\ntext = \u0026#34;\u0026#34;\u0026#34; 第一篇 Python简介 第二篇 Python入门语法 第三篇 Python网络爬虫 第四篇 文本数据编码 第五篇 数据分析 第六篇 可视化\u0026#34;\u0026#34;\u0026#34; pattern = \u0026#39;第[一二三四五六七八九十零百]+篇\u0026#39; res = re.split(pattern, text) res = [r.replace(\u0026#39; \u0026#39;, \u0026#39;\u0026#39;) for r in res if \u0026#39; \u0026#39;!=r] res ['Python简介', 'Python入门语法', 'Python网络爬虫', '文本数据编码', '数据分析', '可视化']  4.6 抽取出数字 比如日期数据\ntext = \u0026#39;中国铁路工程集团有限公司成立于1950年3月，总部位于北京。目前中国中铁已经发展成中国和亚洲最大的多功能综合型建设集团。\u0026#39; pattern = \u0026#39;\\d+\u0026#39; \u0026#39;-\u0026#39;.join(re.findall(pattern, text)) '1950-3'  \n","permalink":"/blog/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8Fre%E5%BA%93/","summary":"正则表达式主要用于数据清洗，比如从脏乱差的文本中抽取出自己需要的信息。常见于爬虫和文本分析。\n一、正则表达式中的符号 按照符号的功能，我将其分为三类，一般情况下表达式都是由这三种符号组成的。\n1.1 正则字符 预警，听不懂看不懂，都不要紧的。不要绞尽脑汁，本节后面会柳暗花明的。\n   正则符号 描述 匹配自己时     \\ 转义字符。例如， \u0026lsquo;n\u0026rsquo; 匹配字符 \u0026lsquo;n\u0026rsquo;。 '\\n'   ( ) 标记一个子表达式的开始和结束位置。 \\( \\)   . 匹配除换行符 \\n 之外的任何单字符。 \\.    | |左右两侧均可参与匹配    \\d 匹配字符串中的单个数字    a-zA-Z 匹配全部英文字符    0-9 匹配全部数字    \\s 匹配字符串中的\\n,\\t,空格    [] 中括号内任意正则符号均可参与匹配 \\[ \\]   ^ 当在方括号表达式中使用，^对其后的正则表达式进行了反义表达。 \\^    1.","title":"内置库-正则表达式re库"},{"content":"在编程中一般不适用excel，而是用一种很像excel的csv来存储数据。而且Excel软件可以打开csv的。\n一、csv存储数据代码步骤 **说明:**代码看不懂没关系，能背过最好。背不过也没关系，能理解代码功能，而且亲自上手调试过，调试正常无误的代码可以加入你的代码笔记本中，然后以后需要的时候复制粘贴修改参数即可\n1. 1 新建一个csv文件 import csv path = \u0026#39;data/test.csv\u0026#39; csvf = open(path, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) 1.2 定义字段名，并初始化csv文件为writer fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() 1.3 将待存储数据整理为字典格式 test_data = {\u0026#39;name\u0026#39;: \u0026#39;David\u0026#39;, \u0026#39;age\u0026#39;: 25} 1.4 用writer往csv中存储数据 writer.writerow(test_data) 1.5 最后记得关闭csv文件 csvf.close() \nimport csv csvf = open(\u0026#39;data/test1.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() csvf.close() \nimport csv csvf = open(\u0026#39;data/test2.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() test_data = {\u0026#39;name\u0026#39;: \u0026#39;David\u0026#39;, \u0026#39;age\u0026#39;: 25} writer.writerow(test_data) csvf.close() \n二、很多数据的存储 如果很多数据存储时，就可以用之前学到的for循环。\ndatas = [{\u0026#39;name\u0026#39;: \u0026#39;David\u0026#39;, \u0026#39;age\u0026#39;: 25}, {\u0026#39;name\u0026#39;: \u0026#39;Mary\u0026#39;, \u0026#39;age\u0026#39;: 30}, {\u0026#39;name\u0026#39;: \u0026#39;Henry\u0026#39;, \u0026#39;age\u0026#39;: 35}] datas [{'name': 'David', 'age': 25}, {'name': 'Mary', 'age': 30}, {'name': 'Henry', 'age': 35}]  import csv csvf = open(\u0026#39;data/test2.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() for data in datas: writer.writerow(data) csvf.close() \n","permalink":"/blog/csv%E6%96%87%E4%BB%B6%E5%BA%93/","summary":"在编程中一般不适用excel，而是用一种很像excel的csv来存储数据。而且Excel软件可以打开csv的。\n一、csv存储数据代码步骤 **说明:**代码看不懂没关系，能背过最好。背不过也没关系，能理解代码功能，而且亲自上手调试过，调试正常无误的代码可以加入你的代码笔记本中，然后以后需要的时候复制粘贴修改参数即可\n1. 1 新建一个csv文件 import csv path = \u0026#39;data/test.csv\u0026#39; csvf = open(path, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) 1.2 定义字段名，并初始化csv文件为writer fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() 1.3 将待存储数据整理为字典格式 test_data = {\u0026#39;name\u0026#39;: \u0026#39;David\u0026#39;, \u0026#39;age\u0026#39;: 25} 1.4 用writer往csv中存储数据 writer.writerow(test_data) 1.5 最后记得关闭csv文件 csvf.close() \nimport csv csvf = open(\u0026#39;data/test1.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.DictWriter(csvf, fieldnames=fieldnames) writer.writeheader() csvf.close() \nimport csv csvf = open(\u0026#39;data/test2.csv\u0026#39;, \u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, newline=\u0026#39;\u0026#39;) fieldnames = [\u0026#39;name\u0026#39;, \u0026#39;age\u0026#39;] writer = csv.","title":"内置库-数据存储csv库"},{"content":"路径是可以让程序知道待操作的文件在哪里，python中有os和pathlib两个内置的路径库，我们就讲这个名字一看就懂的路径库pathlib。\n\n绝对vs相对  相对路径 'img' 绝对路径 'C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/img'  **注意：**当移动文件夹位置或者将代码分享给朋友使用时，你的代码再次运行就会出错。为了避免这个问题，强烈建议用相对路径\nimport pathlib #当前代码所在的文件夹的相对路径 pathlib.Path() WindowsPath('.')  from pathlib import Path #当前代码所在的文件夹的相对路径 Path() WindowsPath('.')  \npathlib.Path()属性方法 **提醒：**下表加粗的都是常用的方法，其他了解即可\n   方法 功能     cwd() 获取代码所在的当前工作路径   joinpath(\u0026hellip;grandpadir, fatherdir, \u0026hellip;file) 生成路径   iterdir() 返回某路径下的文件(夹)目录   glob(pattern) 返回符合pattern的所有文件的文件路径   is_file() 判断某路径是否为文件，返回布尔值   is_dir() 判断某路径是否为文件夹，返回布尔值   exists() 判断某路径是否存在，返回布尔值   mkdir(parents=True, exist_ok=True) 创建某路径对应的文件夹    \ncwd() 例：获取当前代码所在文件夹的绝对路径\nPath().cwd() WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门')  \njoinpath(\u0026hellip;grandpadir, fatherdir, \u0026hellip;file) 把\u0026hellip;grandpadir, fatherdir, \u0026hellip;file加入到某路径中\n例：获得data文件夹的路径\nPath().cwd().joinpath(\u0026#39;data\u0026#39;) WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data')  例：获得data/test.txt文件的路径\nPath().cwd().joinpath(\u0026#39;data\u0026#39;, \u0026#39;test.txt\u0026#39;) WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test.txt')  \niterdir() 返回某路径下的文件(夹)目录\n例：获得02-Python语法入门文件夹里的所有文件(夹)路径\nlist(Path().cwd().iterdir()) [WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/.ipynb_checkpoints'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/01-Python跟英语一样是一门语言.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/02-数据类型之字符串.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/03-数据类型之列表元组集合.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/04-数据类型之字典.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/05-数据类型之布尔值\u0026amp;None.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/06-逻辑语句(if\u0026amp;for\u0026amp;tryexcept).ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/07-列表推导式.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/08-理解函数.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/09-常用内置函数.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/09-常用函数.md'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/10-内置库之文件路径pathlib库.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/11-内置库之csv文件库.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/12. 内置库之正则表达式re库.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/13-初学python常出错误汇总.ipynb'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/img')]  \nglob(pattern) 查找某路径内满足pattern的所有文件路径 。\npattern='*.*' 匹配任意格式任意名字的文件\npattern='*.txt' 匹配出所有的txt文件\n例：获得data文件夹内的所有的文件路径\nlist(Path().cwd().joinpath(\u0026#39;data\u0026#39;).glob(\u0026#39;*.*\u0026#39;)) [WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test.txt'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test2.csv'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test2.txt'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/twitter_sentiment.csv')]  例：获得data文件夹内的所有的txt额路径\nlist(Path().cwd().joinpath(\u0026#39;data\u0026#39;).glob(\u0026#39;*.txt\u0026#39;)) [WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test.txt'), WindowsPath('C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test2.txt')]  例：获得data/reports内的pdf路径\ndirs = Path().cwd().joinpath(\u0026#39;data\u0026#39;, \u0026#39;reports\u0026#39;).iterdir() dirs = list(dirs) for dir in dirs: files = dir.glob(\u0026#39;*.*\u0026#39;) for file in files: print(file) C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600000\\600000_20010901_1.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600004\\600004_2006_n.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600004\\600004_2006_z.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600007\\600007_2001_n.pdf ....... C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600007\\600007_2002_1.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\603937\\603937_2018_z.pdf ...... C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\603937\\603937_2019_3.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\603937\\603937_2019_z.pdf  例**：获得data/reports内的 含有\u0026quot;_n\u0026quot; 额pdf路径\ndirs = Path().cwd().joinpath(\u0026#39;data\u0026#39;, \u0026#39;reports\u0026#39;).iterdir() dirs = list(dirs) for dir in dirs: files = dir.glob(\u0026#39;*_n.pdf\u0026#39;) for file in files: print(file) C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600000\\600000_2006_n.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\600000\\600000_2008_n.pdf ........ C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\601872\\601872_2014_n.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\601872\\601872_2015_n.pdf C:\\Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\\reports\\601872\\601872_2016_n.pdf  \nis_file() 判断某路径是否为一个文件。返回布尔值：\n True 真实存在的文件路径 False 不真实存在或者文件夹路径  例 \u0026lsquo;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test.txt\u0026rsquo;是文件路径？\nfpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test.txt\u0026#39;) fpath.is_file() True  例 \u0026lsquo;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test222.txt\u0026rsquo;是文件路径？\nfpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data/test222.txt\u0026#39;) fpath.is_file() False  fpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data\u0026#39;) fpath.is_file() False  \nis_dir() 判断某路径是否为一个文件夹。返回布尔值，True、False\n例： \u0026lsquo;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data\u0026rsquo; 是 文件夹路径？\nfpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data\u0026#39;) fpath.is_dir() True  \nexists() 判断某路径是否存在。返回布尔值，True、False\n例： \u0026lsquo;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data\u0026rsquo; 是否存在？\nfpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/data\u0026#39;) fpath.exists() True  fpath = Path(\u0026#39;C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/datasss\u0026#39;) fpath.exists() False  mkdir(parents=True, exist_ok=True) 创建某路径\npath = Path().cwd().joinpath(\u0026#39;data\u0026#39;, \u0026#39;stocks\u0026#39;, \u0026#39;800000\u0026#39;) path.mkdir(parents=True, exist_ok=True) \n","permalink":"/blog/%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84pathlib%E5%BA%93/","summary":"路径是可以让程序知道待操作的文件在哪里，python中有os和pathlib两个内置的路径库，我们就讲这个名字一看就懂的路径库pathlib。\n\n绝对vs相对  相对路径 'img' 绝对路径 'C:/Users/thunderhit/Desktop/Python数据分析入门/02-Python语法入门/img'  **注意：**当移动文件夹位置或者将代码分享给朋友使用时，你的代码再次运行就会出错。为了避免这个问题，强烈建议用相对路径\nimport pathlib #当前代码所在的文件夹的相对路径 pathlib.Path() WindowsPath('.')  from pathlib import Path #当前代码所在的文件夹的相对路径 Path() WindowsPath('.')  \npathlib.Path()属性方法 **提醒：**下表加粗的都是常用的方法，其他了解即可\n   方法 功能     cwd() 获取代码所在的当前工作路径   joinpath(\u0026hellip;grandpadir, fatherdir, \u0026hellip;file) 生成路径   iterdir() 返回某路径下的文件(夹)目录   glob(pattern) 返回符合pattern的所有文件的文件路径   is_file() 判断某路径是否为文件，返回布尔值   is_dir() 判断某路径是否为文件夹，返回布尔值   exists() 判断某路径是否存在，返回布尔值   mkdir(parents=True, exist_ok=True) 创建某路径对应的文件夹","title":"内置库-文件路径pathlib库"},{"content":"有三大类内置函数\n 数学相关函数 类型转化函数 功能函数  函数名加粗的是都是重点\n\n数学相关    函数 功能 例子 运行结果     abs(a) 对a取绝对值 abs(-1) 1   max(lst)、min(lst) 寻找lst中的最大、最小值 max([3, 2, 9]) 9   sum(lst) 对lst内所有数字求和 sum([3, 2, 9]) 14   sorted(lst， reverse) 对lst排序； 参数reverse为布尔值控制升降序 sorted([3, 2, 9]) [2, 3, 9]   range(start, end, step) 生成以步长step，生成从start到end的数列,默认step=1，结果取不到end list(range(1,5)) [1, 2, 3,4]    #取绝对值 abs(-1) 1  #取最大 max([3, 2, 9]) 9  #取最小 min([3, 2, 9]) 2  #求和 sum([3, 2, 9]) 14  #排序 sorted([3,2,9]) [2, 3, 9]  #排序(方向调整) sorted([3,2,9], reverse=True) [9, 3, 2]  #生成序列 list(range(1, 10)) [1, 2, 3, 4, 5, 6, 7, 8, 9]  list(range(1, 10, 2)) [1, 3, 5, 7, 9]  \n类型转换    函数 功能 例子 运行结果     int(string) 将字符串数改为整数型 int(\u0026lsquo;9\u0026rsquo;) 9   float(int/str) 将int或str改为浮点型 float(9)、float(\u0026lsquo;9\u0026rsquo;) 9.0   list(iterable) 将可迭代对象为列表。这里的iterable可以为字符串，可以是列表 list(range(1,5)) [1,2,3,4]    enumerate(lst) 返回带有索引值的序列seq,需要list(seq)处理后才能看到seq list(enumerate([\u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;, \u0026lsquo;c\u0026rsquo;])) [(0,\u0026lsquo;a\u0026rsquo;), (1, \u0026lsquo;b\u0026rsquo;), (2, \u0026lsquo;c\u0026rsquo;)]   tuple(lst) 将lst变为tuple tuple([1,2,3]) (1,2,3)   set(lst) 将lst变为集合 set([1,4,4,4,3]) {1,3,4}    a = 9 b = 9 a+b 18  #变转化为整数 int(\u0026#39;9\u0026#39;) 9  #转化为小数 float(\u0026#39;9\u0026#39;) 9.0  float(9) 9.0  #转化为列表 list(range(1, 5)) [1, 2, 3, 4]  #给列表中每个元素分配一个索引值 names = [\u0026#39;张三\u0026#39;, \u0026#39;李四\u0026#39;, \u0026#39;王五\u0026#39;] list(enumerate(names)) [(0, '张三'), (1, '李四'), (2, '王五')]  \n功能函数    函数 功能 例子 运行结果     eval(expression) 执行一个字符串表达式 eval(\u0026lsquo;1+1\u0026rsquo;) 2   zip(lst1,lst2\u0026hellip;) 将lst1,lst2\u0026hellip;合并,返回zip对象。需要list处理一下zip对象 list(zip([1,2,3],[4,5,6])) [(1, 4), (2, 5), (3, 6)]   type(x) 查看X的类型 type(\u0026lsquo;2\u0026rsquo;) \u0026lt;class \u0026lsquo;str\u0026rsquo;\u0026gt;   help(x) 查看X的相关信息 help([1, 2]) Help on list object..   map(func, lst) 对lst中的每一个个体都进行func操作 list(map(sum, [[1,1], [1,2]])) [2, 3]   print(value, end='\\n') 打印value print(\u0026lsquo;abc\u0026rsquo;) abc   open(file， encoding) 打开file文件， encoding是file的文件编码      \neval() eval(str_expression)\nstr_expression 是字符串表达式，可以是变量、函数等\na = 9 b = 9 c = \u0026#39;a+b\u0026#39; print(a+b) print(c) print(eval(c)) 18 a+b 18  eval(\u0026#39;a+b\u0026#39;) 18  d = \u0026#39;hello world\u0026#39; print(\u0026#39;d\u0026#39;) print(eval(\u0026#39;d\u0026#39;)) d hello world  def hello(): print(\u0026#39;hello python\u0026#39;) print(\u0026#39;hello()\u0026#39;) hello()  eval(\u0026#39;hello()\u0026#39;) hello python  \nzip(lst1, lst2,lst3\u0026hellip;) 将lst1， lst2， lst3按照顺序进行合并\nnames = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Unique\u0026#39;] sexs = [\u0026#39;male\u0026#39;, \u0026#39;femal\u0026#39;, \u0026#39;male\u0026#39;, \u0026#39;male\u0026#39;] ages = [25, 22, 30, 40] list(zip(names, sexs, ages)) [('David', 'male', 25), ('Mary', 'femal', 22), ('Henry', 'male', 30), ('Unique', 'male', 40)]  \ntype/help 查看数据类型、查看感兴趣对象的介绍\na = [1,3,5] type(a) list  help(a) Help on list object: class list(object) | list(iterable=(), /) | | Built-in mutable sequence. | | If no argument is given, the constructor creates a new empty list. | The argument must be an iterable if specified. | | Methods defined here: | ......... | append(self, object, /) | Append object to the end of the list. | | | count(self, value, /) | Return number of occurrences of value. | | extend(self, iterable, /) | Extend list by appending elements from the iterable. |  type(print) builtin_function_or_method  help(print) Help on built-in function print in module builtins: print(...) print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream.  map(func, lst)映射运算 将func运算映射到lst上每个元素\nlst = [[1,1], [1,2], [1,2], [1,2], [1,2], [1,2], [1,2]] res = map(sum, lst) list(res) [2, 3, 3, 3, 3, 3, 3]  \nprint(value, end='\\n') 打印value，默认使用换行结束\nhelp(print) Help on built-in function print in module builtins: print(...) print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream.  print(\u0026#39;hello world!\u0026#39;) print(\u0026#39;hello python!\u0026#39;) hello world! hello python!  print(\u0026#39;hello world!\u0026#39;, end=\u0026#39;\\t\u0026#39;) print(\u0026#39;hello python!\u0026#39;) hello world!\thello python!  \nopen(file, mode=\u0026lsquo;r\u0026rsquo;, encoding=None)  file 文件路径 mode 操作方式们，最常用的是r和a+。r读取， a+是追加写入 encoding 编码方式 ，常见的文件编码方式主要是utf-8和gbk  读取返回io对象\nio对象有read()方法\n 相对路径\ndata\n  绝对路径\nC:Users\\thunderhit\\Desktop\\Python数据分析入门\\02-Python语法入门\\data\n 建议大家都要用相对路径\n# 读取数据 open(\u0026#39;data/test.txt\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;).read() '章节设计\\n\\n第一部分 环境配置\\n第二部分 快速入门python\\n第三部分 网络爬虫\\n第四部分 简单的文本分析\\n第五部分 进阶文本分析'  # 新建文件/在已有的文件内插入内容 f = open(\u0026#39;data/test2.txt\u0026#39;, mode=\u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) f.write(\u0026#39;我在学python，现在是下午五点\u0026#39;) f.close() \n# 新建文件/在已有的文件内插入内容 f = open(\u0026#39;data/test2.txt\u0026#39;, mode=\u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) f.write(\u0026#39;\\nLife is short, so to learn Python\u0026#39;) f.close() \n# 新建文件/在已有的文件内插入内容 f = open(\u0026#39;data/test2.txt\u0026#39;, mode=\u0026#39;a+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) f.write(\u0026#39;\\nLife is short, so to learn Python\u0026#39;) f.write(\u0026#39;\\nLife is short, so to learn music\u0026#39;) f.write(\u0026#39;\\nLife is short, so to learn english\u0026#39;) f.close() \n重点函数  sorted(lst， ascending) range(start, end, step) enumerate(lst) eval(expression) zip(lst1, lst2..) map(func, lst) print(x) open(file, mode, encoding)  ","permalink":"/blog/python%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/","summary":"有三大类内置函数\n 数学相关函数 类型转化函数 功能函数  函数名加粗的是都是重点\n\n数学相关    函数 功能 例子 运行结果     abs(a) 对a取绝对值 abs(-1) 1   max(lst)、min(lst) 寻找lst中的最大、最小值 max([3, 2, 9]) 9   sum(lst) 对lst内所有数字求和 sum([3, 2, 9]) 14   sorted(lst， reverse) 对lst排序； 参数reverse为布尔值控制升降序 sorted([3, 2, 9]) [2, 3, 9]   range(start, end, step) 生成以步长step，生成从start到end的数列,默认step=1，结果取不到end list(range(1,5)) [1, 2, 3,4]    #取绝对值 abs(-1) 1  #取最大 max([3, 2, 9]) 9  #取最小 min([3, 2, 9]) 2  #求和 sum([3, 2, 9]) 14  #排序 sorted([3,2,9]) [2, 3, 9]  #排序(方向调整) sorted([3,2,9], reverse=True) [9, 3, 2]  #生成序列 list(range(1, 10)) [1, 2, 3, 4, 5, 6, 7, 8, 9]  list(range(1, 10, 2)) [1, 3, 5, 7, 9]","title":"Python内置常用函数"},{"content":"流水线每个环节都有质量要求，只有符合质量规范，才能流通到下一个环节。这样环环相扣，设计好后经过试运营就可以投产了。Python编程也一样，编程序其实也是设计流水线，而函数就是流水线上的一个个的环节。\n\n函数 可重复使用的代码块\n  def函数常见参数定义\u0026amp;调用方式\n 位置参数 关键词参数 默认参数  \n位置参数 def hello1(name, age): return \u0026#39;我是{0},今年{1}\u0026#39;.format(name, age) hello1(\u0026#39;张三\u0026#39;, 25) '我是张三,今年25'  hello1(25,\u0026#39;张三\u0026#39;) '我是25,今年张三'  \n关键词参数 def hello2(name, age): return \u0026#39;我是{0},今年{1}\u0026#39;.format(name, age) hello2(name=\u0026#39;张三\u0026#39;, age=25) '我是张三,今年25'  hello2(age=25, name=\u0026#39;张三\u0026#39;) '我是张三,今年25'  \n默认参数 def hello3(name, age, gender=\u0026#39;男\u0026#39;): return \u0026#39;我是{0},今年{1}, 性别{2}\u0026#39;.format(name, age, gender) hello3(\u0026#39;David\u0026#39;, 25) '我是David,今年25, 性别男'  hello3(\u0026#39;David\u0026#39;, 25, gender=\u0026#39;male\u0026#39;) '我是David,今年25, 性别male'  \n","permalink":"/blog/%E7%90%86%E8%A7%A3%E5%87%BD%E6%95%B0/","summary":"流水线每个环节都有质量要求，只有符合质量规范，才能流通到下一个环节。这样环环相扣，设计好后经过试运营就可以投产了。Python编程也一样，编程序其实也是设计流水线，而函数就是流水线上的一个个的环节。\n\n函数 可重复使用的代码块\n  def函数常见参数定义\u0026amp;调用方式\n 位置参数 关键词参数 默认参数  \n位置参数 def hello1(name, age): return \u0026#39;我是{0},今年{1}\u0026#39;.format(name, age) hello1(\u0026#39;张三\u0026#39;, 25) '我是张三,今年25'  hello1(25,\u0026#39;张三\u0026#39;) '我是25,今年张三'  \n关键词参数 def hello2(name, age): return \u0026#39;我是{0},今年{1}\u0026#39;.format(name, age) hello2(name=\u0026#39;张三\u0026#39;, age=25) '我是张三,今年25'  hello2(age=25, name=\u0026#39;张三\u0026#39;) '我是张三,今年25'  \n默认参数 def hello3(name, age, gender=\u0026#39;男\u0026#39;): return \u0026#39;我是{0},今年{1}, 性别{2}\u0026#39;.format(name, age, gender) hello3(\u0026#39;David\u0026#39;, 25) '我是David,今年25, 性别男'  hello3(\u0026#39;David\u0026#39;, 25, gender=\u0026#39;male\u0026#39;) '我是David,今年25, 性别male'","title":"高级语法-理解函数"},{"content":"列表推导式唯一的用处就是增强代码的可阅读性，初次接触可能比较难理解，但是大家一定要理解，文本分析中经常会用到ta。\n\n问题1 用列表表示集合X $X= {x| x \\in [1,2,3,4,5,6,7,8,9,10]}$\nX = [1,2,3,4,5,6,7,8,9,10] X [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  X = list(range(1, 10)) X [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n问题2 表示集合Y $Y = {x^2| x \\in [1,2,3,4,5,6,7,8,9,10]}$\nY = [] for x in X: Y.append(x*x) Y [1, 4, 9, 16, 25, 36, 49, 64, 81]  \n列表推导式 实现步骤\n 先复制X 再对X中的元素x进行操作  #先复制X [x for x in X] [1, 2, 3, 4, 5, 6, 7, 8, 9]  #再对x进行操作 [x*x for x in X] [1, 4, 9, 16, 25, 36, 49, 64, 81]  理解列表推导式   带条件的列表推导式 $C= {x^2 | (x \\in X) \\cap (x\u0026gt;5)}$\n#复制X [x for x in X] [1, 2, 3, 4, 5, 6, 7, 8, 9]  #X中的要大于5 #[x for x in X if x\u0026gt;5] [x for x in X if x\u0026gt;5] [6, 7, 8, 9]  #对满足条件的x进行操作 #[x*x for x in X if x\u0026gt;5] [x*x for x in X if x\u0026gt;5] [36, 49, 64, 81]  \n问题3 全部小写 words = [\u0026#39;Life\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;short\u0026#39;, \u0026#39;so\u0026#39;, \u0026#39;we\u0026#39;, \u0026#39;use\u0026#39;, \u0026#39;Python\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;easy\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;learn\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;easy\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;use\u0026#39;] words ['Life', 'is', 'short', 'so', 'we', 'use', 'Python', 'python', 'is', 'easy', 'to', 'learn', 'and', 'easy', 'to', 'use']  lower_words = [w.lower() for w in words] lower_words ['life', 'is', 'short', 'so', 'we', 'use', 'python', 'python', 'is', 'easy', 'to', 'learn', 'and', 'easy', 'to', 'use']  \n问题4 计算lower_words列表数据的单词词频 wordset = set(lower_words) [(w, lower_words.count(w)) for w in wordset] [('so', 1), ('and', 1), ('learn', 1), ('use', 2), ('to', 2), ('we', 1), ('easy', 2), ('python', 2), ('is', 2), ('short', 1), ('life', 1)]  #1 生产词语集合 wordset = set(lower_words) print(wordset) {'so', 'and', 'learn', 'use', 'to', 'we', 'easy', 'python', 'is', 'short', 'life'}  #2. wordset复制wordset自己 [w for w in wordset] #3. 对wordset中每个词语w进行一些操作 [lower_words.count(w) for w in wordset] [1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1]  #3. 词频和词语一起显示 [(w,lower_words.count(w)) for w in wordset] [('so', 1), ('and', 1), ('learn', 1), ('use', 2), ('to', 2), ('we', 1), ('easy', 2), ('python', 2), ('is', 2), ('short', 1), ('life', 1)]  \n问题5 d = {\u0026#39;x\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;y\u0026#39;: \u0026#39;B\u0026#39;, \u0026#39;z\u0026#39;: \u0026#39;C\u0026#39; } 如何将d中的 键值对 拼接,输出为\n['xA', 'yB', 'zC]\nd = {\u0026#39;x\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;y\u0026#39;: \u0026#39;B\u0026#39;, \u0026#39;z\u0026#39;: \u0026#39;C\u0026#39; } d.items() dict_items([('x', 'A'), ('y', 'B'), ('z', 'C')])  #1 自己复制d.items()自己 [i for i in d.items()] [('x', 'A'), ('y', 'B'), ('z', 'C')]  #2 对任何一个元素都要进行字符串的拼接操作 [i[0]+i[1] for i in d.items()] ['xA', 'yB', 'zC']  \n","permalink":"/blog/%E5%88%97%E8%A1%A8%E6%8E%A8%E5%AF%BC%E5%BC%8F/","summary":"列表推导式唯一的用处就是增强代码的可阅读性，初次接触可能比较难理解，但是大家一定要理解，文本分析中经常会用到ta。\n\n问题1 用列表表示集合X $X= {x| x \\in [1,2,3,4,5,6,7,8,9,10]}$\nX = [1,2,3,4,5,6,7,8,9,10] X [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  X = list(range(1, 10)) X [1, 2, 3, 4, 5, 6, 7, 8, 9]  \n问题2 表示集合Y $Y = {x^2| x \\in [1,2,3,4,5,6,7,8,9,10]}$\nY = [] for x in X: Y.append(x*x) Y [1, 4, 9, 16, 25, 36, 49, 64, 81]  \n列表推导式 实现步骤","title":"高级语法-列表推导式"},{"content":"\nif条件判断   condition为布尔值、布尔运算、成员运算符 通常我们理解的condition为布尔值\n#Tab condition = True if condition: print(\u0026#39;condition为True\u0026#39;) else: print(\u0026#39;condition为False\u0026#39;) condition为True  age = 17 if age\u0026gt;=18: print(\u0026#39;你是成年人了\u0026#39;) else: print(\u0026#39;你还是个孩子\u0026#39;) 你还是个孩子  age = 20 if age\u0026gt;=55: print(\u0026#39;老年人\u0026#39;) elif 35\u0026lt;=age\u0026lt;55: print(\u0026#39;中年\u0026#39;) elif 18\u0026lt;=age\u0026lt;35: print(\u0026#39;青年\u0026#39;) elif 0\u0026lt;=age\u0026lt;18: print(\u0026#39;儿童\u0026#39;) 青年  \n特殊的conditon  各种空值(空字符串、空列表等)作用等同于False 各种非空值，作用等同于True  a = None if a: print(\u0026#39;a是非空数据\u0026#39;) else: print(\u0026#39;a是空数据\u0026#39;) a是空数据  \nfor循环  重复做某件事 迭代出数据中的内容(元素)    上面这个图可以解读为 我们想对iterable这个集合中的每一个item:\n做点事(对item做操作)  重复做某事 问题1 计算1+2+3+\u0026hellip;+97+98+99+100=?\n1 + 2 = 3 3 + 3 = 6 6 + 4 = 10 10 + 5 = 15\nresult = 0 #int for i in range(1, 101): result = result + i print(result) 5050  迭代出数据中的内容 从某种“集合”（这个“集合”可以使list、set、tuple等），只要“集合”内部有多个成员就可以使用for循环迭代出内部的成员\nnames = [\u0026#39;David\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Mary\u0026#39;] for name in names: print(name) David Henry Mary  name = \u0026#39;David\u0026#39; for s in name: print(s) D a v i d  infos = {\u0026#39;David\u0026#39;:{\u0026#39;age\u0026#39;:25, \u0026#39;gender\u0026#39;:\u0026#39;Male\u0026#39;}, \u0026#39;Mary\u0026#39;:{\u0026#39;age\u0026#39;:23, \u0026#39;gender\u0026#39;: \u0026#39;Female\u0026#39;}, \u0026#39;Henry\u0026#39;:{\u0026#39;age\u0026#39;:23, \u0026#39;gender\u0026#39;: \u0026#39;Male\u0026#39;} } for item in infos.items(): print(item) ('David', {'age': 25, 'gender': 'Male'}) ('Mary', {'age': 23, 'gender': 'Female'}) ('Henry', {'age': 23, 'gender': 'Male'})  for name, info in infos.items(): print(name, info) David Mary Henry  \ntry-except 遇到无关紧要的bug，不会停下来，让程序有一定的容错能力。通俗点就是此处不留爷，自有留爷处，凡事别钻牛角尖。\nfor x in [1,2,0,2,1]: print(10/x) 10.0 5.0 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) \u0026lt;ipython-input-19-83bea9c92c0e\u0026gt; in \u0026lt;module\u0026gt; 1 for x in [1,2,0,2,1]: ----\u0026gt; 2 print(10/x) ZeroDivisionError: division by zero  for x in [1,2,0,2,1]: try: print(10/x) except: print(\u0026#39;0除错误\u0026#39;) #pass 10.0 5.0 0除错误 5.0 10.0  \n练习1 假设现在某人的能力为1， 每天比前一天进步0.01， 一年后他的能力是多少？\nability = 1 scale = 1.01 records = [] for i in range(365): ability = ability * 1.01 records.append(ability) print(records) [1.01, 1.0201, 1.030301, 1.04060401, 1.0510100501, 1.061520150601, 1.0721353521070098, 1.08285670562808, 1.0936852726843609, 1.1046221254112045, 1.1156683466653166, ...................36.30913774096189, 36.672229118371504, 37.03895140955522, 37.40934092365077, 37.783434332887275]  import matplotlib.pyplot as plt import math %matplotlib inline ability = 1 scale = 1.02 records = [] days = range(1, 365) for day in days: ability = ability*scale records.append(ability) plt.plot(days, records) plt.title(\u0026#39;Be better everyday!\u0026#39;) Text(0.5, 1.0, 'Be better everyday!')    \n安装包的方法  命令行执行 pip install packagename jupyter notebook的Cell中执行!pip install packagename 如果是mac，pip写成pip3  !pip install matplotlib Looking in indexes: https://mirrors.aliyun.com/pypi/simple/ Requirement already satisfied: matplotlib in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (3.2.1) Requirement already satisfied: kiwisolver\u0026gt;=1.0.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (1.2.0) Requirement already satisfied: python-dateutil\u0026gt;=2.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (2.8.1) Requirement already satisfied: cycler\u0026gt;=0.10 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (0.10.0) Requirement already satisfied: numpy\u0026gt;=1.11 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (1.18.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u0026gt;=2.0.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (2.4.7) Requirement already satisfied: six\u0026gt;=1.5 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from python-dateutil\u0026gt;=2.1-\u0026gt;matplotlib) (1.14.0)  练习2 打印九九乘法表格\n  用到的知识点:\n for循环 字符串format方法 print函数(涉及到end参数)  for row in range(1, 10): #print(row) for col in range(1, row+1): formula = \u0026#39;{col}*{row}={res}\u0026#39;.format(col=col, row=row, res=col*row) print(formula, end=\u0026#39;\\t\u0026#39;) print(\u0026#39;\u0026#39;) 1*1=1\t1*2=2\t2*2=4\t1*3=3\t2*3=6\t3*3=9\t1*4=4\t2*4=8\t3*4=12\t4*4=16\t1*5=5\t2*5=10\t3*5=15\t4*5=20\t5*5=25\t1*6=6\t2*6=12\t3*6=18\t4*6=24\t5*6=30\t6*6=36\t1*7=7\t2*7=14\t3*7=21\t4*7=28\t5*7=35\t6*7=42\t7*7=49\t1*8=8\t2*8=16\t3*8=24\t4*8=32\t5*8=40\t6*8=48\t7*8=56\t8*8=64\t1*9=9\t2*9=18\t3*9=27\t4*9=36\t5*9=45\t6*9=54\t7*9=63\t8*9=72\t9*9=81\t help(print) Help on built-in function print in module builtins: print(...) print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream.  for row in range(1, 10): for col in range(1, row+1): formula = \u0026#39;{col}*{row}={res}\u0026#39; content = formula.format(col=col, row=row, res=col*row) print(content, end=\u0026#39;\\t\u0026#39;) print() 1*1=1\t1*2=2\t2*2=4\t1*3=3\t2*3=6\t3*3=9\t1*4=4\t2*4=8\t3*4=12\t4*4=16\t1*5=5\t2*5=10\t3*5=15\t4*5=20\t5*5=25\t1*6=6\t2*6=12\t3*6=18\t4*6=24\t5*6=30\t6*6=36\t1*7=7\t2*7=14\t3*7=21\t4*7=28\t5*7=35\t6*7=42\t7*7=49\t1*8=8\t2*8=16\t3*8=24\t4*8=32\t5*8=40\t6*8=48\t7*8=56\t8*8=64\t1*9=9\t2*9=18\t3*9=27\t4*9=36\t5*9=45\t6*9=54\t7*9=63\t8*9=72\t9*9=81\t \n","permalink":"/blog/%E9%80%BB%E8%BE%91%E8%AF%AD%E5%8F%A5iffortryexcept/","summary":"if条件判断   condition为布尔值、布尔运算、成员运算符 通常我们理解的condition为布尔值\n#Tab condition = True if condition: print(\u0026#39;condition为True\u0026#39;) else: print(\u0026#39;condition为False\u0026#39;) condition为True  age = 17 if age\u0026gt;=18: print(\u0026#39;你是成年人了\u0026#39;) else: print(\u0026#39;你还是个孩子\u0026#39;) 你还是个孩子  age = 20 if age\u0026gt;=55: print(\u0026#39;老年人\u0026#39;) elif 35\u0026lt;=age\u0026lt;55: print(\u0026#39;中年\u0026#39;) elif 18\u0026lt;=age\u0026lt;35: print(\u0026#39;青年\u0026#39;) elif 0\u0026lt;=age\u0026lt;18: print(\u0026#39;儿童\u0026#39;) 青年  \n特殊的conditon  各种空值(空字符串、空列表等)作用等同于False 各种非空值，作用等同于True  a = None if a: print(\u0026#39;a是非空数据\u0026#39;) else: print(\u0026#39;a是空数据\u0026#39;) a是空数据  \nfor循环  重复做某件事 迭代出数据中的内容(元素)    上面这个图可以解读为 我们想对iterable这个集合中的每一个item:","title":"程序语句-逻辑语句(if\u0026for\u0026tryexcept)"},{"content":"\n布尔值Boolean 用于逻辑判断，一般与if结合使用。\na = True print(a) True  True True  False False  其他产生布尔值的方式\n 布尔运算 比较运算 成员运算  布尔运算 中学数学课里的且或非\n   运算符号 功能 例子 等于     x and y 且 True and False False   x or y 或 True or False True   not x 非 not True False    x = True and False print(x) False  x = True or False print(x) True  x = not True print(x) False  x = not False print(x) True  比较运算 注意： =和==的区别，=用来把某个值传给某个变量(赋值操作)，==用来判断两个值(变量)是否相等(判断操作)\n   比较运算符号 功能 例子 等于     == 相等 5==3 False   != 不等于 5!=3 True   \u0026gt; 大于 5\u0026gt;3 True   \u0026lt; 小于 5\u0026lt;3 False   \u0026lt;= 小于等于 5\u0026lt;=3 False   \u0026gt;= 小于 5\u0026gt;=3 True    a = 5 b = 3 x = a\u0026lt;b print(x) print(type(x)) False \u0026lt;class 'bool'\u0026gt;  print(5==5) True  print(5!=5) False  **注意:**比较符两侧必须为同样的数据类型\na = \u0026#39;5\u0026#39; b = 5 print(a\u0026gt;b) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-15-861d7a553a1d\u0026gt; in \u0026lt;module\u0026gt; 2 b = 5 3 ----\u0026gt; 4 print(a\u0026gt;b) TypeError: '\u0026gt;' not supported between instances of 'str' and 'int'  a = \u0026#39;5\u0026#39; b = \u0026#39;5\u0026#39; print(a==b) True  \n成员运算符in 用来判断某个值是否在集合中（这个集合可以使列表、元组、字符串等)\n   案例 结果     4 in [1,2,4] True   4 not in [1,2,4] False   3 in [1,2,4] False   3 not in [1,2,4] True    x = 4 in [1,2,4] x True  y = 4 not in [1,2,4] y False  \nNone 特殊的空值，类似于C语言中的Null。\nNone \u0026#39;\u0026#39; \n''  [] []  dict() {}  type(None) NoneType  \n","permalink":"/blog/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B9%8B%E5%B8%83%E5%B0%94%E5%80%BCnone/","summary":"布尔值Boolean 用于逻辑判断，一般与if结合使用。\na = True print(a) True  True True  False False  其他产生布尔值的方式\n 布尔运算 比较运算 成员运算  布尔运算 中学数学课里的且或非\n   运算符号 功能 例子 等于     x and y 且 True and False False   x or y 或 True or False True   not x 非 not True False    x = True and False print(x) False  x = True or False print(x) True  x = not True print(x) False  x = not False print(x) True  比较运算 注意： =和==的区别，=用来把某个值传给某个变量(赋值操作)，==用来判断两个值(变量)是否相等(判断操作)","title":"数据类型-布尔值"},{"content":" 最有层次感的数据类型，特别干净整洁。在写爬虫时，我们最希望遇到的数据类型就是ta\n 下图就是最简单的字典的样式，键值对 key-value-pairs\n  \n字典 字典是有key，有value的 key-value-pair类型数据（键-值对）\n   id 姓名 年龄 性别 爱好     1 David 25 Male Basketball, Pingpang, Hiking   2 Mary 23 Female Reading, Movies   3 Henry 23 Male Diving, Hiking    将上面的员工信息以关键词name、age、hobbies 组织成字典数据\n空字典 david = dict() print(david) {}  填充 david[\u0026#39;age\u0026#39;] = 23 print(david) {'age': 23}  david[\u0026#39;hobbies\u0026#39;] = [\u0026#39;Basketball\u0026#39;, \u0026#39;Pingpang\u0026#39;, \u0026#39;Hiking\u0026#39;] print(david) {'age': 23, 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}  david[\u0026#39;gender\u0026#39;] = \u0026#39;Male\u0026#39; print(david) {'age': 23, 'hobbies': ['Basketball', 'Pingpang', 'Hiking'], 'gender': 'Male'}  \n更新 david[\u0026#39;age\u0026#39;] = 25 print(david) {'age': 25, 'hobbies': ['Basketball', 'Pingpang', 'Hiking'], 'gender': 'Male'}  完整的信息 infos = {\u0026#39;David\u0026#39;:{\u0026#39;age\u0026#39;:25, \u0026#39;gender\u0026#39;:\u0026#39;Male\u0026#39;, \u0026#39;hobbies\u0026#39;:[\u0026#39;Basketball\u0026#39;, \u0026#39;Pingpang\u0026#39;, \u0026#39;Hiking\u0026#39;]}, \u0026#39;Mary\u0026#39;:{\u0026#39;age\u0026#39;:23, \u0026#39;gender\u0026#39;: \u0026#39;Female\u0026#39;, \u0026#39;hobbies\u0026#39;: [\u0026#39;Reading\u0026#39;, \u0026#39;Movies\u0026#39;]}, \u0026#39;Henry\u0026#39;:{\u0026#39;age\u0026#39;:23, \u0026#39;gender\u0026#39;: \u0026#39;Male\u0026#39;, \u0026#39;hobbies\u0026#39;: [\u0026#39;Diving\u0026#39;, \u0026#39;Hiking\u0026#39;]} } print(infos) {'David': {'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}, 'Mary': {'age': 23, 'gender': 'Female', 'hobbies': ['Reading', 'Movies']}, 'Henry': {'age': 23, 'gender': 'Male', 'hobbies': ['Diving', 'Hiking']}}  字典的方法    方法 效果     dictdata.items() 返回dictdata所有item   dictdata.keys() 返回dictdata的所有关键词   dictdata.values() 返回dictdata的所有值   dictdata.get(keystr) 获取关键词keystr对应的值   dictdata[keystr] 获取关键词keystr对应的值    infos.items() dict_items([('David', {'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}), ('Mary', {'age': 23, 'gender': 'Female', 'hobbies': ['Reading', 'Movies']}), ('Henry', {'age': 23, 'gender': 'Male', 'hobbies': ['Diving', 'Hiking']})])  #把infos.items()转化为列表 list(infos.items())[0] ('David', {'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']})  infos.keys() dict_keys(['David', 'Mary', 'Henry'])  infos.values() dict_values([{'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}, {'age': 23, 'gender': 'Female', 'hobbies': ['Reading', 'Movies']}, {'age': 23, 'gender': 'Male', 'hobbies': ['Diving', 'Hiking']}])  print(infos[\u0026#39;David\u0026#39;]) print(infos.get(\u0026#39;David\u0026#39;)) {'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']} {'age': 25, 'gender': 'Male', 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}  注意： 两种功能等同，但是get获取方法更加安全稳定。\n例如\nprint(infos[\u0026#39;Will\u0026#39;]) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) \u0026lt;ipython-input-16-f7c283c8ad8e\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 print(infos['Will']) KeyError: 'Will'  print(infos.get(\u0026#39;Will\u0026#39;)) None  ","permalink":"/blog/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B9%8B%E5%AD%97%E5%85%B8/","summary":"最有层次感的数据类型，特别干净整洁。在写爬虫时，我们最希望遇到的数据类型就是ta\n 下图就是最简单的字典的样式，键值对 key-value-pairs\n  \n字典 字典是有key，有value的 key-value-pair类型数据（键-值对）\n   id 姓名 年龄 性别 爱好     1 David 25 Male Basketball, Pingpang, Hiking   2 Mary 23 Female Reading, Movies   3 Henry 23 Male Diving, Hiking    将上面的员工信息以关键词name、age、hobbies 组织成字典数据\n空字典 david = dict() print(david) {}  填充 david[\u0026#39;age\u0026#39;] = 23 print(david) {'age': 23}  david[\u0026#39;hobbies\u0026#39;] = [\u0026#39;Basketball\u0026#39;, \u0026#39;Pingpang\u0026#39;, \u0026#39;Hiking\u0026#39;] print(david) {'age': 23, 'hobbies': ['Basketball', 'Pingpang', 'Hiking']}  david[\u0026#39;gender\u0026#39;] = \u0026#39;Male\u0026#39; print(david) {'age': 23, 'hobbies': ['Basketball', 'Pingpang', 'Hiking'], 'gender': 'Male'}","title":"数据类型-字典"},{"content":" 当你不知道用什么数据类型的时候，一定要记得list，大多数的脏活累活ta都帮你搞定\n \n列表list 定义 list是一种有序的集合,内部可以由任何数据类型的组成的\n现在有5位员工的汇总信息，\n   id 姓名 年龄 性别 爱好     1 David 25 Male Basketball, Pingpang, Hiking   2 Mary 23 Female Reading, Movies   3 Henry 23 Male Diving, Hiking   4 Swift 21 Male Football, Music   5 Lenard 26 Male Stay at Home    现在我们需要用一种格式去组织5位员工的信息，以列表为例\nnames = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] ages = [25, 23, 23, 21, 26] hobbies = [[\u0026#39;Basketball\u0026#39;, \u0026#39;Pingpang\u0026#39;, \u0026#39;Hiking\u0026#39;], [\u0026#39;Reading\u0026#39;, \u0026#39;Movies\u0026#39;], [\u0026#39;Diving\u0026#39;, \u0026#39;Hiking\u0026#39;], [\u0026#39;Football\u0026#39;, \u0026#39;Music\u0026#39;], [\u0026#39;Stay at Home\u0026#39;]] print(type(names)) print(type(ages)) print(type(hobbies)) print(names) print(ages) print(hobbies) \u0026lt;class 'list'\u0026gt; \u0026lt;class 'list'\u0026gt; \u0026lt;class 'list'\u0026gt; ['David', 'Mary', 'Henry', 'Swift', 'Lenard'] [25, 23, 23, 21, 26] [['Basketball', 'Pingpang', 'Hiking'], ['Reading', 'Movies'], ['Diving', 'Hiking'], ['Football', 'Music'], ['Stay at Home']]  **多想一下： **组织数据的方式有很多种，list也可以用不同的方式去组织，大家可以自己想一想。\n\n拼接 a1 = [\u0026#39;Michael\u0026#39;, \u0026#39;Bob\u0026#39;] a2 = [\u0026#39;David\u0026#39;, \u0026#39;Lee\u0026#39;] print(a1 + a2) print(a2 + a1) ['Michael', 'Bob', 'David', 'Lee'] ['David', 'Lee', 'Michael', 'Bob']  \n拆包 注意： 等号两边长度相同\nname, age = [\u0026#39;David\u0026#39;, 25] print(name) print(age) David 25  \n切片 列表的切片与字符串类似\n   id 姓名 年龄 性别 爱好 正索引 倒索引     1 David 25 Male Basketball, Pingpang, Hiking 0 -5   2 Mary 23 Female Reading, Movies 1 -4   3 Henry 23 Male Diving, Hiking 2 -3   4 Swift 21 Male Football, Music 3 -2   5 Lenard 26 Male Stay at Home 4 -1    names = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] names ['David', 'Mary', 'Henry', 'Swift', 'Lenard']  print(names[2]) print(names[-3]) Henry Henry  print(names[0:3]) print(names[-5:-2]) ['David', 'Mary', 'Henry'] ['David', 'Mary', 'Henry']  print(names[2:]) print(names[-3:]) ['Henry', 'Swift', 'Lenard'] ['Henry', 'Swift', 'Lenard']  print(names[:2]) print(names[:-3]) ['David', 'Mary'] ['David', 'Mary']  \n列表常用方法    常用方法 功能     list.append(a) 向list中添加元素a   list.extend(lst) 向list中添加列表lst   list.count(a) 统计list中a的个数    names = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] print(names) names.append(\u0026#39;Mary\u0026#39;) print(names) ['David', 'Mary', 'Henry', 'Swift', 'Lenard'] ['David', 'Mary', 'Henry', 'Swift', 'Lenard', 'Mary']  names = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] print(names) names.extend([\u0026#39;Mary\u0026#39;]) print(names) ['David', 'Mary', 'Henry', 'Swift', 'Lenard'] ['David', 'Mary', 'Henry', 'Swift', 'Lenard', 'Mary']  ages = [25, 23, 23, 21, 26] print(ages.count(23)) 2  one = [\u0026#39;David\u0026#39;] print(one[-1]) print(one[0]) David David  \n元组tuple 形似列表，也有\n 元组拼接 切片 拆包  name_list = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] name_tuple = (\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;) print(type(name_list)) print(type(name_tuple)) print(name_list) print(name_tuple) \u0026lt;class 'list'\u0026gt; \u0026lt;class 'tuple'\u0026gt; ['David', 'Mary', 'Henry', 'Swift', 'Lenard'] ('David', 'Mary', 'Henry', 'Swift', 'Lenard')  print(name_list==name_tuple) False  \n集合 names2 = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;David\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;David\u0026#39;] print(set(names2)) {'Swift', 'Mary', 'David'}  集合的方法  setdata.add(ele) 向setdata中加入ele  name_set = set(names2) print(name_set) name_set.add(\u0026#39;William\u0026#39;) print(name_set) {'Swift', 'Mary', 'David'} {'William', 'Swift', 'Mary', 'David'}  name_set[2] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-20-98cb669cc173\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 name_set[2] TypeError: 'set' object is not subscriptable  注意： 集合不能切片\n\n","permalink":"/blog/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B9%8B%E5%88%97%E8%A1%A8%E5%85%83%E7%BB%84%E9%9B%86%E5%90%88/","summary":"当你不知道用什么数据类型的时候，一定要记得list，大多数的脏活累活ta都帮你搞定\n \n列表list 定义 list是一种有序的集合,内部可以由任何数据类型的组成的\n现在有5位员工的汇总信息，\n   id 姓名 年龄 性别 爱好     1 David 25 Male Basketball, Pingpang, Hiking   2 Mary 23 Female Reading, Movies   3 Henry 23 Male Diving, Hiking   4 Swift 21 Male Football, Music   5 Lenard 26 Male Stay at Home    现在我们需要用一种格式去组织5位员工的信息，以列表为例\nnames = [\u0026#39;David\u0026#39;, \u0026#39;Mary\u0026#39;, \u0026#39;Henry\u0026#39;, \u0026#39;Swift\u0026#39;, \u0026#39;Lenard\u0026#39;] ages = [25, 23, 23, 21, 26] hobbies = [[\u0026#39;Basketball\u0026#39;, \u0026#39;Pingpang\u0026#39;, \u0026#39;Hiking\u0026#39;], [\u0026#39;Reading\u0026#39;, \u0026#39;Movies\u0026#39;], [\u0026#39;Diving\u0026#39;, \u0026#39;Hiking\u0026#39;], [\u0026#39;Football\u0026#39;, \u0026#39;Music\u0026#39;], [\u0026#39;Stay at Home\u0026#39;]] print(type(names)) print(type(ages)) print(type(hobbies)) print(names) print(ages) print(hobbies) \u0026lt;class 'list'\u0026gt; \u0026lt;class 'list'\u0026gt; \u0026lt;class 'list'\u0026gt; ['David', 'Mary', 'Henry', 'Swift', 'Lenard'] [25, 23, 23, 21, 26] [['Basketball', 'Pingpang', 'Hiking'], ['Reading', 'Movies'], ['Diving', 'Hiking'], ['Football', 'Music'], ['Stay at Home']]  **多想一下： **组织数据的方式有很多种，list也可以用不同的方式去组织，大家可以自己想一想。","title":"数据类型-列表元组集合"},{"content":"网络爬虫与文本分析实际上主要是对字符串做的处理，一定要熟悉字符串哦\n\n字符串string 定义 字符串是以 单引号 或 双引号 或 三引号 括起来的任意文本，如\n \u0026lsquo;abc\u0026rsquo; \u0026ldquo;abc\u0026rdquo; \u0026lsquo;\u0026lsquo;\u0026lsquo;abc\u0026rsquo;\u0026rsquo;\u0026rsquo; \u0026ldquo;\u0026ldquo;\u0026ldquo;abc\u0026rdquo;\u0026rdquo;\u0026rdquo;  a = \u0026#39;abc\u0026#39; a 'abc'  \u0026#34;abc\u0026#34; 'abc'  \u0026#39;\u0026#39;\u0026#39;abc\u0026#39;\u0026#39;\u0026#39; 'abc'  \u0026#34;\u0026#34;\u0026#34;abc\u0026#34;\u0026#34;\u0026#34; 'abc'  print(\u0026#39;abc\u0026#39;) print(\u0026#34;abc\u0026#34;) print(\u0026#39;\u0026#39;\u0026#39;abc\u0026#39;\u0026#39;\u0026#39;) print(\u0026#34;\u0026#34;\u0026#34;abc\u0026#34;\u0026#34;\u0026#34;) abc abc abc abc  print(\u0026#39;abc\u0026#39;) print(\u0026#39;efg\u0026#39;) abc efg  注意:\n 这里所说的引号都是英文引号 '' 或 \u0026quot;\u0026quot; 本身不是字符串的一部分，因此，字符串 'abc' 只有a，b，c这3个字符。 notebook中只显示最后一个，所以这里用了两个print   **Tips: **这里所说的引号都是英文引号\n'' 或 \u0026quot;\u0026quot; 本身不是字符串的一部分，因此，字符串 'abc' 只有a，b，c这3个字符。\nnotebook中只显示最后一个，所以这里用了两个print\n \n拼接+ 按顺序拼接\na = \u0026#39;P\u0026#39; b = \u0026#39;ython\u0026#39; print(a+b) print(b+a) Python ythonP  a = \u0026#39;P\u0026#39; print(a+b) print(b+a) \n切片 切片就像切糕，把自己想要的那块切下来\n  name = \u0026#39;My Name is Mike\u0026#39; name[0] 'M'  name[-15] 'M'  name[3:6] 'Nam'  name[-12:-9] 'Nam'  name[0:2] 'My'  name[:2] 'My'  name[5:] 'me is Mike'  name[-7:] 'is Mike'  \n切片总结    切片表达式 解读     从左往右 索引值从0开始，0表示\u0026rsquo;第一个'   从右向左 -1表示倒数第一个，-2表示倒数第二个   a:b 选取列表索引位置为a，a+1...b-2, b-1的值   a: 选取列表中a之后的所有元素(含a)   :b 选取列表中b之前的所有元素(不含b)    字符串常用方法 再次强调，数据类(型)与猪牛羊不同的动物类型一样，都有满足人类需要的特殊本领(方法)。方法可以理解为数据类(型)一种特殊的本性、属性、特性\n   字符串常用方法 功能     str.lower() 变小写   str.upper() 变大写   str.split(sep) 使用sep将字符串分割，默认sep为空格   str.replace(old, new) 将str中的old替换为new   str.format() 向str中填充内容    words = \u0026#39;Python is poweful!\u0026#39; words.lower() 'python is poweful!'  words.upper() 'PYTHON IS POWEFUL!'  words 'Python is poweful!'  words.split(\u0026#39; \u0026#39;) ['Python', 'is', 'poweful!']  words.replace(\u0026#39;Python\u0026#39;, \u0026#39;Python programing language\u0026#39;) 'Python programing language is poweful!'  需要发送每个员工的工资组成详情。\n\u0026#34;张三,你这个月的工资是2310元；以下是你的工资详情。。。。\u0026#34; \u0026#34;李四,你这个月的工资是3456元；以下是你的工资详情。。。。\u0026#34; \u0026#34;王五,你这个月的工资是2431元；以下是你的工资详情。。。。\u0026#34; 如何自动化自动化填充?\ntemplate = \u0026#39;{name},你这个月的工资是{salary}元；以下是你的工资详情\u0026#39; print(template.format(name=\u0026#39;张三\u0026#39;, salary=\u0026#39;2310\u0026#39;)) print(template.format(name=\u0026#39;李四\u0026#39;, salary=\u0026#39;3456\u0026#39;)) print(template.format(name=\u0026#39;王五\u0026#39;, salary=\u0026#39;2431\u0026#39;)) 张三,你这个月的工资是2310元；以下是你的工资详情 李四,你这个月的工资是3456元；以下是你的工资详情 王五,你这个月的工资是2431元；以下是你的工资详情  \n转义符\\ 如果字符串内部既包含 单引号 又包含 双引号， 会发生什么？\nprint(\u0026#39;I\u0026#39;m \u0026#34;OK\u0026#34;!\u0026#39;)  File \u0026quot;\u0026lt;ipython-input-28-af5dc904b170\u0026gt;\u0026quot;, line 1 print('I'm \u0026quot;OK\u0026quot;!') ^ SyntaxError: invalid syntax  # 可以用 转义字符\\ 来标识，比如： print(\u0026#39;I\\\u0026#39;m \\\u0026#34;OK\\\u0026#34;!\u0026#39;) I'm \u0026quot;OK\u0026quot;!  常见的转义符还有\n \\n 换行 \\t 制表符 字符\\ 本身也要转义，所以 \\\\ 表示的字符就是 \\  可以试着自己运行下面代码，看看下面打印结果是?：\nprint(\u0026#39;Hello\\tWorld!\u0026#39;) print(\u0026#39;转义\\n换行!\u0026#39;) print(\u0026#39;反斜杠线\\\\\u0026#39;) \nprint(\u0026#39;Hello\\tWorld!\u0026#39;) print(\u0026#39;Hello World!\u0026#39;) Hello\tWorld! Hello World!  print(\u0026#39;转义\\n换行!\u0026#39;) 转义 换行!  print(\u0026#39;反斜杠线\\\\\u0026#39;) 反斜杠线\\  \nr 如果字符串里面有很多字符都需要转义，就需要加很多\\,\n为了简化，Python还允许用r''表示''内部的字符串默认不转义,例如\nprint(\u0026#39;\\\\\\t\\\\\u0026#39;) print(r\u0026#39;\\\\\\t\\\\\u0026#39;) print(\u0026#39;hello world!\u0026#39;) \nprint(\u0026#39;\\\\\\t\\\\\u0026#39;) \\\t\\  print(r\u0026#39;\\\\\\t\\\\\u0026#39;) \\\\\\t\\\\  \n","permalink":"/blog/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2/","summary":"网络爬虫与文本分析实际上主要是对字符串做的处理，一定要熟悉字符串哦\n\n字符串string 定义 字符串是以 单引号 或 双引号 或 三引号 括起来的任意文本，如\n \u0026lsquo;abc\u0026rsquo; \u0026ldquo;abc\u0026rdquo; \u0026lsquo;\u0026lsquo;\u0026lsquo;abc\u0026rsquo;\u0026rsquo;\u0026rsquo; \u0026ldquo;\u0026ldquo;\u0026ldquo;abc\u0026rdquo;\u0026rdquo;\u0026rdquo;  a = \u0026#39;abc\u0026#39; a 'abc'  \u0026#34;abc\u0026#34; 'abc'  \u0026#39;\u0026#39;\u0026#39;abc\u0026#39;\u0026#39;\u0026#39; 'abc'  \u0026#34;\u0026#34;\u0026#34;abc\u0026#34;\u0026#34;\u0026#34; 'abc'  print(\u0026#39;abc\u0026#39;) print(\u0026#34;abc\u0026#34;) print(\u0026#39;\u0026#39;\u0026#39;abc\u0026#39;\u0026#39;\u0026#39;) print(\u0026#34;\u0026#34;\u0026#34;abc\u0026#34;\u0026#34;\u0026#34;) abc abc abc abc  print(\u0026#39;abc\u0026#39;) print(\u0026#39;efg\u0026#39;) abc efg  注意:\n 这里所说的引号都是英文引号 '' 或 \u0026quot;\u0026quot; 本身不是字符串的一部分，因此，字符串 'abc' 只有a，b，c这3个字符。 notebook中只显示最后一个，所以这里用了两个print   **Tips: **这里所说的引号都是英文引号\n'' 或 \u0026quot;\u0026quot; 本身不是字符串的一部分，因此，字符串 'abc' 只有a，b，c这3个字符。\nnotebook中只显示最后一个，所以这里用了两个print\n \n拼接+ 按顺序拼接","title":"数据类型-字符串string"},{"content":"学Python一段时间后，都会听到一句“Life is short, so we learn Python! ”，恭喜你选择Python这门强大而有趣的语言。\n输出数字a的绝对数 在学习代码之前，我给大家看一段话\n There is such a number a, if a is greater than or equal to 0, we will print a; if a is less than 0, we will print -a\n 相信大家一看就明白了这是求某数的绝对值方法的英文描述。下面我们用精炼的Python语言表示\na = -50 if a \u0026gt;= 0: print(a) else: print(-a) 50  Python号称是最说人话的编程语言，以最接近人类理解的方式构建代码。\nPython与英语对比 从上面的例子中，我们已经知道了Python和英语一样都是一种语言，学习语言就需要学习基本的知识点，包括背单词和了解语法。\n   英语 Python 例如     单词 数据类型 列表、字符串、字典等   语法 逻辑语句 if条件判读语句、for循环语句等    每天积累一点点 本部分非必须，仅仅为了展示python也可以作图\n#mac #!pip3 install matplotlib #win !pip install matplotlib Looking in indexes: https://mirrors.aliyun.com/pypi/simple/ Requirement already satisfied: matplotlib in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (3.2.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,\u0026gt;=2.0.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (2.4.7) Requirement already satisfied: cycler\u0026gt;=0.10 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (0.10.0) Requirement already satisfied: numpy\u0026gt;=1.11 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (1.18.2) Requirement already satisfied: kiwisolver\u0026gt;=1.0.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (1.2.0) Requirement already satisfied: python-dateutil\u0026gt;=2.1 in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from matplotlib) (2.8.1) Requirement already satisfied: six in c:\\users\\thunderhit\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from cycler\u0026gt;=0.10-\u0026gt;matplotlib) (1.14.0)  import matplotlib.pyplot as plt import math %matplotlib inline ability = 1 scale = 1.01 records = [] days = range(1, 365) for day in days: ability = ability*scale records.append(ability) plt.plot(days, records) plt.title(\u0026#39;Be better everyday!\u0026#39;) Text(0.5, 1.0, 'Be better everyday!')    \nPython是面向对象的编程语言 面向对象是最难理解的部分，这里大家只需要记住\n 类型和对象是紧密绑定的，说对象就是在说类型。 不同的类型有不同的功能，都是为了更高效的实现人类需求或者数据分析需求     类型 实例(对象) 实例(对象) 方法     猪 村东头老王家的猪 把猪把粮食变成肉 猪.产肉   牛 村东头老张家的耕牛 把粮食变成畜力 牛.耕地   列表 hobbies = ['跑步', '乒乓球'， '篮球'，'篮球'] 统计某群体爱好的分布, 查看各爱好的人数 hobbies.count('篮球')   字符串 str1 = \u0026quot;Hello，World!\u0026quot; 将文本内容由World更改为Python str1.replace('World', 'Python')   字典 grade = {'David':98, 'Mary':88,...} 方便数据检索 grade.get('David')   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;    Python中的数据类型 不同的数据类型适合处理不同的数据，有不同的应用场景。\n   数据类型 例子     数字 a = 5   字符串 my_str = \u0026quot;abcdefg\u0026quot;   列表 my_list = [1, 2, 3]   元组 my_tuple = (1, 2, 3)   字典 my_dict = {'David': 25, 'Mark':30}   空值 None    \n","permalink":"/blog/python%E8%B7%9F%E8%8B%B1%E8%AF%AD%E4%B8%80%E6%A0%B7%E6%98%AF%E4%B8%80%E9%97%A8%E8%AF%AD%E8%A8%80/","summary":"学Python一段时间后，都会听到一句“Life is short, so we learn Python! ”，恭喜你选择Python这门强大而有趣的语言。\n输出数字a的绝对数 在学习代码之前，我给大家看一段话\n There is such a number a, if a is greater than or equal to 0, we will print a; if a is less than 0, we will print -a\n 相信大家一看就明白了这是求某数的绝对值方法的英文描述。下面我们用精炼的Python语言表示\na = -50 if a \u0026gt;= 0: print(a) else: print(-a) 50  Python号称是最说人话的编程语言，以最接近人类理解的方式构建代码。\nPython与英语对比 从上面的例子中，我们已经知道了Python和英语一样都是一种语言，学习语言就需要学习基本的知识点，包括背单词和了解语法。\n   英语 Python 例如     单词 数据类型 列表、字符串、字典等   语法 逻辑语句 if条件判读语句、for循环语句等    每天积累一点点 本部分非必须，仅仅为了展示python也可以作图","title":"Python是一门语言"},{"content":"注意：\n Anaconda和Python都是python。一定要记住二选其一，不要都安装，不然在学习python第三方包安装的时，你会遇到一些麻烦。 如果之前没安装过两个软件，建议从头到尾按照我的视频进行电脑配置。  Mac环境配置 软件包下载 链接: https://pan.baidu.com/s/1tbgGBcAnYSMZXp80F0nM1Q 密码: t307\n一、Python安装 官网 https://www.python.org/\nmac自带python2，为了与python2区别，凡是在命令行中使用pip和python，我们都要加上3。\n安装成功的标准是==命令行可以调用python3==\n$ python3 \n命令行打开的方法 ==command+空格== 启动 ==聚焦搜索Spotlight==，再输入terminal\n\n二、pip3设置 pip3是python的命令行安装工具，可以帮我们安装第三方库。\n2.1 更改pip3镜像 为了保证安装的速度和成功率，命令行执行\npip3 config set global.index-url https://mirrors.aliyun.com/pypi/simple/ 2.2 使用方法 pip3 install packagename 2.3 第三方库安装方法   pip安装法\n 单个库的安装，命令行执行  pip3 install 库的名字   多个库的安装， 命令行执行  pip3 install -r requirements.txt      pypi本地安装\n  在https://pypi.org/ 搜库，点进去\n  找Download files，下载whl或压缩文件到桌面。例如文件名 xxx.whl\n  命令行依次执行\n   cd desktop\n  pip3 install xxx.whl\n      github本地安装（如github项目中存在setup.py文件，可以安装使用）\n  下载github项目至桌面，解压\n  命令依次执行 - cd desktop - python3 setup.py install\n  \n三、Jupyter notebook 3.1 安装 命令行执行\npip3 install jupyter 3.2 调用 命令行执行\njupyter notebook 3.3 常用快捷键    jupyter内快捷键 功能     ESC+A（ESC+B） 当前单元格上(下)新建一个新的Cell   D+D 删除当前单元格   Shift+Enter 执行单元格内的Python代码   ESC+M 单元格由代码模式转为Markdown标记模式    推荐： Markdown语法特别好用，强烈建议学习，顺便安装一个Typora软件。\n\n四、Tips 环境配置太难，而且有时候电脑还会出现一些视频里出现不了的问题。这时不妨在==淘宝==搜python环境配置，寻找一对一远程协助\n","permalink":"/blog/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEmac/","summary":"注意：\n Anaconda和Python都是python。一定要记住二选其一，不要都安装，不然在学习python第三方包安装的时，你会遇到一些麻烦。 如果之前没安装过两个软件，建议从头到尾按照我的视频进行电脑配置。  Mac环境配置 软件包下载 链接: https://pan.baidu.com/s/1tbgGBcAnYSMZXp80F0nM1Q 密码: t307\n一、Python安装 官网 https://www.python.org/\nmac自带python2，为了与python2区别，凡是在命令行中使用pip和python，我们都要加上3。\n安装成功的标准是==命令行可以调用python3==\n$ python3 \n命令行打开的方法 ==command+空格== 启动 ==聚焦搜索Spotlight==，再输入terminal\n\n二、pip3设置 pip3是python的命令行安装工具，可以帮我们安装第三方库。\n2.1 更改pip3镜像 为了保证安装的速度和成功率，命令行执行\npip3 config set global.index-url https://mirrors.aliyun.com/pypi/simple/ 2.2 使用方法 pip3 install packagename 2.3 第三方库安装方法   pip安装法\n 单个库的安装，命令行执行  pip3 install 库的名字   多个库的安装， 命令行执行  pip3 install -r requirements.txt      pypi本地安装\n  在https://pypi.org/ 搜库，点进去\n  找Download files，下载whl或压缩文件到桌面。例如文件名 xxx.","title":"Mac电脑Python环境配置"},{"content":"注意：\n Anaconda和Python都是python。一定要记住二选其一，不要都安装，不然在学习python第三方包安装的时，你会遇到一些麻烦。 如果之前没安装过两个软件，建议从头到尾按照我的视频进行电脑配置。  Win环境配置 软件包下载 链接: https://pan.baidu.com/s/1tbgGBcAnYSMZXp80F0nM1Q 密码: t307\n一、Python安装 官网 https://www.python.org/\n安装注意事项  推荐选择3.7.5. 最新的bug比较多 ==选择Install Now默认安装方式== 勾选Add Python 3.x to PATH，这样命令行可以调用python  \n二、pip配置 pip是python的命令行安装工具，可以帮我们安装第三方库。\n2.1 更改pip镜像 为了保证安装的速度和成功率，命令行执行\npip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ 2.2 使用方法 pip install packagename 2.3 第三方库安装方法   pip安装法\n 单个库的安装，命令行执行  pip3 install 库的名字   多个库的安装， 命令行执行  pip3 install -r requirements.txt      pypi本地安装\n  在https://pypi.org/ 搜库，点进去\n  找Download files，下载whl或压缩文件到桌面。例如文件名 xxx.whl\n  命令行依次执行\n   cd desktop\n  pip3 install xxx.whl\n      github本地安装（如github项目中存在setup.py文件，可以安装使用）\n  下载github项目至桌面，解压\n  命令依次执行 - cd desktop - python3 setup.py install\n  \n三、Jupyter notebook 3.1 安装 命令行执行\npip install jupyter 3.2 调用 命令行执行\njupyter notebook 3.3 常用快捷键    jupyter内快捷键 功能     ESC+A（ESC+B） 当前单元格上(下)新建一个新的Cell   D+D 删除当前单元格   Shift+Enter 执行单元格内的Python代码   ESC+M 单元格由代码模式转为标记模式    个人建议： Markdown语法特别好用，强烈建议学习，顺便安装一个Typora软件。\n\n四、Tips 环境配置太难，而且有时候电脑还会出现一些视频里出现不了的问题。这时不妨在==淘宝==搜python环境配置，寻找一对一远程协助\n","permalink":"/blog/python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEwin/","summary":"注意：\n Anaconda和Python都是python。一定要记住二选其一，不要都安装，不然在学习python第三方包安装的时，你会遇到一些麻烦。 如果之前没安装过两个软件，建议从头到尾按照我的视频进行电脑配置。  Win环境配置 软件包下载 链接: https://pan.baidu.com/s/1tbgGBcAnYSMZXp80F0nM1Q 密码: t307\n一、Python安装 官网 https://www.python.org/\n安装注意事项  推荐选择3.7.5. 最新的bug比较多 ==选择Install Now默认安装方式== 勾选Add Python 3.x to PATH，这样命令行可以调用python  \n二、pip配置 pip是python的命令行安装工具，可以帮我们安装第三方库。\n2.1 更改pip镜像 为了保证安装的速度和成功率，命令行执行\npip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ 2.2 使用方法 pip install packagename 2.3 第三方库安装方法   pip安装法\n 单个库的安装，命令行执行  pip3 install 库的名字   多个库的安装， 命令行执行  pip3 install -r requirements.txt      pypi本地安装\n  在https://pypi.","title":"Windows电脑Python环境配置"}]