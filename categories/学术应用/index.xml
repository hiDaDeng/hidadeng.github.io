<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>学术应用 on 大邓和他的PYTHON</title>
    <link>/categories/%E5%AD%A6%E6%9C%AF%E5%BA%94%E7%94%A8/</link>
    <description>Recent content in 学术应用 on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Wed, 22 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="/categories/%E5%AD%A6%E6%9C%AF%E5%BA%94%E7%94%A8/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LIST | 社科(经管)数据挖掘文献资料汇总</title>
      <link>https://textdata.cn/blog/the_text_analysis_list_about_ms/</link>
      <pubDate>Sat, 16 Apr 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/the_text_analysis_list_about_ms/</guid>
      <description>如何从网络世界中高效地采集数据？是否能从文本中挖掘出人类的偏见等认知信息？如何从杂乱的文本数据中抽取文本信息(变量)？本文汇总的列表将让你对文本、对Python文本分析个全面的了解</description>
      <content:encoded><![CDATA[<p>个人感觉博客 <strong><a href="https://textdata.cn/">textdata.cn</a></strong> 精华就在这里了。 不定期更新， 内容聚焦于Python文本分析在经管、社科等领域的应用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 营销
- 会计学
- 经济学
- 心理学
- 社会学
- ...
</code></pre></div><p>读几篇文章能加深对各领域文本分析方法应用的理解。</p>
<br>
<h2 id="管理学">管理学</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/read_this_you_will_know_what_is_text_mining/">读完本文你就了解什么是文本分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-05-xjtu-text-mining-in-ms/">2023分享 | 文本分析在经济管理研究中的应用</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-09-08-dufe-text-mining-in-ms/">视频2022 | 文本分析在经济管理研究中的应用</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-10-11-how-can-machine-learning-empower-management-research/">管理世界 | 机器学习如何赋能管理学研究？——国内外前沿综述和未来展望</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-08-measurement_of_psychological_factors_and_their_economic_impact/">管理世界 | 政府与市场心理因素的经济影响及其测度</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/">MS2022 | 使用语言差异性测量 <strong>团队认知差异性</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-10-10-measure-the-speed-of-policy-diffusion-from-top-to-down/">管理科学学报 | 使用LDA算法计算政策扩散速度与扩散程度</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-09-07-management-science-disrupt-science-and-technology">Management Science | 使用网络算法识别创新的颠覆性与否</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/research_with_tm_in_chinese_top_ms_journal/">近年《管理世界》《管理科学学报》使用文本分析论文</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="营销">营销</h2>
<ul>
<li><a href="https://textdata.cn/blog/text_mining_in_marketing_research/">文本分析在市场营销研究中的应用</a></li>
<li><a href="https://textdata.cn/blog/jcr_concreteness_computation/">JCR2021 | 计算文本的 <strong>语言具体性</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-10-16-measurement-of-consumer-certainty-in-language/">JMR2023 | 测量消费者的 <strong>语言确定性</strong></a></li>
<li><a href="https://textdata.cn/blog/2022-12-03-scraping-web-data-for-marketing-insights/">JM2022综述 | 黄金领域: 为营销研究(新洞察)采集网络数据</a></li>
<li><a href="https://textdata.cn/blog/automate_text_analysis_in_market/">营销研究中文本分析应用概述(含案例及代码)</a></li>
</ul>
<p><br><br></p>
<h2 id="会计金融">会计&amp;金融</h2>
<ul>
<li><a href="https://textdata.cn/blog/2022-11-16-literature-review-textmining-in-finance-yao2020/">转载 | 金融学文本大数据挖掘方法与研究进展</a></li>
<li><a href="https://textdata.cn/blog/2023-01-12-review_about_accounting_text_mining/">转载 | 国外会计文本信息实证研究述评与展望</a></li>
<li><a href="https://textdata.cn/blog/accountingtext/">视频分享 | 会计领域中的Python文本分析</a></li>
<li><a href="https://textdata.cn/blog/fintech_quant_with_python/">视频分享 | Python数据挖掘与金融科技 </a></li>
<li><a href="https://textdata.cn/blog/2023-08-26-text-analysis-in-accounting/">CAR2023 | 文本分析在会计中的应用</a></li>
<li><a href="https://textdata.cn/blog/manager_tone_analysis_with_lm/">管理世界 | 使用LM中文金融词典对年报进行语调分析</a></li>
<li><a href="https://textdata.cn/blog/text_mining_in_2021_management_world/">管理世界| 使用文本分析&amp;机器学习测量短视主义</a></li>
<li><a href="https://textdata.cn/blog/2022-11-03-mda-measure-digitalization/">管理世界 | 使用 经营讨论与分析测量企业数字化</a></li>
<li><a href="https://textdata.cn/blog/2024-12-31-using-regex-to-compute-the-financial_constraints/">管理世界（付费） | 使用md&amp;a数据中计算 「企业融资约束指标」</a></li>
<li><a href="https://textdata.cn/blog/2024-12-31-the-experience-of-ceo-to-vector-with-graphe-embeddings/">如何用图嵌入(网络思维和嵌入思维)表征企业，表征高管的职业经历</a></li>
<li><a href="https://textdata.cn/blog/2024-12-31-measure-corporate-culture-using-word2vec/">使用 Word2Vec 和 TF-IDF 计算五类企业文化</a></li>
<li><a href="https://textdata.cn/blog/2019-12-08-lazy-prices/">文本相似 | Lazy Prices公司年报内容变动预示重大风险</a></li>
<li><a href="https://textdata.cn/blog/2023-10-07-esg-measurement/">使用文本分析度量企业ESG属性</a></li>
<li><a href="https://textdata.cn/blog/2023-05-23-soft-cosine-similarity/"><strong>管理科学学报(付费)  |  使用「软余弦相似度」测量业绩说明会「答非所问程度」</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-09-08-earnings-communication-conference-forward-looking-statements-information/"><strong>中国管理科学(付费) | 使用业绩说明会文本数据测量上市公司前瞻性信息</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-01-06-mda_informative_content/"><strong>中国工业经济（付费） | MD&amp;A信息含量指标构建代码实现</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-01-13-information-content-of-critical-audit/"><strong>金融研究(付费) | 使用Python构建「关键审计事项信息含量」</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-01-10-similarity_of_cental_bank_monetary_policy/">金融研究 | 央行货币政策文本相似度计算与可视化</a></li>
<li><a href="https://textdata.cn/blog/2023-01-16-papers-using-text-mining-tech-in-journal-of-economic-research/">近年《经济研究》中「文本分析」相关论文</a></li>
<li><a href="https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/">代码 | 使用「新闻数据」计算 「经济政策不确定性」指数</a></li>
</ul>
<br>
<br>
<h2 id="经济学">经济学</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-04-09-narrative-economic-method/">叙事经济学：揭示经济中的叙事</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-12-30-review-about-socioeconomic-status-analysis/">转载 | 大数据驱动的「社会经济地位」分析研究综述</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-09-19-quantitative-history-economic/">文献汇总 | 量化历史学与经济学研究</a></p>
</li>
</ul>
<br>
<br>
<h2 id="心理学">心理学</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-03-31-pnas-measure-replicability-of-psychology-with-ml/">PNAS | 14000+篇心理学顶刊论文可复现性调研</a></li>
<li><a href="https://textdata.cn/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/">PNAS | 使用语义距离测量一个人的创新力(发散思维)得分</a></li>
<li><a href="https://textdata.cn/blog/2023-03-10-psychological-research-with-word-embeddings/">基于词嵌入技术的心理学研究: 方法及应用</a></li>
<li><a href="https://textdata.cn/blog/2023-10-18-the-relationship-between-semantic-distance-with-creativity/">心理科学进展 | 语义距离与创造性思维关系的元分析</a></li>
<li><a href="https://textdata.cn/blog/2023-02-13-computing-cultural-psychology-with-big-data/">转载 | 大数据时代的「计算文化心理学」</a></li>
</ul>
<p><br><br></p>
<h2 id="社会学">社会学</h2>
<ul>
<li><a href="https://textdata.cn/blog/2022-12-03-social-computing-methodology-about-big-data-and-artificial-intelligence/">转载 | 社会计算驱动的社会科学研究方法</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/"><strong>可视化 | 人民日报语料反映七十年文化演变</strong></a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
<li><a href="https://textdata.cn/blog/2021-12-19-pnas_historical_language/">PNAS | 历史语言记录揭示了近几十年来认知扭曲的激增</a></li>
<li><a href="https://textdata.cn/blog/2022-01-02-pnas_love_separate/">PNAS | 情侣分手3个月前就有预兆！聊天记录还能反映分手后遗症</a></li>
<li><a href="https://textdata.cn/blog/2023-03-13-linguistic-positivity-in-historical-texts-reflects-dynamic-environmental-and-psychological-factors/">PNAS | 历史文本中的语言积极性反映了动态的环境和心理因素(含Python代码)</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2021-12-28-pnas_culture_bridges/">PNAS | 文本网络分析&amp;文化桥梁 Python 代码实现</a></li>
<li><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
<li><a href="https://textdata.cn/blog/2022-04-01-embeddings-and-attitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
<li><a href="https://textdata.cn/blog/2023-03-03-extracts-cognitive-information-and-visualization-with-embedings/">可视化  |  词嵌入模型用于计算社科领域刻板印象等信息（含代码）</a></li>
<li><a href="https://textdata.cn/blog/2021-12-27-pnas_text_fluency/">PNAS | 词汇熟悉度对线上参与和资金筹集的预测性效用</a></li>
</ul>
<p><br><br></p>
<h2 id="其他">其他</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-04-07-sapir-whorf-hypothesis/">语言相对性论 | 语言是否决定/影响人的思维和认知</a></li>
<li><a href="https://textdata.cn/blog/2023-11-16-how-to-understand-the-meaning-of-gpt/">Word Embeddings、Transformer与GPT：一文揭示三者关系</a></li>
<li><a href="https://textdata.cn/blog/2023-11-13-violatating-privacy-via-inference-with-large-language-model/">大模型的隐私推断能力 | 不可不防的大模型“人肉搜索”能力</a></li>
<li><a href="https://textdata.cn/blog/text_readability/">文本可读性研究及应用清单</a></li>
<li><a href="https://mp.weixin.qq.com/s/mefUYQnTn8vdWV78c9lRBw">多维度、细粒度情感词库的核心思想与建设过程概述</a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><strong>付费视频课程 | Python实证指标构建与文本分析</strong>
<ul>
<li>大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与<a href="https://textdata.cn/blog/2022-05-workshop/7-Python.html">直播课</a>。</li>
<li>如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的<a href="https://textdata.cn/blog/management_python_course">录播课</a>。</li>
<li>如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读<a href="https://textdata.cn/blog/paid_for_service">有偿说明</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>LIST| 文本分析代码资料汇总</title>
      <link>https://textdata.cn/blog/text_analysis_code_list_about_ms/</link>
      <pubDate>Wed, 22 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/text_analysis_code_list_about_ms/</guid>
      <description>如何使用Python从网络中爬取数据，如何从文本数据中抽取信息。本文汇总了常见的python代码案例，方便大家快速学习</description>
      <content:encoded><![CDATA[<p>个人感觉博客 <strong><a href="https://textdata.cn/">textdata.cn</a></strong> 文本分析代码案例都集中在这里了，我将内容按大类分成</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- Python语法
- 数据采集
- 数据处理&amp;Pandas
  - 正则表达式
  - pandas常用方法
  - pandas性能优化
  - 其他操作
- 文本分析
  - 概览
  - 词典法
  - 词向量
  - 大语言模型
- 数据标注&amp;机器学习
  - 数据标注
  - 监督机器学习
  - 非监督机器学习
- 可视化
- R语言
- 其他
</code></pre></div><p><br><br></p>
<h2 id="一python语法">一、Python语法</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/30_days_of_python/">30天Python编程学习挑战</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/dadeng_python_basic_tutorial/">Python语法入门 | 含视频代码</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-07-19-advanced-python-mastery/"><strong>免费下载 | 进阶Python学习资料</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-18-how-to-use-if-elif-else-in-one-line/">如何在一行代码中实现if-elif-else三分支语句</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-01-tricks-for-better-python-code-with-examples/">12个优雅的python代码使用案例</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/course_recommendation_about_social_science/">免费社科类Python编程课程列表</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-10-bidirectional-mapping-library/">bidict库 | Python双向映射功能，让字典更好用</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="二数据采集">二、数据采集</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-10-13-crawler-for-qyer/">网络爬虫 |  采集穷游网某城市旅游景点</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-05-07-bilibili-video-info-list/">网络爬虫 | 使用Python披露采集 Up 主视频列表详情信息</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-05-12-welcome-to-zibo-barbecue/"><strong>网络爬虫 | 批量采集话题「如何评价淄博烧烤？」的回答</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-23-data-collector-for-douban-group-parent-child-relationship/">网络爬虫 | 使用Python采集豆瓣「全职儿女」小组组员信息</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-23-data-collector-for-bilibili-danmu/">网络爬虫(付费) | 使用Python采集B站弹幕和评论数据</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/qdata_collect_baidu_index/">百度指数 | 使用qdata采集百度指数</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-10-08-find-sns-account-information-with-maigret/"> Maigret库 | 查询某用户名在各平台网站的使用情况</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="三数据处理pandas">三、数据处理&amp;Pandas</h2>
<h3 id="31-文本处理">3.1 文本处理</h3>
<p>使用正则表达式可以筛选文本数据，做数据预处理(数据清洗)</p>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-02-18-regex-expression-examples/">正则表达式 | 词频统计、情感分析、融资约束</a></p>
</li>
<li>
<p><a href="https://textdata.cn/2023-10-30-raw-mbti-users/">文本分析 | 使用正则表达式判别微博用户mbti类型</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-12-regex-expression-generated-by-chatgpt/">数据清洗 | 借助 chatGPT 设计正则表达式</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-17-how-to-generate-panel-data-from-gov-report-dataset/"><strong>代码 | 使用地方gov工作报告生成某类概念词频「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-27-measure-gov-digitalization/">代码 | 使用gov工作报告生成数字化词频「面板数据」</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-18-how-to-generate-panel-data-from-daily-news-dataset/"><strong>代码 | 使用「新闻数据」构造概念词提及量「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-26-cctv1-xwlb-news-text-dataset/"><strong>数据代码| 使用cctv新闻联播文稿构造「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-19-word-in-context/">word_in_context | 查看某类词的上下文，更好的理解文本数据</a></p>
</li>
</ul>
<br>
<h3 id="32-常用方法">3.2 常用方法</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2028-12-18-how-to-extract-data-from-patent-application-dataset/"><strong>代码 | 使用3571w专利申请数据集构造「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/">代码 | 使用「新闻数据」计算 「经济政策不确定性」指数</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-26-matching-listed-corporate-with-patent-dataset/">从3571w条专利数据集「匹配」上市公司的专利信息</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-05-31-resample-groupby-in-pandas/">可视化 | 使用groupby或resample按月份分组绘制高管违规量趋势图</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-08-31-data-visualization-how-to-plot-a-map-with-geopandas/">可视化 | 使用geopandas可视化地图数据</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-27-cheatsheet-about-text-manipulate-in-python/">CheatSheet | Python文本数据处理速查表</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-27-pandas-dataframe-tutorial-in-python/">Pandas库 | DataFrame类常用知识点总结</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-30-query-method-in-dataframe/">Pandas库 | 使用 df.query 字符串表达式进行数据筛选</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-08-07-using-str-contains-method-to-judge-some-specific-content-in-excel/">Pandas库 | 对高管数据xlsx中的简介字段做文本分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/text_analysis_in_pandas/">使用Pandas处理文本数据</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/pandas_example_company_analysis/">Pandas小案例 | 对某公司同年的某指标批量汇总</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-11-xiaohongshu-data-analysis/">数据分析 | 使用决策树分析小红书帖子数据(含代码)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-25-zhihu-parent-child-relationship/">数据分析 | 知乎热门话题「全职儿女」</a></p>
</li>
</ul>
<br>
<h3 id="33-性能优化其他操作">3.3 性能优化&amp;其他操作</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-12-27-polars-tutorial-an-altertaive-of-pandas/"><strong>Polars库 | 最强 Pandas 平替来了</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-17-modin-accecerate-your-process/">Modin库，只需一行代码加速你的Pandas</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-19-pandarallel-speed-up-pandas/"><strong>pandarallel库 | 多核运行提升pandas速度</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-17-how-handle-mega-csv-that-far-exceed-memory/"><strong>推荐 | 如何处理远超电脑内存的csv文件</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-08-pandas-5-trips-you-may-or-not-may-know/">5个你或许不知道的pandas数据导入技巧</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-11-16-how-to-fix-string-unicode-decode-error/">如何正确读入文本数据不乱码(解决文本乱码问题)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-30-pipeline-for-data-analysis/">使用流水线pipeline模式设计并处理数据</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="四文本分析">四、文本分析</h2>
<h3 id="41-概览">4.1 概览</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/liwc_python_text_mining/">LIWC vs Python  | 文本分析之词典词频法略讲(含代码)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/text_mining_in_accouting_research/">在会计研究中使用Python进行文本分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-15-how-to-learn-python-data-mining-with-chatgpt/">借助chatGPT更高效地学习「Python实证指标构建与文本分析」</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-10-08-nlp-roadmap/">nlp-roadmap | 文本分析知识点思维脑图</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/culture_analysis/">Python与文化分析入门</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog2023-02-01-chatgpt-usage-first-time/">使用 chatGPT 撰写 Python 文本分析代码</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/chinese_emobank/">EmoBank | 中文维度情感词典</a></p>
</li>
</ul>
<br>
<h3 id="42-词典法">4.2 词典法</h3>
<ul>
<li><a href="https://textdata.cn/blog/cntext_tutorial/">cntext库 | 中文情感分析包</a></li>
<li><a href="https://textdata.cn/blog/weighted_tfidf_sentiment_analysis/">tfidf有权重的情感分析</a></li>
<li><a href="https://textdata.cn/blog/asent_sentiment_analysis/">Asent库 | 英文文本数据情感分析</a></li>
<li><a href="https://textdata.cn/blog/share_your_dict_to_cntext/">欢迎各位向cntext库分享情感词典</a></li>
<li><a href="https://textdata.cn/blog/chinese_financial_dictionary/">中文金融情感词典</a></li>
<li><a href="https://textdata.cn/blog/how_chinese_tmtai_impact_corporate_inovation/">文本分析 | 中国企业高管团队创新注意力</a></li>
</ul>
<br>
<h3 id="43-词向量">4.3 词向量</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/"><strong>可视化 | 人民日报语料反映七十年文化演变</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-26-chinese-it-industry-slangs-words/"><strong>实验 | 互联网黑话与MD&amp;A</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/">词向量 | 使用1亿B站用户签名训练word2vec词向量</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-11-07-embeddings-theory-applicaiton-liuhuanyong/">预训练词向量模型的方法、应用场景、变体延伸与实践总结</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-10-16-python-word-mover-s-distance/"> Python | 词移距离(Word Mover&rsquo;s Distance)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-11-22-glove-embeddings-model/">训练&amp;使用 Glove 语言模型， 可度量刻板印象等</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/bertopic_tutorial/">BERTopic库 | 使用预训练模型做话题建模</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-12-03-dynamic_topic_model_with_bertopic/">BERTopic | 使用推特数据构建 <strong>动态主题模型模</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/keybert_tutorial/">KeyBERT | 关键词发现库</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/top2vec_tutorial/">Top2Vec | 主题建模和语义搜索库</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-11-17-finbert-finance-bert-model/">FinBERT | 金融文本BERT模型，可情感分析、识别ESG和FLS类型</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/sentence-transformer-tutorial/">sentence-transformer库 | 句子语义向量化</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/wordbias/">WordBias库 | 发现偏见(刻板印象)的交互式工具</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-10-27-nlp_gte_sentence-embedding_chinese/">GTE中文通用文本向量表示模型</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/shifterator_text_vis/">Shifterator库 | 词移图分辨两文本用词风格差异</a></p>
</li>
</ul>
<br>
<h3 id="44-大语言模型">4.4 大语言模型</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-02-23-simplet5-one-line-summary/">simpleT5 库 | 根据英文摘要内容生成标题</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-20-how-to-use-llms-tobuild-better-clustering-models/"><strong>以聚类为例 | 使用大语言模型LLM做文本分析</strong></a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="五提取特征机器学习">五、提取特征&amp;机器学习</h2>
<h3 id="51--监督机器学习">5.1  监督机器学习</h3>
<ul>
<li><a href="https://textdata.cn/blog/ml_credit_card_fraud_detection/">机器学习实战 | 信用卡欺诈检测</a></li>
<li><a href="https://textdata.cn/blog/speed_up_sklearn_code_with_sklearnex/">sklearnex库 | 让你的scikit-learn代码加速百倍</a></li>
<li><a href="https://textdata.cn/blog/label_studio_test/">Label-Studio|多媒体数据标注工具</a></li>
<li><a href="https://textdata.cn/blog/doccano_text_anotation/">doccano|为机器学习建模做数据标注</a></li>
</ul>
<br>
<h3 id="52-非监督机器学习">5.2 非监督机器学习</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/hierarchy_dendrogram_tutorial/">使用scipy实现层次聚类分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/svd_in_recommendation_system/">推荐系统与协同过滤、奇异值分解</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/customer_segment_with_kmeans/">实战 | 构建基于客户细分的 K-Means 聚类算法！</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-14-using-lda-to-predict-topic/">代码 | 使用LDA预测文本的话题类型</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-25-tomotopy_is_the_fastest_topic_model/">tomotopy库 | 速度最快的LDA主题模型</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="六可视化">六、可视化</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-25-r-patchwork/">使用patchwork包进行多图排版</a></li>
<li><a href="https://textdata.cn/blog/2023-05-11-bilibili-dongbei-big-brother/">B站 | &ldquo;高铁互殴&quot;视频词云图绘制</a></li>
<li><a href="https://textdata.cn/blog/2023-03-22-bedtime-topic_model_visualization/">可视化 | 睡前消息的科学社会、科学技术、社会化抚养话题可视化</a></li>
<li><a href="https://textdata.cn/blog/whatlies_word2vec/">可视化 | 使用whatlies库可视化词向量</a></li>
<li><a href="https://textdata.cn/blog/2022-11-29-santi-relationship-visualization-with-pyecharts/">可视化 | 绘制《三体》人物关系网络图</a></li>
<li><a href="https://textdata.cn/blog/2023-04-03-visualization-wordcloud-similarity-for-santi/">可视化 | 文本数据分成n等份、词云图、情绪变化趋势、相似度变化趋势</a></li>
<li><a href="https://textdata.cn/blog/2023-05-18-weibo-sentiment-score-line-plot/">可视化 | 微博用户群体情绪随时间变化趋势</a></li>
<li><a href="https://textdata.cn/blog/2023-02-11-chatgpt-plus-for-text-mining/">可视化 | 使用 chatGPT 做词频统计&amp;词云图</a></li>
<li><a href="https://textdata.cn/blog/2023-08-28-best-practice-netflix-data-visualization/"><strong>可视化（推荐） | Netflix 数据可视化最佳实践</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-08-31-data_eda_2021_happiness_and_population/"><strong>可视化 | 2021年幸福指数&amp;人口数据可视化最佳实践</strong></a></li>
<li><a href="https://textdata.cn/blog/pyplutchik_emotion_circle/">可视化 | 使用PyPlutchik库可视化文本的情绪轮(情绪指纹)</a></li>
<li><a href="https://textdata.cn/blog/2023-02-11-pyanimate-create-vis-video/">可视化 | 使用pynimate库绘制动态可视化图</a></li>
<li><a href="https://textdata.cn/blog/2022-12-10-lovelyplots/">可视化 | 使用LovelyPlots库绘制科学论文、论文和演示文稿的可视化图形</a></li>
<li><a href="https://textdata.cn/blog/2023-06-02-r-ggdag/">可视化 | 使用ggdag包绘制有向图</a></li>
<li><a href="https://textdata.cn/blog/2023-04-13-prettymaps/">prettymaps库 | 绘制绝美地图</a></li>
</ul>
<p><br><br></p>
<h2 id="七r语言">七、R语言</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-25-ppsr-predictive-power-sccore/">相关性分析 | 从模型预测出发挖掘更多特征之间的关系</a></li>
<li><a href="https://textdata.cn/blog/2022-09-04-r-ggplot2-scatter/">R语言 | ggplot2简明绘图之散点图</a></li>
<li><a href="https://textdata.cn/blog/2022-09-04-r-ggplot2-histogram/">R语言 | ggplot2简明绘图之直方图</a></li>
<li><a href="https://textdata.cn/blog/2022-09-04-r-ggplot2-ggplotly/">R语言 | ggplot2简明绘图之动态图</a></li>
<li><a href="https://textdata.cn/blog/2022-09-04-posterdown/">R语言 | 使用posterdown包制作学术会议海报</a></li>
<li><a href="https://textdata.cn/blog/2022-09-20-r-ggsci/">R语言 | 使用ggsci包绘制sci风格图表</a></li>
<li><a href="https://textdata.cn/blog/2022-09-20-r-ggplot2-ggpubr/">R语言 | ggpubr包让数据可视化更加优雅</a></li>
<li><a href="https://textdata.cn/blog/2022-09-21-r-easystats-report/">R语言 | 让统计更easy的easystats集合包</a></li>
<li><a href="https://textdata.cn/blog/2022-10-07-r-shiny-reactive/">R语言 | 使用shiny的reactive表达式写应用程序</a></li>
<li><a href="https://textdata.cn/blog/2022-10-07-r-stargazer/">R语言 | 使用stargazer包输出格式化回归结果</a></li>
<li><a href="https://textdata.cn/blog/2022-10-12-r-word2vec/">R语言 | 使用word2vec词向量模型</a></li>
<li><a href="https://textdata.cn/blog/2023-01-20-visualization-of-sentiment-analysis-of-historical-text-data-with-r/">R语言 | 绘制文本数据情感历时趋势图</a></li>
</ul>
<p><br><br></p>
<h2 id="八其他">八、其他</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/causal_inference/">causalinference库 | 使用Python做因果推断</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-26-using-ruptures-to-detect-change-point/">使用 Ruptures 识别时间序列数据中的变化点</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-31-using-poetry-to-manage-your-project-env/">硬核 | 使用Poetry发布Python库到PyPi的方法</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/karateclub_tutorial/">karateclub库 | 计算社交网络中节点的向量</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-21-create-brower-data-label-tools-with-nicegui/">NiceGUI库 | 简单易懂的Web GUI开发包； 可开发数据标注工具、心理学实验工具等</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-09-01-how_to_use_tinytex/">Latex | 为Rmarkdown配置tinytex环境</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-13-add-cls-to-tex-global-enviroment-path/">Latex | 将 .cls 更新到本地 Tex 发行版的搜索路径</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-11-25-faker-generate-test-data/">Faker库 | 生成实验数据</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="九工具">九、工具</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-03-16-free-chatgpt-list/">免费可用的chatGPT镜像站点清单</a></li>
<li><a href="https://textdata.cn/blog/2023-03-26-chatgpt-for-jupyter/">在 Jupyter Notebook 内使用 ChatGPT 服务</a></li>
<li><a href="https://textdata.cn/blog/2023-02-15-how-to-sign-up-the-chatgpt-accout-and-upgrade-to-plus/">如何注册chatGPT账号</a></li>
<li><a href="https://textdata.cn/blog/2023-02-11-credit_card_for_chatgpt-plus/">使用虚拟信用卡，国内用户升级为chatGPT plus会员</a></li>
<li><a href="https://textdata.cn/blog/2023-02-15-write-web-scraper-with-chatgpt/">使用 chatGPT 写 Python 网络爬虫</a></li>
<li><a href="https://textdata.cn/blog/2023-01-18-rath-next-generation-business-intelligence/">Rath | 自动化数据分析工具</a></li>
<li><a href="https://textdata.cn/blog/2023-02-01-v2net-science-network/">科学上网工具v2net</a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><strong>付费视频课程 | Python实证指标构建与文本分析</strong>
<ul>
<li>大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与<a href="https://textdata.cn/blog/2022-05-workshop/7-Python.html">直播课</a>。</li>
<li>如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的<a href="https://textdata.cn/blog/management_python_course">录播课</a>。</li>
<li>如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读<a href="https://textdata.cn/blog/paid_for_service">有偿说明</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>LIST | 可供社科(经管)领域使用的数据集汇总</title>
      <link>https://textdata.cn/blog/datasets_available_for_management_science/</link>
      <pubDate>Wed, 22 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/datasets_available_for_management_science/</guid>
      <description>可供社科(经管)使用的数据集</description>
      <content:encoded><![CDATA[<p>这篇资源帖按照汇总</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 社会
- 企业
- 用户

- 词向量
- 词典
</code></pre></div><p><br><br></p>
<h2 id="社会">社会</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-12-22-renmin-gov-leader-comment-board/"><span style="color: red;"><strong>数据集(付费) | 民网地方领导留言板原始文本(2011-2023.12)</strong></span></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-14-daily-news-dataset/"><span style="color: red;"><strong>新闻数据集(付费) | 含 人民日报/经济日报/光明日报 等 7 家媒体(2023.12.18)</strong></span></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-17-gov-anual-report-dataset/"><strong>数据集(付费) | 国、省、市三级政府工作报告文本(1954-2023)</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-26-cctv1-xwlb-news-text-dataset/"><strong>数据集(付费) | cctv新闻联播文稿数据集</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-05-17-china-200-city-real-estate-policy/">实验数据 | 194城市楼市政策梳理(2010-2022)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-09-03-government-procurement-contract-data/"><strong>数据集(付费) | 288w政府采购合同公告明细数据（2023.09）</strong></a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="企业">企业</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-05-07-china-law-judgment-documents-datasets/"><strong>数据集(付费) | 中国裁判文书网(2010-2021.10)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-12-03-china-mainland-corporate-registration-information/"><strong>数据集(付费) | 2.49亿条中国工商注册企业信息(23.9更新)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/"><strong>数据集(付费) | 3571万条专利申请数据集(1985-2022年)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-12-07-patent-application-dataset-of-listed-company-in-china-a-market/"><strong>数据集(付费) | 上市公司 208 万条专利数据集 (1991-2022)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-12-22-patent-transform-exchange-dataset/"><strong>数据集(付费) |  专利转让数据库(1985-2021)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/"><strong>数据集(付费) | 2001年-2022年A股上市公司年报&amp;管理层讨论与分析</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-04-12-china-poi-datasets/"><strong>数据集(付费) |  3.9G全国POI地点兴趣点数据集</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"><strong>词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-04-17-china-a-market-inquiry-letter-datasets/"><strong>数据集(付费) | 2014年-2021年「问询函」</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-08-11-china-a-market-corporate-social-responsibility-dataste/"><strong>数据集(付费) | 2006年-2022年沪深企业社会责任报告</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-09-08-china-a-share-market-listed-company-earnings-communication-conference/"><strong>数据集(付费) | 84w条业绩说明会问答数据(2005-2023)</strong></a></li>
<li><a href="https://textdata.cn/blog/2022-11-25-senior-manager-resume-dataset/"><strong>数据集(付费) | 90w条中国上市公司高管数据</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-05-17-top-manager-violation/">数据集 | 上市公司高管违规数据(2008-2022)</a></li>
<li><a href="https://textdata.cn/blog/2023-04-26-entrusted-loan-dataset/">数据集 | 07-21年上市公司「委托贷款公告」</a></li>
<li><a href="https://textdata.cn/blog/coporate_social_responsibility_datasets/">数据集 | 企业社会责任报告数据集</a></li>
<li><a href="https://textdata.cn/blog/2022-11-02-27g-python-27g-a-share-market-prospectus/">27G数据集 | 使用Python对27G招股说明书进行文本分析</a></li>
<li><a href="https://textdata.cn/blog/70g_china_market_anunal_report_datasets/">70G数据集 | 上交所定期报告数据集</a></li>
<li><a href="https://textdata.cn/blog/2022-10-21-2007-2021-a-share-reports-dataset/">14G数据集 | 2007-2021年A股上市公司年度报告（txt文件）</a></li>
<li><a href="https://textdata.cn/blog/2022-12-10-1850w-poi-dataset/">1850万条 | 世界地图POI兴趣点数据集</a></li>
<li><a href="https://textdata.cn/blog/2023-10-18-google-local-data/">数据集 | 谷歌地图美国区域内poi、评论信息等信息</a></li>
</ul>
<p><br><br></p>
<h2 id="用户">用户</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-22-1000w-github-developer-dataset/">数据集 | 1000万 Github 用户数据</a></li>
<li><a href="https://textdata.cn/blog/2023-11-22-open-dataset-gharchive-org/">2T数据集 | 使用GH Archive获取Github社区用户数据</a></li>
<li><a href="https://textdata.cn/blog/2023-12-24-instagram-influencer-dataset/">数据集 | 3.3万 Instagram Influencer的 1018万条推文数据</a></li>
<li><a href="https://textdata.cn/blog/yelpdataset_10g/">10G数据集 | YelpDaset酒店管理类数据集</a></li>
<li><a href="https://textdata.cn/blog/2022-12-08-indiegogo-dataset/">1.5G数据集 | 200万条Indiegogo众筹项目信息</a></li>
<li><a href="https://textdata.cn/blog/2022-12-04-kickstarters_dataset/">12G数据集 | 23w条Kickstarter项目信息</a></li>
<li><a href="https://textdata.cn/blog/2023-05-10-100m-bilibili-user-info-dataset/">数据集 | B站/哔哩哔哩 1 亿用户数据</a></li>
<li><a href="https://textdata.cn/blog/2023-03-06-zhihurec-dataset/">数据集 | 80w知乎用户问答数据</a></li>
<li><a href="https://textdata.cn/blog/2023-03-06-bedtime-news-datasets/">数据集 |马前卒工作室 睡前消息文稿汇总</a></li>
</ul>
<p><br><br></p>
<h2 id="词向量">词向量</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/"><strong>词向量(付费) | 使用3751w专利申请数据集按年份(按省份)训练词向量</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/"><strong>词向量(付费) | 使用1985年-2022年专利申请摘要训练word2vec模型</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"> <strong>词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-11-18-show-word-meaning-shift-using-word2vec/">案例分享|  使用裁判文书数据集逐年训练年份词向量</a></li>
<li><a href="https://textdata.cn/blog/embeddings_resource_usage_method/">中文词向量资源汇总 &amp; 使用方法</a></li>
<li><a href="https://textdata.cn/blog/pretained_nlp_models/">NLP资源 | 汽车、金融等9大领域预训练词向量模型下载资源</a></li>
<li><a href="https://textdata.cn/blog/2023-03-08-edgar-w2v-and-corpus/">EDGAR | 25年数据的预训练词向量模型</a></li>
<li><a href="https://textdata.cn/blog/2022-10-16-aligned-word-vectors/">数据集 | 多语言对齐词向量预训练模型</a></li>
</ul>
<p><br><br></p>
<h2 id="词典">词典</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-04-05-chinese-concreteness-dictionary-from-behavior-research-method/">中文心理词典，含具体性、可成象性等指标</a></li>
<li><a href="https://textdata.cn/blog/2023-03-20-nature-six-semantic-dimension-database/">Nature | 通用中英文六维语义情感词典</a></li>
<li><a href="https://textdata.cn/blog/chinese_semantic_kb/">ChineseSemanticKB | 中文语义常用词典</a></li>
<li><a href="https://textdata.cn/blog/2022-11-07-domainwordsdict-liuhuanyong/">DomainWordsDict | 领域词库构建方法与68领域、916万级专业词库分享</a></li>
<li><a href="https://textdata.cn/blog/2022-11-07-financial-invest-merge/">小规模金融并购、投资事件图谱设计概述与数据构成解析</a></li>
<li><a href="https://textdata.cn/blog/2022-09-27-r-ngramr/">Google Books Ngram Viewer显示英文词汇历史使用趋势</a></li>
<li><a href="https://textdata.cn/blog/2022-11-07-chinese-casual-text-datasets/">十万级 | 多领域因果事件对数据集对外开源</a></li>
</ul>
<p><br><br></p>
<h2 id="最后">最后</h2>
<p>数据集和模型资源比较少，各位如果有新资源，欢迎留言分享或者邮箱thunderhit@qq.com联系我。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><strong>付费视频课程 | Python实证指标构建与文本分析</strong>
<ul>
<li>大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与<a href="https://textdata.cn/blog/2022-05-workshop/7-Python.html">直播课</a>。</li>
<li>如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的<a href="https://textdata.cn/blog/management_python_course">录播课</a>。</li>
<li>如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读<a href="https://textdata.cn/blog/paid_for_service">有偿说明</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>天**  |  使用selenium做数据采集</title>
      <link>https://textdata.cn/blog/tian_ya_cha_spider_code/</link>
      <pubDate>Sun, 17 Jul 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/tian_ya_cha_spider_code/</guid>
      <description>天** 数据采集教程</description>
      <content:encoded><![CDATA[<blockquote>
<p>作者: 张延丰</p>
<p>哈工程管工在读博士，擅长数据采集&amp;挖掘。</p>
<p>文末可获取代码下载链接</p>
</blockquote>
<p>马云在接受CNBC（美国消费者新闻与商业频道）采访时提出：“整个世界将变成数据，我认为这还是只是数据时代的开始。新浪潮即将来临，很多就业机会将被夺走。有些人会赶上潮流，变得更加富有和成功。但是对于那些落后的人，未来将是痛苦的。”就小编看来，这种说法在人文社科研究当中也同样适用。在当前数以万计甚至数以十万计研究样本“遍地走”的时代，若我们还拘泥于传统的“小样本”研究（比如样本量为100多的调查问卷数据等），不仅难以跟随时代的脚步，还会逐渐丧失学术竞争力、从而被时代淘汰。那么，究竟该如何获取属于自己的大样本数据呢？今天小编就带大家用selenium库来爬取国内某知名第三方企业信息平台（天**）的企业工商信息。</p>
<p><br><br></p>
<h2 id="一自动打开网站页面">一、自动打开网站页面</h2>
<p>首先，数据爬取的第一步是利用selenium库启动浏览器，打开我们的目标网站。部分代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># @Author  : Jacob-ZHANG</span>
<span class="kn">import</span> <span class="nn">requests</span><span class="o">,</span><span class="nn">base64</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">csv</span><span class="o">,</span><span class="nn">re</span>
<span class="kn">from</span> <span class="nn">selenium</span> <span class="kn">import</span> <span class="n">webdriver</span>
<span class="kn">import</span> <span class="nn">time</span><span class="o">,</span><span class="nn">random</span>

<span class="c1">#1启动浏览器。</span>
<span class="c1">#win</span>
<span class="n">browser</span><span class="o">=</span><span class="n">webdriver</span><span class="o">.</span><span class="n">Chrome</span><span class="p">(</span><span class="n">executable_path</span><span class="o">=</span><span class="s1">&#39;driver/chromedriver.exe&#39;</span><span class="p">)</span>
<span class="c1">#mac</span>
<span class="c1">#browser=webdriver.Chrome(executable_path=&#39;driver/chromedriver&#39;)</span>

<span class="c1">#2加入这个脚本可以避免被识别</span>
<span class="n">browser</span><span class="o">.</span><span class="n">execute_cdp_cmd</span><span class="p">(</span><span class="s2">&#34;Page.addScriptToEvaluateOnNewDocument&#34;</span><span class="p">,</span> <span class="p">{</span>
<span class="s2">&#34;source&#34;</span><span class="p">:</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">     Object.defineProperty(navigator, &#39;webdriver&#39;, {
</span><span class="s2">      get: () =&gt; undefined
</span><span class="s2">    })
</span><span class="s2">   &#34;&#34;&#34;</span> <span class="p">})</span>

<span class="c1">#3延迟10s启动</span>
<span class="n">browser</span><span class="o">.</span><span class="n">implicitly_wait</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="c1">#4利用谷歌浏览器打开目标网页</span>
<span class="n">browser</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;https://pro.xxxxxx.com/searchx&#39;</span><span class="p">)</span>

<span class="c1">#5将窗口最大化</span>
<span class="n">browser</span><span class="o">.</span><span class="n">maximize_window</span><span class="p">()</span>

<span class="c1">#6给网页一些时间加载</span>
<span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="o">...</span>
<span class="o">...</span>
<span class="o">...</span>
</code></pre></div><p>非常简单，如下所示。</p>
<p>需要说明的是：</p>
<ul>
<li>第一，selenium库距今已经有近20年的历史，各类网站、浏览器大多都能对它进行识别，因此为了避免被反爬，我可以通过加入上述脚本（#2）来防止网站的识别。</li>
<li>第二，因为在完成#4的操作，也即利用浏览器打开网页后，显示的网页页面并非是最大化的窗口，这导致页面元素存在难以定位、从而报错的情况，因此我们在此处加上了窗口最大化的操作（#5）。</li>
<li>第三，在以后的操作里，类似#6的代码会频频出现，这主要是因为受限于网速，不得不为网页加载提供更多时间。</li>
</ul>
<p><br><br></p>
<h2 id="二模拟登陆">二、模拟登陆</h2>
<p>天**反扒的第一关便是需要登录才能够查看具体的页面信息。相比于利用复杂JS逆向技术完成登陆而言，利用selenium库模拟人的操作、从而实现网站自动化登陆的做法则显得更为简单易行。从下图来看，我们需要利用selenium库来完成“点击密码登录（切换到密码登录页面，也即下图所示页面）-向账号对话框内输入账号-向密码对话框内输入密码-向验证码对话框内输入验证码-点击登录”等一系列操作后，才能登录到网站的信息页面，获取自己要想的数据。</p>
<p><img loading="lazy" src="img/%e5%a4%a9%e7%9c%bc%e6%9f%a5-%e7%99%bb%e5%bd%95%e9%aa%8c%e8%af%81%e7%95%8c%e9%9d%a2.png" alt=""  />
</p>
<p>对于网站的登录我们提供了以下两种方法：一种是<strong>自动化登录</strong>；另一种则是<strong>手动登录</strong>。</p>
<br>
<h3 id="21-自动化登录">2.1 自动化登录</h3>
<p>首先，我们先来看看较为复杂的<strong>自动化登录</strong>。要想实现网页的自动化登录，其关键在于利用 「外部力量」 来识别验证码并完成导入。具体而言，我们首先需要定位验证码在网页当中的元素位置，其次利用截图软件根据验证码元素位置来截取验证码图片，再次利用外部库对验证码图片进行识别，最后将识别出的验证码录入对话框。自动化登录的具体过程可以分为 <strong>get_code_image函数</strong> 和 <strong>parse_code函数</strong> 两个步骤进行，具体代码如下所示。其中，验证码的解析小编是调用了百度AI的开源库进行的。另外，需要注意的是，利用selenium库打开的登录页面一开始是不显示验证码的，必须向账号框和密码框输入内容以后，它才会显示验证码。因此，对于验证码的识别和录入，我们将它放在了所有操作中的最后部分。</p>
<p><strong>parse_code函数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">parse_code</span><span class="p">():</span>
    <span class="c1">#用百度API解析图片</span>
    <span class="n">request_url</span> <span class="o">=</span> <span class="s2">&#34;https://aip.baidubce.com/rest/2.0/ocr/v1/numbers&#34;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;temp/验证码.png&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;image&#34;</span><span class="p">:</span> <span class="n">img</span><span class="p">}</span>
    <span class="n">access_token</span> <span class="o">=</span> <span class="s1">&#39;24.a7fbbfb9dcab2e1054cc827f09d09234.2592000.1625930266.282335-19004069&#39;</span>

    <span class="n">request_url</span> <span class="o">=</span> <span class="n">request_url</span> <span class="o">+</span> <span class="s2">&#34;?access_token=&#34;</span> <span class="o">+</span> <span class="n">access_token</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;content-type&#39;</span><span class="p">:</span> <span class="s1">&#39;application/x-www-form-urlencoded&#39;</span><span class="p">}</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">request_url</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>
    
    <span class="c1">#得到解析结果</span>
    <span class="n">dictionary</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
    
    <span class="c1">#得到验证码</span>
    <span class="n">yanzhengma</span><span class="o">=</span><span class="n">dictionary</span><span class="p">[</span><span class="s1">&#39;words_result&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;words&#39;</span><span class="p">]</span>
    
    <span class="c1">#录入验证码</span>
    <span class="n">browser</span><span class="o">.</span><span class="n">find_element_by_xpath</span><span class="p">(</span><span class="s1">&#39;//*[@id=&#34;web-content&#34;]/div/div[2]/div[3]/form/div[6]/input&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">send_keys</span><span class="p">(</span><span class="n">yanzhengma</span><span class="p">)</span>
    
    <span class="c1"># 点击登录按钮</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">browser</span><span class="o">.</span><span class="n">find_element_by_xpath</span><span class="p">(</span><span class="s1">&#39;//*[@id=&#34;web-content&#34;]/div/div[2]/div[3]/form/div[8]&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">click</span><span class="p">()</span>

</code></pre></div><br>
<p><strong>parse_code函数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_code_image</span><span class="p">():</span>
    <span class="c1"># 1启动浏览器。</span>
    <span class="n">browser</span> <span class="o">=</span> <span class="n">webdriver</span><span class="o">.</span><span class="n">Chrome</span><span class="p">()</span>
    <span class="c1"># 2加入这个脚本可以避免被识别</span>
    <span class="n">browser</span><span class="o">.</span><span class="n">execute_cdp_cmd</span><span class="p">(</span><span class="s2">&#34;Page.addScriptToEvaluateOnNewDocument&#34;</span><span class="p">,</span> <span class="p">{</span>
        <span class="s2">&#34;source&#34;</span><span class="p">:</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">         Object.defineProperty(navigator, &#39;webdriver&#39;, {
</span><span class="s2">          get: () =&gt; undefined
</span><span class="s2">        })
</span><span class="s2">       &#34;&#34;&#34;</span><span class="p">})</span>
    <span class="c1"># 3延迟10s启动</span>
    <span class="n">browser</span><span class="o">.</span><span class="n">implicitly_wait</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    
    <span class="c1"># 4利用谷歌浏览器打开目标网页</span>
    <span class="n">browser</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;https://pro.xxxxxx.com/searchx&#39;</span><span class="p">)</span>
    
    <span class="c1"># 5将窗口最大化</span>
    <span class="n">browser</span><span class="o">.</span><span class="n">maximize_window</span><span class="p">()</span>
    
    <span class="c1"># 6给网页一些时间加载</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="c1">#将页面从快捷登录切换到密码登录</span>
    <span class="n">browser</span><span class="o">.</span><span class="n">find_element_by_xpath</span><span class="p">(</span><span class="s1">&#39;//*[@id=&#34;web-content&#34;]/div/div[2]/div[3]/div[1]/div[2]&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">click</span><span class="p">()</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    
    <span class="c1">#输入账号</span>
    <span class="n">browser</span><span class="o">.</span><span class="n">find_element_by_xpath</span><span class="p">(</span><span class="s1">&#39;//*[@id=&#34;web-content&#34;]/div/div[2]/div[3]/form/div[2]/input&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">send_keys</span><span class="p">(</span>
        <span class="s1">&#39;******&#39;</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    
    <span class="c1"># 输入密码</span>
    <span class="n">browser</span><span class="o">.</span><span class="n">find_element_by_xpath</span><span class="p">(</span><span class="s1">&#39;//*[@id=&#34;web-content&#34;]/div/div[2]/div[3]/form/div[4]/input&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">send_keys</span><span class="p">(</span><span class="s1">&#39;*******&#39;</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">browser</span><span class="o">.</span><span class="n">save_screenshot</span><span class="p">(</span><span class="s1">&#39;temp/屏幕.png&#39;</span><span class="p">)</span><span class="c1">#截图整个页面】</span>
    
    <span class="c1">#定位验证码x,y坐标</span>
    <span class="n">left_angle</span><span class="o">=</span><span class="n">browser</span><span class="o">.</span><span class="n">find_element_by_xpath</span><span class="p">(</span><span class="s1">&#39;//*[@id=&#34;web-content&#34;]/div/div[2]/div[3]/form/div[6]/img&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">location</span>
    <span class="n">image</span><span class="o">=</span><span class="n">browser</span><span class="o">.</span><span class="n">find_element_by_xpath</span><span class="p">(</span><span class="s1">&#39;//*[@id=&#34;web-content&#34;]/div/div[2]/div[3]/form/div[6]/img&#39;</span><span class="p">)</span>
    
    <span class="c1">#获取验证码的长和宽</span>
    <span class="n">size</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">size</span>
    
    <span class="c1">#设定我们需要截取的位置</span>
    <span class="n">rangle</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">left_angle</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="n">left_angle</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">left_angle</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">size</span><span class="p">[</span><span class="s1">&#39;width&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">230</span><span class="p">),</span>

              <span class="nb">int</span><span class="p">(</span><span class="n">left_angle</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">size</span><span class="p">[</span><span class="s1">&#39;height&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">300</span><span class="p">))</span>
    
    <span class="c1">#打开截图</span>
    <span class="n">open_image</span><span class="o">=</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;temp/屏幕.png&#39;</span><span class="p">)</span>
    
    <span class="c1">#从图片中截取我们需要的的区域</span>
    <span class="n">jietu</span><span class="o">=</span><span class="n">open_image</span><span class="o">.</span><span class="n">crop</span><span class="p">(</span><span class="n">rangle</span><span class="p">)</span>
    <span class="n">jietu</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;temp/验证码.png&#39;</span><span class="p">)</span>
</code></pre></div><p>接下来，我们再来看看如何实现手动登录。相比于自动化登录，手动登录的操作更为简单。具体地，我们只要在完成自动打开网站页面的代码后加入<code>input()函数</code>，然后自己手动向网站的对话框内输入账号、密码、验证码并点击登录，就可以进入到网站的信息页面。</p>
<p><br><br></p>
<h2 id="三获取自己想要的数据">三、获取自己想要的数据</h2>
<p>完成登陆之后，就会跳转到如下页面。然后，大家就可以根据自己的目标继续撰写属于自己的“个性化代码”了。下面，小编以获取31个省市的特定类型的企业数据为例，给大家分享一下自己获取数据的过程。</p>
<p><img loading="lazy" src="img/clip_image005.png" alt=""  />
</p>
<p>其实，代码撰写的逻辑很简单。首先要做的就是先选中我们要爬取的目标城市。下来就是根据自己的需求来定制个性化的筛选标准。以小编自己的需求为例，先通过点击高级模式，向企业名称对话框里输入关键词，比如医院（当然，大家也可以通过限定行业来挑选目标）；然后，去掉机构类型中已勾选的企业，选择事业单位；接下来，勾选全部企业类型；最后点击查看结果。</p>
<p>至此就完成了筛选，得到了满足我们要求的所有企业（见下图1）。接下来，我们要做的就是遍历每一页里的每一家企业，然后获取企业页面信息（见下图2）中自己想要的数据了。</p>
<p><img loading="lazy" src="img/clip_image009.png" alt="img"  />
<img loading="lazy" src="img/clip_image010.png" alt=""  />
</p>
<p>由于后续代码较长，就不在这里一一列举了。  有需要的小伙伴可以在后台留言，然后向xx索取。</p>
<p><br><br></p>
<h2 id="代码下载">代码下载</h2>
<p>对于初学者而言，直接上手可能比较难，建议先收藏本文，待熟练掌握爬虫可以实验本文的代码。</p>
<p>如果想复现本文代码，需熟悉</p>
<ul>
<li>python基础语法</li>
<li>selenium驱动器driver的适配</li>
<li>百度api注册</li>
<li>涉及开发者工具xpath定位</li>
</ul>
<p>本文教程&amp;代码免费分享，但作者时间和精力宝贵，无法做到一一指导，尽请包涵。</p>
<blockquote>
<p>代码链接: <a href="https://pan.baidu.com/s/1VmNoTN8hFRCBI9g770mUCw">https://pan.baidu.com/s/1VmNoTN8hFRCBI9g770mUCw</a> 提取码: ob2j</p>
</blockquote>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>EmoBank | 中文维度情感词典</title>
      <link>https://textdata.cn/blog/chinese_emobank/</link>
      <pubDate>Sat, 16 Jul 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/chinese_emobank/</guid>
      <description>中文情绪银行 (Chinese EmoBank)是由人工标注产生的 中文维度情感词典，含效价valence和唤醒度arousal两个维度。</description>
      <content:encoded><![CDATA[<h2 id="引言">引言</h2>
<p><strong>『中文情绪银行』</strong> (Chinese EmoBank)是由人工标注产生的 <strong>中文维度情感词典</strong>  ，含效价valence和唤醒度arousal两个维度。</p>
<ul>
<li>效价valence，可测量出文本中的积极/消极情感程度。</li>
<li>唤醒度arousal，可测量文本中平静/兴奋状态的程度。</li>
</ul>
<p>该词典包括</p>
<ul>
<li>CVAW(Chinese valence-arousal words)， 5512词</li>
<li>CVAP(Chinese valence-arousal phrases)， 含2998词组</li>
<li>语料CVAS(Chinese valence-arousal sentences) 含2582个单句</li>
<li>语料CVAT(Chinese valence-arousal texts）  2969个句子</li>
</ul>
<p>需要注意该词典是繁体中文词典，经过繁体转简体，已将CVAW嵌入到最新的cntext包。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install --upgrade cntext 
</code></pre></div><br>
<blockquote>
<p>本文图片来源于</p>
<p><a href="http://nlp.innobic.yzu.edu.tw/resources/ChineseEmoBank.html">http://nlp.innobic.yzu.edu.tw/resources/ChineseEmoBank.html</a></p>
</blockquote>
<br>
<h2 id="cvawchinese-valence-arousal-words">CVAW(Chinese valence-arousal words)</h2>
<p><img loading="lazy" src="img/cvaw.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th style="text-align:left">Word</th>
<th style="text-align:left">Valence_Mean</th>
<th style="text-align:left">Arousal_Mean</th>
<th style="text-align:left">Valence_SD</th>
<th style="text-align:left">Arousal_SD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">乏味</td>
<td style="text-align:left">3.4</td>
<td style="text-align:left">3.0</td>
<td style="text-align:left">0.800</td>
<td style="text-align:left">1.414</td>
</tr>
<tr>
<td style="text-align:left">放鬆</td>
<td style="text-align:left">6.2</td>
<td style="text-align:left">2.0</td>
<td style="text-align:left">0.748</td>
<td style="text-align:left">0.894</td>
</tr>
<tr>
<td style="text-align:left">勝利</td>
<td style="text-align:left">7.8</td>
<td style="text-align:left">7.2</td>
<td style="text-align:left">0.748</td>
<td style="text-align:left">1.166</td>
</tr>
<tr>
<td style="text-align:left">痛苦</td>
<td style="text-align:left">2.4</td>
<td style="text-align:left">6.8</td>
<td style="text-align:left">0.490</td>
<td style="text-align:left">0.748</td>
</tr>
</tbody>
</table>
<br>
<h2 id="cvapchinese-valence-arousal-phrases-">CVAP(Chinese valence-arousal phrases )</h2>
<p><img loading="lazy" src="img/cvap.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th style="text-align:left">Modifier Type</th>
<th style="text-align:left">Phrase</th>
<th style="text-align:left">Valence_Mean</th>
<th style="text-align:left">Arousal_Mean</th>
<th style="text-align:left">Valence_SD</th>
<th style="text-align:left">Arousal_SD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">deg</td>
<td style="text-align:left">十分有趣</td>
<td style="text-align:left">8.222</td>
<td style="text-align:left">7.063</td>
<td style="text-align:left">0.533</td>
<td style="text-align:left">0.390</td>
</tr>
<tr>
<td style="text-align:left">mod</td>
<td style="text-align:left">應該開心</td>
<td style="text-align:left">5.986</td>
<td style="text-align:left">5.350</td>
<td style="text-align:left">0.242</td>
<td style="text-align:left">0.456</td>
</tr>
<tr>
<td style="text-align:left">neg</td>
<td style="text-align:left">不喜歡</td>
<td style="text-align:left">3.033</td>
<td style="text-align:left">5.788</td>
<td style="text-align:left">0.481</td>
<td style="text-align:left">0.605</td>
</tr>
<tr>
<td style="text-align:left">neg_deg</td>
<td style="text-align:left">沒有太難過</td>
<td style="text-align:left">4.478</td>
<td style="text-align:left">4.675</td>
<td style="text-align:left">0.413</td>
<td style="text-align:left">0.538</td>
</tr>
</tbody>
</table>
<br>
<h2 id="cvaschinese-valence-arousal-sentences">CVAS(Chinese valence-arousal sentences)</h2>
<p><img loading="lazy" src="img/cvas.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th style="text-align:left">Text</th>
<th style="text-align:left">Valence_Mean</th>
<th style="text-align:left">Arousal_Mean</th>
<th style="text-align:left">Valence_SD</th>
<th style="text-align:left">Arousal_SD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">這是我觀賞過的最令人驚歎的演出。</td>
<td style="text-align:left">7.000</td>
<td style="text-align:left">7.750</td>
<td style="text-align:left">0.000</td>
<td style="text-align:left">0.433</td>
</tr>
<tr>
<td style="text-align:left">簡直是人生惡夢的開端。</td>
<td style="text-align:left">2.600</td>
<td style="text-align:left">6.750</td>
<td style="text-align:left">0.490</td>
<td style="text-align:left">0.829</td>
</tr>
<tr>
<td style="text-align:left">從小我經常覺得現實很無聊。</td>
<td style="text-align:left">3.667</td>
<td style="text-align:left">4.333</td>
<td style="text-align:left">0.471</td>
<td style="text-align:left">0.471</td>
</tr>
<tr>
<td style="text-align:left">過去他們很輕鬆地賺錢。</td>
<td style="text-align:left">5.667</td>
<td style="text-align:left">4.000</td>
<td style="text-align:left">1.247</td>
<td style="text-align:left">0.816</td>
</tr>
</tbody>
</table>
<br>
<h2 id="cvatchinese-valence-arousal-texts">CVAT(Chinese valence-arousal texts)</h2>
<p><img loading="lazy" src="img/cvat.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th style="text-align:left">Text</th>
<th style="text-align:left">Valence_Mean</th>
<th style="text-align:left">Arousal_Mean</th>
<th style="text-align:left">Valence_SD</th>
<th style="text-align:left">Arousal_SD</th>
<th style="text-align:left">Category</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">很多車主抱怨新車怠速抖動嚴重&mdash;-冷車時更嚴重。</td>
<td style="text-align:left">3.250</td>
<td style="text-align:left">5.667</td>
<td style="text-align:left">1.090</td>
<td style="text-align:left">1.054</td>
<td style="text-align:left">Car</td>
</tr>
<tr>
<td style="text-align:left">房間裏黴味，煙味撲鼻，沒有窗戶通風，骯髒的地毯上的斑斑點點的污蹟，令人觸目驚心。</td>
<td style="text-align:left">1.889</td>
<td style="text-align:left">6.875</td>
<td style="text-align:left">0.737</td>
<td style="text-align:left">0.927</td>
<td style="text-align:left">Hotel</td>
</tr>
<tr>
<td style="text-align:left">CPU顯卡也完全夠用，接口也非常齊全，總體來說很滿意！</td>
<td style="text-align:left">7.143</td>
<td style="text-align:left">5.000</td>
<td style="text-align:left">0.350</td>
<td style="text-align:left">0.816</td>
<td style="text-align:left">Laptop</td>
</tr>
<tr>
<td style="text-align:left">飛安帶來更多保障，也提供旅客更安心的服務品質。</td>
<td style="text-align:left">7.000</td>
<td style="text-align:left">4.222</td>
<td style="text-align:left">0.535</td>
<td style="text-align:left">1.133</td>
<td style="text-align:left">News</td>
</tr>
</tbody>
</table>
<br>
<h2 id="文献">文献</h2>
<p>如果用到Chinese EmoBank词典，请注明出处。</p>
<p>Lung-Hao Lee, Jian-Hong Li and Liang-Chih Yu, &ldquo;<a href="https://dl.acm.org/doi/pdf/10.1145/3489141">Chinese EmoBank: Building Valence-Arousal Resources for Dimensional Sentiment Analysis,</a>&rdquo; <em>ACM Trans. Asian and Low-Resource Language Information Processing</em>, vol. 21, no. 4, article 65, 2022.</p>
<p>Liang-Chih Yu, Lung-Hao Lee, Shuai Hao, Jin Wang, Yunchao He, Jun Hu, K. Robert Lai, and Xuejie Zhang. 2016. &ldquo;<a href="http://www.aclweb.org/anthology/N16-1066.pdf">Building Chinese affective resources in valence-arousal dimensions.</a> In <em>Proceedings of NAACL/HLT-16</em>, pages 540-545.</p>
<br>
<h2 id="代码">代码</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;ChineseEmoBank.pkl&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;Referer-1&#39;: &#39;Lee, Lung-Hao, Jian-Hong Li, and Liang-Chih Yu. &#34;Chinese EmoBank: Building Valence-Arousal Resources for Dimensional Sentiment Analysis.&#34; Transactions on Asian and Low-Resource Language Information Processing 21, no. 4 (2022): 1-18.&#39;,
 
 &#39;Referer-2&#39;: &#39;Liang-Chih Yu, Lung-Hao Lee, Shuai Hao, Jin Wang, Yunchao He, Jun Hu, K. Robert Lai, and Xuejie Zhang. 2016. &#34;Building Chinese affective resources in valence-arousal dimensions. In Proceedings of NAACL/HLT-16, pages 540-545.&#39;,
 
 &#39;Desc&#39;: &#39;Chinese Sentiment Dictionary, includes 「valence」「arousal」. In cntext, we only take single word into account, ignore phrase.&#39;,
 
 &#39;ChineseEmoBank&#39;:       word  valence  arousal
 0     不可思议      5.4      7.2
 1       不平      3.6      5.8
 2       不甘      3.2      6.4
 3       不安      3.8      5.4
 4       不利      3.6      5.6
 ...    ...      ...      ...
 5505    黏闷      2.8      5.6
 5506    黏腻      2.7      5.8
 5507    艳丽      5.8      4.5
 5508    苗条      6.7      3.8
 5509    修长      7.0      4.5
</code></pre></div><br>
<p>ChineseEmoBank的CVAW词典(Chinese valence-arousal words)原有 5512词，经过繁体转简体处理，得到5510个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;ChineseEmoBank.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;ChineseEmoBank&#39;</span><span class="p">]</span>
<span class="n">diction_df</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<p>测量一段文本的valence和arousal，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;很多车主抱怨新车怠速抖动严重---冷车时更严重。&#39;</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_weight</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Help on function sentiment_by_weight in module cntext.stats:

sentiment_by_weight(text, diction, params, lang=&#39;english&#39;)
    calculate the occurrences of each sentiment category words in text;
    the complex influence of intensity adverbs and negative words on emotion is not considered.
    :param text:  text sring
    :param diction:  sentiment dictionary dataframe with weight.；
    :param params:  set sentiment category weight, such as params=[&#39;valence&#39;, &#39;arousal&#39;]
    :param lang: &#34;chinese&#34; or &#34;english&#34;; default lang=&#34;english&#34;
    
    :return:
</code></pre></div><br>
<p>计算文本text中chinese_emobank词两维度的汇总得分，得到valence、arousal、word_num</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;很多车主抱怨新车怠速抖动严重---冷车时更严重。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_weight</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="p">,</span> 
                       <span class="n">diction</span> <span class="o">=</span> <span class="n">diction_df</span><span class="p">,</span>
                       <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;valence&#39;</span><span class="p">,</span> <span class="s1">&#39;arousal&#39;</span><span class="p">],</span>
                       <span class="n">lang</span> <span class="o">=</span> <span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;valence&#39;: 14.8, 
&#39;arousal&#39;: 24.8, 
&#39;word_num&#39;: 13}
</code></pre></div><ul>
<li>valence是句子中各个chinese_emobank词valence得分的加总。</li>
<li>arousal是句子中各个chinese_emobank词arousal得分的加总。</li>
<li>word_num是句子中的词语数(含标点符号)，短文本的情况下，word_num会不太准确，长文本情况下无限接近真实词语数。</li>
</ul>
<p>需要注意，文本越长，valence和arousal指标应该会越大。使用这两个指标时，需要结合word_num进行均值处理，即</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Valence = valence/word_num

Arousal = arousal/word_num
</code></pre></div><p>这里未做均值处理，尽量保留文本的原始信息。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>chinese-converter | 中文繁简互换Python库</title>
      <link>https://textdata.cn/blog/chinese_converter/</link>
      <pubDate>Mon, 11 Jul 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/chinese_converter/</guid>
      <description>中文繁简互换</description>
      <content:encoded><![CDATA[<p>网上有一些繁体中文资源不能直接利用，通过chinese-convertor库，我们可以进行中文繁简互换。</p>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install chinese-converter
</code></pre></div><br>
<h2 id="快速上手">快速上手</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">chinese_converter</span>

<span class="n">chinese_converter</span><span class="o">.</span><span class="n">to_traditional</span><span class="p">(</span><span class="s2">&#34;中国&#34;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">中國
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">chinese_converter</span><span class="o">.</span><span class="n">to_simplified</span><span class="p">(</span><span class="s2">&#34;中國&#34;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">中国
</code></pre></div><br>
<br>
<h2 id="长期征稿">长期征稿</h2>
<div style="text-align: center;">
<figure >
    <a href="https://textdata.cn/blog/call_for_paper/">
        <img src="/images/blog/call_for_paper.png" width="100%" />
    </a>
    <figcaption><small><i>点击了解投稿</i></small></figcaption>
</figure>
</div>
<br>
<h2 id="招募小伙伴">招募小伙伴</h2>
<div style="text-align: center;">
<figure >
    <a href="https://textdata.cn/blog/we_need_you/">
        <img src="/images/blog/we_need_you.png" width="100%" />
    </a>
    <figcaption><small><i>点击加入我们</i></small></figcaption>
</figure>
</div>
<br>
<h2 id="了解课程">了解课程</h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://textdata.cn/blog/management_python_course/">点击进入详情页</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>2022暑期工作坊 | Python实证指标构建与文本分析</title>
      <link>https://textdata.cn/blog/2022_summer_workshop/</link>
      <pubDate>Sun, 10 Jul 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/2022_summer_workshop/</guid>
      <description>在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：网络爬虫技术 解决如何从网络世界中高效地采集数据？文本分析技术 解决如何从杂乱的文本数据中抽取实证指标(情绪、不确定、态度、认知等变量)</description>
      <content:encoded><![CDATA[<h2 id="课程介绍">课程介绍</h2>
<p>在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！</p>
<p>在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：</p>
<ul>
<li>网络爬虫技术 解决如何从网络世界中高效地采集数据？</li>
<li>文本分析技术 解决如何从杂乱的文本数据中抽取实证指标(情绪、不确定、态度、认知等变量)？</li>
</ul>
<br>
<h3 id="授课方式">授课方式</h3>
<ul>
<li>线上直播（电脑端与手机端皆可播放，回放十天）。</li>
<li>开课前会建立讲师微信群并发布最新学习资料，群聊长期有效，助教全程跟随。</li>
<li>第一时段-在线讲座 2022.8.16~17  上午&amp;下午</li>
<li>第二时段-论文指导 2022.8.24 下午
<ul>
<li>Python实证指标构建与文本分析课程结束一周后</li>
<li>半天时间</li>
<li>学员利用一周的时间用Python收集、整理数据、文本分析，撰写一个初步的论文与老师交流，老师一对一地指导如何修改文本数据挖掘的实证论文。</li>
</ul>
</li>
</ul>
<h3 id="费用与优惠">费用与优惠</h3>
<ul>
<li>报名总费用2500元（包含在线讲座费用2000元、论文指导费用500元、课后长期答疑以及全部讲义以及代码等资料）</li>
<li>个人报名优惠：报名两科9折；三科8折；四科及以上7.5折；老朋友9折；凭本人学生证报名可再减200元/人。</li>
<li>团队报名优惠：三人成团及以上9折；五人成团及以上8折。</li>
<li>7月10日之前报名可享每人优惠100元。</li>
<li>各项优惠叠加不超过总价的7.5折。</li>
</ul>
<h3 id="报名方式">报名方式</h3>
<ul>
<li>从即日起可加老师微信咨询与报名。</li>
<li>17816181460（同微信）（汪老师）</li>
</ul>
<p><img loading="lazy" src="img/wechat.png" alt=""  />
</p>
<h3 id="缴费方式">缴费方式</h3>
<ul>
<li>扫码付款</li>
<li>添加汪老师微信获取,支持公务卡支付</li>
</ul>
<h3 id="对公转账">对公转账</h3>
<ul>
<li>单位：杭州国商智库信息技术服务有限公司</li>
<li>开户银行：中国银行杭州大学城支行</li>
<li>银行账户：6232636200100260588</li>
</ul>
<p><br><br></p>
<h2 id="内容安排">内容安排</h2>
<h3 id="一python语法入门">一、Python语法入门</h3>
<ul>
<li>Python跟英语一样是一门语言</li>
<li>数据类型之字符串</li>
<li>数据类型之列表元组集合</li>
<li>数据类型之字典</li>
<li>数据类型之布尔值、None</li>
<li>逻辑语句(if&amp;for&amp;tryexcept)</li>
<li>列表推导式</li>
<li>理解函数</li>
<li>常用的内置函数</li>
<li>os路径库</li>
<li>内置库csv文件库</li>
<li>常见错误汇总</li>
</ul>
<br>
<h3 id="二数据采集">二、数据采集</h3>
<ul>
<li>网络爬虫原理</li>
<li>寻找网址规律</li>
<li>获取网页-requests库</li>
<li>pyquery库解析html网页</li>
<li>案例 1：豆瓣小说</li>
<li>json库解析json网页</li>
<li>案例 2：豆瓣电影</li>
<li>案例 3：微博</li>
<li>案例 4： 批量下载文档、多媒体文件</li>
<li>案例 5：上市公司定期报告pdf批量下载</li>
<li>区分动态网站与静态网站</li>
</ul>
<br>
<h3 id="三文本分析入门">三、文本分析入门</h3>
<ul>
<li>文本分析在经管领域中的应用</li>
<li>读取文件中的数据(txt、pdf、docx、xlsx、csv)</li>
<li>数据清洗re库-从文本中抽取姓名、年龄、电话、数字等各种信息</li>
<li>案例 6：如何将多个文件中的数据整理到一个excel中</li>
<li>中文jieba分词</li>
<li>案例 7：词频统计、制作词云图</li>
<li>案例 8：共现法扩展情感词典</li>
<li>案例 9：词向量word2vec扩展情感词典</li>
<li>案例 10：中文情感分析(无权重词典法)</li>
<li>数据分析pandas库快速入门</li>
<li>案例 11：使用pandas对excel中的文本进行情感分析</li>
<li>案例 12: 计算地图中两点(经纬度)距离及方位角</li>
</ul>
<br>
<h3 id="四机器学习">四、机器学习</h3>
<ul>
<li>了解机器学习</li>
<li>理解特征工程</li>
<li>文本特征工程-将文本转化为机器可处理的数字向量</li>
<li>认识词袋法、one-hot、Tf-Idf、word2vec</li>
<li>案例 13：使用tf-idf进行情感分析（有权重词典法）</li>
<li>案例 14： 使用标注工具对文本数据进行标注</li>
<li>案例 15：在线评论文本分类</li>
<li>文本相似性计算</li>
<li>案例 16：使用文本相似性识别变化(政策连续性)</li>
<li>案例 17：Kmeans聚类算法</li>
<li>案例 18：LDA话题模型</li>
<li>案例 19: 识别图片中的文本</li>
<li>python爬虫、文本分析、机器学习等技术在论文中的应用赏析</li>
</ul>
<br>
<h3 id="五词嵌入与认知">五、词嵌入与认知</h3>
<ul>
<li>词嵌入</li>
<li>豆瓣影评-gensim导入词向量模型</li>
<li>认知偏见(刻板印象)</li>
<li>总结: 文本分析在经管领域中的应用概述</li>
</ul>
<p><br><br></p>
<h2 id="文本分析应用案例">文本分析应用案例</h2>
<p>参照两篇论文的摘要，可以通过场景化等的方式帮助我们迅速理解上面两个问题。摘要部分的加粗内容是论文用到的分析技术，在我们的课程中均有与之对应的知识点和代码。</p>
<p><strong>王伟,陈伟,祝效国,王洪伟.众筹融资成功率与语言风格的说服性——基于Kickstarter的实证研究[J].管理世界,2016(05):81-98.</strong></p>
<blockquote>
<p>摘要：众筹融资效果决定着众筹平台的兴衰。众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。</p>
<p>然后，借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。</p>
<p>最后，建立语言说服风格对项目筹资影响的计量模型，并对 Kickstarter 平台上的 128345 个项目进行实证分析。总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。</p>
</blockquote>
<br>
<p><a href="https://textdata.cn/blog/text_mining_in_2021_management_world/">胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.</a></p>
<blockquote>
<p>在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间 导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现，年报 MD&amp;A 中披露的“短期视域” 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴。</p>
</blockquote>
<br>
<p><strong>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</strong></p>
<blockquote>
<p>摘要：尽管移动应用程序市场的增长为移动应用程序开发人员创新提供了巨大的市场机会和经济诱因，但它也不可避免地刺激了模仿者开发盗版软件。原始应用的从业人员和开发人员声称，模仿者窃取了原始应用的想法和潜在需求，并呼吁应用平台对此类模仿者采取行动。令人惊讶的是，很少有严格的研究来分析模仿者是否以及如何影响原始应用的需求。</p>
<p>进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原始或模仿者并检测两种模仿者的方法：欺骗性和非欺骗性。</p>
<p>根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的5,141个开发人员的10,100个动作游戏应用程序样本中，模仿应用程序对原始应用程序需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原件的需求。</p>
<p>结果表明，从总体上讲，模仿者对原始移动应用程序需求的影响在统计上是微不足道的。我们的研究通过提供一种识别模仿者的方法，并提供模仿者对原始应用需求的影响的证据，为越来越多的移动应用消费文献做出了贡献。</p>
</blockquote>
<br>
<p><strong>Markowitz, D. M., &amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).</strong></p>
<blockquote>
<p>处理流畅性等元认知框架通常表明人们对简单和通用的语言的反应比复杂和技术性语言更有利。与复杂的信息相比，人们更容易处理简单和非技术性的信息，因此会更多地与目标进行互动。在涵盖 12 个现场样本（总 n = 1,064,533）的两项研究中，我们通过展示人们在付出时间和注意力时更多地使用非技术语言（例如，简单的在线语言往往会获得更多社交信息）来建立并复制这种越简单越好的现象订婚）。然而，人们在捐款时会对复杂的语言做出反应（例如，慈善捐赠活动和赠款摘要中的复杂语言往往会收到更多的钱）。这一证据表明，人们根据时间或金钱目标以不同的方式使用复杂语言的启发式方法。这些结果强调语言是社会和心理过程的镜头，以及大规模测量文本模式的计算方法。</p>
</blockquote>
<br>
<h2 id="文献汇总">文献汇总</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html
[2]沈艳,陈赟,黄卓．文本大数据分析在经济学和金融学中的应用：一个文献综述[EB/OL].http://www.ccer.pku.edu.cn/yjcg/tlg/242968.htm,2018-11-19
[3]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.*管理世界*.2016;5:81-98.
[4]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[5]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
[6]Kenneth Benoit. July 16, 2019. “Text as Data: An Overview.” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. Handbook of Research Methods in Political Science and International Relations. Thousand Oaks: Sage.
[7]Loughran T, McDonald B. Textual analysis in accounting and finance: A survey[J]. *Journal of Accounting Research*, 2016, 54(4): 1187-1230. Author links open overlay panelComputational socioeconomics
[8]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &#34;Uniting the tribes: Using text for marketing insight.&#34; *Journal of Marketing* 84, no. 1 (2020): 1-25.
[9]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. &#34;A review of best practice recommendations for text analysis in R (and a user-friendly app).&#34; *Journal of Business and Psychology* 33, no. 4 (2018): 445-459.
[10]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. &#34;Lazy prices.&#34; *The Journal of Finance* 75, no. 3 (2020): 1371-1415.
[11]孟庆斌, 杨俊华, 鲁冰. 管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J]. *中国工业经济*, 2017 (12): 132-150.
[12]Wang, Quan, Beibei Li, and Param Vir Singh. &#34;Copycats vs. Original Mobile Apps: A Machine Learning Copycat-Detection Method and Empirical Analysis.&#34; *Information Systems Research* 29.2 (2018): 273-291.
[13]Hoberg, Gerard, and Gordon Phillips. 2016, Text-based network industries and endogenous product differentiation,?*Journal of Political Economy* 124, 1423-1465
[14]Loughran, Tim, and Bill McDonald. &#34;When is a liability not a liability? Textual analysis, dictionaries, and 10‐Ks.&#34; *The Journal of Finance* 66, no. 1 (2011): 35-65.
[15]Fairclough, Norman. 2003. Analysing discourse: Textual analysis for social research (Psychology Press)
[16]Grimmer, Justin, and Brandon M Stewart. 2013, Text as data: The promise and pitfalls of automatic content analysis methods for political texts, *Political analysis*21, 267-297.
[17]Markowitz, D. M., &amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).
[18]Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” Journal of Consumer Research 47, no. 5 (2021): 787-806.
[19]Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Asent库 | 英文文本数据情感分析</title>
      <link>https://textdata.cn/blog/asent_sentiment_analysis/</link>
      <pubDate>Sun, 10 Jul 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/asent_sentiment_analysis/</guid>
      <description>使用Python做英文情感分析，考虑否定词、程度副词对情感词的修饰作用。</description>
      <content:encoded><![CDATA[<p>Asent 是一个新的Python情感分析库， 依据情感词典，按照一定的规则，可用于评判词语、句子、文档的情感信息(正、负)。</p>
<p>目前与情感有关的规则有</p>
<ul>
<li>否定（即“不高兴”）</li>
<li>加强词（“非常高兴”）</li>
<li>对比共轭（即“但是”）</li>
<li>其他强调标记，如感叹号、大小写和问号。</li>
</ul>
<p>Asent目前仅支持<code>英语、丹麦、挪威、瑞典4种语言</code>。</p>
<br>
<h2 id="安装配置">安装配置</h2>
<p>学习课程之前，需要先下载并配置spacy模型， <a href="https://github.com/explosion/spacy-models/releases">https://github.com/explosion/spacy-models/releases</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="n">spacy</span><span class="o">==</span><span class="mf">3.2.0</span>
<span class="n">pip3</span> <span class="n">install</span> <span class="n">asent</span><span class="o">==</span><span class="mf">0.4.2</span>

<span class="c1">#下载en_core_web_lg-3.3.0-py3-none-any.whl到桌面</span>
<span class="c1">#下载链接: https://pan.baidu.com/s/13hFWFjy9uRxzC-9lqrp7SQ 提取码: em8l </span>

<span class="c1">#然后使用如下安装命令</span>
<span class="n">pip3</span> <span class="n">install</span> <span class="n">Desktop</span><span class="o">/</span><span class="n">en_core_web_lg</span><span class="o">-</span><span class="mf">3.2.0</span><span class="o">-</span><span class="n">py3</span><span class="o">-</span><span class="n">none</span><span class="o">-</span><span class="nb">any</span><span class="o">.</span><span class="n">whl</span>
</code></pre></div><br>
<h2 id="快速上手">快速上手</h2>
<p>以下将带您逐步了解情绪是如何计算的。</p>
<p>首先，我们需要一个 spaCy 管道，并且我们需要向其中添加 asent 管道。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">asent</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># load spacy pipeline</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;en_core_web_lg&#34;</span><span class="p">)</span>

<span class="c1"># add the rule-based sentiment model</span>
<span class="n">nlp</span><span class="o">.</span><span class="n">add_pipe</span><span class="p">(</span><span class="s2">&#34;asent_en_v1&#34;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>&lt;asent.component.Asent at 0x7fd6b3243130&gt;
</code></pre>
<br>
<h2 id="效价和极性">效价和极性</h2>
<p>如下所示， token的效价信息来自于人工标注的词典。例如<code>I am not very happy</code>中词语<code>happy</code>的人类情感评分是2.7。</p>
<p><img loading="lazy" src="img/token_polarity.png" alt=""  />
</p>
<p>首先我们查看每个词语对应的效价。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&#34;I am not very happy.&#34;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="s2">&#34;</span><span class="se">\t</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">valence</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>I 	 0.0
am 	 0.0
not 	 0.0
very 	 0.0
happy 	 2.7
. 	 0.0
</code></pre>
<p>在该语境中， <code>happy</code>前面有否定词not修饰，所以情感极性方面应该被看做消极的。一般否定词和副词可以将形容词的情感进行反转和放大(缩小)。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>polarity=0.0 token=I span=I
polarity=0.0 token=am span=am
polarity=0.0 token=not span=not
polarity=0.0 token=very span=very
polarity=-2.215 token=happy span=not very happy
polarity=0.0 token=. span=.
</code></pre>
<p>注意到， 词语在<code>happy</code>拥有-2.215的极性分，该分是由<code>not very happy</code>确定的。</p>
<br>
<h2 id="可视化">可视化</h2>
<p>asent拥有多种情感极性可视化的方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">asent</span><span class="o">.</span><span class="n">visualize</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&#34;prediction&#34;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/fig1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">asent</span><span class="o">.</span><span class="n">visualize</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&#34;analysis&#34;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/fig2.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">sents</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="p">)</span>
</code></pre></div><pre><code>neg=0.391 neu=0.609 pos=0.0 compound=-0.4964 span=I am not very happy.
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span>
</code></pre></div><pre><code>DocPolarityOutput(neg=0.391, neu=0.609, pos=0.0, compound=-0.4964)
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&#34;I am not very happy.I am very very happy.It is awesome!!&#34;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;doc2情感极性信息: &#39;</span><span class="p">,</span> <span class="n">doc2</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;doc2情感得分:&#39;</span><span class="p">,</span> <span class="n">doc2</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="o">.</span><span class="n">compound</span><span class="p">)</span>
</code></pre></div><pre><code>doc2情感极性信息:  neg=0.13 neu=0.536 pos=0.333 compound=0.2794

doc2情感得分: 0.279353567721562
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#每个句子的情感极性信息</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">doc2</span><span class="o">.</span><span class="n">sents</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="p">)</span>
</code></pre></div><pre><code>neg=0.391 neu=0.609 pos=0.0 compound=-0.4964 span=I am not very happy.
neg=0.0 neu=0.539 pos=0.461 compound=0.6453 span=I am very very happy.
neg=0.0 neu=0.461 pos=0.539 compound=0.6892 span=It is awesome!!
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#每个句子的情感得分</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">doc2</span><span class="o">.</span><span class="n">sents</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="o">.</span><span class="n">compound</span><span class="p">)</span>
</code></pre></div><pre><code>-0.4964238981617178
0.6452764659402158
0.689208135386188
</code></pre>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>视频分享| Python数据挖掘与金融科技 </title>
      <link>https://textdata.cn/blog/fintech_quant_with_python/</link>
      <pubDate>Fri, 24 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/fintech_quant_with_python/</guid>
      <description>应云南大学管理学院邀请，参加第一届中国研究生金融科技创新大赛讲座。做Python文本数据挖掘在金融科技中的应用。</description>
      <content:encoded><![CDATA[<blockquote>
<p>第一届中国研究生金融科技创新大赛讲座</p>
<p>2022/06/24 13:43</p>
<p>录制文件：https://dwz.win/ayS8</p>
</blockquote>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1xW4y1r7L1&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<h1 id="另类数据与投资算法">另类数据与投资算法</h1>
<blockquote>
<p>信息通信技术的创新、互联网和移动终端的普及，产生了了大量的区别于 传统财务数据的新型数据，这类非财务数据具有数据量大、实时性高、颗粒度细及“原始”等特点，影响着资本市场，在投资领域的应用受到了越来越多的关注。投资者可以用<strong>较低的成本</strong>获取大量的数据和信息，对这类信息进行筛选、分析，辅助制定投资决策。</p>
<p>能否选择一种尚未在资本市场广泛使用的另类数据，利用合适的算法把该数据应用于 A 股市场投资当中，并寻找合适的算法解决方案，研究其在投资中的价值，并构建出可行性的投资方案？</p>
</blockquote>
<br>
<p><img loading="lazy" src="img/unstructrueddata.png" alt=""  />
</p>
<br>
<br>
<h2 id="另类数据alternative-data">另类数据alternative data</h2>
<p>大数据思维， 快、多、大、异。</p>
<p>另类大数据产生的更多更快，与传统指标相关性小，能提供更多的信息增益。</p>
<p>另类数据alternative data主要包含以下三种:</p>
<table>
<thead>
<tr>
<th>另类数据</th>
<th>包括</th>
<th>结构化</th>
<th>类型</th>
<th>python技术</th>
</tr>
</thead>
<tbody>
<tr>
<td>个人产生的数据</td>
<td>社交媒体帖子、产品评论、互联网搜索趋势等</td>
<td>非结构</td>
<td>网页</td>
<td>爬虫</td>
</tr>
<tr>
<td>由业务流程产生的数据</td>
<td>公司工商数据、专利数据、尾气数据、招聘数据、商业交易、事件数据、招标数据、阿里巴巴、京东、美团等电商平台数据、app排行榜、直播和搜索指数数据等</td>
<td>结构化</td>
<td>数字</td>
<td>爬虫</td>
</tr>
<tr>
<td>传感器产生的数据</td>
<td>卫星图像数据、行人和车辆流量、船舶位置等，地图数据。</td>
<td>非结构</td>
<td>图像</td>
<td>图片分析</td>
</tr>
<tr>
<td>第三方数据</td>
<td>分析师研报情感数据、一致性预期。</td>
<td>结构</td>
<td>数字</td>
<td>付费</td>
</tr>
</tbody>
</table>
<p>国内提供另类数据的开源网站有:</p>
<ul>
<li><a href="https://tushare.pro/">tushare</a> 付费</li>
<li><a href="https://www.akshare.xyz/">akshare</a> 免费</li>
</ul>
<br>
<br>
<h2 id="文本">文本</h2>
<p>文化研究之父斯图亚特·霍尔（Stuart Hall）在《电视话语中的编码和解码》（<em>Encoding and decoding inthe television discourse</em>）一文中提出了“<strong>编码解码</strong>”理论。</p>
<ul>
<li><strong>编码（encoding）</strong>，信息传播者将所传递的讯息、意图或观点，转化为具有特定规则的代码。</li>
<li><strong>解码（decoding）</strong>，信息接受者，将上述代码按特定规则进行解读。</li>
</ul>
<p>信息传播学的编码解码理论</p>
<p><img loading="lazy" src="img/SenderReceiver.png" alt=""  />
<img loading="lazy" src="img/consumer_org_society.png" alt=""  />
</p>
<br>
<table>
<thead>
<tr>
<th>角度</th>
<th>解释</th>
<th>难度</th>
<th>python库</th>
</tr>
</thead>
<tbody>
<tr>
<td>信息检索</td>
<td>新闻咨询中是否出现某类信息(某类词)</td>
<td>低</td>
<td>re、jieba</td>
</tr>
<tr>
<td><strong>情感分析</strong></td>
<td>文本中正面词与负面词含量的对比</td>
<td>低</td>
<td>jieba、nltk</td>
</tr>
<tr>
<td>文本相似度</td>
<td>两文本向量化后的cosine余弦值的</td>
<td>中</td>
<td>jieba、scikit-learn</td>
</tr>
<tr>
<td>文本分类</td>
<td>标注数据，使用文本数据做类别预测(利好、利空)</td>
<td>中</td>
<td>scikit-learn</td>
</tr>
<tr>
<td>词向量</td>
<td>- 不同主体对同一概念的认知(偏见、刻板印象)等。<br><br />- 同一主体对不同概念的认知。</td>
<td>高</td>
<td>gensim</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="文本相似度提前预警股价暴跌">文本相似度提前预警股价暴跌。</h3>
<blockquote>
<p>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. &ldquo;Lazy prices.&rdquo; <em>The Journal of Finance</em> 75, no. 3 (2020): 1371-1415.</p>
</blockquote>
<p><img loading="lazy" src="img/%e8%82%a1%e4%bb%b7%e6%b3%a2%e5%8a%a8.png" alt=""  />
</p>
<p><img loading="lazy" src="img/%e7%9b%b8%e4%bc%bc%e5%ba%a6%e8%af%86%e5%88%ab.png" alt=""  />
</p>
<br>
<h2 id="图片">图片</h2>
<p>OCR图像识别，识别有没有、有多少。</p>
<ul>
<li>停车场汽车停放量，识别有多少量车，预测沃尔玛等商超的经营情况</li>
</ul>
<p><img loading="lazy" src="img/%e9%99%88%e7%a1%95-%e9%81%a5%e6%84%9f-%e8%b4%ab%e7%a9%b7%e5%9c%b0%e5%9b%be.png" alt=""  />
</p>
<br>
<h2 id="音频视频">音频、视频</h2>
<ul>
<li>演讲音频转文本，用到文本分析，度量讲话的语气语调。</li>
</ul>
<br>
<br>
<h2 id="金融资讯舆情分析">金融资讯舆情分析</h2>
<blockquote>
<p>新闻舆情作为金融投资市场上的重要信息可以及时披露上市公司的经营状 况或股价异动情况，常常可作为投资决策的重要参考，但市场中海量的舆情信息难以通过人工的方式逐一分析，往往只能主观挑选某些个人认为比较重要的 新闻媒体进行舆情的跟踪，并忽略和抛弃其他新闻媒体的舆情信息，这极有可能遗漏掉一部分有价值的重要信息。</p>
<p>请各参赛队伍根据赛方提供的上市公司新闻资讯数据，利用深度学习、自然语言处理算法进行建模分析，<strong>及时、准确地</strong>判断新闻资讯的 <strong>舆情倾向</strong>（利好、中性、利空等）</p>
</blockquote>
<p>新闻中的可以挖掘的金融指标</p>
<ul>
<li>
<p>分析师情绪    买在分歧，卖在一致。</p>
</li>
<li>
<p>新闻情绪  机构、媒体、散户。</p>
</li>
</ul>
<h2 id="测度算法">测度算法</h2>
<p>使用文本分析对咨询中的舆情倾向（利好、中性、利空等）进行分析。</p>
<table>
<thead>
<tr>
<th>算法</th>
<th>功能</th>
<th>类比</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>词典法</td>
<td>把文档转为某个数。<br>例如政府工作报告中提到&quot;创新&quot;、&ldquo;创业&quot;的个数。</td>
<td>原子</td>
<td></td>
</tr>
<tr>
<td>机器学习</td>
<td>把 文档 转为 vector</td>
<td>分子</td>
<td></td>
</tr>
<tr>
<td>词嵌入</td>
<td>比机器学习更深入彻底，将word看做vector。工程师，含有<code>男性、技术、高薪。。。</code></td>
<td>夸克</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img loading="lazy" src="img/%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%a4%a7%e8%87%b4%e6%a1%86%e6%9e%b6.png" alt=""  />
</p>
<br>
<h2 id="需要的技术">需要的技术</h2>
<ol>
<li>
<p>词典法-构造金融情感词典</p>
<ul>
<li>
<p>共现法，上下文共同出现。</p>
</li>
<li>
<p>词向量法</p>
</li>
</ul>
</li>
<li>
<p>ML做文本分类</p>
</li>
</ol>
<br>
<br>
<h2 id="构造金融词典">构造金融词典</h2>
<h3 id="共现法">共现法</h3>
<p>物以类聚，词以群分。近义词更容易出现在同一个上下文中。</p>
<p>以「利好」「利空」为例</p>
<ol>
<li>人工选定「利好」「利空」初始词</li>
<li>构建语料内的词语共现矩阵</li>
<li>得到与「利好」「利空」共现得分较高的前n个候选词</li>
<li>分别输出到txt内</li>
<li>人工筛查剔除</li>
</ol>
<h3 id="词向量">词向量</h3>
<p><img loading="lazy" src="img/word2vec.png" alt=""  />
</p>
<p>以「利好」「利空」为例</p>
<ol>
<li>人工选定「利好」「利空」初始词</li>
<li>训练语料内的词向量模型</li>
<li>得到与「利好」「利空」向量相似度较高的前n个候选词</li>
<li>分别输出到txt内</li>
<li>人工筛选剔除</li>
</ol>
<br>
<br>
<h2 id="ml做预测利好1利空0步骤">ML做预测(利好1、利空0)步骤</h2>
<ol>
<li>&hellip;(标注数据)</li>
<li>导入数据</li>
<li>数据清洗(剔除停用词，杂乱字符等)</li>
<li>特征工程（文本转化为向量）</li>
<li>将数据分为训练集和测试集</li>
<li>选择某种ML算法训练模型</li>
<li>评价模型</li>
</ol>
<p><br><br></p>
<h2 id="ml算法">ML算法</h2>
<p>机器学习算法分为 <strong>监督式</strong> 和 <strong>非监督式</strong>。本节特指监督式，即同时含有x1, x2,&hellip;xn和y.</p>
<p>ML训练出的模型，实际上是通过数据，学习 y=f(x1, x2, &hellip;xn)中的 f。</p>
<p><img loading="lazy" src="img/ML/%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th>监督学习算法</th>
<th>代码导入方法</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model">回归</a></td>
<td>from sklearn.linear_model import LinearRegression<br><br>from sklearn.linear_model import LogisticRegression<br>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors">K近邻</a></td>
<td>from sklearn.neighbors import KNeighborsClassifier<br>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm">支持向量机</a></td>
<td>from sklearn.svm import SVC<br/>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree">决策树</a></td>
<td>from sklearn.tree import DecisionTreeClassifier<br/>&hellip;</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h1 id="投保反欺诈模型">投保反欺诈模型</h1>
<p>机器学习可以根据丰富的数据和监控模型，对数据进行多重处理分析，建立实时反欺诈规则和模型，结合当前用户特征，实时识别用户欺诈行为。</p>
<p>请参赛队伍在<strong>了解投保信息收集的基础上</strong>，基于<strong>机器学习技术</strong>，对投保过程中的信息进行收集和分析，从数据中提取客户多维度异常模式，探索大数据反欺诈规则，实现异常识别功能，提前检测投保人在交易过程中是否有欺诈行为，识别可能的欺诈行为，减少欺诈损害。</p>
<blockquote>
<p>了解投保信息收集的基础上&ndash;&gt;提取新的x</p>
</blockquote>
<h2 id="ml做预测步骤">ML做预测步骤</h2>
<ol>
<li>&hellip;(标注数据)</li>
<li>导入数据</li>
<li>数据清洗(剔除停用词，杂乱字符等)</li>
<li>特征工程（构造并加入新的x）</li>
<li>将数据分为训练集和测试集</li>
<li>选择某种ML算法训练模型</li>
<li>评价模型</li>
</ol>
<br>
<table>
<thead>
<tr>
<th>监督学习算法</th>
<th>代码导入方法</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model">回归</a></td>
<td>from sklearn.linear_model import LinearRegression<br><br>from sklearn.linear_model import LogisticRegression<br>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors">K近邻</a></td>
<td>from sklearn.neighbors import KNeighborsClassifier<br>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm">支持向量机</a></td>
<td>from sklearn.svm import SVC<br/>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree">决策树</a></td>
<td>from sklearn.tree import DecisionTreeClassifier<br/>&hellip;</td>
</tr>
</tbody>
</table>
<br>
<h2 id="kaggle代码httpswwwkagglecomcodeahmedmsoliman94-accuracy"><a href="https://www.kaggle.com/code/ahmedmsoliman/94-accuracy">Kaggle代码</a></h2>
<p><br><br></p>
<h1 id="公募产品个性化推荐系统">公募产品个性化推荐系统</h1>
<blockquote>
<p>在客户需求升级和金融市场的竞争环境下，数字化运营将是未来金融机构核心竞争力的来源，是构筑差异化优势的重要手段。</p>
<p>请参赛队伍结合金融行业的数字化运营需求，根据赛方提供的公募基金资讯数据、风险等级数据、用户行为点击序列、公募产品详情页的停留时长、公募产品的自选收藏等行为数据集，运用机器学习、深度学习、推荐算法等科技手段，分析预测用户的下一个兴趣点，在满足风险合规的条件下为合适的用户找到合适的产品。</p>
</blockquote>
<br>
<h2 id="方法论基础">方法论基础</h2>
<p>假设:  相似的人 喜欢做 相似的事情</p>
<p>有三种推荐算法</p>
<table>
<thead>
<tr>
<th>推荐系统算法思想</th>
<th>解释</th>
<th>特征向量化</th>
</tr>
</thead>
<tbody>
<tr>
<td>Demographic Filtering</td>
<td>相似人口特征的人 喜欢 相似的事(物)</td>
<td>将人向量化。[age、gendre、salary、consume、地理、、、]</td>
</tr>
<tr>
<td><a href="https://www.kaggle.com/code/muhammadhananasghar/imdb-movies-content-based-recomendation-system">Content Based Filtering</a></td>
<td>如果一个人喜欢某个特定事(物)，他或她也会喜欢与它相似的项目。</td>
<td>将事物向量化</td>
</tr>
<tr>
<td><a href="https://www.kaggle.com/code/omarkhald/recommendation-system-collaborative-filter">Collaborative Filtering</a> 协同(联合)</td>
<td>人与事(物) 的 配对匹配 存在模式</td>
<td>用户-评价-矩阵</td>
</tr>
</tbody>
</table>
<br>
<h2 id="collaborative-filtering--user-item-matrix">Collaborative Filtering | user-item-matrix</h2>
<p>以用户影评为例，挖掘构造出用户、产品的特点(特征向量）。</p>
<p><img loading="lazy" src="img/03-%e9%a2%84%e6%b5%8b%e7%94%a8%e6%88%b7%e5%af%b9%e7%94%b5%e5%bd%b1%e7%9a%84%e5%96%9c%e5%a5%bd.png" alt=""  />
</p>
<p><img loading="lazy" src="img/04-%e7%94%a8%e6%88%b7%e8%af%84%e4%bb%b7%e7%9f%a9%e9%98%b5.png" alt=""  />
</p>
<p><img loading="lazy" src="img/06-%e9%a2%84%e6%b5%8b%e8%ae%a1%e7%ae%97.png" alt=""  />
</p>
<p><img loading="lazy" src="img/07-%e7%94%a8%e6%88%b7%e7%9b%b8%e4%bc%bc%e5%ba%a6.png" alt=""  />
</p>
<br>
<h2 id="冷启动问题">冷启动问题</h2>
<p>如果某个用户，没有任何影评数据，如何预测该用户的偏好？</p>
<p>思路: 依然假设物以类聚，人以群分。</p>
<p>公募基金公司  有历史记录</p>
<table>
<thead>
<tr>
<th>user</th>
<th>类型</th>
<th>个人风险偏好考试</th>
<th>金额</th>
</tr>
</thead>
<tbody>
<tr>
<td>User1 (age/gender/edu/addr/intro)</td>
<td>债券</td>
<td>保守</td>
<td>5000</td>
</tr>
<tr>
<td>User2 (age/gender/edu/addr/intro)</td>
<td>股票</td>
<td>激进</td>
<td>10000</td>
</tr>
<tr>
<td>&hellip;</td>
<td>..</td>
<td>..</td>
<td>..</td>
</tr>
</tbody>
</table>
<br>
<br>
<h1 id="本文之外">本文之外</h1>
<h2 id="长期征稿">长期征稿</h2>
<div style="text-align: center;">
<figure >
    <a href="https://textdata.cn/blog/call_for_paper/">
        <img src="/images/blog/call_for_paper.png" width="100%" />
    </a>
    <figcaption><small><i>点击了解投稿</i></small></figcaption>
</figure>
</div>
<br>
<h2 id="招募小伙伴">招募小伙伴</h2>
<div style="text-align: center;">
<figure >
    <a href="https://textdata.cn/blog/we_need_you/">
        <img src="/images/blog/we_need_you.png" width="100%" />
    </a>
    <figcaption><small><i>点击加入我们</i></small></figcaption>
</figure>
</div>
<h2 id="文本分析视频课">文本分析视频课</h2>
<p>想轻松而快捷的深刻了解一个领域，看视频(直播)学习是一个不错的方式。</p>
<ul>
<li>
<p>大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与<a href="https://textdata.cn/blog/2022-05-workshop/7-Python.html">直播课</a>。</p>
</li>
<li>
<p>如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的<a href="https://textdata.cn/blog/management_python_course">录播课</a>。</p>
</li>
<li>
<p>如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读<a href="https://textdata.cn/blog/paid_for_service">有偿说明</a></p>
</li>
</ul>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://textdata.cn/blog/management_python_course/">点击进入详情页</a></p>
<br>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理世界 | 使用LM中文金融词典对年报进行语调分析</title>
      <link>https://textdata.cn/blog/manager_tone_analysis_with_lm/</link>
      <pubDate>Wed, 22 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/manager_tone_analysis_with_lm/</guid>
      <description>使用LM中文金融词典对年报进行语调分析</description>
      <content:encoded><![CDATA[<p>今天分享的这篇论文通过 <strong>有道翻译</strong> 这一简单有效的的方式汉化了 LM英文金融词典，并使用 <strong>数词语数个数</strong>  的方式构造了管理层语调这个指标。</p>
<h2 id="文献">文献</h2>
<p><strong>曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.</strong></p>
<p>本文代码实现的视频讲解已更新至付费课 <a href="http://mp.weixin.qq.com/s?__biz=MzI1MTE2ODg4MA==&amp;mid=2650082457&amp;idx=2&amp;sn=de680696e2595e8f4dc894e283436819&amp;chksm=f1f6bd86c6813490e8dc413eaf8446f176c41047cd8f5fc4faff14f12737ff5d68903396e8ec&amp;scene=21#wechat_redirect"><strong>Python实证指标构建与文本分析</strong></a> 中。</p>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<h3 id="本文代码">本文代码</h3>
<p><a href="manager_tone_analysis_with_lm.zip">点击下载</a></p>
<h3 id="摘要">摘要</h3>
<p>基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在**「口是心非」** 的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,<strong>年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与  「年报语调」 相关</strong>。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。</p>
<h3 id="关键词">关键词</h3>
<p>年报; 语调管理; 内部人交易; 信息不对称;</p>
<h2 id="代码">代码</h2>
<ul>
<li>年报数据 <code>data/reports.csv</code></li>
<li>LM金融词典</li>
<li>需要更新cntext库至于1.7.3及以上版本</li>
</ul>
<p><br><br></p>
<h2 id="语调指标">语调指标</h2>
<ul>
<li>算法1 <code>该年报内 「积极词汇数 与消极词汇数 之差」 占 「年报总词汇数」 的比例；</code></li>
<li>算法2 <code>（积极词汇数-消极词汇数）/（ 积极词汇数+消极词汇数）</code></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/reports.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>company</th>
      <th>year</th>
      <th>content</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>四川长虹</td>
      <td>2017</td>
      <td>2017 年，面对复杂多变的外部环境和多重叠加的困难挑战，公司聚焦用户与产品，强化消费洞...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>江苏吴中</td>
      <td>2014</td>
      <td>2014 年，正值公司成立二十周年，上市十五周年，在董事会带领下，公司经营管理团队与全体...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>联美控股</td>
      <td>2017</td>
      <td>报告期内，公司实现营业收入 2,376,375,380.44 元，同比增长 16.24%，营...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>华海药业</td>
      <td>2016</td>
      <td>第三节\t公司业务概要\n\n一、 报告期内公司所从事的主要业务、经营模式及行业情况说明\n...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>江泉实业</td>
      <td>2014</td>
      <td>报告期内，全球经济形势复杂多变、复苏进程缓慢；我国宏观经济进入增速放缓、结构调整加剧的新...</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>500
</code></pre>
<p><br><br></p>
<h2 id="cntext">cntext</h2>
<p>安装cntext</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">cntext</span>
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    1.7.3
</code></pre></div><br>
<p>查看内置词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [&#39;DUTIR.pkl&#39;,
     &#39;HOWNET.pkl&#39;,
     &#39;Chinese_Loughran_McDonald_Financial_Sentiment.pkl&#39;,
     &#39;sentiws.pkl&#39;,
     &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
     &#39;ANEW.pkl&#39;,
     &#39;LSD2015.pkl&#39;,
     &#39;NRC.pkl&#39;,
     &#39;geninqposneg.pkl&#39;,
     &#39;HuLiu.pkl&#39;,
     &#39;Loughran_McDonald_Financial_Sentiment.pkl&#39;,
     &#39;AFINN.pkl&#39;,
     &#39;ADV_CONJ.pkl&#39;,
     &#39;STOPWORDS.pkl&#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">clm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;Chinese_Loughran_McDonald_Financial_Sentiment.pkl&#39;</span><span class="p">)</span>

<span class="c1">#print(clm)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;clm关键词: &#39;</span><span class="p">,</span><span class="n">clm</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Desc: &#39;</span><span class="p">,</span> <span class="n">clm</span><span class="p">[</span><span class="s1">&#39;Desc&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Referer: &#39;</span><span class="p">,</span> <span class="n">clm</span><span class="p">[</span><span class="s1">&#39;Referer&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Chinese_Loughran_McDonald_Financial_Sentiment词典含2类情感词</span><span class="se">\n</span><span class="s1">，分别是&#39;</span><span class="p">,</span> <span class="n">clm</span><span class="p">[</span><span class="s1">&#39;Chinese_Loughran_McDonald_Financial_Sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    clm关键词:  dict_keys([&#39;Chinese_Loughran_McDonald_Financial_Sentiment&#39;, &#39;Desc&#39;, &#39;Referer&#39;])
    
    Desc:  参照该论文，cntext库使用百度翻译、有道翻译对LM词典进行汉化处理。原文使用的有道翻译、金山词霸。
    
    Referer:  曾庆生, 周波, 张程, and 陈信元. &#34;年报语调与内部人交易: 表里如一还是口是心非?.&#34; 管理世界 34, no. 09 (2018): 143-160.
    
    Chinese_Loughran_McDonald_Financial_Sentiment词典含2类情感词
    ，分别是 dict_keys([&#39;negative&#39;, &#39;positive&#39;])
</code></pre></div><p><br><br></p>
<h2 id="cntexthttpsgithubcomhidadengcntext语调的实现"><a href="https://github.com/hiDaDeng/cntext">cntext</a>语调的实现</h2>
<ul>
<li>算法1 <code>该年报内 「积极词汇数 与消极词汇数 之差」 占 「年报总词汇数」 的比例；</code></li>
<li>算法2 <code>（积极词汇数-消极词汇数）/（ 积极词汇数+消极词汇数）</code></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    {&#39;pos_num&#39;: 3,
     &#39;neg_num&#39;: 0,
     &#39;stopword_num&#39;: 8,
     &#39;word_num&#39;: 14,
     &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">diction</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;Chinese_Loughran_McDonald_Financial_Sentiment.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;Chinese_Loughran_McDonald_Financial_Sentiment&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">tone</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
                       <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span>
                       <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
  
<span class="c1">#第一个年报的语调</span>
<span class="n">tone</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    negative_num      67
    positive_num      76
    stopword_num     462
    word_num        1831
    sentence_num      52
    dtype: int64
</code></pre></div><br>
<p>选中文本列content， 对content整体实施tone计算，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#计算tone语调</span>
<span class="n">tone_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">tone</span><span class="p">)</span>
<span class="n">tone_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>negative_num</th>
      <th>positive_num</th>
      <th>stopword_num</th>
      <th>word_num</th>
      <th>sentence_num</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>67</td>
      <td>76</td>
      <td>462</td>
      <td>1831</td>
      <td>52</td>
    </tr>
    <tr>
      <th>1</th>
      <td>32</td>
      <td>59</td>
      <td>372</td>
      <td>1266</td>
      <td>34</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18</td>
      <td>33</td>
      <td>178</td>
      <td>816</td>
      <td>19</td>
    </tr>
    <tr>
      <th>3</th>
      <td>81</td>
      <td>114</td>
      <td>1055</td>
      <td>3619</td>
      <td>90</td>
    </tr>
    <tr>
      <th>4</th>
      <td>27</td>
      <td>17</td>
      <td>134</td>
      <td>453</td>
      <td>16</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<ul>
<li>算法1 <code>该年报内 「积极词汇数 与消极词汇数 之差」 占 「年报总词汇数」 的比例；</code></li>
<li>算法2 <code>（积极词汇数-消极词汇数）/（ 积极词汇数+消极词汇数）</code></li>
</ul>
<p>将得到的正、负面、总词数分别按照算法1和算法2进行计算。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 算法1</span>
<span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;tone1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;positive_num&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;negative_num&#39;</span><span class="p">])</span><span class="o">/</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="c1">#tone_df[&#39;tone1&#39;] = (tone_df[&#39;positive_num&#39;]-tone_df[&#39;negative_num&#39;])/(tone_df[&#39;word_num&#39;]+1)</span>

<span class="c1">#算法2</span>
<span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;tone2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;positive_num&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;negative_num&#39;</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;positive_num&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;negative_num&#39;</span><span class="p">])</span>
<span class="c1">#tone_df[&#39;tone2&#39;] = (tone_df[&#39;positive_num&#39;]-tone_df[&#39;negative_num&#39;])/(tone_df[&#39;positive_num&#39;]+tone_df[&#39;negative_num&#39;]+1)</span>

<span class="n">tone_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

</code></pre></div><p>Run</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>negative_num</th>
      <th>positive_num</th>
      <th>stopword_num</th>
      <th>word_num</th>
      <th>sentence_num</th>
      <th>tone1</th>
      <th>tone2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>67</td>
      <td>76</td>
      <td>462</td>
      <td>1831</td>
      <td>52</td>
      <td>0.004915</td>
      <td>0.062937</td>
    </tr>
    <tr>
      <th>1</th>
      <td>32</td>
      <td>59</td>
      <td>372</td>
      <td>1266</td>
      <td>34</td>
      <td>0.021327</td>
      <td>0.296703</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18</td>
      <td>33</td>
      <td>178</td>
      <td>816</td>
      <td>19</td>
      <td>0.018382</td>
      <td>0.294118</td>
    </tr>
    <tr>
      <th>3</th>
      <td>81</td>
      <td>114</td>
      <td>1055</td>
      <td>3619</td>
      <td>90</td>
      <td>0.009119</td>
      <td>0.169231</td>
    </tr>
    <tr>
      <th>4</th>
      <td>27</td>
      <td>17</td>
      <td>134</td>
      <td>453</td>
      <td>16</td>
      <td>-0.022075</td>
      <td>-0.227273</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<p>将结果存储到xlsx中</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">tone_df</span><span class="o">.</span><span class="n">to_excel</span><span class="p">(</span><span class="s1">&#39;output/管理层-语调分析.xlsx&#39;</span><span class="p">)</span>
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>近年《管理世界》《管理科学学报》《金融研究》使用文本分析论文</title>
      <link>https://textdata.cn/blog/research_with_tm_in_chinese_top_ms_journal/</link>
      <pubDate>Fri, 17 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/research_with_tm_in_chinese_top_ms_journal/</guid>
      <description>近年《管理世界》《管理科学学报》期刊中使用文本分析论文汇总</description>
      <content:encoded><![CDATA[<h2 id="管理世界">管理世界</h2>
<p><strong>曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.DOI:10.19744/j.cnki.11-1235/f.2018.09.012.</strong></p>
<p><a href="https://textdata.cn/blog/manager_tone_analysis_with_lm/">本文代码实现</a></p>
<blockquote>
<p><strong>摘要</strong>: 基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在&quot;口是心非&quot;的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与年报语调相关。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。</p>
<p>**关键词：**年报; 语调管理; 内部人交易; 信息不对称;</p>
</blockquote>
<br>
<p><strong>洪永淼,汪寿阳.大数据如何改变经济学研究范式？[J].管理世界,2021,37(10):40-55+72+56.DOI:10.19744/j.cnki.11-1235/f.2021.0153.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文首先从经济学视角探讨大数据给经济学实证研究所带来的范式变革,包括从理性经济人到非完全理性经济人,从孤立的经济人到互相关联的社会经济人,从代表性经济人到异质性经济主体,以及从经济分析到经济社会活动的系统分析。然后,从方法论视角讨论大数据给经济学实证研究方法所带来的变革,包括从模型驱动到数据驱动,从参数不确定性到模型不确定性,从无偏估计到有偏估计,从低维建模到高维建模,从低频数据到高频甚至实时数据,从结构化数据到非结构化数据,从传统结构化数据到新型结构化数据,以及从人工分析到智能分析等。大数据引起的经济学研究范式与研究方法变革,正在深刻重塑经济学发展方向,不但加强了经济学实证研究范式的趋势,而且还进一步突破了现代西方经济学的一些基本假设的局限性,使经济学研究日益呈现出科学化、严谨化、精细化、多元化(跨学科)与系统化的趋势,并且与社会科学其他领域在方法论上日益趋同。中国大数据资源,为从中国经济实践中总结经济发展规律,从中国特殊性中凝练可复制的经济发展模式,从而构建具有深厚学理基础的原创性中国经济理论体系,提供了一个得天独厚的&quot;富矿&quot;。</p>
<p><strong>关键词：</strong>	大数据;文本分析;机器学习;研究范式;研究方法;反身性;</p>
</blockquote>
<br>
<p><strong>张宗新,吴钊颖.媒体情绪传染与分析师乐观偏差——基于机器学习文本分析方法的经验证据[J].管理世界,2021,37(01):170-185+11+20-22.DOI:10.19744/j.cnki.11-1235/f.2021.0011.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文利用2013～2017年上市公司的百度新闻报道作为文本,运用机器学习文本分析方法测算情绪倾向得分,考察了媒体情绪对分析师预测行为的影响及其传染机制与风险后果。研究发现:(1)媒体乐观情绪会显著正向影响分析师盈利预测的乐观偏差度;(2)媒体情绪通过&quot;分析师有限关注&quot;与&quot;投资者情绪&quot;两条路径来影响分析师预测的乐观倾向;(3)分析师乐观情绪和媒体乐观情绪均会加剧股价波动及尾部风险,且分析师乐观情绪是媒体情绪影响股价波动的传导路径;(4)明星分析师与非明星分析师均会受到媒体情绪的感染,前者理性程度相对更高但其行为对股价波动冲击更为明显。本研究对于规范媒体行为,矫正分析师过度乐观偏差,合理引导理性投资具有重要意义。</p>
<p><strong>关键词：</strong>	媒体报道情绪;分析师乐观偏差;股价波动;有限理性;</p>
</blockquote>
<br>
<p><strong>胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.DOI:10.19744/j.cnki.11-1235/f.2021.0070.</strong></p>
<blockquote>
<p>**摘要：**在可持续发展战略导向下,秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基石。然而,作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间导向理论,提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系,并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现,年报MD&amp;A中披露的&quot;短期视域&quot;语言能够反映管理者内在的短视主义特质,管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时,管理者短视主义对这些长期投资的负向影响越易受到抑制。最终,管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析,对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时,本文将文本分析和机器学习方法引入管理者短视主义的研究,为未来该领域的研究提供了参考和借鉴。</p>
<p><strong>关键词：</strong> 管理者短视;长期投资;文本分析;机器学习;</p>
</blockquote>
<br>
<p><strong>底璐璐,罗勇根,江伟,陈灿.客户年报语调具有供应链传染效应吗？——企业现金持有的视角[J].管理世界,2020,36(08):148-163.DOI:10.19744/j.cnki.11-1235/f.2020.0124.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 利用我国供应商企业前五名上市客户及其管理层语调的文本数据,本文考察了跨企业关系情形下客户年报语调对供应商企业现金持有决策的影响。研究结果发现,客户的年报语调越消极,供应商企业则会持有更多的现金,表明客户年报净负面语调在供应链上存在传染效应。进一步的研究发现,非国有性质、相对议价能力较低的供应商企业现金持有与客户年报净负面语调的正相关关系分别显著强于国有性质、相对议价能力较高的供应商企业。此外,当客户融资融券程度较高时,客户年报净负面语调对供应商企业现金持有的正向影响会有所增强。本文的研究不仅在考察跨企业情形下企业现金持有的影响因素以及客户文本信息的经济后果两个方面弥补了国内外现有研究的不足,而且对于企业如何进行现金持有决策提供了一定的经验证据与参考,这对于管理供应链相关风险,推动我国企业的供应链整合进而提升我国企业的全球竞争力具有重要的启示意义。</p>
<p><strong>关键词：</strong>	年报语调;现金持有;供应链传染;文本分析;</p>
</blockquote>
<br>
<p><strong>林晚发,赵仲匡,宋敏.管理层讨论与分析的语调操纵及其债券市场反应[J].管理世界,2022,38(01):164-180.DOI:10.19744/j.cnki.11-1235/f.2022.0012.</strong></p>
<blockquote>
<p>**摘要: **本文研究了管理层讨论与分析（MD&amp;A）语调的操纵行为及其债券市场反应。研究发现,MD&amp;A异常积极语调与预警Z值负相关,债务重组正相关,这表明MD&amp;A异常积极语调暗示了企业较高的未来风险,这与语调的信息增量解释相悖,因此MD&amp;A异常积极语调更可能是操纵的结果。进一步研究发现,MD&amp;A异常积极语调越大,债券信用评级越高,且该正向关系在与评级机构利益冲突大、信息透明度低的公司子样本中更显著;此外,债券投资者能够识别语调操纵行为,但随着债券市场公众投资者的参与,MD&amp;A异常积极语调与债券信用利差之间呈现出一定的负向关系,且这种负向关系在信息透明度低的企业组中更加显著。本文较早使用中国资本市场数据度量了MD&amp;A异常积极语调,且证实这种异常语调是管理层操纵的结果,并探讨了MD&amp;A语调操纵对于债券市场信息效率的影响,相关结论对于完善MD&amp;A文本信息披露监管法规、提高评级机构独立性以及提升债券市场信息效率具有重要启示。</p>
<p><strong>关键词：</strong></p>
<p>MD＆A语调操纵; 利益冲突; 债券信用评级; 债券信用利差;</p>
</blockquote>
<p><br><br></p>
<h2 id="管理科学学报">管理科学学报</h2>
<p><strong>马长峰, 陈志娟, 张顺明. 基于文本大数据分析的会计和金融研究综述[J]. 管理科学学报, 2020, 23(9):12..</strong></p>
<blockquote>
<p>**摘要：**作为一种非结构化数据,文本大数据最近十年深刻影响会计学和金融学研究.这种影响体现在两类文献:第一类以信息为中心,将文本分析技术用于信息的品质(可读性)和数量(文本信息含量),信息披露和市场异象等方面的研究;第二类与信息无关,主要是利用文本大数据分析技术构建全新指标,例如基于文本分析的公司竞争力,创新和经济政策不确定性等新变量,梳理上述文献研究脉络,揭示文本分析技术的优缺点,并且指出在会计和金融领域应用文本大数据技术的研究面临的挑战和机遇。</p>
<p>**关键词：**可读性; 信息; 欺诈; 创新; 经济政策不确定性</p>
</blockquote>
<br>
<p><strong>杨晓兰,王伟超,高媚.股市政策对股票市场的影响——基于投资者社会互动的视角[J].管理科学学报,2020,v.23;No.187(01):15-32.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文将影响股市的政策分为五类,检验股市的政策效应;并以新浪财经博客为投资者之间社会互动的媒介,利用文本挖掘技术和社会网络研究方法,构建反映投资者之间社会互动程度、情绪属性以及社会网络中心程度的变量,探讨社会互动对股市政策效应的影响.实证研究表明,舆论导向政策对股市收益率存在显著的正向影响;证券供给需求性政策、货币政策显著提高股市波动率,市场创新与市场交易制度显著降低市场波动率.同时,投资者对专业性政策的解读显著依赖于社会互动,社会互动会放大货币政策对股市收益率的正向影响,加剧证券供给需求性政策对股市波动的影响,平缓市场创新与市场交易制度对股市波动的影响,而不影响舆论导向政策对股市产生的效应.</p>
<p><strong>关键词：</strong>	政策;社交网络;社会互动;股票市场;文本挖掘;</p>
</blockquote>
<br>
<p><strong>赵子夜,杨庆,杨楠.言多必失?管理层报告的样板化及其经济后果[J].管理科学学报,2019,22(03):53-70.</strong></p>
<blockquote>
<p>**摘要：**样板化报告在古今中外都有广泛的运用,但报告者面临两难:一方面,样板化有利于规避披露风险;但另一方面,样板化又不利于传递内部信息.那么,投资者如何评价中国上市公司的报告的样板化程度?以中国上市公司的管理层讨论与分析的文字为样本,用公司t期和t-1期报告的纵向文本相似度以及本公司和其他公司同期的报告的平均横向相似度来衡量样板化的水平,并考察了其经济后果.检验结果表明,纵向样板化的经济后果呈现相机抉择性,当公司财务风险高（亏损、微利或者被出具非标准审计意见）时,信息效应占优,样板化的报告引发负面的市场评价,而当公司财务风险较低,风险效应占优,样板化的报告则引发市场的好评.另一方面,报告横向样板化则引起了整体的负面评价.在调节效应方面,纵向样板化的经济后果受公司创新、特质信息、董事长权力和停牌次数的影响,横向样板化的经济后果则受到公司独立董事的社会网络位置的影响.综合结果表明,公司管理层讨论与分析的横向样板化,以及在高财务风险条件下的纵向样本化都会因信息披露不足而引起负面的经济后果.</p>
<p><strong>关键词：</strong> 管理层报告;样板化;文本分析;经济后果;</p>
</blockquote>
<br>
<p><strong>卞世博, 管之凡, 阎志鹏. 答非所问与市场反应:基于业绩说明会的研究[J]. 管理科学学报, 2021, 24(4):18.</strong></p>
<blockquote>
<p><strong>摘要:</strong> 对上市公司业绩说明会中投资者与管理层问答互动中管理层答非所问的现象进行了研究.本文以中小板和创业板上市公司召开的业绩说明会作为研究样本,利用文本分析方法对业绩说明会中管理层在回答投资者提问时答非所问的程度进行度量,进而实证分析了管理层的答非所问与市场反应和公司未来业绩表现之间的可能关联.结果 发现:在控制其它因素之后,管理层的答非所问与市场反应之间呈现显著的负相关关系,即公司管理层的答非所问程度越高,随后公司股票的市场表现则就会越差,并且对于那些低分析师关注的公司尤为明显;而在公司未来业绩表现方面,管理层答非所问的程度越高,则公司未来的业绩表现则会越差.。</p>
<p>**关键词：**业绩说明会; 答非所问; 市场反应; 未来业绩</p>
</blockquote>
<br>
<p><strong>逯东, 宋昕倍. 媒体报道,上市公司年报可读性与融资约束[J]. 管理科学学报, 2021, 24(12):17..</strong></p>
<blockquote>
<p>**摘要：**采用文本分析方法,深入考察了上市公司年报可读性与融资约束的关系,并考虑媒体报道这一外部信息的调节效应研究发现,上市公司的年报可读性越低,其面临的融资约束越高;媒体报道的增多可以弱化年报可读性与融资约束的关系,且媒体报道情绪越正向,其调节作用越显著.进一步分析发现:机构投资者持股比例较高能减弱年报可读性和融资约束的关系;当年报可读性较低时,媒体报道的信息效应更为显著;只有官方媒体和地方媒体的报道数量与正向报道情绪能够显著缓解年报可读性低带来的融资约束;同时,较低的年报可读性是通过提高融资成本路径来加大公司的融资约束,且使得公司未来的融资方式呈现出内部融资增加,外部融资减少的特点.从融资约束角度拓展了关于财务报告文本信息披露质量的研究,并揭示了媒体报道如何有效改善内部信息披露不足的作用机理,为企业如何通过改善内,外部的信息环境来缓解自身的融资困境提供了理论依据。</p>
<p>**关键词：**年报可读性；融资约束；媒体报道；文本分析</p>
</blockquote>
<br>
<p><strong>姚加权, 冯绪, 王赞钧,等. 语调,情绪及市场影响:基于金融情绪词典[J]. 管理科学学报, 2021, 24(5):21.</strong></p>
<blockquote>
<p>**摘要：**金融文本的语调与情绪含有上市公司管理层以及个体投资者表达的情感信息,并对股票市场产生影响.通过词典重组和深度学习算法构建了适用于正式文本与非正式文本的金融领域中文情绪词典,并基于词典构建了上市公司的年报语调和社交媒体情绪指标.构建的年报语调指标和社交媒体情绪指标能有效地预测上市公司股票的收益率,成交量,波动率和非预期盈余等市场因素,并优于基于其他广泛使用情绪词典构建的指标.此外,年报语调指标和社交媒体情绪指标对上市公司的股价崩盘风险具有显著的预测作用.为文本大数据在金融市场的应用提供了分析工具,也为大数据时代的金融市场预测和监管等活动提供了决策支持。</p>
<p>**关键词：**情绪词典；语调；投资者情绪；市场影响</p>
</blockquote>
<br>
<p><strong>姜富伟, 马甜, 张宏伟. 高风险低收益? 基于机器学习的动态CAPM模型解释[J]. 管理科学学报, 2021.</strong></p>
<blockquote>
<p>**摘要：**我国股票市场存在高风险股票反而伴随较低收益的低风险定价异象,这有悖于传统资产定价理论.本文使用宏观经济和微观企业特征构建了六百多个变量的宏微观混合大数据集,并结合多种经典机器学习算法开发了基于大数据和机器学习的智能动态CAPM模型,检验了时变系统性风险对我国股市收益解释能力.实证结果表明:本文的智能动态CAPM定价模型能够显著解释我国股市低风险定价异象;随机森林等非线性机器学习算法表现最佳;影响股票时变系统风险的主要因素是市场类因子,基本面因子居次.本文对于我国股市系统性风险测度,动态资产定价模型构建和金融与大数据和人工智能融合创新有重要理论与实践指导意义.</p>
<p>**关键词：**系统性风险; 动态CAPM; 机器学习; 金融大数据</p>
</blockquote>
<br>
<p><strong>陆瑶, 张叶青, 黎波,等. 高管个人特征与公司业绩——基于机器学习的经验证据[J]. 管理科学学报, 2020, 23(2):21.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 在目前的公司治理文献中,大部分的高管特征研究一方面仅关注单一的高管特征与公司业绩之间的关联,缺乏全面的高管特征分析;另一方面主要围绕因果推断进行研究,缺乏从预测能力出发的系统定量的结论.本文首次采用机器学习算法中的Boosting回归树,全面考察了多维度高管特征对公司业绩的预测性.以我国2008年～2016年的上市公司为样本,研究了高管的多维个人特征是否能预测公司业绩,并进一步分析了对公司业绩预测能力较强的高管个人特征及其预测模式.研究发现:1)整体而言,在我国公司CEO和董事长的特征对公司业绩的预测能力较弱;2)在众多高管个人特征之中,高管持股比例和年龄对公司业绩的预测能力较强;3)高管持股比例和年龄与公司业绩之间的关联都呈现出非线性的特点,与以往的理论较为吻合.本研究不仅利用机器学习方法从一个更为全面的视角对中国的高管特征进行了研究,也为公司高管聘任和激励机制设计等方面提供了有益的启发.</p>
<p>**关键词：**机器学习；Boosting回归树；公司治理；公司业绩</p>
</blockquote>
<br>
<p><strong>吴武清, 赵越, 闫嘉文,等. 分析师文本语调会影响股价同步性吗?&ndash;基于利益相关者行为的中介效应检验[J]. 管理科学学报, 2020, 23(9):19.</strong></p>
<blockquote>
<p>**摘要：**文章考察了分析师研究报告的文本语调对股价同步性的影响与作用机制.首先爬取2006年至2018年中国A股上市公司377644份分析师研究报告,随机选出10434句文本并人工分为积极,中性,消极三类形成语料库,以此训练11种机器学习方法并比较各方法的预测准确性,最终选择朴素贝叶斯方法估计出分析师研究报告的文本语调.实证分析发现,分析师积极的文本语调显著降低了所追踪公司的股价同步性.这一结果与已有多数研究结论不同,但在做空机制欠发达的中国资本市场,个体选择性知觉理论为此提供了很好的解释.进一步地,中介效应检验结果表明,分析师积极的文本语调通过激励公司发布更多公告,引导机构投资者买入和吸引其他分析师发布研究报告,显著降低了股价同步性.该研究对于投资者关注研报语调指标,上市公司加强信息披露,政府部门完善资本市场制度均具有重要启示。</p>
<p>**关键词：**分析师文本语调; 股价同步性; 朴素贝叶斯; 选择性知觉; 中介效应</p>
</blockquote>
<br>
<p><strong>刘冠男, 曲金铭, 李小琳,等. 基于深度强化学习的救护车动态重定位调度研究[J]. 管理科学学报, 2020, 23(2):15.</strong></p>
<blockquote>
<p>**摘要：**救护车是挽救患者生命的重要医疗资源,合理调配有限的救护车资源可以降低呼叫响应时间,提高医疗服务水平.本文面向救护车动态重定位调度问题,提出了一种基于强化学习的调度策略结构.为解决传统强化学习所面临的高维状态空间的挑战,本文基于深度Q值网络(DQN)方法,提出了一种考虑多种调度交互因子的算法RedCon-DQN,以在给定环境状态下得到最优的重定位调度策略.在此基础上,本文还提出了急救网络弹性概念,以评估各站点对全局救护优化目标的影响力.最后,基于南京市2016年～2017年的实际救护车呼叫及响应数据,构造了环境交互模拟器.在模拟器中通过大规模数据实验,验证了模型得到的调度策略相比已有方法的优越性,并分析了不同时段下调度策略的有效性及其特点.</p>
<p>**关键词：**强化学习; DQN; 救护车调度; 重定位</p>
</blockquote>
<br>
<p><strong>黄丽华, 何晓, 卢向华. 企业在线社群内容组合策略的影响研究[J]. 管理科学学报, 2020, 23(2):15..</strong></p>
<blockquote>
<p>**摘要：**现代企业通过建立在线社群实现与消费者的互动,希望在向消费者提供服务的同时进行更好的营销,然而如何提供在线社群中的营销与服务内容却是一大难题.本文在营销—服务二元理论的基础上,提出了在线社群内容二元性的平衡维度与结合维度概念,并研究平衡维度与结合维度如何影响销售业绩与消费者的满意度.结合机器学习方法,本文发现,平衡维度对消费者满意度和销售绩效有提高作用,但是,结合维度对消费者满意度及企业绩效的影响呈倒U型;另外,企业员工的技能水平对内容二元性策略的效果有着显著的调节作用.研究结论对企业理解在线社群中的营销内容与服务内容之间的二元关系,以及内容提供策略的价值机制有重要的指导意义。</p>
<p>**关键词：**在线社群; 内容二元性; 销售绩效; 消费者满意度</p>
</blockquote>
<br>
<p><strong>部慧,解峥,李佳鸿,吴俊杰.基于股评的投资者情绪对股票市场的影响[J].管理科学学报,2018,v.21;No.166(04):86-101.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 探讨投资者情绪对我国股票市场的影响.为刻画投资者情绪,基于东方财富网股吧帖文与朴素贝叶斯方法,提出融合股评看涨看跌预期和投资者关注程度的投资者情绪度量指标.进一步,利用Granger因果检验、瞬时Granger因果检验、跨期回归分析等方法,探讨了投资者情绪对我国股票收益率、交易量和波动性是否具有预测能力及影响.实证结果揭示:虽然投资者情绪对股票市场收益率、交易量和波动性均无预测能力,但投资者情绪对股票收益率和交易量有当期影响;开盘前非交易时段的股评情绪对开盘价具有预测力,开盘后交易时段的股评情绪对收盘价和日交易量具有更显著的影响.此外,股票收益率是投资者情绪的Granger原因,即投资者情绪的形成依赖于前期市场收益率.这些实证结果为深入理解参与股吧评论的交易者的行为以及行为对市场产生的影响提供了证据.</p>
<p><strong>关键词:</strong> 	投资者情绪; 噪声交易者; 文本挖掘; Granger因果检验;</p>
</blockquote>
<p><br><br></p>
<p>##金融研究</p>
<p>姜富伟, 胡逸驰, &amp; 黄楠. (2021). 央行货币政策报告文本信息, 宏观经济与股票市场. <em>金融研究</em>, <em>492</em>(6), 95-113.</p>
<blockquote>
<p>**摘要: ** 本文利用金融情感词典和文本分析技术,分析中国人民银行货币政策执行报告的文本情绪、文本相似度和文本可读性等多维文本信息,刻画央行货币政策执行报告的文本特征,探究货币政策报告的文本信息与宏观经济和股票市场的关系。实证研究发现,货币政策报告的文本情绪的改善会引起显著为正的股票市场价格反应,报告文本相似度的增加会引起股票市场波动性的显著降低,报告可读性对公布后股票市场的波动性影响不显著。货币政策报告文本情绪还与诸多宏观经济指标显著相关。进一步研究发现,引起股票市场显著反应的是报告文本情绪中反映货币政策指引的部分,而反映宏观经济历史状态的部分对股票市场的影响不显著。本文从文本大数据分析角度证明了我国央行沟通的有效性,对国内央行沟通相关研究形成了有益补充。</p>
<p><strong>关键词:</strong>  文本情绪分析  中央银行沟通  股票市场  宏观经济</p>
</blockquote>
<br>
<p>孙彤, 薛爽, &amp; 崔庆慧. (2021). 企业家前台化影响企业价值吗?——基于新浪微博的实证证据. 金融研究, 491(5), 189-206.</p>
<blockquote>
<p><strong>摘要:</strong> 互联网时代信息传递成本和沟通成本显著降低。微博作为自媒体的主要代表之一,为企业家从企业的幕后走向台前提供了一条便捷的途径。本文以信息传递理论为基础,利用新浪微博数据,检验了企业家前台化行为对企业价值的影响。实证结果表明:(1)企业家发布微博这一前台化行为有助于提升企业价值。从对价值影响的路径看,企业家微博发布后,企业经营活动现金流增加,系统性风险降低;(2)针对企业家微博进行文本分析,发现企业家微博中个性化微博比例越高、“艾特”人数越多或者微博内容中正向语调比例越高,对企业价值的正向影响越显著;(3)相对于信息不对称程度较低的企业,信息不对称程度较高的企业中企业家更倾向于发布微博。上述实证结果说明自媒体对缓解企业、企业家与投资者之间的信息不对称具有一定作用,为企业家前台化决策及路径选择提供了参考。</p>
<p><strong>关键词:</strong>  企业家  前台化  微博  企业价值  信息传递</p>
</blockquote>
<br>
<p>阮睿, 孙宇辰, 唐悦, &amp; 聂辉华. (2021). 资本市场开放能否提高企业信息披露质量?——基于 “沪港通” 和年报文本挖掘的分析. 金融研究, 488(2), 188-206.</p>
<blockquote>
<p><strong>摘要:</strong> 提高信息披露质量对于改善上市公司治理结构和保护股东权益具有重要意义。本文利用2014年开通的“沪港通”机制这一准自然实验,研究资本市场开放是否提高了企业的信息披露质量。从2010-2019年A股上市公司年报文本中提炼可读性指标衡量信息披露质量,使用匹配和双重差分方法进行实证研究,发现“沪港通”机制实施以后,标的公司(纳入“沪港通”的A股上市公司)的信息披露质量显著提高。这一结论对不同的估计方法、样本区间及控制变量组均保持稳健。异质性分析表明,对于盈余操纵水平较高、股价信息含量较低的企业,资本市场开放能够更好地改善其信息披露质量。本文丰富了资本市场开放对企业行为和绩效影响的实证研究,为继续推进资本市场开放政策提供了理论依据。</p>
<p><strong>关键词:</strong>  沪港通  资本市场开放  信息披露  文本分析</p>
</blockquote>
<br>
<p>李哲, &amp; 王文翰. (2021). “多言寡行” 的环境责任表现能否影响银行信贷获取——基于 “言” 和 “行” 双维度的文本分析. 金融研究, 498(12), 116-132.</p>
<blockquote>
<p><strong>摘要:</strong> 基于我国推行绿色信贷的政策背景,本文考察了企业“多言寡行”的环境责任表现能否影响银行的信贷决策。研究发现:(1)从总体来看,“多言寡行”的环境责任表现有助于企业获取更多的银行借款。(2)相比于长期银行借款,“多言寡行”对于短期银行借款的正向影响更为明显。(3)《关于构建绿色金融体系的指导意见》的出台抑制了“多言寡行”对银行借款的正向影响。(4)进一步分析发现,相比于环境责任表现“少言多行”以及“少言寡行”的企业,企业“多言寡行”的环境责任表现对于银行的信贷资源具有显著的正向影响;“多言寡行”对银行借款的正向影响在无背景关联、价值较低以及市场环境更差的企业中更为明显。本文有助于信贷机构认识到绿色信贷政策面临的执行风险,为确保绿色信贷的健康发展提供了新的决策参考。
<strong>关键词:</strong>  环境责任表现  绿色金融  绿色信贷  文本分析</p>
</blockquote>
<br>
<p>潘健平, 潘越, &amp; 马奕涵. (2019). 以 “合” 为贵? 合作文化与企业创新. 金融研究, 463(1), 148-167.</p>
<blockquote>
<p><strong>摘要:</strong> 本文以2006-2015年沪深A股非金融上市公司为样本,基于上市公司网站对于企业文化的叙述和年报董事会报告两份本文,采用文本分析方法,构建两个度量企业合作文化强弱的指标,并研究企业合作文化对企业创新产出和创新效率的影响。研究发现,企业文化越强调合作,企业的创新产出越多,创新效率越高。这一结论在采用增加控制变量、利用水稻播种面积作为工具变量以及以董事长的非正常离职事件为冲击进行PSM-DID等多种方法后仍然稳健。渠道检验的结果显示,合作文化是通过提高企业内部员工的凝聚力和促进企业的“产学研”合作这两种渠道来促进企业创新。进一步的研究表明,合作文化的促进作用在竞争性行业以及地区信任程度和产业集群程度较高的地区中尤为显著。本文不仅从微观层面揭示企业文化对公司财务行为的影响机理,丰富和补充了当前方兴未艾的“文化与金融”研究,而且为国家制定建设社会主义文化强国的方针战略提供理论基础和实证支持。</p>
<p><strong>关键词:</strong>  合作  企业文化  企业创新</p>
</blockquote>
<br>
<p>彭红枫, &amp; 林川. (2018). 言之有物: 网络借贷中语言有用吗?——来自人人贷借款描述的经验证据. 金融研究, 461(11), 133-153.</p>
<blockquote>
<p><strong>摘要:</strong> 本文以“人人贷”平台的388522条借款标的为样本,基于借款描述文本构造P2P网络借贷词典,并探究文本中六种类型词语比重对网络借贷行为的影响,实证结果表明:首先,各类词语比重发出的信号对贷款人的投资决策有显著影响,积极类词语和金融类词语比重与借款成功率呈正相关,消极类词语比重、强语气词语比重和弱语气词语比重均与借款成功率呈负相关关系;其次,不同年龄层次和不同收入水平的借款人提供的描述性文本中词语信号对贷款人行为的影响存在较大差异,而性别差异和学历高低基本不影响词语信号作用的发挥;最后,各类词语比重发出的质量信号是部分有效的,金融类词语比重发出的信号有效且被投资者正确识别,强语气词语比重发出的信号同样有效却未被投资者准确识别,其他类别词语比重不是有效质量信号。</p>
<p><strong>关键词:</strong>  网络借贷  文本分析  信号理论</p>
</blockquote>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>长期征稿</title>
      <link>https://textdata.cn/blog/call_for_paper/</link>
      <pubDate>Fri, 17 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/call_for_paper/</guid>
      <description>欢迎向我们提供Python/R技术、文本(数据)分析、经管科研(含Python或R)等内容的稿件</description>
      <content:encoded><![CDATA[<ul>
<li></li>
</ul>
<h2 id="引言">引言</h2>
<p>总有一些你不认识的人，知道你想知道的东西。『大邓和他的Python』 或许可以成为一座桥梁，在大数据时代，促使不同背景、不同方向的学者学术灵感相互碰撞，迸发出更多的可能性。</p>
<p>『大邓和他的Python』 鼓励分享 Python/R技术、文本(数据)分析、经管科研(含Python或R)等内容。目的只有一个，让数据科学在社会科学中更接地气。</p>
  <br>
<h2 id="内容选题">内容选题</h2>
<p>未来公众号的选题内容规划</p>
<ol>
<li>网络爬虫(数据采集)</li>
<li>文本、音频、视频、文件等数据处理</li>
<li>机器学习、自然语言处理</li>
<li>经管、社科领域，借助数据挖掘的研究和技术</li>
<li>Python相关技术分享</li>
<li>其他(待定)</li>
</ol>
  <br>
<h2 id="稿件要求">稿件要求</h2>
<ul>
<li>
<p>文章确系个人原创作品，未曾在公开渠道发表，
如为其他平台已发表或待发表的文章，请明确标注</p>
</li>
<li>
<p>稿件建议以 markdown 格式撰写，
示例链接: <a href="https://pan.baidu.com/s/1ZpvWhrGGbah71YbkW-7pjg">https://pan.baidu.com/s/1ZpvWhrGGbah71YbkW-7pjg</a> 提取码: upuc</p>
</li>
<li>
<p>投递邮件发送至 <a href="mailto:thunderhit@qq.com">thunderhit@qq.com</a></p>
</li>
</ul>
  <br>
<h2 id="作者福利">作者福利</h2>
<ul>
<li>
<p>尊重原作者署名权，并将为每篇被采纳的原创首发稿件，
提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算。</p>
</li>
<li>
<p>如作者内容分享成体系，文稿质量高，公众号可组织付费直播课。</p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>机器学习实战 | 信用卡欺诈检测</title>
      <link>https://textdata.cn/blog/ml_credit_card_fraud_detection/</link>
      <pubDate>Thu, 16 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/ml_credit_card_fraud_detection/</guid>
      <description>本文旨在使用 XGBoost、随机森林、KNN、逻辑回归、SVM 和决策树解决金融领域信用卡欺诈识别的分类问题</description>
      <content:encoded><![CDATA[<blockquote>
<p>作者: 小猴子</p>
<p>公众号: 机器学习研习院</p>
</blockquote>
<p>本文旨在使用 XGBoost、随机森林、KNN、逻辑回归、SVM 和决策树解决分类问题</p>
<h2 id="案例简介">案例简介</h2>
<p>假设你受雇于帮助一家信用卡公司检测潜在的欺诈案件，你的工作是确保客户不会因未购买的商品而被收取费用。给你一个包含人与人之间交易的数据集，他们是欺诈与否的信息，并要求你区分它们。我们的最终目的是通过构建分类模型来对欺诈交易进行分类区分来解决上述情况。</p>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="ml_credit_card_fraud_detection.zip">点击下载</a></p>
<br>
<p>对于这个案例，所需要用到的主要模块是处理数据的 Pandas、处理数组的 NumPy、用于数据拆分、构建和评估分类模型的 scikit-learn，最后是用于 xgboost 分类器模型算法的 xgboost 包。</p>
<br>
<h2 id="导入数据">导入数据</h2>
<p>关于数据： 我们将要使用的数据是 <strong>Kaggle 信用卡欺诈检测数据集</strong>。它包含特征 V1 到 V28，是 PCA 获得的主要成分，并忽略对构建模型没有用的时间特征。其余的特征是包含交易总金额的&quot;金额&quot;特征和包含交易是否为欺诈案件的&quot;类别&quot;特征。</p>
<p>现在使用&rsquo;pd.read_csv&rsquo;方法导入数据，并查看部分数据样例。</p>
<p>Kaggle 信用卡欺诈检测数据集: <a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">https://www.kaggle.com/mlg-ulb/creditcardfraud</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;creditcard.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>...</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.359807</td>
      <td>-0.072781</td>
      <td>2.536347</td>
      <td>1.378155</td>
      <td>-0.338321</td>
      <td>0.462388</td>
      <td>0.239599</td>
      <td>0.098698</td>
      <td>0.363787</td>
      <td>0.090794</td>
      <td>...</td>
      <td>-0.018307</td>
      <td>0.277838</td>
      <td>-0.110474</td>
      <td>0.066928</td>
      <td>0.128539</td>
      <td>-0.189115</td>
      <td>0.133558</td>
      <td>-0.021053</td>
      <td>149.62</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.191857</td>
      <td>0.266151</td>
      <td>0.166480</td>
      <td>0.448154</td>
      <td>0.060018</td>
      <td>-0.082361</td>
      <td>-0.078803</td>
      <td>0.085102</td>
      <td>-0.255425</td>
      <td>-0.166974</td>
      <td>...</td>
      <td>-0.225775</td>
      <td>-0.638672</td>
      <td>0.101288</td>
      <td>-0.339846</td>
      <td>0.167170</td>
      <td>0.125895</td>
      <td>-0.008983</td>
      <td>0.014724</td>
      <td>2.69</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.358354</td>
      <td>-1.340163</td>
      <td>1.773209</td>
      <td>0.379780</td>
      <td>-0.503198</td>
      <td>1.800499</td>
      <td>0.791461</td>
      <td>0.247676</td>
      <td>-1.514654</td>
      <td>0.207643</td>
      <td>...</td>
      <td>0.247998</td>
      <td>0.771679</td>
      <td>0.909412</td>
      <td>-0.689281</td>
      <td>-0.327642</td>
      <td>-0.139097</td>
      <td>-0.055353</td>
      <td>-0.059752</td>
      <td>378.66</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.966272</td>
      <td>-0.185226</td>
      <td>1.792993</td>
      <td>-0.863291</td>
      <td>-0.010309</td>
      <td>1.247203</td>
      <td>0.237609</td>
      <td>0.377436</td>
      <td>-1.387024</td>
      <td>-0.054952</td>
      <td>...</td>
      <td>-0.108300</td>
      <td>0.005274</td>
      <td>-0.190321</td>
      <td>-1.175575</td>
      <td>0.647376</td>
      <td>-0.221929</td>
      <td>0.062723</td>
      <td>0.061458</td>
      <td>123.50</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.158233</td>
      <td>0.877737</td>
      <td>1.548718</td>
      <td>0.403034</td>
      <td>-0.407193</td>
      <td>0.095921</td>
      <td>0.592941</td>
      <td>-0.270533</td>
      <td>0.817739</td>
      <td>0.753074</td>
      <td>...</td>
      <td>-0.009431</td>
      <td>0.798278</td>
      <td>-0.137458</td>
      <td>0.141267</td>
      <td>-0.206010</td>
      <td>0.502292</td>
      <td>0.219422</td>
      <td>0.215153</td>
      <td>69.99</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 30 columns</p>
</div>
<p>接下来将进行一些数据预处理和探索性数据分析（EDA）。</p>
<br>
<h2 id="探索性数据分析">探索性数据分析</h2>
<p>看看数据集中有多少欺诈案件和非欺诈案件。此外，还计算整个记录交易中欺诈案件的百分比。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">termcolor</span> <span class="kn">import</span> <span class="n">colored</span> <span class="k">as</span> <span class="n">cl</span>

<span class="n">cases</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">nonfraud_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">fraud_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">fraud_percentage</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">fraud_count</span><span class="o">/</span><span class="n">nonfraud_count</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;CASE COUNT&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Total number of cases are </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cases</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Number of Non-fraud cases are </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nonfraud_count</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Number of Non-fraud cases are </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fraud_count</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Percentage of fraud cases is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fraud_percentage</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mCASE COUNT[0m
[1m--------------------------------------------[0m
[1mTotal number of cases are 284807[0m
[1mNumber of Non-fraud cases are 284315[0m
[1mNumber of Non-fraud cases are 492[0m
[1mPercentage of fraud cases is 0.17[0m
[1m--------------------------------------------[0m
</code></pre>
<p>我们可以看到，在 <strong>284,807</strong> 个样本中，只有 <strong>492</strong> 个欺诈案例，仅占样本总数的 <strong>0.17%</strong> 。所以，可以说我们正在处理的数据是高度不平衡的数据，需要在建模和评估时谨慎处理。</p>
<p>接下来，我们将使用 Python 中的**&ldquo;describe&rdquo;**方法获取欺诈和非欺诈交易金额数据的统计视图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">nonfraud_cases</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">fraud_cases</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;CASE AMOUNT STATISTICS&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;NON-FRAUD CASE AMOUNT STATS&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nonfraud_cases</span><span class="o">.</span><span class="n">Amount</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;FRAUD CASE AMOUNT STATS&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fraud_cases</span><span class="o">.</span><span class="n">Amount</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mCASE AMOUNT STATISTICS[0m
[1m--------------------------------------------[0m
[1mNON-FRAUD CASE AMOUNT STATS[0m
count    284315.000000
mean         88.291022
std         250.105092
min           0.000000
25%           5.650000
50%          22.000000
75%          77.050000
max       25691.160000
Name: Amount, dtype: float64
[1m--------------------------------------------[0m
[1mFRAUD CASE AMOUNT STATS[0m
count     492.000000
mean      122.211321
std       256.683288
min         0.000000
25%         1.000000
50%         9.250000
75%       105.890000
max      2125.870000
Name: Amount, dtype: float64
[1m--------------------------------------------[0m
</code></pre>
<p>在查看统计数据时，可以看到与其余变量相比，&quot;<strong>金额</strong>&quot; 变量中的值变化很大。为了减少其广泛的值，我们可以使用 python 中的 &ldquo;<strong>StandardScaler()</strong>&rdquo; 方法对其进行标准化。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">amount</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Amount&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Amount&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">amount</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Amount&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1m0    0.244964
1   -0.342475
2    1.160686
3    0.140534
4   -0.073403
5   -0.338556
6   -0.333279
7   -0.190107
8    0.019392
9   -0.338516
Name: Amount, dtype: float64[0m
</code></pre>
<br>
<h2 id="特征选择和数据集拆分">特征选择和数据集拆分</h2>
<p>在这个过程中，定义自变量 (X) 和因变量 (Y)。使用定义的变量将数据分成训练集和测试集，进一步用于建模和评估。可以使用 python 中的 &ldquo;<strong>train_test_split</strong>&rdquo; 算法轻松拆分数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c1"># DATA SPLIT</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;X_train samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">X_train</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;X_test samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;y_train samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;y_test samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">y_test</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<pre><code>[1mX_train samples : [0m [[-1.11504743  1.03558276  0.80071244 -1.06039825  0.03262117  0.85342216
  -0.61424348 -3.23116112  1.53994798 -0.81690879 -1.30559201  0.1081772
  -0.85960958 -0.07193421  0.90665563 -1.72092961  0.79785322 -0.0067594
   1.95677806 -0.64489556  3.02038533 -0.53961798  0.03315649 -0.77494577
   0.10586781 -0.43085348  0.22973694 -0.0705913  -0.30145418]]
[1mX_test samples : [0m [[-0.32333357  1.05745525 -0.04834115 -0.60720431  1.25982115 -0.09176072
   1.1591015  -0.12433461 -0.17463954 -1.64440065 -1.11886302  0.20264731
   1.14596495 -1.80235956 -0.24717793 -0.06094535  0.84660574  0.37945439
   0.84726224  0.18640942 -0.20709827 -0.43389027 -0.26161328 -0.04665061
   0.2115123   0.00829721  0.10849443  0.16113917 -0.19330595]]
[1my_train samples : [0m [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[1my_test samples : [0m [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
</code></pre>
<p>到目前为止，已经做好了构建分类模型所需的所有准备。</p>
<br>
<h2 id="模型建立">模型建立</h2>
<p>这里构建六种不同类型的分类模型，即<strong>决策树、K-最近邻 (KNN)、逻辑回归、支持向量机 (SVM)、随机森林和 XGBoost</strong>。虽然我们还可以使用更多其他的模型，但我们选用的是用于解决分类问题的最流行模型。所有这些模型构建均比较方便，都可以使用 <strong>scikit-learn</strong> 包提供的算法来构建。仅对于 XGBoost 模型，将使用 xgboost 包。接下来在 python 中实现这些模型，所使用的算法可能需要花费一定的时间来实现。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>


<span class="c1"># MODELING</span>

<span class="c1"># 1. Decision Tree</span>
<span class="n">tree_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">criterion</span> <span class="o">=</span> <span class="s1">&#39;entropy&#39;</span><span class="p">)</span>
<span class="n">tree_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">tree_yhat</span> <span class="o">=</span> <span class="n">tree_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 2. K-Nearest Neighbors</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">n</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">knn_yhat</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 3. Logistic Regression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr_yhat</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 4. SVM </span>
<span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">svm_yhat</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 5. Random Forest Tree</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rf_yhat</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 6. XGBoost</span>
<span class="n">xgb</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">xgb_yhat</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div><p>至此我们构建了从决策树模型到 XGBoost 模型的六种不同类型的分类模型。</p>
<p>在决策树模型中，使用 <strong>&ldquo;DecisionTreeClassifier&rdquo;</strong> 算法来构建模型。在算法中，设置 <strong>&ldquo;max_depth=4&rdquo;</strong>，意味着允许树最大分裂四次，<strong>&ldquo;criterion = &lsquo;entropy&rdquo;</strong>，与**&ldquo;max_depth&rdquo;**最相似，但决定何时分裂停止分裂树。最后拟合模型后将预测值存储到 <strong>&ldquo;tree_yhat&rdquo;</strong> 变量中。</p>
<p>在K-最近邻 (KNN)中，使用 <strong>&ldquo;KNeighborsClassifier&rdquo;</strong> 算法构建了模型，并设置 <strong>&ldquo;n_neighbors=5&rdquo;</strong>。 <strong>&lsquo;n_neighbors&rsquo;</strong> 的值是随机选择的，其实可以通过迭代一系列值来有目的地选择，然后拟合模型后将预测值存储到 <strong>&ldquo;knn_yhat&rdquo;</strong> 变量中。</p>
<p>逻辑回归的代码没有什么可解释的，因为我使用 <strong>&ldquo;LogisticRegression&rdquo;</strong> 算法并全部使用默认值，并拟合模型后将预测值存储到 <strong>&ldquo;lr_yhat&rdquo;</strong> 变量中。</p>
<p>使用&quot;SVC&quot;算法构建了支持向量机模型，并且同样使用默认值，并且默认内核就是我们所希望用到的模型，即&quot;rbf&quot;内核。之后，我们在拟合模型后将预测值存储到 &ldquo;svm_yhat&rdquo; 中。</p>
<p>接下来使用 <strong>&ldquo;RandomForestClassifier&rdquo;</strong> 算法构建的随机森林模型，设置参数 <strong>&ldquo;max_depth=4&rdquo;</strong>，就像构建决策树模型的方式一样。最后在拟合模型后将预测值存储到 <strong>&ldquo;rf_yhat&rdquo;</strong> 中。请记住，决策树和随机森林之间的主要区别在于，决策树使用整个数据集来构建单个模型，而随机森林使用随机选择的特征来构建多个模型。这就是为什么很多情况下选择使用随机森林模型而不是决策树的原因。</p>
<p>最后是 XGBoost 模型。使用 xgboost 包提供的 <strong>&ldquo;XGBClassifier&rdquo;</strong> 算法构建模型。设置 <strong>&ldquo;max_depth=4&rdquo;</strong>，最后在拟合模型后将预测值存储到 &ldquo;xgb_yhat&rdquo; 中。</p>
<p>至此，我们成功构建了六种分类模型，为了便于理解，对代码进行了简单解释。接下来需要评估每个模型，并找到最适合我们案例的模型。</p>
<br>
<h2 id="模型评估">模型评估</h2>
<p>之前有提到过，我们将使用 scikit-learn 包提供的评估指标来评估我们构建的模型。在此过程中的主要目标是为给定案例找到最佳模型。这里将使用的评估指标是<strong>准确度评分指标、f1 评分指标，及混淆矩阵</strong>。</p>
<h3 id="准确率">准确率</h3>
<p>准确率是最基本的评价指标之一，广泛用于评价分类模型。准确率分数的计算方法很简单，就是将模型做出的正确预测的数量除以模型做出的预测总数（可以乘以 100 将结果转换为百分比）。一般可以表示为：</p>
<p><strong>准确度分数 = 正确预测数 / 总预测数</strong></p>
<p>我们检查我们所构建的六种不同分类模型的准确率分数。要在 python 中完成，我们可以使用 scikit-learn 包提供的 <strong>&ldquo;accuracy_score&rdquo;</strong> 方法。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># 1. Accuracy score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;ACCURACY SCORE&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the Decision Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the KNN model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the Logistic Regression model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the SVM model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">svm_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the Random Forest Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the XGBoost model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">xgb_yhat</span><span class="p">)),</span> 
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mACCURACY SCORE[0m
[1mAccuracy score of the Decision Tree model is 0.9993679997191109[0m
[1m[32mAccuracy score of the KNN model is 0.9995259997893332[0m
[1m[31mAccuracy score of the Logistic Regression model is 0.9991924440855307[0m
[1mAccuracy score of the SVM model is 0.9993153330290369[0m
[1mAccuracy score of the Random Forest Tree model is 0.9993153330290369[0m
[1mAccuracy score of the XGBoost model is 0.9994908886626171[0m
</code></pre>
<p>根据准确性评分评估指标来看，<strong>KNN</strong> 模型为最准确的模型，而 <strong>Logistic</strong> 回归模型最不准确。然而，当我们对每个模型的结果进行四舍五入时，得到 99% 的准确性，这看是一个非常好的分数。</p>
<h3 id="f1-score">F1-score</h3>
<p>F1-score 或 F-score 是用于评估分类模型的最流行的评估指标之一。它可以简单地定义为<strong>模型的准确率和召回率的调和平均值</strong>。它的计算方法是将 模型的精度和召回率的乘积除以模型的精度和召回率相加得到的值，最后乘以 2 得到的值。可以表示为：</p>
<p><strong>F1-score  = 2( (精度 * 召回率) / (精度 + 召回率) )</strong></p>
<p>可以使用 scikit-learn 包提供的 &ldquo;f1_score&rdquo; 方法轻松计算 <strong>F1-score</strong> 。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span> 

<span class="c1"># 2. F1 score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 SCORE&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the Decision Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the KNN model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the Logistic Regression model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the SVM model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">svm_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the Random Forest Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the XGBoost model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">xgb_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mF1 SCORE[0m
[1mF1 score of the Decision Tree model is 0.8105263157894738[0m
[1m[32mF1 score of the KNN model is 0.8571428571428572[0m
[1m[31mF1 score of the Logistic Regression model is 0.7356321839080459[0m
[1mF1 score of the SVM model is 0.7771428571428572[0m
[1mF1 score of the Random Forest Tree model is 0.7796610169491525[0m
[1mF1 score of the XGBoost model is 0.8449197860962566[0m
</code></pre>
<p>模型的排名几乎与之前的评估指标相似。在 F1-score 评估指标的基础上，KNN 模型再次夺得第一，Logistic 回归模型仍然是最不准确的模型。</p>
<h3 id="混淆矩阵">混淆矩阵</h3>
<p>通常，混淆矩阵是分类模型的可视化，显示模型与原始结果相比预测结果的程度。通常，预测结果存储在一个变量中，然后将其转换为相关表。使用相关表，以热图的形式绘制混淆矩阵。尽管有多种内置方法可以可视化混淆矩阵，但我们将从零开始定义和可视化它，以便更好地理解。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 3. Confusion Matrix</span>
<span class="c1"># defining the plot function</span>
<span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Confusion Matrix of </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    
    <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
    <span class="kn">import</span> <span class="nn">itertools</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span> <span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span> <span class="o">=</span> <span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">rotation</span> <span class="o">=</span> <span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

    <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;.2f&#39;</span> <span class="k">if</span> <span class="n">normalize</span> <span class="k">else</span> <span class="s1">&#39;d&#39;</span>
    <span class="n">thresh</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">fmt</span><span class="p">),</span>
                 <span class="n">horizontalalignment</span> <span class="o">=</span> <span class="s1">&#39;center&#39;</span><span class="p">,</span>
                 <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;white&#39;</span> <span class="k">if</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span> <span class="k">else</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True label&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted label&#39;</span><span class="p">)</span>
    
<span class="c1"># Compute confusion matrix for the models</span>

<span class="n">tree_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Decision Tree</span>
<span class="n">knn_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> 
                <span class="n">knn_yhat</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># K-Nearest Neighbors</span>
<span class="n">lr_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Logistic Regression</span>
<span class="n">svm_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">svm_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Support Vector Machine</span>
<span class="n">rf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Random Forest Tree</span>
<span class="n">xgb_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">xgb_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># XGBoost</span>

<span class="c1"># Plot the confusion matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</code></pre></div><h3 id="decision-tree">Decision tree</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">tree_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">tree_matrix</span><span class="p">,</span> 
                      <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                      <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Decision Tree&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;tree_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_21_0.png" alt="png"  />

​</p>
<h3 id="k-nearest-neighbors">K-Nearest Neighbors</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">knn_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">knn_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;KNN&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;knn_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_23_0.png" alt="png"  />

​</p>
<h3 id="logistic-regression">Logistic regression</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">lr_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">lr_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Logistic Regression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;lr_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_25_0.png" alt="png"  />

​</p>
<h3 id="support-vector-machine">Support Vector Machine</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">svm_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">svm_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;SVM&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;svm_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_27_0.png" alt="png"  />

​</p>
<h3 id="random-forest">Random Forest</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">rf_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">rf_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Random Forest Tree&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;rf_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_29_0.png" alt="png"  />

​</p>
<h3 id="xgboost">XGBoost</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">xgb_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">xgb_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;XGBoost&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;xgb_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_31_0.png" alt="png"  />

​</p>
<p><strong>混淆矩阵理解</strong>： 以XGBoost模型的混淆矩阵为例。</p>
<ul>
<li>
<p>第一行。
第一行是测试集中实际欺诈值为0的交易。可以计算，其中56861笔欺诈值为0。在这56861笔非欺诈交易中，分类器正确预测了其中的56854笔为 0 和 预测了其中 7 为 1。这意味着，对于 56854 笔非欺诈交易，测试集中的实际流失值为 0，分类器也正确预测为 0。可以说我们的模型已经对非欺诈交易进行了分类交易还不错。</p>
</li>
<li>
<p>第二行。
看起来有 101 笔交易的欺诈值为 1。分类器正确预测其中 79 笔为 1，错误预测值为 0 的 22 笔。错误预测值可以视为模型的错误。</p>
</li>
</ul>
<p>在比较所有模型的混淆矩阵时可以看出，K-Nearest Neighbors 模型在从非欺诈交易中分类欺诈交易方面做得非常好，其次是 XGBoost 模型。所以可以得出结论，最适合本次案例的模型是 <strong>K-Nearest Neighbors</strong> 模型，可以忽略的模型是 Logistic 回归模型。</p>
<br>
<h2 id="写在最后">写在最后</h2>
<p>经过一连串的过程，我们已经成功构建了从决策树模型到XGBoost模型的六种不同类型的分类模型。随后使用评估指标评估了每个模型，并选择了最适合给定案例的模型。</p>
<p>在本文中，我们只选用了6个相对流行的模型，其实还有更多模型需要探索。此外，虽然我们很轻松地在 python 中可行地构建了模型，但是每个模型背后都有很多的数学和统计数据，在有精力的情况下，可以去了解下这么模型背后的数学推理。</p>
<h2 id="参考资料">参考资料</h2>
<p>[1] Kaggle 信用卡欺诈检测数据集: <a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">https://www.kaggle.com/mlg-ulb/creditcardfraud</a></p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>欢迎各位向cntext库分享情感词典</title>
      <link>https://textdata.cn/blog/share_your_dict_to_cntext/</link>
      <pubDate>Sun, 12 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/share_your_dict_to_cntext/</guid>
      <description>随着cntext内置词典丰富了，使用cntext做文本分析会更好用、更易用。</description>
      <content:encoded><![CDATA[<p>前几天刚刚分享 <a href="https://textdata.cn/blog/liwc_python_text_mining/">LIWC vs Python  | 文本分析之词典词频法略讲(含代码)</a>，<strong>借鉴LIWC，我觉得中文也需要有社科类的中文情感词典库，如果能汇聚已发表论文中的中文情感词典，如用户生成内容UGC那样，那么中文文本分析也会变的容易</strong>。下图是LIWC用户分享词典界面。</p>
<h2 id="liwc用户分享词典">LIWC用户分享词典</h2>
<p>没有购买LIWC是看不到截图中的「USER-CREATED LIWC DICTIONARIES」。涉及版权，英文词典文件不作分享，一起尊重知识。</p>
<p><img loading="lazy" src="liwc-ugc.png" alt=""  />
</p>
<p><strong>中文领域有很多发表出来的各研究领域的情感词典，如果有词典推荐，欢迎thunderhit@qq.com联系我，我可以将词典整理为cntext内置格式。</strong></p>
<br>
<p><strong>假设cntext内置词典丰富了，使用cntext做如下文本分析操作。</strong></p>
<br>
<h2 id="案例-cntext操作">案例: cntext操作</h2>
<h3 id="cntext内置词典">cntext内置词典</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#cntext版本</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;cntext版本: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>

<span class="c1">#查看cntext内置词典</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&#39;cntext版本: 1.7.1&#39;

[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;,
 &#39;concreteness.pkl&#39;]
</code></pre></div><br>
<h3 id="导入内置pkl词典">导入内置pkl词典</h3>
<p>cntext内词典正在规范化，理想的规范词典应该含有词语列表、Desc简介和Referer参考文献三部分。例如，大连理工大学情感本体库词典DUTIR.pkl</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dutir</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)</span>
<span class="n">dutir</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, &#39;伤害&#39;,...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...],
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,...]},
 
 &#39;Desc&#39;: &#39;大连理工大学情感本体库，细粒度情感词典。含七大类情绪，依次是哀, 好, 惊, 惧, 乐, 怒, 恶&#39;,
 
 &#39;Referer&#39;: &#39;徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.&#39;}
</code></pre></div><p>dutir返回了</p>
<ul>
<li>词典数据</li>
<li>Desc词典介绍</li>
<li>Referer词典文献出处</li>
</ul>
<br>
<br>
<h3 id="用cntext做情感计算">用cntext做情感计算</h3>
<p>情感分析，统计文本中某类词出现个数，使用cntext.sentiment函数即可实现。</p>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text:  文本字符串</li>
<li>diction:  情感词典</li>
<li>lang: 语言类型，&ldquo;chinese&rdquo; or &ldquo;english&rdquo;; 默认lang=&ldquo;chinese&rdquo;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#自定义词典</span>
<span class="n">diy_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="c1">#cntext内置词典-DUTIR</span>
<span class="n">dutir</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">]</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="c1">#使用diy_dict做情感分析</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
                   <span class="n">diction</span><span class="o">=</span><span class="n">diy_dict</span><span class="p">,</span> 
                   <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
<span class="c1">#使用DUTIR做情感分析    </span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
                   <span class="n">diction</span><span class="o">=</span><span class="n">dutir</span><span class="p">,</span> 
                   <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
 
 
 {&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<h2 id="liwc用户分享词典-1">LIWC用户分享词典</h2>
<p>以下内容整理自LIWC网站，我添加了doi及中文翻译。由于没有阅读每个词典对应的文献，词典简介翻译可能会有差错。</p>
<p>以下词典仅仅是介绍，有疑惑的可以点击doi，找到对应论文进行理解。</p>
<p><strong>由于版权问题，词典文件资源不作分享</strong>。</p>
<p><img loading="lazy" src="liwc-ugc.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th>Dictionary</th>
<th>Desc</th>
<th>Author</th>
<th>Date</th>
<th>DOI</th>
</tr>
</thead>
<tbody>
<tr>
<td>Absolutist</td>
<td>Measure absolutist thinking in texts (eg, always, never)衡量文本中的绝对主义思维（例如，always、never）</td>
<td>Al-Mosaiwi &amp; Johnstone</td>
<td>2018</td>
<td><a href="https://doi.org/10.1177/2167702617747074">https://doi.org/10.1177/2167702617747074</a></td>
</tr>
<tr>
<td>Age_Stereotypes</td>
<td>Reflects eight broadly-defined stereotypes identified in past research as descriptive of older adults,such as <code>impaired, despondent, shrew,        recluse, vulnerable, golden, grandparent, conservative</code><br>反映过去研究中确定的八种广泛定义的刻板印象(用于描述老年人)，例如“受损、沮丧、泼妇、隐士、脆弱、黄金、祖父母、保守”</td>
<td>Jessica Remedios</td>
<td>2010</td>
<td><a href="https://doi.org/10.1080/15298860903054175">https://doi.org/10.1080/15298860903054175</a></td>
</tr>
<tr>
<td>Agitation&amp;Dejection</td>
<td>Based on studies linking promotion versus prevention focus with the emotions “Agitation” and “Dejection”<br>基于将促进与预防重点与情绪“激动”和“沮丧”联系起来的研究</td>
<td>Johnsen et al.</td>
<td>2014</td>
<td><a href="https://doi.org/10.2147/PRBM.S54947">https://doi.org/10.2147/PRBM.S54947</a></td>
</tr>
<tr>
<td>Behavioral_Activation</td>
<td>Captures linguistic indicators of planning and participation in enjoyable activities<br>捕捉规划和参与愉快活动的语言指标</td>
<td>Burkhardt et al.</td>
<td>2021</td>
<td><a href="https://doi.org/10.2196/28244">https://doi.org/10.2196/28244</a></td>
</tr>
<tr>
<td>Big_Two</td>
<td>Measure the degree to which a person is thinking in terms of <strong>Agency/Communion</strong>.<br>衡量一个人在<strong>机构/交流</strong>方面的思考程度。</td>
<td>Pietraszkiewicz et al.</td>
<td>2019</td>
<td><a href="https://doi.org/10.1002/ejsp.2561">https://doi.org/10.1002/ejsp.2561</a></td>
</tr>
<tr>
<td>Brand_Personality</td>
<td>Assesses Aaker’s five brand personality dimensions as well as 42 personality trait norms<br>评估 Aaker 的五个品牌个性维度以及 42 个个性特征规范</td>
<td>Opoku et al.</td>
<td>2008</td>
<td><a href="https://doi.org/10.1080/08841240802100386">https://doi.org/10.1080/08841240802100386</a></td>
</tr>
<tr>
<td>Controversial_Terms</td>
<td>A lexicon of terms that range in their degree of controversiality, particularly in terms of their use in the media.<br>具有争议程度的术语词典，特别是在媒体中的使用方面。</td>
<td>Mejova et al.</td>
<td>2014</td>
<td><a href="http://arxiv.org/abs/1409.8152">http://arxiv.org/abs/1409.8152</a></td>
</tr>
<tr>
<td>Corporate_Social_Responsibility</td>
<td>Reveals four dimensions of corporate social responsibility<br>揭示企业社会责任的四个维度</td>
<td>Nadra Pencle &amp; Irina Mălăescu</td>
<td>2016</td>
<td><a href="https://doi.org/10.2308/jeta-51615">https://doi.org/10.2308/jeta-51615</a></td>
</tr>
<tr>
<td>Cost_Benefit</td>
<td>Measures language related to perceived costs and benefits that result from a decision or behavior.<br>衡量与决策或行为导致的感知成本和收益相关的语言。</td>
<td>Michael McCullough</td>
<td>2006</td>
<td><a href="https://doi.org/10.1037/0022-006X.74.5.887">https://doi.org/10.1037/0022-006X.74.5.887</a></td>
</tr>
<tr>
<td>Creativity&amp;Innovation</td>
<td>Language describing creation and/or innovation<br>描述创造和/或创新的语言</td>
<td>Neufeld and Gaucher</td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td>Crovitz_Innovator_Identification</td>
<td>Identify “innovators” and “non-innovators” using Hebert F. Crovitz’s 42 relational words<br>使用 Hebert F. Crovitz 的 42 个相关词识别“创新者”和“非创新者”</td>
<td>Greco et al.</td>
<td>2021</td>
<td><a href="https://doi.org/10.1007/s11135-020-01038-x">https://doi.org/10.1007/s11135-020-01038-x</a></td>
</tr>
<tr>
<td>extended_Moral_Foundations_Dictionary(eMFD)</td>
<td>The eMFD, unlike previous methods, is constructed from text annotations generated by a large sample of human coders.<br>与以前的方法不同，eMFD 是由大量人类编码人员生成的文本注释构成的。</td>
<td>Hopp et al.</td>
<td>2021</td>
<td><a href="https://doi.org/10.3758/s13428-020-01433-0">https://doi.org/10.3758/s13428-020-01433-0</a></td>
</tr>
<tr>
<td>Foresight</td>
<td>Measures the degree to which anticipation/foresight occurs. That is, words pointing to indicate where things are heading (often on the basis of recurrent behaviors).<br>衡量预期/预见发生的程度。 也就是说，指向事物前进方向的词语（通常基于反复出现的行为）。</td>
<td>Robert Hogenraad</td>
<td>2020</td>
<td><a href="https://doi.org/10.1007/s11135-020-01071-w">https://doi.org/10.1007/s11135-020-01071-w</a></td>
</tr>
<tr>
<td>Imagination</td>
<td>Digital lexicon of 627 entries relative to imagination and transfiguration, i.e., words pointing to the unbelievable and whatever is beyond the real.<br>与想象和变形相关的 627 个条目的数字词典，即指向令人难以置信的事物和超越真实事物的词语。</td>
<td>Robert Hogenraad</td>
<td>2019</td>
<td><a href="https://doi.org/10.1007/s11135-018-0813-7">https://doi.org/10.1007/s11135-018-0813-7</a></td>
</tr>
<tr>
<td>Global_Citizen</td>
<td>A dictionary to assess language usage related to global citizenship<br>用于评估与全球公民相关的语言使用情况的词典</td>
<td>Stephen Reysen et al.</td>
<td>2014</td>
<td><a href="https://doi.org/10.4018/ijcbpl.2014100101">https://doi.org/10.4018/ijcbpl.2014100101</a></td>
</tr>
<tr>
<td>Grant_Evaluation</td>
<td>Captures categories relevant to scientific grant review (ability, achievement, agentic, research, standout, pos eval, neg eval)<br>捕获与科学资助审查相关的类别（能力、成就、代理、研究、杰出、正面、负面）</td>
<td>Kaatz et al.</td>
<td>2015</td>
<td><a href="https://doi.org/10.1097/ACM.0000000000000442">https://doi.org/10.1097/ACM.0000000000000442</a></td>
</tr>
<tr>
<td>Home_Perceptions</td>
<td>Calculates the frequency of words describing clutter, a sense of the home as unfinished, restful words, and nature words<br>计算描述杂乱、未完成的家感、宁静的词和自然词的频率</td>
<td>Saxbe &amp; Repetti</td>
<td>2022-01-01</td>
<td><a href="https://doi.org/10.1177/0146167209352864">https://doi.org/10.1177/0146167209352864</a></td>
</tr>
<tr>
<td>Invective Dictionary</td>
<td>Use this dictionary to detect invective language in narrative<br/></td>
<td>A. T. Panter</td>
<td>2022-01-01</td>
<td></td>
</tr>
<tr>
<td>Linguistic_Category_Model</td>
<td>A computerized LCM analysis method<br/>使用这本词典检测叙事中的谩骂语言</td>
<td>Yi-Tai Seih</td>
<td>2017</td>
<td><a href="https://doi.org/10.1177/0261927X16657855">https://doi.org/10.1177/0261927X16657855</a></td>
</tr>
<tr>
<td>Loughran_McDonald_Financial_Sentiment</td>
<td>Dictionary for measuring positive and negative sentiment specifically in financial texts.This is the 2018 version of the dictionary.<br/>专门用于衡量金融文本中正面和负面情绪的字典。这是 2018 年版的字典。</td>
<td>Loughran &amp; McDonald</td>
<td>2011</td>
<td><a href="https://doi.org/10.1111/j.1540-6261.2010.01625.x">https://doi.org/10.1111/j.1540-6261.2010.01625.x</a></td>
</tr>
<tr>
<td>Masculine_and_Feminine</td>
<td>List of masculine and feminine words from Gaucher et al. (2011)<br/>Gaucher 等人的男性化和女性化词列表。 (2011)</td>
<td>Maureen McCusker</td>
<td>2011</td>
<td><a href="https://doi.org/10.1037/a0022530">https://doi.org/10.1037/a0022530</a></td>
</tr>
<tr>
<td>Mindfulness</td>
<td>Two categories of mindfulness language describing the mindfulness state and the more encompassing “mindfulness journey”<br/>描述正念状态的两类正念语言和更全面的“正念之旅”</td>
<td>Collins et al.</td>
<td>2009</td>
<td><a href="https://doi.org/10.1037/a0017579">https://doi.org/10.1037/a0017579</a></td>
</tr>
<tr>
<td>Mind_Perception</td>
<td>Measures linguistic use of mind perception (words related to “agency” and “experience”) in naturalistic settings<br/>在自然主义环境中测量心理感知（与“agency”和“experience”相关的词）的语言使用</td>
<td>Schweitzer &amp; Waytz</td>
<td>2020</td>
<td><a href="https://doi.org/10.1037/xge0001013">https://doi.org/10.1037/xge0001013</a></td>
</tr>
<tr>
<td>Moral_Foundations_v2.0</td>
<td>An updated version of the Moral Foundations Dictionary that is recommended over the original by its creators.<br/>道德词典的更新版本，由其创建者推荐。</td>
<td>Jeremy Frimer</td>
<td>2019</td>
<td><a href="https://doi.org/10.1016/j.jrp.2019.103906">https://doi.org/10.1016/j.jrp.2019.103906</a></td>
</tr>
<tr>
<td>Moral_Justification</td>
<td>Measures variation in justification content (deontological, consequentialist, or emotive) as a function of moral foundations<br/>衡量辩护内容（道义论、后果论或情感论）随道德基础的变化</td>
<td>Wheeler &amp; Laham</td>
<td>2016</td>
<td><a href="https://doi.org/10.1177/0146167216653374">https://doi.org/10.1177/0146167216653374</a></td>
</tr>
<tr>
<td>Personal_Values_Dictionary</td>
<td>Measures the 10 Schwartz Values (and 4 higher-order value dimensions).<br/>测量 10 个 Schwartz 值（和 4 个高阶值维度）。</td>
<td>Ponizovskiy et al.</td>
<td>2020</td>
<td><a href="https://doi.org/10.1002/per.2294">https://doi.org/10.1002/per.2294</a></td>
</tr>
<tr>
<td>Prosocial_Words</td>
<td>Calculates the density of prosocial words in anything that a person says<br/>计算一个人所说的任何内容中亲社会词的密度</td>
<td>Jeremy Frimer</td>
<td>2022-01-01</td>
<td><a href="https://doi.org/10.1073/pnas.1500355112">https://doi.org/10.1073/pnas.1500355112</a></td>
</tr>
<tr>
<td>Regulatory_Mode</td>
<td>Locomotion and Assessment States of Goal Pursuit<br/>目标追求的运动和评估状态</td>
<td>Dana Kanze, Mark A. Conley, and E. Tory Higgins</td>
<td>2019</td>
<td><a href="https://doi.org/10.1016/j.obhdp.2019.04.002">https://doi.org/10.1016/j.obhdp.2019.04.002</a></td>
</tr>
<tr>
<td>Security_Language</td>
<td>Provides a reference for the comparative study of security-related linguistic repertoires in political texts (speeches, policy documents, etc.).<br/>为政治文本（演讲、政策文件等）中与安全相关的语言库的比较研究提供参考。</td>
<td>Stephane Baele &amp; Olivier Sterck</td>
<td>2014</td>
<td><a href="https://doi.org/10.1111/1467-9248.12147">https://doi.org/10.1111/1467-9248.12147</a></td>
</tr>
<tr>
<td>Self-Care</td>
<td>Measures the degree to which self-care words are used (e.g., diet, yoga)<br/>衡量自我保健词的使用程度（例如，饮食、瑜伽）</td>
<td>Xunyi Wang et al.</td>
<td>2018</td>
<td><a href="https://doi.org/10.1093/jamia/ocy012">https://doi.org/10.1093/jamia/ocy012</a></td>
</tr>
<tr>
<td>Stereotype_Content</td>
<td>A stereotype content dictionary, made using a semi-automated method, to capture the Stereotype Content Model in text<br/>使用半自动化方法制作的刻板印象内容字典，用于捕获文本中的刻板印象内容模型</td>
<td>Nicolas et al.</td>
<td>2022-01-01</td>
<td><a href="https://doi.org/10.1002/ejsp.2724">https://doi.org/10.1002/ejsp.2724</a></td>
</tr>
<tr>
<td>Stress</td>
<td>A dictionary used to measure psychological stress. Created based on the LIWC2007 English Dictionary.<br/>用来测量心理压力的字典。 根据 LIWC2007 英语词典创建。</td>
<td>Wei Wang et al.</td>
<td>2022-01-01</td>
<td><a href="https://doi.org/10.1111/apps.12065">https://doi.org/10.1111/apps.12065</a></td>
</tr>
<tr>
<td>Well_Being</td>
<td>Words that might indicate the presence of purpose or meaning<br/>可能表明存在目的或意义的词</td>
<td>Ratner et al.</td>
<td>2019</td>
<td><a href="https://doi.org/10.1080/10888691.2019.1659140">https://doi.org/10.1080/10888691.2019.1659140</a></td>
</tr>
</tbody>
</table>
<br>
<h2 id="分享词典">分享词典</h2>
<p><strong>中文领域有很多发表出来的各研究领域的情感词典，如果有词典推荐，欢迎thunderhit@qq.com联系我，我会将词典整理为cntext内置格式。</strong></p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>实战 | 构建基于客户细分的 K-Means 聚类算法！</title>
      <link>https://textdata.cn/blog/customer_segment_with_kmeans/</link>
      <pubDate>Thu, 09 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/customer_segment_with_kmeans/</guid>
      <description>客群细分对于企业了解目标受众非常重要。根据受众群体的不同，我们可以给采取不同的营销策略。目前有许多无监督的机器学习算法可以帮助公司识别他们的用户群并创建消费群体。</description>
      <content:encoded><![CDATA[<p>客群细分对于企业了解目标受众非常重要。根据受众群体的不同，我们可以给采取不同的营销策略。目前有许多无监督的机器学习算法可以帮助公司识别他们的用户群并创建消费群体。</p>
<p>在本文中，我将分享一种目前比较流行的 K-Means 聚类的无监督学习技术。K-Means的目标是将所有可用的数据分组为彼此不同的不重叠的子组。K-Means聚类是数据科学家用来帮助公司进行客户细分的常用技术。</p>
<p>在本文中，你将了解以下内容：</p>
<ul>
<li>K-Means聚类的数据预处理</li>
<li>从头构建K-Means聚类算法</li>
<li>用于评估聚类模型性能的指标</li>
<li>可视化构建簇类</li>
<li>簇类构建的解读与分析</li>
</ul>
<h2 id="代码下载">代码下载</h2>
<p><a href="customer_segment_with_kmeans.zip">点击下载</a></p>
<br>
<h2 id="预备知识">预备知识</h2>
<p>在开始之前安装以下库：pandas、numpy、matplotlib、seaborn、sciket learn、kneed。完成后，我们就可以开始制作模型了！</p>
<p>本文中要的数据集可以文末下载，运行以下代码行以导入必要的库并读取数据集：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Mall_Customers.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CustomerID</th>
      <th>Gender</th>
      <th>Age</th>
      <th>Annual Income (k$)</th>
      <th>Spending Score (1-100)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Male</td>
      <td>19</td>
      <td>15</td>
      <td>39</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Male</td>
      <td>21</td>
      <td>15</td>
      <td>81</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Female</td>
      <td>20</td>
      <td>16</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Female</td>
      <td>23</td>
      <td>16</td>
      <td>77</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Female</td>
      <td>31</td>
      <td>17</td>
      <td>40</td>
    </tr>
  </tbody>
</table>
</div>
<p>数据集中有五个变量。CustomerID是数据集中每个客户的唯一标识符，我们可以删除这个变量。它没有为我们提供任何有用的集群信息。由于 gender 是一个分类变量，它需要编码并转换成数字。</p>
<p>在输入模型之前，其他所有变量都将按正态分布进行缩放。我们将标准化这些变量，平均值为0，标准偏差为1。</p>
<br>
<h2 id="标准化变量">标准化变量</h2>
<p>首先，让我们标准化数据集中的所有变量，使它们在相同的范围内。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">col_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Annual Income (k$)&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Spending Score (1-100)&#39;</span><span class="p">]</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col_names</span><span class="p">]</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">scaled_features</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">col_names</span><span class="p">)</span>
<span class="n">scaled_features</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Annual Income (k$)</th>
      <th>Age</th>
      <th>Spending Score (1-100)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.738999</td>
      <td>-1.424569</td>
      <td>-0.434801</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.738999</td>
      <td>-1.281035</td>
      <td>1.195704</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.700830</td>
      <td>-1.352802</td>
      <td>-1.715913</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.700830</td>
      <td>-1.137502</td>
      <td>1.040418</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.662660</td>
      <td>-0.563369</td>
      <td>-0.395980</td>
    </tr>
  </tbody>
</table>
</div>
<p>我们可以看到所有的变量都被转换了，现在都以零为中心。</p>
<br>
<h2 id="热编码">热编码</h2>
<p>变量&quot;gender&quot;是分类变量，我们需要把它转换成一个数值变量，可以用pd.get_dummies()来处理。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">gender</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Gender&#39;</span><span class="p">]</span>
<span class="n">newdf</span> <span class="o">=</span> <span class="n">scaled_features</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gender</span><span class="p">)</span>

<span class="n">newdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">newdf</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix_sep</span><span class="o">=</span><span class="s1">&#39;_&#39;</span><span class="p">,</span> <span class="n">dummy_na</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">newdf</span> <span class="o">=</span> <span class="n">newdf</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Gender_Male&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">newdf</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Annual Income (k$)</th>
      <th>Age</th>
      <th>Spending Score (1-100)</th>
      <th>Gender_Female</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.738999</td>
      <td>-1.424569</td>
      <td>-0.434801</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.738999</td>
      <td>-1.281035</td>
      <td>1.195704</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.700830</td>
      <td>-1.352802</td>
      <td>-1.715913</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.700830</td>
      <td>-1.137502</td>
      <td>1.040418</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.662660</td>
      <td>-0.563369</td>
      <td>-0.395980</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<p>可以看到，性别变量已经发生了变化，从数据框中删除了“Gender_Male”。这是因为不需要再保留变量了。</p>
<br>
<h2 id="建立聚类模型">建立聚类模型</h2>
<p>让我们构建一个 K-means 聚类模型，并将其拟合到数据集中的所有变量上，我们用肘部图可视化聚类模型的性能，它会告诉我们在构建模型时使用的「最佳聚类数」。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">SSE</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">newdf</span><span class="p">)</span>
    <span class="n">SSE</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="c1"># converting the results into a dataframe and plotting them</span>

<span class="n">frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Cluster&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="s1">&#39;SSE&#39;</span><span class="p">:</span><span class="n">SSE</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">frame</span><span class="p">[</span><span class="s1">&#39;Cluster&#39;</span><span class="p">],</span> <span class="n">frame</span><span class="p">[</span><span class="s1">&#39;SSE&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of clusters&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Inertia&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>Text(0, 0.5, 'Inertia')
</code></pre>
<p>​ <br>
<img loading="lazy" src="output_9_1.png" alt="png"  />

​</p>
<p>根据上面的「肘部图」，我们可以看到最佳聚类数为「4」</p>
<br>
<h2 id="轮廓系数">轮廓系数</h2>
<p>轮廓系数或轮廓分数是用于评估该算法创建的簇的质量的方法。轮廓分数在-1到+1之间。轮廓分数越高，模型越好。轮廓分数度量同一簇中所有数据点之间的距离。这个距离越小，轮廓分数就越好。</p>
<p>让我们计算一下我们刚刚建立的模型的轮廓分数：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span>
<span class="c1"># First, build a model with 4 clusters</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">newdf</span><span class="p">)</span>

<span class="c1"># Now, print the silhouette score of this model</span>

<span class="nb">print</span><span class="p">(</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">newdf</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))</span>
</code></pre></div><pre><code>0.35027020434653977
</code></pre>
<p>轮廓线得分约为「0.35」。这是一个不错的模型，但我们可以做得更好，并尝试获得更高的簇群分离。</p>
<p>在我们尝试这样做之前，让我们将刚刚构建的聚类可视化，以了解模型的运行情况：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">newdf</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:])</span>

<span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;label&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">clusters</span>
 
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> 
           <span class="n">df</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> 
           <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> 
           <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> 
           <span class="n">df</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> 
           <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> 
           <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">185</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_13_0.png" alt="png"  />

​</p>
<p>从上图可以看出，簇类分离度不是很大。红点与蓝色混合，绿色与黄色重叠，这与轮廓分数一起向我们表明该模型表现不佳。现在，让我们创建一个比这个模型具有更好集群可分离性的新模型。</p>
<br>
<h2 id="建立聚类模型2">建立聚类模型2</h2>
<p>对于这个模型，让我们做一些特征选择。我们可以使用一种叫做主成分分析（PCA）的技术。</p>
<p>PCA 是一种帮助我们降低数据集维数的技术。现在，让我们在数据集上运行PCA：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">principalComponents</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">newdf</span><span class="p">)</span>

<span class="n">features</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;PCA features&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;variance %&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="n">PCA_components</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">principalComponents</span><span class="p">)</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_15_0.png" alt="png"  />

​</p>
<p>这张图表显示了每个主成分分析的组成，以及它的方差。我们可以看到前两个主成分解释了大约70%的数据集方差。我们可以将这两个组件输入到模型中再次构建模型，并选择要使用的簇的数量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">inertias</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">PCA_components</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">inertias</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">inertias</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;number of clusters, k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;inertia&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ks</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="output_17_0.png" alt="png"  />

​</p>
<p>同样，看起来「最佳簇数是4」。我们可以用4个簇来计算此模型的轮廓分数：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">PCA_components</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">])</span>

<span class="c1"># silhouette score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">PCA_components</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))</span>
</code></pre></div><pre><code>0.6025604455573874
</code></pre>
<p>这个模型的轮廓分数是「0.42」，这比我们之前创建的模型要好。我们可以像前面一样可视化此模型：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">clusters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">PCA_components</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;label&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">clusters</span>
 
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="o">.</span><span class="n">Age</span><span class="p">[</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="o">.</span><span class="n">Age</span><span class="p">[</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="o">.</span><span class="n">Age</span><span class="p">[</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="o">.</span><span class="n">Age</span><span class="p">[</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">185</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_21_0.png" alt="png"  />

​</p>
<br> 
<h2 id="模型1与模型2">模型1与模型2</h2>
<p>让我们比较一下这个模型和第一个模型的聚类可分性：</p>
<p>第二个模型中的簇比第一个模型中的簇分离得好得多。此外，第二个模型的轮廓分数要高得多。基于这些原因，我们可以选择第二个模型进行分析。</p>
<br>
<h2 id="聚类分析">聚类分析</h2>
<p>首先，让我们将簇类映射回数据集，并查看数据帧。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Mall_Customers.csv&#39;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;CustomerID&#39;</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># map back clusters to dataframe</span>

<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">PCA_components</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">frame</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred</span>
<span class="n">frame</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Gender</th>
      <th>Age</th>
      <th>Annual Income (k$)</th>
      <th>Spending Score (1-100)</th>
      <th>cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Male</td>
      <td>19</td>
      <td>15</td>
      <td>39</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Male</td>
      <td>21</td>
      <td>15</td>
      <td>81</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Female</td>
      <td>20</td>
      <td>16</td>
      <td>6</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Female</td>
      <td>23</td>
      <td>16</td>
      <td>77</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Female</td>
      <td>31</td>
      <td>17</td>
      <td>40</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
<p>数据帧中的每一行现在都分配给一个集群。要比较不同群集的属性，请查找每个群集上所有变量的平均值：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">avg_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;cluster&#39;</span><span class="p">],</span> <span class="n">as_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">avg_df</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cluster</th>
      <th>Age</th>
      <th>Annual Income (k$)</th>
      <th>Spending Score (1-100)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>25.521739</td>
      <td>26.304348</td>
      <td>78.565217</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>51.681818</td>
      <td>62.125000</td>
      <td>33.750000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>32.904762</td>
      <td>84.380952</td>
      <td>80.500000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>26.659574</td>
      <td>53.106383</td>
      <td>40.042553</td>
    </tr>
  </tbody>
</table>
</div>
<p>如果我们将这些簇可视化，我们可以更容易地解释它们。运行以下代码以获得每个变量的不同可视化效果：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;Age&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">avg_df</span><span class="p">)</span>
</code></pre></div><pre><code>&lt;AxesSubplot:xlabel='cluster', ylabel='Age'&gt;
</code></pre>
<p>​ <br>
<img loading="lazy" src="output_27_1.png" alt="png"  />

​</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;Spending Score (1-100)&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">avg_df</span><span class="p">)</span>
</code></pre></div><pre><code>&lt;AxesSubplot:xlabel='cluster', ylabel='Spending Score (1-100)'&gt;
</code></pre>
<p>​ <br>
<img loading="lazy" src="output_28_1.png" alt="png"  />

​</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;Annual Income (k$)&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">avg_df</span><span class="p">)</span>
</code></pre></div><pre><code>&lt;AxesSubplot:xlabel='cluster', ylabel='Annual Income (k$)'&gt;
</code></pre>
<p>​ <br>
<img loading="lazy" src="output_29_1.png" alt="png"  />

​</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span><span class="s1">&#39;Gender&#39;</span><span class="p">])[</span><span class="s1">&#39;Gender&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">())</span>
<span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Gender</th>
    </tr>
    <tr>
      <th>cluster</th>
      <th>Gender</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2" valign="top">0</th>
      <th>Female</th>
      <td>14</td>
    </tr>
    <tr>
      <th>Male</th>
      <td>9</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">1</th>
      <th>Female</th>
      <td>47</td>
    </tr>
    <tr>
      <th>Male</th>
      <td>41</td>
    </tr>
    <tr>
      <th>2</th>
      <th>Female</th>
      <td>23</td>
    </tr>
  </tbody>
</table>
</div>
<p>各细分市场的主要特点</p>
<p><strong>簇类0</strong>:</p>
<ul>
<li>年平均收入高，支出低。</li>
<li>平均年龄在40岁左右，性别以男性为主。</li>
</ul>
<p><strong>簇类1</strong>：</p>
<ul>
<li>中低收入，平均消费能力。</li>
<li>平均年龄在50岁左右，性别以女性为主。</li>
</ul>
<p><strong>簇类2</strong>：</p>
<ul>
<li>平均收入低，消费分数高。</li>
<li>平均年龄在25岁左右，性别以女性为主。</li>
</ul>
<p><strong>簇类3</strong>：</p>
<ul>
<li>平均收入高，消费分数高。</li>
<li>平均年龄在30岁左右，性别以女性为主。</li>
</ul>
<p>值得注意的是，计算年龄中位数将有助于更好地了解每个集群内的年龄分布。</p>
<p>而且，女性在整个数据集中的代表性更高，这就是为什么大多数集群中女性的数量比男性多。我们可以找到每个性别相对于整个数据集中的数字的百分比，以便更好地了解性别分布。</p>
<br>
<h2 id="为每个簇类构建角色">为每个簇类构建角色</h2>
<p>作为一名数据科学家，能够用你的分析讲述一个故事是一项重要的技能，这将帮助你的客户或利益相关者更容易理解你的发现。下面是一个基于创建的簇类构建消费者角色的示例：</p>
<p><strong>簇类0</strong></p>
<p>这个角色由对金钱非常谨慎的中年人组成。尽管与所有其他群体中的个人相比，他们的平均收入最高，但花费最少。这可能是因为他们有经济责任——比如为孩子的高等教育存钱。</p>
<p>建议：促销、优惠券和折扣代码将吸引这一领域的个人，因为他们倾向于少花钱。</p>
<p><strong>簇类1</strong></p>
<p>这部分人包括一个年龄较大的群体。他们挣的少，花的少，而且可能正在为退休储蓄。</p>
<p>建议：针对这些人的营销可以向这一领域的人推广医疗保健相关产品。</p>
<p><strong>簇类2</strong></p>
<p>这一部分由较年轻的年龄组组成。这部分人最有可能是第一批求职者。与其他人相比，他们赚的钱最少。然而，这些人都是热情的年轻人，他们喜欢过上好的生活方式，而且往往超支消费。</p>
<p>建议：由于这些年轻人花费很多，给他们提供旅游优惠券或酒店折扣可能是个好主意。为他们提供折扣的顶级服装和化妆品品牌也将很好地为这一部分。</p>
<p><strong>簇类3</strong></p>
<p>这部分人是由中年人组成的。这些人努力工作，积累了大量财富。他们也花大量的钱来过好的生活。</p>
<p>建议：由于他们的消费能力和人口结构，这些人很可能会寻找房产购买或投资。</p>
<br>
<h2 id="结论">结论</h2>
<p>在本文中，我已经详细的建立了一个用于客户细分的 K-Means 聚类模型。我们还探讨了聚类分析，并分析了每个聚类中个体的行为。最后，我们看了一些可以根据集群中每个人的属性提供的业务建议。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本分析 | 中国企业高管团队创新注意力</title>
      <link>https://textdata.cn/blog/how_chinese_tmtai_impact_corporate_inovation/</link>
      <pubDate>Thu, 09 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/how_chinese_tmtai_impact_corporate_inovation/</guid>
      <description>How does TMT attention to innovation of Chinese firms influence firm innovation activities? A study on the moderating role of corporate governance.</description>
      <content:encoded><![CDATA[<h2 id="代码下载">代码下载</h2>
<p><a href="%E9%AB%98%E7%AE%A1%E5%88%9B%E6%96%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BB%A3%E7%A0%81.zip">点击下载本文代码</a></p>
<br>
<h2 id="title">Title</h2>
<p>中国企业高管团队创新注意力 对 如何影响企业创新活动？ 公司治理的调节作用研究</p>
<blockquote>
<p>Chen, Shouming, Miao Bu, Sibin Wu, and Xin Liang. &ldquo;How does TMT attention to innovation of Chinese firms influence firm innovation activities? A study on the moderating role of corporate governance.&rdquo; <em>Journal of Business Research</em> 68, no. 5 (2015): 1127-1135.</p>
</blockquote>
<br>
<h2 id="摘要">摘要</h2>
<p>本文借鉴高层梯队理论，探讨了高管团队创新注意力对中国企业创新活动的影响。本文预测 <strong>高管团队创新注意力</strong> 对企业创新活动的影响受到公司治理特征的调节作用。进一步利用从2006年至2011年6年间394家中国制造企业收集的1747个公司年度观察数据，实证检验上述假设。</p>
<p>研究结果表明:企业高管团队创新注意力与企业专利申请之间存在正相关关系，且当企业为民营企业、董事会规模较大或独立董事较少时，该正相关关系更强。</p>
<br>
<h2 id="tmtai指标构建">TMTAI指标构建</h2>
<p><strong>高管团队创新注意力TMTAI</strong>（ TMT attention to innovation）：利用6个创新相关的关键词对该指标进行测量，可以使用词典词频法， 计算TMTAI词在年报中的词频。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">innovations</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;知识产权&#39;</span><span class="p">,</span> <span class="s1">&#39;自主创新&#39;</span><span class="p">,</span> <span class="s1">&#39;专利保护&#39;</span><span class="p">,</span> <span class="s1">&#39;专利侵权&#39;</span><span class="p">,</span> <span class="s1">&#39;技术创新&#39;</span><span class="p">,</span> <span class="s1">&#39;核心技术&#39;</span><span class="p">]</span>
</code></pre></div><br>
<h2 id="待分析的数据">待分析的数据</h2>
<p><strong>原论文使用2006-2011年中国上市企业年报</strong>， 这里我自己随便找了点年报数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;reports.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
 <br>
<p>写代码，一定秉承先简单，再复杂，先局部后整体。只要在具体的局部成功了，就可以推而广之。</p>
<p>那么我们拿出一条文本，对一条文本做高管团队创新注意力tmtai词语的计算</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">tmtai_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;知识产权&#39;</span><span class="p">,</span> <span class="s1">&#39;自主创新&#39;</span><span class="p">,</span> <span class="s1">&#39;专利保护&#39;</span><span class="p">,</span> <span class="s1">&#39;专利侵权&#39;</span><span class="p">,</span> <span class="s1">&#39;技术创新&#39;</span><span class="p">,</span> <span class="s1">&#39;核心技术&#39;</span><span class="p">]</span>
<span class="n">tmtai_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;tmtai&#39;</span><span class="p">:</span> <span class="n">tmtai_words</span><span class="p">}</span>

<span class="c1">#加入自定义词典，放置文本被错分</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tmtai_words</span><span class="p">:</span>
    <span class="n">jieba</span><span class="o">.</span><span class="n">add_word</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    
<span class="c1"># 我瞎编的</span>
<span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;我们公司尊重知识产权，但也要避免专利侵权，在下一阶段会加强自主创新，培育核心技术&#39;</span>


<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">test_text</span><span class="p">,</span>
                       <span class="n">diction</span><span class="o">=</span><span class="n">tmtai_dict</span><span class="p">,</span>
                       <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">tmtai_num        4
stopword_num     9
word_num        19
sentence_num     1
dtype: int64
</code></pre></div><p>实验成功，接下来就可以推广到所有<strong>text</strong>这一列</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">tmtai_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;知识产权&#39;</span><span class="p">,</span> <span class="s1">&#39;自主创新&#39;</span><span class="p">,</span> <span class="s1">&#39;专利保护&#39;</span><span class="p">,</span> <span class="s1">&#39;专利侵权&#39;</span><span class="p">,</span> <span class="s1">&#39;技术创新&#39;</span><span class="p">,</span> <span class="s1">&#39;核心技术&#39;</span><span class="p">]</span>
<span class="n">tmtai_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;tmtai&#39;</span><span class="p">:</span> <span class="n">tmtai_words</span><span class="p">}</span>

<span class="c1">#加入自定义词典，放置文本被错分</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tmtai_words</span><span class="p">:</span>
    <span class="n">jieba</span><span class="o">.</span><span class="n">add_word</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">tmtai_count</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
                                  <span class="n">diction</span><span class="o">=</span><span class="n">tmtai_dict</span><span class="p">,</span>
                                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">))</span>

<span class="c1">#选中text这列，统计其中每条文本中tmtai词出现次数</span>
<span class="n">tdf</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">tmtai_count</span><span class="p">)</span>
<span class="n">tdf</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#合并新旧两个dataframe</span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">tdf</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#tmtai指标是 词频，因此需要tmtai_num/word_num</span>
<span class="n">result_df</span><span class="p">[</span><span class="s1">&#39;tmtai_score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_df</span><span class="p">[</span><span class="s1">&#39;tmtai_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">result_df</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">result_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<p>查看结果，最后一列出现了我们感兴趣的 tmtai_score 指标</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">result_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df4.png" alt=""  />
</p>
<p>tmtai_score平均分</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#500家公司tmtai指标平均值</span>
<span class="n">result_df</span><span class="p">[</span><span class="s1">&#39;tmtai_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.0004088675846679111
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>LIWC vs Python  | 文本分析之词典词频法略讲(含代码)</title>
      <link>https://textdata.cn/blog/liwc_python_text_mining/</link>
      <pubDate>Wed, 08 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/liwc_python_text_mining/</guid>
      <description>语言查询和词数统计 (LIWC「Linguistic Inquiry and Word Count」) 数十年的实证研究——尤其是使用 LIWC 作为科学工具的研究——也为我们提供了理解、解释和量化心理、社会和行为现象的专业方法。LIWC-22 带有 100 多个内置字典，用于捕捉人们的社会和心理状态。每本词典都包含一系列单词、词干、表情符号和其他特定的语言结构，这些结构已被识别为反映感兴趣的心理类别。例如，「认知过程cognitive processes」词典包括 1,000 多个条目，这些条目反映了一个人何时通过一般和更具体的方式积极处理信息。 「从属关系affiliation」词典包括超过 350 个条目，这些条目反映了一个人与他人联系的需要，其中包括 「community」 和 「together」 等词。 LIWC 读取给定文本并将文本中的每个单词与字典单词列表进行比较，并计算文本中与每个字典类别匹配的总单词的百分比。例如，如果 LIWC 使用内置的 LIWC-22 词典分析包含 1000 个单词的单个语音，它可能会发现其中 50 个单词与积极情绪有关，10 个单词与从属关系有关。 LIWC 会将这些数字转换为百分比：5.0% 的积极情绪和 1.0% 的从属关系。</description>
      <content:encoded><![CDATA[<blockquote>
<p>本文原理介绍翻译自  <a href="https://www.liwc.app/help/howitworks">https://www.liwc.app/help/howitworks</a></p>
<p>对比及Python代码主要是自创作</p>
</blockquote>
<p><img loading="lazy" src="img/LIWC.png" alt=""  />
</p>
<p>LIWC是一种付费的文本分析软件，在学界知名度挺高的。今天翻译了LIWC: how it works <a href="https://www.liwc.app/help/howitworks">https://www.liwc.app/help/howitworks</a>  ，通过LIWC来侧面加深对 <strong>词典情感分析</strong> 的理解。</p>
<h2 id="词频可靠的指标">词频：可靠的指标</h2>
<p><strong>语言查询和词数统计 (LIWC「Linguistic Inquiry and Word Count」) 的核心逻辑来自数十年的科学研究表明，人们的语言可以提供极其丰富心理状态信息，包括情绪、思维方式和社会关注点</strong>。有时，这些见解是相当明显和直截了当的。例如，如果某人使用了很多像 <strong>「happy、excited、elated」</strong> 这样的词，他们可能会感到快乐，我们可以使用这些信息来可靠地估计他们当前的情绪状态。然而，<strong>言语行为和心理之间的关系往往不那么明显</strong>。例如，更自信、社会地位更高的人倾向于使用相对较高的 「you」 词，而使用 「me」词的频率相对较低。在这里，数十年的实证研究——尤其是使用 LIWC 作为科学工具的研究——也为我们提供了理解、解释和量化心理、社会和行为现象的专业方法。</p>
<p>但作为算法，实际上主要的计算方法是词频。而这点，借助Python可以完成从数据清洗到数据分析全部过程。</p>
<br>
<h2 id="丰富的词典">丰富的词典</h2>
<p>LIWC-22 带有 100 多个内置字典，用于捕捉人们的社会和心理状态。每本词典都包含一系列单词、词干、表情符号和其他特定的语言结构，这些结构已被识别为反映感兴趣的心理类别。例如，「认知过程cognitive processes」词典包括 1,000 多个条目，这些条目反映了一个人何时通过一般和更具体的方式积极处理信息。 「从属关系affiliation」词典包括超过 350 个条目，这些条目反映了一个人与他人联系的需要，其中包括 「community」 和 「together」 等词。</p>
<p>LIWC 读取给定文本并将文本中的每个单词与字典单词列表进行比较，并计算文本中与每个字典类别匹配的总单词的百分比。例如，如果 LIWC 使用内置的 LIWC-22 词典分析包含 1000 个单词的单个语音，它可能会发现其中 50 个单词与积极情绪有关，10 个单词与从属关系有关。 LIWC 会将这些数字转换为百分比：5.0% 的积极情绪和 1.0% 的从属关系。</p>
<p>请注意，许多 LIWC-22 类别是按层次结构组织的。根据定义，所有愤怒的词都被归类为负面情绪词，而负面情绪词又被归类为情绪词。另请注意，同一个词可能会被分类在多个字典中。例如，「celebrate」一词在积极情绪和成就词典中都有。</p>
<p>下图是liwc用户上传分享的自定义词典，目前有77个。好像需要购买liwc服务，才能下载里面的文件</p>
<p><img loading="lazy" src="img/liwc_user_diy_dict.png" alt=""  />
</p>
<br>
<h2 id="文本越长越好">文本越长越好</h2>
<p>不要忘记，LIWC 和所有文本分析工具一样，是一种相对粗糙的工具。它有时会在识别和计算单个单词时出错。考虑一下「mad」这个词——一个在愤怒词典中被计算在内的词。通常，今天，「mad」这个词确实反映了某种程度的愤怒。然而，有时它表达了喜悦（「he&rsquo;s mad for her.」）或精神不稳定（「mad as hatter」）。幸运的是，这很少成为问题，因为 LIWC 利用了语言使用的概率模型。是的，在给定的句子中，「mad」这个词可能被用来表达积极的情绪。然而，如果作者实际上正在经历积极情绪，他们通常会倾向于使用一个以上的积极情绪词，并且很可能很少使用其他愤怒词，这应该会导致积极情绪得分高而愤怒得分低。要记住的重要一点是，您分析的单词越多，结果就越值得信赖。 10,000 字的文本比 100 字的文本产生的结果可靠得多。任何少于 25-50 个单词的文本都应该以一定的怀疑态度来看待。</p>
<br>
<p>至此翻译结束</p>
<h2 id="简单对比python与liwc">简单对比：Python与LIWC</h2>
<table>
<thead>
<tr>
<th>工具</th>
<th>简介</th>
<th>算法</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python</td>
<td>编程语言</td>
<td>词频(典)法、词嵌入法</td>
<td>接近全能, 可以用Python搞定从数据采集、清洗、分析全流程<br><br/>可以把最新前沿应用到自己研究中 (nature、science、pnas相关文本分析方法的论文会大多会开源自己的Python代码)。</td>
<td>有一定的学习门槛<br></td>
</tr>
<tr>
<td>LIWC</td>
<td>软件</td>
<td>主要是词典法</td>
<td>学界认可<br><br>内置丰富的词典,  拿来即用。</td>
<td>不够灵活， 对中文支持不友好，内置词典几乎全是西方语言。</td>
</tr>
</tbody>
</table>
<br>
<h2 id="考虑数据清洗">考虑数据清洗</h2>
<p>综合来看，如果只使用 <strong>词频(词典)法</strong> 统计某一构念相关词语在文中出现的占比， LIWC 较 Python和R等编程语言有微弱优势。这里需要说明一下，完整的文本(数据)分析包含采集、清洗、分析。其中清洗部分工作量是最大的，数据科学家有个形象的统计，认为清洗占整个数据分析工作量的70%左右。</p>
<p>LIWC的上游环节往往需要借助Python和R等其他语言对原始数据做数据清洗和整理。</p>
<p>如果数据分析的代码量一共有100行，那么清洗的代码可能有70行，数据分析的代码只需再写30行。为了数据清洗任务，你可能不得不学Python，之后可再用LIWC；也可以  LIWC&amp;Python一起用。</p>
<br>
<h2 id="好消息">好消息</h2>
<p>大家可能觉得 <strong>词频(词典)法</strong> 算法过于粗暴， 通过对LIWC工作原理了解，我们知道LIWC软件底层算法也是词频(词典)法。</p>
<p>现在大家应该对 词频(词典)法 有了新的认识，更加有理论自信，技术自信。而Python对这种算法的运行其实很擅长的，</p>
<p>cntext是我一直在开发更新的一个包，一直想将常见的文本分析代码工作量压缩至 个位行数。</p>
<p>功能模块含</p>
<ul>
<li>
<p>stats 文本统计指标</p>
<ul>
<li>词频统计</li>
<li>可读性</li>
<li>内置pkl词典</li>
<li>情感分析</li>
</ul>
</li>
<li>
<p>dictionary构建词表(典)</p>
<ul>
<li>Sopmi 互信息扩充词典法</li>
<li>W2Vmodels 词向量扩充词典法</li>
<li>Glove Glove词向量模型</li>
</ul>
</li>
<li>
<p>similarity 文本相似度</p>
</li>
<li>
<p>cos相似度</p>
</li>
<li>
<p>jaccard相似度</p>
</li>
<li>
<p>编辑距离相似度</p>
</li>
<li>
<p>mind 计算文本中的认知方向（态度、偏见）</p>
</li>
</ul>
<p>比如对一条测试数据test_text， 使用 <strong>词频(词典)法</strong> 做情感分析，代码量不到5行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 自定义情感词典</span>
<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span> <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">]}</span>

<span class="c1"># 测试数据</span>
<span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="c1"># 情感计算</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">test_text</span><span class="p">,</span>  <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<p>即时对一个csv或excel文件，某一列文本做情感分析，代码量不超过10行。我们先看一下数据</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;test_sentiment_texts.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/res1.png" alt=""  />
</p>
<p>对text列做情感分析，使用自定义情感词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入自定义情感词典</span>
<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span> <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">]}</span>

<span class="c1"># 情感计算</span>
<span class="k">def</span> <span class="nf">diy_senti</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>  <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">))</span>

<span class="c1">#读取数据</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;test_sentiment_texts.csv&#39;</span><span class="p">)</span>

<span class="c1">#选中text列，对该列进行情感计算，得到dataframe</span>
<span class="n">senti_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">diy_senti</span><span class="p">)</span>

<span class="c1">#将df和senti_df两个dataframe合并</span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">senti_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1">#存储 &amp; 显示结果</span>
<span class="n">result_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;result_of_sentiment_texts.csv&#39;</span><span class="p">)</span>
<span class="n">result_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/res2.png" alt=""  />
</p>
<br>
<h2 id="本文代码">本文代码</h2>
<p><a href="liwc_python_text_mining.zip">点击下载</a></p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>招募小伙伴</title>
      <link>https://textdata.cn/blog/we_need_you/</link>
      <pubDate>Wed, 08 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/we_need_you/</guid>
      <description>一人行快，众人行远。聚集热爱编程(Python、R)、数据挖掘技术、社科(经管)科研的小伙伴，一起做技术分享。</description>
      <content:encoded><![CDATA[<p>截止今日，「公众号: 大邓和他的Python」聚集了27000位Python爱好者。首先要感谢大家的信任和支持！</p>
<h2 id="引言">引言</h2>
<p>数据挖掘在社科、经管等领域中的应用已成为潮流和趋势。数据采集、数据分析、文本编码(清洗)、机器学习、深度学习等，借助Python一门语言可以全部搞定。互联网时代下，海量的、不规则的数据散落在各处，等待着大家去整理去探索。科学研究的进展离不开测量方法和工具的革新，在大数据时代，掌握Python会让我们的实证研究选题更广、更深、更新。</p>
<p>随着Python技术社区发展，针对不规则数据的处理，如网页文本、报告pdf、图片、音频、视频， 技术的可行性和有用性越来越高，学习Python的价值也在越来越大。大邓一直在学习和分享Python，如果要学习高质量的内容，还得翻出去查看英文社区资料。</p>
<p><strong>一人行快，众人行远。聚集热爱编程(Python、R)、数据挖掘技术、社科(经管)科研的小伙伴，一起做技术分享。</strong></p>
<br>
<h2 id="内容规划">内容规划</h2>
<p>未来公众号的选题内容规划</p>
<ol>
<li>网络爬虫(数据采集)</li>
<li>文本、音频、视频、文件等数据处理</li>
<li>机器学习、自然语言处理</li>
<li>经管、社科领域，借助数据挖掘的研究和技术</li>
<li>Python相关技术分享</li>
<li>其他(待定)</li>
</ol>
<br>
<h2 id="招募小伙伴">招募小伙伴</h2>
<h3 id="小伙伴气质">小伙伴气质</h3>
<ol>
<li>
<p>有Python、R基础</p>
</li>
<li>
<p>对数据分析感兴趣</p>
</li>
<li>
<p>高校社科(经管)专业 在读硕博</p>
</li>
</ol>
<br>
<h3 id="工作内容">工作内容</h3>
<p><strong>结合小伙伴兴趣、特长，划定工作内容：</strong></p>
<ol>
<li>
<p>技术：python或R相关资料收集、选题、代码整理。</p>
</li>
<li>
<p>运营：新媒体运营、社群、内容运营</p>
</li>
</ol>
<br>
<h2 id="福利">福利</h2>
<ul>
<li>
<p>尊重原作者署名权，并将为每篇被采纳的原创首发稿件，
提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算。</p>
</li>
<li>
<p>如作者内容分享成体系，文稿质量高，公众号可组织付费直播课。</p>
</li>
</ul>
<blockquote>
<p>根据小伙伴的工作绩效，大邓会从公众号产生的收益中，发放一定的补贴。</p>
</blockquote>
<br>
<h2 id="参与流程">参与流程</h2>
<ol>
<li>
<p>准备个人word简历：个人信息、科研经历、兴趣爱好、考/保研经历、技术分享等</p>
</li>
<li>
<p>投递个人简历至 <a href="mailto:thunderhit@qq.com">thunderhit@qq.com</a></p>
</li>
</ol>
]]></content:encoded>
    </item>
    
    <item>
      <title>karateclub库 | 计算社交网络中节点的向量</title>
      <link>https://textdata.cn/blog/karateclub_tutorial/</link>
      <pubDate>Tue, 10 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/karateclub_tutorial/</guid>
      <description>使用karateclub计算社交网络中节点的向量，有了节点的向量，就可以基于向量思维比较节点异同</description>
      <content:encoded><![CDATA[<p><a href="https://karateclub.readthedocs.io/en/latest">karateclub</a>是小规模图挖掘研究的一把瑞士军刀， 可以对图形结构化数据进行无监督学习。</p>
<ul>
<li>首先，可以计算出节点、图的特征向量</li>
<li>其次，它包括多种重叠和非重叠的社区发现方法。</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="karateclub_example_code.zip">click to download</a></p>
<br>
<h2 id="数据格式">数据格式</h2>
<p>karateclub假设用户提供的用于<strong>节点嵌入</strong>和<strong>社区检测</strong>的 NetworkX 图具有以下重要属性：</p>
<ul>
<li>节点用整数索引</li>
<li>节点索引从零开始，索引是连续的</li>
</ul>
<p><strong>节点的属性矩阵</strong>可以提供为 scipy sparse 和 numpy 数组。返回的社区成员字典和嵌入矩阵使用相同的数字连续索引。</p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="n">karateclub</span>
</code></pre></div><br>
<h2 id="准备数据">准备数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;karate_club_graph.csv&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_markdown</span><span class="p">())</span>

<span class="nb">print</span><span class="p">()</span>

<span class="n">edges</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;src&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;tgt&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<pre><code>Index(['src', 'tgt'], dtype='object')

|    |   src |   tgt |
|---:|------:|------:|
|  0 |     0 |     1 |
|  1 |     0 |     2 |
|  2 |     0 |     3 |
|  3 |     0 |     4 |
|  4 |     0 |     5 |

[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 10), (0, 11), (0, 12), (0, 13), (0, 17), (0, 19), (0, 21), (0, 31), (1, 2), (1, 3), (1, 7), (1, 13), (1, 17), (1, 19), (1, 21), (1, 30), (2, 3), (2, 7), (2, 8), (2, 9), (2, 13), (2, 27), (2, 28), (2, 32), (3, 7), (3, 12), (3, 13), (4, 6), (4, 10), (5, 6), (5, 10), (5, 16), (6, 16), (8, 30), (8, 32), (8, 33), (9, 33), (13, 33), (14, 32), (14, 33), (15, 32), (15, 33), (18, 32), (18, 33), (19, 33), (20, 32), (20, 33), (22, 32), (22, 33), (23, 25), (23, 27), (23, 29), (23, 32), (23, 33), (24, 25), (24, 27), (24, 31), (25, 31), (26, 29), (26, 33), (27, 33), (28, 31), (28, 33), (29, 32), (29, 33), (30, 32), (30, 33), (31, 32), (31, 33), (32, 33)]
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">graph</span><span class="o">.</span><span class="n">add_edges_from</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<p>​ <br>
<img loading="lazy" src="output_4_0.png" alt="png"  />

​</p>
<br>
<h2 id="社区发现">社区发现</h2>
<p>现在让我们使用LabelPropagation算法来发现网络中的社区结构。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">karateclub</span> <span class="kn">import</span> <span class="n">LabelPropagation</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LabelPropagation</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
<span class="n">cluster_membership</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_memberships</span><span class="p">()</span>
<span class="n">cluster_membership</span>
</code></pre></div><p>Run</p>
<pre><code>{23: 8,
 33: 8,
 5: 10,
 7: 1,
 28: 31,
 4: 10,
 3: 1,
 31: 31,
 20: 8,
 19: 1,
 6: 10,
 32: 8,
 29: 8,
 9: 1,
 14: 8,
 2: 1,
 0: 1,
 17: 1,
 25: 31,
 22: 8,
 11: 1,
 13: 1,
 1: 1,
 24: 31,
 15: 8,
 18: 8,
 26: 8,
 27: 8,
 16: 10,
 12: 1,
 30: 8,
 21: 1,
 8: 8,
 10: 10}
</code></pre>
<p>在有34个节点的图中，发现了4个社区，分别是1、8、10、31。</p>
<br>
<h2 id="node-embeddings">Node embeddings</h2>
<p>计算节点的向量。​使用 Diff2vec 拟合数据的节点嵌入(向量)，具有少量维度、每个源节点的扩散和短欧拉游走。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">karateclub</span> <span class="kn">import</span> <span class="n">Diff2Vec</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Diff2Vec</span><span class="p">(</span><span class="n">diffusion_number</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                 <span class="n">diffusion_cover</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                 <span class="n">dimensions</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_embedding</span><span class="p">()</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Run</p>
<pre><code>(34, 5)
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">X</span>
</code></pre></div><p>Run</p>
<pre><code>array([[ 1.3687179 , -0.33502993, -0.3294797 ,  0.40154558,  1.0270709 ],
       [ 0.88167036, -0.3201618 , -0.34293872,  0.41519755,  0.71964073],
       [ 0.8756805 , -0.21934716, -0.33261183,  0.33785722,  0.51631075],
       [ 0.9768452 , -0.39260587, -0.39460638,  0.28851682,  0.8665034 ],
       [ 0.4809215 , -0.28729865, -0.19276802,  0.22588767,  0.07305563],
       [ 0.5580538 , -0.28137547, -0.1947159 ,  0.23712516,  0.49257705],
       [ 0.23477663,  0.04262228,  0.07154325,  0.02909669,  0.33999097],
       [ 1.1882199 , -0.21742308, -0.26985615,  0.44171503,  0.6679048 ],
       [ 1.0287609 , -0.27409104, -0.04119629,  0.30143994,  0.704676  ],
       [ 0.5700088 , -0.26341844,  0.01560158, -0.08039217,  0.41796318],
       [ 0.5753763 , -0.2242508 , -0.1795436 ,  0.0705331 ,  0.46571913],
       [ 0.46763912, -0.17108741, -0.22459361,  0.03058788,  0.05998428],
       [ 0.5500626 , -0.12745889, -0.28661036,  0.16889155,  0.48200938],
       [ 0.6217582 , -0.10251168, -0.0713837 ,  0.13550574,  0.60422456],
       [ 0.9797377 , -0.46282482, -0.09380057,  0.2749968 ,  0.7020155 ],
       [ 0.38830167, -0.30841848, -0.20950563, -0.02130592,  0.0836651 ],
       [ 0.57225037, -0.04150235, -0.1246101 ,  0.06918757,  0.23083903],
       [ 0.6431406 , -0.04898892, -0.05708801,  0.1311793 ,  0.46377632],
       [ 0.541667  , -0.16031542, -0.33119023,  0.10385639,  0.39525154],
       [ 0.65543544, -0.27534947, -0.28757   ,  0.2080029 ,  0.5288213 ],
       [ 0.46381798, -0.07729273, -0.09209982,  0.11292508,  0.36836028],
       [ 0.53826964, -0.09915172, -0.09243581,  0.15036733,  0.5449071 ],
       [ 0.31599265, -0.22078821, -0.02872767,  0.07436654,  0.28573534],
       [ 1.0706906 , -0.27783617, -0.16653039,  0.2631594 ,  0.6408689 ],
       [ 0.67875004, -0.34441757, -0.10262538,  0.2588695 ,  0.38405937],
       [ 0.41786563, -0.10344986, -0.19508548,  0.19657765,  0.22006002],
       [ 0.7855942 , -0.27200857,  0.02204541,  0.09168041,  0.42220354],
       [ 0.7773458 , -0.11727296, -0.24145149,  0.04537854,  0.5737133 ],
       [ 0.75732976, -0.314953  , -0.15383345,  0.02065313,  0.51843405],
       [ 0.7226543 , -0.31919608, -0.18878649,  0.15413427,  0.42012522],
       [ 0.43411565, -0.17342259, -0.28042233,  0.26853496,  0.49947587],
       [ 1.1565564 , -0.36802933, -0.12613232,  0.32381424,  0.75113887],
       [ 1.1192797 , -0.162529  , -0.17195942,  0.39265418,  0.83656436],
       [ 1.2231556 , -0.5336606 , -0.14015286,  0.14054438,  0.5695296 ]],
      dtype=float32)
</code></pre>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 |  Python文本分析包更新</title>
      <link>https://textdata.cn/blog/cntext_tutorial/</link>
      <pubDate>Mon, 09 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_tutorial/</guid>
      <description>扩展词典、情感分析、可阅读性，内置9种情感词典，涵盖中英文</description>
      <content:encoded><![CDATA[<p><a href="https://github.com/hidadeng/cntext"><img loading="lazy" src="https://img.shields.io/badge/cntext-%e4%b8%ad%e6%96%87%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%ba%93-orange?style=for-the-badge&amp;logo=appveyor" alt=""  />
</a></p>
<p><a href="version1.2.md">旧版cntext入口</a></p>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> <strong>stats</strong>  文本统计指标
<ul>
<li><input checked="" disabled="" type="checkbox"> 词频统计</li>
<li><input checked="" disabled="" type="checkbox"> 可读性</li>
<li><input checked="" disabled="" type="checkbox"> 内置pkl词典</li>
<li><input checked="" disabled="" type="checkbox"> <strong>情感分析</strong></li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>dictionary</strong> 构建词表(典)
<ul>
<li><input checked="" disabled="" type="checkbox"> Sopmi 互信息扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> W2Vmodels 词向量扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> Glove Glove词嵌入模型</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>similarity</strong>   文本相似度
<ul>
<li><input checked="" disabled="" type="checkbox"> cos相似度</li>
<li><input checked="" disabled="" type="checkbox"> jaccard相似度</li>
<li><input checked="" disabled="" type="checkbox"> 编辑距离相似度</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>mind.py</strong> 计算文本中的认知方向（态度、偏见）</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="cntext_examples.zip">click to download</a></p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext
</code></pre></div><br>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">mind</span>
    <span class="nx">dictionary</span>
    <span class="nx">similarity</span>
    <span class="nx">stats</span>
</code></pre></div><br>
<h2 id="一stats">一、stats</h2>
<p>目前stats内置的函数有</p>
<ul>
<li><strong>readability</strong>  文本可读性</li>
<li><strong>term_freq</strong> 词频统计函数</li>
<li><strong>dict_pkl_list</strong>  获取cntext内置词典列表(pkl格式)</li>
<li><strong>load_pkl_dict</strong> 导入pkl词典文件</li>
<li><strong>sentiment</strong> 情感分析</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="11--readability">1.1  readability</h3>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p>readability(text, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text: 文本字符串数据</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p><strong>中文可读性</strong> 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>


<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 28.0,
 &#39;readability2&#39;: 0.15789473684210525,
 &#39;readability3&#39;: 14.078947368421053}
</code></pre></div><br>
<p>句子中的符号变更会影响结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.16666666666666666,
 &#39;readability3&#39;: 13.583333333333334}
</code></pre></div><p><br><br></p>
<h3 id="12--term_freq">1.2  term_freq</h3>
<p>词频统计函数，返回Counter类型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="13-dict_pkl_list">1.3 dict_pkl_list</h3>
<p>获取cntext内置词典列表(pkl格式)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取cntext内置词典列表(pkl格式)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;, 
 &#39;concreteness.pkl&#39;]
</code></pre></div><p>词典对应关系, 部分情感词典资料整理自 <a href="https://github.com/quanteda/quanteda.sentiment">quanteda.sentiment</a></p>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>DUTIR.pkl</td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td>HOWNET.pkl</td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>sentiws.pkl</td>
<td>SentimentWortschatz (SentiWS)</td>
<td>英文</td>
<td>正面词、负面词；<br>效价</td>
</tr>
<tr>
<td>ChineseFinancialFormalUnformalSentiment.pkl</td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td>ANEW.pkl</td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>词语效价信息</td>
</tr>
<tr>
<td>LSD2015.pkl</td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>NRC.pkl</td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td>geninqposneg.pkl</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HuLiu.pkl</td>
<td>Hu&amp;Liu (2004)正、负情感词典</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>AFINN.pkl</td>
<td>尼尔森 (2011) 的“新 ANEW”效价词表</td>
<td>英文</td>
<td>情感效价信息valence</td>
</tr>
<tr>
<td>LoughranMcDonald.pkl</td>
<td>会计金融LM词典</td>
<td>英文</td>
<td>金融领域正、负面情感词</td>
</tr>
<tr>
<td>ADV_CONJ.pkl</td>
<td>副词连词</td>
<td>中文</td>
<td></td>
</tr>
<tr>
<td>STOPWORDS.pkl</td>
<td></td>
<td>中、英</td>
<td>停用词</td>
</tr>
<tr>
<td>concreteness.pkl</td>
<td>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</td>
<td>English</td>
<td>word &amp; concreateness score</td>
</tr>
</tbody>
</table>
<h3 id="注意">注意:</h3>
<ul>
<li>
<p>如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</li>
<li>
<p>如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue</p>
</li>
</ul>
<br>
<h3 id="14-load_pkl_dict">1.4 load_pkl_dict</h3>
<p>导入pkl词典文件，返回字典样式数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入pkl词典文件,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, ...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...], 
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;, &#39;刿目怵心&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;, &#39;喜糖&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,]
           }
</code></pre></div><br>
<h3 id="15-sentiment">1.5 sentiment</h3>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)
使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>diction:  情感词字典；</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">],</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p>如果不适用pkl词典，可以自定义自己的词典，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<h3 id="16-sentiment_by_valence">1.6 sentiment_by_valence</h3>
<p>sentiment函数默认所有情感词权重均为1，只需要统计文本中情感词的个数，即可得到文本情感得分。</p>
<p>sentiment_by_valence(text, diction, lang=&lsquo;english&rsquo;)函数考虑了词语的效价(valence)</p>
<ul>
<li>text 待输入文本</li>
<li>diction 带效价的词典，DataFrame格式。</li>
<li>lang 语言类型&rsquo;chinese' 或 &lsquo;english&rsquo;，默认&rsquo;english'</li>
</ul>
<p>这里我们以文本具体性度量为例， <strong>concreteness.pkl</strong> 整理自 Brysbaert2014的文章。</p>
<blockquote>
<p>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># load the concreteness.pkl dictionary file</span>
<span class="n">concreteness_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;concreteness.pkl&#39;</span><span class="p">)</span>
<span class="n">concreteness_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">word</th>
<th style="text-align:right">valence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">roadsweeper</td>
<td style="text-align:right">4.85</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">traindriver</td>
<td style="text-align:right">4.54</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">tush</td>
<td style="text-align:right">4.45</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">hairdress</td>
<td style="text-align:right">3.93</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">pharmaceutics</td>
<td style="text-align:right">3.77</td>
</tr>
</tbody>
</table>
<br>
<p>先看一条文本的具体性度量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">reply</span> <span class="o">=</span> <span class="s2">&#34;I&#39;ll go look for that&#34;</span>

<span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                              <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                              <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">score</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1.85
</code></pre></div><br>
<p>很多条文本的具体性度量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">employee_replys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I&#39;ll go look for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that top&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go look for that t-shirt in grey&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt in grey&#34;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">reply</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">employee_replys</span><span class="p">):</span>
    <span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                                  <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    
    <span class="n">template</span> <span class="o">=</span> <span class="s2">&#34;Concreteness Score: </span><span class="si">{score:.2f}</span><span class="s2"> | Example-</span><span class="si">{idx}</span><span class="s2">: </span><span class="si">{exmaple}</span><span class="s2">&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="n">score</span><span class="p">,</span> 
                          <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> 
                          <span class="n">exmaple</span><span class="o">=</span><span class="n">reply</span><span class="p">))</span>
    
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Concreteness Score: 1.55 | Example-0: I&#39;ll go look for that
Concreteness Score: 1.55 | Example-1: I&#39;ll go search for that
Concreteness Score: 1.89 | Example-2: I&#39;ll go search for that top
Concreteness Score: 2.04 | Example-3: I&#39;ll go search for that t-shirt
Concreteness Score: 2.37 | Example-4: I&#39;ll go look for that t-shirt in grey
Concreteness Score: 2.37 | Example-5: I&#39;ll go search for that t-shirt in grey
</code></pre></div><br>
<p><br><br></p>
<h2 id="二dictionary">二、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="21-sopmi">2.1 SoPmi</h3>
<p>SoPmi 共现法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                   <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                   <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                   <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span>
                   <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 44.49 s
</code></pre></div><br>
<h3 id="22-w2vmodels">2.2 W2VModels</h3>
<p>W2VModels 词向量</p>
<p><strong>特别要注意代码需要设定lang语言参数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   174 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s

</code></pre></div><br>
<h3 id="需要注意">需要注意</h3>
<p>训练出的w2v模型可以后续中使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">model路径</span><span class="p">)</span>
<span class="c1">#找出word的词向量</span>
<span class="c1">#w2v_model.get_vector(word)</span>
<span class="c1">#更多w2_model方法查看</span>
<span class="c1">#help(w2_model)</span>
</code></pre></div><p>例如本代码，运行生成的结果路径<code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;technology&#39;, 0.689210832118988),
 (&#39;infrastructure&#39;, 0.669672966003418),
 (&#39;resources&#39;, 0.6695448160171509),
 (&#39;talent&#39;, 0.6627111434936523),
 (&#39;execution&#39;, 0.6549549102783203),
 (&#39;marketing&#39;, 0.6533523797988892),
 (&#39;merchandising&#39;, 0.6504817008972168),
 (&#39;diversification&#39;, 0.6479553580284119),
 (&#39;expertise&#39;, 0.6446896195411682),
 (&#39;digital&#39;, 0.6326863765716553)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.45616838, -0.7799563 ,  0.56367606, -0.8570078 ,  0.600359  ,
       -0.6588043 ,  0.31116748, -0.11956959, -0.47599426,  0.21840936,
       -0.02268819,  0.1832016 ,  0.24452794,  0.01084935, -1.4213187 ,
        0.22840202,  0.46387577,  1.198386  , -0.621511  , -0.51598716,
        0.13352732,  0.04140598, -0.23470387,  0.6402956 ,  0.20394802,
        0.10799981,  0.24908689, -1.0117126 , -2.3168423 , -0.0402851 ,
        1.6886286 ,  0.5357047 ,  0.22932841, -0.6094084 ,  0.4515793 ,
       -0.5900931 ,  1.8684244 , -0.21056202,  0.29313338, -0.221067  ,
       -0.9535679 ,  0.07325   , -0.15823542,  1.1477109 ,  0.6716076 ,
       -1.0096023 ,  0.10605699,  1.4148282 ,  0.24576302,  0.5740349 ,
        0.19984631,  0.53964925,  0.41962907,  0.41497853, -1.0322098 ,
        0.01090925,  0.54345983,  0.806317  ,  0.31737605, -0.7965337 ,
        0.9282971 , -0.8775608 , -0.26852605, -0.06743863,  0.42815775,
       -0.11774074, -0.17956367,  0.88813037, -0.46279573, -1.0841943 ,
       -0.06798118,  0.4493006 ,  0.71962464, -0.02876493,  1.0282255 ,
       -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922,
       -0.16753489,  0.14065655, -1.8609751 ,  0.03587054,  1.2792674 ,
        1.2732009 , -0.74120265, -0.98000383,  0.4521185 , -0.26387128,
        0.37045383,  0.3680011 ,  0.7197629 , -0.3570571 ,  0.8016917 ,
        0.39243212, -0.5027844 , -1.2106236 ,  0.6412354 , -0.878307  ],
      dtype=float32)
</code></pre></div><p><br><br></p>
<h3 id="23-co_occurrence_matrix">2.3 co_occurrence_matrix</h3>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence2.png" alt=""  />
</p>
<p><br><br></p>
<h3 id="24--glove">2.4  Glove</h3>
<p>构建Glove词嵌入模型，使用英文数据<code>data/brown_corpus.txt</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Glove</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">create_vocab</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/brown_corpus.txt&#39;</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">cooccurrence_matrix</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_embeddings</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4: ...Create vocabulary for Glove.
Step 2/4: ...Create cooccurrence matrix.
Step 3/4: ...Train glove embeddings. 
             Note, this part takes a long time to run
Step 3/4: ... Finish! Use 175.98 s
</code></pre></div><p>生成的Glove词嵌入文件位于<code>output/Glove</code> 。</p>
<p><br><br></p>
<h2 id="三similarity">三、similarity</h2>
<p>四种相似度计算函数</p>
<ul>
<li>cosine_sim(text1, text2)  cos余弦相似</li>
<li>jaccard_sim(text1, text2)     jaccard相似</li>
<li>minedit_sim(text1, text2)  最小编辑距离相似度；</li>
<li>simple_sim(text1, text2) 更改变动算法</li>
</ul>
<p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩啊&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.82
0.67
2.00
0.87
</code></pre></div><p><br><br></p>
<h2 id="四text2mind">四、Text2Mind</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<p>- <strong>多个对象在某概念的远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
<p>- 多个对象在某<strong>概念的分量(fen，一声)的多少</strong>， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
<p>这两种认知分别可以用向量距离、向量语义投影计算得来。</p>
<ul>
<li>tm.sematic_distance(words, c_words1, c_words2)  向量距离</li>
<li>tm.sematic_projection(words, c_words1, c_words2)  向量语义投影</li>
</ul>
<h3 id="41-tmsematic_distancewords-c_words1-c_words2">4.1 tm.sematic_distance(words, c_words1, c_words2)</h3>
<p>分别计算words与c_words1、c_words2语义距离，返回距离差值。</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果d1-d2&lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<p><strong>下载glove_w2v.6B.100d.txt</strong>链接: <a href="https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw">https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw</a> 提取码: 72l0</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#Note: this is a word2vec format model</span>
<span class="n">tm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Text2Mind</span><span class="p">(</span><span class="n">w2v_model_path</span><span class="o">=</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="n">engineer</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">mans</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">womans</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">mans</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">womans</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><p>-0.38 意味着工程师更接近于男人，而不是女人。</p>
<br>
<h3 id="42-tmsematic_projectionwords-c_words1-c_words2">4.2 tm.sematic_projection(words, c_words1, c_words2)</h3>
<p><strong>语义投影</strong>，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，word越位于c_words2一侧。</p>
<p>下图是语义投影示例图，本文算法和图片均来自 &ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&rdquo;</p>
<p><img loading="lazy" src="img/Nature_Semantic_projection_recovering_human_knowledge_of.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">smalls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">bigs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">smalls</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">bigs</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p>在这几个动物尺寸的感知上，人类觉得老鼠体型是最小，马的体型是最大。</p>
<p><br><br></p>
<h2 id="引用说明">引用说明</h2>
<p>如果研究中使用cntext，请使用以下格式进行引用</p>
<h3 id="apalike">apalike</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Deng X., Nan P. (2022). cntext: a Python tool for text mining (version 1.7.9). DOI: 10.5281/zenodo.7063523 URL: https://github.com/hiDaDeng/cntext
</code></pre></div><h3 id="bibtex">bibtex</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@misc{YourReferenceHere,
author = {Deng, Xudong and Nan, Peng},
doi = {10.5281/zenodo.7063523},
month = {9},
title = {cntext: a Python tool for text mining},
url = {https://github.com/hiDaDeng/cntext},
year = {2022}
}
</code></pre></div><h3 id="endnote">endnote</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">%0 Generic
%A Deng, Xudong
%A Nan, Peng
%D 2022
%K text mining
%K text analysi
%K social science
%K management science
%K semantic analysis
%R 10.5281/zenodo.7063523
%T cntext: a Python tool for text mining
%U https://github.com/hiDaDeng/cntext
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>sentence-transformer库 | 句子语义向量化</title>
      <link>https://textdata.cn/blog/sentence-transformer-tutorial/</link>
      <pubDate>Mon, 09 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/sentence-transformer-tutorial/</guid>
      <description>使用sentence-transformer库BERT技术，将句子语义向量化</description>
      <content:encoded><![CDATA[<blockquote>
<p>内容摘自</p>
<p>刘焕勇博客: <a href="https://liuhuanyong.github.io/">https://liuhuanyong.github.io/</a></p>
<p>原文地址: <a href="https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q">https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q</a></p>
<p>公众号: 老刘说NLP</p>
</blockquote>
<p>word2vec、glove是两种静态的词向量模型，即每个词语只有一个固定的向量表示。但在不同语境中，词语的语义会发生变化，按道理词向量也应该动态调整。相比word2vec、glove生成的静态词向量， BERT是一种动态的技术，可以根据上下文情景，得到语义变化的词向量。</p>
<p>HuggingFace网站提供了简易可用的数据集、丰富的预训练语言模型， 通过sentence-transformer库，我们可以使用HuggingFace内的预训练模型，得到不同情景的文本的语义向量。</p>
<p>HuggingFace网站  <a href="https://huggingface.co/">https://huggingface.co/</a></p>
<p><img loading="lazy" src="img/HuggingFace.png" alt=""  />
</p>
<br>
<h2 id="动态句向量">动态句向量</h2>
<p>sentence-transformer框架提供了一种简便的方法来计算句子和段落的向量表示（也称为句子嵌入）</p>
<p><img loading="lazy" src="img/sentence-transformer.png" alt=""  />
</p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">sentence</span><span class="o">-</span><span class="n">transformers</span>
</code></pre></div><br>
<h2 id="代码">代码</h2>
<p><a href="sentence-transformer-tutorial.zip">click to download the code</a></p>
<p>使用huggingface中的distiluse-base-multilingual-cased与训练模型，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">util</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;distiluse-base-multilingual-cased&#39;</span><span class="p">)</span>
</code></pre></div><p>第一次运行上方的代码，需要运行一定的时间用于下载。下载完成后，我们使用同种语义的中英文句子，分别计算得到emb1和emb2两个句向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">emb1 = model.encode(&#39;Natural language processing is a hard task for human&#39;)

emb2 = model.encode(&#39;自然语言处理对于人类来说是个困难的任务&#39;)
emb1
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 2.58186590e-02,  4.65703346e-02,  4.25276496e-02, -1.67875513e-02,
        5.56012690e-02, -3.44308838e-02, -6.53978735e-02,  1.77450478e-02,
       -3.47155109e-02,  2.86140274e-02,  2.48657260e-02,  7.94188876e-04,
        5.09755425e-02, -1.76107027e-02, -1.04308855e-02,  7.61642214e-03,
        ...
        4.28482369e-02,  1.76657233e-02, -5.83355911e-02,  1.92921527e-03,
        2.81221420e-02,  5.24400780e-03,  2.10703332e-02,  7.96715263e-03,
       -6.80630878e-02, -2.05304120e-02, -2.43293475e-02, -1.87458862e-02],
      dtype=float32)
</code></pre></div><p>在distiluse-base-multilingual-cased这种模型中， 不同语言的同义句应该具有类似的语义，那么cos相似度应该是很大的。越接近于1越相似；越接近于0，越不相似。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">cos_sim</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">pytorch_cos_sim</span><span class="p">(</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb2</span><span class="p">)</span>
<span class="n">cos_sim</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[0.8960]])
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>SimpleTransformers库 | 使用BERT实现文本向量化</title>
      <link>https://textdata.cn/blog/simple_transformer/</link>
      <pubDate>Thu, 05 May 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/simple_transformer/</guid>
      <description>基于BERT预训练模型，对文本进行向量化</description>
      <content:encoded><![CDATA[<p><code>Simple Transformers</code> 库基于 HuggingFace 的 <a href="https://github.com/huggingface/transformers">Transformers</a> 库，可让您快速训练和评估 Transformer 模型， <strong>初始化</strong>、<strong>训练</strong>和<strong>评估</strong>模型只需要 3 行代码。</p>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install simpletransformers
</code></pre></div><p><strong>Simple Transformer</strong> 模型在构建时考虑了特定的自然语言处理 (NLP) 任务。 每个这样的模型都配备了旨在最适合它们打算执行的任务的特性和功能。 使用 Simple Transformers 模型的高级过程遵循相同的模式。</p>
<ol>
<li>初始化一个特定于任务的模型
2.用<code>train_model()</code>训练模型</li>
<li>使用 <code>eval_model()</code> 评估模型</li>
<li>使用 <code>predict()</code> 对（未标记的）数据进行预测</li>
</ol>
<p>但是，不同模型之间存在必要的差异，以确保它们非常适合其预期任务。 关键差异通常是输入/输出数据格式和任何任务特定功能/配置选项的差异。 这些都可以在每个任务的文档部分中找到。</p>
<p>当前实现的特定于任务的“Simple Transformer”模型及其任务如下所示。</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Binary and multi-class text classification文本二分类、多分类</td>
<td><code>ClassificationModel</code></td>
</tr>
<tr>
<td>Conversational AI (chatbot training)对话机器人训练</td>
<td><code>ConvAIModel</code></td>
</tr>
<tr>
<td>Language generation语言生成</td>
<td><code>LanguageGenerationModel</code></td>
</tr>
<tr>
<td>Language model training/fine-tuning语言模型训练、微调</td>
<td><code>LanguageModelingModel</code></td>
</tr>
<tr>
<td>Multi-label text classification多类别文本分类</td>
<td><code>MultiLabelClassificationModel</code></td>
</tr>
<tr>
<td>Multi-modal classification (text and image data combined)多模态分类</td>
<td><code>MultiModalClassificationModel</code></td>
</tr>
<tr>
<td>Named entity recognition命名实体识别</td>
<td><code>NERModel</code></td>
</tr>
<tr>
<td>Question answering问答</td>
<td><code>QuestionAnsweringModel</code></td>
</tr>
<tr>
<td>Regression回归</td>
<td><code>ClassificationModel</code></td>
</tr>
<tr>
<td>Sentence-pair classification句对分类</td>
<td><code>ClassificationModel</code></td>
</tr>
<tr>
<td><strong>Text Representation Generation文本表征生成</strong></td>
<td><strong>RepresentationModel</strong></td>
</tr>
<tr>
<td>Document Retrieval文档抽取</td>
<td><code>RetrievalModel</code></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>有关如何使用这些模型的更多信息，请参阅 <a href="https://simpletransformers.ai/">docs</a> 中的相关部分。</strong></li>
<li>示例脚本可以在 <a href="https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples">examples</a> 目录中找到。</li>
<li>有关项目的最新更改，请参阅 <a href="https://github.com/ThilinaRajapakse/simpletransformers/blob/master/CHANGELOG.md">Changelog</a>。</li>
</ul>
<h2 id="生成句子嵌入">生成句子嵌入</h2>
<p>使用huggingface网站https://huggingface.co/ 提供的模型</p>
<ul>
<li>英文模型 bert-base-uncased</li>
<li>中文模型 bert-base-chinese</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">simpletransformers.language_representation</span> <span class="kn">import</span> <span class="n">RepresentationModel</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Machine Learning and Deep Learning are part of AI&#34;</span><span class="p">,</span> 
             <span class="s2">&#34;Data Science will excel in future&#34;</span><span class="p">]</span> <span class="c1">#it should always be a list</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">RepresentationModel</span><span class="p">(</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&#34;bert&#34;</span><span class="p">,</span>
        <span class="n">model_name</span><span class="o">=</span><span class="s2">&#34;bert-base-uncased&#34;</span><span class="p">,</span> <span class="c1">#英文模型</span>
        <span class="n">use_cuda</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">sentence_vectors</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_sentences</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">combine_strategy</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sentence_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_vectors</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(2, 768)

array([[-0.10800573,  0.19615649, -0.10756102, ..., -0.26362818,
         0.56403756, -0.30985302],
       [ 0.0201617 , -0.19381572,  0.4360792 , ..., -0.2979438 ,
         0.04984972, -0.702381  ]], dtype=float32)
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 从符号到嵌入：计算社会科学的两种文本表示</title>
      <link>https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/</link>
      <pubDate>Mon, 25 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/from_sysbol_to_embeddings_in_computational_social_science/</guid>
      <description>如何有效地表示数据以挖掘我们想要的计算社会科学的含义？为了探索答案，我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示。How to efficiently represent data to mine the implications we want for computational social science? To explore the answer, we conduct a thorough review of data representations for text and the web in CSS, and we summarize existing representations into two schemes, symbol-based and embedding-based</description>
      <content:encoded><![CDATA[<p>B站看到大牛刘知远关于文本分析在计算社会科学领域应用的分享，解答了我对文本表示的疑惑，看完了能对文本的特征工程加深理解，同时也能更清晰未来如何借助计算机科学技术开展社会科学研究。</p>
<blockquote>
<p><strong>全文摘抄自</strong></p>
<p>Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.</p>
</blockquote>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1qi4y1Q7qj&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<h2 id="摘要">摘要</h2>
<p><strong>计算社会科学</strong>（CSS），旨在利用计算方法来解决社会科学问题，是一个新兴和快速发展的领域。 CSS 的研究是数据驱动的，并且显着受益于在线用户生成内容和社交网络的可用性，其中包含用于调查的富文本和网络数据。然而，这些大规模、多模态的数据也给研究人员带来了很大的挑战：<strong>如何有效地表示数据以挖掘我们想要的 CSS 含义</strong>？为了探索答案，<strong>我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示</strong>，并为每个方案介绍了一系列典型的方法。随后，我们基于对来自 6 个涉及 CSS 的顶级场所的 400 多篇研究文章的调查，展示了上述表示的应用。从这些应用程序的统计数据中，<strong>我们挖掘出每种表示的强度，并发现基于嵌入的表示在过去十年中出现并获得越来越多的关注的趋势</strong>。最后，我们讨论了几个关键挑战和未来方向的开放性问题。本调查旨在为 CSS 研究人员提供对数据表示的更深入理解和更明智的应用。</p>
<p><strong>关键词</strong>：计算社会科学；基于符号的表示；基于嵌入的表示；社交网络</p>
<br>
<h2 id="一计算社会学数据分析流程">一、计算社会学数据分析流程</h2>
<p>其中第二步，数据表示目前有两大类表示(特征工程)方法</p>
<ul>
<li><strong>基于符号的文本表示</strong>；符号可以是单词(或词组)，也可以是概念(如正面情感、负面情感)</li>
<li><strong>基于嵌入(分布式)的文本表示</strong>；相比于符号法，将词(词组)看做一个点。嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(king)-v(queen)约等于v(man)-v(woman)</li>
</ul>
<p><img loading="lazy" src="img/fig1.png" alt=""  />
</p>
<br>
<h2 id="二基于符号的文本表示">二、基于符号的文本表示</h2>
<p>基于符号的文本表示一般来说默认词语是不可分的符号，每个词能根据词频统计出现次数的多与少，或是否存在。</p>
<h3 id="21-词语层面">2.1 词语层面</h3>
<ul>
<li>
<p>基于词频表示</p>
<ul>
<li>是否出现，出现标位1，反之标位0。</li>
<li>出现多少，词语出现几次，标为几个。</li>
</ul>
</li>
<li>
<p>基于特征表示，如每个词带有权重(得分)</p>
</li>
<li>
<p>基于网络表示，如词语共现网络(矩阵)</p>
</li>
</ul>
<h3 id="22-句子层面">2.2 句子层面</h3>
<ul>
<li>
<p>基于词频的表示</p>
<ul>
<li>one-hot 将文本转为向量，向量中每个数，词语出现标位1，反之标位0</li>
<li>bag-of-words，将文本转为向量，向量中每个数，词语出现n次标记为n</li>
<li>n-grams，对词组的处理，将词组看做一个单词(整体)。</li>
<li>Tf-Idf ,该算法分为tf和idf两部分。其中tf与bag-of-words类似，考虑词语出现次数。而idf还考虑词语在语料中出现场景的稀缺性程度。</li>
</ul>
</li>
<li>
<p>基于语法特征，如句法依存关系，类似于英语语法，将句子分为主谓宾、动词、名词等。</p>
</li>
<li>
<p>词典法，如使用正、负情感词典，对文本数据进行情感分析，可以得到pos和neg的各自得分</p>
</li>
</ul>
<p><img loading="lazy" src="img/fig2.png" alt=""  />
</p>
<br>
<h2 id="三基于嵌入的文本表示">三、基于嵌入的文本表示</h2>
<h3 id="31词语层面">3.1词语层面</h3>
<p>嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(best)-v(good)约等于v(worst)-v(bad)</p>
<h3 id="32-句子层面">3.2 句子层面</h3>
<p>词语是向量，那么由词语组成的句子也会加权得到一个向量。含有相似话题或含义相近的句子在多维向量空间中会比较接近。</p>
<p><img loading="lazy" src="img/fig7.png" alt=""  />
</p>
<br>
<h2 id="四任务分类文本的用法">四、任务分类：文本的用法</h2>
<p><img loading="lazy" src="img/fig16.png" alt=""  />
</p>
<p>有了文本数据，刚刚解决了如何表示文本。接下来，需要明确，我们使用文本目的是为了做哪类分析，得到哪些信息。有8种常见的文本分析图式</p>
<ul>
<li>描述性。如随时间推移，词频的发展趋势是变大的</li>
<li>相关性。</li>
<li>聚类。如lda话题分析、k-means聚类</li>
<li>相似度。两个文档转为向量后，可以通过cosine计算相似度</li>
<li>分类。机器学习分类，判断某文本隶属于哪个类别</li>
<li>回归。例如根据文本，判断某件事发生的概率</li>
<li>语言模型。</li>
<li>排序。</li>
</ul>
<br>
<h2 id="五发文趋势-符号vs嵌入">五、发文趋势-符号vs嵌入</h2>
<p>基于上一节中对应用程序的介绍，可以观察到基于符号和基于嵌入的表示在 <strong>计算社会科学</strong>中都得到了相当大的采用。为了明确研究它们的覆盖范围，我们计算了每年使用两种表示中的一种或两种的作品数量，如图 17 所示。通过比较nature、science、pnas三大顶级期刊，我们可以发现使用<strong>基于嵌入表示</strong>的文章比例在过去几年中逐渐。这表明越来越多的 计算社会科学文章 已经考虑并受益于基于嵌入表示。</p>
<p>图 18 显示了在 计算机领域ACL、WWW 和 KDD 的会议上中，发现使用基于嵌入的表示的文章数量已大大超过使用基于符号的表示的文章数量。然而，与图 17 相比，计算机科学会议中基于嵌入的表示的数量与三个多学科期刊之间存在很大差距。</p>
<p><img loading="lazy" src="img/3_top_journals.png" alt=""  />
</p>
<p><img loading="lazy" src="img/nlp.png" alt=""  />
</p>
<p>总而言之，在过去十年中，基于嵌入的表示已经出现并在 计算社会科学 中发挥着越来越重要的作用。</p>
<br>
<h2 id="六趋势解读">六、趋势解读</h2>
<p>基于它们的内部机制和现有应用，对趋势解读，我们总结出以下三个关键点。</p>
<p>基于符号的表示因其明确性和可解释性而擅长描述和关系的任务。</p>
<p>基于符号的表示中的每个值都表示一定的人类可读的含义，因此我们可以直接使用它来观察数据的分布，以及提取对象之间的关系。例如，基于频率的词表示用于观察文化变化并捕捉新闻中提及次数与公司股票交易量之间的关系。虽然基于主题模型的表示和一些基于神经的表示在一定程度上具有实际意义，但它们对于社会科学研究人员来说仍然是模糊的并且不那么引人注目。</p>
<p>由于神经网络具有强大的拟合数据和提取深度语义的能力，基于嵌入的表示在预测（例如分类和回归）和相似性任务中表现更好。一方面，神经网络通过大规模神经元的连接实现高效的输入输出映射功能。另一方面，通过多层网络的构建，实现深层语义和抽象概念的提取。现有研究表明，深层捕获相对于浅层更抽象的特征。诸如社会偏见和道德化之类的抽象概念都可以通过基于嵌入的表示来很好地衡量。虽然我们提到基于符号的表示可以通过一些定义的符号来代表抽象概念，但这种表示仍然是部分和肤浅的，很难捕捉到它们的全貌。</p>
<p>基于嵌入的表示需要更少的人力。基于符号的表示通常需要大量的专家知识来定义研究对象的特征，这是劳动密集型的。此外，对于一些没有充分特征的抽象概念或对象，它们的表现将受到限制。与它们不同的是，基于嵌入的表示是从数据中自动提取的，几乎不需要人工干预，甚至可以补充人类知识。例如，可以使用神经网络来自动恢复丢失的巴比伦文本，这即使对专家来说也是具有挑战性的。此外，基于嵌入的表示可以在没有手动定义的情况下描述语言的复杂性和歧义性。</p>
<br> 
<h2 id="七未来展望">七、未来展望</h2>
<p>尽管在过去十年中出现了从符号到嵌入的趋势，但仍有许多挑战和悬而未决的问题有待探索。展望未来，我们列出了一些与计算社会科学 中的数据表示相关的基本和潜在的未来方向。</p>
<p>预训练的语言模型。近年来，预训练的语言模型受到了相当大的关注，并在处理文本数据方面取得了巨大的成功 [100, 240]。这些模型从百科全书和书籍等海量文本数据中学习丰富的语义信息，仅在下游任务中进行微调以实现有效的基于嵌入的表示。因此，对于 计算社会科学，我们可以借助预训练的语言模型获得更通用、更健壮的文本表示。与从传统神经网络模型中学习的表示相比，这些表示不仅可以更广泛、更准确地从文本中分析社会现象，而且还可以减少那些需要大量标记数据的任务的人工注释。</p>
<p>图神经网络。通过消息传递机制，图神经网络 [461] 可以同时有效地对网络拓扑和节点/边缘特征（例如文本信息）进行建模，从而提供一个统一的框架来利用来自异构来源的信息。 计算社会科学 中的许多场景需要处理社交网络以及个人特征。因此，图神经网络技术在 计算社会科学 研究中具有很大的应用潜力，可以学习融合文本和网络信息的表示。事实上，计算机科学中的各种应用，例如自然语言处理 [418] 和推荐系统 [439]，已经采用图神经网络进行建模。</p>
<p>设计为预测和相似性。基于嵌入的表示以丰富和深层次的语义而闻名，而基于符号的表示通常保留在部分和浅层语义中。同时，基于嵌入的表示擅长预测和相似性的任务。因此，为了充分利用嵌入中的强语义，鼓励 计算社会科学 研究人员尽可能将研究问题设计为预测或相似性任务。例如，我们可以将社会偏见问题设计为性别词和中性词嵌入之间的相似性度量 [59, 133]。此外，人类语言的复杂性可以设计为一项预测任务，它以语言模型为指标查看单词或句子的预测概率[155]。</p>
<p>可解释性。诚然，基于嵌入的方法的一个缺点是缺乏可解释性。这个问题会损害与道德、安全或隐私相关的决策关键系统的应用。尽管嵌入模型，尤其是神经网络模型的可解释性尚未完全解决，但计算机科学领域的研究人员已经做出了一些努力，以提高基于神经模型的可解释性 [16]。因此，利用基于嵌入的模型和可解释性分析方法进行有效和（部分）可解释的预测将是一个有趣的方向。</p>
<br>
<h2 id="结论">结论</h2>
<p>计算社会科学作为一个新兴且有前途的跨学科领域，近年来吸引了相当多的研究兴趣。 计算社会科学 研究中广泛使用两种主要类型的数据，即文本数据和网络数据。在本次调查中，我们首先将数据表示总结为基于符号和基于嵌入的表示，并在构建这些表示时进一步介绍典型的方法。之后，我们基于来自 6 个经典期刊和会议的 400 多篇高被引文献，对这两类表示的应用进行了全面回顾。根据对这些应用的统计，发现了 计算社会科学 中基于嵌入的文本和网络表示正在出现和增长的趋势，我们进一步讨论了其中的原因。最后，我们提出了 计算社会科学 中的四个挑战和未解决的问题，它们是需要探索的基本和潜在方向。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>中文词向量资源汇总 &amp; 使用方法</title>
      <link>https://textdata.cn/blog/embeddings_resource_usage_method/</link>
      <pubDate>Thu, 21 Apr 2022 15:40:10 +0600</pubDate>
      
      <guid>/blog/embeddings_resource_usage_method/</guid>
      <description>数十种中文词向量模型资源下载&amp;amp;使用方法。Dozens of Chinese word vector model resource downloads &amp;amp; usage methods</description>
      <content:encoded><![CDATA[<br>
<h2 id="项目地址">项目地址</h2>
<p><a href="https://github.com/Embedding/Chinese-Word-Vectors">https://github.com/Embedding/Chinese-Word-Vectors</a></p>
<p>Chinese-Word-Vectors项目提供超过100种中文词向量，其中包括不同的表示方式（稠密SGNS和稀疏PPMI）、不同的上下文特征（词、N元组、字等等）、以及不同的训练语料。获取预训练词向量非常方便，下载后即可用于下游任务。</p>
<br>
<h2 id="参考文献">参考文献</h2>
<p>如果使用了本项目的词向量和CA8数据集请进行如下引用：</p>
<p>Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, <a href="http://aclweb.org/anthology/P18-2023"><em>Analogical Reasoning on Chinese Morphological and Semantic Relations</em></a>, ACL 2018.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@InProceedings{P18-2023,
  author =  &#34;Li, Shen
    and Zhao, Zhe
    and Hu, Renfen
    and Li, Wensi
    and Liu, Tao
    and Du, Xiaoyong&#34;,
  title =   &#34;Analogical Reasoning on Chinese Morphological and Semantic Relations&#34;,
  booktitle =   &#34;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&#34;,
  year =  &#34;2018&#34;,
  publisher =   &#34;Association for Computational Linguistics&#34;,
  pages =   &#34;138--143&#34;,
  location =  &#34;Melbourne, Australia&#34;,
  url =   &#34;http://aclweb.org/anthology/P18-2023&#34;
}
</code></pre></div><br>
<h3 id="不同领域">不同领域</h3>
<p>下列词向量基于不同的表示方式、不同的上下文特征以及不同领域的语料训练而成。</p>
<table align="center">
    <tr align="center">
        <td colspan="5"><b>Word2vec / Skip-Gram with Negative Sampling (SGNS)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1IiIbQGJ_AooTj5s8aZYcvA">300d</a> / PWD: 5555</td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1AmXYWVgkxrG4GokevPtNgA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZKePwxwsDdzNrfkc6WKdGQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZBVVD4mUSUuXOxlZ3V71ZA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19wQrclyynOnco3JBvnI5pA">300d</td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/19sqMz-JAhhxh3o6ecvQxQw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1upPkA8KJnxTZBfjuNDtaeQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1BvKk2QjbtQMch7EISppW2A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19Vso_k79FZb5OZCWQPAnFQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1tUghuTno5yOvOx4LXA9-wg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13yVrXeGYkxdGW3P6juiQmA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1pUqyn7mnPcUmzxT64gGpSw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1svFOwFBKnnlsqrF1t99Lnw">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/1EhtsbDa3ekzZPODWNLHcXA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FcPHv7S4vUgnL7WeWf4_PA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13CAxY5ffRFuOcHZu8VmArw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sqvrUtGBAZ7YWEsGz41DRQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VGOs0RH7DXE5vRrtw6boQA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1OQ6fQLCgqT43WTwh5fh_lg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1_xogqF9kJT6tmQHSAYrYeg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Fo27Lv_0nz8FXg-xbOz14Q">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/1zbuUJEEEpZRNHxZ7Gezzmw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/11PWBcvruXEDvKf2TiIXntg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/10bhJpaXMCUK02nHvRAttqA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FHl_bQkYucvVk-j2KG4dxA">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1ciq8iXtcrHpu3ir_VhK0zg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Oa4CkPd8o2xd6LEAaa4gmg">300d</a> / PWD: z5b4</td>
      <td><a href="https://pan.baidu.com/s/1IG8IxNp2s7vVklz-vyZR9A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1SEOKrJYS14HpqIaQT462kA">300d</a> / PWD: yenb</td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1vPSeUsSiWYXEWAuokLR0qQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sS9E7sclvS_UZcBgHN7xLQ">300d</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合<br>Baidu Netdisk / Google Drive</td>
      <td>
        <a href="https://pan.baidu.com/s/1luy-GlTdqqvJ3j-A4FcIOw">300d</a><br>
        <a href="https://drive.google.com/open?id=1Zh9ZCEu8_eSQ-qkYVQufQDNKPC4mtEKR">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1oJol-GaRMk4-8Ejpzxo6Gw">300d</a><br>
        <a href="https://drive.google.com/open?id=1WUU9LnoAjs--1E_WqcghLJ-Pp8bb38oS">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1DjIGENlhRbsVyHW-caRePg">300d</a><br>
        <a href="https://drive.google.com/open?id=1aVAK0Z2E5DkdIH6-JHbiWSL5dbAcz6c3">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/14JP1gD7hcmsWdSpTvA3vKA">300d</a><br>
        <a href="https://drive.google.com/open?id=1kSAl4_AOg3_6ayU7KRM0Nk66uGdSZdnk">300d</a>
      </td>
    </tr>
</table>
<table align="center">
    <tr align="center">
        <td colspan="5"><b>Positive Pointwise Mutual Information (PPMI)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1_itcjrQawCwcURa7WZLPOA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1cEZzN1S2senwWSyHOnL7YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1KcfFdyO0-kE9S9CwzIisfw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FXYM3CY161_4QMgiH8vasQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1MGXRrc54nITPzQ7sfEUjMA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1mtxZna8UJ7xBIxhBFntumQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dDImpAx41V73Byl2julOGA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1bsBQHXFpxMHGBexYof1_rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/1NLr1K7aapU2sYBvzbVny5g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1LJl3Br0ccGDHP0XX2k3pVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1GQQXGMn1AHh-BlifT0JD2g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1Xm9Ec3O3rJ6ayrwVwonC7g">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1ECA51CZLp9_JB_me7YZ9-Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FO39ZYy1mStERf_b53Y_yQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1lLBFBk8nn3spFAvKY9IJ6A">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1f-dLQZlZo_-B5ZKcPIc6rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/10wtgdmrTsTrjpSDvI0KzOw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b6zjvhOIqTdACSSbriisVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1w24vCfgqcoJvPxsB5VrRvw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b9BPiDRhiEZ-6ybTcovrqQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VaUP3YJC0IZKTbJ-1_8HZg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1g39PKwT0kSmpneKOgXR5YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1d8Bsuak0fyXxQOVUiNr-2w">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1D5fteBX0Vy4czEqpxXjlrQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/15O2EbToOzjNSkzJwAOk_Ug">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/11Dqywn0hfMhysto7bZS1Dw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1wY-7mfV6nwDj_tru6W9h4Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1DMW-MgLApbQnWwDd-pT_qw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1HTHhlr8zvzhTwed7dO0sDg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1jAuGJBxKqgapt__urGsBOQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/173AJfCoAV0ZA8Z31tKBdTA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dFCxke_Su3lLsuwZr7co3A">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1NJ1Gc99oE0-GV0QxBqy-qw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1YGEgyXIbw0O4NtoM1ohjdA">Sparse</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
    </tr>
</table>
<p><sup>*</sup>由于古汉语中绝大部份词均为单字词，因此只需字向量。</p>
<br>
<h2 id="语料">语料</h2>
<p>项目花费了大量精力来收集了来自多个领域的语料。所有的文本数据均移除了html和xml标记，仅保留了纯文本。之后采用了<a href="https://github.com/hankcs/HanLP">HanLP(v_1.5.3)</a>对文本进行了分词。此外，我们将繁体中文用<a href="https://github.com/BYVoid/OpenCC">Open Chinese Convert (OpenCC)</a>转换为了简体中文。更详细的语料信息如下所示：</p>
<table align="center">
	<tr align="center">
		<td><b>语料</b></td>
		<td><b>大小</b></td>
		<td><b>词数量</b></td>
		<td><b>词汇量</b></td>
		<td><b>详情</b></td>
	</tr>
	<tr align="center">
		<td>Baidu Encyclopedia<br />百度百科</td>
		<td>4.1G</td>
		<td>745M</td>
		<td>5422K</td>
		<td>中文百科<br />https://baike.baidu.com/</td>
	</tr>
	<tr align="center">
		<td>Wikipedia_zh<br />中文维基百科</td>
		<td>1.3G</td>
		<td>223M</td>
		<td>2129K</td>
		<td>中文维基百科<br />https://dumps.wikimedia.org/</td>
	</tr>
	<tr align="center">
		<td>People's Daily News<br />人民日报</td>
		<td>3.9G</td>
		<td>668M</td>
		<td>1664K</td>
		<td>人民日报新闻数据(1946-2017)<br />http://data.people.com.cn/</td>
	</tr>
	<tr align="center">
		<td>Sogou News<br />搜狗新闻</td>
		<td>3.7G</td>
		<td>649M</td>
		<td>1226K</td>
		<td>Sogou labs的新闻数据<br />http://www.sogou.com/labs/</td>
	</tr>
  <tr align="center">
    <td>Financial News<br />金融新闻</td>
    <td>6.2G</td>
    <td>1055M</td>
    <td>2785K</td>
    <td>从多个网站收集到的金融新闻</td>
  </tr>
	<tr align="center">
		<td>Zhihu_QA<br />知乎问答</td>
		<td>2.1G</td>
		<td>384M</td>
		<td>1117K</td>
		<td>中文问答数据<br />https://www.zhihu.com/</td>
	</tr>
	<tr align="center">
		<td>Weibo<br />微博</td>
		<td>0.73G</td>
		<td>136M</td>
		<td>850K</td>
		<td>NLPIR Lab提供的微博数据<br />http://www.nlpir.org/wordpress/download/weibo.7z</td>
	</tr>
	<tr align="center">
		<td>Literature<br />文学作品</td>
		<td>0.93G</td>
		<td>177M</td>
		<td>702K</td>
		<td>8599篇现代文学作品</td>
	</tr>
	<tr align="center">
		<td>Mixed-large<br />综合</td>
		<td>22.6G</td>
    <td>4037M</td>
    <td>10653K</td>
		<td>上述所有数据的汇总</td>
	</tr>
  <tr align="center">
    <td>Complete Library in Four Sections<br />四库全书</td>
    <td>1.5G</td>
    <td>714M</td>
    <td>21.8K</td>
    <td>目前最大的古代文献汇总</td>
  </tr>
</table>
上述统计结果中，所有词都被计算在内，包括低频词。
<br>
<h2 id="导入模型代码">导入模型(代码)</h2>
<p>例如我下载了多个词模型，下载得到bz2结尾的文件名，例如<code>sgns.financial.bigram.bz2</code>。</p>
<p><img loading="lazy" src="models.png" alt=""  />
</p>
<p>使用方式</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1">#以金融sgns.financial.bigram.bz2为例</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.financial.bigram.bz2&#39;</span><span class="p">,</span> 
                                          <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                          <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>


<span class="n">model</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x7fe7fad79d60&gt;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;投资&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.084635,  0.890228, -0.23223 , -0.308985,  0.058241,  0.458777,
       -0.152547, -0.413471,  0.269701, -0.078043, -0.4155  ,  0.074735,
        0.35714 ,  0.103431,  0.601784, -0.390854,  0.814801, -0.122664,
       -1.076744,  0.516941, -0.293319, -0.310251, -0.407794,  0.003898,
       -0.210962,  0.378095, -0.345955, -0.223848,  0.700162,  0.207644,
        0.426249, -0.272832, -0.110305, -0.701062, -0.173407, -0.172121,
       -0.682592,  0.593414,  0.279591, -0.408284, -0.166693,  0.753402,
        0.037375,  0.141865, -0.246024, -0.108663, -0.225255, -0.856601,
        0.381026,  0.401248,  0.012108, -0.126305, -0.374255,  0.728795,
        0.219549, -0.354029, -0.353131,  0.064867,  0.49565 , -0.503267,
       -0.304075,  0.145036,  0.688948,  0.063382, -0.223243,  0.474251,
        0.80543 ,  0.683178,  0.118159,  0.408411, -0.020066,  0.009045,
       -0.135446, -0.069633,  0.206357,  0.482845, -0.075307,  0.06433 ,
       -0.112367,  0.011816,  0.87427 , -0.120287, -0.31036 ,  0.369985,
        0.560386, -0.215248,  0.389631,  0.042943, -0.319149,  0.951551,
       -0.335188,  0.642246, -0.55546 ,  0.322397,  0.659618, -0.213124,
        0.346696, -0.342239,  0.31479 ,  0.078533, -0.345148,  0.815577,
       -0.530134,  0.303419, -0.158916, -0.190564,  0.436046, -0.112251,
       -0.339966,  0.253645,  0.181076,  0.122875, -0.310951, -0.126253,
        1.641405,  0.357906,  0.165796,  0.398656, -0.330591,  0.20328 ,
       -0.077191, -0.421248, -0.078504, -0.734519,  0.146212,  0.535727,
        0.014134,  0.040322, -0.44809 , -0.758205, -0.151237,  0.248258,
       -0.319704,  0.656033, -0.518857,  0.932356, -1.01786 , -0.46354 ,
        0.160921, -0.243597,  0.106666, -0.03404 ,  0.010672,  0.260243,
        0.899813,  0.171735, -0.108209, -0.009843, -0.18113 ,  0.302494,
        0.187285,  0.064669, -0.502041, -0.724377, -0.294312, -0.522256,
        0.334543,  0.740455, -0.357653,  0.540747,  0.256146,  0.513839,
        1.116628, -0.626111,  0.505574,  0.089774, -0.381137, -0.282352,
       -0.457542,  0.198909,  0.313638,  0.560809,  0.25295 ,  0.878158,
       -0.289311, -0.629047,  0.011103,  0.041058, -0.291302, -0.014001,
       -0.027697, -0.445817, -0.070086,  0.159816, -0.120071,  1.280489,
       -0.108866,  0.01586 , -0.505574, -0.679772, -0.343165,  0.595633,
        0.438108, -0.364066, -0.393667,  0.442285,  0.24979 , -0.191607,
        0.425692,  0.535577, -0.480332, -0.737461,  0.588498, -0.380264,
        0.151292,  0.077519, -0.221384,  0.699436,  0.401642,  0.509026,
       -0.411141,  0.206719, -0.097051, -0.451834, -0.825617,  0.602984,
        0.2853  ,  0.46055 ,  0.96472 ,  0.322712, -0.373446,  0.207944,
        0.236688,  0.566523,  0.037644,  1.241091,  0.025682,  0.373211,
        0.097712, -0.195355,  0.264579, -0.072992, -0.121629,  0.041688,
        0.213666,  0.329652, -0.015182,  0.396307,  0.117955,  0.119577,
       -0.334761, -0.135917,  0.409983,  0.512367, -0.292204,  0.302897,
       -0.325733,  0.383173, -0.92419 , -0.377535, -0.059801, -0.606275,
       -0.240482,  0.054021, -0.581386, -0.555691,  0.158354,  0.103765,
        0.107681,  0.248877, -0.597925,  0.193332,  0.844085,  0.00584 ,
        0.041622, -0.111235,  0.617778,  0.234883, -0.09562 ,  0.408324,
       -0.107121,  0.717875,  0.674794,  0.127214, -0.178357,  0.331436,
        0.417898, -0.650833, -0.428309, -0.576132,  0.210533, -0.057879,
       -0.578397,  0.468586,  0.103365, -0.403216, -0.398776,  0.094514,
       -0.130387,  0.628187, -0.463082, -0.951649,  0.561544,  0.118903,
        0.448327, -0.171685, -0.672348,  0.069471,  0.556452, -0.335425],
      dtype=float32)
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">model.similar_by_key(&#39;投资&#39;)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;长期投资&#39;, 0.5135656595230103),
 (&#39;投资规模&#39;, 0.5089880228042603),
 (&#39;智百扬&#39;, 0.49565914273262024),
 (&#39;投资总额&#39;, 0.4955061078071594),
 (&#39;洛辉&#39;, 0.489188551902771),
 (&#39;337409&#39;, 0.48917514085769653),
 (&#39;洛盛&#39;, 0.4819018244743347),
 (&#39;洛腾&#39;, 0.4728960692882538),
 (&#39;394150&#39;, 0.4704836308956146),
 (&#39;投资额&#39;, 0.4685181975364685)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">similar_by_key</span><span class="p">(</span><span class="s1">&#39;风险&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;提示&#39;, 0.6549968123435974),
 (&#39;经营风险&#39;, 0.6316577792167664),
 (&#39;景气衰退&#39;, 0.544153094291687),
 (&#39;风险分析&#39;, 0.5439289212226868),
 (&#39;遇宏观&#39;, 0.5435716509819031),
 (&#39;信用风险&#39;, 0.5345730185508728),
 (&#39;承受能力&#39;, 0.5291797518730164),
 (&#39;防范&#39;, 0.5271924138069153),
 (&#39;系统性&#39;, 0.5178108811378479),
 (&#39;不确定性&#39;, 0.5173759460449219)]
</code></pre></div><p>向量运行效果还行，感兴趣的同学也可以根据自己的数据训练word2vec模型，训练及使用的办法参照文章</p>
<p><a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>将年报数据汇总至xlsx文件中</title>
      <link>https://textdata.cn/blog/sh_market/</link>
      <pubDate>Thu, 21 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/sh_market/</guid>
      <description>分散在各处的pdf很难分析，如何将pdf汇总至excel。本文将pdf汇总与excel分析结合。</description>
      <content:encoded><![CDATA[<h2 id="整理到csv中">整理到csv中</h2>
<p>将70G定期报告披露数据集下载</p>
<p>链接: <a href="https://pan.baidu.com/s/1oboFUswiAMdA_Wn3xCh6YQ">https://pan.baidu.com/s/1oboFUswiAMdA_Wn3xCh6YQ</a> 提取码: g7bd</p>
<p><img loading="lazy" src="img/sh_marketing.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pdfdocx</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">clean</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;\s&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>

<span class="c1">#文件夹列表</span>
<span class="n">dirs</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;reports&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;DS&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">d</span><span class="p">]</span>
<span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">dirs</span><span class="p">:</span>
    <span class="n">datas</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;reports/</span><span class="si">{d}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="n">di</span><span class="p">))</span> <span class="k">if</span> <span class="s1">&#39;z&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">file</span> <span class="o">=</span> <span class="s1">&#39;reports/</span><span class="si">{di}</span><span class="s1">/</span><span class="si">{f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">di</span><span class="o">=</span><span class="n">di</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>
            <span class="n">code</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">year</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;_(\d</span><span class="si">{4}</span><span class="s1">)_&#39;</span><span class="p">,</span> <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">clean</span><span class="p">(</span><span class="n">pdfdocx</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="n">file</span><span class="p">))</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="n">code</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">text</span><span class="p">,</span> <span class="s1">&#39;year&#39;</span><span class="p">:</span><span class="n">year</span><span class="p">}</span>
            <span class="n">datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">datas</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">])</span>
    <span class="c1">#将每家公司的年报导出到csv中</span>
    <span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;data.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
    
    
<span class="c1">#读取</span>
<span class="n">ndf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data.csv&#39;</span><span class="p">)</span>
<span class="c1">#去重</span>
<span class="n">ndf</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1">#清洗</span>
<span class="n">ndf</span> <span class="o">=</span> <span class="n">ndf</span><span class="p">[</span><span class="n">ndf</span><span class="o">.</span><span class="n">code</span><span class="o">=!=</span><span class="s1">&#39;code&#39;</span><span class="p">]</span>
<span class="c1">#导出到xlsx</span>
<span class="n">ndf</span><span class="o">.</span><span class="n">to_excel</span><span class="p">(</span><span class="s1">&#39;data.xlsx&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="导入数据">导入数据</h2>
<p>excel数据下载链接: <a href="https://pan.baidu.com/s/1r4YRyxb7bTsx-_ayT4GDKQ">https://pan.baidu.com/s/1r4YRyxb7bTsx-_ayT4GDKQ</a> 提取码: ew4v</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;data.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>code</th>
      <th>year</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>603859</td>
      <td>2017</td>
      <td>2017年半年度报告1/116公司代码：603859公司简称：能科股份能科节能技术股份有限公...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>603859</td>
      <td>2019</td>
      <td>2019年半年度报告1/141公司代码：603859公司简称：能科股份能科科技股份有限公司2...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>603859</td>
      <td>2018</td>
      <td>2018年半年度报告1/120公司代码：603859公司简称：能科股份能科科技股份有限公司2...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>601500</td>
      <td>2017</td>
      <td>2017年半年度报告1/114公司代码：601500公司简称：通用股份江苏通用科技股份有限公...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>601500</td>
      <td>2019</td>
      <td>2019年半年度报告1/140公司代码：601500公司简称：通用股份江苏通用科技股份有限公...</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<h3 id="查看数据量">查看数据量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><pre><code>16984
</code></pre>
<br>
<h3 id="公司数">公司数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">code</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
</code></pre></div><pre><code>1476
</code></pre>
<br>
<h3 id="含有的年份">含有的年份</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">year</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</code></pre></div><pre><code>[2002,
 2003,
 2004,
 2005,
 2006,
 2007,
 2008,
 2009,
 2010,
 2011,
 2012,
 2013,
 2014,
 2015,
 2016,
 2017,
 2018,
 2019]
</code></pre>
<br>
<h3 id="每家公司年报数">每家公司年报数</h3>
<p>数据集中，平均每家公司的年报数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">avg</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">/</span><span class="n">df</span><span class="o">.</span><span class="n">code</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
<span class="nb">round</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div><pre><code>11.51
</code></pre>
<br>
<h2 id="说明">说明</h2>
<p>数据是19年获取的，数据不全，下载过程中有部分pdf是破损的文件。</p>
<p>大家可以尝试该数据集训练会计年报词向量，看看有没有有趣的应用。</p>
<p>本数据可作探索实验性质，如果想在会计领域深入挖掘，建议找更全更精准的数据集。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>豆瓣影评 | 探索词向量妙处</title>
      <link>https://textdata.cn/blog/douban_w2v/</link>
      <pubDate>Thu, 21 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/douban_w2v/</guid>
      <description>使用cntext训练、使用词向量。</description>
      <content:encoded><![CDATA[<p>本文要点</p>
<ul>
<li>读取csv</li>
<li>cntext训练词向量模型</li>
<li>cntext扩展pos、neg词典</li>
<li>导入词向量模型</li>
<li>运用词向量模型</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p>链接: <a href="https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg">https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg</a> 提取码: og9t</p>
<p><br><br></p>
<h2 id="一读取数据">一、读取数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;douban.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;电影  : </span><span class="si">{}</span><span class="s2"> 部&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Movie_Name_CN</span><span class="o">.</span><span class="n">nunique</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;评论  : </span><span class="si">{}</span><span class="s2"> 条&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)))</span>
</code></pre></div><pre><code>电影  : 28 部
评论  : 2125056 条
</code></pre>
<br>
<h2 id="二训练模型">二、训练模型</h2>
<p>使用<a href="https://textdata.cn/blog/cntext_simplification/">cntext库</a>训练词向量word2vec模型,这里我把csv数据整理为txt</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext</span> <span class="kn">import</span> <span class="n">W2VModels</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#训练word2vec模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>  <span class="c1">#语料数据</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;douban.txt&#39;</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...预处理    语料 ...
Step 2/4:...训练   word2vec模型
            耗时   2001 s
        
</code></pre></div><p>cntext可以用于扩展词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;pos.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;neg.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 3/4:...准备 每个seed在word2vec模型中的相似候选词...
Step 4/4 完成! 耗时 2302 s
Step 3/4:...准备 每个seed在word2vec模型中的相似候选词...
Step 4/4 完成! 耗时 2303 s
</code></pre></div><p>在代码所在文件夹内可以找到</p>
<ul>
<li>output/w2v_candi_words/w2v.model</li>
<li>新的  pos.txt</li>
<li>新的  neg.txt</li>
</ul>
<p>新的pos.txt是对pos.txt词典的扩展。</p>
<br>
<br>
<h2 id="三导入w2v模型">三、导入w2v模型</h2>
<p>有的时候数据量特别大，模型训练十分不易。</p>
<p>这时，保存已训练好的模型，不止下次不用再同样的数据再次训练，也可分享给其他人使用。</p>
<p>训练结束后，在代码所在文件夹内可以找到 <code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span>
</code></pre></div><pre><code>&lt;gensim.models.keyedvectors.KeyedVectors at 0x7face0574880&gt;
</code></pre>
<p>w2v_models数据类型为KeyedVectors， 在本文中使用w2v_models代指KeyedVectors</p>
<br>
<h2 id="四玩转词向量">四、玩转词向量</h2>
<p>用户级的数据(如在线评论)感觉生成的向量会准一些，<strong>词向量的方向，近义反义在向量中都有体现</strong>。</p>
<p><img loading="lazy" src="man-woman.png" alt=""  />
</p>
<p>例如本文使用的是28部电影的2125056条影评， 一般评论内容包含电影相关信息，如电影题材、是否值的观影等。</p>
<p>而在我们训练出模型w2v_models存在一些常用的方法</p>
<ul>
<li><strong>w2v_model.get_vector(key)</strong> 获取key的词向量</li>
<li><strong>w2v_model.most_similar_to_given(key1, keys_list)</strong>  从 keys_list 中获取与 key1 最相似的词</li>
<li><strong>w2v_model.n_similarity(ws1, ws2)</strong> 两组词ws1, ws2 的相似度</li>
<li><strong>w2v_model.closer_than(key1, key2)</strong> 更接近于key1的词向量(相比于key2)</li>
<li><strong>w2v_model.most_similar(positive, negative)</strong> 找出与positive同方向，与negative反向相反的词。</li>
</ul>
<h3 id="41-get_vectorkey">4.1 get_vector(key)</h3>
<p>w2v_model.get_vector(key) 获取key的词向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取某词语的向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>array([ 0.06488553,  0.74188954,  0.25468495,  0.89755714,  1.8139195 ,
       -0.6950082 ,  0.24339403, -1.2188634 ,  0.543618  , -0.9988698 ,
        0.27471313,  0.9325699 , -0.5860608 , -0.5081917 ,  1.6423215 ,
       -0.0490295 , -0.3927043 ,  0.659067  ,  0.03185922, -1.021391  ,
       -1.3214804 , -0.28208104, -0.7819419 , -0.30637202, -1.5944146 ,
       -0.12383854, -0.70463836,  0.45689437,  1.223081  , -1.9453759 ,
       -0.5538997 , -0.9750523 , -0.10031194, -0.9568689 ,  0.30341247,
        1.1102395 ,  0.667315  , -1.1600997 , -0.26674765, -0.55144155,
       -0.3246094 ,  0.82902473, -0.47339582, -0.9009957 ,  1.7722464 ,
        0.28959563, -0.03453476,  0.4786787 , -0.48074463, -0.23090109,
       -0.49390873,  0.71246386,  2.1557336 ,  2.4899387 , -0.51481706,
        0.5579966 , -0.6973235 , -1.1408254 ,  0.72495663, -1.0326954 ,
       -0.5455598 ,  0.98941576, -1.2155218 , -0.9088408 ,  1.9184568 ,
       -0.21800426, -1.2009395 ,  0.29684314,  1.3672423 , -2.269391  ,
        0.6188098 , -0.02714545, -0.44811317,  1.4397241 , -1.0594722 ,
       -0.08088647, -0.13015983, -0.99255013,  0.62044877,  2.5046496 ,
        0.4054545 , -0.38767585, -0.6956541 ,  0.22991426,  0.5928579 ,
       -0.12684819, -0.17408212,  0.25033692, -1.4419957 , -0.27390227,
        1.166638  , -0.00624323, -1.6046506 ,  2.1633575 , -0.395548  ,
       -1.1297956 , -3.1474566 ,  0.38729438, -2.0434535 , -1.5511289 ],
      dtype=float32)
</code></pre>
<br>
<h3 id="42-most_similar_to_givenkey1-keys_list">4.2 most_similar_to_given(key1, keys_list)</h3>
<p>从 keys_list 中获取与 key1 最相似的词。例如在212w影评中，从<code>'爱情', '悬疑', '飞船', '历史', '战争'</code>找出最接近<code>'太空'</code>，最后返回<code>'飞船'</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#从 `keys_list` 中获取与 `key1` 最相似的 `key`。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar_to_given</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;太空&#39;</span><span class="p">,</span> 
                                <span class="n">keys_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;爱情&#39;</span><span class="p">,</span> <span class="s1">&#39;悬疑&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;历史&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>'飞船'
</code></pre>
<br> 
<h3 id="43-w2v_modeln_similarityws1-ws2">4.3 w2v_model.n_similarity(ws1, ws2)</h3>
<p>两组词ws1, ws2 的相似度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="n">cosine_similarity</span><span class="p">([</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;理想&#39;</span><span class="p">)],</span>  
                  <span class="p">[</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;现实&#39;</span><span class="p">)])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div><pre><code>0.5371934
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cosine算法</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.5371934
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#计算两组键之间的余弦相似度。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;赞&#39;</span><span class="p">,</span> <span class="s1">&#39;推荐&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;无聊&#39;</span><span class="p">,</span> <span class="s1">&#39;尴尬&#39;</span><span class="p">,</span> <span class="s1">&#39;垃圾&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.35008422
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> <span class="s1">&#39;梦想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">,</span> <span class="s1">&#39;生活&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.48020104
</code></pre>
<br>
<h3 id="44-w2v_modelcloser_thankey1-key2">4.4 w2v_model.closer_than(key1, key2)</h3>
<p>更接近于key1的词向量(相比于key2)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取所有更接近 `key1` 的键，而不是 `key2` 。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">closer_than</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> 
                      <span class="n">key2</span><span class="o">=</span><span class="s1">&#39;现实&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>['梦想', '妥协', '追梦', '愿望', '骨感']
</code></pre>
<br>
<h3 id="45-w2v_modelmost_similarpositive-negative">4.5 w2v_model.most_similar(positive, negative)</h3>
<p>找出与positive同方向，与negative反向相反的词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;过瘾&#39;</span><span class="p">],</span>
                       <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;垃圾&#39;</span><span class="p">],</span>
                       <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><pre><code>[('激动人心', 0.6859163045883179),
 ('惊心动魄', 0.6767394542694092),
 ('带感', 0.6723690032958984),
 ('惊险刺激', 0.667783796787262),
 ('刺激', 0.6445038318634033),
 ('燃', 0.6429688930511475),
 ('爽快', 0.6287934184074402),
 ('带劲', 0.6254130005836487),
 ('爽', 0.624543309211731),
 ('酣畅淋漓', 0.6140543818473816)]
</code></pre>
<br>
<h3 id="46-类比king-manwomanqueen">4.6 类比king-man+woman~queen</h3>
<p><img loading="lazy" src="kingqueenformular.png" alt=""  />
</p>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。</p>
<p>这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p>gender_direction_1 = vector(man)-vector(woman)</p>
<p>gender_direction_2 = vector(king)-vector(queen)</p>
<p>那两个性别方向应该近似，假设这里将其gender_direction_1=gender_direction_2，则对于公式中任意一个词，都可以由等式中的其他三个词经过运算得到。例如</p>
<p>vector(queen) =  vector(king)-vector(man)+vector(woman)</p>
<p>这里构造了一个情绪的公式，计算如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 开心 - 难过 ~=  享受 - d</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;开心&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;难过&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;享受&#39;</span><span class="p">)</span>

<span class="c1">#d = a-b+c</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
</code></pre></div><pre><code>[('享受', 0.7833479046821594),
 ('开心', 0.6825607419013977),
 ('愉快', 0.6298696994781494),
 ('娱乐', 0.6215130090713501),
 ('感官', 0.6085000038146973),
 ('图个', 0.6052624583244324),
 ('图一乐', 0.6039161682128906),
 ('休闲', 0.60273677110672),
 ('视觉享受', 0.6006160378456116),
 ('轻松愉快', 0.5961319804191589)]
</code></pre>
<p>很遗憾，d没有运算出煎熬之类的词语，但好在都是形容词，而且是快乐居多的形容词，类别是对的，就是方向是反的。</p>
<br>
<h3 id="词向量总结">词向量总结</h3>
<p>需要注意的是经典的运算king-man+woman~queen来自glove模型，而不是本文使用的word2vec模型。两者相同点，glove与word2vec均为词嵌入embeddings技术。区别在于glove获取的词的全局语义空间，而word2vec一般是某个词前后n个词(例如前后5个词)范围内的语义。做概念四则运算，以后如可能，建议用glove。</p>
<p>此外，即时使用glove，尽量使用概念的词组均值向量。首先要训练数据要存在这些人类认知的线索。其次，认知概念往往不是由一个词决定的，可能需要相关的很多词。例如人类社会中的<code>雄雌(没有贬义，包含了男女在内的概念)</code>，</p>
<ul>
<li>雄性概念词有<code>他、男人、男孩、父亲、爷爷、爸爸、姥爷...</code></li>
<li>雌性概念词有<code>她、女人、女孩、母亲、奶奶、妈妈、姥姥...</code></li>
<li>国王概念词有<code>查理n世、乔治、路易...</code></li>
<li>女王概念词有<code>伊丽莎白n世、维多利亚女王、叶卡捷琳娜二世...</code></li>
</ul>
<p>或许改成概念向量四则运算，公式可能更容易成立。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>推荐 | Python文本分析与会计(视频) </title>
      <link>https://textdata.cn/blog/accountingtext/</link>
      <pubDate>Sat, 16 Apr 2022 15:40:10 +0600</pubDate>
      
      <guid>/blog/accountingtext/</guid>
      <description>会计Python文本分析, 文本是如何产生，信息的发布者与接收者如何相互影响，分析师为了预测还是解释现象。为了开展研究，如何获取数据，如何测量文本中的态度、偏见、情感分析。。Accounting Python text analysis, how the text is generated, how the publisher and receiver of the information interact, and whether the analyst predicts or explains the phenomenon. How to get data, how to measure attitudes, biases, sentiment analysis in texts in order to conduct research. .</description>
      <content:encoded><![CDATA[<iframe
    src="//player.bilibili.com/player.html?bvid=BV1vA4y197YR&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<h1 id="文本分析与会计">文本分析与会计</h1>
<h2 id="资料下载accountingtextzip"><a href="accountingtext.zip">资料下载</a></h2>
<ol>
<li>
<p>数据挖掘一般会遇到两个难题</p>
<ol>
<li>
<p>如何从网络中高效地 <strong>采集数据</strong>？</p>
<p>批量下载、汇总、清洗、整理</p>
</li>
<li>
<p>如何从文本数据中<strong>抽取文本信息(变量)</strong>？</p>
<p>情感、客观性、主观性、偏见</p>
</li>
</ol>
</li>
</ol>
<p><img loading="lazy" src="img/unstructrueddata.png" alt=""  />
</p>
<h2 id="目录">目录</h2>
<h3 id="1-认识python">1. 认识Python</h3>
<ul>
<li>学Python的n理由</li>
<li>Python是一门语言</li>
<li>数据类型与语法</li>
<li>如何写Python代码</li>
</ul>
<p>​</p>
<h3 id="2-text-as-data">2. Text as Data</h3>
<ul>
<li>
<p>角色-Producer/Receiver</p>
</li>
<li>
<p>机制-Reflects/Impacts</p>
</li>
<li>
<p>目的-Predict/Understanding</p>
</li>
<li>
<p>方法-定性vs定量</p>
</li>
<li>
<p>文本分析的常用指标</p>
</li>
</ul>
<h3 id="3-文本特征工程">3. 文本特征工程</h3>
<ul>
<li>词袋法(文档向量)</li>
<li>词向量</li>
<li>文档向量化</li>
<li>词向量</li>
</ul>
<h3 id="4-文本分析指标">4. 文本分析指标</h3>
<h3 id="5-文本分析应用案例会计">5. 文本分析应用案例(会计)</h3>
<p><br><br></p>
<h1 id="一认识python">一、认识Python</h1>
<p><img loading="lazy" src="img/whatcandowithpython.png" alt=""  />
</p>
<h2 id="11-学python的n理由">1.1 学Python的n理由</h2>
<ul>
<li>
<p><strong>简单</strong></p>
</li>
<li>
<p><strong>用户多</strong></p>
</li>
<li>
<p><strong>能做很多有意思的事</strong></p>
<ul>
<li>自动化办公
<ul>
<li>群发邮件</li>
<li>自动生成报表</li>
</ul>
</li>
<li>网络爬虫
<ul>
<li>在线秒杀</li>
<li>下载音频、视频pdf报告</li>
<li>明星的微博有新消息后邮箱提醒你</li>
</ul>
</li>
<li>数据分析</li>
<li>可视化</li>
<li>机器学习</li>
<li>物联网</li>
<li>制作网站</li>
</ul>
</li>
</ul>
<br>
<h2 id="12-python是一门语言">1.2 Python是一门语言</h2>
<p><img loading="lazy" src="img/simplecode.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">There is such a number a, if a is greater than or equal to 0, we will print a; if a is less than 0, we will print -a.
</code></pre></div><br>
<h2 id="13-数据类型与语法">1.3 数据类型与语法</h2>
<table>
<thead>
<tr>
<th style="text-align:left">英语</th>
<th style="text-align:left">Python</th>
<th style="text-align:left">例如</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>单词</strong></td>
<td style="text-align:left">数据类型</td>
<td style="text-align:left">数字、列表、字符串、字典等</td>
</tr>
<tr>
<td style="text-align:left"><strong>语法</strong></td>
<td style="text-align:left">逻辑语句</td>
<td style="text-align:left">if条件判断语句、for循环语句等</td>
</tr>
</tbody>
</table>
<br>
<table>
<thead>
<tr>
<th style="text-align:left">数据类型</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">数字</td>
<td><code>age = 25</code></td>
</tr>
<tr>
<td style="text-align:left">字符串</td>
<td><code>intro = &quot;Hello, my name is ...&quot;</code></td>
</tr>
<tr>
<td style="text-align:left">列表</td>
<td><code>my_list = [1, 2, 3]</code></td>
</tr>
<tr>
<td style="text-align:left">字典</td>
<td><code>ages = {'David': 25, 'Mark':30}</code></td>
</tr>
<tr>
<td style="text-align:left">空值</td>
<td><code>None</code></td>
</tr>
<tr>
<td style="text-align:left">布尔值</td>
<td><code>True, False</code></td>
</tr>
</tbody>
</table>
<br>
<h2 id="14-如何写python代码">1.4 如何写Python代码</h2>
<p>实现一定功能, 代码一般由三部分组成</p>
<ol>
<li>数据类型</li>
<li>逻辑语句</li>
<li>相应功能Python包(库)</li>
</ol>
<p><img loading="lazy" src="img/bebetter.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#数据类型</span>
<span class="n">ability</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">scale</span> <span class="o">=</span> <span class="mf">1.01</span>
<span class="n">records</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1">#逻辑语句</span>
<span class="n">days</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">365</span><span class="p">)</span>
<span class="k">for</span> <span class="n">day</span> <span class="ow">in</span> <span class="n">days</span><span class="p">:</span>
    <span class="n">ability</span> <span class="o">=</span> <span class="n">ability</span><span class="o">*</span><span class="n">scale</span>
    <span class="n">records</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ability</span><span class="p">)</span>
    
<span class="c1">#相应的库  </span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">days</span><span class="p">,</span> <span class="n">records</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Day day Up in one field!&#39;</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h1 id="二text-as-data">二、Text as Data</h1>
<h2 id="heading"></h2>
<h2 id="21--producerreceiver">2.1  Producer/Receiver</h2>
<p>本节讨论的是涉及到文本的形形色色的角色</p>
<p><img loading="lazy" src="img/SenderReceiver.png" alt=""  />
</p>
<p>文本信息的==producer== 与 ==receiver==，涵盖 ==个人、公司(组织)、国家(社会)==三个层面。</p>
<p><img loading="lazy" src="img/consumer_org_society.png" alt=""  />
</p>
<p><img loading="lazy" src="img/%e7%94%9f%e4%ba%a7%e4%b8%8e%e6%b6%88%e8%b4%b9.png" alt=""  />
</p>
<br>
<h2 id="22-reflectsimpacts">2.2 Reflects/Impacts</h2>
<blockquote>
<p><strong>编码解码理论</strong></p>
<p>文化研究之父斯图亚特·霍尔（Stuart Hall）在《电视话语中的编码和解码》（<em>Encoding and decoding inthe television discourse</em>）一文中提出了“编码解码”理论。</p>
<ul>
<li><strong>编码（encoding）</strong>，信息传播者将所传递的讯息、意图或观点，转化为具有特定规则的代码。</li>
<li><strong>解码（decoding）</strong>，信息接受者，将上述代码按特定规则进行解读。</li>
</ul>
</blockquote>
<p>需要注意的是文本的 ==反映Reflects== 和==影响Impacts==并不是非此即彼，往往会同时起作用。</p>
<table>
<thead>
<tr>
<th>&mdash;</th>
<th>研究目的</th>
<th>自变量</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Reflects</strong></td>
<td>文本可以反映<strong>producer</strong>的一些特质，帮助研究者理解producer。<br>例如试图挖掘producer的个性personality或隶属于什么社会团体。</td>
<td>了解公司的品牌个性；<br>年报含有未来业绩表现的线索；<br>消费者们在品牌社区的言语能更深的投射出消费者对品牌的态度；<br>而更宏大的层面，文本也能反映出文化差异。<br>了解消费者是否喜欢新产品，消费者如何看待品牌，消费者最看重什么</td>
</tr>
<tr>
<td><strong>Affects</strong></td>
<td>知道文本如何影响<strong>receiver</strong>，receiver会有什么样的行为和选择。</td>
<td>检验文本是否以及如何导致消费者诸如购买、分享和卷入行为。<br>广告会塑造消费者的消费行为<br>消费者杂志会扭曲消费者产品分类感知<br>电影剧本会影响观众的反应</td>
</tr>
</tbody>
</table>
<br>
<h2 id="23-predictunderstanding">2.3 Predict/Understanding</h2>
<p>使用文本数据的目的是</p>
<table>
<thead>
<tr>
<th>&mdash;</th>
<th>Reflects</th>
<th>Affects</th>
<th>目的</th>
<th>应用</th>
<th>难点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Predict</strong></td>
<td>预测 <strong>producer</strong>的状态、特性、性格等</td>
<td>预测 <strong>receiver</strong>阅读、分享和购买行为</td>
<td>相比搞清楚作用机制(因果关系)，更关心预测的准确性。</td>
<td>什么消费者最喜欢贷款;<br>什么电影会大火;<br>未来股市走向;<br></td>
<td>文本数据可以生成成千上万的特征(相当于变量x1，x2&hellip;xn)，而文本数据记录数甚至可能少于特征数。<br>为了解决这个为题，使用新的特征分类方法，减少特征数量，又有可能存在拟合问题。</td>
</tr>
<tr>
<td><strong>Understanding</strong></td>
<td>为什么当人们压抑的时候会使用特殊人称。</td>
<td>来理解为何带有情绪的文本会更容易被阅读和分享</td>
<td>理解为什么事情发生以及如何发生的<br/>这类研究往往会用到心理学、社会学的<strong>实证方法</strong>，旨在<strong>理解某个文本特征会导致什么后续结果，以及为什么产生这样的后果</strong>。</td>
<td>消费者怎样表达会如何影响口碑;<br>为何某些推文会被挑中分享？<br> 歌曲为何变火？<br> 品牌如何让消费者忠诚？</td>
<td>找出观测数据背后的因果关系。相应的，该领域的工作可能会强调实验数据，以允许对关键的独立变量进行操作。<br>另一个挑战是解释文本特征之间的关系。</td>
</tr>
</tbody>
</table>
<br>
<h2 id="24--定性定量">2.4  定性/定量</h2>
<p>经过刚刚定量技术的介绍，现在对定性与定量粗略做个对比。</p>
<table>
<thead>
<tr>
<th style="text-align:left">定性/量</th>
<th>分析方法</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>定性</strong>（text as text）</td>
<td>质性（扎根）</td>
<td>依靠研究者领域知识，可以对少量的数据做出深刻洞见。</td>
<td>难以应对大规模数据；<br>编码过程并不能保证唯一；</td>
</tr>
<tr>
<td style="text-align:left"><strong>定量</strong> textual data(text as data)</td>
<td>明显的文本特征，如词频、可阅读性</td>
<td>标准如一;<br>适合大规模文本挖掘；<br>纷繁复杂中涌现出潜在规律</td>
<td><strong>需要破坏文本的结构，丧失了部分信息量</strong></td>
</tr>
</tbody>
</table>
<br>
<h2 id="25-文本指标">2.5 文本指标</h2>
<p><strong>词典法，对某个词、某类词(词典)的统计个数多少</strong>，。特点容易理解，简单，实施性强。</p>
<ul>
<li><strong>数量</strong>； 如文本长度(e.g., Godes and Mayzlin 2004; Moe and Trusov2011)</li>
<li><strong>主观性</strong>； 情感得分，情感词词典(e.g., Godes and Silva 2012; Moe and Schweidel 2012; Ying, Feinberg and Wedel 2006)·</li>
<li><strong>客观性</strong>，如方差、信息墒(e.g., Godes and Mayzlin 2004).
<ul>
<li>A  <code>产品不错， 包装破损， 态度很好， 综合还是推荐大家购买!</code>  [5,1,5,4]</li>
<li>B<code>产品垃圾，使用垃圾， 包装破损， 差评!! </code>    [1,  1,  1,  1]</li>
<li>A的方差更大，信息量更客观公正。</li>
</ul>
</li>
<li><strong>实体词词频</strong>； 例如“电脑”商品的在线评论中“电脑”出现次数会远多于其他词。</li>
<li><strong>可读性</strong>；阅读难易程度，根据词典或词的字母数测量</li>
<li><strong>不确定性</strong>；经济政策不确定性词典</li>
<li><strong>偏见，态度</strong>；将每个词看做向量，对向量进行计算</li>
</ul>
<p><br><br></p>
<h1 id="三文本特征工程">三、文本特征工程</h1>
<h2 id="31-文档向量化">3.1 文档向量化</h2>
<h3 id="311-词袋法">3.1.1 词袋法</h3>
<p>以**词典法(语料中所有词均列入词典)**为基础，文档向量化</p>
<p><img loading="lazy" src="img/05-bagofwords.png" alt=""  />
</p>
<h3 id="312-one-hot">3.1.2 one-hot</h3>
<p>与词袋非常类似的算法还有one-hot</p>
<p><img loading="lazy" src="img/03-one-hot.png" alt=""  />
</p>
<h3 id="313-tf-idf">3.1.3 tf-idf</h3>
<p>不止考虑出现次数，还要考虑词语出现场景的可诊断性</p>
<p><img loading="lazy" src="img/tf.png" alt=""  />

<img loading="lazy" src="img/idf.png" alt=""  />

<img loading="lazy" src="img/tfidf.png" alt=""  />

<img loading="lazy" src="img/TFIDFExample.png" alt=""  />
</p>
<h2 id="32-词向量">3.2 词向量</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">Docs</span> <span class="o">=</span><span class="p">[</span><span class="s2">&#34;Mom is a happy woman&#34;</span><span class="p">,</span>
       <span class="s2">&#34;Dad is happy.!&#34;</span><span class="p">]</span>
</code></pre></div><p>词典中带顺序<code>[Mom, is a happy woman dad]</code></p>
<table>
<thead>
<tr>
<th>技术</th>
<th>技术</th>
<th>维度类比</th>
<th>任务</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>字典法</strong>（词频）</td>
<td>数个数</td>
<td>原子</td>
<td>统计每句话里的名词个数</td>
<td>sent_num1 = 2<br>sent_num2 = 1</td>
</tr>
<tr>
<td><strong>词袋法</strong></td>
<td>bag of words<br>one-hot<br>Tf-idf</td>
<td>分子</td>
<td>转化为词向量, 计算两个句子相似度。</td>
<td>vec1 = [1, 1, 1, 1, 1, 0]<br>vec2 = [0, 1, 0, 1, 0, 1]<br>similarity = cosine(vec1, vec2)</td>
</tr>
<tr>
<td><strong>词嵌入</strong></td>
<td>word2vec、<br>glove等</td>
<td>中子、质子、电子</td>
<td>词语相似度。(语义上大小相近，方向相反)</td>
<td>mom = [0.2, 0.7, 0.1]<br/>dad   = [0.3, 0.5, -0.2]</td>
</tr>
</tbody>
</table>
<p>有意思的是，词嵌入Embeddings，尤其是glove，通过一定的向量化运算，可以挖掘出人类留下的认知信息，如态度、偏见等。词嵌入模型训练的方式不同，能做不同的计算。</p>
<p><img loading="lazy" src="img/word2vec.png" alt=""  />
</p>
<h3 id="321-按群体">3.2.1 按群体</h3>
<p>将数据按照producer划分，对每类producer的文本数据分别训练<strong>词嵌入模型</strong></p>
<p><img loading="lazy" src="img/musicSuccessGenderbias.png" alt=""  />
</p>
<p><img loading="lazy" src="img/americanClass.jpeg" alt=""  />
</p>
<h3 id="322-按时间">3.2.2 按时间</h3>
<p>将时间分为不同时间段，对每个时间段内的文本数据分别训练<strong>词嵌入模型</strong></p>
<p><img loading="lazy" src="img/DiachronicWordEmbeddings.png" alt=""  />
</p>
<p><br><br></p>
<h1 id="四技术对比">四、技术对比</h1>
<p><img loading="lazy" src="img/%e5%88%86%e6%9e%90%e6%96%b9%e6%b3%95.png" alt=""  />
</p>
<p>从左向右，自动化程度越来越高； 相对而言， 后期人工介入的越来越少。</p>
<table>
<thead>
<tr>
<th>技术</th>
<th>描述</th>
<th>优点</th>
<th>缺点</th>
<th>应用领域</th>
<th>Python包</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>主题分析</strong></td>
<td>人工编码</td>
<td>使用参与者自己的话语或者构念来挖掘数据，对少量文本理解的更深入</td>
<td>属于时间、劳动密集型任务，不适合大规模数据。<br>由于不同的编码人员有不同的经历和偏好，编码过程的标准不可靠</td>
<td>社会学、管理学</td>
<td></td>
</tr>
<tr>
<td><strong>字典法</strong></td>
<td>统计文本中词语的出现个数(占比)</td>
<td>允许对研究的数据进行定量分析，有标准，规格唯一</td>
<td>采用的词典应尽量与研究问题适应，词典适配性问题突出。情感分析，形容词词典。</td>
<td>管理学</td>
<td>jieba</td>
</tr>
<tr>
<td><strong>词袋法</strong></td>
<td>文本向量化</td>
<td>编码标准稳定简单，扩展性强</td>
<td>编码过程忽略词语的先后顺序；舍弃了一些信息量</td>
<td>管理学</td>
<td>jieba<br>scikit-learn</td>
</tr>
<tr>
<td><strong>监督学习</strong></td>
<td><strong>文本分类</strong></td>
<td>允许事先定义编码规则；逻辑简单</td>
<td>需要高质量的标注数据(工作量大)；特征词太多，训练的模型很容易过拟合。</td>
<td>计算机学、政治学、管理学</td>
<td>scikit-learn</td>
</tr>
<tr>
<td><strong>无监督学习</strong></td>
<td>主题建模<br>LDA话题模型</td>
<td>在没有人工标注的情况下，加速了数据的“标注”或“分类”</td>
<td>“标注”是机器按照数字特征进行的分组，需要研究者解读才可以赋予“标准“意义；训练过程需要大量的调参</td>
<td>计算机学、政治学、管留学</td>
<td>scikit-learn<br/></td>
</tr>
<tr>
<td><strong>自然语言处理</strong></td>
<td>考虑词语上下文语境顺序，word2vec、glove等</td>
<td>计算机自动化；可分析语义</td>
<td>大多数模型是人类无法解读的黑箱；<br>虽然代码编程量小，但训练代码耗时巨大</td>
<td>计算科学；市场营销；心理学</td>
<td>gensim<br>等</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h1 id="五文本分析论文解读">五、文本分析论文解读</h1>
<h2 id="51-应用">5.1 应用</h2>
<table>
<thead>
<tr>
<th>论文</th>
<th>定性</th>
<th>词典</th>
<th>向量</th>
</tr>
</thead>
<tbody>
<tr>
<td>胡楠, 薛付婧 and 王昊楠, 2021. 管理者短视主义影响企业长期投资吗———基于文本分析和机器学习. <em>管理世界</em>, <em>37</em>(5), pp.139-156.</td>
<td></td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</td>
<td></td>
<td>Y</td>
<td>Y</td>
</tr>
<tr>
<td>王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</td>
<td>Y</td>
<td>Y</td>
<td></td>
</tr>
</tbody>
</table>
<br>
<p><img loading="lazy" src="img/%e7%9f%ad%e8%a7%86%e4%b8%bb%e4%b9%89.png" alt=""  />
</p>
<h2 id="52-分析步骤">5.2 分析步骤</h2>
<table>
<thead>
<tr>
<th>步骤</th>
<th>任务</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 研究问题</td>
<td><strong>Produce/Receive</strong> 、 <strong>Reflects/Impact</strong><br><strong>管理层短视特质x -&gt; 企业资本支出和研发支出y</strong></td>
<td></td>
</tr>
<tr>
<td>2. 数据收集</td>
<td>巨潮资讯网； <br>所有 A 股；<br>2007~2018 年年度财务报告文件</td>
<td>Python网络爬虫</td>
</tr>
<tr>
<td>3. 设计构念</td>
<td>短视主义词有哪些<br>训练word2vec模型，找到”尽快“近义词，<br>如、”尽早“、”抓紧“、”力争“、”加紧“</td>
<td>word2vec</td>
</tr>
<tr>
<td>4. 测量构念</td>
<td>统计不同年报中MD&amp;A中的短视主义词出现占比</td>
<td>词典法</td>
</tr>
<tr>
<td>5. 计量建模</td>
<td>计算 x 与y之间的关系</td>
<td></td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="相关文献">相关文献</h2>
<p>冉雅璇,李志强,刘佳妮,张逸石.<strong>大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</strong>.南开管理评论1-27</p>
<p>沈艳, 陈赟, &amp; 黄卓. (2019). <strong>文本大数据分析在经济学和金融学中的应用: 一个文献综述</strong>. <em>经济学 (季刊)</em>, <em>18</em>(4), 1153-1186.</p>
<p>Berger, J., Humphreys, A., Ludwig, S., Moe, W.W., Netzer, O. and Schweidel, D.A., 2020. <strong>Uniting the tribes: Using text for marketing insight</strong>. <em>Journal of Marketing</em>, <em>84</em>(1), pp.1-25.</p>
<p>Kenneth Benoit. July 16, 2019. “Text as Data: An Overview” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. <em>Handbook of Research Methods in Political Science and International Relations</em>. Thousand Oaks: Sage.</p>
<p>Anand, V., Bochkay, K., Chychyla, R. and Leone, A.J., 2020. <strong>Using Python for text analysis in accounting research</strong>. <em>Vic Anand, Khrystyna Bochkay, Roman Chychyla and Andrew Leone (2020),&quot; Using Python for Text Analysis in Accounting Research&quot;, Foundations and Trends® in Accounting</em>, <em>14</em>(3-4), pp.128-359.</p>
<p>Cohen, L., Malloy, C. and Nguyen, Q., 2020. <strong>Lazy prices</strong>. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</p>
<p>胡楠, 薛付婧 and 王昊楠, 2021. 管理者短视主义影响企业长期投资吗———基于文本分析和机器学习. <em>管理世界</em>, <em>37</em>(5), pp.139-156.</p>
<p>王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</p>
<p>Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. “A review of best practice recommendations for text analysis in R (and a user-friendly app).” <em>Journal of Business and Psychology</em> 33, no. 4 (2018): 445-459.</p>
<p>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</p>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>WordBias库 | 发现偏见(刻板印象)的交互式工具</title>
      <link>https://textdata.cn/blog/wordbias/</link>
      <pubDate>Thu, 14 Apr 2022 22:43:10 +0600</pubDate>
      
      <guid>/blog/wordbias/</guid>
      <description>一种用于发现偏见(刻板印象)的交互式可视化工具An interactive visualization tool for spotting biases (stereotypes)</description>
      <content:encoded><![CDATA[<p>词嵌入做为一种词向量模型，可以从文本中计算出隐含的上下文情景信息，态度及偏见。通过词向量距离的测算，就可以间接测得不同群体对某概念(组织、群体、品牌、地域等)的态度偏见。偏见(刻板印象)的介绍有</p>
<p><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></p>
<p><em><strong>计算机科学家，正研究如何在AI中减弱甚至剔除刻板印象；但在社会科学领域，接受已有数据中存在的刻板印象，在数据中测量Bias，发现Bias，应用Bias，也能更好的认识和改造社会</strong></em> 。今天介绍一个挺好玩的工具WordBias。</p>
<br>
<h2 id="wordbias">WordBias</h2>
<p><a href="https://github.com/bhavyaghai/WordBias"><strong>WordBias</strong></a>：一种用于发现词嵌入偏见(刻板印象)的交互式可视化工具， 旨在探索子群体（intersectional groups，直译为交叉群体）（如黑人女性、黑人穆斯林男性等）在词嵌入中的编码偏见。 我们的工具认为一个词与一个交叉组相关联，例如“Christian Males”，如果它与它的每个构成子集（Christians 和 Males）密切相关。 我们的工具旨在为专家提供有效的<i>审核</i>工具，为非专家提供<i>教育工具</i>，并增强领域专家的<i>可访问性</i>。</p>
<blockquote>
<p>例如对“黑人男性”的刻板印象，是由“男人”和“黑人”两类刻板印象加上一些其他线索组成的。</p>
<p>所以这里intersectional groups，直译为交叉群体, 感觉不太好理解， 我把intersectional groups理解为群体中的子群体。个人理解，不一定合理，欢迎留言。</p>
</blockquote>
<ul>
<li>
<p><a href="https://arxiv.org/abs/2103.03598"><strong>Read paper</strong> </a></p>
</li>
<li>
<p><a href="https://www.youtube.com/watch?v=LcwlyU3QT0w"><strong>视频演示(5min)</strong></a></p>
</li>
<li>
<p><a href="http://130.245.128.219:6999/"><strong>在线Demo</strong></a></p>
</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<ul>
<li>
<p><a href="https://github.com/hiDaDeng/WordBias.git">下载这个仓库</a></p>
</li>
<li>
<p>命令行切换至WordBias文件夹,安装依赖包</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd Path_of_WordBias_Directory
pip3 install -r req.txt
</code></pre></div></li>
<li>
<p>运行WordBias，命令行执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">python3 app.py
</code></pre></div></li>
<li>
<p>在浏览器中打开打开链接</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">https://localhost:6999
</code></pre></div></li>
</ul>
<p>浏览器中会出现界面，如下图</p>
<p><img loading="lazy" src="img/teaser.png" alt=""  />
</p>
<br>
<h2 id="界面">界面</h2>
<p>上图为WordBias的可视化界面。 图片可以分为3部分：<br />
(A) 控制面板提供选择要投影到平行坐标图上的单词的选项<br />
(B) 主视图显示所选单词（蓝线）沿不同偏见类型（轴）的偏差分数<br />
(C) 搜索面板使用户能够搜索单词并显示搜索/刷新结果。 <br /></p>
<br>
<h2 id="案例1-极端主义">案例1-极端主义</h2>
<p><img loading="lazy" src="img/teaser.jpg" alt=""  />
</p>
<p>在上图A位置选择恐怖主义类别词<strong>Extremism</strong></p>
<p>在图中B位置，可以看到这些负面词在不同维度上存在不同的偏见。</p>
<ul>
<li>性别: 这类词主要倾向于男性</li>
<li>地区： 这类词倾向于伊斯兰地区</li>
<li>年龄: 这类词倾向于年轻人</li>
<li>经济:  这类词倾向于贫穷</li>
</ul>
<p>这表明 Word2vec 嵌入包含对穆斯林地区的贫困男性存在偏见。 <br /></p>
<br>
<h2 id="案例2-prettybeautifull">案例2-pretty/beautifull</h2>
<p>根据WordBias，描述女性美丽，可能不同的词使用范围不太一样。</p>
<p><img loading="lazy" src="img/pretty.png" alt=""  />
</p>
<p><img loading="lazy" src="img/beautiful.png" alt=""  />
</p>
<p>在年龄维度，pretty更适合描述小女生，而beautifull适合成熟女性。</p>
<p>岁月从不败美人，说的就是beautifull woman吧。</p>
<br>
<h2 id="论文">论文</h2>
<p>使用到wordbias，请注明出处</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@inproceedings{ghai2021wordbias,
  title={WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings},
  author={Ghai, Bhavya and Hoque, Md Naimul and Mueller, Klaus},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}
</code></pre></div><p>基于训练好的词嵌入模型，WordBias计算每个词与性别、宗教等不同社会分类（<strong>类别词典</strong>）的<strong>偏见分数</strong>（关联系数），研究者定义了多个类别，如子类别，</p>
<table>
<thead>
<tr>
<th>类</th>
<th>子类</th>
<th>词表</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Gender</strong></td>
<td>Male</td>
<td>he, son, his, him, father, man, boy, himself, male, brother, sons, fathers, men, boys, males, brothers, uncle, uncles, nephew, nephews</td>
</tr>
<tr>
<td><strong>Gender</strong></td>
<td>Femal</td>
<td>she, daughter, hers, her, mother, woman, girl, herself, female, sister, daughters, mothers, women, girls, sisters, aunt, aunts, niece, nieces</td>
</tr>
<tr>
<td><strong>Age</strong></td>
<td>Young</td>
<td>Taylor, Jamie, Daniel, Aubrey, Alison, Miranda, Jacob, Arthur, Aaron, Ethan</td>
</tr>
<tr>
<td><strong>Age</strong></td>
<td>Old</td>
<td>Ruth, William, Horace, Mary, Susie, Amy, John, Henry, Edward, Elizabeth</td>
</tr>
<tr>
<td><strong>Religion</strong></td>
<td>Islam</td>
<td>allah, ramadan, turban, emir, salaam, sunni, koran, imam, sultan, prophet, veil, ayatollah, shiite, mosque, islam, sheik, muslim, muhammad</td>
</tr>
<tr>
<td><strong>Religion</strong></td>
<td>Christainity</td>
<td>baptism, messiah, catholicism, resurrection, christianity, salvation, protestant, gospel, trinity, jesus, christ, christian, cross, catholic, church</td>
</tr>
<tr>
<td><strong>Race</strong></td>
<td>Black</td>
<td>black, blacks, Black, Blacks, African, african, Afro</td>
</tr>
<tr>
<td><strong>Race</strong></td>
<td>White</td>
<td>white, whites, White, Whites, Caucasian, caucasian, European, european, Anglo</td>
</tr>
<tr>
<td><strong>Economic</strong></td>
<td>Rich</td>
<td>rich, richer, richest, affluence, advantaged, wealthy, costly, exorbitant, expensive, exquisite, extravagant, flush, invaluable, lavish, luxuriant, luxurious, luxury, moneyed, opulent, plush, precious, priceless, privileged, prosperous, classy</td>
</tr>
<tr>
<td><strong>Economic</strong></td>
<td>Poor</td>
<td>poor, poorer, poorest, poverty, destitude, needy, impoverished, economical, inexpensive, ruined, cheap, penurious, underprivileged, penniless, valueless, penury, indigence, bankrupt, beggarly, moneyless, insolvent</td>
</tr>
</tbody>
</table>
<p>其中偏见分数使用了Relative Norm Difference算法。设向量g1、g2分别表示一个类别中的两个子群体(如黑人，g1黑女 g2黑男) ，给定一个词w， 分别计算w与g1、g2的距离。如果不等距，则表示存在刻板印象，距离差值越大，偏见得分(BiasScore)越深。<br></p>
<p>$$𝐵𝑖𝑎𝑠S𝑐𝑜𝑟𝑒(𝑤) = 𝑐𝑜𝑠𝑖𝑛𝑒D𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑤, 𝑔1) − 𝑐𝑜𝑠𝑖𝑛𝑒D𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑤, 𝑔2) $$</p>
<br>
<p>然后使用新颖的交互式界面将它们可视化。</p>
<p><img loading="lazy" src="img/teaser.jpg" alt=""  />
</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>结构模型|DSGE|Stata实证前沿|空间计量|Python数据挖掘2022五一工作坊</title>
      <link>https://textdata.cn/blog/2022-05-workshop/</link>
      <pubDate>Mon, 11 Apr 2022 03:43:10 +0600</pubDate>
      
      <guid>/blog/2022-05-workshop/</guid>
      <description>为推动我国经济、统计等社会科学量化研究方法学习与应用，培养和训练社会科学相关领域的青年学者、硕博士研究生，促进社会科学相关领域研究方法科学化规范化，“结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘”五一工作坊为广大学者提供了一个高水平学术交流、研究方法普及与研究经验分享的平台。工作坊采用模块式教学方法，不仅侧重经济、统计等社会科学量化基本方法的介绍，而且更加注重研究设计与研究选题训练，注重理论实践相结合，培养学员社会科学量化分析研究的综合能力。</description>
      <content:encoded><![CDATA[<p>大家好，五一劳动节假期我们将迎来了新的一期“结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘”工作坊，欢迎大家报名参加。我们将分七次为大家介绍本次工作坊的详细内容，敬请期待。</p>
<br>
<h2 id="关于我们">关于我们</h2>
<p>为推动我国经济、统计等社会科学量化研究方法学习与应用，培养和训练社会科学相关领域的青年学者、硕博士研究生，促进社会科学相关领域研究方法科学化规范化，“<strong>结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘</strong>”五一工作坊为广大学者提供了一个高水平学术交流、研究方法普及与研究经验分享的平台。工作坊采用模块式教学方法，不仅侧重经济、统计等社会科学量化基本方法的介绍，而且更加注重研究设计与研究选题训练，注重理论实践相结合，培养学员社会科学量化分析研究的综合能力。</p>
<p><strong>结构模型又称为结构计量模型</strong>，是将经济学模型和统计模型结合，用于估计描述现实的深层参数，模拟现实世界，<strong>以便合理地评估政策效果的实证工具</strong>。结构模型通过建立引起因果关系的数据生成具体方式（机制）的模型来解决简化型中的问题。模型中明确地指明了一些重要的外部因素（如政策）是如何影响通过某些参数来影响参与人决策的，那么通过改变这些外部因素并结合现有数据所估计出来的参数，结构模型便可以提供一系列反事实推断，<strong>对政策的制定有重要的意义</strong>。政策评估需要建立在理解对政策不变的“深层”参数之上。在结构式方法中，理论和实证的联系是紧密的。由于其建模技术的优雅和深刻，不仅是当今经济政策评估领域的前沿，也是发展经济理论的有力武器，<strong>在世界顶级期刊中，采用结构模型建模的文章引起广泛关注和引用，为所在学科的理论发展和政策评估带来深刻影响</strong>。</p>
<p>实证研究过程中学者普遍面临<strong>数据获取、清洗和编码</strong>的两大问题。在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用计量分析方法对数据进行分析。但大数据时代，网络数据成为亟待挖掘的潜在宝藏，大量商业信息、社会信息以<strong>文本等非结构化、异构型数据格式</strong>存储于海量网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两大问题，即：<strong>①从网络世界中高效地采集数据  ②从纷杂的文本数据抽取文本信息</strong>。</p>
<p>在获取数据及文本信息后，需要使用计量方法对数据进行分析处理。Stata、ArcGIS、Matlab等软件功能日益强大，理论也与时俱进。前沿分析固然可能会给你的Paper加分，但不理解其理论依据，会导致前沿方法的滥用, 使你的研究大为失色。</p>
<p><strong>DSGE</strong>，全称是dynamic stochastic general equilibrium，即<strong>动态随机一般均衡模型</strong>。是目前在宏观经济学研究占重要地位（甚至是主导地位）的模型方法，主要用于讨论<strong>经济增长、经济周期</strong>以及讨论<strong>政策工具效果</strong>（财政和货币政策）。我们需要对DSGE的深入学习。</p>
<p>为此，本次五一工作坊特别邀请七位走在理论实证、数据分析前沿的学者，为广大热爱经济学的学生、青年教师，讲解当下前沿模型的核心思想，基于Matlab、ArcGIS、Stata、Python等软件的实践操作。在这个知识与思想碰撞的时代，<strong>我们将与您分享最前沿的思想与实践技巧</strong>！为您带来最前沿计量经济理论与扎实操作并重的高质量课程。</p>
<br>
<h2 id="工作坊详情页">工作坊详情页</h2>
<p>由<strong>刘文革</strong>老师总筹划、<strong>谢杰</strong>老师协调发起工作坊，工作坊由7位老师分讲。</p>
<p><strong>点击下方链接</strong>，进入课程详情页，<strong>每门课程费用2000元(邓建鹏老师课程1000元)</strong>。</p>
<ul>
<li><a href="1_structural_model_1.html"><font color=blue>结构模型(一) -邹建文(中南财经政法大学)</font></a></li>
<li><a href="2_structural_model_2.html"><font color=blue>结构模型(二) -邓建鹏(上海财经大学)</font></a></li>
<li><a href="3-DSGE.html"><font color=blue>DSGE-王文甫(四川大学)</font></a></li>
<li><a href="4-Stata1.html"><font color=blue>Stata实证前沿(一)-王非(中国人民大学)</font></a></li>
<li><a href="5-Stata2.html"><font color=blue>Stata实证前沿(二)-司继春(上海对外经贸大学)</font></a></li>
<li><a href="6-Geo.html"><font color=blue>空间计量-李光勤(安徽财经大学)</font></a></li>
<li><a href="7-Python.html"><font color=blue>Python数据挖掘-邓旭东(哈尔滨工业大学)</font></a></li>
</ul>
<br>
<h2 id="授课方式">授课方式</h2>
<ul>
<li><strong>时间</strong>
<ul>
<li>2022年<strong>五一</strong>期间（<strong>具体时间待定</strong>）</li>
<li>每天6小时（8:30 — 11:30；14:00 — 17:00）+ <strong>30分钟答疑</strong>（部分课程晚间18:30-21:30进行）</li>
</ul>
</li>
<li>地点: 小鹅通平台（<strong>线上直播</strong>）</li>
<li><strong>每门课程2000元，视频保留10天</strong>；<strong>邓建鹏老师课程1000元</strong></li>
</ul>
<p><br><br></p>
<h2 id="报名信息">报名信息</h2>
<p>全国高等院校及研究机构从事经济科学研究的青年师生。尤其适合那些希望掌握高级实证方法，提升量化研究设计能力和国家课题申报能力的研究者。</p>
<h3 id="费用">费用</h3>
<ul>
<li><strong>每门课程2000元(每位老师讲授一门)</strong>；<strong>邓建鹏老师课程1000元</strong></li>
</ul>
<h3 id="优惠政策">优惠政策</h3>
<ul>
<li><strong>个人报名优惠</strong>：报名两位老师的课程9折；三位老师的课程8折；四位及以上老师的课程7.5折；老学员9折；学生优惠200元/人。</li>
<li><strong>团队报名优惠</strong>：三人成团及以上9折；五人成团及以上8折。</li>
</ul>
<h3 id="报名时间">报名时间</h3>
<p>从即日起</p>
<h3 id="报名咨询">报名咨询</h3>
<ul>
<li>17816181460（同微信）（汪老师）</li>
</ul>
<img src="img/wechat.png" style="zoom:40%;" />
<h3 id="缴费信息">缴费信息</h3>
<ul>
<li>单位：杭州国商智库信息技术服务有限公司</li>
<li>开户银行： 中国银行杭州大学城支行</li>
<li>银行账户：6232636200100260588</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</title>
      <link>https://textdata.cn/blog/2022-04-09-literature-about-embeddings/</link>
      <pubDate>Sat, 09 Apr 2022 22:43:10 +0600</pubDate>
      
      <guid>/blog/2022-04-09-literature-about-embeddings/</guid>
      <description>人类在留下语言、文字的过程中，也留下了自己的偏见、态度等主观认知信息（偏见、态度）。通过词向量距离的测算，就可以间接测得不同群体 对 某概念(组织、群体、品牌、地域等)的态度偏见。</description>
      <content:encoded><![CDATA[<h2 id="一词嵌入">一、词嵌入</h2>
<p>前几天刚刚分享了，</p>
<p><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">大数据时代下社会科学研究方法的拓展—基于词嵌入技术的文本分析的应用</a></p>
<p>人类在书信、网络论坛留下语言、文字的过程中，也留下了自己的偏见、态度等主观认知信息（偏见、态度）。</p>
<p>词嵌入做为一种词向量模型，可以从文本中计算出隐含的上下文情景信息，态度及偏见。通过词向量距离的测算，就可以间接测得不同群体对某概念(组织、群体、品牌、地域等)的态度偏见。感觉<strong>词嵌入</strong>技术用处很大，最近整理了下pnas、nature、science中的文献，<em>对了，相当部分的pnas关于词嵌入的论文经常会提供原始数据及代码。</em></p>
<p><strong>目前有些Python库可以使用词嵌入模型展示人类认知偏见， 如:</strong></p>
<ul>
<li><a href="https://github.com/koaning/whatlies/">whatlies</a></li>
<li><a href="https://github.com/uber-research/parallax">parallax</a></li>
<li><a href="https://github.com/bhavyaghai/WordBias">wordbias</a></li>
</ul>
<p><br><br></p>
<h2 id="二相关文献">二、相关文献</h2>
<ul>
<li>
<p>冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html</p>
</li>
<li>
<p>Kozlowski, A.C., Taddy, M. and Evans, J.A., 2019. The geometry of culture: Analyzing the meanings of class through word embeddings. American Sociological Review, 84(5), pp.905-949.</p>
</li>
<li>
<p>Toubia, O., Berger, J. and Eliashberg, J., 2021. How quantifying the shape of stories predicts their success. <em>Proceedings of the National Academy of Sciences</em>, <em>118</em>(26).</p>
</li>
<li>
<p>Caliskan A, Bryson JJ, Narayanan A. Semantics derived automatically from language corpora contain human-like biases. Science. 2017;356: 183–186.</p>
</li>
<li>
<p>Garg N, Schiebinger L, Jurafsky D, Zou J. Word embeddings quantify 100 years of gender and ethnic stereotypes . Proceedings of the National Academy of Sciences. 2018. pp. E3635–E3644. doi:10.1073/pnas.1720347115</p>
</li>
<li>
<p>Garg, N., Schiebinger, L., Jurafsky, D. and Zou, J., 2018. Word embeddings quantify 100 years of gender and ethnic stereotypes. <em>Proceedings of the National Academy of Sciences</em>, <em>115</em>(16), pp.E3635-E3644.</p>
</li>
<li>
<p>Peng, H., Ke, Q., Budak, C., Romero, D.M. and Ahn, Y.Y., 2021. Neural embeddings of scholarly periodicals reveal complex disciplinary organizations. <em><strong>Science Advances</strong></em>, <em>7</em>(17), p.eabb9004.</p>
</li>
<li>
<p>Waller, I. and Anderson, A., 2021. Quantifying social organization and political polarization in online platforms. <em>Nature</em>, <em>600</em>(7888), pp.264-268.</p>
</li>
<li>
<p>Arseniev-Koehler, A., Cochran, S.D., Mays, V.M., Chang, K.W. and Foster, J.G., 2022. Integrating topic modeling and word embedding to characterize violent deaths. <em>Proceedings of the National Academy of Sciences</em>, <em>119</em>(10), p.e2108801119.</p>
</li>
<li>
<p>Bollen, J., Ten Thij, M., Breithaupt, F., Barron, A.T., Rutter, L.A., Lorenzo-Luaces, L. and Scheffer, M., 2021. Historical language records reveal a surge of cognitive distortions in recent decades. <em>Proceedings of the National Academy of Sciences</em>, <em>118</em>(30).</p>
</li>
<li>
<p>Kim, L., Smith, D.S., Hofstra, B. and McFarland, D.A., 2022. Gendered knowledge in fields and academic careers. <em>Research Policy</em>, <em>51</em>(1), p.104411.</p>
</li>
<li>
<p>Lawson, M.A., Martin, A.E., Huda, I. and Matz, S.C., 2022. Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language. <em>Proceedings of the National Academy of Sciences</em>, <em>119</em>(9), p.e2026443119.</p>
</li>
<li>
<p>Brady, W.J., McLoughlin, K., Doan, T.N. and Crockett, M.J., 2021. How social learning amplifies moral outrage expression in online social networks. <em>Science Advances</em>, <em>7</em>(33), p.eabe5641.</p>
</li>
<li>
<p>Bailey, A.H., Williams, A. and Cimpian, A., 2022. Based on billions of words on the internet, people= men. <em>Science Advances</em>, <em>8</em>(13), p.eabm2463.</p>
</li>
<li>
<p>Lewis, M. and Lupyan, G., 2020. Gender stereotypes are reflected in the distributional structure of 25 languages. <em>Nature human behaviour</em>, <em>4</em>(10), pp.1021-1028.</p>
</li>
<li>
<p>Schramowski, P., Turan, C., Andersen, N., Rothkopf, C.A. and Kersting, K., 2022. Large pre-trained language models contain human-like biases of what is right and wrong to do. <em>Nature Machine Intelligence</em>, <em>4</em>(3), pp.258-268.</p>
</li>
<li>
<p>Costa-jussà, M.R., 2019. An analysis of gender bias studies in natural language processing. <em>Nature Machine Intelligence</em>, <em>1</em>(11), pp.495-496.</p>
</li>
<li>
<p>Rodman, E., 2020. A timely intervention: Tracking the changing meanings of political concepts with word vectors. <em><strong>Political Analysis</strong></em>, <em>28</em>(1), pp.87-111.</p>
</li>
<li>
<p>Bhatia, S., 2017. Associative judgment and vector space semantics. <em><strong>Psychological review</strong></em>, <em>124</em>(1), p.1.</p>
</li>
<li>
<p>Kurdi, B., Mann, T.C., Charlesworth, T.E. and Banaji, M.R., 2019. The relationship between implicit intergroup attitudes and beliefs. <em>Proceedings of the National Academy of Sciences</em>, <em>116</em>(13), pp.5862-5871.</p>
</li>
<li>
<p>Charlesworth, T.E., Yang, V., Mann, T.C., Kurdi, B. and Banaji, M.R., 2021. Gender stereotypes in natural language: Word embeddings show robust consistency across child and adult language corpora of more than 65 million words. <em><strong>Psychological Science</strong></em>, <em>32</em>(2), pp.218-240.</p>
</li>
<li>
<p>Bhatia, S., 2019. Predicting risk perception: New insights from data science. <em><strong>Management Science</strong></em>, <em>65</em>(8), pp.3800-3823.</p>
</li>
<li>
<p>Rheault, L. and Cochrane, C., 2020. Word embeddings for the analysis of ideological placement in parliamentary corpora. <em>Political Analysis</em>, <em>28</em>(1), pp.112-133.</p>
</li>
<li>
<p>Yang, K., Lau, R.Y. and Abbasi, A., 2022. Getting Personal: A Deep Learning Artifact for Text-Based Measurement of Personality. <em><strong>Information Systems Research</strong></em>.</p>
</li>
<li>
<p>Rodman, E., 2020. A timely intervention: Tracking the changing meanings of political concepts with word vectors. <em>Political Analysis</em>, <em>28</em>(1), pp.87-111.</p>
</li>
<li>
<p>Margulis, E.H., Wong, P.C., Turnbull, C., Kubit, B.M. and McAuley, J.D., 2022. Narratives imagined in response to instrumental music reveal culture-bounded intersubjectivity. <em>Proceedings of the National Academy of Sciences</em>, <em>119</em>(4).</p>
</li>
<li>
<p>Thompson, B., Roberts, S.G. and Lupyan, G., 2020. Cultural influences on word meanings revealed through large-scale semantic alignment. <em>Nature Human Behaviour</em>, <em>4</em>(10), pp.1029-1038.</p>
</li>
</ul>
<p><br><br></p>
<h2 id="三相关代码">三、相关代码</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>营销研究中的图像分析</title>
      <link>https://textdata.cn/blog/image_analytics_in_marketing_code_examples_book_chapter/</link>
      <pubDate>Tue, 05 Apr 2022 22:43:10 +0600</pubDate>
      
      <guid>/blog/image_analytics_in_marketing_code_examples_book_chapter/</guid>
      <description>营销研究中的图像分析</description>
      <content:encoded><![CDATA[<h2 id="heading"></h2>
<p>这个笔记本是教程中的部分方法</p>
<blockquote>
<p>Dzyabura, El Kihal and Peres (2020), &ldquo;Image Analytics in Marketing&rdquo;, in <em>The Handbook of Market Research</em>, Ch 14, Editors: Christian Homburg, Martin Klarmann, Arnd Vomberg. Springer, 2021.</p>
</blockquote>
<br>
<h2 id="安装">安装</h2>
<p>代码是用 Python 编写的。 用于编程和运行代码的理想界面是通过 Anaconda 的 Jupyter Notebook。</p>
<p>除了标准 Anaconda 库之外，还需要安装几个额外的库。 我们在代码中标记它们。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">opencv</span><span class="o">-</span><span class="n">contrib</span><span class="o">-</span><span class="n">python</span><span class="o">==</span><span class="mf">4.5.5.64</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">opencv</span><span class="o">-</span><span class="n">python</span><span class="o">==</span><span class="mf">4.5.5.62</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">scikit</span><span class="o">-</span><span class="n">image</span><span class="o">==</span><span class="mf">0.18.3</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">cv2</span> <span class="c1"># needs to be installed separately</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">skimage.io</span> <span class="kn">import</span> <span class="n">imread</span><span class="p">,</span> <span class="n">imshow</span>
<span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="nn">mpimg</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div><br>
<h2 id="预定义特征提取">预定义特征提取</h2>
<p>我们从可以从图像中提取基本特征,如亮度、颜色等。</p>
<p>首先，我们加载一张图片并显示它。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">img</span> <span class="o">=</span> <span class="n">mpimg</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;azreali.jpg&#39;</span><span class="p">)</span>
<span class="n">imgplot</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><div style="text-align: center;">
<figure >
    
        <img src="img/output_5_0.png" width="100%" />
    
    
</figure>
</div>
<br>
<h2 id="颜色提取">颜色提取</h2>
<h3 id="灰度直方图">灰度直方图</h3>
<p>颜色是基本的图像特征。 以下是创建颜色直方图的一些示例，这些直方图捕获图像的颜色组成。</p>
<p>将彩色招照片转为灰度照片</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">img_gray</span> <span class="o">=</span> <span class="n">imread</span><span class="p">(</span><span class="s1">&#39;azreali.jpg&#39;</span><span class="p">,</span><span class="n">as_gray</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">imshow</span><span class="p">(</span><span class="n">img_gray</span><span class="p">)</span>
</code></pre></div><pre><code>&lt;matplotlib.image.AxesImage at 0x7fd5ffe7c910&gt;
</code></pre>
<p>​    <div style="text-align: center;">
<figure >
    
        <img src="img/output_7_1.png" width="100%" />
    
    
</figure>
</div></p>
<p>​</p>
<p>我们可以为灰度图像创建一个颜色直方图，计算每个像素的强度，范围在 0 为黑色和 256 为白色之间。</p>
<p>你能猜出直方图的样子吗？</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;azreali.jpg&#39;</span><span class="p">)</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">hist</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">gray</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="p">[</span><span class="mi">256</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Grayscale Histogram&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Bins&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;# of Pixels&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">])</span>
</code></pre></div><pre><code>(0.0, 256.0)
</code></pre>
<p>​ <br>
<div style="text-align: center;">
<figure >
    
        <img src="img/output_9_1.png" width="100%" />
    
    
</figure>
</div> <br>
​</p>
<h3 id="rgb直方图">RGB直方图</h3>
<p>实际上，直方图向右倾斜。 图片主要包含刻度亮侧的像素和相对较少的暗像素。</p>
<p>接下来，我们可以为 RGB 颜色空间创建颜色直方图，0 表示颜色的最低强度，256 表示最高强度。</p>
<p>你能猜出三座塔图片中的主要颜色是什么吗？</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">color</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
    <span class="n">histr</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">calcHist</span><span class="p">([</span><span class="n">image</span><span class="p">],[</span><span class="n">i</span><span class="p">],</span><span class="kc">None</span><span class="p">,[</span><span class="mi">256</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">histr</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="n">col</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">256</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><div style="text-align: center;">
<figure >
    
        <img src="img/output_11_0.png" width="100%" />
    
    
</figure>
</div>
<p>确实。 图片中有很多蓝色，直方图检测到这些蓝色像素。</p>
<p>今天讲的都是很简单的图片处理，图片是一种比文本体积更大的文件数据，受限制于个人技术水平以及电脑性能，大邓无法展示机器学习、深度学习的图片分析算法。感兴趣的同学可以阅读论文，了解图片分析在营销中的新应用新进展。</p>
<blockquote>
<p>Dzyabura, El Kihal and Peres (2020), &ldquo;Image Analytics in Marketing&rdquo;, in The Handbook of Market Research, Ch 14, Editors: Christian Homburg, Martin Klarmann, Arnd Vomberg. Springer, 2021.</p>
</blockquote>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 |  Python文本分析包更新</title>
      <link>https://textdata.cn/blog/cntext_simplification/</link>
      <pubDate>Fri, 01 Apr 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_simplification/</guid>
      <description>扩展词典、情感分析、可阅读性，内置9种情感词典，涵盖中英文</description>
      <content:encoded><![CDATA[<p><a href="https://github.com/hidadeng/cntext"><img loading="lazy" src="https://img.shields.io/badge/cntext-%e4%b8%ad%e6%96%87%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%ba%93-orange?style=for-the-badge&amp;logo=appveyor" alt=""  />
</a></p>
<p><a href="version1.2.md">旧版cntext入口</a></p>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> <strong>stats</strong>  文本统计指标
<ul>
<li><input checked="" disabled="" type="checkbox"> 词频统计</li>
<li><input checked="" disabled="" type="checkbox"> 可读性</li>
<li><input checked="" disabled="" type="checkbox"> 内置pkl词典</li>
<li><input checked="" disabled="" type="checkbox"> <strong>情感分析</strong></li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>dictionary</strong> 构建词表(典)
<ul>
<li><input checked="" disabled="" type="checkbox"> Sopmi 互信息扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> W2Vmodels 词向量扩充词典法</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>similarity</strong>   文本相似度
<ul>
<li><input checked="" disabled="" type="checkbox"> cos相似度</li>
<li><input checked="" disabled="" type="checkbox"> jaccard相似度</li>
<li><input checked="" disabled="" type="checkbox"> 编辑距离相似度</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>mind.py</strong> 计算文本中的认知方向（态度、偏见）</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext
</code></pre></div><br>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">mind</span>
    <span class="nx">dictionary</span>
    <span class="nx">similarity</span>
    <span class="nx">stats</span>
</code></pre></div><br>
<h2 id="一stats">一、stats</h2>
<p>目前stats内置的函数有</p>
<ul>
<li><strong>readability</strong>  文本可读性</li>
<li><strong>term_freq</strong> 词频统计函数</li>
<li><strong>dict_pkl_list</strong>  获取cntext内置词典列表(pkl格式)</li>
<li><strong>load_pkl_dict</strong> 导入pkl词典文件</li>
<li><strong>sentiment</strong> 情感分析</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="11--readability">1.1  readability</h3>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p>readability(text, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text: 文本字符串数据</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p>**中文可读性 ** 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>


<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 28.0,
 &#39;readability2&#39;: 0.15789473684210525,
 &#39;readability3&#39;: 14.078947368421053}
</code></pre></div><br>
<p>句子中的符号变更会影响结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.16666666666666666,
 &#39;readability3&#39;: 13.583333333333334}
</code></pre></div><p><br><br></p>
<h3 id="12--term_freq">1.2  term_freq</h3>
<p>词频统计函数，返回Counter类型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="13-dict_pkl_list">1.3 dict_pkl_list</h3>
<p>获取cntext内置词典列表(pkl格式)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取cntext内置词典列表(pkl格式)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;]
</code></pre></div><p>词典对应关系, 部分情感词典资料整理自 <a href="https://github.com/quanteda/quanteda.sentiment">quanteda.sentiment</a></p>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>DUTIR.pkl</td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td>HOWNET.pkl</td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>sentiws.pkl</td>
<td>SentimentWortschatz (SentiWS)</td>
<td>英文</td>
<td>正面词、负面词；<br>效价</td>
</tr>
<tr>
<td>ChineseFinancialFormalUnformalSentiment.pkl</td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td>ANEW.pkl</td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>词语效价信息</td>
</tr>
<tr>
<td>LSD2015.pkl</td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>NRC.pkl</td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td>geninqposneg.pkl</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HuLiu.pkl</td>
<td>Hu&amp;Liu (2004)正、负情感词典</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>AFINN.pkl</td>
<td>尼尔森 (2011) 的“新 ANEW”效价词表</td>
<td>英文</td>
<td>情感效价信息valence</td>
</tr>
<tr>
<td>LoughranMcDonald.pkl</td>
<td>会计金融LM词典</td>
<td>英文</td>
<td>金融领域正、负面情感词</td>
</tr>
<tr>
<td>ADV_CONJ.pkl</td>
<td>副词连词</td>
<td>中文</td>
<td></td>
</tr>
<tr>
<td>STOPWORDS.pkl</td>
<td></td>
<td>中、英</td>
<td>停用词</td>
</tr>
</tbody>
</table>
<h3 id="注意">注意:</h3>
<ul>
<li>
<p>如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</li>
<li>
<p>如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue</p>
</li>
</ul>
<br>
<h3 id="14-load_pkl_dict">1.4 load_pkl_dict</h3>
<p>导入pkl词典文件，返回字典样式数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入pkl词典文件,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, ...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...], 
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;, &#39;刿目怵心&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;, &#39;喜糖&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,]
           }
</code></pre></div><br>
<h3 id="15-sentiment">1.5 sentiment</h3>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)
使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>diction:  情感词字典；</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">],</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p>如果不适用pkl词典，可以自定义自己的词典，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p><br><br></p>
<h2 id="二dictionary">二、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="21-sopmi-共现法">2.1 SoPmi 共现法</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                   <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                   <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                   <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 44.49 s
</code></pre></div><br>
<h3 id="22-w2vmodels-词向量">2.2 W2VModels 词向量</h3>
<p><strong>特别要注意代码需要设定lang语言参数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   174 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s

</code></pre></div><br>
<h3 id="需要注意">需要注意</h3>
<p>训练出的w2v模型可以后续中使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">model路径</span><span class="p">)</span>
<span class="c1">#找出word的词向量</span>
<span class="c1">#w2v_model.get_vector(word)</span>
<span class="c1">#更多w2_model方法查看</span>
<span class="c1">#help(w2_model)</span>
</code></pre></div><p>例如本代码，运行生成的结果路径<code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;technology&#39;, 0.689210832118988),
 (&#39;infrastructure&#39;, 0.669672966003418),
 (&#39;resources&#39;, 0.6695448160171509),
 (&#39;talent&#39;, 0.6627111434936523),
 (&#39;execution&#39;, 0.6549549102783203),
 (&#39;marketing&#39;, 0.6533523797988892),
 (&#39;merchandising&#39;, 0.6504817008972168),
 (&#39;diversification&#39;, 0.6479553580284119),
 (&#39;expertise&#39;, 0.6446896195411682),
 (&#39;digital&#39;, 0.6326863765716553)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.45616838, -0.7799563 ,  0.56367606, -0.8570078 ,  0.600359  ,
       -0.6588043 ,  0.31116748, -0.11956959, -0.47599426,  0.21840936,
       -0.02268819,  0.1832016 ,  0.24452794,  0.01084935, -1.4213187 ,
        0.22840202,  0.46387577,  1.198386  , -0.621511  , -0.51598716,
        0.13352732,  0.04140598, -0.23470387,  0.6402956 ,  0.20394802,
        0.10799981,  0.24908689, -1.0117126 , -2.3168423 , -0.0402851 ,
        1.6886286 ,  0.5357047 ,  0.22932841, -0.6094084 ,  0.4515793 ,
       -0.5900931 ,  1.8684244 , -0.21056202,  0.29313338, -0.221067  ,
       -0.9535679 ,  0.07325   , -0.15823542,  1.1477109 ,  0.6716076 ,
       -1.0096023 ,  0.10605699,  1.4148282 ,  0.24576302,  0.5740349 ,
        0.19984631,  0.53964925,  0.41962907,  0.41497853, -1.0322098 ,
        0.01090925,  0.54345983,  0.806317  ,  0.31737605, -0.7965337 ,
        0.9282971 , -0.8775608 , -0.26852605, -0.06743863,  0.42815775,
       -0.11774074, -0.17956367,  0.88813037, -0.46279573, -1.0841943 ,
       -0.06798118,  0.4493006 ,  0.71962464, -0.02876493,  1.0282255 ,
       -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922,
       -0.16753489,  0.14065655, -1.8609751 ,  0.03587054,  1.2792674 ,
        1.2732009 , -0.74120265, -0.98000383,  0.4521185 , -0.26387128,
        0.37045383,  0.3680011 ,  0.7197629 , -0.3570571 ,  0.8016917 ,
        0.39243212, -0.5027844 , -1.2106236 ,  0.6412354 , -0.878307  ],
      dtype=float32)
</code></pre></div><p><br><br></p>
<h2 id="23-co_occurrence_matrix">2.3 co_occurrence_matrix</h2>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三similarity">三、similarity</h2>
<p>四种相似度计算函数</p>
<ul>
<li>cosine_sim(text1, text2)  cos余弦相似</li>
<li>jaccard_sim(text1, text2)     jaccard相似</li>
<li>minedit_sim(text1, text2)  最小编辑距离相似度；</li>
<li>simple_sim(text1, text2) 更改变动算法</li>
</ul>
<p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩啊&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.82
0.67
2.00
0.87
</code></pre></div><p><br><br></p>
<h2 id="四text2mind">四、Text2Mind</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<p>- <strong>多个对象在某概念的远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
<p>- 多个对象在某<strong>概念的分量(fen，一声)的多少</strong>， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
<p>这两种认知分别可以用向量距离、向量语义投影计算得来。</p>
<ul>
<li>tm.sematic_distance(words, c_words1, c_words2)  向量距离</li>
<li>tm.sematic_projection(words, c_words1, c_words2)  向量语义投影</li>
</ul>
<h3 id="41-tmsematic_distancewords-c_words1-c_words2">4.1 tm.sematic_distance(words, c_words1, c_words2)</h3>
<p>分别计算words与c_words1、c_words2语义距离，返回距离差值。</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果d1-d2&lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<p><strong>下载glove_w2v.6B.100d.txt</strong>链接: <a href="https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw">https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw</a> 提取码: 72l0</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#Note: this is a word2vec format model</span>
<span class="n">tm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Text2Mind</span><span class="p">(</span><span class="n">w2v_model_path</span><span class="o">=</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="n">engineer</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">mans</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">womans</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">mans</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">womans</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><br>
<h3 id="42-tmsematic_projectionwords-c_words1-c_words2">4.2 tm.sematic_projection(words, c_words1, c_words2)</h3>
<p><strong>语义投影</strong>，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，word越位于c_words2一侧。</p>
<p>下图是语义投影示例图，本文算法和图片均来自 &ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&rdquo;</p>
<p><img loading="lazy" src="img/Nature_Semantic_projection_recovering_human_knowledge_of.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">smalls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">bigs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">smalls</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">bigs</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本数据: 展开、过滤和分块</title>
      <link>https://textdata.cn/blog/text_features_tutorial/</link>
      <pubDate>Tue, 28 Dec 2021 10:43:10 +0600</pubDate>
      
      <guid>/blog/text_features_tutorial/</guid>
      <description>TF-IDF刻画参与者信息的“新且熟悉” ,构建参与者文化网络</description>
      <content:encoded><![CDATA[<p>前段时间发现apachecn在github上翻译了一本和特征工程相关的书籍：《Feature Engineering for Machine Learning》，中文名为《面向机器学习的特征工程》。</p>
<p><a href="Feature_Engineering_for_Machine_Learning.pdf">Feature_Engineering_for_Machine_Learning.pdf</a></p>
<h2 id="三文本数据-展开过滤和分块httpfe4mlapachecnorgdocs3文本数据id三文本数据-展开过滤和分块"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E4%B8%89%E3%80%81%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE-%E5%B1%95%E5%BC%80%E3%80%81%E8%BF%87%E6%BB%A4%E5%92%8C%E5%88%86%E5%9D%97">三、文本数据: 展开、过滤和分块</a></h2>
<blockquote>
<p>译者：<a href="https://github.com/kkejili">@kkejili</a></p>
<p>校对者：<a href="https://github.com/KyrieHee">@HeYun</a></p>
</blockquote>
<p>如果让你来设计一个算法来分析以下段落，你会怎么做？</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Emma knocked on the door. No answer. She knocked again and waited. There was a large maple tree next to the house. Emma looked up the tree and saw a giant raven perched at the treetop. Under the afternoon sun, the raven gleamed magnificently. Its beak was hard and pointed, its claws sharp and strong. It looked regal and imposing. It reigned the tree it stood on. The raven was looking straight at Emma with its beady black eyes. Emma felt slightly intimidated. She took a step back from the door and tentatively said, “hello?” 复制ErrorOK!
</code></pre></div><p>该段包含很多信息。我们知道它谈到了到一个名叫Emma的人和一只乌鸦。这里有一座房子和一棵树，艾玛正想进屋，却看到了乌鸦。这只华丽的乌鸦注意到艾玛，她有点害怕，但正在尝试交流。</p>
<p>那么，这些信息的哪些部分是我们应该提取的显着特征？首先，提取主要角色艾玛和乌鸦的名字似乎是个好主意。接下来，注意房子，门和树的布置可能也很好。关于乌鸦的描述呢？Emma的行为呢，敲门，退后一步，打招呼呢？</p>
<p>本章介绍文本特征工程的基础知识。我们从词袋（bags of words）开始，这是基于字数统计的最简单的文本功能。一个非常相关的变换是 tf-idf，它本质上是一种特征缩放技术。它将被我在（下一篇）章节进行全面讨论。本章首先讨论文本特征提取，然后讨论如何过滤和清洗这些特征。</p>
<br>
<h2 id="bag-of-x把自然文本变成平面向量httpfe4mlapachecnorgdocs3文本数据idbag-of-x把自然文本变成平面向量"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=bag-of-x%EF%BC%9A%E6%8A%8A%E8%87%AA%E7%84%B6%E6%96%87%E6%9C%AC%E5%8F%98%E6%88%90%E5%B9%B3%E9%9D%A2%E5%90%91%E9%87%8F">Bag of X：把自然文本变成平面向量</a></h2>
<p>无论是构建机器学习模型还是特征工程，其结果应该是通俗易懂的。简单的事情很容易尝试，可解释的特征和模型相比于复杂的更易于调试。简单和可解释的功能并不总是会得到最精确的模型。但从简单开始就是一个好主意，仅在绝对必要时我们可以增加其复杂性。</p>
<p>对于文本数据，我们可以从称为 BOW 的字数统计开始。字数统计表中并没有特别费力来寻找<code>&quot;Emma&quot;</code>或乌鸦这样有趣的实体。但是这两个词在该段落中被重复提到，并且它们在这里的计数比诸如<code>&quot;hello&quot;</code>之类的随机词更高。对于此类简单的文档分类任务，字数统计通常比较适用。它也可用于信息检索，其目标是检索与输入文本相关的文档集。这两个任务都很好解释词级特征，因为某些特定词的存在可能是本文档主题内容的重要指标。</p>
<br>
<h2 id="词袋httpfe4mlapachecnorgdocs3文本数据id词袋"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E8%AF%8D%E8%A2%8B">词袋</a></h2>
<p>在词袋特征中，文本文档被转换成向量。（向量只是 n 个数字的集合。）向量包含词汇表中每个单词可能出现的数目。 如果单词<code>&quot;aardvark&quot;</code>在文档中出现三次，则该特征向量在与该单词对应的位置上的计数为 3。 如果词汇表中的单词没有出现在文档中，则计数为零。 例如，“这是一只小狗，它是非常可爱”的句子具有如图所示的 BOW 表示</p>

<figure >
    
        <img src="img/3-1.png" width="100%" />
    
    
</figure>

<p>图 3-1 转换词成向量描述图</p>
<p>BOW 将文本文档转换为平面向量。 它是“平面的”，因为它不包含任何原始的文本结构。 原文是一系列词语。但是词袋向量并没有序列；它只是记得每个单词在文本中出现多少次。 它不代表任何词层次结构的概念。 例如，“动物”的概念包括“狗”，“猫”，“乌鸦”等。但是在一个词袋表示中，这些词都是矢量的相同元素。</p>

<figure >
    
        <img src="img/3-2.png" width="100%" />
    
    
</figure>

<p>图 3-2 两个等效的词向量，向量中单词的排序不重要，只要它在数据集中的个数和文档中出现数量是一致的。</p>
<p>重要的是特征空间中数据的几何形状。 在一个词袋矢量中，每个单词成为矢量的一个维度。如果词汇表中有 n 个单词，则文档将成为n维空间中的一个点。 很难想象二维或三维以外的任何物体的几何形状，所以我们必须使用我们的想象力。 图3-3显示了我们的例句在对应于“小狗”和“可爱”两个维度的特征空间中的样子。</p>

<figure >
    
        <img src="img/3-3.png" width="100%" />
    
    
</figure>

<p>图 3-3 特征空间中文本文档的图示</p>

<figure >
    
        <img src="img/3-4.png" width="100%" />
    
    
</figure>

<p>图 3-4 三维特征空间</p>
<p>图 3-3 和图 3-4 描绘了特征空间中的数据向量。 坐标轴表示单个单词，它们是词袋表示下的特征，空间中的点表示数据点（文本文档）。 有时在数据空间中查看特征向量也是有益的。 特征向量包含每个数据点中特征的值。 轴表示单个数据点和点表示特征向量。 图 3-5 展示了一个例子。 通过对文本文档进行词袋特征化，一个特征是一个词，一个特征向量包含每个文档中这个词的计数。 这样，一个单词被表示为一个“一个词向量”。正如我们将在第 4 章中看到的那样，这些文档词向量来自词袋向量的转置矩阵。</p>

<figure >
    
        <img src="img/3-5.png" width="100%" />
    
    
</figure>

<br>
<h2 id="bag-of-n-gramhttpfe4mlapachecnorgdocs3文本数据idbag-of-n-gram"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=bag-of-n-gram">Bag-of-N-gram</a></h2>
<p>Bag-of-N-gram 或者 bag-of-ngram 是 BOW 的自然延伸。 n-gram 是 n 个有序的记号（token）。一个词基本上是一个 1-gram，也被称为一元模型。当它被标记后，计数机制可以将单个词进行计数，或将重叠序列计数为 n-gram。例如，<code>&quot;Emma knocked on the door&quot;</code>这句话会产生 n-gram，如<code>&quot;Emma knocked&quot;</code>，<code>&quot;knocked on&quot;</code>，<code>&quot;on the&quot;</code>，<code>&quot;the door&quot;</code>。 N-gram 保留了文本的更多原始序列结构，故 bag-of-ngram可以提供更多信息。但是，这是有代价的。理论上，用 k 个独特的词，可能有 k 个独立的 2-gram（也称为 bigram）。在实践中，并不是那么多，因为不是每个单词后都可以跟一个单词。尽管如此，通常有更多不同的 n-gram（n &gt; 1）比单词更多。这意味着词袋会更大并且有稀疏的特征空间。这也意味着 n-gram 计算，存储和建模的成本会变高。n 越大，信息越丰富，成本越高。</p>
<p>为了说明随着 n 增加 n-gram 的数量如何增加，我们来计算纽约时报文章数据集上的 n-gram。我们使用 Pandas 和 scikit-learn 中的<code>CountVectorizer</code>转换器来计算前 10,000 条评论的 n-gram。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">pandas</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">json</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span> 
<span class="c1"># Load the first 10,000 reviews </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json&#39;</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">js</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span> 
<span class="o">...</span> <span class="n">js</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()))</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">review_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">js</span><span class="p">)</span> 
<span class="c1"># Create feature transformers for unigram, bigram, and trigram. </span>
<span class="c1"># The default ignores single-character words, which is useful in practice because it trims </span>
<span class="c1"># uninformative words. But we explicitly include them in this example for illustration purposes. </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">bow_converter</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">token_pattern</span><span class="o">=</span><span class="s1">&#39;(?u)</span><span class="se">\\</span><span class="s1">b</span><span class="se">\\</span><span class="s1">w+</span><span class="se">\\</span><span class="s1">b&#39;</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">bigram_converter</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">token_pattern</span><span class="o">=</span><span class="s1">&#39;(?u)</span><span class="se">\\</span><span class="s1">b</span><span class="se">\\</span><span class="s1">w+</span><span class="se">\\</span><span class="s1">b&#39;</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">trigram_converter</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">token_pattern</span><span class="o">=</span><span class="s1">&#39;(?u)</span><span class="se">\\</span><span class="s1">b</span><span class="se">\\</span><span class="s1">w+</span><span class="se">\\</span><span class="s1">b&#39;</span><span class="p">)</span> 
<span class="c1"># Fit the transformers and look at vocabulary size </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">bow_converter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">review_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="n">bow_converter</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">bigram_converter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">review_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">bigram</span> <span class="o">=</span> <span class="n">bigram_converter</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">trigram_converter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">review_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">trigram</span> <span class="o">=</span> <span class="n">trigram_converter</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">bigram</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">trigram</span><span class="p">))</span> 
<span class="mi">26047</span> <span class="mi">346301</span> <span class="mi">847545</span> 
<span class="c1"># Sneak a peek at the ngram themselves</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">words</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span> 
<span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;00&#39;</span><span class="p">,</span> <span class="s1">&#39;000&#39;</span><span class="p">,</span> <span class="s1">&#39;0002&#39;</span><span class="p">,</span> <span class="s1">&#39;00am&#39;</span><span class="p">,</span> <span class="s1">&#39;00ish&#39;</span><span class="p">,</span> <span class="s1">&#39;00pm&#39;</span><span class="p">,</span> <span class="s1">&#39;01&#39;</span><span class="p">,</span> <span class="s1">&#39;01am&#39;</span><span class="p">,</span> <span class="s1">&#39;02&#39;</span><span class="p">]</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">bigram</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span> 
<span class="p">[</span><span class="s1">&#39;zucchinis at&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zucchinis took&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zucchinis we&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zuma over&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zuppa di&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zuppa toscana&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zuppe di&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zurich and&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zz top&#39;</span><span class="p">,</span> 
<span class="s1">&#39;à la&#39;</span><span class="p">]</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">trigram</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span> 
<span class="p">[</span><span class="s1">&#39;0 10 definitely&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 2 also&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 25 per&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 3 miles&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 30 a&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 30 everything&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 30 lb&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 35 tip&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 5 curry&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 5 pork&#39;</span><span class="p">]</span> <span class="n">复制ErrorOK</span><span class="err">!</span>
</code></pre></div>
<figure >
    
        <img src="img/3-6.png" width="100%" />
    
    
</figure>

<p>图3-6 Number of unique n-gram in the first 10,000 reviews of the Yelp dataset</p>
<br>
<h3 id="过滤清洗特征httpfe4mlapachecnorgdocs3文本数据id过滤清洗特征"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E8%BF%87%E6%BB%A4%E6%B8%85%E6%B4%97%E7%89%B9%E5%BE%81">过滤清洗特征</a></h3>
<p>我们如何清晰地将信号从噪声中分离出来？ 通过过滤，使用原始标记化和计数来生成简单词表或 n-gram 列表的技术变得更加可用。 短语检测，我们将在下面讨论，可以看作是一个特别的 bigram 过滤器。 以下是执行过滤的几种方法。</p>
<br>
<h3 id="停用词httpfe4mlapachecnorgdocs3文本数据id停用词"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%81%9C%E7%94%A8%E8%AF%8D">停用词</a></h3>
<p>分类和检索通常不需要对文本有深入的理解。 例如，在<code>&quot;Emma knocked on the door&quot;</code>一句中，<code>&quot;on&quot;</code>和<code>&quot;the&quot;</code>这两个词没有包含很多信息。 代词、冠词和介词大部分时间并没有显示出其价值。流行的 Python NLP 软件包 NLTK 包含许多语言的语言学家定义的停用词列表。 （您将需要安装 NLTK 并运行<code>nltk.download()</code>来获取所有的好东西。）各种停用词列表也可以在网上找到。 例如，这里有一些来自英语停用词的示例词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Sample words from the nltk stopword list
a, about, above, am, an, been, didn’t, couldn’t, i’d, i’ll, itself, let’s, myself, our, they, through, when’s, whom, ... 复制ErrorOK!
</code></pre></div><p>请注意，该列表包含撇号，并且这些单词没有大写。 为了按原样使用它，标记化过程不得去掉撇号，并且这些词需要转换为小写。</p>
<br>
<h3 id="基于频率的过滤httpfe4mlapachecnorgdocs3文本数据id基于频率的过滤"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%9F%BA%E4%BA%8E%E9%A2%91%E7%8E%87%E7%9A%84%E8%BF%87%E6%BB%A4">基于频率的过滤</a></h3>
<p>停用词表是一种去除空洞特征常用词的方法。还有其他更统计的方法来理解“常用词”的概念。在搭配提取中，我们看到依赖于手动定义的方法，以及使用统计的方法。同样的想法也适用于文字过滤。我们也可以使用频率统计。</p>
<br>
<h3 id="高频词httpfe4mlapachecnorgdocs3文本数据id高频词"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E9%AB%98%E9%A2%91%E8%AF%8D">高频词</a></h3>
<p>频率统计对滤除语料库专用常用词以及通用停用词很有用。例如，纽约时报文章数据集中经常出现“纽约时报”和其中单个单词。“议院”这个词经常出现在加拿大议会辩论的Hansard语料库中的“众议院”一词中，这是一种用于统计机器翻译的流行数据集，因为它包含所有文档的英文和法文版本。这些词在普通语言中有意义，但不在语料库中。手动定义的停用词列表将捕获一般停用词，但不是语料库特定的停用词。</p>
<p>表 3-1 列出了 Yelp 评论数据集中最常用的 40 个单词。在这里，频率被认为是它们出现在文件（评论）中的数量，而不是它们在文件中的数量。正如我们所看到的，该列表涵盖了许多停用词。它也包含一些惊喜。<code>&quot;s&quot;</code>和<code>&quot;t&quot;</code>在列表中，因为我们使用撇号作为标记化分隔符，并且诸如<code>&quot;Mary's&quot;</code>或<code>&quot;did not&quot;</code>之类的词被解析为<code>&quot;Mary s&quot;</code>和<code>&quot;didn t&quot;</code>。词<code>&quot;good&quot;</code>，<code>&quot;food&quot;</code>和<code>&quot;great&quot;</code>分别出现在三分之一的评论中。但我们可能希望保留它们，因为它们对于情感分析或业务分类非常有用。</p>

<figure >
    
        <img src="img/biao.png" width="100%" />
    
    
</figure>

<p>最常用的单词最可以揭示问题，并突出显示通常有用的单词通常在该语料库中曾出现过多次。 例如，纽约时报语料库中最常见的词是“时代”。实际上，它有助于将基于频率的过滤与停用词列表结合起来。还有一个棘手的问题，即何处放置截止点。 不幸的是这里没有统一的答案。在大多数情况下截断还需手动确定，并且在数据集改变时可能需要重新检查。</p>
<br>
<h3 id="稀有词httpfe4mlapachecnorgdocs3文本数据id稀有词"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E7%A8%80%E6%9C%89%E8%AF%8D">稀有词</a></h3>
<p>根据任务的不同，可能还需要筛选出稀有词。对于统计模型而言，仅出现在一个或两个文档中的单词更像噪声而非有用信息。例如，假设任务是根据他们的 Yelp 评论对企业进行分类，并且单个评论包含<code>&quot;gobbledygook&quot;</code>这个词。基于这一个词，我们将如何说明这家企业是餐厅，美容院还是一间酒吧？即使我们知道在这种情况下的这种生意发生在酒吧，它也会对于其他包含<code>&quot;gobbledygook&quot;</code>这个词的评论来说，这可能是一个错误。</p>
<p>不仅稀有词不可靠，而且还会产生计算开销。这套 160 万个 Yelp 评论包含 357,481 个独特单词（用空格和标点符号表示），其中 189,915 只出现在一次评论中，41,162 次出现在两次评论中。超过 60% 的词汇很少发生。这是一种所谓的重尾分布，在现实世界的数据中非常普遍。许多统计机器学习模型的训练时间随着特征数量线性地变化，并且一些模型是二次的或更差的。稀有词汇会产生大量的计算和存储成本，而不会带来额外的收益。</p>
<p>根据字数统计，可以很容易地识别和修剪稀有词。或者，他们的计数可以汇总到一个特殊的垃圾箱中，可以作为附加功能。图3-7展示了一个短文档中的表示形式，该短文档包含一些常用单词和两个稀有词<code>&quot;gobbledygook&quot;</code>和<code>&quot;zylophant&quot;</code>。通常单词保留自己的计数，可以通过停用词列表或其他频率进一步过滤方法。这些难得的单词会失去他们的身份并被分组到垃圾桶功能中.</p>

<figure >
    
        <img src="img/3-7.png" width="100%" />
    
    
</figure>

<p>由于在计算整个语料库之前不会知道哪些词很少，因此需要收集垃圾桶功能作为后处理步骤。</p>
<p>由于本书是关于特征工程的，因此我们将重点放在特征上。但稀有概念也适用于数据点。如果文本文档很短，那么它可能不包含有用的信息，并且在训练模型时不应使用该信息。</p>
<p>应用此规则时必须谨慎。维基百科转储包含许多不完整的存根，可能安全过滤。另一方面，推文本身就很短，并且需要其他特征和建模技巧。</p>
<br>
<h3 id="词干解析stemminghttpfe4mlapachecnorgdocs3文本数据id词干解析stemming"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E8%AF%8D%E5%B9%B2%E8%A7%A3%E6%9E%90%EF%BC%88stemming%EF%BC%89">词干解析（Stemming）</a></h3>
<p>简单解析的一个问题是同一个单词的不同变体会被计算为单独的单词。例如，<code>&quot;flower&quot;</code>和<code>&quot;flowers&quot;</code>在技术上是不同的记号，<code>&quot;swimmer&quot;</code>，<code>&quot;swimming&quot;</code>和<code>&quot;swim&quot;</code>也是如此，尽管它们的含义非常接近。如果所有这些不同的变体都映射到同一个单词，那将会很好。</p>
<p>词干解析是一项 NLP 任务，试图将单词切分为基本的语言词干形式。有不同的方法。有些基于语言规则，其他基于观察统计。被称为词形化的算法的一个子类将词性标注和语言规则结合起来。</p>
<p>Porter stemmer 是英语中使用最广泛的免费词干工具。原来的程序是用 ANSI C 编写的，但是很多其他程序包已经封装它来提供对其他语言的访问。尽管其他语言的努力正在进行，但大多数词干工具专注于英语。</p>
<p>以下是通过 NLTK Python 包运行 Porter stemmer 的示例。正如我们所看到的，它处理了大量的情况，包括将<code>&quot;sixties&quot;</code>和<code>&quot;sixty&quot;</code>转变为同一根<code>&quot;sixti&quot;</code>。但这并不完美。单词<code>&quot;goes&quot;</code>映射到<code>&quot;goe&quot;</code>，而<code>&quot;go&quot;</code>映射到它自己。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">nltk</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">porter</span><span class="o">.</span><span class="n">PorterStemmer</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;flowers&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;lemon&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;zeroes&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;zero&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;stemmer&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;stem&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;sixties&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;sixti&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;sixty&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;sixty&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;goes&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;goe&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;go&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;go&#39;</span> <span class="n">复制ErrorOK</span><span class="err">!</span>
</code></pre></div><p>词干解析的确有一个计算成本。 最终收益是否大于成本取决于应用程序。</p>
<br>
<h3 id="含义的原子从单词到-n-gram-到短语httpfe4mlapachecnorgdocs3文本数据id含义的原子从单词到-n-gram-到短语"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%90%AB%E4%B9%89%E7%9A%84%E5%8E%9F%E5%AD%90%EF%BC%9A%E4%BB%8E%E5%8D%95%E8%AF%8D%E5%88%B0-n-gram-%E5%88%B0%E7%9F%AD%E8%AF%AD">含义的原子：从单词到 N-gram 到短语</a></h3>
<p>词袋的概念很简单。但是，一台电脑怎么知道一个词是什么？文本文档以数字形式表示为一个字符串，基本上是一系列字符。也可能会遇到 JSON blob 或 HTML 页面形式的半结构化文本。但即使添加了标签和结构，基本单位仍然是一个字符串。如何将字符串转换为一系列的单词？这涉及解析和标记化的任务，我们将在下面讨论。</p>
<br>
<h3 id="解析和分词httpfe4mlapachecnorgdocs3文本数据id解析和分词"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E8%A7%A3%E6%9E%90%E5%92%8C%E5%88%86%E8%AF%8D">解析和分词</a></h3>
<p>当字符串包含的不仅仅是纯文本时，解析是必要的。例如，如果原始数据是网页，电子邮件或某种类型的日志，则它包含额外的结构。人们需要决定如何处理日志中的标记，页眉，页脚或无趣的部分。如果文档是网页，则解析器需要处理 URL。如果是电子邮件，则可能需要特殊字段，例如 From，To 和 Subject 需要被特别处理，否则，这些标题将作为最终计数中的普通单词统计，这可能没有用处。</p>
<p>解析后，文档的纯文本部分可以通过标记。这将字符串（一系列字符）转换为一系列记号。然后可以将每个记号计为一个单词。分词器需要知道哪些字符表示一个记号已经结束，另一个正在开始。空格字符通常是好的分隔符，正如标点符号一样。如果文本包含推文，则不应将井号（<code>#</code>）用作分隔符（也称为分隔符）。</p>
<p>有时，分析需要使用句子而不是整个文档。例如，n-gram 是一个句子的概括，不应超出句子范围。更复杂的文本特征化方法，如 word2vec 也适用于句子或段落。在这些情况下，需要首先将文档解析为句子，然后将每个句子进一步标记为单词。</p>
<br>
<h3 id="字符串对象httpfe4mlapachecnorgdocs3文本数据id字符串对象"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AF%B9%E8%B1%A1">字符串对象</a></h3>
<p>字符串对象有各种编码，如 ASCII 或 Unicode。纯英文文本可以用 ASCII 编码。 一般语言需要 Unicode。 如果文档包含非 ASCII 字符，则确保分词器可以处理该特定编码。否则，结果将不正确。</p>
<br>
<h3 id="短语检测的搭配提取httpfe4mlapachecnorgdocs3文本数据id短语检测的搭配提取"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E7%9F%AD%E8%AF%AD%E6%A3%80%E6%B5%8B%E7%9A%84%E6%90%AD%E9%85%8D%E6%8F%90%E5%8F%96">短语检测的搭配提取</a></h3>
<p>连续的记号能立即被转化成词表和 n-gram。但从语义上讲，我们更习惯于理解短语，而不是 n-gram。在计算自然语言处理中，有用短语的概念被称为搭配。用 Manning 和 Schütze（1999：141）的话来说：“搭配是一个由两个或两个以上单词组成的表达，它们对应于某种常规的说话方式。”</p>
<p>搭配比其部分的总和更有意义。例如，<code>&quot;strong tea&quot;</code>具有超越<code>&quot;great physical strength&quot;</code>和<code>&quot;tea&quot;</code>的不同含义，因此被认为是搭配。另一方面，“可爱的小狗”这个短语恰恰意味着它的部分总和：“可爱”和“小狗”。因此，它不被视为搭配。</p>
<p>搭配不一定是连续的序列。<code>&quot;Emma knocked on the door&quot;</code>一词被认为包含搭配<code>&quot;knock door&quot;</code>，因此不是每一个搭配都是一个 n-gram。相反，并不是每个 n-gram 都被认为是一个有意义的搭配。</p>
<p>由于搭配不仅仅是其部分的总和，它们的含义也不能通过单个单词计数来充分表达。作为一种表现形式，词袋不足。袋子的 ngram 也是有问题的，因为它们捕获了太多无意义的序列（考虑<code>&quot;this is in the bag-of-ngram example&quot;</code>），而没有足够的有意义的序列。</p>
<p>搭配作为功能很有用。但是，如何从文本中发现并提取它们呢？一种方法是预先定义它们。如果我们努力尝试，我们可能会找到各种语言的全面成语列表，我们可以通过文本查看任何匹配。这将是非常昂贵的，但它会工作。如果语料库是非常特定领域的并且包含深奥的术语，那么这可能是首选的方法。但是这个列表需要大量的手动管理，并且需要不断更新语料库。例如，分析推文，博客和文章可能不太现实。</p>
<p>自从统计 NLP 过去二十年出现以来，人们越来越多地选择用于查找短语的统计方法。统计搭配提取方法不是建立固定的短语和惯用语言列表，而是依赖不断发展的数据来揭示当今流行的语言。</p>
<br>
<h3 id="基于频率的方法httpfe4mlapachecnorgdocs3文本数据id基于频率的方法"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%9F%BA%E4%BA%8E%E9%A2%91%E7%8E%87%E7%9A%84%E6%96%B9%E6%B3%95">基于频率的方法</a></h3>
<p>一个简单的黑魔法是频繁发生的 n-gram。这种方法的问题是最常发生的，这种可能不是最有用的。 表 3-2 显示了整个 Yelp 评论数据集中最流行的 bigram（<code>n=2</code>）。 正如我们所知的，按文件计数排列的最常见的十大常见术语是非常通用的术语，并不包含太多含义。</p>

<figure >
    
        <img src="img/biaod.png" width="100%" />
    
    
</figure>

<br>
<h3 id="用于搭配提取的假设检验httpfe4mlapachecnorgdocs3文本数据id用于搭配提取的假设检验"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E7%94%A8%E4%BA%8E%E6%90%AD%E9%85%8D%E6%8F%90%E5%8F%96%E7%9A%84%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C">用于搭配提取的假设检验</a></h3>
<p>原始流行度计数（Raw popularity count）是一个比较粗糙的方法。我们必须找到更聪慧的统计数据才能够轻松挑选出有意义的短语。关键的想法是看两个单词是否经常出现在一起。回答这个问题的统计机制被称为假设检验。</p>
<p>假设检验是将噪音数据归结为“是”或“否”的答案。它涉及将数据建模为从随机分布中抽取的样本。随机性意味着人们永远无法 100% 的确定答案；总会有异常的机会。所以答案附在概率上。例如，假设检验的结果可能是“这两个数据集来自同一分布，其概率为 95%”。对于假设检验的温和介绍，请参阅可汗学院关于假设检验和 p 值的教程。</p>
<p>在搭配提取的背景下，多年来已经提出了许多假设检验。最成功的方法之一是基于似然比检验（Dunning，1993）。对于给定的一对单词，该方法测试两个假设观察的数据集。假设 1（原假设）表示，词语 1 独立于词语 2 出现。另一种说法是说，看到词语1对我们是否看到词语2没有影响。假设 2（备选假设）说，看到词 1 改变了看到单词 2 的可能性。我们采用备选假设来暗示这两个单词形成一个共同的短语。因此，短语检测（也称为搭配提取）的似然比检验提出了以下问题：给定文本语料库中观察到的单词出现更可能是从两个单词彼此独立出现的模型中生成的，或者模型中两个词的概率纠缠？</p>
<p>这是有用的。让我们算一点。（数学非常精确和简洁地表达事物，但它确实需要与自然语言完全不同的分析器。）</p>

<figure >
    
        <img src="img/gongshi.png" width="100%" />
    
    
</figure>

<p>似然函数<code>L(Data; H)</code>表示在单词对的独立模型或非独立模型下观察数据集中词频的概率。为了计算这个概率，我们必须对如何生成数据做出另一个假设。最简单的数据生成模型是二项模型，其中对于数据集中的每个单词，我们抛出一个硬币，并且如果硬币朝上出现，我们插入我们的特殊单词，否则插入其他单词。在此策略下，特殊词的出现次数遵循二项分布。二项分布完全由词的总数，词的出现次数和词首概率决定。</p>
<p>似然比检验分析常用短语的算法收益如下。</p>
<ol>
<li>
<p>计算所有单体词的出现概率：<code>p(w)</code>。</p>
</li>
<li>
<p>计算所有唯一双元的条件成对词发生概率：<code>p(W2 × W1)</code></p>
</li>
<li>
<p>计算所有唯一的双对数似然比对数。</p>
</li>
<li>
<p>根据它们的似然比排序双字节。</p>
</li>
<li>
<p>以最小似然比值作为特征。</p>
<br>
</li>
</ol>
<h3 id="掌握似然比测试httpfe4mlapachecnorgdocs3文本数据id掌握似然比测试"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E6%8E%8C%E6%8F%A1%E4%BC%BC%E7%84%B6%E6%AF%94%E6%B5%8B%E8%AF%95">掌握似然比测试</a></h3>
<p>关键在于测试比较的不是概率参数本身，而是在这些参数（以及假设的数据生成模型）下观察数据的概率。可能性是统计学习的关键原则之一。但是在你看到它的前几次，这绝对是一个令人困惑的问题。一旦你确定了逻辑，它就变得直观了。</p>
<p>还有另一种基于点互信息的统计方法。但它对真实世界文本语料库中常见的罕见词很敏感。因此它不常用，我们不会在这里展示它。</p>
<p>请注意，搭配抽取的所有统计方法，无论是使用原始频率，假设测试还是点对点互信息，都是通过过滤候选词组列表来进行操作的。生成这种清单的最简单和最便宜的方法是计算 n-gram。它可能产生不连续的序列，但是它们计算成本颇高。在实践中，即使是连续 n-gram，人们也很少超过 bi-gram 或 tri-gram，因为即使在过滤之后，它们的数量也很多。为了生成更长的短语，还有其他方法，如分块或与词性标注相结合。</p>
<br>
<h3 id="分块chunking和词性标注part-of-speech-tagginghttpfe4mlapachecnorgdocs3文本数据id分块chunking和词性标注part-of-speech-tagging"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%88%86%E5%9D%97%EF%BC%88chunking%EF%BC%89%E5%92%8C%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%EF%BC%88part-of-speech-tagging%EF%BC%89">分块（Chunking）和词性标注（part-of-Speech Tagging）</a></h3>
<p>分块比 n-gram 要复杂一点，因为它基于词性，基于规则的模型形成了记号序列。</p>
<p>例如，我们可能最感兴趣的是在问题中找到所有名词短语，其中文本的实体，主题最为有趣。 为了找到这个，我们使用词性标记每个作品，然后检查该标记的邻域以查找词性分组或“块”。 定义单词到词类的模型通常是语言特定的。 几种开源 Python 库（如 NLTK，Spacy 和 TextBlob）具有多种语言模型。</p>
<p>为了说明 Python 中的几个库如何使用词性标注非常简单地进行分块，我们再次使用 Yelp 评论数据集。 我们将使用 spacy 和 TextBlob 来评估词类以找到名词短语。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">json</span> 
<span class="c1"># Load the first 10 reviews </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json&#39;</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">js</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span> 
<span class="n">js</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()))</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">review_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">js</span><span class="p">)</span> 
<span class="c1">## First we&#39;ll walk through spaCy&#39;s functions </span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">spacy</span> 
<span class="c1"># preload the language model </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;en&#39;</span><span class="p">)</span> 
<span class="c1"># We can create a Pandas Series of spaCy nlp variables </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">doc_df</span> <span class="o">=</span> <span class="n">review_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span> 
<span class="c1"># spaCy gives you fine grained parts of speech using: (.pos_) </span>
<span class="c1"># and coarse grained parts of speech using: (.tag_) </span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">doc_df</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span> 
<span class="nb">print</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">doc</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">doc</span><span class="o">.</span><span class="n">tag_</span><span class="p">])</span> 
<span class="n">Got</span> <span class="n">VERB</span> <span class="n">VBP</span> 
<span class="n">a</span> <span class="n">DET</span> <span class="n">DT</span> 
<span class="n">letter</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="ow">in</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">the</span> <span class="n">DET</span> <span class="n">DT</span> 
<span class="n">mail</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="n">last</span> <span class="n">ADJ</span> <span class="n">JJ</span> 
<span class="n">week</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="n">that</span> <span class="n">ADJ</span> <span class="n">WDT</span> 
<span class="n">said</span> <span class="n">VERB</span> <span class="n">VBD</span> 
<span class="n">Dr</span><span class="o">.</span> <span class="n">PROPN</span> <span class="n">NNP</span> 
<span class="n">Goldberg</span> <span class="n">PROPN</span> <span class="n">NNP</span> 
<span class="ow">is</span> <span class="n">VERB</span> <span class="n">VBZ</span> 
<span class="n">moving</span> <span class="n">VERB</span> <span class="n">VBG</span> 
<span class="n">to</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">Arizona</span> <span class="n">PROPN</span> <span class="n">NNP</span> 
<span class="n">to</span> <span class="n">PART</span> <span class="n">TO</span> 
<span class="n">take</span> <span class="n">VERB</span> <span class="n">VB</span> 
<span class="n">a</span> <span class="n">DET</span> <span class="n">DT</span> 
<span class="n">new</span> <span class="n">ADJ</span> <span class="n">JJ</span> 
<span class="n">position</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="n">there</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="ow">in</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">June</span> <span class="n">PROPN</span> <span class="n">NNP</span> 
<span class="o">.</span> <span class="n">PUNCT</span> <span class="o">.</span> 
<span class="n">SPACE</span> <span class="n">SP</span> 
<span class="n">He</span> <span class="n">PRON</span> <span class="n">PRP</span> 
<span class="n">will</span> <span class="n">VERB</span> <span class="n">MD</span> 
<span class="n">be</span> <span class="n">VERB</span> <span class="n">VB</span> 
<span class="n">missed</span> <span class="n">VERB</span> <span class="n">VBN</span> 
<span class="n">very</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="n">much</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="o">.</span> <span class="n">PUNCT</span> <span class="o">.</span> 
<span class="n">SPACE</span> <span class="n">SP</span> 
<span class="n">I</span> <span class="n">PRON</span> <span class="n">PRP</span> 
<span class="n">think</span> <span class="n">VERB</span> <span class="n">VBP</span> 
<span class="n">finding</span> <span class="n">VERB</span> <span class="n">VBG</span> 
<span class="n">a</span> <span class="n">DET</span> <span class="n">DT</span> 
<span class="n">new</span> <span class="n">ADJ</span> <span class="n">JJ</span> 
<span class="n">doctor</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="ow">in</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">NYC</span> <span class="n">PROPN</span> <span class="n">NNP</span> 
<span class="n">that</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">you</span> <span class="n">PRON</span> <span class="n">PRP</span> 
<span class="n">actually</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="n">like</span> <span class="n">INTJ</span> <span class="n">UH</span> 
<span class="n">might</span> <span class="n">VERB</span> <span class="n">MD</span> 
<span class="n">almost</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="n">be</span> <span class="n">VERB</span> <span class="n">VB</span> 
<span class="k">as</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="n">awful</span> <span class="n">ADJ</span> <span class="n">JJ</span> 
<span class="k">as</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">trying</span> <span class="n">VERB</span> <span class="n">VBG</span> 
<span class="n">to</span> <span class="n">PART</span> <span class="n">TO</span> 
<span class="n">find</span> <span class="n">VERB</span> <span class="n">VB</span> 
<span class="n">a</span> <span class="n">DET</span> <span class="n">DT</span> 
<span class="n">date</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="err">!</span> <span class="n">PUNCT</span> <span class="o">.</span> 
<span class="c1"># spaCy also does some basic noun chunking for us </span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">([</span><span class="n">chunk</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">doc_df</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">noun_chunks</span><span class="p">])</span> 
<span class="p">[</span><span class="n">a</span> <span class="n">letter</span><span class="p">,</span> <span class="n">the</span> <span class="n">mail</span><span class="p">,</span> <span class="n">Dr</span><span class="o">.</span> <span class="n">Goldberg</span><span class="p">,</span> <span class="n">Arizona</span><span class="p">,</span> <span class="n">a</span> <span class="n">new</span> <span class="n">position</span><span class="p">,</span> <span class="n">June</span><span class="p">,</span> <span class="n">He</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">a</span> <span class="n">new</span> <span class="n">doctor</span><span class="p">,</span> <span class="n">NYC</span><span class="p">,</span> <span class="n">you</span><span class="p">,</span> <span class="n">a</span> <span class="n">date</span><span class="p">]</span> 
<span class="c1">##### </span>
<span class="c1">## We can do the same feature transformations using Textblob </span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">textblob</span> <span class="kn">import</span> <span class="n">TextBlob</span> 
<span class="c1"># The default tagger in TextBlob uses the PatternTagger, which is fine for our example. </span>
<span class="c1"># You can also specify the NLTK tagger, which works better for incomplete sentences. </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">blob_df</span> <span class="o">=</span> <span class="n">review_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">TextBlob</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">blob_df</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">tags</span> 
<span class="p">[(</span><span class="s1">&#39;Got&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;DT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;letter&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;the&#39;</span><span class="p">,</span> <span class="s1">&#39;DT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;mail&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;last&#39;</span><span class="p">,</span> <span class="s1">&#39;JJ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;week&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s1">&#39;WDT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;said&#39;</span><span class="p">,</span> <span class="s1">&#39;VBD&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;Dr.&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;Goldberg&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;VBZ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;moving&#39;</span><span class="p">,</span> <span class="s1">&#39;VBG&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;TO&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;Arizona&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;TO&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;take&#39;</span><span class="p">,</span> <span class="s1">&#39;VB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;DT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;new&#39;</span><span class="p">,</span> <span class="s1">&#39;JJ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;position&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;there&#39;</span><span class="p">,</span> <span class="s1">&#39;RB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;June&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;He&#39;</span><span class="p">,</span> <span class="s1">&#39;PRP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;will&#39;</span><span class="p">,</span> <span class="s1">&#39;MD&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;be&#39;</span><span class="p">,</span> <span class="s1">&#39;VB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;missed&#39;</span><span class="p">,</span> <span class="s1">&#39;VBN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;very&#39;</span><span class="p">,</span> <span class="s1">&#39;RB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;much&#39;</span><span class="p">,</span> <span class="s1">&#39;JJ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;PRP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;think&#39;</span><span class="p">,</span> <span class="s1">&#39;VBP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;finding&#39;</span><span class="p">,</span> <span class="s1">&#39;VBG&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;DT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;new&#39;</span><span class="p">,</span> <span class="s1">&#39;JJ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;doctor&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;NYC&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;you&#39;</span><span class="p">,</span> <span class="s1">&#39;PRP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;actually&#39;</span><span class="p">,</span> <span class="s1">&#39;RB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;like&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;might&#39;</span><span class="p">,</span> <span class="s1">&#39;MD&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;almost&#39;</span><span class="p">,</span> <span class="s1">&#39;RB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;be&#39;</span><span class="p">,</span> <span class="s1">&#39;VB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;as&#39;</span><span class="p">,</span> <span class="s1">&#39;RB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;awful&#39;</span><span class="p">,</span> <span class="s1">&#39;JJ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;as&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;trying&#39;</span><span class="p">,</span> <span class="s1">&#39;VBG&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;TO&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;find&#39;</span><span class="p">,</span> <span class="s1">&#39;VB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;DT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">)]</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">([</span><span class="n">np</span> <span class="k">for</span> <span class="n">np</span> <span class="ow">in</span> <span class="n">blob_df</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">noun_phrases</span><span class="p">])</span> 
<span class="p">[</span><span class="s1">&#39;got&#39;</span><span class="p">,</span> <span class="s1">&#39;goldberg&#39;</span><span class="p">,</span> <span class="s1">&#39;arizona&#39;</span><span class="p">,</span> <span class="s1">&#39;new position&#39;</span><span class="p">,</span> <span class="s1">&#39;june&#39;</span><span class="p">,</span> <span class="s1">&#39;new doctor&#39;</span><span class="p">,</span> <span class="s1">&#39;nyc&#39;</span> <span class="n">复制ErrorOK</span><span class="err">!</span>
</code></pre></div><p>你可以看到每个库找到的名词短语有些不同。spacy 包含英语中的常见单词，如<code>&quot;a&quot;</code>和<code>&quot;the&quot;</code>，而 TextBlob 则删除这些单词。这反映了规则引擎的差异，它驱使每个库都认为是“名词短语”。 你也可以写你的词性关系来定义你正在寻找的块。使用 Python 进行自然语言处理可以深入了解从头开始用 Python 进行分块。</p>
<br>
<h2 id="总结httpfe4mlapachecnorgdocs3文本数据id总结"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E6%80%BB%E7%BB%93">总结</a></h2>
<p>词袋模型易于理解和计算，对分类和搜索任务很有用。但有时单个单词太简单，不足以将文本中的某些信息封装起来。为了解决这个问题，人们寄希望于比较长的序列。Bag-of-ngram 是 BOW 的自然概括，这个概念仍然容于理解，而且它的计算开销这就像 BOW 一样容易。</p>
<p>Bag of-ngram 生成更多不同的 ngram。它增加了特征存储成本，以及模型训练和预测阶段的计算成本。虽然数据点的数量保持不变，但特征空间的维度现在更大。因此数据密度更为稀疏。n 越高，存储和计算成本越高，数据越稀疏。由于这些原因，较长的 n-gram 并不总是会使模型精度的得到提高（或任何其他性能指标）。人们通常在<code>n = 2</code>或 3 时停止。较少的 n-gram 很少被使用。</p>
<p>防止稀疏性和成本增加的一种方法是过滤 n-gram 并保留最有意义的短语。这是搭配抽取的目标。理论上，搭配（或短语）可以在文本中形成非连续的标记序列。然而，在实践中，寻找非连续词组的计算成本要高得多并且没有太多的收益。因此搭配抽取通常从一个候选人名单中开始，并利用统计方法对他们进行过滤。</p>
<p>所有这些方法都将一系列文本标记转换为一组断开的计数。与一个序列相比，一个集合的结构要少得多；他们导致平面特征向量。</p>
<p>在本章中，我们用简单的语言描述文本特征化技术。这些技术将一段充满丰富语义结构的自然语言文本转化为一个简单的平面向量。我们讨论一些常用的过滤技术来降低向量维度。我们还引入了 ngram 和搭配抽取作为方法，在平面向量中添加更多的结构。下一章将详细介绍另一种常见的文本特征化技巧，称为 tf-idf。随后的章节将讨论更多方法将结构添加回平面向量。</p>
<br>
<h2 id="参考文献httpfe4mlapachecnorgdocs3文本数据id参考文献"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">参考文献</a></h2>
<p>Dunning, Ted. 1993. “Accurate methods for the statistics of surprise and</p>
<p>coincidence.” ACM Journal of Computational Linguistics, special issue on using large corpora , 19:1 (61—74).</p>
<p>“Hypothesis Testing and p-Values.” Khan Academy, accessed May 31,</p>
<p>2016,https://www.khanacademy.org/math/probability/statistics-inferential/hypothesis-testing/v/hypothesis-testing-and-p-values.</p>
<p>Manning,Christopher D. and Hinrich Schütze. 1999. Foundations of StatisticalNatural Language Processing . Cambridge, Massachusettes: MIT Press.</p>
<p>Sometimes people call it the document “vector.” The vector extends from the original and ends at the specified point. For our purposes, “vector” and “point” are the same thing.</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 文本网络分析&amp;文化桥梁Python代码实现</title>
      <link>https://textdata.cn/blog/2021-12-28-pnas_culture_bridges/</link>
      <pubDate>Tue, 28 Dec 2021 09:43:10 +0600</pubDate>
      
      <guid>/blog/2021-12-28-pnas_culture_bridges/</guid>
      <description> PNAS2016这篇使用“自闭症谱系障碍ASD疾病的治病原因大讨论”做研究，文中使用TF-IDF刻画参与者信息的“新且熟悉” ,构建参与者文化网络。使用自动文本分析发现，如果组织方建立文化桥梁，在很少能一起讨论的议题领域内产生可连接的对话主题信息，这类信息不仅能引起多个受众的共鸣，而且还能让这些受众彼此进行对话，那么组织方更有可能激发新社交媒体受众的评论</description>
      <content:encoded><![CDATA[<h2 id="代码下载codezip"><a href="code.zip">代码下载</a></h2>
<p>现在一提到文本分析，除了词频统计、情感分析，就属话题分析最火，主流技术路线是使用LDA话题模型进行主题分析。但是LDA适合文档区分度大，文本档数较大。如果不满足这两点，LDA虽然能跑出模型，但是跑出的topic无法解读，没有意义。今天分享一个技术文，在看技术文之前，将技术文的背景文献稍微整理翻译了下，方便大家更好的理解textnets的应用场景。</p>
<p>网络分析通常用于描述人与人之间的关系——尤其是在社会科学中——但它也可以应用于词之间的关系。例如，网络关系可以通过文档中单个单词的共现来创建，或者可以使用双模式网络投影在文档之间创建关系。</p>
<p>基于网络的自动文本分析方法的优点是</p>
<ul>
<li>
<p>像社会群体一样，可以通过三元闭包更准确地测量词组的含义——或者任何两个词或术语相互的含义的原则如果将它们放在第三个词的上下文中，可以更准确地理解；</p>
</li>
<li>
<p><strong>文本网络可以应用于任何长度的文档</strong>，这与通常需要大量单词才能正常运行的主题模型不同。在简短的社交媒体文本变得普遍的时代，这是一个显着的优势。</p>
</li>
<li>
<p>最后，这种方法受益于<strong>社区检测</strong>跨学科文献的最新进展，可以说它提供了更准确的单词分组方法，这些方法受益于网络内观察到的聚类，而不是词袋模型。</p>
</li>
</ul>
<br>
<h2 id="背景-文化桥梁">背景-文化桥梁</h2>
<p>文化信息传递理论和公共审议和计算技术。</p>
<blockquote>
<p>Markowitz, D. M., &amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).</p>
</blockquote>
<p>由于每天光顾此类论坛的人数迅速增加，社交媒体为倡导组织塑造公共辩论提供了有力的机会。 然而，社会科学家还没有解释为什么一些<strong>议题发起者</strong>能成功发起大规模的广泛参与性(公开辩论/广泛对话)，而大多数其他组织却没做到。 本文使用自动文本分析发现，如果组织方建立<strong>文化桥梁</strong>，在很少能一起讨论的议题领域内产生可连接的对话主题信息，这类信息不仅能引起多个受众的共鸣，而且还能让这些受众彼此进行对话，那么组织方更有可能激发新社交媒体受众的评论。
在控制这些因素的情况下，建立实质性文化桥梁的组织， 其所发布信息， 比那些没有建立实质性文化桥梁的组织， 得到的评论数多 2.52 倍。</p>

<figure >
    
        <img src="img/large.jpg" />
    
    
</figure>

<p>社交网络分析通常用于描述个人之间的友谊或其他关系，但它也可通过参与者的消息或想法的类型来描述参与者之间的关系（如下图) 是“文化网络”中的一个小区域。</p>
<ul>
<li>每个节点描述一个参与议题公开对话的参与者</li>
<li>节点间的边代表那些在社交媒体倡导领域内讨论类似议题的人。</li>
</ul>
<p>PNAS2016这篇使用“<strong>自闭症谱系障碍ASD疾病的治病原因大讨论</strong>”做的数据分享，图中 t1 处的一类用户认为ASD致病可能跟疫苗有关，而另一类人可能认为ASD可能与遗传因素的有关。从图形看，t1这类议题发起方式，没有产生广泛参与性。而t2和t3，文化网络中因为文化桥梁的存在，产生了广泛参与性。</p>
<p><strong>假设的文化网络</strong>，其中节点代表参与有关议题的对话参与者，而节点之间的边则描述了其消息内容的相似性。议题广泛参与性，除了话题发起者影响力、话题投放资源等因素影响，还有一个因素就是发起的话题是否吸引了受众。对于参与者而言，最有吸引力的话题需要满足“<strong>新颖，且熟悉</strong>”。</p>
<p><strong>用TF-IDF刻画文化网络中的“新且熟悉”</strong>
在文本分析中有一个文本特征提取技术tf-idf</p>
<ul>
<li>tf指词语在某文档中出现的次数；从词语的角度，该值越大越熟悉</li>
<li>idf逆文档数，即词语出现在多少个文档中；从词语的角度，该值越小越新颖</li>
</ul>

<figure >
    
        <img src="img/large2.jpg" />
    
    
</figure>

<p>本教程将引导您完成使用文本网络分析和可视化数据所需的所有步骤。 在解决与使用文本网络相关的其他杂项问题之前，本教程首先介绍了一个独立的示例。</p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="n">textnets</span>
</code></pre></div><br>
<h2 id="1-查看数据">1. 查看数据</h2>
<p>pnas2016这篇的数据没有开源，通过文本构建文化网络、发现文化桥梁。这里使用一个特别特别小的新闻数据，关于人类第一次登月。如果我们使用<a href="https://github.com/jboynyc/textnets">textnets</a>，准备的数据需要有两个列</p>
<ul>
<li>议题参与者，类比报刊</li>
<li>议题参与者发布的内容，如评论等</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;test.csv&#39;</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">|    | Unnamed: 0        | headlines                                                                 |
|---:|:------------------|:--------------------------------------------------------------------------|
|  0 | The Guardian      | 3:56 am: Man Steps On to the Moon                                         |
|  1 | New York Times    | Men Walk on Moon -- Astronauts Land on Plain, Collect Rocks, Plant Flag   |
|  2 | Boston Globe      | Man Walks on Moon                                                         |
|  3 | Houston Chronicle | Armstrong and Aldrich &#34;Take One Small Step for Man&#34; on the Moon           |
|  4 | Washington Post   | The Eagle Has Landed -- Two Men Walk on the Moon                          |
|  5 | Chicago Tribune   | Giant Leap for Mankind -- Armstrong Takes 1st Step on Moon                |
|  6 | Los Angeles Times | Walk on Moon -- That\&#39;s One Small Step for Man, One Giant Leap for Mankind |
</code></pre></div><br>
<h2 id="2-导入corpus">2. 导入corpus</h2>
<p>使用textnets库的将数据导入为其特有的语料格式。从下方可以看到textnets可能会用spacy，如果要配置英文en_core_web_sm或中文zh_core_web_sm, 请查看该文 <a href="https://t.hk.uy/aCmr">https://t.hk.uy/aCmr</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">textnets</span> <span class="k">as</span> <span class="nn">tn</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1">#设置随机种子，保证代码可重复性</span>
<span class="n">tn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;seed&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">42</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">tn</span><span class="o">.</span><span class="n">Corpus</span><span class="o">.</span><span class="n">from_csv</span><span class="p">(</span><span class="s1">&#39;test.csv&#39;</span><span class="p">)</span>
<span class="n">corpus</span>
</code></pre></div>
<figure >
    
        <img src="img/corpus.png" width="100%" />
    
    
</figure>

<br>
<h2 id="3-构建网络">3. 构建网络</h2>
<p>需要注意的是corpus.tokenized()是textnets特有的分词方法，如果所处理的新闻是中文，需要提前分词去停用词整理为像英文数据格式，用空格间隔单词。</p>
<p>textnets提供了构建网络的方法</p>
<p>tn.Textnet(data, min_docs, connected, doc_attrs)</p>
<ul>
<li>data DataFrame类型, 三列，自己可以运行 corpus.tokenized() 查看样式</li>
<li>min_docs 一个词语存在于至少多少个文档中，默认为2。一个词至少出现在两个doc中，才会让两个doc产生连接</li>
<li>connected 仅保留网络的最大连接组件（默认值：False）</li>
<li>doc_attrs 文档节点的属性，字典的字典(双层嵌套字典)</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">t</span> <span class="o">=</span> <span class="n">tn</span><span class="o">.</span><span class="n">Textnet</span><span class="p">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">tokenized</span><span class="p">(),</span> <span class="n">min_docs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>使用所有默认参数， textnets 会帮我们删除英文停用词，词干化(合并同类词)，并删除标点符号、数字、URL 等。</p>
<p>但这里我们将破例将 min_docs 设置为1（因为数据只有几句话几十个单词，这里破例设置为1，正常这里至少是2）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">t</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>   <span class="c1">#标记节点名(单词、媒体)</span>
       <span class="n">show_clusters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1">#绘制簇的边界</span>
</code></pre></div>
<figure >
    
        <img src="img/output_10_0.svg" width="100%" />
    
    
</figure>

<p>show_clusters 使用 <strong>Leiden社区检测算法</strong>(Leiden community detection algorithm)找到了分区成簇，它似乎识别了<strong>同一主题</strong>(登月)下不同词之间的远近(相似的词在一个簇中，不同的词处于不同的簇中)。</p>
<p>你可能会疑惑：为什么网络图中的<strong>单词: moon</strong>会自己漂移？ 那是因为moon这个词在每个文档中只出现一次，所以每个文档moon的tf-idf得分为0。</p>
<p>让我们再次可视化相同的事情，但这次根据节点的 BiRank（二部网络的中心性度量）缩放节点，根据权重缩放边缘。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">t</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
       <span class="n">show_clusters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
       <span class="n">scale_nodes_by</span><span class="o">=</span><span class="s2">&#34;birank&#34;</span><span class="p">,</span>
       <span class="n">scale_edges_by</span><span class="o">=</span><span class="s2">&#34;weight&#34;</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_12_0.svg" width="100%" />
    
    
</figure>

<p>我们还可以只可视化报刊网络，不显示词语。这里设置node_type=&lsquo;doc&rsquo;</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#node_type有两种值， doc、term</span>
<span class="n">papers</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">project</span><span class="p">(</span><span class="n">node_type</span><span class="o">=</span><span class="s2">&#34;doc&#34;</span><span class="p">)</span>
<span class="n">papers</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_14_0.svg" width="100%" />
    
    
</figure>

<p>和之前的双向网络一样，我们可以看到Houston Chronicle、  Chicago Tribune、  Los Angeles Times更紧密地聚集在一起。</p>
<p>接下来，词网络：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">words</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">project</span><span class="p">(</span><span class="n">node_type</span><span class="o">=</span><span class="s2">&#34;term&#34;</span><span class="p">)</span>
<span class="n">words</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">show_clusters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_16_0.svg" width="100%" />
    
    
</figure>

<p>除了可视化之外，我们还可以使用<strong>社交网络指标</strong>分析我们的语料库。 例如，具有教高<strong>介数中心性betweenness centrality</strong>的文档可能将主题不同簇联系起来，起到文化桥梁的作用，从而刺激跨越符号鸿沟的交流(Bail,2016)。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">papers</span><span class="o">.</span><span class="n">top_betweenness</span><span class="p">()</span>
</code></pre></div><pre><code>Los Angeles Times    7.0
Boston Globe         0.0
Chicago Tribune      0.0
Houston Chronicle    0.0
New York Times       0.0
The Guardian         0.0
Washington Post      0.0
dtype: float64
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">words</span><span class="o">.</span><span class="n">top_betweenness</span><span class="p">()</span>
</code></pre></div><pre><code>walk         72.00
man          18.00
step         16.00
small        12.75
land          6.00
giant         6.00
leap          6.00
mankind       6.00
armstrong     3.25
plain         0.00
dtype: float64
</code></pre>
<p>这是因为New York Times在其标题中使用了“walk”一词，将“one small step”簇与“man on moon”簇联系起来。</p>
<p>我们可以再次生成词网络图，这次根据节点的中介中心性缩放节点，并使用“骨干提取”从网络中修剪边缘：cite:p<code>Serrano2009</code>。</p>
<p>我们还可以使用 color_clusters（而不是 show_clusters）根据节点的分区为节点着色。</p>
<p>我们可以过滤节点标签，只标记那些中间中心性betweenness centrality分数高于中位数的节点。 这在高阶网络中特别有用，其中标记每个节点会导致视觉混乱。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">words</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">scale_nodes_by</span><span class="o">=</span><span class="s2">&#34;betweenness&#34;</span><span class="p">,</span>
           <span class="n">color_clusters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
           <span class="n">edge_width</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="o">*</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="o">.</span><span class="n">edges</span><span class="p">[</span><span class="s2">&#34;weight&#34;</span><span class="p">]],</span>
           <span class="n">edge_opacity</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
           <span class="n">node_label_filter</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">n</span><span class="o">.</span><span class="n">betweenness</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">words</span><span class="o">.</span><span class="n">betweenness</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>
</code></pre></div>
<figure >
    
        <img src="img/output_21_0.svg" width="100%" />
    
    
</figure>

<h2 id="其他textnets案例资料">其他textnets案例资料</h2>
<p><a href="https://www.jboy.space/blog/enemies-foreign-and-partisan.html">https://www.jboy.space/blog/enemies-foreign-and-partisan.html</a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS|词汇熟悉度对线上参与和资金筹集的预测性效用</title>
      <link>https://textdata.cn/blog/2021-12-27-pnas_text_fluency/</link>
      <pubDate>Mon, 27 Dec 2021 10:43:10 +0600</pubDate>
      
      <guid>/blog/2021-12-27-pnas_text_fluency/</guid>
      <description>人们对简单和通用的语言的反应比复杂和技术语言更有利;本文提供了文本分析的新思路，使用LIWC测量术语流畅性、复杂词汇。</description>
      <content:encoded><![CDATA[<p>[<strong>论文下载The predictive utility of word familiarity for online engagements and funding.pdf</strong>](The predictive utility of word familiarity for online engagements and funding.pdf)</p>
<blockquote>
<p>Markowitz, D. M., &amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>处理流畅性等元认知框架通常表明人们对简单和通用的语言的反应比复杂和技术语言更有利。与复杂的信息相比，人们更容易处理简单和非技术性的信息，因此会更多地与目标进行互动。在涵盖 12 个现场样本（总 n = 1,064,533）的两项研究中，我们通过展示人们在付出时间和注意力时更多地使用非技术语言（例如，简单的在线语言往往会获得更多社交信息）来建立并复制这种越简单越好的现象订婚）。然而，人们在捐款时会对复杂的语言做出反应（例如，慈善捐赠活动和赠款摘要中的复杂语言往往会收到更多的钱）。这一证据表明，人们根据时间或金钱目标以不同的方式使用复杂语言的启发式方法。这些结果强调语言是社会和心理过程的镜头，以及大规模测量文本模式的计算方法。</p>
<ul>
<li>processing fluency</li>
<li>field studies</li>
<li>automated text analysis</li>
<li>common words</li>
<li>jargon</li>
</ul>
<br>
## 研究背景-复杂词汇的负面效应
<p><strong>术语（jargon）</strong>，是复杂的、技术的、专业的语言，与日常语言相比，加工难度更大、更不流畅。许多关于<strong>加工流畅度</strong>（processing fluency）的研究都发现了使用术语的负面结果： 由于术语会给予人们不熟悉、加工困难的感觉，从而导致其较难理解。使用术语来描述手术过程的医生可能导致病人关于健康风险的错误估计；阅读了关于一项技术的复杂描述的人们（与阅读简单描述的人们相比）对该技术的理解更差并高估其风险。因此，不常用的、技术性的词汇通常不被看好，因为人们对其不熟悉而感觉较难加工，并给人们带来理解上的挑战。</p>
<p>然而，对于复杂词汇的影响，以往研究基本基于实验室结果，效应的强度、健壮性、对真实行为的预测性等仍不清楚。此外，以往大多数关于加工流畅度（processing fluency）的研究都依赖于人们的主观判断，即通过询问被试对于简单或复杂文本的感受来判断效应的大小。该研究则弥补了这两点不足，将加工流畅度操作性定义为词法流畅度（lexical fluency，即所用的词汇为通用词汇还是复杂词汇），并考察复杂词汇对人们在真实世界中行为的影响。</p>
<br>
<h2 id="工具性启发法">工具性启发法</h2>
<p>工具性启发法（instrumentality heuristic）认为，如果一个感觉很困难的经历是有助于达到特定目标的，人们会给予这个经历更高的评价。由此，如果工具性目标被激活，那么加工流畅性低的复杂文本，反而可能会被给予更高的评价。对此，该研究同时考察了复杂词汇对于线上参与度（社会参与度）和资金筹集的影响。</p>
<br>
<h2 id="研究假设">研究假设</h2>
<ul>
<li>假设一：没有工具性目标被激活时，人们更喜欢简单的语言，表现为更高的社会参与度</li>
<li>假设二：工具性目标被激活时，人们更喜欢复杂的语言，表现为更多的资金支持</li>
</ul>
<p>实验结果支持这两个假设：通用词汇与更多的线上支持（高社会参与度）相关，复杂词汇则与更多的资金支持相关。</p>
<br>
<h2 id="数据">数据</h2>
<h3 id="研究一的数据包括">研究一的数据包括：</h3>
<ol>
<li>
<p>来自左倾（纽约时报）、右倾（福克斯新闻）、中立（美联社）的新闻媒体的推特</p>
</li>
<li>
<p>随机选择来自上述三个组织的的记者/名人的个人推特</p>
</li>
<li>
<p>共和党政治家和特朗普手下的推特</p>
</li>
<li>
<p>Reddit文章标题</p>
</li>
<li>
<p>科学论文（来自PLoS One）的标题和、摘要</p>
</li>
<li>
<p>TED演讲标题、内容</p>
</li>
</ol>
<h3 id="研究二的数据包括">研究二的数据包括：</h3>
<ol>
<li>三个慈善平台</li>
</ol>
<p>  a) Kickstarter，主要是关于对创意项目的投资</p>
<p>  b) Indiegogo，主要是关于对创意项目和初创企业的投资</p>
<p>  c) GoFundMe，时要是关于生活事件的筹募（医疗、事故等）</p>
<ol start="2">
<li>NIH基金申请书的摘要</li>
</ol>
<br>
<h2 id="数据分析">数据分析</h2>
<p>自动文本分析工具：研究使用自动文本分析工具LIWC（Linguistic Inquiry and Word Count）来对文本进行分析。LIWC词典是一个经过专家和统计分析认证的工具，其包含了6400个代表“非正式、非专业”的英语单词。研究者把通用词汇的比例操作性定义为文本中LIWC词典中词汇的比例。</p>
<p>混合效应回归分析：使用混合效应回归分析的方法对数据进行分析。其中，回归模型中的控制变量主要有5类，分别是信息源（如新闻来源、演讲者、作者），时间（如年份、视频长度、发帖距今时间、发表时间），主题（如社会/政治等），金钱（如申请成功与否、货币类型）和投入程度（如出资人的数量、股份的数量）。</p>
<p>数据转换：</p>
<ol>
<li>
<p>研究一中，由于发表时间更长的信息更可能有更高的线上参与度，因此计算中所有参与度指标均除以了数据提取日期与发表日期之间的时间距离（数据提取-发表日期）。此外，对于考察的社会参与度指标，均进行了log转换。下文（表XX）中的点赞率、转发率等，均指代经过了上述转换后的点赞数、转发数等。</p>
</li>
<li>
<p>对研究一参与度相关指标求和时（如推特点赞率与转发率之和），对各指标标准化后再求和。</p>
</li>
<li>
<p>研究二中的因变量（各数据集中的所得资金数额）亦均进行了log转换。</p>
</li>
</ol>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 历史语言记录揭示了近几十年来认知扭曲的激增</title>
      <link>https://textdata.cn/blog/2021-12-19-pnas_historical_language/</link>
      <pubDate>Sun, 19 Dec 2021 20:43:10 +0600</pubDate>
      
      <guid>/blog/2021-12-19-pnas_historical_language/</guid>
      <description> 患有抑郁症的人容易出现适应不良的思维模式，即认知扭曲，他们以过于消极和不准确的方式思考自己、世界和未来。 这些扭曲与个人情绪、行为和语言的显着变化有关。 **我们假设社会可以经历类似的集体心理变化，这些变化会反映在语言使用的历史记录中**。我们调查了过去 **125 年超 1400 万本书中认知扭曲（congnition disorder）**的文本标记的流行情况，并观察到自 1980 年代以来它们的流行程度激增，达到超过大萧条和两次世界大战的水平。 **这种模式似乎不是由词义、出版和写作标准或 Google 图书样本的变化驱动的**。 我们的研究结果发现，通过语言分析**最近的社会转向与认知扭曲和内化障碍相关**。</description>
      <content:encoded><![CDATA[<blockquote>
<p>Bollen, Johan, et al. &ldquo;Historical language records reveal a surge of cognitive distortions in recent decades.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 118.30 (2021).</p>
</blockquote>
<br>
<h2 id="摘要">摘要</h2>
<p>患有抑郁症的人容易出现适应不良的思维模式，即认知扭曲，他们以过于消极和不准确的方式思考自己、世界和未来。 这些扭曲与个人情绪、行为和语言的显着变化有关。 <strong>我们假设社会可以经历类似的集体心理变化，这些变化会反映在语言使用的历史记录中</strong>。我们调查了过去 <strong>125 年超 1400 万本书中认知扭曲（congnition disorder）<strong>的文本标记的流行情况，并观察到自 1980 年代以来它们的流行程度激增，达到超过大萧条和两次世界大战的水平。 <strong>这种模式似乎不是由词义、出版和写作标准或 Google 图书样本的变化驱动的</strong>。 我们的研究结果发现，通过语言分析</strong>最近的社会转向与认知扭曲和内化障碍相关</strong>。</p>
<br>
<h2 id="正文">正文</h2>
<p>抑郁症与独特且可识别的适应不良思维模式有关，称为<strong>认知扭曲</strong>，其中个人以不准确和过于消极的方式思考自己、未来和世界 (9-12)。例如，当个体用消极的、绝对主义的术语（例如，“I am a loser”）给自己贴上标签时，就会出现在抑郁症中看到的认知扭曲。他们可能会用二分法、极端的术语谈论未来事件（例如，“My meeting will be a complete disaster”）或对别人的心态做出毫无根据的假设（例如，“Everybody will think that I am a failure”）。</p>
<p>认知扭曲的类型通常区分许多部分重叠的类型，例如“灾难化”、“二分推理”、“否定积极的”、“情感推理”、“算命”、“标记和错误标记”、“放大和最小化”、“心理过滤”、“读心术”、“过度概括”、“个性化”和“应该陈述”。</p>
<p>**认知行为疗法 (cognitive-behavioral therapy，CBT) ** 是治疗抑郁症和其他内化障碍的黄金标准 (13)，其基础理论认为认知扭曲与内化障碍有关； 它们反映了环境压力下的负面情感和回避行为模式 (14, 15)。 <strong>语言与这种动态密切相关。 事实上，最近的研究表明，患有内化障碍的个体在他们的语言中表现出明显更高水平的认知扭曲 (16, 17)，以至于他们的患病率可能被用作抑郁症易感性的指标 (18, 19)。</strong></p>
<p>我们分析了过去 125 年中大量以英语、西班牙语和德语出版的超过 1400 万本书籍（谷歌图书）中的大量认知扭曲标记的流行情况。具体来说，我们正在研究由 CBT 专家、计算语言学家和双语母语人士组成的团队设计的数百个 1 到 5 个单词 (n-gram)、 标记的认知失真图式 (cognitive distortion schemata,CDS) 的纵向流行情况，以及由 CBT 专家小组外部验证，以捕捉 12 种认知扭曲的表达 (9)。 <strong>CDS n-gram</strong> 被设计为简短、明确和独立的语句，使用频率很高的术语表达特定认知扭曲类型的核心（图 1 和 SI 附录，表 S1-S3）。例如，3-gram 的“I am a”捕获了标签和错误标签失真，而不管其上下文或所涉及的精确标签（“女士”、“尊贵的人”、“失败者”等）。这些相同的 n-gram 在早期的研究中被证明显着更多。</p>
<br>
<h2 id="cds流行度测量">CDS流行度测量</h2>
<p><img loading="lazy" src="img/CDS_n-gram.png" alt=""  />
</p>
<p><strong>CDS n-gram</strong> 显示在灰色框内的示例，周围是合理的上下文词，这些词可能会有所不同，而不会影响 n-gram 是否标记给定类型的认知扭曲的表达（例如，<strong>读心术Mindreading、情感推理Emotiona lReasoning、标记Labeling和错误标记Mislabeling</strong>） . CDS 是由 CBT 专家、语言学家和母语使用者组成的团队设计的，用于捕捉特定认知扭曲类型的表达，而不管其特定的词汇上下文。 对于英语（美国）、西班牙语和德语，专家团队分别定义了 241、435 和 296 个 n-gram 来标记 12 种常见的认知扭曲类型。 请注意，我们的<strong>流行度测量只计算 CDS n-gram 的出现，而不管上下文（“每个人都在思考”、“仍然感觉”和“我是一个”）</strong>。 按失真类型提供的所有 CDS n-gram 的完整列表在SI Appendix, Tables S1–S3.</p>
<p><img loading="lazy" src="img/fig2.png" alt=""  />
</p>
<p>(A-C) 美国英语 (A)、西班牙语 (B) 和德语 (C) 从 1855 年到 2020 年 (125 y) 的 CDS n-gram 流行时间序列的中值 z 分数，其中添加了年份标记 对于重大历史事件。 在 20 世纪的大部分时间里，所有时间序列都显示出稳定或下降的水平，随后在过去的 30 年里认知扭曲急剧增加。</p>
<p>美国英语从 1899 年到 1978 年呈下降趋势，在 1914 年和 1940 年（第一次世界大战和第二次世界大战）以及特别是 1968 年出现小高峰。随后是 CDS 流行率从 1978 年开始激增，并持续到 2019 年。</p>
<p>对于西班牙语 我们发现从 1895 年到 1980 年代初期的稳定水平，在这一点上出现了一个趋势，即 CDS 患病率水平高于之前观察到的任何水平。</p>
<p>德国表现出稳定的 CDS 流行水平，除了第一次世界大战和第二次世界大战前后和之后的强劲高峰，直到 2007 年突然激增。</p>
<br>
<h2 id="data">Data</h2>
<p>研究数据谷歌已经开源，开源下载哦</p>
<p><a href="https://storage.googleapis.com/books/ngrams/books/datasetsv3.html">https://storage.googleapis.com/books/ngrams/books/datasetsv3.html</a></p>
<p><img loading="lazy" src="img/googlebooks.png" alt=""  />
</p>
<br>
<h2 id="cds-ngram词表">CDS ngram词表</h2>
<p>该论文CDS ngram词表</p>
<p><img loading="lazy" src="img/cdsngramlist.png" alt=""  />
</p>
<br>
<h2 id="代码">代码</h2>
<p>ngram代码实现</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">ngrams</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&#34;Historical language records reveal a surge of cognitive distortions in recent decades&#34;</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;分词结果: &#39;</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
<span class="c1">#2-gram</span>
<span class="n">two_grams</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tw</span><span class="p">)</span> <span class="k">for</span> <span class="n">tw</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;2-gram处理结果: &#39;</span><span class="p">,</span> <span class="n">two_grams</span><span class="p">)</span>
<span class="c1">#3-gram</span>
<span class="n">three_grams</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tw</span><span class="p">)</span> <span class="k">for</span> <span class="n">tw</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;3-gram处理结果: &#39;</span><span class="p">,</span> <span class="n">three_grams</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">分词结果:  [&#39;Historical&#39;, &#39;language&#39;, &#39;records&#39;, &#39;reveal&#39;, &#39;a&#39;, &#39;surge&#39;, &#39;of&#39;, &#39;cognitive&#39;, &#39;distortions&#39;, &#39;in&#39;, &#39;recent&#39;, &#39;decades&#39;]

2-gram处理结果:  [&#39;Historical language&#39;, &#39;language records&#39;, &#39;records reveal&#39;, &#39;reveal a&#39;, &#39;a surge&#39;, &#39;surge of&#39;, &#39;of cognitive&#39;, &#39;cognitive distortions&#39;, &#39;distortions in&#39;, &#39;in recent&#39;, &#39;recent decades&#39;]

3-gram处理结果:  [&#39;Historical language records&#39;, &#39;language records reveal&#39;, &#39;records reveal a&#39;, &#39;reveal a surge&#39;, &#39;a surge of&#39;, &#39;surge of cognitive&#39;, &#39;of cognitive distortions&#39;, &#39;cognitive distortions in&#39;, &#39;distortions in recent&#39;, &#39;in recent decades&#39;]

</code></pre></div><p>统计统计CDS-ngram与ngram频数，进而计算出CDS流行度。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 管理决策情境下大数据驱动的研究和应用挑战</title>
      <link>https://textdata.cn/blog/management_challenge_in_big_data_era/</link>
      <pubDate>Wed, 08 Dec 2021 12:42:10 +0600</pubDate>
      
      <guid>/blog/management_challenge_in_big_data_era/</guid>
      <description>在数字化生活背景下, 传统的管理变成或正在变成数据的管理, 传统的决策变成或正在变成基于数据分析的决策.从大数据的数据特征、问题特征和管理决策特征出发, 讨论管理决策研究和应用的范式转变.大数据驱动范式可以从外部嵌入、技术增强和使能创新三个角度来审视, 并体现出“数据驱动&#43;模型驱动”的“关联&#43;因果”含义.此外, 围绕大数据特征和重要研究方向, 阐述了全景式PAGE框架及其要素.</description>
      <content:encoded><![CDATA[<p><strong>摘  要：</strong></p>
<p>在数字化生活背景下, 传统的管理变成或正在变成数据的管理, 传统的决策变成或正在变成基于数据分析的决策.从大数据的数据特征、问题特征和管理决策特征出发, 讨论管理决策研究和应用的范式转变.大数据驱动范式可以从外部嵌入、技术增强和使能创新三个角度来审视, 并体现出“数据驱动+模型驱动”的“关联+因果”含义.此外, 围绕大数据特征和重要研究方向, 阐述了全景式PAGE框架及其要素.</p>
<p><strong>关键词</strong>： 大数据 ； 管理决策 ； 研究范式 ； 全景式 PAGE 框架</p>
<br>
<p>信息科技的飞速发展和深度融合开启了数字化生活的新篇章, 把人们带入了大数据 (big data) 时代.一方面, 随着各种感应探测技术、智能终端以及移动互联的广泛应用, 使得社会经济生活的方方面面以更细粒度的数据形式呈现, 进而整个社会的“像素”得到显著提升;另一方面, 社会“像素”的提升促进了数字“成像”的发展, 使得通过数据世界可以更清晰地描绘社会经济活动情境, 进而基于数据的商务分析 (business analytics, BA) 正在成为使能创新的核心竞争力.在此背景下, 传统的管理变成或正在变成数据的管理, 传统的决策变成或正在变成基于数据分析的决策.</p>
<p>近年来, 大数据成为学界、政界和业界持续关注的热点.在学术界, 早在2008年和2011年, 《Nature》与《Science》杂志分别从互联网技术、互联网经济学、超级计算、环境科学以及生物医药等多方面讨论大数据的处理与应用45此后, 大数据在各个学科领域包括医学、经济学、管理学以及公共管理等领域得到了广泛的探讨与研究6789同时, 大数据也引起世界各国高度重视, 美国、欧盟、澳大利亚以及日本等国部署了一系列大数据相关战略和关键领域。在产业界, 国内外大批知名企业掀起了技术产业创新浪潮, 通过收购与合作构建和提升大数据技术与应用能力, 布局和开拓相关的业态和市场.</p>
<p>我国政府对大数据高度重视并有一系列前瞻性洞见和部署.2015年十八届五中全会提出实施国家大数据战略  , 国务院发布《促进大数据发展行动纲要》  , 指出大数据是国家基础性战略资源, 旨在全面推进我国大数据发展和应用, 加快建设数据强国.2017年十九大报告进一步强调要推动互联网、大数据、人工智能和实体经济深度融合.通过国家需求、政策支持、产业结合以及企业研发等形式, 近些年来涌现出一大批重大规划和政产学研项目, 包括国家自然科学基金委员会 (NSFC) 于2015年9月启动的“大数据驱动的管理与决策研究”重大研究计划 (简称NSFC大数据重大研究计划, 参见附注)  .</p>
<p>大数据在给社会经济生活带来深刻变革的同时, 也对管理与决策研究带来一系列新的重要课题.从信息技术 (IT) 范畴来看, 可以从两个视角来认识大数据, 即大数据的“造”与“用”视角 (如图1所示) .这和产品的属性类似, 一方面, 人们关心产品是如何设计和制造出来的;另一方面, 人们关心产品是如何使用和有用的.大数据以IT的形式呈现, 通常可以概括为数据和系统 (包括算法、应用、平台等) .从造的视角出发, 涉及的主要问题包括大数据分析 (如画像、学习、推断等) 和大数据系统建设 (如体系、功能、集成等) .从用的视角出发, 涉及的主要问题包括大数据使用行为 (如采纳、影响、管理等) 和大数据使能创新 (如要素、价值、市场等) .</p>

<figure >
    
        <img src="img/1.jpg" width="800" />
    
    
</figure>

<p>值得一提的是, 大数据相关的研究不仅需要对相关领域的理论与应用进行探索和创新, 也需要对许多惯常的认识视角和方法论范式进行审视和发展.同时, 我国学者和研究人员也面临着“严谨 (rigor) 与相关 (relevance) ” (学术规范与实践影响) 和“世界与中国” (国际视野与中国根基) 既分野又统一的挑战, 当然应对这些挑战也为创新机遇开拓了广袤的空间.</p>
<br>
<br>
<h2 id="大数据特征">大数据特征</h2>
<p>概括说来, 大数据的特征可以从三个方面来描述:数据特征, 问题特征和管理决策特征, 分别刻画大数据具有的数据属性、大数据问题的特点、以及管理决策大数据问题的视角.</p>
<h3 id="11-大数据的数据特征">1.1 大数据的数据特征</h3>
<p>大数据作为数据, 具有体量大、多样性、 (价值) 密度低、速率高等属性特征 (即4V等特征）.第一, 数字化生活各要素的数据生成和交互加速了数据的海量积累, 使得数据规模剧增.体量大可以从超规模 (即超出传统规模) 和问题领域角度来理解, 因为规模是与问题领域相关, 而不是拘泥于统一量纲标准.例如, 市场营销领域的客户满意度调查的传统方式是问卷和访谈, 那么进一步考虑海量网上购物评论和社交媒体体验分享的用户生成内容 (user generated content, UGC) 就构成了一个大数据情境.第二, 数字化生活各要素的数据生成和交互丰富了数据类型, 使得数据多样性成为常态.多样性强调数据的多源异构和富媒体 (如文本、语音、图片、视频等) 特点.例如, 社交网络上的公众声音、智慧交通平台上的影像信息等均为富媒体形态且来源广泛.第三, 数字化生活各要素的数据生成和交互在加速海量积累的同时也减少了价值数据的占比, 使得价值发现的难度提升.价值密度低意味着数据挖掘和商务分析是大数据应用的关键.例如, 对于在线企业或服务平台来讲, 随着网络访问的增加和业务活动的扩展, 识别高价值的潜在用户变得相对困难, 也凸显出大数据分析的重要性.第四, 数字化生活各要素的数据生成和交互强化了流数据形态和即时性, 使得数据传输和交换速率显著升高.速率高对平滑流通和连续商务提出了更高要求.例如, 智能手机客户端应用软件 (Apps) 的使用需要在服务内容和效果方面 (包括相关内容的浏览、下载、上传、响应、展现等) 有良好的临场感和实时体验.</p>
<br>
<h3 id="12-大数据的问题特征">1.2 大数据的问题特征</h3>
<p>在各类研究和应用问题中, 有一类问题可以归为大数据问题.大数据问题应至少具有以下三个特点:粒度缩放、跨界关联和全局视图.首先, 粒度缩放是指问题要素的数据化, 并能够在不同粒度层级间进行缩放.这需要通过数据感知、连接和采集获得足够细的粒度性, 同时对于不同层级间的粒度转换具有分解和聚合能力.其次, 跨界关联是指问题的要素空间外拓.这需要扩展惯常的要素约束和领域视角, 强调“外部性”和“跨界”, 在问题要素空间中通过引入外部视角与传统视角联动, 将内部数据 (如个体自身、企业组织和行业等内部数据) 与外部数据 (如社会媒体内容等) 予以关联.最后, 全局视图是指问题定义与求解的全局性, 强调对相关情境的整体画像及其动态演化的把控和诠释.这需要基于数据分析和平台集成的全景式“成像”能力.</p>
<p>在数字化生活的背景下, 具有粒度缩放、跨界关联和全局视图特点的应用问题不断涌现, 进而激发了大量创新并催生了许多新模式、新业态.例如, 在医疗健康领域, 传统疾病诊疗中的病人就医关系正在被扩展为融合院外检测、干预、康复数据的新型诊疗模式.其中, 不仅涉及传统意义上的生化、影像和诊疗等医院内部数据, 也涉及医院外病人和社区相关的体征、体验、社会关系、环境等外部数据.这里, 需要获取相关生化组织、疾病、人、社区、环境等微观宏观粒度信息;同时进行视角拓展和关联, 包括从科室内外到医院内外的跨界融合;进而, 可以在全局层面进行更为有效的诊疗决策和管理.此外, 近年来发展迅速的新型医疗健康服务平台, 通过整合社会和行业资源, 连接医生、公众、医院以及相关上下游企业提供信息咨询、诊疗链入、健康指导等服务产品, 形成了一类新业态并呈现显著的大数据问题特征.再如, 在新型商务领域, 共享单车体现了大数据问题的粒度缩放、跨界关联和全局视图特点.通过车载传感器、定位系统以及智能手机终端等设备获得调度和管理需要的“人—车—路”粒度信息;同时, 打通导航、支付、通讯、商铺以及餐饮等诸多业务功能, 实现跨界联动;进而, 企业和平台可以从全局出发形成整体画像, 并优化布局和运作以做出相应的管理决策.</p>
<br>
<h3 id="13-大数据管理决策特征">1.3 大数据管理决策特征</h3>
<p>一般而言, 管理者在业务活动中通常有三个关注:发生了什么 (what) , 为什么发生 (why) 以及将发生什么 (will) .在大数据问题特征的情境下, 这三个关注可以从业务层面、数据层面和决策层面进行刻画, 进而形成管理决策大数据问题的特征框架 (如图2所示) .</p>

<figure >
    
        <img src="img/2.jpg" width="800" />
    
    
</figure>

<p>首先, 对于发生了什么 (what) 的关注, 业务层面需要反映业务的状态, 即已经发生或者正在发生的事件和活动 (如市场份额、交易现状、KPI表现等) ;数据层面需要体现业务环节的数据粒度, 即现有的数据能否足够支撑管理者对不同粒度层级的业务状态进行了解和把握 (如感知、采集、解析、融合等) ;决策层面需要构建问题的全局视角, 即定期整合汇总以及随需要素展现 (如:按时统计报表、实时信息查询等) .</p>
<p>接着, 对于为什么会发生 (why) 的关注, 业务层面需要反映业务及其要素之间的联系, 即业务特定状态的发生与哪些环节和要素有关联;数据层面需要体现不同业务数据路径的连接, 即不同粒度层级和跨界关联的业务数据是否有效融通, 并能够支持对数据的分析处理 (如多维、切分、回溯等) ;决策层面需要发现关联业务/要素之间的因果关系, 即厘清业务逻辑和状态转换机理.在此, 特别需要指出的是, 在很多情形下, 尤其在管理决策领域, 大数据需要既讲关联也讲因果.对于许多管理问题而言, 如果决策者对事件之间的因果关系没有准确的分析与判断, 则难以做出有效的决策, 当管理者面临重大决策时更是如此 (如投融资、进入新市场、业务转型、结构重组等) .</p>
<p>进而, 对于将发生什么 (will) 的关注, 业务层面需要反映业务发展轨迹, 即勾勒出由决策或变化导致的业务走向;数据层面需要体现数据的动态演化情况, 即对于相关事件进行不确定性动态建模并能够支持智能学习和推断 (如模拟、预测、人工智能等) ;决策层面需要提升前瞻性和风险洞见, 即获得决策情境映现和趋势预判能力.</p>
<br>
<h2 id="大数据驱动范式">大数据驱动范式</h2>
<p>系统化管理理论的产生及其发展, 包括行为理论、决策理论、权变理论和战略管理等理论体系和管理模型的研究[19], 在提炼管理思想、诠释管理模式和指导管理实践方面发挥了重要作用.长期以来, 管理学研究一直以模型驱动范式为领域主流.在模型驱动范式下, 研究者基于观察抽象和理论推演建立概念模型和关联假设, 再借助解析手段 (例如运筹学和博弈论等分析工具) 对模型进行求解和优化, 或利用相关数据 (包括仿真数据、调研数据、观测数据、系统记录数据等) 对假设进行统计检验.此外, 建立在归纳逻辑基础上的扎根理论等研究范式, 传统上强调从文献概括、实地调研、深度访谈中进行定性推演形成理论和认识.</p>
<p>但是, 在大数据背景下, 一些新的挑战正在涌现[20,21].这里, 以传统的行为模型或计量模型 (简称传统模型) 为例.第一, 传统模型基于观察抽象、理论推演以及经验提炼确定变量 (或构念) 组合, 以此构建变量关系和理论假设, 并通过数据实证进行模型检验.然而, 在大数据背景下, 常常需要检验大量的变量组合 (如指数级组合数) , 这就使得逐一构建传统模型并进行检验成为难以完成的任务.第二, 有些重要潜在影响因素和隐变量没有被意识到, 因而没有被考虑到传统模型的变量组合中, 这常常导致传统模型的假设与数据的适配性不强, 模型解释力不高.第三, 虽然知道有些影响因素和变量是重要的, 但是由于这些因素和变量在传统意义上不可测或不可获 (如文本、图像、语音等富媒体数据) , 难以容纳到传统模型变量组合中, 进而造成模型解释力不理想.第四, 当样本数据规模大幅增加时, 对一些变量的显著性检验有效性下降, 可能出现联系缺失或拟合过度等情形.</p>
<p>面对上述挑战, 数据驱动范式的优势不断凸显.概括说来, 数据驱动范式的作用有两个:一是直接发现特定变量关系模式, 形成问题解决方案;二是与模型驱动范式进行补充扩展, 形成融合范式.值得指出的是, 数据驱动范式发现的一类重要关系模式是关联 (association) 及其扩展形式 (如关联规则、层次关联、数量关联、时态关联、类关联、模式关联等) , 并广泛应用到许多领域 (如搜索、推荐、模式识别等) [22].然而, 许多管理决策情形不仅需要关联也需要因果, 这在一定程度上催生了融合范式及其应用.例如, 首先利用数据驱动范式的关联挖掘方法发现变量间的关联, 以缩减变量空间和组合规模;进而利用模型驱动范式的行为方法辨识构念影响路径, 或计量方法解析变量间的因果关系.这是一个“数据驱动+模型驱动”思路, 体现“关联+因果”的诉求, 这对于管理决策尤为重要.这里, 与传统模型相比一个重要区别是, 此时的变量空间中可能存在着一些新颖且潜在的变量及其关联, 在进一步融合运用模型驱动方法构建变量关系时存在困难, 因为已有的理论知识和领域经验不能直接支持相关的建模逻辑和关系形式.这就需要在更深 (包括间接、潜隐) 层面上探寻新的变量影响机理和理论, 并在方法论上另辟新径 (如通过步进/层次/迭代的试错和启发建模方式) .</p>
<p>特别地, 当数据具有4V等特征并且面对管理决策大数据问题时, 考虑数据驱动与模型驱动的结合、管理决策的关联因果特点、使能创新等元素的一类新型范式 (在此称作大数据驱动范式) 应运而生, 并在深入研究与应用过程中得到进一步发展完善.一般而言, 大数据驱动范式具有“数据驱动+模型驱动”的“关联+因果”性质.具体说来, 大数据驱动范式的框架可从三个角度来审视:外部嵌入、技术增强以及使能创新 (如图3所示) .前两个角度主要涉及方法论层面, 后一个角度主要涉及价值创造层面.</p>

<figure >
    
        <img src="img/3.jpg" width="800" />
    
    
</figure>

<br>
<h3 id="21-外部嵌入">2.1 外部嵌入</h3>
<p>外部嵌入指外部视角引入, 即将传统模型视角之外的一些重要变量 (包括构念、因素等) 引入到模型中.假设自变量集合为X'={x1, x2, …, xm, xm+1, …, xn}, 其中x1, x2, …, xm为传统建模变量, xm+1, …, xn为通过数据驱动方法新引入的变量 (多为富媒体形态) .如果没有变量引入 (n=m) , 传统模型的变量关系是Y=f (X) , X={x1, x2, …, xm}.在跨界关联情境下 (n&gt;m) , 将形成新变量关系Y'=f' (X') .换句话说, Y=f (X) 可以是Y'=f' (X') 的特例;一般意义上讲, X'≠X, f'≠f, Y'≠Y.显然, 新变量关系的构建面临着深刻的挑战, 既有新变量空间的发现, 又有新视角的洞察, 也有新变量关系的辨识和新理论的生成.当然, 对于研究和应用来讲, 这些挑战同时也是创新的机遇.例如, 在金融领域, 可以考虑引入搜索平台上的股票关注数据变量以及社交媒体平台上的相关公共事件数据变量等, 以构建新型股价预测模型;在商务领域, 可以考虑引入购物平台上的评论数据变量以及朋友圈中的体验和口碑数据变量等, 以构建新型商品营销模型;在医疗健康领域, 可以考虑引入院外病友智能检测终端数据变量以及区域环境诱因数据变量等, 以构建新型呼吸疾病预防诊疗模型;在公共管理领域, 可以考虑引入社交平台上的受众意见数据变量以及相关领域联动影响数据变量等, 以构建新型公共政策模型.</p>
<br>
<h3 id="22-技术增强">2.2 技术增强</h3>
<p>对于传统模型来讲, 通过外部嵌入而引入的变量多为富媒体、潜隐性、不可测或不可获, 通常需要利用数据驱动方法和技术.可以说, 数据和技术意识及其能力是大数据背景下研究和应用的核心竞争力, 也是大数据驱动范式的关键要素.技术增强旨在提升这样的能力与要素水平.</p>
<p>从大数据的“用”与“造”视角出发, 技术增强具有两方面含义.一方面, “用”的视角要求管理模型驱动的研究和应用能够增强对外部大数据的敏感性, 引入外部变量并构建其关系;同时, 能够增强对大数据分析技术的敏感性, 构建方法和工具的获取和使用能力.研究和应用创新通常体现在通过新型范式开发新的变量关系, 进而形成新的管理学模型和应用 (如面向管理问题的新型行为模型或计量模型) , 以获得更深入和更具解释力的管理决策洞见和策略.</p>
<p>另一方面, “造”的视角要求数据驱动的研究和应用能够增强对于管理决策问题的敏感性, 构建面向管理决策问题的方法和技术.研究和应用创新通常体现在根据管理决策问题特点及其数据属性开发相关性质、测度和策略, 以获得新颖有效的算法和解决方案.值得指出的是, 这里许多算法 (特别是启发式算法和近似解法) 需要经过实验数据的验证以评估其效率和效果.</p>
<p>多年来, 不管是“用”的视角还是“造”的视角在数据的使用标准上也经历了一个不断升级的过程, 从模拟数据到标杆数据, 再到相当规模的实际数据, 形成一个逐步丰富和叠加的验证实践.在大数据情境下, 实际数据的规模化得到了进一步强化.此外, 在算法比较中, 更关注算法带来的实用效果提升的显著性, 特别在涉及相关用户的场景中, 通常需要进行用户行为实验及其效果感知评测.</p>
<p>在数据类型方面, 富媒体形态 (如文本、图像、音频、视频等) 成为主流.其中, 音频数据、视频数据具有时间连续性特点.由于计算机中通常采用编码、采样等方式表示富媒体数据, 因而数据变换成为大数据分析的重要内容.常用的数据变换方法包括文本处理的向量空间模型 (VSM) [23]、主题模型 (topic model) [24], 图像处理的尺度不变特征转换 (SIFT) [25], 音频处理的短时傅里叶变换 (STFT) [26], 视频处理的时空兴趣点检测 (STIP) [27]等方法.近年来, 随着大数据平台化运算能力的显著提升, 基于深度神经网络的相关方法进一步发展, 并在富媒体数据变换上展现出良好的应用效果和发展前景.例如, 用于文本数据的单词嵌入 (word embedding) [28], 用于图像数据的卷积神经网络 (CNN) [29]和胶囊神经网络 (capsnet) [30], 用于音视频等具有时间序列特征数据的循环神经网络 (RNN) [31]、长短时记忆神经网络 (LSTM) [32]等.其他较新的数据变换方法还包括多层感知机 (MLP) 、自学习编码器 (AE) 、受限制玻尔兹曼机 (RBM) 、深度语义相似模型 (DSSM) 、神经自回归分布估计 (NADE) 、生成对抗网络 (GAN) 等[33,34].</p>
<br>
<h3 id="23-使能创新">2.3 使能创新</h3>
<p>大数据驱动的一个重要含义是大数据使能 (enabling) .大数据能力主要包括大数据战略、大数据基础设施、大数据分析 (6) 方法与技术等.大数据使能是指大数据能力带动的价值创造.例如, 从研究和应用范式角度看, 外部嵌入是一种使能情形, Y'=f' (X') 中, 大数据能力通过自变量X'体现, 创造的价值通过因变量Y'体现, 使能转换方式通过f'体现.从研究和应用情境角度看, 企业的价值创造可以体现在其价值链的环节上, 既包括价值链的主环节及其活动, 也包括价值链的支持环节及其活动[35].在企业内外部大数据环境下, 企业使能创新是通过构建大数据能力, 带动新洞察、新模式、新机会的发现, 进而推动产品/服务创新和商业模式创新, 以实现企业的价值创造 (如图4所示) .</p>

<figure >
    
        <img src="img/4.jpg" width="800" />
    
    
</figure>

<p>综上所述, 大数据驱动范式通过技术增强引入了新视角, 进而推动了新型变量关系、要素机理和理论模型构建, 并提升了大数据使能创新的价值创造.这对于应对新型商务形态的进一步发展机遇和挑战具有重要意义.简单说来, 新型商务可以通过两个阶段予以描述.第一个阶段称作数据商务 (digital business或data-centric business) , 即“数据化+商务分析 (BA) ”.此时通过细化数据粒度使得商务要素的“像素”显著提升, 并在此基础上进行商务分析, 针对不同管理场景和层次进行“成像”和决策.第二个阶段称作算法商务 (algorithmic business) , 即“商务分析+”.此时, 在已有的商务高像素基础上, 成像算法成为关注重点, 旨在获得面对新模式、新业态、新人群[3]的发展策略和竞争优势.这里, “商务分析+”包括BA算法创新和BA使能创新.</p>
<p>近年来, 人工智能 (artificial intelligence, AI) 的研究和应用得到了快速发展, 并受到各界的广泛重视.人工智能自二十世纪50年代以来的发展起起伏伏[36], 虽然在相关思想、模型和方法等方面取得了许多重要进展和成果, 但是由于常常受限于数据基础以及计算能力的不足, 其学习、进化以及推理等方面的能力难以得到发挥, 应用效果也受到影响.直至进入大数据时代, 人工智能的许多成果得到了工程化和产品化实现, 开始在深度和广度上渗透到社会经济活动中, 并引发人们对于未来产业和人类生存的遐想和担忧.机器人和智能产品早期用于替代人类简单重复体力性工作, 现在则可以开始尝试用于替代不少复杂并具有智力的工作, 诸如围棋[37]、翻译[38]、绘画[39,40]、作曲[40]、作诗[41]、无人驾驶[42]、人脸识别[42]、意念控制[43,44]等等.人工智能在管理领域的应用也初见端倪, 比如财务机器人[45]、自动金融交易[42,45]、竞争智能[46]、客户服务[45,47]、人力资源管理[48]、市场营销[42,45]等等.毫无疑问, 人工智能将在新型商务中发挥着越来越重要的角色.另一方面, 伴随着从弱人工智能到强人工智能乃至超人工智能的进阶, 人们对于人工智能应用在隐私和伦理方面的担忧也在不断加重[49].此外, 人工智能理论和技术发展也面临众多挑战 (如“黑盒子”特点、学习机理、语义理解等) , 这些对于强调“关联+因果”的管理决策领域尤为重要.</p>
<p>最后, 管理学是一门融合了“科学”与“艺术”的学科.在大数据背景下, “科学”层面的可测性、程式化和可重复性等要素正在越来越多地被数据和算法表达;而“艺术”层面的情感、心理以及认知等要素也开始被不断“量化”, 包括借助一些感知技术 (affective technologies) (如眼动、脑电技术等) .未来的管理学在探究组织内外“任务”与“人”有机结合的过程中, 数据驱动特征将愈加凸显, 相关范式转变也将进一步深化.</p>
<br>
<h2 id="全景式page框架">全景式PAGE框架</h2>
<p>全景式PAGE框架是融合大数据特征和重要研究方向的要素矩阵, 旨在刻画大数据驱动的“全景式”管理决策框架.全景式PAGE框架具有三个要件:大数据问题特征、PAGE内核、领域情境 (如图5所示) .大数据问题特征涵盖粒度缩放、跨界关联和全局视图, 并作为管理决策背景下的特征视角映射到研究内容方向上.PAGE内核是指四个研究方向, 即理论范式 (paradigm) 、分析技术 (analytics) 、资源治理 (governance) 以及使能创新 (enabling) .领域情境是指针对具体行业/领域 (如商务、金融、医疗健康和公共管理等) 进行集成升华.</p>

<figure >
    
        <img src="img/5.jpg" width="800" />
    
    
</figure>

<p>围绕PAGE内核, 在大数据问题特征映射下可以形成一个4×3的要素矩阵.在理论范式 (P) 研究方向上, 重点关注管理决策范式转变机理与理论.传统的管理决策正在从以管理流程为主的线性范式逐渐向以数据为中心的新型扁平化互动范式转变, 管理决策中各参与方的角色和相关信息流向更趋于多元和交互.概括说来, 新型管理决策范式呈现出大数据驱动的全景式特点.进而, 由于全景式的多维交互动态性以及全要素参与特点, 在研究上需要采用新型的研究范式 (即大数据驱动范式) .具体说来, 在粒度缩放方面, 需要决策要素在宏观和微观层面可测可获;在跨界关联方面, 需要引入外部要素并形成内外要素互动;在全局视图方面, 需要多维整合并能够针对不同决策环境进行情境映现和评估.</p>
<p>在分析技术 (A) 研究方向上, 重点关注管理决策问题导向的大数据分析方法和支撑技术.在粒度缩放方面, 需要数据的感知与采集, 并能够在不同维度和层次上进行分解与聚合;在跨界关联方面, 需要捕捉数据关系及其动态变化, 并能够进行针对多源异构的内外数据融合;在全局视图方面, 需要体系构建和平台计算能力, 并能够形成各类画像以及开展智能应用.</p>
<p>在资源治理 (G) 研究方向上, 重点关注大数据资源治理机制设计与协同管理.在粒度缩放方面, 需要进行资源要素的数据化, 并明确数据标准和权属;在跨界关联方面, 需要刻画资源流通的契约关系, 并形成有效协调共享模式;在全局视图方面, 需要建立资源管理机制, 并制定组织的资源战略.</p>
<p>在使能创新 (E) 研究方向上, 重点关注大数据使能的价值创造与模式创新.在粒度缩放方面, 需要提升业务价值环节的像素, 并把握业务状态;在跨界关联方面, 需要梳理业务逻辑和联系, 并辨识影响业务状态的因果关系;在全局视图方面, 需要提升大数据使能创新能力, 并促进组织发展与价值创造.</p>
<p>围绕领域情境, 可以对PAGE相关研究和应用进行凝练、整合和升华.以NSFC大数据重大研究计划集成平台构建为例, 一般来讲, 集成平台由三个部件组成, 分别是平台体系、内置部件、整合部件.作为简化示例, 对于商务领域集成平台, 平台体系由一个商务管理决策相关的数据池, 以及相应的数据管理和应用管理平台系统 (包括模型、方法、工具库) 等组成;内置部件由针对特定行业 (如汽车) 和特定领域 (如营销) 的研究成果及示范系统组成;整合部件由商务领域内 (不限于内置部件领域) 其它相关项目成果在平台体系框架下经过提炼升华汇集而成.对于金融领域集成平台, 平台体系由一个金融监测预警服务平台, 以及相应的数据管理和应用管理平台系统 (包括模型、方法、工具库) 等组成;内置部件由针对特定行业 (如互联网金融) 和特定领域 (如征信评估、风险预警等) 的研究成果及示范系统组成;整合部件由金融领域内 (不限于内置部件领域) 其它相关项目成果在平台体系框架下经过提炼升华汇集而成.</p>
<br>
<h2 id="结束语">结束语</h2>
<p>面向管理决策研究和应用的大数据驱动范式通过技术增强引入了新视角, 进而推动了新型变量关系、要素机理和理论模型构建, 并提升了大数据使能创新的价值创造.这对于应对新型商务形态的进一步机遇和挑战具有重要意义.此外, 全景式PAGE框架刻画了在粒度缩放、跨界关联和全局视图特征视角映射下的理论范式、分析技术、资源治理、使能创新等重要研究方向.</p>
<p>附注:国家自然科学基金委员会“大数据驱动的管理与决策研究”重大研究计划是一个具有统一目标的项目集群, 旨在充分发挥管理、信息、数理、医学等多学科交叉合作研究的优势, 以全景式PAGE框架作为总体思路框架, 坚持“有限目标、稳定支持、集成升华、跨越发展”的原则, 围绕学科领域趋势、理论应用特点, 注重基础性、前瞻性和交叉性研究创新.自2015年底至2017年底, 此重大研究计划部署了包括培育项目、重点项目和集成项目等一系列项目.其后续的项目部署将在全景式PAGE框架下, 进一步突出凝练、整合与升华, 强调与总体思路框架内容的契合性和贡献度.</p>
<p>本文素材部分来自国家自然科学基金委“大数据驱动的管理与决策研究”重大研究计划相关的系列研讨.由衷感谢不同学科领域专家学者 (包括NSFC大数据重大研究计划指导专家组、顾问专家组、管理工作组等专家学者) 的真知灼见和思想贡献!</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>在会计研究中使用Python进行文本分析</title>
      <link>https://textdata.cn/blog/text_mining_in_accouting_research/</link>
      <pubDate>Fri, 26 Nov 2021 22:40:10 +0600</pubDate>
      
      <guid>/blog/text_mining_in_accouting_research/</guid>
      <description>会计文本分析知识大全</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="img/%e5%b0%81%e9%9d%a2.png" alt=""  />
</p>
<p><img loading="lazy" src="img/%e4%bb%a3%e7%a0%81.png" alt=""  />
</p>
<p>最近在google搜Python在经管中的内容，意外发现<strong>专著： 在会计研究中使用Python进行文本分析</strong>，内容特别新，专著中含有Python代码，也有会计领域文本分析的应用成果。跟 <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">视频专栏课| Python网络爬虫文本分析</a> 结合起来，特别适合会计领域python初学者，将文本分析应用于会计研究中。</p>
<blockquote>
<p>Vic Anand, Khrystyna Bochkay, Roman Chychyla and Andrew Leone (2020 isbn), “Using Python for Text Analysis in Accounting Research (forthcoming)”, Foundations and Trends ® in Accounting: Vol. xx, No. xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.</p>
</blockquote>
<p><a href="http://dx.doi.org/10.1561/1400000062">http://dx.doi.org/10.1561/1400000062</a></p>
<h3 id="摘要">摘要</h3>
<p>会计研究中文本数据的重要性显着增加。为了帮助研究人员理解和使用文本数据，本专著定义和描述了文本数据的常用度量，然后演示了使用 Python 编程语言收集和处理文本数据。该专著充满了示例代码，这些<strong>代码复现了最近研究论文中的文本分析任务</strong>。</p>
<p>在专著的第一部分，我们提供了 Python 入门指南。我们首先描述 Anaconda，它是 Python 的一个发行版，它提供了文本分析所需的库及其安装。然后，我们介绍了 Jupyter notebook，这是一种改进研究工作流程并促进可复制研究的编程环境。接下来，我们将教授 Python 编程的基础知识，并演示使用 Pandas 包中的表格数据的基础知识。</p>
<p>专著的第二部分侧重于会计研究中常用的具体文本分析方法和技术。我们首先介绍<strong>正则表达式</strong>，这是一种用于在文本中查找模式的复杂语言。然后我们将展示<strong>如何使用正则表达式从文本中提取特定部分</strong>。接下来，我们介绍<strong>将文本数据（非结构化数据）转换为表示感兴趣变量（结构化数据）的数值度量的想法</strong>。具体来说，我们介绍了基于字典的方法</p>
<ol>
<li><strong>测量文档情绪</strong>，</li>
<li><strong>计算文本复杂度</strong>，</li>
<li><strong>识别前瞻性句子和风险披露</strong>，</li>
<li><strong>收集文本中的信息量</strong>，以及</li>
<li><strong>计算不同文本片段的相似度</strong>。</li>
</ol>
<p>对于这些任务中的每一个，我们引用相关论文并提供代码片段来实现这些论文中的相关指标。</p>
<p>最后，专著的第三部分侧重于<strong>自动化文本数据的收集</strong>。我们介绍了网页抓取并提供了从 EDGAR 下载文件的代码。</p>
<h3 id="关键词">关键词</h3>
<p><strong>文本分析，数据收集，Python，自然语言处理</strong></p>
<br>
<h2 id="using-python-for-text-analysis-in-accounting-research-forthcoming目录">Using Python for Text Analysis in Accounting Research (forthcoming)目录</h2>
<h3 id="1-引言">1. 引言</h3>
<h3 id="2-在电脑中配置python">2. 在电脑中配置Python</h3>
<ul>
<li>2.1 Python包的作用</li>
<li>2.2 Anaconda软件版本</li>
<li>2.3 安装Anaconda</li>
<li>2.4 Anaconda的使用</li>
</ul>
<h3 id="3--jupyter-notebook">3.  Jupyter Notebook</h3>
<ul>
<li>3.1 案例</li>
<li>JupyterLab: Jupyter Notebook的开发版(最新版)</li>
<li>如何启动JupyterLab</li>
<li>在JupyterLab中写代码</li>
<li>Markdown标记语言与格式化文本代码块</li>
</ul>
<h3 id="4-python编程语言简要介绍">4. Python编程语言简要介绍</h3>
<ul>
<li>4.1 基础知识</li>
<li>4.2 变量与数据类型</li>
<li>4.3 操作</li>
<li>4.4 print函数</li>
<li>4.5 控制流</li>
<li>4.6 函数</li>
<li>4.7 集合类型数据-list、tuple、dictionaries</li>
<li>4.8 处理字符串</li>
</ul>
<h3 id="5-处理表数据-pandas包">5. 处理表数据： Pandas包</h3>
<ul>
<li>5.1 Pandas使用场景</li>
<li>5.2 导入import 声明</li>
<li>5.3 加载数据、导出数据</li>
<li>5.4 在pandas中查看数据</li>
<li>5.5 筛选数据</li>
<li>5.6 创建新列（字段）</li>
<li>5.7 删除列（字段）、列（字段）名重命名</li>
<li>5.8 对数据排序</li>
<li>5.9 合并数据</li>
</ul>
<h3 id="6-正则表达式介绍">6 正则表达式介绍</h3>
<ul>
<li>6.1 查看文本中的模式</li>
<li>6.2 字符与字符集</li>
<li>6.3 Regex的定位与边界</li>
<li>6.4 模式匹配次数限定</li>
<li>6.5 分组</li>
<li>&hellip;</li>
</ul>
<h3 id="7-基于字典法-的文本分析">7. 基于字典法 的文本分析</h3>
<ul>
<li>7.1 字典法文本分析的优势</li>
<li>7.2 理解字典</li>
<li>7.3 识别文本中的词语与句子</li>
<li>7.4 词干化、词形还原</li>
<li>7.5 词语权重</li>
<li>7.6 基于词典法的词频统计函数</li>
</ul>
<h3 id="8-量化文本复杂度">8. 量化文本复杂度</h3>
<ul>
<li>8.1 理解文本复杂度</li>
<li>8.2 计算文本字符长度</li>
<li>8.3 使用Fog指数测量文本可读性</li>
<li>8.4 使用BOG指数测量文本可读性</li>
</ul>
<h3 id="9-句子结构与分类">9. 句子结构与分类</h3>
<ul>
<li>9.1 识别前瞻性陈述forward-looking sentences</li>
<li>9.2 使用字典法做文本分类</li>
<li>9.3 识别句子的主语与宾语</li>
<li>9.4 识别命名实体</li>
<li>9.5 词性标注与命名实体识别任务</li>
</ul>
<h3 id="10-测量文本相似度">10. 测量文本相似度</h3>
<ul>
<li>10.1 使用相似度比较文本</li>
<li>10.2 长文本使用cosine相似度计算相似度</li>
<li>10.3 短文本使用Levenshtein距离计算相似度</li>
<li>10.4 使用word2vec词嵌入计算语义相似度</li>
</ul>
<h3 id="11-识别文本中的具体信息">11. 识别文本中的具体信息</h3>
<ul>
<li>11.1 文本识别与抽取</li>
<li>11.2 案例: 从10-k filing中提取出MD&amp;A</li>
<li>11.3 案例: 从10-k html网页文件中提取处MD&amp;A</li>
<li>11.4 从XBRL金融报告中抽取文本</li>
</ul>
<h3 id="12-从网络中收集数据">12. 从网络中收集数据</h3>
<ul>
<li>12.1 在互联网中采集数据</li>
<li>12.2 证券交易委员会的EDGAR数据</li>
<li>12.3 网络爬虫</li>
<li>12.4 关于api接口</li>
</ul>
<h3 id="致谢">致谢</h3>
<br>
<h3 id="参考文献部分">参考文献(部分)</h3>
<blockquote>
<p>Bentley, J. W., T. E. Christensen, K. H. Gee, and B. C. Whipple. 2018. “Disentangling managers’ and analysts’ non-GAAP reporting”. Journal of Accounting Research. 56(4): 1039–1081.</p>
<p>Blankespoor, E. 2019. “The impact of information processing costs on ﬁrm disclosure choice: Evidence from the XBRL mandate”. Journal of Accounting Research. 57(4): 919–967.</p>
<p>Bochkay, K., R. Chychyla, and D. Nanda. 2019. “Dynamics of CEO disclosure style”. The Accounting Review. 94(4): 103–140.</p>
<p>Bochkay, K., J. Hales, and S. Chava. 2020. “Hyperbole or reality? Investor response to extreme language in earnings conference calls”. The Accounting Review. 95(2): 31–60.</p>
<p>Bochkay, K. and C. B. Levine. 2019. “Using MD&amp;A to improve earnings forecasts”. Journal of Accounting, Auditing &amp; Finance. 34(3): 458482.</p>
<p>Bonsall, S. B., A. J. Leone, B. P. Miller, and K. Rennekamp. 2017. “A plain English measure of ﬁnancial reporting readability”. Journal of Accounting and Economics. 63(2): 329–357.</p>
<p>Bozanic, Z., D. T. Roulstone, and A. Van Buskirk. 2018. “Management earnings forecasts and other forward-looking statements”. Journal of Accounting and Economics. 65(1): 1–20.</p>
<p>Chychyla, R., A. J. Leone, and M. Minutti-Meza. 2019. “Complexity of ﬁnancial reporting standards and accounting expertise”. Journal of Accounting and Economics. 67(1): 226–253.</p>
<p>Gow, I. D., D. F. Larcker, and A. A. Zakolyukina. 2019. “Non-answers during conference calls”. Chicago Booth Research Paper. (19-01). Guay, W., D. Samuels, and D. Taylor. 2016. “Guiding through the Fog:Financial statement complexity and voluntary disclosure”. Journal of Accounting and Economics. 62(2): 234–269.</p>
<p>Heitmann, M., C. Siebert, J. Hartmann, and C. Schamp. 2020. “More Than a Feeling: Benchmarks for Sentiment Analysis Accuracy”. Working Paper, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract">https://papers.ssrn.com/sol3/papers.cfm?abstract</a>_ id=3489963.</p>
</blockquote>
<br>
<h2 id="本书下载">本书下载</h2>
<p><a href="https://github.com/hiDaDeng/DaDengAndHisPython/blob/master/Using_Python_For_Text_Analysis_In_Accounting_Research.pdf">https://github.com/hiDaDeng/DaDengAndHisPython/blob/master/Using_Python_For_Text_Analysis_In_Accounting_Research.pdf</a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本可读性研究及应用清单</title>
      <link>https://textdata.cn/blog/text_readability/</link>
      <pubDate>Wed, 24 Nov 2021 23:40:10 +0600</pubDate>
      
      <guid>/blog/text_readability/</guid>
      <description>京语言大学智能计算机辅助语言学习（ICALL）研究组维护的文本可读性阅读清单。含综述、项目、代码等</description>
      <content:encoded><![CDATA[<p>这是北京语言大学智能计算机辅助语言学习（ICALL）研究组维护的文本可读性阅读清单。</p>
<table>
<thead>
<tr>
<th>目录</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. <a href="#1">综述</a></td>
</tr>
<tr>
<td>2. <a href="#2">相关研究</a></td>
</tr>
<tr>
<td>2.1 <a href="#2.2">中文可读性</a></td>
</tr>
<tr>
<td>2.2 <a href="#2.3">其他语言可读性</a></td>
</tr>
<tr>
<td>3. <a href="#3">可读性分析工具</a></td>
</tr>
<tr>
<td>4 <a href="#4">中文数据</a></td>
</tr>
</tbody>
</table>
<br>
<h2 id="1">1. 综述</h2>
<ul>
<li>
<p>Klare, G. R. (1974–1975). <a href="https://scholar.google.com/scholar_url?url=https://www.jstor.org/stable/747086&amp;hl=zh-TW&amp;sa=T&amp;oi=gsb&amp;ct=res&amp;cd=0&amp;d=6838320539766870596&amp;ei=-1t9Xoq9M8SBywSKyJqgDg&amp;scisig=AAGBfm1iWtmdPfAMXqFhp5eCXdApCr8JfQ">Assessing readability</a>. <em>Reading Research Quarterly</em>.</p>
</li>
<li>
<p>吴思远, 蔡建永, 于东, 江新. 2018. <a href="https://www.researchgate.net/profile/Xin_Jiang26/publication/332834238_A_Survey_on_the_Automatic_Text_Readability_Measureswenbenkeduxingdezidongfenxiyanjiuzongshu/links/5ccc04ca299bf11c2a3d46f3/A-Survey-on-the-Automatic-Text-Readability-Measureswenbenkeduxingdezidongfenxiyanjiuzongshu.pdf">文本可读性的自动分析研究综述</a>. <em>中文信息学报</em>.</p>
</li>
<li>
<p>郭凯、金檀、陆小飞. 2018. <a href="http://www.cnki.com.cn/Article/CJFDTotal-WYCJ201803005.htm">文本难度调控的研究与实践——从可读公式、多维特征到智能改编</a>. <em>外语测试与教学</em>.</p>
</li>
</ul>
<br>
<h2 id="2">2. Related Task</h2>
<h3 id="2.1">2.1 Research on Chinese Readability</h3>
<ul>
<li>
<p>Yao-Ting Sung, Tao Hsing Chang. 2016. <a href="https://link.springer.com/content/pdf/10.3758%2Fs13428-015-0649-1.pdf">CRIE: An automated analyzer for Chinese texts</a>. <em>Behavior Research Methods</em>.</p>
</li>
<li>
<p>Yao-Ting Sung, Weic Lin, SB Dyson, Kuoen Chang. 2015. <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/modl.12213">Leveling L2 Texts Through Readability: Combining Multilevel Linguistic Features with the CEFR</a>. *<em>The Modern Language Journal</em>.</p>
</li>
<li>
<p>LAU Tak Pang. 2006. <a href="https://core.ac.uk/download/pdf/48538871.pdf">Chinese Readability Analysis and its Applications on the Internet</a>. <em>Master&rsquo;s thesis, The Chinese University of Hong Kong</em>.</p>
</li>
<li>
<p>Yu Qiaona. 2016.<a href="https://scholarspace.manoa.hawaii.edu/bitstream/10125/51627/1/2016-12-phd-yu.pdf">Defining and Assessing Chinese Syntactic Complexity via TC-Units</a>. <em>Doctor&rsquo;s thesis, University of Hawaii at Manoa</em>.</p>
</li>
</ul>
<br>
<h3 id="2.2">2.2 Research on Readability in Other Languages</h3>
<ul>
<li>
<p>Arthur C. Graesser, Danielle S. McNamara. 2004. <a href="https://link.springer.com/content/pdf/10.3758%2FBF03195564.pdf">Coh-Metrix: Analysis of text on cohesion and language</a>. <em>Behavior Research Methods, Instruments, &amp; Computers</em>.</p>
</li>
<li>
<p>Arthur C. Graesser, Danielle S. McNamara. 2011. <a href="http://sage.cnpereading.com/paragraph/download/10.3102/0013189X11413260">Coh-Metrix: Providing multilevel analysis of text characteristic</a>. <em>Educational Researcher</em>.</p>
</li>
<li>
<p>Xiaofei Lu. 2010. <a href="https://www.jbe-platform.com/docserver/fulltext/ijcl.15.4.02lu.pdf?expires=1561207415&amp;id=id&amp;accname=jbid110151&amp;checksum=0E423CA22C4B7AAB06AEC4C0359EBEF9">Automatic analysis of syntactic complexity in second language writing</a>. <em>International Journal of Corpus Linguistics</em>.</p>
</li>
<li>
<p>Xiaofei Lu. 2013. <a href="https://s3.amazonaws.com/academia.edu.documents/32693735/Ai_Lu_2013_syntactic_complexity.pdf?response-content-disposition=inline%3B%20filename%3DA_corpus-based_comparison_of_syntactic_c.pdf&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWOWYYGZ2Y53UL3A%2F20190623%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190623T072057Z&amp;X-Amz-Expires=3600&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=ec1c034b7a2f9191914b65ec60cc3d41a8ca932fcf137c45d23a87194977b080">A corpus-based comparison of syntactic complexity in NNS and NS university students’ writing</a>. <em>Automatic Treatment and Analysis of Learner Corpus Data</em></p>
</li>
<li>
<p>陆小飞, 许琪. 2016. <a href="http://www.cnki.com.cn/Article/CJFDTotal-WJYY201603008.htm">二语句法复杂度分析器及其在二语写作研究中的应用</a>. <em>外语教学与研究</em></p>
</li>
<li>
<p>Xiaofei Lu. 2017. <a href="http://sage.cnpereading.com/paragraph/download/10.1177/0265532217710675">Automated measurement of syntactic complexity in corpus-based L2 writing research and implications for writing assessment. Language Testing</a>. <em>Language Testing</em></p>
</li>
<li>
<p>Jin, T., Lu, X., &amp; Ni, J. (2020). <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/modl.12622">Syntactic complexity in adapted teaching materials: Differences among grade levels and implications for benchmarking</a>. <em>The Modern Language Journal</em></p>
</li>
<li>
<p>Menglin Xia ,Ekaterina Kochmar ,Ted Briscoe. 2016. <a href="https://www.aclweb.org/anthology/W16-0502.pdf">Text Readability Assessment for Second Language Learners</a>. <em>Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications</em></p>
</li>
<li>
<p>Xiaobin Chen, Detmar Meurers. 2016. <a href="https://www.aclweb.org/anthology/W16-4113.pdf">CTAP: A Web-Based Tool Supporting Automatic Complexity Analysis</a>. <em>Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity</em>.</p>
</li>
<li>
<p>Chen, X. 2018. <a href="https://publikationen.uni-tuebingen.de/xmlui/bitstream/handle/10900/85888/main.pdf?sequence=1">Automatic Analysis of Linguistic Complexity and Its Application in Language Learning Research</a>, <em>PhD thesis in computational linguistics,  Eberhard Karls Universität Tübingen</em>.</p>
</li>
<li>
<p>Nadezda Okinina, Jennifer-Carmen Frey. CTAP for Italian: Integrating Components for the Analysis of Italian into a Multilingual Linguistic Complexity Analysis Tool.</p>
</li>
<li>
<p>Zarah Weiss, Z. 2017. <a href="https://www.researchgate.net/profile/Zarah_Weiss/publication/334318057_Using_Measures_of_Linguistic_Complexity_to_Assess_German_L2_Proficiency_in_Learner_Corpora_under_Consideration_of_Task-Effects/links/5d24456c299bf1547ca4fe92/Using-Measures-of-Linguistic-Complexity-to-Assess-German-L2-Proficiency-in-Learner-Corpora-under-Consideration-of-Task-Effects.pdf">Using Measures of Linguistic Complexity to Assess German L2 Proficiency in Learner Corpora under Consideration of Task-Effects</a>. <em>M.A. Thesis in Computational Linguistics</em>.</p>
</li>
<li>
<p>Weiss Z, Meurers D. 2019. <a href="http://www.sfs.uni-tuebingen.de/~zweiss/rsrc/Weiss.Meurers-17-LCR-Presentation.pdf">Broad Linguistic Modeling is Beneficial for German L2 Proficiency Assessment</a>. <em>Widening the Scope of Learner Corpus Research, Selected Papers from the Fourth Learner Corpus Research Conference</em>.</p>
</li>
<li>
<p>S Tonelli, KT Manh, E Pianta. 2012. <a href="https://dl.acm.org/doi/pdf/10.5555/2390916.2390924?download=true">Making readability indices readable</a>. <em>Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations</em>.</p>
</li>
<li>
<p>Lijun Feng. 2010. <a href="https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=2964&amp;context=gc_etds">Automatic Readability Assessment</a>. *<em>Doctor&rsquo;s thesis, City University of New York</em>.</p>
</li>
</ul>
<br>
<h2 id="3">3. Readability Analysis Tools</h2>
<ul>
<li>
<p>Lu Xiaofei (2010). <a href="https://www.jbe-platform.com/docserver/fulltext/ijcl.15.4.02lu.pdf?">Automatic analysis of syntactic complexity in second language writing</a>. <em>International Journal of Corpus Linguistics</em>.
(<a href="https://aihaiyang.com/software/l2sca/">Web-based L2 Syntactical Complexity Analyzer (L2SCA)</a>)</p>
</li>
<li>
<p>Yao-Ting Sung, Tao Hsing Chang. 2016. <a href="https://link.springer.com/content/pdf/10.3758%2Fs13428-015-0649-1.pdf">CRIE: An automated analyzer for Chinese texts</a>. <em>Behavior Research Methods</em>.
(<a href="http://www.chinesereadability.net/CRIE/index.aspx?LANG=CHT">文本可读性指标自动化分析系统(Chinese Readability Index Explorer, CRIE)</a>)</p>
</li>
<li>
<p>Arthur C. Graesser, Danielle S. McNamara . 2011. <a href="http://sage.cnpereading.com/paragraph/download/10.3102/0013189X11413260">Coh-Metrix: Providing multilevel analysis of text characteristic</a>. <em>Educational Researcher</em>.
(<a href="http://210.240.188.161/Chinese_CohMetrix/index.html">中文文本自动化分析系统: Coh-Metrix</a>)</p>
</li>
<li>
<p>Xiaobin Chen, Detmar Meurers. 2016. <a href="https://www.aclweb.org/anthology/W16-4113.pdf">CTAP: A Web-Based Tool Supporting Automatic Complexity Analysis</a>. <em>Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC))</em>.
(<a href="http://samos.sfs.uni-tuebingen.de:8080/ctapweb/">CTAP</a>)</p>
</li>
<li>
<p>金檀、陆小飞、郭凯、李百川. 2018. Eng-Editor: An online English text evaluation and adaptation system. 广州：语言数据网(languagedata.net/tester).
( <a href="https://www.languagedata.net/tester/">英语阅读分级指难针</a> )</p>
</li>
</ul>
<br>
<h2 id="4">4. Chinese Data Resources</h2>
<ul>
<li><a href="https://mp.weixin.qq.com/s/VRiNJyILWMwNOAzXJUoKyA">汉语词法难度分级表</a></li>
<li><a href="https://mp.weixin.qq.com/s/IRSqMm75mjoI95VGArW9Jw">汉语句法难度分级表</a></li>
<li>国际汉语教师语法教学手册</li>
<li>国际汉语教师中级语法教学手册</li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理世界 | 使用文本分析&amp;机器学习测量短视主义</title>
      <link>https://textdata.cn/blog/text_mining_in_2021_management_world/</link>
      <pubDate>Tue, 23 Nov 2021 21:33:10 +0600</pubDate>
      
      <guid>/blog/text_mining_in_2021_management_world/</guid>
      <description>本文基于高层梯队理论和社会心理学中的时间导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现，年报 MD&amp;amp;A 中披露的“短期视域” 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越 易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴。The Chief executive officer (CEO) plays a prominent role in ensuring sustainable business growth and enhancing long-term performance. However, not all CEOs have a long-term vision. In this paper, based on the upper echelons and time orientation theory, we investigate whether and how managerial myopia, defined as managers&amp;#39; innate traits to focus on short-term goals, affects their behaviors and decisions. While extant literature extensively centered on the impact of managers&amp;#39; demographic characteristics on long-term investment, little is known about the role of managerial myopia. One reason is the difficulty in coming up with an ex-ante measure for persons&amp;#39; inner and stable time orientation traits. In this paper, based on the textual analytical platform WinGo, we use textual analysis and machine learning technology to measure the managerial myopia. Since the controller of a firm in China is usually the chairman of the board (COB) rather than the CEO, we focus on the COB&amp;#39;s myopia in this study. Through a bat⁃ tery of validations and variance decomposition methods, this study verifies that our proposed textual measure effective⁃ ly captures COBs&amp;#39; innate myopia traits, rather than myopia induced by the external environment.</description>
      <content:encoded><![CDATA[<h2 id="案例文献">案例文献</h2>
<p>胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.</p>
<h2 id="摘要">摘要：</h2>
<p>在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于<strong>高层梯队理论</strong>和社会心理学中的<strong>时间导向理论</strong>，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并<strong>采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验</strong>。研究结果发现，<strong>年报 MD&amp;A 中披露的“短期视域” 语言</strong> 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越 易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。<strong>同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴</strong>。</p>
<h2 id="关键词">关键词：</h2>
<p>管理者短视 长期投资 文本分析 机器学习</p>
<h2 id="变量测量论证">变量测量论证</h2>
<p><strong>语言能够反映人的认知、偏好和个性（Webb et al.，1966），研究者可通过分析实验对象语言中使用的词语类型和词频来捕捉人的特质</strong>（Miller and Ross，1975；Pennebaker et al.，2003）。如一个人的语言中越强调“过去”、“ 曾经”等词汇，反映其越关注过去；一个人的语言中越强调“将来”、“ 可能”、“ 要去”等词汇，反映其越关注未来（Pennebaker et al.，2003）。<strong>基于此研究范式，本文结合已有的英文“短期视域”词集、MD&amp;A 中文语料特点以及 Word2Vec 机器学习制定出能够反映管理者“短期视域”的中文词集，随后通过词典法构建出管理者的短视主义指标。</strong></p>
<p>MD&amp;A 是管理者对报告期内企业经营状况的回顾以及对下一年度经营计划以及企业未来发展所面临的机遇、挑战和各种风险的阐述。已有利用 MD&amp;A 等文本刻画管理者特质的研究成果在一定程度上证实了其可靠性（Li，2012；蒋艳辉、冯楚建，2014）。如</p>
<ul>
<li>Li（2012）利用美国上市公司 MD&amp;A 文本来刻画管理者的 <strong>自我归因偏差</strong>。</li>
<li>蒋艳辉和冯楚建（2014）利用 MD&amp;A 中“我们”、“ 我公司”、“ 我们公司”等词语出现的频率刻画管理者的<strong>自我指涉度</strong>，从而衡量管理层对公司的认知和努力程度。</li>
<li>同时，国外文献表明 CEO 对企业的经营决策起着绝对的主导作用，能够直接影响企业未来的发展方向和命运（Chandler，1962；Finkelstein and Hambrick，1996）。CEO 的特质如自恋程度、学历和任期等都会极大影响公司的信息披露特点（Marquez Illescas et al.，2019；Lewis et al.，2019），因此年报披露的文本信息更多地反映了 CEO 的意思。而在我国，上市公司的董事长更像发达国家的 CEO（姜付秀等，2009；陈传明、孙俊华，2008；李健等，2012）。</li>
</ul>
<p><strong>因此，我们从 MD&amp;A 中捕获的管理者短视主义特质更多反映的是董事长的短视主义特质，本文的管理者指的是企业的董事长。</strong></p>
<h2 id="指标构建过程">指标构建过程</h2>
<p>具体来讲**，管理者短视主义指标**的构建过程如下。</p>
<ol>
<li>借鉴 Brochet 等（2015）的英文“<strong>短期视域</strong>”词集与 Li（2010）构建文本指标的思路，我们阅读了 500 份 MD&amp;A 语料以获取中文文本信息的特点，制定出中文 MD&amp;A 中有关“短期视域”的种子词集，包括直接和间接 两大类。<strong>直接短期视域</strong>大类包括：“ 天内”、“ 数月”、“ 年内”、“ 尽快”、“ 立刻”、“ 马上”；<strong>间接短期视域</strong>大类包括：“ 契机”、“ 之际”、 “压力”、“ 考验”。</li>
<li>针对同一概念或者事物，表达者往往使用多个语义相似的词汇进行描述，因此需要对种子词集进行相似词扩充。本文采用 Word2Vec 中的 CBOW 模型（Continuous Bag-of-words Model）对中文年度财务报告语料进行训练。</li>
<li>我们通过邀请 3 名业界和学术界专家以及对比 MD&amp;A 文本样例对指标词集进行核验，最终确定词集包含 43 个“短期视域”词汇（词集和语句示例详见《管理世界》网络发行版附录 2）。随后，本文基于词典法计算 “短期视域”词汇总词频占 MD&amp;A 总词频的比例，乘以 100 后得到<strong>管理者短视主义指标</strong>。该指标值越大，表明管理者越短视。</li>
</ol>
<h2 id="技术分析">技术分析</h2>
<blockquote>
<p>纯技术讨论，非论文内容</p>
</blockquote>
<p>这篇管理世界的论文，主要难点有两个：</p>
<ol>
<li>
<p>如何构建 <strong>短视主义词典(集)</strong> ？</p>
</li>
<li>
<ul>
<li>根据对研究和数据的了解，<strong>人工摘选</strong>一些 短视主义词典(集)种子词；人工，不需要python编程</li>
<li>使用Word2Vec技术扩充 短视主义词典(集)；需要python编程</li>
</ul>
</li>
<li>
<p>如何使用 <strong>短视主义词典(集)</strong> 计算  <strong>短视主义指标</strong>？</p>
</li>
<li>
<ul>
<li>需要使用Python编程语言，根据 <strong>词典法</strong> 实现短视主义指标的计算。</li>
</ul>
</li>
</ol>
<h2 id="python学习与实现">python学习与实现</h2>
<p>难点主要可在掌握 <strong><a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">视频专栏课| Python网络爬虫与文本分析</a></strong>  后，结合以下两个技能点实现</p>
<ul>
<li>扩充词集可以用到之前分享的<strong>wordexpansion库</strong>  <a href="https://github.com/DataPlusCommunity/wordexpansion">https://github.com/DataPlusCommunity/wordexpansion</a></li>
<li>计算短视主义指标，即词典法可以用到<strong>cnsenti库</strong>  <a href="https://github.com/DataPlusCommunity/cnsenti">https://github.com/DataPlusCommunity/cnsenti</a></li>
</ul>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本分析在市场营销研究中的应用</title>
      <link>https://textdata.cn/blog/text_mining_in_marketing_research/</link>
      <pubDate>Tue, 23 Nov 2021 21:32:10 +0600</pubDate>
      
      <guid>/blog/text_mining_in_marketing_research/</guid>
      <description>语言文字是营销场景中最常用的交互方式，比如在线评论、消费者服务热线、新闻发布、营销传播等活动都创造了有价值的文本数据。但营销研究者如何用好这些数据？本文回顾了文本分析相关研究，并详细介绍了如何用文本数据做市场研究。作者讨论了文本如何**反映**文本生产者， 文本信息如何**影响**信息接受者。接下来，本文讨论了文本如何**预测**并**理解**文本背后的信息，回顾了文本分析的方法和测量指标(metrics),提供了一整套的文本分析操作流程。最后，作者提到文本分析内部信度和外部效度问题，研究者如何解决。本文讨论营销各个领域可能存在的研究机会，虽然目前市场营销的研究问题大都是跨学科的，但是营销的各个子领域经常都是孤立，借助文本分析可能架构起连接营销各个子领域的桥梁。Language and text are the most commonly used interaction methods in marketing scenarios. Activities such as online reviews, consumer service hotlines, press releases, and marketing communications all create valuable text data. But how can marketing researchers make good use of this data? This article reviews the research on text analytics and details how to use text data for market research. The authors discuss how texts *reflect* text producers, and how textual information *influences* information recipients. Next, this article discusses how to **predict** and **understand** the information behind the text, reviews the methods and metrics of text analysis, and provides a complete set of text analysis operation procedures. Finally, the author mentions the problems of internal reliability and external validity of text analysis, and how researchers can solve them. This article discusses the research opportunities that may exist in various fields of marketing. Although the current marketing research issues are mostly interdisciplinary, each subfield of marketing is often isolated. With the help of text analysis, it is possible to build a bridge connecting various subfields of marketing.</description>
      <content:encoded><![CDATA[<br>
<blockquote>
<p>翻译自</p>
<p>Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &ldquo;Uniting the tribes: Using text for marketing insight.&rdquo; Journal of Marketing (2019): 0022242919873106.</p>
</blockquote>
<p>论文作者们的报告视频已上传到B站(下图)，感兴趣的童鞋可以先收藏再收看</p>
<p><a href="https://www.bilibili.com/video/BV1rJ411b7G6"><strong>Jourmal of Marketing Webinar｜2019市场营销</strong></a></p>
<br>
<h2 id="摘要">摘要</h2>
<p>语言文字是营销场景中最常用的交互方式，比如在线评论、消费者服务热线、新闻发布、营销传播等活动都创造了有价值的文本数据。但营销研究者如何用好这些数据？本文回顾了文本分析相关研究，并详细介绍了如何用文本数据做市场研究。作者讨论了文本如何<strong>反映</strong>文本生产者， 文本信息如何<strong>影响</strong>信息接受者。</p>
<p>接下来，本文讨论了文本如何<strong>预测</strong>并<strong>理解</strong>文本背后的信息，回顾了文本分析的方法和测量指标(metrics),提供了一整套的文本分析操作流程。最后，作者提到文本分析内部信度和外部效度问题，研究者如何解决。本文讨论营销各个领域可能存在的研究机会，虽然目前市场营销的研究问题大都是跨学科的，但是营销的各个子领域经常都是孤立，借助文本分析可能架构起连接营销各个子领域的桥梁。</p>
<br>
<h2 id="关键词">关键词</h2>
<ul>
<li>计算语义学coputational linguistics</li>
<li>机器学习machine learning</li>
<li>市场洞察marketing insight</li>
<li>跨学科interdisciplinary</li>
<li>自然语言处理natural language processing</li>
<li>文本分析text analysis</li>
<li>文本挖掘 text mining</li>
</ul>
<br>
<h2 id="无所不在文本">无所不在文本</h2>
<p>交流沟通是营销的重要组成部分，消费者、企业、消费者投资者、社会，不同水平或者统一水平都有信息交流与沟通。而信息交流的过程中往往会产生或者转化为文本数据。</p>
<p>最简单的的文本数据世界模型是<strong>生产者</strong>与<strong>消费者</strong>。模型内生产者和接受者都可能是消费者、企业、投资者和社会。消费者书写在线评论，公司制作会计年报，文化生产者代表社会意义制作出书籍、影片和艺术品（Table 1）</p>
<p>在此情形下，研究者可能选择文本如何反映或如何影响？</p>
<ul>
<li><strong>How text reflects its producer？</strong></li>
<li><strong>How text impacts its receiver？</strong></li>
</ul>
<p>尤其是文本可以反映一定的信息，这些信息是可以帮助营销人员洞察市场规律，进而利用规律影响文本信息的接受者。</p>
<p><img loading="lazy" src="img/textproducerReceiver.png" alt=""  />
</p>
<br>
<h2 id="文本反映生产者">文本反映生产者</h2>
<p>首先，文本可以反映了个人的一些信息。例如“在社交媒体某推特上写着某人谈论着上周他们做了什么。”这句话有很多待挖掘的信息，比如他们这些人什么情况，是内向还是外向、神经质还是严肃认真、他们感觉如何、某时刻他们想了什么(Moon and Kamakura 2017)。总之，文本可以看作指纹或签名(Pennebaker 2011)。</p>
<p>通过文本也可以用于理解领导人、机构或者文化精英。例如领导人用词表达会反映出其领导风格，对利益相关方的态度。透过广告、网站或者消费者服务商(consumer service agent)的言语，人们会了解公司的品牌个性(Opoku, Abratt, and Pitt 2006)，公司是如何看待消费者(Packard and Berger 2019a)，管理层对终端用户的定位(Molner, Prabhu, and Yadav 2019)。年报也会有未来业绩表现的有价值线索(Loughran and McDonald 2016)。</p>
<p>除了单独分析个人或组织的言语，也可以对多个内容生产者合并起来进行更大层面的研究。透过人群或组织产生的文本，我们可以更好理解他们的本质。例如，分析微博，可以得出老年人和年轻人之间如何看待幸福(as excitement vs. peacefulness; Mogilner, Kamvar, and Aaker 2011)。消费者们在品牌社区的言语能更深的投射出消费者对品牌的态度(Homburg, Ehm, and Artz 2015)。</p>
<p>而更宏大的层面，文本也能反映出文化差异。如美国人的表达相比东亚人具有更高的唤醒水平(Tsai 2007)，更喜欢用“我”而不是“我们”，也透露着崇尚个人主义，而不是集体主义。</p>
<p>透过时间，研究者也可以监测美国国民情绪是否在911恐怖袭击前后发生变化(Cohn, Mehl, and Pennebaker 2004)。透过新闻报告、歌词等内容也可以帮助研究者了解社会态度和社会规范，分析有关对女性、少数族裔(Boghrati and Berger 2019; Garg et al. 2018)和特定产业态度的时代变迁(Humphreys 2010)。</p>
<p>虽然文本分析并不容易，但企业和组织可以使用社交网络倾听民声。了解消费者是否喜欢新产品，消费者如何看待品牌，消费者最看重什么(Lee and Bradlow 2011; Netzer et al. 2012)。监管机构可以确定什么药物有不良部反映(Feldman et al. 2015; Netzer et al. 2012),公共卫生部门可以提前了解流感今年爆发最严重的地区(Alessa and Faezipour 2018),投资者可以预测股价涨跌 (Bollen, Mao, and Zeng 2011; Tirunillai and Tellis 2012).</p>
<br>
<h2 id="文本作用于消费者">文本作用于消费者</h2>
<p>文本不止可以反映生产者的信息，也可以知道文本如何影响消费者，消费者会有什么样的行为和选择。广告会塑造消费者的消费行为(Stewart and Furse 1986),报纸用语会改变消费者的态度(Humphreys and LaTour 2013), 消费者杂志会扭曲消费者产品分类感知(e.g., Rosa et al. 1999),电影剧本会影响观众的反应(Berger, Kim, and Meyer 2019; Eliashberg, Hui, and Zhang 2014; Reagan et al. 2016),等等。</p>
<p>需要注意的是文本的<strong>反映reflects</strong>和<strong>影响impacts</strong>并不是非此即彼，往往会同时起作用，尽管如此，研究人员倾向于使用<strong>文本差异</strong>来研究它俩。</p>
<p>当研究文本的<strong>reflects</strong>时，倾向于将reflects当作<strong>因变量</strong>，试图挖掘文本生产者的个性personality或属于什么社会团体。</p>
<p>当研究文本的<strong>impacts</strong>时，倾向于将impacts看作<strong>自变量</strong>，检验文本是否以及如何导致消费者诸如购买、分享和卷入行为。在本框架中，文本信息潜藏着某些潜在的影响力，是被当作诱因，对后续或者其他主体有作用力的。</p>
<h4 id="文本内容也会被客观条件影响"><strong>文本内容也会被客观条件影响</strong></h4>
<p>文本内容还可以被客观条件所塑造，如</p>
<ul>
<li>技术限制和社会文化基因(社会规范)</li>
<li>文本信息生产者与消费者之间的领域知识</li>
<li>先前客观历史</li>
</ul>
<p>首先，不同题材因社会规范，表达内容和方式有所不同。例如观点陈述时，新闻不如报告来的客观(Ljung 2000).酒店评论卡和其他反馈主要被极端观点占据。在Snapchat和其他SNS平台的推文达多较短，且昙花一现；而自在线评论经常较长且可以回溯到多年以前。</p>
<p>技术和物理也会改变文本表达。推特只能发少于280字符的推文。移动电话在键入方面受到限制，并且可能会影响人们在其上产生的文本（Melumad，Inman和Pham 2019; Ransbotham，Lurie和Liu 2019）。</p>
<p>其次，信息生产者和消费者之间的关系会影响说什么，怎么说。当生产者和消费者彼此很熟悉，文本表达会更非正式(Goffman 1959)，导致第三方很难通过直接明确的信息了解生产者与消费者之间的对话的态度。</p>
<p>这些因素对于解读文本信息至关重要，消费者给好朋友分享什么往往跟其他不同。企业可能会因为特定的冬季，其年报中可能会含有利好市场的信息。</p>
<p>最后，历史可能也会影响文本的内容。在留言板上，以前的帖子可能会影响以后的帖子；如果有人在先前的帖子中提出了要点，则被访者很可能会在以后的帖子中提及该要点。如果转发的帖子含有自己的分析，其内容会偏离大多数的帖子。更广泛地说，＃metoo或#blacklivesmatter之类的媒体框架可能使某些概念或事实更容易被演讲者使用，因此即使看起来似乎无关，它们也更可能出现在文本中（McCombs&amp;Shaw 1972; Xiong，Cho&amp;Boatwright 2019）。</p>
<br>
<h2 id="使用文本预测与理解">使用文本预测与理解</h2>
<p>文本除了<strong>reflects</strong> 和 <strong>impacts</strong>之外，还有<strong>predict</strong>和<strong>understanding</strong>。</p>
<h4 id="预测">预测</h4>
<p>某些文本研究出发点是做预测</p>
<ul>
<li>什么消费者最喜欢贷款(Netzer, Lemaire, and Herzenstein 2019)?</li>
<li>什么电影会大火(Eliashberg et al. 2014)?</li>
<li>未来股市走向(Bollen, Mao, and Zeng 2011; Tirunillai and Tellis 2012)?</li>
</ul>
<p>类似上面的研究，会使用很多文本特征来做机器学习和预测，研究人员不怎么关系任意的文本特质，他们更关心预测的表现。</p>
<p>用文本做预测的主要难点是，文本数据可以生成成千上万的特征(相当于变量x1，x2&hellip;xn)，而文本数据记录数甚至可能少于特征数。为了解决这个为题，使用新的特征分类方法，减少特征数量，又有可能存在拟合问题。</p>
<h4 id="理解">理解</h4>
<p>预测之外的研究主要是理解文本</p>
<ul>
<li>消费者怎样表达会如何影响口碑(Packard and Berger 2017)?</li>
<li>为何某些推文会被挑中分享？</li>
<li>歌曲为何变火？</li>
<li>品牌如何让消费者忠诚？</li>
</ul>
<p>理解的目标是理解为什么事情发生以及如何发生的。这类研究往往会用到心理学、社会学的方法，旨在理解文本的什么特征会导致什么后续结果，以及为什么产生这样的后果。</p>
<p>用文本做理解的难点是找出观测数据背后的因果关系。相应的，该领域的工作可能会强调实验数据，以允许对关键的独立变量进行操作。另一个挑战是解释文本特征之间的关系。使用第二人称的歌曲往往较火(Packard and Berger 2019b),但是为什么使用第二人称会火，单纯的文本数据很难挖掘出来作用机制。</p>
<p><strong>在prediction领域</strong>，研究人员利用 <strong>文本的reflects方面</strong> 来预测 生产者的状态、特性、满意度、性格等。研究人员利用 *文本impacts方面 * 来预测 消费者的阅读、分享和购买行为。</p>
<p><strong>在understand领域</strong>，研究人员利用 <strong>文本的reflects方面</strong> 来理解为什么当人们压抑的时候会使用特殊人称。利用 *文本impacts方面 * 来理解为何带有情绪的文本会更容易被阅读和分享。</p>
<br>
<h2 id="粘合营销各领域">粘合营销各领域</h2>
<p>尽管有reflects vs impacts， prediction vs understanding之分，做文本分析需要整合多种技能·技术和不同营销领域的相关知识。</p>
<p>就拿消费者行为学来说，在行为经济学大放异彩之前，假设情景操纵是存在争议的。实验可重复性问题，研究者开始寻找试图增强信度、效度的新工具。使用二手数据经常受限于只能做“是什么”的研究，不能做“为什么”的研究。但文本数据提供了做为什么的可能。例如在线评论可以用来理解为何某人购买了此商品的决策，尽管人们可能并不总是知道为什么要做某事，但他们的语言常常提供解释的痕迹（Pennebaker 2011），甚至超出了他们有意识地表达的范围。</p>
<p>定量建模人员一直在寻找新的数据源和工具来解释和预测行为。非结构化数据提供了一组丰富的预测变量，这些预测变量通常可以随时大规模获得，并且可以与结构化度量一起作为因变量或自变量组合。通过产品评论，用户驱动的社交媒体活动以及公司驱动的营销活动，文本可以实时提供可以阐明消费者需求/偏好的数据。这提供了对传统营销研究工具的替代或补充。在许多情况下，文本可以追溯到个人，从而可以区分个人差异和动态。</p>
<p>营销策略研究人员希望企业能实现其营销目标，并更好地理解影响组织成功的因素。文本分析提供了一种客观而系统的解决方案，以评估可能更有效的自然数据（例如，致股东的信，新闻稿，专利文本，营销信息，与分析师的电话会议）中可能的因素，如了解客户、合作伙伴和员工关系性质以及品牌情感强度(Kubbler，Colicev和Pauwels2017）使用词典和支持向量机方法来提取情绪并将其与消费者心态指标相关联。</p>
<p>也有学者借鉴人口和社会学领域，使用定性和内容分析研究文本数据。消费者文化领域，研究者对字里行间的意义、规范和价值观更感兴趣。文本分析提供了事物变化或比较不同事物的量化指标。文本分析为营销学者解锁了非结构化数据的开锁姿势，提供了文本的定性与定量研究的新疆界。</p>
<br>
<h2 id="文本分析工具方法和指标">文本分析工具、方法和指标</h2>
<p>给予前任做的文本数据驱动的洞察，有学者可能好奇如何开启文本研究之路。在本节会评述文本分析相关研究，包括</p>
<ul>
<li>构念如何用文本数据构建</li>
<li>将提取的文本信息整合到后续建模和分析中所需的过程</li>
</ul>
<p>本节目的是提供综合的入门指导，而是把可用的技术路线留给各位</p>
<ul>
<li>讨论各种方法如何恰当的使用</li>
<li>各种方法在使用时应该注意什么</li>
</ul>
<p>文本处理分析包括的步骤有</p>
<ol>
<li>数据预处理</li>
<li>文本信息提取</li>
<li>常用的文本分析指标</li>
</ol>
<br>
<h2 id="数据预处理">数据预处理</h2>
<p>文本数据是非结构化的脏数据。在任何常规数据分析之前，都要先将文本数据预先清洗处理，进而产生出类似excel表的干净的数据。常用的工具有R语言和Python语言，两种编程语言都有一套易用的数据预处理包。使用某些软件，如Linguistic Inquiry and Word Count (LIWC; Tausczik and Pennebaker 2010) 和WordStat (Peladeau 2016)之前，文本数据需要做少量的预处理。预处理可见Table 2和 Table 3 。</p>
<p><img loading="lazy" src="img/workflow.png" alt=""  />
</p>
<p><img loading="lazy" src="img/workflow2.png" alt=""  />
</p>
<h3 id="1-数据获取">1. 数据获取</h3>
<p>巧妇难为无米之炊，做文本研究的第一步就是采集数据，文本存在于邮件、公司年报、在线评论之中，无所不在，浩瀚无比。可以用人工手动复制粘贴到excel之中，但是效率太低，我们可以使用python设计网络爬虫采集数据。常见访问库requests、数据解析库pyquery和BeautifulSoup、数据存储库csv。</p>
<h3 id="2-分词">2. 分词</h3>
<p>将文本分词(切词)，数据尺度从章节段落拆解成颗粒度更小的词语层面，方便进行分析。但是要注意，英文是用空格间隔词语，而中文没有空格，还要注意粒度分的不能太细，如“the U. S.”按照空格分词会分出“the”、“U.” 和“S.”，导致美国这个实体被切分消失。</p>
<h3 id="3-清洗">3. 清洗</h3>
<p>网络爬虫在采集数据阶段，采集的并不是干净的文本数据，还有一些像HTML标签、图片、链接等字符，需要采集时清除掉。</p>
<h3 id="4-剔除停止词">4. 剔除停止词</h3>
<p>文本中有很多经常出现的无意义或者意义微乎其微的词，如&quot;a&quot;、the&quot;、&ldquo;is&rdquo; 等。一般情况下，这些词是需要剔除的。但是当研究的是书写者的语言风格，这些无意义词语往往含有千丝万缕的写作习惯信息，所以此时不能剔除。(e.g., Packard, Moore, and McFerran 2018；Pennebaker 2011).</p>
<h3 id="5-拼写">5. 拼写</h3>
<p>一般情况下，还需要将错误书写的词正确修改过来。但是当研究者对错误率感兴趣的时候，这时候就不要更正拼写问题。(e.g., Netzer, Lemaire, and Herzenstein 2019).</p>
<h3 id="6-词干化">6. 词干化</h3>
<p>词干化是为了将相同或者相近意思的词合并为一个词，如“car” ` “cars” 统一识别为 “car,”</p>
<h2 id="文本信息提取">文本信息提取</h2>
<p><img loading="lazy" src="img/%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e6%8f%90%e5%8f%96.png" alt=""  />
</p>
<h3 id="1-命名实体抽取">1. 命名实体抽取</h3>
<p>这是文本分析最基础、最简单、最常用的部分。例如姓名、地址、品牌、产品属性、情绪、词性等等都可以看作一种实体信息。实体抽取可以用来</p>
<ul>
<li>监测啥叫媒体讨论，商业竞争情报</li>
<li>也可用作机器学习中的特征（预测指标），预测是否是欺诈信息</li>
<li>构建更复杂的文本表达方式的度量指标，如情感、情绪、写作风格</li>
</ul>
<p>这部分一般需要强大的编程语言，如Python和R；当然有些情况下不用编程，使用WordStat也能做实体抽取。大多数情况下实体抽取经常伴随着专业词典或词表的使用，如(概念、品牌、分类、地址等)。通用的词典包括LIWC(Pennebaker et al. 2015)， EL 2.0 (Rocklage, Rucker, and Nordgren 2018), Diction 5.0 或General Inquirer for psychological states and traits (Berger and Milkman [2012]; Ludwig et al. [2013]; Netzer, Lemaire, and Herzenstein [2019]).</p>
<p>情感词典，如Hedonometer (Dodds et al. 2011), VADER (Hutto and Gilbert 2014), 和LIWC能计算出文本中含有的情感信息。情感分析经常使用词袋法（Bag of Words）计算文本中的情感。但是该方法不考虑词语在文本中的顺序，而顺序是能影响情感信息的。尽管词典法对构建构念和比较构念比较简单，但基于人工编码的机器学习方法(e.g.,Borah and Tellis 2016; Hartmann et al. 2018; Hennig-Thurau, Wiertz, and Feldhaus 2015)更适合做精准概念的度量(Hartmannetal.2019)，尤其是这个领域是不常见或者比较复杂。</p>
<p>如果研究者想挖掘出实体之间的关系就用到word2vec或者词嵌入word embedding (Mikolov et al. 2013)，这两种方法都把每一个词分配一个长度固定的向量，我们知道向量可以在空间中比较，如cos余弦计算词语之间的相似度。</p>
<h3 id="2-话题模型">2. 话题模型</h3>
<p>实体抽取有两个大问题:</p>
<ul>
<li>维度太高，经常能从文本数据中抽取出数千个实体</li>
<li>实体的解读与解释</li>
</ul>
<p>话题模型更多的是对文本的解释，而非预测(e.g., Berger and Packard 2018; Tirunillai and Tellis 2014)。话题模型最常见的是LDA，某个词以一定的概率属于话题，文本以多种话题按照一定的概率分布。</p>
<p>LDA是无监督学习，需要事先指定话题数，输出的结果是不同的类分布，需要研究者解读每一个话题到底是什么题材内容。话题区间范围一般建议结合统计分布和研究者经验确定话题数目。</p>
<h3 id="3-关系抽取">3. 关系抽取</h3>
<p>关系抽取可以用实体共现性来捕捉(e.g., Boghrati and Berger 2019; Netzer et al. 2012; Toubia and Netzer 2017).但营销学者对诸如产品、属性和情感之间的关系感兴趣。例如，研究者对评论中是否提及某个产品属性的问题。Feldman et al. (2015) and Netzer et al. (2012) 提供了药物与不良反应之间的关系来识别问题药物。</p>
<p>关系抽取用的实现大多思路不难，多是一些人工规则的设计，如产品“Ford”、属性“oid consumption”和问题“excessive”共现性来捕捉福特车耗油。然而这样的方法需要手写复杂的规则，现在变得慢慢不流行。</p>
<p>更通用的方法是机器学习法，人工标注相关的数据，训练机器学习模型。这类实现方法需要大量的人工标注，一种可用的工具是Stanford Sentence and Grammatical Dependency Parser (<a href="http://nlp.stanford.edu:8080/parser/">http://nlp.stanford.edu:8080/parser/</a>) 。该工具可以识别词语依存关系，如“the hotel was very nice,” ，“nice” 与 “hotel”相关联，说明这个hotel挺nice的。</p>
<p>当然，也可以扩文本之间做比较，这里不过多赘述。</p>
<br>
<h2 id="文本分析指标">文本分析指标</h2>
<p>早起市场营销，如在线评论领域的文本分析指标多为</p>
<ul>
<li>数量(e.g., Godes and Mayzlin 2004; Moe and Trusov2011)</li>
<li>效价，评论评分t (e.g., Godes and Silva 2012; Moe and Schweidel 2012; Ying, Feinberg and Wedel 2006)·</li>
<li>方差，如信息墒(e.g., Godes and Mayzlin 2004).</li>
</ul>
<p>然而如今这些指标经常忽略了文本的丰富度。以下几种是更好用的指标</p>
<h3 id="1-count-measure">1. count measure</h3>
<p>使用相应的词典，统计实体出现次数，这样可以对不同实体进行比较(Berger and Milkman 2012; Borah and Tellis 2016; Pennebaker et al. 2015; Schweidel and Moe 2014; Tirunillai and Tellis 2014)。缺点是更长的文本通常含有更多的实体(的数量)，还有一个局限就是某些实体会比其他实体更多的出现，如“电脑”商品的在线评论中“电脑”出现次数会远多于其他词。</p>
<h3 id="2-相似度">2. 相似度</h3>
<p>在某些情况下，研究者更对文档之间的相似度感兴趣(e.g., Ludwig et al. 2013).。两个广告之间的相似程度如何？两首歌的歌词相似程度多少？相似度的计算方法有cos余弦相似、jaccard相似 (e.g., Toubia and Netzer 2017)</p>
<h3 id="3-可读性">3. 可读性</h3>
<p>同样的意思可以用不同的难度的词汇去表达，造成阅读的难易程度。可读性反映了作者的内容复杂度和读者的阅读难度。(e.g., Ghose and Ipeirotis 2011)。</p>
<p>常见的可读性算法有Flesch–Kincaid和the simple measure of gobbledygook (SMOG)。可阅读性经常将得分设置到1-12分之间，在美国学校里阅读理解成绩水平得分就是1-12分。</p>
<br>
<h2 id="未来营销研究新机会">未来营销研究新机会</h2>
<h3 id="1-借鉴融合">1. 借鉴融合</h3>
<p>文本分析在营销界中可以起到促进各个子领域交叉授粉，避免同质化学术繁殖。品牌社群是最早被来自社会学背景的研究者发现和研究的(Mun˜iz and O’Guinn 2001)。随后，定性和定量范式研究者逐渐界定了概念、识别了社群中的地位和作用(e.g., Mathwick, Wiertz, and De Ruyter 2007)。文本分析可以让学者研究如何在更大尺度层面去量化社群中的消费者沟通行为。例如，社群中不同权利地位的人使用的语言是否存在差异，使用不同动态指标预测社群产出情况(e.g., Manchanda, Packard, and Pattabhitamaiah 2015)。研究人员也可以追踪到底哪类用户发明新用语，又是哪些人跟随使用这些新用语。研究可以检查人们是否随着时间的开始使用社群语言，并根据他们对群体语言的适应程度来预测哪些人可能会留下或离开(Danescu-Niculescu-Mizil et al. 2013; Srivastava and Goldberg 2017)。定量或机器学习的研究人员可能会发现社群中最常讨论的主题，以及这些主题如何随着社群的发展而动态变化。阐述性范式的研究人员可能会研究这些话语在概念上如何关联，以找到是哪些潜在社区准则促成成员留下。然后，营销战略领域的研究人员可能会使用或开发词典来将这些社区与公司绩效联系起来，并为公司提供有关如何保持不同品牌社区（或环境）成员参与度的指导。</p>
<p>不同子领域的营销学者会使用不同的技能集，研究不同的文本传播类型。消费者与消费者(consumer-to-consumer)之间的沟通主要研究的是两者间的行为，而营销战略学者倾向于研究企业与消费者、企业与企业之间的沟通。不同营销子领域的学者间的合作，能帮助他们结合不同的文本数据源。</p>
<p>它山之石可以攻玉，例如营销战略学者借鉴经济学领域的交易理论(代理理论)来研究企业间的关系，但现在营销战略相关发现可以用于研究消费者之间的沟通行为。</p>
<h3 id="2-扩展文本领域研究">2. 扩展文本领域研究</h3>
<p>我们希望看到更多的消费者-企业间的沟通的研究(e.g., Packard and Berger 2019a; Packard, Moore, and McFerran 2018)，这些沟通经常都是非约束非的，这其中蕴涵着有价值的关系数据，可以有很多应用价值。</p>
<p>而在企业间沟通方面，大多数侧重于沟通(Communication)的角色(e.g., Palmatier, Dant, and Grewal 2007)。然而在文本数据上，在词语层面上，有相关研究很少。例如很少有研究销售人员与消费者之间的信息交换类型。</p>
<p>类似的，在会计金融领域有很多人采用年报作为数据源(for a review, see Loughran and McDonald [2016])，但营销学者很少注意到公司与投资者之间的存在的研究机会。大多数学者只是用来研究如何预测公司股价或者开发新的公司市值估值模型。鉴于最近有兴趣将营销相关活动与公司估值联系起来（例如McCarthy和Fader 2018），这可能是一个需要进一步追求的领域。公司的所有沟通，包括年度报告等必需的文件，或广告和销售互动等任意形式的沟通，都可以用做观测变量，例如市场定位，营销能力，营销领导风格，甚至公司的品牌个性。</p>
<p>在消费者、企业、社会之间也存在着大量的研究机会。有关企业文化(规范)的数据，例如新闻媒体和政府报告，可能有助于阐明影响市场的力量。例如，要了解Uber这样的公司如何抵抗市场变化，可以研究市政厅会议的笔录和其他听取并回答市民意见的政府文件。诸如#metoo和#blacklivesmatter之类的社会运动形式的外来冲击影响了营销传播和品牌形象。未来研究的一种潜在途径是采用文化品牌化方法（Holt，2016年），研究不同公众如何定义，塑造和倡导市场中的特定含义。公司及其品牌并不是凭空存在的，它们独立于其经营所在的社会。但是，在市场营销方面的有限研究已经考虑了如何使用文本在社会层面上得出公司的意图和行为。例如，学者们展示了诸如locavores（这类人只食用当地产的食品；Thompson和Coskuner-Balli，2007年），时尚达人（Scaraboto和Fischer，2012年）以及博主（McQuarrie，Miller和Phillips，2012年），这几类人群塑造了市场。通过文本分析，可以衡量和更好地理解这些社会群体的意图对市场的影响。</p>
<p>未来研究的另一个机会是使用文本数据来研究文化和文化成功。跨学科研究了文化传播，艺术变革和创新传播等主题，目的是理解某些产品为何成功而其他产品却失败的原因(Bass 1969; Boyd and Richerson 1986; Cavalli-Sforza and Feldman 1981; Rogers 1995; Salganik, Dodds, and Watts 2006; Simonton 1980). While success may be random (Bielby and Bielby 1994; Hirsch 1972),可能的原因是没把握好消费者的口味偏好 (Berger and Heath 2005)。</p>
<p>通过在大范围更快速度地量化书籍、电影或其他文化物品，研究人员可以测量具体的叙事是否更具吸引力，更具情感波动性的电影是否更成功，使用某些语言特征的歌曲是否更有可能登上广告牌榜首 ，以及唤起特定情感的书籍是否售出更多。尽管没有像社交媒体数据那样广泛可用，但最近越来越多的文化项目数据可用。诸如Google Books语料库（Akpinar&amp;Berger 2015），歌曲歌词网站或电影脚本数据库等数据集可提供大量信息。此类数据可以使叙事结构分析，以识别&quot;基本情节&quot;'(Reagan et al 2016; Van Laer et al2019）。</p>
<h3 id="3-用文本测量关键构念">3. 用文本测量关键构念</h3>
<p>在个体层面上，情感和满意度可能是最常用的测量变量(e.g., Bu¨schken and Allenby, 2016; Homburg, Ehm, and Artz 2015; Herhausen et al. 2019; Ma, Baohung, and Kekre 2015; Schweidel and Moe 2014)其他从文本数据中提取的测量变量包括语言的真实性authenticity和情绪性emotion(e.g., Mogilner, Kamvar, and Aaker 2011; Van Laer et al. 2019)。也有心理学测量变量，如性格类型presonality type和建构水平construal level(Kern et al. 2016; Snefjella and Kuperman 2015),这都是潜在的可以借鉴应用到消费者话语研究的。</p>
<p>未来个体层面的研究会考虑社会认同和社会参与度， 研究人员目前对消费者已经可以测量情绪的积极或消极，但他们才刚刚开始探索重点（Rocklage&amp;Fazio 2015），信任，承诺和其他模式属性。为此，利用语用学的语言理论并研究语义学上的阶段性可能是有用的（Villarroel et al2017）。一旦开展了此类工作，我们建议研究人员仔细验证建议的方法，以按照上述方法测量此类构念。</p>
<p>在公司层面，已在公司生产的文本（例如年度报告和新闻稿）中确定了一些构念。诸如市场定位、广告目标、未来定位、欺骗意图、公司重点和创新定位均已使用此材料进行了测量和验证（详见Web Appendix Table 1)。未来企业层面的营销研究需要重新界定和丰富战略定位的测量(创新定位、市场驱动vs市场驱动定位)。组织文化、结构和能力由于难于测量，可以从企业、雇员和外部利益相关者的文本数据来测量(see Molner, Prabhu, and Yadav [2019])。类似的，企业领导层的思维和管理风格可以从他们怎么说来侦测(see Yadav, Prabhu, and Chandy [2007])。公司的绩效指标可以通过之前的公司相关文本数据进行预测(e.g., Herhausen et al. 2019)。从这个角度看，我们有很多使用数据的新机会。例如，从企业内部员工的相关信息(LinkedIn 和 Glassdoor)可以测量基于员工的品牌价值。最后，企业语言的更多微妙属性，如冲突、歧义、开放性都可以为管理学增加新发现。再比如，使用一些非正式文本数据，如员工邮件记录、销售通话记录或消费者服务中心通话记录。</p>
<p>营销工作较少在社会或文化层面上衡量结构，但这种工作趋向于集中于公司如何适应现有意义和规范的文化结构。例如，制度逻辑和合法性是通过分析媒体文本来衡量的，Berger等人的品牌公众崛起也增加了文化中对品牌的讨论（Arvidsson and Caliandro 2016）。在文化层面，营销研究可能会继续关注企业如何适应文化环境，但也可能会关注文化环境如何影响消费者。例如，对文化不确定性，风险，敌意和变化的测量可以理解文化对消费者和企业影响。通过文本衡量开放性和多样性也是适时探索的主题，并且可能会促进测量方面的创新，例如侧重于语言多样性。通过文本分析，也可以更好地理解重要的文化论述，例如围绕债务和信用的语言。与性别和种族有关的语言的测量可能有助于探索多样性和包容性，从而使公司和消费者对来自不同作家的文本做出反应。</p>
<br>
<h2 id="机遇与挑战">机遇与挑战</h2>
<p>本节是从技术角度出发探讨文本分析方法的新机遇与挑战。</p>
<h3 id="1-机遇">1. 机遇</h3>
<p>虽然我们的讨论集中于文本内容，但文本只是非结构化数据的一个示例，而音频，视频和图像则是其他示例。社交媒体帖子通常将文字与图片或视频结合在一起。平面广告通常会在精心构造的视觉效果上覆盖文字。尽管电视广告可能不会在屏幕上包含文本，但它可能具有音频轨道，其中包含与视频同步进行的文本。</p>
<p>直到最近，文本数据一直受到最多关注，这主要是由于存在提取有意义特征的工具。也就是说，诸如Praat（Boersma 2001）之类的工具允许研究人员从音频中提取信息（Van Zant和Berger 2019）。音频数据相对于文本数据的优势之一是，它以音调和语音标记的形式提供了丰富的内容，可以添加到所表达的实际单词中（Xiao，Kim和Ding 2013）。这使研究人员不仅可以研究说的内容，还可以研究说的方式，检查音调，语气以及其他声音或副语言特征如何塑造行为。</p>
<p>同样，最近的研究开发了分析图像的方法（Liu，Xuan等人2018），既可以表征图像的内容，也可以识别图像中的特征。文本和图像组合的影响的研究很少（例如Hartmann等人2019）。例如，可以根据图像的颜色来描述图像。在印刷广告的上下文中，当与特定调色板的图像结合使用时，文本内容的说服力可能会降低，而其他调色板可能会增强文本的说服力。与简单的图像结合使用，文本的重要性可能会非常明显。但是，当文本与复杂的图像配对时，观看者可能会主要关注图像，从而减少了文本的影响。在这种情况下，作为广告精美图片一部分的法律披露可能不会引起受众的注意。</p>
<p>当文本加到视频中时，其扮演的角色也引发了类似的问题。研究已经提出了表征视频内容的方法（例如Liu等人2018）。除了包含视频脚本之外，文本还可能在视觉上出现。除了在其中显示文本的音频上下文之外，其影响可能还取决于同时显示的视觉效果。也可能是其在视频中相对于视频开头的位置可能会降低其效果。例如，由于多种原因，在视频中稍后说出的情感性文字内容可能缺乏说服力（例如，观众在讲出文字时可能已经不再注意了）。或者，与音频配对的视觉效果可能对观众更具吸引力，或者视频的先前内容可能耗尽了观众的注意力资源。正如我们对图像和视频的讨论所暗示的那样，文本只是营销传播的一个组成部分。未来的研究必须调查其与其他特征的相互作用，不仅包括其出现的内容，还包括其出现的时间（Kanuri，Chen和Sridhar 2018），以及在哪种媒体上。</p>
<h3 id="2-挑战">2. 挑战</h3>
<p>尽管机会众多，但文本数据也带来了各种挑战。首先是面临可解释性的挑战。在某些方面，文本分析似乎提供了衡量行为过程的更客观的方法。例如，一个人可以计算第一人称“ I”和第二人称“ you”。第一人称在文本中越多，说明这个人更关心自己 （Berger 2014），这种量化词语数量的方法提供看起来更像很客观像真理的东西。但是，尽管该过程的一部分肯定是更客观的（例如，不同类型的代词的数量），但此类度量与基础过程（即，关于口碑传播者的说法）之间的联系仍然需要一定程度的解释。其他潜在的行为方式甚至更难以计数。例如，虽然某些词（例如“love”）通常是积极的，但它们的积极性可能在很大程度上取决于特质个体差异和上下文。</p>
<p>更普遍地，在理解文本信息出现的上下文中存在挑战和机遇。例如，餐厅评论可能包含很多否定词，但这是否意味着该人更讨厌食物，服务或餐厅？包含更多第二人称代词（“ you”）的歌曲可能会更成功（Packard and Berger 2019b），但要了解原因，了解歌词是否使用“ you”作为句子的主语或宾语是有帮助的。上下文提供了含义，而且越多的人不仅了解正在使用的单词，而且还了解如何使用它们，则越容易获得新知识新洞察。基于词典工具特别容易对使用场景变化特别敏感，建议尽可能使用针对特定研究环境创建的词典（例如，Loughran和McDonald [2016]开发的财务情感工具）。</p>
<p>数据隐私挑战是一个重大问题。研究通常使用从网站上抓取的在线产品评论和销售排名数据（Wang，Mai和Chiang 2013）或从社交媒体平台上抓取的消费者的活动数据（Godes和Mayzlin 2004；Tirunillai和Tellis 2012）。尽管这种方法很普遍，但是法律问题已经开始出现。LinkedIn未能成功阻止一家初创公司抓取用户公共资料中发布的数据（Rodriguez，2017）。虽然根据法律可能允许收集公共数据，但它可能与那些拥有研究人员感兴趣的数据的平台的服务条款相冲突。</p>
<p>随着从数字化文本和其他形式的数字化内容（例如图像，视频）中提取见解的兴趣日益浓厚，研究人员应确保他们已获得进行工作的适当权限。不这样做可能导致开展此类项目变得更加困难。一种潜在的解决方案是创建一个学术数据集，例如Yelp提供的数据集（https://www.yelp.com/dataset），该数据集可能包含过时或经过清理的数据，以确保不会产生 公司的运营或用户隐私风险。</p>
<p>对数字化文本以及其他用户创建的内容的收集和分析，也引发了有关用户对隐私的期望的问题。随着欧盟《通用数据保护条例》的发布以及有关Cambridge Analytica从Facebook收集用户数据的能力的启示，研究人员必须注意其工作的潜在滥用。我们还应考虑超出用户生成内容的预期用途的程度。例如，尽管用户可能会理解，Facebook采取的行动可能会导致他们针对与其互动的品牌进行专门的广告宣传，但他们可能无法预期其Facebook和Instagram活动的全部内容都将被用于构建其他品牌可能使用的心理特征。了解消费者关于其在线行为及其提供的文字的隐私偏好可以为从业者和研究人员提供重要的指导。未来研究的另一个亮点是可以提高营销的精确度，同时最大限度地减少对隐私的侵犯（Provost et al 2009）。</p>
<br>
<h2 id="总结">总结</h2>
<p>沟通是营销的重要方面，涵盖组织与合作伙伴之间，企业与消费者之间以及消费者之间的沟通。文本数据包含这些交流的详细信息，并且通过自动文本分析，研究人员已准备好将这种原始材料转换成有价值的见解。文本数据使用方面的许多最新进展是在营销之外的领域开发的。当我们展望未来和营销人员的角色时，这些最新进展应作为示例。营销人员在消费者，公司和组织之间的接口上处于有利位置，可以利用和改进工具来提取文本信息，以解决当今企业和社会所面临的一些关键问题，例如错误的信息滥用。营销提供了一种宝贵的观点，对这次对话至关重要，但这只有通过更广阔的视野，打破理论和方法论的孤岛，并与其他学科合作，我们的研究才能吸引尽可能多的受众来影响公众话语。我们希望这个框架能够鼓励人们对界定营销的界限进行反思，并为未来的突破性见解开辟道路。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本分析在经管领域中的应用概述</title>
      <link>https://textdata.cn/blog/review_about_the_application_of_text_mining_in_management_science/</link>
      <pubDate>Tue, 23 Nov 2021 21:30:10 +0600</pubDate>
      
      <guid>/blog/review_about_the_application_of_text_mining_in_management_science/</guid>
      <description>在大数据的今天，通过互联网超文本链接，无数的**个人、团体、公司、政府**等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。**社会、管理、经济、营销、金融**等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是三份文档翻译汇总而来，我觉得讲的挺明白的，其中加入了我的一点点理解和扩充。In today&amp;#39;s big data world, through Internet hypertext links, countless individuals, groups, companies, governments and other organizational entities are deeply embedded in the Internet world, leaving a large amount of text in the Internet world. **Society, management, economics, marketing, finance** and other disciplines can study a large amount of texts on the Internet, and broaden the research objects and research fields. Most of the content below is a summary of the translations of the three documents. I think it is quite clear, and I have added a little bit of my understanding and expansion.</description>
      <content:encoded><![CDATA[<p>在大数据的今天，通过互联网超文本链接，无数的<strong>个人、团体、公司、政府</strong>等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。<strong>社会、管理、经济、营销、金融</strong>等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是三份文档翻译汇总而来，我觉得讲的挺明白的，其中加入了我的一点点理解和扩充。</p>
<h2 id="一文本产生及其作用方式">一、文本产生及其作用方式</h2>
<ul>
<li>How text <strong>reflects</strong> its producer？</li>
<li>How text <strong>impacts</strong> its receiver？</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">graph LR
   Text_Producer --&gt; Text
   Text --&gt; Text_Receiver
   Text_Receiver --&gt;Text
   Text --&gt; Text_Producer
</code></pre></div><p>文本信息的==生产者producer== 与 ==消费者receiver==，涵盖 ==个人、公司(组织)、国家(社会)==三个层面。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">graph LR
    Consumers --&gt; Firms
    Consumers --&gt; Investors
    Consumers --&gt; Society
    Firms --&gt; Consumers
    Firms --&gt; Investors
    Investors --&gt; Firms
    Investors --&gt; Society
    Firms --&gt; Society
    Society --&gt; Investors
    Society --&gt; Consumers
    
</code></pre></div><p><img loading="lazy" src="img/%e7%94%9f%e4%ba%a7%e4%b8%8e%e6%b6%88%e8%b4%b9.png" alt=""  />
</p>
<blockquote>
<p>需要注意的是文本的==反映Reflects==和==影响Impacts==并不是非此即彼，往往会同时起作用。</p>
</blockquote>
<table>
<thead>
<tr>
<th>&mdash;</th>
<th>研究目的</th>
<th>自变量</th>
<th>因变量</th>
<th>因变量</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Reflects</strong></td>
<td>文本可以反映<strong>producer</strong>的一些信息，帮助研究者理解producer。<br>例如试图挖掘producer的个性personality或属于什么社会团体。</td>
<td>了解公司的品牌个性；<br>年报也会有未来业绩表现的有价值线索；<br>消费者们在品牌社区的言语能更深的投射出消费者对品牌的态度；<br>而更宏大的层面，文本也能反映出文化差异。<br>了解消费者是否喜欢新产品，消费者如何看待品牌，消费者最看重什么</td>
<td>文本</td>
<td>文本</td>
</tr>
<tr>
<td><strong>Affects</strong></td>
<td>知道文本如何影响<strong>receiver</strong>，receiver会有什么样的行为和选择。</td>
<td>检验文本是否以及如何导致消费者诸如购买、分享和卷入行为。<br>广告会塑造消费者的消费行为<br>消费者杂志会扭曲消费者产品分类感知<br>电影剧本会影响观众的反应</td>
<td>文本消费者</td>
<td>文本消费者</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="二如何使用文本数据">二、如何使用文本数据</h2>
<table>
<thead>
<tr>
<th>&mdash;</th>
<th>Reflects</th>
<th>Affects</th>
<th>目的</th>
<th>应用</th>
<th>难点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Predict</strong></td>
<td>预测 <strong>producer</strong>的状态、特性、性格等</td>
<td>预测 <strong>receiver</strong>阅读、分享和购买行为</td>
<td>研究人员不怎么关系任意的文本特质，他们更关心预测的表现。</td>
<td>什么消费者最喜欢贷款;<br>什么电影会大火;<br>未来股市走向;<br></td>
<td>文本数据可以生成成千上万的特征(相当于变量x1，x2&hellip;xn)，而文本数据记录数甚至可能少于特征数。<br>为了解决这个为题，使用新的特征分类方法，减少特征数量，又有可能存在拟合问题。</td>
</tr>
<tr>
<td><strong>Understanding</strong></td>
<td>为什么当人们压抑的时候会使用特殊人称。</td>
<td>来理解为何带有情绪的文本会更容易被阅读和分享</td>
<td>理解为什么事情发生以及如何发生的<br/>这类研究往往会用到心理学、社会学的方法，旨在理解文本的什么特征会导致什么后续结果，以及为什么产生这样的后果。</td>
<td>消费者怎样表达会如何影响口碑;<br>为何某些推文会被挑中分享？<br> 歌曲为何变火？<br> 品牌如何让消费者忠诚？</td>
<td>找出观测数据背后的因果关系。相应的，该领域的工作可能会强调实验数据，以允许对关键的独立变量进行操作。<br>另一个挑战是解释文本特征之间的关系。</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="三文本信息的指标">三、文本信息的指标</h2>
<p>粗略的分，文本信息可以分为定性与定量两种类型</p>
<table>
<thead>
<tr>
<th style="text-align:left">定性/量</th>
<th>分析方法</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>定性（text as text）</strong></td>
<td>质性（扎根）</td>
<td>依靠研究者领域知识，可以对少量的数据做出深刻洞见。</td>
<td>难以应对大规模数据；<br>编码过程并不能保证唯一；</td>
</tr>
<tr>
<td style="text-align:left"><strong>定量 textual data(text as data)</strong></td>
<td>明显的文本特征，如词频、可阅读性</td>
<td>标准如一;<br>适合大规模文本挖掘；<br>纷繁复杂中涌现出潜在规律</td>
<td>需要破坏文本的结构，丧失了部分信息量</td>
</tr>
</tbody>
</table>
<p>早先的营销领域，如在线评论文本分析指标多为</p>
<ul>
<li><strong>数量，如文本长度</strong>(e.g., Godes and Mayzlin 2004; Moe and Trusov2011)</li>
<li>**情感得分(效价，评论评分) **(e.g., Godes and Silva 2012; Moe and Schweidel 2012; Ying, Feinberg and Wedel 2006)·</li>
<li><strong>方差，如信息墒</strong>(e.g., Godes and Mayzlin 2004).</li>
</ul>
<p>然而如今这些指标经常忽略了文本的丰富度。以下几种是更好用的指标</p>
<table>
<thead>
<tr>
<th style="text-align:left">指标</th>
<th style="text-align:left">功能</th>
<th style="text-align:left">补充</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>实体词词频</strong></td>
<td style="text-align:left">使用相应的实体词典，统计实体出现次数，这样可以对不同实体进行比较</td>
<td style="text-align:left">更长的文本通常含有更多的实体(的数量)；<br>还有一个局限就是某些实体会比其他实体更多的出现，如“电脑”商品的在线评论中“电脑”出现次数会远多于其他词。</td>
</tr>
<tr>
<td style="text-align:left"><strong>相似度</strong></td>
<td style="text-align:left">文档之间的相似度感兴趣。<br>如两个广告之间的相似程度如何？<br>两首歌的歌词相似程度多少？</td>
<td style="text-align:left">相似度的计算方法有<br>cos余弦相似<br>jaccard相似</td>
</tr>
<tr>
<td style="text-align:left"><strong>可读性</strong></td>
<td style="text-align:left">同样的意思可以用不同的难度的词汇去表达，造成阅读的难易程度。可读性反映了作者的内容复杂度和读者的阅读难度。</td>
<td style="text-align:left">常见的可读性算法有Flesch–Kincaid和the simple measure of gobbledygook (SMOG)。<br>可阅读性经常将得分设置到1-12分之间，在美国学校里阅读理解成绩水平得分就是1-12分。</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="四文本分析步骤">四、文本分析步骤</h2>
<p><img loading="lazy" src="img/%e5%88%86%e6%9e%90%e6%ad%a5%e9%aa%a4.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>步骤</th>
<th>解释</th>
<th>中文</th>
<th>英文</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>读取数据</strong></td>
<td>数据一般存储于不同的文件夹不同文件内，需要将其导入到计算机</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td><strong>分词</strong></td>
<td>导入到计算的文本是字符串数据，需要整理为更好用的列表</td>
<td>例如“我爱你中国”分词后<br>得到[&ldquo;我&rdquo;, &ldquo;爱&rdquo;, &ldquo;你&rdquo;, &ldquo;中国&rdquo;]</td>
<td>&ldquo;I love China&quot;分为<br>[&ldquo;I&rdquo;, &ldquo;love&rdquo;, &ldquo;China&rdquo;]</td>
</tr>
<tr>
<td>3</td>
<td><strong>剔除符号和无意义的停止词</strong></td>
<td>为了降低计算机运行时间，对分析结果影响较小的字符，诸如符号和无意义的词语需要剔除掉</td>
<td>如“的”，“她”， ”呢”， “了”</td>
<td>&ldquo;is&rdquo; , &ldquo;a&rdquo;, &ldquo;the&rdquo;</td>
</tr>
<tr>
<td>4</td>
<td><strong>字母变小写，词干化</strong></td>
<td>同义词归并，同主体词归并</td>
<td>“中铁”，“中国铁建”，“中铁集团”都可以归并为“中铁”</td>
<td>先变为小写，这样“I”和“i”都归并为“i”；<br>“was”，“are”，“is”都归并为“be”</td>
</tr>
<tr>
<td>5</td>
<td><strong>构建文档词频矩阵</strong></td>
<td>使用一定的编码方式，即用某种方式表示文本。常见的有词袋法、tf-idf；<br>可以使用scikit-learn构建文档词频矩阵，但中英文略有区别，需要注意</td>
<td>“我爱你中国”需要先整理为“我 爱 你 中国”</td>
<td>“I love China”</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="五文本分析技术对比">五、文本分析技术对比</h2>
<p><img loading="lazy" src="img/%e5%88%86%e6%9e%90%e6%96%b9%e6%b3%95.png" alt=""  />
</p>
<p>从左向右，自动化程度越来越高，人工介入的越来越少</p>
<table>
<thead>
<tr>
<th>技术</th>
<th>描述</th>
<th>优点</th>
<th>缺点</th>
<th>常被应用(领域)</th>
<th>软件</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>主题分析</strong>Thematic analysis</td>
<td>需要有经验的人员基于自身经验和李俊杰，对研究的数据进行挖掘。编码过程为迭代进行</td>
<td>使用参与者自己的话语或者构念来挖掘数据，对少量文本理解的更深入</td>
<td>属于时间、劳动密集型任务，不适合大规模数据。<br>由于不同的编码人员有不同的经历和偏好，编码过程的标准不可靠</td>
<td>社会学、管理学</td>
<td>Nvivo；</td>
</tr>
<tr>
<td><strong>内容分析/基于字典方法</strong></td>
<td>统计文本中词语/词组的出现频率</td>
<td>允许对研究的数据进行定量分析</td>
<td>采用的词典应尽量与研究问题适应，词典适配性问题突出</td>
<td>管理学</td>
<td>LIWC、Nvivo、DICTION；</td>
</tr>
<tr>
<td><strong>词袋法</strong>（Bag of words）</td>
<td>将文本字符串转为计算机能理解的数字化向量</td>
<td>编码标准稳定简单，具有统计学特性，扩展性强</td>
<td>编码过程忽略词语的先后顺序</td>
<td>管理学</td>
<td>Python的scikit-learn、gensim、nltk等；R</td>
</tr>
<tr>
<td><strong>监督学习</strong>(Supervise models),如SVM、Bayes、Logistic Regression</td>
<td>研究者要知道输入数据X和标签y；需要核实的模型需要X和y之间的关系和规律</td>
<td>允许事先定义编码规则(如选择词袋法还是tfidf)；逻辑简单</td>
<td>需要高质量的标注数据(工作量大)；you与特征词太多，训练的模型很容易过拟合。</td>
<td>计算机学、政治学、管理学</td>
<td>Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）</td>
</tr>
<tr>
<td><strong>无监督学习</strong>(Kmeans、 LDA话题模型)</td>
<td>使用聚类、话题分析，让计算机自动对数据进行分组</td>
<td>在没有人工标注的情况下，加速了数据的“标注”或“分类”</td>
<td>“标注”是机器按照数字特征进行的分组，需要研究者解读才可以赋予“标准“意义；训练过程需要大量的调参</td>
<td>计算机学、政治学、管留学</td>
<td>Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）</td>
</tr>
<tr>
<td><strong>自然语言处理</strong></td>
<td>按照人类对语言的理解进行建模，考虑词语顺序</td>
<td>计算机自动化；可分析语义</td>
<td>大多数模型是人类无法解读的黑箱；<br>虽然代码编程量小，但训练代码耗时巨大</td>
<td>计算科学；市场营销；心理学</td>
<td>pytorch、tensorflow</td>
</tr>
</tbody>
</table>
<p>据被压缩成词组频数，定性的文本数据转化为定量的频数。本课程中会涉及到的内容</p>
<ul>
<li><input disabled="" type="checkbox"> Thematic Analysis 定性</li>
<li><input disabled="" type="checkbox"> Content Analysis</li>
<li><input checked="" disabled="" type="checkbox"> Dictionary</li>
<li><input checked="" disabled="" type="checkbox"> Bag of words 词袋法</li>
<li><input checked="" disabled="" type="checkbox"> Supervised ，监督学习 文本分类问题</li>
<li><input checked="" disabled="" type="checkbox"> Unsupervised，如非监督LDA话题模型</li>
<li><input disabled="" type="checkbox"> Natural language processing</li>
</ul>
<p><br><br></p>
<h2 id="应用案例">应用案例</h2>
<h3 id="众筹融资成功率与语言风格的说服性-基于kickstarter的实证研究">众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究</h3>
<p>摘要：众筹融资效果决定着众筹平台的兴衰。 众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。 而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。 不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。</p>
<p>首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。</p>
<p>然后，==借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。==</p>
<p>最后，建立语言说服风格对项目筹资影响的计量模型，并 对 ==Kickstarter 平台上的 128345 个项目进行实证分析==。 总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。</p>
<p>关键词：众筹 融资 语言风格 说服性 投资意愿</p>
<p><img loading="lazy" src="img/%e4%bc%97%e7%ad%b9%e8%af%ad%e8%a8%80%e9%a3%8e%e6%a0%bc.png" alt=""  />
</p>
<h3 id="copycats-vs-original-mobile-apps">Copycats vs. Original Mobile Apps</h3>
<p>摘要: 尽管移动应用程序市场的增长为移动应用程序开发人员创新提供了巨大的市场机会和经济诱因，但它也不可避免地刺激了模仿者开发盗版软件。原始应用的从业人员和开发人员声称，模仿者窃取了原始应用的想法和潜在需求，并呼吁应用平台对此类模仿者采取行动。令人惊讶的是，很少有严格的研究来分析模仿者是否以及如何影响原始应用的需求。</p>
<p>==进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原始或模仿者并检测两种模仿者的方法：欺骗性和非欺骗性。==</p>
<p>根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的==5,141个开发人员的10,100个动作游戏应用程序==样本中，模仿应用程序对原始应用程序需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原件的需求。</p>
<p>结果表明，从总体上讲，模仿者对原始移动应用程序需求的影响在统计上是微不足道的。==我们的研究通过提供一种识别模仿者的方法==，并提供模仿者对原始应用需求的影响的证据，为越来越多的移动应用消费文献做出了贡献。</p>
<p><img loading="lazy" src="img/copycat.png" alt=""  />
</p>
<h3 id="lazy-prices">LAZY PRICES</h3>
<p>摘要: 使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对报告进行积极更改时，这种行为蕴含着公司未来运营的重要信号。</p>
<p>财务报告的语言和结构的变化也对公司的未来收益产生重大影响：做空&quot;变化&quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。</p>
<p>研究发现，对10-K的变化可以预测未来的收益、获利能力、未来的新闻公告，甚至未来的公司破产。同时，不做任何变化的公司将获得显著的异常收益。与资产价格典型的反应不足研究不同，我们发现没有任何与这些变化相关的公告效应–仅在后来通过新闻，事件或收益披露信息时才产生回报–暗示投资者并未注意到整个公众领域的这些变化。</p>
<p><img loading="lazy" src="img/lazyprice1.png" alt=""  />
</p>
<blockquote>
<p>纽约时报在2010年4月23日发了一条FDA将有对输液泵(infusion pumps)更严格对审批管理规定的新闻，新闻中提到了Baxter公司。新闻公布当天，Baxter股价大跌。</p>
<p>10天后的（2010年5月4日），Baxter宣布召回问题的输液泵产品，股价当天再次大跌。</p>
</blockquote>
<p><img loading="lazy" src="img/lazyprice2.png" alt=""  />
</p>
<h2 id="相关文献">相关文献</h2>
<blockquote>
<p>[1]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &ldquo;Uniting the tribes: Using text for marketing insight.&rdquo; Journal of Marketing (2019): 0022242919873106.</p>
<p>[2]Kenneth Benoit. July 16, 2019. “Text as Data: An Overview” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. <em>Handbook of Research Methods in Political Science and International Relations</em>. Thousand Oaks: Sage.</p>
<p>[3]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. &ldquo;A review of best practice recommendations for text analysis in R (and a user-friendly app).&rdquo; <em>Journal of Business and Psychology</em> 33, no. 4 (2018): 445-459.</p>
<p>[4]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.管理世界.2016;5:81-98.</p>
<p>[5]Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; <em>Information Systems Research</em> 29, no. 2 (2018): 273-291.</p>
<p>[6]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. &ldquo;Lazy prices.&rdquo; <em>The Journal of Finance</em> 75, no. 3 (2020): 1371-1415.</p>
</blockquote>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>科研绘图SciencePlots库</title>
      <link>https://textdata.cn/blog/scienceplots/</link>
      <pubDate>Tue, 23 Nov 2021 18:40:10 +0600</pubDate>
      
      <guid>/blog/scienceplots/</guid>
      <description>科研可视化绘图包</description>
      <content:encoded><![CDATA[<h2 id="代码下载">代码下载</h2>
<p><a href="https://github.com/hidadeng/DaDengAndHisPython/blob/master/SciencePlot%E7%A7%91%E7%A0%94%E7%BB%98%E5%9B%BE.zip">https://github.com/hidadeng/DaDengAndHisPython/blob/master/SciencePlot科研绘图.zip</a></p>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python">
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">SciencePlots</span>

</code></pre></div><pre><code>Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/
Collecting SciencePlots
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c2/44/7b5c0ecd6f2862671a076425546f86ac540bc48c1a618a82d6faa3b26f58/SciencePlots-1.0.9.tar.gz (10 kB)
  Installing build dependencies ... [?25l/
</code></pre>
<p><strong>tips</strong>:</p>
<p>SciencePlots库需要电脑安装LaTex，其中</p>
<ul>
<li>MacOS电脑安装MacTex  <a href="https://www.tug.org/mactex/">https://www.tug.org/mactex/</a></li>
<li>Windows电脑安装MikTex  <a href="https://miktex.org/">https://miktex.org/</a></li>
</ul>
<h2 id="初始化绘图样式">初始化绘图样式</h2>
<p>在SciencePlots库中科研绘图样式都是用的science</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;science&#39;</span><span class="p">)</span>
</code></pre></div><p>当然你也可以同时设置多个样式</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;ieee&#39;</span><span class="p">])</span>
</code></pre></div><p>在上面的代码中， <strong>ieee</strong> 会覆盖掉 <strong>science</strong> 中的某些参数（列宽、字号等）， 以达到符合 <strong>IEEE</strong>论文的绘图要求</p>
<p>如果要临时使用某种绘图样式，科研使用如下语法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#注意，此处是语法示例，</span>
<span class="c1">#如要运行， 请提前准备好x和y的数据</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;ieee&#39;</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><h2 id="案例">案例</h2>
<p>定义函数曲线， 准备数据</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">**</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span> <span class="o">**</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">))</span>

<span class="n">pparam</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Voltage (mV)&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Current ($\mu$A)&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mi">201</span><span class="p">)</span>
</code></pre></div><h3 id="science样式">science样式</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">]):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Order&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">autoscale</span><span class="p">(</span><span class="n">tight</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="o">**</span><span class="n">pparam</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/fig1.pdf&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/fig1.jpg&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_12_0.png" width="800" />
    
    
</figure>

<h3 id="scienceieee样式">science+ieee样式</h3>
<p>针对IEEE论文准备的<strong>science+ieee</strong>样式</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;ieee&#39;</span><span class="p">]):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Order&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">autoscale</span><span class="p">(</span><span class="n">tight</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="o">**</span><span class="n">pparam</span><span class="p">)</span>
    <span class="c1"># Note: $\mu$ doesn&#39;t work with Times font (used by ieee style)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Current (\textmu A)&#39;</span><span class="p">)</span>  
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/fig2a.pdf&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/fig2a.jpg&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_14_0.png" width="800" />
    
    
</figure>

<h3 id="sciencescatter样式">science+scatter样式</h3>
<p><strong>IEEE</strong> 要求图形以黑白打印时必须可读。 <strong>ieee</strong> 样式还可以将图形宽度设置为适合IEEE论文的一列。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;scatter&#39;</span><span class="p">]):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">],</span>
                    <span class="n">color</span><span class="o">=</span><span class="s1">&#39;dodgerblue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&#34;$^\#$</span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Sample&#39;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">xlbl</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;$\log_</span><span class="si">{10}</span><span class="s2">\left(\frac{L_\mathrm</span><span class="si">{IR}</span><span class="s2">}{\mathrm</span><span class="si">{L}</span><span class="s2">_\odot}\right)$&#34;</span>
    <span class="n">ylbl</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;$\log_</span><span class="si">{10}</span><span class="s2">\left(\frac{L_\mathrm</span><span class="si">{6.2}</span><span class="s2">}{\mathrm</span><span class="si">{L}</span><span class="s2">_\odot}\right)$&#34;</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlbl</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">ylbl</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/fig3.pdf&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/fig3.jpg&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_16_0.png" width="800" />
    
    
</figure>

<h3 id="dark_background-sciencehigh-vis">dark_background +science+high-vis</h3>
<p>您还可以将这些样式与Matplotlib随附的其他样式结合使用。 例如，dark_background +science+high-vis样式：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">([</span><span class="s1">&#39;dark_background&#39;</span><span class="p">,</span> <span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;high-vis&#39;</span><span class="p">]):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Order&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">autoscale</span><span class="p">(</span><span class="n">tight</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="o">**</span><span class="n">pparam</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/fig5.pdf&#39;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/fig5.jpg&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_18_0.png" width="800" />
    
    
</figure>

<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>读完本文你就了解什么是文本分析</title>
      <link>https://textdata.cn/blog/read_this_you_will_know_what_is_text_mining/</link>
      <pubDate>Mon, 22 Nov 2021 23:40:10 +0600</pubDate>
      
      <guid>/blog/read_this_you_will_know_what_is_text_mining/</guid>
      <description>专注经济管理科研领域的Python数据分析，涵盖数据分析主要环节，如Python网络爬虫、Pandas数据探索性分析、中英文文本数据清洗、机器学习与自然语言处理。开发有专门的Python经济管理文本数据挖掘视频课程.Focus on Python data analysis in the field of economic management research, covering the main links of data analysis, such as Python web crawler, Pandas data exploratory analysis, Chinese and English text data cleaning, machine learning and natural language processing. Developed a special Python economic management text data mining video course</description>
      <content:encoded><![CDATA[<h2 id="一文本的作用">一、文本的作用</h2>
<p>文本涉及两个主体，即<strong>文本生产者</strong>和<strong>文本消费者</strong>：</p>
<ul>
<li>文本生产者: 生成文本的主体；传递生产者想表达的内容，可能也会潜在蕴含着生产者的一些特质属性</li>
<li>文本消费者: 阅读文本的主体；消费者阅读这段文本时，文本又对消费者认知活动产生影响。</li>
</ul>
<p>在大数据的今天，通过互联网超文本链接，无数的<strong>个人、团体、公司、政府</strong>等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。<strong>社会、管理、经济、营销、金融</strong>等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是从政治学和经管领域的两份文档翻译来，我觉得讲的挺明白的，其中加入了我的一些理解和扩充。</p>
<p><br><br></p>
<h2 id="二-理解文本">二、 理解文本</h2>
<ul>
<li>text as text 原始的文本，定性的文本</li>
<li>textual data(text as data)  量化后的文本数据，可定量</li>
</ul>
<h3 id="21-text-as-text">2.1 text as text</h3>
<blockquote>
<p>text as text 原始的文本，定性的文本</p>
</blockquote>
<p>文本的重点是传递着某种东西，从某种意义上说，所有形式的文本都包含可以被视为数据形式的信息。因此，文本总是以某种方式提供信息（即使我们不了解如何操作）。但是，言语活动的主要目标不是记录信息，而是进行交流：传达思想，指令，查询等。我们可以记录下来并将其视为数据，但是将我们的想法或思想表达为单词和句子的目的主要是交流，而不是将我们的想法或思想记录为数据形式。大多数数据是这样的：它表征的活动与数据本身完全不同。</p>
<p>例如，在经济学中，可能是我们想要刻画的经济交易（使用价值媒介交换商品或服务），而数据是以某种聚合形式对这些交易进行抽象，这有助于我们理解交易的意义。通过就抽象的相关特征达成共识，我们可以记录并分析人类活动，例如制造业，服务业或农业。从通信行为中提取文本数据特征的过程遵循相同的过程，但有一个主要区别：由于原始文本可以直接通过记录的语言与我们交谈，因此文本首先不需要进行处理或抽象化待分析。但是，<strong>我在这里的论点是，特征抽象的过程是将文本视为数据而不是直接将其视为文本的方法的独特之处</strong>。</p>
<p><strong>具有讽刺意味的是，只有当我们破坏了直接理解文本的能力时，才有可能利用文本的数据获取洞察力</strong>。为了使它作为数据有用，我们必须消除原始文本的结构，将文本转换为结构化的表格数据。定量分析是理解非语言数据的起点；另一方面，非结构的文本变成丑陋表格数据的过程，出于统计分析或机器学习目的，我们经常质疑这一过程丢失了什么信息。</p>
<p>机器是愚蠢的，但是将文本视为数据意味着让愚蠢的机器处理并可能分析我们的文本。关键是，为了<strong>将文本作为数据</strong> 而不是<strong>文本仅仅是文本</strong>，<strong>我们必须破坏原始文本的直接可解释性，但目的是从其样式化特征中进行更系统，更大规模的推断</strong>。我们应该坚定不移地认识到这一过程，但也不要因此而寝食不安，因为<strong>将文本作为数据进行分析的重点永远不是解释数据而是挖掘其深层次的模式</strong>。数据挖掘是一个破坏性的过程-正如采挖矿山资源-为了开采其宝贵资源，开发过程不可避免会破坏地表形态和环境。</p>
<br>
<h3 id="22-latent-versus-manifest-characteristics-from-textual-data">2.2 Latent versus manifest characteristics from textual data</h3>
<blockquote>
<p>textual data(text as data)  量化后的文本数据，可定量的数据。所以小标题我翻译为“量化后的文本数据隐藏的信息vs直观可见的信息”，</p>
</blockquote>
<p>在政治学领域，我们通常最感兴趣的不是文本本身，而是文本透漏给我们有关作者的一些隐藏特性。在政治（以及心理学）研究中，我们有关政治和社会行为者的一些重要理论，很多时候直接观察行为活动很难观察到其内在的品质。</p>
<p>例如，意识形态是研究政治竞争和政治偏好的基础，但是我们没有直接的衡量工具来记录个人或政党有关“社会和道德自由政策与保守政策”的相对偏好。其他偏好，包括支持或反对特定政策，如1846年废除了英国的《玉米法》（Schonhardt-Bailey，2003年）；在关于《莱肯公约》的辩论中支持或反对进一步的欧洲一体化（Benoit等，2005）；再比如支持或反对不信任运动（Laver和Benoit，2002年）。</p>
<p>这些偏好是作为政治行为者的内部状态而存在的，无论这些行为者是立法者，政党，代表还是候选人，都无法直接观察。<strong>非言语行为指标也可用于推断这些信息，但事实表明，政治行为者所说的话比其他行为形式更为真诚。</strong></p>
<p>因此，<strong>文本数据（Textual data）可能包含有关取向和信念的重要信息，对于这些取向和信念，非语言形式的行为可能会充当不良指标。长期以来，心理学领域也一直将言语行为作为可观察到的潜在兴趣状态的暗示，例如人格特质</strong>（例如Tausczik和Pennebaker，2010年）。缺少增强的询问技术或头脑阅读技术来识别政治和社会行为者的偏好，信念，意图，偏见或个性，下一个最佳选择是根据其说话或书写的内容来收集和分析数据。<strong>关注的对象不是文本包含的内容，而是其内容作为有关潜在特征的数据所揭示的内容，这些潜在特征为其提供了可观察的含义</strong>。最后一句话比较难理解，可以理解为万事万物有联系，通过联系思维来挖掘文本中的信息。</p>
<p>文本数据(Textual data)还可能具有较为明显的特征，例如，政治传播的许多领域都与文本所指出的潜在特征无关，而与文本本身所包含的传播形式和性质有关。举一个经典的例子，<strong>在一个著名的政治局委员对斯大林诞辰70周年之际的文章的研究中，莱特斯，伯努特和加索夫（1951）能够衡量各团体在共产主义意识形态方面的差异</strong>。在这一政治事件中，这些信息不仅预示了潜在的方向，而且还预示了在可预见的斯大林死后事件中有关领导权斗争的某种政治动作。这些信息本身是重要的，<strong>这些信息只能从每个政治局委员撰写的公开文章中搜集而来，它们必须充分了解将在党和苏联苏维埃新闻，并由其他政权参与者解释为信号</strong>。再举一个例子，如果我们对一个政治演说家是使用民粹主义还是种族主义语言感兴趣，那么该语言将直接以民粹主义或种族主义术语或参考形式出现在文本中，而要紧的是它们是否被使用。与其说这些术语代表什么，不如说是什么。<strong>例如Jagers和Walgrave（2007）在研究比利时政党的政党政治广播时，发现极右翼政党Vlaams Blok所使用的民粹词语远比其他比利时政党丰富的多。</strong></p>
<p>在实践中，从文本<strong>可观察到的明显特征</strong>与<strong>潜在特征之间的特征</strong>的有时候这两个概念区分的并不明显。举例来说，文体风格可以用一些明显的特征词对文本进行量化，体现出作者的一些写作偏好。例如，在使用适用于政治文本的<strong>可读性度量改编</strong>的研究中，我们可能会对<strong>政治成熟度</strong>的潜在水平感兴趣，这可以用来衡量说话者的意图或<strong>说话者的特征</strong>，这一点从观察到的文本样本中可以看出。或者，我们可能会对它们在可读性上的明显差异感兴趣，这是传播媒介更直接指标。例如，在对英国议会历史演讲的研究中，Spirling（2016）将19世纪末期向简单语言的转变归因于广播扩展特许经营的民主化效应。Benoit，Munger和Spirling（2019）使用类似的措施，比较了同一位总统当天在同一天发表的美国总统国情咨文演讲的样本，但其口头和书面形式均表明口头形式使用的语言较为简单。前一项研究可能对语言的<strong>易用性</strong>感兴趣，该语言的易用性是政治代表制更潜在的特征的指标，而后一项分析可能更侧重于交付媒介的明显后果。<strong>对于许多使用文本数据的研究设计而言，区别更多是研究目标的问题，而不是结构化和分析文本数据的某些内在方式。</strong></p>
<br>
<h3 id="23-文本分析的步骤">2.3 文本分析的步骤</h3>
<p><img loading="lazy" src="img/textprocesssteps.png" alt=""  />
</p>
<p>完整的文本分析步骤包括:</p>
<ol>
<li>读取数据</li>
<li>分词(中文必须有这一步，由于英文是空格间隔的语言，英文有时候不需要分词）</li>
<li>剔除符号和无意义的停止词</li>
<li>字母变小写，词干化</li>
<li>使用一定的编码方式构建文档词频矩阵</li>
</ol>
<table>
<thead>
<tr>
<th>序号</th>
<th>步骤</th>
<th>解释</th>
<th>中文</th>
<th>英文</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>读取数据</strong></td>
<td>数据一般存储于不同的文件夹不同文件内，需要将其导入到计算机</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td><strong>分词</strong></td>
<td>导入到计算的文本是字符串数据，需要整理为更好用的列表</td>
<td>例如“我爱你中国”分词后<br>得到[&ldquo;我&rdquo;, &ldquo;爱&rdquo;, &ldquo;你&rdquo;, &ldquo;中国&rdquo;]</td>
<td>&ldquo;I love China&quot;分为<br>[&ldquo;I&rdquo;, &ldquo;love&rdquo;, &ldquo;China&rdquo;]</td>
</tr>
<tr>
<td>3</td>
<td><strong>剔除符号和无意义的停止词</strong></td>
<td>为了降低计算机运行时间，对分析结果影响较小的字符，诸如符号和无意义的词语需要剔除掉</td>
<td>如“的”，“她”， ”呢”， “了”</td>
<td>&ldquo;is&rdquo; , &ldquo;a&rdquo;, &ldquo;the&rdquo;</td>
</tr>
<tr>
<td>4</td>
<td><strong>字母变小写，词干化</strong></td>
<td>同义词归并，同主体词归并</td>
<td>“中铁”，“中国铁建”，“中铁集团”都可以归并为“中铁”</td>
<td>先变为小写，这样“I”和“i”都归并为“i”；<br>“was”，“are”，“is”都归并为“be”</td>
</tr>
<tr>
<td>5</td>
<td><strong>构建文档词频矩阵</strong></td>
<td>使用一定的编码方式，即用某种方式表示文本。常见的有词袋法、tf-idf；<br>可以使用scikit-learn构建文档词频矩阵，但中英文略有区别，需要注意</td>
<td>“我爱你中国”需要先整理为“我 爱 你 中国”</td>
<td>“I love China”</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="三常见的文本分析技术有">三、常见的文本分析技术有</h2>
<ul>
<li>主题分析(Thematic analysis)</li>
<li>内容分析(content analysis)</li>
<li>基于词典的方法(dictionary analysis)</li>
<li>文本向量化(Bag-of-words)</li>
<li>监督学习如SVM、Bayes和Regression</li>
<li>无监督学习，如LDA话题模型</li>
<li>自然语言处理</li>
</ul>
<p>上述文本分析技术，按照人与机器参与程度，绘制在下图。一般来说，越向右，文本分析技术的自动化程度越高，需要注意的是自动化越高，并不代表人的工作量就越少。</p>
<p><img loading="lazy" src="img/textautomate.png" alt=""  />
</p>
<br>
<h3 id="31-主题分析thematic-analysis">3.1 主题分析Thematic Analysis</h3>
<p>主题分析(Thematic analysis)是一种专家方法，一般与扎根理论方法相结合(Baumer, Mimno, Guha, Quan, &amp; Gay, 2017)。扎根理论与主题分析的理念是基于专家自身经验和对世界的理解，做出对数据的见解，从而构建新理论。主题分析常见于组织科学和传播学(Gioia, Corley, &amp; Hamilton, 2013; Strauss &amp; Corbin, 1998)。</p>
<p>主题分析涉及一个反复迭代的过程，在此过程中，研究人员将开发出一系列源自文本的代码和类别。除非要精炼理论，否则一般在分析开始之前尚不知道类别。在这种情况下，数据分析需要对文献和数据进行不断的比较。</p>
<ol>
<li>研究人员从参与者自己的语言开始（称为“一阶编码”或“开放式编码”；Gioia等人，2013；Strauss＆Corbin，1998）</li>
<li>然后将相似的代码归为一类（称为“二阶代码”或“主轴编码”；Strauss＆Corbin，1998）。</li>
</ol>
<p>诸如NVivo和ATLAS.ti之类的计算机软件可以帮助简化上述过程，但文本的分类通常依赖于人类编码衍生的类别的操作定义，计算机自动化的程度依旧很低，分析的数据量通常不大。而且编码过程对编码者的要求严格，通常是对该领域有较深理解的人才适合做此类工作。</p>
<br>
<h3 id="32-内容分析基于词典的方法法">3.2 内容分析/基于词典的方法法</h3>
<p><strong>内容分析</strong> 和 <strong>其他基于字典的方法</strong> 通常是通过对特定文本中 <strong>单词/词组</strong> 的频率计数进行的（Reinard，2008；Short，Broberg，Cogliser＆Brigham，2010）。因为按照这种方法，文本数据被压缩成词组频数，定性的文本数据转化为定量的频数，索引可用于回答更多以定量为导向的研究问题（McKenny等，2016；Reinard，2008）。</p>
<p>比如进行文本情感分析，我们可以用很简单的思路。即统计文本中正面词出现的总数和负面词出现的总数，得出文本的情感值。而在此分析过程中，我们需要事先拥有一个正面词词典和负面词词典。</p>
<p>是否有成熟的领域词典、或者构建领域词典，这需要研究者对研究问题和研究的数据有一定的领域知识，工作量也会因是否有词典而不同。一般有现成的成熟的词典，计算机自动化程度高，人工工作量低。</p>
<p>与主题分析类似，计算机软件可以协助内容分析过程。像DICTION这样的程序会使用 <strong>分类字典</strong> 自动对文本评分（即，根据单词或n-gram而非操作定义确定主题）。可以与主题分析类似地使用其他程序，例如NVivo或ATLAS.ti，在主题分析中，通过软件的帮助手动进行编码和分类，以组织数据。</p>
<br>
<h3 id="33-词袋法bag-of-words">3.3 词袋法Bag-of-words</h3>
<p><img loading="lazy" src="img/bagofwords.png" alt=""  />
</p>
<p>文本数据是非结构化的定性数据，计算机并不能直接使用。我们需要按照计算机容易理解的方式去组织数据，类似于上图的第一步骤,四段英文文本被组织成一个文档特征矩阵（document-feature-matrix），矩阵中</p>
<ul>
<li>每一行代表一个英文文档</li>
<li>每一个列代表一个特征词</li>
</ul>
<h4 id="331-词袋法-vs-主题分析中的编码者">3.3.1 词袋法 vs 主题分析中的编码者</h4>
<p>为了理解词袋法，可以类比<strong>主题分析</strong> 中的编码者。我们可以将词袋法看做是一个死板的，不知变通的人，脑子很简单，只知道统计特征词在每个文档中出现的词频。那么据此我们就知道词袋法和人的优缺点。</p>
<p>对于词袋法，优点是规则标准统一，缺点是不知变通，牺牲了文本中很多的信息量。强调编码过程的高标准，牺牲了分析的深度。</p>
<p>对于研究者参与 <strong>主题分析</strong> 这样的编码过程，优点是研究者有很强的领域知识和强大的洞察力，可以灵活洞察规律，缺点是每个研究者都具有特殊的经历和偏好，编码标准不统一。用研究者编码的过程，强调编码的深度和质量，牺牲了编码分析过程的标准性。</p>
<h4 id="332-词袋法的用途">3.3.2 词袋法的用途</h4>
<p>词袋法编码是计算科学领域对文本数据的简化和压缩的方法，后续可以据此进行监督学习和无监督学习。</p>
<br>
<h3 id="34-监督学习">3.4 监督学习</h3>
<p>在有监督的方法中，研究人员事先知道ta正在寻找什么（罗伯茨等，2014）。比如要判断论文的作者身份这个问题，研究人员为程序提供输入（在这种情况下为文本）和输出（例如，文本作者的身份），然后系统创建一种算法来映射两者之间的联系（Janasik， Honkela和Bruun，2009年）。Mosteller and Wallace（1963）通过使用简单的贝叶斯单词概率来预测12篇有争议的联邦主义者论文（詹姆斯·麦迪逊或亚历山大·汉密尔顿）的作者身份。如今，朴素贝叶斯（Bayes）和支持向量机（SVM）等技术是用于文本分析的流行的监督算法（Manning，Prabhakar和Hinrich，2008年）。</p>
<br>
<h3 id="35-无监督学习">3.5 无监督学习</h3>
<p>无监督算法，如主题分析（Janasik等，2009）可识别数据中的<strong>单词簇</strong>和<strong>主题</strong>。但是，与<strong>主题分析</strong>不同，<strong>主题建模</strong>使用高度自动化的方法来确定重要主题，分析过程所需的时间和领域知识相对较少。尽管人类的洞察力仍然对帮助解释出现的主题很重要，主题建模适合分析大规模文本数据（Kobayashi1，Mol，Berkers，Kismihok和Den Hartog，2017）。<strong>主题建模利用了主题分析（即人类洞察力、解释力）和机器学习（即快速分析大量文本）的优势</strong>。</p>
<br>
<h3 id="36-自然语言处理">3.6 自然语言处理</h3>
<p>最后，自然语言处理(Natural Language Processing)通常是文本分析中自动化程度最高的形式（有关综述，请参阅Manning等人，2008）。这种方法模拟了人类如何理解和处理语言（Chowdhury，2003；Collobert等，2011；Joshi，1991）。例如，NLP技术可以标记句子中单词的词性（例如，名词，形容词等），将文档从一种语言翻译成另一种语言，甚至使用句子的上下文来阐明词语的词义（Buntine＆Jakulin，2004年）。</p>
<p>因此，与<strong>词袋法</strong>不同，NLP认为单词顺序很重要。当使用训练集时，使用深度学习和多模式（即结合文本和图像）等尖端技术进行情感分析是NLP的一种流行形式（Kouloumpis，Wilson和Moore，2011）。这种特殊的分析将文本的总体态度，情感或观点分类为肯定，否定或中立。</p>
<p>与<strong>主题分析</strong>形成鲜明对比的是，自然语言处理是一个完全计算机自动化的过程，因此几乎不需要人类的理解和或解释（Quinn等人，2010）。此外，相对于需要人工编码（例如，主题分析）的技术，NLP的执行速度非常快，并且比其他方法更具系统性。例如，计算机科学，信息科学，语言学和心理学的研究人员利用NLP作为文本分析工具（Chowdhury，2003年）。</p>
<p>大邓提醒一下，自然语言处理属于人工智能范畴，人工智能技术没有那么神，我们应该将其理解为“人工”+“智能”可能更妥当一些，即数据准备阶段用大量的人工时对数据进行标注，产生训练数据集合。之后借助于计算机的“智能”学习数据集中的规律，因此人工智能脱离了人工标注数据的喂养，只能做很简单的事情，更像是人工智障。</p>
<br>
<h3 id="37-不同文本分析技术汇总对比">3.7 不同文本分析技术汇总对比</h3>
<table>
<thead>
<tr>
<th>技术</th>
<th>描述</th>
<th>优点</th>
<th>缺点</th>
<th>常被应用(领域)</th>
<th>软件</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>主题分析</strong>Thematic analysis</td>
<td>需要有经验的人员基于自身经验和李俊杰，对研究的数据进行挖掘。编码过程为迭代进行</td>
<td>使用参与者自己的话语或者构念来挖掘数据，对少量文本理解的更深入</td>
<td>属于时间、劳动密集型任务，不适合大规模数据。<br>由于不同的编码人员有不同的经历和偏好，编码过程的标准不可靠</td>
<td>社会学、管理学</td>
<td>Nvivo；</td>
</tr>
<tr>
<td><strong>内容分析/基于字典方法</strong></td>
<td>统计文本中词语/词组的出现频率</td>
<td>允许对研究的数据进行定量分析</td>
<td>采用的词典应尽量与研究问题适应，词典适配性问题突出</td>
<td>管理学</td>
<td>LIWC、Nvivo、DICTION；</td>
</tr>
<tr>
<td><strong>词袋法</strong>（Bag of words）</td>
<td>将文本字符串转为计算机能理解的数字化向量</td>
<td>编码标准稳定简单，具有统计学特性，扩展性强</td>
<td>编码过程忽略词语的先后顺序</td>
<td>管理学</td>
<td>Python的scikit-learn、gensim、nltk等；R</td>
</tr>
<tr>
<td><strong>监督学习</strong>(Supervise models),如SVM、Bayes、Logistic Regression</td>
<td>研究者要知道输入数据X和标签y；需要核实的模型需要X和y之间的关系和规律</td>
<td>允许事先定义编码规则(如选择词袋法还是tfidf)；逻辑简单</td>
<td>需要高质量的标注数据(工作量大)；you与特征词太多，训练的模型很容易过拟合。</td>
<td>计算机学、政治学、管理学</td>
<td>Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）</td>
</tr>
<tr>
<td><strong>无监督学习</strong>(Kmeans、 LDA话题模型)</td>
<td>使用聚类、话题分析，让计算机自动对数据进行分组</td>
<td>在没有人工标注的情况下，加速了数据的“标注”或“分类”</td>
<td>“标注”是机器按照数字特征进行的分组，需要研究者解读才可以赋予“标准“意义；训练过程需要大量的调参</td>
<td>计算机学、政治学、管留学</td>
<td>Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）</td>
</tr>
<tr>
<td><strong>自然语言处理</strong></td>
<td>按照人类对语言的理解进行建模，考虑词语顺序</td>
<td>计算机自动化；可分析语义</td>
<td>大多数模型是人类无法解读的黑箱；<br>虽然代码编程量小，但训练代码耗时巨大</td>
<td>计算科学；市场营销；心理学</td>
<td>pytorch、tensorflow</td>
</tr>
</tbody>
</table>
<br>
<h3 id="38-python能做哪些">3.8 Python能做哪些？</h3>
<p>计算机能做的文本分析，Python都能做到，包括</p>
<ul>
<li>基于词典的分析法；如基于词典法的情感计算</li>
<li>词袋法；可以进行文本相似度计算</li>
<li>有监督机器学习；如基于机器学习的情感分析；文本分类</li>
<li>无监督机器学习；lda话题模型对文本进行话题分析</li>
<li>自然语言处理；考虑词语顺序的LSTM</li>
</ul>
<p>除了自然语言处理部分，四种方法在我的<a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">《Python网络爬虫与文本数据分析》</a>视频课程中都有相关的讲解和实战代码</p>
<p><br><br></p>
<h2 id="相关文献">相关文献</h2>
<blockquote>
<p>[1]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &ldquo;Uniting the tribes: Using text for marketing insight.&rdquo; Journal of Marketing (2019): 0022242919873106.</p>
</blockquote>
<blockquote>
<p>[2]Kenneth Benoit. July 16, 2019. “[Text as Data: An Overview](<a href="https://kenbenoit.net/pdfs/28">https://kenbenoit.net/pdfs/28</a> Benoit Text as Data draft 2.pdf).” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. <em>Handbook of Research Methods in Political Science and International Relations</em>. Thousand Oaks: Sage.</p>
</blockquote>
<blockquote>
<p>[3]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. &ldquo;A review of best practice recommendations for text analysis in R (and a user-friendly app).&rdquo; <em>Journal of Business and Psychology</em> 33, no. 4 (2018): 445-459.</p>
</blockquote>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext中文文本分析库 | 值得收藏</title>
      <link>https://textdata.cn/blog/cntext_v_1/</link>
      <pubDate>Mon, 08 Nov 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_v_1/</guid>
      <description>简单好用的中文Python文本分析包</description>
      <content:encoded><![CDATA[<h2 id="cntext">cntext</h2>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><strong>cntext</strong></li>
<li><strong>stats</strong>  文本统计,可读性等</li>
<li><strong>dictionary</strong> 构建词表(典)</li>
<li><strong>sentiment</strong>  情感分析</li>
<li><strong>similarity</strong>   文本相似度</li>
<li><strong>visualization</strong> 可视化，如词云图</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="https://github.com/hidadeng/cntext/tree/main/examples">https://github.com/hidadeng/cntext/tree/main/examples</a></p>
<br>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext==0.9
</code></pre></div><br>
<h2 id="一cntext">一、cntext</h2>
<p>查看cntext基本信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span>

<span class="n">help</span><span class="p">(</span><span class="n">cntext</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nf">description</span> <span class="p">(</span><span class="kn">package</span><span class="p">)</span>
    <span class="nf">dictionary</span> <span class="p">(</span><span class="kn">package</span><span class="p">)</span>
    <span class="nf">sentiment</span> <span class="p">(</span><span class="kn">package</span><span class="p">)</span>
    <span class="nf">similarity</span> <span class="p">(</span><span class="kn">package</span><span class="p">)</span>
    <span class="nf">visualization</span> <span class="p">(</span><span class="kn">package</span><span class="p">)</span>

<span class="nx">DATA</span>
    <span class="nx">ADV_words</span> <span class="p">=</span> <span class="p">[</span><span class="sc">&#39;都&#39;</span><span class="p">,</span> <span class="sc">&#39;全&#39;</span><span class="p">,</span> <span class="sc">&#39;单&#39;</span><span class="p">,</span> <span class="sc">&#39;共&#39;</span><span class="p">,</span> <span class="sc">&#39;光&#39;</span><span class="p">,</span> <span class="sc">&#39;尽&#39;</span><span class="p">,</span> <span class="sc">&#39;净&#39;</span><span class="p">,</span> <span class="sc">&#39;仅&#39;</span><span class="p">,</span> <span class="sc">&#39;就&#39;</span><span class="p">,</span> <span class="sc">&#39;只&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一共</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="o">...</span>
    <span class="nx">CONJ_words</span> <span class="p">=</span> <span class="p">[</span><span class="sc">&#39;乃&#39;</span><span class="p">,</span> <span class="sc">&#39;乍&#39;</span><span class="p">,</span> <span class="sc">&#39;与&#39;</span><span class="p">,</span> <span class="sc">&#39;无&#39;</span><span class="p">,</span> <span class="sc">&#39;且&#39;</span><span class="p">,</span> <span class="sc">&#39;丕&#39;</span><span class="p">,</span> <span class="sc">&#39;为&#39;</span><span class="p">,</span> <span class="sc">&#39;共&#39;</span><span class="p">,</span> <span class="sc">&#39;其&#39;</span><span class="p">,</span> <span class="sc">&#39;况&#39;</span><span class="p">,</span> <span class="sc">&#39;厥&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="o">...</span>
    <span class="nx">DUTIR_Ais</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">sigh</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一命呜呼</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一场春梦</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一场空</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一头跌在菜刀上</span><span class="err">－</span><span class="nx">切肤之痛</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一念之差</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">..</span>
    <span class="nx">DUTIR_Haos</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="mi">1</span><span class="nx">兒巴经</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="mi">3</span><span class="nx">x</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="mi">8</span><span class="nx">错</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">BUCUO</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">Cool毙</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">NB</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">DUTIR_Jings</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="mi">848</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">FT</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">_god</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">yun</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一个骰子掷七点</span><span class="err">－</span><span class="nx">出乎意料</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一举成名</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">......</span>
    <span class="nx">DUTIR_Jus</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">一则以喜</span><span class="err">，</span><span class="nx">一则以惧</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一发千钧</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一年被蛇咬</span><span class="err">，</span><span class="nx">三年怕草索</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一座皆惊</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一脸横肉</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一蛇两头</span><span class="o">...</span>
    <span class="nx">DUTIR_Les</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="p">:)</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">CC</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">Happy</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">LOL</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">_so</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">haha</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">DUTIR_Nus</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="mi">2</span><span class="nx">气斗狠</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">MD</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">TNND</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">gun</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">kao</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一刀两断</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">DUTIR_Wus</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">B4</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">BD</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">BS</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">HC</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">HJ</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">JJWW</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_deny</span> <span class="p">=</span> <span class="p">{</span><span class="sc">&#39;不&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不可</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不是</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不能</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不要</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;休&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_extreme</span> <span class="p">=</span> <span class="p">{</span><span class="sc">&#39;万&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">万万</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">万分</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">万般</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不亦乐乎</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不可开交</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_ish</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">一些</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一点</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一点儿</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不丁点儿</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不大</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不怎么</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_more</span> <span class="p">=</span> <span class="p">{</span><span class="sc">&#39;多&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">大不了</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">如斯</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">尤甚</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;强&#39;</span><span class="p">,</span> <span class="sc">&#39;愈&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_neg</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">一下子爆发</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一下子爆发的一连串</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一不小心</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一个屁</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一仍旧贯</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一偏</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_pos</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一专多能</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一丝不差</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一丝不苟</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一个心眼儿</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一五一十</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_very</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">不为过</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不少</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不胜</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不过</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">何啻</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">何止</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">STOPWORDS_en</span> <span class="p">=</span> <span class="p">{</span><span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">about</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">above</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">across</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">after</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">afterwards</span><span class="err">&#39;</span><span class="o">...</span>
    <span class="nx">STOPWORDS_zh</span> <span class="p">=</span> <span class="p">{</span><span class="sc">&#39;、&#39;</span><span class="p">,</span> <span class="sc">&#39;。&#39;</span><span class="p">,</span> <span class="sc">&#39;〈&#39;</span><span class="p">,</span> <span class="sc">&#39;〉&#39;</span><span class="p">,</span> <span class="sc">&#39;《&#39;</span><span class="p">,</span> <span class="sc">&#39;》&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    
    <span class="nx">FORMAL_pos_words</span> <span class="p">=</span> <span class="p">[</span><span class="err">&#39;</span><span class="mi">100</span><span class="nx">强</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="mi">3</span><span class="nx">A级</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="mi">50</span><span class="nx">强</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">AAA级</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">AAA企业</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">爱戴</span><span class="err">&#39;</span><span class="p">,..]</span>
    <span class="nx">FORMAL_neg_words</span> <span class="p">=</span> <span class="p">[</span><span class="err">&#39;</span><span class="nx">安于现状</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">暗藏</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">暗淡</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">暗黑</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">暗流</span><span class="err">&#39;</span><span class="p">,</span> <span class="p">..]</span>
    <span class="nx">UNFORMAL_pos_words</span> <span class="p">=</span> <span class="p">[</span><span class="err">&#39;</span><span class="nx">爱心</span><span class="sc">&#39;,&#39;</span><span class="nx">安定</span><span class="sc">&#39;,&#39;</span><span class="nx">安全</span><span class="sc">&#39;,&#39;</span><span class="nx">安然无恙</span><span class="sc">&#39;,&#39;</span><span class="nx">安泰</span><span class="sc">&#39;,&#39;</span><span class="nx">霸主</span><span class="err">&#39;</span><span class="p">,</span><span class="o">...</span><span class="p">]</span>
    <span class="nx">UNFORMAL_neg_words</span> <span class="p">=</span> <span class="p">[</span><span class="err">&#39;</span><span class="nx">哀鸿遍野</span><span class="sc">&#39;,&#39;</span><span class="nx">肮脏</span><span class="sc">&#39;,&#39;</span><span class="nx">罢免</span><span class="sc">&#39;,&#39;</span><span class="nx">白痴</span><span class="sc">&#39;,&#39;</span><span class="nx">败笔</span><span class="sc">&#39;,&#39;</span><span class="nx">败诉</span><span class="sc">&#39;,&#39;</span><span class="nx">半信半疑</span><span class="err">&#39;</span><span class="p">..]</span>



<span class="nx">FILE</span>
    <span class="o">/</span><span class="nx">Library</span><span class="o">/</span><span class="nx">Frameworks</span><span class="o">/</span><span class="nx">Python</span><span class="p">.</span><span class="nx">framework</span><span class="o">/</span><span class="nx">Versions</span><span class="o">/</span><span class="mf">3.7</span><span class="o">/</span><span class="nx">lib</span><span class="o">/</span><span class="nx">python3</span><span class="mf">.7</span><span class="o">/</span><span class="nx">site</span><span class="o">-</span><span class="nx">packages</span><span class="o">/</span><span class="nx">cntext</span><span class="o">/</span><span class="nx">__init__</span><span class="p">.</span><span class="nx">py</span>
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext</span> <span class="kn">import</span> <span class="n">dict_info</span>

<span class="n">dict_info</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> 【大连理工大学情感本体库】
     七大情绪分类，依次是哀、恶、好、惊、惧、乐、怒；对应的情绪词表依次：
    DUTIR_Ais = {&#34;泣血捶膺&#34;, &#34;望断白云&#34;, &#34;日暮途穷&#34;, &#34;身微力薄&#34;...}
    DUTIR_Wus = {&#34;饰非遂过&#34;, &#34;恶语&#34;, &#34;毁害&#34;, &#34;恶籍盈指&#34;, &#34;脾气爆躁&#34;, &#34;淫贱&#34;, &#34;凌乱&#34;...}
    DUTIR_Haos =  {&#34;打破砂锅璺到底&#34;, &#34;多彩&#34;, &#34;披沙拣金&#34;, &#34;见机行事&#34;, &#34;精神饱满&#34;...}
    DUTIR_Jings = {&#34;骇人视听&#34;, &#34;拍案惊奇&#34;, &#34;悬念&#34;, &#34;无翼而飞&#34;, &#34;原来&#34;, &#34;冷门&#34;...}
    DUTIR_Jus ={&#34;山摇地动&#34;, &#34;月黑风高&#34;, &#34;流血&#34;, &#34;老鼠偷猫饭－心惊肉跳&#34;, &#34;一发千钧&#34;...}
    DUTIR_Les ={&#34;含哺鼓腹&#34;, &#34;欢呼鼓舞&#34;, &#34;莺歌蝶舞&#34;, &#34;将伯之助&#34;, &#34;逸兴横飞&#34;, &#34;舒畅&#34;...}
    DUTIR_Nus = {&#34;怨气满腹&#34;, &#34;面有愠色&#34;, &#34;愤愤&#34;, &#34;直眉瞪眼&#34;, &#34;负气斗狠&#34;, &#34;挑眼&#34;...}
    
    【知网Hownet词典】
    含正负形容词、否定词、副词等词表，对应的词表依次:
    HOWNET_deny = {&#34;不&#34;, &#34;不是&#34;, &#34;不能&#34;, &#34;不可&#34;...}
    HOWNET_extreme = {&#34;百分之百&#34;, &#34;倍加&#34;, &#34;备至&#34;, &#34;不得了&#34;...}
    HOWNET_ish = {&#34;点点滴滴&#34;, &#34;多多少少&#34;, &#34;怪&#34;, &#34;好生&#34;, &#34;还&#34;, &#34;或多或少&#34;...}
    HOWNET_more = {&#34;大不了&#34;, &#34;多&#34;, &#34;更&#34;, &#34;比较&#34;, &#34;更加&#34;, &#34;更进一步&#34;, &#34;更为&#34;, &#34;还&#34;, &#34;还要&#34;...}
    HOWNET_neg = {&#34;压坏&#34;, &#34;鲁莽的&#34;, &#34;被控犯罪&#34;, &#34;银根紧&#34;, &#34;警惕的&#34;, &#34;残缺&#34;, &#34;致污物&#34;, &#34;柔弱&#34;...}
    HOWNET_pos = {&#34;无误&#34;, &#34;感激不尽&#34;, &#34;受大众欢迎&#34;, &#34;敬礼&#34;,  &#34;文雅&#34;, &#34;一尘不染&#34;, &#34;高精度&#34;, &#34;兴盛&#34;...}
    HOWNET_very = {&#34;不为过&#34;, &#34;超&#34;, &#34;超额&#34;, &#34;超外差&#34;, &#34;超微结构&#34;, &#34;超物质&#34;, &#34;出头&#34;...}
    
    【停用词表】
    中英文停用词表，依次
    STOPWORDS_zh = {&#34;经&#34;, &#34;得&#34;, &#34;则甚&#34;, &#34;跟&#34;, &#34;好&#34;, &#34;具体地说&#34;...}
    STOPWORDS_en = {&#39;a&#39;, &#39;about&#39;, &#39;above&#39;, &#39;across&#39;, &#39;after&#39;...}
    
    【中文副词/连词】
    副词ADV、连词CONJ
    ADV_words = [&#39;都&#39;, &#39;全&#39;, &#39;单&#39;, &#39;共&#39;, &#39;光&#39;...}
    CONJ_words = [&#39;乃&#39;, &#39;乍&#39;, &#39;与&#39;, &#39;无&#39;, &#39;且&#39;...}
    
    【金融情绪词典】
     姚加权,冯绪,王赞钧,纪荣嵘,张维.语调、情绪及市场影响:基于金融情绪词典[J].管理科学学报,2021,24(05):26-46.
     #正式-肯定情绪词典
     FORMAL_pos_words = [&#39;100强&#39;, &#39;3A级&#39;, &#39;50强&#39;, &#39;AAA级&#39;, &#39;AAA企业&#39;, &#39;爱戴&#39;,...]
     #正式-否定情绪词典
     FORMAL_neg_words = [&#39;安于现状&#39;, &#39;暗藏&#39;, &#39;暗淡&#39;, &#39;暗黑&#39;, &#39;暗流&#39;, ...]
     #非正式-肯定情绪词典
     UNFORMAL_pos_words = [&#39;爱心&#39;,&#39;安定&#39;,&#39;安全&#39;,&#39;安然无恙&#39;,&#39;安泰&#39;,&#39;霸主&#39;,...]
     #非正式-否定情绪词典
     UNFORMAL_neg_words = [&#39;哀鸿遍野&#39;,&#39;肮脏&#39;,&#39;罢免&#39;,&#39;白痴&#39;,&#39;败笔&#39;,&#39;败诉&#39;,&#39;半信半疑&#39;...]
</code></pre></div><br>
<p>查看词表</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext</span> <span class="kn">import</span> <span class="n">CONJ_words</span><span class="p">,</span> <span class="n">ADV_words</span>

<span class="c1">#获取连词词表</span>
<span class="n">CONJ_words</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;乃&#39;,
 &#39;乍&#39;,
 &#39;与&#39;,
 &#39;无&#39;,
 &#39;且&#39;,
 &#39;丕&#39;,
 &#39;为&#39;,
 &#39;共&#39;,
 &#39;其&#39;,
 &#39;况&#39;,
 &#39;厥&#39;,
 &#39;则&#39;,
 &#39;那&#39;,
 &#39;兼&#39;,
 ...
 ]
</code></pre></div><p><br><br></p>
<h2 id="二stats">二、stats</h2>
<p>目前含</p>
<ul>
<li>term_freq 词频统计函数，返回Counter类型</li>
<li>readability 中文可读性</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.stats</span> <span class="kn">import</span> <span class="n">term_freq</span><span class="p">,</span> <span class="n">readability</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更&#39;</span>
<span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<p>**中文可读性 ** 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
</blockquote>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">readability</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.17647058823529413,
 &#39;readability3&#39;: 13.588235294117647}
</code></pre></div><p><br><br></p>
<h2 id="三dictionary">三、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="31-sopmi-共现法">3.1 SoPmi 共现法</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.dictionary</span> <span class="kn">import</span> <span class="n">SoPmi</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">step 1/4:...seg corpus ...
Loading model cost 0.678 seconds.
Prefix dict has been built successfully.
step 1/4 finished:...cost 60.78995203971863...
step 2/4:...collect cowords ...
step 2/4 finished:...cost 0.6169600486755371...
step 3/4:...compute sopmi ...
step 1/4 finished:...cost 0.26422882080078125...
step 4/4:...save candiwords ...
finished! cost 61.8965539932251
</code></pre></div><br>
<h3 id="32-w2vmodels-词向量">3.2 W2VModels 词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.dictionary</span> <span class="kn">import</span> <span class="n">W2VModels</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据预处理开始.......
预处理结束...........
Word2Vec模型训练开始......
已将模型存入 /Users/Desktop/cntext/test/output/w2v_candi_words/w2v.model 

准备寻找每个seed在语料中所有的相似候选词
初步搜寻到 572 个相似的候选词
计算每个候选词 与 integrity 的相似度， 选出相似度最高的前 100 个候选词
已完成 【integrity 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/integrity.txt， 耗时 46 秒

准备寻找每个seed在语料中所有的相似候选词
初步搜寻到 516 个相似的候选词
计算每个候选词 与 innovation 的相似度， 选出相似度最高的前 100 个候选词
已完成 【innovation 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/innovation.txt， 耗时 46 秒

准备寻找每个seed在语料中所有的相似候选词
初步搜寻到 234 个相似的候选词
计算每个候选词 与 quality 的相似度， 选出相似度最高的前 100 个候选词
已完成 【quality 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/quality.txt， 耗时 46 秒

准备寻找每个seed在语料中所有的相似候选词
初步搜寻到 243 个相似的候选词
计算每个候选词 与 respect 的相似度， 选出相似度最高的前 100 个候选词
已完成 【respect 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/respect.txt， 耗时 46 秒

准备寻找每个seed在语料中所有的相似候选词
初步搜寻到 319 个相似的候选词
计算每个候选词 与 teamwork 的相似度， 选出相似度最高的前 100 个候选词
已完成 【teamwork 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/teamwork.txt， 耗时 46 秒
</code></pre></div><p><br><br></p>
<h2 id="四-sentiment">四、 sentiment</h2>
<ul>
<li>senti_by_hownet 使用知网Hownet词典对文本进行<strong>情感</strong>分析</li>
<li>senti_by_dutir  使用大连理工大学情感本体库dutir对文本进行<strong>情绪</strong>分析</li>
<li>senti_by_diydict 使用<strong>自定义词典</strong> 对文本进行<strong>情感</strong>分析</li>
</ul>
<h3 id="41-senti_by_hownettext-adj_advfalse">4.1 senti_by_hownet(text, adj_adv=False)</h3>
<p>使用知网Hownet词典进行(中)文本数据的情感分析，统计正、负情感信息出现次数(得分)</p>
<ul>
<li>text:  待分析的中文文本数据</li>
<li>adj_adv:  是否考虑副词（否定词、程度词）对情绪形容词的反转和情感强度修饰作用，默认False。默认False只统计情感形容词出现个数；</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.sentiment</span> <span class="kn">import</span> <span class="n">senti_by_hownet</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;今天股票大涨，心情倍爽，非常开心啊。&#39;</span>

<span class="n">senti_by_dutir</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;word_num&#39;: 12,
 &#39;sentence_num&#39;: 2,
 &#39;stopword_num&#39;: 4,
 &#39;好_num&#39;: 0,
 &#39;乐_num&#39;: 1,
 &#39;哀_num&#39;: 0,
 &#39;怒_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;惊_num&#39;: 0}
</code></pre></div><br>
<p>考虑副词（否定词、程度词）对情绪形容词的反转和情感强度修饰作用</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">senti_by_hownet</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">adj_adv</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;sentence_num&#39;: 1,
 &#39;word_num&#39;: 12,
 &#39;stopword_num&#39;: 3,
 &#39;pos_score&#39;: 13.0,
 &#39;neg_score&#39;: 0.0}
</code></pre></div><p><br><br></p>
<h3 id="42-senti_by_dutirtext">4.2 senti_by_dutir(text)</h3>
<p>使用大连理工大学情感本体库对文本进行情绪分析，统计各情绪词语出现次数。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.sentiment</span> <span class="kn">import</span> <span class="n">senti_by_dutir</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;今天股票大涨，心情倍爽，非常开心啊。&#39;</span>

<span class="n">senti_by_dutir</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;word_num&#39;: 12,
 &#39;sentence_num&#39;: 2,
 &#39;stopword_num&#39;: 4,
 &#39;好_num&#39;: 0,
 &#39;乐_num&#39;: 1,
 &#39;哀_num&#39;: 0,
 &#39;怒_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;惊_num&#39;: 0}
</code></pre></div><blockquote>
<p>情绪分析使用的大连理工大学情感本体库，如发表论文，请注意用户许可协议</p>
<p>如果用户使用该资源发表论文或取得科研成果，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。</p>
<p>参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</blockquote>
<p><br><br></p>
<h3 id="43-senti_by_diytext">4.3 senti_by_diy(text)</h3>
<p>使用diy词典进行情感分析，计算各个情绪词出现次数，未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>sentiwords:  情感词字典；
{&lsquo;category1&rsquo;:  &lsquo;category1 词语列表&rsquo;,
&lsquo;category2&rsquo;: &lsquo;category2词语列表&rsquo;,
&lsquo;category3&rsquo;: &lsquo;category3词语列表&rsquo;,
&hellip;
}</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">sentiwords</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;开心&#39;</span><span class="p">,</span> <span class="s1">&#39;愉快&#39;</span><span class="p">,</span> <span class="s1">&#39;倍爽&#39;</span><span class="p">],</span>
              <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
              <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;倍&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;今天股票大涨，心情倍爽，非常开心啊。&#39;</span>
<span class="n">senti_by_diydict</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">sentiwords</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 1,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 4,
 &#39;sentence_num&#39;: 2,
 &#39;word_num&#39;: 12}
</code></pre></div><p><br><br></p>
<h3 id="44-注意">4.4 注意</h3>
<p><strong>返回结果</strong>:  <strong>num</strong>表示词语出现次数； score是考虑副词、否定词对情感的修饰，结果不是词频，是情感类别的得分。</p>
<p><br><br></p>
<h2 id="五similarity">五、similarity</h2>
<p>使用cosine、jaccard、miniedit等计算两文本的相似度，算法实现参考自</p>
<blockquote>
<p>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</p>
</blockquote>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">from cntext.similarity import similarity_score

text1 = &#39;编程真好玩编程真好玩&#39;
text2 = &#39;游戏真好玩编程真好玩&#39;

similarity_score(text1, text2)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;Sim_Cosine&#39;: 0.816496580927726,
 &#39;Sim_Jaccard&#39;: 0.6666666666666666,
 &#39;Sim_MinEdit&#39;: 1,
 &#39;Sim_Simple&#39;: 0.9183673469387755}
</code></pre></div><p><br><br></p>
<h2 id="六visualization">六、visualization</h2>
<p>文本信息可视化，含wordcloud、wordshiftor</p>
<ul>
<li>wordcloud 词云图</li>
<li>wordshiftor 两文本词移图</li>
</ul>
<h3 id="61-wordcloudtext-title-html_path">6.1 wordcloud(text, title, html_path)</h3>
<ul>
<li>text:  中文文本字符串数据</li>
<li>title:  词云图标题</li>
<li>html_path:  词云图html文件存储路径</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.visualization</span> <span class="kn">import</span> <span class="n">wordcloud</span>

<span class="n">text1</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;在信息化时代，各种各样的数据被广泛采集和利用，有些数据看似无关紧要甚至好像是公开的，但同样关乎国家安全。11月1日是《反间谍法》颁布实施七周年。近年来，国家安全机关按照《反间谍法》《数据安全法》有关规定，依法履行数据安全监管职责，在全国范围内开展涉外数据专项执法行动，发现一些境外数据公司长期、大量、实时搜集我境内船舶数据，数据安全领域的“商业间谍”魅影重重。
</span><span class="s2">
</span><span class="s2">2020年6月，国家安全机关在反间谍专项行动中发现，有境外数据公司通过网络在境内私下招募“数据贡献员”。广东省湛江市国家安全局据此开展调查，在麻斜军港附近发现有可疑的无线电设备在持续搜集湛江港口舰船数据，并通过互联网实时传往境外。在临近海港的一个居民楼里，国家安全机关工作人员最终锁定了位置。
</span><span class="s2">
</span><span class="s2">一套简易的无线电设备是AIS陆基基站，用来接收AIS系统发射的船舶数据。AIS系统是船舶身份自动识别系统，国际海事组织要求300总吨以上船舶必须强制安装。船只在航行过程中，通过AIS系统向其他船只和主管部门发送船只航向、航速、目的港等信息，用于航行避让、交通导航、轨迹回溯等功能。国家安全机关查获的设备虽然看上去简陋，功能却十分强大。
</span><span class="s2">
</span><span class="s2">国家安全机关进一步调查发现，这个基站的来历并不简单。2016年，湛江市的无线电爱好者郑某偶然收到一封境外某海事数据公司发来的邀请邮件。
</span><span class="s2">
</span><span class="s2">作为资深的无线电爱好者，能免费领取价值几千元的设备还能获取更多的船舶信息，郑某当然心动。而且，这个基站的架设也非常容易，只要简单组装连上家里的网络，自己的任务就算完成。郑某马上浏览了这家公司申请无线电设备的页面，并按对方要求填写了信息。
</span><span class="s2">
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="n">wordcloud</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text1</span><span class="p">,</span> 
          <span class="n">title</span><span class="o">=</span><span class="s1">&#39;词云图测试&#39;</span><span class="p">,</span> 
          <span class="n">html_path</span><span class="o">=</span><span class="s1">&#39;output/词云图测试.html&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>

<figure >
    
        <img src="img/wordcloud.png" width="800" />
    
    
</figure>

<br>
<h3 id="62-wordshiftortext1-text2-title-top_n-matplotlib_family">6.2 wordshiftor(text1, text2, title, top_n, matplotlib_family)</h3>
<ul>
<li>text1:  文本数据1；字符串</li>
<li>text2:  文本数据2；字符串</li>
<li>title:  词移图标题</li>
<li>top_n:  显示最常用的前n词； 默认值15</li>
<li>matplotlib_family matplotlib中文字体，默认&quot;Arial Unicode MS&quot;；如绘图字体乱码请，请参考下面提示</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;在信息化时代，各种各样的数据被广泛采集和利用，有些数据看似无关紧要甚至好像是公开的，但同样关乎国家安全。11月1日是《反间谍法》颁布实施七周年。近年来，国家安全机关按照《反间谍法》《数据安全法》有关规定，依法履行数据安全监管职责，在全国范围内开展涉外数据专项执法行动，发现一些境外数据公司长期、大量、实时搜集我境内船舶数据，数据安全领域的“商业间谍”魅影重重。
</span><span class="s2">
</span><span class="s2">2020年6月，国家安全机关在反间谍专项行动中发现，有境外数据公司通过网络在境内私下招募“数据贡献员”。广东省湛江市国家安全局据此开展调查，在麻斜军港附近发现有可疑的无线电设备在持续搜集湛江港口舰船数据，并通过互联网实时传往境外。在临近海港的一个居民楼里，国家安全机关工作人员最终锁定了位置。
</span><span class="s2">
</span><span class="s2">一套简易的无线电设备是AIS陆基基站，用来接收AIS系统发射的船舶数据。AIS系统是船舶身份自动识别系统，国际海事组织要求300总吨以上船舶必须强制安装。船只在航行过程中，通过AIS系统向其他船只和主管部门发送船只航向、航速、目的港等信息，用于航行避让、交通导航、轨迹回溯等功能。国家安全机关查获的设备虽然看上去简陋，功能却十分强大。
</span><span class="s2">
</span><span class="s2">国家安全机关进一步调查发现，这个基站的来历并不简单。2016年，湛江市的无线电爱好者郑某偶然收到一封境外某海事数据公司发来的邀请邮件。
</span><span class="s2">
</span><span class="s2">作为资深的无线电爱好者，能免费领取价值几千元的设备还能获取更多的船舶信息，郑某当然心动。而且，这个基站的架设也非常容易，只要简单组装连上家里的网络，自己的任务就算完成。郑某马上浏览了这家公司申请无线电设备的页面，并按对方要求填写了信息。
</span><span class="s2">
</span><span class="s2">&#34;&#34;&#34;</span>


<span class="n">text2</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">通知强调，各地商务主管部门要紧紧围绕保供稳价工作目标，压实“菜篮子”市长负责制，细化工作措施；强化横向协作与纵向联动，加强与有关部门的工作协调，形成工作合力；建立完善省际间和本地区联保联供机制，健全有关工作方案，根据形势及时开展跨区域调运；加强市场运行监测，每日跟踪蔬菜、肉类等重点生活必需品供求和价格变化情况，及时预测，及早预警。
</span><span class="s2">
</span><span class="s2">通知要求，各地支持鼓励大型农产品流通企业与蔬菜、粮油、畜禽养殖等农产品生产基地建立紧密合作关系，签订长期供销协议；耐储蔬菜要提前采购，锁定货源，做好本地菜与客菜之间，北菜与南菜之间、设施菜与露天菜之间的梯次轮换和衔接供应；健全完备本地肉类储备规模及管理制度；北方省份要按时完成本年度冬春蔬菜储备计划，南方省份要根据自身情况建立完善蔬菜储备；及时投放肉类、蔬菜等生活必需品储备，补充市场供应。
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="kn">from</span> <span class="nn">cntext.visualization</span> <span class="kn">import</span> <span class="n">wordshiftor</span>

<span class="n">wordshiftor</span><span class="p">(</span><span class="n">text1</span><span class="o">=</span><span class="n">text1</span><span class="p">,</span> 
            <span class="n">text2</span><span class="o">=</span><span class="n">text2</span><span class="p">,</span> 
            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;两文本对比&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>

<figure >
    
        <img src="img/wordshiftor.png" width="800" />
    
    
</figure>

<h3 id="63-textpictitlepython测试-subtitle使用python生成图片-fontalibaba-puhuiti-boldotf-titlesize18-subsize14">6.3 textpic(title=&lsquo;PYTHON测试&rsquo;, subtitle=&lsquo;使用Python生成图片&rsquo;, font=&lsquo;Alibaba-PuHuiTi-Bold.otf&rsquo;, titlesize=1.8, subsize=14)</h3>
<ul>
<li>title:  主标题</li>
<li>subtitle: 副标题</li>
<li>font:  本地中文字体路径</li>
<li>titlesize: 主标题字体大小</li>
<li>subsize: 副标题字体大小</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">textpic</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;PYTHON测试&#39;</span><span class="p">,</span> 
        <span class="n">subtitle</span><span class="o">=</span><span class="s1">&#39;使用Python生成图片&#39;</span><span class="p">,</span> 
        <span class="n">font</span><span class="o">=</span><span class="s1">&#39;data/Alibaba-PuHuiTi-Bold.otf&#39;</span><span class="p">,</span> 
        <span class="n">titlesize</span><span class="o">=</span><span class="mf">1.8</span><span class="p">,</span> 
        <span class="n">subsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/result.png" width="800" />
    
    
</figure>

<br>
<p><strong>注意</strong></p>
<blockquote>
<p>设置参数matplotlib_family，需要先运行下面代码获取本机字体列表
from matplotlib.font_manager import FontManager
mpl_fonts = set(f.name for f in FontManager().ttflist)
print(mpl_fonts)</p>
</blockquote>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>中文金融情感词典</title>
      <link>https://textdata.cn/blog/chinese_financial_dictionary/</link>
      <pubDate>Tue, 13 Jul 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/chinese_financial_dictionary/</guid>
      <description>基于 姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报 开发了中文的金融情感词典第一个权威的中文情感词典|配合cnsenti使用</description>
      <content:encoded><![CDATA[<p>可以使用cnsenti库中的自定义方法，计算年报或财经类社交媒体的文本情绪。</p>
<blockquote>
<p>姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报，2021. 24(5), 26-46.</p>
</blockquote>
<p>该论文开发了中文的金融情感词典，已有的中文金融情感词典有以下不足：</p>
<ul>
<li>大多采用形容情绪词，对于金融场景适用性差</li>
<li>将LM英文词典本土化，制作中文金融情绪词典</li>
<li>词典构建方法多为人工</li>
</ul>
<p>该论文开发中文情绪词典，从年报和社交媒体两个数据源出发，借助数据挖掘和深度学习算法，构建了正式用语 和 非正式用于两大类情感词典。</p>
<h2 id="标注思路">标注思路</h2>
<p>一般构建词典要么用多个词典融合，要么人工标准训练。该论文采用了一定的技巧，不需要人工标注即可实现近乎人工标注的效果。</p>
<h3 id="正式词典标注思路">正式词典标注思路</h3>
<p>正式用语情感词典，通过年报公布后3个交易日累积正负收益率为标准，将年报标记为正负面情绪两类。</p>
<h3 id="非正式词典标注思路">非正式词典标注思路</h3>
<p>使用所有中国上市公司在雪球论坛和东方财富股吧内相关帖子，共8130万条。</p>
<p>在网络股票论坛，用户发表自己的意见时，经常带有表情符号，从而使得帖子带有明显的情绪指标。 这种含有特殊指标的帖子，省去了人工标注文本情绪的工作。</p>
<br>
<p>具体构建词典的步骤，大家可以阅读论文原文。论文已经公开了中文情感词典，我已将其整理为4个txt文件</p>
<ul>
<li>formal_pos.txt  正式用语<strong>正面</strong>情绪词典</li>
<li>formal_neg.txt  正式用语<strong>负面</strong>情绪词典</li>
<li>unformal_pos.txt  非正式用语<strong>正面</strong>情绪词典</li>
<li>unformal_neg.txt  非正式用语<strong>负面</strong>情绪词典</li>
</ul>
<br>
<h2 id="中文金融词典使用方法">中文金融词典使用方法</h2>
<p>cnsenti实现了自定义词典功能，导入不同的txt词典文件，即可实现不同方面的情绪词统计。</p>
<h3 id="年报正式用语词典">年报正式用语词典</h3>
<ul>
<li>dict/formal_pos.txt   正式用语<strong>正面</strong>情绪词典</li>
<li>dict/formal_neg.txt    正式用语<strong>负面</strong>情绪词典</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cnsenti</span> <span class="kn">import</span> <span class="n">Sentiment</span>

<span class="n">senti</span> <span class="o">=</span> <span class="n">Sentiment</span><span class="p">(</span><span class="n">pos</span><span class="o">=</span><span class="s1">&#39;dict/formal_pos.txt&#39;</span><span class="p">,</span>  <span class="c1">#正面词典txt文件相对路径</span>
                  <span class="n">neg</span><span class="o">=</span><span class="s1">&#39;dict/formal_neg.txt&#39;</span><span class="p">,</span>  <span class="c1">#负面词典txt文件相对路径</span>
                  <span class="n">merge</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>             <span class="c1">#是否将cnsenti自带词典和用户导入的自定义词典融合</span>
                  <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>      <span class="c1">#两txt均为utf-8编码</span>

<span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;这家公司是行业的引领者，是中流砥柱。今年的业绩非常好。&#39;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">senti</span><span class="o">.</span><span class="n">sentiment_count</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sentiment_count&#39;</span><span class="p">,</span><span class="n">result</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sentiment_count {&#39;words&#39;: 16, &#39;sentences&#39;: 2, &#39;pos&#39;: 3, &#39;neg&#39;: 0}
</code></pre></div><br>
<h3 id="财经社交媒体非正式用语词典">财经社交媒体非正式用语词典</h3>
<ul>
<li>dict/unformal_pos.txt   非正式用语<strong>正面</strong>情绪词典</li>
<li>dict/unformal_neg.txt    非正式用语<strong>负面</strong>情绪词典</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cnsenti</span> <span class="kn">import</span> <span class="n">Sentiment</span>

<span class="n">senti</span> <span class="o">=</span> <span class="n">Sentiment</span><span class="p">(</span><span class="n">pos</span><span class="o">=</span><span class="s1">&#39;dict/unformal_pos.txt&#39;</span><span class="p">,</span>  <span class="c1">#正面词典txt文件相对路径</span>
                  <span class="n">neg</span><span class="o">=</span><span class="s1">&#39;dict/unformal_neg.txt&#39;</span><span class="p">,</span>  <span class="c1">#负面词典txt文件相对路径</span>
                  <span class="n">merge</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>             <span class="c1">#融合cnsenti自带词典和用户导入的自定义词典</span>
                  <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>      <span class="c1">#两txt均为utf-8编码</span>

<span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;这个股票前期走势承压，现在阴跌，散户只能割肉离场，这股票真垃圾&#39;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">senti</span><span class="o">.</span><span class="n">sentiment_count</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sentiment_count&#39;</span><span class="p">,</span><span class="n">result</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sentiment_count {&#39;words&#39;: 18, &#39;sentences&#39;: 1, &#39;pos&#39;: 0, &#39;neg&#39;: 2}
</code></pre></div><br>
<h2 id="说明">说明</h2>
<p>读者如需使用本项目词典，请引用如下参考文献：</p>
<blockquote>
<p>姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报，2021. 24(5), 26-46.</p>
</blockquote>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>当cnsenti遇上streamlit</title>
      <link>https://textdata.cn/blog/cnsenti_streamlit/</link>
      <pubDate>Thu, 07 Jun 2018 10:40:10 +0600</pubDate>
      
      <guid>/blog/cnsenti_streamlit/</guid>
      <description>streamlit是web包，cnsenti是文本分析包，两者结合即可制造在线文本分析网站。</description>
      <content:encoded><![CDATA[<h1 id="cnsentidemo">cnsentiDemo</h1>
<p>这是使用streamlit库将中文情感分析[<strong>cnsenti</strong> 部署到网络世界，可<strong>在线提供简单的中文文本的情绪及情感计算</strong>。</p>
<p><strong>streamlit库</strong>(<a href="https://docs.streamlit.io/en/stable/">https://docs.streamlit.io/en/stable/</a>)， 是目前简单易用的数据可视化web框架，比flask和django少了很多的扩展性，但是容易学习上手，适合初学者把玩。</p>
<iframe
    src="//player.bilibili.com/player.html?bvid=bv17V411H7sZ&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<p><a href="https://cnsenti.herokuapp.com/"><strong>Demo</strong></a>
<img loading="lazy" src="img/%e6%95%88%e6%9e%9c%e5%9b%be.png" alt=""  />
</p>
<p><br><br></p>
<h1 id="网站">网站</h1>
<p>现在技术有限，该网站大致内容分为三部分</p>
<ul>
<li>准备数据</li>
<li>数据分析
<ul>
<li>情感分析</li>
<li>词云图</li>
</ul>
</li>
<li>谢谢支持</li>
</ul>
<p><br><br></p>
<h1 id="本地使用">本地使用</h1>
<p>本网站的<strong>cnsentiDemo项目文件夹</strong>的文件有</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- main.py
- cnsenti_example.csv
- 大邓和他的Python.png
- requirements.txt
- 其他文件
</code></pre></div><p>将cnsentiDemo项目下载，在<strong>电脑本地离线使用cnsenti的方法</strong></p>
<ol>
<li><a href="cnsentiDemo.zip">下载解压到桌面desktop</a></li>
<li>命令行, 执行 <code>cd desktop/cnsentiDemo</code></li>
<li>命令行，执行 <code>pip3 install -r requirements.txt</code></li>
<li>命令行, 执行 <code>streamlit run main.py</code></li>
<li>根据命令行的提示，复制粘贴网址到桌面。我这里是 <code>**http://localhost:8501**</code></li>
<li>浏览器打开效果就会与视频等同</li>
</ol>
<p>上述过程中，Mac和Win会有一些缺点导致无法使用，需要根据命令行提示解决各自系统的小问题，例如</p>
<ol>
<li>Win需要使用64位的Python</li>
<li>Mac可能需要安装Xcode-install</li>
<li>其他可能的问题</li>
</ol>
<p><br><br></p>
<h1 id="web部署方法">Web部署方法</h1>
<p>如果想将自己的streamlit项目部署成网站，可以使用Heroku和github帮助你完成人生第一个小网站。操作方法：</p>
<ol>
<li>将写好的streamlit项目上传至github自有仓库</li>
<li>Heroku注册账号</li>
<li>点击Heroku网页右上角New， 选择Create new app</li>
<li>绑定github，连接github里的streamlit项目</li>
<li>部署</li>
</ol>
<p>部署方法也可参考  <a href="https://www.youtube.com/watch?v=zK4Ch6e1zq8&amp;list=PLtqF5YXg7GLmCvTswG32NqQypOuYkPRUE&amp;index=5">Youtube视频</a></p>
<br>
<br>
<h1 id="广而告之">广而告之</h1>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>推荐系统与协同过滤、奇异值分解</title>
      <link>https://textdata.cn/blog/svd_in_recommendation_system/</link>
      <pubDate>Thu, 07 Jun 2018 10:40:10 +0600</pubDate>
      
      <guid>/blog/svd_in_recommendation_system/</guid>
      <description>通过网络中留下的痕迹，例如观影记录，挖掘人潜在的偏好向量，进而物以类聚人以群分，开展个性化推荐Through the traces left in the network, such as movie viewing records, the potential preference vectors of people are mined, and then people are grouped together to carry out personalized recommendations</description>
      <content:encoded><![CDATA[<p>昨天我从PyData2018发现一个视频，讲如何在数据缺失的情况下挖掘出用户和产品的特征向量, 用于产品推荐系统。</p>
<blockquote>
<p>Daniel Pyrathon - A practical guide to Singular Value Decomposition in Python PyCon2018</p>
</blockquote>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/d7iIb_XVkZs" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>如果视频无法观看，可以前往<a href="https://v.qq.com/x/page/i0679novb10.html">腾讯视频</a></p>
<br>
<h1 id="一预备知识">一、预备知识</h1>
<h2 id="11-协同过滤">1.1 协同过滤</h2>
<p>日常生活中，像亚马逊、淘宝、京东、今日头条等各大互联网公司会无时不刻的收集我们的网络用户行为数据，并根据积累的历史行为数据对我们推送推荐内容或者推荐商品。这就是我们不曾感受到存在的推荐算法所起到的作用，这之中比较常见的实现方式是<strong>协同过滤</strong>（Collaberative Filtering）。数据设计到用户、产品及产品评价三种信息，数据类似于下图
<img loading="lazy" src="img/01-%e7%94%a8%e6%88%b7%e8%af%84%e5%88%86%e4%bf%a1%e6%81%af.png" alt=""  />
</p>
<br>
<h2 id="12-相似的人更容易做相似的事">1.2 相似的人更容易做相似的事</h2>
<p>协同过滤的核心想法是相似的人往往会做相似的事情。比如，A 和 B 是两个崇尚科技的人（相似信息源于大量的观影数据），而 B 喜欢 看科幻片 ，那么我们猜测 A 也喜欢 科幻片。
<img loading="lazy" src="img/02-%e7%94%a8%e6%88%b7%e8%af%84%e5%88%86%e7%9f%a9%e9%98%b5.png" alt=""  />

<img loading="lazy" src="img/03-%e9%a2%84%e6%b5%8b%e7%94%a8%e6%88%b7%e5%af%b9%e7%94%b5%e5%bd%b1%e7%9a%84%e5%96%9c%e5%a5%bd.png" alt=""  />
</p>
<br>
<h2 id="13-问题提出">1.3 问题提出</h2>
<p>上面我们展示的用户电影可视化图，实际上就是推荐算法中经常用到的<strong>用户-评价矩阵</strong>,</p>
<ul>
<li>那么我们如何对矩阵进行计算，才能获取相似性信息？</li>
<li>有了相似性信息我们又如何去利用相似性信息去做产品推荐？</li>
<li>我们知道两个向量通过余弦相似计算就可以得出两个向量的近似程度，那么这些向量我们又该如何从<strong>用户-评价矩阵</strong>提取呢？</li>
</ul>
<br>
<h2 id="14-奇异值分解svd">1.4 奇异值分解SVD</h2>
<p>这就用到奇异值分解（Singular Value Decompositon），简称SVD。具体怎么提取不是我们本文的重点，Python都帮我们实现了，我们只需要稍微了解下SVD，就直接上手用。</p>
<p>比如我们现在有了<strong>用户-评价矩阵</strong>
<img loading="lazy" src="img/04-%e7%94%a8%e6%88%b7%e8%af%84%e4%bb%b7%e7%9f%a9%e9%98%b5.png" alt=""  />
</p>
<br>
<p>给定一个矩阵，我们都可以分解得到两种矩阵，一种是用户信息矩阵，一种是评价信息（产品）矩阵。这两种矩阵在本例中使用了n_features = 2，即对于用户向量或者产品评价向量长度均为2，实际上也可以为其他数字（比如3，4。。）
<img loading="lazy" src="img/05-%e4%b8%a4%e7%a7%8d%e7%9f%a9%e9%98%b5.png" alt=""  />
</p>
<p>那么User1对于蓝色电影的喜欢程度是可以通过向量计算得出3.52
<img loading="lazy" src="img/06-%e9%a2%84%e6%b5%8b%e8%ae%a1%e7%ae%97.png" alt=""  />
</p>
<br>
<h2 id="15-用户相似性">1.5 用户相似性</h2>
<p>如下图，在二维坐标中我们可以看出不同用户间的相似度。
<img loading="lazy" src="img/07-%e7%94%a8%e6%88%b7%e7%9b%b8%e4%bc%bc%e5%ba%a6.png" alt=""  />
</p>
<p><br><br></p>
<h1 id="二项目实战">二、项目实战</h1>
<p>我们将使用Python的surprise库，对MovieLens数据集构建一个简单的协同过滤推荐系统。</p>
<p>安装方法:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="n">scikit</span><span class="o">-</span><span class="n">surprise</span>
</code></pre></div><p>如果你的anaconda自带jupyter notebook。那么你可能需要使用下面的安装方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="n">scikit</span><span class="o">-</span><span class="n">surprise</span>
</code></pre></div><p>从安装名我们发现其余scikit的特殊关系，所以熟悉scikit的同学看本文会比较轻松。</p>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="svd_in_recommendation_system.zip">点击下载</a></p>
<br>
## 2.1 准备数据
MovieLens数据集含有1000个用户的100000个观影评分记录。其中我们只需要使用该数据集中的u.data文件，该文件以行存储，每一行包括``userID itemID rating timestamp``,且各个字段之间以``\t``间隔。部分数据如下
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;196\t242\t3\t881250949\n&#39;, 
&#39;186\t302\t3\t891717742\n&#39;, 
&#39;22\t377\t1\t878887116\n&#39;, 
&#39;244\t51\t2\t880606923\n&#39;, 
&#39;166\t346\t1\t886397596\n&#39;]
</code></pre></div><br>
<h2 id="22-切割数据">2.2 切割数据</h2>
<p>在surprise库中我们可以创建读取器Reader的格式。在本例中，我们使用<code>\t</code>将每行数据分隔后分配给</p>
<p><code>user item rating timestamp</code></p>
<p>定义好Reader格式后，我们使用Dataset对象对数据进行读取操作。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">surprise</span> <span class="kn">import</span> <span class="n">Reader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="c1">#定义数据格式</span>
<span class="n">reader</span> <span class="o">=</span> <span class="n">Reader</span><span class="p">(</span><span class="n">line_format</span><span class="o">=</span><span class="s1">&#39;user item rating timestamp&#39;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1">#使用reader格式从u.data文件中读取数据</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">load_from_file</span><span class="p">(</span><span class="s1">&#39;u.data&#39;</span><span class="p">,</span> <span class="n">reader</span><span class="o">=</span><span class="n">reader</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="23-交叉检验">2.3 交叉检验</h2>
<p>surprise提供了交叉验证（crossvalidation）的接口，crossvalidation是啥？</p>
<p>我们先看图解释下</p>
<p>一份数据平均的分成5份，如果4份做训练集，1份做测试集。那么当我们训练模型的时候有1/5的数据我们的模型是无法学习的，这就浪费了20%。</p>
<p>但是我们又不能拿把所有的数据经过一次训练，再拿其中训练过的数据去做预测。因为这样会导致准确率a非常高，但放到实践中这个模型的预测准确率实际上是低于a的。</p>
<p>所以就有了crossvalidation交叉检验。我们一份数据训练5次，每次完整的数据分成4份训练1份测试。这样就解决了上面遇到的问题。如下图</p>
<p><img loading="lazy" src="img/08-crossvalidation.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#n_folds=5是指数据分成5份，做5次训练预测</span>
<span class="n">data</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">n_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="24-最优化optimization">2.4 最优化Optimization</h2>
<p>训练怎么达到最优，那就要有Optimization，也就是要有一个可供参考的标准。</p>
<p>训练的方式与其他机器学习方法类似，要使得一种算法试图优化其<strong>预测值</strong>尽可能接近<strong>真实值</strong>。在协作过滤应用中，我们的算法将尝试预测某个<strong>用户-电影</strong>组合的评级，并将该<strong>预测值</strong>与<strong>真实值</strong>进行比较。 使用经典误差测量如均方根误差（Root mean squared error，RMSE）和平均绝对误差（Mean absolute error，MAE）来测量预测值和真实值之间的差异。</p>
<p>在surprise库中，我们有广泛的算法可供选择，并为每种算法（SVD，NMF，KNN）提供多种参数选择。 就我们的例子而言，我们将使用SVD算法。 优化目标<code>measures</code>采用<code>RMSE', 'MAE</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">surprise</span> <span class="kn">import</span> <span class="n">SVD</span><span class="p">,</span> <span class="n">evaluate</span>

<span class="c1">#相当于scikit的机器学习算法的初始化</span>
<span class="n">svd</span> <span class="o">=</span> <span class="n">SVD</span><span class="p">()</span>

<span class="c1">#相当于scikit中的score，模型评估</span>
<span class="n">evaluate</span><span class="p">(</span><span class="n">svd</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">measures</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;RMSE&#39;</span><span class="p">,</span> <span class="s1">&#39;MAE&#39;</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<pre><code>Evaluating RMSE, MAE of algorithm SVD.

------------
Fold 1
RMSE: 0.9324
MAE:  0.7346
------------
Fold 2
RMSE: 0.9422
MAE:  0.7423
------------
Fold 3
RMSE: 0.9367
MAE:  0.7398
------------
Fold 4
RMSE: 0.9310
MAE:  0.7323
------------
Fold 5
RMSE: 0.9393
MAE:  0.7422
------------
------------
Mean RMSE: 0.9363
Mean MAE : 0.7382
------------
------------





CaseInsensitiveDefaultDict(list,
                           {'mae': [0.734621556055766,
                             0.7422621194493935,
                             0.7398192302116903,
                             0.7323079165231016,
                             0.7422361108902022],
                            'rmse': [0.9324301825022976,
                             0.9421845177536299,
                             0.9366580726086371,
                             0.9310376368987473,
                             0.9392636694333337]})
</code></pre>
<p>从上面运行结果看，optimizer选用RMSE后，5次训练的平均准确率高达93.63%。</p>
<br>
<h2 id="25-预测">2.5 预测</h2>
<p>最后我们还是很想看看训练出模型，其预测能力到底结果怎么样？</p>
<p>这次我们就做交叉验证了，省事点直接全部丢给SVD去训练</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">surprise</span> <span class="kn">import</span> <span class="n">SVD</span>
<span class="kn">from</span> <span class="nn">surprise</span> <span class="kn">import</span> <span class="n">Reader</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="c1">#读取数据</span>
<span class="n">reader</span> <span class="o">=</span> <span class="n">Reader</span><span class="p">(</span><span class="n">line_format</span><span class="o">=</span><span class="s1">&#39;user item rating timestamp&#39;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">load_from_file</span><span class="p">(</span><span class="s1">&#39;u.data&#39;</span><span class="p">,</span> <span class="n">reader</span><span class="o">=</span><span class="n">reader</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">build_full_trainset</span><span class="p">()</span> 

<span class="c1">#初始化svd模型,用data训练模型</span>
<span class="n">svd</span> <span class="o">=</span><span class="n">SVD</span><span class="p">()</span>
<span class="n">svd</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    &lt;surprise.prediction_algorithms.matrix_factorization.SVD at 0x10ab7d7f0&gt;
</code></pre></div><p>上面的代码</p>
<p>data = data.build_full_trainset()</p>
<p>这一行本来我没有写，但是当我注释掉这一行。出现下面的错误，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">DatasetAutoFolds&#39; object has no attribute &#39;global_mean&#39; on python surprise
</code></pre></div><p>最后在stackoverflow中找到解决办法，需要将data转化为surprise能够用的trainset类。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">https://stackoverflow.com/questions/49263964/datasetautofolds-object-has-no-attribute-global-mean-on-python-surprise
</code></pre></div><p>下面继续我们的预测，userid为196，itemid为302， 其真实评分为4。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">userid</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="mi">196</span><span class="p">)</span>
<span class="n">itemid</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="mi">302</span><span class="p">)</span>
<span class="n">actual_rating</span> <span class="o">=</span> <span class="mi">4</span>
<span class="nb">print</span><span class="p">(</span><span class="n">svd</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">userid</span><span class="p">,</span> <span class="mi">302</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</code></pre></div><pre><code>user: 196        item: 302        r_ui = 4.00   est = 3.41   {'was_impossible': False}
</code></pre>
<p>预测值为3.41， 真实值为4。还是相对靠谱的。</p>
<br>
<br>
<h1 id="广而告之">广而告之</h1>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用scipy实现层次聚类分析</title>
      <link>https://textdata.cn/blog/hierarchy_dendrogram_tutorial/</link>
      <pubDate>Fri, 18 May 2018 10:40:10 +0600</pubDate>
      
      <guid>/blog/hierarchy_dendrogram_tutorial/</guid>
      <description>使用scipy实现层次聚类分析</description>
      <content:encoded><![CDATA[<h2 id="代码下载">代码下载</h2>
<p><a href="hierarchy_dendrogram_code.zip"><strong>click to download</strong></a></p>
<h2 id="实验目的">实验目的</h2>
<p>如果您以前从未使用过树状图，那么使用树状图是查看多维数据如何聚集在一起的好方法。 在这本笔记本中，我将简单探索通过层次分析，借助树状图将其可视化。</p>
<br>
<h2 id="层次分析">层次分析</h2>
<p>层次分析是聚类分析的一种，scipy有这方面的封装包。</p>
<p>linkage函数从字面意思是链接，层次分析就是不断链接的过程，最终从n条数据，经过不断链接，最终聚合成一类，算法就此停止。</p>
<p>dendrogram是用来绘制树形图的函数。</p>
<br>
<h2 id="实验数据">实验数据</h2>
<p>grain_variety是标签，其他列为多种属性的值（特征）。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">dendrogram</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">seeds_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;seeds-less-rows.csv&#39;</span><span class="p">)</span>
<span class="n">seeds_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>area</th>
      <th>perimeter</th>
      <th>compactness</th>
      <th>length</th>
      <th>width</th>
      <th>asymmetry_coefficient</th>
      <th>groove_length</th>
      <th>grain_variety</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>14.88</td>
      <td>14.57</td>
      <td>0.8811</td>
      <td>5.554</td>
      <td>3.333</td>
      <td>1.018</td>
      <td>4.956</td>
      <td>Kama wheat</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14.69</td>
      <td>14.49</td>
      <td>0.8799</td>
      <td>5.563</td>
      <td>3.259</td>
      <td>3.586</td>
      <td>5.219</td>
      <td>Kama wheat</td>
    </tr>
    <tr>
      <th>2</th>
      <td>14.03</td>
      <td>14.16</td>
      <td>0.8796</td>
      <td>5.438</td>
      <td>3.201</td>
      <td>1.717</td>
      <td>5.001</td>
      <td>Kama wheat</td>
    </tr>
    <tr>
      <th>3</th>
      <td>19.31</td>
      <td>16.59</td>
      <td>0.8815</td>
      <td>6.341</td>
      <td>3.810</td>
      <td>3.477</td>
      <td>6.238</td>
      <td>Rosa wheat</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.99</td>
      <td>15.86</td>
      <td>0.8992</td>
      <td>5.890</td>
      <td>3.694</td>
      <td>2.068</td>
      <td>5.837</td>
      <td>Rosa wheat</td>
    </tr>
  </tbody>
</table>
</div>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#移除文本数据列</span>
<span class="n">varieties</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">seeds_df</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;grain_variety&#39;</span><span class="p">))</span>
<span class="n">varieties</span>
</code></pre></div><pre><code>['Kama wheat',
 'Kama wheat',
 'Kama wheat',
 'Rosa wheat',
 'Rosa wheat',
 'Rosa wheat',
 'Rosa wheat',
 'Rosa wheat',
 'Canadian wheat',
 'Canadian wheat',
 'Canadian wheat',
 'Canadian wheat',
 'Canadian wheat',
 'Canadian wheat']
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">samples</span> <span class="o">=</span> <span class="n">seeds_df</span><span class="o">.</span><span class="n">values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;samples的维度&#39;</span><span class="p">,</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div><pre><code>[[14.88   14.57    0.8811  5.554   3.333   1.018   4.956 ]
 [14.69   14.49    0.8799  5.563   3.259   3.586   5.219 ]
 [14.03   14.16    0.8796  5.438   3.201   1.717   5.001 ]
 [19.31   16.59    0.8815  6.341   3.81    3.477   6.238 ]
 [17.99   15.86    0.8992  5.89    3.694   2.068   5.837 ]
 [18.85   16.17    0.9056  6.152   3.806   2.843   6.2   ]
 [19.38   16.72    0.8716  6.303   3.791   3.678   5.965 ]
 [17.36   15.76    0.8785  6.145   3.574   3.526   5.971 ]
 [13.32   13.94    0.8613  5.541   3.073   7.035   5.44  ]
 [11.43   13.13    0.8335  5.176   2.719   2.221   5.132 ]
 [11.26   13.01    0.8355  5.186   2.71    5.335   5.092 ]
 [12.46   13.41    0.8706  5.236   3.017   4.987   5.147 ]
 [11.81   13.45    0.8198  5.413   2.716   4.898   5.352 ]
 [11.23   12.88    0.8511  5.14    2.795   4.325   5.003 ]]
samples的维度 (14, 7)
</code></pre>
<h3 id="使用linkage函数对samples进行层次聚类">使用linkage函数对samples进行层次聚类</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">X = linkage(y, method=&#39;single&#39;, metric=&#39;euclidean&#39;) 
</code></pre></div><p>sacipy中y是距离矩阵，我对此只是傻傻的理解成特征矩阵。 (m*n) m行代表m条记录,n代表n个特征</p>
<p>返回结果X是(m-1)*4的矩阵。 具体含义请看下面的案例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mergings</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1">#我们发现mergings比samples少一行</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sample维度&#39;</span><span class="p">,</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;mergings维度&#39;</span><span class="p">,</span><span class="n">mergings</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div><pre><code>sample维度 (14, 7)
mergings维度 (13, 4)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#层次分析可视化，leaf的字体不旋转，大小为10。</span>
<span class="c1">#这里我们不显示每一条数据的具体名字标签（varieties），默认以数字标签显示</span>
<span class="n">dendrogram</span><span class="p">(</span><span class="n">mergings</span><span class="p">,</span><span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#在图中显示的数字是最细粒度的叶子，相当于每个样本数据点。</span>
</code></pre></div><p><img loading="lazy" src="output_7_0.png" alt="png"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mergings</span>
</code></pre></div><pre><code>array([[ 3.        ,  6.        ,  0.37233454,  2.        ],
       [11.        , 12.        ,  0.77366442,  2.        ],
       [10.        , 15.        ,  0.89804259,  3.        ],
       [ 5.        , 14.        ,  0.90978998,  3.        ],
       [13.        , 16.        ,  1.02732924,  4.        ],
       [ 0.        ,  2.        ,  1.18832161,  2.        ],
       [ 4.        , 17.        ,  1.28425969,  4.        ],
       [ 7.        , 20.        ,  1.62187345,  5.        ],
       [ 1.        , 19.        ,  2.02587613,  3.        ],
       [ 9.        , 18.        ,  2.13385537,  5.        ],
       [ 8.        , 23.        ,  2.323123  ,  6.        ],
       [22.        , 24.        ,  2.87625877,  9.        ],
       [21.        , 25.        ,  3.12231564, 14.        ]])
</code></pre>
<p>层次分析图从上到下看，依次是枝和叶。</p>
<p>第一列和第二列代表类标签，包含叶子和枝子。</p>
<p>第三列代表叶叶（或叶枝，枝枝）之间的距离</p>
<p>第四列代表该层次类中含有的样本数（记录数）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">X = linkage(y, method=&#39;single&#39;, metric=&#39;euclidean&#39;) 
</code></pre></div><p>method是指计算类间距离的方法,比较常用的有3种:</p>
<p>(1)single:最近邻,把类与类间距离最近的作为类间距</p>
<p>(2)average:平均距离,类与类间所有pairs距离的平均</p>
<p>(3)complete:最远邻,把类与类间距离最远的作为类间距</p>
<p>我们写曾侧分析法函数，看看不同的method从图中有什么区别</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">hierarchy_analysis</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">):</span>
    <span class="n">mergings</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>

    <span class="n">dendrogram</span><span class="p">(</span><span class="n">mergings</span><span class="p">,</span>
              <span class="n">labels</span><span class="o">=</span><span class="n">varieties</span><span class="p">,</span>
              <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span>
              <span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#single</span>
<span class="n">hierarchy_analysis</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="output_12_0.png" alt="png"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#average</span>
<span class="n">hierarchy_analysis</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;average&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="output_13_0.png" alt="png"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#complete</span>
<span class="n">hierarchy_analysis</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;complete&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="output_14_0.png" alt="png"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">由于数据量比较少</span><span class="err">，</span><span class="n">complete和average方法做出来的图完全一样</span><span class="err">。</span>
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Python实证指标构建与文本分析</title>
      <link>https://textdata.cn/blog/management_python_course/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/blog/management_python_course/</guid>
      <description>在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题： 网络爬虫技术 解决 如何从网络世界中高效地 采集数据?文本分析技术 解决 如何从杂乱的文本数据中实证指标(情感、态度、刻板印象等)？In scientific research, data acquisition and analysis are the most important and also the most difficult two links! In the pre-big data era, experimental methods, questionnaires, interviews, or second-hand data were generally used to organize data into structured tabular data, and then use various econometric analysis methods to analyze these tabular data. However, in the era of big data, network data has become a potential treasure that scholars from all walks of life urgently need to discover. A large amount of business information and social information are stored in massive web pages in unstructured and heterogeneous data formats such as text. So for the humanities and social sciences professional researchers represented by economics and management, Python can help scholars solve two problems faced by using Web data for scientific research: Web crawler technology solves how to efficiently collect data from the Internet world? Text analysis How can technical solutions extract empirical indicators (sentiment, attitudes, stereotypes, etc.) from messy text data?</description>
      <content:encoded><![CDATA[<h2 id="python实证指标构建与文本分析httpsmqlchatcomwechatpagechannel-introchannelid2000015158133596"><a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">Python实证指标构建与文本分析</a></h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<br>
<h2 id="概览">概览</h2>
<h3 id="为何要学python">为何要学Python？</h3>
<p>在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！</p>
<p>在<strong>前大数据时代</strong>，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但<strong>大数据时代</strong>，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：</p>
<ol>
<li><strong>网络爬虫技术</strong> 解决 如何从网络世界中高效地 <strong>采集数据</strong>？</li>
<li><strong>文本分析技术</strong> 解决 如何从杂乱的文本数据中<strong>实证指标(情感、态度、刻板印象等)</strong>？</li>
</ol>
<br>
<h3 id="发票事项">发票事项</h3>
<p>如需发票，请先加微信372335839， 咨询发票细节，再作购买</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 企业名称：哈尔滨所以然信息技术有限公司 
- 企业税号：91230109MABT7KBC4M 
- 银行账户:  6228400176412884160
- 开户行:   中国农业银行哈尔滨香坊支行
</code></pre></div><p><img loading="lazy" src="img/dadeng_wechat.jpg" alt=""  />
</p>
<br>
<h3 id="课程纲要">课程纲要</h3>
<ul>
<li><strong>课程目标：</strong> 掌握Python语法、网络爬虫、数据分析Pandas、文本分析、机器学习、词嵌入与认知</li>
<li><strong>核心知识点：</strong> 爬虫原理及应用、 非结构化文本数据挖掘的思路及方法、机器学习应用等</li>
<li><strong>环境配置:</strong>  本文使用Anaconda作为Python的软件安装包，注意安装过程中勾选<strong>Add Anaconda 3.x to PATH</strong></li>
<li><strong>课件资料：</strong> 本课程全部使用jupyter notebook文件作为课程课件</li>
</ul>
<br>
<h3 id="课程特色">课程特色</h3>
<ul>
<li><strong>接地气：</strong> 以经管学术需求为导向， 将Python分为语法篇、采集数据篇、文本分析篇、机器学习篇四大部分</li>
<li><strong>好理解：</strong> 知识点力求通俗易懂，少了晦涩的计算机术语，多了通俗易懂的使用场景和实战讲解</li>
<li><strong>上手快：</strong> 所有知识点均有可重复使用的代码块，犹如一块块的积木，课后您可以根据分析需要，快速搭建出自己的Python代码</li>
<li><strong>技术新</strong>： 最新词嵌入，可挖掘文本中的态度、偏见、刻板印象等。</li>
</ul>
<br>
<h2 id="经管-经典文本分析方法">经管-经典文本分析方法</h2>
<p>在这里我把技术细分为词频、词袋、w2v建词典、w2v认知变迁四个维度，这四大技术方法在本课程中均有体现。为了直观了解课程价值，这里附上9篇文献，大家可以购课前以做参考。</p>
<table>
<thead>
<tr>
<th>文献</th>
<th>定性</th>
<th>词频</th>
<th>词袋</th>
<th>W2V建词典</th>
<th>W2V认知变迁</th>
</tr>
</thead>
<tbody>
<tr>
<td>王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</td>
<td>Y</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/jcr_concreteness_computation/">语言具体性如何影响顾客满意度</a><br>Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” <em>Journal of Consumer Research</em> 47, no. 5 (2021): 787-806.</td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2019-12-08-lazy-prices/">文本相似度</a><br>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>胡楠, 薛付婧 and 王昊楠, 2021. <a href="https://textdata.cn/blog/text_mining_in_2021_management_world/">管理者短视主义</a>影响企业长期投资吗———基于文本分析和机器学习. <em>管理世界</em>, <em>37</em>(5), pp.139-156.</td>
<td></td>
<td>Y</td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>Kai Li, Feng Mai, Rui Shen, Xinyan Yan, <a href="https://github.com/MS20190155/Measuring-Corporate-Culture-Using-Machine-Learning">Measuring Corporate Culture Using Machine Learning</a>, The Review of Financial Studies, 2020</td>
<td></td>
<td></td>
<td>Y</td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>女性就职高管改变组织内性别偏见<br>Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 119, no. 9 (2022): e2026443119.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
<tr>
<td>使用词嵌入技术，量化近百年以来性别和族群的刻板印象<br>Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. &ldquo;Word embeddings quantify 100 years of gender and ethnic stereotypes.&rdquo; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/">利用词嵌入技术，通过计算团队的话语多样性衡量团队的认知多样性</a><br>Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &ldquo;Aligning differences: Discursive diversity and team performance.&rdquo; <em>Management Science</em> 68, no. 11 (2022): 8430-8448.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
</tbody>
</table>
<br>
<h2 id="一课件下载">一、课件下载</h2>
<ol>
<li>
<p>课程介绍</p>
</li>
<li>
<p>Win中的Anaconda软件配置</p>
</li>
<li>
<p>Mac中的Anaconda软件配置</p>
</li>
</ol>
<br>
<h2 id="二python语法入门">二、Python语法入门</h2>
<ol>
<li>Python跟英语一样是一门语言</li>
<li>数据类型之字符串</li>
<li>数据类型之列表元组集合</li>
<li>数据类型之字典</li>
<li>数据类型之布尔值、None</li>
<li>逻辑语句(if&amp;for&amp;tryexcept)</li>
<li>列表推导式</li>
<li>理解函数</li>
<li>常用的内置函数</li>
<li>内置库文件路径pathlib库</li>
<li>内置库csv文件库</li>
<li>内置库正则表达式re库</li>
<li>初学python常出错误汇总</li>
</ol>
<br>
<h2 id="三数据采集">三、数据采集</h2>
<ol>
<li>网络爬虫原理</li>
<li>网络访问requests库</li>
<li>网页解析pyquery库</li>
<li><strong>「案例」</strong> 豆瓣读书</li>
<li><strong>「案例」</strong> Boss直聘</li>
<li>如何解析json数据</li>
<li><strong>「案例」</strong> 豆瓣电影</li>
<li><strong>「案例」</strong> 京东商城</li>
<li><strong>「案例」</strong> 用爬虫下载文档及多媒体文件</li>
<li><strong>「案例」</strong> 上市公司定期报告pdf批量下载</li>
<li><strong>「案例」</strong> 上交所招股说明pdf批量下载</li>
<li><strong>「案例」</strong> 深交所招股说明pdf批量下载</li>
<li>爬虫知识点总结</li>
</ol>
<br>
<h2 id="四数据分析">四、数据分析</h2>
<ol>
<li>Pandas基础知识</li>
<li>数据去重与缺失值处理</li>
<li>合并数据</li>
<li>重塑数据</li>
<li>选取表中指定记录(行)</li>
<li>选取表中指定字段(列)</li>
<li>描述性统计</li>
<li>在表中创建新字段(列)</li>
<li>批操作apply与agg</li>
<li>透视表pivot_table</li>
<li>数据分组groupby</li>
<li>时间序列时间点创建</li>
<li>日期数据的dt属性</li>
<li>日期行索引操作(选取指定日期的数据)</li>
<li>时间序列date_range</li>
<li>时间序列重采样resample</li>
<li>时间序列时间窗口rolling</li>
<li><strong>「案例」</strong> Kaggle titanic数据集探索性分析</li>
<li><strong>「案例」</strong> Boss直聘Python岗位分析</li>
</ol>
<br>
<h2 id="五初识文本分析">五、初识文本分析</h2>
<ol>
<li>
<p>从编码/解码视角重新理解文本</p>
</li>
<li>
<p>读取不同格式文件中的数据</p>
</li>
<li>
<p>如何将多个年报整理到一个excel中</p>
</li>
<li>
<p><strong>「案例」</strong> 中文分词及数据清洗</p>
</li>
<li>
<p><strong>「案例」</strong> 词频统计&amp;词云图</p>
</li>
<li>
<p><strong>「案例」</strong> 共现法扩展情感词典(领域词典)</p>
</li>
<li>
<p><strong>「案例」</strong> 词向量word2vec扩展领域词典</p>
</li>
<li>
<p><strong>「案例」</strong> 中文情感分析(词典法)</p>
</li>
<li>
<p>cntext库 情感分析代码操作</p>
</li>
<li>
<p><strong>「案例」</strong> 对excel中的文本进行情感分析  91</p>
</li>
<li>
<p><strong>「案例」</strong>:  语言具体性与心理距离 | 以JCR2021论文为例</p>
</li>
<li>
<p><strong>「案例」</strong>: 使用LM金融词典对年报进行「语调分析」 | 2018管理世界</p>
</li>
<li>
<p><strong>「案例」</strong>:  使用md&amp;a数据测量企业数字化 | 管理世界、财经研究</p>
</li>
<li>
<p><strong>「案例」</strong>:  使用md&amp;a数据构建标准信息、信息含量  |  中国工业经济</p>
</li>
</ol>
<br>    
<h2 id="六机器学习与文本分析">六、机器学习与文本分析</h2>
<ol>
<li>了解机器学习ML</li>
<li>使用机器学习做文本分析的流程</li>
<li>scikit-learn机器学习库简介</li>
<li>文本特征抽取(特征工程)</li>
<li><strong>「案例」</strong> 在线评论文本分类</li>
<li>使用标注工具对数据进行标注</li>
<li><strong>「案例」</strong> 计算文本情感分析(有权重)</li>
<li><strong>「案例」</strong>  文本相似性计算</li>
<li><strong>「案例」</strong> 使用文本相似性识别变化(政策连续性)</li>
<li><strong>「案例」</strong> 央行货币政策报告文本相似度计算与可视化 | 金融研究</li>
<li><strong>「案例」</strong> Kmeans聚类算法</li>
<li><strong>「案例」</strong> LDA话题模型</li>
<li>使用机器学习从图片中提取文本信息</li>
</ol>
<br>
<h2 id="七词嵌入与认知">七、词嵌入与认知</h2>
<ol>
<li>词嵌入原理及应用概述</li>
<li><strong>「案例」</strong>  豆瓣影评-训练词向量&amp;使用词向量</li>
<li><strong>「案例」</strong>  使用词向量做话题建模</li>
<li><strong>「案例」</strong>  认知指标(态度、偏见等)的测量</li>
<li>总结-文本分析在社科(经管)领域中的应用</li>
</ol>
<p><br><br></p>
<h2 id="相关应用">相关应用</h2>
<p>参照两篇论文的摘要，可以通过场景化等的方式帮助我们迅速理解上面两个问题。摘要部分的加粗内容是论文用到的分析技术，在我们的课程中均有与之对应的知识点和代码。</p>
<p><strong>曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">该文汉化了LM金融词典，并使用LM中文词典进行语调分析。 课程已整理了 LM中英文词典 及 对应代码。
</code></pre></div><blockquote>
<p><strong>摘要</strong>: 基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在**「口是心非」** 的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,<strong>年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与  「年报语调」 相关</strong>。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。</p>
<p><strong>关键词：</strong> 年报; 语调管理; 内部人交易; 信息不对称;</p>
</blockquote>
<br>
<p><strong>王伟,陈伟,祝效国,王洪伟.众筹融资成功率与语言风格的说服性——基于Kickstarter的实证研究[J].管理世界,2016(05):81-98.</strong></p>
<blockquote>
<p><strong>摘要</strong>：众筹融资效果决定着众筹平台的兴衰。众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。</p>
<p>然后，<strong>借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。</strong></p>
<p>最后，建立语言说服风格对项目筹资影响的计量模型，并对 <strong>Kickstarter 平台上的 128345 个项目进行实证分析</strong>。总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。</p>
</blockquote>
<br>
<p><strong>胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.</strong></p>
<blockquote>
<p><strong>摘要</strong> : 在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间 导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并<strong>采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验</strong>。研究结果发现，<strong>年报 MD&amp;A 中披露的「短期视域」 语言</strong> 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。<strong>同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴。</strong></p>
<p><strong>关键词</strong>: 管理者短视; 长期投资; 文本分析; 机器学习</p>
</blockquote>
<br>
<h2 id="相关文献">相关文献</h2>
<p>在这里我把技术细分为词频、词袋、w2v建词典、w2v认知变迁四个维度，整理了经管7篇论文。大家可以阅读这9篇论文，掌握文本分析的应用场景。</p>
<table>
<thead>
<tr>
<th>文献</th>
<th>定性</th>
<th>词频</th>
<th>词袋</th>
<th>W2V建词典</th>
<th>W2V认知变迁</th>
</tr>
</thead>
<tbody>
<tr>
<td>王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</td>
<td>Y</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/jcr_concreteness_computation/">语言具体性如何影响顾客满意度</a><br>Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” <em>Journal of Consumer Research</em> 47, no. 5 (2021): 787-806.</td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2019-12-08-lazy-prices/">文本相似度</a><br>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-01-10-similarity-of-cental-bank-monetary-policy/">文本相似度</a><br>姜富伟,胡逸驰,黄楠.央行货币政策报告文本信息、宏观经济与股票市场[J].金融研究,2021,(06):95-113.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>胡楠, 薛付婧 and 王昊楠, 2021. <a href="https://textdata.cn/blog/text_mining_in_2021_management_world/">管理者短视主义</a>影响企业长期投资吗———基于文本分析和机器学习. <em>管理世界</em>, <em>37</em>(5), pp.139-156.</td>
<td></td>
<td>Y</td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>Kai Li, Feng Mai, Rui Shen, Xinyan Yan, <a href="https://github.com/MS20190155/Measuring-Corporate-Culture-Using-Machine-Learning">Measuring Corporate Culture Using Machine Learning</a>, The Review of Financial Studies, 2020</td>
<td></td>
<td></td>
<td>Y</td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>女性就职高管改变组织内性别偏见<br>Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 119, no. 9 (2022): e2026443119.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
<tr>
<td>使用词嵌入技术，量化近百年以来性别和族群的刻板印象<br>Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. &ldquo;Word embeddings quantify 100 years of gender and ethnic stereotypes.&rdquo; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/">利用词嵌入技术，通过计算团队的话语多样性衡量团队的认知多样性</a><br>Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &ldquo;Aligning differences: Discursive diversity and team performance.&rdquo; <em>Management Science</em> 68, no. 11 (2022): 8430-8448.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216.
[1]洪永淼,刘俸奇,薛涧坡.政府与市场心理因素的经济影响及其测度[J].管理世界,2023,39(03):30-51.
[2]沈艳, 陈赟, &amp; 黄卓. (2019). 文本大数据分析在经济学和金融学中的应用: 一个文献综述. 经济学 (季刊), 18(4), 1153-1186.
[3]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.
[4]张楠,黄梅银,罗亚,马宝君.全国政府网站内容数据中的知识发现：从注意力分配到政策层级扩散[J].管理科学学报,2023,26(05):154-173.
[5]许帅,邵帅,何贤杰.业绩说明会前瞻性信息对分析师盈余预测准确性的影响——信口雌黄还是言而有征[J].中国管理科学:1-15.
[6]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.管理世界.2016;5:81-98.
[7]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[8]孟庆斌, 杨俊华, 鲁冰. 管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J]. 中国工业经济, 2017 (12): 132-150.
[9]曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.
[10]彭红枫, &amp; 林川. (2018). 言之有物: 网络借贷中语言有用吗?——来自人人贷借款描述的经验证据[J]. 金融研究, 461(11), 133-153.
[11]吴非, 胡慧芷, 林慧妍, and 任晓怡. “企业数字化转型与资本市场表现——来自股票流动性的经验证据[J].” 管理世界 (2021).
[12]姜富伟,胡逸驰,黄楠.央行货币政策报告文本信息、宏观经济与股票市场[J].金融研究,2021,(06):95-113.
[13]陈霄,叶德珠,邓洁.借款描述的可读性能够提高网络借款成功率吗[J].中国工业经济,2018,(03):174-192.
[14]罗勇根,饶品贵,陈灿.高管宏观认知具有管理者“烙印”吗?——基于管理者风格效应的实证检验[J].金融研究,2021(05):171-188.
[15]吴胜涛,茅云云,吴舒涵,冯健仁,张庆鹏,谢天,陈浩,朱廷劭.基于大数据的文化心理分析[J].心理科学进展:1-13.
[16]Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &#34;Aligning differences: Discursive diversity and team performance.&#34; *Management Science* 68, no. 11 (2022): 8430-8448.
[17]Rocklage, Matthew D., Sharlene He, Derek D. Rucker, and Loran F. Nordgren. &#34;Beyond Sentiment: The Value and Measurement of Consumer Certainty in Language.&#34; Journal of Marketing Research (2023): 00222437221134802.
[18]Wang, Quan, Beibei Li, and Param Vir Singh. &#34;Copycats vs. Original Mobile Apps: A Machine Learning Copycat-Detection Method and Empirical Analysis.&#34; *Information Systems Research* 29.2 (2018): 273-291.
[19]Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” _Journal of Consumer Research_ 47, no. 5 (2021): 787-806.
[20]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
[21]Loughran T, McDonald B. Textual analysis in accounting and finance: A survey[J]. *Journal of Accounting Research*, 2016, 54(4): 1187-1230. Author links open overlay panelComputational socioeconomics
[22]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &#34;Uniting the tribes: Using text for marketing insight.&#34; *Journal of Marketing* 84, no. 1 (2020): 1-25.
[23]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. &#34;Lazy prices.&#34; *The Journal of Finance* 75, no. 3 (2020): 1371-1415.
[24]Bellstam, Gustaf, Sanjai Bhagat, and J. Anthony Cookson. &#34;A text-based analysis of corporate innovation.&#34; _Management Science_ 67, no. 7 (2021): 4004-4031.
[25]Arts, Sam, Bruno Cassiman, and Jianan Hou. &#34;Position and Differentiation of Firms in Technology Space.&#34; Management Science (2023).
[26]Cookson, J. Anthony, and Marina Niessner. &#34;Why don&#39;t we agree? Evidence from a social network of investors.&#34; The Journal of Finance 75, no. 1 (2020): 173-228.
[27]Mansouri S, Momtaz P P. Financing sustainable entrepreneurship: ESG measurement, valuation, and performance[J]. Journal of Business Venturing, 2022, 37(6):106258.

</code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>词嵌入测量不同群体对某概念的态度(偏见)</title>
      <link>https://textdata.cn/blog/2022-04-01-embeddings-and-attitude/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-04-01-embeddings-and-attitude/</guid>
      <description>人类在留下语言、文字的过程中，也留下了自己的偏见、态度等主观认知信息（偏见、态度）。词嵌入做为一种词向量模型，可以隐含上下文的情景信息，态度及偏见很容易保留在词向量的某些维度中。通过**词向量距离**的测算，就可以间接测得**不同群体** 对 **某概念**(组织、群体、品牌、地域等)的态度偏见。</description>
      <content:encoded><![CDATA[<p>前几天刚刚分享了，</p>
<p><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">大数据时代下社会科学研究方法的拓展—基于词嵌入技术的文本分析的应用</a></p>
<p>人类在留下语言、文字的过程中，也留下了自己的偏见、态度等主观认知信息（偏见、态度）。词嵌入做为一种词向量模型，可以隐含上下文的情景信息，态度及偏见很容易保留在词向量的某些维度中。通过<strong>词向量距离</strong>的测算，就可以间接测得<strong>不同群体</strong> 对 <strong>某概念</strong>(组织、群体、品牌、地域等)的态度偏见。</p>
<p>下面整理了几篇 <strong>集智俱乐部</strong> 分享过词嵌入解读文章， 部分含视频讲解。文章末尾还有更多词嵌入的最新文献，感兴趣的同学也可以收藏。</p>
<h2 id="tips">Tips</h2>
<p>pnas的数据挖掘的论文，大多都含有数据及代码。这里有几个python库，可以可视化刻板印象</p>
<ul>
<li>
<p><a href="https://textdata.cn/blog/whatlies/">whatlies库|可视化词向量</a></p>
</li>
<li>
<p><a href="https://github.com/uber-research/parallax">parallax</a></p>
</li>
<li>
<p><a href="https://github.com/bhavyaghai/WordBias">WordBias</a></p>
</li>
</ul>
<br>
<h2 id="偏见">偏见</h2>
<h3 id="文化中的几何词嵌入如何捕捉文化社会学的微妙关系">文化中的几何：词嵌入如何捕捉文化社会学的微妙关系</h3>
<blockquote>
<p>Kozlowski, A.C., Taddy, M. and Evans, J.A., 2019. The geometry of culture: Analyzing the meanings of class through word embeddings. American Sociological Review, 84(5), pp.905-949.</p>
</blockquote>
<p>来自芝加哥大学和亚马逊的研究者，针对海量文本资料，将所有词向量分解为性别，阶级和种族三个维度，并通过将不同词向量在这三个维度上的投影来给出该词的性别、阶级和种族属性。本文是对这项工作的解读。</p>
<p><a href="https://mp.weixin.qq.com/s/vhtlIggfSp7GUUXNSocYmA">点击查看详细解读</a></p>
<br>
<h3 id="故事的形态可预期其成功">故事的形态可预期其成功</h3>
<blockquote>
<p>Toubia, O., Berger, J. and Eliashberg, J., 2021. How quantifying the shape of stories predicts their success. <em>Proceedings of the National Academy of Sciences</em>, <em>118</em>(26).</p>
</blockquote>
<p>通过NLP，分析了电影、电视剧及科研论文的叙事模式，与其成功之间的关系。发现不同类型的文章，由于大众的认知偏好，促成其成功的叙事模式是不同。作为计算社会学的一部分，该研究通过量化分析，确认了面对不同的叙事模式，存在普遍的认知偏好。</p>
<p><a href="https://mp.weixin.qq.com/s/Y0pDte4GeAAqoZhmP8B8WA">点击查看详细解读</a></p>
<br>
<h3 id="童话里都是骗人的用词向量解析故事中的性别偏见">童话里都是骗人的？用词向量解析故事中的性别偏见</h3>
<blockquote>
<ul>
<li>Xu H, Zhang Z, Wu L, Wang C_J. The Cinderella Complex: Word Embeddings Quantify Gender Stereotypes in Movies and Books. Available from <a href="https://arxiv.org/abs/1811.04599">https://arxiv.org/abs/1811.04599</a>. 2019.06.</li>
<li>Caliskan A, Bryson JJ, Narayanan A. Semantics derived automatically from language corpora contain human-like biases. Science. 2017;356: 183–186.</li>
<li>Garg N, Schiebinger L, Jurafsky D, Zou J. Word embeddings quantify 100 years of gender and ethnic stereotypes . Proceedings of the National Academy of Sciences. 2018. pp. E3635–E3644. doi:10.1073/pnas.1720347115</li>
<li>Dowling C. The Cinderella Complex: Women’s Hidden Fear of Independence. 1982.</li>
</ul>
</blockquote>
<p>“男人是女人通往幸福的道路”——这种偏见是如何通过一个精心设计的故事创造出来的？灰姑娘式的叙事结构形成并强化了&quot;灰姑娘情结&quot;，即女性对独立的恐惧和被他人照顾的无意识欲望。&ldquo;灰姑娘情结&quot;在不同时期和不同文化中广泛存在，这提醒研究我们有必要通过教育、政策和其他方面创造新的叙述方式来与之作斗争。</p>
<p>研究者提出了计算机化的框架分析，通过描绘故事的形状来测量性别刻板印象。词嵌入技术提供了一个强大的替代情感词典的方法，首先，研究团队构建一个“高兴——不高兴”的情感轴，然后计算余弦相似性来得到每一个词的情感得分。</p>
<p><a href="https://mp.weixin.qq.com/s/jY_hobh589D9mEN2-IZKVA">点击查看详细解读</a></p>
<br>
<h3 id="词向量带你洞悉美国性别与种族歧视的100年历史演变">词向量带你洞悉美国性别与种族歧视的100年历史演变</h3>
<p>性别歧视、种族歧视都是存在了上百年的社会现象，这些现象在不同历史时期有怎样的发展变化呢？发表在PNAS这篇论文中，研究者用词向量的方法研究大量文本数据，挖掘出美国近一百年文化刻板印象的演化。</p>
<blockquote>
<p>Garg, N., Schiebinger, L., Jurafsky, D. and Zou, J., 2018. Word embeddings quantify 100 years of gender and ethnic stereotypes. <em>Proceedings of the National Academy of Sciences</em>, <em>115</em>(16), pp.E3635-E3644.</p>
</blockquote>
<p>详细解读请看 <a href="https://mp.weixin.qq.com/s/VroknX42MBdckptv4tELJg">https://mp.weixin.qq.com/s/VroknX42MBdckptv4tELJg</a></p>
<br>
<h3 id="利用向量表征挖掘知识的创造和组织">利用向量表征挖掘知识的创造和组织</h3>
<p>词向量是自然语言处理中的一项基础性技术，通过词语之间的共同出现网络，可以在低维空间表征词汇间的语义相关性。4月23日发表在 Science Advences 的论文，通过论文引用网络，结合神经网络为不同的学科的科研期刊构建了连续的向量化嵌入表征，从中可以了解新知是如何被创造和组织的。</p>
<blockquote>
<p>Peng, H., Ke, Q., Budak, C., Romero, D.M. and Ahn, Y.Y., 2021. Neural embeddings of scholarly periodicals reveal complex disciplinary organizations. <em><strong>Science Advances</strong></em>, <em>7</em>(17), p.eabb9004.</p>
</blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/372087496">点击查看详细解读</a></p>
<br>
<h3 id="量化在线平台中的社会组织和政治两极分化">量化在线平台中的社会组织和政治两极分化</h3>
<p>大量选择志同道合的人可能会分裂和极化网络社会，特别是在党派差异方面。 通过利用大规模的聚合行为模式来量化在线社区在社会维度上的定位。应用 14 年来在 Reddit 上 10,000 个社区中发表的 51 亿条评论，我们衡量了宏观社区结构在年龄、性别和美国政治党派方面的组织方式。</p>
<p>检查政治内容，我们发现 Reddit 在 2016 年美国总统大选前后经历了一次重大的两极分化事件。然而，与传统观念相反，个人层面的两极分化是罕见的。 2016 年的系统级转变主要是由新用户的到来推动的。 Reddit 上的政治两极分化与平台上的先前活动无关，而是在时间上与外部事件保持一致。</p>
<p>研究还观察到明显的意识形态不对称，2016 年两极分化的急剧增加完全归因于右翼活动的变化。这种方法广泛适用于在线互动的研究，我们的研究结果对在线平台的设计、理解在线行为的社会背景以及量化在线两极分化的动态和机制具有重要意义。</p>
<blockquote>
<p>Waller, I. and Anderson, A., 2021. Quantifying social organization and political polarization in online platforms. <em>Nature</em>, <em>600</em>(7888), pp.264-268.
<a href="https://www.bilibili.com/video/av422602096">点击查看详细解读</a></p>
</blockquote>
<br>
<br>
<h2 id="更多文献">更多文献</h2>
<ul>
<li>Arseniev-Koehler, A., Cochran, S.D., Mays, V.M., Chang, K.W. and Foster, J.G., 2022. Integrating topic modeling and word embedding to characterize violent deaths. <em>Proceedings of the National Academy of Sciences</em>, <em>119</em>(10), p.e2108801119.</li>
<li>Bollen, J., Ten Thij, M., Breithaupt, F., Barron, A.T., Rutter, L.A., Lorenzo-Luaces, L. and Scheffer, M., 2021. Historical language records reveal a surge of cognitive distortions in recent decades. <em>Proceedings of the National Academy of Sciences</em>, <em>118</em>(30).</li>
<li>Kim, L., Smith, D.S., Hofstra, B. and McFarland, D.A., 2022. Gendered knowledge in fields and academic careers. <em>Research Policy</em>, <em>51</em>(1), p.104411.</li>
<li>Lawson, M.A., Martin, A.E., Huda, I. and Matz, S.C., 2022. Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language. <em>Proceedings of the National Academy of Sciences</em>, <em>119</em>(9), p.e2026443119.</li>
<li>Brady, W.J., McLoughlin, K., Doan, T.N. and Crockett, M.J., 2021. How social learning amplifies moral outrage expression in online social networks. <em>Science Advances</em>, <em>7</em>(33), p.eabe5641.</li>
<li>Bailey, A.H., Williams, A. and Cimpian, A., 2022. Based on billions of words on the internet, people= men. <em>Science Advances</em>, <em>8</em>(13), p.eabm2463.</li>
<li>Lewis, M. and Lupyan, G., 2020. Gender stereotypes are reflected in the distributional structure of 25 languages. <em>Nature human behaviour</em>, <em>4</em>(10), pp.1021-1028.</li>
<li>Schramowski, P., Turan, C., Andersen, N., Rothkopf, C.A. and Kersting, K., 2022. Large pre-trained language models contain human-like biases of what is right and wrong to do. <em>Nature Machine Intelligence</em>, <em>4</em>(3), pp.258-268.</li>
<li>Costa-jussà, M.R., 2019. An analysis of gender bias studies in natural language processing. <em>Nature Machine Intelligence</em>, <em>1</em>(11), pp.495-496.</li>
<li>Rodman, E., 2020. A timely intervention: Tracking the changing meanings of political concepts with word vectors. <em><strong>Political Analysis</strong></em>, <em>28</em>(1), pp.87-111.</li>
<li>Bhatia, S., 2017. Associative judgment and vector space semantics. <em><strong>Psychological review</strong></em>, <em>124</em>(1), p.1.</li>
<li>Kurdi, B., Mann, T.C., Charlesworth, T.E. and Banaji, M.R., 2019. The relationship between implicit intergroup attitudes and beliefs. <em>Proceedings of the National Academy of Sciences</em>, <em>116</em>(13), pp.5862-5871.</li>
<li>Charlesworth, T.E., Yang, V., Mann, T.C., Kurdi, B. and Banaji, M.R., 2021. Gender stereotypes in natural language: Word embeddings show robust consistency across child and adult language corpora of more than 65 million words. <em><strong>Psychological Science</strong></em>, <em>32</em>(2), pp.218-240.</li>
<li>Bhatia, S., 2019. Predicting risk perception: New insights from data science. <em><strong>Management Science</strong></em>, <em>65</em>(8), pp.3800-3823.</li>
<li>Rheault, L. and Cochrane, C., 2020. Word embeddings for the analysis of ideological placement in parliamentary corpora. <em>Political Analysis</em>, <em>28</em>(1), pp.112-133.</li>
<li>Yang, K., Lau, R.Y. and Abbasi, A., 2022. Getting Personal: A Deep Learning Artifact for Text-Based Measurement of Personality. <em><strong>Information Systems Research</strong></em>.</li>
<li>Rodman, E., 2020. A timely intervention: Tracking the changing meanings of political concepts with word vectors. <em>Political Analysis</em>, <em>28</em>(1), pp.87-111.</li>
<li>Margulis, E.H., Wong, P.C., Turnbull, C., Kubit, B.M. and McAuley, J.D., 2022. Narratives imagined in response to instrumental music reveal culture-bounded intersubjectivity. <em>Proceedings of the National Academy of Sciences</em>, <em>119</em>(4).</li>
<li>Thompson, B., Roberts, S.G. and Lupyan, G., 2020. Cultural influences on word meanings revealed through large-scale semantic alignment. <em>Nature Human Behaviour</em>, <em>4</em>(10), pp.1029-1038.</li>
</ul>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
