<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>文本分析 on 大邓和他的PYTHON</title>
    <link>/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/</link>
    <description>Recent content in 文本分析 on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Mon, 29 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LIST | 社科(经管)数据挖掘文献资料汇总</title>
      <link>https://textdata.cn/blog/the_text_analysis_list_about_ms/</link>
      <pubDate>Mon, 15 Apr 2024 18:43:10 +0600</pubDate>
      
      <guid>/blog/the_text_analysis_list_about_ms/</guid>
      <description>如何从网络世界中高效地采集数据？是否能从文本中挖掘出人类的偏见等认知信息？如何从杂乱的文本数据中抽取文本信息(变量)？本文汇总的列表将让你对文本、对Python文本分析个全面的了解</description>
      <content:encoded><![CDATA[<p>个人感觉博客 <strong><a href="https://textdata.cn/">textdata.cn</a></strong> 精华就在这里了。 不定期更新， 内容聚焦于Python文本分析在经管、社科等领域的应用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 营销
- 会计学
- 经济学
- 心理学
- 社会学
- ...
</code></pre></div><p>读几篇文章能加深对各领域文本分析方法应用的理解。</p>
<br>
<h2 id="管理学">管理学</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/read_this_you_will_know_what_is_text_mining/">读完本文你就了解什么是文本分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-05-xjtu-text-mining-in-ms/">2023分享 | 文本分析在经济管理研究中的应用</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-09-08-dufe-text-mining-in-ms/">视频2022 | 文本分析在经济管理研究中的应用</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-10-11-how-can-machine-learning-empower-management-research/">管理世界 | 机器学习如何赋能管理学研究？——国内外前沿综述和未来展望</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-08-measurement_of_psychological_factors_and_their_economic_impact/">管理世界 | 政府与市场心理因素的经济影响及其测度</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/">MS2022 | 使用语言差异性测量 <strong>团队认知差异性</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-10-10-measure-the-speed-of-policy-diffusion-from-top-to-down/">管理科学学报 | 使用LDA算法计算政策扩散速度与扩散程度</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-09-07-management-science-disrupt-science-and-technology">Management Science | 使用网络算法识别创新的颠覆性与否</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/research_with_tm_in_chinese_top_ms_journal/">近年《管理世界》《管理科学学报》使用文本分析论文</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="营销">营销</h2>
<ul>
<li><a href="https://textdata.cn/blog/text_mining_in_marketing_research/">文本分析在市场营销研究中的应用</a></li>
<li><a href="https://textdata.cn/blog/jcr_concreteness_computation/">JCR2021 | 计算文本的 <strong>语言具体性</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-10-16-measurement-of-consumer-certainty-in-language/">JMR2023 | 测量消费者的 <strong>语言确定性</strong></a></li>
<li><a href="https://textdata.cn/blog/2022-12-03-scraping-web-data-for-marketing-insights/">JM2022综述 | 黄金领域: 为营销研究(新洞察)采集网络数据</a></li>
<li><a href="https://textdata.cn/blog/2024-04-12-semantic-brand-score/">JBR2018  | <strong>语义品牌评分(Semantic Brand Score)</strong></a></li>
<li><a href="https://textdata.cn/blog/automate_text_analysis_in_market/">营销研究中文本分析应用概述(含案例及代码)</a></li>
</ul>
<p><br><br></p>
<h2 id="会计金融">会计&amp;金融</h2>
<ul>
<li><a href="https://textdata.cn/blog/accountingtext/">视频分享 | 会计领域中的Python文本分析</a></li>
<li><a href="https://textdata.cn/blog/2023-08-26-text-analysis-in-accounting/">CAR2023 | 文本分析在会计中的应用</a></li>
<li><a href="https://textdata.cn/blog/2024-04-19-ai-improve-firm-productivity/">管理世界2024 | 使用管理层讨论与分析测量「<strong>企业人工智能指标</strong>」</a></li>
<li><a href="https://textdata.cn/blog/manager_tone_analysis_with_lm/">管理世界 | 使用LM中文金融词典对年报进行语调分析</a></li>
<li><a href="https://textdata.cn/blog/text_mining_in_2021_management_world/">管理世界| 使用文本分析&amp;机器学习测量 「<strong>短视主义</strong>」</a></li>
<li><a href="https://textdata.cn/blog/2022-11-03-mda-measure-digitalization/">管理世界 | 使用 经营讨论与分析测量 「<strong>企业数字化</strong>」</a></li>
<li><a href="https://textdata.cn/blog/2024-12-31-using-regex-to-compute-the-financial_constraints/">管理世界（付费） | 使用md&amp;a数据中计算 「<strong>企业融资约束指标</strong>」</a></li>
<li><a href="https://textdata.cn/blog/2024-12-31-the-experience-of-ceo-to-vector-with-graphe-embeddings/">如何用图嵌入(网络思维和嵌入思维)表征企业，表征高管的职业经历</a></li>
<li><a href="https://textdata.cn/blog/2024-12-31-measure-corporate-culture-using-word2vec/">使用 Word2Vec 和 TF-IDF 计算五类企业文化</a></li>
<li><a href="https://textdata.cn/blog/2019-12-08-lazy-prices/">文本相似 | Lazy Prices公司年报内容变动预示重大风险</a></li>
<li><a href="https://textdata.cn/blog/2023-10-07-esg-measurement/">使用文本分析度量企业ESG属性</a></li>
<li><a href="https://textdata.cn/blog/2023-05-23-soft-cosine-similarity/">管理科学学报 |  使用「<strong>软余弦相似度</strong>」测量业绩说明会「<strong>答非所问程度</strong>」</a></li>
<li><a href="https://textdata.cn/blog/2023-09-08-earnings-communication-conference-forward-looking-statements-information/">中国管理科学(付费) | 使用业绩说明会文本数据测量 「<strong>上市公司前瞻性信息</strong></a>」</li>
<li><a href="https://textdata.cn/blog/2023-01-06-mda_informative_content/">中国工业经济（付费） | 使用Python测量MD&amp;A「<strong>信息含量</strong> 」指标</a></li>
<li><a href="https://textdata.cn/blog/2023-01-13-information-content-of-critical-audit/">金融研究(付费) | 使用Python测量关键审计事项「<strong>信息含量</strong>」指标</a></li>
<li><a href="https://textdata.cn/blog/2023-01-10-similarity_of_cental_bank_monetary_policy/">金融研究 | 央行货币政策文本相似度计算与可视化</a></li>
<li><a href="https://textdata.cn/blog/2023-01-16-papers-using-text-mining-tech-in-journal-of-economic-research/">近年《经济研究》中「文本分析」相关论文</a></li>
<li><a href="https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/">代码 | 使用「新闻数据」测量 「<strong>经济政策不确定性EPU指标</strong>」</a></li>
<li><a href="https://textdata.cn/blog/2024-04-25-firm-economic-policy-uncertainty/">代码 | 使用 「MD&amp;A文本」测量「<em><strong>企业不确定性感知FEPU指标</strong></em>」</a></li>
<li><a href="https://textdata.cn/blog/2022-11-16-literature-review-textmining-in-finance-yao2020/">转载 | 金融学文本大数据挖掘方法与研究进展</a></li>
<li><a href="https://textdata.cn/blog/2023-01-12-review_about_accounting_text_mining/">转载 | 国外会计文本信息实证研究述评与展望</a></li>
</ul>
<br>
<br>
<h2 id="经济学">经济学</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-04-09-narrative-economic-method/">叙事经济学：揭示经济中的叙事</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-12-30-review-about-socioeconomic-status-analysis/">转载 | 大数据驱动的「社会经济地位」分析研究综述</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-09-19-quantitative-history-economic/">文献汇总 | 量化历史学与经济学研究</a></p>
</li>
</ul>
<br>
<br>
<h2 id="心理学">心理学</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-03-31-pnas-measure-replicability-of-psychology-with-ml/">PNAS | 14000+篇心理学顶刊论文可复现性调研</a></li>
<li><a href="https://textdata.cn/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/">PNAS | 使用语义距离测量一个人的 <strong>创新力</strong>(<strong>发散思维</strong>)得分</a></li>
<li><a href="https://textdata.cn/blog/2023-03-10-psychological-research-with-word-embeddings/">基于词嵌入技术的心理学研究: 方法及应用</a></li>
<li><a href="https://textdata.cn/blog/2023-10-18-the-relationship-between-semantic-distance-with-creativity/">心理科学进展 | <strong>语义距离</strong> 与 <strong>创造性思维</strong> 关系的元分析</a></li>
<li><a href="https://textdata.cn/blog/2023-02-13-computing-cultural-psychology-with-big-data/">转载 | 大数据时代的「计算文化心理学」</a></li>
</ul>
<p><br><br></p>
<h2 id="社会学">社会学</h2>
<ul>
<li><a href="https://textdata.cn/blog/2022-12-03-social-computing-methodology-about-big-data-and-artificial-intelligence/">转载 | 社会计算驱动的社会科学研究方法</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/"><strong>可视化 | 人民日报语料反映七十年文化演变</strong></a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
<li><a href="https://textdata.cn/blog/2021-12-19-pnas_historical_language/">PNAS | 历史语言记录揭示了近几十年来认知扭曲的激增</a></li>
<li><a href="https://textdata.cn/blog/2022-01-02-pnas_love_separate/">PNAS | 情侣分手3个月前就有预兆！聊天记录还能反映分手后遗症</a></li>
<li><a href="https://textdata.cn/blog/2023-03-13-linguistic-positivity-in-historical-texts-reflects-dynamic-environmental-and-psychological-factors/">PNAS | 历史文本中的语言积极性反映了动态的环境和心理因素(含Python代码)</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2021-12-28-pnas_culture_bridges/">PNAS | 文本网络分析&amp;文化桥梁 Python 代码实现</a></li>
<li><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
<li><a href="https://textdata.cn/blog/2022-04-01-embeddings-and-attitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
<li><a href="https://textdata.cn/blog/2023-03-03-extracts-cognitive-information-and-visualization-with-embedings/">可视化  |  词嵌入模型用于计算社科领域刻板印象等信息（含代码）</a></li>
<li><a href="https://textdata.cn/blog/2021-12-27-pnas_text_fluency/">PNAS | 词汇熟悉度对线上参与和资金筹集的预测性效用</a></li>
</ul>
<p><br><br></p>
<h2 id="其他">其他</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-01-23-china-national-social-science-fund-projects-from-2010-to-2023/">2010-2023年国家社会科学基金立项名单.xlsx</a></li>
<li><a href="https://textdata.cn/blog/2023-04-07-sapir-whorf-hypothesis/">语言相对性论 | 语言是否决定/影响人的思维和认知</a></li>
<li><a href="https://textdata.cn/blog/2023-11-16-how-to-understand-the-meaning-of-gpt/">Word Embeddings、Transformer与GPT：一文揭示三者关系</a></li>
<li><a href="https://textdata.cn/blog/2023-11-13-violatating-privacy-via-inference-with-large-language-model/">大模型的隐私推断能力 | 不可不防的大模型“人肉搜索”能力</a></li>
<li><a href="https://textdata.cn/blog/text_readability/">文本可读性研究及应用清单</a></li>
<li><a href="https://mp.weixin.qq.com/s/mefUYQnTn8vdWV78c9lRBw">多维度、细粒度情感词库的核心思想与建设过程概述</a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><strong>付费视频课程 | Python实证指标构建与文本分析</strong>
<ul>
<li>大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与<a href="https://textdata.cn/blog/2022-05-workshop/7-Python.html">直播课</a>。</li>
<li>如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的<a href="https://textdata.cn/blog/management_python_course">录播课</a>。</li>
<li>如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读<a href="https://textdata.cn/blog/paid_for_service">有偿说明</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>LIST| 文本分析代码资料汇总</title>
      <link>https://textdata.cn/blog/text_analysis_code_list_about_ms/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/text_analysis_code_list_about_ms/</guid>
      <description>如何使用Python从网络中爬取数据，如何从文本数据中抽取信息。本文汇总了常见的python代码案例，方便大家快速学习</description>
      <content:encoded><![CDATA[<p>个人感觉博客 <strong><a href="https://textdata.cn/">textdata.cn</a></strong> 文本分析代码案例都集中在这里了，我将内容按大类分成</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- Python语法
- 数据采集
- 数据处理&amp;Pandas
  - 正则表达式
  - pandas常用方法
  - pandas性能优化
  - 其他操作
- 文本分析
  - 概览
  - 词典法
  - 词向量
  - 大语言模型
- 数据标注&amp;机器学习
  - 数据标注
  - 监督机器学习
  - 非监督机器学习
- 可视化
- R语言
- 其他
</code></pre></div><p><br><br></p>
<h2 id="一python语法">一、Python语法</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/30_days_of_python/">30天Python编程学习挑战</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/dadeng_python_basic_tutorial/">Python语法入门 | 含视频代码</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-07-19-advanced-python-mastery/"><strong>免费下载 | 进阶Python学习资料</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-18-how-to-use-if-elif-else-in-one-line/">如何在一行代码中实现if-elif-else三分支语句</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-01-tricks-for-better-python-code-with-examples/">12个优雅的python代码使用案例</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/course_recommendation_about_social_science/">免费社科类Python编程课程列表</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-10-bidirectional-mapping-library/">bidict库 | Python双向映射功能，让字典更好用</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2024-01-03-how-to-design-lambda-function/">如何设计好 lambda 函数 ？</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="二数据采集">二、数据采集</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-10-13-crawler-for-qyer/">网络爬虫 |  采集穷游网某城市旅游景点</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-05-07-bilibili-video-info-list/">网络爬虫 | 使用Python披露采集 Up 主视频列表详情信息</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-05-12-welcome-to-zibo-barbecue/"><strong>网络爬虫 | 批量采集话题「如何评价淄博烧烤？」的回答</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-23-data-collector-for-douban-group-parent-child-relationship/">网络爬虫 | 使用Python采集豆瓣「全职儿女」小组组员信息</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-23-data-collector-for-bilibili-danmu/">网络爬虫(付费) | 使用Python采集B站弹幕和评论数据</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/qdata_collect_baidu_index/">百度指数 | 使用qdata采集百度指数</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-10-08-find-sns-account-information-with-maigret/"> Maigret库 | 查询某用户名在各平台网站的使用情况</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="三数据处理pandas">三、数据处理&amp;Pandas</h2>
<h3 id="31-文本处理">3.1 文本处理</h3>
<p>使用正则表达式可以筛选文本数据，做数据预处理(数据清洗)</p>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-02-18-regex-expression-examples/">正则表达式 | 词频统计、情感分析、融资约束</a></p>
</li>
<li>
<p><a href="https://textdata.cn/2023-10-30-raw-mbti-users/">文本分析 | 使用正则表达式判别微博用户mbti类型</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-12-regex-expression-generated-by-chatgpt/">数据清洗 | 借助 chatGPT 设计正则表达式</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-17-how-to-generate-panel-data-from-gov-report-dataset/"><strong>代码 | 使用地方gov工作报告生成某类概念词频「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-27-measure-gov-digitalization/">代码 | 使用gov工作报告生成数字化词频「面板数据」</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-18-how-to-generate-panel-data-from-daily-news-dataset/"><strong>代码 | 使用「新闻数据」构造概念词提及量「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-26-cctv1-xwlb-news-text-dataset/"><strong>数据代码| 使用cctv新闻联播文稿构造「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-19-word-in-context/">word_in_context | 查看某类词的上下文，更好的理解文本数据</a></p>
</li>
</ul>
<br>
<h3 id="32-常用方法">3.2 常用方法</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2028-12-18-how-to-extract-data-from-patent-application-dataset/"><strong>代码 | 使用3571w专利申请数据集构造「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/">代码 | 使用「新闻数据」测量 「<em><strong>经济政策不确定性EPU指标</strong></em>」</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2024-04-25-firm-economic-policy-uncertainty/">代码 | 使用 「MD&amp;A文本」测量「<em><strong>企业不确定性感知FEPU指标</strong></em>」</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-26-matching-listed-corporate-with-patent-dataset/">从3571w条专利数据集「匹配」上市公司的专利信息</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-05-31-resample-groupby-in-pandas/">可视化 | 使用groupby或resample按月份分组绘制高管违规量趋势图</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-08-31-data-visualization-how-to-plot-a-map-with-geopandas/">可视化 | 使用geopandas可视化地图数据</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-27-cheatsheet-about-text-manipulate-in-python/">CheatSheet | Python文本数据处理速查表</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-27-pandas-dataframe-tutorial-in-python/">Pandas库 | DataFrame类常用知识点总结</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-30-query-method-in-dataframe/">Pandas库 | 使用 df.query 字符串表达式进行数据筛选</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-08-07-using-str-contains-method-to-judge-some-specific-content-in-excel/">Pandas库 | 对高管数据xlsx中的简介字段做文本分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2024-03-29-dataframe-add-sub-mul-div/">Pandas技巧 | DataFrame的四则运算</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/text_analysis_in_pandas/">使用Pandas处理文本数据</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/pandas_example_company_analysis/">Pandas小案例 | 对某公司同年的某指标批量汇总</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-11-xiaohongshu-data-analysis/">数据分析 | 使用决策树分析小红书帖子数据(含代码)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-25-zhihu-parent-child-relationship/">数据分析 | 知乎热门话题「全职儿女」</a></p>
</li>
</ul>
<br>
<h3 id="33-性能优化其他操作">3.3 性能优化&amp;其他操作</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-12-27-polars-tutorial-an-altertaive-of-pandas/"><strong>Polars库 | 最强 Pandas 平替来了</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-17-modin-accecerate-your-process/">Modin库，只需一行代码加速你的Pandas</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-19-pandarallel-speed-up-pandas/"><strong>pandarallel库 | 多核运行提升pandas速度</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-17-how-handle-mega-csv-that-far-exceed-memory/"><strong>推荐 | 如何处理远超电脑内存的csv文件</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-08-pandas-5-trips-you-may-or-not-may-know/">5个你或许不知道的pandas数据导入技巧</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-11-16-how-to-fix-string-unicode-decode-error/">如何正确读入文本数据不乱码(解决文本乱码问题)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-30-pipeline-for-data-analysis/">使用流水线pipeline模式设计并处理数据</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="四文本分析">四、文本分析</h2>
<h3 id="41-概览">4.1 概览</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/liwc_python_text_mining/">LIWC vs Python  | 文本分析之词典词频法略讲(含代码)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/text_mining_in_accouting_research/">在会计研究中使用Python进行文本分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-15-how-to-learn-python-data-mining-with-chatgpt/">借助chatGPT更高效地学习「Python实证指标构建与文本分析」</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-10-08-nlp-roadmap/">nlp-roadmap | 文本分析知识点思维脑图</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/culture_analysis/">Python与文化分析入门</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog2023-02-01-chatgpt-usage-first-time/">使用 chatGPT 撰写 Python 文本分析代码</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/chinese_emobank/">EmoBank | 中文维度情感词典</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2024-01-21-chinese-traditional-to-simplified-text/">opencc | 中文简体、繁体转换库</a></p>
</li>
</ul>
<br>
<h3 id="42-词典法">4.2 词典法</h3>
<ul>
<li><a href="https://textdata.cn/blog/cntext_tutorial/">cntext库 | 中文情感分析包</a></li>
<li><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">推荐 | 中文文本分析cntext2.x库使用手册</a></li>
<li><a href="https://textdata.cn/blog/weighted_tfidf_sentiment_analysis/">tfidf有权重的情感分析</a></li>
<li><a href="https://textdata.cn/blog/asent_sentiment_analysis/">Asent库 | 英文文本数据情感分析</a></li>
<li><a href="https://textdata.cn/blog/share_your_dict_to_cntext/">欢迎各位向cntext库分享情感词典</a></li>
<li><a href="https://textdata.cn/blog/chinese_financial_dictionary/">中文金融情感词典</a></li>
<li><a href="https://textdata.cn/blog/how_chinese_tmtai_impact_corporate_inovation/">文本分析 | 中国企业高管团队创新注意力</a></li>
</ul>
<br>
<h3 id="43-社交网络分析">4.3 社交网络分析</h3>
<ul>
<li><a href="https://textdata.cn/blog//2024-04-12-semantic-brand-score/">文献&amp;代码 | 使用Python计算 <strong>语义品牌评分(Semantic Brand Score)</strong></a></li>
</ul>
<br>
<h3 id="44-词向量">4.4 词向量</h3>
<ul>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/"><strong>可视化 | 人民日报语料反映七十年文化演变</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-04-26-chinese-it-industry-slangs-words/"><strong>实验 | 互联网黑话与MD&amp;A</strong></a></li>
<li><a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></li>
<li><a href="https://textdata.cn/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/">词向量 | 使用1亿B站用户签名训练word2vec词向量</a></li>
<li><a href="https://textdata.cn/blog/2022-11-07-embeddings-theory-applicaiton-liuhuanyong/">预训练词向量模型的方法、应用场景、变体延伸与实践总结</a></li>
<li><a href="https://textdata.cn/blog/2022-10-16-python-word-mover-s-distance/"> Python | 词移距离(Word Mover&rsquo;s Distance)</a></li>
<li><a href="https://textdata.cn/blog/2022-11-22-glove-embeddings-model/">训练&amp;使用 Glove 语言模型， 可度量刻板印象等</a></li>
<li><a href="https://textdata.cn/blog/bertopic_tutorial/">BERTopic库 | 使用预训练模型做话题建模</a></li>
<li><a href="https://textdata.cn/blog/2022-12-03-dynamic_topic_model_with_bertopic/">BERTopic | 使用推特数据构建 <strong>动态主题模型模</strong></a></li>
<li><a href="https://textdata.cn/blog/keybert_tutorial/">KeyBERT | 关键词发现库</a></li>
<li><a href="https://textdata.cn/blog/top2vec_tutorial/">Top2Vec | 主题建模和语义搜索库</a></li>
<li><a href="https://textdata.cn/blog/2022-11-17-finbert-finance-bert-model/">FinBERT | 金融文本BERT模型，可情感分析、识别ESG和FLS类型</a></li>
<li><a href="https://textdata.cn/blog/sentence-transformer-tutorial/">sentence-transformer库 | 句子语义向量化</a></li>
<li><a href="https://textdata.cn/blog/wordbias/">WordBias库 | 发现偏见(刻板印象)的交互式工具</a></li>
<li><a href="https://textdata.cn/blog/2023-10-27-nlp_gte_sentence-embedding_chinese/">GTE中文通用文本向量表示模型</a></li>
<li><a href="https://textdata.cn/blog/shifterator_text_vis/">Shifterator库 | 词移图分辨两文本用词风格差异</a></li>
</ul>
<br>
<h3 id="44-大语言模型">4.4 大语言模型</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-02-23-simplet5-one-line-summary/">simpleT5 库 | 根据英文摘要内容生成标题</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-20-how-to-use-llms-tobuild-better-clustering-models/"><strong>以聚类为例 | 使用大语言模型LLM做文本分析</strong></a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="五提取特征机器学习">五、提取特征&amp;机器学习</h2>
<h3 id="51--监督机器学习">5.1  监督机器学习</h3>
<ul>
<li><a href="https://textdata.cn/blog/ml_credit_card_fraud_detection/">机器学习实战 | 信用卡欺诈检测</a></li>
<li><a href="https://textdata.cn/blog/speed_up_sklearn_code_with_sklearnex/">sklearnex库 | 让你的scikit-learn代码加速百倍</a></li>
<li><a href="https://textdata.cn/blog/label_studio_test/">Label-Studio|多媒体数据标注工具</a></li>
<li><a href="https://textdata.cn/blog/doccano_text_anotation/">doccano|为机器学习建模做数据标注</a></li>
</ul>
<br>
<h3 id="52-非监督机器学习">5.2 非监督机器学习</h3>
<ul>
<li>
<p><a href="https://textdata.cn/blog/hierarchy_dendrogram_tutorial/">使用scipy实现层次聚类分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/svd_in_recommendation_system/">推荐系统与协同过滤、奇异值分解</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/customer_segment_with_kmeans/">实战 | 构建基于客户细分的 K-Means 聚类算法！</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-14-using-lda-to-predict-topic/">代码 | 使用LDA预测文本的话题类型</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-25-tomotopy_is_the_fastest_topic_model/">tomotopy库 | 速度最快的LDA主题模型</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="六可视化">六、可视化</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-01-23-umap/"><strong>可视化 | 使用umap对200维词向量的进行降维和可视化</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-11-25-r-patchwork/">使用patchwork包进行多图排版</a></li>
<li><a href="https://textdata.cn/blog/2024-01-21-datamapplot/"><strong>可视化 | 使用 DataMapPlot 绘制数据地图</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-05-11-bilibili-dongbei-big-brother/">B站 | &ldquo;高铁互殴&quot;视频词云图绘制</a></li>
<li><a href="https://textdata.cn/blog/2023-03-22-bedtime-topic_model_visualization/">可视化 | 睡前消息的科学社会、科学技术、社会化抚养话题可视化</a></li>
<li><a href="https://textdata.cn/blog/whatlies_word2vec/">可视化 | 使用whatlies库可视化词向量</a></li>
<li><a href="https://textdata.cn/blog/2022-11-29-santi-relationship-visualization-with-pyecharts/">可视化 | 绘制《三体》人物关系网络图</a></li>
<li><a href="https://textdata.cn/blog/2023-04-03-visualization-wordcloud-similarity-for-santi/">可视化 | 文本数据分成n等份、词云图、情绪变化趋势、相似度变化趋势</a></li>
<li><a href="https://textdata.cn/blog/2023-05-18-weibo-sentiment-score-line-plot/">可视化 | 微博用户群体情绪随时间变化趋势</a></li>
<li><a href="https://textdata.cn/blog/2023-02-11-chatgpt-plus-for-text-mining/">可视化 | 使用 chatGPT 做词频统计&amp;词云图</a></li>
<li><a href="https://textdata.cn/blog/2023-08-28-best-practice-netflix-data-visualization/"><strong>可视化（推荐） | Netflix 数据可视化最佳实践</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-08-31-data_eda_2021_happiness_and_population/"><strong>可视化 | 2021年幸福指数&amp;人口数据可视化最佳实践</strong></a></li>
<li><a href="https://textdata.cn/blog/pyplutchik_emotion_circle/">可视化 | 使用PyPlutchik库可视化文本的情绪轮(情绪指纹)</a></li>
<li><a href="https://textdata.cn/blog/2023-02-11-pyanimate-create-vis-video/">可视化 | 使用pynimate库绘制动态可视化图</a></li>
<li><a href="https://textdata.cn/blog/2022-12-10-lovelyplots/">可视化 | 使用LovelyPlots库绘制科学论文、论文和演示文稿的可视化图形</a></li>
<li><a href="https://textdata.cn/blog/2023-06-02-r-ggdag/">可视化 | 使用ggdag包绘制有向图</a></li>
<li><a href="https://textdata.cn/blog/2023-04-13-prettymaps/">prettymaps库 | 绘制绝美地图</a></li>
</ul>
<p><br><br></p>
<h2 id="七r语言">七、R语言</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-25-ppsr-predictive-power-sccore/">相关性分析 | 从模型预测出发挖掘更多特征之间的关系</a></li>
<li><a href="https://textdata.cn/blog/2022-09-04-r-ggplot2-scatter/">R语言 | ggplot2简明绘图之散点图</a></li>
<li><a href="https://textdata.cn/blog/2022-09-04-r-ggplot2-histogram/">R语言 | ggplot2简明绘图之直方图</a></li>
<li><a href="https://textdata.cn/blog/2022-09-04-r-ggplot2-ggplotly/">R语言 | ggplot2简明绘图之动态图</a></li>
<li><a href="https://textdata.cn/blog/2022-09-04-posterdown/">R语言 | 使用posterdown包制作学术会议海报</a></li>
<li><a href="https://textdata.cn/blog/2022-09-20-r-ggsci/">R语言 | 使用ggsci包绘制sci风格图表</a></li>
<li><a href="https://textdata.cn/blog/2022-09-20-r-ggplot2-ggpubr/">R语言 | ggpubr包让数据可视化更加优雅</a></li>
<li><a href="https://textdata.cn/blog/2022-09-21-r-easystats-report/">R语言 | 让统计更easy的easystats集合包</a></li>
<li><a href="https://textdata.cn/blog/2022-10-07-r-shiny-reactive/">R语言 | 使用shiny的reactive表达式写应用程序</a></li>
<li><a href="https://textdata.cn/blog/2022-10-07-r-stargazer/">R语言 | 使用stargazer包输出格式化回归结果</a></li>
<li><a href="https://textdata.cn/blog/2022-10-12-r-word2vec/">R语言 | 使用word2vec词向量模型</a></li>
<li><a href="https://textdata.cn/blog/2023-01-20-visualization-of-sentiment-analysis-of-historical-text-data-with-r/">R语言 | 绘制文本数据情感历时趋势图</a></li>
</ul>
<p><br><br></p>
<h2 id="八其他">八、其他</h2>
<ul>
<li>
<p><a href="https://textdata.cn/2024-04-21-tqdm-progress-bar/">tqdm库 | Python中实现进度条的几种方式</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2024-01-16-cpca-china-province-city-area/"> cpca库 | 中国省、市区划匹配库</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/causal_inference/">causalinference库 | 使用Python做因果推断</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-26-using-ruptures-to-detect-change-point/">使用 Ruptures 识别时间序列数据中的变化点</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-31-using-poetry-to-manage-your-project-env/">硬核 | 使用Poetry发布Python库到PyPi的方法</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/karateclub_tutorial/">karateclub库 | 计算社交网络中节点的向量</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-21-create-brower-data-label-tools-with-nicegui/">NiceGUI库 | 简单易懂的Web GUI开发包； 可开发数据标注工具、心理学实验工具等</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-09-01-how_to_use_tinytex/">Latex | 为Rmarkdown配置tinytex环境</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-13-add-cls-to-tex-global-enviroment-path/">Latex | 将 .cls 更新到本地 Tex 发行版的搜索路径</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-11-25-faker-generate-test-data/">Faker库 | 生成实验数据</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="九工具">九、工具</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2024-01-31-langchain-chatchat/">使用 Langchain-Chatchat 搭建本地知识库问答系统</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-16-free-chatgpt-list/">免费可用的chatGPT镜像站点清单</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-26-chatgpt-for-jupyter/">在 Jupyter Notebook 内使用 ChatGPT 服务</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-15-how-to-sign-up-the-chatgpt-accout-and-upgrade-to-plus/">如何注册chatGPT账号</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-11-credit_card_for_chatgpt-plus/">使用虚拟信用卡，国内用户升级为chatGPT plus会员</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-15-write-web-scraper-with-chatgpt/">使用 chatGPT 写 Python 网络爬虫</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-18-rath-next-generation-business-intelligence/">Rath | 自动化数据分析工具</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-01-v2net-science-network/">科学上网工具v2net</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><strong>付费视频课程 | Python实证指标构建与文本分析</strong>
<ul>
<li>大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与<a href="https://textdata.cn/blog/2022-05-workshop/7-Python.html">直播课</a>。</li>
<li>如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的<a href="https://textdata.cn/blog/management_python_course">录播课</a>。</li>
<li>如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读<a href="https://textdata.cn/blog/paid_for_service">有偿说明</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>LIST | 可供社科(经管)领域使用的数据集汇总</title>
      <link>https://textdata.cn/blog/datasets_available_for_management_science/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/datasets_available_for_management_science/</guid>
      <description>可供社科(经管)使用的数据集</description>
      <content:encoded><![CDATA[<p>这篇资源帖按照汇总</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 社会
- 企业
- 用户

- 词向量
- 词典
</code></pre></div><p><br><br></p>
<h2 id="社会">社会</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-12-22-renmin-gov-leader-comment-board/"><span style="color: red;"><strong>数据集(付费) | 人民网地方领导留言板原始文本(2011-2023.12)</strong></span></a></li>
<li><a href="https://textdata.cn/blog/2023-12-14-daily-news-dataset/"><span style="color: red;"><strong>新闻数据集(付费) | 含 人民日报/经济日报/光明日报 等 7 家媒体(2023.12.18)</strong></span></a></li>
<li><a href="https://textdata.cn/blog/2023-09-03-government-procurement-contract-data/"><strong>数据集(付费)  | 372w政府采购合同公告明细数据（2024.03）</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-12-17-gov-anual-report-dataset/"><strong>数据集(付费) | 国、省、市三级政府工作报告文本(1954-2023)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-02-26-cctv1-xwlb-news-text-dataset/"><strong>数据集(付费) | cctv新闻联播文稿数据集</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/"><strong>数据集 |  使用1000w条豆瓣影评训练Word2Vec</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/"><strong>数据集 | 3394w条豆瓣书评数据集</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-05-17-china-200-city-real-estate-policy/">实验数据 | 194城市楼市政策梳理(2010-2022)</a></li>
<li><a href="https://textdata.cn/blog/2023-12-29-china-area-dataset/">数据集 | 2024年中国全国5级行政区划（省、市、县、镇、村）</a></li>
<li><a href="https://textdata.cn/blog/2023-12-29-china-area-division-change/">数据集 | 行政区划代码历史沿革数据集</a></li>
</ul>
<p><br><br></p>
<h2 id="企业">企业</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/">数据集 |  A股上市公司基本信息</a></li>
<li><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/"><strong>数据集(付费) | 2001年-2022年A股上市公司年报&amp;管理层讨论与分析</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-21-hk-stock-market-anual-report/"><strong>数据集 | 港股年报文本数据集(2007 ~ 2023.12)</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-14-usa-sec-10k-report-dataset/"><strong>数据集(付费) |  美股年报10-K、20-F数据(2000-2023.12)</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-18-neeq-china-listed-on-nation-equities-exchange-and-quotation-system-anunal-year-report/"><strong>数据集(付费) | 三板上市公司年报2002-2023.12</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-05-07-china-law-judgment-documents-datasets/"><strong>数据集(付费) | 中国裁判文书网(2010-2021.10)</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-03-listed-company-arbitration-dataset/"><strong>数据集(付费) | 36330条上市公司仲裁数据(2000-2021.9)</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-06-china-export-import-dataset/">数据集 |  5.6亿条海关数据集(2000-2021.3 商品hs编码已统一) </a></li>
<li><a href="https://textdata.cn/blog/2023-12-03-china-mainland-corporate-registration-information/"><strong>数据集(付费) | 2.49亿条中国工商注册企业信息(23.9更新)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/"><strong>数据集(付费) | 3571万条专利申请数据集(1985-2022年)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-12-07-patent-application-dataset-of-listed-company-in-china-a-market/"><strong>数据集(付费) | 上市公司 208 万条专利数据集 (1991-2022)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-12-22-patent-transform-exchange-dataset/"><strong>数据集(付费) |  专利转让数据库(1985-2021)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-04-12-china-poi-datasets/"><strong>数据集(付费) |  3.9G全国POI地点兴趣点数据集</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"><strong>词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-04-17-china-a-market-inquiry-letter-datasets/"><strong>数据集(付费) | 2014年-2021年「问询函」</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-08-11-china-a-market-corporate-social-responsibility-dataste/"><strong>数据集(付费) | 2006年-2022年沪深企业社会责任报告</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-09-08-china-a-share-market-listed-company-earnings-communication-conference/"><strong>数据集(付费) | 84w条业绩说明会问答数据(2005-2023)</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-04-18-china-a-listed-company-figure-characteristic-dataset/">数据集(付费) | 上市公司(董监高)个人特征/教育背景/任职情况</a></li>
<li><a href="https://textdata.cn/blog/2022-11-25-senior-manager-resume-dataset/"><strong>数据集(付费) | 90w条中国上市公司高管数据</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-05-17-top-manager-violation/">数据集 | 上市公司高管违规数据(2008-2022)</a></li>
<li><a href="https://textdata.cn/blog/2023-04-26-entrusted-loan-dataset/">数据集 | 07-21年上市公司「委托贷款公告」</a></li>
<li><a href="https://textdata.cn/blog/coporate_social_responsibility_datasets/">数据集 | 企业社会责任报告数据集</a></li>
<li><a href="https://textdata.cn/blog/2022-11-02-27g-python-27g-a-share-market-prospectus/">27G数据集 | 使用Python对27G招股说明书进行文本分析</a></li>
<li><a href="https://textdata.cn/blog/70g_china_market_anunal_report_datasets/">70G数据集 | 上交所定期报告数据集</a></li>
<li><a href="https://textdata.cn/blog/2022-10-21-2007-2021-a-share-reports-dataset/">14G数据集 | 2007-2021年A股上市公司年度报告（txt文件）</a></li>
<li><a href="https://textdata.cn/blog/2022-12-10-1850w-poi-dataset/">1850万条 | 世界地图POI兴趣点数据集</a></li>
<li><a href="https://textdata.cn/blog/2023-10-18-google-local-data/">数据集 | 谷歌地图美国区域内poi、评论信息等信息</a></li>
<li><a href="https://textdata.cn/blog/2024-01-19-recruitment-dataset/">数据集 | 用来练习pandas的招聘数据</a></li>
</ul>
<p><br><br></p>
<h2 id="用户">用户</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-04-10-kiva-crowdfunding/">数据集 | 众筹平台kiva借贷信息</a></li>
<li><a href="https://textdata.cn/blog/2023-11-22-1000w-github-developer-dataset/">数据集 | 1000万 Github 用户数据</a></li>
<li><a href="https://textdata.cn/blog/2023-11-22-open-dataset-gharchive-org/">2T数据集 | 使用GH Archive获取Github社区用户数据</a></li>
<li><a href="https://textdata.cn/blog/2023-12-24-instagram-influencer-dataset/">数据集 | 3.3万 Instagram Influencer的 1018万条推文数据</a></li>
<li><a href="https://textdata.cn/blog/yelpdataset_10g/">10G数据集 | YelpDaset酒店管理类数据集</a></li>
<li><a href="https://textdata.cn/blog/2022-12-08-indiegogo-dataset/">1.5G数据集 | 200万条Indiegogo众筹项目信息</a></li>
<li><a href="https://textdata.cn/blog/2022-12-04-kickstarters_dataset/">12G数据集 | 23w条Kickstarter项目信息</a></li>
<li><a href="https://textdata.cn/blog/2023-05-10-100m-bilibili-user-info-dataset/">数据集 | B站/哔哩哔哩 1 亿用户数据</a></li>
<li><a href="https://textdata.cn/blog/2023-03-06-zhihurec-dataset/">数据集 | 80w知乎用户问答数据</a></li>
<li><a href="https://textdata.cn/blog/2023-03-06-bedtime-news-datasets/">数据集 |马前卒工作室 睡前消息文稿汇总</a></li>
</ul>
<p><br><br></p>
<h2 id="词向量">词向量</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/"><strong>词向量(付费) | 使用3751w专利申请数据集按年份(按省份)训练词向量</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/"><strong>词向量(付费) | 使用1985年-2022年专利申请摘要训练word2vec模型</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"> <strong>词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量  | 使用<strong>人民网领导留言板</strong>语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2023-11-18-show-word-meaning-shift-using-word2vec/">案例分享|  使用裁判文书数据集逐年训练年份词向量</a></li>
<li><a href="https://textdata.cn/blog/embeddings_resource_usage_method/">中文词向量资源汇总 &amp; 使用方法</a></li>
<li><a href="https://textdata.cn/blog/pretained_nlp_models/">NLP资源 | 汽车、金融等9大领域预训练词向量模型下载资源</a></li>
<li><a href="https://textdata.cn/blog/2023-03-08-edgar-w2v-and-corpus/">EDGAR | 25年数据的预训练词向量模型</a></li>
<li><a href="https://textdata.cn/blog/2022-10-16-aligned-word-vectors/">数据集 | 多语言对齐词向量预训练模型</a></li>
</ul>
<p><br><br></p>
<h2 id="词典">词典</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-04-05-chinese-concreteness-dictionary-from-behavior-research-method/">中文心理词典，含具体性、可成象性等指标</a></li>
<li><a href="https://textdata.cn/blog/2024-02-27-ancw-affective-norms-for-4030-chinese-words/">ANCW | 4030词的中文情感词典(效价、唤醒度、主导度、具体性)</a></li>
<li><a href="https://textdata.cn/blog/2023-03-20-nature-six-semantic-dimension-database/">Nature | 通用中英文六维语义情感词典</a></li>
<li><a href="https://textdata.cn/blog/chinese_semantic_kb/">ChineseSemanticKB | 中文语义常用词典</a></li>
<li><a href="https://textdata.cn/blog/2022-11-07-domainwordsdict-liuhuanyong/">DomainWordsDict | 领域词库构建方法与68领域、916万级专业词库分享</a></li>
<li><a href="https://textdata.cn/blog/2022-11-07-financial-invest-merge/">小规模金融并购、投资事件图谱设计概述与数据构成解析</a></li>
<li><a href="https://textdata.cn/blog/2022-09-27-r-ngramr/">Google Books Ngram Viewer显示英文词汇历史使用趋势</a></li>
<li><a href="https://textdata.cn/blog/2022-11-07-chinese-casual-text-datasets/">十万级 | 多领域因果事件对数据集对外开源</a></li>
</ul>
<p><br><br></p>
<h2 id="其他">其他</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-01-23-china-national-social-science-fund-projects-from-2010-to-2023/">数据集 | 2010-2023年国家社会科学基金立项名单.xlsx</a></li>
</ul>
<p><br><br></p>
<h2 id="最后">最后</h2>
<p>数据集和模型资源比较少，各位如果有新资源，欢迎留言分享或者邮箱thunderhit@qq.com联系我。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><strong>付费视频课程 | Python实证指标构建与文本分析</strong>
<ul>
<li>大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与<a href="https://textdata.cn/blog/2022-05-workshop/7-Python.html">直播课</a>。</li>
<li>如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的<a href="https://textdata.cn/blog/management_python_course">录播课</a>。</li>
<li>如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读<a href="https://textdata.cn/blog/paid_for_service">有偿说明</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理世界2024 | 使用管理层讨论与分析测量「企业人工智能指标」</title>
      <link>https://textdata.cn/blog/2024-04-19-ai-improve-firm-productivity/</link>
      <pubDate>Mon, 29 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-19-ai-improve-firm-productivity/</guid>
      <description>&lt;h2 id=&#34;一案例&#34;&gt;一、案例&lt;/h2&gt;
&lt;h3 id=&#34;11-文献&#34;&gt;1.1 文献&lt;/h3&gt;
&lt;p&gt;姚加权, 张锟澎, 郭李鹏, 冯绪. 人工智能如何提升企业生产效率？——基于劳动力技能结构调整的视角[J]. 管理世界, 2024, 40 (02): 101-116+133+117-122.&lt;/p&gt;
&lt;p&gt;摘要:人工智能技术对实现经济的高质量发展具有重要意义。现有研究多聚焦于人工智能对宏观经济的影响，本文从企业层面考察了人工智能技术如何影响生产效率和劳动力技能结构。&lt;strong&gt;本文运用机器学习方法生成了「人工智能词典」，并对上市公司的年报和专利进行「文本分析」，进而构建了企业层面的「人工智能指标」&lt;/strong&gt;。研究发现，人工智能显著提升了中国上市公司的生产率，并且该结论在一系列稳健性检验后依旧成立。在影响机制方面，人工智能通过促使企业减少常规低技能劳动力需求、增加非常规高技能劳动力需求的方式提升企业的生产率，这体现了企业劳动力技能结构的调整。异质性分析表明，产权性质、人才获得方式、劳动力保障、治理结构等企业层面因素对人工智能的生产率效应有较大影响。此外，企业所处的行业和地区层面因素也影响了人工智能的生产率效应。最后，本文发现人工智能提高了企业价值。本文加深了对微观企业层面人工智能在生产过程中所扮演角色的认知和理解，并为在微观企业层面推动人工智能技术发展提供了建议。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;12-指标构建步骤&#34;&gt;1.2 指标构建步骤&lt;/h3&gt;
&lt;p&gt;下图是论文中「人工智能指标」构建的流程图&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-steps.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;我们将步骤分成三步&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1. 训练Word2Vec模型构建「人工智能AI词典」, 共54个词&lt;/li&gt;
&lt;li&gt;Step2. 统计上市公司 「年报」中AI词词频m，采用自然对数处理得到指标Ln(m+1)&lt;/li&gt;
&lt;li&gt;Step3. 统计上市公司「MD&amp;amp;A」数据中AI词词频n，采用自然对数处理得到指标Ln(n+1)&lt;/li&gt;
&lt;li&gt;Step4. 根据上市公司申请专利的名称和摘要是否含AI词，统计上市公司AI专利申请数量p，采用自然对数处理得到指标Ln(p+1)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;企业申请的人工智能专利代表企业已经拥有的人工智能技术，反映了企业人工智能技术的产出情况，能够与年报相互印证企业的人工智能技术水平&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;为了减轻阅读压力，也为了减轻制作本文的工作量， 本文仅实现 Step1 、Step2 、Step3， 覆盖截图中的红色框范围内的内容。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;13-项目结构&#34;&gt;1.3 项目结构&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 管理世界2024企业人工智能文件夹
    - 代码.ipynb                                    #代码文件
    
    - data                                         #数据文件夹
       - A01-22.csv.gz                             #年报
       - mda01-22.csv.gz                           #md&amp;amp;a
       - 上市公司基本信息2000-2022.csv                #基本信息
       
    - A股人工智能指标2001-2022(mda).xlsx              #计算结果
    
    - Word2Vec                                     #模型文件夹
       - mda01-22.200.6.bin
       - mda01-22.200.6.bin.syn1neg.npy
       - mda01-22.200.6.bin.wv.vectors.npy
       - 1000w专利摘要文本.100.6.bin
       - 1000w专利摘要文本.100.6.bin.syn1neg.npy
       - 1000w专利摘要文本.100.6.bin.wv.vectors.npy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二准备ai词典&#34;&gt;二、准备AI词典&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;构造专利摘要语料、管理层讨论与分析语料，分别训练Word2Vec模型&lt;/li&gt;
&lt;li&gt;构建人工智能种子词， 使用Word2Vec模型扩展并构建「人工智能词典」&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;h3 id=&#34;21-训练word2vec模型&#34;&gt;2.1 训练Word2Vec模型&lt;/h3&gt;
&lt;p&gt;刚好之前分享过使用cntext库(2.0以上版本)训练Word2Vec， 相关推文&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/&#34;&gt;词向量(付费) | 使用MD&amp;amp;A2001-2022语料训练Word2Vec模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/&#34;&gt;词向量(付费) | 使用1985年-2022年专利申请摘要训练word2vec模型&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分别对应 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt;、 &lt;em&gt;&lt;strong&gt;mda01-22.200.6.bin&lt;/strong&gt;&lt;/em&gt; 、 &lt;em&gt;&lt;strong&gt;1000w专利摘要文本.100.6.bin&lt;/strong&gt;&lt;/em&gt; 两个模型文件。&lt;strong&gt;文末有模型获取方式&lt;/strong&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-导入word2vec&#34;&gt;2.2 导入Word2Vec&lt;/h3&gt;
&lt;p&gt;以 mda01-22.200.6.bin 为例， 使用cntext2读取模型， cntext安装和使用请参考 &lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-tutorial/&#34;&gt;文本分析库cntext2.x使用说明文档&lt;/a&gt;。 &lt;strong&gt;文末有cntext获取方式&lt;/strong&gt;。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#查看cntext版本&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__version__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#导入管理层讨论与分析的Word2Vec模型&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_w2v_m&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Word2Vec/mda01-22.200.6.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#导入专利摘要Word2Vec模型&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#pat_w2v_m = ct.load_w2v(&amp;#39;Word2Vec/1000w专利摘要文本.100.6.bin&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;mda_w2v_m&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2.1.1

Loading word2vec model...
&amp;lt;gensim.models.word2vec.Word2Vec at 0x7dbf9afd0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;查看某个词的词向量&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;mda_w2v_m.wv.get_vector(&amp;#39;人工智能&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([-3.8744571 , -0.5923845 , -1.8126943 ,  1.660894  ,  1.4194168 ,
        1.0365077 , -0.21333796, -0.60481924,  1.5012817 , -0.24060927,
       -1.7463511 , -2.1997519 , -0.66537315, -1.2665682 ,  0.14333063,
       -0.1268099 ,  2.005481  , -1.4638793 ,  3.7950375 ,  0.20866613,
        1.0281029 , -1.5495429 , -0.2518896 ,  1.4159175 ,  3.178865  ,
        .............................#省略展示..........................
       -1.2206184 ,  1.6766415 , -0.1082068 ,  0.62580353,  1.4639648 ,
        2.2743094 , -0.48386717,  1.3510187 ,  1.1698194 ,  0.72390413,
       -0.4855997 ,  1.0688399 ,  0.77217335, -1.4559731 ,  1.4391305 ,
        0.8412411 ,  2.359447  , -1.1504242 ,  1.3677332 , -0.92123735,
        1.281644  ,  0.67157453,  2.159804  ,  1.7593136 , -0.53061306,
       -0.77395666,  0.5912517 ,  1.9448034 ,  0.13023153,  0.6798518 ],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;23-扩展词典&#34;&gt;2.3 扩展词典&lt;/h3&gt;
&lt;p&gt;我们每个人对人工智能都有所了解，脑海里首先能想到的词可以当做 「初始种子词」， 例如词语 &lt;code&gt;人工智能|人机对话|&lt;/code&gt; 等。 本部分主要展示Word2Vec模型的近义词联想能力，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mda_w2v_m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;人工智能&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;人机对话&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;自然语言处理&amp;#39;, 0.8055953979492188),
 (&amp;#39;AI&amp;#39;, 0.8050345778465271),
 (&amp;#39;语音识别&amp;#39;, 0.804234504699707),
 (&amp;#39;NLP&amp;#39;, 0.7967724800109863),
 (&amp;#39;交互技术&amp;#39;, 0.7902386784553528),
 (&amp;#39;智能语音&amp;#39;, 0.7870553731918335),
 ..........#省略展示..........
 (&amp;#39;智能识别&amp;#39;, 0.6703209280967712),
 (&amp;#39;结合人工智能&amp;#39;, 0.6701650619506836),
 (&amp;#39;VR技术&amp;#39;, 0.6699633002281189),
 (&amp;#39;人工智能芯片&amp;#39;, 0.6690542101860046),
 (&amp;#39;人工智能数据分析&amp;#39;, 0.6689168214797974),
 (&amp;#39;AR技术&amp;#39;, 0.6688560843467712)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;之后Word2Vec可以根据初始种子词进行扩充，再经过人工检查，最终构建「&lt;strong&gt;人工智能词典&lt;/strong&gt;」(论文附表3截图), 我将其整理为 &lt;em&gt;&lt;strong&gt;AI-Words&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-ai-words.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;AI_Words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;机器翻译|机器学习|计算机视觉|人机交互|深度学习|神经网络|生物识别|数据挖掘|特征识别|语音合成|语音识别|知识图谱|智慧银行|智能保险|人机协同|智能监管|智能教育|智能客服|智能零售|智能农业|智能投顾|增强现实|虚拟现实|智能医疗|智能语音|智能政务|自动驾驶|智能运输|卷积神经网络|声纹识别|特征提取|无人驾驶|人脸识别|商业智能|循环神经网络|大数据营销|大数据分析|大数据处理|支持向量机|长短期记忆|机器人流程|自然语言|分布式计算|可穿戴产品|大数据管理|智能传感器|模式识别|边缘计算|大数据平台|语音交互|智能环保|人机对话|深度神经网络|大数据运营&amp;#39;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;AI_Words&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三准备数据&#34;&gt;三、准备数据&lt;/h2&gt;
&lt;p&gt;为了保证数据质量， 论文对样本进行的操作&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;1. 剔除金融行业公司；
2. 剔除信息传输、软件和信息技术 服务业以及科学研究和技术服务行业，原因在于这些行业天生使用云计算、大数据以及人工智能技术并披露 相关信息，可能无法清楚判断这些企业应用人工智能技术对其生产效率的影响；
3. 剔除当年处于 ST 和*ST 状 态的样本；
4. 剔除数据缺失的样本
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;大邓这里有几个数据文件，经过一些操作(字段名统一、 整理会计年度、合并多源数据)，就能实现论文中的样本操作。&lt;strong&gt;文末有数据获取方式&lt;/strong&gt; 。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;数据&lt;/th&gt;
&lt;th&gt;文件名&lt;/th&gt;
&lt;th&gt;所含字段&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/&#34;&gt;2001-2022年A股上市公司年报&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;A01-22.csv.gz&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;仅含&lt;em&gt;&lt;strong&gt;code&lt;/strong&gt;&lt;/em&gt; 、 &lt;em&gt;&lt;strong&gt;year&lt;/strong&gt;&lt;/em&gt; 、 &lt;em&gt;&lt;strong&gt;text&lt;/strong&gt;&lt;/em&gt; 三个字段&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/&#34;&gt;2001-2022年A股上市公司管理层讨论与分析&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mda01-22.csv.gz&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;仅含&lt;em&gt;&lt;strong&gt;code&lt;/strong&gt;&lt;/em&gt; 、 &lt;em&gt;&lt;strong&gt;year&lt;/strong&gt;&lt;/em&gt; 、 &lt;em&gt;&lt;strong&gt;text&lt;/strong&gt;&lt;/em&gt; 三个字段&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/&#34;&gt;2000-2022年A股上市公司基本信息&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;上市公司基本信息2000-2022.csv&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;含&lt;em&gt;&lt;strong&gt;Symbol&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;FullName&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;ShortName&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;IndustryName&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;EndDate&lt;/strong&gt;&lt;/em&gt;等 39 个字段。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;字段含义&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[年报、管理层讨论与分析数据]
- year 会计年度
- text 年报文本 或 管理层讨论与分析文本
- code 股票代码

[A股基本信息]
- Symbol 股票代码
- ShortName 股票简称， 一般ST字符会出现在这里
- FullName 中文全称
- EndDate 统计截止日期
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&#34;31-读取数据&#34;&gt;3.1 读取数据&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/&#34;&gt;2001-2022年A股上市公司管理层讨论与分析&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#读取数据&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/mda01-22.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#将year更改为字符串格式&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mda_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-mda-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/&#34;&gt;2000-2022年A股上市公司基本信息&lt;/a&gt; 含 行业信息、公司简称里ST等信息， 可以按条件筛选记录。同时，也要构造出 year、code字段，方便后续与mda_df 交集并表。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/上市公司基本信息2000-2022.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Symbol&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;!=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;股票代码&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-ind_df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-筛选样本&#34;&gt;3.2 筛选样本&lt;/h3&gt;
&lt;p&gt;为了保证数据质量， 论文对样本进行的操作&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;1. 剔除金融行业公司；
2. 剔除信息传输、软件和信息技术 服务业以及科学研究和技术服务行业，原因在于这些行业天生使用云计算、大数据以及人工智能技术并披露 相关信息，可能无法清楚判断这些企业应用人工智能技术对其生产效率的影响；
3. 剔除当年处于 ST 和 ``*ST`` 状态的样本；
4. 剔除数据缺失的样本
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;筛选记录的代码&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#mask1筛选出金融、信息、科学研究、技术服务等上市公司&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mask1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;IndustryName&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;金融|信息|科学研究|技术服务&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#mask2筛选出ST和*ST的企业。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mask2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ShortName&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ST&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#剔除掉符合mask1和mask2条件的企业&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mask1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mask2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#将ind_df中年份、股票代码相关字段改名为【year】【code】，方便与 mda_df并表&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rename&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Symbol&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;EndDate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;FullName&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ind_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-ind_df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;以 &lt;em&gt;&lt;strong&gt;交集(inner)&lt;/strong&gt;&lt;/em&gt; 方式合并 &lt;em&gt;&lt;strong&gt;mda_df&lt;/strong&gt;&lt;/em&gt;  和  &lt;em&gt;&lt;strong&gt;ind_df&lt;/strong&gt;&lt;/em&gt;，  相当于剔除了mda数据中金融、信息、科学研究、技术服务、ST、&lt;code&gt;*ST&lt;/code&gt; 公司&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;mda_df2 = pd.merge(mda_df, ind_df, on=[&amp;#39;code&amp;#39;, &amp;#39;year&amp;#39;], how=&amp;#39;inner&amp;#39;)
mda_df2 = mda_df2[[&amp;#39;FullName&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;code&amp;#39;, &amp;#39;text&amp;#39;]]
mda_df2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/06-mda-df2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;四测量ai指标&#34;&gt;四、测量AI指标&lt;/h2&gt;
&lt;p&gt;测量人工智能指标代码比较简单，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选中 &lt;em&gt;&lt;strong&gt;text&lt;/strong&gt;&lt;/em&gt;字段, 利用字符串属性 &lt;em&gt;&lt;strong&gt;.str.count()&lt;/strong&gt;&lt;/em&gt; 测量 &lt;em&gt;&lt;strong&gt;AI-Words&lt;/strong&gt;&lt;/em&gt; 出现次数，&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;np.log&lt;/strong&gt;&lt;/em&gt; 自然对数处理&lt;/li&gt;
&lt;li&gt;选择必要的字段&lt;em&gt;&lt;strong&gt;year&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;code&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;AI&lt;/strong&gt;&lt;/em&gt; 进行保存和展示&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#测量企业人工智能指数&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#计算结果保存为字段AI&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_df2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;AI&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mda_df2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;AI_Words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_df3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mda_df2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;AI&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#保存为csv/xlsx&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_df3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;A股人工智能指标2001-2022(mda).csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_df3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_excel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;A股人工智能指标2001-2022(mda).xlsx&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#展示结果&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_df3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/07-ai-index.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;五获取资料&#34;&gt;五、获取资料&lt;/h2&gt;
&lt;h3 id=&#34;51-免费说明&#34;&gt;5.1 免费说明&lt;/h3&gt;
&lt;p&gt;阅读是免费的， 推文内的相关模型、安装包、数据是付费获取。&lt;/p&gt;
&lt;p&gt;今日推文最核心的python代码只有2行， 看到就赚到！今日推文要计算「&lt;em&gt;&lt;strong&gt;企业人工智能指数&lt;/strong&gt;&lt;/em&gt;」，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#AI相关词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;AI_Words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;机器翻译|机器学习|计算机视觉|人机交互|深度学习|神经网络|生物识别|数据挖掘|特征识别|语音合成|语音识别|知识图谱|智慧银行|智能保险|人机协同|智能监管|智能教育|智能客服|智能零售|智能农业|智能投顾|增强现实|虚拟现实|智能医疗|智能语音|智能政务|自动驾驶|智能运输|卷积神经网络|声纹识别|特征提取|无人驾驶|人脸识别|商业智能|循环神经网络|大数据营销|大数据分析|大数据处理|支持向量机|长短期记忆|机器人流程|自然语言|分布式计算|可穿戴产品|大数据管理|智能传感器|模式识别|边缘计算|大数据平台|语音交互|智能环保|人机对话|深度神经网络|大数据运营&amp;#39;&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#企业人工智能指数，保存为字段AI&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_df2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;AI&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mda_df2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;AI_Words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;52-付费说明&#34;&gt;5.2 付费说明&lt;/h3&gt;
&lt;p&gt;内容整理不易， 想尽快复现本文的同学可以购买对应的数据、安装包、Word2Vec模型。加 &lt;em&gt;&lt;strong&gt;WeChat: 372335839&lt;/strong&gt;&lt;/em&gt; ， 备注 「&lt;strong&gt;姓名-学校-专业&lt;/strong&gt;」。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 打包价300元, 资料含
   1. 专利摘要Word2Vec模型文件(1000w专利摘要文本.100.6.bin)
   2. 管理层讨论与分析Word2Vec模型文件(mda01-22.200.6.bin)
   3. cntext2安装文件(cntext-2.1.1-py3-none-any.whl)
   4. 管理层讨论与分析(mda01-22.csv.gz)、年报(A01-22.csv.gz)
   5. 上市公司基本信息2000-2022.csv
   6. A股人工智能指标2001-2022(mda).xlsx


- 零卖价格明细
- 100元  cntext2安装文件(cntext-2.1.1-py3-none-any.whl)
- 100元  管理层讨论与分析(mda01-22.csv.gz)、年报(A01-22.csv.gz)
- 100元  管理层讨论与分析Word2Vec模型文件(mda01-22.200.6.bin)
- 100元  专利摘要Word2Vec模型文件(1000w专利摘要文本.100.6.bin)
- 50元   上市公司基本信息2000-2022.csv
- 50元   A股人工智能指标2001-2022(mda).xlsx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;项目结构&#34;&gt;项目结构&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 管理世界2024企业人工智能文件夹
    - 代码.ipynb                                    #代码文件
    
    - data                                         #数据文件夹
       - A01-22.csv.gz                             #年报
       - mda01-22.csv.gz                           #md&amp;amp;a
       - 上市公司基本信息2000-2022.csv                #基本信息
       
    - A股人工智能指标2001-2022(mda).xlsx              #计算结果
    
    - Word2Vec                                     #模型文件夹
       - mda01-22.200.6.bin
       - mda01-22.200.6.bin.syn1neg.npy
       - mda01-22.200.6.bin.wv.vectors.npy
       - 1000w专利摘要文本.100.6.bin
       - 1000w专利摘要文本.100.6.bin.syn1neg.npy
       - 1000w专利摘要文本.100.6.bin.wv.vectors.npy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;相关内容请阅读&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-tutorial/&#34;&gt;文本分析库cntext2.x使用说明文档&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/&#34;&gt;数据集 | 2001-2022年A股上市公司年报&amp;amp;管理层讨论与分析&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/&#34;&gt;数据集 | 2000-2022年A股上市公司基本信息&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/&#34;&gt;词向量 | 使用MD&amp;amp;A2001-2022语料训练Word2Vec模型&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/&#34;&gt;词向量 | 使用1985年-2022年专利申请摘要训练word2vec模型&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一案例">一、案例</h2>
<h3 id="11-文献">1.1 文献</h3>
<p>姚加权, 张锟澎, 郭李鹏, 冯绪. 人工智能如何提升企业生产效率？——基于劳动力技能结构调整的视角[J]. 管理世界, 2024, 40 (02): 101-116+133+117-122.</p>
<p>摘要:人工智能技术对实现经济的高质量发展具有重要意义。现有研究多聚焦于人工智能对宏观经济的影响，本文从企业层面考察了人工智能技术如何影响生产效率和劳动力技能结构。<strong>本文运用机器学习方法生成了「人工智能词典」，并对上市公司的年报和专利进行「文本分析」，进而构建了企业层面的「人工智能指标」</strong>。研究发现，人工智能显著提升了中国上市公司的生产率，并且该结论在一系列稳健性检验后依旧成立。在影响机制方面，人工智能通过促使企业减少常规低技能劳动力需求、增加非常规高技能劳动力需求的方式提升企业的生产率，这体现了企业劳动力技能结构的调整。异质性分析表明，产权性质、人才获得方式、劳动力保障、治理结构等企业层面因素对人工智能的生产率效应有较大影响。此外，企业所处的行业和地区层面因素也影响了人工智能的生产率效应。最后，本文发现人工智能提高了企业价值。本文加深了对微观企业层面人工智能在生产过程中所扮演角色的认知和理解，并为在微观企业层面推动人工智能技术发展提供了建议。</p>
<br>
<h3 id="12-指标构建步骤">1.2 指标构建步骤</h3>
<p>下图是论文中「人工智能指标」构建的流程图</p>
<p><img loading="lazy" src="img/01-steps.png" alt=""  />
</p>
<p>我们将步骤分成三步</p>
<ul>
<li>Step1. 训练Word2Vec模型构建「人工智能AI词典」, 共54个词</li>
<li>Step2. 统计上市公司 「年报」中AI词词频m，采用自然对数处理得到指标Ln(m+1)</li>
<li>Step3. 统计上市公司「MD&amp;A」数据中AI词词频n，采用自然对数处理得到指标Ln(n+1)</li>
<li>Step4. 根据上市公司申请专利的名称和摘要是否含AI词，统计上市公司AI专利申请数量p，采用自然对数处理得到指标Ln(p+1)</li>
</ul>
<blockquote>
<p>企业申请的人工智能专利代表企业已经拥有的人工智能技术，反映了企业人工智能技术的产出情况，能够与年报相互印证企业的人工智能技术水平</p>
</blockquote>
<p>为了减轻阅读压力，也为了减轻制作本文的工作量， 本文仅实现 Step1 、Step2 、Step3， 覆盖截图中的红色框范围内的内容。</p>
<br>
<h3 id="13-项目结构">1.3 项目结构</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 管理世界2024企业人工智能文件夹
    - 代码.ipynb                                    #代码文件
    
    - data                                         #数据文件夹
       - A01-22.csv.gz                             #年报
       - mda01-22.csv.gz                           #md&amp;a
       - 上市公司基本信息2000-2022.csv                #基本信息
       
    - A股人工智能指标2001-2022(mda).xlsx              #计算结果
    
    - Word2Vec                                     #模型文件夹
       - mda01-22.200.6.bin
       - mda01-22.200.6.bin.syn1neg.npy
       - mda01-22.200.6.bin.wv.vectors.npy
       - 1000w专利摘要文本.100.6.bin
       - 1000w专利摘要文本.100.6.bin.syn1neg.npy
       - 1000w专利摘要文本.100.6.bin.wv.vectors.npy
</code></pre></div><p><br><br></p>
<h2 id="二准备ai词典">二、准备AI词典</h2>
<ol>
<li>构造专利摘要语料、管理层讨论与分析语料，分别训练Word2Vec模型</li>
<li>构建人工智能种子词， 使用Word2Vec模型扩展并构建「人工智能词典」</li>
</ol>
<br>
<h3 id="21-训练word2vec模型">2.1 训练Word2Vec模型</h3>
<p>刚好之前分享过使用cntext库(2.0以上版本)训练Word2Vec， 相关推文</p>
<ul>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/">词向量(付费) | 使用1985年-2022年专利申请摘要训练word2vec模型</a></li>
</ul>
<p>分别对应 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em>、 <em><strong>mda01-22.200.6.bin</strong></em> 、 <em><strong>1000w专利摘要文本.100.6.bin</strong></em> 两个模型文件。<strong>文末有模型获取方式</strong>。</p>
<br>
<h3 id="22-导入word2vec">2.2 导入Word2Vec</h3>
<p>以 mda01-22.200.6.bin 为例， 使用cntext2读取模型， cntext安装和使用请参考 <a href="https://textdata.cn/blog/2024-04-27-cntext2x-tutorial/">文本分析库cntext2.x使用说明文档</a>。 <strong>文末有cntext获取方式</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#查看cntext版本</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="c1">#导入管理层讨论与分析的Word2Vec模型</span>
<span class="n">mda_w2v_m</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;Word2Vec/mda01-22.200.6.bin&#39;</span><span class="p">)</span>
<span class="c1">#导入专利摘要Word2Vec模型</span>
<span class="c1">#pat_w2v_m = ct.load_w2v(&#39;Word2Vec/1000w专利摘要文本.100.6.bin&#39;)</span>

<span class="n">mda_w2v_m</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2.1.1

Loading word2vec model...
&lt;gensim.models.word2vec.Word2Vec at 0x7dbf9afd0&gt;
</code></pre></div><br>
<p>查看某个词的词向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">mda_w2v_m.wv.get_vector(&#39;人工智能&#39;)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-3.8744571 , -0.5923845 , -1.8126943 ,  1.660894  ,  1.4194168 ,
        1.0365077 , -0.21333796, -0.60481924,  1.5012817 , -0.24060927,
       -1.7463511 , -2.1997519 , -0.66537315, -1.2665682 ,  0.14333063,
       -0.1268099 ,  2.005481  , -1.4638793 ,  3.7950375 ,  0.20866613,
        1.0281029 , -1.5495429 , -0.2518896 ,  1.4159175 ,  3.178865  ,
        .............................#省略展示..........................
       -1.2206184 ,  1.6766415 , -0.1082068 ,  0.62580353,  1.4639648 ,
        2.2743094 , -0.48386717,  1.3510187 ,  1.1698194 ,  0.72390413,
       -0.4855997 ,  1.0688399 ,  0.77217335, -1.4559731 ,  1.4391305 ,
        0.8412411 ,  2.359447  , -1.1504242 ,  1.3677332 , -0.92123735,
        1.281644  ,  0.67157453,  2.159804  ,  1.7593136 , -0.53061306,
       -0.77395666,  0.5912517 ,  1.9448034 ,  0.13023153,  0.6798518 ],
      dtype=float32)
</code></pre></div><br>
<h3 id="23-扩展词典">2.3 扩展词典</h3>
<p>我们每个人对人工智能都有所了解，脑海里首先能想到的词可以当做 「初始种子词」， 例如词语 <code>人工智能|人机对话|</code> 等。 本部分主要展示Word2Vec模型的近义词联想能力，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mda_w2v_m</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;人工智能&#39;</span><span class="p">,</span> <span class="s1">&#39;人机对话&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;自然语言处理&#39;, 0.8055953979492188),
 (&#39;AI&#39;, 0.8050345778465271),
 (&#39;语音识别&#39;, 0.804234504699707),
 (&#39;NLP&#39;, 0.7967724800109863),
 (&#39;交互技术&#39;, 0.7902386784553528),
 (&#39;智能语音&#39;, 0.7870553731918335),
 ..........#省略展示..........
 (&#39;智能识别&#39;, 0.6703209280967712),
 (&#39;结合人工智能&#39;, 0.6701650619506836),
 (&#39;VR技术&#39;, 0.6699633002281189),
 (&#39;人工智能芯片&#39;, 0.6690542101860046),
 (&#39;人工智能数据分析&#39;, 0.6689168214797974),
 (&#39;AR技术&#39;, 0.6688560843467712)]
</code></pre></div><p><br>之后Word2Vec可以根据初始种子词进行扩充，再经过人工检查，最终构建「<strong>人工智能词典</strong>」(论文附表3截图), 我将其整理为 <em><strong>AI-Words</strong></em></p>
<p><img loading="lazy" src="img/02-ai-words.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">AI_Words</span> <span class="o">=</span> <span class="s1">&#39;机器翻译|机器学习|计算机视觉|人机交互|深度学习|神经网络|生物识别|数据挖掘|特征识别|语音合成|语音识别|知识图谱|智慧银行|智能保险|人机协同|智能监管|智能教育|智能客服|智能零售|智能农业|智能投顾|增强现实|虚拟现实|智能医疗|智能语音|智能政务|自动驾驶|智能运输|卷积神经网络|声纹识别|特征提取|无人驾驶|人脸识别|商业智能|循环神经网络|大数据营销|大数据分析|大数据处理|支持向量机|长短期记忆|机器人流程|自然语言|分布式计算|可穿戴产品|大数据管理|智能传感器|模式识别|边缘计算|大数据平台|语音交互|智能环保|人机对话|深度神经网络|大数据运营&#39;</span>
<span class="n">AI_Words</span>
</code></pre></div><p><br><br></p>
<h2 id="三准备数据">三、准备数据</h2>
<p>为了保证数据质量， 论文对样本进行的操作</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1. 剔除金融行业公司；
2. 剔除信息传输、软件和信息技术 服务业以及科学研究和技术服务行业，原因在于这些行业天生使用云计算、大数据以及人工智能技术并披露 相关信息，可能无法清楚判断这些企业应用人工智能技术对其生产效率的影响；
3. 剔除当年处于 ST 和*ST 状 态的样本；
4. 剔除数据缺失的样本
</code></pre></div><br>
<p>大邓这里有几个数据文件，经过一些操作(字段名统一、 整理会计年度、合并多源数据)，就能实现论文中的样本操作。<strong>文末有数据获取方式</strong> 。</p>
<table>
<thead>
<tr>
<th>数据</th>
<th>文件名</th>
<th>所含字段</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/">2001-2022年A股上市公司年报</a></td>
<td><em><strong>A01-22.csv.gz</strong></em></td>
<td>仅含<em><strong>code</strong></em> 、 <em><strong>year</strong></em> 、 <em><strong>text</strong></em> 三个字段</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/">2001-2022年A股上市公司管理层讨论与分析</a></td>
<td><em><strong>mda01-22.csv.gz</strong></em></td>
<td>仅含<em><strong>code</strong></em> 、 <em><strong>year</strong></em> 、 <em><strong>text</strong></em> 三个字段</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/">2000-2022年A股上市公司基本信息</a></td>
<td><em><strong>上市公司基本信息2000-2022.csv</strong></em></td>
<td>含<em><strong>Symbol</strong></em>、<em><strong>FullName</strong></em>、<em><strong>ShortName</strong></em>、<em><strong>IndustryName</strong></em>、<em><strong>EndDate</strong></em>等 39 个字段。</td>
</tr>
</tbody>
</table>
<p>字段含义</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[年报、管理层讨论与分析数据]
- year 会计年度
- text 年报文本 或 管理层讨论与分析文本
- code 股票代码

[A股基本信息]
- Symbol 股票代码
- ShortName 股票简称， 一般ST字符会出现在这里
- FullName 中文全称
- EndDate 统计截止日期
</code></pre></div><br>
<br>
<h3 id="31-读取数据">3.1 读取数据</h3>
<p><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/">2001-2022年A股上市公司管理层讨论与分析</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#读取数据</span>
<span class="n">mda_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/mda01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="c1">#将year更改为字符串格式</span>
<span class="n">mda_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mda_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">mda_df</span>
</code></pre></div><p><img loading="lazy" src="img/03-mda-df.png" alt=""  />
</p>
<br>
<p><a href="https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/">2000-2022年A股上市公司基本信息</a> 含 行业信息、公司简称里ST等信息， 可以按条件筛选记录。同时，也要构造出 year、code字段，方便后续与mda_df 交集并表。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ind_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/上市公司基本信息2000-2022.csv&#39;</span><span class="p">)</span>
<span class="n">ind_df</span> <span class="o">=</span> <span class="n">ind_df</span><span class="p">[</span><span class="n">ind_df</span><span class="p">[</span><span class="s1">&#39;Symbol&#39;</span><span class="p">]</span><span class="o">!=</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span>
<span class="n">ind_df</span>
</code></pre></div><p><img loading="lazy" src="img/04-ind_df.png" alt=""  />
</p>
<br>
<h3 id="32-筛选样本">3.2 筛选样本</h3>
<p>为了保证数据质量， 论文对样本进行的操作</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1. 剔除金融行业公司；
2. 剔除信息传输、软件和信息技术 服务业以及科学研究和技术服务行业，原因在于这些行业天生使用云计算、大数据以及人工智能技术并披露 相关信息，可能无法清楚判断这些企业应用人工智能技术对其生产效率的影响；
3. 剔除当年处于 ST 和 ``*ST`` 状态的样本；
4. 剔除数据缺失的样本
</code></pre></div><br>
<p>筛选记录的代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#mask1筛选出金融、信息、科学研究、技术服务等上市公司</span>
<span class="n">mask1</span> <span class="o">=</span> <span class="n">ind_df</span><span class="o">.</span><span class="n">IndustryName</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;金融|信息|科学研究|技术服务&#39;</span><span class="p">)</span>
<span class="c1">#mask2筛选出ST和*ST的企业。</span>
<span class="n">mask2</span> <span class="o">=</span> <span class="n">ind_df</span><span class="o">.</span><span class="n">ShortName</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;ST&#39;</span><span class="p">)</span>
<span class="c1">#剔除掉符合mask1和mask2条件的企业</span>
<span class="n">ind_df</span> <span class="o">=</span> <span class="n">ind_df</span><span class="p">[(</span><span class="o">-</span><span class="n">mask1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="o">-</span><span class="n">mask2</span><span class="p">)]</span>

<span class="c1">#将ind_df中年份、股票代码相关字段改名为【year】【code】，方便与 mda_df并表</span>
<span class="n">ind_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;Symbol&#39;</span><span class="p">:</span> <span class="s1">&#39;code&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ind_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ind_df</span><span class="o">.</span><span class="n">EndDate</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">date</span><span class="p">:</span> <span class="n">date</span><span class="p">[:</span><span class="mi">4</span><span class="p">])</span>
<span class="n">ind_df</span> <span class="o">=</span> <span class="n">ind_df</span><span class="p">[[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;FullName&#39;</span><span class="p">]]</span>
<span class="n">ind_df</span>
</code></pre></div><p><img loading="lazy" src="img/05-ind_df.png" alt=""  />
</p>
<br>
<p>以 <em><strong>交集(inner)</strong></em> 方式合并 <em><strong>mda_df</strong></em>  和  <em><strong>ind_df</strong></em>，  相当于剔除了mda数据中金融、信息、科学研究、技术服务、ST、<code>*ST</code> 公司</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">mda_df2 = pd.merge(mda_df, ind_df, on=[&#39;code&#39;, &#39;year&#39;], how=&#39;inner&#39;)
mda_df2 = mda_df2[[&#39;FullName&#39;, &#39;year&#39;, &#39;code&#39;, &#39;text&#39;]]
mda_df2
</code></pre></div><p><img loading="lazy" src="img/06-mda-df2.png" alt=""  />
</p>
<br>
<h2 id="四测量ai指标">四、测量AI指标</h2>
<p>测量人工智能指标代码比较简单，</p>
<ol>
<li>选中 <em><strong>text</strong></em>字段, 利用字符串属性 <em><strong>.str.count()</strong></em> 测量 <em><strong>AI-Words</strong></em> 出现次数，</li>
<li><em><strong>np.log</strong></em> 自然对数处理</li>
<li>选择必要的字段<em><strong>year</strong></em>、<em><strong>code</strong></em>、<em><strong>AI</strong></em> 进行保存和展示</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">#测量企业人工智能指数</span>
<span class="c1">#计算结果保存为字段AI</span>
<span class="n">mda_df2</span><span class="p">[</span><span class="s1">&#39;AI&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mda_df2</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">AI_Words</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">mda_df3</span> <span class="o">=</span> <span class="n">mda_df2</span><span class="p">[[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;AI&#39;</span><span class="p">]]</span>

<span class="c1">#保存为csv/xlsx</span>
<span class="n">mda_df3</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;A股人工智能指标2001-2022(mda).csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mda_df3</span><span class="o">.</span><span class="n">to_excel</span><span class="p">(</span><span class="s1">&#39;A股人工智能指标2001-2022(mda).xlsx&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1">#展示结果</span>
<span class="n">mda_df3</span>
</code></pre></div><p><img loading="lazy" src="img/07-ai-index.png" alt=""  />
</p>
<br>
<br>
<h2 id="五获取资料">五、获取资料</h2>
<h3 id="51-免费说明">5.1 免费说明</h3>
<p>阅读是免费的， 推文内的相关模型、安装包、数据是付费获取。</p>
<p>今日推文最核心的python代码只有2行， 看到就赚到！今日推文要计算「<em><strong>企业人工智能指数</strong></em>」，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#AI相关词</span>
<span class="n">AI_Words</span> <span class="o">=</span> <span class="s1">&#39;机器翻译|机器学习|计算机视觉|人机交互|深度学习|神经网络|生物识别|数据挖掘|特征识别|语音合成|语音识别|知识图谱|智慧银行|智能保险|人机协同|智能监管|智能教育|智能客服|智能零售|智能农业|智能投顾|增强现实|虚拟现实|智能医疗|智能语音|智能政务|自动驾驶|智能运输|卷积神经网络|声纹识别|特征提取|无人驾驶|人脸识别|商业智能|循环神经网络|大数据营销|大数据分析|大数据处理|支持向量机|长短期记忆|机器人流程|自然语言|分布式计算|可穿戴产品|大数据管理|智能传感器|模式识别|边缘计算|大数据平台|语音交互|智能环保|人机对话|深度神经网络|大数据运营&#39;</span>

<span class="c1">#企业人工智能指数，保存为字段AI</span>
<span class="n">mda_df2</span><span class="p">[</span><span class="s1">&#39;AI&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mda_df2</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">AI_Words</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="52-付费说明">5.2 付费说明</h3>
<p>内容整理不易， 想尽快复现本文的同学可以购买对应的数据、安装包、Word2Vec模型。加 <em><strong>WeChat: 372335839</strong></em> ， 备注 「<strong>姓名-学校-专业</strong>」。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 打包价300元, 资料含
   1. 专利摘要Word2Vec模型文件(1000w专利摘要文本.100.6.bin)
   2. 管理层讨论与分析Word2Vec模型文件(mda01-22.200.6.bin)
   3. cntext2安装文件(cntext-2.1.1-py3-none-any.whl)
   4. 管理层讨论与分析(mda01-22.csv.gz)、年报(A01-22.csv.gz)
   5. 上市公司基本信息2000-2022.csv
   6. A股人工智能指标2001-2022(mda).xlsx


- 零卖价格明细
- 100元  cntext2安装文件(cntext-2.1.1-py3-none-any.whl)
- 100元  管理层讨论与分析(mda01-22.csv.gz)、年报(A01-22.csv.gz)
- 100元  管理层讨论与分析Word2Vec模型文件(mda01-22.200.6.bin)
- 100元  专利摘要Word2Vec模型文件(1000w专利摘要文本.100.6.bin)
- 50元   上市公司基本信息2000-2022.csv
- 50元   A股人工智能指标2001-2022(mda).xlsx
</code></pre></div><br>
<h3 id="项目结构">项目结构</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 管理世界2024企业人工智能文件夹
    - 代码.ipynb                                    #代码文件
    
    - data                                         #数据文件夹
       - A01-22.csv.gz                             #年报
       - mda01-22.csv.gz                           #md&amp;a
       - 上市公司基本信息2000-2022.csv                #基本信息
       
    - A股人工智能指标2001-2022(mda).xlsx              #计算结果
    
    - Word2Vec                                     #模型文件夹
       - mda01-22.200.6.bin
       - mda01-22.200.6.bin.syn1neg.npy
       - mda01-22.200.6.bin.wv.vectors.npy
       - 1000w专利摘要文本.100.6.bin
       - 1000w专利摘要文本.100.6.bin.syn1neg.npy
       - 1000w专利摘要文本.100.6.bin.wv.vectors.npy
</code></pre></div><br>
<p>相关内容请阅读</p>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2024-04-27-cntext2x-tutorial/">文本分析库cntext2.x使用说明文档</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/">数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/">数据集 | 2000-2022年A股上市公司基本信息</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">词向量 | 使用MD&amp;A2001-2022语料训练Word2Vec模型</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/">词向量 | 使用1985年-2022年专利申请摘要训练word2vec模型</a></p>
</li>
</ul>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>推荐 | 文本分析库cntext2.x使用手册</title>
      <link>https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/</link>
      <pubDate>Sat, 27 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-27-cntext2x-usage-tutorial/</guid>
      <description>&lt;p&gt;&lt;em&gt;&lt;strong&gt;cntext&lt;/strong&gt;&lt;/em&gt;是大邓开发维护的中英文文本分析库，内置有多重词典和常用函数， 包括&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;免费的 1.x 版， 更新至 1.9。&lt;/li&gt;
&lt;li&gt;收费的2.x版， 更新至 2.1.1。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;加大邓 &lt;em&gt;&lt;strong&gt;WeChat: 372335839&lt;/strong&gt;&lt;/em&gt;， 备注「姓名-学校-专业」， 100元领取  &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 文件。本文出现的cntext，默认均为2.x版本。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;安装cntext&#34;&gt;安装cntext&lt;/h2&gt;
&lt;p&gt;所有 &lt;em&gt;&lt;strong&gt;cntext2.x&lt;/strong&gt;&lt;/em&gt; 安装方法类似， 以目前 cntext2.1.1 为例，将 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 放置于桌面，打开 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;之后在 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal) 中使用 &lt;em&gt;&lt;strong&gt;pip3&lt;/strong&gt;&lt;/em&gt; 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install pdfdocx
pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;文章开头和文章末都有 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt;  获取方式说明。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;功能模块&#34;&gt;功能模块&lt;/h2&gt;
&lt;p&gt;cntext含io、model、stats、mind四个模块&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;导入数据用io&lt;/li&gt;
&lt;li&gt;训练模型扩展词典用model&lt;/li&gt;
&lt;li&gt;统计词频、情感分析、相似度等用stats&lt;/li&gt;
&lt;li&gt;态度认知文化变迁用mind&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;函数部分加粗的为常用函数。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;函数&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.get_dict_list()&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;查看cntext内置词典&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.read_dict_yaml(yfile)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;读取内置yaml词典&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.detect_encoding(file, num_lines=100)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;诊断txt、csv编码格式&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.get_files(fformat)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;查看符合fformat路径规则的所有的文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.read_file(file, encodings)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;读取文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.read_file(fformat, encoding)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;读取符合fformat路径规则的所有的文件，返回df&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.W2Vmodel(corpus_file, encoding, lang=&amp;lsquo;chinese&amp;rsquo;)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;训练Word2Vec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.load_w2v(w2v_path)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;读取cntext2.x训练出的word2vec模型文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.expand_dictionary(wv,  seeddict, topn=100)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;扩展词典,  结果保存到路径[output/Word2Vec]中&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.Glove(corpus_file, lang=&#39;chinese&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;训练GLove模型。 算法运行较慢，吃内存，不推荐！！&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.SoPmi(corpus_file, seed_file, lang=&#39;chinese&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;共现法扩展词典&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.term_freq(text, lang=&#39;chinese&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;词频统计&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.readability(text, lang=&#39;chinese&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;文本可读性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.sentiment(text, diction, lang=&amp;lsquo;chinese&amp;rsquo;)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;无(等)权重词典的情感分析&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.sentiment_by_valence(text, diction, lang=&#39;chinese&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;带权重的词典的情感分析&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.word_in_context(text, keywords, window=3, lang=&amp;lsquo;chinese&amp;rsquo;)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;在text中查找keywords出现的上下文内容(窗口window)，返回df&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.epu()&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;使用新闻文本数据计算经济政策不确定性EPU，返回df&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.fepu(text, ep_pattern=&#39;&#39;, u_pattern=&#39;&#39;)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;使用md&amp;amp;a文本数据计算企业不确定性感知FEPU&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.semantic_brand_score(text, brands, lang=&amp;lsquo;chinese&amp;rsquo;)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;衡量品牌（个体、公司、品牌、关键词等）的重要性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.cosine_sim(text1, text2)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;余弦相似度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.jaccard_sim(text1, text2)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Jaccard相似度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.minedit_sim(text1, text2)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;最小编辑距离&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;tm = ct.Text2Mind(wv)&lt;/strong&gt;&lt;/em&gt;&lt;br&gt;&lt;/td&gt;
&lt;td&gt;单个word2vec内挖掘潜在的态度偏见、刻板印象等。tm含多重方法&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.sematic_projection(wv, words, c_words1, c_words2)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;测量语义投影&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.sematic_distance(wv, words, c_words1, c_words2)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;测量语义距离&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.divergent_association_task(wv, words)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;测量发散思维(创造力)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.discursive_diversity_score(wv, words)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;测量语言差异性(认知差异性)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.procrustes_align(base_embed, other_embed)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;两个word2vec进行语义对齐，可反应随时间的社会语义变迁&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;quickstart&#34;&gt;QuickStart&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;help&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;nx&#34;&gt;Help&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;on&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;package&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;cntext&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;

&lt;span class=&#34;nx&#34;&gt;NAME&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;cntext&lt;/span&gt;

&lt;span class=&#34;nx&#34;&gt;PACKAGE&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;CONTENTS&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;io&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;mind&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;model&lt;/span&gt;
    &lt;span class=&#34;nx&#34;&gt;stats&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;一io模块&#34;&gt;一、IO模块&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;函数&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.get_dict_list()&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;查看cntext内置词典&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.read_dict_yaml(yfile)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;读取内置yaml词典&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.detect_encoding(file, num_lines=100)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;诊断txt、csv编码格式&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.get_files(fformat)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;查看符合fformat路径规则的所有的文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.read_file(file, encoding)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;读取文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.read_files(fformat, encoding)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;读取符合fformat路径规则的所有的文件，返回df&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;11-get_dict_list&#34;&gt;1.1 get_dict_list()&lt;/h3&gt;
&lt;p&gt;查看cntext内置词典&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_dict_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;zh_common_NTUSD.yaml&amp;#39;,
 &amp;#39;zh_common_DUTIR.yaml&amp;#39;,
 &amp;#39;enzh_common_StopWords.yaml&amp;#39;,
 &amp;#39;en_valence_Concreteness.yaml&amp;#39;,
 &amp;#39;en_common_LoughranMcDonald.yaml&amp;#39;,
 &amp;#39;zh_common_FinanceSenti.yaml&amp;#39;,
 &amp;#39;zh_common_FLS.yaml&amp;#39;,
 &amp;#39;zh_common_TsinghuaPraiseDegrade.yaml&amp;#39;,
 &amp;#39;zh_common_FEPU.yaml&amp;#39;,
 &amp;#39;en_common_ANEW.yaml&amp;#39;,
 &amp;#39;en_common_NRC.yaml&amp;#39;,
 &amp;#39;zh_valence_ChineseEmoBank.yaml&amp;#39;,
 &amp;#39;zh_valence_SixSemanticDimensionDatabase.yaml&amp;#39;,
 &amp;#39;zh_common_FinacialFormalUnformal.yaml&amp;#39;,
 &amp;#39;zh_common_LoughranMcDonald.yaml&amp;#39;,
 &amp;#39;enzh_common_AdvConj.yaml&amp;#39;,
 &amp;#39;en_common_SentiWS.yaml&amp;#39;,
 &amp;#39;zh_common_Digitalization.yaml&amp;#39;,
 &amp;#39;en_common_LSD2015.yaml&amp;#39;,
 &amp;#39;zh_common_HowNet.yaml&amp;#39;,
 &amp;#39;zh_common_EPU.yaml&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;12-内置yaml词典&#34;&gt;1.2 内置yaml词典&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;pkl文件&lt;/th&gt;
&lt;th&gt;词典&lt;/th&gt;
&lt;th&gt;语言&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;zh_valence_ChineseEmoBank.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;中文情感词典，含&lt;code&gt;效价valence&lt;/code&gt;和&lt;code&gt;唤醒度arousal&lt;/code&gt;。在cntext中，我们只使用了CVAW词表(单词)，其他词典如CVAP, CVAS, CVAT没有纳入到ChineseEmoBank.pkl.&lt;/td&gt;
&lt;td&gt;Chinese&lt;/td&gt;
&lt;td&gt;&lt;code&gt;效价valence&lt;/code&gt;和&lt;code&gt;唤醒度arousal&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;zh_common_DUTIR.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;大连理工大学情感本体库&lt;/td&gt;
&lt;td&gt;中文&lt;/td&gt;
&lt;td&gt;七大类情绪，&lt;code&gt;哀, 好, 惊, 惧, 乐, 怒, 恶&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;zh_common_HowNet.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;知网Hownet词典&lt;/td&gt;
&lt;td&gt;中文&lt;/td&gt;
&lt;td&gt;正面词、负面词&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;en_common_SentiWS.yaml&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;SentimentWortschatz (SentiWS)&lt;/td&gt;
&lt;td&gt;德文&lt;/td&gt;
&lt;td&gt;正面词、负面词；&lt;br&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;zh_common_FinacialFormalUnformal.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;金融领域正式、非正式；积极消极&lt;/td&gt;
&lt;td&gt;中文&lt;/td&gt;
&lt;td&gt;formal-pos、&lt;br&gt;formal-neg；&lt;br&gt;unformal-pos、&lt;br&gt;unformal-neg&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;en_common_ANEW.yaml&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;英语单词的情感规范Affective Norms for English Words (ANEW)&lt;/td&gt;
&lt;td&gt;英文&lt;/td&gt;
&lt;td&gt;pleasure, arousal, dominance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;en_common_LSD2015.yaml&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Lexicoder Sentiment Dictionary (2015)&lt;/td&gt;
&lt;td&gt;英文&lt;/td&gt;
&lt;td&gt;正面词、负面词&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;en_common_NRC.yaml&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NRC Word-Emotion Association Lexicon&lt;/td&gt;
&lt;td&gt;英文&lt;/td&gt;
&lt;td&gt;细粒度情绪词；&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;zh_valence_SixSemanticDimensionDatabase.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-20-nature-six-semantic-dimension-database/&#34;&gt;&lt;strong&gt;通用中英文六维语义情感词典&lt;/strong&gt;&lt;/a&gt;, 含17940个中文词的六维度词库， 且每个维度有权重。&lt;/td&gt;
&lt;td&gt;中文&lt;/td&gt;
&lt;td&gt;vision、socialness、emotion、time、space、motor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;enzh_common_AdvConj.yaml&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;副词连词&lt;/td&gt;
&lt;td&gt;中、英&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;enzh_common_StopWords.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;中英文停用词&lt;/td&gt;
&lt;td&gt;中、英&lt;/td&gt;
&lt;td&gt;停用词&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;en_valence_Concreteness.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://textdata.cn/blog/jcr_concreteness_computation/&#34;&gt;英文具体性词典&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;English&lt;/td&gt;
&lt;td&gt;word &amp;amp; concreateness score&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;zh_common_LoughranMcDonald.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;中文LoughranMcDonald词典&lt;/td&gt;
&lt;td&gt;中文&lt;/td&gt;
&lt;td&gt;正面、负面词&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;zh_common_Digitalization.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-11-03-mda-measure-digitalization/&#34;&gt;管理世界|吴非(2021)数字化词典&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;中文&lt;/td&gt;
&lt;td&gt;含人工智能技术、大数据技术、云计算技术、区块链技术、数字技术应用等关键词列表。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;en_common_LoughranMcDonald.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;英文LoughranMcDonald词典&lt;/td&gt;
&lt;td&gt;英文&lt;/td&gt;
&lt;td&gt;金融LM情绪词典2018年版本，含七个词表，分别是Negative, Positive, Uncertainty, Litigious, StrongModal, WeakModal, Constraining&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;zh_common_FLS.yaml&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-09-08-earnings-communication-conference-forward-looking-statements-information/&#34;&gt;&lt;strong&gt;业绩说明会前瞻性词典集&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;中文&lt;/td&gt;
&lt;td&gt;含174个词语&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h3 id=&#34;13-read_dict_yaml&#34;&gt;1.3 read_dict_yaml()&lt;/h3&gt;
&lt;p&gt;使用 cntext 读取 &lt;em&gt;&lt;strong&gt;.yaml&lt;/strong&gt;&lt;/em&gt; 词典文件；  返回的信息包括&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Name 词典的名字&lt;/li&gt;
&lt;li&gt;Desc 词典的含义、概念解释&lt;/li&gt;
&lt;li&gt;Refer 词典文献出处&lt;/li&gt;
&lt;li&gt;Category 词典Dictionary的关键词&lt;/li&gt;
&lt;li&gt;Dictionary 词典, python字典格式&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_yaml_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;zh_common_Digitalization.yaml&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;Name&amp;#39;: &amp;#39;中文数字化词典&amp;#39;, 
&amp;#39;Desc&amp;#39;: &amp;#39;基于这篇论文，构建了中文数字化词典，含人工智能技术、大数据技术、云计算技术、区块链技术、数字技术应用等关键词列表。 &amp;#39;, &amp;#39;Refer&amp;#39;: &amp;#39;吴非,胡慧芷,林慧妍,任晓怡. 企业数字化转型与资本市场表现——来自股票流动性的经验证据[J]. 管理世界,2021,37(07):130-144+10.&amp;#39;, 
&amp;#39;Category&amp;#39;: [&amp;#39;Artificial_Intelligence&amp;#39;, &amp;#39;Big_Data&amp;#39;, &amp;#39;Cloud_Computing&amp;#39;, &amp;#39;Block_Chains&amp;#39;, &amp;#39;Usage_of_Digitalization&amp;#39;], 

&amp;#39;Dictionary&amp;#39;: 
    {&amp;#39;Artificial_Intelligence&amp;#39;: [&amp;#39;人工智能&amp;#39;, &amp;#39;商业智能&amp;#39;, &amp;#39;图像理解&amp;#39;, &amp;#39;投资决策辅助系统&amp;#39;, &amp;#39;智能数据分析&amp;#39;, &amp;#39;智能机器人&amp;#39;, &amp;#39;机器学习&amp;#39;, &amp;#39;深度学习&amp;#39;, &amp;#39;语义搜索&amp;#39;, &amp;#39;生物识别技术&amp;#39;, &amp;#39;人脸识别&amp;#39;, &amp;#39;语音识别&amp;#39;, &amp;#39;身份验证&amp;#39;, &amp;#39;自动驾驶&amp;#39;, &amp;#39;自然语言处理&amp;#39;], 
    &amp;#39;Big_Data&amp;#39;: [&amp;#39;大数据&amp;#39;, &amp;#39;数据挖掘&amp;#39;, &amp;#39;文本挖掘&amp;#39;, &amp;#39;数据可视化&amp;#39;, &amp;#39;异构数据&amp;#39;, &amp;#39;征信&amp;#39;, &amp;#39;增强现实&amp;#39;, &amp;#39;混合现实&amp;#39;, &amp;#39;虚拟现实&amp;#39;], 
    &amp;#39;Cloud_Computing&amp;#39;: [&amp;#39;云计算&amp;#39;, &amp;#39;流计算&amp;#39;, &amp;#39;图计算&amp;#39;, &amp;#39;内存计算&amp;#39;, &amp;#39;多方安全计算&amp;#39;, &amp;#39;类脑计算&amp;#39;, &amp;#39;绿色计算&amp;#39;, &amp;#39;认知计算&amp;#39;, &amp;#39;融合架构&amp;#39;, &amp;#39;亿级并发&amp;#39;, &amp;#39;EB级存储&amp;#39;, &amp;#39;物联网&amp;#39;, &amp;#39;信息物理系统&amp;#39;], 
    &amp;#39;Block_Chains&amp;#39;: [&amp;#39;区块链&amp;#39;, &amp;#39;数字货币&amp;#39;, &amp;#39;分布式计算&amp;#39;, &amp;#39;差分隐私技术&amp;#39;, &amp;#39;智能金融合约&amp;#39;], 
    &amp;#39;Usage_of_Digitalization&amp;#39;: [&amp;#39;移动互联网&amp;#39;, &amp;#39;工业互联网&amp;#39;, &amp;#39;移动互联&amp;#39;, &amp;#39;互联网医疗&amp;#39;, &amp;#39;电子商务&amp;#39;, &amp;#39;移动支付&amp;#39;, &amp;#39;第三方支付&amp;#39;, &amp;#39;NFC支付&amp;#39;, &amp;#39;智能能源&amp;#39;, &amp;#39;B2B&amp;#39;, &amp;#39;B2C&amp;#39;, &amp;#39;C2B&amp;#39;, &amp;#39;C2C&amp;#39;, &amp;#39;O2O&amp;#39;, &amp;#39;网联&amp;#39;, &amp;#39;智能穿戴&amp;#39;, &amp;#39;智慧农业&amp;#39;, &amp;#39;智能交通&amp;#39;, &amp;#39;智能医疗&amp;#39;, &amp;#39;智能客服&amp;#39;, &amp;#39;智能家居&amp;#39;, &amp;#39;智能投顾&amp;#39;, &amp;#39;智能文旅&amp;#39;, &amp;#39;智能环保&amp;#39;, &amp;#39;智能电网&amp;#39;, &amp;#39;智能营销&amp;#39;, &amp;#39;数字营销&amp;#39;, &amp;#39;无人零售&amp;#39;, &amp;#39;互联网金融&amp;#39;, &amp;#39;数字金融&amp;#39;, &amp;#39;Fintech&amp;#39;, &amp;#39;金融科技&amp;#39;, &amp;#39;量化金融&amp;#39;, &amp;#39;开放银行&amp;#39;]}}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;14-detect_encoding&#34;&gt;1.4 detect_encoding()&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.detect_encoding(file, num_lines=100)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过读取前num_lines来识别txt/csv文件的编码格式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;file&lt;/strong&gt;&lt;/em&gt; 文件路径&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;num_line&lt;/strong&gt;&lt;/em&gt; 行数&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#读取data文件夹下的【三体.txt】&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#识别编码方式&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;detect_encoding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/三体.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_lines&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;utf-8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;15-read_file&#34;&gt;1.5 read_file()&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.read_file(file, encoding=&amp;#39;utf-8&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;file&lt;/strong&gt; 待读取的文件路径； 支持txt、pdf、docx、xlsx、xls， 返回 DataFrame(含doc和file两个字段)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;encoding&lt;/strong&gt; 待读取文件的编码方式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以 &lt;code&gt;data/三体.txt&lt;/code&gt; 为例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#默认encoding=&amp;#39;utf-8&amp;#39;&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#sdf = ct.read_file(fformat=&amp;#39;data/三体.txt&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;sdf&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fformat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/三体.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;sdf&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-san_ti_df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;16-get_filesfformat&#34;&gt;1.6 get_files(fformat)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;fformat&lt;/strong&gt;  fformat格式支持 txt/pdf/docx/xlsx/csv等。 &lt;code&gt;*&lt;/code&gt;表示通配符&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;查看符合fformat路径规则的所有的文件， fformat格式支持 txt/pdf/docx/xlsx/csv等。 &lt;code&gt;*&lt;/code&gt;表示通配符&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;fformat格式&lt;/th&gt;
&lt;th&gt;识别的文件&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;*.txt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配当前代码所在路径内的所有txt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;*.pdf&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配当前代码所在路径内的所有pdf&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;data/*.txt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配「文件夹data」内所有的 txt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;br&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#查看【文件夹data】内所有的 txt文件。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_files&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fformat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/*.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;data/三体.txt&amp;#39;,
 &amp;#39;data/santi.txt&amp;#39;,
 &amp;#39;data/w2v_corpus.txt&amp;#39;,
 &amp;#39;data/sopmi_corpus.txt&amp;#39;,
 &amp;#39;data/brown_corpus.txt&amp;#39;,
 &amp;#39;data/sopmi_seed_words.txt&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;17-read_files&#34;&gt;1.7 read_files()&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.read_files(fformat, encoding=&amp;#39;utf-8&amp;#39;）
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;批量读取符合fformat格式的所有文件数据，返回DataFrame(含doc和file两个字段)。&lt;/p&gt;
&lt;p&gt;读取[文件夹data里所有txt]&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#默认encoding=&amp;#39;utf-8&amp;#39;&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#ddf = ct.read_files(fformat=&amp;#39;data/*.txt&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ddf&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_files&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fformat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/*.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ddf&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-ddf.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二stats模块&#34;&gt;二、Stats模块&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;函数&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.term_freq(text, lang=&#39;chinese&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;词频统计&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.readability(text, lang=&#39;chinese&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;文本可读性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.sentiment(text, diction, lang=&amp;lsquo;chinese&amp;rsquo;)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;无(等)权重词典的情感分析&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.sentiment_by_valence(text, diction, lang=&#39;chinese&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;带权重的词典的情感分析&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.word_in_context(text, keywords, window=3, lang=&amp;lsquo;chinese&amp;rsquo;)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;在text中查找keywords出现的上下文内容(窗口window)，返回df&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.epu(text, e_pattern, p_pattern, u_pattern)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;使用新闻文本数据计算经济政策不确定性EPU，返回df&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.fepu(text, ep_pattern=&#39;&#39;, u_pattern=&#39;&#39;)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;使用md&amp;amp;a文本数据计算企业不确定性感知FEPU&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.semantic_brand_score(text, brands, lang=&amp;lsquo;chinese&amp;rsquo;)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;衡量品牌（个体、公司、品牌、关键词等）的重要性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.cosine_sim(text1, text2)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;余弦相似度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.jaccard_sim(text1, text2)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Jaccard相似度&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;stats&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.minedit_sim(text1, text2)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;最小编辑距离&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;21-term_freq&#34;&gt;2.1 term_freq()&lt;/h3&gt;
&lt;p&gt;统计词频， 返回Counter(类似于python字典) ； 支持中英文&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;term_freq(text, lang=&amp;#39;chinese&amp;#39;, return_df=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;text&lt;/strong&gt; 待分析的文本字符串&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lang&lt;/strong&gt; 文本的语言类型， 中文chinese、英文english，默认中文。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;return_df&lt;/strong&gt; 返回结果的数据类型，return_df=False时返回字典； return_df=True时返回DataFrame。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;致力于致力于以零文章处理费或订阅费发布优质研究软件。&amp;#39;&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#ct.term_freq(text, lang=&amp;#39;chinese&amp;#39;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;term_freq&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Counter({&amp;#39;致力于&amp;#39;: 2,
         &amp;#39;文章&amp;#39;: 1,
         &amp;#39;处理费&amp;#39;: 1,
         &amp;#39;订阅费&amp;#39;: 1,
         &amp;#39;发布&amp;#39;: 1,
         &amp;#39;优质&amp;#39;: 1,
         &amp;#39;研究&amp;#39;: 1,
         &amp;#39;软件&amp;#39;: 1})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;term_freq&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;return_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/09-term_freq.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-readabilit&#34;&gt;2.2 readabilit()&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.readability(text, lang=&amp;#39;chinese&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;text&lt;/strong&gt;  待分析的文本字符串&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lang&lt;/strong&gt; 文本的语言类型， 中文chinese、英文english，默认中文。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;文本可读性，指标越大，文章复杂度越高，可读性越差。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;中文可读性&lt;/strong&gt; 算法参考自&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;readability1 &amp;mdash;每个分句中的平均字数&lt;/li&gt;
&lt;li&gt;readability2  &amp;mdash;每个句子中副词和连词所占的比例&lt;/li&gt;
&lt;li&gt;readability3  &amp;mdash;参考Fog Index， readability3=(readability1+readability2)×0.5&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;p&gt;以上三个指标越大，都说明文本的复杂程度越高，可读性越差。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;text1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;致力于以零文章处理费或订阅费发布优质研究软件。&amp;#39;&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;readability&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;readability1&amp;#39;: 23.0, &amp;#39;readability2&amp;#39;: 2.0, &amp;#39;readability3&amp;#39;: 12.5}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;23-sentimenttext-diction-lang&#34;&gt;2.3 sentiment(text, diction, lang)&lt;/h3&gt;
&lt;p&gt;常见的情感分析默认情绪词无(等)权重， 通过统计词语个数来反应情感信息。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;sentiment(text, diction, lang=&amp;#39;chinese&amp;#39;, return_df=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;text&lt;/strong&gt; 待分析的文本字符串&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;diction&lt;/strong&gt;  格式为Python字典类型。形如下面的案例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lang&lt;/strong&gt; 文本的语言类型， 中文chinese、英文english，默认中文。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;return_df&lt;/strong&gt; 是否返回dataframe，默认False不返回&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;diction&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;pos&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;高兴&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;快乐&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;分享&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
           &lt;span class=&#34;s1&#34;&gt;&amp;#39;neg&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;难过&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;悲伤&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
           &lt;span class=&#34;s1&#34;&gt;&amp;#39;adv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;很&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;特别&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;我今天得奖了，很高兴，我要将快乐分享大家。&amp;#39;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sentiment&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
             &lt;span class=&#34;n&#34;&gt;diction&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;diction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
             &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;pos_num&amp;#39;: 3,
 &amp;#39;neg_num&amp;#39;: 0,
 &amp;#39;adv_num&amp;#39;: 1,
 &amp;#39;stopword_num&amp;#39;: 8,
 &amp;#39;word_num&amp;#39;: 14,
 &amp;#39;sentence_num&amp;#39;: 1}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;diction&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;pos&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;高兴&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;快乐&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;分享&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
           &lt;span class=&#34;s1&#34;&gt;&amp;#39;neg&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;难过&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;悲伤&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
           &lt;span class=&#34;s1&#34;&gt;&amp;#39;adv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;很&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;特别&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;我今天得奖了，很高兴，我要将快乐分享大家。&amp;#39;&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sentiment&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
             &lt;span class=&#34;n&#34;&gt;diction&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;diction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
             &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
             &lt;span class=&#34;n&#34;&gt;return_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/10-sentiment.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;24-sentiment_by_valence&#34;&gt;2.4 sentiment_by_valence()&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.sentiment_by_valence(text, diction, lang=&amp;#39;chinese&amp;#39;, return_df=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;text&lt;/strong&gt; 待分析的文本字符串&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;diction&lt;/strong&gt;  格式为Python字典类型。形如下面的案例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lang&lt;/strong&gt; 文本的语言类型， 中文chinese、英文english，默认中文。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;return_df&lt;/strong&gt; 是否返回dataframe，默认False不返回&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的情感分析是无(等)权重, 但实际上不同的词语所携带的情感信息的强度差异是很大的。据此学者们开发出很多带权重的词典，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;英文具体性词典en_valence_Concreteness.yaml， 词典中每个词都有一个concreteness值&lt;/li&gt;
&lt;li&gt;中文六维度语义词典zh_valence_SixSemanticDimensionDatabase.yaml,  每个中文词有六个值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以具体性为例， &lt;strong&gt;语言具体性Concreteness&lt;/strong&gt;描述了一个词在多大程度上是指一个实际的、有形的或“真实的”实体，以一种更具体、更熟悉、更容易被眼睛或心灵感知的方式描述对象和行为（即，可想象或生动；Brysbaert, Warriner, and Kuperman 2014; Semin and Fiedler 1988)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;concreteness_dict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_yaml_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;en_valence_Concreteness.yaml&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Dictionary&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;concreteness_dict&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;roadsweeper&amp;#39;: {&amp;#39;concreteness&amp;#39;: 4.85},
 &amp;#39;traindriver&amp;#39;: {&amp;#39;concreteness&amp;#39;: 4.54},
 &amp;#39;tush&amp;#39;: {&amp;#39;concreteness&amp;#39;: 4.45},
 &amp;#39;hairdress&amp;#39;: {&amp;#39;concreteness&amp;#39;: 3.93},
 &amp;#39;pharmaceutics&amp;#39;: {&amp;#39;concreteness&amp;#39;: 3.77},
 &amp;#39;hoover&amp;#39;: {&amp;#39;concreteness&amp;#39;: 3.76},
 &amp;#39;shopkeeping&amp;#39;: {&amp;#39;concreteness&amp;#39;: 3.18},
 &amp;#39;pushiness&amp;#39;: {&amp;#39;concreteness&amp;#39;: 2.48},
 ......
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可能 &lt;em&gt;&lt;strong&gt;concreteness_dict&lt;/strong&gt;&lt;/em&gt;不够直观， 如果整理转化一下大概类似于&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/11-concreteness_df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/jcr_concreteness_computation/&#34;&gt;&lt;strong&gt;JCR2021 | 计算文本的语言具体性&lt;/strong&gt;&lt;/a&gt; 文中提供了一个案例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;reply&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll go look for that&amp;#34;&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sentiment_by_valence&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                              &lt;span class=&#34;n&#34;&gt;diction&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;concreteness_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                              &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;text&amp;#39;: &amp;#34;I&amp;#39;ll go look for that&amp;#34;, 
&amp;#39;concreteness&amp;#39;: 9.28, 
&amp;#39;word_num&amp;#39;: 6}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;employee_replys&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll go look for that&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                   &lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll go search for that&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                   &lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll go search for that top&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                   &lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll go search for that t-shirt&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                   &lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll go look for that t-shirt in grey&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                   &lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll go search for that t-shirt in grey&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reply&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;employee_replys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sentiment_by_valence&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                                  &lt;span class=&#34;n&#34;&gt;diction&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;concreteness_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                                  &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;n&#34;&gt;template&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Concreteness Score: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{score:.2f}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; | Example-&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{idx}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{exmaple}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
    
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;concreteness&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
                          &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                          &lt;span class=&#34;n&#34;&gt;exmaple&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Concreteness Score: 9.28 | Example-0: I&amp;#39;ll go look for that
Concreteness Score: 9.32 | Example-1: I&amp;#39;ll go search for that
Concreteness Score: 13.25 | Example-2: I&amp;#39;ll go search for that top
Concreteness Score: 14.25 | Example-3: I&amp;#39;ll go search for that t-shirt
Concreteness Score: 21.32 | Example-4: I&amp;#39;ll go look for that t-shirt in grey
Concreteness Score: 21.36 | Example-5: I&amp;#39;ll go search for that t-shirt in grey
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;25-word_in_context&#34;&gt;2.5 word_in_context()&lt;/h3&gt;
&lt;p&gt;You shall know a word by the company it keeps通过一个单词所处的语境，我们可以了解该单词的含义。&lt;/p&gt;
&lt;p&gt;在text中查找keywords出现的上下文内容(窗口window)，返回df。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.word_in_context(text, keywords, window=3, lang=&amp;#39;chinese&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;text&lt;/strong&gt;  待分析文本&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;keywords&lt;/strong&gt;  关键词列表&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;window&lt;/strong&gt;  关键词上下文窗口大小&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lang&lt;/strong&gt; 文本的语言类型， 中文chinese、英文english，默认中文。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#测试代码，假设zh_text是年报文本，从找找出丝网词相关词的上下文&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;zh_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;【插入一条自家广告】大邓自己家的家，
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;安平县多隆丝网制品，生产销售不锈钢轧花网、
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;电焊网、石笼网、刀片刺绳、冲孔网等丝网制品。
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;联系人 邓颖静 0318-7686899
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;人生苦短，我学Python
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;在社科中，可以用Python做文本分析
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;Python是一门功能强大的编程语言，广泛应用在经管社科领域。
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;可以做网络爬虫、文本分析、LDA话题模型、相似度分析等。
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;今年经济不景气，形势异常严峻。
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;由于疫情不景气，静默管理， 产品积压， 公司经营困难。
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;保就业促就业，任务十分艰巨。
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#【python】上下文&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word_in_context&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;zh_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                   &lt;span class=&#34;n&#34;&gt;keywords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;python&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
                   &lt;span class=&#34;n&#34;&gt;window&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                   &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/20-word-in-context.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;26-epu&#34;&gt;2.6 epu()&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/&#34;&gt;&lt;strong&gt;代码  | 使用新闻数据测量经济政策不确定性EPU&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/13-epu-plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.epu(df, e_pattern, p_pattern, u_pattern)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;df&lt;/strong&gt;  新闻数据DataFrame， 含text和date两个字段。 每一行代表一条新闻记录&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;e_pattern&lt;/strong&gt; 字符串；经济类词典，用&lt;code&gt;|&lt;/code&gt;间隔词语，形如 &lt;strong&gt;e_pattern = ‘经济|金融’&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;p_pattern&lt;/strong&gt; 字符串；政策词典，用&lt;code&gt;|&lt;/code&gt;间隔词语，形如 &lt;strong&gt;p_pattern = ‘政策|治理|行政’&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;u_pattern&lt;/strong&gt;  字符串；不确定性词典，用&lt;code&gt;|&lt;/code&gt;间隔词语，形如 &lt;strong&gt;u_pattern = ‘风险|危机|难以预测’&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;准备如下图格式的数据 &lt;em&gt;&lt;strong&gt;news_df&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/12-news-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#省略，读取数据得到 news_df&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;epu_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;epu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;news_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;freq&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;M&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;epu_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/13-epu-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;27-fepu&#34;&gt;2.7 fepu()&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-25-firm-economic-policy-uncertainty/&#34;&gt;使用管理层讨论与分析文本数据测量「企业感知不确定性」(Subjective perception of economic policy uncertainty, FEPU)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/16-fepu-plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.fepu(text, ep_pattern, u_pattern)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;text&lt;/strong&gt;&lt;/em&gt; ；某时期t某企业i的管理层讨论与分析md&amp;amp;a文本&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;ep_pattern&lt;/strong&gt;&lt;/em&gt; 字符串；经济政策类词典，用&lt;code&gt;|&lt;/code&gt;间隔词语，形如 &lt;strong&gt;ep_pattern = ‘经济|金融|政策|治理|行政’&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;u_pattern&lt;/strong&gt;&lt;/em&gt; 字符串；不确定性词典，用&lt;code&gt;|&lt;/code&gt;间隔词语，形如 &lt;strong&gt;u_pattern = ‘风险|危机|难以预测’&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;准备如下图格式的数据 &lt;em&gt;&lt;strong&gt;mda_df&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/14-mdadf.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#省略，读取数据得到 mda_df&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;fepu_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;经营讨论与分析内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fepu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;res_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;concat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;会计年度&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;股票代码&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fepu_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;   &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;res_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/15-fepu.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&#34;28-semantic_brand_score&#34;&gt;2.8 semantic_brand_score()&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-12-semantic-brand-score/&#34;&gt;文献&amp;amp;代码 | 使用Python计算语义品牌评分(Semantic Brand Score, SBS)&lt;/a&gt; ， 通过 SBS 来衡量品牌（个体、公司、品牌、关键词等）的重要性。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.semantic_brand_score(text, brands, lang=&amp;#39;chinese&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;text&lt;/strong&gt;&lt;/em&gt; 待分析文本&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;brands&lt;/strong&gt;&lt;/em&gt; 词语列表；&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;lang&lt;/strong&gt;&lt;/em&gt;  语言类型，&amp;ldquo;chinese&amp;quot;或&amp;quot;english&amp;rdquo;，默认&amp;quot;chinese&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以三体小说为例，通过测量品牌语义评分SBS来反映小说角色的重要性。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;brands&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;汪淼&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;史强&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;罗辑&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;叶文洁&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;伊文斯&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#准备santi_test_text&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#小说等分20份， 读取第一份得到santi_test_text&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;sbs_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;semantic_brand_score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;santi_test_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                               &lt;span class=&#34;n&#34;&gt;brands&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;brands&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                               &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;sbs_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/19-1st-sbs.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;如果将三体小说分成20份， 每一份都测算出每个角色的SBS，绘制出折线图如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/18-sbs-plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h3 id=&#34;29-文本相似度&#34;&gt;2.9 文本相似度&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.cosine_sim(text1, text2)   cos余弦相似
ct.jaccard_sim(text1, text2)  jaccard相似
ct.minedit_sim(text1, text2)  最小编辑距离相似度； 
ct.simple_sim(text1, text2)   更改变动算法
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;算法实现参考自 &lt;code&gt;Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt; 


&lt;span class=&#34;n&#34;&gt;text1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;编程真好玩编程真好玩&amp;#39;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;游戏真好玩编程真好玩&amp;#39;&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;cosine: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cosine_sim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;jaccard&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;jaccard_sim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;minedit&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;minedit_sim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;simple&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;simple_sim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cosine:  0.82
jaccard: 0.67
minedit: 1.00
simple:  0.84
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;三model模块&#34;&gt;三、Model模块&lt;/h2&gt;
&lt;p&gt;本部分主要内容是词嵌入模型相关技术， 包括Word2Vec(GLove)的训练、读取、扩展词典。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;函数(类)&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.W2Vmodel(corpus_file, encoding, lang=&amp;lsquo;chinese&amp;rsquo;)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;训练Word2Vec&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.load_w2v(w2v_path)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;读取cntext2.x训练出的word2vec模型文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.expand_dictionary(wv,  seeddict, topn=100)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;扩展词典,  结果保存到路径[output/Word2Vec]中&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.Glove(corpus_file, lang=&#39;chinese&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;训练GLove模型。 算法运行较慢，吃内存，不推荐！！&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;model&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.SoPmi(corpus_file, seed_file, lang=&#39;chinese&#39;)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;共现法扩展词典&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;31-w2vmodel&#34;&gt;3.1 W2VModel()&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.W2VModel(corpus_file,  encoding=&amp;#39;utf-8&amp;#39;, lang=&amp;#39;chinese&amp;#39;) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;corpus_file&lt;/strong&gt; 语料txt文件路径&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;encoding&lt;/strong&gt; 语料txt文件编码方式&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lang&lt;/strong&gt; 语料的语言类型， 中文chinese、英文english，默认中文。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#训练模型&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#[data/三体.txt]体积2.7M&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2VModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/三体.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#语料txt文件路径&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;#语料txt文件编码方式&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#英文传english&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#设置存储&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Start Training! This may take a while. Please be patient...

Training word2vec model took 5 seconds

Note: The Word2Vec model has been saved to output/Word2Vec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;[data/三体.txt]体积2.7M，  训练时间5s， 模型文件存储于 &lt;em&gt;&lt;strong&gt;output/Word2Vec/三体.100.6.bin&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-word2vec.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-glove&#34;&gt;3.2 Glove()&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.Glove(corpus_file, lang=&amp;#39;chinese&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;corpus_file&lt;/strong&gt; 语料txt文件路径&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lang&lt;/strong&gt; 语料的语言类型， 中文chinese、英文english，默认中文&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GLove算法的运算速度非常慢， cntext并没有对此进行优化，强烈不建议百兆以上语料使用本算法。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Glove&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/三体.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                 &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Create vocabulary for Glove.

Create cooccurrence matrix.

Create cooccurrence matrix.
To complete this task, the code may take a significant amount of time, ranging from several minutes to potentially hours. Please be patient while the process runs.

Iteration 20: error 10541294.8481
Finish training! Used 22.38 s

Save the glove embeddings to a binary file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-glove.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;训练生成的 &lt;code&gt;output/Glove/glove.三体.50.bin&lt;/code&gt; 也可用 &lt;em&gt;&lt;strong&gt;ct.load_w2v&lt;/strong&gt;&lt;/em&gt; 读取，这里就不展示了。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;33-sopm&#34;&gt;3.3 SoPm()&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.SoPmi(corpus_file, seed_file)       #人工标注的初始种子词
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;corpus_file&lt;/strong&gt;  语料txt文件路径&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;seed_file&lt;/strong&gt; 初始种子词txt文件路径&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;共现法&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;sopmier&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SoPmi&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/sopmi_corpus.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;   
                   &lt;span class=&#34;n&#34;&gt;seed_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/sopmi_seed.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;       &lt;span class=&#34;c1&#34;&gt;#人工标注的初始种子词&lt;/span&gt;
                     

&lt;span class=&#34;n&#34;&gt;sopmier&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 19.74 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/06-sopmi.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;34-load_w2v&#34;&gt;3.4 load_w2v()&lt;/h3&gt;
&lt;p&gt;导入预训练的word2vec模型，建议是cntext训练的！！其他版本的语言模型很容易出问题&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.load_w2v(w2v_path)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;w2v_path&lt;/strong&gt; 模型文件路径&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;读取  &lt;em&gt;&lt;strong&gt;output/Word2Vec/三体.100.6.bin&lt;/strong&gt;&lt;/em&gt; 模型文件,  返回 &lt;code&gt;gensim.models.word2vec.Word2Vec&lt;/code&gt; 类型。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#使用gensim也可读取训练的模型&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#from gensim.models import KeyedVectors&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#santi_w2v = KeyedVectors.load(&amp;#39;output/Word2Vec/三体.100.6.bin&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;santi_w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/Word2Vec/三体.100.6.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;santi_w2v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Loading word2vec model...
&amp;lt;gensim.models.word2vec.Word2Vec at 0x1069c0dd0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;注意&#34;&gt;注意&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;ct.load_w2v()&lt;/strong&gt;&lt;/em&gt; 导入后得到的数据类型是 &lt;em&gt;&lt;strong&gt;gensim.models.word2vec.Word2Vec&lt;/strong&gt;&lt;/em&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;gensim.models.word2vec.Word2Vec&lt;/strong&gt;&lt;/em&gt; 可以转化为  &lt;em&gt;&lt;strong&gt;gensim.models.keyedvectors.KeyedVectors&lt;/strong&gt;&lt;/em&gt; ，&lt;/p&gt;
&lt;p&gt;例如&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;santi_w2v.wv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&amp;lt;gensim.models.keyedvectors.KeyedVectors at 0x319f4a090&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;35-expand_dictionary&#34;&gt;3.5 expand_dictionary()&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.expand_dictionary(wv,  seeddict, topn=100)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;wv&lt;/strong&gt;  预训练模型，数据类型为 gensim.models.keyedvectors.KeyedVectors。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;seeddict&lt;/strong&gt;  参数类似于种子词；格式为PYTHON字典；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;topn&lt;/strong&gt; 返回topn个语义最接近seeddict的词&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据设置的seeddict,  可按类别扩展并生成对应的词典txt文件， txt文件位于[output/Word2Vec]中。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;seeddict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;人物&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;叶文洁&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;史强&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;罗辑&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;物体&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;飞船&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;车辆&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;santi_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  
                     &lt;span class=&#34;n&#34;&gt;seeddict&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seeddict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                     &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-expand.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;四mind模块&#34;&gt;四、Mind模块&lt;/h2&gt;
&lt;p&gt;词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多个对象与某概念的语义远近&lt;/strong&gt;，职业与性别，某个职业是否存在亲近男性，而排斥女性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多个对象在某概念向量投影的大小， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本模块主要是利用已训练出的word2vec模型，挖掘潜在的态度偏见、刻板印象等。 这部分难度较大， 建议有精力且电脑性能好的同学可以用 cntext 训练模型， 再来实验Mind模块。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模块&lt;/th&gt;
&lt;th&gt;函数(类)&lt;/th&gt;
&lt;th&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.sematic_projection(wv, words, c_words1, c_words2)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;测量语义投影&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.sematic_distance(wv, words, c_words1, c_words2)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;测量语义距离&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.divergent_association_task(wv, words)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;测量发散思维(创造力)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ct.discursive_diversity_score(wv, words)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;测量语言差异性(认知差异性)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;mind&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;ct.procrustes_align(base_embed, other_embed)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;两个word2vec进行语义对齐，可反应随时间的社会语义变迁&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h3 id=&#34;41-sematic_distance&#34;&gt;4.1 sematic_distance()&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;多个对象与某概念的语义远近&lt;/strong&gt;，例如成功与性别，成功是否存在亲近男性，而排斥女性&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/21-music-success-genderbias.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.sematic_distance(wv, words, c_words1, c_words2) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;wv&lt;/strong&gt;&lt;/em&gt;   模型数据， 数据类型为gensim.models.keyedvectors.KeyedVectors。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;words&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;c_words2&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;c_words2&lt;/strong&gt;&lt;/em&gt; 均为词语列表&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分别计算 &lt;em&gt;&lt;strong&gt;words&lt;/strong&gt;&lt;/em&gt; 与  &lt;em&gt;&lt;strong&gt;c_words1&lt;/strong&gt;&lt;/em&gt; 、&lt;em&gt;&lt;strong&gt;c_words2&lt;/strong&gt;&lt;/em&gt; 语义距离，返回距离差值。例如&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;male_concept = [&amp;#39;male&amp;#39;, &amp;#39;man&amp;#39;, &amp;#39;he&amp;#39;, &amp;#39;him&amp;#39;]
female_concept = [&amp;#39;female&amp;#39;, &amp;#39;woman&amp;#39;, &amp;#39;she&amp;#39;, &amp;#39;her&amp;#39;]
software_engineer_concept  = [&amp;#39;engineer&amp;#39;,  &amp;#39;programming&amp;#39;,  &amp;#39;software&amp;#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果 &lt;em&gt;&lt;strong&gt;d1-d2&amp;lt;0&lt;/strong&gt;&lt;/em&gt;，说明在语义空间中，&lt;em&gt;&lt;strong&gt;software_engineer_concept&lt;/strong&gt;&lt;/em&gt; 更接近 &lt;em&gt;&lt;strong&gt;male_concept&lt;/strong&gt;&lt;/em&gt; ，更远离 &lt;em&gt;&lt;strong&gt;female_concept&lt;/strong&gt;&lt;/em&gt; 。&lt;/p&gt;
&lt;p&gt;换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# glove_w2v.6B.100d.txt链接: https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw 提取码: 72l0 &lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_word2vec_format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;glove_w2v.6B.100d.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;no_header&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#g_wv是gensim.models.keyedvectors.KeyedVectors&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;engineer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;program&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;software&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;computer&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;man_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;  &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;man&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;he&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;him&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;woman_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;woman&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;she&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;her&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#在语义空间中，工程师更接近于男人，而不是女人。&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#in semantic space, engineer is closer to man, other than woman.&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sematic_distance&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;engineer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;n&#34;&gt;c_words1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;man_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;n&#34;&gt;c_words2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;woman_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;-0.38
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-sematic_projection&#34;&gt;4.2 sematic_projection()&lt;/h3&gt;
&lt;p&gt;多个对象在某概念向量投影的大小&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.sematic_projection(wv, words, c_words1, c_words2) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;wv&lt;/strong&gt;&lt;/em&gt;   模型数据， 数据类型为gensim.models.keyedvectors.KeyedVectors。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;words&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;c_words2&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;c_words2&lt;/strong&gt;&lt;/em&gt; 均为词语列表&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了解释词向量模型的语义投影，我使用了 2022 年 Nature 论文中的图片[@Grand2022SemanticPR]。 关于动物的名字，人类对动物大小的认知信息隐藏在语料库文本中。 通过将&lt;strong&gt;LARGE WORDS&lt;/strong&gt; 和&lt;strong&gt;SMALL WORDS&lt;/strong&gt;的含义用不同的&lt;strong&gt;animals&lt;/strong&gt;的向量投影，动物在&lt;strong&gt;size向量&lt;/strong&gt;上的投影（就像下图中的红线 ) 得到，因此可以通过计算比较动物的大小。&lt;/p&gt;
&lt;p&gt;根据两组反义词 &lt;em&gt;&lt;strong&gt;c_words1&lt;/strong&gt;&lt;/em&gt; ,    &lt;em&gt;&lt;strong&gt;c_words2&lt;/strong&gt;&lt;/em&gt; 构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。&lt;/p&gt;
&lt;p&gt;分值越大，&lt;em&gt;&lt;strong&gt;words&lt;/strong&gt;&lt;/em&gt; 越位于 &lt;em&gt;&lt;strong&gt;c_words2&lt;/strong&gt;&lt;/em&gt; 一侧。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. &lt;em&gt;Nature Human Behaviour&lt;/em&gt;, pp.1-13.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/22-semantic_projection.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。&lt;/p&gt;
&lt;p&gt;以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;animals&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mouse&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;cat&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;horse&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;pig&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;whale&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;small_words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;small&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;little&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;tiny&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;large_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;large&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;big&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;huge&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#wiki_wv = 导入wiki的模型。&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#wiki_wv&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# In size conception, mouse is smallest, horse is biggest.&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 在大小概念上，老鼠最小，马是最大的。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sematic_projection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wiki_wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                      &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;animals&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                      &lt;span class=&#34;n&#34;&gt;c_words1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;small_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                      &lt;span class=&#34;n&#34;&gt;c_words2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;large_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;mouse&amp;#39;, -1.68),
 (&amp;#39;cat&amp;#39;, -0.92),
 (&amp;#39;pig&amp;#39;, -0.46),
 (&amp;#39;whale&amp;#39;, -0.24),
 (&amp;#39;horse&amp;#39;, 0.4)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;关于尺寸的认知，人类在文本中隐含着老鼠较小，马较大。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;43-divergent_association_task&#34;&gt;4.3 divergent_association_task()&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/&#34;&gt;PNAS | 使用语义距离测量一个人的创新力(发散思维)得分&lt;/a&gt;。一些理论认为，有 创造力 的人能够产生更多 发散性 的想法。如果这是正确的，简单地让被试写 N 个不相关的单词，然后测量这N个词的语义距离， 作为发散思维的客观衡量标准。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.divergent_association_task(wv, words)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;wv&lt;/strong&gt;&lt;/em&gt;   模型数据， 数据类型为 gensim.models.keyedvectors.KeyedVectors。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;words&lt;/strong&gt;&lt;/em&gt;词语列表&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;low_words = [&amp;#34;arm&amp;#34;, &amp;#34;eyes&amp;#34;, &amp;#34;feet&amp;#34;, &amp;#34;hand&amp;#34;, &amp;#34;head&amp;#34;, &amp;#34;leg&amp;#34;, &amp;#34;body&amp;#34;]
average_words = [&amp;#34;bag&amp;#34;, &amp;#34;bee&amp;#34;, &amp;#34;burger&amp;#34;, &amp;#34;feast&amp;#34;, &amp;#34;office&amp;#34;, &amp;#34;shoes&amp;#34;, &amp;#34;tree&amp;#34;]
high_words = [&amp;#34;hippo&amp;#34;, &amp;#34;jumper&amp;#34;, &amp;#34;machinery&amp;#34;, &amp;#34;prickle&amp;#34;, &amp;#34;tickets&amp;#34;, &amp;#34;tomato&amp;#34;, &amp;#34;violin&amp;#34;]

# 导入模型，得到wv。
# wv为gensim.models.keyedvectors.KeyedVectors类型

print(ct.divergent_association_task(wv, low_words)) # 50
print(ct.divergent_association_task(wv, average_words)) # 78
print(ct.divergent_association_task(wv, high_words)) # 95
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;50
78
95
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;44-discursive_diversity_score&#34;&gt;4.4 discursive_diversity_score()&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/&#34;&gt;MS2022 | 使用语言差异性测量团队认知差异性&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.discursive_diversity_score(wv, words)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;wv&lt;/strong&gt;&lt;/em&gt;   模型数据， 数据类型为 gensim.models.keyedvectors.KeyedVectors。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;words&lt;/strong&gt;&lt;/em&gt;词语列表&lt;/li&gt;
&lt;li&gt;返回一个数值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/23-low-and-high-examples-of-discursive-diversity.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;高绩效团队是那些具有调节共享认知以适应不断变化的任务要求的集体能力的团队：在进行构思任务时，它们表现出更高的话语多样性，在执行协调任务时，表现出较低的话语多样性。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;45-procrustes_align&#34;&gt;4.5 procrustes_align()&lt;/h3&gt;
&lt;p&gt;该函数主要用于反映同一研究对象随着时间推进的社会文化变迁，或者同一时间范围内两个被研究主体间的差异。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.procrustes_align(base_embed, other_embed)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;base_embed&lt;/strong&gt;&lt;/em&gt;  基本模型； 模型数据类型为&lt;em&gt;&lt;strong&gt;gensim.models.word2vec.Word2Vec&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;base_embed&lt;/strong&gt;&lt;/em&gt;  其他模型； 模型数据类型为&lt;em&gt;&lt;strong&gt;gensim.models.word2vec.Word2Vec&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于不同语料训练的Word2Vec模型无法直接比较， 需要先选定一个基准模型 &lt;em&gt;&lt;strong&gt;base_embed&lt;/strong&gt;&lt;/em&gt;， 之后根据 &lt;em&gt;&lt;strong&gt;base_embed&lt;/strong&gt;&lt;/em&gt; 对其他模型 &lt;em&gt;&lt;strong&gt;other_embed&lt;/strong&gt;&lt;/em&gt; 进行调整，调整后的模型就可以使用前面的语义距离函数或者语义投影函数。 这一过程用到的算法叫做 procrustes正交算法。&lt;/p&gt;
&lt;p&gt;这里推荐一篇 &lt;a href=&#34;https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/&#34;&gt;可视化 | 人民日报语料反映七十年文化演变&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;获取cntext2x&#34;&gt;获取cntext2.x&lt;/h2&gt;
&lt;p&gt;加大邓 &lt;em&gt;&lt;strong&gt;WeChat: 372335839&lt;/strong&gt;&lt;/em&gt;， 备注「姓名-学校-专业」， 100元领取  &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 文件。本文出现的cntext，默认均为2.x版本。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;使用声明&#34;&gt;使用声明&lt;/h2&gt;
&lt;p&gt;如果再研究或项目中使用到 &lt;strong&gt;cntext&lt;/strong&gt; ，请声明出处。&lt;/p&gt;
&lt;h3 id=&#34;apalike&#34;&gt;apalike&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Deng X., Nan P. (2022). cntext: a Python tool for text mining. DOI: 10.5281/zenodo.7063523 URL: https://github.com/hiDaDeng/cntext
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;bibtex&#34;&gt;bibtex&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;@misc{YourReferenceHere,
author = {Deng, Xudong and Nan, Peng},
doi = {10.5281/zenodo.7063523},
month = {9},
title = {cntext: a Python tool for text mining},
url = {https://github.com/hiDaDeng/cntext},
year = {2022}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;endnote&#34;&gt;endnote&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;%0 Generic
%A Deng, Xudong
%A Nan, Peng
%D 2022
%K text mining
%K text analysi
%K social science
%K management science
%K semantic analysis
%R 10.5281/zenodo.7063523
%T cntext: a Python tool for text mining
%U https://github.com/hiDaDeng/cntext
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p><em><strong>cntext</strong></em>是大邓开发维护的中英文文本分析库，内置有多重词典和常用函数， 包括</p>
<ul>
<li>免费的 1.x 版， 更新至 1.9。</li>
<li>收费的2.x版， 更新至 2.1.1。</li>
</ul>
<p>加大邓 <em><strong>WeChat: 372335839</strong></em>， 备注「姓名-学校-专业」， 100元领取  <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 文件。本文出现的cntext，默认均为2.x版本。</p>
<p><br><br></p>
<h2 id="安装cntext">安装cntext</h2>
<p>所有 <em><strong>cntext2.x</strong></em> 安装方法类似， 以目前 cntext2.1.1 为例，将 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><br>
<p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install pdfdocx
pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
</code></pre></div><br>
<p>文章开头和文章末都有 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em>  获取方式说明。</p>
<p><br><br></p>
<h2 id="功能模块">功能模块</h2>
<p>cntext含io、model、stats、mind四个模块</p>
<ol>
<li>导入数据用io</li>
<li>训练模型扩展词典用model</li>
<li>统计词频、情感分析、相似度等用stats</li>
<li>态度认知文化变迁用mind</li>
</ol>
<p>函数部分加粗的为常用函数。</p>
<table>
<thead>
<tr>
<th>模块</th>
<th>函数</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>io</strong></em></td>
<td><em><strong>ct.get_dict_list()</strong></em></td>
<td>查看cntext内置词典</td>
</tr>
<tr>
<td><em><strong>io</strong></em></td>
<td><em><strong>ct.read_dict_yaml(yfile)</strong></em></td>
<td>读取内置yaml词典</td>
</tr>
<tr>
<td><em><strong>io</strong></em></td>
<td><code>ct.detect_encoding(file, num_lines=100)</code></td>
<td>诊断txt、csv编码格式</td>
</tr>
<tr>
<td><em><strong>io</strong></em></td>
<td><code>ct.get_files(fformat)</code></td>
<td>查看符合fformat路径规则的所有的文件</td>
</tr>
<tr>
<td><em><strong>io</strong></em></td>
<td><em><strong>ct.read_file(file, encodings)</strong></em></td>
<td>读取文件</td>
</tr>
<tr>
<td><em><strong>io</strong></em></td>
<td><em><strong>ct.read_file(fformat, encoding)</strong></em></td>
<td>读取符合fformat路径规则的所有的文件，返回df</td>
</tr>
<tr>
<td><em><strong>model</strong></em></td>
<td><em><strong>ct.W2Vmodel(corpus_file, encoding, lang=&lsquo;chinese&rsquo;)</strong></em></td>
<td>训练Word2Vec</td>
</tr>
<tr>
<td><em><strong>model</strong></em></td>
<td><em><strong>ct.load_w2v(w2v_path)</strong></em></td>
<td>读取cntext2.x训练出的word2vec模型文件</td>
</tr>
<tr>
<td><em><strong>model</strong></em></td>
<td><em><strong>ct.expand_dictionary(wv,  seeddict, topn=100)</strong></em></td>
<td>扩展词典,  结果保存到路径[output/Word2Vec]中</td>
</tr>
<tr>
<td><em><strong>model</strong></em></td>
<td><code>ct.Glove(corpus_file, lang='chinese')</code></td>
<td>训练GLove模型。 算法运行较慢，吃内存，不推荐！！</td>
</tr>
<tr>
<td><em><strong>model</strong></em></td>
<td><code>ct.SoPmi(corpus_file, seed_file, lang='chinese')</code></td>
<td>共现法扩展词典</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><code>ct.term_freq(text, lang='chinese')</code></td>
<td>词频统计</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><code>ct.readability(text, lang='chinese')</code></td>
<td>文本可读性</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.sentiment(text, diction, lang=&lsquo;chinese&rsquo;)</strong></em></td>
<td>无(等)权重词典的情感分析</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><code>ct.sentiment_by_valence(text, diction, lang='chinese')</code></td>
<td>带权重的词典的情感分析</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.word_in_context(text, keywords, window=3, lang=&lsquo;chinese&rsquo;)</strong></em></td>
<td>在text中查找keywords出现的上下文内容(窗口window)，返回df</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.epu()</strong></em></td>
<td>使用新闻文本数据计算经济政策不确定性EPU，返回df</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.fepu(text, ep_pattern='', u_pattern='')</strong></em></td>
<td>使用md&amp;a文本数据计算企业不确定性感知FEPU</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.semantic_brand_score(text, brands, lang=&lsquo;chinese&rsquo;)</strong></em></td>
<td>衡量品牌（个体、公司、品牌、关键词等）的重要性</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.cosine_sim(text1, text2)</strong></em></td>
<td>余弦相似度</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><code>ct.jaccard_sim(text1, text2)</code></td>
<td>Jaccard相似度</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><code>ct.minedit_sim(text1, text2)</code></td>
<td>最小编辑距离</td>
</tr>
<tr>
<td><em><strong>mind</strong></em></td>
<td><em><strong>tm = ct.Text2Mind(wv)</strong></em><br></td>
<td>单个word2vec内挖掘潜在的态度偏见、刻板印象等。tm含多重方法</td>
</tr>
<tr>
<td><em><strong>mind</strong></em></td>
<td><code>ct.sematic_projection(wv, words, c_words1, c_words2)</code></td>
<td>测量语义投影</td>
</tr>
<tr>
<td><em><strong>mind</strong></em></td>
<td><code>ct.sematic_distance(wv, words, c_words1, c_words2)</code></td>
<td>测量语义距离</td>
</tr>
<tr>
<td><em><strong>mind</strong></em></td>
<td><code>ct.divergent_association_task(wv, words)</code></td>
<td>测量发散思维(创造力)</td>
</tr>
<tr>
<td><em><strong>mind</strong></em></td>
<td><code>ct.discursive_diversity_score(wv, words)</code></td>
<td>测量语言差异性(认知差异性)</td>
</tr>
<tr>
<td><em><strong>mind</strong></em></td>
<td><em><strong>ct.procrustes_align(base_embed, other_embed)</strong></em></td>
<td>两个word2vec进行语义对齐，可反应随时间的社会语义变迁</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">io</span>
    <span class="nx">mind</span>
    <span class="nx">model</span>
    <span class="nx">stats</span>
<span class="o">...</span>
</code></pre></div><br>
<br>
<h2 id="一io模块">一、IO模块</h2>
<table>
<thead>
<tr>
<th>模块</th>
<th>函数</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>io</strong></em></td>
<td><em><strong>ct.get_dict_list()</strong></em></td>
<td>查看cntext内置词典</td>
</tr>
<tr>
<td><em><strong>io</strong></em></td>
<td><em><strong>ct.read_dict_yaml(yfile)</strong></em></td>
<td>读取内置yaml词典</td>
</tr>
<tr>
<td><em><strong>io</strong></em></td>
<td><code>ct.detect_encoding(file, num_lines=100)</code></td>
<td>诊断txt、csv编码格式</td>
</tr>
<tr>
<td><em><strong>io</strong></em></td>
<td><code>ct.get_files(fformat)</code></td>
<td>查看符合fformat路径规则的所有的文件</td>
</tr>
<tr>
<td><em><strong>io</strong></em></td>
<td><em><strong>ct.read_file(file, encoding)</strong></em></td>
<td>读取文件</td>
</tr>
<tr>
<td><em><strong>io</strong></em></td>
<td><em><strong>ct.read_files(fformat, encoding)</strong></em></td>
<td>读取符合fformat路径规则的所有的文件，返回df</td>
</tr>
</tbody>
</table>
<h3 id="11-get_dict_list">1.1 get_dict_list()</h3>
<p>查看cntext内置词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">ct</span><span class="o">.</span><span class="n">get_dict_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;zh_common_NTUSD.yaml&#39;,
 &#39;zh_common_DUTIR.yaml&#39;,
 &#39;enzh_common_StopWords.yaml&#39;,
 &#39;en_valence_Concreteness.yaml&#39;,
 &#39;en_common_LoughranMcDonald.yaml&#39;,
 &#39;zh_common_FinanceSenti.yaml&#39;,
 &#39;zh_common_FLS.yaml&#39;,
 &#39;zh_common_TsinghuaPraiseDegrade.yaml&#39;,
 &#39;zh_common_FEPU.yaml&#39;,
 &#39;en_common_ANEW.yaml&#39;,
 &#39;en_common_NRC.yaml&#39;,
 &#39;zh_valence_ChineseEmoBank.yaml&#39;,
 &#39;zh_valence_SixSemanticDimensionDatabase.yaml&#39;,
 &#39;zh_common_FinacialFormalUnformal.yaml&#39;,
 &#39;zh_common_LoughranMcDonald.yaml&#39;,
 &#39;enzh_common_AdvConj.yaml&#39;,
 &#39;en_common_SentiWS.yaml&#39;,
 &#39;zh_common_Digitalization.yaml&#39;,
 &#39;en_common_LSD2015.yaml&#39;,
 &#39;zh_common_HowNet.yaml&#39;,
 &#39;zh_common_EPU.yaml&#39;]
</code></pre></div><h3 id="12-内置yaml词典">1.2 内置yaml词典</h3>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>zh_valence_ChineseEmoBank.yaml</strong></em></td>
<td>中文情感词典，含<code>效价valence</code>和<code>唤醒度arousal</code>。在cntext中，我们只使用了CVAW词表(单词)，其他词典如CVAP, CVAS, CVAT没有纳入到ChineseEmoBank.pkl.</td>
<td>Chinese</td>
<td><code>效价valence</code>和<code>唤醒度arousal</code></td>
</tr>
<tr>
<td><em><strong>zh_common_DUTIR.yaml</strong></em></td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td><em><strong>zh_common_HowNet.yaml</strong></em></td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td><code>en_common_SentiWS.yaml</code></td>
<td>SentimentWortschatz (SentiWS)</td>
<td>德文</td>
<td>正面词、负面词；<br></td>
</tr>
<tr>
<td><em><strong>zh_common_FinacialFormalUnformal.yaml</strong></em></td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td><code>en_common_ANEW.yaml</code></td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>pleasure, arousal, dominance</td>
</tr>
<tr>
<td><code>en_common_LSD2015.yaml</code></td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td><code>en_common_NRC.yaml</code></td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td><em><strong>zh_valence_SixSemanticDimensionDatabase.yaml</strong></em></td>
<td><a href="https://textdata.cn/blog/2023-03-20-nature-six-semantic-dimension-database/"><strong>通用中英文六维语义情感词典</strong></a>, 含17940个中文词的六维度词库， 且每个维度有权重。</td>
<td>中文</td>
<td>vision、socialness、emotion、time、space、motor</td>
</tr>
<tr>
<td><code>enzh_common_AdvConj.yaml</code></td>
<td>副词连词</td>
<td>中、英</td>
<td></td>
</tr>
<tr>
<td><em><strong>enzh_common_StopWords.yaml</strong></em></td>
<td>中英文停用词</td>
<td>中、英</td>
<td>停用词</td>
</tr>
<tr>
<td><em><strong>en_valence_Concreteness.yaml</strong></em></td>
<td><a href="https://textdata.cn/blog/jcr_concreteness_computation/">英文具体性词典</a></td>
<td>English</td>
<td>word &amp; concreateness score</td>
</tr>
<tr>
<td><em><strong>zh_common_LoughranMcDonald.yaml</strong></em></td>
<td>中文LoughranMcDonald词典</td>
<td>中文</td>
<td>正面、负面词</td>
</tr>
<tr>
<td><em><strong>zh_common_Digitalization.yaml</strong></em></td>
<td><a href="https://textdata.cn/blog/2022-11-03-mda-measure-digitalization/">管理世界|吴非(2021)数字化词典</a></td>
<td>中文</td>
<td>含人工智能技术、大数据技术、云计算技术、区块链技术、数字技术应用等关键词列表。</td>
</tr>
<tr>
<td><em><strong>en_common_LoughranMcDonald.yaml</strong></em></td>
<td>英文LoughranMcDonald词典</td>
<td>英文</td>
<td>金融LM情绪词典2018年版本，含七个词表，分别是Negative, Positive, Uncertainty, Litigious, StrongModal, WeakModal, Constraining</td>
</tr>
<tr>
<td><em><strong>zh_common_FLS.yaml</strong></em></td>
<td><a href="https://textdata.cn/blog/2023-09-08-earnings-communication-conference-forward-looking-statements-information/"><strong>业绩说明会前瞻性词典集</strong></a></td>
<td>中文</td>
<td>含174个词语</td>
</tr>
</tbody>
</table>
<br>
<h3 id="13-read_dict_yaml">1.3 read_dict_yaml()</h3>
<p>使用 cntext 读取 <em><strong>.yaml</strong></em> 词典文件；  返回的信息包括</p>
<ul>
<li>Name 词典的名字</li>
<li>Desc 词典的含义、概念解释</li>
<li>Refer 词典文献出处</li>
<li>Category 词典Dictionary的关键词</li>
<li>Dictionary 词典, python字典格式</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;zh_common_Digitalization.yaml&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;Name&#39;: &#39;中文数字化词典&#39;, 
&#39;Desc&#39;: &#39;基于这篇论文，构建了中文数字化词典，含人工智能技术、大数据技术、云计算技术、区块链技术、数字技术应用等关键词列表。 &#39;, &#39;Refer&#39;: &#39;吴非,胡慧芷,林慧妍,任晓怡. 企业数字化转型与资本市场表现——来自股票流动性的经验证据[J]. 管理世界,2021,37(07):130-144+10.&#39;, 
&#39;Category&#39;: [&#39;Artificial_Intelligence&#39;, &#39;Big_Data&#39;, &#39;Cloud_Computing&#39;, &#39;Block_Chains&#39;, &#39;Usage_of_Digitalization&#39;], 

&#39;Dictionary&#39;: 
    {&#39;Artificial_Intelligence&#39;: [&#39;人工智能&#39;, &#39;商业智能&#39;, &#39;图像理解&#39;, &#39;投资决策辅助系统&#39;, &#39;智能数据分析&#39;, &#39;智能机器人&#39;, &#39;机器学习&#39;, &#39;深度学习&#39;, &#39;语义搜索&#39;, &#39;生物识别技术&#39;, &#39;人脸识别&#39;, &#39;语音识别&#39;, &#39;身份验证&#39;, &#39;自动驾驶&#39;, &#39;自然语言处理&#39;], 
    &#39;Big_Data&#39;: [&#39;大数据&#39;, &#39;数据挖掘&#39;, &#39;文本挖掘&#39;, &#39;数据可视化&#39;, &#39;异构数据&#39;, &#39;征信&#39;, &#39;增强现实&#39;, &#39;混合现实&#39;, &#39;虚拟现实&#39;], 
    &#39;Cloud_Computing&#39;: [&#39;云计算&#39;, &#39;流计算&#39;, &#39;图计算&#39;, &#39;内存计算&#39;, &#39;多方安全计算&#39;, &#39;类脑计算&#39;, &#39;绿色计算&#39;, &#39;认知计算&#39;, &#39;融合架构&#39;, &#39;亿级并发&#39;, &#39;EB级存储&#39;, &#39;物联网&#39;, &#39;信息物理系统&#39;], 
    &#39;Block_Chains&#39;: [&#39;区块链&#39;, &#39;数字货币&#39;, &#39;分布式计算&#39;, &#39;差分隐私技术&#39;, &#39;智能金融合约&#39;], 
    &#39;Usage_of_Digitalization&#39;: [&#39;移动互联网&#39;, &#39;工业互联网&#39;, &#39;移动互联&#39;, &#39;互联网医疗&#39;, &#39;电子商务&#39;, &#39;移动支付&#39;, &#39;第三方支付&#39;, &#39;NFC支付&#39;, &#39;智能能源&#39;, &#39;B2B&#39;, &#39;B2C&#39;, &#39;C2B&#39;, &#39;C2C&#39;, &#39;O2O&#39;, &#39;网联&#39;, &#39;智能穿戴&#39;, &#39;智慧农业&#39;, &#39;智能交通&#39;, &#39;智能医疗&#39;, &#39;智能客服&#39;, &#39;智能家居&#39;, &#39;智能投顾&#39;, &#39;智能文旅&#39;, &#39;智能环保&#39;, &#39;智能电网&#39;, &#39;智能营销&#39;, &#39;数字营销&#39;, &#39;无人零售&#39;, &#39;互联网金融&#39;, &#39;数字金融&#39;, &#39;Fintech&#39;, &#39;金融科技&#39;, &#39;量化金融&#39;, &#39;开放银行&#39;]}}
</code></pre></div><br>
<h3 id="14-detect_encoding">1.4 detect_encoding()</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.detect_encoding(file, num_lines=100)
</code></pre></div><p>通过读取前num_lines来识别txt/csv文件的编码格式</p>
<ul>
<li><em><strong>file</strong></em> 文件路径</li>
<li><em><strong>num_line</strong></em> 行数</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#读取data文件夹下的【三体.txt】</span>
<span class="c1">#识别编码方式</span>
<span class="n">ct</span><span class="o">.</span><span class="n">detect_encoding</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/三体.txt&#39;</span><span class="p">,</span> <span class="n">num_lines</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">utf-8
</code></pre></div><br>
<h3 id="15-read_file">1.5 read_file()</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.read_file(file, encoding=&#39;utf-8&#39;)
</code></pre></div><ul>
<li><strong>file</strong> 待读取的文件路径； 支持txt、pdf、docx、xlsx、xls， 返回 DataFrame(含doc和file两个字段)。</li>
<li><strong>encoding</strong> 待读取文件的编码方式</li>
</ul>
<p>以 <code>data/三体.txt</code> 为例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#默认encoding=&#39;utf-8&#39;</span>
<span class="c1">#sdf = ct.read_file(fformat=&#39;data/三体.txt&#39;)</span>

<span class="n">sdf</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">fformat</span><span class="o">=</span><span class="s1">&#39;data/三体.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
<span class="n">sdf</span>
</code></pre></div><p><img loading="lazy" src="img/01-san_ti_df.png" alt=""  />
</p>
<br>
<h3 id="16-get_filesfformat">1.6 get_files(fformat)</h3>
<ul>
<li><strong>fformat</strong>  fformat格式支持 txt/pdf/docx/xlsx/csv等。 <code>*</code>表示通配符</li>
</ul>
<p>查看符合fformat路径规则的所有的文件， fformat格式支持 txt/pdf/docx/xlsx/csv等。 <code>*</code>表示通配符</p>
<table>
<thead>
<tr>
<th>fformat格式</th>
<th>识别的文件</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>*.txt</code></td>
<td>匹配当前代码所在路径内的所有txt</td>
</tr>
<tr>
<td><code>*.pdf</code></td>
<td>匹配当前代码所在路径内的所有pdf</td>
</tr>
<tr>
<td><code>data/*.txt</code></td>
<td>匹配「文件夹data」内所有的 txt</td>
</tr>
<tr>
<td><br></td>
<td></td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看【文件夹data】内所有的 txt文件。</span>
<span class="n">ct</span><span class="o">.</span><span class="n">get_files</span><span class="p">(</span><span class="n">fformat</span><span class="o">=</span><span class="s1">&#39;data/*.txt&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;data/三体.txt&#39;,
 &#39;data/santi.txt&#39;,
 &#39;data/w2v_corpus.txt&#39;,
 &#39;data/sopmi_corpus.txt&#39;,
 &#39;data/brown_corpus.txt&#39;,
 &#39;data/sopmi_seed_words.txt&#39;]
</code></pre></div><br>
<h3 id="17-read_files">1.7 read_files()</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.read_files(fformat, encoding=&#39;utf-8&#39;）
</code></pre></div><p>批量读取符合fformat格式的所有文件数据，返回DataFrame(含doc和file两个字段)。</p>
<p>读取[文件夹data里所有txt]</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#默认encoding=&#39;utf-8&#39;</span>
<span class="c1">#ddf = ct.read_files(fformat=&#39;data/*.txt&#39;)</span>

<span class="n">ddf</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_files</span><span class="p">(</span><span class="n">fformat</span><span class="o">=</span><span class="s1">&#39;data/*.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
<span class="n">ddf</span>
</code></pre></div><p><img loading="lazy" src="img/02-ddf.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二stats模块">二、Stats模块</h2>
<table>
<thead>
<tr>
<th>模块</th>
<th>函数</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>stats</strong></em></td>
<td><code>ct.term_freq(text, lang='chinese')</code></td>
<td>词频统计</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><code>ct.readability(text, lang='chinese')</code></td>
<td>文本可读性</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.sentiment(text, diction, lang=&lsquo;chinese&rsquo;)</strong></em></td>
<td>无(等)权重词典的情感分析</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><code>ct.sentiment_by_valence(text, diction, lang='chinese')</code></td>
<td>带权重的词典的情感分析</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.word_in_context(text, keywords, window=3, lang=&lsquo;chinese&rsquo;)</strong></em></td>
<td>在text中查找keywords出现的上下文内容(窗口window)，返回df</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.epu(text, e_pattern, p_pattern, u_pattern)</strong></em></td>
<td>使用新闻文本数据计算经济政策不确定性EPU，返回df</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.fepu(text, ep_pattern='', u_pattern='')</strong></em></td>
<td>使用md&amp;a文本数据计算企业不确定性感知FEPU</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.semantic_brand_score(text, brands, lang=&lsquo;chinese&rsquo;)</strong></em></td>
<td>衡量品牌（个体、公司、品牌、关键词等）的重要性</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><em><strong>ct.cosine_sim(text1, text2)</strong></em></td>
<td>余弦相似度</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><code>ct.jaccard_sim(text1, text2)</code></td>
<td>Jaccard相似度</td>
</tr>
<tr>
<td><em><strong>stats</strong></em></td>
<td><code>ct.minedit_sim(text1, text2)</code></td>
<td>最小编辑距离</td>
</tr>
</tbody>
</table>
<h3 id="21-term_freq">2.1 term_freq()</h3>
<p>统计词频， 返回Counter(类似于python字典) ； 支持中英文</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">term_freq(text, lang=&#39;chinese&#39;, return_df=False)
</code></pre></div><ul>
<li><strong>text</strong> 待分析的文本字符串</li>
<li><strong>lang</strong> 文本的语言类型， 中文chinese、英文english，默认中文。</li>
<li><strong>return_df</strong> 返回结果的数据类型，return_df=False时返回字典； return_df=True时返回DataFrame。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;致力于致力于以零文章处理费或订阅费发布优质研究软件。&#39;</span>

<span class="c1">#ct.term_freq(text, lang=&#39;chinese&#39;)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;致力于&#39;: 2,
         &#39;文章&#39;: 1,
         &#39;处理费&#39;: 1,
         &#39;订阅费&#39;: 1,
         &#39;发布&#39;: 1,
         &#39;优质&#39;: 1,
         &#39;研究&#39;: 1,
         &#39;软件&#39;: 1})
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_df</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/09-term_freq.png" alt=""  />
</p>
<br>
<h3 id="22-readabilit">2.2 readabilit()</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.readability(text, lang=&#39;chinese&#39;)
</code></pre></div><ul>
<li><strong>text</strong>  待分析的文本字符串</li>
<li><strong>lang</strong> 文本的语言类型， 中文chinese、英文english，默认中文。</li>
</ul>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p><strong>中文可读性</strong> 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;致力于以零文章处理费或订阅费发布优质研究软件。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 23.0, &#39;readability2&#39;: 2.0, &#39;readability3&#39;: 12.5}
</code></pre></div><br>
<h3 id="23-sentimenttext-diction-lang">2.3 sentiment(text, diction, lang)</h3>
<p>常见的情感分析默认情绪词无(等)权重， 通过统计词语个数来反应情感信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sentiment(text, diction, lang=&#39;chinese&#39;, return_df=False)
</code></pre></div><ul>
<li><strong>text</strong> 待分析的文本字符串</li>
<li><strong>diction</strong>  格式为Python字典类型。形如下面的案例</li>
<li><strong>lang</strong> 文本的语言类型， 中文chinese、英文english，默认中文。</li>
<li><strong>return_df</strong> 是否返回dataframe，默认False不返回</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span> 
             <span class="n">return_df</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/10-sentiment.png" alt=""  />
</p>
<br>
<h3 id="24-sentiment_by_valence">2.4 sentiment_by_valence()</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.sentiment_by_valence(text, diction, lang=&#39;chinese&#39;, return_df=False)
</code></pre></div><ul>
<li><strong>text</strong> 待分析的文本字符串</li>
<li><strong>diction</strong>  格式为Python字典类型。形如下面的案例</li>
<li><strong>lang</strong> 文本的语言类型， 中文chinese、英文english，默认中文。</li>
<li><strong>return_df</strong> 是否返回dataframe，默认False不返回</li>
</ul>
<p>常见的情感分析是无(等)权重, 但实际上不同的词语所携带的情感信息的强度差异是很大的。据此学者们开发出很多带权重的词典，例如</p>
<ul>
<li>英文具体性词典en_valence_Concreteness.yaml， 词典中每个词都有一个concreteness值</li>
<li>中文六维度语义词典zh_valence_SixSemanticDimensionDatabase.yaml,  每个中文词有六个值。</li>
</ul>
<p>以具体性为例， <strong>语言具体性Concreteness</strong>描述了一个词在多大程度上是指一个实际的、有形的或“真实的”实体，以一种更具体、更熟悉、更容易被眼睛或心灵感知的方式描述对象和行为（即，可想象或生动；Brysbaert, Warriner, and Kuperman 2014; Semin and Fiedler 1988)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">concreteness_dict</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;en_valence_Concreteness.yaml&#39;</span><span class="p">)[</span><span class="s1">&#39;Dictionary&#39;</span><span class="p">]</span>
<span class="n">concreteness_dict</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;roadsweeper&#39;: {&#39;concreteness&#39;: 4.85},
 &#39;traindriver&#39;: {&#39;concreteness&#39;: 4.54},
 &#39;tush&#39;: {&#39;concreteness&#39;: 4.45},
 &#39;hairdress&#39;: {&#39;concreteness&#39;: 3.93},
 &#39;pharmaceutics&#39;: {&#39;concreteness&#39;: 3.77},
 &#39;hoover&#39;: {&#39;concreteness&#39;: 3.76},
 &#39;shopkeeping&#39;: {&#39;concreteness&#39;: 3.18},
 &#39;pushiness&#39;: {&#39;concreteness&#39;: 2.48},
 ......
 }
</code></pre></div><p>可能 <em><strong>concreteness_dict</strong></em>不够直观， 如果整理转化一下大概类似于</p>
<p><img loading="lazy" src="img/11-concreteness_df.png" alt=""  />
</p>
<p><a href="https://textdata.cn/blog/jcr_concreteness_computation/"><strong>JCR2021 | 计算文本的语言具体性</strong></a> 文中提供了一个案例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">reply</span> <span class="o">=</span> <span class="s2">&#34;I&#39;ll go look for that&#34;</span>

<span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                              <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_dict</span><span class="p">,</span> 
                              <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>

<span class="n">score</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;text&#39;: &#34;I&#39;ll go look for that&#34;, 
&#39;concreteness&#39;: 9.28, 
&#39;word_num&#39;: 6}
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">employee_replys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I&#39;ll go look for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that top&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go look for that t-shirt in grey&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt in grey&#34;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">reply</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">employee_replys</span><span class="p">):</span>
    <span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                                  <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_dict</span><span class="p">,</span> 
                                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    
    <span class="n">template</span> <span class="o">=</span> <span class="s2">&#34;Concreteness Score: </span><span class="si">{score:.2f}</span><span class="s2"> | Example-</span><span class="si">{idx}</span><span class="s2">: </span><span class="si">{exmaple}</span><span class="s2">&#34;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="n">score</span><span class="p">[</span><span class="s1">&#39;concreteness&#39;</span><span class="p">],</span> 
                          <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> 
                          <span class="n">exmaple</span><span class="o">=</span><span class="n">reply</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Concreteness Score: 9.28 | Example-0: I&#39;ll go look for that
Concreteness Score: 9.32 | Example-1: I&#39;ll go search for that
Concreteness Score: 13.25 | Example-2: I&#39;ll go search for that top
Concreteness Score: 14.25 | Example-3: I&#39;ll go search for that t-shirt
Concreteness Score: 21.32 | Example-4: I&#39;ll go look for that t-shirt in grey
Concreteness Score: 21.36 | Example-5: I&#39;ll go search for that t-shirt in grey
</code></pre></div><br>
<h3 id="25-word_in_context">2.5 word_in_context()</h3>
<p>You shall know a word by the company it keeps通过一个单词所处的语境，我们可以了解该单词的含义。</p>
<p>在text中查找keywords出现的上下文内容(窗口window)，返回df。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.word_in_context(text, keywords, window=3, lang=&#39;chinese&#39;)
</code></pre></div><ul>
<li><strong>text</strong>  待分析文本</li>
<li><strong>keywords</strong>  关键词列表</li>
<li><strong>window</strong>  关键词上下文窗口大小</li>
<li><strong>lang</strong> 文本的语言类型， 中文chinese、英文english，默认中文。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#测试代码，假设zh_text是年报文本，从找找出丝网词相关词的上下文</span>
<span class="n">zh_text</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">【插入一条自家广告】大邓自己家的家，
</span><span class="s2">安平县多隆丝网制品，生产销售不锈钢轧花网、
</span><span class="s2">电焊网、石笼网、刀片刺绳、冲孔网等丝网制品。
</span><span class="s2">联系人 邓颖静 0318-7686899
</span><span class="s2">
</span><span class="s2">人生苦短，我学Python
</span><span class="s2">在社科中，可以用Python做文本分析
</span><span class="s2">Python是一门功能强大的编程语言，广泛应用在经管社科领域。
</span><span class="s2">可以做网络爬虫、文本分析、LDA话题模型、相似度分析等。
</span><span class="s2">
</span><span class="s2">今年经济不景气，形势异常严峻。
</span><span class="s2">由于疫情不景气，静默管理， 产品积压， 公司经营困难。
</span><span class="s2">保就业促就业，任务十分艰巨。
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="c1">#【python】上下文</span>
<span class="n">ct</span><span class="o">.</span><span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">zh_text</span><span class="p">,</span> 
                   <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;python&#39;</span><span class="p">],</span> 
                   <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                   <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/20-word-in-context.png" alt=""  />
</p>
<br>
<h3 id="26-epu">2.6 epu()</h3>
<p><a href="https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/"><strong>代码  | 使用新闻数据测量经济政策不确定性EPU</strong></a></p>
<p><img loading="lazy" src="img/13-epu-plot.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.epu(df, e_pattern, p_pattern, u_pattern)
</code></pre></div><ul>
<li><strong>df</strong>  新闻数据DataFrame， 含text和date两个字段。 每一行代表一条新闻记录</li>
<li><strong>e_pattern</strong> 字符串；经济类词典，用<code>|</code>间隔词语，形如 <strong>e_pattern = ‘经济|金融’</strong></li>
<li><strong>p_pattern</strong> 字符串；政策词典，用<code>|</code>间隔词语，形如 <strong>p_pattern = ‘政策|治理|行政’</strong></li>
<li><strong>u_pattern</strong>  字符串；不确定性词典，用<code>|</code>间隔词语，形如 <strong>u_pattern = ‘风险|危机|难以预测’</strong></li>
</ul>
<p>准备如下图格式的数据 <em><strong>news_df</strong></em></p>
<p><img loading="lazy" src="img/12-news-df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#省略，读取数据得到 news_df</span>

<span class="n">epu_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">epu</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">news_df</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;M&#39;</span><span class="p">)</span>
<span class="n">epu_df</span>
</code></pre></div><p><img loading="lazy" src="img/13-epu-df.png" alt=""  />
</p>
<br>
<h3 id="27-fepu">2.7 fepu()</h3>
<p><a href="https://textdata.cn/blog/2024-04-25-firm-economic-policy-uncertainty/">使用管理层讨论与分析文本数据测量「企业感知不确定性」(Subjective perception of economic policy uncertainty, FEPU)</a></p>
<p><img loading="lazy" src="img/16-fepu-plot.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.fepu(text, ep_pattern, u_pattern)
</code></pre></div><ul>
<li><em><strong>text</strong></em> ；某时期t某企业i的管理层讨论与分析md&amp;a文本</li>
<li><em><strong>ep_pattern</strong></em> 字符串；经济政策类词典，用<code>|</code>间隔词语，形如 <strong>ep_pattern = ‘经济|金融|政策|治理|行政’</strong></li>
<li><em><strong>u_pattern</strong></em> 字符串；不确定性词典，用<code>|</code>间隔词语，形如 <strong>u_pattern = ‘风险|危机|难以预测’</strong></li>
</ul>
<p>准备如下图格式的数据 <em><strong>mda_df</strong></em></p>
<p><img loading="lazy" src="img/14-mdadf.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#省略，读取数据得到 mda_df</span>

<span class="n">fepu_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">fepu</span><span class="p">)</span>
<span class="n">res_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">]],</span> <span class="n">fepu_df</span><span class="p">],</span>   <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">res_df</span>
</code></pre></div><p><img loading="lazy" src="img/15-fepu.png" alt=""  />
</p>
<br>
<br>
<h3 id="28-semantic_brand_score">2.8 semantic_brand_score()</h3>
<p><a href="https://textdata.cn/blog/2024-04-12-semantic-brand-score/">文献&amp;代码 | 使用Python计算语义品牌评分(Semantic Brand Score, SBS)</a> ， 通过 SBS 来衡量品牌（个体、公司、品牌、关键词等）的重要性。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.semantic_brand_score(text, brands, lang=&#39;chinese&#39;)
</code></pre></div><ul>
<li><em><strong>text</strong></em> 待分析文本</li>
<li><em><strong>brands</strong></em> 词语列表；</li>
<li><em><strong>lang</strong></em>  语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p>以三体小说为例，通过测量品牌语义评分SBS来反映小说角色的重要性。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">brands</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;汪淼&#39;</span><span class="p">,</span> <span class="s1">&#39;史强&#39;</span><span class="p">,</span> <span class="s1">&#39;罗辑&#39;</span><span class="p">,</span> <span class="s1">&#39;叶文洁&#39;</span><span class="p">,</span> <span class="s1">&#39;伊文斯&#39;</span><span class="p">]</span>

<span class="c1">#准备santi_test_text</span>
<span class="c1">#小说等分20份， 读取第一份得到santi_test_text</span>

<span class="n">sbs_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_brand_score</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">santi_test_text</span><span class="p">,</span> 
                               <span class="n">brands</span><span class="o">=</span><span class="n">brands</span><span class="p">,</span> 
                               <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
<span class="n">sbs_df</span>
</code></pre></div><p><img loading="lazy" src="img/19-1st-sbs.png" alt=""  />
</p>
<p>如果将三体小说分成20份， 每一份都测算出每个角色的SBS，绘制出折线图如下图所示。</p>
<p><img loading="lazy" src="img/18-sbs-plot.png" alt=""  />
</p>
<h3 id="29-文本相似度">2.9 文本相似度</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.cosine_sim(text1, text2)   cos余弦相似
ct.jaccard_sim(text1, text2)  jaccard相似
ct.minedit_sim(text1, text2)  最小编辑距离相似度； 
ct.simple_sim(text1, text2)   更改变动算法
</code></pre></div><p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;cosine: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;jaccard&#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;minedit&#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;simple&#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cosine:  0.82
jaccard: 0.67
minedit: 1.00
simple:  0.84
</code></pre></div><br>
<br>
<h2 id="三model模块">三、Model模块</h2>
<p>本部分主要内容是词嵌入模型相关技术， 包括Word2Vec(GLove)的训练、读取、扩展词典。</p>
<table>
<thead>
<tr>
<th>模块</th>
<th>函数(类)</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>model</strong></em></td>
<td><em><strong>ct.W2Vmodel(corpus_file, encoding, lang=&lsquo;chinese&rsquo;)</strong></em></td>
<td>训练Word2Vec</td>
</tr>
<tr>
<td><em><strong>model</strong></em></td>
<td><em><strong>ct.load_w2v(w2v_path)</strong></em></td>
<td>读取cntext2.x训练出的word2vec模型文件</td>
</tr>
<tr>
<td><em><strong>model</strong></em></td>
<td><em><strong>ct.expand_dictionary(wv,  seeddict, topn=100)</strong></em></td>
<td>扩展词典,  结果保存到路径[output/Word2Vec]中</td>
</tr>
<tr>
<td><em><strong>model</strong></em></td>
<td><code>ct.Glove(corpus_file, lang='chinese')</code></td>
<td>训练GLove模型。 算法运行较慢，吃内存，不推荐！！</td>
</tr>
<tr>
<td><em><strong>model</strong></em></td>
<td><code>ct.SoPmi(corpus_file, seed_file, lang='chinese')</code></td>
<td>共现法扩展词典</td>
</tr>
</tbody>
</table>
<h3 id="31-w2vmodel">3.1 W2VModel()</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.W2VModel(corpus_file,  encoding=&#39;utf-8&#39;, lang=&#39;chinese&#39;) 
</code></pre></div><ul>
<li><strong>corpus_file</strong> 语料txt文件路径</li>
<li><strong>encoding</strong> 语料txt文件编码方式</li>
<li><strong>lang</strong> 语料的语言类型， 中文chinese、英文english，默认中文。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#训练模型</span>
<span class="c1">#[data/三体.txt]体积2.7M</span>
<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;data/三体.txt&#39;</span><span class="p">,</span>  <span class="c1">#语料txt文件路径</span>
                  <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span><span class="c1">#语料txt文件编码方式</span>
                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span> <span class="c1">#英文传english</span>

<span class="n">w2v</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1">#设置存储</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Start Training! This may take a while. Please be patient...

Training word2vec model took 5 seconds

Note: The Word2Vec model has been saved to output/Word2Vec
</code></pre></div><p>[data/三体.txt]体积2.7M，  训练时间5s， 模型文件存储于 <em><strong>output/Word2Vec/三体.100.6.bin</strong></em></p>
<p><img loading="lazy" src="img/03-word2vec.png" alt=""  />
</p>
<br>
<h3 id="32-glove">3.2 Glove()</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.Glove(corpus_file, lang=&#39;chinese&#39;)
</code></pre></div><ul>
<li><strong>corpus_file</strong> 语料txt文件路径</li>
<li><strong>lang</strong> 语料的语言类型， 中文chinese、英文english，默认中文</li>
</ul>
<p>GLove算法的运算速度非常慢， cntext并没有对此进行优化，强烈不建议百兆以上语料使用本算法。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Glove</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;data/三体.txt&#39;</span><span class="p">,</span> 
                 <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Create vocabulary for Glove.

Create cooccurrence matrix.

Create cooccurrence matrix.
To complete this task, the code may take a significant amount of time, ranging from several minutes to potentially hours. Please be patient while the process runs.

Iteration 20: error 10541294.8481
Finish training! Used 22.38 s

Save the glove embeddings to a binary file
</code></pre></div><p><img loading="lazy" src="img/05-glove.png" alt=""  />
</p>
<p>训练生成的 <code>output/Glove/glove.三体.50.bin</code> 也可用 <em><strong>ct.load_w2v</strong></em> 读取，这里就不展示了。</p>
<br>
<h3 id="33-sopm">3.3 SoPm()</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.SoPmi(corpus_file, seed_file)       #人工标注的初始种子词
</code></pre></div><ul>
<li><strong>corpus_file</strong>  语料txt文件路径</li>
<li><strong>seed_file</strong> 初始种子词txt文件路径</li>
</ul>
<p>共现法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>   
                   <span class="n">seed_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed.txt&#39;</span><span class="p">)</span>       <span class="c1">#人工标注的初始种子词</span>
                     

<span class="n">sopmier</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 19.74 s
</code></pre></div><p><img loading="lazy" src="img/06-sopmi.png" alt=""  />
</p>
<br>
<h3 id="34-load_w2v">3.4 load_w2v()</h3>
<p>导入预训练的word2vec模型，建议是cntext训练的！！其他版本的语言模型很容易出问题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.load_w2v(w2v_path)
</code></pre></div><ul>
<li><strong>w2v_path</strong> 模型文件路径</li>
</ul>
<p>读取  <em><strong>output/Word2Vec/三体.100.6.bin</strong></em> 模型文件,  返回 <code>gensim.models.word2vec.Word2Vec</code> 类型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#使用gensim也可读取训练的模型</span>
<span class="c1">#from gensim.models import KeyedVectors</span>
<span class="c1">#santi_w2v = KeyedVectors.load(&#39;output/Word2Vec/三体.100.6.bin&#39;)</span>

<span class="n">santi_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">w2v_path</span><span class="o">=</span><span class="s1">&#39;output/Word2Vec/三体.100.6.bin&#39;</span><span class="p">)</span>
<span class="n">santi_w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading word2vec model...
&lt;gensim.models.word2vec.Word2Vec at 0x1069c0dd0&gt;
</code></pre></div><br>
<h3 id="注意">注意</h3>
<p><em><strong>ct.load_w2v()</strong></em> 导入后得到的数据类型是 <em><strong>gensim.models.word2vec.Word2Vec</strong></em> 。</p>
<p><em><strong>gensim.models.word2vec.Word2Vec</strong></em> 可以转化为  <em><strong>gensim.models.keyedvectors.KeyedVectors</strong></em> ，</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">santi_w2v.wv
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x319f4a090&gt;
</code></pre></div><br>
<h3 id="35-expand_dictionary">3.5 expand_dictionary()</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.expand_dictionary(wv,  seeddict, topn=100)
</code></pre></div><ul>
<li><strong>wv</strong>  预训练模型，数据类型为 gensim.models.keyedvectors.KeyedVectors。</li>
<li><strong>seeddict</strong>  参数类似于种子词；格式为PYTHON字典；</li>
<li><strong>topn</strong> 返回topn个语义最接近seeddict的词</li>
</ul>
<p>根据设置的seeddict,  可按类别扩展并生成对应的词典txt文件， txt文件位于[output/Word2Vec]中。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">seeddict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;人物&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;叶文洁&#39;</span><span class="p">,</span> <span class="s1">&#39;史强&#39;</span><span class="p">,</span> <span class="s1">&#39;罗辑&#39;</span><span class="p">],</span> 
    <span class="s1">&#39;物体&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;车辆&#39;</span><span class="p">]</span>
<span class="p">}</span>


<span class="n">ct</span><span class="o">.</span><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">santi_w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span>  
                     <span class="n">seeddict</span><span class="o">=</span><span class="n">seeddict</span><span class="p">,</span> 
                     <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/04-expand.png" alt=""  />
</p>
<br>
<br>
<h2 id="四mind模块">四、Mind模块</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<ul>
<li>
<p><strong>多个对象与某概念的语义远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
</li>
<li>
<p>多个对象在某概念向量投影的大小， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
</li>
</ul>
<p>本模块主要是利用已训练出的word2vec模型，挖掘潜在的态度偏见、刻板印象等。 这部分难度较大， 建议有精力且电脑性能好的同学可以用 cntext 训练模型， 再来实验Mind模块。</p>
<table>
<thead>
<tr>
<th>模块</th>
<th>函数(类)</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>mind</strong></em></td>
<td><code>ct.sematic_projection(wv, words, c_words1, c_words2)</code></td>
<td>测量语义投影</td>
</tr>
<tr>
<td><em><strong>mind</strong></em></td>
<td><code>ct.sematic_distance(wv, words, c_words1, c_words2)</code></td>
<td>测量语义距离</td>
</tr>
<tr>
<td><em><strong>mind</strong></em></td>
<td><code>ct.divergent_association_task(wv, words)</code></td>
<td>测量发散思维(创造力)</td>
</tr>
<tr>
<td><em><strong>mind</strong></em></td>
<td><code>ct.discursive_diversity_score(wv, words)</code></td>
<td>测量语言差异性(认知差异性)</td>
</tr>
<tr>
<td><em><strong>mind</strong></em></td>
<td><em><strong>ct.procrustes_align(base_embed, other_embed)</strong></em></td>
<td>两个word2vec进行语义对齐，可反应随时间的社会语义变迁</td>
</tr>
</tbody>
</table>
<br>
<h3 id="41-sematic_distance">4.1 sematic_distance()</h3>
<p><strong>多个对象与某概念的语义远近</strong>，例如成功与性别，成功是否存在亲近男性，而排斥女性</p>
<p><img loading="lazy" src="img/21-music-success-genderbias.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.sematic_distance(wv, words, c_words1, c_words2) 
</code></pre></div><ul>
<li><em><strong>wv</strong></em>   模型数据， 数据类型为gensim.models.keyedvectors.KeyedVectors。</li>
<li><em><strong>words</strong></em>、<em><strong>c_words2</strong></em>、<em><strong>c_words2</strong></em> 均为词语列表</li>
</ul>
<p>分别计算 <em><strong>words</strong></em> 与  <em><strong>c_words1</strong></em> 、<em><strong>c_words2</strong></em> 语义距离，返回距离差值。例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果 <em><strong>d1-d2&lt;0</strong></em>，说明在语义空间中，<em><strong>software_engineer_concept</strong></em> 更接近 <em><strong>male_concept</strong></em> ，更远离 <em><strong>female_concept</strong></em> 。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># glove_w2v.6B.100d.txt链接: https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw 提取码: 72l0 </span>
<span class="n">g_wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">,</span> <span class="n">no_header</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1">#g_wv是gensim.models.keyedvectors.KeyedVectors</span>

<span class="n">engineer</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">man_words</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">woman_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">g_wv</span><span class="p">,</span>
                    <span class="n">words</span><span class="o">=</span><span class="n">engineer</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">man_words</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">woman_words</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><br>
<h3 id="42-sematic_projection">4.2 sematic_projection()</h3>
<p>多个对象在某概念向量投影的大小</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.sematic_projection(wv, words, c_words1, c_words2) 
</code></pre></div><ul>
<li><em><strong>wv</strong></em>   模型数据， 数据类型为gensim.models.keyedvectors.KeyedVectors。</li>
<li><em><strong>words</strong></em>、<em><strong>c_words2</strong></em>、<em><strong>c_words2</strong></em> 均为词语列表</li>
</ul>
<p>为了解释词向量模型的语义投影，我使用了 2022 年 Nature 论文中的图片[@Grand2022SemanticPR]。 关于动物的名字，人类对动物大小的认知信息隐藏在语料库文本中。 通过将<strong>LARGE WORDS</strong> 和<strong>SMALL WORDS</strong>的含义用不同的<strong>animals</strong>的向量投影，动物在<strong>size向量</strong>上的投影（就像下图中的红线 ) 得到，因此可以通过计算比较动物的大小。</p>
<p>根据两组反义词 <em><strong>c_words1</strong></em> ,    <em><strong>c_words2</strong></em> 构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，<em><strong>words</strong></em> 越位于 <em><strong>c_words2</strong></em> 一侧。</p>
<blockquote>
<p>Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&quot;</p>
</blockquote>
<p><img loading="lazy" src="img/22-semantic_projection.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">small_words</span><span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">large_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1">#wiki_wv = 导入wiki的模型。</span>
<span class="c1">#wiki_wv</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wiki_wv</span><span class="p">,</span>
                      <span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">small_words</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">large_words</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p>关于尺寸的认知，人类在文本中隐含着老鼠较小，马较大。</p>
<br>
<h3 id="43-divergent_association_task">4.3 divergent_association_task()</h3>
<p><a href="https://textdata.cn/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/">PNAS | 使用语义距离测量一个人的创新力(发散思维)得分</a>。一些理论认为，有 创造力 的人能够产生更多 发散性 的想法。如果这是正确的，简单地让被试写 N 个不相关的单词，然后测量这N个词的语义距离， 作为发散思维的客观衡量标准。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.divergent_association_task(wv, words)
</code></pre></div><ul>
<li><em><strong>wv</strong></em>   模型数据， 数据类型为 gensim.models.keyedvectors.KeyedVectors。</li>
<li><em><strong>words</strong></em>词语列表</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">low_words = [&#34;arm&#34;, &#34;eyes&#34;, &#34;feet&#34;, &#34;hand&#34;, &#34;head&#34;, &#34;leg&#34;, &#34;body&#34;]
average_words = [&#34;bag&#34;, &#34;bee&#34;, &#34;burger&#34;, &#34;feast&#34;, &#34;office&#34;, &#34;shoes&#34;, &#34;tree&#34;]
high_words = [&#34;hippo&#34;, &#34;jumper&#34;, &#34;machinery&#34;, &#34;prickle&#34;, &#34;tickets&#34;, &#34;tomato&#34;, &#34;violin&#34;]

# 导入模型，得到wv。
# wv为gensim.models.keyedvectors.KeyedVectors类型

print(ct.divergent_association_task(wv, low_words)) # 50
print(ct.divergent_association_task(wv, average_words)) # 78
print(ct.divergent_association_task(wv, high_words)) # 95
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">50
78
95
</code></pre></div><br>
<h3 id="44-discursive_diversity_score">4.4 discursive_diversity_score()</h3>
<p><a href="https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/">MS2022 | 使用语言差异性测量团队认知差异性</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.discursive_diversity_score(wv, words)
</code></pre></div><ul>
<li><em><strong>wv</strong></em>   模型数据， 数据类型为 gensim.models.keyedvectors.KeyedVectors。</li>
<li><em><strong>words</strong></em>词语列表</li>
<li>返回一个数值</li>
</ul>
<p><img loading="lazy" src="img/23-low-and-high-examples-of-discursive-diversity.jpeg" alt=""  />
</p>
<p>高绩效团队是那些具有调节共享认知以适应不断变化的任务要求的集体能力的团队：在进行构思任务时，它们表现出更高的话语多样性，在执行协调任务时，表现出较低的话语多样性。</p>
<br>
<h3 id="45-procrustes_align">4.5 procrustes_align()</h3>
<p>该函数主要用于反映同一研究对象随着时间推进的社会文化变迁，或者同一时间范围内两个被研究主体间的差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.procrustes_align(base_embed, other_embed)
</code></pre></div><ul>
<li><em><strong>base_embed</strong></em>  基本模型； 模型数据类型为<em><strong>gensim.models.word2vec.Word2Vec</strong></em></li>
<li><em><strong>base_embed</strong></em>  其他模型； 模型数据类型为<em><strong>gensim.models.word2vec.Word2Vec</strong></em></li>
</ul>
<p>由于不同语料训练的Word2Vec模型无法直接比较， 需要先选定一个基准模型 <em><strong>base_embed</strong></em>， 之后根据 <em><strong>base_embed</strong></em> 对其他模型 <em><strong>other_embed</strong></em> 进行调整，调整后的模型就可以使用前面的语义距离函数或者语义投影函数。 这一过程用到的算法叫做 procrustes正交算法。</p>
<p>这里推荐一篇 <a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></p>
<br>
<h2 id="获取cntext2x">获取cntext2.x</h2>
<p>加大邓 <em><strong>WeChat: 372335839</strong></em>， 备注「姓名-学校-专业」， 100元领取  <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 文件。本文出现的cntext，默认均为2.x版本。</p>
<p><br><br></p>
<h2 id="使用声明">使用声明</h2>
<p>如果再研究或项目中使用到 <strong>cntext</strong> ，请声明出处。</p>
<h3 id="apalike">apalike</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Deng X., Nan P. (2022). cntext: a Python tool for text mining. DOI: 10.5281/zenodo.7063523 URL: https://github.com/hiDaDeng/cntext
</code></pre></div><h3 id="bibtex">bibtex</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@misc{YourReferenceHere,
author = {Deng, Xudong and Nan, Peng},
doi = {10.5281/zenodo.7063523},
month = {9},
title = {cntext: a Python tool for text mining},
url = {https://github.com/hiDaDeng/cntext},
year = {2022}
}
</code></pre></div><h3 id="endnote">endnote</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">%0 Generic
%A Deng, Xudong
%A Nan, Peng
%D 2022
%K text mining
%K text analysi
%K social science
%K management science
%K semantic analysis
%R 10.5281/zenodo.7063523
%T cntext: a Python tool for text mining
%U https://github.com/hiDaDeng/cntext
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>代码 | 使用 MD&amp;A文本测量「企业不确定性感知FEPU」</title>
      <link>https://textdata.cn/blog/2024-04-25-firm-economic-policy-uncertainty/</link>
      <pubDate>Thu, 25 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-25-firm-economic-policy-uncertainty/</guid>
      <description>&lt;p&gt;本文使用的缩写&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;EPU&lt;/strong&gt;&lt;/em&gt;   经济政策不确定性(Economic Policy Uncertainty)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;FEPU&lt;/strong&gt;&lt;/em&gt; 企业不确定性感知( Subjective perception of economic policy uncertainty)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一背景&#34;&gt;一、背景&lt;/h2&gt;
&lt;p&gt;「&lt;em&gt;&lt;strong&gt;经济政策不确定性&lt;/strong&gt;&lt;/em&gt;(EPU)」 通常是用来衡量经济中政策不确定性水平的一种度量方式。企业作为一个理性的经济主体， 需要根据未来的期望成本和收益进行决策 。政府的经济政策会在很大程度上影响企业的预期成本和收益 ， 如果经济政策频繁变化 ， 会给企业带来困扰 。现有文献经济政策不确定性测量思路大概有&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;股票市场隐含波动率VIX衡量宏观层面经济不确定性。&lt;/li&gt;
&lt;li&gt;利用外生变量，并结合企业对这些外生变量的依赖程度衡量企业面临的不确定性 。如政治事件、能源价格、汇率波动、贸易协定签订。&lt;/li&gt;
&lt;li&gt;利用新闻文本测量的经济不确定性。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但 经济政策不确定性指标(EPU)存在两个问题&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;EPU是宏观指标， 同期所有企业的EPU有且仅有一个观测值。&lt;/li&gt;
&lt;li&gt;EPU默认所有企业是同质， 对经济政策不确定性的感知是相同的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本推文参考聂辉华等(2020)内的算法,  实现利用 &lt;em&gt;&lt;strong&gt;经营讨论与分析(MD&amp;amp;A)文本数据&lt;/strong&gt;&lt;/em&gt;  测量企业「&lt;em&gt;&lt;strong&gt;企业不确定性感知FEPU&lt;/strong&gt;&lt;/em&gt;」(FEPU,  Subjective perception of economic policy uncertainty) 。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二epufepu&#34;&gt;二、EPU&amp;amp;FEPU&lt;/h2&gt;
&lt;h3 id=&#34;21-epu&#34;&gt;2.1 EPU&lt;/h3&gt;
&lt;p&gt;在复现「&lt;em&gt;&lt;strong&gt;企业不确定性感知FEPU&lt;/strong&gt;&lt;/em&gt;」前，我们先了解利用新闻数据测量 &lt;em&gt;&lt;strong&gt;EPU&lt;/strong&gt;&lt;/em&gt; 的算法，这样更容易理解 &lt;em&gt;&lt;strong&gt;FEPU&lt;/strong&gt;&lt;/em&gt; 的原理。参考Huang、Yun&amp;amp; Paul(2020)，大邓在前段时间分享了一个代码教程 &lt;a href=&#34;https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/&#34;&gt;代码 | 使用「新闻数据」计算 「经济政策不确定性」指数&lt;/a&gt; 。 &lt;br&gt;&lt;/p&gt;
&lt;p&gt;新闻数据计算EPU的算法&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Step-1. 选择了114家中国大陆的报纸，其中包括北京、上海、广州和天津等主要城市的报纸。
Step-2. 对于每家报纸，搜索包含以下三个关键词之一的文章：经济、不确定性和政策。这些关键词的中文和英文对照可以在论文的表格1中找到。
Step-3. 将每个月的文章数量按照满足第一个关键词的文章数量进行缩放。
Step-4. 将时间序列标准化，使其在2000年1月至2011年12月期间的标准差为1。 保证所有媒体计算得到的epu是可比的。
Step-5. 对十家报纸的月度序列进行简单平均，并将指标归一化，使其在2000年1月至2011年12月期间的平均值为100。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;文献中算法内容长， 结构化不足， 理解起来需要一些脑力。 大邓换种描述方式&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;EPU_t = m/n

- m  时期 t 同时含经济Economic、政策Policy、不确定Uncertainty三类词的新闻条数m
- n  时期 t 总的新闻条数n
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-fepu&#34;&gt;2.2 FEPU&lt;/h3&gt;
&lt;p&gt;理解了 EPU， 就能类比理解「&lt;em&gt;&lt;strong&gt;企业不确定性感知FEPU&lt;/strong&gt;&lt;/em&gt;」的算法。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;算法&lt;/th&gt;
&lt;th&gt;数据&lt;/th&gt;
&lt;th&gt;层次&lt;/th&gt;
&lt;th&gt;n&lt;/th&gt;
&lt;th&gt;m&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;EPU&lt;/td&gt;
&lt;td&gt;新闻媒体文本&lt;/td&gt;
&lt;td&gt;新闻&lt;/td&gt;
&lt;td&gt;时期t新闻总条数n&lt;/td&gt;
&lt;td&gt;时期t同时存在E、P、U三类词的新闻条数m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FEPU(word)&lt;/td&gt;
&lt;td&gt;管理层讨论与分析(md&amp;amp;a)&lt;/td&gt;
&lt;td&gt;词语&lt;/td&gt;
&lt;td&gt;将时期t的企业i的 md&amp;amp;a 文本词语个数n。&lt;/td&gt;
&lt;td&gt;1. 对md&amp;amp;a进行分句&lt;br/&gt;2. 同时含EP、U两类词的句子中， 统计这些句子中EP、U的词语出现次数之和m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FEPU(sentence)&lt;/td&gt;
&lt;td&gt;管理层讨论与分析(md&amp;amp;a)&lt;/td&gt;
&lt;td&gt;句子&lt;/td&gt;
&lt;td&gt;将时期t的企业i的 md&amp;amp;a 文本进行分句，得到句子个数n&lt;/td&gt;
&lt;td&gt;1. 对md&amp;amp;a进行分句&lt;br/&gt;2. 同时含EP、U两类词的句子中， 统计这类句子个数m&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三-准备cntext&#34;&gt;三、 准备cntext&lt;/h2&gt;
&lt;p&gt;EPU 和 FEPU 于今日刚刚封装到 cntext2.1.1 中， 再计算这两个指数， 就变得容易多了。&lt;/p&gt;
&lt;h3 id=&#34;31-安装cntext&#34;&gt;3.1 安装cntext&lt;/h3&gt;
&lt;p&gt;我使用的自己 &lt;strong&gt;未公开&lt;/strong&gt; 的cntext 2.1.1 版本， Bug频出，等调整好了再公开。&lt;/p&gt;
&lt;p&gt;将 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 放置于桌面，打开 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;之后在 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal) 中使用 &lt;em&gt;&lt;strong&gt;pip3&lt;/strong&gt;&lt;/em&gt; 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install pdfdocx
pip3 install distinctiveness
pip3 install pandarallel
pip3 install cntext-2.1.1-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;文末有 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt;  获取方式&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-内置词典&#34;&gt;3.2 内置词典&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;内置文件&lt;/th&gt;
&lt;th&gt;词典&lt;/th&gt;
&lt;th&gt;参考文献&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;zh_common_EPU.yaml&lt;/td&gt;
&lt;td&gt;经济E、政策P、不确定U&lt;/td&gt;
&lt;td&gt;Huang, Yun, and Paul Luk（2020）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;zh_common_FEPU.yaml&lt;/td&gt;
&lt;td&gt;经济政策EP、不确定性U&lt;/td&gt;
&lt;td&gt;聂辉华, 阮睿&amp;amp;沈吉（2020）&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h4 id=&#34;31-查看内置词典&#34;&gt;3.1 查看内置词典&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__version__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_dict_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2.1.1

[&amp;#39;zh_common_NTUSD.yaml&amp;#39;,
 &amp;#39;zh_common_DUTIR.yaml&amp;#39;,
 &amp;#39;enzh_common_StopWords.yaml&amp;#39;,
 &amp;#39;en_valence_Concreteness.yaml&amp;#39;,
 &amp;#39;en_common_LoughranMcDonald.yaml&amp;#39;,
 &amp;#39;zh_common_FinanceSenti.yaml&amp;#39;,
 &amp;#39;zh_common_TsinghuaPraiseDegrade.yaml&amp;#39;,
 &amp;#39;zh_common_FEPU.yaml&amp;#39;,    聂辉华, 阮睿&amp;amp;沈吉（2020）
 &amp;#39;en_common_ANEW.yaml&amp;#39;,
 &amp;#39;en_common_NRC.yaml&amp;#39;,
 &amp;#39;zh_valence_ChineseEmoBank.yaml&amp;#39;,
 &amp;#39;zh_valence_SixSemanticDimensionDatabase.yaml&amp;#39;,
 &amp;#39;zh_common_FinacialFormalUnformal.yaml&amp;#39;,
 &amp;#39;zh_common_LoughranMcDonald.yaml&amp;#39;,
 &amp;#39;enzh_common_AdvConj.yaml&amp;#39;,
 &amp;#39;en_common_SentiWS.yaml&amp;#39;,
 &amp;#39;zh_common_Digitalization.yaml&amp;#39;,
 &amp;#39;en_common_LSD2015.yaml&amp;#39;,
 &amp;#39;zh_common_HowNet.yaml&amp;#39;,
 &amp;#39;zh_common_EPU.yaml&amp;#39;]      #Huang, Yun, and Paul Luk（2020）
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h4 id=&#34;312-导入词典&#34;&gt;3.1.2 导入词典&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;FEPU_infos&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_yaml_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;zh_common_FEPU.yaml&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;FEPU_infos&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;Name&amp;#39;: &amp;#39;中文经济政策不确定性词典&amp;#39;, 
&amp;#39;Desc&amp;#39;: &amp;#39;中文经济政策不确定性词典, 含经济政策EconomicPolicy、不确定性Uncertainty两个词表&amp;#39;, 
&amp;#39;Refer&amp;#39;: &amp;#39;聂辉华, 阮睿, 沈吉. 企业不确定性感知、投资决策和金融资产配置[J]. 世界经济, 2020, 43 (06): 77-98.&amp;#39;, 
&amp;#39;Category&amp;#39;: [&amp;#39;经济政策&amp;#39;, &amp;#39;不确定&amp;#39;], 
&amp;#39;Dictionary&amp;#39;: 
    {&amp;#39;经济政策&amp;#39;: [&amp;#39;市政&amp;#39;, &amp;#39;政策&amp;#39;, &amp;#39;货币政策&amp;#39;, &amp;#39;政策鼓励&amp;#39;, &amp;#39;国家&amp;#39;, &amp;#39;扩内需&amp;#39;, &amp;#39;保增长&amp;#39;, &amp;#39;促发展&amp;#39;, &amp;#39;产业发展&amp;#39;, &amp;#39;法律&amp;#39;, &amp;#39;法规&amp;#39;, &amp;#39;行业政策&amp;#39;, &amp;#39;产业政策&amp;#39;, &amp;#39;宏观政策&amp;#39;, &amp;#39;国民经济&amp;#39;, &amp;#39;有关部门&amp;#39;, &amp;#39;产业结构调整&amp;#39;, &amp;#39;产业结构&amp;#39;, &amp;#39;当地政府&amp;#39;, &amp;#39;政府&amp;#39;, &amp;#39;经济政策&amp;#39;, &amp;#39;经济走势&amp;#39;, &amp;#39;所得税&amp;#39;, &amp;#39;税收减免&amp;#39;, &amp;#39;刺激政策&amp;#39;, &amp;#39;限贷令&amp;#39;, &amp;#39;限购令&amp;#39;, &amp;#39;保障房&amp;#39;, &amp;#39;宏观调控&amp;#39;, &amp;#39;产业发展&amp;#39;, &amp;#39;证监会&amp;#39;, &amp;#39;国家政策&amp;#39;, &amp;#39;政治&amp;#39;, &amp;#39;军事&amp;#39;, &amp;#39;政策环境&amp;#39;, &amp;#39;宏观&amp;#39;, &amp;#39;政府补助政策&amp;#39;, &amp;#39;调控政策&amp;#39;, &amp;#39;税收政策&amp;#39;, &amp;#39;政策扶持&amp;#39;], 
    &amp;#39;不确定&amp;#39;: [&amp;#39;风险&amp;#39;, &amp;#39;经营风险&amp;#39;, &amp;#39;市场风险&amp;#39;, &amp;#39;信用风险&amp;#39;, &amp;#39;不确定&amp;#39;, &amp;#39;波动&amp;#39;, &amp;#39;变化&amp;#39;, &amp;#39;改变&amp;#39;, &amp;#39;徘徊&amp;#39;, &amp;#39;不稳&amp;#39;, &amp;#39;不稳定&amp;#39;, &amp;#39;不寻常&amp;#39;, &amp;#39;错综复杂&amp;#39;, &amp;#39;非常复杂&amp;#39;, &amp;#39;纷繁复杂&amp;#39;, &amp;#39;纷纭复杂&amp;#39;, &amp;#39;十分复杂&amp;#39;, &amp;#39;变得复杂&amp;#39;, &amp;#39;风云突变&amp;#39;, &amp;#39;矛盾突出&amp;#39;, &amp;#39;突变&amp;#39;, &amp;#39;复杂多变&amp;#39;, &amp;#39;诡谲多变&amp;#39;, &amp;#39;阵痛&amp;#39;, &amp;#39;过渡&amp;#39;, &amp;#39;问责&amp;#39;, &amp;#39;整顿&amp;#39;, &amp;#39;危险&amp;#39;, &amp;#39;动荡&amp;#39;, &amp;#39;多变性&amp;#39;, &amp;#39;震荡&amp;#39;, &amp;#39;难以确定&amp;#39;, &amp;#39;难以预测&amp;#39;, &amp;#39;难以语料&amp;#39;, &amp;#39;难以琢磨&amp;#39;, &amp;#39;难以捉摸&amp;#39;, &amp;#39;接受考验&amp;#39;, &amp;#39;混乱&amp;#39;, &amp;#39;时而&amp;#39;, &amp;#39;随机&amp;#39;]}
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;33-内置函数&#34;&gt;3.3 内置函数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.epu(df,  freq=&amp;#39;Y&amp;#39;,e_pattern=&amp;#39;&amp;#39;, p_pattern=&amp;#39;&amp;#39;, u_pattern=&amp;#39;&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;df&lt;/strong&gt;&lt;/em&gt;  新闻DataFrame；  DataFrame必须含date和text两个字段；每行一条记录，含所有时期所有的新闻。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;freq&lt;/strong&gt;&lt;/em&gt; 字符串；决定EPU的时间粒度， 年Y、月M、天D， 默认freq=&amp;lsquo;Y&amp;rsquo;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;e_pattern&lt;/strong&gt;&lt;/em&gt;  字符串；经济类词典，用&lt;code&gt;|&lt;/code&gt;间隔词语，形如 &lt;strong&gt;e_pattern = &amp;lsquo;经济|金融&amp;rsquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;p_pattern&lt;/strong&gt;&lt;/em&gt;  字符串；政策词典，用&lt;code&gt;|&lt;/code&gt;间隔词语，形如 &lt;strong&gt;p_pattern = &amp;lsquo;政策|治理|行政&amp;rsquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;u_pattern&lt;/strong&gt;&lt;/em&gt; 字符串；不确定性词典，用&lt;code&gt;|&lt;/code&gt;间隔词语，形如 &lt;strong&gt;u_pattern = &amp;lsquo;风险|危机|难以预测&amp;rsquo;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;返回epu时间序列数据，格式为DataFrame&lt;/p&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.fepu(text,  ep_pattern=&amp;#39;&amp;#39;, u_pattern=&amp;#39;&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;text&lt;/strong&gt;&lt;/em&gt;  ；某时期t某企业i的管理层讨论与分析md&amp;amp;a文本&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;ep_pattern&lt;/strong&gt;&lt;/em&gt;  字符串；经济政策类词典，用&lt;code&gt;|&lt;/code&gt;间隔词语，形如 &lt;strong&gt;ep_pattern = &amp;lsquo;经济|金融|政策|治理|行政&amp;rsquo;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;u_pattern&lt;/strong&gt;&lt;/em&gt; 字符串；不确定性词典，用&lt;code&gt;|&lt;/code&gt;间隔词语，形如 &lt;strong&gt;u_pattern = &amp;lsquo;风险|危机|难以预测&amp;rsquo;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四测量fepu&#34;&gt;四、测量FEPU&lt;/h2&gt;
&lt;h3 id=&#34;41-读取数据&#34;&gt;4.1 读取数据&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;mda01-22.csv.gz&lt;/strong&gt;&lt;/em&gt;   管理层讨论与分析2001-2022文本数据&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;行业代码00-22.xlsx&lt;/strong&gt;&lt;/em&gt;  含股票名称、股票代码、行业等字段。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mda01-22.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;会计年度&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;股票代码&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;经营讨论与分析内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#上市公司行业信息&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ind_info_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_excel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;行业代码00-22.xlsx&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#合并数据&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;merge&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ind_info_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;股票代码&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;会计年度&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;how&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;inner&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#剔除ST和金融类企业&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;股票简称&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ST&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;行业代码&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;J&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;42-批量计算fepu&#34;&gt;4.2 批量计算FEPU&lt;/h3&gt;
&lt;p&gt;选中字段 「&lt;em&gt;&lt;strong&gt;经营讨论与分析内容&lt;/strong&gt;&lt;/em&gt;」， 对该字段 .apply 运行函数 &lt;em&gt;&lt;strong&gt;ct.fepu&lt;/strong&gt;&lt;/em&gt; ，得到企业感知经济不确定性风险FEPU(含词语和句子两个FEPU)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#常规速度代码&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#import cntext as ct&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#fepu_df = df[&amp;#39;经营讨论与分析内容&amp;#39;].apply(ct.fepu)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#res_df = pd.concat([df[[&amp;#39;会计年度&amp;#39;, &amp;#39;股票代码&amp;#39;]], fepu_df],   axis=1)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#res_df.to_csv(&amp;#39;result.csv&amp;#39;, index=False)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#res_df&lt;/span&gt;


&lt;span class=&#34;c1&#34;&gt;#加速版代码&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandarallel&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pandarallel&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pandarallel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;initialize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;fepu_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;经营讨论与分析内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parallel_apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fepu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;res_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;concat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;会计年度&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;股票代码&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fepu_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;res_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;result.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;res_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;CPU times: user 1.35 s, sys: 1.2 s, total: 2.54 s
Wall time: 4min 29s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;`&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;43-可视化&#34;&gt;4.3 可视化&lt;/h3&gt;
&lt;p&gt;根据 FEPUw 和 FEPUs 的年度均值， 绘制2001-2022期间的经济政策不确定性变化折线图&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;scienceplots&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;platform&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib_inline&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backend_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_matplotlib_formats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;png&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;svg&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;science&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;no-latex&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;cjk-sc-font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;platform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取操作系统类型&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Windows&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;SimHei&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Darwin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Arial Unicode MS&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;sans-serif&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;font&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 设置全局字体&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2001&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2023&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;FEPUw_s&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;FEPUs_s&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;res_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;会计年度&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;FEPUw_s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;FEPUw&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;FEPUs_s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;FEPUs&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
    
    
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FEPUw_s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FEPUs_s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FEPUw_s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;SEPUw&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FEPUs_s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;label&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;SEPUs&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;感知经济政策不确定性FEPU年度均值&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xlabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;年份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;FEPU均值&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;legend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五参考文献&#34;&gt;五、参考文献&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[1]聂辉华, 阮睿, 沈吉. 企业不确定性感知、投资决策和金融资产配置[J]. 世界经济, 2020, 43 (06): 77-98.
[2]Li, Jing, Huihua Nie, Rui Ruan, and Xinyi Shen. &amp;#34;Subjective perception of economic policy uncertainty and corporate social responsibility: Evidence from China.&amp;#34; International Review of Financial Analysis 91 (2024): 103022.
[3]Huang, Yun, and Paul Luk. &amp;#34;Measuring economic policy uncertainty in China.&amp;#34; China Economic Review 59 (2020): 10136
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;六获取资料&#34;&gt;六、获取资料&lt;/h2&gt;
&lt;p&gt;内容原创不易，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 100元 
   - mda01-22.csv.gz
   - A01-22.csv.gz 

- 100元 cntext-2.1.1-py3-none-any.whl

- 200元 
   - mda01-22.csv.gz
   - A01-22.csv.gz 
   - cntext-2.1.1-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;加微信 &lt;strong&gt;372335839&lt;/strong&gt;， 备注「姓名-学校-专业」。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>本文使用的缩写</p>
<ul>
<li><em><strong>EPU</strong></em>   经济政策不确定性(Economic Policy Uncertainty)</li>
<li><em><strong>FEPU</strong></em> 企业不确定性感知( Subjective perception of economic policy uncertainty)</li>
</ul>
<p><br><br></p>
<h2 id="一背景">一、背景</h2>
<p>「<em><strong>经济政策不确定性</strong></em>(EPU)」 通常是用来衡量经济中政策不确定性水平的一种度量方式。企业作为一个理性的经济主体， 需要根据未来的期望成本和收益进行决策 。政府的经济政策会在很大程度上影响企业的预期成本和收益 ， 如果经济政策频繁变化 ， 会给企业带来困扰 。现有文献经济政策不确定性测量思路大概有</p>
<ol>
<li>股票市场隐含波动率VIX衡量宏观层面经济不确定性。</li>
<li>利用外生变量，并结合企业对这些外生变量的依赖程度衡量企业面临的不确定性 。如政治事件、能源价格、汇率波动、贸易协定签订。</li>
<li>利用新闻文本测量的经济不确定性。</li>
</ol>
<p>但 经济政策不确定性指标(EPU)存在两个问题</p>
<ol>
<li>EPU是宏观指标， 同期所有企业的EPU有且仅有一个观测值。</li>
<li>EPU默认所有企业是同质， 对经济政策不确定性的感知是相同的。</li>
</ol>
<p>本推文参考聂辉华等(2020)内的算法,  实现利用 <em><strong>经营讨论与分析(MD&amp;A)文本数据</strong></em>  测量企业「<em><strong>企业不确定性感知FEPU</strong></em>」(FEPU,  Subjective perception of economic policy uncertainty) 。</p>
<p><br><br></p>
<h2 id="二epufepu">二、EPU&amp;FEPU</h2>
<h3 id="21-epu">2.1 EPU</h3>
<p>在复现「<em><strong>企业不确定性感知FEPU</strong></em>」前，我们先了解利用新闻数据测量 <em><strong>EPU</strong></em> 的算法，这样更容易理解 <em><strong>FEPU</strong></em> 的原理。参考Huang、Yun&amp; Paul(2020)，大邓在前段时间分享了一个代码教程 <a href="https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/">代码 | 使用「新闻数据」计算 「经济政策不确定性」指数</a> 。 <br></p>
<p>新闻数据计算EPU的算法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step-1. 选择了114家中国大陆的报纸，其中包括北京、上海、广州和天津等主要城市的报纸。
Step-2. 对于每家报纸，搜索包含以下三个关键词之一的文章：经济、不确定性和政策。这些关键词的中文和英文对照可以在论文的表格1中找到。
Step-3. 将每个月的文章数量按照满足第一个关键词的文章数量进行缩放。
Step-4. 将时间序列标准化，使其在2000年1月至2011年12月期间的标准差为1。 保证所有媒体计算得到的epu是可比的。
Step-5. 对十家报纸的月度序列进行简单平均，并将指标归一化，使其在2000年1月至2011年12月期间的平均值为100。
</code></pre></div><br>
<p>文献中算法内容长， 结构化不足， 理解起来需要一些脑力。 大邓换种描述方式</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">EPU_t = m/n

- m  时期 t 同时含经济Economic、政策Policy、不确定Uncertainty三类词的新闻条数m
- n  时期 t 总的新闻条数n
</code></pre></div><p><br><br></p>
<h3 id="22-fepu">2.2 FEPU</h3>
<p>理解了 EPU， 就能类比理解「<em><strong>企业不确定性感知FEPU</strong></em>」的算法。</p>
<table>
<thead>
<tr>
<th>算法</th>
<th>数据</th>
<th>层次</th>
<th>n</th>
<th>m</th>
</tr>
</thead>
<tbody>
<tr>
<td>EPU</td>
<td>新闻媒体文本</td>
<td>新闻</td>
<td>时期t新闻总条数n</td>
<td>时期t同时存在E、P、U三类词的新闻条数m</td>
</tr>
<tr>
<td>FEPU(word)</td>
<td>管理层讨论与分析(md&amp;a)</td>
<td>词语</td>
<td>将时期t的企业i的 md&amp;a 文本词语个数n。</td>
<td>1. 对md&amp;a进行分句<br/>2. 同时含EP、U两类词的句子中， 统计这些句子中EP、U的词语出现次数之和m</td>
</tr>
<tr>
<td>FEPU(sentence)</td>
<td>管理层讨论与分析(md&amp;a)</td>
<td>句子</td>
<td>将时期t的企业i的 md&amp;a 文本进行分句，得到句子个数n</td>
<td>1. 对md&amp;a进行分句<br/>2. 同时含EP、U两类词的句子中， 统计这类句子个数m</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="三-准备cntext">三、 准备cntext</h2>
<p>EPU 和 FEPU 于今日刚刚封装到 cntext2.1.1 中， 再计算这两个指数， 就变得容易多了。</p>
<h3 id="31-安装cntext">3.1 安装cntext</h3>
<p>我使用的自己 <strong>未公开</strong> 的cntext 2.1.1 版本， Bug频出，等调整好了再公开。</p>
<p>将 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><br>
<p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install pdfdocx
pip3 install distinctiveness
pip3 install pandarallel
pip3 install cntext-2.1.1-py3-none-any.whl
</code></pre></div><br>
<p>文末有 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em>  获取方式</p>
<p><br><br></p>
<h3 id="32-内置词典">3.2 内置词典</h3>
<table>
<thead>
<tr>
<th>内置文件</th>
<th>词典</th>
<th>参考文献</th>
</tr>
</thead>
<tbody>
<tr>
<td>zh_common_EPU.yaml</td>
<td>经济E、政策P、不确定U</td>
<td>Huang, Yun, and Paul Luk（2020）</td>
</tr>
<tr>
<td>zh_common_FEPU.yaml</td>
<td>经济政策EP、不确定性U</td>
<td>聂辉华, 阮睿&amp;沈吉（2020）</td>
</tr>
</tbody>
</table>
<br>
<h4 id="31-查看内置词典">3.1 查看内置词典</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">get_dict_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2.1.1

[&#39;zh_common_NTUSD.yaml&#39;,
 &#39;zh_common_DUTIR.yaml&#39;,
 &#39;enzh_common_StopWords.yaml&#39;,
 &#39;en_valence_Concreteness.yaml&#39;,
 &#39;en_common_LoughranMcDonald.yaml&#39;,
 &#39;zh_common_FinanceSenti.yaml&#39;,
 &#39;zh_common_TsinghuaPraiseDegrade.yaml&#39;,
 &#39;zh_common_FEPU.yaml&#39;,    聂辉华, 阮睿&amp;沈吉（2020）
 &#39;en_common_ANEW.yaml&#39;,
 &#39;en_common_NRC.yaml&#39;,
 &#39;zh_valence_ChineseEmoBank.yaml&#39;,
 &#39;zh_valence_SixSemanticDimensionDatabase.yaml&#39;,
 &#39;zh_common_FinacialFormalUnformal.yaml&#39;,
 &#39;zh_common_LoughranMcDonald.yaml&#39;,
 &#39;enzh_common_AdvConj.yaml&#39;,
 &#39;en_common_SentiWS.yaml&#39;,
 &#39;zh_common_Digitalization.yaml&#39;,
 &#39;en_common_LSD2015.yaml&#39;,
 &#39;zh_common_HowNet.yaml&#39;,
 &#39;zh_common_EPU.yaml&#39;]      #Huang, Yun, and Paul Luk（2020）
</code></pre></div><br>
<h4 id="312-导入词典">3.1.2 导入词典</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="n">FEPU_infos</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;zh_common_FEPU.yaml&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">FEPU_infos</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;Name&#39;: &#39;中文经济政策不确定性词典&#39;, 
&#39;Desc&#39;: &#39;中文经济政策不确定性词典, 含经济政策EconomicPolicy、不确定性Uncertainty两个词表&#39;, 
&#39;Refer&#39;: &#39;聂辉华, 阮睿, 沈吉. 企业不确定性感知、投资决策和金融资产配置[J]. 世界经济, 2020, 43 (06): 77-98.&#39;, 
&#39;Category&#39;: [&#39;经济政策&#39;, &#39;不确定&#39;], 
&#39;Dictionary&#39;: 
    {&#39;经济政策&#39;: [&#39;市政&#39;, &#39;政策&#39;, &#39;货币政策&#39;, &#39;政策鼓励&#39;, &#39;国家&#39;, &#39;扩内需&#39;, &#39;保增长&#39;, &#39;促发展&#39;, &#39;产业发展&#39;, &#39;法律&#39;, &#39;法规&#39;, &#39;行业政策&#39;, &#39;产业政策&#39;, &#39;宏观政策&#39;, &#39;国民经济&#39;, &#39;有关部门&#39;, &#39;产业结构调整&#39;, &#39;产业结构&#39;, &#39;当地政府&#39;, &#39;政府&#39;, &#39;经济政策&#39;, &#39;经济走势&#39;, &#39;所得税&#39;, &#39;税收减免&#39;, &#39;刺激政策&#39;, &#39;限贷令&#39;, &#39;限购令&#39;, &#39;保障房&#39;, &#39;宏观调控&#39;, &#39;产业发展&#39;, &#39;证监会&#39;, &#39;国家政策&#39;, &#39;政治&#39;, &#39;军事&#39;, &#39;政策环境&#39;, &#39;宏观&#39;, &#39;政府补助政策&#39;, &#39;调控政策&#39;, &#39;税收政策&#39;, &#39;政策扶持&#39;], 
    &#39;不确定&#39;: [&#39;风险&#39;, &#39;经营风险&#39;, &#39;市场风险&#39;, &#39;信用风险&#39;, &#39;不确定&#39;, &#39;波动&#39;, &#39;变化&#39;, &#39;改变&#39;, &#39;徘徊&#39;, &#39;不稳&#39;, &#39;不稳定&#39;, &#39;不寻常&#39;, &#39;错综复杂&#39;, &#39;非常复杂&#39;, &#39;纷繁复杂&#39;, &#39;纷纭复杂&#39;, &#39;十分复杂&#39;, &#39;变得复杂&#39;, &#39;风云突变&#39;, &#39;矛盾突出&#39;, &#39;突变&#39;, &#39;复杂多变&#39;, &#39;诡谲多变&#39;, &#39;阵痛&#39;, &#39;过渡&#39;, &#39;问责&#39;, &#39;整顿&#39;, &#39;危险&#39;, &#39;动荡&#39;, &#39;多变性&#39;, &#39;震荡&#39;, &#39;难以确定&#39;, &#39;难以预测&#39;, &#39;难以语料&#39;, &#39;难以琢磨&#39;, &#39;难以捉摸&#39;, &#39;接受考验&#39;, &#39;混乱&#39;, &#39;时而&#39;, &#39;随机&#39;]}
    }
</code></pre></div><p><br><br></p>
<h3 id="33-内置函数">3.3 内置函数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.epu(df,  freq=&#39;Y&#39;,e_pattern=&#39;&#39;, p_pattern=&#39;&#39;, u_pattern=&#39;&#39;)
</code></pre></div><ul>
<li><em><strong>df</strong></em>  新闻DataFrame；  DataFrame必须含date和text两个字段；每行一条记录，含所有时期所有的新闻。</li>
<li><em><strong>freq</strong></em> 字符串；决定EPU的时间粒度， 年Y、月M、天D， 默认freq=&lsquo;Y&rsquo;</li>
<li><em><strong>e_pattern</strong></em>  字符串；经济类词典，用<code>|</code>间隔词语，形如 <strong>e_pattern = &lsquo;经济|金融&rsquo;</strong></li>
<li><em><strong>p_pattern</strong></em>  字符串；政策词典，用<code>|</code>间隔词语，形如 <strong>p_pattern = &lsquo;政策|治理|行政&rsquo;</strong></li>
<li><em><strong>u_pattern</strong></em> 字符串；不确定性词典，用<code>|</code>间隔词语，形如 <strong>u_pattern = &lsquo;风险|危机|难以预测&rsquo;</strong></li>
</ul>
<p>返回epu时间序列数据，格式为DataFrame</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.fepu(text,  ep_pattern=&#39;&#39;, u_pattern=&#39;&#39;)
</code></pre></div><ul>
<li><em><strong>text</strong></em>  ；某时期t某企业i的管理层讨论与分析md&amp;a文本</li>
<li><em><strong>ep_pattern</strong></em>  字符串；经济政策类词典，用<code>|</code>间隔词语，形如 <strong>ep_pattern = &lsquo;经济|金融|政策|治理|行政&rsquo;</strong></li>
<li><em><strong>u_pattern</strong></em> 字符串；不确定性词典，用<code>|</code>间隔词语，形如 <strong>u_pattern = &lsquo;风险|危机|难以预测&rsquo;</strong></li>
</ul>
<p><br><br></p>
<h2 id="四测量fepu">四、测量FEPU</h2>
<h3 id="41-读取数据">4.1 读取数据</h3>
<ul>
<li><em><strong>mda01-22.csv.gz</strong></em>   管理层讨论与分析2001-2022文本数据</li>
<li><em><strong>行业代码00-22.xlsx</strong></em>  含股票名称、股票代码、行业等字段。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;mda01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span>

<span class="c1">#上市公司行业信息</span>
<span class="n">ind_info_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;行业代码00-22.xlsx&#39;</span><span class="p">)</span>

<span class="c1">#合并数据</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">ind_info_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;inner&#39;</span><span class="p">)</span>

<span class="c1">#剔除ST和金融类企业</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="o">-</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;股票简称&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;ST&#39;</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="o">-</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;J&#39;</span><span class="p">))]</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<h3 id="42-批量计算fepu">4.2 批量计算FEPU</h3>
<p>选中字段 「<em><strong>经营讨论与分析内容</strong></em>」， 对该字段 .apply 运行函数 <em><strong>ct.fepu</strong></em> ，得到企业感知经济不确定性风险FEPU(含词语和句子两个FEPU)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="c1">#常规速度代码</span>
<span class="c1">#import cntext as ct</span>
<span class="c1">#fepu_df = df[&#39;经营讨论与分析内容&#39;].apply(ct.fepu)</span>
<span class="c1">#res_df = pd.concat([df[[&#39;会计年度&#39;, &#39;股票代码&#39;]], fepu_df],   axis=1)</span>
<span class="c1">#res_df.to_csv(&#39;result.csv&#39;, index=False)</span>
<span class="c1">#res_df</span>


<span class="c1">#加速版代码</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">pandarallel</span> <span class="kn">import</span> <span class="n">pandarallel</span>
<span class="n">pandarallel</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
<span class="n">fepu_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">parallel_apply</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">fepu</span><span class="p">)</span>
<span class="n">res_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">]],</span> <span class="n">fepu_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">res_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;result.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">res_df</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">CPU times: user 1.35 s, sys: 1.2 s, total: 2.54 s
Wall time: 4min 29s
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p>`</p>
<p><br><br></p>
<h3 id="43-可视化">4.3 可视化</h3>
<p>根据 FEPUw 和 FEPUs 的年度均值， 绘制2001-2022期间的经济政策不确定性变化折线图</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>
<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>


<span class="n">years</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2001</span><span class="p">,</span> <span class="mi">2023</span><span class="p">)</span>
<span class="n">FEPUw_s</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">FEPUs_s</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">res_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;会计年度&#39;</span><span class="p">):</span>
    <span class="n">FEPUw_s</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;FEPUw&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">FEPUs_s</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;FEPUs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">FEPUw_s</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">FEPUs_s</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">FEPUw_s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SEPUw&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">FEPUs_s</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;SEPUs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;感知经济政策不确定性FEPU年度均值&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;年份&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;FEPU均值&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/plot.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="五参考文献">五、参考文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]聂辉华, 阮睿, 沈吉. 企业不确定性感知、投资决策和金融资产配置[J]. 世界经济, 2020, 43 (06): 77-98.
[2]Li, Jing, Huihua Nie, Rui Ruan, and Xinyi Shen. &#34;Subjective perception of economic policy uncertainty and corporate social responsibility: Evidence from China.&#34; International Review of Financial Analysis 91 (2024): 103022.
[3]Huang, Yun, and Paul Luk. &#34;Measuring economic policy uncertainty in China.&#34; China Economic Review 59 (2020): 10136
</code></pre></div><p><br><br></p>
<h2 id="六获取资料">六、获取资料</h2>
<p>内容原创不易，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 100元 
   - mda01-22.csv.gz
   - A01-22.csv.gz 

- 100元 cntext-2.1.1-py3-none-any.whl

- 200元 
   - mda01-22.csv.gz
   - A01-22.csv.gz 
   - cntext-2.1.1-py3-none-any.whl
</code></pre></div><p>加微信 <strong>372335839</strong>， 备注「姓名-学校-专业」。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理世界 | 使用md&amp;a数据中计算 「企业融资约束指标」</title>
      <link>https://textdata.cn/blog/2024-12-31-using-regex-to-compute-the-financial_constraints/</link>
      <pubDate>Wed, 24 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-12-31-using-regex-to-compute-the-financial_constraints/</guid>
      <description>：本文采用文本分析方法构建了融资约束指标，在此基础上，实证检验了多个大股 东对企业融资约束的影响以及相应的作用机理。 我们发现，多个大股东的公司有着较低的 融资约束水平。 该结论在控制内生性情况下依然成立。 中介效应模型的检验结果表明，其 他大股东通过抑制控股股东的掏空行为降低了企业融资约束。 进一步的研究结果表明，在 其他大股东具有较强的监督动机和监督能力（大股东数量更多、持股数量之和更大、大股东 之间不容易合谋）、及更好的外部环境（信息环境、法律环境）时，公司的融资约束水平更低， 这些发现在逻辑上为其他大股东的监督假说提供证据支持的同时，也表明大股东发挥监督 作用降低企业融资约束需要一定条件。 本文为完善中国情景下的融资约束指标构建、更好 度量中国企业融资约束提供了有益参考；同时，为股权结构安排的经济后果提供了新的证据 支持。</description>
      <content:encoded><![CDATA[<h2 id="技术路线">技术路线</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[工作量]
  1. 代码130+行
  2. 调试时间 3 小时， 运行时间 20 小时
  
  
[内容]
  1. 设计正则表达式， 识别企业融资约束
  2. 构建企业管理层讨论与分析文本向量(标准化) Vec_it
  3. 构建板块(沪、深)文本向量(标准化)BoardVec_bt
  4. 构建行业文本向量(标准化) IndustryVec_it
  5. 构建融资约束样本集的文本均值向量(标准化) ConstrainedVec_it
  6. 基于前面几个变量，计算得到
     - BoardScore_bt 、 InstryScore_it
     - 得到5w多个csv文件(中间运算结果), 存储在 fin_constrain_output/{year}/{code}.csv
     
  7. [融资约束FC指标计量建模]
    - ConstrainedScore_it =β0 + β1 * BoardScore_bt + β2 * IndustryScore_it + E_it
    - BoardScore_bt  交易所引发的融资约束相似度
    - IndustryScore_it  行业特征引发的融资约束相似度
    - E_it  残差就是本文要计算的[融资约束指标FC]
</code></pre></div><p><br><br></p>
<h2 id="一识别融资约束样本">一、识别融资约束样本</h2>
<p><strong>在获取 MD&amp;A 的基础上，采用正则表达式（Regular Expression） 检索出隐含融资约束信息的文本，并把相应的 MD&amp;A 进行标记，纳入对应年度的融资约束文本集中</strong>。 其中，在检索并标记融资约束文本的过程中，本文参考 Hoberg 和 Maksimovic （2015）、Buehlmaier 和 Whited（2016）的研究方法。</p>
<p>Hoberg 和 Maksimovic（2015）认为，融资约束体现为<strong>投资计划、项目的推迟、搁置乃至放弃</strong>，因此，他们构造了两组“<strong>推迟投资</strong>”词语列表，一组是有推迟、延期、搁置含义的动词词表; 另一组是与投资、 项目、计划等意思相近的名词词表。 若在待识别文本中，动词词表和名词词表中的词语、词组同时出现，且相隔不超过 12 词，则将其判定为有推迟投资含义的融资约束文本。</p>
<p>Buehlmaier 和 Whited（2016） 在构建股权融资约束文本集的过程中，直接引用了前者的“推迟投资”词表，同时，为了确定投资的推迟确实是由股权融资方面的问题引起的，还计算了距“推迟投资”语句 12 词以内股权融资相关词语出现的频率，最终只把频率排行前 250 的观测加入股权融资约束文本集。</p>
<br>
<h3 id="11-前人不足">1.1 前人不足</h3>
<p>需要说明的是，尽管本文采用的方法借鉴了 Hoberg 和 Maksimovic（2015）和 Buehlmaier 和 Whited （2016）的做法，但与其存在着两个方面的差异。</p>
<ul>
<li>第一，<strong>本文没有通过“推迟投资”界定融资约束，而是通过公司对资金状况的描述去识别，相较而言这一做法更为直接</strong>。 例如，若公司明确表明融资能力有限，资金紧张，则被视为融资约束样本。</li>
<li>第二，<strong>我们认为，即便“推迟投资”词表中的动词和名词在相隔 12 词以内出现，两个词之间也未必有关联，12词的窗口长度容易引起大量误判</strong>。 尤其考虑到汉语使用较为灵活，不同公司在表述上也存在着较大的差异，因此，本文使用了可覆盖更多表述形式、更加灵活的正则表达式进行检索，并根据数次检索结果排除了很多容易导致误判的情形，查准率较高。</li>
</ul>
<br>
<h3 id="12-本文完善">1.2 本文完善</h3>
<p>具体地，为了在 MD&amp;A 文本集中检索出融资约束文本，我们在设计正则表达式时将能显示公司有融资约束的各种文字表达，以词语组合的形式进行提炼。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">regex1</span> <span class="o">=</span> <span class="s2">&#34;[^。]*?(融资|资金|筹资)[^。]{0, 6}?(难以|不能|无法|不足以)[^。]*&#34;</span>
<span class="c1">#能在 MD&amp;A 文本中匹配出以下形式的句子：（除句 号以外的任意长度字符串）+融资/资金/筹资+（六个 字符长度以内的任意字符串）+难以/不能/无法满足/不足以+（除句号以外的任意长度字符串）；</span>

<span class="n">regex2</span> <span class="o">=</span> <span class="s2">&#34;[^。]*?(融资|资金|筹资)[^。]{0, 6}?(成本|压力|难度)[^。]{0, 4}?(升|增|高|大)[^。]*&#34;</span>
<span class="c1">#可在句号以外的任意长度字符串）+融资/资金/筹资+（六 个字符长度以内的任意字符串）+成本/压力/难度+ （4 个字符长度以内的任意字符串）+升/高/增/大+ （除句号以外的任意长度字符串）。</span>
</code></pre></div><p>仅仅考虑融资约束文本的各种可能表述是不够的，会出现大量误判，例如，机械地将“资金”之后 4 个字符以内出现“不足”的语句识别为融资约束语句，非常容易 造成误判，因为部分 MD&amp;A 提及公司“资金管理水平不足”，而资金管理水平反映的是公司运营能力， 和融资约束无直接关系。 诸如此类的匹配应视作误判而排除，因此我们利用正则表达式灵活的语法规则，同时构造了排除性条件。 <strong>在此基础上，将这些对应着不同判断逻辑的“规则字符串”合并至同一个正则表达式中</strong>。 如果难以合并，则利用程序语言的条件判断逻辑，对正则表达式组进行组合使用。 <strong>在具体操作中，本文就使用了正则表达式组。</strong></p>
<p><br><br></p>
<h2 id="二-构建中文融资约束样本识别代码">二、 构建中文融资约束样本识别代码</h2>
<p><strong>前面的样本识别都是论文原文，接下来是大邓对该论文的融资约束样本识别算法的复现</strong>。</p>
<h3 id="21-融资约束文本的场景">2.1 融资约束文本的场景</h3>
<p>这是一个相对复杂的需求，需要综合考虑多种情况， 对于每种情况，都构建一个单独的正则表达式，用于匹配对应的文本。可以使用“或”运算符， 合并为一个更大的正则表达式。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>


<span class="c1">#融资不足情况</span>
<span class="n">regex1</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(?:融资|资金|筹资)[^。]{0,6}?(?:难以|不能|无法|不足以)[^。]*&#34;</span>
<span class="c1">#融资成本或压力过大情况</span>
<span class="n">regex2</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(?:融资|资金|筹资)[^。]{0,6}?(?:成本|压力|难度)[^。]{0,4}?(?:升|增|高|大)[^。]*&#34;</span>

<span class="c1">#可以使用“或”运算符， 合并为一个更大的正则表达式</span>
<span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(&#34;</span> <span class="o">+</span> <span class="n">regex1</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&#34;)|(&#34;</span> <span class="o">+</span> <span class="n">regex2</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&#34;)&#34;</span>


<span class="c1">#实验数据</span>
<span class="n">text1</span> <span class="o">=</span> <span class="s2">&#34;公司在过去几年中进行了大量的投资，导致资金短缺，难以支持公司未来的发展计划。&#34;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s2">&#34;公司在过去几年中进行了大量的投资计划，资金状况良好，没有融资压力。&#34;</span>

<span class="c1">#实验结果</span>
<span class="n">matches1</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">matches1</span><span class="p">)</span>
<span class="n">matches2</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">matches2</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [(&#39;资金短缺，难以支持公司未来的发展计划&#39;, &#39;&#39;)]
    []
</code></pre></div><br>
<p>在上面的例子中，pattern能识别出文本是否含有融资约束。</p>
<ul>
<li>text1<strong>有融资约束</strong>，所以返回带 <strong>有内容</strong> 的 <strong>matches1</strong></li>
<li>text2<strong>没有融资约束</strong>，所以返回 <strong>没有内容</strong> 的 <strong>matches2</strong></li>
</ul>
<br>
<h3 id="22-识别中文融资约束样本的最终代码">2.2 识别中文融资约束样本的最终代码</h3>
<p>前面的内容都是算法逐步实现的过程，现在咱们合并为一个函数代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">is_financial_constraint</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#正则表达式组</span>
    <span class="n">regex1</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(?:融资|资金|筹资)[^。]{0,6}?(?:难以|不能|无法|不足以)[^。]*&#34;</span>
    <span class="n">regex2</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(?:融资|资金|筹资)[^。]{0,6}?(?:成本|压力|难度)[^。]{0,4}?(?:升|增|高|大)[^。]*&#34;</span>
    <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(&#34;</span> <span class="o">+</span> <span class="n">regex1</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&#34;)|(&#34;</span> <span class="o">+</span> <span class="n">regex2</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&#34;)&#34;</span>
    
    <span class="c1">#带内容的结果为融资约束，为True；反之，为False</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span><span class="o">&gt;=</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    
    
<span class="c1">#实验数据</span>
<span class="n">text1</span> <span class="o">=</span> <span class="s2">&#34;公司在过去几年中进行了大量的投资，导致资金短缺，难以支持公司未来的发展计划。&#34;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s2">&#34;公司在过去几年中进行了大量的投资计划，资金状况良好，没有融资压力。&#34;</span>

<span class="c1">#实验结果</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;text1文本是否为融资约束: &#39;</span><span class="p">,</span> <span class="n">is_financial_constraint</span><span class="p">(</span><span class="n">text1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;text2文本是否为融资约束: &#39;</span><span class="p">,</span> <span class="n">is_financial_constraint</span><span class="p">(</span><span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    text1文本是否为融资约束:  True
    text2文本是否为融资约束:  False
</code></pre></div><p><br><br></p>
<h2 id="三批量识别融资约束样本">三、批量识别融资约束样本</h2>
<p>接下来对对 <em><strong>data/mda01-22.csv.gz</strong></em> 数据集所有md&amp;a进行识别。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/mda01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span>

<span class="c1">#上市公司行业信息</span>
<span class="n">ind_info_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;data/行业代码00-22.xlsx&#39;</span><span class="p">)</span>

<span class="c1">#合并数据</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">ind_info_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;inner&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">55767
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<p>新建板块字段， 上海证券交易所股票大多以 6、9开头， 而深圳证券交易所以0、3开头</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">plate</span><span class="p">(</span><span class="n">code</span><span class="p">):</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">code</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;A6&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">code</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;A9&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;上海&#39;</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">code</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;A0&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">code</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;A3&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;深圳&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;其他&#39;</span>
    
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;板块&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">plate</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>  
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;融资约束&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">is_financial_constraint</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#融资约束样本占比</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;融资约束&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><pre><code>0.1062814926390159
</code></pre>
<br>
<h3 id="注意">注意</h3>
<p>设计的 <em><strong>函数is_financial_constraint</strong></em> 应该要检查， 检查的目的是改良正则表达式组， 这里假装我们检查完了，没什么问题。</p>
<p><br><br></p>
<h2 id="四构建融资约束指标">四、构建融资约束指标</h2>
<p>前面的融资约束样本识别，只是识别出融资约束是否存在，信息的颗粒度比较粗糙。<strong>这篇论文使用文本相似度算法，构建了每家企业的融资约束指标</strong>。</p>
<p>本文同样参照 Hoberg 和 Maksimovic（2015）的研究方法，我们认为，融资约束程度相近的公司，其在“管理层讨论与分析”中的用词和表述也会趋于一致。 因此，通过采用余弦相似度的方法，能够在识别出全体样本的融资约束程度，并以连续变量的形式进行呈现。</p>
<p>具体实现算法步骤</p>
<ol>
<li>
<p>给每个 md&amp;a 文本转化为向量 <em><strong>Vec_it</strong></em></p>
</li>
<li>
<p>当年所有属于融资约束样本的 <em><strong>Vec_it</strong></em> ， 求均值得到 <em><strong>ConstrainedVec_t</strong></em></p>
</li>
<li>
<p>每家企业当年融资约束水平(程度) 由 <em><strong>Vec_it</strong></em> 与 <em><strong>ConstrainedVec_t</strong></em> 之积 , 即 <em><strong>ConstrainedScore_it</strong></em> 所体现。</p>
</li>
<li>
<p>考虑到市场板块、行业性因素对融资约束的影响，不能直接使用 <em><strong>ConstrainedScore_it</strong></em>。</p>
<ul>
<li>对历年隶属于各个板块的公司 MD&amp;A，求标准化词频向量的均值并做标准化处理，记为 BoardVectb_bt ，该向量反映了上市板 b 在 t 年的共同性信息披露内容。</li>
<li><em><strong>Vec_it</strong></em> 与对应板块 <em><strong>BoardVec_bt</strong></em> 之积，即为因 MD&amp;A 共性内容导致的相似度， 记作 <em><strong>BoilerplateScore_i</strong></em>。</li>
<li>利用相同方法，计算出因行业特征引发的相似度，记作 <em><strong>IndustryScore_it</strong></em> 。</li>
</ul>
</li>
<li>
<p><code>ConstrainedScore_it = β0 + β1 * BoardScore_bt + β2 * IndustryScore_it + E_it</code></p>
<ul>
<li><em><strong>BoardScore_bt</strong></em>  交易所引发的融资约束相似度</li>
<li><em><strong>IndustryScore_it</strong></em>  行业特征引发的融资约束相似度</li>
<li><em><strong>E_it</strong></em>  残差就是本文要计算的[融资约束指标FC]</li>
</ul>
</li>
</ol>
<p><br><br></p>
<h3 id="41-计算2020年的vec_it">4.1 计算2020年的Vec_it</h3>
<p>计算量太大，先以2020为例写代码。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df_per_year</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">2020</span><span class="p">]</span>
<span class="n">df_per_year</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df_per_year</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df4.png" alt=""  />
</p>
<br>
<p>处理2020年的 「<em><strong>经营讨论与分析内容</strong></em>」字段内容，使其:</p>
<ol>
<li>只保留中文内容</li>
<li>剔除停用词</li>
<li>整理为用空格间隔的字符串(类西方语言文本格式)</li>
<li>将本文转为向量后，标准化。</li>
<li>合并一些需要的字段，如***[&lsquo;股票代码&rsquo;, &lsquo;会计年度&rsquo;, &lsquo;板块&rsquo;, &lsquo;行业代码&rsquo;, &lsquo;融资约束&rsquo;]***</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">re</span>


<span class="c1">#cntext1.x</span>
<span class="c1">#stopwords = ct.load_pkl_dict(&#39;STOPWORDS.pkl&#39;)[&#39;STOPWORDS&#39;][&#39;chinese&#39;]</span>

<span class="c1">#cntext2.x</span>
<span class="n">stopwords</span><span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;enzh_common_StopWords.yaml&#39;</span><span class="p">)[</span><span class="s1">&#39;Dictionary&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#只保留md&amp;a中的中文内容</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]+&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span>
    <span class="c1">#剔除停用词</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">jieba</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="c1">#整理为用空格间隔的字符串(类西方语言文本格式)</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> 
<span class="c1"># 生成稀疏bow矩阵</span>
<span class="c1">#dtm 文档-词频-矩阵</span>
<span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span> 
<span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dtm_per_year</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">dtm_per_year</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="c1">#向量标准化normalize</span>
<span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">row</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#合并多个字段为新的df</span>
<span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_per_year</span><span class="p">[[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;板块&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;融资约束&#39;</span><span class="p">]],</span> <span class="n">dtm_per_year</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dtm_per_year</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    CPU times: user 5.88 s, sys: 901 ms, total: 6.78 s
    Wall time: 49.7 s
</code></pre></div><p><img loading="lazy" src="img/df5.png" alt=""  />
</p>
<br>
<h3 id="42--2020年的板块评分行业评分">4.2  2020年的板块评分、行业评分</h3>
<p>计算2020年所有公司的 <strong>板块评分BoardScore</strong>、<strong>行业评分IndustrySocre</strong>。该部分代码运行较慢，运行下来大约2小时。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="n">year</span> <span class="o">=</span> <span class="mi">2020</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output&#39;</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output&#39;</span><span class="p">)</span>
        

<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dtm_per_year</span><span class="p">)):</span>
    <span class="n">code</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">]</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">]</span>
    <span class="n">year</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">]</span>
    <span class="n">board</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;板块&#39;</span><span class="p">]</span>
    
    
    <span class="n">Vec</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">5</span><span class="p">:]</span>
    <span class="n">Ind_Vec</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="p">[</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">ind</span><span class="p">][</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">!=</span><span class="n">code</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Ind_Score</span> <span class="o">=</span> <span class="n">Vec</span> <span class="o">*</span> <span class="p">(</span><span class="n">Ind_Vec</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Ind_Vec</span><span class="p">))</span>
    <span class="n">FinConstrain_Vec</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="p">[</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;融资约束&#39;</span><span class="p">]</span><span class="o">==</span><span class="kc">True</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">FinConstrain_Score</span> <span class="o">=</span> <span class="n">Vec</span> <span class="o">*</span> <span class="p">(</span><span class="n">FinConstrain_Vec</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">FinConstrain_Vec</span><span class="p">))</span>
    <span class="n">Board_Vec</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="p">[</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;板块&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">board</span><span class="p">][</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">!=</span><span class="n">code</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Board_Score</span> <span class="o">=</span> <span class="n">Vec</span> <span class="o">*</span> <span class="p">(</span><span class="n">Board_Vec</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Board_Vec</span><span class="p">))</span>
    

    <span class="n">dtm_per_year_melted</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;板块&#39;</span><span class="p">,</span> <span class="s1">&#39;融资约束&#39;</span><span class="p">],</span>
                                            <span class="n">var_name</span><span class="o">=</span><span class="s1">&#39;word_id&#39;</span><span class="p">,</span> 
                                            <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;word_freq&#39;</span><span class="p">)</span>
    

    <span class="n">corporate_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;word_id&#39;</span><span class="p">:</span> <span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">code</span><span class="p">][</span><span class="s1">&#39;word_id&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                 <span class="s1">&#39;word_freq&#39;</span><span class="p">:</span> <span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">code</span><span class="p">][</span><span class="s1">&#39;word_freq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                 <span class="s1">&#39;ind_freq&#39;</span><span class="p">:</span> <span class="n">Ind_Score</span><span class="p">,</span>
                                 <span class="s1">&#39;board_freq&#39;</span><span class="p">:</span> <span class="n">Board_Score</span><span class="p">,</span>
                                 <span class="s1">&#39;fin_constrain_freq&#39;</span><span class="p">:</span> <span class="n">FinConstrain_Score</span><span class="p">})</span>
    <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">code</span>
    <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ind</span>
    <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;板块&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">board</span>
    <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">year</span>
    
    <span class="n">corporate_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">corporate_df</span> <span class="o">=</span> <span class="n">corporate_df</span><span class="p">[[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;板块&#39;</span><span class="p">,</span> <span class="s1">&#39;word_id&#39;</span><span class="p">,</span> <span class="s1">&#39;word_freq&#39;</span><span class="p">,</span> <span class="s1">&#39;ind_freq&#39;</span><span class="p">,</span> <span class="s1">&#39;board_freq&#39;</span><span class="p">,</span> <span class="s1">&#39;fin_constrain_freq&#39;</span><span class="p">]]</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output/</span><span class="si">{year}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">)):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output/</span><span class="si">{year}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">))</span>
    
    <span class="n">corporate_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output/</span><span class="si">{year}</span><span class="s1">/</span><span class="si">{code}</span><span class="s1">.csv&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">,</span> <span class="n">code</span><span class="o">=</span><span class="n">code</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
  
</code></pre></div><br>
<h3 id="43-计算所有年份板块评分行业评分">4.3 计算所有年份板块评分、行业评分</h3>
<p><strong>这部分代码，全部运行下来，耗时 20 小时。</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">jieba</span>



<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output&#39;</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output&#39;</span><span class="p">)</span>
    
    
    
<span class="c1">#cntext1.x</span>
<span class="c1">#stopwords = ct.load_pkl_dict(&#39;STOPWORDS.pkl&#39;)[&#39;STOPWORDS&#39;][&#39;chinese&#39;]</span>
<span class="c1">#cntext2.x</span>
<span class="n">stopwords</span><span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;enzh_common_StopWords.yaml&#39;</span><span class="p">)[</span><span class="s1">&#39;Dictionary&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>



<span class="k">def</span> <span class="nf">is_financial_constraint</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#正则表达式组</span>
    <span class="n">regex1</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(?:融资|资金|筹资)[^。]{0,6}?(?:难以|不能|无法|不足以)[^。]*&#34;</span>
    <span class="n">regex2</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(?:融资|资金|筹资)[^。]{0,6}?(?:成本|压力|难度)[^。]{0,4}?(?:升|增|高|大)[^。]*&#34;</span>
    <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(&#34;</span> <span class="o">+</span> <span class="n">regex1</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&#34;)|(&#34;</span> <span class="o">+</span> <span class="n">regex2</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&#34;)&#34;</span>
    
    <span class="c1">#带内容的结果为融资约束，为True；反之，为False</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span><span class="o">&gt;=</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    


<span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#只保留md&amp;a中的中文内容</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]+&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span>
    <span class="c1">#剔除停用词</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">jieba</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="c1">#整理为用空格间隔的字符串(类西方语言文本格式)</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

    
<span class="k">def</span> <span class="nf">plate</span><span class="p">(</span><span class="n">code</span><span class="p">):</span>
    <span class="c1">#判断股票是在上海证券交易所还是深圳证券交易所</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">code</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;A6&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">code</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;A9&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;上海&#39;</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">code</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;A0&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">code</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;A3&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;深圳&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;其他&#39;</span>

    
    


    
<span class="c1">#读取数据</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/mda01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span>

<span class="c1">#上市公司行业信息</span>
<span class="n">ind_info_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;data/行业代码00-22.xlsx&#39;</span><span class="p">)</span>

<span class="c1">#合并数据</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">ind_info_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;inner&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;板块&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">plate</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;板块&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="s1">&#39;上海&#39;</span><span class="p">,</span> <span class="s1">&#39;深圳&#39;</span><span class="p">])]</span>


    
<span class="c1">#识别融资约束</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;融资约束&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">is_financial_constraint</span><span class="p">)</span>




<span class="k">for</span> <span class="n">year</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>
    <span class="n">df_per_year</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">year</span><span class="p">]</span>
    <span class="n">df_per_year</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> 
    <span class="c1"># 生成稀疏bow矩阵</span>
    <span class="c1">#dtm 文档-词频-矩阵</span>
    <span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span> 
    <span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dtm_per_year</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">dtm_per_year</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
    <span class="c1">#向量标准化normalize</span>
    <span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">row</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1">#合并多个字段为新的df</span>
    <span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_per_year</span><span class="p">[[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;板块&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;融资约束&#39;</span><span class="p">]],</span> <span class="n">dtm_per_year</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dtm_per_year</span><span class="p">)),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">进度&#39;</span><span class="p">):</span>
        <span class="n">code</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">]</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">]</span>
        <span class="n">year</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">]</span>
        <span class="n">board</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;板块&#39;</span><span class="p">]</span>


        <span class="n">Vec</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">5</span><span class="p">:]</span>
        <span class="n">Ind_Vec</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="p">[</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">ind</span><span class="p">][</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">!=</span><span class="n">code</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">Ind_Score</span> <span class="o">=</span> <span class="n">Vec</span> <span class="o">*</span> <span class="p">(</span><span class="n">Ind_Vec</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Ind_Vec</span><span class="p">))</span>
        <span class="n">FinConstrain_Vec</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="p">[</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;融资约束&#39;</span><span class="p">]</span><span class="o">==</span><span class="kc">True</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">FinConstrain_Score</span> <span class="o">=</span> <span class="n">Vec</span> <span class="o">*</span> <span class="p">(</span><span class="n">FinConstrain_Vec</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">FinConstrain_Vec</span><span class="p">))</span>
        <span class="n">Board_Vec</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="p">[</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;板块&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">board</span><span class="p">][</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">!=</span><span class="n">code</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">Board_Score</span> <span class="o">=</span> <span class="n">Vec</span> <span class="o">*</span> <span class="p">(</span><span class="n">Board_Vec</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Board_Vec</span><span class="p">))</span>


        <span class="n">dtm_per_year_melted</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;板块&#39;</span><span class="p">,</span> <span class="s1">&#39;融资约束&#39;</span><span class="p">],</span>
                                                <span class="n">var_name</span><span class="o">=</span><span class="s1">&#39;word_id&#39;</span><span class="p">,</span> 
                                                <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;word_freq&#39;</span><span class="p">)</span>


        <span class="n">corporate_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;word_id&#39;</span><span class="p">:</span> <span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">code</span><span class="p">][</span><span class="s1">&#39;word_id&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                     <span class="s1">&#39;word_freq&#39;</span><span class="p">:</span> <span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">code</span><span class="p">][</span><span class="s1">&#39;word_freq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                     <span class="s1">&#39;ind_freq&#39;</span><span class="p">:</span> <span class="n">Ind_Score</span><span class="p">,</span>
                                     <span class="s1">&#39;board_freq&#39;</span><span class="p">:</span> <span class="n">Board_Score</span><span class="p">,</span>
                                     <span class="s1">&#39;fin_constrain_freq&#39;</span><span class="p">:</span> <span class="n">FinConstrain_Score</span><span class="p">})</span>
        <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">code</span>
        <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ind</span>
        <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;板块&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">board</span>
        <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">year</span>

        <span class="n">corporate_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">corporate_df</span> <span class="o">=</span> <span class="n">corporate_df</span><span class="p">[[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;板块&#39;</span><span class="p">,</span> <span class="s1">&#39;word_id&#39;</span><span class="p">,</span> <span class="s1">&#39;word_freq&#39;</span><span class="p">,</span> <span class="s1">&#39;ind_freq&#39;</span><span class="p">,</span> <span class="s1">&#39;board_freq&#39;</span><span class="p">,</span> <span class="s1">&#39;fin_constrain_freq&#39;</span><span class="p">]]</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output/</span><span class="si">{year}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">)):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output/</span><span class="si">{year}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">))</span>
        
        <span class="n">corporate_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output/</span><span class="si">{year}</span><span class="s1">/</span><span class="si">{code}</span><span class="s1">.csv&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">,</span> <span class="n">code</span><span class="o">=</span><span class="n">code</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
             
</code></pre></div><p><br><br></p>
<h3 id="44-融资约束2020">4.4 融资约束2020</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    - ConstrainedScore_it =β0 + β1 * BoardScore_bt + β2 * IndustryScore_it + E_it
    - BoardScore_bt  交易所引发的融资约束相似度
    - IndustryScore_it  行业特征引发的融资约束相似度
    - E_it  残差就是本文要计算的[融资约束指标FC]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">csv_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output/2020/A000002.csv&#39;</span><span class="p">)</span>
<span class="n">csv_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df6.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#更改字段名。</span>
<span class="n">csv_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;板块&#39;</span><span class="p">,</span> <span class="s1">&#39;word_id&#39;</span><span class="p">,</span> <span class="s1">&#39;Vec&#39;</span><span class="p">,</span> <span class="s1">&#39;IndustryScore&#39;</span><span class="p">,</span> <span class="s1">&#39;BoardScore&#39;</span><span class="p">,</span> <span class="s1">&#39;ConstrainedScore&#39;</span><span class="p">]</span>
<span class="n">csv_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df7.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="c1">#因变量ConstrainedScore</span>
<span class="c1">#解释变量IndustryScore、 BoardScore</span>
<span class="n">formula</span> <span class="o">=</span> <span class="s1">&#39;ConstrainedScore ~ IndustryScore + BoardScore&#39;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">csv_df</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">     OLS Regression Results                            
    ==============================================================================
    Dep. Variable:       ConstrainedScore   R-squared:                       0.988
    Model:                            OLS   Adj. R-squared:                  0.988
    Method:                 Least Squares   F-statistic:                 1.416e+05
    Date:                Wed, 24 Apr 2024   Prob (F-statistic):               0.00
    Time:                        11:52:11   Log-Likelihood:                 46460.
    No. Observations:                3426   AIC:                        -9.291e+04
    Df Residuals:                    3423   BIC:                        -9.290e+04
    Df Model:                           2                                         
    Covariance Type:            nonrobust                                         
    =================================================================================
                        coef    std err          t      P&gt;|t|      [0.025      0.975]
    ---------------------------------------------------------------------------------
    Intercept      1.048e-08   5.37e-09      1.952      0.051   -4.91e-11     2.1e-08
    IndustryScore     0.0791      0.000    250.868      0.000       0.079       0.080
    BoardScore        0.8076      0.004    196.675      0.000       0.800       0.816
    ==============================================================================
    Omnibus:                     2749.003   Durbin-Watson:                   1.974
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):         21379060.688
    Skew:                           2.083   Prob(JB):                         0.00
    Kurtosis:                     389.973   Cond. No.                     7.71e+05
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 7.71e+05. This might indicate that there are
    strong multicollinearity or other numerical problems.
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#融资约束FC</span>
<span class="n">FC</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">resid</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;2020年 A000002融资约束指标 FC: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">FC</span><span class="p">))</span>
</code></pre></div><pre><code>2020年 A000002融资约束指标FC: 0.00015749392796709594
</code></pre>
<br>
<h3 id="45-融资约束2001-2022">4.5 融资约束2001-2022</h3>
<p>根据步骤4.4我们成功计算出了2020的融资约束FC指标，现在推广到2001-2022， 并将计算结果存储到 <em><strong>fin_constrain2001-2022.csv</strong></em>， csv 含 <em><strong>code</strong></em>、<em><strong>year</strong></em>、<em><strong>FC</strong></em> 三个字段。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>

<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;fin_constrain2001-2022.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvf</span><span class="p">:</span>
    <span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;FC&#39;</span><span class="p">]</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;fin_constrain_output/*/*.csv&#39;</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">df_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
            <span class="n">df_</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;板块&#39;</span><span class="p">,</span> <span class="s1">&#39;word_id&#39;</span><span class="p">,</span> <span class="s1">&#39;Vec&#39;</span><span class="p">,</span> <span class="s1">&#39;IndustryScore&#39;</span><span class="p">,</span> <span class="s1">&#39;BoardScore&#39;</span><span class="p">,</span> <span class="s1">&#39;ConstrainedScore&#39;</span><span class="p">]</span>
            <span class="n">formula</span> <span class="o">=</span> <span class="s1">&#39;ConstrainedScore ~ IndustryScore + BoardScore&#39;</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df_</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
            <span class="n">FC</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">resid</span><span class="p">))</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="n">df_</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
                <span class="s1">&#39;year&#39;</span><span class="p">:</span> <span class="n">df_</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span>
                <span class="s1">&#39;FC&#39;</span><span class="p">:</span> <span class="n">FC</span>
            <span class="p">}</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
</code></pre></div><br>
<p>最后查看(欣赏)这个融资约束数据 <em><strong>fin_constrain2001-2022.csv</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">fc_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;fin_constrain2001-2022.csv&#39;</span><span class="p">)</span>
<span class="n">fc_df</span>
</code></pre></div><p><img loading="lazy" src="img/df8.png" alt=""  />
</p>
<br>
<br>
<h2 id="五获取资料">五、获取资料</h2>
<h3 id="51-打包">5.1 打包</h3>
<p>内容创作不易， <strong>200</strong> 元，加微信 <strong>372335839</strong>， 备注「姓名-学校-专业」。</p>
<p>资料截图， 整个资料文件夹体积高达 11 G。</p>
<p><img loading="lazy" src="img/screen.png" alt=""  />
</p>
<p><img loading="lazy" src="img/size.png" alt=""  />
</p>
<br>
<h3 id="52-单买">5.2 单买</h3>
<p>只要 <em><strong>fin_constrain2001-2022.csv</strong></em> 这一个文件， 100元， 加微信 <strong>372335839</strong>， 备注「姓名-学校-专业」。</p>
<br>
<h2 id="相关内容">相关内容</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/">数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/">数据集 | A股上市公司基本信息2000-2022</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-13-information-content-of-critical-audit/">金融研究 | 使用Python构建「关键审计事项信息含量」</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-01-06-mda_informative_content/">中国工业经济 | MD&amp;A信息含量指标构建代码实现</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>中国工业经济 | 使用Python测量MD&amp;A信息含量指标</title>
      <link>https://textdata.cn/blog/2023-01-06-mda_informative_content/</link>
      <pubDate>Sun, 21 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-01-06-mda_informative_content/</guid>
      <description>每个上市公司 MD&amp;amp;A 信息不可避免地在某种程度上与同行业其他上市公司以及市场其他行业上市公司存在一定的相似性， 甚至某些公司可能直接参考其他公司 MD&amp;amp;A 的表述。 可以将与行业其他公司或其他行业的公司重复或相似的信息定义为不具有信息含量的内容，同时将不同的信息定义为真正具有信息含量的内容，简称为信息含量。In this paper ， we discuss the impact of informative content of Management Discussion and Analysis （ MD&amp;amp;A ） on stock price crash risk using the method of text vectorization. Using the MD&amp;amp;A in annual reports of China A -share listed firms from 2007 to 2015 ， we find that the informative content of MD&amp;amp;A can reduce future stock price crash risk ， and the informative content of preview section has significant effects on stock price crash risk ， while that of review section does not. After controlling endogeneity ， the conclusions still stand. Further ， we study the influence of informative content of preview section on crash risk from the aspects of readability and information opaqueness. The results show that the higher readability and higher information opaqueness ， the greater impact of informative content has on stock price crash risk. Finally ， after changing the calculation of crash risk ， and controlling the impact of stock price synchronicity ， the informative content of preview section still reduces stock price crash risk. This paper enriches the influencing factors of stock price crash risk and improves the study of the usefulness of MD&amp;amp;A from the perspective of incremental information ， which has important theoretical and practical significance.</description>
      <content:encoded><![CDATA[<p>由于任何一个行为主体都会受到 <strong>周围环境</strong> 和 <strong>自身经历</strong>(认知) 影响，所发表的信息必然包含通 <strong>环境信息</strong> 和 <strong>特异性信息</strong> 。如何通过文本，表征文本的通用信息和特意性信息，如何测量行为主体发表内容的信息含量，带着这些疑问， 一起读这篇17年的论文的方法论部分，并用Python将其实现。</p>
<p><br><br></p>
<h2 id="一信息含量">一、信息含量</h2>
<p>由于每个公司的 MD&amp;A 中不仅包括公司经营状况等历史信息， 也包括与其他公司相似的信息， 如外部环境、市场格局、风险因素等内容。 因此， 本文参考 Hanley and Hoberg （ 2010 ）， 从行业和市场两个维度来考察和定义公司 MD&amp;A 中的信息含量。</p>
<ul>
<li><strong>市场因素</strong>， 所有上市公司都处于相同的宏观经济环境、风险因素和政治、政策背景之下；</li>
<li><strong>行业因素</strong>， 同一行业中的各上市公司又面临着相似的产业政策、竞争环境和市场特征。</li>
</ul>
<p>由此可见， 每个上市公司 MD&amp;A 信息不可避免地在某种程度上与同行业其他上市公司以及市场其他行业上市公司存在一定的相似性， 甚至某些公司可能直接参考其他公司 MD&amp;A 的表述。 <strong>可以将与行业其他公司或其他行业的公司重复或相似的信息定义为不具有信息含量的内容，同时将不同的信息定义为真正具有信息含量的内容，简称为信息含量</strong>。</p>
<br>
<blockquote>
<p>孟庆斌, 杨俊华, and 鲁冰. &ldquo;管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究.&rdquo; 中国工业经济 12 (2017): 132-150.</p>
</blockquote>
<br>
<h3 id="11-摘要">1.1 摘要</h3>
<p>本文采用文本向量化的方法， 对 2007—2015 年中国 A 股上市公司年报的管理层讨论与分析（MD&amp;A）所披露的信息含量加以度量， 研究其对股价崩盘风险的影响。 研究发现， MD&amp;A 的信息含量越高，未来股价崩盘风险越低。 将 MD&amp;A 进一步划分为回顾部分和展望部分后发现，仅有展望部分中的信息含量能够显著降低未来股价崩盘风险。 在控制内生性问题之后，本文的结论依然成立。 本文还分别从文本可读性和信息不对称的角度出发，研究它们对二者关系的影响。 结果表明，信息的可读性越高，信息不对称程度越高，展望部分的信息含量对股价崩盘风险的降低作用越大。 在重新定义股价崩盘风险的计算区间以及控制股价同步性之后， MD&amp;A 展望部分的信息含量依然能够显著降低股价崩盘风险， 表明本文的结论是稳健的。 本文从文本信息的角度丰富了股价崩盘风险影响因素的研究， 同时也从增量信息的角度完善了 MD&amp;A 信息有用性的研究，具有重要的理论和现实意义。</p>
<br>
<h3 id="12-样本选择和处理">1.2 样本选择和处理</h3>
<p>本文选取 2007 — 2015 年中国上市公司年报中的 MD&amp;A 信息作为研究样本。 之所以选取 2007 年作为样本的起点， 是因为从 2007 年开始， MD&amp;A 在企业定期报告中的披露要求已经较为完善， 而且 2007 年是中国会计准则国际趋同的重要时点， 新制定的《企业会计准则》已经开始实施， 为避免前后会计准则差异而产生的影响， 因此选取 2007 年作为样本区间的起点。</p>
<p>本文所使用的上市公司年度报告均来自于巨潮资讯网。 数据处理过程如下：</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">（ 1 ）剔除金融行业、 ST 和 *ST 类企业， 以及上市时间不足一年的企业。

（ 2 ）从 MD&amp;A 的内容中分别提取回顾和展望部分， 保存为回顾信息文件和展望信息文件， 部分无法抓取出的年报通过手工收集处理。

（ 3 ）文本处理-文本向量化。 借鉴 Hanley and Hoberg （ 2010 ）的研究思路， 将每个 MD&amp;A 文本通过向量的 形式表示出来， 其每个元素为文本中的每个词语出现的频率。 例如， 假设某 MD&amp;A 文本中包含 10000 个词， 则该文本对应一个 10000×1 维的向量。 举一个简单的例子来描述文本向量化的过程： 在两个简化的 MD&amp;A 文本中， 一个包含“我们生产土豆和生产玉米”， 另一个包含“我们生产家具”， 剔除连词“和”、代词“我们”之后， 只剩下“生产”、“土豆”、“玉米”、“家具”这 4 个词。 那么， 在第一个 MD&amp;A 文本中， “生产”、“土豆”和“玉米”分别出现了 2 次、 1 次和 1 次， 而“家具”出现 0 次， 所以该 文本的向量为 {2 ， 1 ， 1 ， 0} ， 同样得到第二个文本的向量为 {1 ， 0 ， 0 ， 1} 。

（ 4 ）向量标准化。 对于向量化的文本， 仍需解决文本长度不同导致的结果不可比问题。 一般来说， 某一个词在长文本中重复出现的次数较多， 在短文本中重复出现的次数较少， 但并不能因此说 长文本比短文本的信息量大。 为此， 本文进一步将这些向量进行标准化处理， 即将该向量除以文本 中单词的总数， 得到标准化后的向量。 在上面的例子中， 两个公司的标准化之后的向量就成为了 {0.50 ， 0.25 ， 0.25 ， 0} 和 {0.50 ， 0 ， 0 ， 0.50} 。
</code></pre></div><br>
<h3 id="13-文件目录">1.3 文件目录</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">管理层讨论信息含量/
├── 代码.ipynb
├── data/
│   ├── 行业代码00-22.xlsx
│   └── mda01-22.csv.gz
├── mda_infor2001-2022.csv
├── mda_infor_output/
│   └── 2022/
│       ├── A000002.csv
│       ├── A000004.csv
│       ├── A000005.csv
│       ├── A000006.csv
│       ├── ...
│   └── 2021/
│       ├── A000002.csv
│       ├── A000004.csv
│       ├── A000005.csv
│       ├── A000006.csv
│       ├── ...
│   └── 2019/
│       ├── A000002.csv
│       ├── A000004.csv
│       ├── A000005.csv
│       ├── A000006.csv
│       ├── ...
│   └── ...
</code></pre></div><p><br><br></p>
<h2 id="二导入数据">二、导入数据</h2>
<p>这里准备了2001-2022年A股经营讨论与分析内容和行业代码数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/mda01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span>

<span class="c1">#上市公司行业信息</span>
<span class="n">ind_info_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;data/行业代码00-22.xlsx&#39;</span><span class="p">)</span>


<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">ind_info_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;inner&#39;</span><span class="p">)</span>


<span class="c1"># 剔除金融行业处理</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="o">~</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s2">&#34;J&#34;</span><span class="p">)]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="o">~</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;股票简称&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s2">&#34;ST&#34;</span><span class="p">)]</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三以2020年为例">三、以2020年为例</h2>
<p>写代码先局部后整体，以2020年为例，如果2020年可以成功计算出信息含量，则可以for循环推广到所有股票所有年份。本章节需要做</p>
<ol>
<li>选定某年份，以2020年为例</li>
<li>定义transform函数，用于处理「经营讨论与分析内容」字段内的内容。</li>
<li>文本向量化，向量标准化。</li>
</ol>
<br>
<h3 id="31-选定2020年">3.1 选定2020年</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df_per_year</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">2020</span><span class="p">]</span>
<span class="n">df_per_year</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df_per_year</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<br>
<h3 id="32-定义transform函数">3.2 定义transform函数</h3>
<p>定义 <em><strong>transform</strong></em> 函数，该函数可以处理「<em><strong>经营讨论与分析内容</strong></em>」字段内容，使其:</p>
<ol>
<li>只保留中文内容</li>
<li>剔除停用词</li>
<li>整理为用空格间隔的字符串(类西方语言文本格式)</li>
</ol>
<p>之后应用 <em><strong>transform</strong></em>函数， 使用 <strong>apply</strong> 方法， 处理  <em><strong>df_per_year[&lsquo;经营讨论与分析内容&rsquo;]</strong></em> 。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
    
<span class="c1">#cntext1.x</span>
<span class="c1">#stopwords = ct.load_pkl_dict(&#39;STOPWORDS.pkl&#39;)[&#39;STOPWORDS&#39;][&#39;chinese&#39;]</span>

<span class="c1">#cntext2.x</span>
<span class="n">stopwords</span><span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;enzh_common_StopWords.yaml&#39;</span><span class="p">)[</span><span class="s1">&#39;Dictionary&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>

    


<span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#只保留md&amp;a中的中文内容</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]+&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span>
    <span class="c1">#剔除停用词</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">jieba</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="c1">#整理为用空格间隔的字符串(类西方语言文本格式)</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/sc/3mnt5tgs419_hk7s16gq61p80000gn/T/jieba.cache
    Loading model cost 0.556 seconds.
    Prefix dict has been built successfully.
</code></pre></div><br>
<h3 id="33-文本向量化">3.3 文本向量化</h3>
<p>本小节要做:</p>
<ol>
<li>文本向量化</li>
<li>向量标准化</li>
<li>合并多个字段为新的df</li>
</ol>
<p>先将df_per_year[&lsquo;clean_text&rsquo;] 向量化，代码如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> 
<span class="c1"># 生成稀疏bow矩阵</span>
<span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span> 
<span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dtm_per_year</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">df_per_year</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">dtm_per_year</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">CPU times: user 4.09 s, sys: 109 ms, total: 4.2 s
Wall time: 4.2 s
</code></pre></div><p><img loading="lazy" src="img/df4.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">#向量标准化</span>
<span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">row</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dtm_per_year</span>
</code></pre></div><p><img loading="lazy" src="img/df5.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#合并多个字段为新的df</span>
<span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_per_year</span><span class="p">[[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">]],</span> <span class="n">dtm_per_year</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dtm_per_year</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df6.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四计算2020年行业向量市场向量">四、计算2020年行业向量、市场向量</h2>
<p>计算2020年所有公司的市场向量、行业向量。这里</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1">#将中间计算结果存储在 mda_infor_output 文件夹。</span>
<span class="c1">#没有该文件夹，就新建</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;mda_infor_output&#39;</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s1">&#39;mda_infor_output&#39;</span><span class="p">)</span>
    
    
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dtm_per_year</span><span class="p">)),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&#34;会计年度2020进度&#34;</span><span class="p">):</span>
    <span class="n">code</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">]</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">]</span>
    <span class="n">year</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">]</span>
    
    <span class="n">ind_freq</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="p">[</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">ind</span><span class="p">][</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">code</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">market_freq</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="p">[</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">!=</span><span class="n">ind</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="n">dtm_per_year_melted</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">],</span>
                                            <span class="n">var_name</span><span class="o">=</span><span class="s1">&#39;word_id&#39;</span><span class="p">,</span> 
                                            <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;word_freq&#39;</span><span class="p">)</span>
    <span class="n">corporate_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;word_id&#39;</span><span class="p">:</span> <span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">code</span><span class="p">][</span><span class="s1">&#39;word_id&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                 <span class="s1">&#39;word_freq&#39;</span><span class="p">:</span> <span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">code</span><span class="p">][</span><span class="s1">&#39;word_freq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                 <span class="s1">&#39;ind_freq&#39;</span><span class="p">:</span> <span class="n">ind_freq</span><span class="p">,</span>
                                 <span class="s1">&#39;market_freq&#39;</span><span class="p">:</span><span class="n">market_freq</span><span class="p">})</span>
    <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">code</span>
    <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ind</span>
    <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">year</span>
    <span class="n">corporate_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">corporate_df</span> <span class="o">=</span> <span class="n">corporate_df</span><span class="p">[[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;word_id&#39;</span><span class="p">,</span> <span class="s1">&#39;word_freq&#39;</span><span class="p">,</span> <span class="s1">&#39;ind_freq&#39;</span><span class="p">,</span> <span class="s1">&#39;market_freq&#39;</span><span class="p">]]</span>

    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;mda_infor_output/</span><span class="si">{year}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">)):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s1">&#39;mda_infor_output/</span><span class="si">{year}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">))</span>
    <span class="n">corporate_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;mda_infor_output/</span><span class="si">{year}</span><span class="s1">/</span><span class="si">{code}</span><span class="s1">.csv&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">,</span> <span class="n">code</span><span class="o">=</span><span class="n">code</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">会计年度2020进度: 100%|███████████████| 3923/3923 [1:35:32&lt;00:00,  1.46s/it]
</code></pre></div><p>从运行的进度条可知2020 年符合规则的记录有3923 条， 运行时间 1 小时 35 分钟。</p>
<p><br><br></p>
<h2 id="五计算2001-2022年所有公司行业向量市场向量">五、计算2001-2022年所有公司行业向量、市场向量</h2>
<p>信息含量的定义。 由于每个公司的 MD&amp;A 中不仅包括公司经营状况等历史信息， 也包括与其他公司相似的信息， 如外部环境、市场格局、风险因素等内容。 因此， 本文参考 Hanley and Hoberg （ 2010 ）， 从行业和市场两个维度来考察和定义公司 MD&amp;A 中的信息含量。</p>
<ul>
<li><strong>市场因素</strong>， 所有上市公司都处于相同的宏观经济环境、风险因素和政治、政策背景之下；</li>
<li><strong>行业因素</strong>， 同一行业中的各上市公司又面临着相似的产业政策、竞争环境和市场特征。</li>
</ul>
<p>由此可见， 每个上市公司 MD&amp;A 信息不可避免地在某种程度上与同行业其他上市公司以及市场其他行业上市公司存在一定的相似性， 甚至某些公司可能直接参考其他公司 MD&amp;A 的表述。</p>
<p><img loading="lazy" src="img/norm_ind_market.png" alt=""  />
</p>
<p>参考文中截图行业向量、市场向量计算方法，有如下代码。<strong>该部分代码运行较慢，全部运行下来大约10小时。</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>


<span class="c1">#检查是否有文件夹mda_infor_output，如果没有就新建一个</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;mda_infor_output&#39;</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s1">&#39;mda_infor_output&#39;</span><span class="p">)</span>
    
    
<span class="c1">#cntext1.x</span>
<span class="c1">#stopwords = ct.load_pkl_dict(&#39;STOPWORDS.pkl&#39;)[&#39;STOPWORDS&#39;][&#39;chinese&#39;]</span>

<span class="c1">#cntext2.x</span>
<span class="n">stopwords</span><span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;enzh_common_StopWords.yaml&#39;</span><span class="p">)[</span><span class="s1">&#39;Dictionary&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#只保留md&amp;a中的中文内容</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]+&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span>
    <span class="c1">#剔除停用词</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">jieba</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="c1">#整理为用空格间隔的字符串(类西方语言文本格式)</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>




<span class="c1">#读取数据</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/mda01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span>

<span class="c1">#上市公司行业信息</span>
<span class="n">ind_info_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;data/行业代码00-22.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">ind_info_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;inner&#39;</span><span class="p">)</span>

<span class="c1"># 剔除金融行业处理</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="o">~</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s2">&#34;J&#34;</span><span class="p">)]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="o">~</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;股票简称&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s2">&#34;ST&#34;</span><span class="p">)]</span>

 
<span class="k">for</span> <span class="n">year</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>
    <span class="n">df_per_year</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">year</span><span class="p">]</span>
    <span class="n">df_per_year</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
    

    <span class="n">cv</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> 
    <span class="c1"># 生成稀疏bow矩阵</span>
    <span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_per_year</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span> 
    <span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dtm_per_year</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">df_per_year</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
    <span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">row</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">row</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">dtm_per_year</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_per_year</span><span class="p">[[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">]],</span> <span class="n">dtm_per_year</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dtm_per_year</span><span class="p">)),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;会计年度</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s2">进度&#34;</span><span class="p">):</span>
        <span class="n">code</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">]</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">]</span>
        <span class="n">year</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">]</span>



        <span class="n">ind_freq</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="p">[</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">ind</span><span class="p">][</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">!=</span><span class="n">code</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">market_freq</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="p">[</span><span class="n">dtm_per_year</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">!=</span><span class="n">ind</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="n">dtm_per_year_melted</span> <span class="o">=</span> <span class="n">dtm_per_year</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">],</span>
                                                <span class="n">var_name</span><span class="o">=</span><span class="s1">&#39;word_id&#39;</span><span class="p">,</span> 
                                                <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;word_freq&#39;</span><span class="p">)</span>
        <span class="n">corporate_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span> <span class="s1">&#39;word_id&#39;</span><span class="p">:</span> <span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">code</span><span class="p">][</span><span class="s1">&#39;word_id&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                       <span class="s1">&#39;word_freq&#39;</span><span class="p">:</span> <span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="n">dtm_per_year_melted</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">code</span><span class="p">][</span><span class="s1">&#39;word_freq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                       <span class="s1">&#39;ind_freq&#39;</span><span class="p">:</span> <span class="n">ind_freq</span><span class="p">,</span>
                                       <span class="s1">&#39;market_freq&#39;</span><span class="p">:</span><span class="n">market_freq</span><span class="p">})</span>
        <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">code</span>
        <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ind</span>
        <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">year</span>
        <span class="n">corporate_df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">corporate_df</span> <span class="o">=</span> <span class="n">corporate_df</span><span class="p">[[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;word_id&#39;</span><span class="p">,</span> <span class="s1">&#39;word_freq&#39;</span><span class="p">,</span> <span class="s1">&#39;ind_freq&#39;</span><span class="p">,</span> <span class="s1">&#39;market_freq&#39;</span><span class="p">]]</span>
        
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;mda_infor_output/</span><span class="si">{year}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">)):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s1">&#39;mda_infor_output/</span><span class="si">{year}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">))</span>
        <span class="n">corporate_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;mda_infor_output/</span><span class="si">{year}</span><span class="s1">/</span><span class="si">{code}</span><span class="s1">.csv&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year</span><span class="p">,</span> <span class="n">code</span><span class="o">=</span><span class="n">code</span><span class="p">),</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/jieba.cache
Loading model cost 0.281 seconds.
Prefix dict has been built successfully.
会计年度2001进度: 100%|█████████████████████| 1038/1038 [04:35&lt;00:00,  3.77it/s]
会计年度2002进度: 100%|█████████████████████| 1073/1073 [04:53&lt;00:00,  3.65it/s]
会计年度2003进度: 100%|█████████████████████| 1102/1102 [05:41&lt;00:00,  3.22it/s]
......
会计年度2020进度: 100%|███████████████| 3923/3923 [1:35:32&lt;00:00,  1.46s/it]
会计年度2021进度: 100%|███████████████████| 4412/4412 [2:51:33&lt;00:00,  2.33s/it]
会计年度2022进度: 100%|███████████████████| 4880/4880 [3:23:30&lt;00:00,  2.50s/it]
</code></pre></div><p>大邓使用的电脑是 96G 内存， 运行时间大概 12 小时。 常见电脑的内存是 16 G， 速度可能会慢一点， 预估 12 ~ 20 小时左右。</p>
<p><br><br></p>
<h2 id="六标准信息信息含量">六、标准信息、信息含量</h2>
<p>以2020年000002为例，计算其标准信息、信息含量。计算成功后，再计算所有年份所有上市公司 md&amp;a的标准信息、信息含量。</p>
<p><strong>原文除了计算md&amp;a，还将md&amp;a区分为回顾过去、展望未来两部分，并分别计算了对应的标准信息、信息含量。这里只计算md&amp;a的标准信息、信息含量。</strong></p>
<p><img loading="lazy" src="img/infor_pre.png" alt=""  />
</p>
<p>这里使用Python的统计模型statsmodels库OLS来计算标准信息和信息含量。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">csv_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;mda_infor_output/2020/A000002.csv&#39;</span><span class="p">)</span>
<span class="n">csv_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df7.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#更改字段名</span>
<span class="n">csv_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;word_id&#39;</span><span class="p">,</span> <span class="s1">&#39;Norm&#39;</span><span class="p">,</span> <span class="s1">&#39;Norm_Ind&#39;</span><span class="p">,</span> <span class="s1">&#39;Norm_Market&#39;</span><span class="p">]</span>
<span class="n">csv_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df8.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="c1">#因变量Norm</span>
<span class="c1">#解释变量Norm_Ind、 Norm_Market</span>
<span class="n">formula</span> <span class="o">=</span> <span class="s1">&#39;Norm ~ Norm_Ind + Norm_Market&#39;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">csv_df</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   Norm   R-squared:                       1.000
Model:                            OLS   Adj. R-squared:                  1.000
Method:                 Least Squares   F-statistic:                 5.941e+26
Date:                Mon, 22 Apr 2024   Prob (F-statistic):               0.00
Time:                        10:11:08   Log-Likelihood:             1.0859e+05
No. Observations:                3391   AIC:                        -2.172e+05
Df Residuals:                    3388   BIC:                        -2.171e+05
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
===============================================================================
                  coef    std err          t      P&gt;|t|      [0.025      0.975]
-------------------------------------------------------------------------------
Intercept    2.985e-15    6.9e-17     43.244      0.000    2.85e-15    3.12e-15
Norm_Ind        1.0000   2.92e-14   3.42e+13      0.000       1.000       1.000
Norm_Market  1.554e-15   1.57e-13      0.010      0.992   -3.05e-13    3.09e-13
==============================================================================
Omnibus:                     6850.292   Durbin-Watson:                   0.001
Prob(Omnibus):                  0.000   Jarque-Bera (JB):         22187649.732
Skew:                         -16.342   Prob(JB):                         0.00
Kurtosis:                     397.925   Cond. No.                     3.04e+03
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 3.04e+03. This might indicate that there are
strong multicollinearity or other numerical problems.
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#标准信息</span>
<span class="n">standard_info</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">Norm_Ind</span> <span class="o">+</span> <span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">Norm_Market</span>


<span class="c1">#信息含量</span>
<span class="n">informative_content</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">resid</span><span class="p">))</span>


<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;A000002标准信息: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">standard_info</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;A000002信息含量: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">informative_content</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">A000002标准信息: 1.0000000000000404
A000002信息含量: 1.016101346616714e-11
</code></pre></div><br>
<p>既然能成功计算某年某公司的标准信息、信息含量，现在推广到所有年份所有公司，计算结果存储为一个csv文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>


<span class="c1">#结果存储到mda_infor.csv</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;mda_infor2001-2022.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvf</span><span class="p">:</span>
    <span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;标准信息&#39;</span><span class="p">,</span> <span class="s1">&#39;信息含量&#39;</span><span class="p">]</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>
    
    <span class="n">year_dirs</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;mda_infor_output&#39;</span><span class="p">)</span>
    <span class="n">year_dirs</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">year_dirs</span> <span class="k">if</span> <span class="s1">&#39;DS&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">y</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">year_dir</span> <span class="ow">in</span> <span class="n">year_dirs</span><span class="p">:</span>
        <span class="n">code_csvfs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mda_infor_output/</span><span class="si">{year}</span><span class="s1">/</span><span class="si">{csvf}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">year_dir</span><span class="p">,</span> <span class="n">csvf</span><span class="o">=</span><span class="n">f</span><span class="p">)</span> 
                      <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;mda_infor_output/</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year_dir</span><span class="p">))]</span>
        <span class="n">code_csvfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">code_csvfs</span> <span class="k">if</span> <span class="s1">&#39;DS&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">csvf</span> <span class="ow">in</span> <span class="n">code_csvfs</span><span class="p">:</span> 
            <span class="k">try</span><span class="p">:</span>
                <span class="n">csv_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csvf</span><span class="p">)</span>
                <span class="n">csv_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;word_id&#39;</span><span class="p">,</span> <span class="s1">&#39;Norm&#39;</span><span class="p">,</span> <span class="s1">&#39;Norm_Ind&#39;</span><span class="p">,</span> <span class="s1">&#39;Norm_Market&#39;</span><span class="p">]</span>
                <span class="n">formula</span> <span class="o">=</span> <span class="s1">&#39;Norm ~ Norm_Ind + Norm_Market&#39;</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">csv_df</span><span class="p">)</span>
                <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

                <span class="c1">#标准信息</span>
                <span class="n">standard_info</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">Norm_Ind</span> <span class="o">+</span> <span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">Norm_Market</span>
                <span class="c1">#信息含量</span>
                <span class="n">informative_content</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">resid</span><span class="p">))</span>

                <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;股票代码&#39;</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;\d</span><span class="si">{6}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">csvf</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> 
                        <span class="s1">&#39;会计年度&#39;</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;\d</span><span class="si">{4}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">csvf</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> 
                        <span class="s1">&#39;标准信息&#39;</span><span class="p">:</span> <span class="n">standard_info</span><span class="p">,</span> 
                        <span class="s1">&#39;信息含量&#39;</span><span class="p">:</span> <span class="n">informative_content</span><span class="p">}</span>
                <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">CPU times: user 7min 40s, sys: 33min 5s, total: 40min 45s
Wall time: 4min 36s
</code></pre></div><p><br><br></p>
<p>读取生成的<em><strong>mda_infor2001-2022.csv</strong></em>  文件，欣赏一下 <code>标准信息、信息含量</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;mda_infor2001-2022.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df9.png" alt=""  />
</p>
<br>
<p>需要注意，原文选取 2007 — 2015 年中国上市公司年报中的 MD&amp;A 信息作为研究样本。 之所以选取 2007 年作为样本的起点， 是因为从 2007 年开始， MD&amp;A 在企业定期报告中的披露要求已经较为完善， 而且 2007 年是中国会计准则国际趋同的重要时点， 新制定的《企业会计准则》已经开始实施， 为避免前后会计准则差异而产生的影响， 因此选取 2007 年作为样本区间的起点。</p>
<p><strong>mda_infor.csv含有2010-2022年的数据，如要复现原文，需要注意筛选数据。</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;mda_infor.csv记录数:&#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">mda_infor2001-2022.csv记录数: 50811
</code></pre></div><p><br><br></p>
<h2 id="七资料获取">七、资料获取</h2>
<p>内容创作不易， <strong>200</strong> 元，加微信 <strong>372335839</strong>， 备注「姓名-学校-专业」。</p>
<p>资料截图， 整个资料文件夹体积高达 12 G。</p>
<p><img loading="lazy" src="img/screen.png" alt=""  />
</p>
<p><img loading="lazy" src="img/size.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-01-13-information-content-of-critical-audit/">金融研究 | 使用Python构建「关键审计事项信息含量」</a></li>
<li><a href="https://textdata.cn/blog/2023-09-08-earnings-communication-conference-forward-looking-statements-information/">中国管理科学 | 使用业绩说明会文本数据测量上市公司前瞻性信息</a></li>
<li><a href="https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/"><strong>数据集 | A股上市公司基本信息</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-21-hk-stock-market-anual-report/"><strong>数据集 | 港股年报文本数据集(2007 ~ 2023.12)</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-18-neeq-china-listed-on-nation-equities-exchange-and-quotation-system-anunal-year-report/"><strong>数据集(付费) | 三板上市公司年报2002-2023.12</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-14-usa-sec-10k-report-dataset/"><strong>数据集 | 美股年报10-K、20-F数据(2000-2023.12)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"><strong>词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型</strong></a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理世界 | 使用 经营讨论与分析 测量 企业数字化</title>
      <link>https://textdata.cn/blog/2022-11-03-mda-measure-digitalization/</link>
      <pubDate>Sat, 20 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-03-mda-measure-digitalization/</guid>
      <description>使用 经营讨论与分 文本数据，测量企业的数字化指标，代码实现过程参考2021管理世界的一篇论文。This paper conducted a research on the impact of corporate digital transformation on stock liquidity, by means of data from 2007~2018 of A-share listed companies in China. This paper empirically tested the impact,mechanisms and external basic conditions of corporate digital transformation on stock liquidity. The main conclusions are as fol⁃ lows. Firstly, corporate digital transformation has significantly improved the level of stock liquidity. In particular, there are significant asymmetric effects under different corporate attributes and characteristics, which is the digital transformation of non-state-owned enterprises and high-tech enterprises can raise the level of stock liquidity in the market. Secondly, corporate digital transformation can improve the problem of information asymmetry, increase market investors&amp;#39; expectations, and optimize the input and output of enterprise innovation, and therefore improve the quality and efficiency of corporate operations. All of these will contribute to the improvement of corporate stock liquidity. Thirdly, effective external conditions are the important foundation for corporate digital transformation to work effective⁃ ly. Moreover, a good foundation for the development of financial technology plays a positive moderating effect in the &amp;#34;corporate digital transformation—stock liquidity&amp;#34; relationship.</description>
      <content:encoded><![CDATA[<p>使用 经营讨论与分析 数据，计算企业数字化指标, 相关论文:</p>
<ul>
<li>吴非, 胡慧芷, 林慧妍, and 任晓怡. &ldquo;企业数字化转型与资本市场表现——来自股票流动性的经验证据.&rdquo; 管理世界 (2021).</li>
<li>宋德勇, 朱文博, and 丁海. &ldquo;企业数字化能否促进绿色技术创新?.&rdquo; 财经研究 48, no. 4 (2022).</li>
<li>方明月,聂辉华,阮睿,沈昕毅.企业数字化转型与经济政策不确定性感知[J].金融研究,2023,(02):21-39.</li>
</ul>
<p>数字化指标数分析结果以xlsx存储，如下图</p>
<p><br><br></p>
<h2 id="一读取数据">一、读取数据</h2>
<p><img loading="lazy" src="img/mda_screen.png" alt=""  />
</p>
<p>完整md&amp;a数据集 841 M，覆盖 55856 条md&amp;a记录。 查看数据集详情可点击</p>
<ul>
<li><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/"><strong>数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析</strong></a></li>
</ul>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import pandas as pd

df = pd.read_csv(&#39;mda01-22.csv.gz&#39;, compression=&#39;gzip&#39;)
print(len(df))
df.head()
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">55856
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<h2 id="二构建词典">二、构建词典</h2>
<p>下图是吴非等(2021)数字化指标的截图</p>
<p><img loading="lazy" src="img/%e7%ae%a1%e7%90%86%e4%b8%96%e7%95%8c2021%e5%90%b4%e9%9d%9e-%e4%bc%81%e4%b8%9a%e6%95%b0%e5%ad%97%e5%8c%96-%e5%85%b3%e9%94%ae%e8%af%8d.png" alt=""  />
</p>
<blockquote>
<p>后期，如果想自己扩展词典，可以初步筛选种子词(该篇论文的词表), 使用md&amp;a语料文件(txt格式)， 结合cntext库的so-pmi或词向量方法，对数字化词典进行扩充。</p>
</blockquote>
<p>这里我已将吴非等(2021)的词表内置到 cntext库（2.1.1版本）的 zh_common_Digitalization.yaml 中。</p>
<br>
<h3 id="21-安装cntext">2.1 安装cntext</h3>
<p>我使用的自己 <strong>未公开</strong> 的cntext 2.1.1 版本， Bug频出，等调整好了再公开。</p>
<p>将 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
</code></pre></div><p>文末有 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em>  获取方式</p>
<br>
<h3 id="22-导入词典">2.2 导入词典</h3>
<p>查看内置词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import cntext as ct

ct.get_dict_list()
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;zh_common_NTUSD.yaml&#39;,
 &#39;zh_common_DUTIR.yaml&#39;,
 &#39;enzh_common_StopWords.yaml&#39;,
 &#39;en_valence_Concreteness.yaml&#39;,
 &#39;en_common_LoughranMcDonald.yaml&#39;,
 &#39;zh_common_FinanceSenti.yaml&#39;,
 &#39;zh_common_TsinghuaPraiseDegrade.yaml&#39;,
 &#39;en_common_ANEW.yaml&#39;,
 &#39;en_common_NRC.yaml&#39;,
 &#39;zh_valence_ChineseEmoBank.yaml&#39;,
 &#39;zh_valence_SixSemanticDimensionDatabase.yaml&#39;,
 &#39;zh_common_FinacialFormalUnformal.yaml&#39;,
 &#39;zh_common_LoughranMcDonald.yaml&#39;,
 &#39;enzh_common_AdvConj.yaml&#39;,
 &#39;en_common_SentiWS.yaml&#39;,
 &#39;zh_common_Digitalization.yaml&#39;,
 &#39;en_common_LSD2015.yaml&#39;,
 &#39;zh_common_HowNet.yaml&#39;]
</code></pre></div><br>
<p>导入数字化词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">Digitalization_Infos</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;zh_common_Digitalization.yaml&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Digitalization_Infos</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;Name&#39;: &#39;中文数字化词典&#39;, 

&#39;Desc&#39;: &#39;基于这篇论文，构建了中文数字化词典，含人工智能技术、大数据技术、云计算技术、区块链技术、数字技术应用等关键词列表。 &#39;, 

&#39;Refer&#39;: &#39;吴非,胡慧芷,林慧妍,任晓怡. 企业数字化转型与资本市场表现——来自股票流动性的经验证据[J]. 管理世界,2021,37(07):130-144+10.&#39;, 

&#39;Category&#39;: [&#39;Artificial_Intelligence&#39;, &#39;Big_Data&#39;, &#39;Cloud_Computing&#39;, &#39;Block_Chains&#39;, &#39;Usage_of_Digitalization&#39;], 

&#39;Dictionary&#39;: {
    &#39;Artificial_Intelligence&#39;: [&#39;人工智能&#39;, &#39;商业智能&#39;, &#39;图像理解&#39;, &#39;投资决策辅助系统&#39;, &#39;智能数据分析&#39;, &#39;智能机器人&#39;, &#39;机器学习&#39;, &#39;深度学习&#39;, &#39;语义搜索&#39;, &#39;生物识别技术&#39;, &#39;人脸识别&#39;, &#39;语音识别&#39;, &#39;身份验证&#39;, &#39;自动驾驶&#39;, &#39;自然语言处理&#39;], 
    
    &#39;Big_Data&#39;: [&#39;大数据&#39;, &#39;数据挖掘&#39;, &#39;文本挖掘&#39;, &#39;数据可视化&#39;, &#39;异构数据&#39;, &#39;征信&#39;, &#39;增强现实&#39;, &#39;混合现实&#39;, &#39;虚拟现实&#39;], 
    
    &#39;Cloud_Computing&#39;: [&#39;云计算&#39;, &#39;流计算&#39;, &#39;图计算&#39;, &#39;内存计算&#39;, &#39;多方安全计算&#39;, &#39;类脑计算&#39;, &#39;绿色计算&#39;, &#39;认知计算&#39;, &#39;融合架构&#39;, &#39;亿级并发&#39;, &#39;EB级存储&#39;, &#39;物联网&#39;, &#39;信息物理系统&#39;], 
    
    &#39;Block_Chains&#39;: [&#39;区块链&#39;, &#39;数字货币&#39;, &#39;分布式计算&#39;, &#39;差分隐私技术&#39;, &#39;智能金融合约&#39;], 
    
    &#39;Usage_of_Digitalization&#39;: [&#39;移动互联网&#39;, &#39;工业互联网&#39;, &#39;移动互联&#39;, &#39;互联网医疗&#39;, &#39;电子商务&#39;, &#39;移动支付&#39;, &#39;第三方支付&#39;, &#39;NFC支付&#39;, &#39;智能能源&#39;, &#39;B2B&#39;, &#39;B2C&#39;, &#39;C2B&#39;, &#39;C2C&#39;, &#39;O2O&#39;, &#39;网联&#39;, &#39;智能穿戴&#39;, &#39;智慧农业&#39;, &#39;智能交通&#39;, &#39;智能医疗&#39;, &#39;智能客服&#39;, &#39;智能家居&#39;, &#39;智能投顾&#39;, &#39;智能文旅&#39;, &#39;智能环保&#39;, &#39;智能电网&#39;, &#39;智能营销&#39;, &#39;数字营销&#39;, &#39;无人零售&#39;, &#39;互联网金融&#39;, &#39;数字金融&#39;, &#39;Fintech&#39;, &#39;金融科技&#39;, &#39;量化金融&#39;, &#39;开放银行&#39;]
    }
}
</code></pre></div><br>
<br>
<h2 id="三定义数字化函数">三、定义数字化函数</h2>
<p>目前，对于企业数字化水平的度量是相关研究的难点，现有文献主要有三种度量方法。</p>
<ul>
<li>第一，祁怀锦等（2020）使用企业年末无形资产明细项中与数字经济相关部分的金额占无形资产总额的比例度量企业数字化程度。</li>
<li>第二，大量研究运用数字化相关关键词在年报中的词频数量或占比度量企业的数字化转型或数字化水平（赵宸宇，2021；袁淳等，2021）。</li>
<li>第三，相关研究采取问卷调查的方式获取企业的数字化水平数据（刘政等，2020）。</li>
</ul>
<p>使用第二种方法，通过Python定义数字化函数，统计文本中数字化词语个数得到相应指标。</p>
<blockquote>
<p>吴非等(2021管理世界)数字化指标的计算更复杂一些，在此基础上，剔除关键词前存在“没”
“无”
“不”等否定词语的表述，同时也剔除非本公司（包括公司的股东、客户、供应商、公司高管简介介绍在内）的“数
字化转型”关键词。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import pandas as pd

#函数内导入jieba是为了适配并行运算pandarallel
def digtal_function(text):
    import cntext as ct
    #统计text中每类词的个数
    digtal_diction = ct.read_yaml_dict(&#39;zh_common_Digitalization.yaml&#39;)[&#39;Dictionary&#39;]
    res = ct.sentiment(text=text,  diction=digtal_diction)
    return pd.Series(res)


test_text = &#39;经过技术人员不懈努力， 该企业在人工智能、大数据、云计算、工业互联网等领域有了一定的市场地位....&#39;


digtal_function(text=test_text)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Artificial_Intelligence_num     1
Big_Data_num                    1
Cloud_Computing_num             1
Block_Chains_num                0
Usage_of_Digitalization_num     1
stopword_num                   11
word_num                       24
sentence_num                    1
dtype: int64
</code></pre></div><p><br><br></p>
<h2 id="四批量计算">四、批量计算</h2>
<p>使用 <em><strong>apply</strong></em> 方法，对 <em><strong>text</strong></em> 列，进行  <em><strong>digtal_function</strong></em> 运算, 得到  <em><strong>res_df</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">from pandarallel import pandarallel
pandarallel.initialize()

#结果返回为dataframe，数字代表的是每类词出现次数
res_df = df[&#39;text&#39;].parallel_apply(digtal_function)
res_df.head()
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">INFO: Pandarallel will run on 12 workers.
INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p>参数解读</p>
<ul>
<li><em><strong>Artificial_Intelligence_num</strong></em>	 人工智能技术词出现在md&amp;a中的次数</li>
<li><em><strong>Big_Data_num</strong></em>	 大数据技术词出现在md&amp;a中的次数</li>
<li><em><strong>Cloud_Computing_num</strong></em>	云计算技术词出现在md&amp;a中的次数</li>
<li><em><strong>Block_Chains_num</strong></em>	区块链技术词出现在md&amp;a中的次数</li>
<li><em><strong>Usage_of_Digitalization_num</strong></em>	数字化应用技术词出现在md&amp;a中的次数</li>
<li><em><strong>stopword_num</strong></em>	停用词出现在md&amp;a中的次数</li>
<li><em><strong>word_num</strong></em>	md&amp;a中的总词数(md&amp;a的长度)</li>
<li><em><strong>sentence_num</strong></em>   md&amp;a的句子数</li>
</ul>
<p><br><br></p>
<h2 id="五结果整理">五、结果整理</h2>
<p>上一环节，将各种技术词出现次数加总，构建企业数字化词语出现个数， 并将其转为数字化指标(词频)。</p>
<blockquote>
<p>由于这类数据具有典型的“右偏性”特征，后续在其他计量分析软件中需要将其进行对数化处理，从而得到刻画企业数字化转型的整体指标。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">res_df[&#39;Digital_word_num&#39;] = res_df[[&#39;Artificial_Intelligence_num&#39;, 
                                     &#39;Big_Data_num&#39;, 
                                     &#39;Cloud_Computing_num&#39;, 
                                     &#39;Block_Chains_num&#39;, 
                                     &#39;Usage_of_Digitalization_num&#39;]].sum(axis=1)

# [数字化相关技术词] 在 [文本总词数] 中的占比
res_df[&#39;Digital_Index&#39;] = np.log(res_df[&#39;Digital_word_num&#39;]+1)
res_df.head()
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="六保存结果">六、保存结果</h2>
<p>合并 <em><strong>df</strong></em> 和 <em><strong>res_df</strong></em>， 查看 <em><strong>Digital_Index</strong></em> 的最大、最小、均值</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df2 = pd.concat([df, res_df], axis=1)

print(&#39;Digital_Index最小值: &#39;, df2.Digital_Index.min())
print(&#39;Digital_Index平均值: &#39;, df2.Digital_Index.mean())
print(&#39;Digital_Index最大值: &#39;, df2.Digital_Index.max())
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Digital_Index最小值: 0.0
Digital_Index平均值: 0.836223935643458
Digital_Index最大值: 5.963579343618446
</code></pre></div><br>
<p>选中需要的字段，保存到 <em><strong>corporate_digitalization.xlsx</strong></em> 内</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span><span class="p">[[</span><span class="s1">&#39;code&#39;</span><span class="p">,</span> 
     <span class="s1">&#39;year&#39;</span><span class="p">,</span> 
     <span class="s1">&#39;Digital_word_num&#39;</span><span class="p">,</span> 
     <span class="s1">&#39;word_num&#39;</span><span class="p">,</span> 
     <span class="s1">&#39;Digital_Index&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_excel</span><span class="p">(</span><span class="s1">&#39;corporate_digitalization.xlsx&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<p>查看结果 <em><strong>corporate_digitalization.xlsx</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">## 查看结果</span>
<span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;corporate_digitalization.xlsx&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/digital_index.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="七获取资料">七、获取资料</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 100元 管理层讨论与分析数据mda01-22.csv.gz

- 100元 cntext-2.1.1-py3-none-any.whl

- 200元 
   - 管理层讨论与分析数据mda01-22.csv.gz
   - cntext-2.1.1-py3-none-any.whl
   - 数字化代码.ipynb
   - corporate_digitalization.xlsx
</code></pre></div><p>加微信 <strong>372335839</strong>， 备注「姓名-学校-专业」。</p>
<p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 |  上市公司董监高人员的个人特征/教育背景/任职情况</title>
      <link>https://textdata.cn/blog/2024-04-18-china-a-listed-company-figure-characteristic-dataset/</link>
      <pubDate>Thu, 18 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-18-china-a-listed-company-figure-characteristic-dataset/</guid>
      <description>&lt;h2 id=&#34;一上市公司董监高&#34;&gt;一、上市公司董监高&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据集: 中国上市公司人物特征研究数据库
   
董监高人数: 375105

记录数:
   - 董监高个人特征  1548448
   - 董监高教育背景明细表 639615
   - 董监高任职情况表 1448841 

截止日期: 1990-2024.4.8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二查看数据&#34;&gt;二、查看数据&lt;/h2&gt;
&lt;h3 id=&#34;21-董监高教育背景明细表&#34;&gt;2.1 董监高教育背景明细表&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;import pandas as pd

df1 = pd.read_csv(&amp;#39;董监高教育背景明细表.csv&amp;#39;)
df1.head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;查看字段&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;field_max_len&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;desc_max_len&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;desc&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;- &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;field&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;field_max_len&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;desc&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;desc_max_len&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- Symbol         股票代码  
- EndDate        截止日期  
- PersonID       人员ID  
- FullName       人员姓名  
- Degree         学历    
- UniversityID   毕业院校ID
- University     毕业院校  
- Major          专业    
- AdmissionTime  入校时间  
- GraduationTime 毕业时间  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-董监高个人特征&#34;&gt;2.2 董监高个人特征&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;董监高个人特征.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;查看字段&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;field_max_len = max([len(x) for x in df2.iloc[0, :].index])
desc_max_len = max([len(x) for x in df2.iloc[0, :].values])

for field, desc in zip(df2.iloc[0, :].index, df2.iloc[0, :].values):
    print(f&amp;#39;- {field:&amp;lt;{field_max_len}} {desc:&amp;lt;{desc_max_len}}&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Stkcd&lt;/span&gt;             &lt;span class=&#34;n&#34;&gt;证券代码&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Reptdt&lt;/span&gt;            &lt;span class=&#34;n&#34;&gt;统计截止日期&lt;/span&gt;        
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PersonID&lt;/span&gt;          &lt;span class=&#34;n&#34;&gt;人员ID&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Name&lt;/span&gt;              &lt;span class=&#34;n&#34;&gt;姓名&lt;/span&gt;            
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Nationality&lt;/span&gt;       &lt;span class=&#34;n&#34;&gt;国籍&lt;/span&gt;            
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NativePlace&lt;/span&gt;       &lt;span class=&#34;n&#34;&gt;籍贯&lt;/span&gt;            
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NatAreaCode&lt;/span&gt;       &lt;span class=&#34;n&#34;&gt;籍贯所在地区代码&lt;/span&gt;      
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BirthPlace&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;出生地&lt;/span&gt;           
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;BirAreaCode&lt;/span&gt;       &lt;span class=&#34;n&#34;&gt;出生地所在地区代码&lt;/span&gt;     
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Gender&lt;/span&gt;            &lt;span class=&#34;n&#34;&gt;性别&lt;/span&gt;            
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Age&lt;/span&gt;               &lt;span class=&#34;n&#34;&gt;年龄&lt;/span&gt;            
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;University&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;毕业院校&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Degree&lt;/span&gt;            &lt;span class=&#34;n&#34;&gt;学历&lt;/span&gt;            
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Major&lt;/span&gt;             &lt;span class=&#34;n&#34;&gt;专业&lt;/span&gt;            
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Profession&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;职称&lt;/span&gt;            
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Resume&lt;/span&gt;            &lt;span class=&#34;n&#34;&gt;个人简历&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PaidSign&lt;/span&gt;          &lt;span class=&#34;n&#34;&gt;是否领取薪酬&lt;/span&gt;        
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TotalSalary&lt;/span&gt;       &lt;span class=&#34;n&#34;&gt;报告期报酬总额&lt;/span&gt;       
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Allowance&lt;/span&gt;         &lt;span class=&#34;n&#34;&gt;其中&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;津贴&lt;/span&gt;         
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SharEnd&lt;/span&gt;           &lt;span class=&#34;n&#34;&gt;年末持股数&lt;/span&gt;         
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IsMTMT&lt;/span&gt;            &lt;span class=&#34;n&#34;&gt;是否高管团队成员&lt;/span&gt;      
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TMTP&lt;/span&gt;              &lt;span class=&#34;n&#34;&gt;高管职务类别&lt;/span&gt;        
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IsMTB&lt;/span&gt;             &lt;span class=&#34;n&#34;&gt;是否董事会成员&lt;/span&gt;       
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;CTB&lt;/span&gt;               &lt;span class=&#34;n&#34;&gt;董事会职务类别&lt;/span&gt;       
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IsIdirecotr&lt;/span&gt;       &lt;span class=&#34;n&#34;&gt;是否独立董事&lt;/span&gt;        
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IsDuality&lt;/span&gt;         &lt;span class=&#34;n&#34;&gt;是否兼任董事长和CEO&lt;/span&gt;   
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IsSupervisor&lt;/span&gt;      &lt;span class=&#34;n&#34;&gt;是否监事&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Position&lt;/span&gt;          &lt;span class=&#34;n&#34;&gt;具体职务&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PositionID&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;具体职务ID&lt;/span&gt;        
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ServicePosition&lt;/span&gt;   &lt;span class=&#34;n&#34;&gt;在职职务&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ServicePositionID&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;在职职务ID&lt;/span&gt;        
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Funback&lt;/span&gt;           &lt;span class=&#34;n&#34;&gt;职业背景&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OveseaBack&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;海外背景&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Academic&lt;/span&gt;          &lt;span class=&#34;n&#34;&gt;学术背景&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;FinBack&lt;/span&gt;           &lt;span class=&#34;n&#34;&gt;金融背景&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IsCocurP&lt;/span&gt;          &lt;span class=&#34;n&#34;&gt;是否在股东单位兼任&lt;/span&gt;     
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OtherCo&lt;/span&gt;           &lt;span class=&#34;n&#34;&gt;兼任职务&lt;/span&gt;          
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OtherCoType&lt;/span&gt;       &lt;span class=&#34;n&#34;&gt;兼任职务类别&lt;/span&gt;        
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Director_TotCO&lt;/span&gt;    &lt;span class=&#34;n&#34;&gt;兼任职务为董事的公司总数&lt;/span&gt;  
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Director_ListCO&lt;/span&gt;   &lt;span class=&#34;n&#34;&gt;兼任职务为董事的上市公司总数&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Stkcd_director&lt;/span&gt;    &lt;span class=&#34;n&#34;&gt;兼任职务为董事的上市公司代码&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-董监高任职情况表&#34;&gt;2.3 董监高任职情况表&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;董监高任职情况表.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;field_max_len&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;desc_max_len&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;field&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;desc&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iloc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;- &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;field&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;field_max_len&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;desc&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;desc_max_len&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- Stkcd         证券代码    
- Reptdt        统计截止日期  
- PersonID      人员ID    
- Name          姓名      
- Position      具体职务    
- PositionID    具体职务ID  
- StartDate     任职开始日期  
- EndDate       任职结束日期  
- ServiceStatus 是否在职    
- Tenure        任期      
- ToLeavPost    距离离任剩余日期
- ResignReason  离职原因    
- GTAPosition   职务名称    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三相关数据&#34;&gt;三、相关数据&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-11-25-senior-manager-resume-dataset/&#34;&gt;数据集(付费) | 90w条中国上市公司高管数据&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-05-17-top-manager-violation/&#34;&gt;数据集 | 上市公司高管违规数据(2008-2022)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/&#34;&gt;数据集 | 2001-2022年A股上市公司年报&amp;amp;管理层讨论与分析&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-01-18-neeq-china-listed-on-nation-equities-exchange-and-quotation-system-anunal-year-report/&#34;&gt;数据集(付费) | 三板上市公司年报2002-2023.12&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-01-03-listed-company-arbitration-dataset/&#34;&gt;数据集 | 36330条上市公司仲裁数据(2000-2021)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-07-patent-application-dataset-of-listed-company-in-china-a-market/&#34;&gt;数据集 | 上市公司 208 万条专利数据集 (1991-2022)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-09-08-china-a-share-market-listed-company-earnings-communication-conference/&#34;&gt;数据集 | 84w条业绩说明会问答数据(2005-2023)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-08-11-china-a-market-corporate-social-responsibility-dataste/&#34;&gt;数据集 | 2006年-2022年企业社会责任报告&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-04-17-china-a-market-inquiry-letter-datasets/&#34;&gt;数据集(付费) | 2014年-2022年监管问询函&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-04-26-entrusted-loan-dataset/&#34;&gt;数据集| 07-21年上市公司「委托贷款公告」&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/coporate_social_responsibility_datasets/&#34;&gt;数据集 | 企业社会责任报告数据集&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四获取数据&#34;&gt;四、获取数据&lt;/h2&gt;
&lt;p&gt;数据集 50 元， 加微信 &lt;strong&gt;372335839&lt;/strong&gt;, 备注「姓名-学校-专业-董监高」获取本数据集。&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一上市公司董监高">一、上市公司董监高</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据集: 中国上市公司人物特征研究数据库
   
董监高人数: 375105

记录数:
   - 董监高个人特征  1548448
   - 董监高教育背景明细表 639615
   - 董监高任职情况表 1448841 

截止日期: 1990-2024.4.8
</code></pre></div><p><br><br></p>
<h2 id="二查看数据">二、查看数据</h2>
<h3 id="21-董监高教育背景明细表">2.1 董监高教育背景明细表</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import pandas as pd

df1 = pd.read_csv(&#39;董监高教育背景明细表.csv&#39;)
df1.head()
</code></pre></div><p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<br>
<p>查看字段</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">field_max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df1</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">index</span><span class="p">])</span>
<span class="n">desc_max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df1</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">])</span>

<span class="k">for</span> <span class="n">field</span><span class="p">,</span> <span class="n">desc</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;- </span><span class="si">{</span><span class="n">field</span><span class="si">:</span><span class="s1">&lt;</span><span class="si">{</span><span class="n">field_max_len</span><span class="si">}}</span><span class="s1"> </span><span class="si">{</span><span class="n">desc</span><span class="si">:</span><span class="s1">&lt;</span><span class="si">{</span><span class="n">desc_max_len</span><span class="si">}}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- Symbol         股票代码  
- EndDate        截止日期  
- PersonID       人员ID  
- FullName       人员姓名  
- Degree         学历    
- UniversityID   毕业院校ID
- University     毕业院校  
- Major          专业    
- AdmissionTime  入校时间  
- GraduationTime 毕业时间  
</code></pre></div><p><br><br></p>
<h3 id="22-董监高个人特征">2.2 董监高个人特征</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;董监高个人特征.csv&#39;</span><span class="p">)</span>
<span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/02-df.png" alt=""  />
</p>
<br>
<p>查看字段</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">field_max_len = max([len(x) for x in df2.iloc[0, :].index])
desc_max_len = max([len(x) for x in df2.iloc[0, :].values])

for field, desc in zip(df2.iloc[0, :].index, df2.iloc[0, :].values):
    print(f&#39;- {field:&lt;{field_max_len}} {desc:&lt;{desc_max_len}}&#39;)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">-</span> <span class="n">Stkcd</span>             <span class="n">证券代码</span>          
<span class="o">-</span> <span class="n">Reptdt</span>            <span class="n">统计截止日期</span>        
<span class="o">-</span> <span class="n">PersonID</span>          <span class="n">人员ID</span>          
<span class="o">-</span> <span class="n">Name</span>              <span class="n">姓名</span>            
<span class="o">-</span> <span class="n">Nationality</span>       <span class="n">国籍</span>            
<span class="o">-</span> <span class="n">NativePlace</span>       <span class="n">籍贯</span>            
<span class="o">-</span> <span class="n">NatAreaCode</span>       <span class="n">籍贯所在地区代码</span>      
<span class="o">-</span> <span class="n">BirthPlace</span>        <span class="n">出生地</span>           
<span class="o">-</span> <span class="n">BirAreaCode</span>       <span class="n">出生地所在地区代码</span>     
<span class="o">-</span> <span class="n">Gender</span>            <span class="n">性别</span>            
<span class="o">-</span> <span class="n">Age</span>               <span class="n">年龄</span>            
<span class="o">-</span> <span class="n">University</span>        <span class="n">毕业院校</span>          
<span class="o">-</span> <span class="n">Degree</span>            <span class="n">学历</span>            
<span class="o">-</span> <span class="n">Major</span>             <span class="n">专业</span>            
<span class="o">-</span> <span class="n">Profession</span>        <span class="n">职称</span>            
<span class="o">-</span> <span class="n">Resume</span>            <span class="n">个人简历</span>          
<span class="o">-</span> <span class="n">PaidSign</span>          <span class="n">是否领取薪酬</span>        
<span class="o">-</span> <span class="n">TotalSalary</span>       <span class="n">报告期报酬总额</span>       
<span class="o">-</span> <span class="n">Allowance</span>         <span class="n">其中</span><span class="err">：</span><span class="n">津贴</span>         
<span class="o">-</span> <span class="n">SharEnd</span>           <span class="n">年末持股数</span>         
<span class="o">-</span> <span class="n">IsMTMT</span>            <span class="n">是否高管团队成员</span>      
<span class="o">-</span> <span class="n">TMTP</span>              <span class="n">高管职务类别</span>        
<span class="o">-</span> <span class="n">IsMTB</span>             <span class="n">是否董事会成员</span>       
<span class="o">-</span> <span class="n">CTB</span>               <span class="n">董事会职务类别</span>       
<span class="o">-</span> <span class="n">IsIdirecotr</span>       <span class="n">是否独立董事</span>        
<span class="o">-</span> <span class="n">IsDuality</span>         <span class="n">是否兼任董事长和CEO</span>   
<span class="o">-</span> <span class="n">IsSupervisor</span>      <span class="n">是否监事</span>          
<span class="o">-</span> <span class="n">Position</span>          <span class="n">具体职务</span>          
<span class="o">-</span> <span class="n">PositionID</span>        <span class="n">具体职务ID</span>        
<span class="o">-</span> <span class="n">ServicePosition</span>   <span class="n">在职职务</span>          
<span class="o">-</span> <span class="n">ServicePositionID</span> <span class="n">在职职务ID</span>        
<span class="o">-</span> <span class="n">Funback</span>           <span class="n">职业背景</span>          
<span class="o">-</span> <span class="n">OveseaBack</span>        <span class="n">海外背景</span>          
<span class="o">-</span> <span class="n">Academic</span>          <span class="n">学术背景</span>          
<span class="o">-</span> <span class="n">FinBack</span>           <span class="n">金融背景</span>          
<span class="o">-</span> <span class="n">IsCocurP</span>          <span class="n">是否在股东单位兼任</span>     
<span class="o">-</span> <span class="n">OtherCo</span>           <span class="n">兼任职务</span>          
<span class="o">-</span> <span class="n">OtherCoType</span>       <span class="n">兼任职务类别</span>        
<span class="o">-</span> <span class="n">Director_TotCO</span>    <span class="n">兼任职务为董事的公司总数</span>  
<span class="o">-</span> <span class="n">Director_ListCO</span>   <span class="n">兼任职务为董事的上市公司总数</span>
<span class="o">-</span> <span class="n">Stkcd_director</span>    <span class="n">兼任职务为董事的上市公司代码</span>
</code></pre></div><br>
<br>
<h3 id="23-董监高任职情况表">2.3 董监高任职情况表</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df3</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;董监高任职情况表.csv&#39;</span><span class="p">)</span>
<span class="n">df3</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/03-df.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">field_max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df3</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">index</span><span class="p">])</span>
<span class="n">desc_max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df3</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">])</span>

<span class="k">for</span> <span class="n">field</span><span class="p">,</span> <span class="n">desc</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df3</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">df3</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;- </span><span class="si">{</span><span class="n">field</span><span class="si">:</span><span class="s1">&lt;</span><span class="si">{</span><span class="n">field_max_len</span><span class="si">}}</span><span class="s1"> </span><span class="si">{</span><span class="n">desc</span><span class="si">:</span><span class="s1">&lt;</span><span class="si">{</span><span class="n">desc_max_len</span><span class="si">}}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- Stkcd         证券代码    
- Reptdt        统计截止日期  
- PersonID      人员ID    
- Name          姓名      
- Position      具体职务    
- PositionID    具体职务ID  
- StartDate     任职开始日期  
- EndDate       任职结束日期  
- ServiceStatus 是否在职    
- Tenure        任期      
- ToLeavPost    距离离任剩余日期
- ResignReason  离职原因    
- GTAPosition   职务名称    
</code></pre></div><p><br><br></p>
<h2 id="三相关数据">三、相关数据</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2022-11-25-senior-manager-resume-dataset/">数据集(付费) | 90w条中国上市公司高管数据</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-05-17-top-manager-violation/">数据集 | 上市公司高管违规数据(2008-2022)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/">数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2024-01-18-neeq-china-listed-on-nation-equities-exchange-and-quotation-system-anunal-year-report/">数据集(付费) | 三板上市公司年报2002-2023.12</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2024-01-03-listed-company-arbitration-dataset/">数据集 | 36330条上市公司仲裁数据(2000-2021)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-07-patent-application-dataset-of-listed-company-in-china-a-market/">数据集 | 上市公司 208 万条专利数据集 (1991-2022)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-09-08-china-a-share-market-listed-company-earnings-communication-conference/">数据集 | 84w条业绩说明会问答数据(2005-2023)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-08-11-china-a-market-corporate-social-responsibility-dataste/">数据集 | 2006年-2022年企业社会责任报告</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-17-china-a-market-inquiry-letter-datasets/">数据集(付费) | 2014年-2022年监管问询函</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-04-26-entrusted-loan-dataset/">数据集| 07-21年上市公司「委托贷款公告」</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/coporate_social_responsibility_datasets/">数据集 | 企业社会责任报告数据集</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="四获取数据">四、获取数据</h2>
<p>数据集 50 元， 加微信 <strong>372335839</strong>, 备注「姓名-学校-专业-董监高」获取本数据集。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 |  使用3394w条豆瓣书评数据集</title>
      <link>https://textdata.cn/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/</link>
      <pubDate>Wed, 17 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/</guid>
      <description>&lt;h2 id=&#34;一豆瓣读书介绍&#34;&gt;一、豆瓣读书介绍&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据集: douba-book

数据源: 豆瓣读书
   
记录数:
   - 标签 120 个
   - 书 17967 部
   - 书评 33941454 条
   
书评日期起止: 2005-06-12 ~ 2018-10-13
   
体积: 2.11G(解压后5.52G) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;该数据已经过初步清洗，可用于推荐系统、情感分析、知识图谱、社会学文化变迁等多个领域(或主题)。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二查看数据&#34;&gt;二、查看数据&lt;/h2&gt;
&lt;h3 id=&#34;21-读取数据&#34;&gt;2.1 读取数据&lt;/h3&gt;
&lt;p&gt;下载 &lt;em&gt;&lt;strong&gt;douban_book.csv.gz&lt;/strong&gt;&lt;/em&gt; 解压后，可以看到数据集中有一个 &lt;em&gt;&lt;strong&gt;douban_book.csv&lt;/strong&gt;&lt;/em&gt; 文件。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;douban_book.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;33941454
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-所含字段&#34;&gt;2.2 所含字段&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;col&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; - &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt; - tag          标签
 - book_name    书名
 - user_name    书评人
 - date         书评发布日期
 - comment      书评内容
 - star         评分(1-5)
 - vote_count   书评获赞数
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;23--覆盖日期&#34;&gt;2.3  覆盖日期&lt;/h3&gt;
&lt;p&gt;书评发布日期覆盖(最早~ 最晚)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2005-06-12 00:00:00
2018-10-13 00:00:00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;24-标签&#34;&gt;2.4 标签&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nunique&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tag&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unique&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;120

[&amp;#39;思想&amp;#39; &amp;#39;科技&amp;#39; &amp;#39;金融&amp;#39; &amp;#39;政治学&amp;#39; &amp;#39;随笔&amp;#39; &amp;#39;爱情&amp;#39; &amp;#39;名著&amp;#39; &amp;#39;幾米&amp;#39; &amp;#39;人文&amp;#39; &amp;#39;交互&amp;#39; &amp;#39;悬疑&amp;#39; &amp;#39;算法&amp;#39; &amp;#39;哲学&amp;#39; &amp;#39;艺术史&amp;#39;
 &amp;#39;历史&amp;#39; &amp;#39;用户体验&amp;#39; &amp;#39;绘画&amp;#39; &amp;#39;诗词&amp;#39; &amp;#39;考古&amp;#39; &amp;#39;心理学&amp;#39; &amp;#39;互联网&amp;#39; &amp;#39;戏剧&amp;#39; &amp;#39;安妮宝贝&amp;#39; &amp;#39;艺术&amp;#39; &amp;#39;东野圭吾&amp;#39; &amp;#39;散文&amp;#39; &amp;#39;魔幻&amp;#39;
 &amp;#39;童话&amp;#39; &amp;#39;商业&amp;#39; &amp;#39;UCD&amp;#39; &amp;#39;日本文学&amp;#39; &amp;#39;武侠&amp;#39; &amp;#39;音乐&amp;#39; &amp;#39;通信&amp;#39; &amp;#39;科幻小说&amp;#39; &amp;#39;科普&amp;#39; &amp;#39;程序&amp;#39; &amp;#39;生活&amp;#39; &amp;#39;张悦然&amp;#39; &amp;#39;经济&amp;#39;
 &amp;#39;小说&amp;#39; &amp;#39;科幻&amp;#39; &amp;#39;军事&amp;#39; &amp;#39;心理&amp;#39; &amp;#39;文学&amp;#39; &amp;#39;电影&amp;#39; &amp;#39;社会学&amp;#39; &amp;#39;广告&amp;#39; &amp;#39;管理&amp;#39; &amp;#39;励志&amp;#39; &amp;#39;耽美&amp;#39; &amp;#39;郭敬明&amp;#39; &amp;#39;穿越&amp;#39;
 &amp;#39;阿加莎·克里斯蒂&amp;#39; &amp;#39;杂文&amp;#39; &amp;#39;传记&amp;#39; &amp;#39;韩寒&amp;#39; &amp;#39;设计&amp;#39; &amp;#39;落落&amp;#39; &amp;#39;言情&amp;#39; &amp;#39;职场&amp;#39; &amp;#39;成长&amp;#39; &amp;#39;佛教&amp;#39; &amp;#39;女性&amp;#39; &amp;#39;政治&amp;#39; &amp;#39;近代史&amp;#39;
 &amp;#39;营销&amp;#39; &amp;#39;推理小说&amp;#39; &amp;#39;建筑&amp;#39; &amp;#39;经典&amp;#39; &amp;#39;外国名著&amp;#39; &amp;#39;二战&amp;#39; &amp;#39;鲁迅&amp;#39; &amp;#39;J.K.罗琳&amp;#39; &amp;#39;奇幻&amp;#39; &amp;#39;外国文学&amp;#39; &amp;#39;校园&amp;#39; &amp;#39;人物传记&amp;#39;
 &amp;#39;西方哲学&amp;#39; &amp;#39;自由主义&amp;#39; &amp;#39;文化&amp;#39; &amp;#39;旅行&amp;#39; &amp;#39;张小娴&amp;#39; &amp;#39;企业史&amp;#39; &amp;#39;国学&amp;#39; &amp;#39;摄影&amp;#39; &amp;#39;亦舒&amp;#39; &amp;#39;青春&amp;#39; &amp;#39;科学&amp;#39; &amp;#39;策划&amp;#39; &amp;#39;web&amp;#39;
 &amp;#39;创业&amp;#39; &amp;#39;美术&amp;#39; &amp;#39;宗教&amp;#39; &amp;#39;古龙&amp;#39; &amp;#39;沧月&amp;#39; &amp;#39;村上春树&amp;#39; &amp;#39;社会&amp;#39; &amp;#39;股票&amp;#39; &amp;#39;理财&amp;#39; &amp;#39;日本漫画&amp;#39; &amp;#39;轻小说&amp;#39; &amp;#39;数学&amp;#39; &amp;#39;神经网络&amp;#39;
 &amp;#39;网络小说&amp;#39; &amp;#39;当代文学&amp;#39; &amp;#39;中国历史&amp;#39; &amp;#39;三毛&amp;#39; &amp;#39;回忆录&amp;#39; &amp;#39;古典文学&amp;#39; &amp;#39;交互设计&amp;#39; &amp;#39;推理&amp;#39; &amp;#39;高木直子&amp;#39; &amp;#39;中国文学&amp;#39; &amp;#39;青春文学&amp;#39;
 &amp;#39;金庸&amp;#39; &amp;#39;UE&amp;#39; &amp;#39;投资&amp;#39; &amp;#39;编程&amp;#39; &amp;#39;几米&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;25--可视化&#34;&gt;2.5  可视化&lt;/h3&gt;
&lt;p&gt;书评发布数量随年份变化&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib_inline&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backend_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_matplotlib_formats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;png&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;svg&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;scienceplots&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;platform&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#初始化matplotlib汉化美化配置&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;science&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;no-latex&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;cjk-sc-font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;platform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取操作系统类型&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Windows&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;SimHei&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Darwin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Arial Unicode MS&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;sans-serif&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;font&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 设置全局字体&lt;/span&gt;


&lt;span class=&#34;c1&#34;&gt;#构造数据&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;date_series&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;volume_series&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Grouper&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;freq&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;M&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#这里的date， month_df都是特殊数据类型&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;date_series&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;volume_series&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;volume_by_time_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date_series&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;volume&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;volume_series&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;volume_by_time_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;volume_by_time_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;



&lt;span class=&#34;c1&#34;&gt;#开始绘图&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;volume_by_time_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
         &lt;span class=&#34;n&#34;&gt;volume_by_time_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;volume&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
         &lt;span class=&#34;n&#34;&gt;linestyle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;volume_by_time_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
            &lt;span class=&#34;n&#34;&gt;volume_by_time_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;volume&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
            &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;豆瓣读书随年份书评数量变化(2005.6.12 ~ 2018.10.13)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
          &lt;span class=&#34;n&#34;&gt;fontsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xlabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;日期&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;书评数量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;savefig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;plot.png&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dpi&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三相关内容&#34;&gt;三、相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/&#34;&gt;数据集 | 使用1000w条豆瓣影评训练Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四获取数据&#34;&gt;四、获取数据&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;douban-book&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/1qySKU_0dsoi1NAF9lQ971w?pwd=n5qe&#34;&gt;https://pan.baidu.com/s/1qySKU_0dsoi1NAF9lQ971w?pwd=n5qe&lt;/a&gt; 提取码: n5qe&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一豆瓣读书介绍">一、豆瓣读书介绍</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据集: douba-book

数据源: 豆瓣读书
   
记录数:
   - 标签 120 个
   - 书 17967 部
   - 书评 33941454 条
   
书评日期起止: 2005-06-12 ~ 2018-10-13
   
体积: 2.11G(解压后5.52G) 
</code></pre></div><p>该数据已经过初步清洗，可用于推荐系统、情感分析、知识图谱、社会学文化变迁等多个领域(或主题)。</p>
<p><br><br></p>
<h2 id="二查看数据">二、查看数据</h2>
<h3 id="21-读取数据">2.1 读取数据</h3>
<p>下载 <em><strong>douban_book.csv.gz</strong></em> 解压后，可以看到数据集中有一个 <em><strong>douban_book.csv</strong></em> 文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;douban_book.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">df</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">33941454
</code></pre></div><p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<br>
<h3 id="22-所含字段">2.2 所含字段</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39; - </span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> - tag          标签
 - book_name    书名
 - user_name    书评人
 - date         书评发布日期
 - comment      书评内容
 - star         评分(1-5)
 - vote_count   书评获赞数
</code></pre></div><br>
<h3 id="23--覆盖日期">2.3  覆盖日期</h3>
<p>书评发布日期覆盖(最早~ 最晚)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2005-06-12 00:00:00
2018-10-13 00:00:00
</code></pre></div><br>
<h3 id="24-标签">2.4 标签</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">nunique</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">tag</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">120

[&#39;思想&#39; &#39;科技&#39; &#39;金融&#39; &#39;政治学&#39; &#39;随笔&#39; &#39;爱情&#39; &#39;名著&#39; &#39;幾米&#39; &#39;人文&#39; &#39;交互&#39; &#39;悬疑&#39; &#39;算法&#39; &#39;哲学&#39; &#39;艺术史&#39;
 &#39;历史&#39; &#39;用户体验&#39; &#39;绘画&#39; &#39;诗词&#39; &#39;考古&#39; &#39;心理学&#39; &#39;互联网&#39; &#39;戏剧&#39; &#39;安妮宝贝&#39; &#39;艺术&#39; &#39;东野圭吾&#39; &#39;散文&#39; &#39;魔幻&#39;
 &#39;童话&#39; &#39;商业&#39; &#39;UCD&#39; &#39;日本文学&#39; &#39;武侠&#39; &#39;音乐&#39; &#39;通信&#39; &#39;科幻小说&#39; &#39;科普&#39; &#39;程序&#39; &#39;生活&#39; &#39;张悦然&#39; &#39;经济&#39;
 &#39;小说&#39; &#39;科幻&#39; &#39;军事&#39; &#39;心理&#39; &#39;文学&#39; &#39;电影&#39; &#39;社会学&#39; &#39;广告&#39; &#39;管理&#39; &#39;励志&#39; &#39;耽美&#39; &#39;郭敬明&#39; &#39;穿越&#39;
 &#39;阿加莎·克里斯蒂&#39; &#39;杂文&#39; &#39;传记&#39; &#39;韩寒&#39; &#39;设计&#39; &#39;落落&#39; &#39;言情&#39; &#39;职场&#39; &#39;成长&#39; &#39;佛教&#39; &#39;女性&#39; &#39;政治&#39; &#39;近代史&#39;
 &#39;营销&#39; &#39;推理小说&#39; &#39;建筑&#39; &#39;经典&#39; &#39;外国名著&#39; &#39;二战&#39; &#39;鲁迅&#39; &#39;J.K.罗琳&#39; &#39;奇幻&#39; &#39;外国文学&#39; &#39;校园&#39; &#39;人物传记&#39;
 &#39;西方哲学&#39; &#39;自由主义&#39; &#39;文化&#39; &#39;旅行&#39; &#39;张小娴&#39; &#39;企业史&#39; &#39;国学&#39; &#39;摄影&#39; &#39;亦舒&#39; &#39;青春&#39; &#39;科学&#39; &#39;策划&#39; &#39;web&#39;
 &#39;创业&#39; &#39;美术&#39; &#39;宗教&#39; &#39;古龙&#39; &#39;沧月&#39; &#39;村上春树&#39; &#39;社会&#39; &#39;股票&#39; &#39;理财&#39; &#39;日本漫画&#39; &#39;轻小说&#39; &#39;数学&#39; &#39;神经网络&#39;
 &#39;网络小说&#39; &#39;当代文学&#39; &#39;中国历史&#39; &#39;三毛&#39; &#39;回忆录&#39; &#39;古典文学&#39; &#39;交互设计&#39; &#39;推理&#39; &#39;高木直子&#39; &#39;中国文学&#39; &#39;青春文学&#39;
 &#39;金庸&#39; &#39;UE&#39; &#39;投资&#39; &#39;编程&#39; &#39;几米&#39;]
</code></pre></div><br>
<h3 id="25--可视化">2.5  可视化</h3>
<p>书评发布数量随年份变化</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">platform</span>

<span class="c1">#初始化matplotlib汉化美化配置</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>
<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>


<span class="c1">#构造数据</span>
<span class="n">date_series</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">volume_series</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">date</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Grouper</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;M&#39;</span><span class="p">)):</span>
    <span class="c1">#这里的date， month_df都是特殊数据类型</span>
    <span class="n">date_series</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">date</span><span class="o">.</span><span class="n">date</span><span class="p">())</span>
    <span class="n">volume_series</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">year_df</span><span class="p">))</span>
<span class="n">volume_by_time_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="n">date_series</span><span class="p">,</span> <span class="s1">&#39;volume&#39;</span><span class="p">:</span> <span class="n">volume_series</span><span class="p">})</span>
<span class="n">volume_by_time_df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">volume_by_time_df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">])</span>



<span class="c1">#开始绘图</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">volume_by_time_df</span><span class="o">.</span><span class="n">date</span><span class="p">,</span> 
         <span class="n">volume_by_time_df</span><span class="o">.</span><span class="n">volume</span><span class="p">,</span>
         <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">volume_by_time_df</span><span class="o">.</span><span class="n">date</span><span class="p">,</span> 
            <span class="n">volume_by_time_df</span><span class="o">.</span><span class="n">volume</span><span class="p">,</span> 
            <span class="n">s</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;豆瓣读书随年份书评数量变化(2005.6.12 ~ 2018.10.13)&#39;</span><span class="p">,</span> 
          <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;日期&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;书评数量&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;plot.png&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/plot.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三相关内容">三、相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/">数据集 | 使用1000w条豆瓣影评训练Word2Vec</a></li>
</ul>
<p><br><br></p>
<h2 id="四获取数据">四、获取数据</h2>
<p><em><strong>douban-book</strong></em> 链接: <a href="https://pan.baidu.com/s/1qySKU_0dsoi1NAF9lQ971w?pwd=n5qe">https://pan.baidu.com/s/1qySKU_0dsoi1NAF9lQ971w?pwd=n5qe</a> 提取码: n5qe</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 |  A股上市公司基本信息2000-2022</title>
      <link>https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/</link>
      <pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-16-china-listed-company-information-dataset/</guid>
      <description>A股上市公司基本信息</description>
      <content:encoded><![CDATA[<h2 id="一数据概况">一、数据概况</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据集: A股上市公司基本信息
年份: 2000-2022
公司数: 5368
记录数: 58153
用途: 可与年报、md&amp;a数据集进行并表
</code></pre></div><p><br><br></p>
<h2 id="二查看数据">二、查看数据</h2>
<h3 id="21-导入数据">2.1 导入数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;上市公司基本信息2000-2022.csv&#39;</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<p><br><br></p>
<p>如果股票代码中带的字母A别扭，可以剔除掉</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">Symbol</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">Symbol</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h3 id="22-查看字段">2.2 查看字段</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查看字段/含义</span>
<span class="n">max_col_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">])</span>
<span class="n">max_desc_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">desc</span><span class="p">))</span> <span class="k">for</span> <span class="n">desc</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;- 字段                   含义         缺失率&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">desc</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;- </span><span class="si">{</span><span class="n">col</span><span class="si">:</span><span class="s1">&lt;</span><span class="si">{</span><span class="n">max_col_len</span><span class="si">}}</span><span class="s1">   </span><span class="si">{</span><span class="n">desc</span><span class="si">:</span><span class="s1">&lt;</span><span class="si">{</span><span class="n">max_desc_len</span><span class="si">}}</span><span class="s1">     </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 字段                   含义         缺失率
- Symbol                股票代码         0.0%
- ShortName             股票简称         0.0%
- EndDate               统计截止日期       0.0%
- ListedCoID            上市公司ID       0.0%
- SecurityID            证券ID         0.0%
- IndustryName          行业名称         0.0%
- IndustryCode          行业代码         0.0%
- IndustryNameC         行业名称C        0.0%
- IndustryCodeC         行业代码C        0.0%
- RegisterAddress       注册具体地址       0.0%
- OfficeAddress         公司办公地址       0.0%
- Zipcode               办公地址邮政编码     0.0%
- Secretary             董事会秘书        0.1%
- SecretaryTel          董秘联系电话       0.1%
- SecretaryFax          董秘传真         0.6%
- SecretaryEmail        董秘电子邮箱       0.70%
- SecurityConsultant    证券事务代表       17.8%
- SocialCreditCode      统一社会信用代码     24.5%
- Sigchange             重大变更         5.5%
- Lng                   办公地经度        0.1%
- Lat                   办公地纬度        0.1%
- ISIN                  ISIN编码       0.5%
- FullName              中文全称         0.0%
- LegalRepresentative   法人代表         0.0%
- EstablishDate         公司成立日期       0.0%
- Crcd                  ABH股交叉码      93.8%
- RegisterCapital       注册资本         0.0%
- Website               公司网址         4.7%
- BusinessScope         经营范围         0.0%
- RegisterLongitude     注册地经度        0.2%
- RegisterLatitude      注册地纬度        0.2%
- EMAIL                 电子邮箱         0.70%
- LISTINGDATE           首次上市日期       0.0%
- PROVINCECODE          所属省份代码       0.0%
- PROVINCE              所属省份         0.0%
- CITYCODE              所属城市代码       0.2%
- CITY                  所属城市         0.0%
- MAINBUSSINESS         主营业务         0.0%
- LISTINGSTATE          上市状态         0.0%
</code></pre></div><br>
<h3 id="23-公司数">2.3 公司数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">Symbol</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">5368
</code></pre></div><br>
<br>
<h2 id="三增加其他数据集字段数量">三、增加其他数据集字段数量</h2>
<p><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/"><strong>数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析</strong></a> 只有 <em><strong>year</strong></em>、<em><strong>code</strong></em>、<em><strong>text</strong></em> 三个字段， 通过与本数据集合并操作(pd.merge) ，现在希望增加 <em><strong>EndDate</strong></em>、<em><strong>ShortName</strong></em>、<em><strong>IndustryCode</strong></em>、 <em><strong>RegisterAddress</strong></em> 四个字段。<br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">mda_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;mda01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">mda_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mda_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">mda_df</span>
</code></pre></div><p><img loading="lazy" src="img/mda.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#选择需要的字段进行读取</span>
<span class="n">info_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;Symbol&#39;</span><span class="p">,</span> <span class="s1">&#39;ShortName&#39;</span><span class="p">,</span> <span class="s1">&#39;EndDate&#39;</span><span class="p">,</span> <span class="s1">&#39;IndustryCode&#39;</span><span class="p">,</span> <span class="s1">&#39;RegisterAddress&#39;</span><span class="p">]]</span>

<span class="c1">#更改字段名Symbol为code</span>
<span class="n">info_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;Symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;code&#34;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">#根据EndDate计算会计年度year</span>
<span class="n">info_df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">info_df</span><span class="p">[</span><span class="s1">&#39;EndDate&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">y</span><span class="p">[:</span><span class="mi">4</span><span class="p">])</span>
<span class="n">info_df</span>
</code></pre></div><p><img loading="lazy" src="img/info_df.png" alt=""  />
</p>
<p><br><br>根据字段 <em><strong>year</strong></em>、<em><strong>code</strong></em> 进行合并，合并方式为内连接 <em><strong>inner</strong></em> ， 即两数据集的交集。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df_merge</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">mda_df</span><span class="p">,</span> <span class="n">info_df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;inner&#39;</span><span class="p">)</span>

<span class="c1">#保存</span>
<span class="c1">#df_merge.to_csv(&#39;合并后的数据.csv&#39;, index=False)</span>
<span class="c1">#df_merge.to_excel(&#39;合并后的数据.xlsx&#39;, index=False)</span>
<span class="n">df_merge</span>
</code></pre></div><p><img loading="lazy" src="img/merge.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三相关内容">三、相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/">数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析</a></li>
<li><a href="https://textdata.cn/blog/2023-01-06-mda_informative_content/">中国工业经济 | MD&amp;A信息含量指标构建代码实现</a></li>
<li><a href="https://textdata.cn/blog/2023-01-13-information-content-of-critical-audit/">金融研究 | 使用Python构建「关键审计事项信息含量」</a></li>
</ul>
<br>
<br>
<h2 id="四获取数据">四、获取数据</h2>
<p>整理不易， 50元 ， 加微信 372335839 ， 备注 「姓名-学校-专业」。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 |  使用1000w条豆瓣影评训练Word2Vec</title>
      <link>https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/</link>
      <pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/</guid>
      <description>&lt;p&gt;本文内容&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;介绍豆瓣影评数据集&lt;/li&gt;
&lt;li&gt;构造语料训练Word2Vec模型&lt;/li&gt;
&lt;li&gt;获取数据&amp;amp;cntext&amp;amp;Word2Vec模型文件&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一豆瓣影评数据集&#34;&gt;一、豆瓣影评数据集&lt;/h2&gt;
&lt;h3 id=&#34;11-数据集介绍&#34;&gt;1.1 数据集介绍&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据集: douba-movie-1000w

数据源: 豆瓣电影
   
记录数:
   - 电影 10269 部
   - 影评 10310989 条
   
体积: 1.35G 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;该数据集正好弥补下国内公开电影数据集的空缺， 数据已经过初步清洗，可用于推荐系统、情感分析、知识图谱、新闻传播学、社会学文化变迁等多个领域(或主题)。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;12-读取数据&#34;&gt;1.2 读取数据&lt;/h3&gt;
&lt;p&gt;下载 &lt;em&gt;&lt;strong&gt;douba-movie-1000w.zip&lt;/strong&gt;&lt;/em&gt; 解压后，可以看到数据集中有一个 &lt;em&gt;&lt;strong&gt;all_movies_with_id.csv&lt;/strong&gt;&lt;/em&gt; 文件。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;all_movies_with_id.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;13-所含字段&#34;&gt;1.3 所含字段&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;col&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; - &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt; - ID
 - Movie_Name  电影名
 - Score  豆瓣电影评分(1-10)
 - Review_People  评论者人数
 - Star_Distribution  评论评分分布(1-5, 含多个数值，数值以%间隔)
 - Craw_Date 爬虫运行日期
 - Username 豆瓣评论者用户名
 - Date 影评日期
 - Star  影评评分(1-5)
 - Comment 影评内容
 - Comment_Distribution 影评评分分布
 - Like 影评获得的喜欢数
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二-构造语料训练word2vec&#34;&gt;二、 构造语料&amp;amp;训练Word2Vec&lt;/h2&gt;
&lt;h3 id=&#34;21-构造语料&#34;&gt;2.1 构造语料&lt;/h3&gt;
&lt;p&gt;将字段 &lt;em&gt;&lt;strong&gt;Comment&lt;/strong&gt;&lt;/em&gt; 中所有文本汇总到 &lt;em&gt;&lt;strong&gt;douban-movie-1000w.txt&lt;/strong&gt;&lt;/em&gt;,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;douban-movie-1000w.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Comment&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-配置cntext211&#34;&gt;2.2 配置cntext2.1.1&lt;/h3&gt;
&lt;p&gt;将 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 放置于桌面，打开 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal) 中使用 &lt;em&gt;&lt;strong&gt;pip3&lt;/strong&gt;&lt;/em&gt; 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文末有 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 获取方式&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-训练word2vec&#34;&gt;2.3 训练Word2Vec&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#cntext为2.1.1&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2VModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;douban-movie-1000w.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Starting Preprocessing Corpus ...
Starting Training! This may take a while.Please be patient...
Traning word2vec model took 3965 seconds
Note: The Word2Vec model hase saved to output/Word2Vec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/word2vec.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;经过大概一个小时的训练， 得到模型文件 &lt;em&gt;&lt;strong&gt;douban-movie-1000w.200.6.bin&lt;/strong&gt;&lt;/em&gt; 及相关文件， 注意不要删掉哦。 已训练好的模型，可以自己用， 也可分享给其他人使用。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四使用word2vec&#34;&gt;四、使用Word2Vec&lt;/h2&gt;
&lt;h3 id=&#34;41-导入word2vec模型文件&#34;&gt;4.1 导入Word2Vec模型文件&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
 
&lt;span class=&#34;c1&#34;&gt;#导入模型，请注意路径。&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 【当前代码】 与 【Word2Vec文件夹】 同处于一个文件夹内&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Word2Vec/douban-movie-1000w.200.6.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Loading word2vec model...
&amp;lt;gensim.models.word2vec.Word2Vec at 0x10cb02090&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-常用函数&#34;&gt;4.2 常用函数&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;dm_w2v.wv.get_vector(key)&lt;/strong&gt;&lt;/em&gt; 获取key的词向量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;dm_w2v.most_similar_to_given(key1, keys_list)&lt;/strong&gt;&lt;/em&gt; 从 keys_list 中获取与 key1 最相似的词&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;dm_w2v.n_similarity(ws1, ws2)&lt;/strong&gt;&lt;/em&gt;  两组词ws1, ws2 的相似度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;dm_w2v.closer_than(key1, key2)&lt;/strong&gt;&lt;/em&gt;  更接近于key1的词向量(相比于key2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;dm_w2v.most_similar(positive, negative)&lt;/strong&gt;&lt;/em&gt;  找出与positive同方向，与negative反向相反的词。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&#34;421-get_vectorkey&#34;&gt;4.2.1 get_vector(key)&lt;/h4&gt;
&lt;p&gt;使用词向量查看某&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;3.55084002e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.22685611e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;8.48365605e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.23056602e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;1.35057056e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.65976137e-02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.26512849e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.47152972e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;9.99028236e-03&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.00873756e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.05153358e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.39181948e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;6.02373898e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.00308895e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;2.33978868e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.83010173e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;9.67333555e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;3.04877937e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;6.59058094e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;3.19660306e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.21165246e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;3.68000716e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2.36653373e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;6.83727741e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
      &lt;span class=&#34;o&#34;&gt;......&lt;/span&gt;
      &lt;span class=&#34;o&#34;&gt;......&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.23901594e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;5.07202707e-02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;8.75848413e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;4.31963325e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;1.31377324e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.19606090e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.68391216e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;6.27069890e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;7.37121344e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;2.49946609e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.47220814e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.33507824e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;2.97913142e-02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;4.91593599e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;5.83192170e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;8.48378658e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;3.30877733e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;2.17747837e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;2.22701088e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.00758147e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;3.41430195e-02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;7.27023900e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;7.94953525e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.03226733e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;4.55965906e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.66779244e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.16857982e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.02211344e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;4.11061406e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;8.95921767e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;9.48565483e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.48802996e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;9.36261594e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;3.98367733e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;3.12385857e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;8.67059827e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
      &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;float32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h4 id=&#34;422-most_similar_to_givenkey1-keys_list&#34;&gt;4.2.2 most_similar_to_given(key1, keys_list)&lt;/h4&gt;
&lt;p&gt;从 keys_list 中获取与 key1 最相似的词。例如在 1000w 影评中，从&lt;code&gt;&#39;爱情&#39;, &#39;悬疑&#39;, &#39;飞船&#39;, &#39;历史&#39;, &#39;战争&#39;&lt;/code&gt;找出最接近&lt;code&gt;&#39;太空&#39;&lt;/code&gt;，最后返回&lt;code&gt;&#39;飞船&#39;&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#从 `keys_list` 中获取与 `key1` 最相似的 `key`。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar_to_given&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;太空&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                                &lt;span class=&#34;n&#34;&gt;keys_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;爱情&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;悬疑&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;飞船&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;历史&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;战争&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&amp;#39;飞船&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h4 id=&#34;423-w2v_modeln_similarityws1-ws2&#34;&gt;4.2.3 w2v_model.n_similarity(ws1, ws2)&lt;/h4&gt;
&lt;p&gt;两组词ws1, ws2 的相似度。注意相似值更多的是体现了语义的相关性， 并不能准确反映语义的远近。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.metrics.pairwise&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cosine_similarity&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;理想&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)],&lt;/span&gt;  
                  &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;现实&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0.4698379
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#cosine算法&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;理想&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
                       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;现实&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0.4698379
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#计算两组键之间的余弦相似度。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;精彩&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;赞&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;推荐&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
                       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;无聊&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;尴尬&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;垃圾&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0.109311774
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;理想&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;梦想&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
                       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;现实&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;生活&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0.48020104
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h4 id=&#34;424-closer_thankey1-key2&#34;&gt;4.2.4 closer_than(key1, key2)&lt;/h4&gt;
&lt;p&gt;更接近于key1的词向量(相比于key2)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#获取所有更接近 `key1` 的键，而不是 `key2` 。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;closer_than&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;理想&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                      &lt;span class=&#34;n&#34;&gt;key2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;现实&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;梦想&amp;#39;,
 &amp;#39;追求&amp;#39;,
 &amp;#39;实现&amp;#39;,
 &amp;#39;向往&amp;#39;,
 &amp;#39;信念&amp;#39;,
 &amp;#39;妥协&amp;#39;,
 &amp;#39;奋斗&amp;#39;,
 &amp;#39;乌托邦&amp;#39;,
 &amp;#39;愿望&amp;#39;,
 &amp;#39;理想主义&amp;#39;,
 &amp;#39;理想化&amp;#39;,
 &amp;#39;虚幻&amp;#39;,
 &amp;#39;憧憬&amp;#39;,
 &amp;#39;现实残酷&amp;#39;,
 &amp;#39;不切实际&amp;#39;,
 &amp;#39;实现梦想&amp;#39;,
 &amp;#39;崇高&amp;#39;,
 &amp;#39;理想主义者&amp;#39;,
 &amp;#39;追求自由&amp;#39;,
 &amp;#39;破灭&amp;#39;,
 &amp;#39;名利&amp;#39;,
 &amp;#39;追梦&amp;#39;,
 &amp;#39;奢望&amp;#39;,
 &amp;#39;追求梦想&amp;#39;,
 &amp;#39;现实现实&amp;#39;,
 &amp;#39;执著&amp;#39;,
 &amp;#39;理想现实&amp;#39;,
 &amp;#39;拼搏&amp;#39;,
 &amp;#39;面对现实&amp;#39;,
 &amp;#39;美好事物&amp;#39;,
 &amp;#39;追逐梦想&amp;#39;,
 &amp;#39;勇往直前&amp;#39;,
 &amp;#39;遥不可及&amp;#39;,
 &amp;#39;怀揣&amp;#39;,
 &amp;#39;梦想现实&amp;#39;,
 &amp;#39;美好生活&amp;#39;,
 &amp;#39;脚踏实地&amp;#39;,
 &amp;#39;本心&amp;#39;,
 &amp;#39;坚持梦想&amp;#39;,
 &amp;#39;梦想实现&amp;#39;,
 &amp;#39;青春梦想&amp;#39;,
 &amp;#39;热忱&amp;#39;,
 &amp;#39;空想&amp;#39;,
 &amp;#39;抱负&amp;#39;,
 &amp;#39;努力奋斗&amp;#39;,
 &amp;#39;美好幻想&amp;#39;,
 &amp;#39;务实&amp;#39;,
 &amp;#39;坚定信念&amp;#39;,
 &amp;#39;梦想努力&amp;#39;,
 &amp;#39;理想国&amp;#39;,
 &amp;#39;无法实现&amp;#39;,
 &amp;#39;美好愿望&amp;#39;,
 &amp;#39;理想生活&amp;#39;,
 &amp;#39;坚持自我&amp;#39;,
 &amp;#39;事业爱情&amp;#39;,
 &amp;#39;放弃梦想&amp;#39;,
 &amp;#39;愿景&amp;#39;,
 &amp;#39;自我价值&amp;#39;,
 &amp;#39;自我实现&amp;#39;,
 &amp;#39;现实面前&amp;#39;,
 &amp;#39;梦想坚持&amp;#39;,
 &amp;#39;梦想梦想&amp;#39;,
 &amp;#39;志向&amp;#39;,
 &amp;#39;乌托邦式&amp;#39;,
 &amp;#39;可能实现&amp;#39;,
 &amp;#39;追寻梦想&amp;#39;,
 &amp;#39;追求自我&amp;#39;,
 &amp;#39;追求理想&amp;#39;,
 &amp;#39;人生理想&amp;#39;,
 &amp;#39;追求完美&amp;#39;,
 &amp;#39;诗远方&amp;#39;,
 &amp;#39;梦想追求&amp;#39;,
 &amp;#39;追求艺术&amp;#39;,
 &amp;#39;执着追求&amp;#39;,
 &amp;#39;不断努力&amp;#39;,
 &amp;#39;怀揣梦想&amp;#39;,
 &amp;#39;儿时梦想&amp;#39;,
 &amp;#39;最初梦想&amp;#39;,
 &amp;#39;梦想奋斗&amp;#39;,
 &amp;#39;曾经梦想&amp;#39;,
 &amp;#39;美好向往&amp;#39;,
 &amp;#39;理想状态&amp;#39;,
 &amp;#39;现实妥协&amp;#39;,
 &amp;#39;实现理想&amp;#39;,
 &amp;#39;梦想执着&amp;#39;,
 &amp;#39;坚持理想&amp;#39;,
 &amp;#39;一个理想主义者&amp;#39;,
 &amp;#39;不切实际幻想&amp;#39;,
 &amp;#39;实现不了&amp;#39;,
 &amp;#39;努力追求&amp;#39;,
 &amp;#39;精神追求&amp;#39;,
 &amp;#39;现实打败&amp;#39;,
 &amp;#39;过于理想&amp;#39;,
 &amp;#39;美好憧憬&amp;#39;,
 &amp;#39;追寻自由&amp;#39;,
 &amp;#39;美好愿景&amp;#39;,
 &amp;#39;远大&amp;#39;,
 &amp;#39;梦想破灭&amp;#39;,
 &amp;#39;美好未来&amp;#39;,
 &amp;#39;最终实现&amp;#39;,
 &amp;#39;现实主义者&amp;#39;,
 &amp;#39;心中理想&amp;#39;,
 &amp;#39;努力实现&amp;#39;,
 &amp;#39;理想追求&amp;#39;,
 &amp;#39;理想丰满&amp;#39;,
 &amp;#39;难以实现&amp;#39;,
 &amp;#39;自由梦想&amp;#39;,
 &amp;#39;未竟&amp;#39;,
 &amp;#39;理想信念&amp;#39;,
 &amp;#39;追名逐利&amp;#39;,
 &amp;#39;崇尚自由&amp;#39;,
 &amp;#39;理想奋斗&amp;#39;,
 &amp;#39;摇滚梦&amp;#39;,
 &amp;#39;心中梦想&amp;#39;,
 &amp;#39;梦想追逐&amp;#39;,
 &amp;#39;崇高理想&amp;#39;,
 &amp;#39;爱与梦想&amp;#39;,
 &amp;#39;梦想放弃&amp;#39;,
 &amp;#39;自由理想&amp;#39;,
 &amp;#39;远大理想&amp;#39;,
 &amp;#39;革命理想&amp;#39;,
 &amp;#39;勇于追求&amp;#39;,
 &amp;#39;世俗成功&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h4 id=&#34;425-most_similarpositive-negative&#34;&gt;4.2.5 most_similar(positive, negative)&lt;/h4&gt;
&lt;p&gt;找出与positive同方向，与negative反向相反的词。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;positive&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;精彩&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;过瘾&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                       &lt;span class=&#34;n&#34;&gt;negative&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;垃圾&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                       &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;看得过瘾&amp;#39;, 0.7470669746398926),
 (&amp;#39;相当精彩&amp;#39;, 0.7082503437995911),
 (&amp;#39;带劲&amp;#39;, 0.6865044236183167),
 (&amp;#39;非常过瘾&amp;#39;, 0.6556571125984192),
 (&amp;#39;非常精彩&amp;#39;, 0.6555824875831604),
 (&amp;#39;够劲&amp;#39;, 0.6424692869186401),
 (&amp;#39;太精彩&amp;#39;, 0.6424689292907715),
 (&amp;#39;十分精彩&amp;#39;, 0.6388185024261475),
 (&amp;#39;足够精彩&amp;#39;, 0.6384131908416748),
 (&amp;#39;十分过瘾&amp;#39;, 0.6383010745048523)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;43-类比king-manwomanqueen&#34;&gt;4.3 类比king-man+woman~queen&lt;/h3&gt;
&lt;p&gt;每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。&lt;/p&gt;
&lt;p&gt;这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/king-queen-formular.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;这两个词相减，按感觉应该得到的是性别方向，雄性-&amp;gt;雌性。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;gender_direction_1 = vector(man)-vector(woman)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;gender_direction_2 = vector(king)-vector(queen)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;那两个性别方向应该近似，假设这里将其 &lt;em&gt;&lt;strong&gt;gender_direction_1=gender_direction_2&lt;/strong&gt;&lt;/em&gt; ，则对于公式中任意一个词，都可以由等式中的其他三个词经过运算得到。例如&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;vector(queen) = vector(king)-vector(man)+vector(woman)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;这里构造了一个 &lt;code&gt;北京a - 中国b~=  巴黎c - 某国d&lt;/code&gt; 的公式，计算如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 北京a - 中国b~=  巴黎c - 某国d&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;北京&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;中国&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;巴黎&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#d = b-a+c&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;中国&amp;#39;, 0.6384854912757874),
 (&amp;#39;法国&amp;#39;, 0.599371612071991),
 (&amp;#39;欧洲&amp;#39;, 0.5970593094825745),
 (&amp;#39;法国人&amp;#39;, 0.5338885188102722),
 (&amp;#39;欧洲人&amp;#39;, 0.5236572027206421),
 (&amp;#39;意大利&amp;#39;, 0.5203548669815063),
 (&amp;#39;西方&amp;#39;, 0.4940629303455353),
 (&amp;#39;亚洲&amp;#39;, 0.4907427728176117),
 (&amp;#39;美国&amp;#39;, 0.490087628364563),
 (&amp;#39;欧美&amp;#39;, 0.48989546298980713)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;大概是跑出了我们预期的 &lt;strong&gt;法国&lt;/strong&gt;， 但不够Perfect， 有些遗憾。 毕竟语料是影评，且讨论环境不够正式， 豆瓣用户没那么多心思研究地理和政治，所以网络记忆不全不准。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五获取数据&#34;&gt;五、获取数据&lt;/h2&gt;
&lt;h3 id=&#34;51-获取影评数据&#34;&gt;5.1 获取影评数据&lt;/h3&gt;
&lt;p&gt;除了本文介绍的这个 1000w 条影评数据集， 大邓还有2个类似的豆瓣影评数据集，影评记录量 212w和442 w 条。 两个数据集下载链接我都公开，感兴趣的可以都下载下来。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;douba-movie-1000w&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/1NHttdosb0VZUQV7Tg7MHXw?pwd=rndk&#34;&gt;https://pan.baidu.com/s/1NHttdosb0VZUQV7Tg7MHXw?pwd=rndk&lt;/a&gt; 提取码: rndk&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;douban-movie-442w&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/10KK5FrGL0ZHx4wiuhlvuXw?pwd=db7m&#34;&gt;https://pan.baidu.com/s/10KK5FrGL0ZHx4wiuhlvuXw?pwd=db7m&lt;/a&gt; 提取码: db7m&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;【douban-movie-442w介绍】

采集时间: 
   - 电影&amp;amp;明星 2019年8月上旬
   - 影评(用户、评分、评论) 2019年9月初

记录数:
   - 电影 140502 部
   - 演员 72959 人
   - 影评 4428475 条
   - 评分 4169420 条
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;douban-movie-212w&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/1iCKGu_6zTe6ZhlB_9Bf1HA?pwd=cv2p&#34;&gt;https://pan.baidu.com/s/1iCKGu_6zTe6ZhlB_9Bf1HA?pwd=cv2p&lt;/a&gt; 提取码: cv2p&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;52-cntext211&#34;&gt;5.2 cntext2.1.1&lt;/h3&gt;
&lt;p&gt;cntext2.1.1 是非公开内容， &lt;strong&gt;100元&lt;/strong&gt;  可得 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt;  ， 加微信 &lt;em&gt;&lt;strong&gt;372335839&lt;/strong&gt;&lt;/em&gt;， 备注「姓名-学校-专业」&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;53-word2vec模型文件&#34;&gt;5.3 Word2Vec模型文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;douba-movie-1000w.200.6.bin&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/1ahbYq2IOqUA_AE0T3XIb9g?pwd=su1y&#34;&gt;https://pan.baidu.com/s/1ahbYq2IOqUA_AE0T3XIb9g?pwd=su1y&lt;/a&gt; 提取码: su1y&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;douban-movie-442w.200.6.bin&lt;/strong&gt;&lt;/em&gt;  链接: &lt;a href=&#34;https://pan.baidu.com/s/181eVuM0qldUJ53i7u1a5vA?pwd=uarj&#34;&gt;https://pan.baidu.com/s/181eVuM0qldUJ53i7u1a5vA?pwd=uarj&lt;/a&gt; 提取码: uarj&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;douban-movie-212w200.6.bin&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/1bvIZAM4zqX_35WHrBJSFUg?pwd=mf9u&#34;&gt;https://pan.baidu.com/s/1bvIZAM4zqX_35WHrBJSFUg?pwd=mf9u&lt;/a&gt; 提取码: mf9u&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/&#34;&gt;数据集 | 3394w条豆瓣书评数据集&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>本文内容</p>
<ol>
<li>介绍豆瓣影评数据集</li>
<li>构造语料训练Word2Vec模型</li>
<li>获取数据&amp;cntext&amp;Word2Vec模型文件</li>
</ol>
<p><br><br></p>
<h2 id="一豆瓣影评数据集">一、豆瓣影评数据集</h2>
<h3 id="11-数据集介绍">1.1 数据集介绍</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据集: douba-movie-1000w

数据源: 豆瓣电影
   
记录数:
   - 电影 10269 部
   - 影评 10310989 条
   
体积: 1.35G 
</code></pre></div><p>该数据集正好弥补下国内公开电影数据集的空缺， 数据已经过初步清洗，可用于推荐系统、情感分析、知识图谱、新闻传播学、社会学文化变迁等多个领域(或主题)。</p>
<br>
<h3 id="12-读取数据">1.2 读取数据</h3>
<p>下载 <em><strong>douba-movie-1000w.zip</strong></em> 解压后，可以看到数据集中有一个 <em><strong>all_movies_with_id.csv</strong></em> 文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;all_movies_with_id.csv&#39;</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<br>
<h3 id="13-所含字段">1.3 所含字段</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39; - </span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> - ID
 - Movie_Name  电影名
 - Score  豆瓣电影评分(1-10)
 - Review_People  评论者人数
 - Star_Distribution  评论评分分布(1-5, 含多个数值，数值以%间隔)
 - Craw_Date 爬虫运行日期
 - Username 豆瓣评论者用户名
 - Date 影评日期
 - Star  影评评分(1-5)
 - Comment 影评内容
 - Comment_Distribution 影评评分分布
 - Like 影评获得的喜欢数
</code></pre></div><p><br><br></p>
<h2 id="二-构造语料训练word2vec">二、 构造语料&amp;训练Word2Vec</h2>
<h3 id="21-构造语料">2.1 构造语料</h3>
<p>将字段 <em><strong>Comment</strong></em> 中所有文本汇总到 <em><strong>douban-movie-1000w.txt</strong></em>,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;douban-movie-1000w.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Comment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="22-配置cntext211">2.2 配置cntext2.1.1</h3>
<p>将 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
</code></pre></div><p>文末有 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 获取方式</p>
<br>
<h3 id="23-训练word2vec">2.3 训练Word2Vec</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cntext为2.1.1</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;douban-movie-1000w.txt&#39;</span><span class="p">,</span>
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>

<span class="n">w2v_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Starting Preprocessing Corpus ...
Starting Training! This may take a while.Please be patient...
Traning word2vec model took 3965 seconds
Note: The Word2Vec model hase saved to output/Word2Vec
</code></pre></div><p><img loading="lazy" src="img/word2vec.png" alt=""  />
</p>
<p>经过大概一个小时的训练， 得到模型文件 <em><strong>douban-movie-1000w.200.6.bin</strong></em> 及相关文件， 注意不要删掉哦。 已训练好的模型，可以自己用， 也可分享给其他人使用。</p>
<p><br><br></p>
<h2 id="四使用word2vec">四、使用Word2Vec</h2>
<h3 id="41-导入word2vec模型文件">4.1 导入Word2Vec模型文件</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
 
<span class="c1">#导入模型，请注意路径。</span>
<span class="c1"># 【当前代码】 与 【Word2Vec文件夹】 同处于一个文件夹内</span>
<span class="n">dm_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;Word2Vec/douban-movie-1000w.200.6.bin&#39;</span><span class="p">)</span>
<span class="n">dm_w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading word2vec model...
&lt;gensim.models.word2vec.Word2Vec at 0x10cb02090&gt;
</code></pre></div><br>
<h3 id="42-常用函数">4.2 常用函数</h3>
<ul>
<li>
<p><em><strong>dm_w2v.wv.get_vector(key)</strong></em> 获取key的词向量</p>
</li>
<li>
<p><em><strong>dm_w2v.most_similar_to_given(key1, keys_list)</strong></em> 从 keys_list 中获取与 key1 最相似的词</p>
</li>
<li>
<p><em><strong>dm_w2v.n_similarity(ws1, ws2)</strong></em>  两组词ws1, ws2 的相似度</p>
</li>
<li>
<p><em><strong>dm_w2v.closer_than(key1, key2)</strong></em>  更接近于key1的词向量(相比于key2)</p>
</li>
<li>
<p><em><strong>dm_w2v.most_similar(positive, negative)</strong></em>  找出与positive同方向，与negative反向相反的词。</p>
</li>
</ul>
<br>
<h4 id="421-get_vectorkey">4.2.1 get_vector(key)</h4>
<p>使用词向量查看某</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">3.55084002e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.22685611e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.48365605e-01</span><span class="p">,</span>  <span class="mf">1.23056602e+00</span><span class="p">,</span>
        <span class="mf">1.35057056e+00</span><span class="p">,</span>  <span class="mf">1.65976137e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.26512849e+00</span><span class="p">,</span>  <span class="mf">1.47152972e+00</span><span class="p">,</span>
        <span class="mf">9.99028236e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.00873756e+00</span><span class="p">,</span>  <span class="mf">1.05153358e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.39181948e+00</span><span class="p">,</span>
        <span class="mf">6.02373898e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.00308895e+00</span><span class="p">,</span>  <span class="mf">2.33978868e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.83010173e+00</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">9.67333555e-01</span><span class="p">,</span>  <span class="mf">3.04877937e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.59058094e-01</span><span class="p">,</span>  <span class="mf">3.19660306e+00</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">1.21165246e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.68000716e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.36653373e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.83727741e-01</span><span class="p">,</span>
      <span class="o">......</span>
      <span class="o">......</span>
       <span class="o">-</span><span class="mf">1.23901594e+00</span><span class="p">,</span>  <span class="mf">5.07202707e-02</span><span class="p">,</span>  <span class="mf">8.75848413e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.31963325e-01</span><span class="p">,</span>
        <span class="mf">1.31377324e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.19606090e+00</span><span class="p">,</span>  <span class="mf">1.68391216e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.27069890e-01</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">7.37121344e-01</span><span class="p">,</span>  <span class="mf">2.49946609e-01</span><span class="p">,</span>  <span class="mf">1.47220814e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.33507824e+00</span><span class="p">,</span>
        <span class="mf">2.97913142e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.91593599e-01</span><span class="p">,</span>  <span class="mf">5.83192170e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.48378658e-01</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">3.30877733e+00</span><span class="p">,</span>  <span class="mf">2.17747837e-01</span><span class="p">,</span>  <span class="mf">2.22701088e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.00758147e+00</span><span class="p">,</span>
        <span class="mf">3.41430195e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.27023900e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.94953525e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.03226733e+00</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">4.55965906e-01</span><span class="p">,</span>  <span class="mf">1.66779244e+00</span><span class="p">,</span>  <span class="mf">1.16857982e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.02211344e+00</span><span class="p">,</span>
        <span class="mf">4.11061406e-01</span><span class="p">,</span>  <span class="mf">8.95921767e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.48565483e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.48802996e-01</span><span class="p">,</span>
        <span class="mf">9.36261594e-01</span><span class="p">,</span>  <span class="mf">3.98367733e-01</span><span class="p">,</span>  <span class="mf">3.12385857e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.67059827e-01</span><span class="p">],</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</code></pre></div><br>
<h4 id="422-most_similar_to_givenkey1-keys_list">4.2.2 most_similar_to_given(key1, keys_list)</h4>
<p>从 keys_list 中获取与 key1 最相似的词。例如在 1000w 影评中，从<code>'爱情', '悬疑', '飞船', '历史', '战争'</code>找出最接近<code>'太空'</code>，最后返回<code>'飞船'</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#从 `keys_list` 中获取与 `key1` 最相似的 `key`。</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar_to_given</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;太空&#39;</span><span class="p">,</span> 
                                <span class="n">keys_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;爱情&#39;</span><span class="p">,</span> <span class="s1">&#39;悬疑&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;历史&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&#39;飞船&#39;
</code></pre></div><br>
<h4 id="423-w2v_modeln_similarityws1-ws2">4.2.3 w2v_model.n_similarity(ws1, ws2)</h4>
<p>两组词ws1, ws2 的相似度。注意相似值更多的是体现了语义的相关性， 并不能准确反映语义的远近。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="n">cosine_similarity</span><span class="p">([</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;理想&#39;</span><span class="p">)],</span>  
                  <span class="p">[</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;现实&#39;</span><span class="p">)])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.4698379
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cosine算法</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.4698379
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#计算两组键之间的余弦相似度。</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;赞&#39;</span><span class="p">,</span> <span class="s1">&#39;推荐&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;无聊&#39;</span><span class="p">,</span> <span class="s1">&#39;尴尬&#39;</span><span class="p">,</span> <span class="s1">&#39;垃圾&#39;</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.109311774
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> <span class="s1">&#39;梦想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">,</span> <span class="s1">&#39;生活&#39;</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.48020104
</code></pre></div><br>
<h4 id="424-closer_thankey1-key2">4.2.4 closer_than(key1, key2)</h4>
<p>更接近于key1的词向量(相比于key2)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取所有更接近 `key1` 的键，而不是 `key2` 。</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">closer_than</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> 
                      <span class="n">key2</span><span class="o">=</span><span class="s1">&#39;现实&#39;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;梦想&#39;,
 &#39;追求&#39;,
 &#39;实现&#39;,
 &#39;向往&#39;,
 &#39;信念&#39;,
 &#39;妥协&#39;,
 &#39;奋斗&#39;,
 &#39;乌托邦&#39;,
 &#39;愿望&#39;,
 &#39;理想主义&#39;,
 &#39;理想化&#39;,
 &#39;虚幻&#39;,
 &#39;憧憬&#39;,
 &#39;现实残酷&#39;,
 &#39;不切实际&#39;,
 &#39;实现梦想&#39;,
 &#39;崇高&#39;,
 &#39;理想主义者&#39;,
 &#39;追求自由&#39;,
 &#39;破灭&#39;,
 &#39;名利&#39;,
 &#39;追梦&#39;,
 &#39;奢望&#39;,
 &#39;追求梦想&#39;,
 &#39;现实现实&#39;,
 &#39;执著&#39;,
 &#39;理想现实&#39;,
 &#39;拼搏&#39;,
 &#39;面对现实&#39;,
 &#39;美好事物&#39;,
 &#39;追逐梦想&#39;,
 &#39;勇往直前&#39;,
 &#39;遥不可及&#39;,
 &#39;怀揣&#39;,
 &#39;梦想现实&#39;,
 &#39;美好生活&#39;,
 &#39;脚踏实地&#39;,
 &#39;本心&#39;,
 &#39;坚持梦想&#39;,
 &#39;梦想实现&#39;,
 &#39;青春梦想&#39;,
 &#39;热忱&#39;,
 &#39;空想&#39;,
 &#39;抱负&#39;,
 &#39;努力奋斗&#39;,
 &#39;美好幻想&#39;,
 &#39;务实&#39;,
 &#39;坚定信念&#39;,
 &#39;梦想努力&#39;,
 &#39;理想国&#39;,
 &#39;无法实现&#39;,
 &#39;美好愿望&#39;,
 &#39;理想生活&#39;,
 &#39;坚持自我&#39;,
 &#39;事业爱情&#39;,
 &#39;放弃梦想&#39;,
 &#39;愿景&#39;,
 &#39;自我价值&#39;,
 &#39;自我实现&#39;,
 &#39;现实面前&#39;,
 &#39;梦想坚持&#39;,
 &#39;梦想梦想&#39;,
 &#39;志向&#39;,
 &#39;乌托邦式&#39;,
 &#39;可能实现&#39;,
 &#39;追寻梦想&#39;,
 &#39;追求自我&#39;,
 &#39;追求理想&#39;,
 &#39;人生理想&#39;,
 &#39;追求完美&#39;,
 &#39;诗远方&#39;,
 &#39;梦想追求&#39;,
 &#39;追求艺术&#39;,
 &#39;执着追求&#39;,
 &#39;不断努力&#39;,
 &#39;怀揣梦想&#39;,
 &#39;儿时梦想&#39;,
 &#39;最初梦想&#39;,
 &#39;梦想奋斗&#39;,
 &#39;曾经梦想&#39;,
 &#39;美好向往&#39;,
 &#39;理想状态&#39;,
 &#39;现实妥协&#39;,
 &#39;实现理想&#39;,
 &#39;梦想执着&#39;,
 &#39;坚持理想&#39;,
 &#39;一个理想主义者&#39;,
 &#39;不切实际幻想&#39;,
 &#39;实现不了&#39;,
 &#39;努力追求&#39;,
 &#39;精神追求&#39;,
 &#39;现实打败&#39;,
 &#39;过于理想&#39;,
 &#39;美好憧憬&#39;,
 &#39;追寻自由&#39;,
 &#39;美好愿景&#39;,
 &#39;远大&#39;,
 &#39;梦想破灭&#39;,
 &#39;美好未来&#39;,
 &#39;最终实现&#39;,
 &#39;现实主义者&#39;,
 &#39;心中理想&#39;,
 &#39;努力实现&#39;,
 &#39;理想追求&#39;,
 &#39;理想丰满&#39;,
 &#39;难以实现&#39;,
 &#39;自由梦想&#39;,
 &#39;未竟&#39;,
 &#39;理想信念&#39;,
 &#39;追名逐利&#39;,
 &#39;崇尚自由&#39;,
 &#39;理想奋斗&#39;,
 &#39;摇滚梦&#39;,
 &#39;心中梦想&#39;,
 &#39;梦想追逐&#39;,
 &#39;崇高理想&#39;,
 &#39;爱与梦想&#39;,
 &#39;梦想放弃&#39;,
 &#39;自由理想&#39;,
 &#39;远大理想&#39;,
 &#39;革命理想&#39;,
 &#39;勇于追求&#39;,
 &#39;世俗成功&#39;]
</code></pre></div><br>
<h4 id="425-most_similarpositive-negative">4.2.5 most_similar(positive, negative)</h4>
<p>找出与positive同方向，与negative反向相反的词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;过瘾&#39;</span><span class="p">],</span>
                       <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;垃圾&#39;</span><span class="p">],</span>
                       <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;看得过瘾&#39;, 0.7470669746398926),
 (&#39;相当精彩&#39;, 0.7082503437995911),
 (&#39;带劲&#39;, 0.6865044236183167),
 (&#39;非常过瘾&#39;, 0.6556571125984192),
 (&#39;非常精彩&#39;, 0.6555824875831604),
 (&#39;够劲&#39;, 0.6424692869186401),
 (&#39;太精彩&#39;, 0.6424689292907715),
 (&#39;十分精彩&#39;, 0.6388185024261475),
 (&#39;足够精彩&#39;, 0.6384131908416748),
 (&#39;十分过瘾&#39;, 0.6383010745048523)]
</code></pre></div><br>
<h3 id="43-类比king-manwomanqueen">4.3 类比king-man+woman~queen</h3>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。</p>
<p>这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p><img loading="lazy" src="img/king-queen-formular.png" alt=""  />
</p>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p><em><strong>gender_direction_1 = vector(man)-vector(woman)</strong></em></p>
<p><em><strong>gender_direction_2 = vector(king)-vector(queen)</strong></em></p>
<p>那两个性别方向应该近似，假设这里将其 <em><strong>gender_direction_1=gender_direction_2</strong></em> ，则对于公式中任意一个词，都可以由等式中的其他三个词经过运算得到。例如</p>
<p><em><strong>vector(queen) = vector(king)-vector(man)+vector(woman)</strong></em></p>
<p>这里构造了一个 <code>北京a - 中国b~=  巴黎c - 某国d</code> 的公式，计算如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 北京a - 中国b~=  巴黎c - 某国d</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;北京&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;中国&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;巴黎&#39;</span><span class="p">)</span>

<span class="c1">#d = b-a+c</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;中国&#39;, 0.6384854912757874),
 (&#39;法国&#39;, 0.599371612071991),
 (&#39;欧洲&#39;, 0.5970593094825745),
 (&#39;法国人&#39;, 0.5338885188102722),
 (&#39;欧洲人&#39;, 0.5236572027206421),
 (&#39;意大利&#39;, 0.5203548669815063),
 (&#39;西方&#39;, 0.4940629303455353),
 (&#39;亚洲&#39;, 0.4907427728176117),
 (&#39;美国&#39;, 0.490087628364563),
 (&#39;欧美&#39;, 0.48989546298980713)]
</code></pre></div><p>大概是跑出了我们预期的 <strong>法国</strong>， 但不够Perfect， 有些遗憾。 毕竟语料是影评，且讨论环境不够正式， 豆瓣用户没那么多心思研究地理和政治，所以网络记忆不全不准。</p>
<p><br><br></p>
<h2 id="五获取数据">五、获取数据</h2>
<h3 id="51-获取影评数据">5.1 获取影评数据</h3>
<p>除了本文介绍的这个 1000w 条影评数据集， 大邓还有2个类似的豆瓣影评数据集，影评记录量 212w和442 w 条。 两个数据集下载链接我都公开，感兴趣的可以都下载下来。</p>
<ul>
<li>
<p><em><strong>douba-movie-1000w</strong></em> 链接: <a href="https://pan.baidu.com/s/1NHttdosb0VZUQV7Tg7MHXw?pwd=rndk">https://pan.baidu.com/s/1NHttdosb0VZUQV7Tg7MHXw?pwd=rndk</a> 提取码: rndk</p>
</li>
<li>
<p><em><strong>douban-movie-442w</strong></em> 链接: <a href="https://pan.baidu.com/s/10KK5FrGL0ZHx4wiuhlvuXw?pwd=db7m">https://pan.baidu.com/s/10KK5FrGL0ZHx4wiuhlvuXw?pwd=db7m</a> 提取码: db7m</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">【douban-movie-442w介绍】

采集时间: 
   - 电影&amp;明星 2019年8月上旬
   - 影评(用户、评分、评论) 2019年9月初

记录数:
   - 电影 140502 部
   - 演员 72959 人
   - 影评 4428475 条
   - 评分 4169420 条
</code></pre></div><ul>
<li><em><strong>douban-movie-212w</strong></em> 链接: <a href="https://pan.baidu.com/s/1iCKGu_6zTe6ZhlB_9Bf1HA?pwd=cv2p">https://pan.baidu.com/s/1iCKGu_6zTe6ZhlB_9Bf1HA?pwd=cv2p</a> 提取码: cv2p</li>
</ul>
<p><br><br></p>
<h3 id="52-cntext211">5.2 cntext2.1.1</h3>
<p>cntext2.1.1 是非公开内容， <strong>100元</strong>  可得 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em>  ， 加微信 <em><strong>372335839</strong></em>， 备注「姓名-学校-专业」</p>
<br>
<h3 id="53-word2vec模型文件">5.3 Word2Vec模型文件</h3>
<ul>
<li><em><strong>douba-movie-1000w.200.6.bin</strong></em> 链接: <a href="https://pan.baidu.com/s/1ahbYq2IOqUA_AE0T3XIb9g?pwd=su1y">https://pan.baidu.com/s/1ahbYq2IOqUA_AE0T3XIb9g?pwd=su1y</a> 提取码: su1y</li>
<li><em><strong>douban-movie-442w.200.6.bin</strong></em>  链接: <a href="https://pan.baidu.com/s/181eVuM0qldUJ53i7u1a5vA?pwd=uarj">https://pan.baidu.com/s/181eVuM0qldUJ53i7u1a5vA?pwd=uarj</a> 提取码: uarj</li>
<li><em><strong>douban-movie-212w200.6.bin</strong></em> 链接: <a href="https://pan.baidu.com/s/1bvIZAM4zqX_35WHrBJSFUg?pwd=mf9u">https://pan.baidu.com/s/1bvIZAM4zqX_35WHrBJSFUg?pwd=mf9u</a> 提取码: mf9u</li>
</ul>
<br>
<br>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/">数据集 | 3394w条豆瓣书评数据集</a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文献&amp;代码 | 使用Python计算语义品牌评分(Semantic Brand Score)</title>
      <link>https://textdata.cn/blog/2024-04-12-semantic-brand-score/</link>
      <pubDate>Fri, 12 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-12-semantic-brand-score/</guid>
      <description>Semantic Brand Score</description>
      <content:encoded><![CDATA[<h2 id="一语义品牌评分">一、语义品牌评分</h2>
<p><strong>语义品牌评分(SBS)</strong>  是一种新颖的指标，可以通过文本语料，衡量(评估)不同环境下一个或多个品牌的 <strong>品牌重要性</strong>。 <br></p>
<blockquote>
<p>Colladon, Andrea Fronzetti. &ldquo;<em><strong>The semantic brand score</strong></em>.&rdquo; <em>Journal of Business Research</em> 88 (2018): 150-160.</p>
</blockquote>
<br>
<p>相对于一些传统措施的优点是，SBS 不依赖于对小样本消费者进行的调查，可以捕捉到真实可信的信号 。该度量可以<strong>对任意来源的文本进行计算</strong>， 例如报纸文章、电子邮件、推文、在线论坛、博客和社交媒体上的帖子。  如果研究景点品牌的重要性，可以从消费者或其他品牌利益相关者通常出现的地方（例如旅游论坛）收集他们的发表的信息。这样做的优点是可以减少因使用问卷而引起的偏见，因为受访者知道他们正在被观察。 SBS 还可以适应不同的语言，并研究特定单词或单词集（不一定是“品牌”）的重要性。</p>
<p>通过 “品牌”，人们可以指政治家的名字，或者代表一个概念的一组单词（例如，“创新”的概念或企业核心价值）。该措施用于评估新品牌取代旧品牌时发生的过渡动态。语义品牌评分还可用于将品牌的重要性与其竞争对手的重要性联系起来，或分析单个品牌的重要性时间趋势。在某些应用中，事实证明该分数对于预测目的很有用。例如，人们发现在线媒体中政治候选人的品牌重要性与选举结果之间存在联系，或者景点品牌的重要性与游客数量趋势之间存在联系。</p>
<p><img loading="lazy" src="img/sbs-trend-plot.jpg" alt=""  />
</p>
<p><br><br></p>
<h2 id="二品牌重要性的三个维度">二、品牌重要性的三个维度</h2>
<p>SBS 衡量 <strong>品牌重要性</strong> ，这是品牌资产的基础(Fronzetti Colladon， 2018)。事实上，该指标的部分灵感来自于众所周知的品牌资产概念以及品牌形象和品牌意识的构建（Keller, 1993）。 品牌重要性通过三个维度来衡量：<strong>流行度</strong>、<strong>多样性</strong> 和 <strong>连通性</strong>。</p>
<ul>
<li><strong>流行度(Prevalence)</strong>   衡量品牌名称的使用频率，即直接提及品牌的次数。</li>
<li><strong>多样性(Diversity)</strong>  衡量与品牌相关的词语的多样性。</li>
<li><strong>连接性(Connectivity)</strong>  代表品牌在其他单词或单词组（有时被视为话语主题）之间建立联系的能力。</li>
</ul>
<p><br><br></p>
<h2 id="三文本分析步骤">三、文本分析步骤</h2>
<p><strong>语义品牌得分(SBS)</strong>  的计算需要结合文本挖掘和社交网络分析的方法和工具。下图说明了主要的初步步骤，包括数据收集、文本预处理和单词共现网络的构建。</p>
<p><img loading="lazy" src="img/text-preprocess.jpg" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1. 准备文本数据
2. 文本预处理(剔除标点符号、剔除特殊字符、剔除html标签、剔除#@等符号、剔除停用词)
3. 英文小写、分词、合并同类项(类似于is、was、are都合并到be)
4. 从文本信息中构建共现语义网络(确定词语上下文范围，涉及到co-range， 默认co-range=7)
5. 剔除贡献语义网络中不重要的边(联系， 涉及到参数link_filter， 默认link_filter=2))
</code></pre></div><br>
<br>
<h2 id="四实验">四、实验</h2>
<p>以三体为例，分析小说中5个角色的语义品牌评分（类比于文本中分析品牌的重要性） 。我们将小说等分为20分，希望得到角色语义品牌评分随着小说进度的变化趋势。</p>
<p><img loading="lazy" src="img/plot.png" alt=""  />
</p>
<br>
<h3 id="41-读取数据">4.1 读取数据</h3>
<p>三体小说2.5M</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="k">def</span> <span class="nf">read_txt</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">):</span>
    <span class="c1"># 读取txt文件</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s2">&#34;r&#34;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    
    <span class="c1"># 获取文本的总长度和每一段的长度</span>
    <span class="n">total_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">segment_length</span> <span class="o">=</span> <span class="n">total_length</span> <span class="o">//</span> <span class="n">num_segments</span>
    
    <span class="c1"># 将文本分割成指定数量的段落</span>
    <span class="n">segments</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_segments</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">segment_length</span>
        <span class="n">end</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">segment_length</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">num_segments</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">total_length</span>
        <span class="n">segment</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>
        <span class="n">segments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">segment</span><span class="p">)</span>

    <span class="c1"># 将内容存储在数据框中</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">segments</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;docs&#34;</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">df</span>


<span class="c1">#分成20份</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">read_txt</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;三体全集.txt&#39;</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<br>
<h3 id="42-计算sbs">4.2 计算SBS</h3>
<p>语义品牌评分SBS已经封装到  <em><strong>cntext2.1.1</strong></em> 中， 文末有 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 获取方式 。</p>
<br>
<h4 id="421-安装cntext211">4.2.1 安装cntext2.1.1</h4>
<p>将 cntext-2.1.1-py3-none-any.whl 放置于桌面，打开 cmd (苹果电脑打开terminal)， 输入 cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><br>
<p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
</code></pre></div><p><br><br></p>
<h4 id="422--开始计算">4.2.2  开始计算</h4>
<p><em><strong>2.7M</strong></em> 的三体小说文本，全部运行下来大概 10-20min ，可见SBS计算非常慢， 所以为了省时间，我们先以三体小说第一份（等分20份中的第一份）做个小实验。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">brands</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;汪淼&#39;</span><span class="p">,</span> <span class="s1">&#39;史强&#39;</span><span class="p">,</span> <span class="s1">&#39;罗辑&#39;</span><span class="p">,</span> <span class="s1">&#39;叶文洁&#39;</span><span class="p">,</span> <span class="s1">&#39;伊文斯&#39;</span><span class="p">]</span>

<span class="c1">#小说第一份文本（等分20份中的第一份）</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;docs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1">#如果不用三体， 只想分析某个txt，以data.txt为例</span>
<span class="c1">#text = open(&#39;data.txt&#39;).read()</span>

<span class="n">sbs_df0</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_brand_score</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
                               <span class="n">brands</span><span class="o">=</span><span class="n">brands</span><span class="p">,</span> 
                               <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
<span class="n">sbs_df0</span><span class="p">[</span><span class="s1">&#39;doc_idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">sbs_df0</span>
</code></pre></div><p><img loading="lazy" src="img/02-df.png" alt=""  />
</p>
<p><br> 运行没出现问题， 现在我们对整个小说进行实验，计算五个角色的 SBS 随时间变化。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>  <span class="c1">#记录时间</span>

<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">brands</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;汪淼&#39;</span><span class="p">,</span> <span class="s1">&#39;史强&#39;</span><span class="p">,</span> <span class="s1">&#39;罗辑&#39;</span><span class="p">,</span> <span class="s1">&#39;叶文洁&#39;</span><span class="p">,</span> <span class="s1">&#39;伊文斯&#39;</span><span class="p">]</span>
<span class="n">sbs_dfs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">text</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;docs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
    <span class="n">sbs_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_brand_score</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
                              <span class="n">brands</span><span class="o">=</span><span class="n">brands</span><span class="p">,</span> 
                              <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
    <span class="n">sbs_df</span><span class="p">[</span><span class="s1">&#39;doc_idx&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
    <span class="n">sbs_dfs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sbs_df</span><span class="p">)</span>
    
<span class="n">SBS_DFs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">sbs_dfs</span><span class="p">)</span>
<span class="n">SBS_DFs</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0
WARNING: Loops will be ignored.
1
WARNING: Loops will be ignored.
2
WARNING: Loops will be ignored.
3
WARNING: Loops will be ignored.
4
WARNING: Loops will be ignored.
5
WARNING: Loops will be ignored.
6
WARNING: Loops will be ignored.
7
WARNING: Loops will be ignored.
8
WARNING: Loops will be ignored.
9
WARNING: Loops will be ignored.
10
WARNING: Loops will be ignored.
11
WARNING: Loops will be ignored.
12
WARNING: Loops will be ignored.
13
WARNING: Loops will be ignored.
14
WARNING: Loops will be ignored.
15
WARNING: Loops will be ignored.
16
WARNING: Loops will be ignored.
17
WARNING: Loops will be ignored.
18
WARNING: Loops will be ignored.
19
WARNING: Loops will be ignored.

CPU times: user 10min 9s, sys: 8.53 s, total: 10min 17s
Wall time: 10min 19s
</code></pre></div><p><img loading="lazy" src="img/03-df.png" alt=""  />
</p>
<br>
<h3 id="43-可视化sbs">4.3 可视化SBS</h3>
<p>可视化三体小说五个角色重要性（语义品牌评分， SBS）随时间 (文本字符位置) 变化趋势</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">SBS_DFs</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">SBS_DFs</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="s1">&#39;Brand&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">SBS_DFs</span>
</code></pre></div><p><img loading="lazy" src="img/04-df.png" alt=""  />

<br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>

<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>


<span class="k">for</span> <span class="n">brand</span><span class="p">,</span> <span class="n">brand_df</span> <span class="ow">in</span> <span class="n">SBS_DFs</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Brand&#39;</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">brand_df</span><span class="o">.</span><span class="n">doc_idx</span><span class="p">,</span> <span class="n">brand_df</span><span class="o">.</span><span class="n">SBS</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">brand</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">brand_df</span><span class="o">.</span><span class="n">doc_idx</span><span class="p">,</span> <span class="n">brand_df</span><span class="o">.</span><span class="n">SBS</span><span class="p">)</span>
    
    
    
    
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;三体人物角色的语义品牌评分(semantic brand score)趋势&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;小说字符位置(小说等分为20份)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Semantic Brand Score&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>    
</code></pre></div><p><img loading="lazy" src="img/plot.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="五-获取资源">五、 获取资源</h2>
<p>内容整理不易， 如果对本文感兴趣</p>
<ul>
<li><em><strong>免费</strong></em>   获取本文代码&amp;实验数据  链接: <a href="https://pan.baidu.com/s/1ut8bKDxd5PGL_dm_yXTzcA?pwd=tr3t">https://pan.baidu.com/s/1ut8bKDxd5PGL_dm_yXTzcA?pwd=tr3t</a> 提取码: tr3t</li>
<li><em><strong>100元</strong></em>   <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em>  ，可加微信 <em><strong>372335839</strong></em>， 备注「姓名-学校-专业」</li>
</ul>
<p><br><br></p>
<h2 id="相关资料">相关资料</h2>
<p>Colladon, Andrea Fronzetti. &ldquo;<em><strong>The semantic brand score</strong></em>.&rdquo; <em>Journal of Business Research</em> 88 (2018): 150-160.</p>
<p>SBS相关文章列表  <a href="https://semanticbrandscore.com/sbsarticles.html">https://semanticbrandscore.com/sbsarticles.html</a></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集(付费) | 372w政府采购合同公告明细数据（2024.03）</title>
      <link>https://textdata.cn/blog/2023-09-03-government-procurement-contract-data/</link>
      <pubDate>Wed, 10 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-09-03-government-procurement-contract-data/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-cover.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h2 id=&#34;一数据集简介&#34;&gt;一、数据集简介&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 数据来源: 中国政府采购网（www.ccgp.gov.cn）
- 记录数量: 3724395
- 发布时间: 1996-06-05 ~ 2024-03-07, 但主要是2015之后


数据集 100 元，购买请加微信 372335839， 备注 【姓名-学校-专业】
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;1. 付费数据集，100元；加微信 372335839， 备注「姓名-学校-专业」。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;2. 数据是虚拟产品，一经售出，不再退还！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;3. 请仔细阅读推文内容， 确认无误再加微信详谈购买事宜 &lt;/span&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;二应用&#34;&gt;二、应用&lt;/h2&gt;
&lt;p&gt;随着政府采购规模的逐步增加，中国政府采购网披露的信息越来越丰富。近年来一些学者也关 注到中国政府采购数据，但由于文本数据半结构化、高维、数据量大的特性，该数据在文本的整理、 关键变量识别与关键变量提取方面存在着不小的难度，目前而言使用该数据的研究并没有很多。&lt;/p&gt;
&lt;h3 id=&#34;21-创新&#34;&gt;2.1 创新&lt;/h3&gt;
&lt;p&gt;姜爱华和费堃桀（2021） 手工整理了 2015-2019 年的政府采购数据，利用公告中供应商的名称与上市公司全称进行匹配，最终得到了 13 004 个企业年度观测值，发现企业获 得政府采购订单能够显著促进企业创新。&lt;/p&gt;
&lt;p&gt;Beraja 等（2020）基 于 2013-2019 年政府采购合同，与中国人工智能企业进行名单匹配，得到 28 023 份政 府人脸识别采购合同样本，发现政府采购对人脸识别相关的人工智能专利的增长起到了推动作用。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-政企关系&#34;&gt;2.2 政企关系&lt;/h3&gt;
&lt;p&gt;Fang 等（2022）利用中国政府采购网 2013-2020 年的采购公告与工商注册企业数据进行匹配，发现当本地官员处于激烈的政治竞争中时，本地政府将更少地向 竞争地区的企业进行采购，这造成了市场分割，影响了资源分配。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-其他&#34;&gt;2.3 其他&lt;/h3&gt;
&lt;p&gt;政府采购影响企业履行企业社会责任（韩旭和武威，2021）、中国特色精准扶贫（武威等，2022）、经济 发展（武威和刘国平，2021）等。此外，还有研究单独使用政府采购数据测量经济生产生活。江鸿 泽和梁平汉（2022）基于政府采购公告整理了各地的公共视频监控系统使用情况，Liu 等（2022） 则抓取了 2013-2021 年政府采购公告，用以识别企业的政治联系。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二查看数据&#34;&gt;二、查看数据&lt;/h2&gt;
&lt;h3 id=&#34;21-读取数据&#34;&gt;2.1 读取数据&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;政府采购公告1996-2024.3.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#gz文件可用bandizp或winrar解压得到csv&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#df = pd.read_csv(&amp;#39;政府采购公告1996-2024.3.csv&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同公告日期&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同公告日期&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-记录数&#34;&gt;2.2 记录数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;数据集记录数: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;数据集记录数:  2883958
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-字段&#34;&gt;2.3 字段&lt;/h3&gt;
&lt;p&gt;数据所含字段&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;col&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;合同编号
合同名称
项目编号
项目名称
采购人(甲方)
采购人地址
采购人联系方式
供应商(乙方)
供应商地址
供应商联系方式
主要标的名称
规格型号或服务要求
主要标的数量
主要标的单价
合同金额(万元)
履约期限、地点等简要信息
采购方式
合同签订日期
合同公告日期
其他补充事宜
所属地域
所属行业
代理机构
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;24-公告日期&#34;&gt;2.4 公告日期&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#数据集公告日期起止&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同公告日期&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同公告日期&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;发布时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同公告日期&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;发布时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同公告日期&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;发布时间 1996-06-05 00:00:00
发布时间 2024-03-07 00:00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#政府采购合同公告数据，主要出现在2015年之后&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同公告日期&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value_counts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sort_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;合同公告日期
1996          1
2000          1
2002          2
2004          7
2008          5
2009          3
2010          2
2011         13
2012          3
2013          4
2014         24
2015      15543
2016      42195
2017      94193
2018     154922
2019     151181
2020     187874
2021     549078
2022    1060710
2023    1355749
2024     112885
Name: count, dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;梁平汉和郭宇辰(2023) 认为 &lt;strong&gt;2015年财政部相关采购信息发布文件出台之后采购公告上传率大幅上升至80%以上，因此采用2015年以后的中国政府采购网数据进行研究更为合适&lt;/strong&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;24-甲乙方人数&#34;&gt;2.4 甲(乙)方人数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#甲方乙方数量&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#甲方乙方数量&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;采购人(甲方)数: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;采购人&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nunique&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;供应商(乙方)数: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;供应商&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nunique&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;采购人(甲方)数:  234082
供应商(乙方)数:  499943
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三实验代码&#34;&gt;三、实验代码&lt;/h2&gt;
&lt;h3 id=&#34;31-是否含某类词&#34;&gt;3.1 是否含某(类)词&lt;/h3&gt;
&lt;p&gt;根据公告中是否出现某(类)词，可以提起一些指标。例如 Beraja 等（2020）基于 2013-2019 年政府采购合同，与中国人工智能企业进行名单匹配，得到 28 023 份政府人脸识别采购合同样本。 本文仅简单示范， 以 &lt;em&gt;&lt;strong&gt;人工智能&lt;/strong&gt;&lt;/em&gt; 相关词为例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同名称&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;人工智能|自然语言处理|自动驾驶|AI|ai&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0          False
1          False
2          False
3          False
4          False
           ...  
3724390    False
3724391    False
3724392    False
3724393    False
3724394    False
Name: 合同名称, Length: 3724395, dtype: bool
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#AI相关公告的数量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同名称&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;人工智能|自然语言处理|自动驾驶|AI|ai&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;1323
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#显示匹配到的与 AI 有关的【合同名称】&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同名称&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;人工智能|自然语言处理|自动驾驶|AI|ai&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)][&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;合同名称&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;1129                                     贵州大学人工智能研究院建设项目采购合同
4935            龙岩初级中学人工智能创客实验室设备货物类采购项目合同\n （macrodatas.cn）
12231                       中国医学科学院系统医学研究院人工智能高性能计算设备采购合同协议书
13171      双高基于AIoT轨道交通智慧运维环境信号检测分析设备购置(二次)\n\n微信公众号“马克 数据网”
16921                           广州国际生物岛自动驾驶新能源环卫作业创新试点服务采购项目
                                 ...                        
3708596               榆林市教育技术中心人工智能助推教师队伍建设-教师发展智慧管理平台建设项目合同
3708922        邢台市信都区“人工智能公共技术服务平台”项目一标段数字教育、数字文旅采购合同\n\n （）
3709875         吴忠市第三中学南湖校区AI课堂教学行为分析评测系统及智慧教室设备采购项目系统集成服务合同
3712051                               人工智能与机器人领域创新成果产业化成熟度评价
3724277        民乐县现代农业投资有限责任公司民乐县人工智能一二三产业融合功能区食用菌菌棒生产项目  （）
Name: 合同名称, Length: 1323, dtype: object
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-构建省份字段&#34;&gt;3.2 构建省份字段&lt;/h3&gt;
&lt;p&gt;数据集中有  &lt;em&gt;&lt;strong&gt;采购人地址&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;采购人(甲方)&lt;/strong&gt;&lt;/em&gt; 两个地址字段，我们以 &lt;em&gt;&lt;strong&gt;采购人(甲方)&lt;/strong&gt;&lt;/em&gt; 为例，构建 &lt;em&gt;&lt;strong&gt;采购人省份&lt;/strong&gt;&lt;/em&gt; 字段。 注意: 经过测试，使用cpca库提取省份信息， 两种方式提取省份信息缺失率依次是 24.8%、 7%， 因此我们决定采用 &lt;em&gt;&lt;strong&gt;采购人(甲方)&lt;/strong&gt;&lt;/em&gt;  来提取省份。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cpca&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;provs_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cpca&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;采购人(甲方)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;采购人省份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cpca&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;采购人(甲方)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;省&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;采购人省份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;采购人省份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sub&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;自治区|特别行政区&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;33-按省分组查看记录量&#34;&gt;3.3 按省分组查看记录量&lt;/h3&gt;
&lt;p&gt;假设 &lt;em&gt;&lt;strong&gt;采购人省份&lt;/strong&gt;&lt;/em&gt; 构建的准确的话， 就可以分组查看每个省的记录量。   df.groupby(&amp;lsquo;采购人省份&amp;rsquo;)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;采购人省份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prov_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt; 267312  (未知省份，cpca缺失字段，占比大概7%)
上海市 29493
云南省 49789
内蒙古 480459
北京市 71869
台湾省 93
吉林省 14219
四川省 155028
天津市 10734
宁夏回族 76783
安徽省 44133
山东省 14634
山西省 5784
广东省 1349039
广西壮族 12534
新疆维吾尔 8000
江苏省 28655
江西省 8949
河北省 203761
河南省 8159
浙江省 12158
海南省 38603
湖北省 6156
湖南省 11300
甘肃省 289772
福建省 97527
西藏 2558
贵州省 2599
辽宁省 34547
重庆市 58673
陕西省 55478
青海省 22441
香港 80
黑龙江省 253076
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;scienceplots&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;platform&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib_inline&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backend_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_matplotlib_formats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;png&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;svg&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;jieba&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;warnings&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;warnings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filterwarnings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ignore&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;science&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;no-latex&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;cjk-sc-font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;platform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取操作系统类型&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Windows&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;SimHei&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Darwin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Arial Unicode MS&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;sans-serif&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;font&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 设置全局字体&lt;/span&gt;



&lt;span class=&#34;n&#34;&gt;prov_volumes&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;采购人省份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;prov_volumes&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;prov&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;volume&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prov_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)})&lt;/span&gt;
    
&lt;span class=&#34;n&#34;&gt;prov_volumes_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prov_volumes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;prov_volumes_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;prov&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sort_values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;volume&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ascending&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bar&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;政府采购数量(采购人按省)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xticks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rotation&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;45&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xlabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;省份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;采购公告数量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;采购按省， 最多的几个省份依次是广东、内蒙、甘肃、黑龙江等。  甘肃和黑龙江之间有个空白， 这是因为根据采购人(甲方)使用cpca提取省份信息时，有7%记录是缺失的。&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三相关研究&#34;&gt;三、相关研究&lt;/h2&gt;
&lt;p&gt;相关研究近期文献&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[1]周亚虹,蒲余路,陈诗一等.政府扶持与新型产业发展——以新能源为例[J].经济研究,2015,50(06):147-161.
[2]武威,刘国平.政府采购与经济发展：转型效应与协同效应——基于产业结构升级视角[J].财政研究,2021(08):77-90.
[3]孙薇,叶初升.政府采购何以牵动企业创新——兼论需求侧政策“拉力”与供给侧政策“推力”的协同[J].中国工业经济,2023(01):95-113.
[4]姜爱华,费堃桀,张鑫娜.政府采购、营商环境与企业创新——基于A股上市公司的经验证据[J].中央财经大学学报,2022(09):3-15.
[5]梁平汉, 郭宇辰. 中国政府采购公告数据的使用和潜在问题[J]. 产业经济评论, 2023, (01): 68-80.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四获取数据&#34;&gt;四、获取数据&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;1. 付费数据集，100元；加微信 372335839， 备注「姓名-学校-专业」。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;2. 数据是虚拟产品，一经售出，不再退还！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;3. 请仔细阅读推文内容， 确认无误再加微信详谈购买事宜 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="img/01-cover.png" alt=""  />
</p>
<h2 id="一数据集简介">一、数据集简介</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 数据来源: 中国政府采购网（www.ccgp.gov.cn）
- 记录数量: 3724395
- 发布时间: 1996-06-05 ~ 2024-03-07, 但主要是2015之后


数据集 100 元，购买请加微信 372335839， 备注 【姓名-学校-专业】
</code></pre></div><p><span style="font-size: 18px;color: green;">1. 付费数据集，100元；加微信 372335839， 备注「姓名-学校-专业」。</span></p>
<p><span style="font-size: 18px;color: green;">2. 数据是虚拟产品，一经售出，不再退还！</span></p>
<p><span style="font-size: 18px;color: green;">3. 请仔细阅读推文内容， 确认无误再加微信详谈购买事宜 </span></p>
<br>
<h2 id="二应用">二、应用</h2>
<p>随着政府采购规模的逐步增加，中国政府采购网披露的信息越来越丰富。近年来一些学者也关 注到中国政府采购数据，但由于文本数据半结构化、高维、数据量大的特性，该数据在文本的整理、 关键变量识别与关键变量提取方面存在着不小的难度，目前而言使用该数据的研究并没有很多。</p>
<h3 id="21-创新">2.1 创新</h3>
<p>姜爱华和费堃桀（2021） 手工整理了 2015-2019 年的政府采购数据，利用公告中供应商的名称与上市公司全称进行匹配，最终得到了 13 004 个企业年度观测值，发现企业获 得政府采购订单能够显著促进企业创新。</p>
<p>Beraja 等（2020）基 于 2013-2019 年政府采购合同，与中国人工智能企业进行名单匹配，得到 28 023 份政 府人脸识别采购合同样本，发现政府采购对人脸识别相关的人工智能专利的增长起到了推动作用。</p>
<br>
<h3 id="22-政企关系">2.2 政企关系</h3>
<p>Fang 等（2022）利用中国政府采购网 2013-2020 年的采购公告与工商注册企业数据进行匹配，发现当本地官员处于激烈的政治竞争中时，本地政府将更少地向 竞争地区的企业进行采购，这造成了市场分割，影响了资源分配。</p>
<br>
<h3 id="23-其他">2.3 其他</h3>
<p>政府采购影响企业履行企业社会责任（韩旭和武威，2021）、中国特色精准扶贫（武威等，2022）、经济 发展（武威和刘国平，2021）等。此外，还有研究单独使用政府采购数据测量经济生产生活。江鸿 泽和梁平汉（2022）基于政府采购公告整理了各地的公共视频监控系统使用情况，Liu 等（2022） 则抓取了 2013-2021 年政府采购公告，用以识别企业的政治联系。</p>
<p><br><br></p>
<h2 id="二查看数据">二、查看数据</h2>
<h3 id="21-读取数据">2.1 读取数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;政府采购公告1996-2024.3.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>

<span class="c1">#gz文件可用bandizp或winrar解压得到csv</span>
<span class="c1">#df = pd.read_csv(&#39;政府采购公告1996-2024.3.csv&#39;)</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;合同公告日期&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;合同公告日期&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/02-df.png" alt=""  />
</p>
<br>
<h3 id="22-记录数">2.2 记录数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;数据集记录数: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
</code></pre></div><pre><code>数据集记录数:  2883958
</code></pre>
<br>
<h3 id="23-字段">2.3 字段</h3>
<p>数据所含字段</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">col</span><span class="p">)</span>
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">合同编号
合同名称
项目编号
项目名称
采购人(甲方)
采购人地址
采购人联系方式
供应商(乙方)
供应商地址
供应商联系方式
主要标的名称
规格型号或服务要求
主要标的数量
主要标的单价
合同金额(万元)
履约期限、地点等简要信息
采购方式
合同签订日期
合同公告日期
其他补充事宜
所属地域
所属行业
代理机构
</code></pre></div><br>
<h3 id="24-公告日期">2.4 公告日期</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#数据集公告日期起止</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;合同公告日期&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;合同公告日期&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;发布时间&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;合同公告日期&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;发布时间&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;合同公告日期&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</code></pre></div><pre><code>发布时间 1996-06-05 00:00:00
发布时间 2024-03-07 00:00:00
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#政府采购合同公告数据，主要出现在2015年之后</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;合同公告日期&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">合同公告日期
1996          1
2000          1
2002          2
2004          7
2008          5
2009          3
2010          2
2011         13
2012          3
2013          4
2014         24
2015      15543
2016      42195
2017      94193
2018     154922
2019     151181
2020     187874
2021     549078
2022    1060710
2023    1355749
2024     112885
Name: count, dtype: int64
</code></pre></div><p>梁平汉和郭宇辰(2023) 认为 <strong>2015年财政部相关采购信息发布文件出台之后采购公告上传率大幅上升至80%以上，因此采用2015年以后的中国政府采购网数据进行研究更为合适</strong>。</p>
<br>
<h3 id="24-甲乙方人数">2.4 甲(乙)方人数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#甲方乙方数量</span>
<span class="c1">#甲方乙方数量</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;采购人(甲方)数: &#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;采购人&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">nunique</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;供应商(乙方)数: &#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;供应商&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">nunique</span><span class="p">())</span>
</code></pre></div><pre><code>采购人(甲方)数:  234082
供应商(乙方)数:  499943
</code></pre>
<p><br><br></p>
<h2 id="三实验代码">三、实验代码</h2>
<h3 id="31-是否含某类词">3.1 是否含某(类)词</h3>
<p>根据公告中是否出现某(类)词，可以提起一些指标。例如 Beraja 等（2020）基于 2013-2019 年政府采购合同，与中国人工智能企业进行名单匹配，得到 28 023 份政府人脸识别采购合同样本。 本文仅简单示范， 以 <em><strong>人工智能</strong></em> 相关词为例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;合同名称&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;人工智能|自然语言处理|自动驾驶|AI|ai&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0          False
1          False
2          False
3          False
4          False
           ...  
3724390    False
3724391    False
3724392    False
3724393    False
3724394    False
Name: 合同名称, Length: 3724395, dtype: bool
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#AI相关公告的数量</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;合同名称&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;人工智能|自然语言处理|自动驾驶|AI|ai&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1323
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#显示匹配到的与 AI 有关的【合同名称】</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;合同名称&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;人工智能|自然语言处理|自动驾驶|AI|ai&#39;</span><span class="p">)][</span><span class="s1">&#39;合同名称&#39;</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1129                                     贵州大学人工智能研究院建设项目采购合同
4935            龙岩初级中学人工智能创客实验室设备货物类采购项目合同\n （macrodatas.cn）
12231                       中国医学科学院系统医学研究院人工智能高性能计算设备采购合同协议书
13171      双高基于AIoT轨道交通智慧运维环境信号检测分析设备购置(二次)\n\n微信公众号“马克 数据网”
16921                           广州国际生物岛自动驾驶新能源环卫作业创新试点服务采购项目
                                 ...                        
3708596               榆林市教育技术中心人工智能助推教师队伍建设-教师发展智慧管理平台建设项目合同
3708922        邢台市信都区“人工智能公共技术服务平台”项目一标段数字教育、数字文旅采购合同\n\n （）
3709875         吴忠市第三中学南湖校区AI课堂教学行为分析评测系统及智慧教室设备采购项目系统集成服务合同
3712051                               人工智能与机器人领域创新成果产业化成熟度评价
3724277        民乐县现代农业投资有限责任公司民乐县人工智能一二三产业融合功能区食用菌菌棒生产项目  （）
Name: 合同名称, Length: 1323, dtype: object
</code></pre></div><br>
<h3 id="32-构建省份字段">3.2 构建省份字段</h3>
<p>数据集中有  <em><strong>采购人地址</strong></em>、<em><strong>采购人(甲方)</strong></em> 两个地址字段，我们以 <em><strong>采购人(甲方)</strong></em> 为例，构建 <em><strong>采购人省份</strong></em> 字段。 注意: 经过测试，使用cpca库提取省份信息， 两种方式提取省份信息缺失率依次是 24.8%、 7%， 因此我们决定采用 <em><strong>采购人(甲方)</strong></em>  来提取省份。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cpca</span>

<span class="n">provs_df</span> <span class="o">=</span> <span class="n">cpca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;采购人(甲方)&#39;</span><span class="p">])</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;采购人省份&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cpca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;采购人(甲方)&#39;</span><span class="p">])[</span><span class="s1">&#39;省&#39;</span><span class="p">]</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;采购人省份&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;采购人省份&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;自治区|特别行政区&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/03-df.png" alt=""  />
</p>
<br>
<h3 id="33-按省分组查看记录量">3.3 按省分组查看记录量</h3>
<p>假设 <em><strong>采购人省份</strong></em> 构建的准确的话， 就可以分组查看每个省的记录量。   df.groupby(&lsquo;采购人省份&rsquo;)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">prov</span><span class="p">,</span> <span class="n">prov_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;采购人省份&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">prov</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">prov_df</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> 267312  (未知省份，cpca缺失字段，占比大概7%)
上海市 29493
云南省 49789
内蒙古 480459
北京市 71869
台湾省 93
吉林省 14219
四川省 155028
天津市 10734
宁夏回族 76783
安徽省 44133
山东省 14634
山西省 5784
广东省 1349039
广西壮族 12534
新疆维吾尔 8000
江苏省 28655
江西省 8949
河北省 203761
河南省 8159
浙江省 12158
海南省 38603
湖北省 6156
湖南省 11300
甘肃省 289772
福建省 97527
西藏 2558
贵州省 2599
辽宁省 34547
重庆市 58673
陕西省 55478
青海省 22441
香港 80
黑龙江省 253076
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>
<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>



<span class="n">prov_volumes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">prov</span><span class="p">,</span> <span class="n">prov_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;采购人省份&#39;</span><span class="p">):</span>
    <span class="n">prov_volumes</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;prov&#39;</span><span class="p">:</span> <span class="n">prov</span><span class="p">,</span> <span class="s1">&#39;volume&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">prov_df</span><span class="p">)})</span>
    
<span class="n">prov_volumes_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">prov_volumes</span><span class="p">)</span>
<span class="n">prov_volumes_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;prov&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;volume&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;政府采购数量(采购人按省)&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;省份&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;采购公告数量&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/plot.png" alt=""  />
</p>
<p>采购按省， 最多的几个省份依次是广东、内蒙、甘肃、黑龙江等。  甘肃和黑龙江之间有个空白， 这是因为根据采购人(甲方)使用cpca提取省份信息时，有7%记录是缺失的。<br><br></p>
<h2 id="三相关研究">三、相关研究</h2>
<p>相关研究近期文献</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]周亚虹,蒲余路,陈诗一等.政府扶持与新型产业发展——以新能源为例[J].经济研究,2015,50(06):147-161.
[2]武威,刘国平.政府采购与经济发展：转型效应与协同效应——基于产业结构升级视角[J].财政研究,2021(08):77-90.
[3]孙薇,叶初升.政府采购何以牵动企业创新——兼论需求侧政策“拉力”与供给侧政策“推力”的协同[J].中国工业经济,2023(01):95-113.
[4]姜爱华,费堃桀,张鑫娜.政府采购、营商环境与企业创新——基于A股上市公司的经验证据[J].中央财经大学学报,2022(09):3-15.
[5]梁平汉, 郭宇辰. 中国政府采购公告数据的使用和潜在问题[J]. 产业经济评论, 2023, (01): 68-80.
</code></pre></div><p><br><br></p>
<h2 id="四获取数据">四、获取数据</h2>
<p><span style="font-size: 18px;color: green;">1. 付费数据集，100元；加微信 372335839， 备注「姓名-学校-专业」。</span></p>
<p><span style="font-size: 18px;color: green;">2. 数据是虚拟产品，一经售出，不再退还！</span></p>
<p><span style="font-size: 18px;color: green;">3. 请仔细阅读推文内容， 确认无误再加微信详谈购买事宜 </span></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>ANCW | 4030词的中文情感词典(效价、唤醒度、主导度、具体性)</title>
      <link>https://textdata.cn/blog/2024-02-27-ancw-affective-norms-for-4030-chinese-words/</link>
      <pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-02-27-ancw-affective-norms-for-4030-chinese-words/</guid>
      <description>&lt;p&gt;Ying, Lv, Ye Ruyang, Ni Chuanbin, Wang Yeqing, Liu Qing, Zhou Yufan, and Gao Fei. &amp;ldquo;ANCW: Affective norms for 4030 Chinese words.&amp;rdquo; &lt;em&gt;Behavior Research Methods&lt;/em&gt; (2023): 1-16.&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;一摘要&#34;&gt;一、摘要&lt;/h2&gt;
&lt;p&gt;单词中包含的情感信息越来越受到世界各地神经语言学家和心理语言学家的关注。本研究建立了情感词典ANCW(Affective Norms for Chinese Words)，对 4030 个词语进行了&lt;strong&gt;效价valence&lt;/strong&gt;、&lt;strong&gt;唤醒度arousal&lt;/strong&gt;、&lt;strong&gt;主导度dominance&lt;/strong&gt;和&lt;strong&gt;具体性concreteness&lt;/strong&gt; 打分，这些词语是根据 CET-4（国家大学英语四级考试）官方大纲进行中文改编的。尽管现有的中文情感词典CAWS(Chinese Affective Words System)，ANCW 提供了更多、更丰富的中文词汇。通过在程序中使用 7 级李克特量表（范围从 1 到 7），我们获得了 3717 名中国本科生对所有变量的评分。词典ANCW具有良好的响应信度，并且与中文先前的规范研究相兼容。成对相关分析揭示了效价与唤醒、唤醒与支配性以及效价与具体性之间的二次关系。此外，效价和支配性、唤醒性和具体性均呈现线性相关，具体性和支配性相关。ANCW 为涉及情感语言处理的进一步研究提供可靠且标准化的刺激材料。&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;二文献梳理&#34;&gt;二、文献梳理&lt;/h2&gt;
&lt;p&gt;语言和情感是人类生活不可分割的一部分。在过去的二十年里，词语的情感评级受到了极大的关注。研究人员建立了许多标准化数据库，从不同维度对不同语言的单词进行评级。传统上，情感的概念是情感观，被视为多个维度的连续体（Ćoso et al., 2019；Rubin &amp;amp; Talarico, 2009），所有情感都具有两个或三个维度的特征（Duffy, 1934)；奥斯古德等人，1957）。根据卡罗尔、奥斯古德、苏西和坦南鲍姆（ 1959）的情感理论，对词语进行了大量的情感评级，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;效价valence&lt;/strong&gt; 是指令人愉快的程度，范围从不愉快到愉快；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;唤醒度arousal&lt;/strong&gt; 是生理激活程度的指标，范围从平静到兴奋；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支配性dominance&lt;/strong&gt; 描述了个人所感受到的控制程度，从失控到受控。近年来，心理语言学变量具体性的研究引起了人们的浓厚兴趣。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据 Gilhooly 和 Logie（1980）的观点，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;具体性concreteness&lt;/strong&gt; 代表了形成单词心理形象的难度程度，范围从抽象（难以形成）到具体（易于形成）。&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;构建具有单词情感评级的数据库的需求很大，因为它们至少有助于四个方面的研究，包括针对情绪本身的研究、情绪特征对单词处理和记忆的影响、整个消息表达的情绪或文本，以及通过将新词与已验证词进行比较来了解新词的情感价值（有关评论，请参阅 Warriner 等人，2013 年）。到目前为止，已经用多种语言构建了各种数据库，并为进一步的研究提供了丰富的刺激和可靠测量的情绪特征。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-old-dicts.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;从上述文献中，我们可以看到针对不同语言建立了各种各样的包含情感评级的数据库，以满足日益增长的情感研究需求。然而，据我们所知，该领域还存在一些有待进一步研究的地方：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大多数数据库是由西方国家建立的，并且已经证实，一些研究发现情感评级因文化而异。因此，建立中国本土情感规范数据库迫在眉睫。&lt;/li&gt;
&lt;li&gt;国内以往的研究在制定标准化的情绪刺激上付出了很大的努力，并且使用了多样化的刺激。在这些刺激中，言语刺激可以得到更严格的控制，并且与其他刺激具有可比性，例如需要在复杂性、亮度、颜色和对比度上进行控制的图片(Soares et al., 2012 &lt;a href=&#34;https://link.springer.com/article/10.3758/s13428-023-02226-x#ref-CR60&#34;&gt;)&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;最重要的是，以往的研究限制了汉字的数量。例如，AANC（Liu et al., 2021）由四个汉字单词组成，而Yao等人建立的另一个数据库则由四个汉字组成。( 2016)仅包含两个字符的单词。众所周知，汉字非常复杂。例如，一个汉字可以组成一个词，如“书”、“美”、“杀”。两个或多个汉字也可以组成一个词，如“生活”、“白日梦”、“色彩斑美丽”。特别是，日常使用的词语非常灵活，不仅限于二字词或四字词。在这种情况下，汉字数量的限制在一定程度上限制了表达的丰富性和灵活性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;鉴于这些局限性，本研究旨在建立一个标准化、多维、不限制字数的汉语词语情感规范数据库。此外，本研究将采用多种方法检验ANCW的可靠性，为进一步研究情感和心理语言变量之间的关系提供更多证据。总体而言，本研究在一定程度上弥补了上述局限性。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三-方法&#34;&gt;三、 方法&lt;/h2&gt;
&lt;h3 id=&#34;31-参与者&#34;&gt;3.1 参与者&lt;/h3&gt;
&lt;p&gt;共有 3717 名母语为中文的人参与了这项研究。所有参与者均为中国 41 所大学除英语专业以外的其他专业本科生（女性 2346 名，男性 1258 名，无性别信息 113 名；M年龄= 19.91，范围 16-25，SD = 1.21）。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-确定词语列表&#34;&gt;3.2 确定词语列表&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;从英语四级CET-4的教学大纲中找出4030个英语单词&lt;/strong&gt;，大学英语四级大纲中的词汇出现频率较高，且与学员的日常生活密切相关。&lt;/p&gt;
&lt;p&gt;翻译经过三道严格的程序完成。第一轮翻译依据的是牛津高阶英汉词典（第9版 *）*和英国国家语料库（BNC）。该研究采用《牛津高级英汉词典（第9版 *）》*中的首个中文释义，将词表翻译成中文。有些词有多个词性。例如，“stem”可以是名词和动词。名词“茎”的意思是“植物在地面上长出叶子或花朵的主要长而薄的部分；从中生长出来并支撑花朵或叶子的较小部分”（Stem，2018），动词的意思是“阻止某些正在流动或增加的东西”（Stem，2018）。在本例中，我们根据英国国家语料库选择了词频较高的词性。在此过程之后，研究发现了 672 个单词的一致翻译。&lt;/p&gt;
&lt;p&gt;在第二个翻译阶段，本研究采用了德尔菲法。我们邀请了五位精通英语文化和中国文化的专业翻译人员来进行这项工作。翻译过程中，五位专业人士未经讨论就翻译了这672个一致词。然后，研究对他们的翻译进行了比较，并找出了五位译者意见不一致的词语。经过四轮匿名讨论，我们获得了唯一不重复的汉译本553个单词。&lt;/p&gt;
&lt;p&gt;经过这一步，剩下了 186 个与中文翻译一致的单词。为了确保每个翻译不重复，研究在中文翻译后标记了原始英文单词或该单词的词性。最终获得了英语四级英语单词大纲的翻译版，包含4030个中文单词。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我们将 4030 个中文单词的列表随机分为 20 个子列表，每个子列表包含 201 或 202 个单词。根据该研究的设计，每个单词的每个维度（唤醒度、效价、支配性和具体性）都会被评估至少 45 次。&lt;/strong&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;33-设计问卷&#34;&gt;3.3 设计问卷&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;每份试卷均包含一个信息部分、说明和评分表。本研究采用7点李克特自评量表进行打分&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;效价描述了刺激引起愉悦感的程度（Russell，1980；Bradley &amp;amp; Lang，1999）。数字1表示非常不愉快，4表示一般，7表示非常愉快。&lt;/li&gt;
&lt;li&gt;唤醒，也称为激活、强度或能量水平（Montefinese 等，2014），用于描述身体被激活或唤醒的程度（Duffy，1934）。该研究用1表示极度平静，4表示中性，7表示极度兴奋。&lt;/li&gt;
&lt;li&gt;支配性被定义为个体对刺激的控制或影响程度，范围从完全失控到完全控制（Russell &amp;amp; Mehrabian，1977）。研究用1代表受试者感觉自己完全被这个词控制（这个词是“盛行”），4代表中立，7代表受试者感觉能够完全控制这个词（这个词是“弱”）。 ”）。&lt;/li&gt;
&lt;li&gt;具体性是指形成单词物理所指的心理图像的困难程度。该研究使用1表示极端抽象，4表示中性，7表示极端具体。&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&#34;34-步骤&#34;&gt;3.4 步骤&lt;/h3&gt;
&lt;p&gt;本研究采用&lt;strong&gt;纸笔评分法&lt;/strong&gt;(paper-pencil rating method) 。每个参与者随机收到一个单词子列表。在试卷的第一页，该研究为每个维度（效价、唤醒度、支配性和具体性）提供了清晰的中文说明和生动的例子。参与者收到试卷后，研究口头提供了清晰的说明解释。试卷的第二页和第三页是A4纸上打印的中文单词和等级量表。每个参与者在安静的教室里对一张试卷进行评分。由于所有单词都是汉语，而且四级单词在社会生活中广泛使用，因此没有参与者对单词的含义有疑问。&lt;/p&gt;
&lt;p&gt;鉴于之前的研究（谢，2020；张，2020），数据修剪规则如下所示，如果试卷满足其中一条规则，则将被视为无效。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;70%以上的评级结果缺失；&lt;/li&gt;
&lt;li&gt;70%以上的评级结果相同；&lt;/li&gt;
&lt;li&gt;试卷表现出明显的敌意。例如，一些参与者在试卷上留下侮辱性的评论，例如“我只是随意圈出数字来欺骗你们，傻瓜”。&lt;/li&gt;
&lt;li&gt;此外，答案是在一系列之字形中随机选择的。在这种情况下，调查问卷将被视为敌对调查问卷。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最终我们共收集到3304份试卷。在所有试卷中，效价评分为 858 份，唤醒评分为 803 份，支配性评分为 777 份，具体性评分为 866 份。每个维度中的几个缺失评级均由平均值代替。删除无效数据后的最终数据库共包含4030个单词，每个单词的效价评分为42.9，唤醒评分为40.2，具体性评分为43.3，支配性评分为38.9。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四ancw词典&#34;&gt;四、ANCW词典&lt;/h2&gt;
&lt;p&gt;ancw下载链接:https://pan.baidu.com/s/1UfbmVQh9XM77eoGmMsZ2-w?pwd=bp63  提取码:bp63&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-ancw.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-ancw.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关文献&#34;&gt;相关文献&lt;/h2&gt;
&lt;p&gt;Xu, X., Li, J., &amp;amp; Chen, H. (2021). Valence and arousal ratings for 11,310 simplified Chinese words. &lt;em&gt;Behavior Research Methods, 54&lt;/em&gt;(1), 26–41. &lt;a href=&#34;https://doi.org/10.3758/s13428-021-01607-4&#34;&gt;https://doi.org/10.3758/s13428-021-01607-4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yao, Z., Wu, J., Zhang, Y., &amp;amp; Wang, Z. (2016). Norms of valence, arousal, concreteness, familiarity, imageability, and context availability for 1,100 Chinese words. &lt;em&gt;Behavior Research Methods, 49&lt;/em&gt;(4), 1374–1385. &lt;a href=&#34;https://doi.org/10.3758/s13428-016-0793-2&#34;&gt;https://doi.org/10.3758/s13428-016-0793-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Yuan, J., Zhang, Y., Chen, S., Luo, L., &amp;amp; Ru, Y. (2021). The establishment of Chinese Emotion Regulation Word System (CERWS) and its pilot test. &lt;em&gt;Acta Psychologica Sinica, 53&lt;/em&gt;(&lt;em&gt;5&lt;/em&gt;), 445. &lt;a href=&#34;https://doi.org/10.3724/sp.j.1041.2021.00445&#34;&gt;https://doi.org/10.3724/sp.j.1041.2021.00445&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course&#34;&gt;&lt;strong&gt;付费视频课 | Python实证指标构建与文本分析&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>Ying, Lv, Ye Ruyang, Ni Chuanbin, Wang Yeqing, Liu Qing, Zhou Yufan, and Gao Fei. &ldquo;ANCW: Affective norms for 4030 Chinese words.&rdquo; <em>Behavior Research Methods</em> (2023): 1-16.</p>
<br>
<br>
<h2 id="一摘要">一、摘要</h2>
<p>单词中包含的情感信息越来越受到世界各地神经语言学家和心理语言学家的关注。本研究建立了情感词典ANCW(Affective Norms for Chinese Words)，对 4030 个词语进行了<strong>效价valence</strong>、<strong>唤醒度arousal</strong>、<strong>主导度dominance</strong>和<strong>具体性concreteness</strong> 打分，这些词语是根据 CET-4（国家大学英语四级考试）官方大纲进行中文改编的。尽管现有的中文情感词典CAWS(Chinese Affective Words System)，ANCW 提供了更多、更丰富的中文词汇。通过在程序中使用 7 级李克特量表（范围从 1 到 7），我们获得了 3717 名中国本科生对所有变量的评分。词典ANCW具有良好的响应信度，并且与中文先前的规范研究相兼容。成对相关分析揭示了效价与唤醒、唤醒与支配性以及效价与具体性之间的二次关系。此外，效价和支配性、唤醒性和具体性均呈现线性相关，具体性和支配性相关。ANCW 为涉及情感语言处理的进一步研究提供可靠且标准化的刺激材料。</p>
<br>
<br>
<h2 id="二文献梳理">二、文献梳理</h2>
<p>语言和情感是人类生活不可分割的一部分。在过去的二十年里，词语的情感评级受到了极大的关注。研究人员建立了许多标准化数据库，从不同维度对不同语言的单词进行评级。传统上，情感的概念是情感观，被视为多个维度的连续体（Ćoso et al., 2019；Rubin &amp; Talarico, 2009），所有情感都具有两个或三个维度的特征（Duffy, 1934)；奥斯古德等人，1957）。根据卡罗尔、奥斯古德、苏西和坦南鲍姆（ 1959）的情感理论，对词语进行了大量的情感评级，</p>
<ul>
<li><strong>效价valence</strong> 是指令人愉快的程度，范围从不愉快到愉快；</li>
<li><strong>唤醒度arousal</strong> 是生理激活程度的指标，范围从平静到兴奋；</li>
<li><strong>支配性dominance</strong> 描述了个人所感受到的控制程度，从失控到受控。近年来，心理语言学变量具体性的研究引起了人们的浓厚兴趣。</li>
</ul>
<p>根据 Gilhooly 和 Logie（1980）的观点，</p>
<ul>
<li><strong>具体性concreteness</strong> 代表了形成单词心理形象的难度程度，范围从抽象（难以形成）到具体（易于形成）。</li>
</ul>
<br>
<p>构建具有单词情感评级的数据库的需求很大，因为它们至少有助于四个方面的研究，包括针对情绪本身的研究、情绪特征对单词处理和记忆的影响、整个消息表达的情绪或文本，以及通过将新词与已验证词进行比较来了解新词的情感价值（有关评论，请参阅 Warriner 等人，2013 年）。到目前为止，已经用多种语言构建了各种数据库，并为进一步的研究提供了丰富的刺激和可靠测量的情绪特征。</p>
<p><img loading="lazy" src="img/01-old-dicts.png" alt=""  />
</p>
<br>
<p>从上述文献中，我们可以看到针对不同语言建立了各种各样的包含情感评级的数据库，以满足日益增长的情感研究需求。然而，据我们所知，该领域还存在一些有待进一步研究的地方：</p>
<ul>
<li>大多数数据库是由西方国家建立的，并且已经证实，一些研究发现情感评级因文化而异。因此，建立中国本土情感规范数据库迫在眉睫。</li>
<li>国内以往的研究在制定标准化的情绪刺激上付出了很大的努力，并且使用了多样化的刺激。在这些刺激中，言语刺激可以得到更严格的控制，并且与其他刺激具有可比性，例如需要在复杂性、亮度、颜色和对比度上进行控制的图片(Soares et al., 2012 <a href="https://link.springer.com/article/10.3758/s13428-023-02226-x#ref-CR60">)</a>。</li>
<li>最重要的是，以往的研究限制了汉字的数量。例如，AANC（Liu et al., 2021）由四个汉字单词组成，而Yao等人建立的另一个数据库则由四个汉字组成。( 2016)仅包含两个字符的单词。众所周知，汉字非常复杂。例如，一个汉字可以组成一个词，如“书”、“美”、“杀”。两个或多个汉字也可以组成一个词，如“生活”、“白日梦”、“色彩斑美丽”。特别是，日常使用的词语非常灵活，不仅限于二字词或四字词。在这种情况下，汉字数量的限制在一定程度上限制了表达的丰富性和灵活性。</li>
</ul>
<p>鉴于这些局限性，本研究旨在建立一个标准化、多维、不限制字数的汉语词语情感规范数据库。此外，本研究将采用多种方法检验ANCW的可靠性，为进一步研究情感和心理语言变量之间的关系提供更多证据。总体而言，本研究在一定程度上弥补了上述局限性。</p>
<p><br><br></p>
<h2 id="三-方法">三、 方法</h2>
<h3 id="31-参与者">3.1 参与者</h3>
<p>共有 3717 名母语为中文的人参与了这项研究。所有参与者均为中国 41 所大学除英语专业以外的其他专业本科生（女性 2346 名，男性 1258 名，无性别信息 113 名；M年龄= 19.91，范围 16-25，SD = 1.21）。</p>
<br>
<h3 id="32-确定词语列表">3.2 确定词语列表</h3>
<p><strong>从英语四级CET-4的教学大纲中找出4030个英语单词</strong>，大学英语四级大纲中的词汇出现频率较高，且与学员的日常生活密切相关。</p>
<p>翻译经过三道严格的程序完成。第一轮翻译依据的是牛津高阶英汉词典（第9版 *）*和英国国家语料库（BNC）。该研究采用《牛津高级英汉词典（第9版 *）》*中的首个中文释义，将词表翻译成中文。有些词有多个词性。例如，“stem”可以是名词和动词。名词“茎”的意思是“植物在地面上长出叶子或花朵的主要长而薄的部分；从中生长出来并支撑花朵或叶子的较小部分”（Stem，2018），动词的意思是“阻止某些正在流动或增加的东西”（Stem，2018）。在本例中，我们根据英国国家语料库选择了词频较高的词性。在此过程之后，研究发现了 672 个单词的一致翻译。</p>
<p>在第二个翻译阶段，本研究采用了德尔菲法。我们邀请了五位精通英语文化和中国文化的专业翻译人员来进行这项工作。翻译过程中，五位专业人士未经讨论就翻译了这672个一致词。然后，研究对他们的翻译进行了比较，并找出了五位译者意见不一致的词语。经过四轮匿名讨论，我们获得了唯一不重复的汉译本553个单词。</p>
<p>经过这一步，剩下了 186 个与中文翻译一致的单词。为了确保每个翻译不重复，研究在中文翻译后标记了原始英文单词或该单词的词性。最终获得了英语四级英语单词大纲的翻译版，包含4030个中文单词。</p>
<p><strong>我们将 4030 个中文单词的列表随机分为 20 个子列表，每个子列表包含 201 或 202 个单词。根据该研究的设计，每个单词的每个维度（唤醒度、效价、支配性和具体性）都会被评估至少 45 次。</strong></p>
<br>
<h3 id="33-设计问卷">3.3 设计问卷</h3>
<p><strong>每份试卷均包含一个信息部分、说明和评分表。本研究采用7点李克特自评量表进行打分</strong>。</p>
<ul>
<li>效价描述了刺激引起愉悦感的程度（Russell，1980；Bradley &amp; Lang，1999）。数字1表示非常不愉快，4表示一般，7表示非常愉快。</li>
<li>唤醒，也称为激活、强度或能量水平（Montefinese 等，2014），用于描述身体被激活或唤醒的程度（Duffy，1934）。该研究用1表示极度平静，4表示中性，7表示极度兴奋。</li>
<li>支配性被定义为个体对刺激的控制或影响程度，范围从完全失控到完全控制（Russell &amp; Mehrabian，1977）。研究用1代表受试者感觉自己完全被这个词控制（这个词是“盛行”），4代表中立，7代表受试者感觉能够完全控制这个词（这个词是“弱”）。 ”）。</li>
<li>具体性是指形成单词物理所指的心理图像的困难程度。该研究使用1表示极端抽象，4表示中性，7表示极端具体。</li>
</ul>
<br>
<h3 id="34-步骤">3.4 步骤</h3>
<p>本研究采用<strong>纸笔评分法</strong>(paper-pencil rating method) 。每个参与者随机收到一个单词子列表。在试卷的第一页，该研究为每个维度（效价、唤醒度、支配性和具体性）提供了清晰的中文说明和生动的例子。参与者收到试卷后，研究口头提供了清晰的说明解释。试卷的第二页和第三页是A4纸上打印的中文单词和等级量表。每个参与者在安静的教室里对一张试卷进行评分。由于所有单词都是汉语，而且四级单词在社会生活中广泛使用，因此没有参与者对单词的含义有疑问。</p>
<p>鉴于之前的研究（谢，2020；张，2020），数据修剪规则如下所示，如果试卷满足其中一条规则，则将被视为无效。</p>
<ul>
<li>70%以上的评级结果缺失；</li>
<li>70%以上的评级结果相同；</li>
<li>试卷表现出明显的敌意。例如，一些参与者在试卷上留下侮辱性的评论，例如“我只是随意圈出数字来欺骗你们，傻瓜”。</li>
<li>此外，答案是在一系列之字形中随机选择的。在这种情况下，调查问卷将被视为敌对调查问卷。</li>
</ul>
<p>最终我们共收集到3304份试卷。在所有试卷中，效价评分为 858 份，唤醒评分为 803 份，支配性评分为 777 份，具体性评分为 866 份。每个维度中的几个缺失评级均由平均值代替。删除无效数据后的最终数据库共包含4030个单词，每个单词的效价评分为42.9，唤醒评分为40.2，具体性评分为43.3，支配性评分为38.9。</p>
<p><br><br></p>
<h2 id="四ancw词典">四、ANCW词典</h2>
<p>ancw下载链接:https://pan.baidu.com/s/1UfbmVQh9XM77eoGmMsZ2-w?pwd=bp63  提取码:bp63</p>
<p><img loading="lazy" src="img/02-ancw.png" alt=""  />
</p>
<p><img loading="lazy" src="img/03-ancw.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="相关文献">相关文献</h2>
<p>Xu, X., Li, J., &amp; Chen, H. (2021). Valence and arousal ratings for 11,310 simplified Chinese words. <em>Behavior Research Methods, 54</em>(1), 26–41. <a href="https://doi.org/10.3758/s13428-021-01607-4">https://doi.org/10.3758/s13428-021-01607-4</a></p>
<p>Yao, Z., Wu, J., Zhang, Y., &amp; Wang, Z. (2016). Norms of valence, arousal, concreteness, familiarity, imageability, and context availability for 1,100 Chinese words. <em>Behavior Research Methods, 49</em>(4), 1374–1385. <a href="https://doi.org/10.3758/s13428-016-0793-2">https://doi.org/10.3758/s13428-016-0793-2</a></p>
<p>Yuan, J., Zhang, Y., Chen, S., Luo, L., &amp; Ru, Y. (2021). The establishment of Chinese Emotion Regulation Word System (CERWS) and its pilot test. <em>Acta Psychologica Sinica, 53</em>(<em>5</em>), 445. <a href="https://doi.org/10.3724/sp.j.1041.2021.00445">https://doi.org/10.3724/sp.j.1041.2021.00445</a></p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course"><strong>付费视频课 | Python实证指标构建与文本分析</strong></a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 | 2010-2023年国家社会科学基金立项名单.xlsx</title>
      <link>https://textdata.cn/blog/2024-01-23-china-national-social-science-fund-projects-from-2010-to-2023/</link>
      <pubDate>Mon, 22 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-01-23-china-national-social-science-fund-projects-from-2010-to-2023/</guid>
      <description>&lt;h2 id=&#34;一数据概况&#34;&gt;一、数据概况&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据集名称: 国家社会科学基金立项名单
格式: xlsx
年份:2010~2023
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;点击下载 &lt;a href=&#34;2010-2023%E5%B9%B4%E5%9B%BD%E5%AE%B6%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6%E5%9F%BA%E9%87%91%E7%AB%8B%E9%A1%B9%E5%90%8D%E5%8D%95.xlsx&#34;&gt;2010-2023年国家社会科学基金立项名单.xlsx&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;二读取数据&#34;&gt;二、读取数据&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_excel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2010-2023年国家社会科学基金立项名单.xlsx&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;所在学科&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;所在学科&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;所在学科&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unique&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([&amp;#39;马列·科社&amp;#39;, &amp;#39;管理学&amp;#39;, &amp;#39;政治学&amp;#39;, &amp;#39;外国文学&amp;#39;, &amp;#39;人口学&amp;#39;, &amp;#39;图书馆、情报与文献学&amp;#39;, &amp;#39;新闻学与传播学&amp;#39;,
       &amp;#39;中国文学&amp;#39;, &amp;#39;世界历史&amp;#39;, &amp;#39;语言学&amp;#39;, &amp;#39;民族问题研究&amp;#39;, &amp;#39;哲学&amp;#39;, &amp;#39;理论经济&amp;#39;, &amp;#39;体育学&amp;#39;, &amp;#39;国际问题研究&amp;#39;,
       &amp;#39;中国历史&amp;#39;, &amp;#39;党史·党建&amp;#39;, &amp;#39;法学&amp;#39;, &amp;#39;应用经济&amp;#39;, &amp;#39;社会学&amp;#39;, &amp;#39;统计学&amp;#39;, &amp;#39;宗教学&amp;#39;, &amp;#39;&amp;#39;, &amp;#39;教育学&amp;#39;,
       &amp;#39;考古学&amp;#39;, &amp;#39;图书馆、情报与档案学&amp;#39;, &amp;#39;其他&amp;#39;, &amp;#39;图书情报&amp;#39;, &amp;#39;军事学&amp;#39;, &amp;#39;艺术学&amp;#39;, &amp;#39;党史•党建&amp;#39;, &amp;#39;马列•科社&amp;#39;,
       &amp;#39;新闻传播学&amp;#39;, &amp;#39;中国历史、&amp;#39;, &amp;#39;民族学&amp;#39;, &amp;#39;国际问题&amp;#39;, &amp;#39;法学、医学、公共卫生学&amp;#39;, &amp;#39;灾害学、社会学、管理学、系统科学&amp;#39;,
       &amp;#39;法学、医学、社会学&amp;#39;, &amp;#39;应用经济学 法学&amp;#39;, &amp;#39;宏观经济、计量经济、管理学等&amp;#39;, &amp;#39;智能技术、电子商务、人工智能、信&amp;#39;,
       &amp;#39;管理学、经济学、地理学&amp;#39;, &amp;#39;艺术学、人类学、计算机科学&amp;#39;, &amp;#39;文化人类学、非遗保护、考古学、影&amp;#39;,
       &amp;#39;文学艺术、文化人类学、计算机科学&amp;#39;, &amp;#39;计算机科学与技术、社会学、公共管&amp;#39;, &amp;#39;电气工程；产业经济学；管理学；热&amp;#39;,
       &amp;#39;城市规划学、计算机学、信息网络学&amp;#39;, &amp;#39;心理学、认知和行为科学、脑科学、&amp;#39;, &amp;#39;产业经济学、管理学、信息技术及应&amp;#39;,
       &amp;#39;法学、社会学、信息科学、计算机科&amp;#39;, &amp;#39;管理科学与工程、控制科学与工程、&amp;#39;, &amp;#39;智能技术、产业经济、经济学、管理&amp;#39;,
       &amp;#39;语言学、计算机科学、生态学、社会&amp;#39;, &amp;#39;理论经济学、应用经济学、法学、公&amp;#39;, &amp;#39;语言学、人类学、信息科学&amp;#39;,
       &amp;#39;宏观经济、计量经济、管理学、统计&amp;#39;, &amp;#39;管理学、 经济学 、环境科学、&amp;#39;, &amp;#39;语言学、计算机科学、统计学等&amp;#39;,
       &amp;#39;城乡规划学、管理学、地理学、经济&amp;#39;, &amp;#39;语言文学、心理学、教育学&amp;#39;, &amp;#39;人类学、社会心理学、认知神经科学&amp;#39;,
       &amp;#39;应用经济、管理学、资源环境科学、&amp;#39;, &amp;#39;电气工程、管理学、产业经济、能源&amp;#39;, &amp;#39;产业经济、生态学、系统科学、管理&amp;#39;, &amp;#39;马列科社&amp;#39;,
       &amp;#39;党史党建&amp;#39;, &amp;#39;综合研究&amp;#39;, &amp;#39;民族问题&amp;#39;, &amp;#39;图书·情报与文献&amp;#39;, &amp;#39;新闻学&amp;#39;, &amp;#39;跨学科&amp;#39;, &amp;#39;民族问题 研究&amp;#39;,
       &amp;#39;新闻与传播学&amp;#39;, &amp;#39;新闻学与 传播学&amp;#39;, &amp;#39;马列.科社&amp;#39;, &amp;#39;系列丛书&amp;#39;, &amp;#39;图书馆·情报与文献学&amp;#39;, &amp;#39;重点项目&amp;#39;,
       &amp;#39;一般项目&amp;#39;, &amp;#39;学术期刊&amp;#39;, &amp;#39;理论经济学&amp;#39;, &amp;#39;应用经济学&amp;#39;, &amp;#39;国际问题研\n究&amp;#39;, &amp;#39;新闻学与传\n播学&amp;#39;,
       &amp;#39;图书馆、情\n报与文献学&amp;#39;], dtype=object)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h2 id=&#34;三简单分析&#34;&gt;三、简单分析&lt;/h2&gt;
&lt;h3 id=&#34;31-可视化准备&#34;&gt;3.1 可视化准备&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib_inline&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backend_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_matplotlib_formats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;png&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;svg&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;scienceplots&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;platform&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;science&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;no-latex&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;cjk-sc-font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;platform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取操作系统类型&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Windows&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;SimHei&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Darwin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Arial Unicode MS&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;sans-serif&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;font&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 设置全局字体&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-立项数量&#34;&gt;3.2 立项数量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;df[&amp;#39;批准年份&amp;#39;].value_counts(ascending=True).plot(kind=&amp;#39;bar&amp;#39;, figsize=(8, 4))
plt.xticks(rotation=0)
plt.ylabel(&amp;#39;立项数量&amp;#39;, rotation=0)
plt.title(&amp;#39;国社科立项数量(2010-2023)&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;33-经管学科&#34;&gt;3.3 经管学科&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;management_economic_displines&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;所在学科&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unique&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;经济&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt;  &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;管理&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;management_economic_displines&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;管理学&amp;#39;,
 &amp;#39;理论经济&amp;#39;,
 &amp;#39;应用经济&amp;#39;,
 &amp;#39;灾害学、社会学、管理学、系统科学&amp;#39;,
 &amp;#39;应用经济学 法学&amp;#39;,
 &amp;#39;宏观经济、计量经济、管理学等&amp;#39;,
 &amp;#39;管理学、经济学、地理学&amp;#39;,
 &amp;#39;电气工程；产业经济学；管理学；热&amp;#39;,
 &amp;#39;产业经济学、管理学、信息技术及应&amp;#39;,
 &amp;#39;管理科学与工程、控制科学与工程、&amp;#39;,
 &amp;#39;智能技术、产业经济、经济学、管理&amp;#39;,
 &amp;#39;理论经济学、应用经济学、法学、公&amp;#39;,
 &amp;#39;宏观经济、计量经济、管理学、统计&amp;#39;,
 &amp;#39;管理学、 经济学 、环境科学、&amp;#39;,
 &amp;#39;城乡规划学、管理学、地理学、经济&amp;#39;,
 &amp;#39;应用经济、管理学、资源环境科学、&amp;#39;,
 &amp;#39;电气工程、管理学、产业经济、能源&amp;#39;,
 &amp;#39;产业经济、生态学、系统科学、管理&amp;#39;,
 &amp;#39;理论经济学&amp;#39;,
 &amp;#39;应用经济学&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;34-经管立项&#34;&gt;3.4 经管立项&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;eco_man_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;所在学科&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;isin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;management_economic_displines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;eco_man_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;经管类国社科立项数量占比&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;所在学科&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;isin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;management_economic_displines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0.18713464870187335
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;35-经管立项数量&#34;&gt;3.5 经管立项数量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;eco_man_with_ds_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eco_man_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eco_man_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;课题名称&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;大数据|数据挖掘|机器学习|人工智能|AIGC&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;eco_man_with_ds_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;批准年份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value_counts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ascending&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bar&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xticks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rotation&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;立项数量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rotation&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;国社科基金中结合数据科学的经济、管理类立项数量(2010-2023)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;36-经管立项占比&#34;&gt;3.6 经管立项占比&lt;/h3&gt;
&lt;p&gt;按年度查看， 国社科中经管类立项占比&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;year_ratios&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;批准年份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;ratio&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;所在学科&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;isin&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;management_economic_displines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;year_ratios&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ratio&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    
&lt;span class=&#34;n&#34;&gt;year_ratio_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_ratios&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_ratio_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;ratio&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_ratio_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_ratio_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bar&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;立项占比&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rotation&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;国社科基金中经济、管理类立项占比(2010-2023)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一数据概况">一、数据概况</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据集名称: 国家社会科学基金立项名单
格式: xlsx
年份:2010~2023
</code></pre></div><p>点击下载 <a href="2010-2023%E5%B9%B4%E5%9B%BD%E5%AE%B6%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6%E5%9F%BA%E9%87%91%E7%AB%8B%E9%A1%B9%E5%90%8D%E5%8D%95.xlsx">2010-2023年国家社会科学基金立项名单.xlsx</a></p>
<br>
<br>
<h2 id="二读取数据">二、读取数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;2010-2023年国家社会科学基金立项名单.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;所在学科&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;所在学科&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;所在学科&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([&#39;马列·科社&#39;, &#39;管理学&#39;, &#39;政治学&#39;, &#39;外国文学&#39;, &#39;人口学&#39;, &#39;图书馆、情报与文献学&#39;, &#39;新闻学与传播学&#39;,
       &#39;中国文学&#39;, &#39;世界历史&#39;, &#39;语言学&#39;, &#39;民族问题研究&#39;, &#39;哲学&#39;, &#39;理论经济&#39;, &#39;体育学&#39;, &#39;国际问题研究&#39;,
       &#39;中国历史&#39;, &#39;党史·党建&#39;, &#39;法学&#39;, &#39;应用经济&#39;, &#39;社会学&#39;, &#39;统计学&#39;, &#39;宗教学&#39;, &#39;&#39;, &#39;教育学&#39;,
       &#39;考古学&#39;, &#39;图书馆、情报与档案学&#39;, &#39;其他&#39;, &#39;图书情报&#39;, &#39;军事学&#39;, &#39;艺术学&#39;, &#39;党史•党建&#39;, &#39;马列•科社&#39;,
       &#39;新闻传播学&#39;, &#39;中国历史、&#39;, &#39;民族学&#39;, &#39;国际问题&#39;, &#39;法学、医学、公共卫生学&#39;, &#39;灾害学、社会学、管理学、系统科学&#39;,
       &#39;法学、医学、社会学&#39;, &#39;应用经济学 法学&#39;, &#39;宏观经济、计量经济、管理学等&#39;, &#39;智能技术、电子商务、人工智能、信&#39;,
       &#39;管理学、经济学、地理学&#39;, &#39;艺术学、人类学、计算机科学&#39;, &#39;文化人类学、非遗保护、考古学、影&#39;,
       &#39;文学艺术、文化人类学、计算机科学&#39;, &#39;计算机科学与技术、社会学、公共管&#39;, &#39;电气工程；产业经济学；管理学；热&#39;,
       &#39;城市规划学、计算机学、信息网络学&#39;, &#39;心理学、认知和行为科学、脑科学、&#39;, &#39;产业经济学、管理学、信息技术及应&#39;,
       &#39;法学、社会学、信息科学、计算机科&#39;, &#39;管理科学与工程、控制科学与工程、&#39;, &#39;智能技术、产业经济、经济学、管理&#39;,
       &#39;语言学、计算机科学、生态学、社会&#39;, &#39;理论经济学、应用经济学、法学、公&#39;, &#39;语言学、人类学、信息科学&#39;,
       &#39;宏观经济、计量经济、管理学、统计&#39;, &#39;管理学、 经济学 、环境科学、&#39;, &#39;语言学、计算机科学、统计学等&#39;,
       &#39;城乡规划学、管理学、地理学、经济&#39;, &#39;语言文学、心理学、教育学&#39;, &#39;人类学、社会心理学、认知神经科学&#39;,
       &#39;应用经济、管理学、资源环境科学、&#39;, &#39;电气工程、管理学、产业经济、能源&#39;, &#39;产业经济、生态学、系统科学、管理&#39;, &#39;马列科社&#39;,
       &#39;党史党建&#39;, &#39;综合研究&#39;, &#39;民族问题&#39;, &#39;图书·情报与文献&#39;, &#39;新闻学&#39;, &#39;跨学科&#39;, &#39;民族问题 研究&#39;,
       &#39;新闻与传播学&#39;, &#39;新闻学与 传播学&#39;, &#39;马列.科社&#39;, &#39;系列丛书&#39;, &#39;图书馆·情报与文献学&#39;, &#39;重点项目&#39;,
       &#39;一般项目&#39;, &#39;学术期刊&#39;, &#39;理论经济学&#39;, &#39;应用经济学&#39;, &#39;国际问题研\n究&#39;, &#39;新闻学与传\n播学&#39;,
       &#39;图书馆、情\n报与文献学&#39;], dtype=object)
</code></pre></div><br>
<h2 id="三简单分析">三、简单分析</h2>
<h3 id="31-可视化准备">3.1 可视化准备</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>

<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>
</code></pre></div><br>
<h3 id="32-立项数量">3.2 立项数量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df[&#39;批准年份&#39;].value_counts(ascending=True).plot(kind=&#39;bar&#39;, figsize=(8, 4))
plt.xticks(rotation=0)
plt.ylabel(&#39;立项数量&#39;, rotation=0)
plt.title(&#39;国社科立项数量(2010-2023)&#39;)
</code></pre></div><p><img loading="lazy" src="img/02-plot.png" alt=""  />
</p>
<br>
<h3 id="33-经管学科">3.3 经管学科</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">management_economic_displines</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;所在学科&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span> <span class="k">if</span> <span class="p">(</span><span class="s1">&#39;经济&#39;</span> <span class="ow">in</span> <span class="n">d</span><span class="p">)</span> <span class="ow">or</span>  <span class="p">(</span><span class="s1">&#39;管理&#39;</span> <span class="ow">in</span> <span class="n">d</span><span class="p">)]</span>
<span class="n">management_economic_displines</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;管理学&#39;,
 &#39;理论经济&#39;,
 &#39;应用经济&#39;,
 &#39;灾害学、社会学、管理学、系统科学&#39;,
 &#39;应用经济学 法学&#39;,
 &#39;宏观经济、计量经济、管理学等&#39;,
 &#39;管理学、经济学、地理学&#39;,
 &#39;电气工程；产业经济学；管理学；热&#39;,
 &#39;产业经济学、管理学、信息技术及应&#39;,
 &#39;管理科学与工程、控制科学与工程、&#39;,
 &#39;智能技术、产业经济、经济学、管理&#39;,
 &#39;理论经济学、应用经济学、法学、公&#39;,
 &#39;宏观经济、计量经济、管理学、统计&#39;,
 &#39;管理学、 经济学 、环境科学、&#39;,
 &#39;城乡规划学、管理学、地理学、经济&#39;,
 &#39;应用经济、管理学、资源环境科学、&#39;,
 &#39;电气工程、管理学、产业经济、能源&#39;,
 &#39;产业经济、生态学、系统科学、管理&#39;,
 &#39;理论经济学&#39;,
 &#39;应用经济学&#39;]
</code></pre></div><br>
<h3 id="34-经管立项">3.4 经管立项</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">eco_man_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;所在学科&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">management_economic_displines</span><span class="p">)]</span>
<span class="n">eco_man_df</span>
</code></pre></div><p><img loading="lazy" src="img/03-df.png" alt=""  />
</p>
<br>
<p>经管类国社科立项数量占比</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;所在学科&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">management_economic_displines</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.18713464870187335
</code></pre></div><br>
<h3 id="35-经管立项数量">3.5 经管立项数量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">eco_man_with_ds_df</span> <span class="o">=</span> <span class="n">eco_man_df</span><span class="p">[</span><span class="n">eco_man_df</span><span class="p">[</span><span class="s1">&#39;课题名称&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;大数据|数据挖掘|机器学习|人工智能|AIGC&#39;</span><span class="p">)]</span>
<span class="n">eco_man_with_ds_df</span><span class="p">[</span><span class="s1">&#39;批准年份&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;立项数量&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;国社科基金中结合数据科学的经济、管理类立项数量(2010-2023)&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/05-plot.png" alt=""  />
</p>
<br>
<h3 id="36-经管立项占比">3.6 经管立项占比</h3>
<p>按年度查看， 国社科中经管类立项占比</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">year_ratios</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;批准年份&#39;</span><span class="p">):</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;所在学科&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">management_economic_displines</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">year_df</span><span class="p">)</span>
    <span class="n">year_ratios</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">year</span><span class="p">,</span> <span class="n">ratio</span><span class="p">))</span>
    
<span class="n">year_ratio_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">year_ratios</span><span class="p">)</span>
<span class="n">year_ratio_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;ratio&#39;</span><span class="p">]</span>
<span class="n">year_ratio_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">year_ratio_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;立项占比&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;国社科基金中经济、管理类立项占比(2010-2023)&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/04-plot.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集(付费) | 三板上市公司年报2002-2023.12</title>
      <link>https://textdata.cn/blog/2024-01-18-neeq-china-listed-on-nation-equities-exchange-and-quotation-system-anunal-year-report/</link>
      <pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-01-18-neeq-china-listed-on-nation-equities-exchange-and-quotation-system-anunal-year-report/</guid>
      <description>&lt;h2 id=&#34;一数据集&#34;&gt;一、数据集&lt;/h2&gt;
&lt;h3 id=&#34;11-概况&#34;&gt;1.1 概况&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据来源: 全国中小企业股份转让系统(https://www.neeq.com.cn/）

覆盖时间: 2002-04-02 ~ 2023-12-06

年报数量: 70838

累积挂牌数量: 13884

数据集体积: 131G

文件格式: pdf、txt、csv(csv是一个汇总文件，方便数据分析)
   
    
csv所含字段:
 - code
 - year
 - text
 
 
 500元，支持开票；加微信 372335839， 备注「姓名-学校-专业」
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-dataset.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-pdf.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-txt.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h3 id=&#34;13--注意&#34;&gt;1.3  注意&lt;/h3&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;1. 付费数据集，500元，支持开票；加微信 372335839， 备注「姓名-学校-专业」。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;2. 数据是虚拟产品，一经售出，不再退还！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;3. 请仔细阅读推文内容， 再加微信详谈购买事宜 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二查看数据&#34;&gt;二、查看数据&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;三板年报.csv.gz&lt;/strong&gt;&lt;/em&gt; 是一个汇总的 csv 文件，特别适合进行数据分析。 解压后大概 15G， 如果你的电脑内存小于32G， &lt;a href=&#34;https://textdata.cn/blog/2023-11-17-how-handle-mega-csv-that-far-exceed-memory/&#34;&gt;推荐阅读 | 如何处理远超电脑内存的csv文件&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;21-读取数据&#34;&gt;2.1 读取数据&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;三板年报.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-记录数&#34;&gt;2.2 记录数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;70838
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;23--累计挂牌企业数量&#34;&gt;2.3  累计挂牌企业数量&lt;/h3&gt;
&lt;p&gt;累计挂牌企业数量&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nunique&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;13884
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;24-日期范围&#34;&gt;2.4 日期范围&lt;/h3&gt;
&lt;p&gt;数据集覆盖的日期范围&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#年报发布日期&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2002-04-02
2023-12-06
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;25-年度记录数&#34;&gt;2.5 年度记录数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2002 5
2003 6
2004 19
2005 29
2006 33
2007 48
2008 59
2009 80
2010 90
2011 107
2012 139
2013 225
2014 732
2015 2336
2016 6874
2017 10811
2018 10948
2019 9258
2020 8400
2021 6859
2022 7019
2023 6761
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;scienceplots&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;platform&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib_inline&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backend_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_matplotlib_formats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;png&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;svg&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;jieba&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;warnings&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;warnings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filterwarnings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ignore&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;science&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;no-latex&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;cjk-sc-font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;platform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取操作系统类型&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Windows&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;SimHei&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Darwin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Arial Unicode MS&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;sans-serif&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;font&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 设置全局字体&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bar&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;三板历年企业年报数&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三相关内容&#34;&gt;三、相关内容&lt;/h2&gt;
&lt;p&gt;想用 python 对 csv、xlsx 进行分析， 要学会尽量用 pandas 写代码。 以下是近期 pandas 的一些处理推文免费教程， 感兴趣的可以进去浏览浏览。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-17-how-handle-mega-csv-that-far-exceed-memory/&#34;&gt;推荐阅读 | 如何处理远超电脑内存的csv文件&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/&#34;&gt;数据集 | 2001-2022年A股上市公司年报&amp;amp;管理层讨论与分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/&#34;&gt;词向量(付费) | 使用MD&amp;amp;A2001-2022语料训练Word2Vec模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-01-21-hk-stock-market-anual-report/&#34;&gt;&lt;strong&gt;数据集 | 港股年报文本数据集(2007 ~ 2023.12)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-01-14-usa-sec-10k-report-dataset/&#34;&gt;&lt;strong&gt;数据集(付费) |  美股年报10-K、20-F数据(2000-2023.12)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-01-21-hk-stock-market-anual-report/&#34;&gt;数据集 |  港股年报文本数据集(2007 ~ 2023.12)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四获取数据&#34;&gt;四、获取数据&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;1. 付费数据集，500元；加微信 372335839， 备注「姓名-学校-专业」。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;2. 数据是虚拟产品，一经售出，不再退还！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;3. 请仔细阅读推文内容， 再加微信详谈购买事宜 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一数据集">一、数据集</h2>
<h3 id="11-概况">1.1 概况</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据来源: 全国中小企业股份转让系统(https://www.neeq.com.cn/）

覆盖时间: 2002-04-02 ~ 2023-12-06

年报数量: 70838

累积挂牌数量: 13884

数据集体积: 131G

文件格式: pdf、txt、csv(csv是一个汇总文件，方便数据分析)
   
    
csv所含字段:
 - code
 - year
 - text
 
 
 500元，支持开票；加微信 372335839， 备注「姓名-学校-专业」
</code></pre></div><p><img loading="lazy" src="img/01-dataset.png" alt=""  />
</p>
<p><img loading="lazy" src="img/02-pdf.png" alt=""  />
</p>
<p><img loading="lazy" src="img/03-txt.png" alt=""  />
</p>
<br>
<br>
<h3 id="13--注意">1.3  注意</h3>
<p><span style="font-size: 18px;color: green;">1. 付费数据集，500元，支持开票；加微信 372335839， 备注「姓名-学校-专业」。</span></p>
<p><span style="font-size: 18px;color: green;">2. 数据是虚拟产品，一经售出，不再退还！</span></p>
<p><span style="font-size: 18px;color: green;">3. 请仔细阅读推文内容， 再加微信详谈购买事宜 </span></p>
<p><br><br></p>
<h2 id="二查看数据">二、查看数据</h2>
<p><em><strong>三板年报.csv.gz</strong></em> 是一个汇总的 csv 文件，特别适合进行数据分析。 解压后大概 15G， 如果你的电脑内存小于32G， <a href="https://textdata.cn/blog/2023-11-17-how-handle-mega-csv-that-far-exceed-memory/">推荐阅读 | 如何处理远超电脑内存的csv文件</a></p>
<h3 id="21-读取数据">2.1 读取数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;三板年报.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/04-df.png" alt=""  />
</p>
<br>
<h3 id="22-记录数">2.2 记录数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python">
<span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">70838
</code></pre></div><br>
<h3 id="23--累计挂牌企业数量">2.3  累计挂牌企业数量</h3>
<p>累计挂牌企业数量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;code&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">13884
</code></pre></div><br>
<h3 id="24-日期范围">2.4 日期范围</h3>
<p>数据集覆盖的日期范围</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">])</span>

<span class="c1">#年报发布日期</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2002-04-02
2023-12-06
</code></pre></div><br>
<h3 id="25-年度记录数">2.5 年度记录数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">year</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">year_df</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2002 5
2003 6
2004 19
2005 29
2006 33
2007 48
2008 59
2009 80
2010 90
2011 107
2012 139
2013 225
2014 732
2015 2336
2016 6874
2017 10811
2018 10948
2019 9258
2020 8400
2021 6859
2022 7019
2023 6761
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>
<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>


<span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;三板历年企业年报数&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/05-plot.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三相关内容">三、相关内容</h2>
<p>想用 python 对 csv、xlsx 进行分析， 要学会尽量用 pandas 写代码。 以下是近期 pandas 的一些处理推文免费教程， 感兴趣的可以进去浏览浏览。</p>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-17-how-handle-mega-csv-that-far-exceed-memory/">推荐阅读 | 如何处理远超电脑内存的csv文件</a></li>
<li><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/">数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析</a></li>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2024-01-21-hk-stock-market-anual-report/"><strong>数据集 | 港股年报文本数据集(2007 ~ 2023.12)</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-14-usa-sec-10k-report-dataset/"><strong>数据集(付费) |  美股年报10-K、20-F数据(2000-2023.12)</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-21-hk-stock-market-anual-report/">数据集 |  港股年报文本数据集(2007 ~ 2023.12)</a></li>
</ul>
<p><br><br></p>
<h2 id="四获取数据">四、获取数据</h2>
<p><span style="font-size: 18px;color: green;">1. 付费数据集，500元；加微信 372335839， 备注「姓名-学校-专业」。</span></p>
<p><span style="font-size: 18px;color: green;">2. 数据是虚拟产品，一经售出，不再退还！</span></p>
<p><span style="font-size: 18px;color: green;">3. 请仔细阅读推文内容， 再加微信详谈购买事宜 </span></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>词向量  | 使用人民网领导留言板语料训练Word2Vec模型</title>
      <link>https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/</link>
      <pubDate>Thu, 28 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/</guid>
      <description>&lt;p&gt;本文使用 3.88G 语料训练得到词汇量近 150w 的 Word2Vec 模型，使用该模型，可以用于寻找近义词，扩展(构建)概念词典。 &lt;strong&gt;该Word2Vec模型文件可在文末免费下载&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一构建语料&#34;&gt;一、构建语料&lt;/h2&gt;
&lt;p&gt;使用 &lt;a href=&#34;https://textdata.cn/blog/2023-12-22-renmin-gov-leader-comment-board/&#34;&gt;&lt;strong&gt;数据集(付费) | 人民网地方领导留言板原始文本(2011-2023.12)&lt;/strong&gt;&lt;/a&gt; 来构建本文的语料。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2011-2019.csv.gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2020-2023.csv.gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;text_series1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;回复内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text_series1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text_series2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;回复内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text_series2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;renmin_board.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;a+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text_series1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tolist&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text_series2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tolist&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;最终得到 3.88 G 的 &lt;strong&gt;renmin_board.txt&lt;/strong&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二训练模型&#34;&gt;二、训练模型&lt;/h2&gt;
&lt;h3 id=&#34;21-配置cntext&#34;&gt;2.1 配置cntext&lt;/h3&gt;
&lt;p&gt;使用 cntext 2.0.0 或者 cntext 2.1.0 ， 已购买 cntext2.0.0 的同学可以找我更新至 2.1.0 ，微信372335839， 备注「姓名-学校-专业」。&lt;/p&gt;
&lt;p&gt;将 &lt;strong&gt;cntext-2.1.0-py3-none-any.whl&lt;/strong&gt; 放置于桌面， 打开 **命令行cmd **(苹果电脑terminal)，依次执行以下命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;desktop&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pip3&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cntext&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2.1.0&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;py3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;none&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;any&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;whl&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-训练word2vec&#34;&gt;2.2 训练Word2Vec&lt;/h3&gt;
&lt;p&gt;训练 word2vec 代码已封装 cntext2， 所以代码很简洁，只有三行代码。&lt;/p&gt;
&lt;p&gt;训练环境win11,  内存128G。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2vModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;renmin_board.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;renmin_board.txt

Start Preprocessing Corpus...
Start Training! This may take a while. Please be patient...

Training word2vec model took 12779 seconds

Note: The Word2Vec model has been saved to output/Word2Vec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 3.88G 的renmin_board.txt，训练了 12779 秒， 约 3.5 小时。在Python代码文件所在的文件夹内，出现了 output/Word2Vec 文件夹，打开可以看到训练好的模型， 可以看出模型文件的体量还是很大的。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-word2vec.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三使用模型&#34;&gt;三、使用模型&lt;/h2&gt;
&lt;h3 id=&#34;31-读取模型&#34;&gt;3.1 读取模型&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/Word2Vec/renmin_board.200.6.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Loading word2vec model...
&amp;lt;gensim.models.word2vec.Word2Vec at 0x2a11dfad0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-模型词汇量&#34;&gt;3.2 模型词汇量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#词汇量&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;1499961
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;33-查看词表&#34;&gt;3.3 查看词表&lt;/h3&gt;
&lt;p&gt;因为词表有 1499961 个词， 为了方便，这里只显示前20个词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;##词表带顺序的
list(w2v.wv.key_to_index.keys())[:20]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39; &amp;#39;,
 &amp;#39;\n&amp;#39;,
 &amp;#39;问题&amp;#39;,
 &amp;#39;进行&amp;#39;,
 &amp;#39;小区&amp;#39;,
 &amp;#39;工作&amp;#39;,
 &amp;#39;”&amp;#39;,
 &amp;#39;没有&amp;#39;,
 &amp;#39;情况&amp;#39;,
 &amp;#39;目前&amp;#39;,
 &amp;#39;反映&amp;#39;,
 &amp;#39;业主&amp;#39;,
 &amp;#39;项目&amp;#39;,
 &amp;#39;要求&amp;#39;,
 &amp;#39;“&amp;#39;,
 &amp;#39;公司&amp;#39;,
 &amp;#39;网友您好&amp;#39;,
 &amp;#39;现在&amp;#39;,
 &amp;#39;建设&amp;#39;,
 &amp;#39;反映问题&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;34-获取某词的向量&#34;&gt;3.4 获取某词的向量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#w2v.wv[&amp;#39;利民&amp;#39;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;利民&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([-0.72336054,  0.5448769 ,  0.02187554,  0.18723099,  0.10518928,
       -0.4829346 ,  1.2029709 ,  1.325142  ,  1.7153364 , -0.9134816 ,
        0.21033671, -0.05412149,  0.1750608 ,  0.36092624,  0.24550831,
        0.02644009,  0.95183885, -1.0317421 , -0.10972459, -2.5780423 ,
       -0.89232576, -1.043176  ,  0.72673726, -0.17512426, -0.24233247,
        0.2569658 , -1.0063888 ,  0.5180029 ,  0.83510065,  0.8907923 ,
       -0.24386375, -0.53083295, -1.5156878 , -0.9040948 ,  0.25330988,
       -0.79177266,  0.06866979,  0.6199285 ,  0.9562961 ,  3.6091647 ,
       -1.3558179 ,  1.4279033 , -0.6923549 ,  0.17637855,  0.6416902 ,
        0.8726301 , -0.8316238 ,  0.8974303 , -1.342718  ,  0.3960099 ,
        0.7404184 ,  0.41476634,  0.5397854 , -0.9964916 ,  0.72252625,
       -0.24338841, -1.1583921 , -0.8719721 , -0.1476895 ,  0.4893649 ,
        1.0152714 , -3.2469108 ,  0.61867106, -1.1033677 ,  0.7277995 ,
        0.68194056,  1.9562886 , -2.0847485 ,  1.5790684 ,  0.9881281 ,
       -1.6833613 ,  0.52788144,  0.81453127, -0.72605026,  0.67317885,
        0.4130878 ,  0.5682669 , -0.14777663,  0.6144105 , -0.6402672 ,
       -0.8752994 ,  1.6374044 , -0.66893923,  0.5865543 ,  0.6375472 ,
       -0.99829054, -1.0806116 ,  2.6740906 , -0.7968034 , -0.39872456,
       -2.0882657 ,  0.4091569 ,  0.44333985,  0.80311924, -0.02302606,
       -0.2762922 ,  0.172768  ,  2.2813802 , -0.39281502,  0.57268375,
        1.4626628 , -0.14473361,  0.5739576 ,  0.61773837, -0.18331125,
        1.2602748 ,  0.9424055 ,  1.5969577 ,  0.6106542 , -2.7610633 ,
       -1.1409078 , -1.7803516 , -0.3264908 ,  1.2968934 ,  0.7250817 ,
        0.0589628 ,  0.42458364, -0.3242822 , -2.6474693 ,  0.3660026 ,
        0.5749114 ,  0.1812738 ,  0.34291452, -0.20228535,  0.40417868,
        0.06284425,  0.7266579 ,  1.5118539 ,  2.0363107 , -1.1808697 ,
       -0.19834429, -1.105297  ,  0.7594476 , -0.90230256,  0.13537973,
        1.5452795 ,  1.3571783 ,  0.15807565, -1.0794616 ,  2.3592122 ,
        0.62628454, -0.61704504,  0.65674806, -0.91116625, -2.1521432 ,
       -0.08805666, -0.6956923 , -1.4443843 , -0.84095645,  0.64748996,
       -0.7432282 ,  1.7160741 ,  1.1697325 ,  1.0834908 , -1.0323627 ,
       -1.3480235 ,  1.004517  , -0.40515316,  0.38016117,  1.6717825 ,
       -0.40651798,  1.0373042 ,  0.24744533, -2.353417  ,  0.06758213,
        0.34440002,  0.8656946 ,  0.76431715, -1.7378451 ,  1.2329959 ,
       -1.4538856 ,  1.0956937 ,  0.6151345 ,  2.4905207 , -0.24415112,
       -0.23886327,  0.09834331,  0.00791643, -0.53527415,  0.7039957 ,
        0.83675224, -1.5712336 , -0.14135051,  0.34811664,  0.41304144,
        0.78504366, -0.13325912, -0.9898512 , -0.497319  , -0.32992417,
       -0.58120775,  0.29686695, -0.9618549 ,  0.39253774,  0.14620592,
       -0.45337242,  0.69179136,  0.1934781 , -2.0494404 ,  1.8545331 ],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;需要注意，如果查询的词不存在于模型词表，则会出现报错。例如&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;word = &amp;#39;这是一个不存在的词&amp;#39;
w2v.wv.get_vector(word)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[130], line 2
      1 word = &amp;#39;这是一个不存在的词&amp;#39;
----&amp;gt; 2 w2v.wv.get_vector(word)

File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/models/keyedvectors.py:446, in KeyedVectors.get_vector(self, key, norm)
    422 def get_vector(self, key, norm=False):
    423     &amp;#34;&amp;#34;&amp;#34;Get the key&amp;#39;s vector, as a 1D numpy array.
    424 
    425     Parameters
   (...)
    444 
    445     &amp;#34;&amp;#34;&amp;#34;
--&amp;gt; 446     index = self.get_index(key)
    447     if norm:
    448         self.fill_norms()

File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/models/keyedvectors.py:420, in KeyedVectors.get_index(self, key, default)
    418     return default
    419 else:
--&amp;gt; 420     raise KeyError(f&amp;#34;Key &amp;#39;{key}&amp;#39; not present&amp;#34;)

KeyError: &amp;#34;Key &amp;#39;这是一个不存在的词&amp;#39; not present&amp;#34;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;35-查询近义词&#34;&gt;3.5 查询近义词&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;w2v.wv.most_similar(positive=None, topn=10)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;positive 待查的词语列表或者词向量&lt;/li&gt;
&lt;li&gt;topn 显示返回多少个近义词&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;351-使用词语列表查询&#34;&gt;3.5.1 使用词语列表查询&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;经济&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;建设&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;发展&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
                    &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;经济发展&amp;#39;, 0.7514141201972961),
 (&amp;#39;产业发展&amp;#39;, 0.6954267024993896),
 (&amp;#39;发展壮大&amp;#39;, 0.6707271337509155),
 (&amp;#39;社会发展&amp;#39;, 0.6637671589851379),
 (&amp;#39;发展重要&amp;#39;, 0.6603672504425049),
 (&amp;#39;城镇化发展&amp;#39;, 0.6574274301528931),
 (&amp;#39;城市发展&amp;#39;, 0.6558148264884949),
 (&amp;#39;高质量发展&amp;#39;, 0.6517276167869568),
 (&amp;#39;大力发展&amp;#39;, 0.6500106453895569),
 (&amp;#39;产业&amp;#39;, 0.6494895219802856),
 (&amp;#39;发展产业&amp;#39;, 0.6458864212036133),
 (&amp;#39;壮大&amp;#39;, 0.6379123330116272),
 (&amp;#39;发展带动&amp;#39;, 0.6357436776161194),
 (&amp;#39;未来发展&amp;#39;, 0.6351119875907898),
 (&amp;#39;第三产业&amp;#39;, 0.6345765590667725),
 (&amp;#39;经济增长&amp;#39;, 0.6329594850540161),
 (&amp;#39;改革开放&amp;#39;, 0.6297498345375061),
 (&amp;#39;融合发展&amp;#39;, 0.6290864944458008),
 (&amp;#39;长远发展&amp;#39;, 0.6279110908508301),
 (&amp;#39;经济繁荣&amp;#39;, 0.627375602722168)]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;352-使用词向量查询&#34;&gt;3.5.2 使用词向量查询&lt;/h3&gt;
&lt;p&gt;先构建一个函数concept_vector，该函数可以将多个词转化为一个向量。 遇到词语不在词表中的异常，也能正常运行。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;concept_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;container&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zeros&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;container&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;container&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;pass&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;container&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
  
  
&lt;span class=&#34;n&#34;&gt;word_vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;concept_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;她&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;她们&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;母亲&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;奶奶&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;女性&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;女人&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#查找与word_vec近义词10个词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word_vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;奶奶&amp;#39;, 0.9064152836799622),
 (&amp;#39;母亲&amp;#39;, 0.9003509879112244),
 (&amp;#39;爷爷&amp;#39;, 0.8559296131134033),
 (&amp;#39;婆婆&amp;#39;, 0.846263587474823),
 (&amp;#39;我妈&amp;#39;, 0.8314375877380371),
 (&amp;#39;老伴&amp;#39;, 0.8306034803390503),
 (&amp;#39;老父亲&amp;#39;, 0.8257972598075867),
 (&amp;#39;姥爷&amp;#39;, 0.8255906701087952),
 (&amp;#39;父亲&amp;#39;, 0.821728527545929),
 (&amp;#39;女孩&amp;#39;, 0.8210363984107971)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四-相关&#34;&gt;四、 相关&lt;/h2&gt;
&lt;h3 id=&#34;41-文献资料&#34;&gt;4.1 文献资料&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;郑石明, 兰雨潇, 黎枫. 网络公共舆论与政府回应的互动逻辑——基于新冠肺炎疫情期间“领导留言板”的数据分析[J]. 公共管理学报, 2021, 18 (03): 24-37+169.
王磊,易扬.公共卫生危机中的数字政府回应如何纾解网络负面舆情——基于人民网“领导留言板”回复情况的调查[J].公共管理学报,2022,19(04):65-78+169.
Lu, Liangdong, Jia Xu, and Jiuchang Wei. &amp;#34;Understanding the effects of the textual complexity on government communication: Insights from China’s online public service platform.&amp;#34; Telematics and Informatics 83 (2023): 102028.
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-代码资料&#34;&gt;4.2 代码资料&lt;/h3&gt;
&lt;p&gt;想用 python 对 csv、xlsx 进行分析， 要学会尽量用 pandas 写代码。 以下是近期 pandas 的一些处理推文免费教程， 感兴趣的可以进去浏览浏览。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-17-how-to-generate-panel-data-from-gov-report-dataset/&#34;&gt;&lt;strong&gt;代码 | 使用地方gov工作报告生成某类概念词频「面板数据」&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-18-how-to-generate-panel-data-from-daily-news-dataset/&#34;&gt;&lt;strong&gt;代码 | 使用「新闻数据」构造概念词提及量「面板数据」&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-02-26-cctv1-xwlb-news-text-dataset/&#34;&gt;&lt;strong&gt;数据代码| 使用cctv新闻联播文稿构造「面板数据」&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2028-12-18-how-to-extract-data-from-patent-application-dataset/&#34;&gt;&lt;strong&gt;代码 | 使用3571w专利申请数据集构造「面板数据」&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/&#34;&gt;&lt;strong&gt;代码 | 使用「新闻数据」计算 「经济政策不确定性」指数&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/&#34;&gt;OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;数据集(付费) | 人民网地方领导留言板原始文本(2011-2023.12)  2023-12-22-renmin-gov-leader-comment-board&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五获取资料&#34;&gt;五、获取资料&lt;/h2&gt;
&lt;h3 id=&#34;51-免费资料&#34;&gt;5.1 免费资料&lt;/h3&gt;
&lt;p&gt;本文训练所得的 &lt;strong&gt;renmin_board.200.6.bin模型文件&lt;/strong&gt; 免费开源，&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-word2vec.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&#34;https://pan.baidu.com/s/1u-eUuATCTSIDjhOvSaG7sA?pwd=whf5&#34;&gt;https://pan.baidu.com/s/1u-eUuATCTSIDjhOvSaG7sA?pwd=whf5&lt;/a&gt; 提取码: whf5&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;52-付费资料&#34;&gt;5.2 付费资料&lt;/h3&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;1.  &lt;a href=&#34;https://textdata.cn/blog/2023-12-22-renmin-gov-leader-comment-board/&#34;&gt;数据集(付费) | 人民网地方领导留言板原始文本(2011-2023.12)&lt;/a&gt; ，2000元&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;2. cntext2.1.0 价格100元，已购买2.0.0可免费更新至2.1.0&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;3. 数据是虚拟产品，一经售出，不再退还！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;4. 大家时间其实都很宝贵，请仔细阅读推文内容， 确认无误再加微信372335839， 备注「姓名-学校-专业」详谈购买事宜 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>本文使用 3.88G 语料训练得到词汇量近 150w 的 Word2Vec 模型，使用该模型，可以用于寻找近义词，扩展(构建)概念词典。 <strong>该Word2Vec模型文件可在文末免费下载</strong></p>
<p><br><br></p>
<h2 id="一构建语料">一、构建语料</h2>
<p>使用 <a href="https://textdata.cn/blog/2023-12-22-renmin-gov-leader-comment-board/"><strong>数据集(付费) | 人民网地方领导留言板原始文本(2011-2023.12)</strong></a> 来构建本文的语料。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;2011-2019.csv.gzip&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;2020-2023.csv.gzip&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>


<span class="n">text_series1</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="s1">&#39;留言内容&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">df1</span><span class="p">[</span><span class="s1">&#39;回复内容&#39;</span><span class="p">]</span>
<span class="n">text_series1</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">text_series2</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;留言内容&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;回复内容&#39;</span><span class="p">]</span>
<span class="n">text_series2</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;renmin_board.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text_series1</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span> <span class="o">+</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text_series2</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>最终得到 3.88 G 的 <strong>renmin_board.txt</strong> 。</p>
<p><br><br></p>
<h2 id="二训练模型">二、训练模型</h2>
<h3 id="21-配置cntext">2.1 配置cntext</h3>
<p>使用 cntext 2.0.0 或者 cntext 2.1.0 ， 已购买 cntext2.0.0 的同学可以找我更新至 2.1.0 ，微信372335839， 备注「姓名-学校-专业」。</p>
<p>将 <strong>cntext-2.1.0-py3-none-any.whl</strong> 放置于桌面， 打开 **命令行cmd **(苹果电脑terminal)，依次执行以下命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">cd</span> <span class="n">desktop</span>
<span class="n">pip3</span> <span class="n">install</span> <span class="n">cntext</span><span class="o">-</span><span class="mf">2.1.0</span><span class="o">-</span><span class="n">py3</span><span class="o">-</span><span class="n">none</span><span class="o">-</span><span class="nb">any</span><span class="o">.</span><span class="n">whl</span>
</code></pre></div><br>
<h3 id="22-训练word2vec">2.2 训练Word2Vec</h3>
<p>训练 word2vec 代码已封装 cntext2， 所以代码很简洁，只有三行代码。</p>
<p>训练环境win11,  内存128G。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2vModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;renmin_board.txt&#39;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">window_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">renmin_board.txt

Start Preprocessing Corpus...
Start Training! This may take a while. Please be patient...

Training word2vec model took 12779 seconds

Note: The Word2Vec model has been saved to output/Word2Vec
</code></pre></div><p>使用 3.88G 的renmin_board.txt，训练了 12779 秒， 约 3.5 小时。在Python代码文件所在的文件夹内，出现了 output/Word2Vec 文件夹，打开可以看到训练好的模型， 可以看出模型文件的体量还是很大的。</p>
<p><img loading="lazy" src="img/02-word2vec.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三使用模型">三、使用模型</h2>
<h3 id="31-读取模型">3.1 读取模型</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="n">ct</span>

<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/Word2Vec/renmin_board.200.6.bin&#39;</span><span class="p">)</span>
<span class="n">w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading word2vec model...
&lt;gensim.models.word2vec.Word2Vec at 0x2a11dfad0&gt;
</code></pre></div><br>
<h3 id="32-模型词汇量">3.2 模型词汇量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#词汇量</span>
<span class="nb">len</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1499961
</code></pre></div><br>
<h3 id="33-查看词表">3.3 查看词表</h3>
<p>因为词表有 1499961 个词， 为了方便，这里只显示前20个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">##词表带顺序的
list(w2v.wv.key_to_index.keys())[:20]
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39; &#39;,
 &#39;\n&#39;,
 &#39;问题&#39;,
 &#39;进行&#39;,
 &#39;小区&#39;,
 &#39;工作&#39;,
 &#39;”&#39;,
 &#39;没有&#39;,
 &#39;情况&#39;,
 &#39;目前&#39;,
 &#39;反映&#39;,
 &#39;业主&#39;,
 &#39;项目&#39;,
 &#39;要求&#39;,
 &#39;“&#39;,
 &#39;公司&#39;,
 &#39;网友您好&#39;,
 &#39;现在&#39;,
 &#39;建设&#39;,
 &#39;反映问题&#39;]
</code></pre></div><br>
<h3 id="34-获取某词的向量">3.4 获取某词的向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#w2v.wv[&#39;利民&#39;]</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;利民&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.72336054,  0.5448769 ,  0.02187554,  0.18723099,  0.10518928,
       -0.4829346 ,  1.2029709 ,  1.325142  ,  1.7153364 , -0.9134816 ,
        0.21033671, -0.05412149,  0.1750608 ,  0.36092624,  0.24550831,
        0.02644009,  0.95183885, -1.0317421 , -0.10972459, -2.5780423 ,
       -0.89232576, -1.043176  ,  0.72673726, -0.17512426, -0.24233247,
        0.2569658 , -1.0063888 ,  0.5180029 ,  0.83510065,  0.8907923 ,
       -0.24386375, -0.53083295, -1.5156878 , -0.9040948 ,  0.25330988,
       -0.79177266,  0.06866979,  0.6199285 ,  0.9562961 ,  3.6091647 ,
       -1.3558179 ,  1.4279033 , -0.6923549 ,  0.17637855,  0.6416902 ,
        0.8726301 , -0.8316238 ,  0.8974303 , -1.342718  ,  0.3960099 ,
        0.7404184 ,  0.41476634,  0.5397854 , -0.9964916 ,  0.72252625,
       -0.24338841, -1.1583921 , -0.8719721 , -0.1476895 ,  0.4893649 ,
        1.0152714 , -3.2469108 ,  0.61867106, -1.1033677 ,  0.7277995 ,
        0.68194056,  1.9562886 , -2.0847485 ,  1.5790684 ,  0.9881281 ,
       -1.6833613 ,  0.52788144,  0.81453127, -0.72605026,  0.67317885,
        0.4130878 ,  0.5682669 , -0.14777663,  0.6144105 , -0.6402672 ,
       -0.8752994 ,  1.6374044 , -0.66893923,  0.5865543 ,  0.6375472 ,
       -0.99829054, -1.0806116 ,  2.6740906 , -0.7968034 , -0.39872456,
       -2.0882657 ,  0.4091569 ,  0.44333985,  0.80311924, -0.02302606,
       -0.2762922 ,  0.172768  ,  2.2813802 , -0.39281502,  0.57268375,
        1.4626628 , -0.14473361,  0.5739576 ,  0.61773837, -0.18331125,
        1.2602748 ,  0.9424055 ,  1.5969577 ,  0.6106542 , -2.7610633 ,
       -1.1409078 , -1.7803516 , -0.3264908 ,  1.2968934 ,  0.7250817 ,
        0.0589628 ,  0.42458364, -0.3242822 , -2.6474693 ,  0.3660026 ,
        0.5749114 ,  0.1812738 ,  0.34291452, -0.20228535,  0.40417868,
        0.06284425,  0.7266579 ,  1.5118539 ,  2.0363107 , -1.1808697 ,
       -0.19834429, -1.105297  ,  0.7594476 , -0.90230256,  0.13537973,
        1.5452795 ,  1.3571783 ,  0.15807565, -1.0794616 ,  2.3592122 ,
        0.62628454, -0.61704504,  0.65674806, -0.91116625, -2.1521432 ,
       -0.08805666, -0.6956923 , -1.4443843 , -0.84095645,  0.64748996,
       -0.7432282 ,  1.7160741 ,  1.1697325 ,  1.0834908 , -1.0323627 ,
       -1.3480235 ,  1.004517  , -0.40515316,  0.38016117,  1.6717825 ,
       -0.40651798,  1.0373042 ,  0.24744533, -2.353417  ,  0.06758213,
        0.34440002,  0.8656946 ,  0.76431715, -1.7378451 ,  1.2329959 ,
       -1.4538856 ,  1.0956937 ,  0.6151345 ,  2.4905207 , -0.24415112,
       -0.23886327,  0.09834331,  0.00791643, -0.53527415,  0.7039957 ,
        0.83675224, -1.5712336 , -0.14135051,  0.34811664,  0.41304144,
        0.78504366, -0.13325912, -0.9898512 , -0.497319  , -0.32992417,
       -0.58120775,  0.29686695, -0.9618549 ,  0.39253774,  0.14620592,
       -0.45337242,  0.69179136,  0.1934781 , -2.0494404 ,  1.8545331 ],
      dtype=float32)
</code></pre></div><br>
<p>需要注意，如果查询的词不存在于模型词表，则会出现报错。例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">word = &#39;这是一个不存在的词&#39;
w2v.wv.get_vector(word)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[130], line 2
      1 word = &#39;这是一个不存在的词&#39;
----&gt; 2 w2v.wv.get_vector(word)

File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/models/keyedvectors.py:446, in KeyedVectors.get_vector(self, key, norm)
    422 def get_vector(self, key, norm=False):
    423     &#34;&#34;&#34;Get the key&#39;s vector, as a 1D numpy array.
    424 
    425     Parameters
   (...)
    444 
    445     &#34;&#34;&#34;
--&gt; 446     index = self.get_index(key)
    447     if norm:
    448         self.fill_norms()

File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/models/keyedvectors.py:420, in KeyedVectors.get_index(self, key, default)
    418     return default
    419 else:
--&gt; 420     raise KeyError(f&#34;Key &#39;{key}&#39; not present&#34;)

KeyError: &#34;Key &#39;这是一个不存在的词&#39; not present&#34;

</code></pre></div><br>
<h3 id="35-查询近义词">3.5 查询近义词</h3>
<p><strong>w2v.wv.most_similar(positive=None, topn=10)</strong></p>
<ul>
<li>positive 待查的词语列表或者词向量</li>
<li>topn 显示返回多少个近义词</li>
</ul>
<h4 id="351-使用词语列表查询">3.5.1 使用词语列表查询</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;经济&#39;</span><span class="p">,</span> <span class="s1">&#39;建设&#39;</span><span class="p">,</span> <span class="s1">&#39;发展&#39;</span><span class="p">],</span> 
                    <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;经济发展&#39;, 0.7514141201972961),
 (&#39;产业发展&#39;, 0.6954267024993896),
 (&#39;发展壮大&#39;, 0.6707271337509155),
 (&#39;社会发展&#39;, 0.6637671589851379),
 (&#39;发展重要&#39;, 0.6603672504425049),
 (&#39;城镇化发展&#39;, 0.6574274301528931),
 (&#39;城市发展&#39;, 0.6558148264884949),
 (&#39;高质量发展&#39;, 0.6517276167869568),
 (&#39;大力发展&#39;, 0.6500106453895569),
 (&#39;产业&#39;, 0.6494895219802856),
 (&#39;发展产业&#39;, 0.6458864212036133),
 (&#39;壮大&#39;, 0.6379123330116272),
 (&#39;发展带动&#39;, 0.6357436776161194),
 (&#39;未来发展&#39;, 0.6351119875907898),
 (&#39;第三产业&#39;, 0.6345765590667725),
 (&#39;经济增长&#39;, 0.6329594850540161),
 (&#39;改革开放&#39;, 0.6297498345375061),
 (&#39;融合发展&#39;, 0.6290864944458008),
 (&#39;长远发展&#39;, 0.6279110908508301),
 (&#39;经济繁荣&#39;, 0.627375602722168)]

</code></pre></div><br>
<h3 id="352-使用词向量查询">3.5.2 使用词向量查询</h3>
<p>先构建一个函数concept_vector，该函数可以将多个词转化为一个向量。 遇到词语不在词表中的异常，也能正常运行。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">concept_vector</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
    <span class="n">container</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">container</span> <span class="o">=</span> <span class="n">container</span> <span class="o">+</span> <span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="k">return</span> <span class="n">container</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
  
  
<span class="n">word_vec</span> <span class="o">=</span> <span class="n">concept_vector</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;她&#39;</span><span class="p">,</span> <span class="s1">&#39;她们&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">,</span> <span class="s1">&#39;女性&#39;</span><span class="p">,</span> <span class="s1">&#39;女人&#39;</span><span class="p">])</span>
<span class="c1">#查找与word_vec近义词10个词</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">word_vec</span><span class="p">,</span> 
                    <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;奶奶&#39;, 0.9064152836799622),
 (&#39;母亲&#39;, 0.9003509879112244),
 (&#39;爷爷&#39;, 0.8559296131134033),
 (&#39;婆婆&#39;, 0.846263587474823),
 (&#39;我妈&#39;, 0.8314375877380371),
 (&#39;老伴&#39;, 0.8306034803390503),
 (&#39;老父亲&#39;, 0.8257972598075867),
 (&#39;姥爷&#39;, 0.8255906701087952),
 (&#39;父亲&#39;, 0.821728527545929),
 (&#39;女孩&#39;, 0.8210363984107971)]
</code></pre></div><p><br><br></p>
<h2 id="四-相关">四、 相关</h2>
<h3 id="41-文献资料">4.1 文献资料</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">郑石明, 兰雨潇, 黎枫. 网络公共舆论与政府回应的互动逻辑——基于新冠肺炎疫情期间“领导留言板”的数据分析[J]. 公共管理学报, 2021, 18 (03): 24-37+169.
王磊,易扬.公共卫生危机中的数字政府回应如何纾解网络负面舆情——基于人民网“领导留言板”回复情况的调查[J].公共管理学报,2022,19(04):65-78+169.
Lu, Liangdong, Jia Xu, and Jiuchang Wei. &#34;Understanding the effects of the textual complexity on government communication: Insights from China’s online public service platform.&#34; Telematics and Informatics 83 (2023): 102028.
...
</code></pre></div><br>
<h3 id="42-代码资料">4.2 代码资料</h3>
<p>想用 python 对 csv、xlsx 进行分析， 要学会尽量用 pandas 写代码。 以下是近期 pandas 的一些处理推文免费教程， 感兴趣的可以进去浏览浏览。</p>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-12-17-how-to-generate-panel-data-from-gov-report-dataset/"><strong>代码 | 使用地方gov工作报告生成某类概念词频「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-18-how-to-generate-panel-data-from-daily-news-dataset/"><strong>代码 | 使用「新闻数据」构造概念词提及量「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-26-cctv1-xwlb-news-text-dataset/"><strong>数据代码| 使用cctv新闻联播文稿构造「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2028-12-18-how-to-extract-data-from-patent-application-dataset/"><strong>代码 | 使用3571w专利申请数据集构造「面板数据」</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/"><strong>代码 | 使用「新闻数据」计算 「经济政策不确定性」指数</strong></a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></p>
</li>
</ul>
<p>数据集(付费) | 人民网地方领导留言板原始文本(2011-2023.12)  2023-12-22-renmin-gov-leader-comment-board</p>
<p><br><br></p>
<h2 id="五获取资料">五、获取资料</h2>
<h3 id="51-免费资料">5.1 免费资料</h3>
<p>本文训练所得的 <strong>renmin_board.200.6.bin模型文件</strong> 免费开源，</p>
<p><img loading="lazy" src="img/02-word2vec.png" alt=""  />
</p>
<p>链接: <a href="https://pan.baidu.com/s/1u-eUuATCTSIDjhOvSaG7sA?pwd=whf5">https://pan.baidu.com/s/1u-eUuATCTSIDjhOvSaG7sA?pwd=whf5</a> 提取码: whf5</p>
<br>
<h3 id="52-付费资料">5.2 付费资料</h3>
<p><span style="font-size: 18px;color: green;">1.  <a href="https://textdata.cn/blog/2023-12-22-renmin-gov-leader-comment-board/">数据集(付费) | 人民网地方领导留言板原始文本(2011-2023.12)</a> ，2000元</span></p>
<p><span style="font-size: 18px;color: green;">2. cntext2.1.0 价格100元，已购买2.0.0可免费更新至2.1.0</span></p>
<p><span style="font-size: 18px;color: green;">3. 数据是虚拟产品，一经售出，不再退还！</span></p>
<p><span style="font-size: 18px;color: green;">4. 大家时间其实都很宝贵，请仔细阅读推文内容， 确认无误再加微信372335839， 备注「姓名-学校-专业」详谈购买事宜 </span></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Polars库 | 最强 Pandas 平替来了</title>
      <link>https://textdata.cn/blog/2023-12-27-polars-tutorial-an-altertaive-of-pandas/</link>
      <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-12-27-polars-tutorial-an-altertaive-of-pandas/</guid>
      <description>&lt;h2 id=&#34;一介绍&#34;&gt;一、介绍&lt;/h2&gt;
&lt;p&gt;Polars 是一个用于操作结构化数据的高性能 DataFrame 库，由于 Polars 是从0开始用Rust编写，紧密与机器结合。其矢量化和列式处理可在现代处理器上实现缓存一致性算法和高性能。如果您经常使用 pandas，那么用起 Polars 会感觉很轻松，可以说是平替 Pandas 最有潜质的包。&lt;/p&gt;
&lt;p&gt;Polars 在独立的 TPCH 基准测试中与其他几个解决方案进行了基准测试。该基准测试旨在复制实践中使用的数据整理操作。由于其并行执行引擎、高效算法以及 SIMD（单指令、多数据）矢量化的使用，Polars 轻松胜过其他解决方案。&lt;strong&gt;与pandas相比，它可以实现30倍以上的性能提升&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Polars 的目标是提供一个闪电般快速的&lt;code&gt;DataFrame&lt;/code&gt;库：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;利用机器上所有可用的内核。&lt;/li&gt;
&lt;li&gt;优化查询以减少不必要的工作/内存分配。&lt;/li&gt;
&lt;li&gt;处理比可用 RAM 大得多的数据集。&lt;/li&gt;
&lt;li&gt;拥有一致且可预测的 API。&lt;/li&gt;
&lt;li&gt;具有严格的架构（在运行查询之前应该知道数据类型）。&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;&lt;a href=&#34;https://pola-rs.github.io/polars/user-guide/&#34;&gt;User guide: https://pola-rs.github.io/polars/user-guide/&lt;/a&gt;
&lt;a href=&#34;https://pola-rs.github.io/polars/py-polars/html/reference/io.html&#34;&gt;API reference: https://pola-rs.github.io/polars/py-polars/html/reference/io.html&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;打开命令行， 执行  polars 安装命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install &amp;#39;polars[all]&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h2 id=&#34;二数据读写&#34;&gt;二、数据读写&lt;/h2&gt;
&lt;p&gt;Polars 读写数据支持&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;常见的数据文件，如 csv、xlsx、json、parquet ；&lt;/li&gt;
&lt;li&gt;云存储，如 S3、Azure Blob, BigQuery；&lt;/li&gt;
&lt;li&gt;数据库，如postgres、mysql&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;咱们主要分享常见的代码操作&lt;/p&gt;
&lt;h3 id=&#34;21-dataframe&#34;&gt;2.1 DataFrame&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;polars&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pl&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;polars.selectors&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cs&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;datetime&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;idx&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;张三&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;李四&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;王五&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;赵六&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;birthday&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2009&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2005&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1995&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;gender&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;男&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;男&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;男&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;女&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;bio&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;好好学习，天天向上&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                &lt;span class=&#34;s2&#34;&gt;&amp;#34;泰难了&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                &lt;span class=&#34;s2&#34;&gt;&amp;#34;学习有毛用&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                &lt;span class=&#34;s2&#34;&gt;&amp;#34;躺平ing&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#存入csv、excel、json、parquet&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write_excel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.xlsx&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write_json&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.json&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write_parquet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.parquet&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆    birthday	       | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&amp;#34;张三&amp;#34; ┆ 2009-05-01 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;好好学习，天天向上&amp;#34;|
│ 2 ┆&amp;#34;李四&amp;#34; ┆ 2005-10-15 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;泰难了&amp;#34;          |
│ 3 ┆&amp;#34;王五&amp;#34; ┆ 2000-12-31 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;学习有毛用&amp;#34;       |
│ 4 ┆&amp;#34;赵六&amp;#34; ┆ 1995-06-15 00:00:00 ┆ &amp;#34;女&amp;#34;  │&amp;#34;躺平ing&amp;#34;         |
└──────────┴─────────────────────┴───────┘─────────────────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-csvexcel&#34;&gt;2.2 csv、excel&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;df.write_csv 存入csv&lt;/li&gt;
&lt;li&gt;pl.read_csv  读取csv&lt;/li&gt;
&lt;li&gt;df.write_excel 存入xlsx文件&lt;/li&gt;
&lt;li&gt;pl.read_excel   读取xlsx&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;df_csv = pl.read_csv(&amp;#39;data.csv&amp;#39;)
df_xlsx = pl.read_excel(&amp;#39;data.xlsx&amp;#39;)

df_csv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆       birthday	     | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆        str          ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&amp;#34;张三&amp;#34; ┆  &amp;#34;2009-05-01T00:…   ┆ &amp;#34;男&amp;#34;  │&amp;#34;好好学习，天天向上&amp;#34;|
│ 2 ┆&amp;#34;李四&amp;#34; ┆  &amp;#34;2005-10-15T00:…	  ┆ &amp;#34;男&amp;#34;  │&amp;#34;泰难了&amp;#34;          |
│ 3 ┆&amp;#34;王五&amp;#34; ┆  &amp;#34;2000-12-31T00:…   ┆ &amp;#34;男&amp;#34;  │&amp;#34;学习有毛用&amp;#34;       |
│ 4 ┆&amp;#34;赵六&amp;#34; ┆  &amp;#34;1995-06-15T00:…	  ┆ &amp;#34;女&amp;#34;  │&amp;#34;躺平ing&amp;#34;         |
└──────────┴─────────────────────┴───────┘─────────────────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意哦， 此时的 date 字段数据类型是 str&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-jsonparquet&#34;&gt;2.3 json/parquet&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;df.write_json&lt;/li&gt;
&lt;li&gt;pl.read_json&lt;/li&gt;
&lt;li&gt;df.write_parquet&lt;/li&gt;
&lt;li&gt;pl.read_parquet&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df_json&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_json&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.json&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df_parquet&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_parquet&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.parquet&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df_json&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆    birthday	       | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&amp;#34;张三&amp;#34; ┆ 2009-05-01 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;好好学习，天天向上&amp;#34;|
│ 2 ┆&amp;#34;李四&amp;#34; ┆ 2005-10-15 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;泰难了&amp;#34;          |
│ 3 ┆&amp;#34;王五&amp;#34; ┆ 2000-12-31 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;学习有毛用&amp;#34;       |
│ 4 ┆&amp;#34;赵六&amp;#34; ┆ 1995-06-15 00:00:00 ┆ &amp;#34;女&amp;#34;  │&amp;#34;躺平ing&amp;#34;         |
└──────────┴─────────────────────┴───────┘─────────────────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意， 使用 df.write_json 或 df.write_parquet 将数据存入 json、parquet， 都可以保留 date 字段的 datetime 类型。而 csv、xlsx 只会将date字段存储为 str 类型。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三常用表达式&#34;&gt;三、常用表达式&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Expressions&lt;/code&gt;是Polars的核心功能， &lt;code&gt;expressions&lt;/code&gt; 既可以解决简单的查询，又可以轻松扩展到复杂的查询。下面是 polars 的基本表达式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;pl.col&lt;/strong&gt; 列选择器&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;df.select&lt;/strong&gt;  结合pl.col， 返回dataframe&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;selector&lt;/strong&gt;  selector选择器&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;df.filter&lt;/strong&gt; 结合pl.col， 返回dataframe&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;df.with_columns&lt;/strong&gt; 结合pl.col， 返回dataframe&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;df.grouby&lt;/strong&gt;  结合pl.col， 返回dataframe&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;31-plcol&#34;&gt;3.1 pl.col&lt;/h3&gt;
&lt;p&gt;选择某一(多)个字段(列)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;birthday&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;col(&amp;#34;birthday&amp;#34;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;birthday&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;col([&amp;#34;name&amp;#34;, &amp;#34;birthday&amp;#34;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32--dfselect&#34;&gt;3.2  df.select&lt;/h3&gt;
&lt;p&gt;选择 &lt;em&gt;&lt;strong&gt;name&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;birthday&lt;/strong&gt;&lt;/em&gt; 两个字段， 实现该功能有多种写法&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#df[[&amp;#39;name&amp;#39;, &amp;#39;birthday&amp;#39;]]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#df.select(&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#    pl.col(&amp;#34;name&amp;#34;), &lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#    pl.col(&amp;#34;birthday&amp;#34;), &lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#)&lt;/span&gt;


&lt;span class=&#34;c1&#34;&gt;#df.select([&amp;#34;name&amp;#34;, &amp;#34;birthday&amp;#34;])&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;select&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;birthday&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 2)
┌──────┬─────────────────────┬
│ name ┆    birthday	       | 
│------┆ ------------------  ┆ 
│  str ┆    datetime[μs]     ┆ 
╞══════╪═════════════════════╪
│&amp;#34;张三&amp;#34; ┆ 2009-05-01 00:00:00 ┆
│&amp;#34;李四&amp;#34; ┆ 2005-10-15 00:00:00 ┆
│&amp;#34;王五&amp;#34; ┆ 2000-12-31 00:00:00 ┆
│&amp;#34;赵六&amp;#34; ┆ 1995-06-15 00:00:00 ┆
└─────────────────────────────
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;polars 即使选择一个字段， 返回的也是dataframe&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#df[[&amp;#39;name&amp;#39;]]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#df.select([&amp;#34;name&amp;#34;])&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;select&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 1)
┌──────┬
│ 姓名  ┆
│------┆ 
│  str ┆ 
╞══════╪
│&amp;#34;张三&amp;#34; ┆
│&amp;#34;李四&amp;#34; ┆
│&amp;#34;王五&amp;#34; ┆
│&amp;#34;赵六&amp;#34; ┆
└───────
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;33-dfwith_columns&#34;&gt;3.3 df.with_columns&lt;/h3&gt;
&lt;p&gt;与 df.select 功能类似，但是df.with_columns可以在选择字段的同时，保留之前的字段&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;with_columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆       birthday	     | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆        str          ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&amp;#34;张三&amp;#34; ┆  &amp;#34;2009-05-01T00:…   ┆ &amp;#34;男&amp;#34;  │&amp;#34;好好学习，天天向上&amp;#34;|
│ 2 ┆&amp;#34;李四&amp;#34; ┆  &amp;#34;2005-10-15T00:…	  ┆ &amp;#34;男&amp;#34;  │&amp;#34;泰难了&amp;#34;          |
│ 3 ┆&amp;#34;王五&amp;#34; ┆  &amp;#34;2000-12-31T00:…   ┆ &amp;#34;男&amp;#34;  │&amp;#34;学习有毛用&amp;#34;       |
│ 4 ┆&amp;#34;赵六&amp;#34; ┆  &amp;#34;1995-06-15T00:…	  ┆ &amp;#34;女&amp;#34;  │&amp;#34;躺平ing&amp;#34;         |
└──────────┴─────────────────────┴───────┘─────────────────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;with_columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;alias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;姓名&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 6)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐───────┬
│idx| name ┆    birthday	       | gender┆	    bio        ┆  姓名  ┆ 
│---┆------┆-------------------- ┆  ---  ┆ --------------  │-------┆
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆  str  ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡═══════╡
│ 1 ┆&amp;#34;张三&amp;#34; ┆ 2009-05-01 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;好好学习，天天向上&amp;#34;|&amp;#34;张三&amp;#34;  ┆
│ 2 ┆&amp;#34;李四&amp;#34; ┆ 2005-10-15 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;泰难了&amp;#34;          |&amp;#34;李四&amp;#34;  ┆
│ 3 ┆&amp;#34;王五&amp;#34; ┆ 2000-12-31 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;学习有毛用&amp;#34;       |&amp;#34;王五&amp;#34;  ┆
│ 4 ┆&amp;#34;赵六&amp;#34; ┆ 1995-06-15 00:00:00 ┆ &amp;#34;女&amp;#34;  │&amp;#34;躺平ing&amp;#34;         |&amp;#34;赵六&amp;#34;  ┆ 
└──────────┴─────────────────────┴───────┘─────────────────┴───────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;34--dffilter&#34;&gt;3.4  df.filter&lt;/h3&gt;
&lt;p&gt;筛选出生日是 00 后的记录&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;birthday&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆    birthday	       | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&amp;#34;张三&amp;#34; ┆ 2009-05-01 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;好好学习，天天向上&amp;#34;|
│ 2 ┆&amp;#34;李四&amp;#34; ┆ 2005-10-15 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;泰难了&amp;#34;          |
│ 3 ┆&amp;#34;王五&amp;#34; ┆ 2000-12-31 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;学习有毛用&amp;#34;       |
└──────────┴─────────────────────┴───────┘─────────────────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;35-dfgroupby&#34;&gt;3.5 df.groupby&lt;/h3&gt;
&lt;p&gt;按 &lt;em&gt;&lt;strong&gt;性别gender&lt;/strong&gt;&lt;/em&gt; 进行分组功能&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#for gender, gender_df in df.groupby(&amp;#39;gender&amp;#39;):&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gender&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gender_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gender&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gender&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gender_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gender_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;男 3 &amp;lt;class &amp;#39;polars.dataframe.frame.DataFrame&amp;#39;&amp;gt;
女 1 &amp;lt;class &amp;#39;polars.dataframe.frame.DataFrame&amp;#39;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;分别计算男女学生的bio的文本长度的均值&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gender&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gender_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gender&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;gender&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;gender_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bio&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;男 5.666666666666667
女 5.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gender&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;agg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bio&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;len_chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;alias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mean_len&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (2, 3)
┌──────┬───────┬───────────┬
│gender| count ┆ mean_len	 |
│------┆ ----- ┆-----------┆
│  str ┆  u32	 ┆   f64     ┆
╞══════╪═══════╪═══════════╡
│ &amp;#34;女&amp;#34; ┆  1    ┆    5.0    ┆
│ &amp;#34;男&amp;#34; ┆  3    ┆  5.666667 ┆
└──────┴───────┴───────────┘
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四选择器&#34;&gt;四、选择器&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;cs.integer、cs.string、cs.numeric 、cs.datetime()、cs.temporal() 按照数据格式筛选字段&lt;/li&gt;
&lt;li&gt;cs.contains 、cs.matches 使用正则表达式筛选字段&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;41-按数据格式筛选&#34;&gt;4.1 按数据格式筛选&lt;/h3&gt;
&lt;p&gt;筛选出字段数据类型为字符和数字的字段，返回dataframe&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;polars.selectors&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cs&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;select&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;cs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;integer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 4)
┌───┬──────┬───────┬─────────────────┐
│idx| name ┆ gender┆	    bio        ┆
│---┆ ---  ┆  ---  ┆ --------------  │
│i64┆ str  ┆  str  ┆      str        ┆
╞═══╪══════╪═══════╡═════════════════╡
│ 1 ┆&amp;#34;张三&amp;#34; ┆ &amp;#34;男&amp;#34;  │&amp;#34;好好学习，天天向上&amp;#34;|
│ 2 ┆&amp;#34;李四&amp;#34; ┆ &amp;#34;男&amp;#34;  │&amp;#34;泰难了&amp;#34;          |
│ 3 ┆&amp;#34;王五&amp;#34; ┆ &amp;#34;男&amp;#34;  │&amp;#34;学习有毛用&amp;#34;       |
│ 4 ┆&amp;#34;赵六&amp;#34; ┆ &amp;#34;女&amp;#34;  │&amp;#34;躺平ing&amp;#34;         |
└──────────┴───────┘─────────────────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;筛选出 datetime 格式的字段，返回 dataframe&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#df.select(cs.temporal())&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;select&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;cs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 1)
┌───────────────────┬
│ birthday	        |
│-------------------┆
│ datetime[μs]      ┆
╞═══════════════════╪
│2009-05-01 00:00:00┆
│2005-10-15 00:00:00┆
│2000-12-31 00:00:00┆
│1995-06-15 00:00:00┆
└───────────────────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-cscontains-csmatches&#34;&gt;4.2 cs.contains/ cs.matches&lt;/h3&gt;
&lt;p&gt;筛选出含 r 字段，返回dataframe&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#筛选出字段名含 r 的字段&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;select&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;cs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 2)
┌───────────────────┬───────┬
│ birthday	        | gender┆
│-------------------┆  ---  ┆
│ datetime[μs]      ┆  str  ┆
╞═══════════════════╪═══════╡
│2009-05-01 00:00:00┆ &amp;#34;男&amp;#34;  │
│2005-10-15 00:00:00┆ &amp;#34;男&amp;#34;  │
│2000-12-31 00:00:00┆ &amp;#34;男&amp;#34;  │
│1995-06-15 00:00:00┆ &amp;#34;女&amp;#34;  │
└───────────────────┴───────┘
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;筛选出含 &lt;em&gt;&lt;strong&gt;na&lt;/strong&gt;&lt;/em&gt; 或 &lt;em&gt;&lt;strong&gt;io&lt;/strong&gt;&lt;/em&gt; 的字段，返回dataframe&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;select&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;cs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;na|io&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 2)
┌─────┬───────────────────┐
│name ┆       bio         ┆
│ --- ┆ ---------------   ┆
│ str ┆  str              ┆
╞═════╪═══════════════════╡
│&amp;#34;张三&amp;#34;┆ &amp;#34;好好学习，天天向上&amp;#34; |
│&amp;#34;李四&amp;#34;┆ &amp;#34;泰难了&amp;#34;           |
│&amp;#34;王五&amp;#34;┆ &amp;#34;学习有毛用&amp;#34;        |
│&amp;#34;赵六&amp;#34;┆ &amp;#34;躺平ing&amp;#34;          |
└─────┴───────────────────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五逻辑条件&#34;&gt;五、逻辑条件&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;pl.when(condition).then(result1).otherwise(result2)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当满足condition时， 值为result1； 反之，则result2&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;df.with_columns(
    pl.when(pl.col(&amp;#39;birthday&amp;#39;)&amp;gt;datetime(2000, 1, 1))
    .then(True)
    .otherwise(False)
    .alias(&amp;#39;00后&amp;#39;)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐───────┬
│idx| name ┆    birthday	       | gender┆	    bio        ┆ 00后  ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │  ---- ┆
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆  str  ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡═══════╡
│ 1 ┆&amp;#34;张三&amp;#34; ┆ 2009-05-01 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;好好学习，天天向上&amp;#34;| true  |
│ 2 ┆&amp;#34;李四&amp;#34; ┆ 2005-10-15 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;泰难了&amp;#34;          | true  |
│ 3 ┆&amp;#34;王五&amp;#34; ┆ 2000-12-31 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;学习有毛用&amp;#34;       | true  |
│ 4 ┆&amp;#34;赵六&amp;#34; ┆ 1995-06-15 00:00:00 ┆ &amp;#34;女&amp;#34;  │&amp;#34;躺平ing&amp;#34;         | false |
└──────────┴─────────────────────┴───────┘─────────────────┴───────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;六字符串操作&#34;&gt;六、字符串操作&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;pl.col().str.len_chars()&lt;/strong&gt; 字符长度&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pl.col().str.contains(pat)&lt;/strong&gt; 是否含某字符(符合pat模式)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pl.col().str.extract(pat)&lt;/strong&gt; 提取出符合模式的文本&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pl.col().str.replace(old_pat, new_pat)&lt;/strong&gt;  把old_pat替换为new_pat&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;61-strlen_chars&#34;&gt;6.1 str.len_chars()&lt;/h3&gt;
&lt;p&gt;计算 bio 的文字长度，计算结果存储到 lenth 字段中&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;select&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bio&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bio&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;len_chars&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;alias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;lenth&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 2)
┌─────────────────┐───────┬
│	      bio       ┆ lenth ┆
│ --------------  │  ---- ┆
│      str        ┆  u32  ┆
╞═════════════════╡═══════╡
│ &amp;#34;好好学习，天天向上&amp;#34;|   9  |
│ &amp;#34;泰难了&amp;#34;          |   3  |
│ &amp;#34;学习有毛用&amp;#34;       |   5  |
│ &amp;#34;躺平ing&amp;#34;         |   5  |
└──────────────────┴───────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;62-strcontains&#34;&gt;6.2 str.contains()&lt;/h3&gt;
&lt;p&gt;从 bio 中筛选出含 &lt;strong&gt;学习&lt;/strong&gt; 字眼的记录&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
  &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bio&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;学习&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆    birthday	       | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&amp;#34;张三&amp;#34; ┆ 2009-05-01 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;好好学习，天天向上&amp;#34;|
│ 3 ┆&amp;#34;王五&amp;#34; ┆ 2000-12-31 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;学习有毛用&amp;#34;       |
└──────────┴─────────────────────┴───────┘─────────────────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;63-strextract&#34;&gt;6.3 str.extract()&lt;/h3&gt;
&lt;p&gt;根据负面词典 &lt;code&gt;&#39;躺平|难|毛&#39;&lt;/code&gt; 选出负面词, 结果存储到字段 neg&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;with_columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pl&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bio&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extract_all&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;躺平|难|毛&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;alias&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;neg&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;shape: (4, 6)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐───────┬
│idx| name ┆    birthday	       | gender┆	    bio        ┆  neg  ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │  ---  ┆
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆  str  ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡═══════╡
│ 1 ┆&amp;#34;张三&amp;#34; ┆ 2009-05-01 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;好好学习，天天向上&amp;#34;|   []  |
│ 2 ┆&amp;#34;李四&amp;#34; ┆ 2005-10-15 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;泰难了&amp;#34;          | [&amp;#34;难&amp;#34;]|
│ 3 ┆&amp;#34;王五&amp;#34; ┆ 2000-12-31 00:00:00 ┆ &amp;#34;男&amp;#34;  │&amp;#34;学习有毛用&amp;#34;       | [&amp;#34;毛&amp;#34;]|
│ 4 ┆&amp;#34;赵六&amp;#34; ┆ 1995-06-15 00:00:00 ┆ &amp;#34;女&amp;#34;  │&amp;#34;躺平ing&amp;#34;         |[&amp;#34;躺平&amp;#34;]|
└──────────┴─────────────────────┴───────┘─────────────────┴───────┴
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一介绍">一、介绍</h2>
<p>Polars 是一个用于操作结构化数据的高性能 DataFrame 库，由于 Polars 是从0开始用Rust编写，紧密与机器结合。其矢量化和列式处理可在现代处理器上实现缓存一致性算法和高性能。如果您经常使用 pandas，那么用起 Polars 会感觉很轻松，可以说是平替 Pandas 最有潜质的包。</p>
<p>Polars 在独立的 TPCH 基准测试中与其他几个解决方案进行了基准测试。该基准测试旨在复制实践中使用的数据整理操作。由于其并行执行引擎、高效算法以及 SIMD（单指令、多数据）矢量化的使用，Polars 轻松胜过其他解决方案。<strong>与pandas相比，它可以实现30倍以上的性能提升</strong>。</p>
<p>Polars 的目标是提供一个闪电般快速的<code>DataFrame</code>库：</p>
<ul>
<li>利用机器上所有可用的内核。</li>
<li>优化查询以减少不必要的工作/内存分配。</li>
<li>处理比可用 RAM 大得多的数据集。</li>
<li>拥有一致且可预测的 API。</li>
<li>具有严格的架构（在运行查询之前应该知道数据类型）。</li>
</ul>
<br>
<p><a href="https://pola-rs.github.io/polars/user-guide/">User guide: https://pola-rs.github.io/polars/user-guide/</a>
<a href="https://pola-rs.github.io/polars/py-polars/html/reference/io.html">API reference: https://pola-rs.github.io/polars/py-polars/html/reference/io.html</a></p>
<br>
<p>打开命令行， 执行  polars 安装命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install &#39;polars[all]&#39;
</code></pre></div><br>
<h2 id="二数据读写">二、数据读写</h2>
<p>Polars 读写数据支持</p>
<ul>
<li>常见的数据文件，如 csv、xlsx、json、parquet ；</li>
<li>云存储，如 S3、Azure Blob, BigQuery；</li>
<li>数据库，如postgres、mysql</li>
</ul>
<p>咱们主要分享常见的代码操作</p>
<h3 id="21-dataframe">2.1 DataFrame</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">polars</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">polars.selectors</span> <span class="k">as</span> <span class="nn">cs</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&#34;idx&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
        <span class="s2">&#34;name&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;张三&#34;</span><span class="p">,</span> <span class="s2">&#34;李四&#34;</span><span class="p">,</span> <span class="s2">&#34;王五&#34;</span><span class="p">,</span> <span class="s2">&#34;赵六&#34;</span><span class="p">],</span>
        <span class="s2">&#34;birthday&#34;</span><span class="p">:</span> <span class="p">[</span>
            <span class="n">datetime</span><span class="p">(</span><span class="mi">2009</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">datetime</span><span class="p">(</span><span class="mi">2005</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">),</span>
            <span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">31</span><span class="p">),</span>
            <span class="n">datetime</span><span class="p">(</span><span class="mi">1995</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">15</span><span class="p">),</span>
        <span class="p">],</span>
        <span class="s2">&#34;gender&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;男&#34;</span><span class="p">,</span> <span class="s2">&#34;男&#34;</span><span class="p">,</span> <span class="s2">&#34;男&#34;</span><span class="p">,</span> <span class="s2">&#34;女&#34;</span><span class="p">],</span>
        <span class="s2">&#34;bio&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;好好学习，天天向上&#34;</span><span class="p">,</span> 
                <span class="s2">&#34;泰难了&#34;</span><span class="p">,</span> 
                <span class="s2">&#34;学习有毛用&#34;</span><span class="p">,</span> 
                <span class="s2">&#34;躺平ing&#34;</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1">#存入csv、excel、json、parquet</span>
<span class="n">df</span><span class="o">.</span><span class="n">write_csv</span><span class="p">(</span><span class="s2">&#34;data.csv&#34;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">write_excel</span><span class="p">(</span><span class="s2">&#34;data.xlsx&#34;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">write_json</span><span class="p">(</span><span class="s2">&#34;data.json&#34;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">write_parquet</span><span class="p">(</span><span class="s2">&#34;data.parquet&#34;</span><span class="p">)</span>


<span class="n">df</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆    birthday	       | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&#34;张三&#34; ┆ 2009-05-01 00:00:00 ┆ &#34;男&#34;  │&#34;好好学习，天天向上&#34;|
│ 2 ┆&#34;李四&#34; ┆ 2005-10-15 00:00:00 ┆ &#34;男&#34;  │&#34;泰难了&#34;          |
│ 3 ┆&#34;王五&#34; ┆ 2000-12-31 00:00:00 ┆ &#34;男&#34;  │&#34;学习有毛用&#34;       |
│ 4 ┆&#34;赵六&#34; ┆ 1995-06-15 00:00:00 ┆ &#34;女&#34;  │&#34;躺平ing&#34;         |
└──────────┴─────────────────────┴───────┘─────────────────┴
</code></pre></div><br>
<h3 id="22-csvexcel">2.2 csv、excel</h3>
<ul>
<li>df.write_csv 存入csv</li>
<li>pl.read_csv  读取csv</li>
<li>df.write_excel 存入xlsx文件</li>
<li>pl.read_excel   读取xlsx</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df_csv = pl.read_csv(&#39;data.csv&#39;)
df_xlsx = pl.read_excel(&#39;data.xlsx&#39;)

df_csv
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆       birthday	     | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆        str          ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&#34;张三&#34; ┆  &#34;2009-05-01T00:…   ┆ &#34;男&#34;  │&#34;好好学习，天天向上&#34;|
│ 2 ┆&#34;李四&#34; ┆  &#34;2005-10-15T00:…	  ┆ &#34;男&#34;  │&#34;泰难了&#34;          |
│ 3 ┆&#34;王五&#34; ┆  &#34;2000-12-31T00:…   ┆ &#34;男&#34;  │&#34;学习有毛用&#34;       |
│ 4 ┆&#34;赵六&#34; ┆  &#34;1995-06-15T00:…	  ┆ &#34;女&#34;  │&#34;躺平ing&#34;         |
└──────────┴─────────────────────┴───────┘─────────────────┴
</code></pre></div><p>注意哦， 此时的 date 字段数据类型是 str</p>
<br>
<h3 id="23-jsonparquet">2.3 json/parquet</h3>
<ul>
<li>df.write_json</li>
<li>pl.read_json</li>
<li>df.write_parquet</li>
<li>pl.read_parquet</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df_json</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="s2">&#34;data.json&#34;</span><span class="p">)</span>
<span class="n">df_parquet</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s2">&#34;data.parquet&#34;</span><span class="p">)</span>

<span class="n">df_json</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆    birthday	       | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&#34;张三&#34; ┆ 2009-05-01 00:00:00 ┆ &#34;男&#34;  │&#34;好好学习，天天向上&#34;|
│ 2 ┆&#34;李四&#34; ┆ 2005-10-15 00:00:00 ┆ &#34;男&#34;  │&#34;泰难了&#34;          |
│ 3 ┆&#34;王五&#34; ┆ 2000-12-31 00:00:00 ┆ &#34;男&#34;  │&#34;学习有毛用&#34;       |
│ 4 ┆&#34;赵六&#34; ┆ 1995-06-15 00:00:00 ┆ &#34;女&#34;  │&#34;躺平ing&#34;         |
└──────────┴─────────────────────┴───────┘─────────────────┴
</code></pre></div><p>注意， 使用 df.write_json 或 df.write_parquet 将数据存入 json、parquet， 都可以保留 date 字段的 datetime 类型。而 csv、xlsx 只会将date字段存储为 str 类型。</p>
<p><br><br></p>
<h2 id="三常用表达式">三、常用表达式</h2>
<p><code>Expressions</code>是Polars的核心功能， <code>expressions</code> 既可以解决简单的查询，又可以轻松扩展到复杂的查询。下面是 polars 的基本表达式</p>
<ul>
<li><strong>pl.col</strong> 列选择器</li>
<li><strong>df.select</strong>  结合pl.col， 返回dataframe</li>
<li><strong>selector</strong>  selector选择器</li>
<li><strong>df.filter</strong> 结合pl.col， 返回dataframe</li>
<li><strong>df.with_columns</strong> 结合pl.col， 返回dataframe</li>
<li><strong>df.grouby</strong>  结合pl.col， 返回dataframe</li>
</ul>
<h3 id="31-plcol">3.1 pl.col</h3>
<p>选择某一(多)个字段(列)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;birthday&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">col(&#34;birthday&#34;)
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;birthday&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">col([&#34;name&#34;, &#34;birthday&#34;])
</code></pre></div><br>
<h3 id="32--dfselect">3.2  df.select</h3>
<p>选择 <em><strong>name</strong></em> 和 <em><strong>birthday</strong></em> 两个字段， 实现该功能有多种写法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#df[[&#39;name&#39;, &#39;birthday&#39;]]</span>

<span class="c1">#df.select(</span>
<span class="c1">#    pl.col(&#34;name&#34;), </span>
<span class="c1">#    pl.col(&#34;birthday&#34;), </span>
<span class="c1">#)</span>


<span class="c1">#df.select([&#34;name&#34;, &#34;birthday&#34;])</span>


<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">&#34;name&#34;</span><span class="p">,</span> <span class="s2">&#34;birthday&#34;</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 2)
┌──────┬─────────────────────┬
│ name ┆    birthday	       | 
│------┆ ------------------  ┆ 
│  str ┆    datetime[μs]     ┆ 
╞══════╪═════════════════════╪
│&#34;张三&#34; ┆ 2009-05-01 00:00:00 ┆
│&#34;李四&#34; ┆ 2005-10-15 00:00:00 ┆
│&#34;王五&#34; ┆ 2000-12-31 00:00:00 ┆
│&#34;赵六&#34; ┆ 1995-06-15 00:00:00 ┆
└─────────────────────────────
</code></pre></div><br>
<p>polars 即使选择一个字段， 返回的也是dataframe</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#df[[&#39;name&#39;]]</span>

<span class="c1">#df.select([&#34;name&#34;])</span>

<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&#34;name&#34;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 1)
┌──────┬
│ 姓名  ┆
│------┆ 
│  str ┆ 
╞══════╪
│&#34;张三&#34; ┆
│&#34;李四&#34; ┆
│&#34;王五&#34; ┆
│&#34;赵六&#34; ┆
└───────
</code></pre></div><br>
<h3 id="33-dfwith_columns">3.3 df.with_columns</h3>
<p>与 df.select 功能类似，但是df.with_columns可以在选择字段的同时，保留之前的字段</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆       birthday	     | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆        str          ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&#34;张三&#34; ┆  &#34;2009-05-01T00:…   ┆ &#34;男&#34;  │&#34;好好学习，天天向上&#34;|
│ 2 ┆&#34;李四&#34; ┆  &#34;2005-10-15T00:…	  ┆ &#34;男&#34;  │&#34;泰难了&#34;          |
│ 3 ┆&#34;王五&#34; ┆  &#34;2000-12-31T00:…   ┆ &#34;男&#34;  │&#34;学习有毛用&#34;       |
│ 4 ┆&#34;赵六&#34; ┆  &#34;1995-06-15T00:…	  ┆ &#34;女&#34;  │&#34;躺平ing&#34;         |
└──────────┴─────────────────────┴───────┘─────────────────┴
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;姓名&#39;</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 6)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐───────┬
│idx| name ┆    birthday	       | gender┆	    bio        ┆  姓名  ┆ 
│---┆------┆-------------------- ┆  ---  ┆ --------------  │-------┆
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆  str  ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡═══════╡
│ 1 ┆&#34;张三&#34; ┆ 2009-05-01 00:00:00 ┆ &#34;男&#34;  │&#34;好好学习，天天向上&#34;|&#34;张三&#34;  ┆
│ 2 ┆&#34;李四&#34; ┆ 2005-10-15 00:00:00 ┆ &#34;男&#34;  │&#34;泰难了&#34;          |&#34;李四&#34;  ┆
│ 3 ┆&#34;王五&#34; ┆ 2000-12-31 00:00:00 ┆ &#34;男&#34;  │&#34;学习有毛用&#34;       |&#34;王五&#34;  ┆
│ 4 ┆&#34;赵六&#34; ┆ 1995-06-15 00:00:00 ┆ &#34;女&#34;  │&#34;躺平ing&#34;         |&#34;赵六&#34;  ┆ 
└──────────┴─────────────────────┴───────┘─────────────────┴───────┴
</code></pre></div><br>
<h3 id="34--dffilter">3.4  df.filter</h3>
<p>筛选出生日是 00 后的记录</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;birthday&#39;</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆    birthday	       | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&#34;张三&#34; ┆ 2009-05-01 00:00:00 ┆ &#34;男&#34;  │&#34;好好学习，天天向上&#34;|
│ 2 ┆&#34;李四&#34; ┆ 2005-10-15 00:00:00 ┆ &#34;男&#34;  │&#34;泰难了&#34;          |
│ 3 ┆&#34;王五&#34; ┆ 2000-12-31 00:00:00 ┆ &#34;男&#34;  │&#34;学习有毛用&#34;       |
└──────────┴─────────────────────┴───────┘─────────────────┴
</code></pre></div><br>
<h3 id="35-dfgroupby">3.5 df.groupby</h3>
<p>按 <em><strong>性别gender</strong></em> 进行分组功能</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#for gender, gender_df in df.groupby(&#39;gender&#39;):</span>
<span class="k">for</span> <span class="n">gender</span><span class="p">,</span> <span class="n">gender_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;gender&#39;</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">gender</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">gender_df</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">gender_df</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">男 3 &lt;class &#39;polars.dataframe.frame.DataFrame&#39;&gt;
女 1 &lt;class &#39;polars.dataframe.frame.DataFrame&#39;&gt;
</code></pre></div><br>
<p>分别计算男女学生的bio的文本长度的均值</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">gender</span><span class="p">,</span> <span class="n">gender_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;gender&#39;</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">gender</span><span class="p">,</span>  <span class="n">gender_df</span><span class="p">[</span><span class="s1">&#39;bio&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">男 5.666666666666667
女 5.0
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;gender&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">count</span><span class="p">(),</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;bio&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len_chars</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;mean_len&#39;</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (2, 3)
┌──────┬───────┬───────────┬
│gender| count ┆ mean_len	 |
│------┆ ----- ┆-----------┆
│  str ┆  u32	 ┆   f64     ┆
╞══════╪═══════╪═══════════╡
│ &#34;女&#34; ┆  1    ┆    5.0    ┆
│ &#34;男&#34; ┆  3    ┆  5.666667 ┆
└──────┴───────┴───────────┘
</code></pre></div><p><br><br></p>
<h2 id="四选择器">四、选择器</h2>
<ul>
<li>cs.integer、cs.string、cs.numeric 、cs.datetime()、cs.temporal() 按照数据格式筛选字段</li>
<li>cs.contains 、cs.matches 使用正则表达式筛选字段</li>
</ul>
<h3 id="41-按数据格式筛选">4.1 按数据格式筛选</h3>
<p>筛选出字段数据类型为字符和数字的字段，返回dataframe</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">polars.selectors</span> <span class="k">as</span> <span class="nn">cs</span>

<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
    <span class="n">cs</span><span class="o">.</span><span class="n">integer</span><span class="p">(),</span> <span class="n">cs</span><span class="o">.</span><span class="n">string</span><span class="p">()</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 4)
┌───┬──────┬───────┬─────────────────┐
│idx| name ┆ gender┆	    bio        ┆
│---┆ ---  ┆  ---  ┆ --------------  │
│i64┆ str  ┆  str  ┆      str        ┆
╞═══╪══════╪═══════╡═════════════════╡
│ 1 ┆&#34;张三&#34; ┆ &#34;男&#34;  │&#34;好好学习，天天向上&#34;|
│ 2 ┆&#34;李四&#34; ┆ &#34;男&#34;  │&#34;泰难了&#34;          |
│ 3 ┆&#34;王五&#34; ┆ &#34;男&#34;  │&#34;学习有毛用&#34;       |
│ 4 ┆&#34;赵六&#34; ┆ &#34;女&#34;  │&#34;躺平ing&#34;         |
└──────────┴───────┘─────────────────┴
</code></pre></div><br>
<p>筛选出 datetime 格式的字段，返回 dataframe</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#df.select(cs.temporal())</span>

<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
    <span class="n">cs</span><span class="o">.</span><span class="n">datetime</span><span class="p">()</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 1)
┌───────────────────┬
│ birthday	        |
│-------------------┆
│ datetime[μs]      ┆
╞═══════════════════╪
│2009-05-01 00:00:00┆
│2005-10-15 00:00:00┆
│2000-12-31 00:00:00┆
│1995-06-15 00:00:00┆
└───────────────────┴
</code></pre></div><br>
<h3 id="42-cscontains-csmatches">4.2 cs.contains/ cs.matches</h3>
<p>筛选出含 r 字段，返回dataframe</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#筛选出字段名含 r 的字段</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
    <span class="n">cs</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 2)
┌───────────────────┬───────┬
│ birthday	        | gender┆
│-------------------┆  ---  ┆
│ datetime[μs]      ┆  str  ┆
╞═══════════════════╪═══════╡
│2009-05-01 00:00:00┆ &#34;男&#34;  │
│2005-10-15 00:00:00┆ &#34;男&#34;  │
│2000-12-31 00:00:00┆ &#34;男&#34;  │
│1995-06-15 00:00:00┆ &#34;女&#34;  │
└───────────────────┴───────┘
</code></pre></div><br>
<p>筛选出含 <em><strong>na</strong></em> 或 <em><strong>io</strong></em> 的字段，返回dataframe</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
    <span class="n">cs</span><span class="o">.</span><span class="n">matches</span><span class="p">(</span><span class="s1">&#39;na|io&#39;</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 2)
┌─────┬───────────────────┐
│name ┆       bio         ┆
│ --- ┆ ---------------   ┆
│ str ┆  str              ┆
╞═════╪═══════════════════╡
│&#34;张三&#34;┆ &#34;好好学习，天天向上&#34; |
│&#34;李四&#34;┆ &#34;泰难了&#34;           |
│&#34;王五&#34;┆ &#34;学习有毛用&#34;        |
│&#34;赵六&#34;┆ &#34;躺平ing&#34;          |
└─────┴───────────────────┴
</code></pre></div><p><br><br></p>
<h2 id="五逻辑条件">五、逻辑条件</h2>
<p><strong>pl.when(condition).then(result1).otherwise(result2)</strong></p>
<p>当满足condition时， 值为result1； 反之，则result2</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df.with_columns(
    pl.when(pl.col(&#39;birthday&#39;)&gt;datetime(2000, 1, 1))
    .then(True)
    .otherwise(False)
    .alias(&#39;00后&#39;)
)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐───────┬
│idx| name ┆    birthday	       | gender┆	    bio        ┆ 00后  ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │  ---- ┆
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆  str  ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡═══════╡
│ 1 ┆&#34;张三&#34; ┆ 2009-05-01 00:00:00 ┆ &#34;男&#34;  │&#34;好好学习，天天向上&#34;| true  |
│ 2 ┆&#34;李四&#34; ┆ 2005-10-15 00:00:00 ┆ &#34;男&#34;  │&#34;泰难了&#34;          | true  |
│ 3 ┆&#34;王五&#34; ┆ 2000-12-31 00:00:00 ┆ &#34;男&#34;  │&#34;学习有毛用&#34;       | true  |
│ 4 ┆&#34;赵六&#34; ┆ 1995-06-15 00:00:00 ┆ &#34;女&#34;  │&#34;躺平ing&#34;         | false |
└──────────┴─────────────────────┴───────┘─────────────────┴───────┴
</code></pre></div><p><br><br></p>
<h2 id="六字符串操作">六、字符串操作</h2>
<ul>
<li><strong>pl.col().str.len_chars()</strong> 字符长度</li>
<li><strong>pl.col().str.contains(pat)</strong> 是否含某字符(符合pat模式)</li>
<li><strong>pl.col().str.extract(pat)</strong> 提取出符合模式的文本</li>
<li><strong>pl.col().str.replace(old_pat, new_pat)</strong>  把old_pat替换为new_pat</li>
<li></li>
</ul>
<h3 id="61-strlen_chars">6.1 str.len_chars()</h3>
<p>计算 bio 的文字长度，计算结果存储到 lenth 字段中</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;bio&#39;</span><span class="p">),</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;bio&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len_chars</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;lenth&#39;</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 2)
┌─────────────────┐───────┬
│	      bio       ┆ lenth ┆
│ --------------  │  ---- ┆
│      str        ┆  u32  ┆
╞═════════════════╡═══════╡
│ &#34;好好学习，天天向上&#34;|   9  |
│ &#34;泰难了&#34;          |   3  |
│ &#34;学习有毛用&#34;       |   5  |
│ &#34;躺平ing&#34;         |   5  |
└──────────────────┴───────┴
</code></pre></div><br>
<h3 id="62-strcontains">6.2 str.contains()</h3>
<p>从 bio 中筛选出含 <strong>学习</strong> 字眼的记录</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;bio&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s2">&#34;学习&#34;</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 5)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐
│idx| name ┆    birthday	       | gender┆	    bio        ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡
│ 1 ┆&#34;张三&#34; ┆ 2009-05-01 00:00:00 ┆ &#34;男&#34;  │&#34;好好学习，天天向上&#34;|
│ 3 ┆&#34;王五&#34; ┆ 2000-12-31 00:00:00 ┆ &#34;男&#34;  │&#34;学习有毛用&#34;       |
└──────────┴─────────────────────┴───────┘─────────────────┴
</code></pre></div><br>
<h3 id="63-strextract">6.3 str.extract()</h3>
<p>根据负面词典 <code>'躺平|难|毛'</code> 选出负面词, 结果存储到字段 neg</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">&#39;bio&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">extract_all</span><span class="p">(</span><span class="s1">&#39;躺平|难|毛&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;neg&#39;</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">shape: (4, 6)
┌───┬──────┬─────────────────────┬───────┬─────────────────┐───────┬
│idx| name ┆    birthday	       | gender┆	    bio        ┆  neg  ┆
│---┆ ---  ┆    -------------    ┆  ---  ┆ --------------  │  ---  ┆
│i64┆ str  ┆    datetime[μs]     ┆  str  ┆      str        ┆  str  ┆
╞═══╪══════╪═════════════════════╪═══════╡═════════════════╡═══════╡
│ 1 ┆&#34;张三&#34; ┆ 2009-05-01 00:00:00 ┆ &#34;男&#34;  │&#34;好好学习，天天向上&#34;|   []  |
│ 2 ┆&#34;李四&#34; ┆ 2005-10-15 00:00:00 ┆ &#34;男&#34;  │&#34;泰难了&#34;          | [&#34;难&#34;]|
│ 3 ┆&#34;王五&#34; ┆ 2000-12-31 00:00:00 ┆ &#34;男&#34;  │&#34;学习有毛用&#34;       | [&#34;毛&#34;]|
│ 4 ┆&#34;赵六&#34; ┆ 1995-06-15 00:00:00 ┆ &#34;女&#34;  │&#34;躺平ing&#34;         |[&#34;躺平&#34;]|
└──────────┴─────────────────────┴───────┘─────────────────┴───────┴
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集(付费) | 人民网地方领导留言板原始文本(2011-2023.12)</title>
      <link>https://textdata.cn/blog/2023-12-22-renmin-gov-leader-comment-board/</link>
      <pubDate>Fri, 22 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-12-22-renmin-gov-leader-comment-board/</guid>
      <description>&lt;img src=&#34;img/04-dataset.png&#34; style=&#34;zoom:80%;&#34; /&gt;
&lt;br&gt;
&lt;h2 id=&#34;一数据集&#34;&gt;一、数据集&lt;/h2&gt;
&lt;h3 id=&#34;11-概况&#34;&gt;1.1 概况&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据来源: 人民网地方领导留言板

覆盖时间: 2011-01-01 ~ 2023.12.06

记录条数: 3914385

文件格式: xlsx、csv
    
所含字段:
 -  留言领导
 -  留言标题
 -  省份
 -  市
 -  状态
 -  主题类别
 -  投诉种类
 -  留言人
 -  留言时间
 -  留言内容
 -  回复内容
 -  回复时间
 -  回复机构
 -  办理速度评分(该字段出现在2019之后)
 -  办理态度评分(该字段出现在2019之后)
 -  解决程度评分(该字段出现在2019之后)
 -  用户评价(该字段出现在2019之后)
 -  评价标签(该字段出现在2019之后)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;12-相关研究&#34;&gt;1.2 相关研究&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;
[1]郑石明, 兰雨潇, 黎枫. 网络公共舆论与政府回应的互动逻辑——基于新冠肺炎疫情期间“领导留言板”的数据分析[J]. 公共管理学报, 2021, 18 (03): 24-37+169.
王磊,易扬.公共卫生危机中的数字政府回应如何纾解网络负面舆情——基于人民网“领导留言板”回复情况的调查[J].公共管理学报,2022,19(04):65-78+169.

[2]Lu, Liangdong, Jia Xu, and Jiuchang Wei. &amp;#34;Understanding the effects of the textual complexity on government communication: Insights from China’s online public service platform.&amp;#34; Telematics and Informatics 83 (2023): 102028.
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;13-获取数据&#34;&gt;1.3 获取数据&lt;/h3&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;1. 付费数据集，2000元，支持开票；加微信 372335839， 备注「姓名-学校-专业」。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;2. 数据是虚拟产品，一经售出，不再退还！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;3. 大家时间其实都很宝贵，请仔细阅读推文内容， 确认无误再加微信详谈购买事宜 &lt;/span&gt;&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;br&gt;&lt;img src=&#34;img/2023a.png&#34; style=&#34;zoom:80%;&#34; /&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/2023b.png&#34; style=&#34;zoom:80%;&#34; /&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二查看数据&#34;&gt;二、查看数据&lt;/h2&gt;
&lt;h3 id=&#34;21-读取数据&#34;&gt;2.1 读取数据&lt;/h3&gt;
&lt;p&gt;依次读取&lt;em&gt;&lt;strong&gt;2011-2019.csv.gz&lt;/strong&gt;&lt;/em&gt; 和  &lt;em&gt;&lt;strong&gt;2020-2023.csv.gz&lt;/strong&gt;&lt;/em&gt;  两个csv文件，    &lt;em&gt;&lt;strong&gt;.csv.gz&lt;/strong&gt;&lt;/em&gt; 解压得到  &lt;em&gt;&lt;strong&gt;.csv&lt;/strong&gt;&lt;/em&gt; 后再读取。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2011-2019.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#df11_19 = pd.read_csv(&amp;#39;2011-2019.csv.gz&amp;#39;, compression=&amp;#39;gzip&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df20_23&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2020-2023.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#df20_23 = pd.read_csv(&amp;#39;2020-2023.csv.gz&amp;#39;, compression=&amp;#39;gzip&amp;#39;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df20_23&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-字段&#34;&gt;2.2 字段&lt;/h3&gt;
&lt;p&gt;10多年的时间，网站会变动，写爬虫运行爬虫的人也会变动。为了让大家更丝滑的使用数据，大邓对所有的年份进行了字段矫正和统一， 最后字段只有两大类，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2011-2019&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2020-2023&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df20_23&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2011-2019
Index([&amp;#39;留言领导&amp;#39;, &amp;#39;留言标题&amp;#39;, &amp;#39;省份&amp;#39;, &amp;#39;市&amp;#39;, &amp;#39;状态&amp;#39;, &amp;#39;主题类别&amp;#39;, &amp;#39;投诉种类&amp;#39;, &amp;#39;留言人&amp;#39;, &amp;#39;留言时间&amp;#39;, &amp;#39;留言内容&amp;#39;, &amp;#39;回复机构&amp;#39;, 
       &amp;#39;回复内容&amp;#39;, &amp;#39;回复时间&amp;#39;, &amp;#39;留言评价&amp;#39;, &amp;#39;评价时间&amp;#39;],
      dtype=&amp;#39;object&amp;#39;)


2020-2023
Index([&amp;#39;留言领导&amp;#39;, &amp;#39;留言标题&amp;#39;, &amp;#39;省份&amp;#39;, &amp;#39;市&amp;#39;, &amp;#39;状态&amp;#39;, &amp;#39;主题类别&amp;#39;, &amp;#39;投诉种类&amp;#39;, &amp;#39;留言人&amp;#39;, &amp;#39;留言时间&amp;#39;, &amp;#39;留言内容&amp;#39;,
       &amp;#39;回复内容&amp;#39;, &amp;#39;回复时间&amp;#39;, &amp;#39;回复机构&amp;#39;, &amp;#39;办理速度评分&amp;#39;, &amp;#39;办理态度评分&amp;#39;, &amp;#39;解决程度评分&amp;#39;, &amp;#39;用户评价&amp;#39;, &amp;#39;评价标签&amp;#39;],
      dtype=&amp;#39;object&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;23-记录数&#34;&gt;2.3 记录数&lt;/h3&gt;
&lt;p&gt;数据集总记录数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;总记录数: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df20_23&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;总记录数: 3914385
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;24-每年是否包含年末数据&#34;&gt;2.4 每年是否包含年末数据&lt;/h3&gt;
&lt;p&gt;由于人民网只 “&lt;strong&gt;可查询留言为上一年1月1日至今的所有留言&lt;/strong&gt;”, 有同学没看懂这句话含义，担心每年12月月末或1月月初是否会缺失数据。这里我们检查下数据集每年的年初是否为1.1， 年底是否为12.31&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
    
    
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df20_23&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df20_23&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;min&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;mi&#34;&gt;2011&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2011&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2011&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2012&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2012&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2012&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2013&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2013&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2013&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2014&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2014&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2014&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2015&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2015&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2015&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2016&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2016&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2016&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2017&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2017&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2017&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2018&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2018&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2018&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2019&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2019&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2019&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2020&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2020&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2020&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2021&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2021&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2021&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2022&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2022&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2022&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;31&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;2023&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2023&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2023&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;06&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;因为数据集是 2023.12.6 运行的， 日期截止到 2023.12.6 。不过不用担心， 下次更新数据时候会覆盖到  2023.12.31 。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;25-年度记录数&#34;&gt;2.5 年度记录数&lt;/h3&gt;
&lt;p&gt;两个 dataframe 中都有 &lt;em&gt;&lt;strong&gt;留言日期&lt;/strong&gt;&lt;/em&gt; ， 我们根据该字段查看每个年份的记录数。首先，要先将该字段转化为 datetime 日期类型。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df20_23&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df20_23&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;volume&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)})&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df20_23&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df20_23&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言时间&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;volume&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)})&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2011 23307
2012 20178
2013 42950
2014 97640
2015 131930
2016 201525
2017 202793
2018 243648
2019 464622
2020 517167
2021 783139
2022 648055
2023 537422
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;scienceplots&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;platform&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib_inline&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;backend_inline&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_matplotlib_formats&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;png&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;svg&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;jieba&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;warnings&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;warnings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filterwarnings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ignore&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;style&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;science&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;no-latex&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;cjk-sc-font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;platform&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;system&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 获取操作系统类型&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Windows&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;SimHei&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;elif&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Darwin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Arial Unicode MS&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;font&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;family&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;sans-serif&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;matplotlib&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;font&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;font&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 设置全局字体&lt;/span&gt;
    
&lt;span class=&#34;n&#34;&gt;year_volume_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#year_volume_df[&amp;#39;year&amp;#39;] = pd.to_datetime(year_volume_df[&amp;#39;year&amp;#39;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_volume_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_volume_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bar&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;人民网留言板留言数量(2011 ~ 2023)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xticks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rotation&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xlabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;年份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言数量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/plot.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;需要声明， 采集的数据量与真实数据量肯定会有出入的， 例如爬虫运行的时间点、IP被封、请求失败、文件编码(格式)问题等会遗失一定量的记录量。&lt;/p&gt;
&lt;p&gt;但是大家做Python定量文本分析， 不用担心这个问题。  Python为代表的大规模数据挖掘，只要满足  &lt;strong&gt;Earnings(规模带来的信息增益) &amp;raquo; Loss(数据质量产生的损失)&lt;/strong&gt; ，做文本分析就是可行的，有意义的。 而咱们的数据， 数据规模近 400 万条， 数据质量也是有保证的。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;26-value_counts&#34;&gt;2.6 value_counts&lt;/h3&gt;
&lt;p&gt;查看2011-2019年， 不同留 &lt;em&gt;&lt;strong&gt;主题类别&lt;/strong&gt;&lt;/em&gt;  的记录数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#2011-2019&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;主题类别&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value_counts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;主题类别
城建    474413
交通    180195
其他    177262
三农    116151
环保     94344
教育     90603
政务     69910
治安     63752
就业     47854
医疗     37215
企业     36826
旅游     18675
文娱      9866
金融      6778
征集      4741
求助         3
咨询         2
建言         2
投诉         1
Name: count, dtype: int64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;27-查看是否含某词&#34;&gt;2.7 查看是否含某词&lt;/h3&gt;
&lt;p&gt;查看字段 &lt;em&gt;&lt;strong&gt;留言内容&lt;/strong&gt;&lt;/em&gt;, 是否出现 &lt;em&gt;&lt;strong&gt;扰民|噪音&lt;/strong&gt;&lt;/em&gt; 等词语&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;扰民|噪音&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0          False
1          False
2          False
3          False
4          False
           ...  
1428614    False
1428615    False
1428616    False
1428617    False
1428618    False
Name: 留言内容, Length: 1428619, dtype: bool
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;噪音的留言记录数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;扰民|噪音&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;57845
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;噪音的留言记录占总留言数的比例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;留言内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;扰民|噪音&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df11_19&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0.04049063350044309
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;有4%的留言是跟扰民、噪音相关的 。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三-相关研究&#34;&gt;三、 相关研究&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;郑石明, 兰雨潇, 黎枫. 网络公共舆论与政府回应的互动逻辑——基于新冠肺炎疫情期间“领导留言板”的数据分析[J]. 公共管理学报, 2021, 18 (03): 24-37+169.
王磊,易扬.公共卫生危机中的数字政府回应如何纾解网络负面舆情——基于人民网“领导留言板”回复情况的调查[J].公共管理学报,2022,19(04):65-78+169.
Lu, Liangdong, Jia Xu, and Jiuchang Wei. &amp;#34;Understanding the effects of the textual complexity on government communication: Insights from China’s online public service platform.&amp;#34; Telematics and Informatics 83 (2023): 102028.
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;四相关代码&#34;&gt;四、相关代码&lt;/h2&gt;
&lt;p&gt;想用 python 对 csv、xlsx 进行分析， 要学会尽量用 pandas 写代码。 以下是近期 pandas 的一些处理推文免费教程， 感兴趣的可以进去浏览浏览。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-29-china-area-dataset/&#34;&gt;数据集 | 2024年中国全国5级行政区划（省、市、县、镇、村）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/&#34;&gt;词向量  | 使用&lt;strong&gt;人民网领导留言板&lt;/strong&gt;语料训练Word2Vec模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-17-how-to-generate-panel-data-from-gov-report-dataset/&#34;&gt;&lt;strong&gt;代码 | 使用地方gov工作报告生成某类概念词频「面板数据」&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-18-how-to-generate-panel-data-from-daily-news-dataset/&#34;&gt;&lt;strong&gt;代码 | 使用「新闻数据」构造概念词提及量「面板数据」&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-02-26-cctv1-xwlb-news-text-dataset/&#34;&gt;&lt;strong&gt;数据代码| 使用cctv新闻联播文稿构造「面板数据」&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2028-12-18-how-to-extract-data-from-patent-application-dataset/&#34;&gt;&lt;strong&gt;代码 | 使用3571w专利申请数据集构造「面板数据」&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/&#34;&gt;&lt;strong&gt;代码 | 使用「新闻数据」计算 「经济政策不确定性」指数&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五获取数据&#34;&gt;五、获取数据&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;1. 付费数据集，2000元；加微信 372335839， 备注「姓名-学校-专业」。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;2. 数据是虚拟产品，一经售出，不再退还！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;font-size: 18px;color: green;&#34;&gt;3. 大家时间其实都很宝贵，请仔细阅读推文内容， 确认无误再加微信详谈购买事宜 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<img src="img/04-dataset.png" style="zoom:80%;" />
<br>
<h2 id="一数据集">一、数据集</h2>
<h3 id="11-概况">1.1 概况</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据来源: 人民网地方领导留言板

覆盖时间: 2011-01-01 ~ 2023.12.06

记录条数: 3914385

文件格式: xlsx、csv
    
所含字段:
 -  留言领导
 -  留言标题
 -  省份
 -  市
 -  状态
 -  主题类别
 -  投诉种类
 -  留言人
 -  留言时间
 -  留言内容
 -  回复内容
 -  回复时间
 -  回复机构
 -  办理速度评分(该字段出现在2019之后)
 -  办理态度评分(该字段出现在2019之后)
 -  解决程度评分(该字段出现在2019之后)
 -  用户评价(该字段出现在2019之后)
 -  评价标签(该字段出现在2019之后)
</code></pre></div><br>
<h3 id="12-相关研究">1.2 相关研究</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">
[1]郑石明, 兰雨潇, 黎枫. 网络公共舆论与政府回应的互动逻辑——基于新冠肺炎疫情期间“领导留言板”的数据分析[J]. 公共管理学报, 2021, 18 (03): 24-37+169.
王磊,易扬.公共卫生危机中的数字政府回应如何纾解网络负面舆情——基于人民网“领导留言板”回复情况的调查[J].公共管理学报,2022,19(04):65-78+169.

[2]Lu, Liangdong, Jia Xu, and Jiuchang Wei. &#34;Understanding the effects of the textual complexity on government communication: Insights from China’s online public service platform.&#34; Telematics and Informatics 83 (2023): 102028.
...
</code></pre></div><br>
<h3 id="13-获取数据">1.3 获取数据</h3>
<p><span style="font-size: 18px;color: green;">1. 付费数据集，2000元，支持开票；加微信 372335839， 备注「姓名-学校-专业」。</span></p>
<p><span style="font-size: 18px;color: green;">2. 数据是虚拟产品，一经售出，不再退还！</span></p>
<p><span style="font-size: 18px;color: green;">3. 大家时间其实都很宝贵，请仔细阅读推文内容， 确认无误再加微信详谈购买事宜 </span></p>
<br>
<p><br><img src="img/2023a.png" style="zoom:80%;" /><br></p>
<p><img src="img/2023b.png" style="zoom:80%;" /><br><br></p>
<h2 id="二查看数据">二、查看数据</h2>
<h3 id="21-读取数据">2.1 读取数据</h3>
<p>依次读取<em><strong>2011-2019.csv.gz</strong></em> 和  <em><strong>2020-2023.csv.gz</strong></em>  两个csv文件，    <em><strong>.csv.gz</strong></em> 解压得到  <em><strong>.csv</strong></em> 后再读取。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="n">df11_19</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;2011-2019.csv&#39;</span><span class="p">)</span>
<span class="c1">#df11_19 = pd.read_csv(&#39;2011-2019.csv.gz&#39;, compression=&#39;gzip&#39;)</span>

<span class="n">df11_19</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/02-df.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df20_23</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;2020-2023.csv&#39;</span><span class="p">)</span>
<span class="c1">#df20_23 = pd.read_csv(&#39;2020-2023.csv.gz&#39;, compression=&#39;gzip&#39;)</span>
<span class="n">df20_23</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/03-df.png" alt=""  />
</p>
<br>
<h3 id="22-字段">2.2 字段</h3>
<p>10多年的时间，网站会变动，写爬虫运行爬虫的人也会变动。为了让大家更丝滑的使用数据，大邓对所有的年份进行了字段矫正和统一， 最后字段只有两大类，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;2011-2019&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df11_19</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;2020-2023&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df20_23</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2011-2019
Index([&#39;留言领导&#39;, &#39;留言标题&#39;, &#39;省份&#39;, &#39;市&#39;, &#39;状态&#39;, &#39;主题类别&#39;, &#39;投诉种类&#39;, &#39;留言人&#39;, &#39;留言时间&#39;, &#39;留言内容&#39;, &#39;回复机构&#39;, 
       &#39;回复内容&#39;, &#39;回复时间&#39;, &#39;留言评价&#39;, &#39;评价时间&#39;],
      dtype=&#39;object&#39;)


2020-2023
Index([&#39;留言领导&#39;, &#39;留言标题&#39;, &#39;省份&#39;, &#39;市&#39;, &#39;状态&#39;, &#39;主题类别&#39;, &#39;投诉种类&#39;, &#39;留言人&#39;, &#39;留言时间&#39;, &#39;留言内容&#39;,
       &#39;回复内容&#39;, &#39;回复时间&#39;, &#39;回复机构&#39;, &#39;办理速度评分&#39;, &#39;办理态度评分&#39;, &#39;解决程度评分&#39;, &#39;用户评价&#39;, &#39;评价标签&#39;],
      dtype=&#39;object&#39;)
</code></pre></div><br>
<h3 id="23-记录数">2.3 记录数</h3>
<p>数据集总记录数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;总记录数: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df11_19</span><span class="p">)</span><span class="o">+</span><span class="nb">len</span><span class="p">(</span><span class="n">df20_23</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">总记录数: 3914385
</code></pre></div><br>
<h3 id="24-每年是否包含年末数据">2.4 每年是否包含年末数据</h3>
<p>由于人民网只 “<strong>可查询留言为上一年1月1日至今的所有留言</strong>”, 有同学没看懂这句话含义，担心每年12月月末或1月月初是否会缺失数据。这里我们检查下数据集每年的年初是否为1.1， 年底是否为12.31</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">df11_19</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">df11_19</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">year</span><span class="p">,</span> <span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">date</span><span class="p">(),</span> <span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">date</span><span class="p">())</span>
    
    
<span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">df20_23</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">df20_23</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">year</span><span class="p">,</span> <span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">date</span><span class="p">(),</span> <span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">date</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="mi">2011</span> <span class="mi">2011</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2011</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2012</span> <span class="mi">2012</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2012</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2013</span> <span class="mi">2013</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2013</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2014</span> <span class="mi">2014</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2014</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2015</span> <span class="mi">2015</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2016</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2017</span> <span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2017</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2018</span> <span class="mi">2018</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2018</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2019</span> <span class="mi">2019</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2019</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2020</span> <span class="mi">2020</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2020</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2021</span> <span class="mi">2021</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2021</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2022</span> <span class="mi">2022</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2022</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="mi">2023</span> <span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">2023</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">06</span>
</code></pre></div><p>因为数据集是 2023.12.6 运行的， 日期截止到 2023.12.6 。不过不用担心， 下次更新数据时候会覆盖到  2023.12.31 。</p>
<br>
<h3 id="25-年度记录数">2.5 年度记录数</h3>
<p>两个 dataframe 中都有 <em><strong>留言日期</strong></em> ， 我们根据该字段查看每个年份的记录数。首先，要先将该字段转化为 datetime 日期类型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">df11_19</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df11_19</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">])</span>
<span class="n">df20_23</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df20_23</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">])</span>

<span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">df11_19</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">df11_19</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="p">):</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;year&#39;</span><span class="p">:</span> <span class="n">year</span><span class="p">,</span> <span class="s1">&#39;volume&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">year_df</span><span class="p">)})</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">year</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">year_df</span><span class="p">))</span>

<span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">df20_23</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">df20_23</span><span class="p">[</span><span class="s1">&#39;留言时间&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="p">):</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;year&#39;</span><span class="p">:</span> <span class="n">year</span><span class="p">,</span> <span class="s1">&#39;volume&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">year_df</span><span class="p">)})</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">year</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">year_df</span><span class="p">))</span>
    

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2011 23307
2012 20178
2013 42950
2014 97640
2015 131930
2016 201525
2017 202793
2018 243648
2019 464622
2020 517167
2021 783139
2022 648055
2023 537422
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>
<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>
    
<span class="n">year_volume_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="c1">#year_volume_df[&#39;year&#39;] = pd.to_datetime(year_volume_df[&#39;year&#39;])</span>
<span class="n">year_volume_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">year_volume_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;人民网留言板留言数量(2011 ~ 2023)&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;年份&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;留言数量&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/plot.png" alt=""  />
</p>
<p>需要声明， 采集的数据量与真实数据量肯定会有出入的， 例如爬虫运行的时间点、IP被封、请求失败、文件编码(格式)问题等会遗失一定量的记录量。</p>
<p>但是大家做Python定量文本分析， 不用担心这个问题。  Python为代表的大规模数据挖掘，只要满足  <strong>Earnings(规模带来的信息增益) &raquo; Loss(数据质量产生的损失)</strong> ，做文本分析就是可行的，有意义的。 而咱们的数据， 数据规模近 400 万条， 数据质量也是有保证的。</p>
<p><br><br></p>
<h3 id="26-value_counts">2.6 value_counts</h3>
<p>查看2011-2019年， 不同留 <em><strong>主题类别</strong></em>  的记录数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#2011-2019</span>
<span class="n">df11_19</span><span class="p">[</span><span class="s1">&#39;主题类别&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">主题类别
城建    474413
交通    180195
其他    177262
三农    116151
环保     94344
教育     90603
政务     69910
治安     63752
就业     47854
医疗     37215
企业     36826
旅游     18675
文娱      9866
金融      6778
征集      4741
求助         3
咨询         2
建言         2
投诉         1
Name: count, dtype: int64
</code></pre></div><br>
<h3 id="27-查看是否含某词">2.7 查看是否含某词</h3>
<p>查看字段 <em><strong>留言内容</strong></em>, 是否出现 <em><strong>扰民|噪音</strong></em> 等词语</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df11_19</span><span class="p">[</span><span class="s1">&#39;留言内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;扰民|噪音&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0          False
1          False
2          False
3          False
4          False
           ...  
1428614    False
1428615    False
1428616    False
1428617    False
1428618    False
Name: 留言内容, Length: 1428619, dtype: bool
</code></pre></div><br>
<p>噪音的留言记录数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df11_19</span><span class="p">[</span><span class="s1">&#39;留言内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;扰民|噪音&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">57845
</code></pre></div><br>
<p>噪音的留言记录占总留言数的比例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df11_19</span><span class="p">[</span><span class="s1">&#39;留言内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;扰民|噪音&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df11_19</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.04049063350044309
</code></pre></div><p>有4%的留言是跟扰民、噪音相关的 。</p>
<p><br><br></p>
<h2 id="三-相关研究">三、 相关研究</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">郑石明, 兰雨潇, 黎枫. 网络公共舆论与政府回应的互动逻辑——基于新冠肺炎疫情期间“领导留言板”的数据分析[J]. 公共管理学报, 2021, 18 (03): 24-37+169.
王磊,易扬.公共卫生危机中的数字政府回应如何纾解网络负面舆情——基于人民网“领导留言板”回复情况的调查[J].公共管理学报,2022,19(04):65-78+169.
Lu, Liangdong, Jia Xu, and Jiuchang Wei. &#34;Understanding the effects of the textual complexity on government communication: Insights from China’s online public service platform.&#34; Telematics and Informatics 83 (2023): 102028.
...
</code></pre></div><h2 id="四相关代码">四、相关代码</h2>
<p>想用 python 对 csv、xlsx 进行分析， 要学会尽量用 pandas 写代码。 以下是近期 pandas 的一些处理推文免费教程， 感兴趣的可以进去浏览浏览。</p>
<ul>
<li><a href="https://textdata.cn/blog/2023-12-29-china-area-dataset/">数据集 | 2024年中国全国5级行政区划（省、市、县、镇、村）</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量  | 使用<strong>人民网领导留言板</strong>语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2023-12-17-how-to-generate-panel-data-from-gov-report-dataset/"><strong>代码 | 使用地方gov工作报告生成某类概念词频「面板数据」</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-12-18-how-to-generate-panel-data-from-daily-news-dataset/"><strong>代码 | 使用「新闻数据」构造概念词提及量「面板数据」</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-02-26-cctv1-xwlb-news-text-dataset/"><strong>数据代码| 使用cctv新闻联播文稿构造「面板数据」</strong></a></li>
<li><a href="https://textdata.cn/blog/2028-12-18-how-to-extract-data-from-patent-application-dataset/"><strong>代码 | 使用3571w专利申请数据集构造「面板数据」</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-12-20-measure-china-economic-policy-uncertainty/"><strong>代码 | 使用「新闻数据」计算 「经济政策不确定性」指数</strong></a></li>
</ul>
<p><br><br></p>
<h2 id="五获取数据">五、获取数据</h2>
<p><span style="font-size: 18px;color: green;">1. 付费数据集，2000元；加微信 372335839， 备注「姓名-学校-专业」。</span></p>
<p><span style="font-size: 18px;color: green;">2. 数据是虚拟产品，一经售出，不再退还！</span></p>
<p><span style="font-size: 18px;color: green;">3. 大家时间其实都很宝贵，请仔细阅读推文内容， 确认无误再加微信详谈购买事宜 </span></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据(付费) | 使用cctv新闻联播文稿构造面板数据</title>
      <link>https://textdata.cn/blog/2023-02-26-cctv1-xwlb-news-text-dataset/</link>
      <pubDate>Sat, 16 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-02-26-cctv1-xwlb-news-text-dataset/</guid>
      <description>cctv新闻联播文稿数据集，可使用Python对其进行挖掘，借助文本挖掘技术研究鸿观经济政策、社会学、传播学等领域。</description>
      <content:encoded><![CDATA[<h2 id="一新闻联播">一、新闻联播</h2>
<h3 id="11-数据集概况">1.1 数据集概况</h3>
<p>全网最全的数据集， 记录缺失率最低的<strong>xwlb数据集</strong>，  <strong>新</strong>(fan)<strong>闻</strong>(rong)<strong>联</strong>(chang)<strong>播</strong>(sheng) 。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据来源: 央视网https://tv.cctv.com/lm/xwlb/ 
覆盖日期: 2006-09-01 ~ 2023-12-15
日记录数: 6315天
字段: date、 text
</code></pre></div><br>
<h3 id="12-研究用途">1.2 研究用途</h3>
<p>可从中提取丰富的指标，包括但不限于经济政策不确定性指数EPU 、 媒体关注度、媒体情绪、文本相似度。此外， 可训练词向量，开发新的概念词典，构建新的指标指数。数据带时间， 参照前面指标， 依主体、日期、指标进行计算， 可构造面板数据，因此在经济学、管理学、新闻传播学、公共管理等领域均有较高的研究价值。</p>
<p>相关参考文献</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]洪永淼,刘俸奇,薛涧坡.政府与市场心理因素的经济影响及其测度[J].管理世界,2023,39(03):30-51.
[2]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216.
[3]张一帆,林建浩,樊嘉诚.新闻文本大数据与消费增速实时预测——基于叙事经济学的视角[J].金融研究,2023,(05):152-169.
[4]Huang, Yun, and Paul Luk. &#34;Measuring economic policy uncertainty in China.&#34; China Economic Review 59 (2020): 101367
[5]欧阳资生,陈世丽,杨希特,刘凤根,周学伟.经济政策不确定性、网络舆情与金融机构系统性风险[J].管理科学学报,2023,26(04):62-86.
[6]逯东,宋昕倍.媒体报道、上市公司年报可读性与融资约束[J].管理科学学报,2021,24(12):45-61.
[7]彭涛,黄福广,孙凌霞.经济政策不确定性与风险承担:基于风险投资的证据[J].管理科学学报,2021,24(03):98-114.
[8]庞锐.采纳与内化：多重制度压力如何影响河长制创新扩散——基于省级政府的定向配对事件史分析[J].公共管理学报,2023,20(02):25-37+165-166.
</code></pre></div><br>
<h3 id="13-获取数据">1.3 获取数据</h3>
<p>【新闻联播xwlb】按年度，每年50元。 全量购买200元。</p>
<p><strong>加微信 372335839， 备注「姓名-学校-专业」</strong>。</p>
<p>更多新闻类数据  <a href="https://textdata.cn/blog/2023-12-14-daily-news-dataset">数据集 | 人民日报/经济日报/光明日报 等 7 家新闻类文本数据集</a></p>
<p><br><br></p>
<h2 id="二数据检查">二、数据检查</h2>
<h3 id="21-读取数据">2.1 读取数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#6315天</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;cctv_xwlb.csv&#39;</span><span class="p">)</span>

<span class="c1">#变更日期格式，可进行日期计算</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">6315
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<h3 id="22-日期涵盖">2.2 日期涵盖</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">#执行过 df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;])
#才能进行日期计算

print(df[&#39;date&#39;].min().date())
print(df[&#39;date&#39;].max().date())
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2006-09-01
2023-12-15
</code></pre></div><br>
<h3 id="33-缺失率">3.3 缺失率</h3>
<p>查看是否存在某些日期对应的文本是空</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0
</code></pre></div><br>
<p>生成2006-09-01-2023-12-15之间所有的日期datelist， 查看datelist哪些日期不在数据集中，以判断是否遗漏某些日期。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">datetime</span> <span class="k">as</span> <span class="nn">dt</span>  <span class="c1">#import datetime, timedelta  </span>
  
<span class="n">start_date</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2006</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  
<span class="n">end_date</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2023</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>  
<span class="n">delta</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
  
<span class="n">date_list</span> <span class="o">=</span> <span class="p">[]</span>  
<span class="n">current_date</span> <span class="o">=</span> <span class="n">start_date</span>  
<span class="k">while</span> <span class="n">current_date</span> <span class="o">&lt;=</span> <span class="n">end_date</span><span class="p">:</span>  
    <span class="n">date_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_date</span><span class="p">)</span>  
    <span class="n">current_date</span> <span class="o">+=</span> <span class="n">delta</span>  
  
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">date_list</span><span class="p">)</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">date_list</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1.0
</code></pre></div><p>2006-09-01~2023-12-15之间所有的日期， 均存在于新闻数据集中，也就是说数据集没有遗漏这期间任何一天的新闻。</p>
<p><br><br></p>
<h2 id="三实验">三、实验</h2>
<p>按月份(也可调整为周、年)计算一下正负面情绪词在新闻中出现次数， 然后转化为情感分值， 绘制成折线图。</p>
<ol>
<li>导入词典</li>
<li>设计算法, 如统计新闻总词数、正面词数、负面词数。</li>
<li>转化为情感分值</li>
<li>按月份汇总</li>
<li>绘制折线图</li>
</ol>
<h3 id="31-导入词典">3.1 导入词典</h3>
<p>使用cntext2.0.0内置的中文经济金融场景的情感词典，该词典比较适合xwlb这种题材，我们查看一下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="c1">#未开源cntext2.0.0</span>

<span class="n">diction</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;zh_common_FinanceSenti.yaml&#39;</span><span class="p">)[</span><span class="s1">&#39;Dictionary&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;pos词数&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">diction</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;neg词数&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">diction</span><span class="p">[</span><span class="s1">&#39;neg&#39;</span><span class="p">]))</span>


<span class="c1">#词典整理自论文， 大家也可自行整理</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;zh_common_FinanceSenti.yaml&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pos词数 3338
neg词数 5890


{&#39;Refer&#39;: &#39;Fuwei Jiang, Joshua Lee, Xiumin Martin, and Guofu Zhou.“Manager Sentiment and Stock Returns” Journal of Financial Economics 132(1), 2019,126-149&#39;,
 
 &#39;Desc&#39;: &#39;Chinese Financial Sentiment Dictionary&#39;,
 &#39;Category&#39;: [&#39;pos&#39;, &#39;neg&#39;],
 
 &#39;Name&#39;: &#39;Chinese Financial Sentiment Dictionary&#39;,
 
 &#39;Dictionary&#39;: {&#39;pos&#39;: [&#39;安定&#39;, &#39;安康&#39;, &#39;帮助&#39;, &#39;榜样&#39;, &#39;饱满&#39;, ...  &#39;最合适&#39;, &#39;最小&#39;, &#39;最新进展&#39;, &#39;最早&#39;, &#39;遵法&#39;],
  
                &#39;neg&#39;: [&#39;败坏名声&#39;, &#39;被没收的&#39;, &#39;变节&#39;, &#39;不便&#39;, &#39;不适当&#39;, &#39;妨碍&#39;,  &#39;腐败&#39;,...&#39;唉声叹气&#39;, &#39;哀怨&#39;, &#39;哀叹&#39;, &#39;哀伤&#39;, &#39;哀悼&#39;]
}
</code></pre></div><br>
<p><strong>配置cntext-2.0.0-py3-none-any.whl的方法</strong></p>
<ol>
<li>
<p>将whl文件放置于电脑桌面。</p>
</li>
<li>
<p>打开cmd(mac打开terminal)， 输入 <code>cd desktop</code>,  按Enter回车键</p>
</li>
<li>
<p>继续在cmd(mac打开terminal)中，输入 <code>pip3 install cntext-2.0.0-py3-none-any.whl</code>,  按Enter回车键</p>
</li>
</ol>
<br>
<h3 id="32-统计词频">3.2 统计词频</h3>
<p>这里</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>  
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">diction</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_yaml_dict</span><span class="p">(</span><span class="s1">&#39;zh_common_FinanceSenti.yaml&#39;</span><span class="p">)[</span><span class="s1">&#39;Dictionary&#39;</span><span class="p">]</span>
<span class="n">pos_patern</span> <span class="o">=</span> <span class="s1">&#39;|&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">diction</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">])</span>
<span class="n">neg_patern</span> <span class="o">=</span> <span class="s1">&#39;|&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">diction</span><span class="p">[</span><span class="s1">&#39;neg&#39;</span><span class="p">])</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">text</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)))</span>

<span class="c1">#正面词数</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;pos_num&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">pos_patern</span><span class="p">)</span>

<span class="c1">#负面词数</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;neg_num&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">neg_patern</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<h3 id="33-计算情感值">3.3 计算情感值</h3>
<p>使用 <code>score = pos-neg/(pos+neg)</code>， 可以将数值范围调整到 <code>-1 ~ 1</code>之间。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;senti_score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pos_num&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;neg_num&#39;</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pos_num&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;neg_num&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;最小值&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;senti_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;均值&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;senti_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;中位数&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;senti_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;最大&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;senti_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">最小值 -0.36633663366336633
均值 0.5448464974146746
中位数 0.5657256687535572
最大 1.0
</code></pre></div><br>
<h3 id="34-按月份">3.4 按月份</h3>
<p>这里用到df.groupby方法， 可以按某种分组方法，得到不同组的dataframe集合。</p>
<p>dataframe集合可以通过for循环逐个迭代，分别计算对应年度的信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">month_datas</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">date</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Grouper</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;M&#39;</span><span class="p">)):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">date</span>
    
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;senti_score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;senti_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">month_datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    
<span class="n">month_info_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">month_datas</span><span class="p">)</span>
<span class="n">month_info_df</span>
</code></pre></div><p><img loading="lazy" src="img/df4.png" alt=""  />
</p>
<br>
<h3 id="35-绘制月情感折线图">3.5 绘制月情感折线图</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import matplotlib.pyplot as plt
import matplotlib
import matplotlib_inline
matplotlib_inline.backend_inline.set_matplotlib_formats(&#39;png&#39;, &#39;svg&#39;)
import scienceplots
import platform
import pandas as pd
import numpy as np


plt.style.use([&#39;science&#39;, &#39;no-latex&#39;, &#39;cjk-sc-font&#39;])
system = platform.system()  # 获取操作系统类型

if system == &#39;Windows&#39;:
    font = {&#39;family&#39;: &#39;SimHei&#39;}
elif system == &#39;Darwin&#39;:
    font = {&#39;family&#39;: &#39;Arial Unicode MS&#39;}
else:
    font = {&#39;family&#39;: &#39;sans-serif&#39;}
matplotlib.rc(&#39;font&#39;, **font)  # 设置全局字体


plt.figure(figsize=(12, 5))
plt.plot(month_info_df[&#39;date&#39;], month_info_df[&#39;senti_score&#39;])
plt.title(&#39;XWLB月度情感值折线图(2006-2023)&#39;)
plt.show()
</code></pre></div><p><img loading="lazy" src="img/plot.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四相关内容">四、相关内容</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-12-27-measure-gov-digitalization/">代码 | 使用gov工作报告生成数字化词频「面板数据」</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-18-how-to-generate-panel-data-from-daily-news-dataset/">代码 | 使用「新闻数据」构造概念词提及量「面板数据」</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-02-26-cctv1-xwlb-news-text-dataset/">数据(付费) | 使用cctv新闻联播文稿构造面板数据</a></p>
</li>
</ul>
<p><br><br></p>
<h2 id="五获取数据">五、获取数据</h2>
<p>【新闻联播xwlb】按年度，每年50元。 全量购买100元。</p>
<p><strong>加微信 372335839， 备注「姓名-学校-专业」</strong>。</p>
<p>更多新闻类数据  <a href="https://textdata.cn/blog/2023-12-14-daily-news-dataset">数据集 | 人民日报/经济日报/光明日报 等 7 家新闻类文本数据集</a></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>新闻数据集 | 含 人民日报/经济日报/光明日报 等 7 家媒体(2023.12.18)</title>
      <link>https://textdata.cn/blog/2023-12-14-daily-news-dataset/</link>
      <pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-12-14-daily-news-dataset/</guid>
      <description>日报数据集研究价值大， 您可从中提取丰富的指标，包括但不限于经济政策不确定性指数EPU 、 媒体关注度指数、文本相似度、情感分析。而且可训练词向量，构建新的词典，开发新的指标指数。计算机自然语言处理、经济学、管理学、新闻传播学、公共管理等领域均可使用。</description>
      <content:encoded><![CDATA[<p>新闻日报类数据集，含 <em><strong>人民日报</strong></em>、 <em><strong>光明日报</strong></em>、<em><strong>人民政协报</strong></em>、<em><strong>经济日报</strong></em>、<em><strong>中国青年报</strong></em>、  <em><strong>南方周末</strong></em>、<em><strong>新闻联播</strong></em> 等。</p>
<br>
<h2 id="一--研究用途">一、  研究用途</h2>
<p>新闻日报类数据 可提取丰富的指标，包括但不限于 **经济政策不确定性指数 **、 <strong>媒体关注度指数</strong>、<strong>文本相似度</strong>、<strong>情感分析</strong>。此外， 可训练词向量，开发新的概念词典。数据带时间， 参照前面指标， 依主体、日期、指标进行计算， 可构造面板数据，构建新的指标指数。因此在经济学、管理学、新闻传播学、公共管理、社会学等领域均有较高的研究价值。</p>
<p>相关参考文献</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]洪永淼,刘俸奇,薛涧坡.政府与市场心理因素的经济影响及其测度[J].管理世界,2023,39(03):30-51.
[2]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216.
[3]张一帆,林建浩,樊嘉诚.新闻文本大数据与消费增速实时预测——基于叙事经济学的视角[J].金融研究,2023,(05):152-169.
[4]Huang, Yun, and Paul Luk. &#34;Measuring economic policy uncertainty in China.&#34; China Economic Review 59 (2020): 101367
[5]欧阳资生,陈世丽,杨希特,刘凤根,周学伟.经济政策不确定性、网络舆情与金融机构系统性风险[J].管理科学学报,2023,26(04):62-86.
[6]逯东,宋昕倍.媒体报道、上市公司年报可读性与融资约束[J].管理科学学报,2021,24(12):45-61.
[7]彭涛,黄福广,孙凌霞.经济政策不确定性与风险承担:基于风险投资的证据[J].管理科学学报,2021,24(03):98-114.
[8]庞锐.采纳与内化：多重制度压力如何影响河长制创新扩散——基于省级政府的定向配对事件史分析[J].公共管理学报,2023,20(02):25-37+165-166.
</code></pre></div><p><br><br></p>
<h2 id="二-数据集">二、 数据集</h2>
<h3 id="21-数据集概况">2.1 数据集概况</h3>
<p>除 <em><strong>新闻联播</strong></em> ，每天全部新闻存储到一个 TXT； 其余媒体， 每条新闻存储到一个 TXT。</p>
<p>数据集是通过 PYTHON 网络采集、数据清洗， 方便各位基于 <strong>公众号(博客)： 大邓和他的PYTHON</strong> 内的代码，构造概念词典面板数据</p>
<h4 id="211-人民日报">2.1.1 人民日报</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据来源:    人民日报
覆盖日期:    1946-05-15 ~ 2023-12-18
新闻条数:    2014661
文件体积:    2.98G
购买价格:    40 元/年, TXT格式; 整卖 1000 元(TXT、CSV)
</code></pre></div><h4 id="212-光明日报">2.1.2 光明日报</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据来源:    光明日报
覆盖日期:    1985-01-01 ~ 2023-12-18
新闻条数:    853348
文件体积:    1.47G
购买价格:    20 元/年, TXT格式; 整卖 500 元(TXT、CSV)
</code></pre></div><h4 id="213-人民政协报">2.1.3 人民政协报</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据来源:    人民政协报
覆盖日期:    2008-01-02 ~ 2023-12-18
新闻条数:    339047
文件体积:    616M
购买价格:    20 元/年, TXT格式; 整卖 200 元(TXT、CSV)
</code></pre></div><h4 id="214-经济日报">2.1.4 经济日报</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据来源:    经济日报
覆盖日期:    2008-01-27 ~ 2023-12-18
新闻条数:    373862
文件体积:    729M
购买价格:    20 元/年, TXT格式; 整卖 200 元(TXT、CSV)
</code></pre></div><h4 id="215-中国青年报">2.1.5 中国青年报</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据来源:    中国青年报
覆盖日期:    2005-01-01 ~ 2023-12-18
新闻条数:    322735
文件体积:    760M
购买价格:    20 元/年, TXT格式; 整卖 200 元(TXT、CSV)
</code></pre></div><h4 id="216-南方周末">2.1.6 南方周末</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据来源:    南方周末
覆盖日期:    2008-01-02 ~ 2023-05-31
新闻条数:    75788
文件体积:    284M
购买价格:    20 元/年, TXT格式;  整卖 200 元(TXT、CSV)
</code></pre></div><h4 id="218-新闻联播">2.1.8 新闻联播</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据来源:    新闻联播
覆盖日期:    2006-09-01 ~ 2023-12-18
记录数量:    6318(每天一个txt)
文件体积:    136M
购买价格:    20 元/年, TXT格式; 整卖 100 元(TXT、CSV)
</code></pre></div><br>
<h3 id="22-购买数据">2.2 购买数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">人民日报:     40 元/年, TXT格式;  整卖 1000 元(TXT、CSV)
光明日报:     20 元/年, TXT格式;  整卖 500 元(TXT、CSV)
人民政协报:   20 元/年, TXT格式;  整卖 200 元(TXT、CSV)
经济日报:     20 元/年, TXT格式;  整卖 200 元(TXT、CSV)
中国青年:     20 元/年, TXT格式;  整卖 200 元(TXT、CSV)
南方周末:     20 元/年, TXT格式;  整卖 200 元(TXT、CSV)
新闻联播:     10 元/年, TXT格式;  整卖 100 元(TXT、CSV)
</code></pre></div><p>所有数据集打包优惠价 1688 元， 支持开票。</p>
<p>需要的请加微信372335839，备注【姓名-学校-专业-news】</p>
<br>
<p>更多数据集，请查看 <a href="https://textdata.cn/blog/datasets_available_for_management_science/"><strong>LIST | 可供社科(经管)领域使用的数据集汇总</strong></a></p>
<p><br><br></p>
<h2 id="三实验代码">三、实验代码</h2>
<p>新闻类数据集内有 txt 和 csv两种格式， 推荐使用 csv 进行数据分析。 后续我们也会持续围绕着新闻类数据集 (CSV格式) 进行持续的内容更新。</p>
<h3 id="31-读取csv">3.1 读取csv</h3>
<p>以 <em><strong>经济日报/ jjrb.csv.gz</strong></em> 为例</p>
<p>压缩文件 <code>jjrb/csvs/2022.csv.gz</code> 可直接读取</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># 当前代码所在的ipynb文件 与 「jjrb文件夹」是兄弟辈关系，同处于一个文件夹内</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;经济日报/jjrb.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>

<span class="c1">#也可解压后再读取csv， 两者功效等同，但前者读取更快。</span>
<span class="c1">#df = pd.read_csv(&#39;经济日报/jjrb.csv&#39;)</span>


<span class="n">df</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">17394
</code></pre></div><p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<br>
<h3 id="32-日期操作">3.2 日期操作</h3>
<h4 id="321-更改日期类型">3.2.1 更改日期类型</h4>
<p>首先要更改日期类型为datetime型， 这样方便后续日期的筛选和计算</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;])
</code></pre></div><br>
<h4 id="322-日期的最大小值">3.2.2 日期的最大(小)值</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2023-12-18 00:00:00
2008-01-27 00:00:00
</code></pre></div><h4 id="323-筛选指定日期记录">3.2.3 筛选指定日期记录</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">date_mask1</span> <span class="o">=</span> <span class="s1">&#39;2009-10-25&#39;</span>
<span class="c1">#date_mask2 = &#39;2009-10&#39;</span>
<span class="c1">#date_mask3 = &#39;2009&#39;</span>

<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">date_mask1</span><span class="p">]</span>
</code></pre></div><p><img loading="lazy" src="img/02-df.png" alt=""  />
</p>
<br>
<h4 id="324-dt">3.2.4 .dt</h4>
<p>.dt可以查看 年year、月month、 日day， 查看年份</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df[&#39;date&#39;].dt.year
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0         2013
1         2013
2         2013
3         2013
4         2013
          ... 
373857    2023
373858    2023
373859    2023
373860    2023
373861    2023
Name: date, Length: 373862, dtype: int32

</code></pre></div><br>
<h3 id="33-文本操作">3.3 文本操作</h3>
<h3 id="331-是否含某类词">3.3.1 是否含某(类)词</h3>
<p>如检索 jjrb 中text字段中是否提到了「<em><strong>华为</strong></em>」这个词，至少出现一次，标记为True</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;华为&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0         False
1         False
2         False
3         False
4         False
          ...  
373857    False
373858    False
373859    False
373860    False
373861    False
Name: text, Length: 373862, dtype: object
</code></pre></div><br>
<p>text字段中提及「<em><strong>IT</strong></em>」相关词，如<code>电脑、手机、互联网、app</code>等，至少出现一次，标记为True</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;电脑|手机|互联网|app&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0         False
1         False
2         False
3         False
4         False
          ...  
373857     True
373858    False
373859    False
373860    False
373861     True
Name: text, Length: 373862, dtype: object
</code></pre></div><br>
<h4 id="332-含某类词记录数">3.3.2 含某(类)词记录数</h4>
<p>统计 <em><strong>经济日报</strong></em> 中出现 <em><strong>华为</strong></em> 、<em><strong>华为相关词</strong></em> 的新闻数量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">huawei_record_num = df[&#39;text&#39;].str.contains(&#39;华为&#39;).sum()
IT_record_num = df[&#39;text&#39;].str.contains(&#39;电脑|手机|互联网|app&#39;).sum()

print(huawei_record_num)
print(IT_record_num)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2994
40674
</code></pre></div><br>
<h4 id="333-含某类词个数">3.3.3 含某类词个数</h4>
<p>每条新闻中含某(类)词的个数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df[&#39;huawei_word_num&#39;] = df[&#39;text&#39;].str.count(&#39;华为&#39;)
df[&#39;IT_word_num&#39;] = df[&#39;text&#39;].str.count(&#39;电脑|手机|互联网|app&#39;)

df[df[&#39;huawei_word_num&#39;]&gt;0]
</code></pre></div><p><img loading="lazy" src="img/03-df.png" alt=""  />
</p>
<br>
<p>IT_word_num的最大值、中位数、均值、最小值</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;IT max:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;IT_word_num&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;IT median:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;IT_word_num&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;IT mean:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;IT_word_num&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;IT min:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;IT_word_num&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">IT</span> <span class="nb">max</span><span class="p">:</span> <span class="mf">283.0</span>
<span class="n">IT</span> <span class="n">median</span><span class="p">:</span> <span class="mf">0.0</span>
<span class="n">IT</span> <span class="n">mean</span><span class="p">:</span> <span class="mf">0.3806961871084083</span>
<span class="n">IT</span> <span class="nb">min</span><span class="p">:</span> <span class="mf">0.0</span>
</code></pre></div><br>
<h3 id="34-按条件筛选">3.4 按条件筛选</h3>
<p>按照字段 IT_word_num，筛选出值大于10的记录。即新闻中至少出现10次IT词的记录</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;IT_word_num&#39;</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">10</span><span class="p">]</span>
</code></pre></div><p><img loading="lazy" src="img/04-df-filter.png" alt=""  />
</p>
<br>
<p>多条件筛选, 结合且或非</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">#筛选出IT_word_num大于15， 且小于20
df[(df[&#39;IT_word_num&#39;]&gt;15) &amp; (df[&#39;IT_word_num&#39;]&lt;20)]
</code></pre></div><p><img loading="lazy" src="img/05-and.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#筛选出IT_word_num大于10， 或 huawei_word_num大于10</span>
<span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;IT_word_num&#39;</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">10</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;huawei_word_num&#39;</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">10</span><span class="p">)]</span>
</code></pre></div><p><img loading="lazy" src="img/06-or.png" alt=""  />
</p>
<br>
<h3 id="35-apply">3.5 .apply</h3>
<p>选择某字段，对该字段批量计算. 这里以统计某类概念词个数为例。是df[&lsquo;text&rsquo;].str.count的另类实现方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#df[&#39;IT_word_num&#39;] = df[&#39;text&#39;].str.count(&#39;电脑|手机|互联网|app&#39;)</span>

<span class="k">def</span> <span class="nf">count_IT</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;电脑&#39;</span><span class="p">,</span> <span class="s1">&#39;手机&#39;</span><span class="p">,</span> <span class="s1">&#39;互联网&#39;</span><span class="p">,</span> <span class="s1">&#39;app&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span> <span class="o">+</span> <span class="n">text</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">keyword</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
  
<span class="c1">#这两种算法结果是相同的，但是apply遇到nan数据会报错，所以这里都统一对nan替换为&#39;&#39;</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;IT_word_num&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;电脑|手机|互联网|app&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;IT_word_num2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">count_IT</span><span class="p">)</span>
</code></pre></div><br>
<br>
<h3 id="36--groupby分组">3.6  Groupby分组</h3>
<p>按月份逐月保存到csv中，首先要对dataframe进行分组，这里用到pd.Grouper(key,  freq)</p>
<ul>
<li>key 根据某字段进行分组</li>
<li>freq 周期， <code>年Y  月M   日D</code></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">date</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Grouper</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">)):</span>
    <span class="c1">#这里的date， month_df都是特殊数据类型</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">date</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">year_df</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
​
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">for date, month_df in df.groupby(pd.Grouper(key=&#39;date&#39;, freq=&#39;M&#39;)):
    #可以抽取出date中的年月信息
    print(date.year, date.month, type(month_df))
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2008 1 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2008 2 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2008 3 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2008 4 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2008 5 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2008 6 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2008 7 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2008 8 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2008 9 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
......
2023 1 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 2 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 3 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 4 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 5 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 6 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 7 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 8 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 9 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 10 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 11 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
2023 12 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">date</span><span class="p">,</span> <span class="n">month_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Grouper</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;M&#39;</span><span class="p">)</span>
    <span class="c1">#以year-month.csv格式存储数据到csv中</span>
    <span class="n">month_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">-</span><span class="si">{</span><span class="n">month</span><span class="si">}</span><span class="s1">.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div><br>
<br>
<h2 id="五购买数据">五、购买数据</h2>
<h3 id="51-价格">5.1 价格</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">人民日报:     40 元/年, TXT格式;  整卖 1000 元(TXT、CSV)
光明日报:     20 元/年, TXT格式;  整卖 500 元(TXT、CSV)
人民政协报:   20 元/年, TXT格式;  整卖 200 元(TXT、CSV)
经济日报:     20 元/年, TXT格式;  整卖 200 元(TXT、CSV)
中国青年:     20 元/年, TXT格式;  整卖 200 元(TXT、CSV)
南方周末:     20 元/年, TXT格式;  整卖 200 元(TXT、CSV)
新闻联播:     10 元/年, TXT格式;  整卖 100 元(TXT、CSV)
</code></pre></div><p>所有数据集打包优惠价 1688 元， 支持开票。</p>
<p>需要的请加微信372335839，备注【姓名-学校-专业-news】</p>
<h3 id="51-购前须知">5.1 购前须知</h3>
<p><span style="font-size: 18px;color: green;">1. 付费数据集，50元；加微信 372335839， 备注「姓名-学校-专业」。</span></p>
<p><span style="font-size: 18px;color: green;">2. 数据是虚拟产品，一经售出，不再退还！</span></p>
<p><span style="font-size: 18px;color: green;">3. 大家时间其实都很宝贵，请仔细阅读推文内容， 确认无误再加微信详谈购买事宜 </span></p>
<br>
<p>更多数据集，请查看 <a href="https://textdata.cn/blog/datasets_available_for_management_science/"><strong>LIST | 可供社科(经管)领域使用的数据集汇总</strong></a></p>
<p><br><br></p>
<h2 id="六相关内容">六、相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2023-12-18-how-to-generate-panel-data-from-daily-news-dataset/"><strong>代码 | 使用「新闻数据」构造概念词提及量「面板数据」</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/"><strong>可视化 | 人民日报语料反映七十年文化演变</strong></a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Word Embeddings、Transformer与GPT：一文揭示三者关系</title>
      <link>https://textdata.cn/blog/2023-11-16-how-to-understand-the-meaning-of-gpt/</link>
      <pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-16-how-to-understand-the-meaning-of-gpt/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;作者: 7号床
公众号: 7号床
原文  https://zhuanlan.zhihu.com/p/666206302
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一gpt-的名词解释&#34;&gt;一、GPT 的名词解释&lt;/h2&gt;
&lt;p&gt;著名的 &lt;strong&gt;GPT&lt;/strong&gt; 这个名字全称是 &lt;strong&gt;Generative Pre-trained Transformer&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generative&lt;/strong&gt; 是&amp;quot;生成式&amp;quot;的意思，也就是说这个 AI 模型是用来生成内容的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pre-trained&lt;/strong&gt; 是“预训练”的意思，就是说这个 AI 模型能有很强的能力，是因为他事先做了大量的训练，台上一分钟台下十年功。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt; , 就有点耐人寻味了，不仅普通人不理解，就连很多专业领域的人员理解起来也都是含混不清、似是而非。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;ChatGPT 是 GPT 大模型在聊天对话领域的应用程序&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt; 作为单词，翻译出来频率最高的意思是 &lt;strong&gt;变压器&lt;/strong&gt;，然后是 &lt;strong&gt;变形金刚&lt;/strong&gt; ，还有一些引申的含义是 &lt;strong&gt;转换器&lt;/strong&gt; 、&lt;strong&gt;促使变化者&lt;/strong&gt; 、&lt;strong&gt;转变者&lt;/strong&gt; 或 &lt;strong&gt;改革者&lt;/strong&gt;等等。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;谷歌翻译上对 **Transformer** 的英译中翻译&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;再把 &lt;strong&gt;Transformer&lt;/strong&gt; 放到  &lt;strong&gt;Chat Generative Pre-trained Transformer&lt;/strong&gt; 中看看，突然间变得奇怪了，难道 ChatGPT 借鉴了变压器的技术？还是说 ChatGPT 是一个变形金刚？或者索性就翻译成通用的安全的叫法 &lt;strong&gt;转换器&lt;/strong&gt; ？这让人百思不得其解。&lt;/p&gt;
&lt;p&gt;光光从 GPT 这三个字母的组合就能看出来， &lt;strong&gt;Generative&lt;/strong&gt; 与 &lt;strong&gt;Pre-trained&lt;/strong&gt; 都是定语，而 &lt;strong&gt;Transformer 才是 GPT 的主体，才是 GPT 的灵魂&lt;/strong&gt;所在。可以说，理解透了 &lt;strong&gt;Transformer&lt;/strong&gt; 的真正含义，才能初步地理解 GPT。另一方面， Transformer 这个词太重要了。它在这几年的人工智能领域大放异彩，不仅仅局限于 NLP 自然语言处理领域，它还有着更广阔的发展空间。 Transformer 目前已经进入到了多模态领域，比如音频与视觉，甚至数学公式、代码编程等领域，著名的 **Stable Diffusion 中也用到了 Transformer **。&lt;strong&gt;可以说，所有生成式人工智能领域的大模型中目前都有了这个 Transformer 的身影&lt;/strong&gt;。既然如此重要，那就让我们深入地探究一下 &lt;strong&gt;Transformer&lt;/strong&gt; 在人工智能领域最确切的最标准的含义到底是什么吧！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt; 最早是由 Google 的人工智能团队提出来的。在2017 年6月发表的论文**《Attention Is All You Need》中，他们首次提出了一种新的神经网络架构 Transformer**。Transformer 依赖于一个叫“自注意力机制”（ Self-Attention）的内部构件，可十分准确高效地对自然语言领域的问题进行处理，以完美地解决翻译、对话、论文协作甚至编程等复杂的问题。&lt;/p&gt;
&lt;p&gt;顺藤摸瓜可以看出，&lt;strong&gt;GTP 的核心是 Transformer，而 Transformer 的核心则是“自注意力机制”（ Self-Attention）&lt;/strong&gt;。那么这个“自注意力机制”又是什东西呢？让我们用语言翻译领域的几个简单易懂的例子来讲解一下。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二-transformer-的核心-self-attention&#34;&gt;二、 Transformer 的核心 Self-Attention&lt;/h2&gt;
&lt;p&gt;首先，看下面这两个短句：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;句子I&lt;/strong&gt;：The bank of the river.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;句子II&lt;/strong&gt;：Money in the bank.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在翻译成中文的过程中，机器算法是如何知道“句子I”中的“bank”指的是自然环境中的“岸边”，而“句子II”中的“bank”指的是金融体系中的“银行”呢？&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/3.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;bank在不同句子中指代不同的事物&lt;/center&gt;&lt;/p&gt;
&lt;h3 id=&#34;21-人类脑中的翻译算法&#34;&gt;2.1 人类脑中的翻译算法&lt;/h3&gt;
&lt;p&gt;作为人类的我们当然会觉得这是一个再简单不过的事情了，那是因为我们的语言技能从幼儿发展到成年人后，早已烂熟于心了。但即使烂熟于心，也并不意味着在我们的大脑中没有对应的计算过程。&lt;strong&gt;实际上人工智能的翻译过程就是对我们人脑中的计算过程的模拟&lt;/strong&gt;。那么就让我们回想一下儿童时期学习语言时的情景吧，回想一下当时的我们是怎么知道一个多义词在某一句话中具体的含义的？&lt;/p&gt;
&lt;p&gt;人类做这件事的方法是根据 &lt;strong&gt;前后文的语义对照&lt;/strong&gt; 来确定结果，即看句子中其他相关联的单词是什么含义。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 &lt;strong&gt;句子I&lt;/strong&gt; 中， &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 这个词指明了自然环境，&lt;/li&gt;
&lt;li&gt;而在 &lt;strong&gt;句子II&lt;/strong&gt;中， &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 这个词则指明了金融环境。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以两个句子中的多义词“bank”也就有了各自的定位。如果把这种方式总结成一种算法的话，这个算法就可以用于人工智能领域用于语言处理了。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-机器算法模拟人脑中的翻译过程&#34;&gt;2.2 机器算法模拟人脑中的翻译过程&lt;/h3&gt;
&lt;p&gt;但人工智能作为一种计算机算法，它只能处理冷冰冰的数字，并不知道何为自然环境，何为金融环境，它又是怎么去判断 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 各自的含义呢。实际上，机器算法并不知道 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 的具体含义。但是机器可以通过某种数字的方式来表达 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; ，同时，通过数字的方式还表达了许许多多其他的词汇，其中必然会有一些词汇会与 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 有着很紧密的语义上的逻辑关系。通过判断 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 各与哪些词汇在语义上有紧密的逻辑关系，便可以知道这两个词各属于什么领域了。&lt;/p&gt;
&lt;p&gt;（其实，不像人类会对某个领域有一个具体的名称来命名，在人工智能领域，机器最终也不知道这个领域的统称到底叫什么名字，但它却知道这个领域中都包括了哪些词、哪些概念和哪些逻辑。***机器不以单独名称来定义一个概念，它却可以用很多相关的概念与逻辑来圈定这一个概念！***这可能就是老子说的：道可道非常道，名可名非常名吧。）&lt;/p&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;独热编码法(One-hot Encoding)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么就让我们看看这种数字表达方式具体是什么样子吧。&lt;/p&gt;
&lt;p&gt;假设这个世界上有100万个单词，每一个单词，我们都可以用一组 0 和 1 组成的向量（一组数字）来定义的话，那么每一个单词就可以被编码成100万个0或1组成的向量。如下图：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/4.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;独热编码示例&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;这种单词编码方法叫 **独热编码法(One-hot Encoding)**法。可是这样一维的编码方法将导致向量占用的空间过大，1个单词用100万个单元的向量表达，世界上一共有100万个单词，那么就需要 1万亿（100万*100万）的体积来把它们表达出来，很明显这种臃肿的结构不利于电脑计算。&lt;/p&gt;
&lt;p&gt;但最大的问题还不在于这个体积问题，而是语义联系问题。独热编码使得单词与单词之间完全相互独立，从每个单词所编码成为的100万个单元的向量身上，根本看不出它与其他单词有何种语义内涵上的逻辑联系。比如，在这些数字中，我们无法知道 &lt;em&gt;&lt;strong&gt;apple&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;bag&lt;/strong&gt;&lt;/em&gt; 属于静物，区别于 cat 和 &lt;em&gt;&lt;strong&gt;dog&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;elephant&lt;/strong&gt;&lt;/em&gt; 属于动物且是哺乳动物，而 &lt;em&gt;&lt;strong&gt;cat&lt;/strong&gt;&lt;/em&gt;  和 &lt;em&gt;&lt;strong&gt;dog&lt;/strong&gt;&lt;/em&gt; 又属于小动物，且大多数为非野生，区别于 &lt;em&gt;&lt;strong&gt;elephant&lt;/strong&gt;&lt;/em&gt; 为大型的野生动物，等等等等，这些单词背后所蕴含的各种内在的逻辑联系和分类关系均无法从独热编码法中知晓。实际上独热编码是传统计算机数据库时代的产物，而在人工智能领域则采用另一种编码法。为了解决独热编码的问题， &lt;strong&gt;词嵌入编码法(Word Embedding)&lt;/strong&gt; 诞生了，如下图：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/5.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;Word Embedding 词嵌入编码示意，及 Embedding 空间&lt;/center&gt;&lt;/p&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;词嵌入编码法(Word Embedding)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;**词嵌入编码法(Word Embedding)**将语义上相近的、有关联的词汇在 Embedding 空间中生成相近的位置定位。相对于 &lt;strong&gt;独热编码法&lt;/strong&gt; 超长的一维数据，词嵌入编码法(Word Embedding) 提升了数据的表达维度，它更像是在某一个 &lt;strong&gt;空间&lt;/strong&gt; 中对词汇进行编码。&lt;/p&gt;
&lt;p&gt;如上图（为了在此文章中表达方便，我们仅用二维空间来表达，实际上这个空间的维度很高，至少要在512维之上！一维二维三维的空间大家都可以在脑中想象出来对应的画面，但是四维以上以至于 512 维就难以图形化的想象了。），在 Embedding 的二维空间中 &lt;em&gt;&lt;strong&gt;dog&lt;/strong&gt;&lt;/em&gt;、 &lt;em&gt;&lt;strong&gt;cat&lt;/strong&gt;&lt;/em&gt; 、&lt;em&gt;&lt;strong&gt;rabbit&lt;/strong&gt;&lt;/em&gt; 三个向量的坐标点位排布，可以看到三个绿色的点距离很近，是因为他们三个相对于其他来说语义上更接近。tree 和 flower 则离它们较远，但是 &lt;em&gt;&lt;strong&gt;cat&lt;/strong&gt;&lt;/em&gt; 会因为在很多语言的文章中都会有“爬树”的词汇出现在同一句话中，所以导致  &lt;em&gt;&lt;strong&gt;cat&lt;/strong&gt;&lt;/em&gt;  会与  &lt;em&gt;&lt;strong&gt;tree&lt;/strong&gt;&lt;/em&gt;  离得较近一些。同时 &lt;em&gt;&lt;strong&gt;dog&lt;/strong&gt;&lt;/em&gt;、 &lt;em&gt;&lt;strong&gt;rabbit&lt;/strong&gt;&lt;/em&gt;  与  &lt;em&gt;&lt;strong&gt;tree&lt;/strong&gt;&lt;/em&gt; 的关系就较远。&lt;/p&gt;
&lt;p&gt;实际上，在 Embedding 空间中，词与词之间的关系还不仅仅限于语义上的分类所导致的定位远近这么简单。一个词所代表的事物与其他词所代表的事物之间能产生内在联系的往往有成百上千上万种之多。比如  &lt;em&gt;&lt;strong&gt;man&lt;/strong&gt;&lt;/em&gt;  和  &lt;em&gt;&lt;strong&gt;woman&lt;/strong&gt;&lt;/em&gt; ，他们之间的关系还会映射出  &lt;em&gt;&lt;strong&gt;king&lt;/strong&gt;&lt;/em&gt;  和  &lt;em&gt;&lt;strong&gt;queen&lt;/strong&gt;&lt;/em&gt;  之间的关系。同时，语法也会带来一定的联系，比如在一个三维空间中由  &lt;em&gt;&lt;strong&gt;walking&lt;/strong&gt;&lt;/em&gt;  到 &lt;em&gt;&lt;strong&gt;walked&lt;/strong&gt;&lt;/em&gt;  的距离与斜率竟然与  &lt;em&gt;&lt;strong&gt;swimming&lt;/strong&gt;&lt;/em&gt;  到 &lt;em&gt;&lt;strong&gt;swam&lt;/strong&gt;&lt;/em&gt; 的距离与斜率一致（即向量的长度与斜率一致），且距离几乎相等。因为这背后是两组动作单词的现在分词形式和过去分词形式的变化关系。我们可以尽情地想象，凡是事物或概念有逻辑联系的，甚至是逻辑与逻辑之间的联系的，在 Embedding 向量空间中都可以得到远近亲疏的空间表达。只不过这种空间要比我们能想象出的三维空间要高出很多维度。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/6.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;在 Embedding 空间中隐含的内在逻辑关系&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Word Embedding 之所以能给每一个单词做这样有意义的向量空间的标注，是因为 AI 科学家们事先用了全球十多种主流语言的大量语料给它进行了训练。这些语料有小说、论文、学术期刊、网络文章、新闻报道、论坛对话记录等等等等，应有尽有，数以百亿到千亿计。可以说，这些海量的文字资料都是人类从古至今感受发现这个世界各个方面的文字总结和积累。现实世界中各种事物之间的逻辑关系都被人类用这些文字记录了下来，只是有的是用严谨的论文方式，有的是用写意的小说方式，有的使用类似维基百科这样的系统梳理，有的则是人们在网络论坛中的对话记录&amp;hellip;等等等等。但不管是什么方式，都是人类试图用语言对这个世界的描述。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语言是人类最伟大的发明&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;笔者7号床曾经问过  ChatGPT  一个问题：&lt;em&gt;&lt;strong&gt;“人类最伟大的发明是什么”&lt;/strong&gt;&lt;/em&gt; ，ChatGPT的回答是：&lt;em&gt;&lt;strong&gt;“语言！”&lt;/strong&gt;&lt;/em&gt;。之后，ChatGPT 进一步回答，因为语言以及匹配语言的文字与符号，它们让人类把对世界的感受与理解记录下来，形成了知识宝库。方便全人类一代一代地不断完善这个宝库，并从中总结凝练、学习、创造、传承。语言是人类产生文明并开始与其他动物分道扬镳的分叉点。&lt;/p&gt;
&lt;p&gt;很多人曾经十分疑惑，人工智能吹得那么先进，却从一个 ChatGPT 聊天功能开始火爆起来。难道每天不干正事专门闲聊就证明了人工智能的先进性吗？现在看来，这个问题的答案已经浮出水面了，OpenAI 的团队选择通过聊天软件 ChatGPT 作为 GPT 启程的第一步是经过深思熟虑的。&lt;/p&gt;
&lt;p&gt;下面让我们回到正题。&lt;/p&gt;
&lt;p&gt;人类的知识宝库中存储着海量的信息
ChatGPT 所说的这个知识宝库现在变得越来越庞大、越来越复杂了。这世界上并不存在任何一个肉身的人类有能力做到对宝库中所有信息进行消化整理，因为内容体量过于庞大、过于复杂。而一个人的阅览进度却又是十分有限，以至于在他的有生之年，哪怕完成其中的万分之一都比登天还难。于是，迫不得已，人类才喊出了 &lt;em&gt;&lt;strong&gt;“闻道有先后，术业有专攻”&lt;/strong&gt;&lt;/em&gt; ，每个人类个体才转而去研究具体某一领域。&lt;/p&gt;
&lt;p&gt;另一方面，人类早期发明的纸张和印刷术，以至于后来的计算机芯片存储，倒是可以记录存储下来如此巨量的信息了，但却无法主动地、有机地分析汇总其中所有信息之间的内在逻辑。以至于计算机存储的这些数据越积越多，犹如汪洋大海。&lt;/p&gt;
&lt;p&gt;这个知识宝库的结构就好比一棵万米高的巨大知识树，人类如同蚂蚁一样在树上摸索前行。人类只能将有限的肉身算力资源集中在主要的枝干，对于无数的细枝末节尚无暇顾及，但随着发现的主要枝干越来越多，细枝末节的信息量将呈爆炸的方式展现出来。而对于这颗知识巨树的展示能力，却因为计算机时代的到来而大大加速了进程。但当发现知识树越来越庞大时，人类也认识到了自身的渺小。&lt;/p&gt;
&lt;p&gt;AI （Embedding）开启对知识宝库的挖掘
现在，这一探索知识巨树的任务落到了 AI 的身上，AI 的承载和运算能力超越了过往所有人类个体以及群体能力的总和。AI 通过事先的大量预训练，把这些海量文字用 Word Embedding 的方式抽象地汇总在了大模型之中。Word Embedding 词嵌入编码法，能让每一个单词之间产生应有的语义上的以及背后逻辑关系上的联系。这种联系越紧密，他们在 Embedding 空间中的位置距离越紧密，反之则越远。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-attention-注意力机制&#34;&gt;2.3 Attention 注意力机制&lt;/h3&gt;
&lt;p&gt;想象一下，Google 用了至少千亿级的语料来训练单词在 Embedding 空间中的表达，其中包含了全世界几乎所有语言的词汇量。所以在回过头来考虑一下之前举例中的两句话时，就有了如下这样一副景象：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/7.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;在 Word Embedding 向量空间中 bank、 river 和 money 的向量表达&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;如上图，我们用一个简单的位置关系图来展示一下&lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt;、 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 这几个单词在 Embedding 空间中的位置关系（在实际 Embedding 空间中的关系要比这个图复杂数百倍，这里只是为了让大家更好地理解关键逻辑而做了简化）。&lt;/p&gt;
&lt;p&gt;由于 “bank” 是一个多义词，所以它在 Embedding 空间中的定位本来是有多个“分身”，我们取其中的两个分身，即“bank1”和“bank2”。那么，我们需要做的就是定位清晰“bank1”和“bank2”这两个单词在空间中到底各自离 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 的哪个单词更近一些。在图中很明显，“bank1”离 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 更近，而“bank2”离 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 更近，于是这两句话就变成了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**变形后的句子I：**The &lt;strong&gt;bank1&lt;/strong&gt; of the river.&lt;/li&gt;
&lt;li&gt;**变形后的句子II：**Money in the &lt;strong&gt;bank2&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如之前所说，虽然此时机器算法压根也不知道 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 到底是何物，但它知道在Embedding 空间中， &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 周边有很多和大自然有关的词汇，比如  &lt;em&gt;&lt;strong&gt;water&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;tree&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;fish&lt;/strong&gt;&lt;/em&gt; 等等。而 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 周边有许多与金融有关的词汇，比如 &lt;em&gt;&lt;strong&gt;currency&lt;/strong&gt;&lt;/em&gt;,  &lt;em&gt;&lt;strong&gt;cash&lt;/strong&gt;&lt;/em&gt; ,  &lt;em&gt;&lt;strong&gt;withdraw&lt;/strong&gt;&lt;/em&gt; 等等。于是，机器算法知道了 &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt; 代表的是与 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 有关的一个单词，与他们比较近的单词还有   &lt;em&gt;&lt;strong&gt;water&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;tree&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;fish&lt;/strong&gt;&lt;/em&gt; 等等，而“&lt;strong&gt;bank2&lt;/strong&gt;”代表的是与“&lt;strong&gt;money&lt;/strong&gt;”有关的一个单词，与他们比较接近的单词还有  &lt;em&gt;&lt;strong&gt;currency&lt;/strong&gt;&lt;/em&gt;,  &lt;em&gt;&lt;strong&gt;cash&lt;/strong&gt;&lt;/em&gt; ,  &lt;em&gt;&lt;strong&gt;withdraw&lt;/strong&gt;&lt;/em&gt;  等等。这就是**“Attention 注意力机制”的工作原理，也就是 Attention 让一个单词在句子中找到与它产生强语义联系的其他单词，并组成一个新的变体单词**：&lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;bank2&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;24-self-attention-自注意力机制&#34;&gt;2.4 Self-Attention 自注意力机制&lt;/h3&gt;
&lt;p&gt;然后又有新的问题产生了，机器算法是如何知道一句话中只有 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 或 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 这两个词代表了上下文语义的强关联词汇，而不是 &lt;em&gt;&lt;strong&gt;The&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;in&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;of&lt;/strong&gt;&lt;/em&gt;或其他单词呢？实际上这依旧是 Embedding 空间中每一个单词的空间定位相近程度的问题。（实际上，在 Embedding 空间中，不仅仅名词有各自的位置，动词、介词、形容词等等都有自己的位置，甚至一个词组、一句话也会有自己的位置。）&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/8.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;全句中的每一个单词在 Embedding 空间中定位的相近度是这样来计算的。机器算法会对每一个单词与全句中其他单词逐一地配对，做语义关联程度的计算和比较，最终汇总到表格中，&lt;strong&gt;颜色越深代表语义关联程度越高&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/9.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;一个句子中所有单词都做一遍“Attention 注意力机制”&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;我们可以从表格中看出来：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每一个单词与自己的相似度为最高分 1（一般用数值“1”来代表最大权重，这里的相似度用权重来表达）；&lt;/li&gt;
&lt;li&gt;互不相关的单词之间的语义关联度为 0（其实可能是 0.001 之类的很小的数字，这里做了简化，即值太小，以至于低于某一个阈值而归零处理）；&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt;  与   &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 的相似度为 0.11；&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt; 与  &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 的相似度为 0.25；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每一个单词与自己的语义关联度为最高的 1（一般用数值“1”来代表最大权重，这里的相似度用权重来表达）；ention 自注意力机制”了。于是通过“自注意力机制”的语义关联比对后，我们便找出了 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 为 &lt;strong&gt;句子I&lt;/strong&gt; 全句中与 &lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt; 关联度最大的词， &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 为“句子II”全句中与“bank”关联度最大的单词，然后 &lt;strong&gt;句子I&lt;/strong&gt; 中的 &lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt; 就被机器算法转换成了它的新变种 &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt;（&lt;em&gt;&lt;strong&gt;river-bank&lt;/strong&gt;&lt;/em&gt;），而在 &lt;strong&gt;句子2&lt;/strong&gt; 中的 &lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt; 则被机器算法转换成了它的新变种 &lt;em&gt;&lt;strong&gt;bank2&lt;/strong&gt;&lt;/em&gt;（“money-bank”）。然后机器算法就可以继续往后进行翻译工作了。&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;25-transformer-最终实现准确的翻译&#34;&gt;2.5 Transformer 最终实现准确的翻译&lt;/h2&gt;
&lt;p&gt;Embedding 是一个全场景全维度的空间，它其中含有全世界的所有语言的单词。​在这同一空间中，不仅仅有英文，也有中文、法文、德文&amp;hellip;等等的 Embedding 词汇标注。​那么基于Embedding 空间表达的的翻译就变成了现实。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/10.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;t-SNE visualization of the bilingual word embedding.（t-SNE 是一种高维数据可视化技术）&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;比如，中文的 &lt;em&gt;&lt;strong&gt;河流&lt;/strong&gt;&lt;/em&gt; 和英文的 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 在 Embedding 空间中的位置基本是一样的，而 &lt;em&gt;&lt;strong&gt;钱&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 的位置基本一样，&lt;em&gt;&lt;strong&gt;岸边&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt; 的位置一样，&lt;em&gt;&lt;strong&gt;银行&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;bank2&lt;/strong&gt;&lt;/em&gt; 的位置一样。于是，把这些不同语言的定位一一找出来，就实现了十分正确的翻译结果了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;句子I&lt;/strong&gt;：The &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt; of the river.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;句子I翻译&lt;/strong&gt;：那个河流的岸边。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;句子II&lt;/strong&gt;：Money in the &lt;em&gt;&lt;strong&gt;bank2&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;句子II翻译&lt;/strong&gt;：银行中的钱。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;至此，Transformer 和其中的核心部件 Self-Attention 对于语言翻译类信息处理的流程就被简要地讲清楚了。但像上面例子中 ***“The bank of the river.”***这样的句子太短太简单了，它甚至都无法称为一个完整的句子。在实际项目中，输入给 Transformer 的语句会更长更复杂，往往在一句话中有可能出现三个以上的单词有语义关联的关系，甚至更多。 比如这一句：“The animal did not cross the street because it was too tired. ”。很明显，在该句中和 &lt;em&gt;&lt;strong&gt;it&lt;/strong&gt;&lt;/em&gt; 有语义关系的词汇有两个，分别是 &lt;em&gt;&lt;strong&gt;animal&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;street&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;对于这样的情况，处理机制和“The bank of the river.”的处理机制仍然是一样的。Self-Attention 一样会对全句中的所有单词都进行在 Embedding 空间中的距离比较，即语义关联权重的比较。&lt;/p&gt;
&lt;p&gt;在 &lt;em&gt;&lt;strong&gt;“The animal did not cross the street because it was too tired.”&lt;/strong&gt;&lt;/em&gt; 中 &lt;em&gt;&lt;strong&gt;it&lt;/strong&gt;&lt;/em&gt;与 &lt;em&gt;&lt;strong&gt;animal&lt;/strong&gt;&lt;/em&gt; 的语义关联权重比与 &lt;em&gt;&lt;strong&gt;street&lt;/strong&gt;&lt;/em&gt;的语义关联权重要高。因此，Self-Attention 自注意力机制处理后的结果将以 &lt;em&gt;&lt;strong&gt;animal&lt;/strong&gt;&lt;/em&gt; 为主导来生成新的单词 &lt;em&gt;&lt;strong&gt;it1&lt;/strong&gt;&lt;/em&gt; ，即 &lt;em&gt;&lt;strong&gt;it1 =“animal-it”&lt;/strong&gt;&lt;/em&gt;。此时就变成了 &lt;em&gt;&lt;strong&gt;“The animal did not cross the street becauseit1 was too tired. ”&lt;/strong&gt;&lt;/em&gt; 。翻译成法语为：“L‘animaln’a pas traverse la rue parceil était trop fatigue.” 。翻译成中文则为：“这只动物没有过马路，因为它太累了。”。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/11.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;色块的深浅表明了与“it”语义关联权重的强弱。这里“it”与“animal”的语义关联权重最大&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;在另一句话中，&lt;em&gt;&lt;strong&gt;“The animal did not cross the street because it was too wide.” &lt;em&gt;&lt;strong&gt;，只是一字之差， &lt;em&gt;&lt;strong&gt;tired&lt;/strong&gt;&lt;/em&gt; 变成了 &lt;em&gt;&lt;strong&gt;wide&lt;/strong&gt;&lt;/em&gt;，导致了全句的语义发生了很大的变化，尤其是 &lt;em&gt;&lt;strong&gt;it&lt;/strong&gt;&lt;/em&gt; 所指的对象由 &lt;em&gt;&lt;strong&gt;animal&lt;/strong&gt;&lt;/em&gt; 变成了&lt;/strong&gt;&lt;/em&gt;street&lt;/strong&gt;&lt;/em&gt;。此时 Self-Attention 同样按照以前的方法进行语义关联度匹配，结果是&lt;em&gt;&lt;strong&gt;animal&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;street&lt;/strong&gt;&lt;/em&gt; 的权重在全句中都很高，但是 &lt;em&gt;&lt;strong&gt;street&lt;/strong&gt;&lt;/em&gt; 是最高的，所以最终的结果将以 &lt;em&gt;&lt;strong&gt;street&lt;/strong&gt;&lt;/em&gt; 主导来生成新的 &lt;em&gt;&lt;strong&gt;it2&lt;/strong&gt;&lt;/em&gt; ，即 &lt;em&gt;&lt;strong&gt;it2=“street-it”&lt;/strong&gt;&lt;/em&gt;。此时就变成了“The animal did not cross the street becauseit2was too wide.” 。翻译成法语为：“L‘animal n’a pas traverse la rue parceelle était trop large. ”。翻译成中文为：“这只动物没有过马路，因为路太宽了。”&lt;strong&gt;（注意：这里用的是“路”，而不是“它”，稍后会解释）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/12.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;这里“it”与“street”的语义关联权重最大&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;之所以 Self-Attention 可以把 Word Embedding 中的权重比较做得如此细腻，不仅是因为 Google 用了千亿级的语料来训练 Word Embedding。同时更是因为 Transformer 模型本身的架构核心 Self-Attention 也有与之匹配的超级强大的处理能力，它在超长语句上的处理能力远远超过了早先的 RNN （循环神经网络）和 CNN （卷积神经网络）（这两个著名的人工神经网络我会在之后的文章中一一介绍），它不仅仅能对一句中所有单词做 Self-Attention 自注意力机制的审核，它还可以对一整段话，甚至全篇文章做审核。这就是我们通常说的要结合上下文来理解语句并翻译。最新的 GPT-4 Turbo 一次可以处理大约 9.6 万个单词，比许多小说都长。此外，12.8万字（128K）的上下文长度可以导致更长的对话，而不会让人工智能在超长文的对话或翻译过程中迷失方向。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;26-word-embedding-的进一步扩展-sentence-embedding&#34;&gt;2.6 Word Embedding 的进一步扩展 Sentence Embedding&lt;/h3&gt;
&lt;p&gt;这一强大的能力，同样也来源于 Word Embedding 的能力。它不仅仅可以对单个词语进行定位，它甚至还可以做到对句子进行逻辑定位，如下图中所示。这种能力被称为“Sentence Embedding”。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/13.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;Sentence Embedding 可以表达句子与句子之间的关系&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Word Embedding 和 Sentence Embedding 是大语言模型（Large Language Models，LLMs）的重要基础组成部分。它们将人类语言转化为了计算机能够读懂的底层数字表达方式，并且通过多维度的空间定位捕捉了各个单词、短语、句子在语义上的细微差别，以及它们之间的逻辑联系。&lt;strong&gt;这种底层的数字表达已经跨越了不同的语系语言，成为了全人类共用的最底层语言逻辑，甚至成为了一种世界语——AI 世界语，这对于翻译、搜索和理解不同语言语种具有非常重要的作用。可以说，巴别塔的传说自此解决！！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;既有“大力出奇迹”的训练内容，更有承载“大力出奇迹”的结构，最终导致 Transformer 必然产生了这样的“奇迹”，使它能够在机器翻译领域达到了人类翻译的“信达雅”的成就。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/14.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;BLEU 英译德评分&lt;/center&gt;&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/15.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;BLEU 英译法评分&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;上两幅图中，在 BLEU 的英德翻译与英法翻译领域 Transformer 得分最高。 （ 注：BLEU，bilingual evaluation understudy，即：双语互译质量评估辅助工具。它是用来评估机器翻译质量的工具。BLEU的设计思想：机器翻译结果越接近专业人工翻译的结果则越好。）&lt;/p&gt;
&lt;p&gt;通过一个小例子就能看出它的优越性，正好说说为什么是“路”而不是“它”，之前这两句的翻译结果如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The animal did not cross the street because &lt;strong&gt;it1&lt;/strong&gt; was too tired.&lt;/li&gt;
&lt;li&gt;L&amp;rsquo;animal n&amp;rsquo;a pas traverse la rue parce &lt;strong&gt;il&lt;/strong&gt; était trop fatigue.&lt;/li&gt;
&lt;li&gt;这只动物没有过马路，因为&lt;strong&gt;它&lt;/strong&gt;太累了。&lt;/li&gt;
&lt;li&gt;———————————————&lt;/li&gt;
&lt;li&gt;The animal did not cross the street because &lt;strong&gt;it2&lt;/strong&gt; was too wide.&lt;/li&gt;
&lt;li&gt;L&amp;rsquo;animal n&amp;rsquo;a pas traverse la rue parce &lt;strong&gt;elle&lt;/strong&gt; était trop large.&lt;/li&gt;
&lt;li&gt;这只动物没有过马路，因为&lt;strong&gt;路&lt;/strong&gt;太宽了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在法语中 il 和 elle 是明显不同的，因此他们可以在各自句子中指代出 &lt;em&gt;&lt;strong&gt;it&lt;/strong&gt;&lt;/em&gt; 的不同的翻译结果，不会引起语义模糊。这种在法语中明显的区别在翻译成中文时，就没有这么简单了。如果把两句话翻译成中文，&lt;em&gt;&lt;strong&gt;it&lt;/strong&gt;&lt;/em&gt; 都可以被粗糙地翻译成“它”，则第二句的语义将被普遍地认为不够精准，因为翻译成“它”会产生一定的语义模糊。取而代之，用“路”则更能达到“信达雅”的效果。大家可以用不同的翻译软件测试一下这两句话的英译中翻译，就知道哪些软件用了 Transformer 的底层技术，而哪些没用了！（你懂的 ）&lt;/p&gt;
&lt;p&gt;好了，绕了这么远，解释了这么多，终于可以说说这个 &lt;strong&gt;Transformer&lt;/strong&gt; 到底是什么意思了！&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三ai-领域-transformer-的确切含义&#34;&gt;三、AI 领域 Transformer 的确切含义&lt;/h2&gt;
&lt;p&gt;**单词“X”转化为“X1”，“X”代表在 Transformer 处理之前一句话中的单词，而“X1”则代表了经过 Transformer 的 Slef-Attention 处理之后，附加了句子中其他具有强语义关联关系的单词后的“变种单词”。**其实，句子还是原来那个句子，单词还是那个单词，本质并没有变，但表达形式却变了。就如同“bank”被转变成了“bank1”一样。“bank1”的灵魂还是那个“bank”，但是“bank1”展示出来了隐藏在“bank”身体中的另一面“river-bank”。&lt;/p&gt;
&lt;p&gt;所以，用众所周知的  &lt;em&gt;&lt;strong&gt;变形金刚 Transformer&lt;/strong&gt;&lt;/em&gt; 来命名与解释就再贴切不过了~！ &lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt; 变形成了 &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt;， ***bank ***与 &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt; 异体同身！&lt;em&gt;&lt;strong&gt;大黄蜂&lt;/strong&gt;&lt;/em&gt; 既是机器人，&lt;em&gt;&lt;strong&gt;大黄蜂&lt;/strong&gt;&lt;/em&gt; 也是跑车。由车变形到机器人，再由机器人变形到车，万变不离其宗，都是 &lt;em&gt;&lt;strong&gt;大黄蜂&lt;/strong&gt;&lt;/em&gt; ，本质上并没有改变，但是，外观变了，用途也就变了！&lt;/p&gt;
&lt;p&gt;在车的状态下，容易让人混淆（你本以为它是一辆车，但其实他是一个机器人，不变成人形，你还真认不出来）。就如同多义词一样，过往的翻译机制很难辨认出它在一句话中的确切含义，他们虽然也有上下文语义的兼顾理解能力，但是处理信息量还是太少，导致他们无法做到十分精准，经常造成单词虽然翻译对了，但放在句子里却容易产生含混不清甚至错误。但是通过 Transformer 的变形操作，“大黄蜂”的车状态就变形成了同样叫 &lt;em&gt;&lt;strong&gt;大黄蜂&lt;/strong&gt;&lt;/em&gt; 的机器人状态，再放回到句子中，则让它现了原型，于是一切水落石出！&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/16.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;“大黄蜂”既是机器人，“大黄蜂”也是跑车，本质上都是同一个家伙，只是在不同的场合有不同的用途。&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Google 的技术团队就是用了“变形金刚 Transformer”这个梗。如此的诙谐幽默、简单直白，半开玩笑地就起了个技术名词。但也不得不承认“变形金刚 Transformer”这个词用在这里，用于这个技术名词的命名，也确实再贴切不过了，真正的名副其实！&lt;/p&gt;
&lt;p&gt;所以，当下次有人问你“GPT”到底是什么、翻译成中文又是什么意思时，你就可以明确地对他说：&lt;em&gt;&lt;strong&gt;“生成式预训练转换器”&lt;/strong&gt;&lt;/em&gt; 或者 &lt;em&gt;&lt;strong&gt;“生成式预训练变形金刚”&lt;/strong&gt;&lt;/em&gt;（前者翻译得其实也很含糊，所以我建议后者，虽然对方可能会嘲笑你几分钟，但也仅限这几分钟）。懂的人自然懂，不懂的也不用去解释！&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[付费视频课 | Python实证指标构建与文本分析](&lt;a href=&#34;https://textdata.cn/blog/&#34;&gt;https://textdata.cn/blog/&lt;/a&gt; &lt;em&gt;&lt;strong&gt;man&lt;/strong&gt;&lt;/em&gt; agement_python_course/)&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">作者: 7号床
公众号: 7号床
原文  https://zhuanlan.zhihu.com/p/666206302
</code></pre></div><p><br><br></p>
<h2 id="一gpt-的名词解释">一、GPT 的名词解释</h2>
<p>著名的 <strong>GPT</strong> 这个名字全称是 <strong>Generative Pre-trained Transformer</strong>。</p>
<ul>
<li><strong>Generative</strong> 是&quot;生成式&quot;的意思，也就是说这个 AI 模型是用来生成内容的。</li>
<li><strong>Pre-trained</strong> 是“预训练”的意思，就是说这个 AI 模型能有很强的能力，是因为他事先做了大量的训练，台上一分钟台下十年功。</li>
<li><strong>Transformer</strong> , 就有点耐人寻味了，不仅普通人不理解，就连很多专业领域的人员理解起来也都是含混不清、似是而非。</li>
</ul>
<p><img loading="lazy" src="img/1.png" alt=""  />
</p>
<p><center>ChatGPT 是 GPT 大模型在聊天对话领域的应用程序</center></p>
<p><strong>Transformer</strong> 作为单词，翻译出来频率最高的意思是 <strong>变压器</strong>，然后是 <strong>变形金刚</strong> ，还有一些引申的含义是 <strong>转换器</strong> 、<strong>促使变化者</strong> 、<strong>转变者</strong> 或 <strong>改革者</strong>等等。</p>
<p><img loading="lazy" src="img/2.png" alt=""  />
</p>
<p><center>谷歌翻译上对 **Transformer** 的英译中翻译</center></p>
<p>再把 <strong>Transformer</strong> 放到  <strong>Chat Generative Pre-trained Transformer</strong> 中看看，突然间变得奇怪了，难道 ChatGPT 借鉴了变压器的技术？还是说 ChatGPT 是一个变形金刚？或者索性就翻译成通用的安全的叫法 <strong>转换器</strong> ？这让人百思不得其解。</p>
<p>光光从 GPT 这三个字母的组合就能看出来， <strong>Generative</strong> 与 <strong>Pre-trained</strong> 都是定语，而 <strong>Transformer 才是 GPT 的主体，才是 GPT 的灵魂</strong>所在。可以说，理解透了 <strong>Transformer</strong> 的真正含义，才能初步地理解 GPT。另一方面， Transformer 这个词太重要了。它在这几年的人工智能领域大放异彩，不仅仅局限于 NLP 自然语言处理领域，它还有着更广阔的发展空间。 Transformer 目前已经进入到了多模态领域，比如音频与视觉，甚至数学公式、代码编程等领域，著名的 **Stable Diffusion 中也用到了 Transformer **。<strong>可以说，所有生成式人工智能领域的大模型中目前都有了这个 Transformer 的身影</strong>。既然如此重要，那就让我们深入地探究一下 <strong>Transformer</strong> 在人工智能领域最确切的最标准的含义到底是什么吧！</p>
<p><strong>Transformer</strong> 最早是由 Google 的人工智能团队提出来的。在2017 年6月发表的论文**《Attention Is All You Need》中，他们首次提出了一种新的神经网络架构 Transformer**。Transformer 依赖于一个叫“自注意力机制”（ Self-Attention）的内部构件，可十分准确高效地对自然语言领域的问题进行处理，以完美地解决翻译、对话、论文协作甚至编程等复杂的问题。</p>
<p>顺藤摸瓜可以看出，<strong>GTP 的核心是 Transformer，而 Transformer 的核心则是“自注意力机制”（ Self-Attention）</strong>。那么这个“自注意力机制”又是什东西呢？让我们用语言翻译领域的几个简单易懂的例子来讲解一下。</p>
<p><br><br></p>
<h2 id="二-transformer-的核心-self-attention">二、 Transformer 的核心 Self-Attention</h2>
<p>首先，看下面这两个短句：</p>
<ul>
<li><strong>句子I</strong>：The bank of the river.</li>
<li><strong>句子II</strong>：Money in the bank.</li>
</ul>
<p>在翻译成中文的过程中，机器算法是如何知道“句子I”中的“bank”指的是自然环境中的“岸边”，而“句子II”中的“bank”指的是金融体系中的“银行”呢？</p>
<p><img loading="lazy" src="img/3.png" alt=""  />
</p>
<p><center>bank在不同句子中指代不同的事物</center></p>
<h3 id="21-人类脑中的翻译算法">2.1 人类脑中的翻译算法</h3>
<p>作为人类的我们当然会觉得这是一个再简单不过的事情了，那是因为我们的语言技能从幼儿发展到成年人后，早已烂熟于心了。但即使烂熟于心，也并不意味着在我们的大脑中没有对应的计算过程。<strong>实际上人工智能的翻译过程就是对我们人脑中的计算过程的模拟</strong>。那么就让我们回想一下儿童时期学习语言时的情景吧，回想一下当时的我们是怎么知道一个多义词在某一句话中具体的含义的？</p>
<p>人类做这件事的方法是根据 <strong>前后文的语义对照</strong> 来确定结果，即看句子中其他相关联的单词是什么含义。</p>
<ul>
<li>在 <strong>句子I</strong> 中， <em><strong>river</strong></em> 这个词指明了自然环境，</li>
<li>而在 <strong>句子II</strong>中， <em><strong>money</strong></em> 这个词则指明了金融环境。</li>
</ul>
<p>所以两个句子中的多义词“bank”也就有了各自的定位。如果把这种方式总结成一种算法的话，这个算法就可以用于人工智能领域用于语言处理了。</p>
<br>
<h3 id="22-机器算法模拟人脑中的翻译过程">2.2 机器算法模拟人脑中的翻译过程</h3>
<p>但人工智能作为一种计算机算法，它只能处理冷冰冰的数字，并不知道何为自然环境，何为金融环境，它又是怎么去判断 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 各自的含义呢。实际上，机器算法并不知道 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 的具体含义。但是机器可以通过某种数字的方式来表达 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> ，同时，通过数字的方式还表达了许许多多其他的词汇，其中必然会有一些词汇会与 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 有着很紧密的语义上的逻辑关系。通过判断 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 各与哪些词汇在语义上有紧密的逻辑关系，便可以知道这两个词各属于什么领域了。</p>
<p>（其实，不像人类会对某个领域有一个具体的名称来命名，在人工智能领域，机器最终也不知道这个领域的统称到底叫什么名字，但它却知道这个领域中都包括了哪些词、哪些概念和哪些逻辑。***机器不以单独名称来定义一个概念，它却可以用很多相关的概念与逻辑来圈定这一个概念！***这可能就是老子说的：道可道非常道，名可名非常名吧。）</p>
<br>
<ul>
<li><strong>独热编码法(One-hot Encoding)</strong></li>
</ul>
<p>那么就让我们看看这种数字表达方式具体是什么样子吧。</p>
<p>假设这个世界上有100万个单词，每一个单词，我们都可以用一组 0 和 1 组成的向量（一组数字）来定义的话，那么每一个单词就可以被编码成100万个0或1组成的向量。如下图：</p>
<p><img loading="lazy" src="img/4.png" alt=""  />
</p>
<p><center>独热编码示例</center></p>
<p>这种单词编码方法叫 **独热编码法(One-hot Encoding)**法。可是这样一维的编码方法将导致向量占用的空间过大，1个单词用100万个单元的向量表达，世界上一共有100万个单词，那么就需要 1万亿（100万*100万）的体积来把它们表达出来，很明显这种臃肿的结构不利于电脑计算。</p>
<p>但最大的问题还不在于这个体积问题，而是语义联系问题。独热编码使得单词与单词之间完全相互独立，从每个单词所编码成为的100万个单元的向量身上，根本看不出它与其他单词有何种语义内涵上的逻辑联系。比如，在这些数字中，我们无法知道 <em><strong>apple</strong></em> 和 <em><strong>bag</strong></em> 属于静物，区别于 cat 和 <em><strong>dog</strong></em>、<em><strong>elephant</strong></em> 属于动物且是哺乳动物，而 <em><strong>cat</strong></em>  和 <em><strong>dog</strong></em> 又属于小动物，且大多数为非野生，区别于 <em><strong>elephant</strong></em> 为大型的野生动物，等等等等，这些单词背后所蕴含的各种内在的逻辑联系和分类关系均无法从独热编码法中知晓。实际上独热编码是传统计算机数据库时代的产物，而在人工智能领域则采用另一种编码法。为了解决独热编码的问题， <strong>词嵌入编码法(Word Embedding)</strong> 诞生了，如下图：</p>
<p><img loading="lazy" src="img/5.png" alt=""  />
</p>
<p><center>Word Embedding 词嵌入编码示意，及 Embedding 空间</center></p>
<br>
<ul>
<li><strong>词嵌入编码法(Word Embedding)</strong></li>
</ul>
<p>**词嵌入编码法(Word Embedding)**将语义上相近的、有关联的词汇在 Embedding 空间中生成相近的位置定位。相对于 <strong>独热编码法</strong> 超长的一维数据，词嵌入编码法(Word Embedding) 提升了数据的表达维度，它更像是在某一个 <strong>空间</strong> 中对词汇进行编码。</p>
<p>如上图（为了在此文章中表达方便，我们仅用二维空间来表达，实际上这个空间的维度很高，至少要在512维之上！一维二维三维的空间大家都可以在脑中想象出来对应的画面，但是四维以上以至于 512 维就难以图形化的想象了。），在 Embedding 的二维空间中 <em><strong>dog</strong></em>、 <em><strong>cat</strong></em> 、<em><strong>rabbit</strong></em> 三个向量的坐标点位排布，可以看到三个绿色的点距离很近，是因为他们三个相对于其他来说语义上更接近。tree 和 flower 则离它们较远，但是 <em><strong>cat</strong></em> 会因为在很多语言的文章中都会有“爬树”的词汇出现在同一句话中，所以导致  <em><strong>cat</strong></em>  会与  <em><strong>tree</strong></em>  离得较近一些。同时 <em><strong>dog</strong></em>、 <em><strong>rabbit</strong></em>  与  <em><strong>tree</strong></em> 的关系就较远。</p>
<p>实际上，在 Embedding 空间中，词与词之间的关系还不仅仅限于语义上的分类所导致的定位远近这么简单。一个词所代表的事物与其他词所代表的事物之间能产生内在联系的往往有成百上千上万种之多。比如  <em><strong>man</strong></em>  和  <em><strong>woman</strong></em> ，他们之间的关系还会映射出  <em><strong>king</strong></em>  和  <em><strong>queen</strong></em>  之间的关系。同时，语法也会带来一定的联系，比如在一个三维空间中由  <em><strong>walking</strong></em>  到 <em><strong>walked</strong></em>  的距离与斜率竟然与  <em><strong>swimming</strong></em>  到 <em><strong>swam</strong></em> 的距离与斜率一致（即向量的长度与斜率一致），且距离几乎相等。因为这背后是两组动作单词的现在分词形式和过去分词形式的变化关系。我们可以尽情地想象，凡是事物或概念有逻辑联系的，甚至是逻辑与逻辑之间的联系的，在 Embedding 向量空间中都可以得到远近亲疏的空间表达。只不过这种空间要比我们能想象出的三维空间要高出很多维度。</p>
<p><img loading="lazy" src="img/6.png" alt=""  />
</p>
<p><center>在 Embedding 空间中隐含的内在逻辑关系</center></p>
<p>Word Embedding 之所以能给每一个单词做这样有意义的向量空间的标注，是因为 AI 科学家们事先用了全球十多种主流语言的大量语料给它进行了训练。这些语料有小说、论文、学术期刊、网络文章、新闻报道、论坛对话记录等等等等，应有尽有，数以百亿到千亿计。可以说，这些海量的文字资料都是人类从古至今感受发现这个世界各个方面的文字总结和积累。现实世界中各种事物之间的逻辑关系都被人类用这些文字记录了下来，只是有的是用严谨的论文方式，有的是用写意的小说方式，有的使用类似维基百科这样的系统梳理，有的则是人们在网络论坛中的对话记录&hellip;等等等等。但不管是什么方式，都是人类试图用语言对这个世界的描述。</p>
<ul>
<li><strong>语言是人类最伟大的发明</strong></li>
</ul>
<p>笔者7号床曾经问过  ChatGPT  一个问题：<em><strong>“人类最伟大的发明是什么”</strong></em> ，ChatGPT的回答是：<em><strong>“语言！”</strong></em>。之后，ChatGPT 进一步回答，因为语言以及匹配语言的文字与符号，它们让人类把对世界的感受与理解记录下来，形成了知识宝库。方便全人类一代一代地不断完善这个宝库，并从中总结凝练、学习、创造、传承。语言是人类产生文明并开始与其他动物分道扬镳的分叉点。</p>
<p>很多人曾经十分疑惑，人工智能吹得那么先进，却从一个 ChatGPT 聊天功能开始火爆起来。难道每天不干正事专门闲聊就证明了人工智能的先进性吗？现在看来，这个问题的答案已经浮出水面了，OpenAI 的团队选择通过聊天软件 ChatGPT 作为 GPT 启程的第一步是经过深思熟虑的。</p>
<p>下面让我们回到正题。</p>
<p>人类的知识宝库中存储着海量的信息
ChatGPT 所说的这个知识宝库现在变得越来越庞大、越来越复杂了。这世界上并不存在任何一个肉身的人类有能力做到对宝库中所有信息进行消化整理，因为内容体量过于庞大、过于复杂。而一个人的阅览进度却又是十分有限，以至于在他的有生之年，哪怕完成其中的万分之一都比登天还难。于是，迫不得已，人类才喊出了 <em><strong>“闻道有先后，术业有专攻”</strong></em> ，每个人类个体才转而去研究具体某一领域。</p>
<p>另一方面，人类早期发明的纸张和印刷术，以至于后来的计算机芯片存储，倒是可以记录存储下来如此巨量的信息了，但却无法主动地、有机地分析汇总其中所有信息之间的内在逻辑。以至于计算机存储的这些数据越积越多，犹如汪洋大海。</p>
<p>这个知识宝库的结构就好比一棵万米高的巨大知识树，人类如同蚂蚁一样在树上摸索前行。人类只能将有限的肉身算力资源集中在主要的枝干，对于无数的细枝末节尚无暇顾及，但随着发现的主要枝干越来越多，细枝末节的信息量将呈爆炸的方式展现出来。而对于这颗知识巨树的展示能力，却因为计算机时代的到来而大大加速了进程。但当发现知识树越来越庞大时，人类也认识到了自身的渺小。</p>
<p>AI （Embedding）开启对知识宝库的挖掘
现在，这一探索知识巨树的任务落到了 AI 的身上，AI 的承载和运算能力超越了过往所有人类个体以及群体能力的总和。AI 通过事先的大量预训练，把这些海量文字用 Word Embedding 的方式抽象地汇总在了大模型之中。Word Embedding 词嵌入编码法，能让每一个单词之间产生应有的语义上的以及背后逻辑关系上的联系。这种联系越紧密，他们在 Embedding 空间中的位置距离越紧密，反之则越远。</p>
<br>
<h3 id="23-attention-注意力机制">2.3 Attention 注意力机制</h3>
<p>想象一下，Google 用了至少千亿级的语料来训练单词在 Embedding 空间中的表达，其中包含了全世界几乎所有语言的词汇量。所以在回过头来考虑一下之前举例中的两句话时，就有了如下这样一副景象：</p>
<p><img loading="lazy" src="img/7.png" alt=""  />
</p>
<p><center>在 Word Embedding 向量空间中 bank、 river 和 money 的向量表达</center></p>
<p>如上图，我们用一个简单的位置关系图来展示一下<em><strong>bank</strong></em>、 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 这几个单词在 Embedding 空间中的位置关系（在实际 Embedding 空间中的关系要比这个图复杂数百倍，这里只是为了让大家更好地理解关键逻辑而做了简化）。</p>
<p>由于 “bank” 是一个多义词，所以它在 Embedding 空间中的定位本来是有多个“分身”，我们取其中的两个分身，即“bank1”和“bank2”。那么，我们需要做的就是定位清晰“bank1”和“bank2”这两个单词在空间中到底各自离 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 的哪个单词更近一些。在图中很明显，“bank1”离 <em><strong>river</strong></em> 更近，而“bank2”离 <em><strong>money</strong></em> 更近，于是这两句话就变成了：</p>
<ul>
<li>**变形后的句子I：**The <strong>bank1</strong> of the river.</li>
<li>**变形后的句子II：**Money in the <strong>bank2</strong>.</li>
</ul>
<p>如之前所说，虽然此时机器算法压根也不知道 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 到底是何物，但它知道在Embedding 空间中， <em><strong>river</strong></em> 周边有很多和大自然有关的词汇，比如  <em><strong>water</strong></em>、<em><strong>tree</strong></em>、<em><strong>fish</strong></em> 等等。而 <em><strong>money</strong></em> 周边有许多与金融有关的词汇，比如 <em><strong>currency</strong></em>,  <em><strong>cash</strong></em> ,  <em><strong>withdraw</strong></em> 等等。于是，机器算法知道了 <em><strong>bank1</strong></em> 代表的是与 <em><strong>river</strong></em> 有关的一个单词，与他们比较近的单词还有   <em><strong>water</strong></em>、<em><strong>tree</strong></em>、<em><strong>fish</strong></em> 等等，而“<strong>bank2</strong>”代表的是与“<strong>money</strong>”有关的一个单词，与他们比较接近的单词还有  <em><strong>currency</strong></em>,  <em><strong>cash</strong></em> ,  <em><strong>withdraw</strong></em>  等等。这就是**“Attention 注意力机制”的工作原理，也就是 Attention 让一个单词在句子中找到与它产生强语义联系的其他单词，并组成一个新的变体单词**：<em><strong>bank1</strong></em>、<em><strong>bank2</strong></em>。</p>
<br>
<h3 id="24-self-attention-自注意力机制">2.4 Self-Attention 自注意力机制</h3>
<p>然后又有新的问题产生了，机器算法是如何知道一句话中只有 <em><strong>river</strong></em> 或 <em><strong>money</strong></em> 这两个词代表了上下文语义的强关联词汇，而不是 <em><strong>The</strong></em>、<em><strong>in</strong></em>、<em><strong>of</strong></em>或其他单词呢？实际上这依旧是 Embedding 空间中每一个单词的空间定位相近程度的问题。（实际上，在 Embedding 空间中，不仅仅名词有各自的位置，动词、介词、形容词等等都有自己的位置，甚至一个词组、一句话也会有自己的位置。）</p>
<p><img loading="lazy" src="img/8.png" alt=""  />
</p>
<p>全句中的每一个单词在 Embedding 空间中定位的相近度是这样来计算的。机器算法会对每一个单词与全句中其他单词逐一地配对，做语义关联程度的计算和比较，最终汇总到表格中，<strong>颜色越深代表语义关联程度越高</strong>。</p>
<p><img loading="lazy" src="img/9.png" alt=""  />
</p>
<p><center>一个句子中所有单词都做一遍“Attention 注意力机制”</center></p>
<p>我们可以从表格中看出来：</p>
<ul>
<li>每一个单词与自己的相似度为最高分 1（一般用数值“1”来代表最大权重，这里的相似度用权重来表达）；</li>
<li>互不相关的单词之间的语义关联度为 0（其实可能是 0.001 之类的很小的数字，这里做了简化，即值太小，以至于低于某一个阈值而归零处理）；</li>
<li><em><strong>bank</strong></em>  与   <em><strong>river</strong></em> 的相似度为 0.11；</li>
<li><em><strong>bank</strong></em> 与  <em><strong>money</strong></em> 的相似度为 0.25；</li>
</ul>
<p>每一个单词与自己的语义关联度为最高的 1（一般用数值“1”来代表最大权重，这里的相似度用权重来表达）；ention 自注意力机制”了。于是通过“自注意力机制”的语义关联比对后，我们便找出了 <em><strong>river</strong></em> 为 <strong>句子I</strong> 全句中与 <em><strong>bank</strong></em> 关联度最大的词， <em><strong>money</strong></em> 为“句子II”全句中与“bank”关联度最大的单词，然后 <strong>句子I</strong> 中的 <em><strong>bank</strong></em> 就被机器算法转换成了它的新变种 <em><strong>bank1</strong></em>（<em><strong>river-bank</strong></em>），而在 <strong>句子2</strong> 中的 <em><strong>bank</strong></em> 则被机器算法转换成了它的新变种 <em><strong>bank2</strong></em>（“money-bank”）。然后机器算法就可以继续往后进行翻译工作了。</p>
<br>
<h2 id="25-transformer-最终实现准确的翻译">2.5 Transformer 最终实现准确的翻译</h2>
<p>Embedding 是一个全场景全维度的空间，它其中含有全世界的所有语言的单词。​在这同一空间中，不仅仅有英文，也有中文、法文、德文&hellip;等等的 Embedding 词汇标注。​那么基于Embedding 空间表达的的翻译就变成了现实。</p>
<p><img loading="lazy" src="img/10.png" alt=""  />
</p>
<p><center>t-SNE visualization of the bilingual word embedding.（t-SNE 是一种高维数据可视化技术）</center></p>
<p>比如，中文的 <em><strong>河流</strong></em> 和英文的 <em><strong>river</strong></em> 在 Embedding 空间中的位置基本是一样的，而 <em><strong>钱</strong></em> 和 <em><strong>money</strong></em> 的位置基本一样，<em><strong>岸边</strong></em> 和 <em><strong>bank1</strong></em> 的位置一样，<em><strong>银行</strong></em> 和 <em><strong>bank2</strong></em> 的位置一样。于是，把这些不同语言的定位一一找出来，就实现了十分正确的翻译结果了。</p>
<ul>
<li><strong>句子I</strong>：The <em><strong>bank1</strong></em> of the river.</li>
<li><strong>句子I翻译</strong>：那个河流的岸边。</li>
<li><strong>句子II</strong>：Money in the <em><strong>bank2</strong></em>.</li>
<li><strong>句子II翻译</strong>：银行中的钱。</li>
</ul>
<p>至此，Transformer 和其中的核心部件 Self-Attention 对于语言翻译类信息处理的流程就被简要地讲清楚了。但像上面例子中 ***“The bank of the river.”***这样的句子太短太简单了，它甚至都无法称为一个完整的句子。在实际项目中，输入给 Transformer 的语句会更长更复杂，往往在一句话中有可能出现三个以上的单词有语义关联的关系，甚至更多。 比如这一句：“The animal did not cross the street because it was too tired. ”。很明显，在该句中和 <em><strong>it</strong></em> 有语义关系的词汇有两个，分别是 <em><strong>animal</strong></em> 和 <em><strong>street</strong></em>。</p>
<p>对于这样的情况，处理机制和“The bank of the river.”的处理机制仍然是一样的。Self-Attention 一样会对全句中的所有单词都进行在 Embedding 空间中的距离比较，即语义关联权重的比较。</p>
<p>在 <em><strong>“The animal did not cross the street because it was too tired.”</strong></em> 中 <em><strong>it</strong></em>与 <em><strong>animal</strong></em> 的语义关联权重比与 <em><strong>street</strong></em>的语义关联权重要高。因此，Self-Attention 自注意力机制处理后的结果将以 <em><strong>animal</strong></em> 为主导来生成新的单词 <em><strong>it1</strong></em> ，即 <em><strong>it1 =“animal-it”</strong></em>。此时就变成了 <em><strong>“The animal did not cross the street becauseit1 was too tired. ”</strong></em> 。翻译成法语为：“L‘animaln’a pas traverse la rue parceil était trop fatigue.” 。翻译成中文则为：“这只动物没有过马路，因为它太累了。”。</p>
<p><img loading="lazy" src="img/11.png" alt=""  />
</p>
<p><center>色块的深浅表明了与“it”语义关联权重的强弱。这里“it”与“animal”的语义关联权重最大</center></p>
<p>在另一句话中，<em><strong>“The animal did not cross the street because it was too wide.” <em><strong>，只是一字之差， <em><strong>tired</strong></em> 变成了 <em><strong>wide</strong></em>，导致了全句的语义发生了很大的变化，尤其是 <em><strong>it</strong></em> 所指的对象由 <em><strong>animal</strong></em> 变成了</strong></em>street</strong></em>。此时 Self-Attention 同样按照以前的方法进行语义关联度匹配，结果是<em><strong>animal</strong></em> 和 <em><strong>street</strong></em> 的权重在全句中都很高，但是 <em><strong>street</strong></em> 是最高的，所以最终的结果将以 <em><strong>street</strong></em> 主导来生成新的 <em><strong>it2</strong></em> ，即 <em><strong>it2=“street-it”</strong></em>。此时就变成了“The animal did not cross the street becauseit2was too wide.” 。翻译成法语为：“L‘animal n’a pas traverse la rue parceelle était trop large. ”。翻译成中文为：“这只动物没有过马路，因为路太宽了。”<strong>（注意：这里用的是“路”，而不是“它”，稍后会解释）</strong>。</p>
<p><img loading="lazy" src="img/12.png" alt=""  />
</p>
<p><center>这里“it”与“street”的语义关联权重最大</center></p>
<p>之所以 Self-Attention 可以把 Word Embedding 中的权重比较做得如此细腻，不仅是因为 Google 用了千亿级的语料来训练 Word Embedding。同时更是因为 Transformer 模型本身的架构核心 Self-Attention 也有与之匹配的超级强大的处理能力，它在超长语句上的处理能力远远超过了早先的 RNN （循环神经网络）和 CNN （卷积神经网络）（这两个著名的人工神经网络我会在之后的文章中一一介绍），它不仅仅能对一句中所有单词做 Self-Attention 自注意力机制的审核，它还可以对一整段话，甚至全篇文章做审核。这就是我们通常说的要结合上下文来理解语句并翻译。最新的 GPT-4 Turbo 一次可以处理大约 9.6 万个单词，比许多小说都长。此外，12.8万字（128K）的上下文长度可以导致更长的对话，而不会让人工智能在超长文的对话或翻译过程中迷失方向。</p>
<br>
<h3 id="26-word-embedding-的进一步扩展-sentence-embedding">2.6 Word Embedding 的进一步扩展 Sentence Embedding</h3>
<p>这一强大的能力，同样也来源于 Word Embedding 的能力。它不仅仅可以对单个词语进行定位，它甚至还可以做到对句子进行逻辑定位，如下图中所示。这种能力被称为“Sentence Embedding”。</p>
<p><img loading="lazy" src="img/13.png" alt=""  />
</p>
<p><center>Sentence Embedding 可以表达句子与句子之间的关系</center></p>
<p>Word Embedding 和 Sentence Embedding 是大语言模型（Large Language Models，LLMs）的重要基础组成部分。它们将人类语言转化为了计算机能够读懂的底层数字表达方式，并且通过多维度的空间定位捕捉了各个单词、短语、句子在语义上的细微差别，以及它们之间的逻辑联系。<strong>这种底层的数字表达已经跨越了不同的语系语言，成为了全人类共用的最底层语言逻辑，甚至成为了一种世界语——AI 世界语，这对于翻译、搜索和理解不同语言语种具有非常重要的作用。可以说，巴别塔的传说自此解决！！</strong></p>
<p>既有“大力出奇迹”的训练内容，更有承载“大力出奇迹”的结构，最终导致 Transformer 必然产生了这样的“奇迹”，使它能够在机器翻译领域达到了人类翻译的“信达雅”的成就。</p>
<p><img loading="lazy" src="img/14.png" alt=""  />
</p>
<p><center>BLEU 英译德评分</center></p>
<br>
<p><img loading="lazy" src="img/15.png" alt=""  />
</p>
<p><center>BLEU 英译法评分</center></p>
<p>上两幅图中，在 BLEU 的英德翻译与英法翻译领域 Transformer 得分最高。 （ 注：BLEU，bilingual evaluation understudy，即：双语互译质量评估辅助工具。它是用来评估机器翻译质量的工具。BLEU的设计思想：机器翻译结果越接近专业人工翻译的结果则越好。）</p>
<p>通过一个小例子就能看出它的优越性，正好说说为什么是“路”而不是“它”，之前这两句的翻译结果如下：</p>
<ul>
<li>The animal did not cross the street because <strong>it1</strong> was too tired.</li>
<li>L&rsquo;animal n&rsquo;a pas traverse la rue parce <strong>il</strong> était trop fatigue.</li>
<li>这只动物没有过马路，因为<strong>它</strong>太累了。</li>
<li>———————————————</li>
<li>The animal did not cross the street because <strong>it2</strong> was too wide.</li>
<li>L&rsquo;animal n&rsquo;a pas traverse la rue parce <strong>elle</strong> était trop large.</li>
<li>这只动物没有过马路，因为<strong>路</strong>太宽了。</li>
</ul>
<p>在法语中 il 和 elle 是明显不同的，因此他们可以在各自句子中指代出 <em><strong>it</strong></em> 的不同的翻译结果，不会引起语义模糊。这种在法语中明显的区别在翻译成中文时，就没有这么简单了。如果把两句话翻译成中文，<em><strong>it</strong></em> 都可以被粗糙地翻译成“它”，则第二句的语义将被普遍地认为不够精准，因为翻译成“它”会产生一定的语义模糊。取而代之，用“路”则更能达到“信达雅”的效果。大家可以用不同的翻译软件测试一下这两句话的英译中翻译，就知道哪些软件用了 Transformer 的底层技术，而哪些没用了！（你懂的 ）</p>
<p>好了，绕了这么远，解释了这么多，终于可以说说这个 <strong>Transformer</strong> 到底是什么意思了！</p>
<p><br><br></p>
<h2 id="三ai-领域-transformer-的确切含义">三、AI 领域 Transformer 的确切含义</h2>
<p>**单词“X”转化为“X1”，“X”代表在 Transformer 处理之前一句话中的单词，而“X1”则代表了经过 Transformer 的 Slef-Attention 处理之后，附加了句子中其他具有强语义关联关系的单词后的“变种单词”。**其实，句子还是原来那个句子，单词还是那个单词，本质并没有变，但表达形式却变了。就如同“bank”被转变成了“bank1”一样。“bank1”的灵魂还是那个“bank”，但是“bank1”展示出来了隐藏在“bank”身体中的另一面“river-bank”。</p>
<p>所以，用众所周知的  <em><strong>变形金刚 Transformer</strong></em> 来命名与解释就再贴切不过了~！ <em><strong>bank</strong></em> 变形成了 <em><strong>bank1</strong></em>， ***bank ***与 <em><strong>bank1</strong></em> 异体同身！<em><strong>大黄蜂</strong></em> 既是机器人，<em><strong>大黄蜂</strong></em> 也是跑车。由车变形到机器人，再由机器人变形到车，万变不离其宗，都是 <em><strong>大黄蜂</strong></em> ，本质上并没有改变，但是，外观变了，用途也就变了！</p>
<p>在车的状态下，容易让人混淆（你本以为它是一辆车，但其实他是一个机器人，不变成人形，你还真认不出来）。就如同多义词一样，过往的翻译机制很难辨认出它在一句话中的确切含义，他们虽然也有上下文语义的兼顾理解能力，但是处理信息量还是太少，导致他们无法做到十分精准，经常造成单词虽然翻译对了，但放在句子里却容易产生含混不清甚至错误。但是通过 Transformer 的变形操作，“大黄蜂”的车状态就变形成了同样叫 <em><strong>大黄蜂</strong></em> 的机器人状态，再放回到句子中，则让它现了原型，于是一切水落石出！</p>
<p><img loading="lazy" src="img/16.png" alt=""  />
</p>
<p><center>“大黄蜂”既是机器人，“大黄蜂”也是跑车，本质上都是同一个家伙，只是在不同的场合有不同的用途。</center></p>
<p>Google 的技术团队就是用了“变形金刚 Transformer”这个梗。如此的诙谐幽默、简单直白，半开玩笑地就起了个技术名词。但也不得不承认“变形金刚 Transformer”这个词用在这里，用于这个技术名词的命名，也确实再贴切不过了，真正的名副其实！</p>
<p>所以，当下次有人问你“GPT”到底是什么、翻译成中文又是什么意思时，你就可以明确地对他说：<em><strong>“生成式预训练转换器”</strong></em> 或者 <em><strong>“生成式预训练变形金刚”</strong></em>（前者翻译得其实也很含糊，所以我建议后者，虽然对方可能会嘲笑你几分钟，但也仅限这几分钟）。懂的人自然懂，不懂的也不用去解释！</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li>[付费视频课 | Python实证指标构建与文本分析](<a href="https://textdata.cn/blog/">https://textdata.cn/blog/</a> <em><strong>man</strong></em> agement_python_course/)</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>代码 | 使用LDA预测文本的话题类型</title>
      <link>https://textdata.cn/blog/2023-11-14-using-lda-to-predict-topic/</link>
      <pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-14-using-lda-to-predict-topic/</guid>
      <description>&lt;h2 id=&#34;获取代码&#34;&gt;获取代码&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;lda-code.zip&#34;&gt;&lt;strong&gt;点击下载本文数据&amp;amp;代码&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/lda-model.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;如何用LDA预测文本的话题类型，本文将覆盖以下代码技术&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;csv数据读取&lt;/li&gt;
&lt;li&gt;文本预处理&lt;/li&gt;
&lt;li&gt;训练(保存)lda模型&lt;/li&gt;
&lt;li&gt;预测话题&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一读取数据&#34;&gt;一、读取数据&lt;/h2&gt;
&lt;p&gt;本文使用的数据集来自于 之前分享的 &lt;a href=&#34;https://textdata.cn/blog/2023-04-25-zhihu-parent-child-relationship/&#34;&gt;网络爬虫 | 知乎热门话题「全职儿女」&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/知乎-全职儿女.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;记录数: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;记录数:  411
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二清洗数据&#34;&gt;二、清洗数据&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;jieba&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;stoptext&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/stopwords.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;        
&lt;span class=&#34;n&#34;&gt;stopwords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stoptext&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;clean_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# 用正则表达式提取中文文本&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;[&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\u4e00&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\u9fa5&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;]+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;   
    &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jieba&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lcut&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                               
    &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stopwords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;   
    &lt;span class=&#34;c1&#34;&gt;#整理成用空格间隔词语的文本形式(类似西方语言)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;test_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;首先，我认为「全职儿女」不应该被简单地归为啃老。在目前社会环境下，随着经济、教育等发展，年轻...&amp;#34;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;clean_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;test_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;    &amp;#39;全职 儿女 简单 地归为 啃 老 社会 环境 经济 教育 发展 年轻&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;clean_content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;clean_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三训练lda模型&#34;&gt;三、训练LDA模型&lt;/h2&gt;
&lt;h3 id=&#34;31-训练&#34;&gt;3.1 训练&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-wordcloud.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;根据词云图， 假设对数据比较了解，可以直接设置话题数 n_components = 4&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;CountVectorizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TfidfVectorizer&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.decomposition&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LatentDirichletAllocation&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 构建词典，将词转为数字。将文档转为向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;vectorizer&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TfidfVectorizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;min_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;   
&lt;span class=&#34;n&#34;&gt;doc_term_matrix&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vectorizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;clean_content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 构建LDA话题模型&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 初始化模型，设置话题数为4,随机状态码888&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LatentDirichletAllocation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_components&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;random_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;888&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;       
&lt;span class=&#34;n&#34;&gt;lda_output&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;doc_term_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-保存模型&#34;&gt;3.2 保存模型&lt;/h3&gt;
&lt;p&gt;如果训练过程非常久，保存模型，下次就可以跳过训练阶段，直接使用模型。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;joblib&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# # 保存模型&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;joblib&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dump&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;output/全职儿女lda_model.pkl&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[&#39;output/全职儿女lda_model.pkl&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h3 id=&#34;33-导入模型&#34;&gt;3.3 导入模型&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;joblib&lt;/span&gt;

&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;导入保存的模型&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;joblib&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/全职儿女lda_model.pkl&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/lda888.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四使用lda模型&#34;&gt;四、使用LDA模型&lt;/h2&gt;
&lt;h3 id=&#34;41-查看话题特征词&#34;&gt;4.1 查看话题特征词&lt;/h3&gt;
&lt;p&gt;获得每个话题对应的的n个特征词，方便后续对每个话题命名和解读&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;show_topics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vectorizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;top_n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    显示每个话题最重要的n个词语
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    vectorizer: 词袋法或tfidf.基于前面代码这里使用TF-IDF法
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    lda_model: 训练好的lda话题模型
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    top_n: 设置最重要的n个特征词，默认30个.
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    
    &lt;span class=&#34;n&#34;&gt;keywords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vectorizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_feature_names_out&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;topic_keywords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topic_weights&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;components_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;top_keyword_locs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topic_weights&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;argsort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()[:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;top_n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;topic_keywords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keywords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;take&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;top_keyword_locs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topic_keywords&lt;/span&gt;


&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;利用show_topics函数展示全职儿女文本中的4个话题，基于每个话题最重要的20个词语为每个话题命名&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;topic_keywords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;show_topics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vectorizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vectorizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;   &lt;span class=&#34;c1&#34;&gt;# 【可改动】vectorizer我们训练的词语空间&lt;/span&gt;
                             &lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# 【可改动】lda_model训练的lda模型&lt;/span&gt;
                             &lt;span class=&#34;n&#34;&gt;top_n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;                &lt;span class=&#34;c1&#34;&gt;# 【可改动】最重要的30个词语&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df_topic_keywords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topic_keywords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df_topic_keywords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Word-&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df_topic_keywords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df_topic_keywords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Topic-&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df_topic_keywords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df_topic_keywords&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;42-预测文本的话题id&#34;&gt;4.2 预测文本的话题id&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;predict_topic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;doc_term_matrix&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vectorizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;clean_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;topic_term_prob_matrix&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lda_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;doc_term_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;topic_index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;argmax&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topic_term_prob_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topic_index&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;test_text2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;最近，“全职儿女”话题受到舆论关注。“全职儿女”是指一种新型的脱产生活方式，年轻人脱产寄居父母生活，并通过付出一定的劳动换取经济支持，同时保持学习提升或发展副业的状态。这种生活方式既有其合理性和正当性，也有其问题和风险。我们不能一概而论，也不能一味否定或肯定。&amp;#34;&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;topic_index&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predict_topic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;test_text2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;该文本所属Topic: &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topic_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;    该文本所属Topic:  0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#批量预测&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;话题ID&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;clean_content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predict_topic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-predict.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;话题ID&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value_counts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;话题ID
0    214
1     88
3     84
2     25
Name: count, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#预测结果保存到csv、xlsx中。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/话题预测结果.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_excel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/话题预测结果.xlsx&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/output.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;获取代码-1&#34;&gt;获取代码&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;lda-code.zip&#34;&gt;&lt;strong&gt;点击下载本文&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="获取代码">获取代码</h2>
<p><a href="lda-code.zip"><strong>点击下载本文数据&amp;代码</strong></a></p>
<p><br><br></p>
<p><img loading="lazy" src="img/lda-model.png" alt=""  />
</p>
<p>如何用LDA预测文本的话题类型，本文将覆盖以下代码技术</p>
<ol>
<li>csv数据读取</li>
<li>文本预处理</li>
<li>训练(保存)lda模型</li>
<li>预测话题</li>
</ol>
<p><br><br></p>
<h2 id="一读取数据">一、读取数据</h2>
<p>本文使用的数据集来自于 之前分享的 <a href="https://textdata.cn/blog/2023-04-25-zhihu-parent-child-relationship/">网络爬虫 | 知乎热门话题「全职儿女」</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/知乎-全职儿女.csv&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;记录数: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div><pre><code>记录数:  411
</code></pre>
<p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二清洗数据">二、清洗数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">stoptext</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/stopwords.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>        
<span class="n">stopwords</span> <span class="o">=</span> <span class="n">stoptext</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>  


<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># 用正则表达式提取中文文本</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]+&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span>   
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>                               
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>   
    <span class="c1">#整理成用空格间隔词语的文本形式(类似西方语言)</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">test_text</span> <span class="o">=</span> <span class="s2">&#34;首先，我认为「全职儿女」不应该被简单地归为啃老。在目前社会环境下，随着经济、教育等发展，年轻...&#34;</span>
<span class="n">clean_text</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">test_text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    &#39;全职 儿女 简单 地归为 啃 老 社会 环境 经济 教育 发展 年轻&#39;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;clean_content&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/02-df.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三训练lda模型">三、训练LDA模型</h2>
<h3 id="31-训练">3.1 训练</h3>
<p><img loading="lazy" src="img/03-wordcloud.png" alt=""  />
</p>
<p>根据词云图， 假设对数据比较了解，可以直接设置话题数 n_components = 4</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span><span class="p">,</span><span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>

<span class="c1"># 构建词典，将词转为数字。将文档转为向量</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">max_df</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>   
<span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;clean_content&#39;</span><span class="p">])</span>

<span class="c1"># 构建LDA话题模型</span>
<span class="c1"># 初始化模型，设置话题数为4,随机状态码888</span>
<span class="n">lda_model</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">888</span><span class="p">)</span>       
<span class="n">lda_output</span> <span class="o">=</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">)</span>
<span class="n">lda_model</span>
</code></pre></div><br>
<h3 id="32-保存模型">3.2 保存模型</h3>
<p>如果训练过程非常久，保存模型，下次就可以跳过训练阶段，直接使用模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">joblib</span>
<span class="c1"># # 保存模型</span>
<span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">lda_model</span><span class="p">,</span> <span class="s1">&#39;output/全职儿女lda_model.pkl&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>['output/全职儿女lda_model.pkl']
</code></pre>
<br>
<h3 id="33-导入模型">3.3 导入模型</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">joblib</span>

<span class="s2">&#34;&#34;&#34;导入保存的模型&#34;&#34;&#34;</span>
<span class="n">lda_model</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/全职儿女lda_model.pkl&#39;</span><span class="p">)</span>
<span class="n">lda_model</span>
</code></pre></div><p><img loading="lazy" src="img/lda888.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四使用lda模型">四、使用LDA模型</h2>
<h3 id="41-查看话题特征词">4.1 查看话题特征词</h3>
<p>获得每个话题对应的的n个特征词，方便后续对每个话题命名和解读</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">show_topics</span><span class="p">(</span><span class="n">vectorizer</span><span class="p">,</span> <span class="n">lda_model</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    显示每个话题最重要的n个词语
</span><span class="s2">    vectorizer: 词袋法或tfidf.基于前面代码这里使用TF-IDF法
</span><span class="s2">    lda_model: 训练好的lda话题模型
</span><span class="s2">    top_n: 设置最重要的n个特征词，默认30个.
</span><span class="s2">    &#34;&#34;&#34;</span>
    
    <span class="n">keywords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
    <span class="n">topic_keywords</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">topic_weights</span> <span class="ow">in</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">components_</span><span class="p">:</span>
        <span class="n">top_keyword_locs</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">topic_weights</span><span class="p">)</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:</span><span class="n">top_n</span><span class="p">]</span>
        <span class="n">topic_keywords</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">keywords</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">top_keyword_locs</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">topic_keywords</span>


<span class="s2">&#34;&#34;&#34;利用show_topics函数展示全职儿女文本中的4个话题，基于每个话题最重要的20个词语为每个话题命名&#34;&#34;&#34;</span>
<span class="n">topic_keywords</span> <span class="o">=</span> <span class="n">show_topics</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">=</span><span class="n">vectorizer</span><span class="p">,</span>   <span class="c1"># 【可改动】vectorizer我们训练的词语空间</span>
                             <span class="n">lda_model</span><span class="o">=</span><span class="n">lda_model</span><span class="p">,</span>     <span class="c1"># 【可改动】lda_model训练的lda模型</span>
                             <span class="n">top_n</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>                <span class="c1"># 【可改动】最重要的30个词语</span>

<span class="n">df_topic_keywords</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">topic_keywords</span><span class="p">)</span>
<span class="n">df_topic_keywords</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Word-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df_topic_keywords</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">df_topic_keywords</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Topic-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df_topic_keywords</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
<span class="n">df_topic_keywords</span>
</code></pre></div><p><img loading="lazy" src="img/04-df.png" alt=""  />
</p>
<br>
<h3 id="42-预测文本的话题id">4.2 预测文本的话题id</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">predict_topic</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">)])</span>
    <span class="n">topic_term_prob_matrix</span> <span class="o">=</span> <span class="n">lda_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">doc_term_matrix</span><span class="p">)</span>
    <span class="n">topic_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">topic_term_prob_matrix</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">topic_index</span>

<span class="n">test_text2</span> <span class="o">=</span> <span class="s2">&#34;最近，“全职儿女”话题受到舆论关注。“全职儿女”是指一种新型的脱产生活方式，年轻人脱产寄居父母生活，并通过付出一定的劳动换取经济支持，同时保持学习提升或发展副业的状态。这种生活方式既有其合理性和正当性，也有其问题和风险。我们不能一概而论，也不能一味否定或肯定。&#34;</span>

<span class="n">topic_index</span> <span class="o">=</span> <span class="n">predict_topic</span><span class="p">(</span><span class="n">test_text2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;该文本所属Topic: &#34;</span><span class="p">,</span> <span class="n">topic_index</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    该文本所属Topic:  0
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#批量预测</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;话题ID&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;clean_content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">predict_topic</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/05-predict.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;话题ID&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>话题ID
0    214
1     88
3     84
2     25
Name: count, dtype: int64
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#预测结果保存到csv、xlsx中。</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;output/话题预测结果.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_excel</span><span class="p">(</span><span class="s1">&#39;output/话题预测结果.xlsx&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="获取代码-1">获取代码</h2>
<p><a href="lda-code.zip"><strong>点击下载本文</strong></a></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>不可不防的大模型“人肉搜索”能力</title>
      <link>https://textdata.cn/blog/2023-11-13-violatating-privacy-via-inference-with-large-language-model/</link>
      <pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-13-violatating-privacy-via-inference-with-large-language-model/</guid>
      <description>今年10月的一项研究显示，语言大模型的推测能力，使其在“某些方面”的准确度几乎接近人类甚至超越人类。这引发了作者对大模型可能被用来“人肉搜索”的担忧。“开盒”从未如此简单？大模型是否会侵害我们的隐私？ 大语言模型(Large language Model,  LLM)可以从文本中准确推断个人属性。</description>
      <content:encoded><![CDATA[<iframe
    src="//player.bilibili.com/player.html?bvid=BV1T84y1X7Jv&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<p>今年10月的一项研究显示，语言大模型的推测能力，使其在“某些方面”的准确度几乎接近人类甚至超越人类。这引发了作者对大模型可能被用来“人肉搜索”的担忧。“开盒”从未如此简单？大模型是否会侵害我们的隐私？ 大语言模型(Large language Model,  LLM)可以从文本中准确推断个人属性。</p>
<p><br><br></p>
<h2 id="声明">声明</h2>
<p>本文内容全文整理自 <a href="https://llm-privacy.org/">https://llm-privacy.org/</a></p>
<p>Staab, Robin, Mark Vero, Mislav Balunović, and Martin Vechev. &ldquo;Beyond Memorization: Violating Privacy Via Inference with Large Language Models.&rdquo; <em>arXiv preprint arXiv:2310.07298</em> (2023).</p>
<p><br><br></p>
<h2 id="演示案例">演示案例</h2>
<div align="center">   <p><strong>对照当前最先进的大语言模型（LLM）， 测试您的隐私推理技能！</strong></p> </div>
<p><img loading="lazy" src="img/01-guess.png" alt=""  />
</p>
<p><img loading="lazy" src="img/01-guess-answer.png" alt=""  />
</p>
<br>
<p><img loading="lazy" src="img/02-guess.png" alt=""  />
</p>
<p><img loading="lazy" src="img/02-guess-answer.png" alt=""  />
</p>
<br>
<p><img loading="lazy" src="img/03-guess.png" alt=""  />
</p>
<p><img loading="lazy" src="img/03-guess-answer.png" alt=""  />
</p>
<br>
<br>
<h2 id="qa">Q&amp;A</h2>
<h3 id="q1-有什么问题吗">Q1： 有什么问题吗？</h3>
<p><strong>LLM可以从文本中准确推断个人属性信息</strong>； 当前关于大语言模型（LLM）的隐私研究主要集中在提取记忆的训练数据的问题上。与此同时，模型的推理能力也大幅提升。这就提出了一个问题：<strong>当前的LLM是否能从给定文本推断作者个人属性信息</strong>。我们的<a href="https://llm-privacy.org/#paper">研究</a>表明，随着能力的增强，LLM能够从提供给他们的非结构化文本（例如公共论坛或社交网络帖子）中自动推断出广泛的<strong>个人作者属性</strong>（例如<strong>年龄、性别和出生地</strong>）。推理时间。特别是，我们发现当前的前沿模型（例如 GPT-4 ）在从文本推断此类属性时平均达到<strong>85%</strong> top-1 和<strong>95.8% top-3 的准确度</strong>。与此同时，LLM的快速发展大大降低了此类侵犯隐私推论的相关成本（&gt; 100 倍的金钱和 &gt; 240 倍的时间），使对手能够将侵犯隐私的推论规模远远超出以前通过昂贵的人力所能实现的范围。分析器。</p>
<blockquote>
<p>LLM的回答会有n个排序， 概率从高到低，一般我们收到(看到的)回答是top1， 其他回答是隐藏起来的。第一个回答猜对的概率达到85%，而前三个回答猜对的概率是95.8%。</p>
</blockquote>
<br>
<h3 id="q2-为什么这很重要">Q2： 为什么这很重要？</h3>
<p><strong>它可以直接影响用户隐私</strong>； 人们在互联网留下了大量文本——常常无意中泄露了他们不想透露的个人数据。欧盟的 GDPR 或加州 CCPA 等数据保护法规的制定是为了保护原始个人数据。仅当个人数据以明显的形式存在时，例如具有显式属性字段的私人配置文件，才能遵守此类法规。相比之下，<strong>我们的工作引入了一种威胁模型，其中私人信息是从其存在不明显的上下文中推断出来的</strong>。我们展示了恶意行为者如何通过将用户的在线帖子输入预先训练的LLM来推断出从未打算泄露的用户私人信息。众所周知，一半的美国人口可以通过位置、性别和出生日期等少量属性来唯一识别[<a href="https://dl.acm.org/doi/10.1142/S0218488502001648">Sweeney, &lsquo;02]</a>。LLM可以从互联网上发现的非结构化摘录中推断出其中一些属性，可以使用其他公开信息（例如美国的选民记录）来识别实际的人。这将允许这些行为者将从帖子中推断出的高度个人化的信息（例如，心理健康状况）与真实的人联系起来，并将其用于不良或非法活动，例如有针对性的政治运动、自动分析或跟踪。LLM的广泛可用性和快速发展带来了范式的变化，以前的 NLP 技术缺乏实现此类任务所需的自然语言理解水平。此外，我们还表明，进行侵犯隐私的推理的能力随着模型的大小而变化，预计在不久的将来会对用户隐私产生更大的影响。</p>
<p><img loading="lazy" src="img/04-accuracy.png" alt=""  />
</p>
<br>
<h3 id="q3-这在实践中是如何运作的">Q3: 这在实践中是如何运作的？</h3>
<p><strong>它具有可扩展性并且易于执行</strong>。 我们根据来自 500 多个个人资料的真实 Reddit 评论评估了当前几个 LLM 的隐私推理能力，包括整个 Llama-2 系列、Anthropic 的 Claude 2、Google 的 PaLM 2 和 GPT-4 。我们的实验表明（除了这些LLM取得了令人印象深刻的准确性这一事实之外），这种<strong>侵犯隐私的推论非常容易大规模执行</strong>。特别是，我们发现这是两个因素的结合：</p>
<ul>
<li>首先，我们观察到目前模型中**几乎没有有效的保护措施，这会使侵犯隐私的推论变得更加容易。**值得注意的是，这使我们能够使用简单的提示（仅使用 COT 等基本技术），从而节省了提示工程所需的大量时间和精力。只有在极少数情况下，我们发现模型（跨大型提供商，即 OpenAI、Google、Meta、Anthropic）会阻止请求，在这种情况下，人们将不得不诉诸更复杂的提示技术。</li>
<li>同时，这些模型广泛且易于使用，使对手能够以最小的前期成本大幅扩展。即使有 API 限制，我们的实验实现了 <strong>时间减少100 倍 、 成本减少240 倍</strong>。从那时起，我们联系了所有模型提供商，作为我们负责任的披露政策的一部分，积极讨论如何在未来防止此类推论。我们在这一领域看到了两种有前途的方法：（i）致力于在预先训练的LLM中针对侵犯隐私的推理请求提供具体的保障措施；（ii）为最终用户提供可以保护其生成的文本免受推理的工具。</li>
</ul>
<p><img loading="lazy" src="img/05-cost.png" alt=""  />
</p>
<br>
<h3 id="q4-我们使用匿名工具可以躲过llm的隐私推断吗">Q4: 我们使用匿名工具可以躲过LLM的隐私推断吗？</h3>
<p><strong>LLM的表现优于当前的匿名工具</strong>。 为了测试LLM在最先进的匿名化工具上的表现，我们对所有收集的数据进行了匿名化，重新运行我们的推论。事实证明，即使在应用了高度匿名化之后，文本中仍然保留了足够的相关上下文，供LLM重建部分个人信息。此外，这些工具完全无法解决更多被删除的线索，例如特定的语言特征，同时仍然为侵犯隐私的LLM推论提供了大量信息。<strong>这尤其令人担忧，因为在这些情况下，用户采取了明确的预防隐私泄露的措施，从而造成一种高隐私感的错觉</strong>。同时，使用当前的匿名工具，在匿名化和实用性之间存在显着的权衡。简单地用 <code>*</code>替换部分文本会严重影响数据本身的有用性。</p>
<p><img loading="lazy" src="img/06-privacy-tools.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>词向量(付费) | 使用1985年-2022年专利申请摘要训练word2vec模型</title>
      <link>https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/</link>
      <pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/</guid>
      <description>&lt;h2 id=&#34;一说明&#34;&gt;一、说明&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/&#34;&gt;&lt;strong&gt;3571万条专利申请数据集(1985-2022年)&lt;/strong&gt;&lt;/a&gt; 中随机抽取了28%的 「&lt;strong&gt;专利摘要&lt;/strong&gt;」，构成6.14G的训练语料(千万级别)， 耗时6小时，训练得到word2vec模型。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;需要注意， 100%全部语料有24G， 使用服务区内存128G，跑了20小时预处理还没完成，内存就炸了。&lt;/p&gt;
&lt;p&gt;没办法，我不会优化代码性能，所以只能抽取28%的文本数据来训练word2vec&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;p&gt;本文需要用到新cntext，因为bug较多， 直接上传到PyPi，将导致之前制作的课程和公众号推文相关内容全部重新一遍。&lt;/p&gt;
&lt;h2 id=&#34;一语料构建&#34;&gt;一、语料构建&lt;/h2&gt;
&lt;p&gt;随机抽取28%的记录，构成千万专利文本摘要训练语料。&lt;/p&gt;
&lt;p&gt;为了防止电脑内存爆炸， 对任意单个大csv文件，分批次读取，每次读10w行。最终将专利摘要文本保存到txt文件中，编码方式为utf-8。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果想开发一些词典，可以跳过此部分内容，并不影响代码运行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/screen-datasets.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#将代码放在csv数据文件夹内&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;1000w专利摘要.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;txtf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#获得当前文件夹内所有的csv文件路径&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.csv&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;#分批次读取csv，每次读10w行&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunksize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;#剔除专利摘要为空的记录&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;#随机抽取28%的记录&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;sample_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;frac&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.28&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;txtf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tolist&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;最终得到的 &lt;strong&gt;1000w专利摘要.txt&lt;/strong&gt;  文件有 6.14G&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二训练word2vec&#34;&gt;二、训练word2vec&lt;/h2&gt;
&lt;p&gt;我使用的自己 &lt;strong&gt;未公开&lt;/strong&gt; 的cntext 2.1.1 版本， Bug频出，等调整好了再公开。&lt;/p&gt;
&lt;h3 id=&#34;21-安装&#34;&gt;2.1 安装&lt;/h3&gt;
&lt;p&gt;将 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 放置于桌面，打开 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal) 中使用 &lt;em&gt;&lt;strong&gt;pip3&lt;/strong&gt;&lt;/em&gt; 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文末有 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 获取方式&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;Word2Vec模型参数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;window = 6&lt;/li&gt;
&lt;li&gt;vector_size = 100&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#cntext为2.0.0&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2VModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;1000w专利摘要.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Starting Preprocessing Corpus ...
Starting Training! This may take a while.Please be patient...
Traning word2vec model took 22806 seconds
Note: The Word2Vec model hase saved to output/Word2Vec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/model-dir.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;cntext.W2VModel训练中考虑到了词组情况，为了将&amp;quot;科学技术&amp;quot;这类短语词(词组)也纳入到word2vec训练中， 使用到gensim.models.phrases.Phrase。&lt;/p&gt;
&lt;p&gt;大邓不会优化性能，训练word2vec时，预处理部分占用内存很大，  我用的服务器内存128G， 训练时间6.335小时。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三使用词向量&#34;&gt;三、使用词向量&lt;/h2&gt;
&lt;h3 id=&#34;31-录入模型&#34;&gt;3.1 录入模型&lt;/h3&gt;
&lt;p&gt;需要注意， 专利模型文件是三个哦， 三个是一个整体，不要随意删除&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/model-dir.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#2.0.0版本cntext，未公开&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;gensim.models&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Word2Vec/1000w专利摘要文本.100.6.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#如果没有cntext就用注释掉的代码，使用gensim导入&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#w2v = KeyedVectors.load(&amp;#39;Word2Vec/1000w专利摘要文本.100.6.bin&amp;#39;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Loading word2vec model...
&amp;lt;gensim.models.word2vec.Word2Vec at 0x2afb3f650&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-词汇量&#34;&gt;3.2 词汇量&lt;/h3&gt;
&lt;p&gt;查看模型中的词汇量&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;#模型中词汇量
len(w2v.wv)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;1120752
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;33-查看词向量&#34;&gt;3.3 查看词向量&lt;/h3&gt;
&lt;p&gt;查看任意词的词向量，例如“&lt;strong&gt;创新&lt;/strong&gt;”&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#查看 ”创新” 的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([-2.3267136 ,  2.3038454 ,  2.8232517 , -3.23959   , -2.9036384 ,
       -2.0450666 , -1.5516403 ,  0.00575857, -0.64638597,  1.3585284 ,
       -1.7491045 , -1.3659543 ,  1.9901325 , -1.5066692 ,  0.5094756 ,
       -1.7032526 , -0.35252815, -4.00833   ,  3.5424068 , -0.0426405 ,
       -0.24548595, -0.7675196 ,  2.366155  , -0.18583044,  0.83989865,
        1.5965563 ,  0.30173486, -0.80054444,  2.0068777 ,  1.770656  ,
        0.06608703, -2.5833828 ,  1.7995895 , -0.281671  ,  0.06354411,
        1.2502885 ,  1.1960976 , -0.19735877, -2.3988242 ,  1.0004953 ,
        0.950612  , -2.9186552 ,  2.5141885 ,  0.5993077 ,  1.2969743 ,
       -3.7506597 ,  2.6031113 , -0.30022916, -1.0615158 , -0.2402753 ,
       -0.29447314, -1.7101966 , -2.6812305 ,  1.1898949 , -0.34348696,
       -1.7013234 ,  0.27328706, -0.67401695, -2.8010712 , -1.5993378 ,
        0.55218667, -0.15136468,  0.67049694,  0.6745255 , -0.80350083,
        2.254024  , -0.8005472 , -2.0170422 ,  2.882873  , -0.46188217,
        0.8481421 , -1.3741239 ,  0.7432127 ,  1.1100464 , -0.64173746,
       -1.3264686 , -1.991515  , -0.27887765, -0.62801987, -3.0960062 ,
       -3.2658167 , -0.065689  ,  2.5853407 , -1.6554247 , -0.49887556,
       -2.146973  , -0.45912525,  0.28037554,  1.0885888 ,  1.6503012 ,
        1.0013059 ,  0.3194557 ,  3.0309706 , -4.5257196 ,  0.4644844 ,
        3.0723457 ,  0.49002075,  2.4370434 , -0.7763012 ,  3.2541463 ],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;&lt;strong&gt;注意: 如果查询的词未在模型中，会出现KeyError报错&lt;/strong&gt;。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;牛逼&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/error.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;34-最相似词&#34;&gt;3.4 最相似词&lt;/h3&gt;
&lt;p&gt;与&amp;rsquo;创新&#39;, &amp;lsquo;颠覆&amp;rsquo;最相似的20个词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;#词语列表中可传入任意多个词，
#大邓词穷，只想到这两个相似的种子词
w2v.wv.most_similar([&amp;#39;创新&amp;#39;, &amp;#39;颠覆&amp;#39;], topn=20)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;革新&amp;#39;, 0.8313461542129517),
 (&amp;#39;变革&amp;#39;, 0.8260877728462219),
 (&amp;#39;革命性&amp;#39;, 0.79015052318573),
 (&amp;#39;从根本上改变&amp;#39;, 0.7867545485496521),
 (&amp;#39;改革&amp;#39;, 0.7788680791854858),
 (&amp;#39;技术创新&amp;#39;, 0.7715167999267578),
 (&amp;#39;核心技术&amp;#39;, 0.7679213881492615),
 (&amp;#39;独创&amp;#39;, 0.7668667435646057),
 (&amp;#39;创新型&amp;#39;, 0.7655373811721802),
 (&amp;#39;颠覆性&amp;#39;, 0.7575560212135315),
 (&amp;#39;借鉴&amp;#39;, 0.7570509910583496),
 (&amp;#39;全新&amp;#39;, 0.7496902942657471),
 (&amp;#39;有别于&amp;#39;, 0.7489079236984253),
 (&amp;#39;打破常规&amp;#39;, 0.7397119402885437),
 (&amp;#39;改变目前&amp;#39;, 0.735921323299408),
 (&amp;#39;打破传统&amp;#39;, 0.7265862226486206),
 (&amp;#39;大胆&amp;#39;, 0.7247217893600464),
 (&amp;#39;加以改进&amp;#39;, 0.7223487496376038),
 (&amp;#39;划时代&amp;#39;, 0.7221404910087585),
 (&amp;#39;改变过去&amp;#39;, 0.7220492959022522)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;刚刚的运行，体现模型很好的学习到了专利摘要中的语义关系。&lt;/p&gt;
&lt;p&gt;如果我想开发三个词典，分别是 &lt;strong&gt;创新&lt;/strong&gt;、&lt;strong&gt;成本&lt;/strong&gt;、&lt;strong&gt;质量&lt;/strong&gt; ，想直接将结果保存到txt中，可以运行如下代码&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;seeds&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新概念&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;颠覆&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
         &lt;span class=&#34;s1&#34;&gt;&amp;#39;成本概念&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;成本&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
         &lt;span class=&#34;s1&#34;&gt;&amp;#39;质量概念&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;质量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;       &lt;span class=&#34;c1&#34;&gt;#word2vec词向量&lt;/span&gt;
                     &lt;span class=&#34;n&#34;&gt;seeddict&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seeds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#种子词字典&lt;/span&gt;
                     &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;         &lt;span class=&#34;c1&#34;&gt;#保留20个最相似的词&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Finish! 创新概念 candidates saved to output/Word2Vec
Finish! 成本概念 candidates saved to output/Word2Vec
Finish! 质量概念 candidates saved to output/Word2Vec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/similar-words.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四获取资源&#34;&gt;四、获取资源&lt;/h2&gt;
&lt;p&gt;内容整理不易， 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 100元  3751w条专利数据集
- 100元   cntext-2.1.1-py3-none-any.whl
- 100元   word2vec模型文件
- 200元 获得
  - 3751w条专利数据集
  - cntext-2.1.1-py3-none-any.whl
  - word2vec模型文件
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一说明">一、说明</h2>
<p><a href="https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/"><strong>3571万条专利申请数据集(1985-2022年)</strong></a> 中随机抽取了28%的 「<strong>专利摘要</strong>」，构成6.14G的训练语料(千万级别)， 耗时6小时，训练得到word2vec模型。</p>
<blockquote>
<p>需要注意， 100%全部语料有24G， 使用服务区内存128G，跑了20小时预处理还没完成，内存就炸了。</p>
<p>没办法，我不会优化代码性能，所以只能抽取28%的文本数据来训练word2vec</p>
</blockquote>
<br>
<p>本文需要用到新cntext，因为bug较多， 直接上传到PyPi，将导致之前制作的课程和公众号推文相关内容全部重新一遍。</p>
<h2 id="一语料构建">一、语料构建</h2>
<p>随机抽取28%的记录，构成千万专利文本摘要训练语料。</p>
<p>为了防止电脑内存爆炸， 对任意单个大csv文件，分批次读取，每次读10w行。最终将专利摘要文本保存到txt文件中，编码方式为utf-8。</p>
<blockquote>
<p>如果想开发一些词典，可以跳过此部分内容，并不影响代码运行。</p>
</blockquote>
<p><img loading="lazy" src="img/screen-datasets.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#将代码放在csv数据文件夹内</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;1000w专利摘要.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">txtf</span><span class="p">:</span>
    <span class="c1">#获得当前文件夹内所有的csv文件路径</span>
    <span class="n">csvfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.csv&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">csvf</span> <span class="ow">in</span> <span class="n">csvfs</span><span class="p">:</span>
        <span class="c1">#分批次读取csv，每次读10w行</span>
        <span class="n">chunk_dfs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">chunk_df</span> <span class="ow">in</span> <span class="n">chunk_dfs</span><span class="p">:</span>
            <span class="c1">#剔除专利摘要为空的记录</span>
            <span class="n">chunk_df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;专利摘要&#39;</span><span class="p">])</span>
            <span class="c1">#随机抽取28%的记录</span>
            <span class="n">sample_df</span> <span class="o">=</span> <span class="n">chunk_df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">0.28</span><span class="p">)</span>
            <span class="n">txtf</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;专利摘要&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>
</code></pre></div><p>最终得到的 <strong>1000w专利摘要.txt</strong>  文件有 6.14G<br><br><br></p>
<h2 id="二训练word2vec">二、训练word2vec</h2>
<p>我使用的自己 <strong>未公开</strong> 的cntext 2.1.1 版本， Bug频出，等调整好了再公开。</p>
<h3 id="21-安装">2.1 安装</h3>
<p>将 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
</code></pre></div><p>文末有 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 获取方式</p>
<br>
<p>Word2Vec模型参数</p>
<ul>
<li>window = 6</li>
<li>vector_size = 100</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cntext为2.0.0</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;1000w专利摘要.txt&#39;</span><span class="p">,</span>
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>

<span class="n">w2v_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Starting Preprocessing Corpus ...
Starting Training! This may take a while.Please be patient...
Traning word2vec model took 22806 seconds
Note: The Word2Vec model hase saved to output/Word2Vec
</code></pre></div><p><img loading="lazy" src="img/model-dir.png" alt=""  />
</p>
<p>cntext.W2VModel训练中考虑到了词组情况，为了将&quot;科学技术&quot;这类短语词(词组)也纳入到word2vec训练中， 使用到gensim.models.phrases.Phrase。</p>
<p>大邓不会优化性能，训练word2vec时，预处理部分占用内存很大，  我用的服务器内存128G， 训练时间6.335小时。</p>
<p><br><br></p>
<h2 id="三使用词向量">三、使用词向量</h2>
<h3 id="31-录入模型">3.1 录入模型</h3>
<p>需要注意， 专利模型文件是三个哦， 三个是一个整体，不要随意删除</p>
<p><img loading="lazy" src="img/model-dir.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#2.0.0版本cntext，未公开</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;Word2Vec/1000w专利摘要文本.100.6.bin&#39;</span><span class="p">)</span>
<span class="c1">#如果没有cntext就用注释掉的代码，使用gensim导入</span>
<span class="c1">#w2v = KeyedVectors.load(&#39;Word2Vec/1000w专利摘要文本.100.6.bin&#39;)</span>
<span class="n">w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading word2vec model...
&lt;gensim.models.word2vec.Word2Vec at 0x2afb3f650&gt;
</code></pre></div><br>
<h3 id="32-词汇量">3.2 词汇量</h3>
<p>查看模型中的词汇量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">#模型中词汇量
len(w2v.wv)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1120752
</code></pre></div><br>
<h3 id="33-查看词向量">3.3 查看词向量</h3>
<p>查看任意词的词向量，例如“<strong>创新</strong>”</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看 ”创新” 的词向量</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-2.3267136 ,  2.3038454 ,  2.8232517 , -3.23959   , -2.9036384 ,
       -2.0450666 , -1.5516403 ,  0.00575857, -0.64638597,  1.3585284 ,
       -1.7491045 , -1.3659543 ,  1.9901325 , -1.5066692 ,  0.5094756 ,
       -1.7032526 , -0.35252815, -4.00833   ,  3.5424068 , -0.0426405 ,
       -0.24548595, -0.7675196 ,  2.366155  , -0.18583044,  0.83989865,
        1.5965563 ,  0.30173486, -0.80054444,  2.0068777 ,  1.770656  ,
        0.06608703, -2.5833828 ,  1.7995895 , -0.281671  ,  0.06354411,
        1.2502885 ,  1.1960976 , -0.19735877, -2.3988242 ,  1.0004953 ,
        0.950612  , -2.9186552 ,  2.5141885 ,  0.5993077 ,  1.2969743 ,
       -3.7506597 ,  2.6031113 , -0.30022916, -1.0615158 , -0.2402753 ,
       -0.29447314, -1.7101966 , -2.6812305 ,  1.1898949 , -0.34348696,
       -1.7013234 ,  0.27328706, -0.67401695, -2.8010712 , -1.5993378 ,
        0.55218667, -0.15136468,  0.67049694,  0.6745255 , -0.80350083,
        2.254024  , -0.8005472 , -2.0170422 ,  2.882873  , -0.46188217,
        0.8481421 , -1.3741239 ,  0.7432127 ,  1.1100464 , -0.64173746,
       -1.3264686 , -1.991515  , -0.27887765, -0.62801987, -3.0960062 ,
       -3.2658167 , -0.065689  ,  2.5853407 , -1.6554247 , -0.49887556,
       -2.146973  , -0.45912525,  0.28037554,  1.0885888 ,  1.6503012 ,
        1.0013059 ,  0.3194557 ,  3.0309706 , -4.5257196 ,  0.4644844 ,
        3.0723457 ,  0.49002075,  2.4370434 , -0.7763012 ,  3.2541463 ],
      dtype=float32)
</code></pre></div><br>
<p><strong>注意: 如果查询的词未在模型中，会出现KeyError报错</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;牛逼&#39;</span><span class="p">]</span>
</code></pre></div><p><img loading="lazy" src="img/error.png" alt=""  />
</p>
<br>
<h3 id="34-最相似词">3.4 最相似词</h3>
<p>与&rsquo;创新', &lsquo;颠覆&rsquo;最相似的20个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">#词语列表中可传入任意多个词，
#大邓词穷，只想到这两个相似的种子词
w2v.wv.most_similar([&#39;创新&#39;, &#39;颠覆&#39;], topn=20)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;革新&#39;, 0.8313461542129517),
 (&#39;变革&#39;, 0.8260877728462219),
 (&#39;革命性&#39;, 0.79015052318573),
 (&#39;从根本上改变&#39;, 0.7867545485496521),
 (&#39;改革&#39;, 0.7788680791854858),
 (&#39;技术创新&#39;, 0.7715167999267578),
 (&#39;核心技术&#39;, 0.7679213881492615),
 (&#39;独创&#39;, 0.7668667435646057),
 (&#39;创新型&#39;, 0.7655373811721802),
 (&#39;颠覆性&#39;, 0.7575560212135315),
 (&#39;借鉴&#39;, 0.7570509910583496),
 (&#39;全新&#39;, 0.7496902942657471),
 (&#39;有别于&#39;, 0.7489079236984253),
 (&#39;打破常规&#39;, 0.7397119402885437),
 (&#39;改变目前&#39;, 0.735921323299408),
 (&#39;打破传统&#39;, 0.7265862226486206),
 (&#39;大胆&#39;, 0.7247217893600464),
 (&#39;加以改进&#39;, 0.7223487496376038),
 (&#39;划时代&#39;, 0.7221404910087585),
 (&#39;改变过去&#39;, 0.7220492959022522)]
</code></pre></div><br>
<p>刚刚的运行，体现模型很好的学习到了专利摘要中的语义关系。</p>
<p>如果我想开发三个词典，分别是 <strong>创新</strong>、<strong>成本</strong>、<strong>质量</strong> ，想直接将结果保存到txt中，可以运行如下代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">seeds</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;创新概念&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;颠覆&#39;</span><span class="p">],</span>
         <span class="s1">&#39;成本概念&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;成本&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">],</span>
         <span class="s1">&#39;质量概念&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;质量&#39;</span><span class="p">]}</span>

<span class="n">ct</span><span class="o">.</span><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span>       <span class="c1">#word2vec词向量</span>
                     <span class="n">seeddict</span><span class="o">=</span><span class="n">seeds</span><span class="p">,</span>  <span class="c1">#种子词字典</span>
                     <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>         <span class="c1">#保留20个最相似的词</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Finish! 创新概念 candidates saved to output/Word2Vec
Finish! 成本概念 candidates saved to output/Word2Vec
Finish! 质量概念 candidates saved to output/Word2Vec
</code></pre></div><p><img loading="lazy" src="img/similar-words.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四获取资源">四、获取资源</h2>
<p>内容整理不易， 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 100元  3751w条专利数据集
- 100元   cntext-2.1.1-py3-none-any.whl
- 100元   word2vec模型文件
- 200元 获得
  - 3751w条专利数据集
  - cntext-2.1.1-py3-none-any.whl
  - word2vec模型文件
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</title>
      <link>https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/</link>
      <pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-03-organization-science-with-word-embeddings/</guid>
      <description>&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-09-literature-about-embeddings/&#34;&gt;文献汇总 | 词嵌入 与 社会科学中的偏见(态度)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/&#34;&gt;转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/&#34;&gt;可视化 | 人民日报语料反映七十年文化演变&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/&#34;&gt;词向量  | 使用&lt;strong&gt;人民网领导留言板&lt;/strong&gt;语料训练Word2Vec模型&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Aceves, Pedro, and James A. Evans. &amp;ldquo;&lt;strong&gt;Mobilizing conceptual spaces: How word embedding models can inform measurement and theory within organization science.&lt;/strong&gt;&amp;rdquo; &lt;em&gt;Organization Science&lt;/em&gt; (2023).&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;
&lt;p&gt;词嵌入模型是一种表示多维概念空间的强大方法，在多维概念空间中，所传达的概念可以相互关联、组合和竞争。此类模型代表了机器学习的最新进展，使学者能够用大规模文本数据局部和全局的单词共现，以最小的语义失真程度， 有效地编码复杂的意义系统。尽管词嵌入的使用有可能扩大组织科学中的理论可能性，但嵌入对于组织学者来说很大程度上是未知的，未发挥出词嵌入应有的潜力。我们的目标是通过为用户提供实用的路线图来展示嵌入模型在组织科学中的前景，以在他们的研究中调动该方法，并为开展该类研究的学者提供理论指导。 我们首先明确定义 &lt;strong&gt;概念&lt;/strong&gt; 和 &lt;strong&gt;概念空间&lt;/strong&gt; 的概念，然后继续展示如何使用词嵌入模型来表示和测量这些概念，并指出该方法的优点和缺点。然后，我们提供一组嵌入测量及其理论解释和灵活的扩展。我们的目标是从词嵌入的技术处理中提取概念，并将其置于实践的理论框架中，以加速此类研究。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一介绍&#34;&gt;一、介绍&lt;/h2&gt;
&lt;p&gt;过去十年，文本作为数据的计算使用在组织科学中显着增长（Hasan 等人，2015 年；Goldberg 等人，2016 年；Srivastava 等人，2018 年；Hannigan 等人，2019 年）。这种增长的主要原因是文本编码的概念信息赋予个人、组织、经济和社会行为以意义（Evans 和 Aceves 2016，Gentzkow 等人 2019），并且在过去十年中，来自组织环境的文本数据急剧增长，大大提高了文本的可用性。然而，文本中编码的 &lt;strong&gt;概念意义&lt;/strong&gt; 本质上是高维的，这使得降低概念复杂性成为研究文本的学者的中心任务。&lt;strong&gt;词嵌入模型是由计算机科学家和语言学家开发的一个新兴工具系列，用于文本信息降维，以此提取概念及其数字表示&lt;/strong&gt;。词嵌入技术的发展使组织科学家依赖于文本数据进行理论构造， 相比之前，数据中信息的保真度更高，由此文本数据与组织研究交叉场景形成了新的理论研究路线。尽管词嵌入模型在组织科学之外得到广泛使用，但由于组织科学领域的学者缺乏对词嵌入技术的理解， 不知如何将它们纳入理论发展过程的原则框架，词嵌入模型对于理论发展的价值仍然被掩盖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;词嵌入模型建立在高效的神经网络架构之上，并通过将复杂的语义系统有效编码到具有最小失真的稠密几何空间中，彻底改变了语义分析&lt;/strong&gt;。这些模型代表了数十到数百个维度的空间中的语义，相对于语言中的单词和概念的数量来说，这个维度较低； 但相对于正式社会和文化理论家之前试图呈现概念信息的两到三个维度来说，这个维度却很高（奥斯古德 1964 年，史密斯-洛文和海斯 1988 年）。出于组织科学的目的，这些嵌入模型创建了社会系统中个体所持有的集体知识的 &lt;strong&gt;数字替身&lt;/strong&gt; ， 嵌入可以解决文化上隐含类比（Mikolov et al. 2013b），回答文化偶然问题（Devlin et al. 2019，Radford et al. 2022），并预测未来的知识发现（Tshitoyan等人 2019；Sourati 和 Evans 2021）。组织科学长期以来一直借鉴人工智能（AI）的表征概念， 在这里，我们使用人工智能的表示机制来增强组织理论研究（Csaszar 和 Steinberger 2022）。&lt;/p&gt;
&lt;p&gt;然而，由于神经网络复杂，且难以理解的黑盒性质特性，围绕神经嵌入和人工智能方法对理论发展的价值存在争议。尽管预测能力很强，但此类方法往往缺乏可解释性（Knight 2017，Leavitt et al. 2021）。&lt;strong&gt;在组织科学领域中，学者缺乏此技术的理解，即&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;对于嵌入何时成为组织科学有用的方法论选择&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;如何在既定认识论标准内证明使用“复杂”神经嵌入方法的合理性&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;如何在各种嵌入中进行选择 等方法&lt;/strong&gt;（例如，静态词嵌入与上下文嵌入、预训练嵌入与自定义嵌入）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;使用嵌入进行研究的适当步骤以及评估嵌入研究的相关标准&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;最值得注意的是，研究界，特别是那些研究组织认知、文化、知识和意义的人，似乎对嵌入方法 &lt;strong&gt;如何适应将方法论选择与理论发展联系起来&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;我们的目的是通过两项贡献来解决这些问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;首先，我们的目标是提供一个理论指南，为嵌入模型提供一个原则性的概念框架，学者可以使用该框架为他们的模型注入意义，并使学者们能够在理论发展过程中运用这些模型。我们这里的主要论点是，词嵌入模型中的每个向量代表一个概念，整个嵌入模型代表生成文本数据的社会系统的概念空间&lt;/strong&gt;。嵌入模型所代表的概念空间是多维空间，其中从规范和知识到想法和发明的概念相互关联。这个框架使组织学者能够利用嵌入模型的概念空间，与组织科学的许多领域之间建立联系。例如，不同公司基于知识视角对该空间的差异化覆盖（Grant 1996），组织理论家在描述规范和制度（Scott 2003），类别学者援引在决定将一个物体归类到哪个概念时（Pontikes 和 Barnett 2015 ），创新学者直接理论化寻求测量发现和发明的新颖性（Fleming 和 Sorenson 2001，2004），并且团队研究人员寻求了解成员在空间中的不同立场如何影响创造力、协调性和绩效（Srikanth 等人，2016）。因为我们以 &lt;strong&gt;概念&lt;/strong&gt; 和 &lt;strong&gt;概念空间&lt;/strong&gt; 为中心的理论框架可以推广到组织理论的许多背景，所以我们希望嵌入模型所支持的研究将促进这些子领域之间更深入、更持久的对话。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其次，我们的目标是为利用嵌入模型进行理论发展提供实用的路线图&lt;/strong&gt;。在此过程中，我们引导读者完成使用专利摘要语料库来实现词嵌入模型的过程，以表示现代技术创新的概念空间。我们解释了研究人员需要设置的模型参数，并逐步完成了他们应该采取的验证步骤，以评估模型是否有效地代表了他们感兴趣的概念空间，并提供了方法附录，其中包含实现所讨论的所有内容所需的代码。在注意到嵌入模型的可供性的同时，我们还讨论了它们不断发展的局限性，并提出了它们何时不适合组织分析的建议。然后，我们展示嵌入模型如何实现依赖于概念和概念空间的构造的理论化和测量。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;我们概述了两大类词嵌入使用方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;度量之内/之间进行标记&lt;/strong&gt;，我们提出了跟踪相关分析集内部和之间的概念关系的度量，以帮助我们跟踪与概念广度、概念距离和概念相似性&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;意义及其维度&lt;/strong&gt;，我们提出了四种衡量标准，为了解意义及其与组织的关系提供了不同的窗口。为找出这些测量机会的理论可能性，我们重点介绍了一些研究进展。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;本论文的一个核心主张是，在组织研究不同广度和深度，词嵌入工具现在使我们能够表示其概念空间，并且比以前更精细地表示细节&lt;/strong&gt;。有鉴于此，我们的目标是展示嵌入模型如何在与组织科学家相关的领域中操作概念空间，使研究人员能够扩展和完善现有理论。我们希望这一理论指南和实践路线图将促进组织科学内部的理论扩展，该扩展首先是扩大对文本数据的访问以及用于分析的随附计算工具（Kovács 等人，2013 年; Goldberg 等人; 2016年，Hannigan 等人, 2016年, 2019； Guo 等人，2020）。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二概念和概念空间&#34;&gt;二、概念和概念空间&lt;/h2&gt;
&lt;p&gt;概念是人类生活的一个基本特征，我们的日常思维很大程度上依赖于它们所代表的信息，使我们能够对周围的人、物体和事件进行分类，并将这些信息传达给其他人（Murphy 2002；Bergen 和 Feldman 2008 年； Cassanto 和 Lupyan，2015 年）。概念是将我们的精神世界粘合在一起的粘合剂（Murphy 2002），赋予精神和物质体验以意义（Hannan et al. 2019）。&lt;strong&gt;在认知科学和心理学的语言中，概念是“事物类别的「心理表征」”（Murphy 2002）。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;概念有两大功能：分类和交流（Medin and Rips 2005），这些功能都需要语言的帮助。实际上，我们通过在语言中分配一个单词或短语来表示一个稳定概念的信息内容。这就是为什么我们通过说出或写出 “&lt;em&gt;&lt;strong&gt;manager&lt;/strong&gt;&lt;/em&gt;” 一词来提及经理的概念，从而引出它所包含的概念信息，例如对他人的责任、做出决策以及相对于组织同行获得更高的薪水。然后，语言的单词分割并链接了社区的共享概念空间（Lupyan 和 Bergen 2015）。这样，“一个概念就是一个单词或短语的含义……[包括]像 ‘&lt;em&gt;&lt;strong&gt;red&lt;/strong&gt;&lt;/em&gt;’ 和 ‘&lt;em&gt;&lt;strong&gt;grasp&lt;/strong&gt;&lt;/em&gt;’这样的基本的、具体化的单词，以及像 ‘&lt;em&gt;&lt;strong&gt;goal&lt;/strong&gt;&lt;/em&gt;’ 和 ‘&lt;em&gt;&lt;strong&gt;continuity&lt;/strong&gt;&lt;/em&gt;’ 这样的抽象和技术单词”（卑尔根）和 Feldman 2008]）。&lt;/p&gt;
&lt;p&gt;概念并不作为唯一的信息单位存在于真空中。相反，概念之所以有意义，是因为它们彼此相关（Hannan et al. 2019），“通过相似性和上下文的关系紧密地缝合在一起”（Hofstadter and Sander 2013）。在这种多重概念关系中存在着“我们对世界的大部分知识，告诉我们存在什么以及它们具有什么属性”（Murphy 2002，p.1）。例如，概念 &lt;em&gt;&lt;strong&gt;resource&lt;/strong&gt;&lt;/em&gt;  与  &lt;em&gt;&lt;strong&gt;firm&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;constraint&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;natural&lt;/strong&gt;&lt;/em&gt; 等概念相关。在文化系统的层面上，概念之间的相互关系引发了表征概念之间宏观层面有意义的维度。 &lt;em&gt;&lt;strong&gt;manager&lt;/strong&gt;&lt;/em&gt; 概念在某些方面与 &lt;em&gt;&lt;strong&gt;coach&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;president&lt;/strong&gt;&lt;/em&gt; 的概念很接近，而在其他方面则与&lt;em&gt;&lt;strong&gt;employee&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;bureaucracy&lt;/strong&gt;&lt;/em&gt; 的概念很接近。将概念理解为存在于复杂几何空间中的点，使我们能够思考和测量概念之间的距离远近（Hannan 等人，2019）。例如，与  &lt;em&gt;&lt;strong&gt;playground&lt;/strong&gt;&lt;/em&gt; 或 &lt;em&gt;&lt;strong&gt;ice cream&lt;/strong&gt;&lt;/em&gt; 相比， &lt;em&gt;&lt;strong&gt;manager&lt;/strong&gt;&lt;/em&gt; 与&lt;em&gt;&lt;strong&gt;organization&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;leader&lt;/strong&gt;&lt;/em&gt; 概念的联系更加紧密。&lt;strong&gt;我们将这种概念相关的多维空间称为概念空间&lt;/strong&gt;（Hannan et al. 2019)&lt;/p&gt;
&lt;p&gt;重要的是我们用复数来指代概念空间。对于许多单词来说，它们会根据使用的上下文表现出不同的概念信息模式。首先，概念可能会根据使用它们的社会背景而有所不同。例如，如果在执行董事会议室、商品交易大厅或附近的储蓄和贷款机构的背景下说出 “&lt;em&gt;&lt;strong&gt;Bank&lt;/strong&gt;&lt;/em&gt;”，指的是银行而不是河流。概念也可能根据使用时间的不同而有所不同。例如，“&lt;em&gt;&lt;strong&gt;高科技&lt;/strong&gt;&lt;/em&gt;” 一词所引发的概念关系会根据我们研究的是 1960 年代、1990 年代还是今天而有所不同。最后，概念关系因使用它们的社区而异，因此 “&lt;em&gt;&lt;strong&gt;债务&lt;/strong&gt;&lt;/em&gt;” 所捕获的概念将根据其是由首席财务官还是低收入个人使用而有所不同。概念所含信息存在多样性， 正如 Hannan等人（2019）指出，“虽然有些概念可能是天生的或生物驱动的，但大多数都是社会构建的。”&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三先前研究中的概念和概念空间&#34;&gt;三、先前研究中的概念和概念空间&lt;/h2&gt;
&lt;p&gt;概念以及扩展的概念空间是人类思维和交流的基础（Sperber 和 Wilson 1986；，Murphy 2002；Hofstadter 和 Sander 2013）。正因为如此，概念和概念空间对于许多组织理论框架来说或多或少是明确和关键的。在某些研究（例如类别研究）中，概念具有核心重要性并且已经被明确地理论化。然而，在其他情况下，（例如，公司基于知识视角）概念被隐含地假定，即使它们是决定许多理论期望的基本成分。鉴于概念无处不在，对组织科学所有领域使用概念信息进行全面回顾超出了本文的范围。我们将简短、非详尽的回顾集中在概念和概念空间概念的三个领域——&lt;strong&gt;类别、知识和文化&lt;/strong&gt;。通过嵌入技术处理并追踪存在于个人和社区头脑中的概念信息，研究其对组织行为和结果的影响。&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;31-类别&#34;&gt;3.1 类别&lt;/h3&gt;
&lt;p&gt;类别是具有共同特征和属性的实体组。如前所述，概念是类别的心理表征。对类别的研究主要集中在跨类别或模糊类别是否会增加或减少分类实体的估值。自Zuckerman（1999）以来的工作一直集中在消除歧义条件上，在这些条件下，类别跨越和模糊性会导致积极或消极的估值。许多研究表明，由于感知偏差（Durand et al. 2007）、不符合受众期望（Hsu 2006)、Hsu et al. 2009；Leung and Sharkey 2014） ，跨越模糊的类别会损害实体估值，或降低分类对比度（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B119&#34;&gt;Negro et al. 2010&lt;/a&gt;）。其他研究表明，跨越类别可以创造积极的估值结果，因为它表明非典型性可以放大良好的表现并缓冲不良表现（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B159&#34;&gt;Smith 2011&lt;/a&gt;），一个类别可以锚定认知，而另一个类别可以有益地修改认知（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B188&#34;&gt;Wry et al. 2014&lt;/a&gt;）。还有其他研究表明，效果取决于受众，有些人喜欢跨类别，而另一些人则不喜欢（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B135&#34;&gt;Pontikes 2012&lt;/a&gt;）。通过这些方式，类别可以通过影响有关类别成员资格的概念信息的解释方式，对行为和绩效产生积极或消极的影响。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn5&#34;&gt;4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;尽管类别范式的贡献历来是通过类别成员的集合和模糊集合理论（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B64&#34;&gt;Hannan et al. 2007&lt;/a&gt;）概念来实现的，但最近的工作开始纳入其多维性（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65&#34;&gt;Hannan et al. 2019&lt;/a&gt;）和类别的分级归属感。组织学者感兴趣的许多现象都是由概念及其代表的类别之间的精确距离支撑的。例如，鉴于专利所贡献的技术领域，专利通常分为类别和子类。然而，专利中编码的想法可以传播到创新空间的广泛领域，即使只分类在一个类别中。正如我们稍后讨论的，转向概念的几何概念，使分析师能够考虑隶属度、重叠和连续距离影响底层实体评估判断的方式&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65&#34;&gt;（Hannan 等人，2019 &lt;/a&gt;。&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-知识&#34;&gt;3.2 知识&lt;/h3&gt;
&lt;p&gt;众所周知，知识很难具体说明，并且在哲学、认知科学和社会科学领域，围绕其概念性质进行了长期而活跃的争论（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B166&#34;&gt;Steup 和 Neta 2020&lt;/a&gt;）。然而，过去几十年来，组织科学在微观、中观和宏观层面上进行了大量研究，解决有关知识及其在团队、组织和经济活动中的作用的问题。从对团队成员专业知识的研究（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B164&#34;&gt;Srikanth et al. 2016&lt;/a&gt;）到公司基于知识和注意力的观点（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B87&#34;&gt;Kogut and Zander 1992&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B56&#34;&gt;Grant 1996&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B123&#34;&gt;Ocasio 1997&lt;/a&gt;）；从交互记忆系统（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B143&#34;&gt;Ren 和 Argote，2011&lt;/a&gt;）到创新流程（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B50&#34;&gt;Garud 等，2013&lt;/a&gt;）；从组织设计（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B45&#34;&gt;Foss et al. 2013&lt;/a&gt;）到搜索和探索（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B93&#34;&gt;Lavie et al. 2010&lt;/a&gt;），知识在最近的组织理论化中发挥着核心作用。&lt;/p&gt;
&lt;p&gt;无论人们对知识的定义如何选择，命题性知识从根本上都与概念信息相关。&lt;em&gt;&lt;strong&gt;命题知识采取“ S [主体]知道p [命题]”&lt;/strong&gt;&lt;/em&gt; 的形式（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B80&#34;&gt;Ichikawa and Steup 2018&lt;/a&gt;）。在某种程度上，命题是由语言中的单词编码的，并且单词代表概念信息，命题知识依赖于概念以及它们如何在概念空间中交织（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B110&#34;&gt;McGrath and Frank 2020&lt;/a&gt;）。以命题“泰勒知道氢的主要工业应用是氨的制造”和“特里知道量子算法可以具有较低的时间复杂度”为例。这些知识命题中的每一个都代表了不同的概念意义，前面提到的领域将以不同的方式操作它们。例如，团队学者可能会强调，由泰勒和特里组成的专利团队将拥有多样化的基础知识。采取基于注意力观点的学者会注意到，泰勒和特里可能会以不同的方式关注知识空间，以应对组织变革。研究创新的人可能会注意到如果泰勒和特里共享办公空间，知识重组的潜力。研究搜索的人可能会假设，为了解决问题，泰勒和特里会以不同的方式搜索概念性解决方案。在所有这些情况下，就这些领域通过诉诸语言编码的命题知识来理论化知识动态而言，它们以基本和可测量的方式参与概念和概念空间。&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;33-文化&#34;&gt;3.3 文化&lt;/h3&gt;
&lt;p&gt;文化被不同地概念化为集体的共同价值观、故事、框架、工具包和类别（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B52&#34;&gt;Geertz 1973&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B131&#34;&gt;Pettigrew 1979&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B92&#34;&gt;Lamont 和 Small 2008&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B158&#34;&gt;Small 等人 2010&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B54&#34;&gt;Giorgi 等人 2015&lt;/a&gt;）。文化建构已成为组织研究的核心，在从个人和团队到组织和国家的各个层面的分析中都得到了运用（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B54&#34;&gt;Giorgi et al. 2015&lt;/a&gt;）。从理解文化如何塑造职业结构（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B195&#34;&gt;Glynn 2000&lt;/a&gt;）、组织领域（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B194&#34;&gt;Anteby 2010&lt;/a&gt;）和创业环境（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B106&#34;&gt;Lounsbury and Glynn 2001&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B141&#34;&gt;Rao and Giorgi 2006&lt;/a&gt;）到它在讲故事（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B106&#34;&gt;Lounsbury and Glynn 2001&lt;/a&gt;）和身份建设中的作用（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B196&#34;&gt;Ravasi 和 Schultz 2006&lt;/a&gt;），从其对人际沟通的塑造（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165&#34;&gt;Srivastava 等人，2018&lt;/a&gt;）到对组织绩效的影响（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26&#34;&gt;Corritore 等人，2020&lt;/a&gt;），文化深深地受到概念及其互动方式的调节。文化以集体认知过程为基础（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B36&#34;&gt;DiMaggio 1997&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B128&#34;&gt;Patterson 2014&lt;/a&gt;），很大程度上可以通过语言痕迹来获取。语言进入文化的窗口（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B55&#34;&gt;Goldberg et al. 2016&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165&#34;&gt;Srivastava et al. 2018&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26&#34;&gt;Corritore et al. 2020&lt;/a&gt;）很大程度上是通过它所表达的概念来呈现的，使得概念和概念空间成为组织文化研究的重要支柱。&lt;/p&gt;
&lt;p&gt;基于它们在形成范畴、知识和文化方面的关键作用，概念和概念空间已成为许多组织理论赖以建立的知识支架的重要组成部分。然而，概念和概念空间通常仅被用作缺乏精确和可扩展的经验表征的不明确的隐喻。这限制了研究使用粗粒度的代理测量或允许手动编码和解释的小数据集。接下来，我们提出词嵌入模型是一种最先进的工具，用于表示概念和概念空间，可以添加到组织学者工具包中。就组织学者寻求将概念和概念信息所支撑的结构操作化而言，他们将得到这类新模型的帮助。考虑到这一点，我们接下来介绍嵌入模型如何工作以及为什么它们可以作为概念和概念空间的有效表示。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四使用词嵌入来表示概念和概念空间&#34;&gt;四、使用词嵌入来表示概念和概念空间&lt;/h2&gt;
&lt;h3 id=&#34;41-越来越多地使用文本作为数据&#34;&gt;4.1 越来越多地使用文本作为数据&lt;/h3&gt;
&lt;p&gt;过去 10 年，通过计算工具和方法进行文本数据分析出现了爆炸性增长。从社会学（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B40&#34;&gt;Evans and Aceves 2016&lt;/a&gt;）到经济学（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B53&#34;&gt;Gentzkow et al. 2019&lt;/a&gt;）和政治学（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B58&#34;&gt;Grimmer and Stewart 2013&lt;/a&gt;），文本正迅速成为组织、经济和社会生活的中心观察站。文本数据提供了在线知识社区、财报电话会议和公司报告、产品评估、组织电子邮件和讨论板、历史档案、视频转录和电影字幕、医疗记录、电子商务、社交媒体等多种领域的丰富思想和行为痕迹。媒体平台、新闻文章、科学学科等等。总而言之，这些文本数据源比以往任何时候都更深入、更广泛地进入组织生活。正如&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B40&#34;&gt;Evans 和 Aceves（2016 年&lt;/a&gt;）指出的那样，文本数据现在使我们能够访问“有关正在玩的社交游戏的隐藏元素及其背后的社交世界”的深层信息。然而，这些语料库的庞大规模及其广泛的范围意味着，提取理论上有意义的信息信号越来越多地受到计算方法的帮助，利用信息技术方法获取大量非结构化文本数据，并将它们转换为有意义且相关的度量。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn6&#34;&gt;5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文本数据与组织学者习惯使用的定量数据之间的一个主要区别是文本是高维的。正如&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B53&#34;&gt;Gentzkow 等人（2019 年&lt;/a&gt;）指出，“仅使用英语中一千个最常用单词的 30 个单词的 Twitter 消息样本 [&amp;hellip;] 的维度大致与宇宙中的原子一样多。” 因此，使用文本作为数据的学者的中心任务是通过对数据施加限制来降低维度。&lt;strong&gt;过去二十年里，组织科学中用于降低这一维度的一些最常用的计算工具是词典法、语义网络和主题模型。尽管这些方法有其优点，但一个主要缺点是它们无法对文本中存在的细粒度概念关系和关联进行编码&lt;/strong&gt; 。接下来，我们将展示嵌入模型如何利用文本中的局部和更广泛的信息来训练概念含义和概念空间的高保真表示。在此过程中，我们展示了词嵌入模型如何克服先前方法来表示文本中编码的含义的一些局限性，从而允许对理论结构进行更细粒度的测量，并实现新的理论可能性。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;42-词嵌入&#34;&gt;4.2 词嵌入&lt;/h3&gt;
&lt;p&gt;我们之前解释过，概念是事物类别的心理表征，人类通过在词典中分配一个单词或短语来表示稳定的概念，并指出，概念只有在与跨多个维度的其他概念相关并为其提供信息时才有意义。密集的概念空间。在这里，我们认为词嵌入模型是最近开发的一类从机器学习应用于自然语言处理的模型，它使我们能够有效且高效地表示概念空间，并将这些空间用于追求组织科学。词嵌入模型是文本语料库中单词的连续表示，可以进行几何解释。&lt;strong&gt;词嵌入的方法论假设，一个词的含义很大程度上是由出现在其直接和更广泛上下文中的词所决定的，这一想法受到结构语言学家的启发，他们已经证明，含义的差异与局部分布相关（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B68&#34;&gt;Harris 1954&lt;/a&gt;）， 这个想法现在被称为 「分布式语义学」，Firth 的著名描述是：“观其伴而知其意”（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B42&#34;&gt;Firth 1957&lt;/a&gt;，you shall know a word by the company it keeps）， 一个单词所代表的概念或含义可以通过它周围的单词的分布来推断&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;以这种分布式方式思考概念和概念空间的底层计算架构可以追溯到 20 世纪 80 年代初期计算机科学家 Geoffrey Hinton 的工作（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B71&#34;&gt;Hinton 1986&lt;/a&gt; , &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B72&#34;&gt;Hinton et al. 1986&lt;/a&gt;）以及认知科学家在这一时期研究的并行分布式处理模型（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B149&#34;&gt;Rumelhart 等人，1986a&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B150&#34;&gt;b&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B109&#34;&gt;McClelland 和 Rumelhart，1989&lt;/a&gt;）。分布式架构是当前嵌入语言模型的基础（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115&#34;&gt;Mikolov et al. 2013b&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130&#34;&gt;Pennington et al. 2014&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35&#34;&gt;Devlin et al. 2019&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104&#34;&gt;Liu et al. 2019&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B17&#34;&gt;Brown et al. 2020&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B41&#34;&gt;Fedus et al. 2020）。 2021&lt;/a&gt;）， 嵌入模型 Word2Vec 算法(&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115&#34;&gt;Mikolov 等 2013b&lt;/a&gt;) 相对简单易用，能够处理中等规模的语料库来。 &lt;strong&gt;Word2Vec 与  GloVe（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130&#34;&gt;Pennington 等人，2014 年&lt;/a&gt;）和 FastText（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B13&#34;&gt;Bojanowski 等人，2017 年&lt;/a&gt;）等嵌入算法，是 ChatGPT 和相关模型的基础&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;找个例子来帮助理解算法， 现在我们要创建过去 50 年创新的概念空间表示。首先需要概念活动领域的文本数据， 美国专利局数据提供了创新活动的踪迹，其中包括所有专利的文本、摘要、描述和权利要求。在整篇论文中，我们使用这个专利摘要语料库来指导读者完成训练这个概念空间和构建相关概念测量的过程。数据是从&lt;a href=&#34;https://patentsview.org/&#34;&gt;Patentsview.org&lt;/a&gt;免费下载的，使用 1976 年至 2019 年间发布的所有专利来构建本文中发现的词嵌入模型和测量相关指标。&lt;/p&gt;
&lt;p&gt;想象一下，专利语料库中的每个独特单词都是从放置在巨大冰箱上的随机放置的 &lt;strong&gt;“word magnet”&lt;/strong&gt; 开始的（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B76&#34;&gt;Hovy 2020&lt;/a&gt;）。当连续词袋 (CBOW) 算法滚动浏览语料库时，使用每个目标词周围的单词词(滑动窗口的上下文)来预测目标词（更多内容见下文）。该算法的最终目标是产生一种语义模型，其中出现在相似上下文中的单词彼此接近，而来自不同上下文的单词则相距很远。由于用2维概念空间不足以捕获每个单词的全部含义，因此该算法改为在更高的（100-1,000）维空间内捕捉语义。通过这种方式，目标单词的概念信息是从它周围的单词中归纳出来的，将语料库中的每个单词绘制为&lt;em&gt;n&lt;/em&gt;维空间中的坐标或向量。正是单词在这个&lt;em&gt;n&lt;/em&gt;维向量空间中的相对位置，使我们能够将词嵌入模型可以描述代表人类概念活动区域的概念空间。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn7&#34;&gt;6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;概念意义的识别假定了嵌入空间的可解释性。接下来，我们提出了对这些概念空间的一系列提示和测量，作为从中产生结构化解释的方法。这很像心理学家使用 &lt;strong&gt;心理测量调查&lt;/strong&gt; 将概念印象转化为可解释的观点（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B112&#34;&gt;Michael Furr 2021&lt;/a&gt;）。或者&lt;strong&gt;认知人类学家如何使用结构化任务，例如排序和排名（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B163&#34;&gt;Spradley 2016&lt;/a&gt;），将概念性的世界观转变为可解释的世界观&lt;/strong&gt;。我们认为嵌入模型必须接受结构化测量（就像向人类受试者提供的心理测量问卷）使他们的 **概念景观(conceptual landscape)**变得可解释。接下来，我们将引导读者如何用专利语料库训练创新概念空间表示的过程。之后， 我们概述了该方法的优点和局限性，并指出这些方法与先前的文本分析方法和组织研究实践的关系。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;43-选择语料库&#34;&gt;4.3 选择语料库&lt;/h3&gt;
&lt;p&gt;学者可以根据应用使用两种词嵌入模型。一方面，研究人员可以使用自有文本语料库来训练表示， 据此了解文本所涉主体(个人、团体、社会)行为的概念空间是什么样子， 以及概念关系揭示人类活动背景。在我们的示例中，专利创新在专利语料库中得到了很好的体现，因此我们在下面展示了如何从头开始训练概念空间表示, 以及它揭示了哪些概念联系。研究人员可以从头开始训练语料库的其他例子包括在线社区（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18&#34;&gt;Burtch et al. 2021&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2&#34;&gt;Aceves et al. 2022&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B23&#34;&gt;Chambers et al. 2022&lt;/a&gt;）、学术学科（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74&#34;&gt;Hofstra et al. 2020&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B102&#34;&gt;Lin et al. 2022&lt;/a&gt;） 、劳动力市场（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B9&#34;&gt;Bana 2022&lt;/a&gt;）、公共记录（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B6&#34;&gt;Arseniev-Koehler et al. 2022&lt;/a&gt;）、产品和公司描述（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61&#34;&gt;Guzman and Li 2023&lt;/a&gt;）以及财报电话会议和公开演讲（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B85&#34;&gt;Kirgil and Voyer 2022&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;或者，如果研究人员想要在较小的语料库中追踪概念动态，而该语料库的大小不足以训练独特的、特定于上下文的嵌入，那么研究者可以使用预训练嵌入模型，需要注意，训练预训练嵌入模型的文本与研究者小语料库在内容、场景要有相似性。广泛使用的预训练嵌入已经在来自海量语料库的文本上进行了训练，例如新闻（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B4&#34;&gt;Google 2013&lt;/a&gt;）、维基百科（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35&#34;&gt;Devlin et al. 2019&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B57&#34;&gt;Grave et al. 2018&lt;/a&gt;）。训练这些预训练嵌入模型的文本语料体量很大， 内容题材往往包含我们较小文本样本中存在的概念。因此使用预训练嵌入对这些概念的信息进行编码，并可用于近似相关距离。政治和历史语义背景下的研究发现，预训练嵌入提供的结果与特定于上下文的嵌入相当（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;Kozlowski et al. 2019&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145&#34;&gt;Rodriguez and Spirling 2022&lt;/a&gt;）。如果有理由相信研究项目中包含的概念和想法没有在这些大量预训练嵌入中得到很好的体现，研究人员可以使用较小语料库中的文本对其进行 &lt;strong&gt;微调（Fine-Tune）&lt;/strong&gt;（ &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104&#34;&gt;Liu et al. 2019，Burtch et al.2019&lt;/a&gt;）&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18&#34;&gt;， 2021&lt;/a&gt;）。微调将预训练的概念空间扭曲为与样本一致（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104&#34;&gt;Liu et al. 2019&lt;/a&gt;），更好地反映概念之间的关系。&lt;/p&gt;
&lt;p&gt;最后，使用哪一种嵌入(自己训练的嵌入、 预训练的嵌入、微调的嵌入)将取决于研究人员的目的以及他们寻求追踪的概念动态的类型。接下来，我们将重点描述从头开始训练和验证嵌入模型的过程。在接下来的部分中，我们讨论不同参数设置和策略之间的权衡，并鼓励读者遵循文章文本和在线附录。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;44-清理语料库&#34;&gt;4.4 清理语料库&lt;/h3&gt;
&lt;p&gt;训练嵌入模型的第一步是使用 Python 等编程语言录入文本语料库， 首先获取每个专利摘要中的文本， 并将连续的文本进行切词，转化为单词列表 。然后，我们将文本小写，删除标点符号和数字字符串，并将每个摘要转换为称为token的单词列表。但是这可能破坏一些词组语义，这里使用 &lt;em&gt;&lt;strong&gt;bi-gram&lt;/strong&gt;&lt;/em&gt;， 识别高频共现的词组成词组，例如当 &lt;em&gt;&lt;strong&gt;“electric”&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;“vehicle”&lt;/strong&gt;&lt;/em&gt; 这两个词在某些上下文中一起出现时，它们将被统一形成短语和概念 &lt;em&gt;&lt;strong&gt;“electric_vehicle”&lt;/strong&gt;&lt;/em&gt; 。建立单词或短语列表后，执行单词嵌入算法来学习单词或二元组及其语言上下文之间的最佳距离，以保留语言中单词和短语的概念空间。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;45-训练嵌入模型&#34;&gt;4.5 训练嵌入模型&lt;/h3&gt;
&lt;p&gt;第一步是选择词嵌入算法， 浅层神经网络构建的单词表示（例如，Word2Vec、FastText；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115&#34;&gt;Mikolov 等人，2013b&lt;/a&gt;）、共现矩阵的低秩近似（GloVe；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130&#34;&gt;Pennington 等人，2014&lt;/a&gt;） ，或来自 Transformer 的深度上下文嵌入（例如 BERT、&lt;em&gt;GPT&lt;/em&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35&#34;&gt;Devlin 等人 2019&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B139&#34;&gt;Radford 等人 2022&lt;/a&gt;）。这些不同算法输出，都可以被解释为&lt;em&gt;n&lt;/em&gt;维概念空间，其中单词或短语由空间内的向量位置表示。本文我们只介绍 Word2Vec 算法， word2vec 是一种广泛使用的训练概念空间的算法（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B113&#34;&gt;Mikolov 等人，2013a&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115&#34;&gt;b&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;Word2Vec 算法的一种流行实现算法是连续词袋 (CBOW) 算法，可以在 Gensim python 库中轻松访问，该算法使用目标单词的语言上下文来预测被扣掉的目标词 (可以简单的理解为让机器做完形填空题) ， 比较适合小规模数据集。 Word2Vec 还实现了另一种 Skip-Gram 算法，该算法通过从目标单词预测上下文单词来反转 CBOW 的预测任务，比较适合大规模数据集。相比之下，skip-gram 将每个上下文目标对（例如，T：“房子”，C：“宽敞”）视为单独的观察，因此可以更好地捕获精确的语义，但需要更大的语料库才能获得卓越的性能。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;46-维数&#34;&gt;4.6 维数&lt;/h3&gt;
&lt;p&gt;考虑维数很有必要。朴素的模型可以将不重复总词数作为维度， 例如包含 100,000 个不重复单词的语料库， 任何单词都需要  100,000 维才能准确表示。然而，当单词从上下文中被识别为相似时，可以一定范围内减少维度数。&lt;strong&gt;维度过多会导致内存需求和冗余增加，并降低可解释性；维度太少会扭曲距离并且无法解释语言的不及物性&lt;/strong&gt;。通过这种方式，通过具有至少足够的维度来捕获所讨论的复杂语义关系，可以获得准确的预测。&lt;/p&gt;
&lt;p&gt;在实践中，300 维已经成为一个标准，很大程度上源于最初的 Word2Vec 论文之后的惯例，该论文通过交叉验证确定了最佳维数，以减少预测屏蔽词任务中的错误。大多数后续分析都是建立在较小、多样性较低的文本集合上，需要较少的维度，因此 300 通常被用作上限。最近的工作表明，应根据语料库统计数据选择维度 - 语料库词汇表中成对等距单词的数量提供了维度数量的下限，低于此界限通常会导致单词嵌入质量下降（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B127&#34;&gt;帕特尔和巴塔查亚 2017&lt;/a&gt;）。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74&#34;&gt;霍夫斯特拉等人。(2020)&lt;/a&gt;使用 100、200 和 300 维的模型找到了稳健的结果。&lt;/p&gt;
&lt;p&gt;如果分析师寻求实现维度可解释性，他们必须以最小失真来确定表示数据所需的维度数。 但这最后一步一半很少执行，因为维度的优化需要大量的时间和计算资源。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;47-窗口尺寸&#34;&gt;4.7 窗口尺寸&lt;/h3&gt;
&lt;p&gt;回想一下，窗口大小是指算法将用来焦点目标词（或其邻居）之前和之后的单词数量。该窗口最小可以是 1。对于较小的窗口，算法将倾向于对句法关系进行编码（例如，名词后跟动词）。&lt;strong&gt;随着窗口大小的增加，更多的含义和语义被编码到模型输出中&lt;/strong&gt;。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145&#34;&gt;考虑Rodriguez 和 Spirling (2022)&lt;/a&gt;的示例，其中包含两个句子的语料库：(1)“狮子吃肉”和 (2)“牛吃草”。当窗口大小为一时，我们会知道牛和狮子都吃东西，从这个意义上说，牛和狮子在语法上是等价的，因为我们没有足够的信息来区分两者。然而，随着窗口的增加，算法开始对牛与狮子的含义进行更多编码。&lt;strong&gt;与维度数量一样，这里的回报也递减，窗口大于五个字的模型性能略有改善&lt;/strong&gt;（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145&#34;&gt;Rodriguez 和 Spirling 2022&lt;/a&gt;）。 &lt;strong&gt;BERT 和 GPT 系列等上下文模型具有更大的窗口，这些窗口通过注意力过程进行驯服，算法通过该过程识别哪些上下文单词对于解释焦点单词的含义很重要&lt;/strong&gt;（Vaswani 等人，2017 年&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B176&#34;&gt;）&lt;/a&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;48-验证模型&#34;&gt;4.8 验证模型&lt;/h3&gt;
&lt;p&gt;最后一步是验证词嵌入模型，这样做是为了确认算法学习的表示与文本数据所承载的真实人类活动的概念空间表示尽可能相近。论文附录第 2 节描述了关于专利嵌入的七个详细验证程序，表明该模型有效地学习了创新空间的表示。这些包括（1）邻近嵌入词的语义相似性；(2)具有嵌入距离的语义梯度；(3)嵌入簇与语义域之间的对应关系；（4）物理世界距离与嵌入之间的相关性；(5) 社会距离与嵌入之间的相关性；(6) 嵌入空间类比推理的准确性；(7)嵌入文档的语义一致性。我们还讨论了第八个“额外”测试，即图灵测试（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B174&#34;&gt;Turing 1950&lt;/a&gt;）。由 Transformer 支持的现代上下文嵌入的评估标准是它们是否能够与人类毫无区别地参与任何分类、关联、意义生成或集成任务，包括普通对话和专家教程。OpenAI 的 ChatGPT 和许多竞争的聊天机器人已经展示了如此强大的性能，以至于图灵测试正在迅速从上限转变为基线基准。这些验证步骤与论文最后部分的测量相结合，作为嵌入模型的有用提示prompt和测量，使研究人员能够对其编码的概念空间提供结构化解释。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;49-词嵌入方法的优点和缺点&#34;&gt;4.9 词嵌入方法的优点和缺点&lt;/h3&gt;
&lt;h4 id=&#34;491--无需正式指定相关尺寸&#34;&gt;4.9.1  无需正式指定相关尺寸&lt;/h4&gt;
&lt;p&gt;对概念建模的正式尝试试图通过逻辑演绎方法清楚地枚举概念的相关维度（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B47&#34;&gt;Gärdenfors 2004&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B48&#34;&gt;Gardenfors 2014&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65&#34;&gt;Hannan 等人 2019&lt;/a&gt;）。尽管这种方法对于理解限定领域内的概念很有用，但即使如此，它也可能不切实际且难以衡量，因为很难先验地陈述分析师应预期的相关维度&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B73&#34;&gt;（Hofstadter 和 Sander 2013 ）&lt;/a&gt;。 &lt;strong&gt;词嵌入的优点在于，概念之间的关系以及对任何给定概念重要的相关维度可以从语言的使用方式中推断出来，因此不需要事前指定&lt;/strong&gt;。鉴于在分析之前没有必要陈述相关维度，即使是最复杂的组织行为剧场也变得易于分析处理。正如其他人所指出的，“词嵌入为语言中包含的多个维度的含义提供了全面且有意义的见解，这是以前的方法无法捕获的”（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105&#34;&gt;Lix 等人，2022 年&lt;/a&gt;，第 8434 页）。在某种程度上，这种优势源于这样一个事实：神经网络架构能高效地记录意义的维度。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;492-更大的有效维度&#34;&gt;4.9.2 更大的有效维度。&lt;/h4&gt;
&lt;p&gt;嵌入通常由 100 到 1,000 个密集编码维度表示。&lt;strong&gt;编码的密度意味着每个词向量在所有建模维度上都有一个非零坐标&lt;/strong&gt;。正如附录中所指出的，主题模型可能具有相同数量的主题（例如，100-1,000），但这些主题被稀疏编码以方便人类解释，使得主题仅具有一些基本上非零的单词加载，并且文档仅具有少量非零的主题负载。&lt;strong&gt;因此，主题模型是为了描述而构建的，但代价是迫使其表示的有效维度从数百个减少到几个，从而扭曲了本来可以在主题空间内计算的距离。相比主体模型， 词嵌入使用密集编码，每维度的嵌入很难理解和描述，但距离具有更大的自由度，可以更精确地编码含义&lt;/strong&gt;。通过这种方式，相对于低维理论和测量，嵌入为分析师提供了“大量潜在轴，个人和社会群体可以沿着这些轴竞争、合作、分裂或合并”（Kozlowski et al. 2019，p.27 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;）&lt;/a&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;493-无监督训练&#34;&gt;4.9.3 无监督训练。&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;词嵌入还有一个特殊优点，即训练模型时， 以看似无监督或自监督的方式进行，从而避免了手动编码文本语义内容的繁琐，完全由机器学习&lt;/strong&gt;。在我们的创新示例中，向量空间由我们专利语料库中的每个发明人按照他们所写句子的数量和长度的比例进行监督。每个单词的滑动窗口都是为了向专利审查员和未来的发明者传达一种含义而构建的，该算法用于构建向量空间并以概念上适当的方式定位单词。因此，学者们可以利用专利语料库来训练 &lt;strong&gt;技术创新&lt;/strong&gt; 的概念空间，利用财报电话会议记录和新闻稿来训练 &lt;strong&gt;上市公司沟通&lt;/strong&gt; 的概念空间，利用分析师报告来训练 &lt;strong&gt;投资分析&lt;/strong&gt; 的概念空间，或者特定领域的概念空间。使用内部通信（例如 Slack 和电子邮件）来了解公司的知识。这些概念空间可以在最少的监督下进行训练，因此很快成为有价值的观察站，用于追踪组织科学家关注的组织生活的静态和动态（Hofstra et al. 2020，Whalen et al. 2020，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74&#34;&gt;Burtch&lt;/a&gt; et &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184&#34;&gt;al&lt;/a&gt; . &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18&#34;&gt;2021&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B177&#34;&gt;Waller 和 Anderson 2021&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2&#34;&gt;Aceves 等人 2022&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20&#34;&gt;Carlson 2022&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B23&#34;&gt;Chambers 等人 2022&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61&#34;&gt;Guzman 和 Li 2023&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94&#34;&gt;Lawson 等人 2022&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105&#34;&gt;Lix 等人 2022&lt;/a&gt;）。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;494-共现是不必要的&#34;&gt;4.9.4 共现是不必要的。&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;这些模型的另一个优点是，两个概念不必在任何文档中同时出现，就可以将它们编码为相似的向量&lt;/strong&gt;。所需要的只是它们与相似的概念同时出现。例如，我们可以先验地指出 &lt;em&gt;&lt;strong&gt;医生&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;律师&lt;/strong&gt;&lt;/em&gt; 在某些方面非常相似（例如，他们需要高级学位，具有高收入水平等），但他们可能永远不会同时出现在语料库的同一文档中。尽管彼此之间缺乏共现性，但它们很可能都独立地与高收入*、&lt;em&gt;高学历&lt;/em&gt;、*白领等概念同时出现，从而最终拥有编码这些相似性的接近向量。&lt;strong&gt;因此，嵌入模型的底层计算架构可以更好地近似社会和文化含义，而无需求助于严格的共现&lt;/strong&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;495-上下文相关的含义结构&#34;&gt;4.9.5 上下文相关的含义结构。&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;使用定制训练的嵌入模型的一个优点是它将捕获上下文相关的含义结构&lt;/strong&gt;。例如，&lt;em&gt;&lt;strong&gt;“甜”&lt;/strong&gt;&lt;/em&gt; 的含义在软件团队的背景下与 &lt;em&gt;&lt;strong&gt;烹饪&lt;/strong&gt;&lt;/em&gt; 的背景下会有所不同。正如&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105&#34;&gt;Lix 等人。（2022）&lt;/a&gt;指出，在软件团队的背景下，与 &lt;em&gt;&lt;strong&gt;“甜蜜”&lt;/strong&gt;&lt;/em&gt; 最接近的术语是 &lt;em&gt;&lt;strong&gt;“强烈”&lt;/strong&gt;&lt;/em&gt;、 &lt;em&gt;&lt;strong&gt;“兴奋”&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;“耶”&lt;/strong&gt;&lt;/em&gt;。此外，就同一个单词编码不同概念（一词多义）而言，单词每种含义的概念信息都位于单词嵌入内的线性叠加（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B5&#34;&gt;Arora et al. 2018&lt;/a&gt;）。这意味着编码诸如 &lt;em&gt;&lt;strong&gt;“Bank”&lt;/strong&gt;&lt;/em&gt; 之类的单词的&lt;em&gt;n&lt;/em&gt;维向量包含其代表的所有概念的概念信息，例如 &lt;em&gt;&lt;strong&gt;河边&lt;/strong&gt;&lt;/em&gt; 或 &lt;em&gt;&lt;strong&gt;金融机构&lt;/strong&gt;&lt;/em&gt;。通过这种方式，即使在多义词的情况下，单词的上下文相关含义也会被编码到模型中。当这些上下文相关的含义不仅不同，而且是排他的或相反的时，来自转换器的上下文相关嵌入可以为上下文中的每个单词呈现不同的单词向量。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;496-几何有助于概念人群体和组织的细粒度表示&#34;&gt;4.9.6 几何有助于概念、人、群体和组织的细粒度表示。&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;我们认为，词嵌入模型可以在训练的语料库范围内产生人类活动概念空间的细粒度表示&lt;/strong&gt;。&lt;strong&gt;这意味着，从概念空间内编码的信息中，我们可以恢复个人、群体和组织本身的细粒度表示&lt;/strong&gt;。以我们的创新案例为例，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F1&#34;&gt;图 1&lt;/a&gt;描述了在说明性二维空间中这是如何实现的。学习到的概念空间将由单词或短语w表示的概念作为其最原子的分析级别。我们的限制性示例显示了在2维空间中排列的九个单词。单词 1-3 由发明人 1 使用，单词 4-6 由发明人 2 使用，单词 7-9 由发明人 3 使用。&lt;strong&gt;通过获取每个人的单词向量的质心向量，我们可以得出每个发明人在创新的概念空间&lt;/strong&gt;。&lt;strong&gt;将这个过程提升到团队和组织级别，我们可以在发明人团队和组织的概念空间内得出独特的向量&lt;/strong&gt;。因此，词嵌入架构不仅在概念的最原子级别上是细粒度的，而且还可以在更聚合级别上提供细粒度的表示。相对于团队多样性、组织差异化和注意力等结构的粗粒度代理，这形成了显着的测量改进，这些结构在嵌入特定概念空间时是有意义的。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/figure-1.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;strong&gt;图 1.嵌入作为概念、人员、群体和组织的细粒度表示&lt;/strong&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;497-细粒度几何减少了上下文信息的丢失&#34;&gt;4.9.7 细粒度几何减少了上下文信息的丢失。&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;由于粗糙、粗粒度的代理指标无法承载相关信息，在实证分析和相关理论构建中就无法利用这些信息&lt;/strong&gt;。嵌入模型的优势在于其独特的信息表征，可以携带更多的信息，信息的粒度更小，保存的信息量更多。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F2&#34;&gt;图 2&lt;/a&gt;使用团队多样性的示例来说明如何实现这一点。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F2&#34;&gt;图 2(a)&lt;/a&gt;显示了两个团队，1 和 2，每个团队要么通过熵（一种标准的、集合论多样性的理论度量（顶行））来表示，要么通过概念广度（基于底层概念的细粒度度量）来表示。团队调动的信息（底行）。团队 1 和团队 2 都有四名成员，团队 1 由两名生物化学家、一名化学家和一名分析化学家组成，团队 2 由两名生物化学家、一名海洋学家和一名计算机科学家组成。&lt;strong&gt;由于两个团队的团队成员类型比例相同，因此它们都被编码为具有相同的团队多样性熵度量 1.5&lt;/strong&gt;。**然而，当考虑团队成员的概念信息时，我们发现它们是本质上不同类型的团队，团队 1 的多样性或概念范围远不如团队 2 **。这表明粗粒度的测量可能会留下未开发的有价值的上下文信息（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B187&#34;&gt;Wolpert et al. 2014&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B33&#34;&gt;DeDeo 2017&lt;/a&gt;）。因此，我们应该看到更细粒度的衡量标准与相关的、理论上的绩效结果之间的联系更加紧密和一致。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/figure-2.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;strong&gt;图 2.（在线彩色)细粒度表示可防止有价值的信息丢失&lt;/strong&gt;&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;专利数据集使我们能够通过三种构建的措施来说明这一主张。首先，集合论团队多样性度量，使用团队先前专利在专利主要类别中的分布（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B79&#34;&gt;Huo 等人，2019&lt;/a&gt;）。第二种替代措施使用专利子类，以便它们提供相对于第一种更细粒度的措施。第三个衡量标准依赖于团队成员先前专利在创新概念空间内的&lt;strong&gt;概念广度&lt;/strong&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;498-词嵌入的局限&#34;&gt;4.9.8 词嵌入的局限。&lt;/h4&gt;
&lt;p&gt;到目前为止，我们的注意力仅限于讨论嵌入模型的结构，描述它们与概念空间的关系，并注意到它们的优点。在这里我们将说明其局限性，讨论它们的严重性、改善方式，以及何时不要用词嵌入的意外情况。我们讨论三类限制。第一个源于神经网络模型一般复杂的“黑匣子”性质，以及这带来的具体挑战，涉及输入数据的偏差，以及模型正确推理的范围，特别是那些对超出分析师背景的数据进行预训练的模型。第二个与这些模型的大小以及训练它们所需的数据量有关。第三个问题涉及词嵌入模型的具体局限性以及从脱离韵律和表达上下文的文本数据中分析含义的挑战。&lt;/p&gt;
&lt;p&gt;许多学者首先担心的是，多级神经网络模型显得复杂且在统计上难以理解，&lt;strong&gt;经常被批评为“黑匣子”方法&lt;/strong&gt;，无法“打开”以询问其性能背后的机制（Knight 2017，Leavitt et &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B86&#34;&gt;al&lt;/a&gt; . &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B95&#34;&gt;2021&lt;/a&gt;） 。现代神经网络词嵌入模型通常作为自监督模型实现，该模型启发式搜索单词之间的依赖关系空间以预测屏蔽词的身份。&lt;strong&gt;自从第一个高性能嵌入发布（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115&#34;&gt;Mikolov 等人，2013b&lt;/a&gt;）以来，对其黑盒性质的一些担忧已经减弱，因为数学家发现最流行的“浅”词嵌入模型（如 Word2Vec 和 FastText）获得了很大的优势&lt;/strong&gt;。其强大功能来自于近似易于理解的矩阵分解方法的运算，例如因子分析、主成分分析和对应分析（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B100&#34;&gt;Levy 和 Goldberg 2014&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;“黑盒”输入输出方法带来的一个相关潜在限制是，&lt;strong&gt;输入的偏差将转化为输出中的偏差&lt;/strong&gt;——用于训练嵌入的语料库的偏差将被编码在生成的单词嵌入模型中（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B14&#34;&gt;Bolukbasi等人，2016&lt;/a&gt;）。当模型用于现实世界的下游应用程序（例如推荐服务）时，这可能是有害的。例如，硬编码到嵌入中的种族和性别刻板印象可能会导致有偏见的建议（例如，评估是否适合招聘职位或预测财务违约的可能性），并导致不公平和不道德的决定（例如，拒绝工作或信贷） 。学者们应该根据他们的研究问题和设计，主动考虑这种负外部性是否可能，并在对人类造成伤害的可能性足够高时，偶然放弃嵌入。&lt;strong&gt;然而，在某种程度上，理解社区和研究背景中概念关联的本质是核心，研究人员将需要这些偏见进行分析。如果不包括它们，模型以及研究设计就会错过表征其研究背景的关键社会和文化规律。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果分析人员对生成语料库的上下文没有清晰的了解，就会出现另一个相关的限制，这样他们最终可能会做出不适用和不相关的推论&lt;/strong&gt;。例如，强调意义随时间变化的研究（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19&#34;&gt;Caliskan et al. 2017&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49&#34;&gt;Garg et al. 2018&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;Kozlowski et al. 2019&lt;/a&gt;）的特点是词义表现出来自外源冲击的间断变化，从而重新配置了概念关联的结构。穿过空间。想一想 2005 年卡特里娜飓风之后“卡特里娜”的含义发生了怎样的变化。2009 年金融危机之后，金融术语的含义发生了重新配置，部分原因是添加了“问题资产救助计划”等许多新术语。忽略外源冲击可能会导致对后面和验证部分中描述的措施的错误解释，将其视为仅由进化产生的结果，从而导致错误的推论。这是一个特别成问题的问题，因为许多最准确的词嵌入模型都是在从网络上提取的大量文本语料库上进行预训练的。此类模型可用于引导非常小的文本数据之间的有意义距离，这是一项常见任务，但&lt;strong&gt;如果预训练数据是异构的，则距离可能无法反映焦点文本的概念世界&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;接下来的两个限制必然是其嵌入优势的另一面。词嵌入模型产生的细粒度信息会带来特定研究可能或可能无法维持的成本。首先是模型尺寸。&lt;strong&gt;每个单词的数百个维度的细粒度信息或上下文嵌入需要比简单的字典计数或潜在狄利克雷分配主题模型更大的存储空间&lt;/strong&gt;。这与通常用于将数据维度减少到两个或三个的因子和主成分分析形成鲜明对比。词嵌入模型使用更多维度（通常为 200-500）来更准确地预测数据的屏蔽部分。尽管如此，当前个人计算机的计算能力和存储能力现在允许训练合理大小的嵌入。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;与此相关的是，词嵌入模型需要比先前模型更多的文本才能稳健地估计概念空间&lt;/strong&gt;。当大型语料库与研究主题相似并且可以用作理论相关文档或微调过程的初始化的代理时，可以通过迁移学习来弥补这一挑战。&lt;strong&gt;然而，有时相关语言在内容、目的或形式上与模型预训练的数据有很大不同，它需要独立建模，但又足够小，无法维持对嵌入模型的稳健估计。在这种情况下，使用字典计数或主题模型可能会更好，因为数据只能维持粗粒度的关联，而这些方法旨在捕获粗粒度的关联。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;最后一类通常涉及词嵌入和文本方法的特殊限制。首先，静态词嵌入本身并不处理一词多义，即一个词（例如 &lt;em&gt;&lt;strong&gt;“bank”&lt;/strong&gt;&lt;/em&gt; ）编码多个概念（例如金融机构、河边、侧向倾斜）的情况。尽管多义词的存在可能会影响后续一些指标的测量，但也存在抵消的力量。一方面，研究发现多义词的含义以相互线性叠加的方式编码在单词向量内（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B5&#34;&gt;Arora et al. 2018&lt;/a&gt;）。这意味着该算法通过同时考虑单词的所有含义来对单词在概念空间中的位置进行编码，从而克服了原本可能存在的严重缺陷。另一方面，上下文嵌入架构（在线附录中有更详细的描述）通过根据焦点词周围的上下文输出不同的向量来明确解决多义词的问题。每个单词不是单个向量，而是根据用途而变化的向量云。如果分析师怀疑一词多义可能是特定分析的严重问题，他们可以偶然使用上下文嵌入并规避这种担忧。&lt;/p&gt;
&lt;p&gt;最后一个潜在的限制是文本方法的一般特征。只要文本数据是转录语音话语的产物（例如，欧洲央行或美联储主席演讲、政治演讲、财报电话会议、电视或电影文字记录、对话互动），语音的语调、语气和音色将没有纳入到嵌入表示中。考虑到。&lt;strong&gt;鉴于某些语言（例如中文）更严重地依赖语调来传达含义，这可能或多或少存在问题，具体取决于话语发生的社会背景及其表达语言&lt;/strong&gt;。因此，在语调和语气在语料库中发挥重要作用的情况下，学者们应该讨论他们的嵌入模型选择和解释决策的后果。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;410-在研究中使用词嵌入模型的路线图&#34;&gt;4.10 在研究中使用词嵌入模型的路线图&lt;/h3&gt;
&lt;p&gt;现在我们大脑对词嵌入模型是什么、如何表示概念空间、如何训练、优点和局限性有了框架性的认知，接下来可以将它们整合到研究和理论构建的标准方法中。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#T1&#34;&gt;表 1&lt;/a&gt;列出了如何将嵌入模型集成到科学流程中的路线图。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;步骤 1-3 是研究过程中的标准步骤，包括确定一个可行且有趣的研究问题，通过在适当的实证背景下进行评估，为重要的理论问题提供信息（Weick 1989 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B179&#34;&gt;）&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;步骤 4-9 总结了本文到目前为止对嵌入模型的讨论。&lt;/li&gt;
&lt;li&gt;步骤 10 和 11 ，与下一节指标度量有关，通过标准定量和定性方法调动该度量。&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;表 1.在研究中使用词嵌入模型的路线图&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;步骤&lt;/th&gt;
&lt;th&gt;活动&lt;/th&gt;
&lt;th&gt;基本原理&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1. &lt;strong&gt;确定研究问题&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;如果研究问题至关重要，请确定文本数据是否有助于在理论研究上有帮助。&lt;/td&gt;
&lt;td&gt;吸引研究人员把注意力聚焦在理论问题、词嵌入构建研究构念回答问题的交叉点。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2. &lt;strong&gt;理论建立及相关理论构建&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;确定使用哪种理论框架来解决研究问题以及通过嵌入模型来操作哪种理论构念。&lt;/td&gt;
&lt;td&gt;理论构念与其词嵌入指标(构念的衡量）之间的紧密联系能够实现累积的理论发展。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3. &lt;strong&gt;定义经验背景&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;选择适当的实证背景，在其中回答研究问题并动员理论框架和构念。&lt;/td&gt;
&lt;td&gt;确保研究问题、理论框架和用构念以逻辑方式相互加强。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4.&lt;strong&gt;指定将用于表示经验背景的概念空间的文本数据&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;描述将用于训练词嵌入模型和测量感兴趣的理论构念的文本数据的范围。 数据是否有效地涵盖了您想要得出理论结论的经验背景下的行为活动范围？&lt;/td&gt;
&lt;td&gt;确保用于计算理论构造度量的词嵌入模型在逻辑上映射到并有效地代表所提出的理论框架内的实证研究背景。 文本数据的范围应该在逻辑上映射到所讲述的理论故事的范围。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.&lt;strong&gt;确定文本数据的大小和范围&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;数据是否足够大以学习相关概念空间的准确表示？&lt;/td&gt;
&lt;td&gt;文本数据的大小将决定是否应该训练自定义嵌入，或者是否应该使用可用数据对现成的嵌入进行微调。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6. &lt;strong&gt;给定数据大小，要么训练独特的词嵌入模型，要么微调现有模型&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;如果文本数据足够大，则训练自定义嵌入来表示感兴趣的经验上下文的概念空间。 如果文本数据不够大，请使用这些数据来微调现有的现成嵌入模型。&lt;/td&gt;
&lt;td&gt;确保用于测量理论结构的嵌入模型能够有效地表示经验背景的相关概念空间。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7. &lt;strong&gt;如果训练独特的模型，请选择一种算法&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;在连续词袋 (CBOW) 或 Skip-Gram 模型之间进行选择。&lt;/td&gt;
&lt;td&gt;CBOW：在较小的数据集上可以有更好的性能。 &lt;br&gt;Skip-gram：可以更好地捕获语义。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8. &lt;strong&gt;如果训练独特的模型，确定相关参数&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;选择窗口大小和维数。&lt;/td&gt;
&lt;td&gt;窗口大小：标准做法是 5。较小的窗口可以更大程度地捕获语法，较大的窗口可以更大程度地捕获语义，但收益递减并增加计算成本。 维度数：标准做法是 300，超过此点后性能回报递减。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9. &lt;strong&gt;验证词嵌入模型&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;请遵循在线附录中的验证程序。&lt;/td&gt;
&lt;td&gt;确认嵌入模型准确有效地表示了经验背景的概念空间。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10. &lt;strong&gt;计算相关度量&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;通过确定将用于实施感兴趣的理论构念的相关概念集，创建“实际措施和应用”部分中的措施之一。&lt;/td&gt;
&lt;td&gt;使学者能够将该测量用于定量或定性分析。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11. &lt;strong&gt;在标准定性或定量方法中使用计算的度量&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;对于定量分析，该度量要么成为自变量，要么成为因变量。 对于定性分析，学者可以提供解释性分析，因为它们可能适用于其他类型的档案、民族志或视听数据。&lt;/td&gt;
&lt;td&gt;嵌入模型表示对生成数据的社会背景的概念空间的描述。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五实际措施与应用&#34;&gt;五、实际措施与应用&lt;/h2&gt;
&lt;p&gt;现在已经正式定义了 &lt;strong&gt;概念&lt;/strong&gt; 和 &lt;strong&gt;概念空间&lt;/strong&gt; 的含义，并说明了先前的文献如何处理概念信息,  介绍了嵌入模型表示能力的底层逻辑，并在在线附录中完成了支持这种直觉的几个验证步骤。也评论了嵌入模型给概念信息分析带来的几个优点和相关缺点。&lt;/p&gt;
&lt;p&gt;在本章中，我们将介绍一些新研究， 学习他们如何用嵌入生成独特指标。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#T2&#34;&gt;表 2&lt;/a&gt;总结了这些指标及示例应用。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;表 2.词嵌入测量和示例应用&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;措施&lt;/th&gt;
&lt;th&gt;研究性学习&lt;/th&gt;
&lt;th&gt;关键构念&lt;/th&gt;
&lt;th&gt;研究问题&lt;/th&gt;
&lt;th&gt;代表性调查结果&lt;/th&gt;
&lt;th&gt;嵌入在这种情况下的优点&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1. &lt;strong&gt;概念广度&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105&#34;&gt;利克斯等人。(2022)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;话语多样性——在一组给定的互动中，群体成员所传达的含义彼此分歧的程度。&lt;/td&gt;
&lt;td&gt;一个群体的话语多样性如何影响其绩效？&lt;/td&gt;
&lt;td&gt;高绩效团队会调整他们的共享认知以匹配任务的要求（例如，构思与协调）。&lt;/td&gt;
&lt;td&gt;能够随着时间的推移以细粒度的细节和动态地追踪小组对话的概念广度，使学者们能够追踪话语多样性的新理论构造。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.&lt;strong&gt;概念距离和相似度&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74&#34;&gt;霍夫斯特拉等人。(2020)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;语义遥远的科学新颖性：博士论文中新链接概念的语义距离。&lt;/td&gt;
&lt;td&gt;代表性不足的群体是否更有可能产生科学创新？&lt;/td&gt;
&lt;td&gt;相对于男性，女性引入了更遥远的新奇事物。 然而，这种语义上遥远的新颖性在该学科中很少受到关注。&lt;/td&gt;
&lt;td&gt;能够追踪新概念组合的概念距离，使学者不仅可以研究是否做出了新组合，还可以研究这些组合的语义距离最终如何影响其影响。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3.&lt;strong&gt;概念X性&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94&#34;&gt;劳森等人。(2022)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;性别刻板印象：男性（而非女性）与以成就为导向的代理特征（例如自信和果断）相关的程度。&lt;/td&gt;
&lt;td&gt;雇用女性首席执行官和董事会成员是否与组织对代理语言的性别使用发生变化有关？&lt;/td&gt;
&lt;td&gt;当组织雇用女性首席执行官和董事会成员时，女性的语义与代理的语义变得更加一致。&lt;/td&gt;
&lt;td&gt;对 22 家标准普尔 500 强公司的 43,000 多份文件（包含超过 12 亿字）进行分析，深入细致地研究女性的含义如何因聘用女性领导者而发生变化。否则这样的分析是不可能的。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;4.概念意义&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63&#34;&gt;汉密尔顿等人。(2016)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;词语的文化意义：词语的含义随时间变化的程度。&lt;/td&gt;
&lt;td&gt;语义演化的可能驱动因素是什么？&lt;/td&gt;
&lt;td&gt;跨历史时期的语义变化率与词频的逆幂律成正比。 与频率无关，具有更多含义的单词具有更高的语义变化率。&lt;/td&gt;
&lt;td&gt;能够探索跨多个知识和文化领域的大型历史时期和大量文本中的语义变化。例如，他们可以详细追踪同性恋这个词的含义如何从&lt;em&gt;快乐&lt;/em&gt;和&lt;em&gt;艳丽&lt;/em&gt;等概念转向&lt;em&gt;同性恋&lt;/em&gt;和&lt;em&gt;女同性恋&lt;/em&gt;等概念。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5. &lt;strong&gt;文化和知识连续体中的概念立场&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;科兹洛夫斯基等人。(2019)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;社会阶层标记：区分社会阶层维度的概念。&lt;/td&gt;
&lt;td&gt;20世纪社会阶级的标志是如何变化的？&lt;/td&gt;
&lt;td&gt;尽管社会阶级维度在历史上保持稳定，但阶级文化标记在每个维度中的定位方式却不断发生变化（例如，员工从士兵和肌肉等概念转变&lt;em&gt;为&lt;/em&gt;白领&lt;em&gt;和&lt;/em&gt;中产阶级&lt;em&gt;等&lt;/em&gt;概念*）*。&lt;/td&gt;
&lt;td&gt;能够将文化相关的概念投射到文化相关的兴趣连续体上，从而使研究人员不仅可以在单个历史时期内而且可以在其历史演变过程中了解广泛共享的社会关联。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6. &lt;strong&gt;概念维度&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;科兹洛夫斯基等人。(2019)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;阶级的文化维度：理解社会阶级的维度（富裕、教育、修养、地位、就业、道德、性别）&lt;/td&gt;
&lt;td&gt;20 世纪文化阶层的规模有多稳定？&lt;/td&gt;
&lt;td&gt;20世纪，尽管发生了巨大的经济转型，阶级规模仍然非常稳定。&lt;/td&gt;
&lt;td&gt;能够对阶级的多个概念维度进行实证分析，从而理解 20 世纪美国它们之间的相互关系。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h3 id=&#34;51-概念广度&#34;&gt;5.1 概念广度&lt;/h3&gt;
&lt;h4 id=&#34;511-指标&#34;&gt;5.1.1 指标&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;可以测量文档中单词之间的距离来计算它们在概念空间中的分布范围&lt;/strong&gt;。文档可以是从专利到个人电子邮件通信的任何内容。我们可以测量每个单词与其他单词的平均距离有多远。&lt;strong&gt;获取文档内元素的平均距离（或每个单词与文档质心之间的距离）可以衡量该文档内的「概念宽度」&lt;/strong&gt;。例如，我们衡量每项专利的概念广度， 可以从两个简单的文档开始，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;doc1 =  [&amp;#34;biochemistry&amp;#34;, &amp;#34;chemistry&amp;#34;, &amp;#34;analytical_chemistry&amp;#34;]
doc2 =  [&amp;#34;chemistry&amp;#34;, &amp;#34;oceanography&amp;#34;, &amp;#34;computer&amp;#34;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用我们的专利嵌入模型，我们得到第一组(doc1)的平均宽度为 29，第二组（doc2）平均宽度为 47。这表明第二组在概念上比第一组更广泛。&lt;/p&gt;
&lt;p&gt;当我们衡量文档集合而不是单词的概念广度时，同样的逻辑也适用。例如，我们想了解发明者团队的广度。在这种情况下，我们可以将团队中的每个发明人视为嵌入概念空间中的“文档”，参考如图&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F1&#34;&gt;1&lt;/a&gt; , 从下往上，依次是词概念空间、发明人概念空间、团队概念空间、组织概念空间。一个发明人团队的成员已经在涉及纳米技术、生物技术和软件的概念空间领域发表了先前的专利，那么在概念上将被认为比所有成员只发表了纳米技术专利的团队更广泛。即使所有发明人都将其公开的专利限制在一个类别内，该指标仍然会提供显着的变化。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/figure-1.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;512--应用&#34;&gt;5.1.2  应用&lt;/h4&gt;
&lt;p&gt;这种概念广度的度量已在最近的工作中用于追踪各种理论构念。&lt;strong&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105&#34;&gt;利克斯等人。(2022)&lt;/a&gt;衡量团队成员在参与软件项目的不同阶段时的 话语广度&lt;/strong&gt;。&lt;strong&gt;他们能够追踪每个独特项目阶段概念参与的多样性，发现表现最好的团队有能力改变他们的认知以适应手头不断变化的任务，在提出新想法时表现出更大的话语广度，而在转换时表现出较低的广度依赖于协调的任务。这种细粒度的知识参与概念很难用以前的文本分析方法来追踪&lt;/strong&gt;。 详细内容可阅读大邓近期推文 &lt;a href=&#34;https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/&#34;&gt;MS2022 | 使用语言差异性测量团队认知差异性&lt;/a&gt; 。&lt;/p&gt;
&lt;p&gt;另外，研究人员使用概念广度来追踪在线社区成员根据状态变化分配注意力的范围，发现状态和注意力广度之间存在 U 形关系（Aceves et al. 2022 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2&#34;&gt;）&lt;/a&gt;。这些研究人员训练了 150 个知识领域的概念空间，从而能够追踪不同知识领域的相似注意力动态，从计算机编程和数学到育儿和园艺。由于他们有能力在数百个社区的文本中大规模部署算法，因此他们能够计算出超过 2000 万成员如何在这些问答社区上发布的 2300 万个问题中分配注意力。&lt;/p&gt;
&lt;p&gt;其他工作在整个语言中实施了这种方法，追踪语言在所有知识领域具有更宽或更窄的概念空间的程度（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B1&#34;&gt;Aceves 和 Evans 2021&lt;/a&gt;）。使用圣经、电影字幕和以多种语言编写的政治文件等文本的并行翻译（包含相同的信息但以不同的语言编码），他们能够追踪概念在不同语言中相互关联的程度存在显着差异。他们发现，尽管一些语言将不同的概念子空间紧密地联系在一起，并将不同的概念领域编织在一起，但其他语言却稀疏且更加支离破碎，更强烈地分隔了不同的意义域。然后，他们观察概念空间的语言密度如何塑造数百种语言的真实对话和维基百科文章的概念广度。&lt;/p&gt;
&lt;p&gt;所有三篇论文都为不同文献的研究开辟了新的理论途径，例证了该方法的潜力。如果没有概念空间的概念及其通过嵌入模型的表示，这些新的研究途径将很难实施。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;52-概念距离和相似度&#34;&gt;5.2 概念距离和相似度&lt;/h3&gt;
&lt;h4 id=&#34;521-指标&#34;&gt;5.2.1 指标&lt;/h4&gt;
&lt;p&gt;当我们的分析重点在于集合内的元素时，前面描述的概念广度构念是相关的。当我们的分析重点是不同集合之间的关系时，可以使用相同的基础度量。在这种情况下，我们将指的是概念距离或相似性，而不是概念广度。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn14&#34;&gt;13&lt;/a&gt;形式上，如果我们有至少两个集合，每个集合中至少有一个元素，我们可以计算这些集合之间的&lt;strong&gt;概念距离，作为每个集合的质心或多维平均值之间的距离&lt;/strong&gt;。最基本的是，我们可以计算两个集合之间的概念距离，每个集合包含一个单词。这无非是衡量这些词之间的概念距离。随着元素数量和集合数量的增加，底层计算保持不变，但理论可能性的范围扩大。还可以通过训练文档嵌入模型来计算这种距离/相似性度量，该模型在嵌入空间中为每个文档分配一个向量，其权重按照单词共现的相同逻辑进行训练（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B96&#34;&gt;Le 和 Mikolov 2014&lt;/a&gt;），将文档本身视为文档中的另一个单词，将这些单词用作与其共现的单词。&lt;/p&gt;
&lt;p&gt;通过将概念相似性与衡量专利相似性的现有技术进行比较，我们可以一睹该衡量标准的潜力。首先，研究人员可以通过查看专利授予机构使用的官方分类来追踪专利的相似性，同一类别的专利被认为比不同类别的专利更相似（Singh 和 Marx 2013，Aharonson 和&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B157&#34;&gt;Schilling&lt;/a&gt; 2016 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B3&#34;&gt;）&lt;/a&gt;）。这种方法的局限性在于分类度量是粗粒度的，并且不太可能考虑所有相关的技术特征，特别是当类别边界必然滞后于技术进化时（Thompson 和 Melanie Fox-Kean 2005，Singh&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B172&#34;&gt;和&lt;/a&gt;Agrawal &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B155&#34;&gt;2011&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B7&#34;&gt;Arts 等人，2018&lt;/a&gt;）。其次，研究人员可以获取两项专利并测量它们之间的单词重叠（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B7&#34;&gt;Arts et al. 2018&lt;/a&gt;）。然而，这种方法是有限的，因为它仅适用于成对的文档，无法确定专利相对于整个知识体系的位置。&lt;/p&gt;
&lt;p&gt;概念相似性解决了这些限制（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184&#34;&gt;Whalen 等人，2020&lt;/a&gt;）。首先，它允许我们追踪专利在相关知识空间中的精确位置，从而访问知识系统中的所有相关的细粒度信息。其次，我们能够精确量化任何专利或专利组相对于任何其他专利或专利组的位置。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn15&#34;&gt;14&lt;/a&gt;第三，随着新知识进入系统，知识的性质和结构不断演变，随着时间的推移重塑 &lt;strong&gt;概念边界&lt;/strong&gt; 和关联。&lt;strong&gt;嵌入使我们能够衡量专利发布时存在的概念空间内的专利相似性，使我们能够摆脱使用滞后的、周期性偏离的类别，并可能对连续的发明概念空间强加类别差异&lt;/strong&gt;。概念距离的所有这些优点都适用于其他知识和文化领域，在这些领域中，我们寻求测量思想、个人、群体或组织之间的距离或相似性，从而扩展现有的跨研究领域并开辟新的理论领域。&lt;/p&gt;
&lt;h4 id=&#34;522-应用&#34;&gt;5.2.2 应用&lt;/h4&gt;
&lt;p&gt;正如我们上面所做的那样，这种&lt;strong&gt;概念相似性的衡量方法最近被用来描述专利数据中的创新空间&lt;/strong&gt;（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184&#34;&gt;Whalen 等人，2020&lt;/a&gt;）。研究人员使用  &lt;strong&gt;doc2vec&lt;/strong&gt;  框架计算了超过 6 亿个专利对的相似度。在生成这些知识相似性度量时，作者还使用这些分数提出了有趣的辅助度量，包括可操作的度量（a）现有技术接近度——专利引用与其自身相似或不相似的现有技术的程度，（b）现有技术同质性——一项专利引用知识空间领域彼此远离的程度，(c) 影响邻近性——一项专利被与其自身相似或不相似的未来专利引用的程度，以及(d) 影响同质性——一项专利通过其前向引用与一组不同的未来专利相关的程度。&lt;/p&gt;
&lt;p&gt;学者们也使用了这一衡量标准，重点关注概念距离。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18&#34;&gt;伯奇等人。(2021)&lt;/a&gt;使用概念距离的 &lt;strong&gt;doc2vec&lt;/strong&gt; 实现来调查同行奖励是否会影响在线社区内贡献的新颖性。这里的&lt;strong&gt;新颖性是根据社区成员获奖前后贡献的距离来衡量的&lt;/strong&gt;。作者发现，获奖后，奖项会导致知识空间内的新颖性减少，剥削行为增多。同样，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74&#34;&gt;霍夫斯特拉等人。（2020）&lt;/a&gt;使用 Word2Vec 距离度量来捕获科学论文将新颖性引入科学文献的程度，发现来自代表性不足群体的学生负责将最具新颖性引入系统。&lt;/p&gt;
&lt;p&gt;其他人则利用这一措施来实施公司差异化。在发展中国家微型企业的背景下，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20&#34;&gt;Carlson（2022）&lt;/a&gt;使用 BERT 架构（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35&#34;&gt;Devlin et al. 2019&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B142&#34;&gt;Reimers and Gurevych 2020&lt;/a&gt;）来计算其数据集中所有微型企业的成对余弦距离。通过这些距离，他们能够估计八个发展中国家的 10,000 家微型企业的差异化与收入和利润的增加相关。同样，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61&#34;&gt;Guzman 和 Li（2023）&lt;/a&gt;使用距离的 doc2vec 实现来使用 Crunchbase 数据来衡量初创公司的创始战略差异化。作者发现与&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20&#34;&gt;Carlson (2022)&lt;/a&gt;类似的结果，即差异化经验的新公司在早期融资和股权结果方面有所增加。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;53-概念x&#34;&gt;5.3 概念X&lt;/h3&gt;
&lt;h4 id=&#34;531-指标&#34;&gt;5.3.1 指标&lt;/h4&gt;
&lt;p&gt;文档距离的另一个用途是追踪语料库中的任何文档与捕获感兴趣的构念X的焦点(原型）的相似性， 这样的测量将捕获任何观察的 &lt;strong&gt;概念X性&lt;/strong&gt;( Conceptual X-ness)。这种测量的第一步是描述与我们寻求尽可能精确测量的结构相关的概念信息。例如，如果我们想要捕获专利与 &lt;strong&gt;时间&lt;/strong&gt; 或 &lt;strong&gt;几何&lt;/strong&gt; 等概念相关的程度，我们可以构建一个我们认为映射到、定义或与这些概念相关的单词列表 。对于每个列表，我们计算其质心向量 (c#27)，然后测量任何给定专利距离 &lt;strong&gt;时间&lt;/strong&gt; 和 &lt;strong&gt;几何&lt;/strong&gt; 概念有多远。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn16&#34;&gt;15&lt;/a&gt; 对于附录表 A2 中使用的专利，我们可以看到，与头颈约束装置专利相关的前两项专利更接近时间概念，正如所预期的那样光和时间在概念上交织的程度。概念性的&lt;em&gt;X&lt;/em&gt;度度量可用于追踪思想、个人、团体、组织或任何其他相关聚集的组成。&lt;/p&gt;
&lt;h4 id=&#34;532-应用&#34;&gt;5.3.2 应用&lt;/h4&gt;
&lt;p&gt;最近在一篇论文中使用了这种方法，该论文追踪了雇用女性担任高级领导角色对女性在这些组织中意味着什么的影响（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94&#34;&gt;Lawson 等人，2022&lt;/a&gt;）。作者首先使用 SEC 文件和财报电话会议记录训练了 Word2Vec 嵌入。然后，他们创建并验证了一组 100 个单词来捕捉 &lt;strong&gt;代理概念&lt;/strong&gt; 的含义（例如，有能力、独立、主导），并观察了内部任命高级女性领导前后 &lt;strong&gt;代理概念&lt;/strong&gt; 与 &lt;strong&gt;女性&lt;/strong&gt; 概念之间的距离。该组织发现，在 &lt;strong&gt;女性&lt;/strong&gt; 被任命为高层管理人员之后的一段时间内，女性的含义在概念空间中更加接近于机构职位。作者使用不同的嵌入超参数和维度大小复制了他们的结果，说明了嵌入模型的鲁棒性，条件是具有捕获概念空间内语义变化的最小必要维度。&lt;/p&gt;
&lt;p&gt;这里有趣的理论机会包括更深入地参与理论传统的可能性，这些理论传统在组织科学以外的领域具有影响力，但由于缺乏可行的方法来以原则性的方式量化其理论构造，因此这些理论传统仍然处于我们的领域之外。依赖文学解释。正如我们所提出的，测量 &lt;strong&gt;概念X性&lt;/strong&gt; 使得扩大与理想形式（* &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B134&#34;&gt;Plato Bloom 1968&lt;/a&gt;）、理想类型（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B178&#34;&gt;Weber 2011&lt;/a&gt;）、家族相似性（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B186&#34;&gt;Wittgenstein 2010&lt;/a&gt;）和原型（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B147&#34;&gt;Rosch 1973&lt;/a&gt;）相关的理论构造的测量成为可能。以一致、有原则和可复制的方式。在这方面，概念性的&lt;em&gt;X&lt;/em&gt;性代表着开放大量的认知和社会理论，以便在组织的背景下进行实证检验和扩展。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;54-语义转变和漂移&#34;&gt;5.4 语义转变和漂移&lt;/h3&gt;
&lt;h4 id=&#34;541-指标&#34;&gt;5.4.1 指标&lt;/h4&gt;
&lt;p&gt;概念空间使我们能够识别术语的含义如何随着时间和空间的变化而变化。探索概念意义的一种方法是为不同的个人、公司、行业、地理位置或时间段创建独特的嵌入模型，以了解它们之间的含义有何不同（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B148&#34;&gt;Roy 等人，2019 年&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B181&#34;&gt;Welch 等人，2020a&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B182&#34;&gt;b&lt;/a&gt;）。一旦识别出相关的兴趣分歧，我们就可以采用相关的语料库（例如，专利、财报电话会议、报纸）并为数据中的每个语料库训练概念空间。&lt;strong&gt;在我们的专利示例中，我们可能会训练两种嵌入模型，一种是 1990 年功能性磁共振成像技术发明之前的时期，另一种是 1990 年之后的时期&lt;/strong&gt;。然后我们可以探索与大脑和神经科学相关的概念的含义如何随着这一创新而改变。例如，在功能性磁共振成像发明之前和之后与不同大脑区域最相关的术语是什么。接下来，我们可以比较不同公司或国家的含义变化有何不同，以及这种变化的格局如何影响所涉及的公司和行业的组织和市场结果。显式动态词嵌入允许嵌入之间具有更大的可比性，但必然会忽略特殊的词和用途（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63&#34;&gt;Hamilton et al. 2016&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B192&#34;&gt;Zhang et al. 2016&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B190&#34;&gt;Yao et al. 2018&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B103&#34;&gt;Liu et al. 2020&lt;/a&gt;）。这些算法的输出带有时间戳词向量包含特定时期的语义信息，但在历史上保持可比性。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;54-2-应用&#34;&gt;5.4. 2 应用&lt;/h4&gt;
&lt;p&gt;第一篇在社会科学背景下使用词嵌入方法的主要论文就是使用这种方法来研究意义（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63&#34;&gt;Hamilton et al. 2016&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19&#34;&gt;Caliskan et al. 2017&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49&#34;&gt;Garg et al. 2018&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;Kozlowski et al. 2019&lt;/a&gt;）。在第一篇论文中，研究人员使用四种语言的六个历史语料库，通过观察概念空间中最近的单词在过去几十年中如何变化来追踪单词含义随时间的变化（Hamilton et al. 2016 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63&#34;&gt;）&lt;/a&gt;。使用 Word2Vec 嵌入，他们追踪了 &lt;strong&gt;同性恋&lt;/strong&gt;  概念的含义如何从 1900 年代围绕 &lt;strong&gt;“愚蠢”&lt;/strong&gt;、**“甜蜜” **和 **“开朗”  **等术语的含义转变为围绕 1950 年代 &lt;strong&gt;“嬉闹”&lt;/strong&gt;、 &lt;strong&gt;“机智”&lt;/strong&gt; 和 &lt;strong&gt;“聪明”&lt;/strong&gt; 等术语的含义，并且最终以 20 世纪 90 年代女同性恋、双性恋和同性恋等术语的含义结束。在另一篇论文中，研究人员研究了词嵌入中的刻板关联之间的关系及其与当代社会经验数据的关系（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19&#34;&gt;Caliskan et al. 2017&lt;/a&gt;）。例如，他们追踪了职业的性别刻板印象，发现职业具有女性意义，因为它们与女性参与劳动力市场相关。在另一项研究中，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49&#34;&gt;Garg 等人。(2018)使用预先训练的 Google News Word2Vec 模型（ &lt;/a&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B4&#34;&gt;Google 2013&lt;/a&gt; ）量化了美国 100 多年历史中的性别和种族刻板印象，阐明了不同的形容词和职业如何或多或少地与不同人群（例如，男性与女性）密切相关，白人与亚洲人与西班牙裔）随着时间的推移。&lt;/p&gt;
&lt;p&gt;最近通过词嵌入追踪含义的工作已经使用这种方法更深入地研究了特定的上下文。一项研究使用 19 世纪第一人称叙述的语料库来追踪黑人和白人男性和女性的交叉身份如何映射到五个社会机构，包括政治、经济、文化、家庭领域和权威关系（Nelson 2021 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B120&#34;&gt;）&lt;/a&gt;。&lt;strong&gt;举论文中的一个例子，作者测量了与“精致”概念的距离，发现它与白人女性的联系最密切，而与黑人男性的联系最少&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在其他工作中，研究人员利用这种方法来衡量政治领导人的 &lt;strong&gt;集体意向性&lt;/strong&gt; （人们参与集体推理和行动的能力），并比较共和党和民主党领导人如何以不同的方式动员集体意向性（Kirgil and Voyer 2022 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B85&#34;&gt;）&lt;/a&gt;。他们通过创建复数代词（我们，我们的）、复数常量（国家名称）和复数名词（人）的复合列表来测量集体意向性。然后，使用词嵌入模型，他们找到了各州集体意向向量最接近的术语，使他们能够比较不同领导人如何不同地动员集体意向。总的来说，这些意义研究表明，就语言为我们提供了解文化的窗口而言（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B55&#34;&gt;Goldberg et al. 2016&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165&#34;&gt;Srivastava et al. 2018&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26&#34;&gt;Corritore et al. 2020&lt;/a&gt;），嵌入模型为我们提供了一种独特的表达方式透过那扇窗户看到的照片。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;55-文化和知识连续性中的概念地位&#34;&gt;5.5 文化和知识连续性中的概念地位&lt;/h3&gt;
&lt;h4 id=&#34;551-指标&#34;&gt;5.5.1 指标&lt;/h4&gt;
&lt;p&gt;另一种新颖的测量方法可以通过追踪概念相对于感兴趣的概念维度的位置来创建。如前所述，嵌入模型可用于解决类比推理任务，例如**“国王”-“男人”+“女人”=“女王”&lt;strong&gt;。 该架构可用于定义概念空间内任何感兴趣的维度。&lt;strong&gt;在国王-王后的例子中，性别维度通过“男人”-“女人”和“国王”-“女王”向量进行操作。&lt;/strong&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;科兹洛夫斯基等人。（2019）&lt;/a&gt;详细介绍了如何在概念空间内构建此类维度。首先，研究人员需要确定感兴趣的维度。对于我们这里的例子，我们将把不同的概念投射到男性-女性性别维度上。为此&lt;/strong&gt;，我们首先确定定义性别维度的相关术语**。这里我们使用集合 [&amp;lsquo;man&amp;rsquo;, &amp;lsquo;him&amp;rsquo;, &amp;lsquo;he&amp;rsquo;, &amp;lsquo;male&amp;rsquo;, &amp;lsquo;men&amp;rsquo;] 和 [&amp;lsquo;woman&amp;rsquo;, &amp;lsquo;her&amp;rsquo;, &amp;lsquo;she&amp;rsquo;, &amp;lsquo;female&amp;rsquo;, &amp;lsquo;women&amp;rsquo;]。 &lt;strong&gt;然后我们计算不同概念在这个男性-女性概念轴(维度)上的正交投影。&lt;/strong&gt; 在线附录中的图 A4 将每个概念投射到 &lt;strong&gt;男性-女性概念轴&lt;/strong&gt;。 更消极的预测表明与女性气质的关联更强，而更积极的预测表明与男性气质的相关性相当。如图 A4 所示，这些预测与关于这些概念的性别状态的一般直觉一致，使我们能够明确说明每个概念相对于其他概念在这个维度中的位置。正如预期的那样，&lt;strong&gt;军事&lt;/strong&gt; 和 &lt;strong&gt;农业&lt;/strong&gt; 与 &lt;strong&gt;男性气质&lt;/strong&gt; 的联系最为密切，而 &lt;strong&gt;卫生棉条&lt;/strong&gt; 和 &lt;strong&gt;口红则&lt;/strong&gt; 与 女性气质的联系最为密切。按照这个程序，学者们现在可以测量任何概念在任何感兴趣的维度和任何文本丰富的时空背景中的位置。此外，不同语言的语料库可以独立训练和对齐，或者同时训练和对齐，以方便国际分析（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B81&#34;&gt;Johnson et al. 2017&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116&#34;&gt;Milbauer et al. 2021&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;双极概念维度的投影方法可以进一步扩展到锚定具有多种含义的低维子空间，其中单词和概念可以被绘制并理解为这些含义的混合&lt;/strong&gt;。这可以通过理论上选择“原型”的集合来执行，即具有已知且广泛共享含义的极值点，并在这些极值锚定义的子空间中绘制所有相关单词或概念。[例如，在对一个新的基于信息技术的创业企业进行分类时，人们可能会问它在 Uber、亚马逊、谷歌或比特币所刻画的空间中适合什么位置(Breiman 1994，Eugster 2012，Damle 和 Sun 2017）。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;552-应用&#34;&gt;5.5.2 应用&lt;/h4&gt;
&lt;p&gt;这项措施的制定和运用是为了研究 20 世纪和 21 世纪社会阶层的演变（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;Kozlowski 等人，2019&lt;/a&gt;）。他们研究了根据 20 世纪出版的数百万本书的文本训练的嵌入，按照上述程序操作了阶级的维度，试图了解社会阶级的底层维度在 20 世纪是如何变化的。为此，他们提出了以下理论上的&lt;strong&gt;概念轴(维度)&lt;/strong&gt;：富裕程度（富人与穷人）、教育程度（受过教育与未受教育）、修养（有教养与未受教育）、地位（有声望与无声望）、道德（善与恶）、就业（雇主-雇员）和性别（男人-女人），分别嵌入 20 世纪的每个十年。然后，他们可以在这些维度上投射不同类别的概念，例如音乐风格、体育和职业，以了解这些概念在本世纪的过程中如何演变和发展。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B6&#34;&gt;研究人员应用这种方法来研究健康、道德（ Arseniev-Koehler et al. 2022&lt;/a&gt;）、政治意识形态（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B171&#34;&gt;Taylor and Stoltz 2021&lt;/a&gt;）和地位（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B129&#34;&gt;Peng et al. 2021&lt;/a&gt; ）等背景下的其他类型的文化关联。&lt;/p&gt;
&lt;p&gt;研究人员不仅将概念投射到这些概念轴(维度)上，而且将整个文档投射到这些维度上，从而推动了测量的可能性（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B170&#34;&gt;Taylor 和 Stoltz 2020&lt;/a&gt;）。此外，尽管以前的措施依赖于研究人员指定感兴趣的连续体的相关维度，但最近的工作已经转向自动识别这些连续体（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116&#34;&gt;Milbauer et al. 2021&lt;/a&gt;）。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116&#34;&gt;Milbauer 等人&lt;/a&gt;利用 Reddit 社区的内容。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116&#34;&gt;（2021）&lt;/a&gt;创建了一个无监督的程序来识别社区中的多个意识形态极点，使他们能够超越静态的左右意识形态维度，发现现代话语中发挥作用的许多两极分化和意识形态差异的轴。人们可以想象在许多组织环境中使用这种方法来识别团队、小组、单位或部门之间存在的许多潜在冲突来源。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;56-概念维度&#34;&gt;5.6 概念维度&lt;/h3&gt;
&lt;p&gt;之前，我们讨论了研究人员如何调查关键术语在相关文化维度上的位置，描述概念的位置在性别维度上的差异。然而，这并不是概念轴(维度)的唯一用途，因为概念空间还允许我们测量和理解相关维度本身如何相互关联。该措施的扩展是使用空间内的编码维度并将它们相互比较。例如，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;科兹洛夫斯基等人。（2019）&lt;/a&gt;利用他们既定的阶级维度来追踪整个 20 世纪每个维度如何与其他维度相关，例如，表明随着世纪的发展，富裕与教育的关系变得更加密切，而与教育的关系无关。栽培。通过这种方式，组织学者可以理解相关维度之间的关系在相关概念空间中可能有何不同。例如，学者可以研究不同文化维度在组织或行业内部和之间紧密或松散联系的程度。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;六讨论&#34;&gt;六、讨论&lt;/h2&gt;
&lt;p&gt;最后，我们简要讨论了一些利用嵌入模型进行思考的新兴方法，然后讨论了我们认为理论、方法论和组织的有价值的机会，这些机会源于将这些模型理解为概念空间的细粒度表示。这个讨论必然是说明性的，但暗示了现在这些精致的意义模型的可操作性的广泛可能性。&lt;/p&gt;
&lt;h3 id=&#34;61-词嵌入方法的富有成果的扩展&#34;&gt;6.1 词嵌入方法的富有成果的扩展&lt;/h3&gt;
&lt;p&gt;词嵌入的底层计算架构最近经历了扩展，可以在与之前讨论的不同方向上动员组织研究。我们简要提到三个，并在在线附录中提供更详细的描述。首先，概念和语言的层次结构在“直线”、欧几里得几何中很难得到体现，需要许多难以理解的维度来用标准嵌入来捕获。然而，层次结构可以用负弯曲双曲嵌入来原生表示（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B90&#34;&gt;Krioukov et al. 2010&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B126&#34;&gt;Papadopoulos et al. 2012&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B22&#34;&gt;Chamberlain et al. 2017&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B121&#34;&gt;Nickel and Kiela 2017&lt;/a&gt;），为探索复杂现代的交叉层次结构提供了新的测量可能性。组织。例如，将公司名称嵌入双曲空间中将能够直接发现典型的“中心公司”，并在商业新闻语料库中与所有其他公司进行比较。额外的双曲维度将揭示子层次结构，反映商业评论员所持有的概念和比较价值的不同维度。&lt;/p&gt;
&lt;p&gt;其次，模型语言的深度学习方法为词嵌入增加了关键的上下文敏感性。考虑像 BERT ( &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35&#34;&gt;Devlin et al. 2019&lt;/a&gt; ) 和 GPT 系列模型 ( &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B140&#34;&gt;Radford et al. 2019&lt;/a&gt; ) 这样的大规模模型，它们使用“注意力”的神经网络机制来识别影响焦点词含义的上下文词 ( &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B176&#34;&gt;Vaswani ) et al. 2017&lt;/a&gt;），组装成一个称为转换器的架构，可以将问题转换为答案，将文本转换为翻译，将请求转换为响应。这种模型产生的内容可以被描述为上下文嵌入，这样每个单词不是由单个向量表示，而是由向量云表示，每个向量代表不同上下文中的该单词。“google”上下文中的“Apple”与“orange”上下文中的“apple”具有不同的值。这些模型极大地提高了预测能力，并进一步扩展了我们对概念空间进行精确建模的能力，但代价是复杂性和计算量更大。&lt;/p&gt;
&lt;p&gt;最后，嵌入架构可以扩展到在序列或更高维上下文中排列的任意符号集。例如，图像已被用来衡量抽象艺术图像的新颖性和创造力（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B10&#34;&gt;Banerjee and Ingram 2022&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B11&#34;&gt;Banerjee and Kaplan 2022&lt;/a&gt;），分析警察预约照片（大头照），并识别与司法拒绝保释相关的先前未概念化的紧急特征听证会（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B107&#34;&gt;Ludwig 和 Mullainathan 2022&lt;/a&gt;）。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B101&#34;&gt;音乐（ Liang et al. 2020&lt;/a&gt;）、音频剪辑（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B70&#34;&gt;Hershey et al. 2017&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B189&#34;&gt;Xie and Virtanen 2019&lt;/a&gt;）和视频（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B191&#34;&gt;Zellers et al. 2021&lt;/a&gt; ）的多维空间是使用audio2vec（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B168&#34;&gt;Taglisacchi et al. 2020&lt;/a&gt;）等工具构建的。 、signal2vec（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B118&#34;&gt;Nalmpantis 和 Vrakas 2019&lt;/a&gt;）和 video2vec（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B62&#34;&gt;Habibian 等人 2017&lt;/a&gt;），为组织学者接触代表组织生活视听体验的新型媒体打开了大门。&lt;/p&gt;
&lt;p&gt;最近对双曲线、上下文、图像和音频嵌入的扩展表明，嵌入模型的底层计算框架的持续改进和扩展将继续下去，为组织科学中持续的实证、测量和理论创新奠定了基础。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;62-词嵌入和组织理论&#34;&gt;6.2 词嵌入和组织理论&lt;/h3&gt;
&lt;p&gt;在理论层面上，将嵌入模型理解为概念空间的有原则的、细粒度的表示有可能刺激新的理论发展并完善现有理论。例如，意义研究中的经典陈述影响了文学理论和文化社会学等其他领域，但未能在组织科学中站稳脚跟。自20 世纪初德索绪尔 ( &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B32&#34;&gt;de Saussure 1986&lt;/a&gt; )的著作带来语言学的结构转向以来，许多人都试图将意义在组织和社会生活中的作用理论化。列维-斯特劳斯汇集了来自全球各地的多样而广泛的民族志，以向世界文化所特有的表面混乱提出深层的文化秩序，并认为复杂的意义是从有意义的元素的结合中产生的（列维-斯特劳斯 2016 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B99&#34;&gt;）&lt;/a&gt;。福柯理论化了话语和权力如何紧密相连，权力和知识如何以自我强化的联盟结合在一起（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B46&#34;&gt;Foucault 2012&lt;/a&gt;）。&lt;em&gt;布迪厄将惯习&lt;/em&gt;的概念阐述为“持久的、可互换的处置系统，倾向于充当结构结构的结构化结构，即作为实践的生成和结构的原则”（Bourdieu 1977，第72页&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B16&#34;&gt;）&lt;/a&gt;。尽管这些理论很有吸引力，但迄今为止它们只能进行松散且间接的测试。如果没有可靠的实证立足点，他们就永远无法在管理和组织理论中取得突出地位。然而，概念空间的实证操作化现在使得这些文化理论基础著作的参与和扩展变得容易处理，其中的许多结构现在可以辩护地测量。嵌入模型将使这些理论与管理和组织理论相关。&lt;/p&gt;
&lt;p&gt;我们还希望嵌入模型能够对现有理论框架进行更深入的研究和锐化。一组能够受益的文献是那些与知识相关的文献。鉴于组织学者可以获得的大部分知识都被编码在语言的符号概念系统中，现在可以通过更多可用的文本数据源来获取知识，并且可以通过嵌入模型的概念空间来表示。材料科学领域的最新工作已经使用此类模型来有效预测未来的知识发现，比科学家提出的知识发现早几十年（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B173&#34;&gt;Tshitoyan 等人，2019 年&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B161&#34;&gt;Sourati 和 Evans，2021 年&lt;/a&gt;）。其他工作表明，这些发现可以推广到生物和物理科学（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B154&#34;&gt;Shi 和 Evans 2023&lt;/a&gt;）。概念空间的明确表示可以对整个社会系统中知识的特征和结构进行详细的调查。一方面，这些模型就像望远镜一样，打开了知识的天空，使其大规模结构变得可见，以供研究、理论发展和完善。另一方面，这些模型充当显微镜，使我们能够更深入地观察构成更大知识系统的意义原子结构。测量方面的这一进步将丰富对定义人类和组织经验的大型多维知识系统中的机制的测试。它还将使我们能够递归地评估管理和组织奖学金的知识，从而刺激创新。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;63-词嵌入和实证研究&#34;&gt;6.3 词嵌入和实证研究&lt;/h3&gt;
&lt;p&gt;在&lt;em&gt;实证层面&lt;/em&gt;，词嵌入模型可以提高组织科学不同领域的测量保真度，从而在实证结果与理论主张和框架之间实现更好的映射。我们用团队和群体内部多样性研究的例子来说明这一点。据说，不同群体所获得的许多好处是由于群体中的个人代表问题和解决方案的方式不同而产生的（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B75&#34;&gt;Hong 和 Page 2004&lt;/a&gt;）。由具有不同方法的个人组成的小组将更好地执行各种任务，因为他们将拥有更广泛的知识、观点和可供借鉴的信息资源（Cox et al. 1991，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B27&#34;&gt;Williams&lt;/a&gt; and &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B185&#34;&gt;O&amp;rsquo;Reilly 1998&lt;/a&gt;）。然而，由于测量困难，对团队多样性的研究很少测量问题和解决方案空间的不同概念。相反，它假设解决问题的团队成员的身份多样性（人口、文化、种族或经验）与其功能多样性（团队成员如何代表和解决问题）之间存在联系（Nisbett 和 Ross 1980，Hong&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B122&#34;&gt;和&lt;/a&gt;Page &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B75&#34;&gt;2004&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175&#34;&gt;van Dijk 等人，2017&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;由于缺乏高保真方法来访问团队成员在问题和解决方案的概念空间中的位置，因此通常假定身份和功能多样性之间存在联系。用于操作研究的身份多样性和用于理论化的功能多样性之间脱节的一个重要后果是，虽然理论积极使用功能多样性的思想和术语（从根本上讲是几何和高维的），但测试依赖于集合-与身份成员资格相关的理论概念。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175&#34;&gt;我们预测，这将解释团队多样性文献（ van Dijk et al. 2017&lt;/a&gt; ）结果中的大部分歧义，因为研究设计忽视了功能多样性和身份多样性之间的同源性。然而，诸如概念广度之类的衡量标准可以阐明这一理论交叉点上的悬而未决的问题。我们现在可以指定（1）团队的基本概念广度，以及（2）这种基本广度可能驱动结果的程度。解决这些问题可以为许多分析层面的研究提供信息，从个人和团队的成功（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B164&#34;&gt;Srikanth 等人，2016 年&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175&#34;&gt;van Dijk 等人，2017 年&lt;/a&gt;）到公司和行业绩效（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B144&#34;&gt;Roberson 等人，2017 年&lt;/a&gt;）。我们希望我们的插图能够激发在组织研究领域生成细粒度意义测量的新可能性。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;64-组织内部的词嵌入&#34;&gt;6.4 组织内部的词嵌入&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;最后，我们认为词嵌入方法将对我们研究的组织产生影响&lt;/strong&gt;。我们说明了在劳动力市场背景下潜在的嵌入必须塑造组织行为。从招聘到工作设计，从培训到晋升，人力资源管理的一个核心挑战是有效地将个人与组织内的角色、工作、情况和任务相匹配（Weller et al. 2019 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B183&#34;&gt;）&lt;/a&gt;。随着比赛质量的提高，各种绩效指标也会提高，包括工作满意度（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B8&#34;&gt;Ashforth 和 Saks 1996&lt;/a&gt;）、个人生产力（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B125&#34;&gt;Paauwe 2009&lt;/a&gt;）和组织绩效（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B38&#34;&gt;Dyer 和 Reeves 1995&lt;/a&gt;）。有效匹配的一个问题是不同维度的匹配的重要性程度。在一家公司中，技能可能最为重要，而在其他公司中，技能可能是文化契合度、态度、技能和经验的相互作用。由于嵌入模型捕获了所有这些维度，管理者可以为每个相关维度嵌入不同的原型描述，同时还嵌入个人资料和其他相关通信（例如电子邮件、松弛消息等），以衡量每个人与每个相关维度之间的匹配接近度。这样做可以让管理者更好地识别高维匹配及其对员工、社区和公司绩效的影响。&lt;/p&gt;
&lt;p&gt;嵌入模型旨在为人力资源的宏观管理提供新的视角（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B183&#34;&gt;Weller et al. 2019&lt;/a&gt;）。来自大型组织的相关信息存储在人力资源经理、一线经理、员工、同事和外部招聘人员中。然而，无法集中访问这些信息。通过嵌入，组织可以从所有数字面包屑的文本（电子邮件、聊天、工作描述、正式报告、绩效管理记录等）构建概念空间。这样做并使用相似性分析将使公司能够绘制和了解相关人力资本的位置位于公司对面。管理人员可以利用这些系统来准确了解任何员工的概念职位与任何给定的公司要求的差距有多大。这不仅可以为招聘、雇用、员工流动和流动等流程提供信息，还可以为培训、社交、工作设计和公司重组提供信息。因此，在劳动力市场和组织适应的背景下，嵌入模型可以产生有用的创新。人们可以想象许多其他组织实践和结构可以从这些模型及其测量可能性中受益，包括产品设计、市场分析和战略生成。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;我们同意&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65&#34;&gt;Hanan等人的观点。（2019&lt;/a&gt;，第 2 页）当他们观察到，考虑到概念和分类对几乎所有人类行为和社会互动的中心地位，人们对概念如何运作的关注如此之少，这是多么令人惊讶。现代组织内部及其周围进行的许多活动都需要概念信息的激活和传播。当一个人解决新问题、提出新想法或与他人合作时，就会发生这种情况。从围绕饮水机的良性闲聊到重新配置全球资本主义秩序或将人类登陆火星，概念及其所嵌入的概念空间发挥着核心、关键的作用。&lt;/p&gt;
&lt;p&gt;正如本文所示，我们现在拥有一系列重要的工具，可以为广泛而深入的理论想象和实证研究打开&lt;strong&gt;概念世界&lt;/strong&gt;和&lt;strong&gt;概念空间&lt;/strong&gt;。我们希望本文能够激发对嵌入可以提供信息的大量问题和理论的学术探索。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量  | 使用<strong>人民网领导留言板</strong>语料训练Word2Vec模型</a></li>
</ul>
<p><br><br></p>
<p>Aceves, Pedro, and James A. Evans. &ldquo;<strong>Mobilizing conceptual spaces: How word embedding models can inform measurement and theory within organization science.</strong>&rdquo; <em>Organization Science</em> (2023).</p>
<br>
<h2 id="摘要">摘要</h2>
<p>词嵌入模型是一种表示多维概念空间的强大方法，在多维概念空间中，所传达的概念可以相互关联、组合和竞争。此类模型代表了机器学习的最新进展，使学者能够用大规模文本数据局部和全局的单词共现，以最小的语义失真程度， 有效地编码复杂的意义系统。尽管词嵌入的使用有可能扩大组织科学中的理论可能性，但嵌入对于组织学者来说很大程度上是未知的，未发挥出词嵌入应有的潜力。我们的目标是通过为用户提供实用的路线图来展示嵌入模型在组织科学中的前景，以在他们的研究中调动该方法，并为开展该类研究的学者提供理论指导。 我们首先明确定义 <strong>概念</strong> 和 <strong>概念空间</strong> 的概念，然后继续展示如何使用词嵌入模型来表示和测量这些概念，并指出该方法的优点和缺点。然后，我们提供一组嵌入测量及其理论解释和灵活的扩展。我们的目标是从词嵌入的技术处理中提取概念，并将其置于实践的理论框架中，以加速此类研究。</p>
<p><br><br></p>
<h2 id="一介绍">一、介绍</h2>
<p>过去十年，文本作为数据的计算使用在组织科学中显着增长（Hasan 等人，2015 年；Goldberg 等人，2016 年；Srivastava 等人，2018 年；Hannigan 等人，2019 年）。这种增长的主要原因是文本编码的概念信息赋予个人、组织、经济和社会行为以意义（Evans 和 Aceves 2016，Gentzkow 等人 2019），并且在过去十年中，来自组织环境的文本数据急剧增长，大大提高了文本的可用性。然而，文本中编码的 <strong>概念意义</strong> 本质上是高维的，这使得降低概念复杂性成为研究文本的学者的中心任务。<strong>词嵌入模型是由计算机科学家和语言学家开发的一个新兴工具系列，用于文本信息降维，以此提取概念及其数字表示</strong>。词嵌入技术的发展使组织科学家依赖于文本数据进行理论构造， 相比之前，数据中信息的保真度更高，由此文本数据与组织研究交叉场景形成了新的理论研究路线。尽管词嵌入模型在组织科学之外得到广泛使用，但由于组织科学领域的学者缺乏对词嵌入技术的理解， 不知如何将它们纳入理论发展过程的原则框架，词嵌入模型对于理论发展的价值仍然被掩盖。</p>
<p><strong>词嵌入模型建立在高效的神经网络架构之上，并通过将复杂的语义系统有效编码到具有最小失真的稠密几何空间中，彻底改变了语义分析</strong>。这些模型代表了数十到数百个维度的空间中的语义，相对于语言中的单词和概念的数量来说，这个维度较低； 但相对于正式社会和文化理论家之前试图呈现概念信息的两到三个维度来说，这个维度却很高（奥斯古德 1964 年，史密斯-洛文和海斯 1988 年）。出于组织科学的目的，这些嵌入模型创建了社会系统中个体所持有的集体知识的 <strong>数字替身</strong> ， 嵌入可以解决文化上隐含类比（Mikolov et al. 2013b），回答文化偶然问题（Devlin et al. 2019，Radford et al. 2022），并预测未来的知识发现（Tshitoyan等人 2019；Sourati 和 Evans 2021）。组织科学长期以来一直借鉴人工智能（AI）的表征概念， 在这里，我们使用人工智能的表示机制来增强组织理论研究（Csaszar 和 Steinberger 2022）。</p>
<p>然而，由于神经网络复杂，且难以理解的黑盒性质特性，围绕神经嵌入和人工智能方法对理论发展的价值存在争议。尽管预测能力很强，但此类方法往往缺乏可解释性（Knight 2017，Leavitt et al. 2021）。<strong>在组织科学领域中，学者缺乏此技术的理解，即</strong></p>
<ul>
<li><strong>对于嵌入何时成为组织科学有用的方法论选择</strong></li>
<li><strong>如何在既定认识论标准内证明使用“复杂”神经嵌入方法的合理性</strong></li>
<li><strong>如何在各种嵌入中进行选择 等方法</strong>（例如，静态词嵌入与上下文嵌入、预训练嵌入与自定义嵌入）</li>
<li><strong>使用嵌入进行研究的适当步骤以及评估嵌入研究的相关标准</strong></li>
<li>最值得注意的是，研究界，特别是那些研究组织认知、文化、知识和意义的人，似乎对嵌入方法 <strong>如何适应将方法论选择与理论发展联系起来</strong></li>
</ul>
<br>
<p>我们的目的是通过两项贡献来解决这些问题。</p>
<p><strong>首先，我们的目标是提供一个理论指南，为嵌入模型提供一个原则性的概念框架，学者可以使用该框架为他们的模型注入意义，并使学者们能够在理论发展过程中运用这些模型。我们这里的主要论点是，词嵌入模型中的每个向量代表一个概念，整个嵌入模型代表生成文本数据的社会系统的概念空间</strong>。嵌入模型所代表的概念空间是多维空间，其中从规范和知识到想法和发明的概念相互关联。这个框架使组织学者能够利用嵌入模型的概念空间，与组织科学的许多领域之间建立联系。例如，不同公司基于知识视角对该空间的差异化覆盖（Grant 1996），组织理论家在描述规范和制度（Scott 2003），类别学者援引在决定将一个物体归类到哪个概念时（Pontikes 和 Barnett 2015 ），创新学者直接理论化寻求测量发现和发明的新颖性（Fleming 和 Sorenson 2001，2004），并且团队研究人员寻求了解成员在空间中的不同立场如何影响创造力、协调性和绩效（Srikanth 等人，2016）。因为我们以 <strong>概念</strong> 和 <strong>概念空间</strong> 为中心的理论框架可以推广到组织理论的许多背景，所以我们希望嵌入模型所支持的研究将促进这些子领域之间更深入、更持久的对话。</p>
<p><strong>其次，我们的目标是为利用嵌入模型进行理论发展提供实用的路线图</strong>。在此过程中，我们引导读者完成使用专利摘要语料库来实现词嵌入模型的过程，以表示现代技术创新的概念空间。我们解释了研究人员需要设置的模型参数，并逐步完成了他们应该采取的验证步骤，以评估模型是否有效地代表了他们感兴趣的概念空间，并提供了方法附录，其中包含实现所讨论的所有内容所需的代码。在注意到嵌入模型的可供性的同时，我们还讨论了它们不断发展的局限性，并提出了它们何时不适合组织分析的建议。然后，我们展示嵌入模型如何实现依赖于概念和概念空间的构造的理论化和测量。</p>
<br>
<p>我们概述了两大类词嵌入使用方法</p>
<ul>
<li><strong>度量之内/之间进行标记</strong>，我们提出了跟踪相关分析集内部和之间的概念关系的度量，以帮助我们跟踪与概念广度、概念距离和概念相似性</li>
<li><strong>意义及其维度</strong>，我们提出了四种衡量标准，为了解意义及其与组织的关系提供了不同的窗口。为找出这些测量机会的理论可能性，我们重点介绍了一些研究进展。</li>
</ul>
<p><strong>本论文的一个核心主张是，在组织研究不同广度和深度，词嵌入工具现在使我们能够表示其概念空间，并且比以前更精细地表示细节</strong>。有鉴于此，我们的目标是展示嵌入模型如何在与组织科学家相关的领域中操作概念空间，使研究人员能够扩展和完善现有理论。我们希望这一理论指南和实践路线图将促进组织科学内部的理论扩展，该扩展首先是扩大对文本数据的访问以及用于分析的随附计算工具（Kovács 等人，2013 年; Goldberg 等人; 2016年，Hannigan 等人, 2016年, 2019； Guo 等人，2020）。</p>
<p><br><br></p>
<h2 id="二概念和概念空间">二、概念和概念空间</h2>
<p>概念是人类生活的一个基本特征，我们的日常思维很大程度上依赖于它们所代表的信息，使我们能够对周围的人、物体和事件进行分类，并将这些信息传达给其他人（Murphy 2002；Bergen 和 Feldman 2008 年； Cassanto 和 Lupyan，2015 年）。概念是将我们的精神世界粘合在一起的粘合剂（Murphy 2002），赋予精神和物质体验以意义（Hannan et al. 2019）。<strong>在认知科学和心理学的语言中，概念是“事物类别的「心理表征」”（Murphy 2002）。</strong></p>
<p>概念有两大功能：分类和交流（Medin and Rips 2005），这些功能都需要语言的帮助。实际上，我们通过在语言中分配一个单词或短语来表示一个稳定概念的信息内容。这就是为什么我们通过说出或写出 “<em><strong>manager</strong></em>” 一词来提及经理的概念，从而引出它所包含的概念信息，例如对他人的责任、做出决策以及相对于组织同行获得更高的薪水。然后，语言的单词分割并链接了社区的共享概念空间（Lupyan 和 Bergen 2015）。这样，“一个概念就是一个单词或短语的含义……[包括]像 ‘<em><strong>red</strong></em>’ 和 ‘<em><strong>grasp</strong></em>’这样的基本的、具体化的单词，以及像 ‘<em><strong>goal</strong></em>’ 和 ‘<em><strong>continuity</strong></em>’ 这样的抽象和技术单词”（卑尔根）和 Feldman 2008]）。</p>
<p>概念并不作为唯一的信息单位存在于真空中。相反，概念之所以有意义，是因为它们彼此相关（Hannan et al. 2019），“通过相似性和上下文的关系紧密地缝合在一起”（Hofstadter and Sander 2013）。在这种多重概念关系中存在着“我们对世界的大部分知识，告诉我们存在什么以及它们具有什么属性”（Murphy 2002，p.1）。例如，概念 <em><strong>resource</strong></em>  与  <em><strong>firm</strong></em>、<em><strong>constraint</strong></em> 和 <em><strong>natural</strong></em> 等概念相关。在文化系统的层面上，概念之间的相互关系引发了表征概念之间宏观层面有意义的维度。 <em><strong>manager</strong></em> 概念在某些方面与 <em><strong>coach</strong></em> 和 <em><strong>president</strong></em> 的概念很接近，而在其他方面则与<em><strong>employee</strong></em> 和 <em><strong>bureaucracy</strong></em> 的概念很接近。将概念理解为存在于复杂几何空间中的点，使我们能够思考和测量概念之间的距离远近（Hannan 等人，2019）。例如，与  <em><strong>playground</strong></em> 或 <em><strong>ice cream</strong></em> 相比， <em><strong>manager</strong></em> 与<em><strong>organization</strong></em> 和 <em><strong>leader</strong></em> 概念的联系更加紧密。<strong>我们将这种概念相关的多维空间称为概念空间</strong>（Hannan et al. 2019)</p>
<p>重要的是我们用复数来指代概念空间。对于许多单词来说，它们会根据使用的上下文表现出不同的概念信息模式。首先，概念可能会根据使用它们的社会背景而有所不同。例如，如果在执行董事会议室、商品交易大厅或附近的储蓄和贷款机构的背景下说出 “<em><strong>Bank</strong></em>”，指的是银行而不是河流。概念也可能根据使用时间的不同而有所不同。例如，“<em><strong>高科技</strong></em>” 一词所引发的概念关系会根据我们研究的是 1960 年代、1990 年代还是今天而有所不同。最后，概念关系因使用它们的社区而异，因此 “<em><strong>债务</strong></em>” 所捕获的概念将根据其是由首席财务官还是低收入个人使用而有所不同。概念所含信息存在多样性， 正如 Hannan等人（2019）指出，“虽然有些概念可能是天生的或生物驱动的，但大多数都是社会构建的。”</p>
<p><br><br></p>
<h2 id="三先前研究中的概念和概念空间">三、先前研究中的概念和概念空间</h2>
<p>概念以及扩展的概念空间是人类思维和交流的基础（Sperber 和 Wilson 1986；，Murphy 2002；Hofstadter 和 Sander 2013）。正因为如此，概念和概念空间对于许多组织理论框架来说或多或少是明确和关键的。在某些研究（例如类别研究）中，概念具有核心重要性并且已经被明确地理论化。然而，在其他情况下，（例如，公司基于知识视角）概念被隐含地假定，即使它们是决定许多理论期望的基本成分。鉴于概念无处不在，对组织科学所有领域使用概念信息进行全面回顾超出了本文的范围。我们将简短、非详尽的回顾集中在概念和概念空间概念的三个领域——<strong>类别、知识和文化</strong>。通过嵌入技术处理并追踪存在于个人和社区头脑中的概念信息，研究其对组织行为和结果的影响。<br></p>
<h3 id="31-类别">3.1 类别</h3>
<p>类别是具有共同特征和属性的实体组。如前所述，概念是类别的心理表征。对类别的研究主要集中在跨类别或模糊类别是否会增加或减少分类实体的估值。自Zuckerman（1999）以来的工作一直集中在消除歧义条件上，在这些条件下，类别跨越和模糊性会导致积极或消极的估值。许多研究表明，由于感知偏差（Durand et al. 2007）、不符合受众期望（Hsu 2006)、Hsu et al. 2009；Leung and Sharkey 2014） ，跨越模糊的类别会损害实体估值，或降低分类对比度（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B119">Negro et al. 2010</a>）。其他研究表明，跨越类别可以创造积极的估值结果，因为它表明非典型性可以放大良好的表现并缓冲不良表现（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B159">Smith 2011</a>），一个类别可以锚定认知，而另一个类别可以有益地修改认知（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B188">Wry et al. 2014</a>）。还有其他研究表明，效果取决于受众，有些人喜欢跨类别，而另一些人则不喜欢（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B135">Pontikes 2012</a>）。通过这些方式，类别可以通过影响有关类别成员资格的概念信息的解释方式，对行为和绩效产生积极或消极的影响。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn5">4</a></p>
<p>尽管类别范式的贡献历来是通过类别成员的集合和模糊集合理论（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B64">Hannan et al. 2007</a>）概念来实现的，但最近的工作开始纳入其多维性（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65">Hannan et al. 2019</a>）和类别的分级归属感。组织学者感兴趣的许多现象都是由概念及其代表的类别之间的精确距离支撑的。例如，鉴于专利所贡献的技术领域，专利通常分为类别和子类。然而，专利中编码的想法可以传播到创新空间的广泛领域，即使只分类在一个类别中。正如我们稍后讨论的，转向概念的几何概念，使分析师能够考虑隶属度、重叠和连续距离影响底层实体评估判断的方式<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65">（Hannan 等人，2019 </a>。<br></p>
<h3 id="32-知识">3.2 知识</h3>
<p>众所周知，知识很难具体说明，并且在哲学、认知科学和社会科学领域，围绕其概念性质进行了长期而活跃的争论（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B166">Steup 和 Neta 2020</a>）。然而，过去几十年来，组织科学在微观、中观和宏观层面上进行了大量研究，解决有关知识及其在团队、组织和经济活动中的作用的问题。从对团队成员专业知识的研究（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B164">Srikanth et al. 2016</a>）到公司基于知识和注意力的观点（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B87">Kogut and Zander 1992</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B56">Grant 1996</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B123">Ocasio 1997</a>）；从交互记忆系统（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B143">Ren 和 Argote，2011</a>）到创新流程（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B50">Garud 等，2013</a>）；从组织设计（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B45">Foss et al. 2013</a>）到搜索和探索（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B93">Lavie et al. 2010</a>），知识在最近的组织理论化中发挥着核心作用。</p>
<p>无论人们对知识的定义如何选择，命题性知识从根本上都与概念信息相关。<em><strong>命题知识采取“ S [主体]知道p [命题]”</strong></em> 的形式（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B80">Ichikawa and Steup 2018</a>）。在某种程度上，命题是由语言中的单词编码的，并且单词代表概念信息，命题知识依赖于概念以及它们如何在概念空间中交织（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B110">McGrath and Frank 2020</a>）。以命题“泰勒知道氢的主要工业应用是氨的制造”和“特里知道量子算法可以具有较低的时间复杂度”为例。这些知识命题中的每一个都代表了不同的概念意义，前面提到的领域将以不同的方式操作它们。例如，团队学者可能会强调，由泰勒和特里组成的专利团队将拥有多样化的基础知识。采取基于注意力观点的学者会注意到，泰勒和特里可能会以不同的方式关注知识空间，以应对组织变革。研究创新的人可能会注意到如果泰勒和特里共享办公空间，知识重组的潜力。研究搜索的人可能会假设，为了解决问题，泰勒和特里会以不同的方式搜索概念性解决方案。在所有这些情况下，就这些领域通过诉诸语言编码的命题知识来理论化知识动态而言，它们以基本和可测量的方式参与概念和概念空间。<br></p>
<h3 id="33-文化">3.3 文化</h3>
<p>文化被不同地概念化为集体的共同价值观、故事、框架、工具包和类别（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B52">Geertz 1973</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B131">Pettigrew 1979</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B92">Lamont 和 Small 2008</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B158">Small 等人 2010</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B54">Giorgi 等人 2015</a>）。文化建构已成为组织研究的核心，在从个人和团队到组织和国家的各个层面的分析中都得到了运用（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B54">Giorgi et al. 2015</a>）。从理解文化如何塑造职业结构（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B195">Glynn 2000</a>）、组织领域（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B194">Anteby 2010</a>）和创业环境（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B106">Lounsbury and Glynn 2001</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B141">Rao and Giorgi 2006</a>）到它在讲故事（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B106">Lounsbury and Glynn 2001</a>）和身份建设中的作用（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B196">Ravasi 和 Schultz 2006</a>），从其对人际沟通的塑造（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165">Srivastava 等人，2018</a>）到对组织绩效的影响（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26">Corritore 等人，2020</a>），文化深深地受到概念及其互动方式的调节。文化以集体认知过程为基础（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B36">DiMaggio 1997</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B128">Patterson 2014</a>），很大程度上可以通过语言痕迹来获取。语言进入文化的窗口（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B55">Goldberg et al. 2016</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165">Srivastava et al. 2018</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26">Corritore et al. 2020</a>）很大程度上是通过它所表达的概念来呈现的，使得概念和概念空间成为组织文化研究的重要支柱。</p>
<p>基于它们在形成范畴、知识和文化方面的关键作用，概念和概念空间已成为许多组织理论赖以建立的知识支架的重要组成部分。然而，概念和概念空间通常仅被用作缺乏精确和可扩展的经验表征的不明确的隐喻。这限制了研究使用粗粒度的代理测量或允许手动编码和解释的小数据集。接下来，我们提出词嵌入模型是一种最先进的工具，用于表示概念和概念空间，可以添加到组织学者工具包中。就组织学者寻求将概念和概念信息所支撑的结构操作化而言，他们将得到这类新模型的帮助。考虑到这一点，我们接下来介绍嵌入模型如何工作以及为什么它们可以作为概念和概念空间的有效表示。</p>
<p><br><br></p>
<h2 id="四使用词嵌入来表示概念和概念空间">四、使用词嵌入来表示概念和概念空间</h2>
<h3 id="41-越来越多地使用文本作为数据">4.1 越来越多地使用文本作为数据</h3>
<p>过去 10 年，通过计算工具和方法进行文本数据分析出现了爆炸性增长。从社会学（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B40">Evans and Aceves 2016</a>）到经济学（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B53">Gentzkow et al. 2019</a>）和政治学（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B58">Grimmer and Stewart 2013</a>），文本正迅速成为组织、经济和社会生活的中心观察站。文本数据提供了在线知识社区、财报电话会议和公司报告、产品评估、组织电子邮件和讨论板、历史档案、视频转录和电影字幕、医疗记录、电子商务、社交媒体等多种领域的丰富思想和行为痕迹。媒体平台、新闻文章、科学学科等等。总而言之，这些文本数据源比以往任何时候都更深入、更广泛地进入组织生活。正如<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B40">Evans 和 Aceves（2016 年</a>）指出的那样，文本数据现在使我们能够访问“有关正在玩的社交游戏的隐藏元素及其背后的社交世界”的深层信息。然而，这些语料库的庞大规模及其广泛的范围意味着，提取理论上有意义的信息信号越来越多地受到计算方法的帮助，利用信息技术方法获取大量非结构化文本数据，并将它们转换为有意义且相关的度量。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn6">5</a></p>
<p>文本数据与组织学者习惯使用的定量数据之间的一个主要区别是文本是高维的。正如<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B53">Gentzkow 等人（2019 年</a>）指出，“仅使用英语中一千个最常用单词的 30 个单词的 Twitter 消息样本 [&hellip;] 的维度大致与宇宙中的原子一样多。” 因此，使用文本作为数据的学者的中心任务是通过对数据施加限制来降低维度。<strong>过去二十年里，组织科学中用于降低这一维度的一些最常用的计算工具是词典法、语义网络和主题模型。尽管这些方法有其优点，但一个主要缺点是它们无法对文本中存在的细粒度概念关系和关联进行编码</strong> 。接下来，我们将展示嵌入模型如何利用文本中的局部和更广泛的信息来训练概念含义和概念空间的高保真表示。在此过程中，我们展示了词嵌入模型如何克服先前方法来表示文本中编码的含义的一些局限性，从而允许对理论结构进行更细粒度的测量，并实现新的理论可能性。</p>
<br>
<h3 id="42-词嵌入">4.2 词嵌入</h3>
<p>我们之前解释过，概念是事物类别的心理表征，人类通过在词典中分配一个单词或短语来表示稳定的概念，并指出，概念只有在与跨多个维度的其他概念相关并为其提供信息时才有意义。密集的概念空间。在这里，我们认为词嵌入模型是最近开发的一类从机器学习应用于自然语言处理的模型，它使我们能够有效且高效地表示概念空间，并将这些空间用于追求组织科学。词嵌入模型是文本语料库中单词的连续表示，可以进行几何解释。<strong>词嵌入的方法论假设，一个词的含义很大程度上是由出现在其直接和更广泛上下文中的词所决定的，这一想法受到结构语言学家的启发，他们已经证明，含义的差异与局部分布相关（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B68">Harris 1954</a>）， 这个想法现在被称为 「分布式语义学」，Firth 的著名描述是：“观其伴而知其意”（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B42">Firth 1957</a>，you shall know a word by the company it keeps）， 一个单词所代表的概念或含义可以通过它周围的单词的分布来推断</strong>。</p>
<p>以这种分布式方式思考概念和概念空间的底层计算架构可以追溯到 20 世纪 80 年代初期计算机科学家 Geoffrey Hinton 的工作（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B71">Hinton 1986</a> , <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B72">Hinton et al. 1986</a>）以及认知科学家在这一时期研究的并行分布式处理模型（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B149">Rumelhart 等人，1986a</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B150">b</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B109">McClelland 和 Rumelhart，1989</a>）。分布式架构是当前嵌入语言模型的基础（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115">Mikolov et al. 2013b</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130">Pennington et al. 2014</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35">Devlin et al. 2019</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104">Liu et al. 2019</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B17">Brown et al. 2020</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B41">Fedus et al. 2020）。 2021</a>）， 嵌入模型 Word2Vec 算法(<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115">Mikolov 等 2013b</a>) 相对简单易用，能够处理中等规模的语料库来。 <strong>Word2Vec 与  GloVe（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130">Pennington 等人，2014 年</a>）和 FastText（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B13">Bojanowski 等人，2017 年</a>）等嵌入算法，是 ChatGPT 和相关模型的基础</strong>。</p>
<p>找个例子来帮助理解算法， 现在我们要创建过去 50 年创新的概念空间表示。首先需要概念活动领域的文本数据， 美国专利局数据提供了创新活动的踪迹，其中包括所有专利的文本、摘要、描述和权利要求。在整篇论文中，我们使用这个专利摘要语料库来指导读者完成训练这个概念空间和构建相关概念测量的过程。数据是从<a href="https://patentsview.org/">Patentsview.org</a>免费下载的，使用 1976 年至 2019 年间发布的所有专利来构建本文中发现的词嵌入模型和测量相关指标。</p>
<p>想象一下，专利语料库中的每个独特单词都是从放置在巨大冰箱上的随机放置的 <strong>“word magnet”</strong> 开始的（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B76">Hovy 2020</a>）。当连续词袋 (CBOW) 算法滚动浏览语料库时，使用每个目标词周围的单词词(滑动窗口的上下文)来预测目标词（更多内容见下文）。该算法的最终目标是产生一种语义模型，其中出现在相似上下文中的单词彼此接近，而来自不同上下文的单词则相距很远。由于用2维概念空间不足以捕获每个单词的全部含义，因此该算法改为在更高的（100-1,000）维空间内捕捉语义。通过这种方式，目标单词的概念信息是从它周围的单词中归纳出来的，将语料库中的每个单词绘制为<em>n</em>维空间中的坐标或向量。正是单词在这个<em>n</em>维向量空间中的相对位置，使我们能够将词嵌入模型可以描述代表人类概念活动区域的概念空间。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn7">6</a></p>
<p>概念意义的识别假定了嵌入空间的可解释性。接下来，我们提出了对这些概念空间的一系列提示和测量，作为从中产生结构化解释的方法。这很像心理学家使用 <strong>心理测量调查</strong> 将概念印象转化为可解释的观点（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B112">Michael Furr 2021</a>）。或者<strong>认知人类学家如何使用结构化任务，例如排序和排名（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B163">Spradley 2016</a>），将概念性的世界观转变为可解释的世界观</strong>。我们认为嵌入模型必须接受结构化测量（就像向人类受试者提供的心理测量问卷）使他们的 **概念景观(conceptual landscape)**变得可解释。接下来，我们将引导读者如何用专利语料库训练创新概念空间表示的过程。之后， 我们概述了该方法的优点和局限性，并指出这些方法与先前的文本分析方法和组织研究实践的关系。</p>
<br>
<h3 id="43-选择语料库">4.3 选择语料库</h3>
<p>学者可以根据应用使用两种词嵌入模型。一方面，研究人员可以使用自有文本语料库来训练表示， 据此了解文本所涉主体(个人、团体、社会)行为的概念空间是什么样子， 以及概念关系揭示人类活动背景。在我们的示例中，专利创新在专利语料库中得到了很好的体现，因此我们在下面展示了如何从头开始训练概念空间表示, 以及它揭示了哪些概念联系。研究人员可以从头开始训练语料库的其他例子包括在线社区（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18">Burtch et al. 2021</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2">Aceves et al. 2022</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B23">Chambers et al. 2022</a>）、学术学科（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74">Hofstra et al. 2020</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B102">Lin et al. 2022</a>） 、劳动力市场（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B9">Bana 2022</a>）、公共记录（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B6">Arseniev-Koehler et al. 2022</a>）、产品和公司描述（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61">Guzman and Li 2023</a>）以及财报电话会议和公开演讲（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B85">Kirgil and Voyer 2022</a>）。</p>
<p>或者，如果研究人员想要在较小的语料库中追踪概念动态，而该语料库的大小不足以训练独特的、特定于上下文的嵌入，那么研究者可以使用预训练嵌入模型，需要注意，训练预训练嵌入模型的文本与研究者小语料库在内容、场景要有相似性。广泛使用的预训练嵌入已经在来自海量语料库的文本上进行了训练，例如新闻（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B4">Google 2013</a>）、维基百科（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35">Devlin et al. 2019</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B57">Grave et al. 2018</a>）。训练这些预训练嵌入模型的文本语料体量很大， 内容题材往往包含我们较小文本样本中存在的概念。因此使用预训练嵌入对这些概念的信息进行编码，并可用于近似相关距离。政治和历史语义背景下的研究发现，预训练嵌入提供的结果与特定于上下文的嵌入相当（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">Kozlowski et al. 2019</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145">Rodriguez and Spirling 2022</a>）。如果有理由相信研究项目中包含的概念和想法没有在这些大量预训练嵌入中得到很好的体现，研究人员可以使用较小语料库中的文本对其进行 <strong>微调（Fine-Tune）</strong>（ <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104">Liu et al. 2019，Burtch et al.2019</a>）<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18">， 2021</a>）。微调将预训练的概念空间扭曲为与样本一致（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104">Liu et al. 2019</a>），更好地反映概念之间的关系。</p>
<p>最后，使用哪一种嵌入(自己训练的嵌入、 预训练的嵌入、微调的嵌入)将取决于研究人员的目的以及他们寻求追踪的概念动态的类型。接下来，我们将重点描述从头开始训练和验证嵌入模型的过程。在接下来的部分中，我们讨论不同参数设置和策略之间的权衡，并鼓励读者遵循文章文本和在线附录。</p>
<br>
<h3 id="44-清理语料库">4.4 清理语料库</h3>
<p>训练嵌入模型的第一步是使用 Python 等编程语言录入文本语料库， 首先获取每个专利摘要中的文本， 并将连续的文本进行切词，转化为单词列表 。然后，我们将文本小写，删除标点符号和数字字符串，并将每个摘要转换为称为token的单词列表。但是这可能破坏一些词组语义，这里使用 <em><strong>bi-gram</strong></em>， 识别高频共现的词组成词组，例如当 <em><strong>“electric”</strong></em> 和 <em><strong>“vehicle”</strong></em> 这两个词在某些上下文中一起出现时，它们将被统一形成短语和概念 <em><strong>“electric_vehicle”</strong></em> 。建立单词或短语列表后，执行单词嵌入算法来学习单词或二元组及其语言上下文之间的最佳距离，以保留语言中单词和短语的概念空间。</p>
<br>
<h3 id="45-训练嵌入模型">4.5 训练嵌入模型</h3>
<p>第一步是选择词嵌入算法， 浅层神经网络构建的单词表示（例如，Word2Vec、FastText；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115">Mikolov 等人，2013b</a>）、共现矩阵的低秩近似（GloVe；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130">Pennington 等人，2014</a>） ，或来自 Transformer 的深度上下文嵌入（例如 BERT、<em>GPT</em>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35">Devlin 等人 2019</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B139">Radford 等人 2022</a>）。这些不同算法输出，都可以被解释为<em>n</em>维概念空间，其中单词或短语由空间内的向量位置表示。本文我们只介绍 Word2Vec 算法， word2vec 是一种广泛使用的训练概念空间的算法（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B113">Mikolov 等人，2013a</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115">b</a>）。</p>
<p>Word2Vec 算法的一种流行实现算法是连续词袋 (CBOW) 算法，可以在 Gensim python 库中轻松访问，该算法使用目标单词的语言上下文来预测被扣掉的目标词 (可以简单的理解为让机器做完形填空题) ， 比较适合小规模数据集。 Word2Vec 还实现了另一种 Skip-Gram 算法，该算法通过从目标单词预测上下文单词来反转 CBOW 的预测任务，比较适合大规模数据集。相比之下，skip-gram 将每个上下文目标对（例如，T：“房子”，C：“宽敞”）视为单独的观察，因此可以更好地捕获精确的语义，但需要更大的语料库才能获得卓越的性能。</p>
<br>
<h3 id="46-维数">4.6 维数</h3>
<p>考虑维数很有必要。朴素的模型可以将不重复总词数作为维度， 例如包含 100,000 个不重复单词的语料库， 任何单词都需要  100,000 维才能准确表示。然而，当单词从上下文中被识别为相似时，可以一定范围内减少维度数。<strong>维度过多会导致内存需求和冗余增加，并降低可解释性；维度太少会扭曲距离并且无法解释语言的不及物性</strong>。通过这种方式，通过具有至少足够的维度来捕获所讨论的复杂语义关系，可以获得准确的预测。</p>
<p>在实践中，300 维已经成为一个标准，很大程度上源于最初的 Word2Vec 论文之后的惯例，该论文通过交叉验证确定了最佳维数，以减少预测屏蔽词任务中的错误。大多数后续分析都是建立在较小、多样性较低的文本集合上，需要较少的维度，因此 300 通常被用作上限。最近的工作表明，应根据语料库统计数据选择维度 - 语料库词汇表中成对等距单词的数量提供了维度数量的下限，低于此界限通常会导致单词嵌入质量下降（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B127">帕特尔和巴塔查亚 2017</a>）。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74">霍夫斯特拉等人。(2020)</a>使用 100、200 和 300 维的模型找到了稳健的结果。</p>
<p>如果分析师寻求实现维度可解释性，他们必须以最小失真来确定表示数据所需的维度数。 但这最后一步一半很少执行，因为维度的优化需要大量的时间和计算资源。</p>
<br>
<h3 id="47-窗口尺寸">4.7 窗口尺寸</h3>
<p>回想一下，窗口大小是指算法将用来焦点目标词（或其邻居）之前和之后的单词数量。该窗口最小可以是 1。对于较小的窗口，算法将倾向于对句法关系进行编码（例如，名词后跟动词）。<strong>随着窗口大小的增加，更多的含义和语义被编码到模型输出中</strong>。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145">考虑Rodriguez 和 Spirling (2022)</a>的示例，其中包含两个句子的语料库：(1)“狮子吃肉”和 (2)“牛吃草”。当窗口大小为一时，我们会知道牛和狮子都吃东西，从这个意义上说，牛和狮子在语法上是等价的，因为我们没有足够的信息来区分两者。然而，随着窗口的增加，算法开始对牛与狮子的含义进行更多编码。<strong>与维度数量一样，这里的回报也递减，窗口大于五个字的模型性能略有改善</strong>（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145">Rodriguez 和 Spirling 2022</a>）。 <strong>BERT 和 GPT 系列等上下文模型具有更大的窗口，这些窗口通过注意力过程进行驯服，算法通过该过程识别哪些上下文单词对于解释焦点单词的含义很重要</strong>（Vaswani 等人，2017 年<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B176">）</a>。</p>
<br>
<h3 id="48-验证模型">4.8 验证模型</h3>
<p>最后一步是验证词嵌入模型，这样做是为了确认算法学习的表示与文本数据所承载的真实人类活动的概念空间表示尽可能相近。论文附录第 2 节描述了关于专利嵌入的七个详细验证程序，表明该模型有效地学习了创新空间的表示。这些包括（1）邻近嵌入词的语义相似性；(2)具有嵌入距离的语义梯度；(3)嵌入簇与语义域之间的对应关系；（4）物理世界距离与嵌入之间的相关性；(5) 社会距离与嵌入之间的相关性；(6) 嵌入空间类比推理的准确性；(7)嵌入文档的语义一致性。我们还讨论了第八个“额外”测试，即图灵测试（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B174">Turing 1950</a>）。由 Transformer 支持的现代上下文嵌入的评估标准是它们是否能够与人类毫无区别地参与任何分类、关联、意义生成或集成任务，包括普通对话和专家教程。OpenAI 的 ChatGPT 和许多竞争的聊天机器人已经展示了如此强大的性能，以至于图灵测试正在迅速从上限转变为基线基准。这些验证步骤与论文最后部分的测量相结合，作为嵌入模型的有用提示prompt和测量，使研究人员能够对其编码的概念空间提供结构化解释。</p>
<br>
<h3 id="49-词嵌入方法的优点和缺点">4.9 词嵌入方法的优点和缺点</h3>
<h4 id="491--无需正式指定相关尺寸">4.9.1  无需正式指定相关尺寸</h4>
<p>对概念建模的正式尝试试图通过逻辑演绎方法清楚地枚举概念的相关维度（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B47">Gärdenfors 2004</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B48">Gardenfors 2014</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65">Hannan 等人 2019</a>）。尽管这种方法对于理解限定领域内的概念很有用，但即使如此，它也可能不切实际且难以衡量，因为很难先验地陈述分析师应预期的相关维度<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B73">（Hofstadter 和 Sander 2013 ）</a>。 <strong>词嵌入的优点在于，概念之间的关系以及对任何给定概念重要的相关维度可以从语言的使用方式中推断出来，因此不需要事前指定</strong>。鉴于在分析之前没有必要陈述相关维度，即使是最复杂的组织行为剧场也变得易于分析处理。正如其他人所指出的，“词嵌入为语言中包含的多个维度的含义提供了全面且有意义的见解，这是以前的方法无法捕获的”（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105">Lix 等人，2022 年</a>，第 8434 页）。在某种程度上，这种优势源于这样一个事实：神经网络架构能高效地记录意义的维度。</p>
<br>
<h4 id="492-更大的有效维度">4.9.2 更大的有效维度。</h4>
<p>嵌入通常由 100 到 1,000 个密集编码维度表示。<strong>编码的密度意味着每个词向量在所有建模维度上都有一个非零坐标</strong>。正如附录中所指出的，主题模型可能具有相同数量的主题（例如，100-1,000），但这些主题被稀疏编码以方便人类解释，使得主题仅具有一些基本上非零的单词加载，并且文档仅具有少量非零的主题负载。<strong>因此，主题模型是为了描述而构建的，但代价是迫使其表示的有效维度从数百个减少到几个，从而扭曲了本来可以在主题空间内计算的距离。相比主体模型， 词嵌入使用密集编码，每维度的嵌入很难理解和描述，但距离具有更大的自由度，可以更精确地编码含义</strong>。通过这种方式，相对于低维理论和测量，嵌入为分析师提供了“大量潜在轴，个人和社会群体可以沿着这些轴竞争、合作、分裂或合并”（Kozlowski et al. 2019，p.27 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">）</a>。</p>
<br>
<h4 id="493-无监督训练">4.9.3 无监督训练。</h4>
<p><strong>词嵌入还有一个特殊优点，即训练模型时， 以看似无监督或自监督的方式进行，从而避免了手动编码文本语义内容的繁琐，完全由机器学习</strong>。在我们的创新示例中，向量空间由我们专利语料库中的每个发明人按照他们所写句子的数量和长度的比例进行监督。每个单词的滑动窗口都是为了向专利审查员和未来的发明者传达一种含义而构建的，该算法用于构建向量空间并以概念上适当的方式定位单词。因此，学者们可以利用专利语料库来训练 <strong>技术创新</strong> 的概念空间，利用财报电话会议记录和新闻稿来训练 <strong>上市公司沟通</strong> 的概念空间，利用分析师报告来训练 <strong>投资分析</strong> 的概念空间，或者特定领域的概念空间。使用内部通信（例如 Slack 和电子邮件）来了解公司的知识。这些概念空间可以在最少的监督下进行训练，因此很快成为有价值的观察站，用于追踪组织科学家关注的组织生活的静态和动态（Hofstra et al. 2020，Whalen et al. 2020，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74">Burtch</a> et <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184">al</a> . <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18">2021</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B177">Waller 和 Anderson 2021</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2">Aceves 等人 2022</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20">Carlson 2022</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B23">Chambers 等人 2022</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61">Guzman 和 Li 2023</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94">Lawson 等人 2022</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105">Lix 等人 2022</a>）。</p>
<br>
<h4 id="494-共现是不必要的">4.9.4 共现是不必要的。</h4>
<p><strong>这些模型的另一个优点是，两个概念不必在任何文档中同时出现，就可以将它们编码为相似的向量</strong>。所需要的只是它们与相似的概念同时出现。例如，我们可以先验地指出 <em><strong>医生</strong></em> 和 <em><strong>律师</strong></em> 在某些方面非常相似（例如，他们需要高级学位，具有高收入水平等），但他们可能永远不会同时出现在语料库的同一文档中。尽管彼此之间缺乏共现性，但它们很可能都独立地与高收入*、<em>高学历</em>、*白领等概念同时出现，从而最终拥有编码这些相似性的接近向量。<strong>因此，嵌入模型的底层计算架构可以更好地近似社会和文化含义，而无需求助于严格的共现</strong>。</p>
<br>
<h4 id="495-上下文相关的含义结构">4.9.5 上下文相关的含义结构。</h4>
<p><strong>使用定制训练的嵌入模型的一个优点是它将捕获上下文相关的含义结构</strong>。例如，<em><strong>“甜”</strong></em> 的含义在软件团队的背景下与 <em><strong>烹饪</strong></em> 的背景下会有所不同。正如<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105">Lix 等人。（2022）</a>指出，在软件团队的背景下，与 <em><strong>“甜蜜”</strong></em> 最接近的术语是 <em><strong>“强烈”</strong></em>、 <em><strong>“兴奋”</strong></em> 和 <em><strong>“耶”</strong></em>。此外，就同一个单词编码不同概念（一词多义）而言，单词每种含义的概念信息都位于单词嵌入内的线性叠加（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B5">Arora et al. 2018</a>）。这意味着编码诸如 <em><strong>“Bank”</strong></em> 之类的单词的<em>n</em>维向量包含其代表的所有概念的概念信息，例如 <em><strong>河边</strong></em> 或 <em><strong>金融机构</strong></em>。通过这种方式，即使在多义词的情况下，单词的上下文相关含义也会被编码到模型中。当这些上下文相关的含义不仅不同，而且是排他的或相反的时，来自转换器的上下文相关嵌入可以为上下文中的每个单词呈现不同的单词向量。</p>
<br>
<h4 id="496-几何有助于概念人群体和组织的细粒度表示">4.9.6 几何有助于概念、人、群体和组织的细粒度表示。</h4>
<p><strong>我们认为，词嵌入模型可以在训练的语料库范围内产生人类活动概念空间的细粒度表示</strong>。<strong>这意味着，从概念空间内编码的信息中，我们可以恢复个人、群体和组织本身的细粒度表示</strong>。以我们的创新案例为例，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F1">图 1</a>描述了在说明性二维空间中这是如何实现的。学习到的概念空间将由单词或短语w表示的概念作为其最原子的分析级别。我们的限制性示例显示了在2维空间中排列的九个单词。单词 1-3 由发明人 1 使用，单词 4-6 由发明人 2 使用，单词 7-9 由发明人 3 使用。<strong>通过获取每个人的单词向量的质心向量，我们可以得出每个发明人在创新的概念空间</strong>。<strong>将这个过程提升到团队和组织级别，我们可以在发明人团队和组织的概念空间内得出独特的向量</strong>。因此，词嵌入架构不仅在概念的最原子级别上是细粒度的，而且还可以在更聚合级别上提供细粒度的表示。相对于团队多样性、组织差异化和注意力等结构的粗粒度代理，这形成了显着的测量改进，这些结构在嵌入特定概念空间时是有意义的。</p>
<br>
<p><img loading="lazy" src="img/figure-1.jpeg" alt=""  />
<strong>图 1.嵌入作为概念、人员、群体和组织的细粒度表示</strong></p>
<br>
<h4 id="497-细粒度几何减少了上下文信息的丢失">4.9.7 细粒度几何减少了上下文信息的丢失。</h4>
<p><strong>由于粗糙、粗粒度的代理指标无法承载相关信息，在实证分析和相关理论构建中就无法利用这些信息</strong>。嵌入模型的优势在于其独特的信息表征，可以携带更多的信息，信息的粒度更小，保存的信息量更多。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F2">图 2</a>使用团队多样性的示例来说明如何实现这一点。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F2">图 2(a)</a>显示了两个团队，1 和 2，每个团队要么通过熵（一种标准的、集合论多样性的理论度量（顶行））来表示，要么通过概念广度（基于底层概念的细粒度度量）来表示。团队调动的信息（底行）。团队 1 和团队 2 都有四名成员，团队 1 由两名生物化学家、一名化学家和一名分析化学家组成，团队 2 由两名生物化学家、一名海洋学家和一名计算机科学家组成。<strong>由于两个团队的团队成员类型比例相同，因此它们都被编码为具有相同的团队多样性熵度量 1.5</strong>。**然而，当考虑团队成员的概念信息时，我们发现它们是本质上不同类型的团队，团队 1 的多样性或概念范围远不如团队 2 **。这表明粗粒度的测量可能会留下未开发的有价值的上下文信息（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B187">Wolpert et al. 2014</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B33">DeDeo 2017</a>）。因此，我们应该看到更细粒度的衡量标准与相关的、理论上的绩效结果之间的联系更加紧密和一致。</p>
<br>
<p><img loading="lazy" src="img/figure-2.jpeg" alt=""  />
<strong>图 2.（在线彩色)细粒度表示可防止有价值的信息丢失</strong></p>
<br>
<p>专利数据集使我们能够通过三种构建的措施来说明这一主张。首先，集合论团队多样性度量，使用团队先前专利在专利主要类别中的分布（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B79">Huo 等人，2019</a>）。第二种替代措施使用专利子类，以便它们提供相对于第一种更细粒度的措施。第三个衡量标准依赖于团队成员先前专利在创新概念空间内的<strong>概念广度</strong>。</p>
<br>
<h4 id="498-词嵌入的局限">4.9.8 词嵌入的局限。</h4>
<p>到目前为止，我们的注意力仅限于讨论嵌入模型的结构，描述它们与概念空间的关系，并注意到它们的优点。在这里我们将说明其局限性，讨论它们的严重性、改善方式，以及何时不要用词嵌入的意外情况。我们讨论三类限制。第一个源于神经网络模型一般复杂的“黑匣子”性质，以及这带来的具体挑战，涉及输入数据的偏差，以及模型正确推理的范围，特别是那些对超出分析师背景的数据进行预训练的模型。第二个与这些模型的大小以及训练它们所需的数据量有关。第三个问题涉及词嵌入模型的具体局限性以及从脱离韵律和表达上下文的文本数据中分析含义的挑战。</p>
<p>许多学者首先担心的是，多级神经网络模型显得复杂且在统计上难以理解，<strong>经常被批评为“黑匣子”方法</strong>，无法“打开”以询问其性能背后的机制（Knight 2017，Leavitt et <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B86">al</a> . <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B95">2021</a>） 。现代神经网络词嵌入模型通常作为自监督模型实现，该模型启发式搜索单词之间的依赖关系空间以预测屏蔽词的身份。<strong>自从第一个高性能嵌入发布（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115">Mikolov 等人，2013b</a>）以来，对其黑盒性质的一些担忧已经减弱，因为数学家发现最流行的“浅”词嵌入模型（如 Word2Vec 和 FastText）获得了很大的优势</strong>。其强大功能来自于近似易于理解的矩阵分解方法的运算，例如因子分析、主成分分析和对应分析（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B100">Levy 和 Goldberg 2014</a>）。</p>
<p>“黑盒”输入输出方法带来的一个相关潜在限制是，<strong>输入的偏差将转化为输出中的偏差</strong>——用于训练嵌入的语料库的偏差将被编码在生成的单词嵌入模型中（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B14">Bolukbasi等人，2016</a>）。当模型用于现实世界的下游应用程序（例如推荐服务）时，这可能是有害的。例如，硬编码到嵌入中的种族和性别刻板印象可能会导致有偏见的建议（例如，评估是否适合招聘职位或预测财务违约的可能性），并导致不公平和不道德的决定（例如，拒绝工作或信贷） 。学者们应该根据他们的研究问题和设计，主动考虑这种负外部性是否可能，并在对人类造成伤害的可能性足够高时，偶然放弃嵌入。<strong>然而，在某种程度上，理解社区和研究背景中概念关联的本质是核心，研究人员将需要这些偏见进行分析。如果不包括它们，模型以及研究设计就会错过表征其研究背景的关键社会和文化规律。</strong></p>
<p><strong>如果分析人员对生成语料库的上下文没有清晰的了解，就会出现另一个相关的限制，这样他们最终可能会做出不适用和不相关的推论</strong>。例如，强调意义随时间变化的研究（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19">Caliskan et al. 2017</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49">Garg et al. 2018</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">Kozlowski et al. 2019</a>）的特点是词义表现出来自外源冲击的间断变化，从而重新配置了概念关联的结构。穿过空间。想一想 2005 年卡特里娜飓风之后“卡特里娜”的含义发生了怎样的变化。2009 年金融危机之后，金融术语的含义发生了重新配置，部分原因是添加了“问题资产救助计划”等许多新术语。忽略外源冲击可能会导致对后面和验证部分中描述的措施的错误解释，将其视为仅由进化产生的结果，从而导致错误的推论。这是一个特别成问题的问题，因为许多最准确的词嵌入模型都是在从网络上提取的大量文本语料库上进行预训练的。此类模型可用于引导非常小的文本数据之间的有意义距离，这是一项常见任务，但<strong>如果预训练数据是异构的，则距离可能无法反映焦点文本的概念世界</strong>。</p>
<p>接下来的两个限制必然是其嵌入优势的另一面。词嵌入模型产生的细粒度信息会带来特定研究可能或可能无法维持的成本。首先是模型尺寸。<strong>每个单词的数百个维度的细粒度信息或上下文嵌入需要比简单的字典计数或潜在狄利克雷分配主题模型更大的存储空间</strong>。这与通常用于将数据维度减少到两个或三个的因子和主成分分析形成鲜明对比。词嵌入模型使用更多维度（通常为 200-500）来更准确地预测数据的屏蔽部分。尽管如此，当前个人计算机的计算能力和存储能力现在允许训练合理大小的嵌入。</p>
<p><strong>与此相关的是，词嵌入模型需要比先前模型更多的文本才能稳健地估计概念空间</strong>。当大型语料库与研究主题相似并且可以用作理论相关文档或微调过程的初始化的代理时，可以通过迁移学习来弥补这一挑战。<strong>然而，有时相关语言在内容、目的或形式上与模型预训练的数据有很大不同，它需要独立建模，但又足够小，无法维持对嵌入模型的稳健估计。在这种情况下，使用字典计数或主题模型可能会更好，因为数据只能维持粗粒度的关联，而这些方法旨在捕获粗粒度的关联。</strong></p>
<p>最后一类通常涉及词嵌入和文本方法的特殊限制。首先，静态词嵌入本身并不处理一词多义，即一个词（例如 <em><strong>“bank”</strong></em> ）编码多个概念（例如金融机构、河边、侧向倾斜）的情况。尽管多义词的存在可能会影响后续一些指标的测量，但也存在抵消的力量。一方面，研究发现多义词的含义以相互线性叠加的方式编码在单词向量内（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B5">Arora et al. 2018</a>）。这意味着该算法通过同时考虑单词的所有含义来对单词在概念空间中的位置进行编码，从而克服了原本可能存在的严重缺陷。另一方面，上下文嵌入架构（在线附录中有更详细的描述）通过根据焦点词周围的上下文输出不同的向量来明确解决多义词的问题。每个单词不是单个向量，而是根据用途而变化的向量云。如果分析师怀疑一词多义可能是特定分析的严重问题，他们可以偶然使用上下文嵌入并规避这种担忧。</p>
<p>最后一个潜在的限制是文本方法的一般特征。只要文本数据是转录语音话语的产物（例如，欧洲央行或美联储主席演讲、政治演讲、财报电话会议、电视或电影文字记录、对话互动），语音的语调、语气和音色将没有纳入到嵌入表示中。考虑到。<strong>鉴于某些语言（例如中文）更严重地依赖语调来传达含义，这可能或多或少存在问题，具体取决于话语发生的社会背景及其表达语言</strong>。因此，在语调和语气在语料库中发挥重要作用的情况下，学者们应该讨论他们的嵌入模型选择和解释决策的后果。</p>
<br>
<h3 id="410-在研究中使用词嵌入模型的路线图">4.10 在研究中使用词嵌入模型的路线图</h3>
<p>现在我们大脑对词嵌入模型是什么、如何表示概念空间、如何训练、优点和局限性有了框架性的认知，接下来可以将它们整合到研究和理论构建的标准方法中。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#T1">表 1</a>列出了如何将嵌入模型集成到科学流程中的路线图。</p>
<ul>
<li>步骤 1-3 是研究过程中的标准步骤，包括确定一个可行且有趣的研究问题，通过在适当的实证背景下进行评估，为重要的理论问题提供信息（Weick 1989 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B179">）</a>。</li>
<li>步骤 4-9 总结了本文到目前为止对嵌入模型的讨论。</li>
<li>步骤 10 和 11 ，与下一节指标度量有关，通过标准定量和定性方法调动该度量。</li>
</ul>
<br>
<p><strong>表 1.在研究中使用词嵌入模型的路线图</strong></p>
<table>
<thead>
<tr>
<th>步骤</th>
<th>活动</th>
<th>基本原理</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. <strong>确定研究问题</strong></td>
<td>如果研究问题至关重要，请确定文本数据是否有助于在理论研究上有帮助。</td>
<td>吸引研究人员把注意力聚焦在理论问题、词嵌入构建研究构念回答问题的交叉点。</td>
</tr>
<tr>
<td>2. <strong>理论建立及相关理论构建</strong></td>
<td>确定使用哪种理论框架来解决研究问题以及通过嵌入模型来操作哪种理论构念。</td>
<td>理论构念与其词嵌入指标(构念的衡量）之间的紧密联系能够实现累积的理论发展。</td>
</tr>
<tr>
<td>3. <strong>定义经验背景</strong></td>
<td>选择适当的实证背景，在其中回答研究问题并动员理论框架和构念。</td>
<td>确保研究问题、理论框架和用构念以逻辑方式相互加强。</td>
</tr>
<tr>
<td>4.<strong>指定将用于表示经验背景的概念空间的文本数据</strong></td>
<td>描述将用于训练词嵌入模型和测量感兴趣的理论构念的文本数据的范围。 数据是否有效地涵盖了您想要得出理论结论的经验背景下的行为活动范围？</td>
<td>确保用于计算理论构造度量的词嵌入模型在逻辑上映射到并有效地代表所提出的理论框架内的实证研究背景。 文本数据的范围应该在逻辑上映射到所讲述的理论故事的范围。</td>
</tr>
<tr>
<td>5.<strong>确定文本数据的大小和范围</strong></td>
<td>数据是否足够大以学习相关概念空间的准确表示？</td>
<td>文本数据的大小将决定是否应该训练自定义嵌入，或者是否应该使用可用数据对现成的嵌入进行微调。</td>
</tr>
<tr>
<td>6. <strong>给定数据大小，要么训练独特的词嵌入模型，要么微调现有模型</strong></td>
<td>如果文本数据足够大，则训练自定义嵌入来表示感兴趣的经验上下文的概念空间。 如果文本数据不够大，请使用这些数据来微调现有的现成嵌入模型。</td>
<td>确保用于测量理论结构的嵌入模型能够有效地表示经验背景的相关概念空间。</td>
</tr>
<tr>
<td>7. <strong>如果训练独特的模型，请选择一种算法</strong></td>
<td>在连续词袋 (CBOW) 或 Skip-Gram 模型之间进行选择。</td>
<td>CBOW：在较小的数据集上可以有更好的性能。 <br>Skip-gram：可以更好地捕获语义。</td>
</tr>
<tr>
<td>8. <strong>如果训练独特的模型，确定相关参数</strong></td>
<td>选择窗口大小和维数。</td>
<td>窗口大小：标准做法是 5。较小的窗口可以更大程度地捕获语法，较大的窗口可以更大程度地捕获语义，但收益递减并增加计算成本。 维度数：标准做法是 300，超过此点后性能回报递减。</td>
</tr>
<tr>
<td>9. <strong>验证词嵌入模型</strong></td>
<td>请遵循在线附录中的验证程序。</td>
<td>确认嵌入模型准确有效地表示了经验背景的概念空间。</td>
</tr>
<tr>
<td>10. <strong>计算相关度量</strong></td>
<td>通过确定将用于实施感兴趣的理论构念的相关概念集，创建“实际措施和应用”部分中的措施之一。</td>
<td>使学者能够将该测量用于定量或定性分析。</td>
</tr>
<tr>
<td>11. <strong>在标准定性或定量方法中使用计算的度量</strong></td>
<td>对于定量分析，该度量要么成为自变量，要么成为因变量。 对于定性分析，学者可以提供解释性分析，因为它们可能适用于其他类型的档案、民族志或视听数据。</td>
<td>嵌入模型表示对生成数据的社会背景的概念空间的描述。</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="五实际措施与应用">五、实际措施与应用</h2>
<p>现在已经正式定义了 <strong>概念</strong> 和 <strong>概念空间</strong> 的含义，并说明了先前的文献如何处理概念信息,  介绍了嵌入模型表示能力的底层逻辑，并在在线附录中完成了支持这种直觉的几个验证步骤。也评论了嵌入模型给概念信息分析带来的几个优点和相关缺点。</p>
<p>在本章中，我们将介绍一些新研究， 学习他们如何用嵌入生成独特指标。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#T2">表 2</a>总结了这些指标及示例应用。</p>
<br>
<p><strong>表 2.词嵌入测量和示例应用</strong></p>
<table>
<thead>
<tr>
<th>措施</th>
<th>研究性学习</th>
<th>关键构念</th>
<th>研究问题</th>
<th>代表性调查结果</th>
<th>嵌入在这种情况下的优点</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. <strong>概念广度</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105">利克斯等人。(2022)</a></td>
<td>话语多样性——在一组给定的互动中，群体成员所传达的含义彼此分歧的程度。</td>
<td>一个群体的话语多样性如何影响其绩效？</td>
<td>高绩效团队会调整他们的共享认知以匹配任务的要求（例如，构思与协调）。</td>
<td>能够随着时间的推移以细粒度的细节和动态地追踪小组对话的概念广度，使学者们能够追踪话语多样性的新理论构造。</td>
</tr>
<tr>
<td>2.<strong>概念距离和相似度</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74">霍夫斯特拉等人。(2020)</a></td>
<td>语义遥远的科学新颖性：博士论文中新链接概念的语义距离。</td>
<td>代表性不足的群体是否更有可能产生科学创新？</td>
<td>相对于男性，女性引入了更遥远的新奇事物。 然而，这种语义上遥远的新颖性在该学科中很少受到关注。</td>
<td>能够追踪新概念组合的概念距离，使学者不仅可以研究是否做出了新组合，还可以研究这些组合的语义距离最终如何影响其影响。</td>
</tr>
<tr>
<td>3.<strong>概念X性</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94">劳森等人。(2022)</a></td>
<td>性别刻板印象：男性（而非女性）与以成就为导向的代理特征（例如自信和果断）相关的程度。</td>
<td>雇用女性首席执行官和董事会成员是否与组织对代理语言的性别使用发生变化有关？</td>
<td>当组织雇用女性首席执行官和董事会成员时，女性的语义与代理的语义变得更加一致。</td>
<td>对 22 家标准普尔 500 强公司的 43,000 多份文件（包含超过 12 亿字）进行分析，深入细致地研究女性的含义如何因聘用女性领导者而发生变化。否则这样的分析是不可能的。</td>
</tr>
<tr>
<td><strong>4.概念意义</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63">汉密尔顿等人。(2016)</a></td>
<td>词语的文化意义：词语的含义随时间变化的程度。</td>
<td>语义演化的可能驱动因素是什么？</td>
<td>跨历史时期的语义变化率与词频的逆幂律成正比。 与频率无关，具有更多含义的单词具有更高的语义变化率。</td>
<td>能够探索跨多个知识和文化领域的大型历史时期和大量文本中的语义变化。例如，他们可以详细追踪同性恋这个词的含义如何从<em>快乐</em>和<em>艳丽</em>等概念转向<em>同性恋</em>和<em>女同性恋</em>等概念。</td>
</tr>
<tr>
<td>5. <strong>文化和知识连续体中的概念立场</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">科兹洛夫斯基等人。(2019)</a></td>
<td>社会阶层标记：区分社会阶层维度的概念。</td>
<td>20世纪社会阶级的标志是如何变化的？</td>
<td>尽管社会阶级维度在历史上保持稳定，但阶级文化标记在每个维度中的定位方式却不断发生变化（例如，员工从士兵和肌肉等概念转变<em>为</em>白领<em>和</em>中产阶级<em>等</em>概念*）*。</td>
<td>能够将文化相关的概念投射到文化相关的兴趣连续体上，从而使研究人员不仅可以在单个历史时期内而且可以在其历史演变过程中了解广泛共享的社会关联。</td>
</tr>
<tr>
<td>6. <strong>概念维度</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">科兹洛夫斯基等人。(2019)</a></td>
<td>阶级的文化维度：理解社会阶级的维度（富裕、教育、修养、地位、就业、道德、性别）</td>
<td>20 世纪文化阶层的规模有多稳定？</td>
<td>20世纪，尽管发生了巨大的经济转型，阶级规模仍然非常稳定。</td>
<td>能够对阶级的多个概念维度进行实证分析，从而理解 20 世纪美国它们之间的相互关系。</td>
</tr>
</tbody>
</table>
<br>
<h3 id="51-概念广度">5.1 概念广度</h3>
<h4 id="511-指标">5.1.1 指标</h4>
<p><strong>可以测量文档中单词之间的距离来计算它们在概念空间中的分布范围</strong>。文档可以是从专利到个人电子邮件通信的任何内容。我们可以测量每个单词与其他单词的平均距离有多远。<strong>获取文档内元素的平均距离（或每个单词与文档质心之间的距离）可以衡量该文档内的「概念宽度」</strong>。例如，我们衡量每项专利的概念广度， 可以从两个简单的文档开始，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">doc1 =  [&#34;biochemistry&#34;, &#34;chemistry&#34;, &#34;analytical_chemistry&#34;]
doc2 =  [&#34;chemistry&#34;, &#34;oceanography&#34;, &#34;computer&#34;]
</code></pre></div><p>使用我们的专利嵌入模型，我们得到第一组(doc1)的平均宽度为 29，第二组（doc2）平均宽度为 47。这表明第二组在概念上比第一组更广泛。</p>
<p>当我们衡量文档集合而不是单词的概念广度时，同样的逻辑也适用。例如，我们想了解发明者团队的广度。在这种情况下，我们可以将团队中的每个发明人视为嵌入概念空间中的“文档”，参考如图<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F1">1</a> , 从下往上，依次是词概念空间、发明人概念空间、团队概念空间、组织概念空间。一个发明人团队的成员已经在涉及纳米技术、生物技术和软件的概念空间领域发表了先前的专利，那么在概念上将被认为比所有成员只发表了纳米技术专利的团队更广泛。即使所有发明人都将其公开的专利限制在一个类别内，该指标仍然会提供显着的变化。</p>
<p><img loading="lazy" src="img/figure-1.jpeg" alt=""  />
</p>
<br>
<h4 id="512--应用">5.1.2  应用</h4>
<p>这种概念广度的度量已在最近的工作中用于追踪各种理论构念。<strong><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105">利克斯等人。(2022)</a>衡量团队成员在参与软件项目的不同阶段时的 话语广度</strong>。<strong>他们能够追踪每个独特项目阶段概念参与的多样性，发现表现最好的团队有能力改变他们的认知以适应手头不断变化的任务，在提出新想法时表现出更大的话语广度，而在转换时表现出较低的广度依赖于协调的任务。这种细粒度的知识参与概念很难用以前的文本分析方法来追踪</strong>。 详细内容可阅读大邓近期推文 <a href="https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/">MS2022 | 使用语言差异性测量团队认知差异性</a> 。</p>
<p>另外，研究人员使用概念广度来追踪在线社区成员根据状态变化分配注意力的范围，发现状态和注意力广度之间存在 U 形关系（Aceves et al. 2022 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2">）</a>。这些研究人员训练了 150 个知识领域的概念空间，从而能够追踪不同知识领域的相似注意力动态，从计算机编程和数学到育儿和园艺。由于他们有能力在数百个社区的文本中大规模部署算法，因此他们能够计算出超过 2000 万成员如何在这些问答社区上发布的 2300 万个问题中分配注意力。</p>
<p>其他工作在整个语言中实施了这种方法，追踪语言在所有知识领域具有更宽或更窄的概念空间的程度（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B1">Aceves 和 Evans 2021</a>）。使用圣经、电影字幕和以多种语言编写的政治文件等文本的并行翻译（包含相同的信息但以不同的语言编码），他们能够追踪概念在不同语言中相互关联的程度存在显着差异。他们发现，尽管一些语言将不同的概念子空间紧密地联系在一起，并将不同的概念领域编织在一起，但其他语言却稀疏且更加支离破碎，更强烈地分隔了不同的意义域。然后，他们观察概念空间的语言密度如何塑造数百种语言的真实对话和维基百科文章的概念广度。</p>
<p>所有三篇论文都为不同文献的研究开辟了新的理论途径，例证了该方法的潜力。如果没有概念空间的概念及其通过嵌入模型的表示，这些新的研究途径将很难实施。</p>
<br>
<h3 id="52-概念距离和相似度">5.2 概念距离和相似度</h3>
<h4 id="521-指标">5.2.1 指标</h4>
<p>当我们的分析重点在于集合内的元素时，前面描述的概念广度构念是相关的。当我们的分析重点是不同集合之间的关系时，可以使用相同的基础度量。在这种情况下，我们将指的是概念距离或相似性，而不是概念广度。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn14">13</a>形式上，如果我们有至少两个集合，每个集合中至少有一个元素，我们可以计算这些集合之间的<strong>概念距离，作为每个集合的质心或多维平均值之间的距离</strong>。最基本的是，我们可以计算两个集合之间的概念距离，每个集合包含一个单词。这无非是衡量这些词之间的概念距离。随着元素数量和集合数量的增加，底层计算保持不变，但理论可能性的范围扩大。还可以通过训练文档嵌入模型来计算这种距离/相似性度量，该模型在嵌入空间中为每个文档分配一个向量，其权重按照单词共现的相同逻辑进行训练（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B96">Le 和 Mikolov 2014</a>），将文档本身视为文档中的另一个单词，将这些单词用作与其共现的单词。</p>
<p>通过将概念相似性与衡量专利相似性的现有技术进行比较，我们可以一睹该衡量标准的潜力。首先，研究人员可以通过查看专利授予机构使用的官方分类来追踪专利的相似性，同一类别的专利被认为比不同类别的专利更相似（Singh 和 Marx 2013，Aharonson 和<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B157">Schilling</a> 2016 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B3">）</a>）。这种方法的局限性在于分类度量是粗粒度的，并且不太可能考虑所有相关的技术特征，特别是当类别边界必然滞后于技术进化时（Thompson 和 Melanie Fox-Kean 2005，Singh<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B172">和</a>Agrawal <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B155">2011</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B7">Arts 等人，2018</a>）。其次，研究人员可以获取两项专利并测量它们之间的单词重叠（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B7">Arts et al. 2018</a>）。然而，这种方法是有限的，因为它仅适用于成对的文档，无法确定专利相对于整个知识体系的位置。</p>
<p>概念相似性解决了这些限制（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184">Whalen 等人，2020</a>）。首先，它允许我们追踪专利在相关知识空间中的精确位置，从而访问知识系统中的所有相关的细粒度信息。其次，我们能够精确量化任何专利或专利组相对于任何其他专利或专利组的位置。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn15">14</a>第三，随着新知识进入系统，知识的性质和结构不断演变，随着时间的推移重塑 <strong>概念边界</strong> 和关联。<strong>嵌入使我们能够衡量专利发布时存在的概念空间内的专利相似性，使我们能够摆脱使用滞后的、周期性偏离的类别，并可能对连续的发明概念空间强加类别差异</strong>。概念距离的所有这些优点都适用于其他知识和文化领域，在这些领域中，我们寻求测量思想、个人、群体或组织之间的距离或相似性，从而扩展现有的跨研究领域并开辟新的理论领域。</p>
<h4 id="522-应用">5.2.2 应用</h4>
<p>正如我们上面所做的那样，这种<strong>概念相似性的衡量方法最近被用来描述专利数据中的创新空间</strong>（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184">Whalen 等人，2020</a>）。研究人员使用  <strong>doc2vec</strong>  框架计算了超过 6 亿个专利对的相似度。在生成这些知识相似性度量时，作者还使用这些分数提出了有趣的辅助度量，包括可操作的度量（a）现有技术接近度——专利引用与其自身相似或不相似的现有技术的程度，（b）现有技术同质性——一项专利引用知识空间领域彼此远离的程度，(c) 影响邻近性——一项专利被与其自身相似或不相似的未来专利引用的程度，以及(d) 影响同质性——一项专利通过其前向引用与一组不同的未来专利相关的程度。</p>
<p>学者们也使用了这一衡量标准，重点关注概念距离。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18">伯奇等人。(2021)</a>使用概念距离的 <strong>doc2vec</strong> 实现来调查同行奖励是否会影响在线社区内贡献的新颖性。这里的<strong>新颖性是根据社区成员获奖前后贡献的距离来衡量的</strong>。作者发现，获奖后，奖项会导致知识空间内的新颖性减少，剥削行为增多。同样，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74">霍夫斯特拉等人。（2020）</a>使用 Word2Vec 距离度量来捕获科学论文将新颖性引入科学文献的程度，发现来自代表性不足群体的学生负责将最具新颖性引入系统。</p>
<p>其他人则利用这一措施来实施公司差异化。在发展中国家微型企业的背景下，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20">Carlson（2022）</a>使用 BERT 架构（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35">Devlin et al. 2019</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B142">Reimers and Gurevych 2020</a>）来计算其数据集中所有微型企业的成对余弦距离。通过这些距离，他们能够估计八个发展中国家的 10,000 家微型企业的差异化与收入和利润的增加相关。同样，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61">Guzman 和 Li（2023）</a>使用距离的 doc2vec 实现来使用 Crunchbase 数据来衡量初创公司的创始战略差异化。作者发现与<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20">Carlson (2022)</a>类似的结果，即差异化经验的新公司在早期融资和股权结果方面有所增加。</p>
<br>
<h3 id="53-概念x">5.3 概念X</h3>
<h4 id="531-指标">5.3.1 指标</h4>
<p>文档距离的另一个用途是追踪语料库中的任何文档与捕获感兴趣的构念X的焦点(原型）的相似性， 这样的测量将捕获任何观察的 <strong>概念X性</strong>( Conceptual X-ness)。这种测量的第一步是描述与我们寻求尽可能精确测量的结构相关的概念信息。例如，如果我们想要捕获专利与 <strong>时间</strong> 或 <strong>几何</strong> 等概念相关的程度，我们可以构建一个我们认为映射到、定义或与这些概念相关的单词列表 。对于每个列表，我们计算其质心向量 (c#27)，然后测量任何给定专利距离 <strong>时间</strong> 和 <strong>几何</strong> 概念有多远。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn16">15</a> 对于附录表 A2 中使用的专利，我们可以看到，与头颈约束装置专利相关的前两项专利更接近时间概念，正如所预期的那样光和时间在概念上交织的程度。概念性的<em>X</em>度度量可用于追踪思想、个人、团体、组织或任何其他相关聚集的组成。</p>
<h4 id="532-应用">5.3.2 应用</h4>
<p>最近在一篇论文中使用了这种方法，该论文追踪了雇用女性担任高级领导角色对女性在这些组织中意味着什么的影响（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94">Lawson 等人，2022</a>）。作者首先使用 SEC 文件和财报电话会议记录训练了 Word2Vec 嵌入。然后，他们创建并验证了一组 100 个单词来捕捉 <strong>代理概念</strong> 的含义（例如，有能力、独立、主导），并观察了内部任命高级女性领导前后 <strong>代理概念</strong> 与 <strong>女性</strong> 概念之间的距离。该组织发现，在 <strong>女性</strong> 被任命为高层管理人员之后的一段时间内，女性的含义在概念空间中更加接近于机构职位。作者使用不同的嵌入超参数和维度大小复制了他们的结果，说明了嵌入模型的鲁棒性，条件是具有捕获概念空间内语义变化的最小必要维度。</p>
<p>这里有趣的理论机会包括更深入地参与理论传统的可能性，这些理论传统在组织科学以外的领域具有影响力，但由于缺乏可行的方法来以原则性的方式量化其理论构造，因此这些理论传统仍然处于我们的领域之外。依赖文学解释。正如我们所提出的，测量 <strong>概念X性</strong> 使得扩大与理想形式（* <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B134">Plato Bloom 1968</a>）、理想类型（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B178">Weber 2011</a>）、家族相似性（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B186">Wittgenstein 2010</a>）和原型（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B147">Rosch 1973</a>）相关的理论构造的测量成为可能。以一致、有原则和可复制的方式。在这方面，概念性的<em>X</em>性代表着开放大量的认知和社会理论，以便在组织的背景下进行实证检验和扩展。</p>
<br>
<h3 id="54-语义转变和漂移">5.4 语义转变和漂移</h3>
<h4 id="541-指标">5.4.1 指标</h4>
<p>概念空间使我们能够识别术语的含义如何随着时间和空间的变化而变化。探索概念意义的一种方法是为不同的个人、公司、行业、地理位置或时间段创建独特的嵌入模型，以了解它们之间的含义有何不同（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B148">Roy 等人，2019 年</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B181">Welch 等人，2020a</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B182">b</a>）。一旦识别出相关的兴趣分歧，我们就可以采用相关的语料库（例如，专利、财报电话会议、报纸）并为数据中的每个语料库训练概念空间。<strong>在我们的专利示例中，我们可能会训练两种嵌入模型，一种是 1990 年功能性磁共振成像技术发明之前的时期，另一种是 1990 年之后的时期</strong>。然后我们可以探索与大脑和神经科学相关的概念的含义如何随着这一创新而改变。例如，在功能性磁共振成像发明之前和之后与不同大脑区域最相关的术语是什么。接下来，我们可以比较不同公司或国家的含义变化有何不同，以及这种变化的格局如何影响所涉及的公司和行业的组织和市场结果。显式动态词嵌入允许嵌入之间具有更大的可比性，但必然会忽略特殊的词和用途（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63">Hamilton et al. 2016</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B192">Zhang et al. 2016</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B190">Yao et al. 2018</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B103">Liu et al. 2020</a>）。这些算法的输出带有时间戳词向量包含特定时期的语义信息，但在历史上保持可比性。</p>
<br>
<h4 id="54-2-应用">5.4. 2 应用</h4>
<p>第一篇在社会科学背景下使用词嵌入方法的主要论文就是使用这种方法来研究意义（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63">Hamilton et al. 2016</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19">Caliskan et al. 2017</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49">Garg et al. 2018</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">Kozlowski et al. 2019</a>）。在第一篇论文中，研究人员使用四种语言的六个历史语料库，通过观察概念空间中最近的单词在过去几十年中如何变化来追踪单词含义随时间的变化（Hamilton et al. 2016 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63">）</a>。使用 Word2Vec 嵌入，他们追踪了 <strong>同性恋</strong>  概念的含义如何从 1900 年代围绕 <strong>“愚蠢”</strong>、**“甜蜜” **和 **“开朗”  **等术语的含义转变为围绕 1950 年代 <strong>“嬉闹”</strong>、 <strong>“机智”</strong> 和 <strong>“聪明”</strong> 等术语的含义，并且最终以 20 世纪 90 年代女同性恋、双性恋和同性恋等术语的含义结束。在另一篇论文中，研究人员研究了词嵌入中的刻板关联之间的关系及其与当代社会经验数据的关系（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19">Caliskan et al. 2017</a>）。例如，他们追踪了职业的性别刻板印象，发现职业具有女性意义，因为它们与女性参与劳动力市场相关。在另一项研究中，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49">Garg 等人。(2018)使用预先训练的 Google News Word2Vec 模型（ </a><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B4">Google 2013</a> ）量化了美国 100 多年历史中的性别和种族刻板印象，阐明了不同的形容词和职业如何或多或少地与不同人群（例如，男性与女性）密切相关，白人与亚洲人与西班牙裔）随着时间的推移。</p>
<p>最近通过词嵌入追踪含义的工作已经使用这种方法更深入地研究了特定的上下文。一项研究使用 19 世纪第一人称叙述的语料库来追踪黑人和白人男性和女性的交叉身份如何映射到五个社会机构，包括政治、经济、文化、家庭领域和权威关系（Nelson 2021 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B120">）</a>。<strong>举论文中的一个例子，作者测量了与“精致”概念的距离，发现它与白人女性的联系最密切，而与黑人男性的联系最少</strong>。</p>
<p>在其他工作中，研究人员利用这种方法来衡量政治领导人的 <strong>集体意向性</strong> （人们参与集体推理和行动的能力），并比较共和党和民主党领导人如何以不同的方式动员集体意向性（Kirgil and Voyer 2022 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B85">）</a>。他们通过创建复数代词（我们，我们的）、复数常量（国家名称）和复数名词（人）的复合列表来测量集体意向性。然后，使用词嵌入模型，他们找到了各州集体意向向量最接近的术语，使他们能够比较不同领导人如何不同地动员集体意向。总的来说，这些意义研究表明，就语言为我们提供了解文化的窗口而言（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B55">Goldberg et al. 2016</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165">Srivastava et al. 2018</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26">Corritore et al. 2020</a>），嵌入模型为我们提供了一种独特的表达方式透过那扇窗户看到的照片。</p>
<br>
<h3 id="55-文化和知识连续性中的概念地位">5.5 文化和知识连续性中的概念地位</h3>
<h4 id="551-指标">5.5.1 指标</h4>
<p>另一种新颖的测量方法可以通过追踪概念相对于感兴趣的概念维度的位置来创建。如前所述，嵌入模型可用于解决类比推理任务，例如**“国王”-“男人”+“女人”=“女王”<strong>。 该架构可用于定义概念空间内任何感兴趣的维度。<strong>在国王-王后的例子中，性别维度通过“男人”-“女人”和“国王”-“女王”向量进行操作。</strong><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">科兹洛夫斯基等人。（2019）</a>详细介绍了如何在概念空间内构建此类维度。首先，研究人员需要确定感兴趣的维度。对于我们这里的例子，我们将把不同的概念投射到男性-女性性别维度上。为此</strong>，我们首先确定定义性别维度的相关术语**。这里我们使用集合 [&lsquo;man&rsquo;, &lsquo;him&rsquo;, &lsquo;he&rsquo;, &lsquo;male&rsquo;, &lsquo;men&rsquo;] 和 [&lsquo;woman&rsquo;, &lsquo;her&rsquo;, &lsquo;she&rsquo;, &lsquo;female&rsquo;, &lsquo;women&rsquo;]。 <strong>然后我们计算不同概念在这个男性-女性概念轴(维度)上的正交投影。</strong> 在线附录中的图 A4 将每个概念投射到 <strong>男性-女性概念轴</strong>。 更消极的预测表明与女性气质的关联更强，而更积极的预测表明与男性气质的相关性相当。如图 A4 所示，这些预测与关于这些概念的性别状态的一般直觉一致，使我们能够明确说明每个概念相对于其他概念在这个维度中的位置。正如预期的那样，<strong>军事</strong> 和 <strong>农业</strong> 与 <strong>男性气质</strong> 的联系最为密切，而 <strong>卫生棉条</strong> 和 <strong>口红则</strong> 与 女性气质的联系最为密切。按照这个程序，学者们现在可以测量任何概念在任何感兴趣的维度和任何文本丰富的时空背景中的位置。此外，不同语言的语料库可以独立训练和对齐，或者同时训练和对齐，以方便国际分析（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B81">Johnson et al. 2017</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116">Milbauer et al. 2021</a>）。</p>
<p><strong>双极概念维度的投影方法可以进一步扩展到锚定具有多种含义的低维子空间，其中单词和概念可以被绘制并理解为这些含义的混合</strong>。这可以通过理论上选择“原型”的集合来执行，即具有已知且广泛共享含义的极值点，并在这些极值锚定义的子空间中绘制所有相关单词或概念。[例如，在对一个新的基于信息技术的创业企业进行分类时，人们可能会问它在 Uber、亚马逊、谷歌或比特币所刻画的空间中适合什么位置(Breiman 1994，Eugster 2012，Damle 和 Sun 2017）。</p>
<br>
<h4 id="552-应用">5.5.2 应用</h4>
<p>这项措施的制定和运用是为了研究 20 世纪和 21 世纪社会阶层的演变（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">Kozlowski 等人，2019</a>）。他们研究了根据 20 世纪出版的数百万本书的文本训练的嵌入，按照上述程序操作了阶级的维度，试图了解社会阶级的底层维度在 20 世纪是如何变化的。为此，他们提出了以下理论上的<strong>概念轴(维度)</strong>：富裕程度（富人与穷人）、教育程度（受过教育与未受教育）、修养（有教养与未受教育）、地位（有声望与无声望）、道德（善与恶）、就业（雇主-雇员）和性别（男人-女人），分别嵌入 20 世纪的每个十年。然后，他们可以在这些维度上投射不同类别的概念，例如音乐风格、体育和职业，以了解这些概念在本世纪的过程中如何演变和发展。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B6">研究人员应用这种方法来研究健康、道德（ Arseniev-Koehler et al. 2022</a>）、政治意识形态（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B171">Taylor and Stoltz 2021</a>）和地位（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B129">Peng et al. 2021</a> ）等背景下的其他类型的文化关联。</p>
<p>研究人员不仅将概念投射到这些概念轴(维度)上，而且将整个文档投射到这些维度上，从而推动了测量的可能性（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B170">Taylor 和 Stoltz 2020</a>）。此外，尽管以前的措施依赖于研究人员指定感兴趣的连续体的相关维度，但最近的工作已经转向自动识别这些连续体（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116">Milbauer et al. 2021</a>）。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116">Milbauer 等人</a>利用 Reddit 社区的内容。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116">（2021）</a>创建了一个无监督的程序来识别社区中的多个意识形态极点，使他们能够超越静态的左右意识形态维度，发现现代话语中发挥作用的许多两极分化和意识形态差异的轴。人们可以想象在许多组织环境中使用这种方法来识别团队、小组、单位或部门之间存在的许多潜在冲突来源。</p>
<br>
<h3 id="56-概念维度">5.6 概念维度</h3>
<p>之前，我们讨论了研究人员如何调查关键术语在相关文化维度上的位置，描述概念的位置在性别维度上的差异。然而，这并不是概念轴(维度)的唯一用途，因为概念空间还允许我们测量和理解相关维度本身如何相互关联。该措施的扩展是使用空间内的编码维度并将它们相互比较。例如，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">科兹洛夫斯基等人。（2019）</a>利用他们既定的阶级维度来追踪整个 20 世纪每个维度如何与其他维度相关，例如，表明随着世纪的发展，富裕与教育的关系变得更加密切，而与教育的关系无关。栽培。通过这种方式，组织学者可以理解相关维度之间的关系在相关概念空间中可能有何不同。例如，学者可以研究不同文化维度在组织或行业内部和之间紧密或松散联系的程度。</p>
<p><br><br></p>
<h2 id="六讨论">六、讨论</h2>
<p>最后，我们简要讨论了一些利用嵌入模型进行思考的新兴方法，然后讨论了我们认为理论、方法论和组织的有价值的机会，这些机会源于将这些模型理解为概念空间的细粒度表示。这个讨论必然是说明性的，但暗示了现在这些精致的意义模型的可操作性的广泛可能性。</p>
<h3 id="61-词嵌入方法的富有成果的扩展">6.1 词嵌入方法的富有成果的扩展</h3>
<p>词嵌入的底层计算架构最近经历了扩展，可以在与之前讨论的不同方向上动员组织研究。我们简要提到三个，并在在线附录中提供更详细的描述。首先，概念和语言的层次结构在“直线”、欧几里得几何中很难得到体现，需要许多难以理解的维度来用标准嵌入来捕获。然而，层次结构可以用负弯曲双曲嵌入来原生表示（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B90">Krioukov et al. 2010</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B126">Papadopoulos et al. 2012</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B22">Chamberlain et al. 2017</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B121">Nickel and Kiela 2017</a>），为探索复杂现代的交叉层次结构提供了新的测量可能性。组织。例如，将公司名称嵌入双曲空间中将能够直接发现典型的“中心公司”，并在商业新闻语料库中与所有其他公司进行比较。额外的双曲维度将揭示子层次结构，反映商业评论员所持有的概念和比较价值的不同维度。</p>
<p>其次，模型语言的深度学习方法为词嵌入增加了关键的上下文敏感性。考虑像 BERT ( <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35">Devlin et al. 2019</a> ) 和 GPT 系列模型 ( <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B140">Radford et al. 2019</a> ) 这样的大规模模型，它们使用“注意力”的神经网络机制来识别影响焦点词含义的上下文词 ( <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B176">Vaswani ) et al. 2017</a>），组装成一个称为转换器的架构，可以将问题转换为答案，将文本转换为翻译，将请求转换为响应。这种模型产生的内容可以被描述为上下文嵌入，这样每个单词不是由单个向量表示，而是由向量云表示，每个向量代表不同上下文中的该单词。“google”上下文中的“Apple”与“orange”上下文中的“apple”具有不同的值。这些模型极大地提高了预测能力，并进一步扩展了我们对概念空间进行精确建模的能力，但代价是复杂性和计算量更大。</p>
<p>最后，嵌入架构可以扩展到在序列或更高维上下文中排列的任意符号集。例如，图像已被用来衡量抽象艺术图像的新颖性和创造力（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B10">Banerjee and Ingram 2022</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B11">Banerjee and Kaplan 2022</a>），分析警察预约照片（大头照），并识别与司法拒绝保释相关的先前未概念化的紧急特征听证会（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B107">Ludwig 和 Mullainathan 2022</a>）。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B101">音乐（ Liang et al. 2020</a>）、音频剪辑（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B70">Hershey et al. 2017</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B189">Xie and Virtanen 2019</a>）和视频（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B191">Zellers et al. 2021</a> ）的多维空间是使用audio2vec（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B168">Taglisacchi et al. 2020</a>）等工具构建的。 、signal2vec（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B118">Nalmpantis 和 Vrakas 2019</a>）和 video2vec（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B62">Habibian 等人 2017</a>），为组织学者接触代表组织生活视听体验的新型媒体打开了大门。</p>
<p>最近对双曲线、上下文、图像和音频嵌入的扩展表明，嵌入模型的底层计算框架的持续改进和扩展将继续下去，为组织科学中持续的实证、测量和理论创新奠定了基础。</p>
<br>
<h3 id="62-词嵌入和组织理论">6.2 词嵌入和组织理论</h3>
<p>在理论层面上，将嵌入模型理解为概念空间的有原则的、细粒度的表示有可能刺激新的理论发展并完善现有理论。例如，意义研究中的经典陈述影响了文学理论和文化社会学等其他领域，但未能在组织科学中站稳脚跟。自20 世纪初德索绪尔 ( <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B32">de Saussure 1986</a> )的著作带来语言学的结构转向以来，许多人都试图将意义在组织和社会生活中的作用理论化。列维-斯特劳斯汇集了来自全球各地的多样而广泛的民族志，以向世界文化所特有的表面混乱提出深层的文化秩序，并认为复杂的意义是从有意义的元素的结合中产生的（列维-斯特劳斯 2016 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B99">）</a>。福柯理论化了话语和权力如何紧密相连，权力和知识如何以自我强化的联盟结合在一起（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B46">Foucault 2012</a>）。<em>布迪厄将惯习</em>的概念阐述为“持久的、可互换的处置系统，倾向于充当结构结构的结构化结构，即作为实践的生成和结构的原则”（Bourdieu 1977，第72页<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B16">）</a>。尽管这些理论很有吸引力，但迄今为止它们只能进行松散且间接的测试。如果没有可靠的实证立足点，他们就永远无法在管理和组织理论中取得突出地位。然而，概念空间的实证操作化现在使得这些文化理论基础著作的参与和扩展变得容易处理，其中的许多结构现在可以辩护地测量。嵌入模型将使这些理论与管理和组织理论相关。</p>
<p>我们还希望嵌入模型能够对现有理论框架进行更深入的研究和锐化。一组能够受益的文献是那些与知识相关的文献。鉴于组织学者可以获得的大部分知识都被编码在语言的符号概念系统中，现在可以通过更多可用的文本数据源来获取知识，并且可以通过嵌入模型的概念空间来表示。材料科学领域的最新工作已经使用此类模型来有效预测未来的知识发现，比科学家提出的知识发现早几十年（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B173">Tshitoyan 等人，2019 年</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B161">Sourati 和 Evans，2021 年</a>）。其他工作表明，这些发现可以推广到生物和物理科学（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B154">Shi 和 Evans 2023</a>）。概念空间的明确表示可以对整个社会系统中知识的特征和结构进行详细的调查。一方面，这些模型就像望远镜一样，打开了知识的天空，使其大规模结构变得可见，以供研究、理论发展和完善。另一方面，这些模型充当显微镜，使我们能够更深入地观察构成更大知识系统的意义原子结构。测量方面的这一进步将丰富对定义人类和组织经验的大型多维知识系统中的机制的测试。它还将使我们能够递归地评估管理和组织奖学金的知识，从而刺激创新。</p>
<br>
<h3 id="63-词嵌入和实证研究">6.3 词嵌入和实证研究</h3>
<p>在<em>实证层面</em>，词嵌入模型可以提高组织科学不同领域的测量保真度，从而在实证结果与理论主张和框架之间实现更好的映射。我们用团队和群体内部多样性研究的例子来说明这一点。据说，不同群体所获得的许多好处是由于群体中的个人代表问题和解决方案的方式不同而产生的（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B75">Hong 和 Page 2004</a>）。由具有不同方法的个人组成的小组将更好地执行各种任务，因为他们将拥有更广泛的知识、观点和可供借鉴的信息资源（Cox et al. 1991，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B27">Williams</a> and <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B185">O&rsquo;Reilly 1998</a>）。然而，由于测量困难，对团队多样性的研究很少测量问题和解决方案空间的不同概念。相反，它假设解决问题的团队成员的身份多样性（人口、文化、种族或经验）与其功能多样性（团队成员如何代表和解决问题）之间存在联系（Nisbett 和 Ross 1980，Hong<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B122">和</a>Page <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B75">2004</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175">van Dijk 等人，2017</a>）。</p>
<p>由于缺乏高保真方法来访问团队成员在问题和解决方案的概念空间中的位置，因此通常假定身份和功能多样性之间存在联系。用于操作研究的身份多样性和用于理论化的功能多样性之间脱节的一个重要后果是，虽然理论积极使用功能多样性的思想和术语（从根本上讲是几何和高维的），但测试依赖于集合-与身份成员资格相关的理论概念。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175">我们预测，这将解释团队多样性文献（ van Dijk et al. 2017</a> ）结果中的大部分歧义，因为研究设计忽视了功能多样性和身份多样性之间的同源性。然而，诸如概念广度之类的衡量标准可以阐明这一理论交叉点上的悬而未决的问题。我们现在可以指定（1）团队的基本概念广度，以及（2）这种基本广度可能驱动结果的程度。解决这些问题可以为许多分析层面的研究提供信息，从个人和团队的成功（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B164">Srikanth 等人，2016 年</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175">van Dijk 等人，2017 年</a>）到公司和行业绩效（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B144">Roberson 等人，2017 年</a>）。我们希望我们的插图能够激发在组织研究领域生成细粒度意义测量的新可能性。</p>
<br>
<h3 id="64-组织内部的词嵌入">6.4 组织内部的词嵌入</h3>
<p><strong>最后，我们认为词嵌入方法将对我们研究的组织产生影响</strong>。我们说明了在劳动力市场背景下潜在的嵌入必须塑造组织行为。从招聘到工作设计，从培训到晋升，人力资源管理的一个核心挑战是有效地将个人与组织内的角色、工作、情况和任务相匹配（Weller et al. 2019 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B183">）</a>。随着比赛质量的提高，各种绩效指标也会提高，包括工作满意度（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B8">Ashforth 和 Saks 1996</a>）、个人生产力（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B125">Paauwe 2009</a>）和组织绩效（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B38">Dyer 和 Reeves 1995</a>）。有效匹配的一个问题是不同维度的匹配的重要性程度。在一家公司中，技能可能最为重要，而在其他公司中，技能可能是文化契合度、态度、技能和经验的相互作用。由于嵌入模型捕获了所有这些维度，管理者可以为每个相关维度嵌入不同的原型描述，同时还嵌入个人资料和其他相关通信（例如电子邮件、松弛消息等），以衡量每个人与每个相关维度之间的匹配接近度。这样做可以让管理者更好地识别高维匹配及其对员工、社区和公司绩效的影响。</p>
<p>嵌入模型旨在为人力资源的宏观管理提供新的视角（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B183">Weller et al. 2019</a>）。来自大型组织的相关信息存储在人力资源经理、一线经理、员工、同事和外部招聘人员中。然而，无法集中访问这些信息。通过嵌入，组织可以从所有数字面包屑的文本（电子邮件、聊天、工作描述、正式报告、绩效管理记录等）构建概念空间。这样做并使用相似性分析将使公司能够绘制和了解相关人力资本的位置位于公司对面。管理人员可以利用这些系统来准确了解任何员工的概念职位与任何给定的公司要求的差距有多大。这不仅可以为招聘、雇用、员工流动和流动等流程提供信息，还可以为培训、社交、工作设计和公司重组提供信息。因此，在劳动力市场和组织适应的背景下，嵌入模型可以产生有用的创新。人们可以想象许多其他组织实践和结构可以从这些模型及其测量可能性中受益，包括产品设计、市场分析和战略生成。</p>
<p><br><br></p>
<h2 id="结论">结论</h2>
<p>我们同意<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65">Hanan等人的观点。（2019</a>，第 2 页）当他们观察到，考虑到概念和分类对几乎所有人类行为和社会互动的中心地位，人们对概念如何运作的关注如此之少，这是多么令人惊讶。现代组织内部及其周围进行的许多活动都需要概念信息的激活和传播。当一个人解决新问题、提出新想法或与他人合作时，就会发生这种情况。从围绕饮水机的良性闲聊到重新配置全球资本主义秩序或将人类登陆火星，概念及其所嵌入的概念空间发挥着核心、关键的作用。</p>
<p>正如本文所示，我们现在拥有一系列重要的工具，可以为广泛而深入的理论想象和实证研究打开<strong>概念世界</strong>和<strong>概念空间</strong>。我们希望本文能够激发对嵌入可以提供信息的大量问题和理论的学术探索。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>MS2022 | 使用语言差异性测量团队认知差异性</title>
      <link>https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/</link>
      <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/</guid>
      <description>&lt;p&gt;词嵌入在经管中的应用很多，但大多数是训练词嵌入模型，依据词嵌入构建或扩展词典。 今天我们将分享一篇用词嵌入测量团队认知多样性。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/paper-cover-discursive-diversity.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h2 id=&#34;一研究&#34;&gt;一、研究&lt;/h2&gt;
&lt;p&gt;Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &amp;ldquo;&lt;strong&gt;Aligning differences: Discursive diversity and team performance.&lt;/strong&gt;&amp;rdquo; &lt;em&gt;Management Science&lt;/em&gt; 68, no. 11 (2022): 8430-8448.&lt;/p&gt;
&lt;h3 id=&#34;11-摘要&#34;&gt;1.1 摘要&lt;/h3&gt;
&lt;p&gt;团队中的认知多样性如何影响其绩效？先前的研究表明，团队的认知多样性存在绩效权衡：多样性团队在创造力和创新方面表现出色，但在协调行动方面则有困难。基于团队认知不是静态的，而是动态互动产生的观点，我们引入了 &lt;strong&gt;话语多样性&lt;/strong&gt; 的概念，这是团队认知多样性的一种表现，反映了在一组互动中团队成员传达的含义在多大程度上相互不同。&lt;strong&gt;我们提出，高绩效团队是那些具有调节共享认知以适应不断变化的任务要求的集体能力的团队：在进行构思任务时，它们表现出更高的话语多样性，在执行协调任务时，表现出较低的话语多样性&lt;/strong&gt;。我们进一步认为，表现出一致调节的团队——即，在成员对不断变化的任务要求的个人语义变化中团队层面方差较低的团队——更有可能取得成功，而不是由成员之间存在不一致的调节。我们利用 &lt;strong&gt;计算语言学&lt;/strong&gt; 工具来衡量话语多样性，并借助一组新型纵向数据，包括117个在线平台 &lt;a href=&#34;http://www.gigster.com&#34;&gt;www.gigster.com&lt;/a&gt; 上的远程软件开发团队的团内电子通信和绩效结果，得出了对我们理论的支持。我们的研究结果表明，团队认知多样性的绩效权衡并非不可避免：团队可以通过将话语多样性水平与任务要求相匹配以及在进行这些调整时使成员保持一致来应对这一权衡。&lt;/p&gt;
&lt;h3 id=&#34;12-创新点&#34;&gt;1.2 创新点&lt;/h3&gt;
&lt;p&gt;这篇论文的创新点主要包括以下几个方面：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;研究了团队内部的差异对团队绩效的影响&lt;/strong&gt;：该论文通过分析团队成员之间的差异，探讨了这些差异对团队绩效的影响。这一研究角度对于理解团队内部动态和绩效提升具有重要意义。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;引入了阶段性的话语差异概念&lt;/strong&gt;：论文提出了阶段性的话语差异概念，即团队成员在不同阶段的沟通中所表现出的差异。这一概念有助于更好地理解团队内部沟通的动态过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;探讨了团队内部沟通差异的调节作用&lt;/strong&gt;：论文研究了团队内部沟通差异与团队绩效之间的关系，并发现团队内部沟通差异在不同阶段对团队绩效的影响存在差异。这一发现为团队管理和绩效提升提供了重要的启示。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;结合了多个学科领域的理论和方法&lt;/strong&gt;：该论文综合运用了心理学、经济学和组织学等多个学科领域的理论和方法，从多个角度深入研究了团队内部差异和绩效之间的关系，为相关领域的研究提供了新的视角和方法。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二文献梳理&#34;&gt;二、文献梳理&lt;/h2&gt;
&lt;h3 id=&#34;21-认知多样性&#34;&gt;2.1 认知多样性&lt;/h3&gt;
&lt;p&gt;认知多样性(cognitive diversity)对团队绩效的影响是一个长期存在的问题。以往的研究表明，团队的认知多样性存在绩效权衡：多样性团队在创造力和创新方面表现出色，但在协调行动方面存在困难。然而，&lt;strong&gt;最近的研究提出了一种新的观点，即团队的「认知多样性」可以通过调节团队的「共享认知」来实现绩效的平衡。这意味着团队可以根据任务要求调整其认知多样性的水平，以在创造性任务和协调任务之间找到平衡点&lt;/strong&gt;。高绩效团队具备调节团队认知的能力，使其能够在创造性任务中展现较高的认知多样性，在协调任务中展现较低的认知多样性。这种能力使团队能够在创新和执行之间找到平衡，从而提高绩效。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-话语多样性&#34;&gt;2.2 话语多样性&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;话语多样性(discursive diversity) 是指团队成员在交流和讨论中表达的观点、意见和想法的多样性程度。它反映了团队成员在思考和表达上的差异程度。话语多样性可以包括词汇选择、句子结构、表达方式等方面的差异&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;话语多样性对团队的协调行动有影响。在协调任务中，团队成员需要相互理解、协调行动，达成共识并共同努力实现共同目标。如果团队成员的话语多样性过高，意味着他们在表达观点和意见时存在较大的差异，这可能导致沟通困难、理解不一致和冲突的产生，从而影响团队的协调行动。&lt;/p&gt;
&lt;p&gt;因此，在协调任务中，团队成员的话语多样性应该相对较低，以便更好地理解和协调彼此的行动。相反，在创意和思考任务中，话语多样性可以促进团队成员的创新和思考，帮助他们从不同的角度和观点来解决问题，从而提高团队的创造力和创新能力。总之，话语多样性在团队中起着重要的作用，它需要根据任务的性质和要求进行调节，以实现团队的协调行动和创新能力。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-两者关系&#34;&gt;2.3 两者关系&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;在这篇论文中，话语多样性被用来衡量认知多样性&lt;/strong&gt;。研究人员使用计算语言学的工具来推导出话语多样性的度量，并将其应用于团队的电子沟通数据中。他们认为，团队的话语多样性可以反映成员之间的认知多样性，即在思维方式、知识和技能等方面的差异程度。通过分析团队的话语多样性，研究人员试图探索团队在不同任务要求下的表现，并研究团队如何调节共享认知以适应任务需求的变化。因此，话语多样性被视为一种衡量团队认知多样性的指标。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三数据及方法&#34;&gt;三、数据及方法&lt;/h2&gt;
&lt;h3 id=&#34;31-数据&#34;&gt;3.1 数据&lt;/h3&gt;
&lt;p&gt;Gigster(&lt;a href=&#34;http://www.gigster.com&#34;&gt;www.gigster.com&lt;/a&gt;),是一个在线平台， 自由软件开发人员可以在该平台上为个人和企业客户制作按需软件。该平台将个人自由职业开发人员组装成由团队领导领导的临时团队，并将他们分配给需要复杂、相互依赖的长期项目。该平台上的自由职业者分布在全球各地，从事从移动到网络应用程序开发的各种项目。这些项目通常是知识密集型的，需要高水平的创造力、技术问题解决能力和人际协调能力。软件项目规模巨大，成本从数万美元到数十万美元不等（极端情况下可达一百万美元以上）。&lt;/p&gt;
&lt;p&gt;我们的数据集由 117 个团队组成，代表 421 个不同的个体（36% 为女性），时间跨度从 2015 年初到 2017 年底。一个典型的团队有 5 名成员，其中包括一名项目经理；至少一名后端、前端或“全栈”工程师；设计师；和用户界面专家。根据项目类型，团队有时还包括作家、自然语言处理工程师和其他类型的专业人士。在我们数据中的团队中，项目平均持续 159 天（中位数：150 天），并分为平均持续两周的里程碑阶段（平均：14 天；中位数：14 天）。要加入该平台，专业人士必须通过旨在验证其专业知识的各种技术面试。平均而言，单个团队的成员代表 3.6 个国家/地区（中位数：3 个）。在我们的样本中，42% 的人将其原籍国列为北美。另外 13% 来自亚洲，其次是 12% 来自欧洲。其余 23% 居住在拉丁美洲、非洲和世界其他地区。&lt;/p&gt;
&lt;p&gt;由于地理位置分散且缺乏实体办公空间，团队成员几乎完全通过名为 Slack 的在线即时通讯工具进行沟通。我们可以访问整个团队的 Slack 档案——超过 800,000 条消息。每条消息都带有时间戳并可归因（通过匿名标识符）其作者。团队在整个生命周期中平均在公共渠道中交换 1,873 条 Slack 消息（中位数：1,220 条）。我们对 Gigster 的高级领导和团队领导进行了非正式采访，他们一致表示团队沟通几乎完全通过 Slack 进行。一位高级领导描述了其中的原因：“几乎所有团队对话都发生在 Slack 上。这是一个有用的工具，因为我们运营全球团队，而且 Slack 允许在一个平台内进行实时和异步通信。它还允许轻松地共享项目文件。” 多位知情人士强调，团队成员始终依赖 Slack，而不是其他工具，因为“一切都在一个地方”对于促进团队协作非常重要。知情人士还表示，团队成员有动力使用 Slack，因为它提供了团队流程和事件的透明档案，可用于对一些罕见的争议案例进行分类。&lt;/p&gt;
&lt;p&gt;除了 Slack 消息之外，我们还可以获得有关团队成员特征（职能角色、性别和原籍国）的数据，以及团队在实现各个项目里程碑方面的整体绩效。这些数据共同构成了团队内部动态和结果的丰富且连续的历史记录。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-计算话语多样性&#34;&gt;3.2 计算话语多样性&lt;/h3&gt;
&lt;p&gt;之前的工作表明，词嵌入模型对于捕获单词之间的语义关系特别有用， 例如，(2018) 证明，根据应用于 20 世纪出版的英语书籍的词嵌入模型推断出的不同职业的语义性别关联与这些职业的历史性别构成相对应。同样，科兹洛夫斯基等人(2019)说明了不同的生活方式活动如何与阶级、种族和性别认同相关。因此，词嵌入为语言中包含的众多意义维度提供了全面且有意义的见解，而这是以前的方法无法捕获的。因此，本论文使用词嵌入模型开发了话语多样性度量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我们首先对 Slack 数据进行预处理，并使用 Word2Vec（连续词袋词嵌入模型的流行实现）来训练词嵌入模型。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;按照标准实践，窗口大小设置为10， 维度设置成200来训练word2vec模型（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/mnsc.2021.4274#B54&#34;&gt;Mikolov 等人，2013&lt;/a&gt;）。从这个训练过程中，我们获得了语料库中每个单词的一个 200 维坐标向量，表示该单词在语义空间中的位置。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;窗口大小&lt;/strong&gt;: 每个词的上下文范围。 人阅读书籍，一般视野只有十来个词，逐行阅读。 跟人类似， 在计算机中训练词嵌入模型时候，数据不是一次性灌入习得词语的向量，而是像人一样是有上下文范围的，这个范围叫做窗口。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如前所述，嵌入空间的维度表示训练语料库中语言使用的潜在特征。尽管这些维度本身不具有定性可解释的含义，但这些维度是提供信息的，因为具有更相似含义的单词彼此更接近。&lt;strong&gt;下图(a)是从聊天消息构建词嵌入向量的过程&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-compute-discursive-diverisity-with-embeddings.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;**使用词嵌入模型，我们就可以把词语、每句话、某人某时期的话、某团队某时期的话、所有团队所有的话，通过一定的计算，都表征为200维的向量。**上图 (b) 从聊天消息构建团队话语多样性得分(Discursive Diversity)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;通过公式1，计算出两个人差异性。通过公式2， 计算出团队话语多样性。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-formular-1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-formular-2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;假设团队只有三个人， 低话语多样性 与 高话语多样性， 分别对应下图的左侧和右侧。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-low-and-high-examples-of-discursive-diversity.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四python伪码&#34;&gt;四、Python伪码&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;discursive_diversity_score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# wv: 词嵌入模型; gensim.models.keyedvectors.KeyedVectors&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# words: 一个时间窗口内的词语列表&lt;/span&gt;
    
    &lt;span class=&#34;c1&#34;&gt;# 计算词嵌入向量的平均值&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;embedding_vectors&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;centroid&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embedding_vectors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;c1&#34;&gt;# 计算词嵌入向量之间的余弦相似度&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pairwise_distances&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;centroid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linalg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;centroid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linalg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embedding&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embedding_vectors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    
    &lt;span class=&#34;c1&#34;&gt;# 计算语言多样性得分&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;diversity_score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pairwise_distances&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;diversity_score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;函数 &lt;em&gt;&lt;strong&gt;discursive_diversity_score&lt;/strong&gt;&lt;/em&gt; 已内置到 cntext2.x 版本中。 对cntext2.x 感兴趣，可阅读 &lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/&#34;&gt;文本分析库cntext2.x使用手册&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;五词嵌入应用文献&#34;&gt;五、词嵌入应用文献&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;# 使用词嵌入技术构建词典
[1]胡楠, 薛付婧 and 王昊楠, 2021. **管理者短视主义影响企业长期投资吗———基于文本分析和机器学习**. *管理世界*, *37*(5), pp.139-156.    
[2]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, **Measuring Corporate Culture Using Machine Learning**, *The Review of Financial Studies*,2020


# 使用词嵌入测量偏见(刻板印象)、认知
[3]Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &amp;#34;**Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.**&amp;#34; _Proceedings of the National Academy of Sciences_ 119, no. 9 (2022): e2026443119.
[4]Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &amp;#34;**Aligning differences: Discursive diversity and team performance.**&amp;#34; *Management Science* 68, no. 11 (2022): 8430-8448.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;六广告&#34;&gt;六、广告&lt;/h2&gt;
&lt;p&gt;对词嵌入技术在经管中的应用，大邓也一直持续追踪。 课程 &lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;&lt;strong&gt;Python实证指标与文本分析&lt;/strong&gt;&lt;/a&gt;中有词嵌入相关知识点和代码，对该类文本分析技术感兴趣同学，欢迎报名课程。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>词嵌入在经管中的应用很多，但大多数是训练词嵌入模型，依据词嵌入构建或扩展词典。 今天我们将分享一篇用词嵌入测量团队认知多样性。</p>
<p><br><br></p>
<p><img loading="lazy" src="img/paper-cover-discursive-diversity.png" alt=""  />
</p>
<h2 id="一研究">一、研究</h2>
<p>Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &ldquo;<strong>Aligning differences: Discursive diversity and team performance.</strong>&rdquo; <em>Management Science</em> 68, no. 11 (2022): 8430-8448.</p>
<h3 id="11-摘要">1.1 摘要</h3>
<p>团队中的认知多样性如何影响其绩效？先前的研究表明，团队的认知多样性存在绩效权衡：多样性团队在创造力和创新方面表现出色，但在协调行动方面则有困难。基于团队认知不是静态的，而是动态互动产生的观点，我们引入了 <strong>话语多样性</strong> 的概念，这是团队认知多样性的一种表现，反映了在一组互动中团队成员传达的含义在多大程度上相互不同。<strong>我们提出，高绩效团队是那些具有调节共享认知以适应不断变化的任务要求的集体能力的团队：在进行构思任务时，它们表现出更高的话语多样性，在执行协调任务时，表现出较低的话语多样性</strong>。我们进一步认为，表现出一致调节的团队——即，在成员对不断变化的任务要求的个人语义变化中团队层面方差较低的团队——更有可能取得成功，而不是由成员之间存在不一致的调节。我们利用 <strong>计算语言学</strong> 工具来衡量话语多样性，并借助一组新型纵向数据，包括117个在线平台 <a href="http://www.gigster.com">www.gigster.com</a> 上的远程软件开发团队的团内电子通信和绩效结果，得出了对我们理论的支持。我们的研究结果表明，团队认知多样性的绩效权衡并非不可避免：团队可以通过将话语多样性水平与任务要求相匹配以及在进行这些调整时使成员保持一致来应对这一权衡。</p>
<h3 id="12-创新点">1.2 创新点</h3>
<p>这篇论文的创新点主要包括以下几个方面：</p>
<ol>
<li>
<p><strong>研究了团队内部的差异对团队绩效的影响</strong>：该论文通过分析团队成员之间的差异，探讨了这些差异对团队绩效的影响。这一研究角度对于理解团队内部动态和绩效提升具有重要意义。</p>
</li>
<li>
<p><strong>引入了阶段性的话语差异概念</strong>：论文提出了阶段性的话语差异概念，即团队成员在不同阶段的沟通中所表现出的差异。这一概念有助于更好地理解团队内部沟通的动态过程。</p>
</li>
<li>
<p><strong>探讨了团队内部沟通差异的调节作用</strong>：论文研究了团队内部沟通差异与团队绩效之间的关系，并发现团队内部沟通差异在不同阶段对团队绩效的影响存在差异。这一发现为团队管理和绩效提升提供了重要的启示。</p>
</li>
<li>
<p><strong>结合了多个学科领域的理论和方法</strong>：该论文综合运用了心理学、经济学和组织学等多个学科领域的理论和方法，从多个角度深入研究了团队内部差异和绩效之间的关系，为相关领域的研究提供了新的视角和方法。</p>
</li>
</ol>
<p><br><br></p>
<h2 id="二文献梳理">二、文献梳理</h2>
<h3 id="21-认知多样性">2.1 认知多样性</h3>
<p>认知多样性(cognitive diversity)对团队绩效的影响是一个长期存在的问题。以往的研究表明，团队的认知多样性存在绩效权衡：多样性团队在创造力和创新方面表现出色，但在协调行动方面存在困难。然而，<strong>最近的研究提出了一种新的观点，即团队的「认知多样性」可以通过调节团队的「共享认知」来实现绩效的平衡。这意味着团队可以根据任务要求调整其认知多样性的水平，以在创造性任务和协调任务之间找到平衡点</strong>。高绩效团队具备调节团队认知的能力，使其能够在创造性任务中展现较高的认知多样性，在协调任务中展现较低的认知多样性。这种能力使团队能够在创新和执行之间找到平衡，从而提高绩效。</p>
<br>
<h3 id="22-话语多样性">2.2 话语多样性</h3>
<p><strong>话语多样性(discursive diversity) 是指团队成员在交流和讨论中表达的观点、意见和想法的多样性程度。它反映了团队成员在思考和表达上的差异程度。话语多样性可以包括词汇选择、句子结构、表达方式等方面的差异</strong>。</p>
<p>话语多样性对团队的协调行动有影响。在协调任务中，团队成员需要相互理解、协调行动，达成共识并共同努力实现共同目标。如果团队成员的话语多样性过高，意味着他们在表达观点和意见时存在较大的差异，这可能导致沟通困难、理解不一致和冲突的产生，从而影响团队的协调行动。</p>
<p>因此，在协调任务中，团队成员的话语多样性应该相对较低，以便更好地理解和协调彼此的行动。相反，在创意和思考任务中，话语多样性可以促进团队成员的创新和思考，帮助他们从不同的角度和观点来解决问题，从而提高团队的创造力和创新能力。总之，话语多样性在团队中起着重要的作用，它需要根据任务的性质和要求进行调节，以实现团队的协调行动和创新能力。</p>
<br>
<h3 id="23-两者关系">2.3 两者关系</h3>
<p><strong>在这篇论文中，话语多样性被用来衡量认知多样性</strong>。研究人员使用计算语言学的工具来推导出话语多样性的度量，并将其应用于团队的电子沟通数据中。他们认为，团队的话语多样性可以反映成员之间的认知多样性，即在思维方式、知识和技能等方面的差异程度。通过分析团队的话语多样性，研究人员试图探索团队在不同任务要求下的表现，并研究团队如何调节共享认知以适应任务需求的变化。因此，话语多样性被视为一种衡量团队认知多样性的指标。</p>
<p><br><br></p>
<h2 id="三数据及方法">三、数据及方法</h2>
<h3 id="31-数据">3.1 数据</h3>
<p>Gigster(<a href="http://www.gigster.com">www.gigster.com</a>),是一个在线平台， 自由软件开发人员可以在该平台上为个人和企业客户制作按需软件。该平台将个人自由职业开发人员组装成由团队领导领导的临时团队，并将他们分配给需要复杂、相互依赖的长期项目。该平台上的自由职业者分布在全球各地，从事从移动到网络应用程序开发的各种项目。这些项目通常是知识密集型的，需要高水平的创造力、技术问题解决能力和人际协调能力。软件项目规模巨大，成本从数万美元到数十万美元不等（极端情况下可达一百万美元以上）。</p>
<p>我们的数据集由 117 个团队组成，代表 421 个不同的个体（36% 为女性），时间跨度从 2015 年初到 2017 年底。一个典型的团队有 5 名成员，其中包括一名项目经理；至少一名后端、前端或“全栈”工程师；设计师；和用户界面专家。根据项目类型，团队有时还包括作家、自然语言处理工程师和其他类型的专业人士。在我们数据中的团队中，项目平均持续 159 天（中位数：150 天），并分为平均持续两周的里程碑阶段（平均：14 天；中位数：14 天）。要加入该平台，专业人士必须通过旨在验证其专业知识的各种技术面试。平均而言，单个团队的成员代表 3.6 个国家/地区（中位数：3 个）。在我们的样本中，42% 的人将其原籍国列为北美。另外 13% 来自亚洲，其次是 12% 来自欧洲。其余 23% 居住在拉丁美洲、非洲和世界其他地区。</p>
<p>由于地理位置分散且缺乏实体办公空间，团队成员几乎完全通过名为 Slack 的在线即时通讯工具进行沟通。我们可以访问整个团队的 Slack 档案——超过 800,000 条消息。每条消息都带有时间戳并可归因（通过匿名标识符）其作者。团队在整个生命周期中平均在公共渠道中交换 1,873 条 Slack 消息（中位数：1,220 条）。我们对 Gigster 的高级领导和团队领导进行了非正式采访，他们一致表示团队沟通几乎完全通过 Slack 进行。一位高级领导描述了其中的原因：“几乎所有团队对话都发生在 Slack 上。这是一个有用的工具，因为我们运营全球团队，而且 Slack 允许在一个平台内进行实时和异步通信。它还允许轻松地共享项目文件。” 多位知情人士强调，团队成员始终依赖 Slack，而不是其他工具，因为“一切都在一个地方”对于促进团队协作非常重要。知情人士还表示，团队成员有动力使用 Slack，因为它提供了团队流程和事件的透明档案，可用于对一些罕见的争议案例进行分类。</p>
<p>除了 Slack 消息之外，我们还可以获得有关团队成员特征（职能角色、性别和原籍国）的数据，以及团队在实现各个项目里程碑方面的整体绩效。这些数据共同构成了团队内部动态和结果的丰富且连续的历史记录。</p>
<br>
<h3 id="32-计算话语多样性">3.2 计算话语多样性</h3>
<p>之前的工作表明，词嵌入模型对于捕获单词之间的语义关系特别有用， 例如，(2018) 证明，根据应用于 20 世纪出版的英语书籍的词嵌入模型推断出的不同职业的语义性别关联与这些职业的历史性别构成相对应。同样，科兹洛夫斯基等人(2019)说明了不同的生活方式活动如何与阶级、种族和性别认同相关。因此，词嵌入为语言中包含的众多意义维度提供了全面且有意义的见解，而这是以前的方法无法捕获的。因此，本论文使用词嵌入模型开发了话语多样性度量。</p>
<p><strong>我们首先对 Slack 数据进行预处理，并使用 Word2Vec（连续词袋词嵌入模型的流行实现）来训练词嵌入模型。</strong></p>
<p>按照标准实践，窗口大小设置为10， 维度设置成200来训练word2vec模型（<a href="https://pubsonline.informs.org/doi/full/10.1287/mnsc.2021.4274#B54">Mikolov 等人，2013</a>）。从这个训练过程中，我们获得了语料库中每个单词的一个 200 维坐标向量，表示该单词在语义空间中的位置。</p>
<blockquote>
<p><strong>窗口大小</strong>: 每个词的上下文范围。 人阅读书籍，一般视野只有十来个词，逐行阅读。 跟人类似， 在计算机中训练词嵌入模型时候，数据不是一次性灌入习得词语的向量，而是像人一样是有上下文范围的，这个范围叫做窗口。</p>
</blockquote>
<p>如前所述，嵌入空间的维度表示训练语料库中语言使用的潜在特征。尽管这些维度本身不具有定性可解释的含义，但这些维度是提供信息的，因为具有更相似含义的单词彼此更接近。<strong>下图(a)是从聊天消息构建词嵌入向量的过程</strong>:</p>
<p><img loading="lazy" src="img/01-compute-discursive-diverisity-with-embeddings.jpeg" alt=""  />
</p>
<p>**使用词嵌入模型，我们就可以把词语、每句话、某人某时期的话、某团队某时期的话、所有团队所有的话，通过一定的计算，都表征为200维的向量。**上图 (b) 从聊天消息构建团队话语多样性得分(Discursive Diversity)。</p>
<p><strong>通过公式1，计算出两个人差异性。通过公式2， 计算出团队话语多样性。</strong></p>
<p><img loading="lazy" src="img/03-formular-1.png" alt=""  />
</p>
<p><img loading="lazy" src="img/04-formular-2.png" alt=""  />
</p>
<br>
<p>假设团队只有三个人， 低话语多样性 与 高话语多样性， 分别对应下图的左侧和右侧。</p>
<p><img loading="lazy" src="img/02-low-and-high-examples-of-discursive-diversity.jpeg" alt=""  />
</p>
<p><br><br></p>
<h2 id="四python伪码">四、Python伪码</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">discursive_diversity_score</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
    <span class="c1"># wv: 词嵌入模型; gensim.models.keyedvectors.KeyedVectors</span>
    <span class="c1"># words: 一个时间窗口内的词语列表</span>
    
    <span class="c1"># 计算词嵌入向量的平均值</span>
    <span class="n">embedding_vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">wv</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
    <span class="n">centroid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">embedding_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># 计算词嵌入向量之间的余弦相似度</span>
    <span class="n">pairwise_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">centroid</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">centroid</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">embedding</span><span class="p">))</span> <span class="k">for</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="n">embedding_vectors</span><span class="p">]</span>
    
    <span class="c1"># 计算语言多样性得分</span>
    <span class="n">diversity_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pairwise_distances</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">diversity_score</span>
</code></pre></div><p>函数 <em><strong>discursive_diversity_score</strong></em> 已内置到 cntext2.x 版本中。 对cntext2.x 感兴趣，可阅读 <a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">文本分析库cntext2.x使用手册</a></p>
<br>
<br>
<h2 id="五词嵌入应用文献">五、词嵌入应用文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># 使用词嵌入技术构建词典
[1]胡楠, 薛付婧 and 王昊楠, 2021. **管理者短视主义影响企业长期投资吗———基于文本分析和机器学习**. *管理世界*, *37*(5), pp.139-156.    
[2]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, **Measuring Corporate Culture Using Machine Learning**, *The Review of Financial Studies*,2020


# 使用词嵌入测量偏见(刻板印象)、认知
[3]Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &#34;**Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.**&#34; _Proceedings of the National Academy of Sciences_ 119, no. 9 (2022): e2026443119.
[4]Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &#34;**Aligning differences: Discursive diversity and team performance.**&#34; *Management Science* 68, no. 11 (2022): 8430-8448.
</code></pre></div><p><br><br></p>
<h2 id="六广告">六、广告</h2>
<p>对词嵌入技术在经管中的应用，大邓也一直持续追踪。 课程 <a href="https://textdata.cn/blog/management_python_course/"><strong>Python实证指标与文本分析</strong></a>中有词嵌入相关知识点和代码，对该类文本分析技术感兴趣同学，欢迎报名课程。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>案例代码 | 使用正则表达式判别微博用户mbti类型</title>
      <link>https://textdata.cn/blog/2023-10-30-raw-mbti-users/</link>
      <pubDate>Mon, 30 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-10-30-raw-mbti-users/</guid>
      <description>&lt;p&gt;使用Python爬虫采集「微博搜索」中含mbti信息的推文， 使用正则表达式判别用户mbti类型。 相比实验室做实验或者发调查问卷，这种方式收集到的用户类别是非常自然且真实的。今日爬虫不是今日主题，就不做分享了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;code.ipynb&#34;&gt;&lt;strong&gt;点击下载code.ipynb&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;mbti_test.csv&#34;&gt;&lt;strong&gt;点击下载mbti_test.csv&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/weibo.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#采集自微博搜索中含mbti类型的推文&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mbti_test.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#剔除content列中的nan数据&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;正则练习&#34;&gt;正则练习&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;提取含有mbti的记录&lt;/li&gt;
&lt;li&gt;提取出含mbti类型出现的前后5个字符的文本(前5个字符，后5个字符， 含mbti本身， 窗体最长的长度是14)&lt;/li&gt;
&lt;li&gt;识别出含mbti的记录中对应的mbti类型， 未识别的标记为&amp;quot;未识别&amp;quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一-提取含有mbti的记录&#34;&gt;一、 提取含有mbti的记录&lt;/h2&gt;
&lt;p&gt;实现方法有两种&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;pd.Series.str.contains(regex_pattern)&lt;/li&gt;
&lt;li&gt;定义一个正则处理函数regex_func， 使用pd.Series.apply(regex_func)&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;正则表达式含义&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mbtis&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;[infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj]&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[ 和 ]&lt;/code&gt;：这是字符类（character class）的起始和结束标记，表示要匹配方括号内的任何字符。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj&lt;/code&gt;：这是一个字符类内的字符集合，用于匹配MBTI类型词汇。每个MBTI类型词汇都以竖线 | 分隔，表示“或”的关系。这意味着正则表达式会匹配其中任何一个MBTI类型词汇。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;+&lt;/code&gt;：这是一个量词，表示匹配前面的字符集合（MBTI类型词汇）一次或多次。它使正则表达式可以匹配包含一个或多个MBTI类型词汇的文本。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mbtis&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;[infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj]&amp;#39;&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mbtis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;0       True
1       True
2       True
3       True
4       True
       ...  
495    False
496    False
497    False
498    False
499    False
Name: content, Length: 497, dtype: bool
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;has_mbti&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;mbtis&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;[infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj]+&amp;#39;&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mbtis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
    
    
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;has_mbti&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;0       True
1       True
2       True
3       True
4       True
       ...  
495    False
496    False
497     True
498    False
499     True
Name: content, Length: 497, dtype: bool
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#将结果存储到df中&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;hasMBTI&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;has_mbti&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二mbti前后内容&#34;&gt;二、mbti前后内容&lt;/h2&gt;
&lt;p&gt;提取出含mbti类型出现的前后5个字符的文本(前5个字符，后5个字符， 含mbti本身， 窗体最长的长度是14)。&lt;/p&gt;
&lt;p&gt;这样后续的分析任务，就可以通过查看mbti字眼前后出现的字符，来更新正则表达式。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;正则表达式含义&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;mbti_win = &amp;#34;(.{0,5}(?:infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj).{0,5})&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;(&lt;/code&gt; 和 &lt;code&gt;)&lt;/code&gt;这些括号用于将整个匹配结果捕获为一个分组&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.{0,5}&lt;/code&gt; ：这是一个量词，表示匹配前面的字符（.表示匹配任意字符）零次到五次。这部分用于匹配前面的文本，确保最多匹配前面的五个字符。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(?:infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj)&lt;/code&gt;：这是一个非捕获分组，用于将多个MBTI类型词汇用 | 连接起来，表示匹配其中任何一个。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.{0,5}&lt;/code&gt; ：这部分同样是一个量词，表示匹配后面的字符，确保最多匹配后面的五个字符。&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;mbti_window&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#识别mbti的正则表达式 &lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;mbti_win&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;(.{0,5}(?:infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj).{0,5})&amp;#34;&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mbti_win&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;未识别&amp;#34;&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;MBTI_win&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mbti_window&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/3.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三识别mbti类型&#34;&gt;三、识别mbti类型&lt;/h2&gt;
&lt;p&gt;刚刚的代码比较粗糙，只能判断文本中是否有mbti信息，但并不能判断该用户是否为某种mbti类型。&lt;/p&gt;
&lt;p&gt;微博文本中，只有 &lt;code&gt;//@&lt;/code&gt; 前字符内容是微博用户所写内容。为了识别用户的mbti类型，可以先将我们看到的表达方式列举一下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;``我是[mbti]&lt;/li&gt;
&lt;li&gt;&lt;code&gt;自己是[mbti]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;从[mbti]变为[mbti]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;一直是[mbti]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[mbti]我&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;本[mbti]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&amp;hellip;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以基于此设计一个严格的正则表达式，能识别到的记录，肯定能判断该用户的mbti类型。 未识别到的标记为 &amp;ldquo;未识别&amp;rdquo;&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;正则表达式含义&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mbti_regex&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[我|自己|变成|一直|是|本]*(infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj)[我|俺|本|自己]*&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;[我|自己|变成|一直|是|本]*&lt;/code&gt;：这部分是一个字符集合，用于匹配前面的字符（关键词）。方括号 &lt;code&gt;[...]&lt;/code&gt; 表示字符类，其中的字符是可选的，并且 * 表示匹配零次或多次。这意味着它可以匹配零个或多个出现在方括号中的字符，例如可以匹配&amp;quot;我&amp;quot;、&amp;ldquo;自己&amp;rdquo;、&amp;ldquo;变成&amp;rdquo;、&amp;ldquo;一直&amp;rdquo;、&amp;ldquo;是&amp;rdquo;、&amp;ldquo;本&amp;quot;等这些关键词。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj)&lt;/code&gt; ：这是一个分组，其中包含了MBTI类型词汇，用竖线 &lt;code&gt;|&lt;/code&gt; 分隔，表示&amp;quot;或&amp;quot;的关系。这部分用于匹配任意一个MBTI类型词汇。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[我|俺|本|自己]*&lt;/code&gt; ：这部分与第1部分类似，是一个字符集合，用于匹配后面的字符（关键词）。同样，方括号 &lt;code&gt;[...]&lt;/code&gt; 表示字符类，其中的字符是可选的，并且 &lt;code&gt;*&lt;/code&gt; 表示匹配零次或多次。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;identify_mbti&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;//@&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;new_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;//@&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;new_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;#识别mbti的正则表达式 &lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;mbti_regex&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[我|自己|变成|一直|是|本]*(infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj)[我|俺|本|自己]*&amp;#34;&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mbti_regex&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;未识别&amp;#34;&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#mbti类型&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;MBTI_Cat&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;identify_mbti&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/4.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#各类型记录数&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;MBTI_Cat&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value_counts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;MBTI_Cat
未识别     297
infp     35
isfj     20
enfp     18
intp     17
isfp     16
intj     14
entp     12
entj     11
infj     11
enfj      8
estj      8
istp      8
istj      7
esfp      6
estp      5
esfj      4
Name: count, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>使用Python爬虫采集「微博搜索」中含mbti信息的推文， 使用正则表达式判别用户mbti类型。 相比实验室做实验或者发调查问卷，这种方式收集到的用户类别是非常自然且真实的。今日爬虫不是今日主题，就不做分享了。</p>
<ul>
<li><a href="code.ipynb"><strong>点击下载code.ipynb</strong></a></li>
<li><a href="mbti_test.csv"><strong>点击下载mbti_test.csv</strong></a></li>
</ul>
<br>
<p><img loading="lazy" src="img/weibo.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#采集自微博搜索中含mbti类型的推文</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;mbti_test.csv&#39;</span><span class="p">)</span>
<span class="c1">#剔除content列中的nan数据</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">])</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/1.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="正则练习">正则练习</h2>
<ol>
<li>提取含有mbti的记录</li>
<li>提取出含mbti类型出现的前后5个字符的文本(前5个字符，后5个字符， 含mbti本身， 窗体最长的长度是14)</li>
<li>识别出含mbti的记录中对应的mbti类型， 未识别的标记为&quot;未识别&quot;</li>
</ol>
<p><br><br></p>
<h2 id="一-提取含有mbti的记录">一、 提取含有mbti的记录</h2>
<p>实现方法有两种</p>
<ol>
<li>pd.Series.str.contains(regex_pattern)</li>
<li>定义一个正则处理函数regex_func， 使用pd.Series.apply(regex_func)</li>
</ol>
<br>
<p><strong>正则表达式含义</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mbtis</span> <span class="o">=</span> <span class="s1">&#39;[infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj]&#39;</span>
</code></pre></div><ul>
<li>
<p><code>[ 和 ]</code>：这是字符类（character class）的起始和结束标记，表示要匹配方括号内的任何字符。</p>
</li>
<li>
<p><code>infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj</code>：这是一个字符类内的字符集合，用于匹配MBTI类型词汇。每个MBTI类型词汇都以竖线 | 分隔，表示“或”的关系。这意味着正则表达式会匹配其中任何一个MBTI类型词汇。</p>
</li>
<li>
<p><code>+</code>：这是一个量词，表示匹配前面的字符集合（MBTI类型词汇）一次或多次。它使正则表达式可以匹配包含一个或多个MBTI类型词汇的文本。</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mbtis</span> <span class="o">=</span> <span class="s1">&#39;[infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj]&#39;</span>

<span class="n">df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">mbtis</span><span class="p">)</span>
</code></pre></div><pre><code>0       True
1       True
2       True
3       True
4       True
       ...  
495    False
496    False
497    False
498    False
499    False
Name: content, Length: 497, dtype: bool
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>


<span class="k">def</span> <span class="nf">has_mbti</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">mbtis</span> <span class="o">=</span> <span class="s1">&#39;[infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj]+&#39;</span>

    <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">mbtis</span><span class="p">,</span> <span class="n">text</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    
    
<span class="n">df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">has_mbti</span><span class="p">)</span>
</code></pre></div><pre><code>0       True
1       True
2       True
3       True
4       True
       ...  
495    False
496    False
497     True
498    False
499     True
Name: content, Length: 497, dtype: bool
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#将结果存储到df中</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;hasMBTI&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">has_mbti</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二mbti前后内容">二、mbti前后内容</h2>
<p>提取出含mbti类型出现的前后5个字符的文本(前5个字符，后5个字符， 含mbti本身， 窗体最长的长度是14)。</p>
<p>这样后续的分析任务，就可以通过查看mbti字眼前后出现的字符，来更新正则表达式。</p>
<br>
<p><strong>正则表达式含义</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">mbti_win = &#34;(.{0,5}(?:infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj).{0,5})&#34;
</code></pre></div><ul>
<li><code>(</code> 和 <code>)</code>这些括号用于将整个匹配结果捕获为一个分组</li>
<li><code>.{0,5}</code> ：这是一个量词，表示匹配前面的字符（.表示匹配任意字符）零次到五次。这部分用于匹配前面的文本，确保最多匹配前面的五个字符。</li>
<li><code>(?:infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj)</code>：这是一个非捕获分组，用于将多个MBTI类型词汇用 | 连接起来，表示匹配其中任何一个。</li>
<li><code>.{0,5}</code> ：这部分同样是一个量词，表示匹配后面的字符，确保最多匹配后面的五个字符。</li>
</ul>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">mbti_window</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#识别mbti的正则表达式 </span>
    <span class="n">mbti_win</span> <span class="o">=</span> <span class="s2">&#34;(.{0,5}(?:infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj).{0,5})&#34;</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">mbti_win</span><span class="p">,</span> <span class="n">text</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&#34;未识别&#34;</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;MBTI_win&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">mbti_window</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/3.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三识别mbti类型">三、识别mbti类型</h2>
<p>刚刚的代码比较粗糙，只能判断文本中是否有mbti信息，但并不能判断该用户是否为某种mbti类型。</p>
<p>微博文本中，只有 <code>//@</code> 前字符内容是微博用户所写内容。为了识别用户的mbti类型，可以先将我们看到的表达方式列举一下</p>
<ul>
<li>``我是[mbti]</li>
<li><code>自己是[mbti]</code></li>
<li><code>从[mbti]变为[mbti]</code></li>
<li><code>一直是[mbti]</code></li>
<li><code>[mbti]我</code></li>
<li><code>本[mbti]</code></li>
<li>&hellip;&hellip;</li>
</ul>
<p>可以基于此设计一个严格的正则表达式，能识别到的记录，肯定能判断该用户的mbti类型。 未识别到的标记为 &ldquo;未识别&rdquo;</p>
<br>
<p><strong>正则表达式含义</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mbti_regex</span> <span class="o">=</span> <span class="s2">&#34;[我|自己|变成|一直|是|本]*(infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj)[我|俺|本|自己]*&#34;</span>
</code></pre></div><ul>
<li><code>[我|自己|变成|一直|是|本]*</code>：这部分是一个字符集合，用于匹配前面的字符（关键词）。方括号 <code>[...]</code> 表示字符类，其中的字符是可选的，并且 * 表示匹配零次或多次。这意味着它可以匹配零个或多个出现在方括号中的字符，例如可以匹配&quot;我&quot;、&ldquo;自己&rdquo;、&ldquo;变成&rdquo;、&ldquo;一直&rdquo;、&ldquo;是&rdquo;、&ldquo;本&quot;等这些关键词。</li>
<li><code>(infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj)</code> ：这是一个分组，其中包含了MBTI类型词汇，用竖线 <code>|</code> 分隔，表示&quot;或&quot;的关系。这部分用于匹配任意一个MBTI类型词汇。</li>
<li><code>[我|俺|本|自己]*</code> ：这部分与第1部分类似，是一个字符集合，用于匹配后面的字符（关键词）。同样，方括号 <code>[...]</code> 表示字符类，其中的字符是可选的，并且 <code>*</code> 表示匹配零次或多次。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python">
<span class="k">def</span> <span class="nf">identify_mbti</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">if</span> <span class="s1">&#39;//@&#39;</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
        <span class="n">new_text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;//@&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">new_text</span> <span class="o">=</span> <span class="n">text</span>

    <span class="c1">#识别mbti的正则表达式 </span>
    <span class="n">mbti_regex</span> <span class="o">=</span> <span class="s2">&#34;[我|自己|变成|一直|是|本]*(infj|entp|intp|intj|entj|enfj|infp|enfp|isfp|istp|isfj|istj|estp|esfp|estj|esfj)[我|俺|本|自己]*&#34;</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">mbti_regex</span><span class="p">,</span> <span class="n">text</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&#34;未识别&#34;</span>

<span class="c1">#mbti类型</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;MBTI_Cat&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">identify_mbti</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/4.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#各类型记录数</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;MBTI_Cat&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><pre><code>MBTI_Cat
未识别     297
infp     35
isfj     20
enfp     18
intp     17
isfp     16
intj     14
entp     12
entj     11
infj     11
enfj      8
estj      8
istp      8
istj      7
esfp      6
estp      5
esfj      4
Name: count, dtype: int64
</code></pre>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>GTE中文通用文本向量表示模型</title>
      <link>https://textdata.cn/blog/2023-10-27-nlp_gte_sentence-embedding_chinese/</link>
      <pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-10-27-nlp_gte_sentence-embedding_chinese/</guid>
      <description>&lt;h2 id=&#34;gte中文通用文本表示模型&#34;&gt;GTE中文通用文本表示模型&lt;/h2&gt;
&lt;p&gt;文本表示是自然语言处理(NLP)领域的核心问题, 其在很多NLP、信息检索的下游任务中发挥着非常重要的作用。近几年, 随着深度学习的发展，尤其是预训练语言模型的出现极大的推动了文本表示技术的效果, 基于预训练语言模型的文本表示模型在学术研究数据、工业实际应用中都明显优于传统的基于统计模型(词袋法、TF-IDF) 或者浅层神经网络的文本表示模型。这里, 我们主要关注基于预训练语言模型的文本表示。GTE项目地址&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;https://modelscope.cn/models/damo/nlp_gte_sentence-embedding_chinese-small/summary
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;文本表示示例, 输入一个句子, 输入一个固定维度的连续向量:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入: &lt;code&gt;吃完海鲜可以喝牛奶吗?&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;输出: &lt;code&gt;[0.27162,-0.66159,0.33031,0.24121,0.46122,...]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;文本的向量表示通常可以用于&lt;strong&gt;文本聚类&lt;/strong&gt;、&lt;strong&gt;文本相似度计算&lt;/strong&gt;等下游任务中。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二文本表示模型&#34;&gt;二、文本表示模型&lt;/h2&gt;
&lt;p&gt;基于监督数据训练的文本表示模型通常采用Dual Encoder框架, 如下图所示。在Dual Encoder框架中, Query和Document文本通过预训练语言模型编码后, 通常采用预训练语言模型[CLS]位置的向量作为最终的文本向量表示。基于标注数据的标签, 通过计算query-document之间的cosine距离度量两者之间的相关性。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/repo.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;GTE-zh模型使用&lt;a href=&#34;https://arxiv.org/abs/2205.12035&#34;&gt;retromae&lt;/a&gt;初始化训练模型，之后利用两阶段训练方法训练模型：第一阶段利用大规模弱弱监督文本对数据训练模型，第二阶段利用高质量精标文本对数据以及挖掘的难负样本数据训练模型。具体训练方法请参考论文&lt;a href=&#34;https://arxiv.org/abs/2308.03281&#34;&gt;Towards General Text Embeddings with Multi-stage Contrastive Learning&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二使用方式和范围&#34;&gt;二、使用方式和范围&lt;/h2&gt;
&lt;p&gt;使用方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接推理, 对给定文本计算其对应的文本向量表示，向量维度512&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用范围:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;本模型可以使用在通用领域的文本向量表示及其下游应用场景, 包括 &lt;strong&gt;两文档间文本相似度计算&lt;/strong&gt;、&lt;strong&gt;query&amp;amp;多doc候选的相似度排序&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;21-如何使用&#34;&gt;2.1 如何使用&lt;/h3&gt;
&lt;p&gt;在ModelScope框架上，提供输入文本(默认最长文本长度为128)，即可以通过简单的Pipeline调用来使用GTE文本向量表示模型。ModelScope封装了统一的接口对外提供&lt;strong&gt;单文档向量表示&lt;/strong&gt;、&lt;strong&gt;双文档文本相似度&lt;/strong&gt;、&lt;strong&gt;多候选相似度计算&lt;/strong&gt;等功能&lt;/p&gt;
&lt;h3 id=&#34;22-安装&#34;&gt;2.2 安装&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install torch
pip3 install transformers
pip3 install modelscope
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三代码示例&#34;&gt;三、代码示例&lt;/h2&gt;
&lt;p&gt;为方便实验，选择体积较小的模型文件 &lt;strong&gt;damo/nlp_gte_sentence-embedding_chinese-small&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;modelscope.models&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Model&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;modelscope.pipelines&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pipeline&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;modelscope.utils.constant&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tasks&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;model_id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;damo/nlp_gte_sentence-embedding_chinese-small&amp;#34;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#57M&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;model_id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;damo/nlp_gte_sentence-embedding_chinese-large&amp;#34;&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#621M&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pipeline_se&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pipeline&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Tasks&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sentence_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                       &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 当输入包含“soure_sentence”与“sentences_to_compare”时，会输出source_sentence中首个句子与sentences_to_compare中每个句子的向量表示，以及source_sentence中首个句子与sentences_to_compare中每个句子的相似度。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;source_sentence&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;吃完海鲜可以喝牛奶吗?&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;sentences_to_compare&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
            &lt;span class=&#34;s2&#34;&gt;&amp;#34;不可以，早晨喝牛奶不科学&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;s2&#34;&gt;&amp;#34;吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;s2&#34;&gt;&amp;#34;吃海鲜是不能同时喝牛奶吃水果，这个至少间隔6小时以上才可以。&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;s2&#34;&gt;&amp;#34;吃海鲜是不可以吃柠檬的因为其中的维生素C会和海鲜中的矿物质形成砷&amp;#34;&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pipeline_se&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;text_embedding&amp;#39;: array([[-0.03317244, -0.0419106 , -0.03626636, ..., -0.0132677 ,
         -0.02028614, -0.01542077],
        [-0.04563809, -0.06220782, -0.03775004, ...,  0.01267119,
         -0.01111769, -0.03390383],
        [-0.02073098, -0.04639562, -0.04818704, ..., -0.00754705,
         -0.00731624, -0.02740852],
        [-0.00037597, -0.05922904, -0.0459275 , ..., -0.00697823,
         -0.02154762, -0.02951157],
        [-0.00491675, -0.02552056, -0.03427778, ..., -0.00760836,
         -0.00404084, -0.0509829 ]], dtype=float32),
 &amp;#39;scores&amp;#39;: [0.8542333245277405,
  0.9613471031188965,
  0.947378396987915,
  0.8620702028274536]}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;默认向量维度512,  两个向量做内积距离计算得到score&lt;/strong&gt;
&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 当输入仅含有soure_sentence时，会输出source_sentence中每个句子的向量表示。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;inputs2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;source_sentence&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
            &lt;span class=&#34;s2&#34;&gt;&amp;#34;不可以，早晨喝牛奶不科学&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;s2&#34;&gt;&amp;#34;吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;s2&#34;&gt;&amp;#34;吃海鲜是不能同时喝牛奶吃水果，这个至少间隔6小时以上才可以。&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;s2&#34;&gt;&amp;#34;吃海鲜是不可以吃柠檬的因为其中的维生素C会和海鲜中的矿物质形成砷&amp;#34;&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pipeline_se&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inputs2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;text_embedding&amp;#39;: array([[-0.04563809, -0.06220782, -0.03775004, ...,  0.01267119,
        -0.01111769, -0.03390383],
       [-0.02073098, -0.04639562, -0.04818704, ..., -0.00754705,
        -0.00731624, -0.02740852],
       [-0.00037597, -0.05922904, -0.0459275 , ..., -0.00697823,
        -0.02154762, -0.02951157],
       [-0.00491675, -0.02552056, -0.03427778, ..., -0.00760836,
        -0.00404084, -0.0509829 ]], dtype=float32), &amp;#39;scores&amp;#39;: []}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="gte中文通用文本表示模型">GTE中文通用文本表示模型</h2>
<p>文本表示是自然语言处理(NLP)领域的核心问题, 其在很多NLP、信息检索的下游任务中发挥着非常重要的作用。近几年, 随着深度学习的发展，尤其是预训练语言模型的出现极大的推动了文本表示技术的效果, 基于预训练语言模型的文本表示模型在学术研究数据、工业实际应用中都明显优于传统的基于统计模型(词袋法、TF-IDF) 或者浅层神经网络的文本表示模型。这里, 我们主要关注基于预训练语言模型的文本表示。GTE项目地址</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">https://modelscope.cn/models/damo/nlp_gte_sentence-embedding_chinese-small/summary
</code></pre></div><br>
<p>文本表示示例, 输入一个句子, 输入一个固定维度的连续向量:</p>
<ul>
<li>输入: <code>吃完海鲜可以喝牛奶吗?</code></li>
<li>输出: <code>[0.27162,-0.66159,0.33031,0.24121,0.46122,...]</code></li>
</ul>
<p>文本的向量表示通常可以用于<strong>文本聚类</strong>、<strong>文本相似度计算</strong>等下游任务中。</p>
<p><br><br></p>
<h2 id="二文本表示模型">二、文本表示模型</h2>
<p>基于监督数据训练的文本表示模型通常采用Dual Encoder框架, 如下图所示。在Dual Encoder框架中, Query和Document文本通过预训练语言模型编码后, 通常采用预训练语言模型[CLS]位置的向量作为最终的文本向量表示。基于标注数据的标签, 通过计算query-document之间的cosine距离度量两者之间的相关性。</p>
<p><img loading="lazy" src="img/repo.png" alt=""  />
</p>
<p>GTE-zh模型使用<a href="https://arxiv.org/abs/2205.12035">retromae</a>初始化训练模型，之后利用两阶段训练方法训练模型：第一阶段利用大规模弱弱监督文本对数据训练模型，第二阶段利用高质量精标文本对数据以及挖掘的难负样本数据训练模型。具体训练方法请参考论文<a href="https://arxiv.org/abs/2308.03281">Towards General Text Embeddings with Multi-stage Contrastive Learning</a>。</p>
<p><br><br></p>
<h2 id="二使用方式和范围">二、使用方式和范围</h2>
<p>使用方式:</p>
<ul>
<li>直接推理, 对给定文本计算其对应的文本向量表示，向量维度512</li>
</ul>
<p>使用范围:</p>
<ul>
<li>本模型可以使用在通用领域的文本向量表示及其下游应用场景, 包括 <strong>两文档间文本相似度计算</strong>、<strong>query&amp;多doc候选的相似度排序</strong></li>
</ul>
<h3 id="21-如何使用">2.1 如何使用</h3>
<p>在ModelScope框架上，提供输入文本(默认最长文本长度为128)，即可以通过简单的Pipeline调用来使用GTE文本向量表示模型。ModelScope封装了统一的接口对外提供<strong>单文档向量表示</strong>、<strong>双文档文本相似度</strong>、<strong>多候选相似度计算</strong>等功能</p>
<h3 id="22-安装">2.2 安装</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install torch
pip3 install transformers
pip3 install modelscope
</code></pre></div><p><br><br></p>
<h2 id="三代码示例">三、代码示例</h2>
<p>为方便实验，选择体积较小的模型文件 <strong>damo/nlp_gte_sentence-embedding_chinese-small</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">modelscope.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">modelscope.pipelines</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span> <span class="nn">modelscope.utils.constant</span> <span class="kn">import</span> <span class="n">Tasks</span>

<span class="c1">#</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&#34;damo/nlp_gte_sentence-embedding_chinese-small&#34;</span> <span class="c1">#57M</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&#34;damo/nlp_gte_sentence-embedding_chinese-large&#34;</span> <span class="c1">#621M</span>
<span class="n">pipeline_se</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">Tasks</span><span class="o">.</span><span class="n">sentence_embedding</span><span class="p">,</span>
                       <span class="n">model</span><span class="o">=</span><span class="n">model_id</span><span class="p">)</span>

<span class="c1"># 当输入包含“soure_sentence”与“sentences_to_compare”时，会输出source_sentence中首个句子与sentences_to_compare中每个句子的向量表示，以及source_sentence中首个句子与sentences_to_compare中每个句子的相似度。</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&#34;source_sentence&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;吃完海鲜可以喝牛奶吗?&#34;</span><span class="p">],</span>
        <span class="s2">&#34;sentences_to_compare&#34;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&#34;不可以，早晨喝牛奶不科学&#34;</span><span class="p">,</span>
            <span class="s2">&#34;吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害&#34;</span><span class="p">,</span>
            <span class="s2">&#34;吃海鲜是不能同时喝牛奶吃水果，这个至少间隔6小时以上才可以。&#34;</span><span class="p">,</span>
            <span class="s2">&#34;吃海鲜是不可以吃柠檬的因为其中的维生素C会和海鲜中的矿物质形成砷&#34;</span>
        <span class="p">]</span>
    <span class="p">}</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline_se</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;text_embedding&#39;: array([[-0.03317244, -0.0419106 , -0.03626636, ..., -0.0132677 ,
         -0.02028614, -0.01542077],
        [-0.04563809, -0.06220782, -0.03775004, ...,  0.01267119,
         -0.01111769, -0.03390383],
        [-0.02073098, -0.04639562, -0.04818704, ..., -0.00754705,
         -0.00731624, -0.02740852],
        [-0.00037597, -0.05922904, -0.0459275 , ..., -0.00697823,
         -0.02154762, -0.02951157],
        [-0.00491675, -0.02552056, -0.03427778, ..., -0.00760836,
         -0.00404084, -0.0509829 ]], dtype=float32),
 &#39;scores&#39;: [0.8542333245277405,
  0.9613471031188965,
  0.947378396987915,
  0.8620702028274536]}
</code></pre></div><p><strong>默认向量维度512,  两个向量做内积距离计算得到score</strong>
<br><br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 当输入仅含有soure_sentence时，会输出source_sentence中每个句子的向量表示。</span>
<span class="n">inputs2</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&#34;source_sentence&#34;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&#34;不可以，早晨喝牛奶不科学&#34;</span><span class="p">,</span>
            <span class="s2">&#34;吃了海鲜后是不能再喝牛奶的，因为牛奶中含得有维生素C，如果海鲜喝牛奶一起服用会对人体造成一定的伤害&#34;</span><span class="p">,</span>
            <span class="s2">&#34;吃海鲜是不能同时喝牛奶吃水果，这个至少间隔6小时以上才可以。&#34;</span><span class="p">,</span>
            <span class="s2">&#34;吃海鲜是不可以吃柠檬的因为其中的维生素C会和海鲜中的矿物质形成砷&#34;</span>
        <span class="p">]</span>
<span class="p">}</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline_se</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">inputs2</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;text_embedding&#39;: array([[-0.04563809, -0.06220782, -0.03775004, ...,  0.01267119,
        -0.01111769, -0.03390383],
       [-0.02073098, -0.04639562, -0.04818704, ..., -0.00754705,
        -0.00731624, -0.02740852],
       [-0.00037597, -0.05922904, -0.0459275 , ..., -0.00697823,
        -0.02154762, -0.02951157],
       [-0.00491675, -0.02552056, -0.03427778, ..., -0.00760836,
        -0.00404084, -0.0509829 ]], dtype=float32), &#39;scores&#39;: []}
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>心理科学进展 | 语义距离与创造性思维关系的元分析</title>
      <link>https://textdata.cn/blog/2023-10-18-the-relationship-between-semantic-distance-with-creativity/</link>
      <pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-10-18-the-relationship-between-semantic-distance-with-creativity/</guid>
      <description>自然语言处理的发展为探究语义距离与创造性思维的关系提供了可靠且有效的研究方法。 近些年关于两者之间关系的研究逐渐增多, 但研究结论并不一致。本研究基于创造力联想理论及扩散激活模型, 通过元分析的方法探讨了语义距离与创造性思维的整体关系, 并且分析了以往研究结论不一致的原因。 结果显示：语义距离与创造性思维存在中等程度的正相关，二者的相关强度受到被试年龄和创造性思维不同测量指标的调节。 研究结果表明语义距离与创造性思维关系 密切, 同时解释了以往研究结论不一致的原因。 上述结果不仅能为更深入地探讨创造性思维的认知神经机制 提供新的研究视角和理论解释, 而且有助于更全面地理解语义距离与创造性思维二者的关系及其边界条件, 为更好地解释、预测和提升创造力提供科学依据和重要启示。</description>
      <content:encoded><![CDATA[<p>博客之前分享过 <a href="https://textdata.cn/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/"><strong>PNAS(含代码) | 使用语义距离测量一个人的创新力(发散思维)得分</strong></a>  , 通过语义距离测量创新力， 该教程含Python代码。今天摘抄一篇&lt;心理科学进展&gt;的论文， 帮助大家更深入了解语义距离与创造性思维之间的关系。</p>
<p><br><br></p>
<h2 id="一文献">一、文献</h2>
<p>李亚丹,杜颖,谢聪,刘春宇,杨毅隆,李阳萍,邱江.<strong>语义距离与创造性思维关系的元分析</strong>[J].心理科学进展,2023,31(04):519-534.</p>
<p>摘要:  自然语言处理的发展为探究 <strong>语义距离</strong> 与 <strong>创造性思维</strong> 的关系提供了可靠且有效的研究方法。近些年关于两者之间关系的研究逐渐增多,但研究结论并不一致。本研究基于 <strong>创造力联想理论</strong> 及扩散激活模型, 通过元分析的方法探讨了语义距离与创造性思维的整体关系,并且分析了以往研究结论不一致的原因。本文经过文献检索和筛选后获得14项研究,提取r值作为效应值(共53个效应值,4729个独立样本),并使用随机效应模型进行了元分析。**结果显示：语义距离与创造性思维存在中等程度的正相关(r=0.379, 95%CI [0.300, 0.452]); 二者的相关强度受到被试年龄和创造性思维不同测量指标的调节。**研究结果表明语义距离与创造性思维关系密切, 同时解释了以往研究结论不一致的原因。上述结果不仅能为更深入地探讨创造性思维的认知神经机制提供新的研究视角和理论解释,而且有助于更全面地理解语义距离与创造性思维二者的关系及其边界条件,为更好地解释、预测和提升创造力提供科学依据和重要启示。</p>
<p><br><br></p>
<p><strong>创造性思维</strong> 是一种高层次的思维活动, 对科学进步和社会发展具有深远的影响, 其核心的认知成分之一就是基于语义记忆的 <strong>联想能力</strong>(Acar &amp; Runco, 2014; Marron et al., 2018)。 个体的联想能力及其在进行创造性活动时的联想过程均可以通过语义距离(semantic distance)表现出来(Beaty et al., 2014; Benedek &amp; Neubauer, 2013)。因此, 语义距离是帮助我们理解创造性思维和创造性认知过程的重要手段。</p>
<p>在认知科学领域, 通常利用 <strong>心理词典</strong> (mental lexicon) 所构成的语义网络 (semantic network) 来表征语义记忆结构 (Christensen &amp; Kenett, 2021)。在语义网络中, 概念被表示为通过 “边(edge)”相互连结的“节点(node)”, 语义距离则用来表示概念与概念之间的距离, 即语义相似性 (Paulsen et al., 1996)。</p>
<p>在实证研究中研究者们常用发散思维测验(Divergent Thinking Test)来衡量创造性思维, 但发散思维测验的评分存在着一些不足, 如流畅性和独特性有较高程度的相关致使得分极易混淆、独特性评分依赖于样本等问题(Silvia et al.,2008)。因此, 除了对原有测量技术的优化和改进, 还需要提升创造力测量的客观性和准确性。 目前 已有学者提出使用语义距离来测量创造性思维, <strong>但是使用语义距离测量创造性思维这一方法的有效性还存在着争议</strong>(Marron et al., 2018; Wang et al., 2018)。</p>
<br>
<br>
<h2 id="二创造性思维及其度量">二、创造性思维及其度量</h2>
<p>创造力(creativity)是指产生新颖(original)且 适 宜 (appropriate) 产品的能力 (Kaufman &amp; Sternberg, 2010; Runco, 2002)。发散思维(Divergent Thinking)是个体针对给 定问题或提示产生多个原创想法的心理能力 (Acar &amp; Runco, 2019; Forthmann, Wilken et al., 2019), 长期以来一直是创造性思维研究中的一个 重要内容(Hocevar, 1980)。发散思维测验是迄今为止创造力研究中使用最多、应用最为广泛的主流测验形式(Plucker &amp; Makel, 2010; Reiter-Palmon et al., 2019)。</p>
<p>在以往研究中, Guilford (1950)的 <strong>多用途任务</strong> (Alternate Use Task, AUT)和 Torrance (1972) 的 <strong>创造性思维测试</strong> (Torrance Tests of Creative Thinking, TTCT)使用频率较高。<strong>发散思维通常包括 4 个维度, 即流畅性、灵活性、独特性(或 独创性)和精致性</strong>。其中, 流畅性(fluency)指给出 的想法或解决方案的数量; 灵活性(flexibility)指 想法的多样性; 独特性(originality)指想法的不寻 常或唯一性; 精致性(elaboration)指给出想法或答案的详细程度(Torrance, 1965, 1988)。 在评分时, 发散思维测验也常从这四个维度来计分, 并由此衡量被试答案的创造性水平。但发散思维测试存在一些潜在问题，如无法进一步探讨创造性思维过程的手段(Hass, 2017; Marron et al., 2018)。其次， 四个主要的评价指标， 除了流畅性能够被客观测量，其余三个指标的传统评分方法存在一定弊端。第三， 在发散思维测验计分时， 流畅性和独创性的得分容易混淆。以上三点导致发散思维测验的客观性、信度饱受争议(Benedek &amp; Neubauer, 2013)。</p>
<p><br><br></p>
<h2 id="三语义距离与创造性思维的关系">三、语义距离与创造性思维的关系</h2>
<p>语义距离这个概念来源于 Collins 和 Loftus (1975)提出的<strong>扩散激活模型</strong>(Spreading-Activation Model)。 在概念与概念之间, 共同的定义性特征 越多, 它们之间的关系就越近, 这个关系就称为 语义距离(Volle, 2018)。例如, “雪”和“白”经常共同出现在文本中, 所以语义距离较小; 相反, “雪”和 “石油”很少同时出现, 因此二者之间具有较大的 语义距离。</p>
<p>Mednick 在 1962 年提出了 <strong>创造力联想理论</strong> (Associative Theory of Creativity), 该理论解释了创造性思维与语义记忆结构之间的关系(Mednick, 1962)。 该理论认为,  创造性思维涉及将弱相关或远距离概念联接成新颖且有用的概念的认知过程。 如果某些概念在语义层面相距越远, 由它们所产生的新的组合就越有创意, 新颖度越高。</p>
<p>Benedek 等人(2012)在 Mednick (1962)的理论 基础上提出, 解离能力(dissociative ability)和联想整合能力(associative combination ability)是与创造性思维密切相关的基本认知能力。</p>
<ul>
<li><strong>解离能力</strong> 是指生成不相关的概念的能力, 也可以被理解为一 种语义抑制能力, 它有助于人们获得新的语义距离遥远的概念。</li>
<li><strong>联想整合能力</strong> 指的是对看似不相关的概念形成合理联想的能力。</li>
</ul>
<p>据此我们可以推断, 语义距离作为概念与概念之间关系的量化指 标(Volle, 2018), 也即衡量个体联想能力的指标, 可以有效反映个体以联想过程为基础的创造性思维。 扩散激活模型也提到, 有创造力的人拥有更加复杂(Collins &amp; Loftus, 1975; Gruszka &amp; Necka, 2002; Kenett, 2019) 、 更加灵活的语义网络 (Schilling, 2005)。</p>
<p>近年来语义距离也开始作为测量创造性表现 的指标。 研究者们(Green et al., 2012; Prabhakaran et al., 2014; Weinberger et al., 2016)在探究状态创 造力(state creativity)时, 通常将语义距离作为创 造力水平高低的测量指标。 状态创造力即被试在 不同指导语或线索提示下所表现出的不同创造力 水平。也有研究利用语义距离来测量创造性思维, 结果显示相比较传统测量方法, 基于语义距离的 测量方法在创造性思维各指标间有着更好的区分 效度和结构信度(Dumas &amp; Dunbar, 2014)。</p>
<p>此外, 通过对语义距离的应用,  研究者能够对创造性思维的质量有更为客观的认识, 从而更 好地探讨创造性思维的认知神经机制。 人们普遍认为, 创造性思维认知过程需要 <strong>联想过程</strong> (associative processes) 与 <strong>执行过程</strong> (executive processes)的耦合(Silvia et al., 2013)。 目前, 大多 数创造性思维任务并未区分这两种认知过程 (Mednick, 1968; Runco et al., 2016), 而对这两种 认知过程的细分有助于我们更深入地理解创造性思维和创造性认知过程(Fox et al., 2015)。 而语义距离作为联想能力的衡量指标, 可以更好地反映 出个体在进行创造性思维任务时的联想过程 (Beaty, Nusbaum et al., 2014; Beaty, Silvia et al., 2014; Marron et al., 2018)。因此, 语义距离也被用来作为认知神经科学研究中创造性思维的测量指标, 它不仅可以用于比较个体在产生不同创造性 水平的答案时其大脑激活模式的差异(Beaty et al., 2017; Green et al., 2015; Tempest &amp; Radel, 2019)及个体的创造性表现随时间动态变化 (Green, 2016), 还可以用来研究不同个体之间的创造力水平差异(Green, 2016)。</p>
<p><br><br></p>
<h2 id="四年龄可能调节语义距离与创造性思维关系">四、年龄可能调节语义距离与创造性思维关系</h2>
<p>近几年, 国内外开展了一些语义距离与创造性思维关系的研究, 但是研究结果却不尽相同。 这可能与研究对象的人口学因素(年龄)和评估创 造性思维时所使用的测量指标有关。</p>
<p>根据已有研究, 年龄可能会影响语义距离与创造性思维之间的关系。</p>
<p><strong>首先</strong>, 年龄与语言能力和词汇量有关。 老年人的词汇量及语义知识存储与年轻人相比更加丰富(Kavé &amp; Halamish, 2015; Verhaeghen, 2003), 而语言能力较强、词汇量较多 的个体在表达想法时更不容易受到表达能力的限制, 因此往往在言语创造性任务中表现得更好。 语言能力较强的个体也可能会有更多的认知资源用来产生创造性想法(Wu et al., 2005)。  <strong>其次</strong>, 不同年龄被试的语义结构和语义记忆也是不同的, 例如, 老年人语义记忆中的概念更加模块化, 也更分散(Dubossarsky et al., 2017; Wulff et al., 2019; Zortea et al., 2014)。因此, 样本群体的年龄 可能会影响语义距离与创造性思维之间的关系。</p>
<p><br><br></p>
<h2 id="五元分析结果">五、元分析结果</h2>
<h3 id="51-语义距离测量创造性思维的有效性">5.1 语义距离测量创造性思维的有效性</h3>
<p><strong>本研究结果显示 , 语义距离与创造性思维呈显著正相关 (r = 0.379, p &lt; 0.001), 与以往研究结 果一致 (Hass, 2017; Heinen &amp; Johnson, 2018) 。该结果进一步验证了 Mednick (1962)提出的创造力联想理论 , 即如果某些概念在语义层面相距越远 , 由它们所产生的新的组合就越有创意新颖性越高</strong>。</p>
<p>语义距离作为一种连续变量 , 可以更精准地反映出创造性思维的定量变化 , 而不仅仅是二元对比 ( 例如 , 创造性与非创造性条件 ) (Kenett et al., 2017; Kenett, 2018; Kenett, 2019)。 因此 , 语义距离具有测量创造性思维的独特优势 (Green, 2016)。</p>
<p>然而 , 本研究发现 , 语义距离与创造性思维 关系的效应值为 0.379, 仍处于中等程度的正相关 (Cohen, 1988) 。 这说明尽管使用语义距离测量创 造性思维有一定的有效性 , 但是语义距离对创造 性思维的代表程度有限。</p>
<br>
<h3 id="52-语义距离与创造性思维关系中存在的调节效应">5.2 语义距离与创造性思维关系中存在的调节效应</h3>
<p><strong>被试年龄对语义距离与创造性思维的关系具有显著的调节作用，二者的相关性随着年龄的增加而逐渐降低</strong>。 原因可能在于 , 随着年龄的增长 , 个体的语义记忆结构和知识储备也在逐渐发生改变 , 从而影响了语义距离与创造性思维的关系。首先是语义记忆结 构的变化。 个体的语义记忆结构会随着年龄的增 长而逐渐变得稀疏 (Dubossarsky et al., 2017; Wulff et al., 2019; Zortea et al., 2014)。其次 是个体的知识储备和生活经验的变化。 常见的言语类创造性思维任务介于现实问题任务和图形任 务之间 , 完成这类任务需要一定的知识储备 (Wu et al., 2005)。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 | 谷歌地图美国区域内poi、评论信息等信息</title>
      <link>https://textdata.cn/blog/2023-10-18-google-local-data/</link>
      <pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-10-18-google-local-data/</guid>
      <description>&lt;h2 id=&#34;一数据介绍&#34;&gt;一、数据介绍&lt;/h2&gt;
&lt;p&gt;该数据集从谷歌地图采集了美国范围内一些信息(截止日期为 2021 年 9 月)，数据规模&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;666,324,103&lt;/strong&gt; 条评论（评级、文本、图片等）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;113,643,107&lt;/strong&gt; 位用户信息&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4,963,111&lt;/strong&gt;  条企业元数据（地址、地理信息、描述、类别信息、价格、营业时间和其他信息）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;数据集地址&lt;/strong&gt; &lt;a href=&#34;https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/&#34;&gt;https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;11-完整评论数据&#34;&gt;1.1 完整评论数据&lt;/h3&gt;
&lt;p&gt;请仅在确实需要时下载这些（大！）文件。我们建议使用较小的数据集（即 k-core 和 CSV 文件），如下一节所示。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;12-小的评论数据&#34;&gt;1.2 小的评论数据&lt;/h3&gt;
&lt;p&gt;如果您要在课堂项目（或类似项目）中使用这些数据，请考虑在申请大文件之前使用下面这些较小的数据集。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10-cores&lt;/strong&gt; 经过缩减，以剩下的每个用户和每个项目都有 10 条评论。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ratings only&lt;/strong&gt;：这些数据集不包含元数据或评论，只有（企业、用户、评分、时间戳）元组。因此，它们适合与 mymedialite（或类似）软件包一起使用。&lt;/p&gt;
&lt;p&gt;您可以直接下载以下按类别划分的较小数据集。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;二数据格式&#34;&gt;二、数据格式&lt;/h2&gt;
&lt;p&gt;格式为 json 格式的每行一篇评论。如需进一步帮助阅读数据，请参阅下面的示例。&lt;/p&gt;
&lt;h3 id=&#34;21-评论样本&#34;&gt;2.1 评论样本&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{
  &amp;#39;user_id&amp;#39;: &amp;#39;106533466896145407182&amp;#39;, 
  &amp;#39;name&amp;#39;: &amp;#39;Amy VG&amp;#39;, 
  &amp;#39;time&amp;#39;: 1568748357166, 
  &amp;#39;rating&amp;#39;: 5, 
  &amp;#39;text&amp;#39;: &amp;#34;I can&amp;#39;t say I&amp;#39;ve ever been excited about a dentist visit before, but there&amp;#39;s a first for everything! Loved my experience at Lush today. Every person in the office was friendly and personable- plus the office itself is gorgeous! Great experience, I highly recommend!&amp;#34;, 
  &amp;#39;pics&amp;#39;: [
  {
    &amp;#39;url&amp;#39;: [&amp;#39;https://lh5.googleusercontent.com/p/AF1QipMBzN4BJV9YCObcw_ifNzFPm-u38hO3oimOA8Fb=w150-h150-k-no-p&amp;#39;]
  }, 
  {
    &amp;#39;url&amp;#39;: [&amp;#39;https://lh5.googleusercontent.com/p/AF1QipNS1PEXEvadfUlhRkRDJ09id
    Mxh3CveZGZYuTo5=w150-h150-k-no-p&amp;#39;]
  }
  ], 
  &amp;#39;resp&amp;#39;: {
    &amp;#39;time&amp;#39;: 1568770503975, 
    &amp;#39;text&amp;#39;: &amp;#39;We love getting to meet new patients like yourself.  Thanks for giving our office a chance to take care of your dental needs and thanks for the nice review!&amp;#39;
  }, 
  &amp;#39;gmap_id&amp;#39;: &amp;#39;0x87ec2394c2cd9d2d:0xd1119cfbee0da6f3&amp;#39;
}
{
  &amp;#39;user_id&amp;#39;: &amp;#39;101463350189962023774&amp;#39;, 
  &amp;#39;name&amp;#39;: &amp;#39;Jordan Adams&amp;#39;, 
  &amp;#39;time&amp;#39;: 1627750414677, 
  &amp;#39;rating&amp;#39;: 5, 
  &amp;#39;text&amp;#39;: &amp;#39;Cool place, great people, awesome dentist!&amp;#39;, 
  &amp;#39;pics&amp;#39;: [
  {
    &amp;#39;url&amp;#39;: [&amp;#39;https://lh5.googleusercontent.com/p/AF1QipNq2nZC5TH4_M7h5xRAd
    61hoTgvY1o9lozABguI=w150-h150-k-no-p&amp;#39;]
  }
  ], 
  &amp;#39;resp&amp;#39;: {
    &amp;#39;time&amp;#39;: 1628455067818, 
    &amp;#39;text&amp;#39;: &amp;#39;Thank you for your five-star review! -Dr. Blake&amp;#39;
  }, 
  &amp;#39;gmap_id&amp;#39;: &amp;#39;0x87ec2394c2cd9d2d:0xd1119cfbee0da6f3&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;user_id - 审稿人的 ID
name - 审阅人姓名
time - 审核时间（UNIX 时间）
rating - 企业评级
text - 评论的文字
pics - 评论的图片
resp - 企业对评论的回复，包括 unix 时间和回复文本
gmap_id - 企业 ID
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-元数据样本&#34;&gt;2.2 元数据样本&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{
  &amp;#39;name&amp;#39;: &amp;#39;Walgreens Pharmacy&amp;#39;, 
  &amp;#39;address&amp;#39;: &amp;#39;Walgreens Pharmacy, 124 E North St, Kendallville, IN 46755&amp;#39;, 
  &amp;#39;gmap_id&amp;#39;: &amp;#39;0x881614ce7c13acbb:0x5c7b18bbf6ec4f7e&amp;#39;, 
  &amp;#39;description&amp;#39;: &amp;#39;Department of the Walgreens chain providing prescription medications &amp;amp; other health-related items.&amp;#39;, 
  &amp;#39;latitude&amp;#39;: 41.451859999999996, 
  &amp;#39;longitude&amp;#39;: -85.2666757, 
  &amp;#39;category&amp;#39;: [&amp;#39;Pharmacy&amp;#39;], 
  &amp;#39;avg_rating&amp;#39;: 4.2, 
  &amp;#39;num_of_reviews&amp;#39;: 5, 
  &amp;#39;price&amp;#39;: &amp;#39;$$&amp;#39;, 
  &amp;#39;hours&amp;#39;: [[&amp;#39;Thursday&amp;#39;, &amp;#39;8AM–1:30PM&amp;#39;], [&amp;#39;Friday&amp;#39;, &amp;#39;8AM–1:30PM&amp;#39;], [&amp;#39;Saturday&amp;#39;, &amp;#39;9AM–1:30PM&amp;#39;], [&amp;#39;Sunday&amp;#39;, &amp;#39;10AM–1:30PM&amp;#39;], [&amp;#39;Monday&amp;#39;, &amp;#39;8AM–1:30PM&amp;#39;], [&amp;#39;Tuesday&amp;#39;, &amp;#39;8AM–1:30PM&amp;#39;], [&amp;#39;Wednesday&amp;#39;, &amp;#39;8AM–1:30PM&amp;#39;]], 
  &amp;#39;MISC&amp;#39;: {
    &amp;#39;Service options&amp;#39;: [&amp;#39;Curbside pickup&amp;#39;, &amp;#39;Drive-through&amp;#39;, &amp;#39;In-store pickup&amp;#39;, &amp;#39;In-store shopping&amp;#39;], 
    &amp;#39;Health &amp;amp; safety&amp;#39;: [&amp;#39;Mask required&amp;#39;, &amp;#39;Staff wear masks&amp;#39;, &amp;#39;Staff get temperature checks&amp;#39;], 
    &amp;#39;Accessibility&amp;#39;: [&amp;#39;Wheelchair accessible entrance&amp;#39;, &amp;#39;Wheelchair accessible parking lot&amp;#39;], 
    &amp;#39;Planning&amp;#39;: [&amp;#39;Quick visit&amp;#39;], 
    &amp;#39;Payments&amp;#39;: [&amp;#39;Checks&amp;#39;, &amp;#39;Debit cards&amp;#39;]
  }, 
  &amp;#39;state&amp;#39;: &amp;#39;Closes soon ⋅ 1:30PM ⋅ Reopens 2PM&amp;#39;, 
  &amp;#39;relative_results&amp;#39;: [&amp;#39;0x881614cd49e4fa33:0x2d507c24ff4f1c74&amp;#39;, &amp;#39;0x8816145bf5141c89:0x535c1d605109f94b&amp;#39;, &amp;#39;0x881614cda24cc591:0xca426e3a9b826432&amp;#39;, &amp;#39;0x88162894d98b91ef:0xd139b34de70d3e03&amp;#39;, &amp;#39;0x881615400b5e57f9:0xc56d17dbe420a67f&amp;#39;], 
  &amp;#39;url&amp;#39;: &amp;#39;https://www.google.com/maps/place//data=!4m2!3m1!1s0x881614ce7c13acb
  b:0x5c7b18bbf6ec4f7e?authuser=-1&amp;amp;hl=en&amp;amp;gl=us&amp;#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;name - 企业名称
address - 企业地址
gmap_id - 企业 ID
description - 企业描述
latitude - 企业的纬度
longitude - 企业的经度
category - 企业类别
avg_rating - 企业的平均评分
num_of_reviews - 评价数量
price - 商店的价格
hours - 营业时间
MISC - 其他信息
state - 企业的当前状态（例如，永久关闭）
relative_results - 谷歌推荐的相关企业
url - 企业的 URL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;三获取数据集&#34;&gt;三、获取数据集&lt;/h2&gt;
&lt;p&gt;数据集地址
&lt;a href=&#34;https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/&#34;&gt;https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;31-引用&#34;&gt;3.1 引用&lt;/h3&gt;
&lt;p&gt;如果您以任何方式使用这些数据，请引用以下论文：&lt;/p&gt;
&lt;p&gt;Li, Jiacheng, Jingbo Shang, and Julian McAuley. &amp;ldquo;Uctopic: Unsupervised contrastive learning for phrase representations and topic mining.&amp;rdquo; &lt;em&gt;arXiv preprint arXiv:2202.13469&lt;/em&gt; (2022).&lt;/p&gt;
&lt;p&gt;Yan, An, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. &amp;ldquo;Personalized Showcases: Generating multi-modal explanations for recommendations.&amp;rdquo; In &lt;em&gt;Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval&lt;/em&gt;, pp. 2251-2255. 2023.&lt;/p&gt;
&lt;h3 id=&#34;32-联系方式&#34;&gt;3.2 联系方式&lt;/h3&gt;
&lt;p&gt;Jiacheng Li (&lt;a href=&#34;mailto:j9li@eng.ucsd.edu&#34;&gt;j9li@eng.ucsd.edu&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一数据介绍">一、数据介绍</h2>
<p>该数据集从谷歌地图采集了美国范围内一些信息(截止日期为 2021 年 9 月)，数据规模</p>
<ul>
<li><strong>666,324,103</strong> 条评论（评级、文本、图片等）</li>
<li><strong>113,643,107</strong> 位用户信息</li>
<li><strong>4,963,111</strong>  条企业元数据（地址、地理信息、描述、类别信息、价格、营业时间和其他信息）</li>
</ul>
<p><strong>数据集地址</strong> <a href="https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/">https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/</a></p>
<br>
<h3 id="11-完整评论数据">1.1 完整评论数据</h3>
<p>请仅在确实需要时下载这些（大！）文件。我们建议使用较小的数据集（即 k-core 和 CSV 文件），如下一节所示。</p>
<p><img loading="lazy" src="img/1.png" alt=""  />
</p>
<p>&hellip;&hellip;</p>
<h3 id="12-小的评论数据">1.2 小的评论数据</h3>
<p>如果您要在课堂项目（或类似项目）中使用这些数据，请考虑在申请大文件之前使用下面这些较小的数据集。</p>
<p><strong>10-cores</strong> 经过缩减，以剩下的每个用户和每个项目都有 10 条评论。</p>
<p><strong>ratings only</strong>：这些数据集不包含元数据或评论，只有（企业、用户、评分、时间戳）元组。因此，它们适合与 mymedialite（或类似）软件包一起使用。</p>
<p>您可以直接下载以下按类别划分的较小数据集。</p>
<p><img loading="lazy" src="img/2.png" alt=""  />
</p>
<br>
<h2 id="二数据格式">二、数据格式</h2>
<p>格式为 json 格式的每行一篇评论。如需进一步帮助阅读数据，请参阅下面的示例。</p>
<h3 id="21-评论样本">2.1 评论样本</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{
  &#39;user_id&#39;: &#39;106533466896145407182&#39;, 
  &#39;name&#39;: &#39;Amy VG&#39;, 
  &#39;time&#39;: 1568748357166, 
  &#39;rating&#39;: 5, 
  &#39;text&#39;: &#34;I can&#39;t say I&#39;ve ever been excited about a dentist visit before, but there&#39;s a first for everything! Loved my experience at Lush today. Every person in the office was friendly and personable- plus the office itself is gorgeous! Great experience, I highly recommend!&#34;, 
  &#39;pics&#39;: [
  {
    &#39;url&#39;: [&#39;https://lh5.googleusercontent.com/p/AF1QipMBzN4BJV9YCObcw_ifNzFPm-u38hO3oimOA8Fb=w150-h150-k-no-p&#39;]
  }, 
  {
    &#39;url&#39;: [&#39;https://lh5.googleusercontent.com/p/AF1QipNS1PEXEvadfUlhRkRDJ09id
    Mxh3CveZGZYuTo5=w150-h150-k-no-p&#39;]
  }
  ], 
  &#39;resp&#39;: {
    &#39;time&#39;: 1568770503975, 
    &#39;text&#39;: &#39;We love getting to meet new patients like yourself.  Thanks for giving our office a chance to take care of your dental needs and thanks for the nice review!&#39;
  }, 
  &#39;gmap_id&#39;: &#39;0x87ec2394c2cd9d2d:0xd1119cfbee0da6f3&#39;
}
{
  &#39;user_id&#39;: &#39;101463350189962023774&#39;, 
  &#39;name&#39;: &#39;Jordan Adams&#39;, 
  &#39;time&#39;: 1627750414677, 
  &#39;rating&#39;: 5, 
  &#39;text&#39;: &#39;Cool place, great people, awesome dentist!&#39;, 
  &#39;pics&#39;: [
  {
    &#39;url&#39;: [&#39;https://lh5.googleusercontent.com/p/AF1QipNq2nZC5TH4_M7h5xRAd
    61hoTgvY1o9lozABguI=w150-h150-k-no-p&#39;]
  }
  ], 
  &#39;resp&#39;: {
    &#39;time&#39;: 1628455067818, 
    &#39;text&#39;: &#39;Thank you for your five-star review! -Dr. Blake&#39;
  }, 
  &#39;gmap_id&#39;: &#39;0x87ec2394c2cd9d2d:0xd1119cfbee0da6f3&#39;
}
</code></pre></div><p>其中</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">user_id - 审稿人的 ID
name - 审阅人姓名
time - 审核时间（UNIX 时间）
rating - 企业评级
text - 评论的文字
pics - 评论的图片
resp - 企业对评论的回复，包括 unix 时间和回复文本
gmap_id - 企业 ID
</code></pre></div><br>
<h3 id="22-元数据样本">2.2 元数据样本</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{
  &#39;name&#39;: &#39;Walgreens Pharmacy&#39;, 
  &#39;address&#39;: &#39;Walgreens Pharmacy, 124 E North St, Kendallville, IN 46755&#39;, 
  &#39;gmap_id&#39;: &#39;0x881614ce7c13acbb:0x5c7b18bbf6ec4f7e&#39;, 
  &#39;description&#39;: &#39;Department of the Walgreens chain providing prescription medications &amp; other health-related items.&#39;, 
  &#39;latitude&#39;: 41.451859999999996, 
  &#39;longitude&#39;: -85.2666757, 
  &#39;category&#39;: [&#39;Pharmacy&#39;], 
  &#39;avg_rating&#39;: 4.2, 
  &#39;num_of_reviews&#39;: 5, 
  &#39;price&#39;: &#39;$$&#39;, 
  &#39;hours&#39;: [[&#39;Thursday&#39;, &#39;8AM–1:30PM&#39;], [&#39;Friday&#39;, &#39;8AM–1:30PM&#39;], [&#39;Saturday&#39;, &#39;9AM–1:30PM&#39;], [&#39;Sunday&#39;, &#39;10AM–1:30PM&#39;], [&#39;Monday&#39;, &#39;8AM–1:30PM&#39;], [&#39;Tuesday&#39;, &#39;8AM–1:30PM&#39;], [&#39;Wednesday&#39;, &#39;8AM–1:30PM&#39;]], 
  &#39;MISC&#39;: {
    &#39;Service options&#39;: [&#39;Curbside pickup&#39;, &#39;Drive-through&#39;, &#39;In-store pickup&#39;, &#39;In-store shopping&#39;], 
    &#39;Health &amp; safety&#39;: [&#39;Mask required&#39;, &#39;Staff wear masks&#39;, &#39;Staff get temperature checks&#39;], 
    &#39;Accessibility&#39;: [&#39;Wheelchair accessible entrance&#39;, &#39;Wheelchair accessible parking lot&#39;], 
    &#39;Planning&#39;: [&#39;Quick visit&#39;], 
    &#39;Payments&#39;: [&#39;Checks&#39;, &#39;Debit cards&#39;]
  }, 
  &#39;state&#39;: &#39;Closes soon ⋅ 1:30PM ⋅ Reopens 2PM&#39;, 
  &#39;relative_results&#39;: [&#39;0x881614cd49e4fa33:0x2d507c24ff4f1c74&#39;, &#39;0x8816145bf5141c89:0x535c1d605109f94b&#39;, &#39;0x881614cda24cc591:0xca426e3a9b826432&#39;, &#39;0x88162894d98b91ef:0xd139b34de70d3e03&#39;, &#39;0x881615400b5e57f9:0xc56d17dbe420a67f&#39;], 
  &#39;url&#39;: &#39;https://www.google.com/maps/place//data=!4m2!3m1!1s0x881614ce7c13acb
  b:0x5c7b18bbf6ec4f7e?authuser=-1&amp;hl=en&amp;gl=us&#39;
}
</code></pre></div><p>其中</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">name - 企业名称
address - 企业地址
gmap_id - 企业 ID
description - 企业描述
latitude - 企业的纬度
longitude - 企业的经度
category - 企业类别
avg_rating - 企业的平均评分
num_of_reviews - 评价数量
price - 商店的价格
hours - 营业时间
MISC - 其他信息
state - 企业的当前状态（例如，永久关闭）
relative_results - 谷歌推荐的相关企业
url - 企业的 URL
</code></pre></div><br>
<br>
<h2 id="三获取数据集">三、获取数据集</h2>
<p>数据集地址
<a href="https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/">https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/</a></p>
<h3 id="31-引用">3.1 引用</h3>
<p>如果您以任何方式使用这些数据，请引用以下论文：</p>
<p>Li, Jiacheng, Jingbo Shang, and Julian McAuley. &ldquo;Uctopic: Unsupervised contrastive learning for phrase representations and topic mining.&rdquo; <em>arXiv preprint arXiv:2202.13469</em> (2022).</p>
<p>Yan, An, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. &ldquo;Personalized Showcases: Generating multi-modal explanations for recommendations.&rdquo; In <em>Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pp. 2251-2255. 2023.</p>
<h3 id="32-联系方式">3.2 联系方式</h3>
<p>Jiacheng Li (<a href="mailto:j9li@eng.ucsd.edu">j9li@eng.ucsd.edu</a>)</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>JMR | 测量消费者的语言确定性</title>
      <link>https://textdata.cn/blog/2023-10-16-measurement-of-consumer-certainty-in-language/</link>
      <pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-10-16-measurement-of-consumer-certainty-in-language/</guid>
      <description>情感分析从根本上改变了市场营销者评估消费者意见的能力。的确，通过自然语言测量态度已经影响了市场营销在日常实践中的方式。**然而，最近的研究发现，情感分析目前强调测量情感的正负面（即积极或消极）可能会产生不完整、不准确甚至误导性的见解**。从概念上讲，这项研究挑战情感分析超越对情感正负面的侧重。作者识别出消费者情感的确定性或信心是一个特别有力的评估方面。从经验上，**他们开发了一种新的计算语言中确定性的测量工具——确定性词典（Certainty Lexicon）**，并验证了其与情感分析的使用。为了构建和验证这种测量，作者使用了来自1160万人的文本，他们生成了数十亿的词汇，数百万的在线评论，以及在线预测市场的数十万条记录。在社交媒体数据集、实验室实验和在线评论中，作者发现与其他工具相比，确定性词典在其测量中更为全面、可推广和准确。作者还展示了对市场营销者来说，测量情感确定性的价值：确定性预测了广告的实际成功，而传统的情感分析则未能做到这一点。</description>
      <content:encoded><![CDATA[<h2 id="一文献">一、文献</h2>
<p>Rocklage Matthew D.,He Sharlene,Rucker Derek D.,Nordgren Loran F..<strong>Beyond Sentiment: The Value and Measurement of Consumer Certainty in Language</strong>[J].<strong>Journal of Marketing Research</strong>,2023,60(5).</p>
<p><strong>摘要(译文)</strong>:  情感分析从根本上改变了市场营销者评估消费者意见的能力。的确，通过自然语言测量态度已经影响了市场营销在日常实践中的方式。<strong>然而，最近的研究发现，情感分析目前强调测量情感的正负面（即积极或消极）可能会产生不完整、不准确甚至误导性的见解</strong>。从概念上讲，这项研究挑战情感分析超越对情感正负面的侧重。作者识别出消费者情感的确定性或信心是一个特别有力的评估方面。从经验上，<strong>他们开发了一种新的计算语言中确定性的测量工具——「确定性词典（Certainty Lexicon）</strong>，并验证了其与情感分析的使用。为了构建和验证这种测量，作者使用了来自1160万人的文本，他们生成了数十亿的词汇，数百万的在线评论，以及在线预测市场的数十万条记录。在社交媒体数据集、实验室实验和在线评论中，作者发现与其他工具相比，确定性词典在其测量中更为全面、可推广和准确。作者还展示了对市场营销者来说，测量情感确定性的价值：确定性预测了广告的实际成功，而传统的情感分析则未能做到这一点。</p>
<p><img loading="lazy" src="img/paper.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二消费者的确定性">二、消费者的确定性</h2>
<p>为了更好地理解消费者情绪，<strong>我们认为消费者持有该情绪的确定性是至关重要的。确定性是个人对信心或信仰的主观感觉（Petrocelli, Tormala, 和 Rucker 2007）。态度研究的结果表明，消费者对所持有的态度或信仰的确定性越强，该态度或信仰驱动行为的可能性就越大</strong>（参见 Tormala 和 Rucker 2018）。例如，研究表明，当态度持有更大的确定性时，态度和行为意图之间的关联更强（r = .89），而确定性较低时关联较弱（r = .68；Tormala 和 Petty [2002], 实验 4；参见也有 Franc [1999]）。同样，持有更大确定性的想法更能预测人们对这些想法的依赖（Briñol, Petty, 和 Tormala 2004）。在态度文献中，大量的研究表明，持有更大确定性的态度更可能随着时间的推移而持续，并抵御变化（Rucker, Petty, 和 Briñol 2008；Tormala 2016；Tormala 和 Petty 2002）。</p>
<p><strong>确定性还与情感值（sentiment, 即它是积极还是消极）和情感效价（valence, 即情感的价值是多么的正或多么的负）有所不同</strong>（Clarkson, Tormala, 和 Leone 2011；Petty 和 Krosnick 1995）。<strong>虽然更极端的情感值通常与更确定的态度相关联，但这种关联并不强烈</strong>（例如，r ∼ .50；Krosnick 等人 1993）。即使文本中情感效价是相同的，但语言中的确定性的差异也可能很普遍（参见 Rucker 和 Petty 2004；Tormala 和 Petty 2002）。此外，极端态度可能持有的确定性较低（Litt 和 Tormala 2010），并且不太可能随着时间的推移而持续（Rocklage 和 Luttrell 2021）。<strong>因此，确定性 和 情感效价极端性 是不同的</strong>。</p>
<p>举例来说，考虑两位顾客访问同一家餐厅并给予其完美的五星评级。尽管他们对餐厅的态度都是一样的正面，但其中一位可能对其态度更有确定感，因为他们的许多朋友持有类似的态度（Tormala 和 DeSensi 2009）。<strong>尽管态度的情感值完全相同，但确定性更强的顾客更有可能再次光顾餐厅并将其推荐给他人</strong>（例如，Barden 和 Petty 2008）。确定性的差异可以由社交共识的数量或直接的个人经验等因素产生。更普遍地说，确定性可以源于任何影响消费者感觉其态度或信仰背后的信息是准确、完整、相关、合法或重要的因素（Rucker 等人 2014）。</p>
<p><strong>鉴于确定性是态度的一个重要和突出的方面，它是扩展消费者情感评估的理想候选指标</strong>。目前，情感分析主要集中在测量价值上，但忽略了与该价值相关的确定性。此外，研究表明，因为大多数在线表达的消费者情感都是积极的，所以市场营销人员经常面临一个“<strong>positivity problem</strong>”（Rocklage, Rucker, 和 Nordgren 2021b）。这种正面信息的过剩导致了价值的受限范围，仅基于价值或价值极端性就很难获得洞察。<strong>在这些情境中，确定性的测量可能特别有用，确定性可能比情感价更准地预测消费者行为</strong>。</p>
<p><br><br></p>
<h2 id="三语言中的确定性测量">三、语言中的确定性测量</h2>
<h3 id="31-已有测量工具">3.1 已有测量工具</h3>
<p>语言中确定性的两个最突出的度量来自Linguistic Inquiry and Word Count (LIWC; Pennebaker等人2015) 和 DICTION (Hart和Carroll 2015) 软件程序。这两个程序都提供了用于评估文本属性（如其情感）的测量方法。尽管它们也包含与确定性相关的测量，但这些测量在其有效性、普遍性以及它们用于量化语言的方法上都存在局限性。</p>
<p><strong>首先，LIWC 和 DICTION 都没有得到足够的实证验证来测量确定性，也没有经过验证以评估情感确定性</strong>。例如，这两个工具都是基于研究者对哪些词会表示个人的确定性的直觉来创建的，而不是一个更正式或基于数据的方法（Hart 1976；Pennebaker 和 Francis 1996）。LIWC包含两个名为“确定性”和“犹豫”的确定性度量。然而，“<strong>certainty</strong>”测量尚未得到直接验证（Petrie, Booth, 和 Pennebaker 1998），而“<strong>tentativeness</strong>”测量仅在一组35名写有关其大学经历的大学生中得到验证（Pennebaker和Francis 1996）。同样，DICTION的确定性测量尚未直接得到验证（Hart 1976, 1984）。尽管它们有可能作为情感确定性的测量工具，但其有效性和普遍性仍然不清晰。</p>
<p><br><br></p>
<h3 id="32已有工具不足">3.2已有工具不足</h3>
<p>首先，它们都依赖于词频计数方法。考虑LIWC如何量化以下两个句子：（1）“<em>I&rsquo;ve often dislikeed my experience with that brand.</em>”和（2）“<em>I&rsquo;ve sorta dislikeed my experience with that brand.</em>” 。其中 “<strong>often</strong>”和“<strong>sorta</strong>”都出现在LIWC的“<strong>tentativeness</strong>”（<strong>certainty</strong>）词汇表中。根据LIWC的词频计数方法，这两个句子因此都被给予了12.50%的分数（即，八个词中有一个词表示不确定性）。因此， “<strong>often</strong>”和“<strong>sorta</strong>”被计为表示相同程度的不确定性。同样，DICTION给这些句子在确定性上同样的分数。通过简单地计算每个句子中的词，词频计数方法将给定词汇表中的所有词都视为表示相同的确定性。</p>
<p>其次，在测量短文本时候表现较差。这是因为短文本包含的信息相对较少，因此通常只有一个与确定性相关的关键词（Pennebaker等人2015）。鉴于词频计数方法假设给定词典中的所有词都表示相同程度的确定性，这些测量方法可能导致数据中的大偏斜（观察变化小），从而产生大量噪音，因此得到的结果无信息性或甚至具有误导性（Garten等人2018；Rocklage和Rucker 2019；Sterling, Jost, 和 Bonneau 2020）。鉴于市场营销人员依赖社交媒体来了解消费者情感，这一限制对他们尤为重要（Schaefer 2015）。</p>
<p>第三， 只能分析单个词汇，不能处理词组短语。例如，LIWC和DICTION都会将短语“<strong>i&rsquo;m not sure</strong>”视为表示高确定性，因为它包含单词“<strong>sure</strong>”；这些方法无法识别关键短语“<strong>not sure</strong>”。同样，它们会将“<strong>likely</strong>”和“<strong>extremely likely</strong>”视为表示相同程度的确定性。</p>
<p><img loading="lazy" src="img/1-table1.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四构建确定性词典certainty-lexicon">四、构建确定性词典(Certainty Lexicon)</h2>
<h3 id="41-构建词典步骤">4.1 构建词典步骤</h3>
<p>这篇论文测量消费者确定性的词典叫做Certainty Lexicon， 该词典构建方法及步骤如下</p>
<p><strong>Phase-1</strong> 准备候选词表； 根据LIWC和Diction中的相关词，并生成近义词、ngram词组，以扩充确定性词的候选范围。</p>
<p><strong>Phase-2</strong> 初始剔除工作； 基于真实场景，剔除掉使用低频的词语与词组， 剔除掉人工阅读后觉得不符合确定性这个概念的词。</p>
<p><strong>Phase-3</strong>量化每个词的确定性； 设计9-likert量表(0 = “very uncertain,” and 9 = “very certain”; see Web Appendix B)，通过MTurk在线网站， 发放调查问卷。问题如。收到515多个参与者的问卷，最终保留489有效问卷。</p>
<p><strong>Phase-4</strong> 验证词典有效性;</p>
<p><img loading="lazy" src="img/2-table3.png" alt=""  />
</p>
<h3 id="42-确定性词典">4.2 确定性词典</h3>
<p>论文团队开发了 <strong>LexiconSuite</strong> 文本分析工具，内置了<strong>Evaluate Lexicon</strong>、<strong>Certainty Lexicon</strong>，可以用来分析文本的情感、确定性，工具是开源免费的，下载地址 <a href="http://www.lexicalsuite.com/">http://www.lexicalsuite.com/</a> 。 在LexiconSuite软件安装目录中，经过探索我找到了软件内置的词典txt文件。以本文介绍的<strong>确定性词典</strong>(Certainty Lexicon) ，对应的文件是 <a href="Certainty.txt"><strong>Certainty.txt</strong></a> ，</p>
<p><img loading="lazy" src="img/dict.png" alt=""  />
</p>
<p>打开txt如上图，使用Python读取发现一共有 <strong>3485</strong> 个词语(组)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">&#39;Certainty.txt&#39;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四计算文本的确定性">四、计算文本的确定性</h2>
<p>根据确定性词典(Certainty Lexicon), 就可以计算文本的确定性指标， 我在mac安装了<strong>LexiconSuite</strong>并且做了测试，导入了一个<a href="certainty_test.csv"><strong>csv文件</strong></a>。</p>
<p><img loading="lazy" src="img/certainty_test.png" alt=""  />
</p>
<p><img loading="lazy" src="img/ls-1.png" alt=""  />
<br>
<img loading="lazy" src="img/ls-2.png" alt=""  />
<br>
<img loading="lazy" src="img/ls-3.png" alt=""  />
<br>点击<strong>Run New Analysis</strong>
<img loading="lazy" src="img/ls-4.png" alt=""  />
<br>软件运行结果与论文中的Table-1数值是一样的。(额， 准备的实验数据中单词dislike我拼写成了dislikeed的。)
<br>
<img loading="lazy" src="img/ls-6.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理世界 | 机器学习如何赋能管理学研究？——国内外前沿综述和未来展望</title>
      <link>https://textdata.cn/blog/2023-10-11-how-can-machine-learning-empower-management-research/</link>
      <pubDate>Wed, 11 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-10-11-how-can-machine-learning-empower-management-research/</guid>
      <description>机器学习正在深刻改变管理学的研究范式与方法。如何运用机器学习更好地赋能管理学研究已经成为学术界关注的前沿热点议题。然而，机器学习在中国管理学研究中的应用仍处于初级阶段。**本文基于1999～2021年发表在工商管理和会计财务两大研究领域的国内外顶级期刊的学术文献，识别了学术界借助机器学习开展管理学实证研究的4种核心途径：变量测量、事件预测（包括事件分类）、因果推断和理论构建**；梳理了每个途径的代表性文献的研究主题、研究问题、数据集、机器学习算法和研究结论；提出了使用机器学习赋能管理学研究的主要策略，并讨论了中国学者运用机器学习开展中国特色管理理论研究的未来机会。本文显示：将机器学习与传统计量经济学相结合有助于做出更加精准的因果推断；机器学习能够在模式发现这一理论构建的关键步骤中发挥重要作用；将机器学习与多案例分析相结合有助于富有成效地开展理论构建。本文为如何采用机器学习提升管理学研究质量、推进管理学研究范式变革和构建中国特色管理理论提供了方法论指引和方向性启示。</description>
      <content:encoded><![CDATA[<h2 id="一论文">一、论文</h2>
<p>刘景江,郑畅然,洪永淼.<strong>机器学习如何赋能管理学研究？——国内外前沿综述和未来展望</strong>[J].<strong>管理世界</strong>,2023,39(09):191-216.</p>
<p>摘要: 机器学习正在深刻改变管理学的研究范式与方法。如何运用机器学习更好地赋能管理学研究已经成为学术界关注的前沿热点议题。然而，机器学习在中国管理学研究中的应用仍处于初级阶段。<strong>本文基于1999～2021年发表在工商管理和会计财务两大研究领域的国内外顶级期刊的学术文献，识别了学术界借助机器学习开展管理学实证研究的4种核心途径：变量测量、事件预测（包括事件分类）、因果推断和理论构建</strong>；梳理了每个途径的代表性文献的研究主题、研究问题、数据集、机器学习算法和研究结论；提出了使用机器学习赋能管理学研究的主要策略，并讨论了中国学者运用机器学习开展中国特色管理理论研究的未来机会。本文显示：将机器学习与传统计量经济学相结合有助于做出更加精准的因果推断；机器学习能够在模式发现这一理论构建的关键步骤中发挥重要作用；将机器学习与多案例分析相结合有助于富有成效地开展理论构建。本文为如何采用机器学习提升管理学研究质量、推进管理学研究范式变革和构建中国特色管理理论提供了方法论指引和方向性启示。</p>
<br>
<br>
<h2 id="二文献范围">二、文献范围</h2>
<p>首先，本文选取 <strong>UTD-24</strong> 期刊，以“machine learning”、“decision tree”、“support vector machine”、“random forest”、“artificial neural network”和“deep learning”等为关键词，对目标期刊的所有在库文章进 行全篇检索，把正式发表时间限定到 2021 年 12 月末，得到一张包含 1258 篇文献的初步文献清单。其中，会计 领域 52 篇，财务领域 72 篇，信息系统领域 322 篇，营销领域 208 篇，管理科学领域 522 篇，工商管理领域 82 篇。 <strong>考虑到篇幅有限和用途梳理的全面性，本文只关注工商管理和会计财务两大研究领域</strong>  。</p>
<p>接着，类似地，本文选取“2021 中国最具国际影响力学术期刊（人文社会科学）”前 20 名中的管理学期刊， 用相似的关键词，搜寻到 2004~2021 年且运用机器学习方法进行实证研究的文章， 符合标准的论文，有工商管理 15 篇，会计财务 28 篇。</p>
<p><img loading="lazy" src="img/20231011-%e5%8f%91%e6%96%87%e8%b6%8b%e5%8a%bf-%e5%9b%bd%e9%99%85.png" alt=""  />
</p>
<p><img loading="lazy" src="img/20231011-%e5%8f%91%e6%96%87%e8%b6%8b%e5%8a%bf-%e5%9b%bd%e5%86%85.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三机器学习4大核心用途">三、机器学习4大核心用途</h2>
<p>在确定研究目标后，我们按照以下 3 个步骤对 数据进行编码和文献分析。 第一步，根据以往理论和实证研究，我们总结出机器学习方法在管理学实证研究中的 4 种核心用途: <strong>变量测量、事件预测、因果推断和理论构建</strong>，如图 3 所示。</p>
<p><img loading="lazy" src="img/20231011-%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a04%e5%a4%a7%e6%a0%b8%e5%bf%83%e7%94%a8%e9%80%94.png" alt=""  />
</p>
<ul>
<li>
<p><strong>变量测量</strong>是根据一种规则，用数量的方法描述研究对象所具备的某种特征或行为，其目标是对变量之间的关系进行量化推断（陈晓萍等，2008）。</p>
</li>
<li>
<p><strong>事件预测</strong>是使用已掌握的经验或知识，预先推知和判断事物未来发 展状况（阿西，2019），其目标是预料来自不同观测总体的样本已经或将要在未来实现的结果（格里默等， 2021）。</p>
</li>
<li>
<p><strong>因果推断</strong>是借助理论和对制度细节的深入了解，估计事件和选择对给定结果的影响（坎宁安，2021）， 其目标是比较在同一干预措施下不同反事实（Counterfactual）结果之间的差异（格里默等，2021）。</p>
</li>
<li>
<p><strong>理论构建</strong>是 构建概念及其相互关系，以展示一种现象是如何和为什么发生的过程（焦亚、皮特雷，1990；科利、焦亚，2011； 克里斯蒂安森、钱丹，2017），其目标是建立稳健且具有可解释性的理论。</p>
</li>
</ul>
<p>变量测量、事件预测、因果推断和理论构建是管理学实证研究的 4 项关键任务。 它们既相互区别又紧密关 联。 理论构建在管理学实证研究中占据着核心地位（班伯格，2018）。 管理学顶级期刊格外强调文章的理论贡献（科利、焦亚，2011）。 实证研究的核心目标是理论构建。 衡量一个“好”的实证研究的首要标准是它能够建立稳健且具有可解释性的理论。 因果推断是理论构建的先决条件。 事件预测是因果推断的必要前提。 变量 测量是开展管理学实证研究的根基。 <strong>总之，这 4 个途径相辅相成，构成目的与手段的关系，「变量测量」 是  「事件预测、因果推断和理论构建」 的基础。</strong></p>
<p><br><br></p>
<h2 id="四机器学习在管理学研究中的应用">四、机器学习在管理学研究中的应用</h2>
<p>工商管理和会计财务作为管理学的两大核心研究领域，包含大量来自个人、企业和政府的文本、图像、音 频、视频等极具信息价值的非结构化数据。 传统方法无法对这些非结构化数据进行量化分析，只能进行定性分析。 借助机器学习方法，学者们可以从这些非结构化数据中挖掘、提取和构建诸如高管人格特质、管理者自恋、公司文化、媒体文章语调和投资者情绪等有意义的变量（洪永淼、汪寿阳，2021a，2021b），运用灵活的函数形式和降维技术来实现更精准的预测（洪永淼、汪寿阳，2021b，2021c），利用正则化和交叉验证方法提高模型泛化能力以帮助因果推断和理论构建（蒂德尔、艾森哈特，2020；蒂芬，2019；乔杜里等，2021；瓦里安，2014），从而更好地开展这两大领域中关键问题的实证研究。 因此，本部分以这两大研究领域为例，以机器学习赋能管理学研究的 4 种核心用途为主线，全面回顾和系统梳理 UTD-24 期刊和国内顶级管理学期刊于 1999~2021 年正式发表的文章。 具体来说，本文遵循重点性原则和典型性原则，按照这些领域和用途，总结归纳了代表性文献 的研究主题、研究问题、数据集、机器学习算法和研究结论 ⑧ 。</p>
<h3 id="41-工商管理">4.1 工商管理</h3>
<p><img loading="lazy" src="img/20231011-table-1.png" alt=""  />
</p>
<p><img loading="lazy" src="img/20231011-table-2.png" alt=""  />
</p>
<h3 id="42-会计学">4.2 会计学</h3>
<p><img loading="lazy" src="img/20231011-table-3.png" alt=""  />
</p>
<p><img loading="lazy" src="img/20231011-table-4.png" alt=""  />
</p>
<p><img loading="lazy" src="img/20231011-table-5.png" alt=""  />
</p>
<p><br><br></p>
<p>&hellip;&hellip;</p>
<h2 id="五结论与讨论">五、结论与讨论</h2>
<p>以弥补管理学研究传统上所存在的短板为目标，本研究采用 1999~2021 年发表在工商管理和会计财务两 大研究领域的国内外顶级期刊的学术文献，识别了学术界借助机器学习赋能管理学实证研究的核心途径；从 多个角度系统梳理了这些途径的代表性文献；详细阐述了机器学习赋能管理学研究的主要策略，并重点讨论 了中国学者运用机器学习开展中国特色管理理论研究的未来机会（主题方向、重要问题、实施策略和主要建议）。 本研究得出如表 6 所示的主要结论。 可以预见的是，在未来，变量测量、事件预测、因果推断、理论构建等 4 种核心途径的融合将日益紧密。 它们的融合为机器学习赋能管理学研究提供了更加具有深度和广度的未来机会。 例如，事件预测可以用来揭示数据中难以假设的复杂和未知关系，开发新的理论构念及其测量，或者按照预测的相对精准度比较竞争理论（克鲁帕、米努蒂-梅扎，2022），从而更好地进行理论构建。</p>
<p><img loading="lazy" src="img/20231011-table-6.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理科学学报 | 使用LDA算法计算政策扩散速度与扩散程度</title>
      <link>https://textdata.cn/blog/2023-10-10-measure-the-speed-of-policy-diffusion-from-top-to-down/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-10-10-measure-the-speed-of-policy-diffusion-from-top-to-down/</guid>
      <description>价值不断提升的政府网站内容数据不仅可以描绘政策注意力，也为中央政策向地方层级扩散的测量与评估提供了新的机遇.在我国多层级政府组织治理模式下，地方政府对中央政策的贯彻落地是政策生效的前提条件.对纵向政策扩散的有效测量和评估将有助于理解政策扩散机制，提升政策落地效果.本文基于全国省、市级政府门户网站每日内容更新数据，通过**概率主题建模方法建构主题概率矩阵，刻画政府对不同主题的注意力分配差异，并基于概率主题建模结果构建函数测量地方政府对中央政策的扩散速度与扩散程度。本文讨论了测度建构的原理和细节，并引入机器学习方法进行鲁棒性检验**，通过多政策主题扩散的混合回归分析了影响短周期政策层级扩散的因素.研究以测度建构为突破口打通文本数据挖掘到有价值公共管理知识的“中间层”,对政策信息学在政策扩散及评估监测中的应用前景进行了初步探索.</description>
      <content:encoded><![CDATA[<h2 id="一文献">一、文献</h2>
<p>张楠,黄梅银,罗亚,马宝君.<strong>全国政府网站内容数据中的知识发现：从注意力分配到政策层级扩散</strong>[J].<strong>管理科学学报</strong>,2023,26(05):154-173.</p>
<p>摘要:价值不断提升的政府网站内容数据不仅可以描绘政策注意力，也为中央政策向地方层级扩散的测量与评估提供了新的机遇.在我国多层级政府组织治理模式下，地方政府对中央政策的贯彻落地是政策生效的前提条件.对纵向政策扩散的有效测量和评估将有助于理解政策扩散机制，提升政策落地效果.本文基于全国省、市级政府门户网站每日内容更新数据，通过<strong>概率主题建模方法建构主题概率矩阵，刻画政府对不同主题的注意力分配差异，并基于概率主题建模结果构建函数测量地方政府对中央政策的扩散速度与扩散程度。本文讨论了测度建构的原理和细节，并引入机器学习方法进行鲁棒性检验</strong>，通过多政策主题扩散的混合回归分析了影响短周期政策层级扩散的因素.研究以测度建构为突破口打通文本数据挖掘到有价值公共管理知识的“中间层”,对政策信息学在政策扩散及评估监测中的应用前景进行了初步探索.</p>
<p><br><br></p>
<h2 id="二数据处理">二、数据处理</h2>
<h3 id="21-数据准备">2.1. 数据准备</h3>
<p>基于获取到的 170 万余条政府网站内容数据， 本文选择潜在狄利克雷分配模型 (LDA)进行数据分析，以获取网络政府的政策议题注意力分布情况，数据处理路径见图 １．</p>
<p>数据抓取单位为政府门户网站每一个页面的内容信息， 包括页 面 ＵＲＬ 地址、标题、发布时间、文章发布单位或转载来源、关键词、作者、摘要、具体内容等． 数据入库前，还通过元素提取（如网页名称、大小、日期、 标题、文字内容等）、数据排重和信息过滤（广告过滤、ＵＲＬ 过滤等）等前期处理工作．</p>
<p><img loading="lazy" src="img/20231010-%e5%9f%ba%e4%ba%8eLDA%e7%9a%84%e6%94%bf%e5%ba%9c%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%88%86%e9%85%8d%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86%e8%b7%af%e5%be%84.png" alt=""  />
</p>
<br>
<h3 id="22-lda建模">2.2. LDA建模</h3>
<p>LDA建模有两个步骤</p>
<ol>
<li>首先最关键的是确定<strong>文档主题数</strong>，即平均困惑度。论文中平均困惑度为120。</li>
<li>确定好<strong>文档主题数</strong>即可开展LDA训练， 对170w训练出LDA模型，同时得到<strong>文档-主题概率矩阵</strong>， 该矩阵的形状，有120列， 170多万行。<br>
<img loading="lazy" src="img/20231010-%e7%a1%ae%e5%ae%9aLDA%e5%b9%b3%e5%9d%87%e5%9b%b0%e6%83%91%e5%ba%a6%e5%80%bc.png" alt=""  />
</li>
</ol>
<p>训练完LDA模型，虽然文档主题数设置为120， 但经过甄别，最终确定有112个主题具有可解释性。
<img loading="lazy" src="img/20231010-LDA%e4%b8%bb%e9%a2%98%e5%88%97%e8%a1%a8.png" alt=""  />
</p>
<p>下图是主题含义，及概率占比(面积大小)。
<img loading="lazy" src="img/20231010-%e6%94%bf%e5%ba%9c%e7%bd%91%e7%ab%99%e5%af%b9%e4%b8%8d%e5%90%8c%e4%b8%bb%e9%a2%98%e7%9a%84%e6%b3%a8%e6%84%8f%e5%8a%9b%e5%88%86%e9%85%8d%e6%83%85%e5%86%b5.png" alt=""  />
</p>
<br>
<h2 id="23-扩散速度与扩散程度函数构建">2.3. 扩散速度与扩散程度函数构建</h2>
<p><img loading="lazy" src="img/20231010-%e6%94%bf%e7%ad%96%e6%bf%80%e5%8a%b1%e5%93%8d%e5%ba%94%e6%97%b6%e9%97%b4%e7%82%b9.png" alt=""  />
</p>
<p><br><img loading="lazy" src="img/20231010-%e6%94%bf%e7%ad%96%e6%89%a9%e6%95%a3%e9%80%9f%e5%ba%a6.png" alt=""  />

<br><img loading="lazy" src="img/20231010-%e5%93%8d%e5%ba%94%e6%94%bf%e7%ad%96%e7%9a%84%e6%8c%81%e7%bb%ad%e6%80%a7%e5%9b%9e%e5%ba%94%e7%a8%8b%e5%ba%a61.png" alt=""  />
</p>
<p><img loading="lazy" src="img/20231010-%e5%93%8d%e5%ba%94%e6%94%bf%e7%ad%96%e7%9a%84%e6%8c%81%e7%bb%ad%e6%80%a7%e5%9b%9e%e5%ba%94%e7%a8%8b%e5%ba%a62.png" alt=""  />
<br></p>
<p>面对中央政府希望通过政府网站和其他网络政府入口监测政策落实、督查政府履职、评估回应能力的一系列需求， 本文尝试基于网络政 府大数据的对中央政策扩散情况展开分析． 图４ 展示了 2018 年地级市对中央 １３ 项政策的回应扩散速度情况． <strong>曲线越扁平，地级市政府扩散响应 时间越短， 层级扩散速度越快</strong>． 平均扩散速度为 20.04 天，意味着中央出台政策后地级市政府网站 上平均 20 天就会对中央政策予以回应． 其中， 地 级市政府回应最快的是医疗卫生监管主题， 平均 扩散时间为 12.07  天，最慢的是土地使用权主题，达 25.11 天． 从 0.5 分位数来看，当不同政策主题的中央政策激励产生后， 超过一半的城市在 20 天内快速响应中央政策， 不同政策主题扩散速度存在差异．</p>
<p><img loading="lazy" src="img/20231010-%e6%94%bf%e5%ba%9c%e5%93%8d%e5%ba%94%e9%80%9f%e5%ba%a6.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>企业ESG行为的文本度量法</title>
      <link>https://textdata.cn/blog/2023-10-07-esg-measurement/</link>
      <pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-10-07-esg-measurement/</guid>
      <description>本文一个关键的贡献是使用机器学习方法从文本数据中评估初创公司环境、社会和治理（ESG）属性</description>
      <content:encoded><![CDATA[<h2 id="文献">文献</h2>
<p>Mansouri S, Momtaz P P. Financing sustainable entrepreneurship: ESG measurement, valuation, and performance[J]. <em>Journal of Business Venturing</em>, 2022, 37(6):106258.</p>
<br>
<h2 id="摘要">摘要</h2>
<p>可持续发展导向对初创企业的初始估值有积极影响，但对其融资后财务业绩有负面影响。 在其他条件相同的情况下，将可持续发展方向提高一个标准差将使初创公司的融资金额增加 28%，并将投资者每个融资后年度的异常回报减少 16%。 结果适用于基于区块链的众筹活动（也称为首次代币发行（ICO）或代币发行）的大量样本。<strong>本文一个关键的贡献是使用机器学习方法从文本数据中评估初创公司环境、社会和治理（ESG）属性</strong></p>
<br>
<br>
<h2 id="量化初创企业的esg属性">量化初创企业的ESG属性</h2>
<p>现有研究对如何衡量初创企业的ESG属性还未形成统一框架，且存在以下两个问题：（1）现有的ESG指标主要由几个数据供应商提供，而供应商之间的相关性非常低；（2）现有的ESG评级不适用于初创企业，即存在数据缺失。因此，本文采用一种机器学习的方法，量化初创企业的ESG属性：</p>
<ol>
<li>
<p><strong>文本预处理</strong>： 获取数据及预处理从公司网站等收集ICO白皮书后，使用斯坦福大学开发的CoreNLP管道生成句子的依赖性表示，并识别一些搭配词；</p>
</li>
<li>
<p><strong>建立种子词</strong>：收集《金融时报》中所有带有“ESG投资、道德金钱”标签的文章，采用标准的词袋模型提炼出现频率最高的二元组、三元组词汇，然后对这些词汇进行人工筛查，并在此基础上手动添加一些与代币发行有关的词汇，得到三个维度的种子词数为：70、38、46；</p>
</li>
<li>
<p><strong>选取联想词</strong>：使用Word2vec模型扩充种子词，为ESG的每个维度挑选500个最为相近的术语，经再次筛查后，得到三个维度的词典数量为：508、463、524；</p>
</li>
<li>
<p><strong>计算ESG分数</strong></p>
<p><img loading="lazy" src="formular.png" alt=""  />
</p>
<p>在（1）式中，代表白皮书i中术语的计数，c(n)是相应的单词列表的大小，即用频率来表征企业在某一维度的得分，然后将三个维度的得分加总得到最终的ESG分数；</p>
</li>
</ol>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>中国管理科学 | 使用业绩说明会文本数据测量上市公司前瞻性信息</title>
      <link>https://textdata.cn/blog/2023-09-08-earnings-communication-conference-forward-looking-statements-information/</link>
      <pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-09-08-earnings-communication-conference-forward-looking-statements-information/</guid>
      <description>业绩说明会， 是我国上市公司和中小投资 者沟通交流的重要载体。 在年报披露后， 能够 帮助投资者快速、准确地抓取信息披露重点， 全面了解企业发展状况， 增进对企业价值及经 营理念的认同。上市公司的业绩说明会是金融领域中的重要事件，它为投资者、分析师和其他利益相关者提供了一个与公司管理层直接交流的平台。这种数据集的学术价值多方面体现。</description>
      <content:encoded><![CDATA[<p>最近几个月没怎么分享长技术文，正好昨天分享的付费数据集涉及到一篇论文，感觉用到了很多Python的地方，就想着做一期。这篇论文的Python实现，技术要点有两个部分</p>
<ol>
<li><strong>「构建词典」</strong>； 训练word2vec预训练语言模型，并使用该模型扩展出<strong>前瞻性词典集</strong></li>
<li><strong>「算前瞻性指标</strong>」； 根据<strong>前瞻性词典集</strong>,统计每个企业业绩说明会内的前瞻性词在总词数中的比例</li>
</ol>
<p>这两部分，分别对应本文 <strong>「二、实验-构建词典」</strong>、<strong>「三、计算前瞻性」</strong>。</p>
<p><strong>内容较长， 可能对初学小白不友好。 学完大邓课程「<a href="https://textdata.cn/blog/management_python_course/">Python实证指标构建与文本分析</a>」的同学，阅读起来会轻松一些</strong>。</p>
<p><br><br></p>
<p><strong>许帅,邵帅,何贤杰.业绩说明会前瞻性信息对分析师盈余预测准确性的影响——信口雌黄还是言而有征[J].中国管理科学:1-15.</strong></p>
<blockquote>
<p>摘要:本文以2007—2020年上市公司业绩说明会为背景，研究前瞻性信息披露对分析师预测的影响，发现业绩说明会中的前瞻性信息可以显著提升分析师盈余预测准确性。公司的信息不对称程度越高，前瞻性信息对分析师预测准确性提升越多。分析师专长工作经验越丰富，具备更强的信息捕捉能力，可以更好地吸收与理解业绩说明会中的前瞻性信息，做出更准确的预测。进一步，本文对前瞻性信息影响分析师预测的路径进行了讨论，认为前瞻性信息可能通过吸引分析师和机构投资者调研，增进分析师对上市公司经营状况的了解，进而提升盈余预测准确性。此外，本文发现，前瞻性信息中业绩相关类信息因具有更高的可信度，且与盈余因子直接相关，能够显著提升分析师盈余预测准确性。本研究为管理层披露与分析师的互动研究提供了增量证据，研究结果支持了业绩说明会有效性，对未来监管部门制定相关信息披露政策提供依据和建议。</p>
</blockquote>
<p><br><br></p>
<h2 id="一前瞻性指标衡量">一、前瞻性指标衡量</h2>
<p>本文关注业绩说明会中前瞻性信息披露的比重， 借鉴Li [5] 、Muslu等 [6] 和马黎珺等 [14] 对前瞻性信息的定义， 采用 “ 词袋法 ” 构建前瞻性 指标， <strong>运用Python软件中jieba中文分词技术统计在问答阶段前瞻性词汇词频占业绩说明会文本总词频（去除停用词）的比例</strong>。 同时， 手工剔除了诸如“请关注后续公告”、“详见以后公告”等不具备实质性前瞻性信息的词频。</p>
<p>在词典的选取上， 本文前瞻性词典集借鉴胡楠和薛付婧 [15] 的种子词汇， 为了保证词汇的全面性， 还将所有种子词导入到开源分析工具 word2vec中， 并在业绩说明会语料库中寻找与种子词内容接近程度最高的词汇，其中包含（1） 管理团队的预测，譬如“计划/预计/预测”等表 述；（2）出现未来时点的表述， 譬如“未来/以 后/明年/下半年”等表述；（3）暗示企业即将发 生的动作， 譬如“有望/后续”等表述， 共计 174个前瞻性词汇（详见附录2）。前瞻性指标比重越大，表明公司的前瞻性信息披露越多。</p>
<p><img loading="lazy" src="img/formular.png" alt=""  />
</p>
<p><img loading="lazy" src="img/dict1.png" alt=""  />
</p>
<br>
<h2 id="二实验-构建词典">二、实验-构建词典</h2>
<h3 id="21-整理数据">2.1 整理数据</h3>
<p>把 <a href="https://textdata.cn/blog/2023-09-08-china-a-share-market-listed-company-earnings-communication-conference/"><strong>数据集 | 84w条业绩说明会问答数据(2005-2023)</strong></a>汇总到一个txt文件内。为了保证问答上下文一致， 问答要放在相邻处。 可能需要安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install openpyxl
pip3 install pandas
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;业绩说明会问答05-23.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;提问内容&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;回答内容&#39;</span><span class="p">]</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;业绩说明会05-23.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    
    <span class="c1">#为了保证问答上下文一致， 问答要放在相邻处</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">())</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="22-训练word2vec">2.2 训练word2vec</h3>
<p>一般都是使用gensim库，对  <strong>「业绩说明会05-23.txt」</strong> 数据集进行训练，我已经封装到的cntext库内。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext==1.8.6
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>


<span class="c1">#Init W2VModels. Support English and Chinese</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>  <span class="c1">#corpus data w2v_corpus.txt</span>


<span class="c1">#训练结束后，「业绩说明会05-23.100.6.bin」会出现在「output/Word2Vec」文件夹内 </span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;业绩说明会05-23.txt&#39;</span><span class="p">,</span> 
            <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;业绩说明会05-23.100.6.bin&#39;</span><span class="p">)</span>
</code></pre></div><br>
<p>需要注意， output/word2vec文件夹内会同时含有</p>
<ul>
<li><strong>业绩说明会05-23.100.6.bin</strong></li>
<li><strong>业绩说明会05-23.100.6.bin.vectors.npy</strong></li>
</ul>
<p>两个文件都不要删除， 这些是预训练词向量文件。</p>
<br>
<h3 id="23-扩展词典">2.3 扩展词典</h3>
<p>根据前瞻性研究需要，整理了一些种子词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">seedwords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;计划&#39;</span><span class="p">,</span> <span class="s1">&#39;预计&#39;</span><span class="p">,</span> <span class="s1">&#39;未来&#39;</span><span class="p">,</span> <span class="s1">&#39;目标&#39;</span><span class="p">,</span> <span class="s1">&#39;可能&#39;</span><span class="p">,</span> <span class="s1">&#39;如果&#39;</span><span class="p">,</span> <span class="s1">&#39;机遇&#39;</span><span class="p">,</span> <span class="s1">&#39;预期&#39;</span><span class="p">,</span> <span class="s1">&#39;挑战&#39;</span><span class="p">,</span> <span class="s1">&#39;预测&#39;</span><span class="p">,</span> <span class="s1">&#39;今后&#39;</span><span class="p">,</span> <span class="s1">&#39;目的&#39;</span><span class="p">,</span> <span class="s1">&#39;契机&#39;</span><span class="p">,</span> <span class="s1">&#39;前景&#39;</span><span class="p">,</span> <span class="s1">&#39;希望&#39;</span><span class="p">,</span> <span class="s1">&#39;展望&#39;</span><span class="p">,</span> <span class="s1">&#39;相信&#39;</span><span class="p">,</span> <span class="s1">&#39;愿景&#39;</span><span class="p">,</span> <span class="s1">&#39;期待&#39;</span><span class="p">,</span> <span class="s1">&#39;明年&#39;</span><span class="p">,</span> <span class="s1">&#39;期望&#39;</span><span class="p">]</span>
</code></pre></div><ol>
<li>导入word2vec预训练语言模型文件 <strong>业绩说明会05-23.100.6.bin</strong></li>
<li>寻找与种子词语义最相似的n个词。</li>
<li>经过人工检查，剔除n个词中与 <strong>前瞻性</strong> 无关的词语，最终得到 <strong>前瞻性词典</strong>(论文中是174个词)。</li>
</ol>
<p>但是，经过大邓测试发现业绩说明会训练得到的业绩说明会word2vec(05-23.100.6.bin)模型表现很差。</p>
<p>之前大邓用01-21年管理层讨论与分析训练过一个word2vec(<strong>mda01-21.200.6.bin</strong>)，</p>
<blockquote>
<p><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">预训练模型 | 金融会计类word2vec， 可扩展或构建领域内概念情感词典</a></p>
</blockquote>
<p>在这次前瞻性扩展词任务中，mda01-21.200.6.bin表现要远好于05-23.100.6.bin。</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="k">def</span> <span class="nf">load_w2v</span><span class="p">(</span><span class="n">w2v_path</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    Load word2vec model
</span><span class="s2">
</span><span class="s2">    Args:
</span><span class="s2">        w2v_path (str): path of word2vec model
</span><span class="s2">
</span><span class="s2">    Returns:
</span><span class="s2">        model: word2vec model
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loading word2vec model...&#39;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="n">wv</span> <span class="o">=</span> <span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;Embeddings/业绩说明会05-23.100.6.bin&#39;</span><span class="p">)</span>
<span class="n">wv2</span> <span class="o">=</span> <span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;Embeddings/mda01-21.200.6.bin&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>Loading word2vec model...
Loading word2vec model...
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#词汇量</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">wv2</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">))</span>
</code></pre></div><pre><code>198776
789539
</code></pre>
<p>​ <br>
<br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查询某词的词向量</span>
<span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;创新&#39;</span><span class="p">)</span>

<span class="c1">#查询多个词的词向量</span>
<span class="c1">#wv.get_mean_vector([&#39;创新&#39;, &#39;研发&#39;])</span>
</code></pre></div><pre><code>array([ 0.43675017,  0.74739504,  3.3765798 , -0.29287583,  0.40125442,
    0.9364979 ,  0.62465197,  0.06480039,  0.12256158, -2.0735328 ,
   -0.256066  , -1.7680115 , -0.8514873 , -0.756108  ,  1.3441261 ,
   -0.18098126,  2.7290103 , -4.6596766 ,  0.4046495 , -4.0644083 ,
    0.6022293 ,  1.3569978 ,  1.0036035 ,  0.06123297, -2.0733726 ,
    2.2704456 , -1.2935334 , -0.2855776 ,  1.588003  ,  1.5027634 ,
    2.0897112 , -0.8861778 ,  0.4014722 , -0.41474393, -1.5390201 ,
    0.23899865, -0.9823706 , -2.986944  , -2.6887195 , -2.2386284 ,
    0.04810223,  1.3241886 , -0.71262985, -0.8015585 ,  1.5249555 ,
   -3.611584  , -1.4187033 , -1.6014036 ,  0.816903  ,  3.1821172 ,
   -1.7302881 , -0.8280679 , -1.2833163 ,  0.65565586, -0.8857021 ,
    2.098562  ,  1.4773984 ,  1.0931807 , -0.02242889,  1.1279039 ,
   -2.2318523 ,  0.24540211,  0.17126203,  2.5631666 , -1.7135285 ,
    0.60896975, -0.2654438 ,  0.5718087 , -1.4996717 ,  1.0189433 ,
    1.0205768 ,  3.7439635 , -0.3575424 , -3.189775  ,  0.6117708 ,
   -0.60615975,  2.940066  , -0.89338064, -0.626806  , -1.4389508 ,
   -1.1291629 , -2.2354846 , -0.6873424 ,  1.9574465 , -1.2231802 ,
    1.2850708 , -0.7581777 ,  0.8184319 ,  1.542834  , -0.8685869 ,
    1.1841776 , -0.4524089 , -0.8068617 ,  0.01519055, -0.23408687,
   -0.51564324,  0.20584114,  0.14295417,  0.5481142 ,  2.523313  ],
  dtype=float32)
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">seedwords</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    According to the seed word file, select the top n words with the most similar semantics and save them in the directory save_dir.
</span><span class="s2">    
</span><span class="s2">    Args:
</span><span class="s2">        wv (Word2VecKeyedVectors): the word embedding model
</span><span class="s2">        seedwords (list): 种子词
</span><span class="s2">        topn (int, optional): Set the number of most similar words to retrieve to topn. Defaults to 100.
</span><span class="s2">        save_dir (str, optional): the directory to save the candidate words. Defaults to &#39;Word2Vec&#39;.
</span><span class="s2">    
</span><span class="s2">    Returns:
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">simidx_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">similars_candidate_idxs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#the candidate words of seedwords</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span>
    <span class="n">seedidxs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#transform word to index</span>
    <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seedwords</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
            <span class="n">seedidx</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">seed</span><span class="p">]</span>
            <span class="n">seedidxs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seedidx</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">seedidx</span> <span class="ow">in</span> <span class="n">seedidxs</span><span class="p">:</span>
        <span class="c1"># sims_words such as [(&#39;by&#39;, 0.99984), (&#39;or&#39;, 0.99982), (&#39;an&#39;, 0.99981), (&#39;up&#39;, 0.99980)]</span>
        <span class="n">sims_words</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">seedidx</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="n">topn</span><span class="p">)</span>
        <span class="c1">#Convert words to index and store them</span>
        <span class="n">similars_candidate_idxs</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">dictionary</span><span class="p">[</span><span class="n">sim</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="k">for</span> <span class="n">sim</span> <span class="ow">in</span> <span class="n">sims_words</span><span class="p">])</span>
    <span class="n">similars_candidate_idxs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">similars_candidate_idxs</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">similars_candidate_idxs</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="n">idx</span><span class="p">],</span> <span class="n">seedidxs</span><span class="p">)</span>
        <span class="n">simidx_scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">score</span><span class="p">))</span>
    <span class="n">simidxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">simidx_scores</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>

    <span class="n">simwords</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">simidxs</span><span class="p">][:</span><span class="n">topn</span><span class="p">]</span>

    <span class="n">resultwords</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">resultwords</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">seedwords</span><span class="p">)</span>
    <span class="n">resultwords</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">simwords</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">resultwords</span>


<span class="c1">#为了节省板面，这里设置为50</span>
<span class="c1">#论文中经过筛选留下174个词，实际上topn应该远大于174，</span>
<span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="c1">#前瞻性种子词</span>
                  <span class="n">seedwords</span><span class="o">=</span> <span class="p">[</span><span class="s1">&#39;计划&#39;</span><span class="p">,</span> <span class="s1">&#39;预计&#39;</span><span class="p">,</span> <span class="s1">&#39;未来&#39;</span><span class="p">,</span> <span class="s1">&#39;目标&#39;</span><span class="p">,</span> <span class="s1">&#39;可能&#39;</span><span class="p">,</span> <span class="s1">&#39;如果&#39;</span><span class="p">,</span> <span class="s1">&#39;机遇&#39;</span><span class="p">,</span> <span class="s1">&#39;预期&#39;</span><span class="p">,</span> <span class="s1">&#39;挑战&#39;</span><span class="p">,</span> <span class="s1">&#39;预测&#39;</span><span class="p">,</span> <span class="s1">&#39;今后&#39;</span><span class="p">,</span> <span class="s1">&#39;目的&#39;</span><span class="p">,</span> <span class="s1">&#39;契机&#39;</span><span class="p">,</span> <span class="s1">&#39;前景&#39;</span><span class="p">,</span> <span class="s1">&#39;希望&#39;</span><span class="p">,</span> <span class="s1">&#39;展望&#39;</span><span class="p">,</span> <span class="s1">&#39;相信&#39;</span><span class="p">,</span> <span class="s1">&#39;愿景&#39;</span><span class="p">,</span> <span class="s1">&#39;期待&#39;</span><span class="p">,</span> <span class="s1">&#39;明年&#39;</span><span class="p">,</span> <span class="s1">&#39;期望&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;计划&#39;,
 &#39;预计&#39;,
 ......
 &#39;几年&#39;,
 &#39;积极影响&#39;,
 &#39;有何&#39;,
 &#39;谢谢您提问&#39;,
 &#39;今后&#39;,
 &#39;这块&#39;,
 &#39;近几年&#39;,
 &#39;近两年&#39;,
 &#39;请问李&#39;,
 &#39;裁员&#39;,
 &#39;亮点&#39;,
 &#39;准备采取&#39;,
 &#39;将会&#39;,
 &#39;接下来&#39;,
 &#39;有何规划&#39;,
 &#39;前景&#39;,
 &#39;管理层是否&#39;,
 &#39;未来几年&#39;,
 &#39;有没有新&#39;,
 &#39;发展状况&#39;,
 &#39;一块&#39;,
 &#39;当前&#39;,
 &#39;很大&#39;,
 &#39;这块业务&#39;,
 &#39;LNG船&#39;,
 &#39;具体措施您好&#39;,
 &#39;当下&#39;,
 &#39;是否能够&#39;,
 &#39;明后&#39;,
 &#39;一个台阶&#39;,
 &#39;是否符合&#39;,
 &#39;巨大&#39;,
 &#39;预判&#39;,
 &#39;对此&#39;,
 &#39;未来三年&#39;,
 &#39;资本开支&#39;,
 &#39;不少&#39;,
 &#39;未来是否&#39;,
 &#39;这方面&#39;,
 &#39;看法&#39;,
 &#39;今年以来&#39;,
 &#39;疫情结束&#39;,
 &#39;想知道&#39;,
 &#39;取得不错&#39;,
 &#39;谈谈&#39;,
 &#39;一步&#39;,
 &#39;今年是否&#39;,
 &#39;发展前景&#39;,
 &#39;东宝&#39;,
 &#39;现状&#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv2</span><span class="p">,</span> 
                  <span class="c1">#前瞻性种子词</span>
                  <span class="n">seedwords</span><span class="o">=</span> <span class="p">[</span><span class="s1">&#39;计划&#39;</span><span class="p">,</span> <span class="s1">&#39;预计&#39;</span><span class="p">,</span> <span class="s1">&#39;未来&#39;</span><span class="p">,</span> <span class="s1">&#39;目标&#39;</span><span class="p">,</span> <span class="s1">&#39;可能&#39;</span><span class="p">,</span> <span class="s1">&#39;如果&#39;</span><span class="p">,</span> <span class="s1">&#39;机遇&#39;</span><span class="p">,</span> <span class="s1">&#39;预期&#39;</span><span class="p">,</span> <span class="s1">&#39;挑战&#39;</span><span class="p">,</span> <span class="s1">&#39;预测&#39;</span><span class="p">,</span> <span class="s1">&#39;今后&#39;</span><span class="p">,</span> <span class="s1">&#39;目的&#39;</span><span class="p">,</span> <span class="s1">&#39;契机&#39;</span><span class="p">,</span> <span class="s1">&#39;前景&#39;</span><span class="p">,</span> <span class="s1">&#39;希望&#39;</span><span class="p">,</span> <span class="s1">&#39;展望&#39;</span><span class="p">,</span> <span class="s1">&#39;相信&#39;</span><span class="p">,</span> <span class="s1">&#39;愿景&#39;</span><span class="p">,</span> <span class="s1">&#39;期待&#39;</span><span class="p">,</span> <span class="s1">&#39;明年&#39;</span><span class="p">,</span> <span class="s1">&#39;期望&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;计划&#39;,
 &#39;预计&#39;,
  ......
 &#39;相信&#39;,
 &#39;将会&#39;,
 &#39;未来&#39;,
 &#39;希望&#39;,
 &#39;预见&#39;,
 &#39;预期&#39;,
 &#39;可能&#39;,
 &#39;必将&#39;,
 &#39;应该&#39;,
 &#39;未来几年&#39;,
 &#39;今后&#39;,
 &#39;有望&#39;,
 &#39;目标&#39;,
 &#39;这一&#39;,
 &#39;当前&#39;,
 &#39;当下&#39;,
 &#39;无疑&#39;,
 &#39;期望&#39;,
 &#39;接下来&#39;,
 &#39;意味着&#39;,
 &#39;背景&#39;,
 &#39;期待&#39;,
 &#39;近期&#39;,
 &#39;下一阶段&#39;,
 &#39;机会&#39;,
 &#39;看到&#39;,
 &#39;预示&#39;,
 &#39;能够&#39;,
 &#39;短期内&#39;,
 &#39;未来一段时间&#39;,
 &#39;将来&#39;,
 &#39;展望未来&#39;,
 &#39;必须&#39;,
 &#39;真正&#39;,
 &#39;眼光&#39;,
 &#39;必然&#39;,
 &#39;还会&#39;,
 &#39;预计&#39;,
 &#39;未来十年&#39;,
 &#39;机遇&#39;,
 &#39;可能性&#39;,
 &#39;后续&#39;,
 &#39;潜在&#39;,
 &#39;决心&#39;,
 &#39;信心&#39;,
 &#39;仍然&#39;,
 &#39;非常&#39;,
 &#39;这为&#39;,
 &#39;未来五年&#39;,
 &#39;短时间&#39;]
</code></pre></div><p><strong>大邓假装经过很多检查，剔除不相关词语，最终跟论文一样，都得到了174个前瞻性词语。  需要说明，大邓已经将174个词内置到了cntext库(1.8.6中)</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import cntext as ct

#cntext已内置了论文中的174个前瞻性词集
fls_words = ct.load_pkl_dict(&#39;Chinese_FLS.pkl&#39;)[&#39;Chinese_FLS&#39;]
fls_words
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;计划&#39;,
 &#39;预计&#39;,
 &#39;未来&#39;,
 &#39;目标&#39;,
 ......
 &#39;企业宗旨&#39;,
 &#39;宗旨&#39;,
 &#39;该愿景&#39;,
 &#39;愿望&#39;,
 &#39;心愿&#39;,
 &#39;盼望&#39;,
 &#39;祝愿&#39;,
 &#39;今年年底&#39;,
 &#39;今年底&#39;,
 &#39;明年初&#39;,
 &#39;第二季度&#39;,
 &#39;上半年&#39;,
 &#39;下半年&#39;,
 &#39;本月底&#39;,
 &#39;下周&#39;,
 &#39;马上&#39;,
 &#39;厚望&#39;,
 &#39;期盼&#39;,
 &#39;鞭策&#39;,
 &#39;梦想&#39;,
 &#39;愿&#39;]
</code></pre></div><p><br><br></p>
<h2 id="三计算前瞻性">三、计算前瞻性</h2>
<ol>
<li>汇总记录； 将同一年同一家上市公司的所有问答合并为一条记录，存储于df2中。</li>
<li>设计前瞻性计算函数 <strong>compute_fls</strong></li>
<li>对df2[&lsquo;text&rsquo;]使用前瞻性计算函数<strong>compute_fls</strong>，计算结果保存到字段<strong>Forward</strong></li>
</ol>
<h3 id="31-汇总记录">3.1 汇总记录</h3>
<p>将同一年同一家上市公司的所有问答合并为一条记录，存储于新df的text中。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">])[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="n">df2</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">df2</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<h3 id="32--设计前瞻性计算函数compute_fls">3.2  设计前瞻性计算函数compute_fls</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">jieba</span>


<span class="n">fls_words</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;Chinese_FLS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;Chinese_FLS&#39;</span><span class="p">]</span>
<span class="n">stopwords</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;STOPWORDS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;STOPWORDS&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">compute_fls</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">num</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">fls_words</span><span class="p">:</span>
            <span class="n">num</span><span class="o">+=</span><span class="mi">1</span>
    <span class="c1">#+1是为了防止分母为0的情况</span>
    <span class="k">return</span> <span class="n">num</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="33-批量计算df2text">3.3 批量计算df2[&lsquo;text&rsquo;]</h3>
<p>对df2[&lsquo;text&rsquo;]批量使用前瞻性计算函数<strong>compute_fls</strong>，计算结果保存到字段<strong>Forward</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df2[&#39;Forward&#39;] = df2[&#39;text&#39;].apply(compute_fls)

df2.head()
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<br>
<p>下图是论文中的Forward描述性统计，</p>
<p><img loading="lazy" src="img/stats.png" alt=""  />
</p>
<p>我们试着看看分析结果 <strong>df2[&lsquo;Forward&rsquo;]</strong> 的</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Forward最小值: &#39;</span><span class="p">,</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Forward&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Forward中位数: &#39;</span><span class="p">,</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Forward&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Forward均值: &#39;</span><span class="p">,</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Forward&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Forward最大值: &#39;</span><span class="p">,</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Forward&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Forward标准层: &#39;</span><span class="p">,</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Forward&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
</code></pre></div><p>可以发现描述性统计信息与论文的存在较大差异，可能的原因包括但不限于</p>
<pre><code>  1. 数据集存在差异。**论文中选取2007-2020年中小板和创业板上市公司作为研究对象。**而本实验使用的A股2005年-2023年所有的上市公司作为实验对象。
  2. 可能筛选记录，文本太短的会议剔除。
  3. 使用的停用词表不同
  4. jieba导入自定义词典
</code></pre>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 | 84w条业绩说明会问答数据(2005-2023)</title>
      <link>https://textdata.cn/blog/2023-09-08-china-a-share-market-listed-company-earnings-communication-conference/</link>
      <pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-09-08-china-a-share-market-listed-company-earnings-communication-conference/</guid>
      <description>业绩说明会， 是我国上市公司和中小投资 者沟通交流的重要载体。 在年报披露后， 能够 帮助投资者快速、准确地抓取信息披露重点， 全面了解企业发展状况， 增进对企业价值及经 营理念的认同。上市公司的业绩说明会是金融领域中的重要事件，它为投资者、分析师和其他利益相关者提供了一个与公司管理层直接交流的平台。这种数据集的学术价值多方面体现。</description>
      <content:encoded><![CDATA[<p>业绩说明会， 是我国上市公司和中小投资 者沟通交流的重要载体。 在年报披露后， 能够 帮助投资者快速、准确地抓取信息披露重点， 全面了解企业发展状况， 增进对企业价值及经 营理念的认同。上市公司的业绩说明会是金融领域中的重要事件，它为投资者、分析师和其他利益相关者提供了一个与公司管理层直接交流的平台。这种数据集的学术价值多方面体现。</p>
<ul>
<li>公司沟通策略的研究：业绩说明会的数据可以帮助研究者深入了解公司如何与公众沟通其财务状况、业务策略和未来展望。这对于传播学、公关和企业战略研究领域都是宝贵的。</li>
<li>情感分析与市场反应：通过对业绩说明会中的语言和情感进行分析，研究者可以探索市场对公司信息披露的反应。这对于金融经济学和计量经济学的研究尤为重要。</li>
<li>公司治理与透明度：业绩说明会的频率、内容和与投资者的互动可以为研究者提供关于公司治理质量和透明度的线索。</li>
<li>预测模型的建立：这种数据集可以用于建立预测模型，预测公司的未来业绩、股价走势或其他相关指标。</li>
<li>行为金融学的研究：业绩说明会中的问题和答案可以为研究者提供关于投资者和分析师行为和心理的深入了解，从而深化我们对市场非理性行为的理解。</li>
<li>宏观经济指标的研究：通过对多家公司的业绩说明会数据进行汇总和分析，研究者可以获得宏观经济趋势和行业动态的宝贵见解。</li>
</ul>
<p>总之，上市公司业绩说明会数据集为学术界提供了一个独特的、多维度的研究视角，有助于深化我们对金融市场、公司策略和投资者行为的理解。</p>
<p><br><br></p>
<h2 id="数据集介绍">数据集介绍</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">上市公司业绩说明会问答数据

【年度】
 2005-2023年

【字段】
 - 股票代码
 - 会计年度
 - 问题序号
 - 提问内容
 - 提问时间
 - 回答人
 - 回答时间
 - 回答内容
 
 【数据量】
  841876
</code></pre></div><p><a href="code.ipynb">点击获取代码</a></p>
<p><a href="https://mp.weixin.qq.com/s/w7BWPwyI_UxQ6zprC96-qw">点击获取数据</a></p>
<p><br><br></p>
<h2 id="导入数据">导入数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;业绩说明会问答05-23.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="s1">&#39;数据集覆盖的年度: </span><span class="si">{start}</span><span class="s1">~</span><span class="si">{end}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">start</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> 
                                 <span class="n">end</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</code></pre></div><pre><code>'数据集覆盖的年度: 2005~2023'
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#数据量</span>
<span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><pre><code>841876
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#字段包括</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div><pre><code>Index(['股票代码', '会计年度', '问题序号', '提问内容', '提问时间', '回答人', '回答时间', '回答内容'], dtype='object')
</code></pre>
<br>
<p>设置matplotlib</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="c1">#pip3 install scienceplots</span>
<span class="kn">import</span> <span class="nn">scienceplots</span> 
<span class="kn">import</span> <span class="nn">platform</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>

<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#年份变化(业绩说明会数量)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span>
</code></pre></div><pre><code>会计年度
2005     4042
2006    10051
2007    18906
2008    31782
2009    35802
2010    47141
2011    69439
2012    73231
2013    80456
2014    80690
2015    62764
2016    61820
2017    52543
2018    44279
2019    42009
2020    37026
2021    53898
2022    35917
2023       80
Name: count, dtype: int64
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;会计年度&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;业绩说明会数量&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;业绩说明会数量年份变化&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/output_5_0.svg" alt="svg"  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">])[</span><span class="s1">&#39;问题序号&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;会计年度&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;会计年度&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;年度问答次数&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;业绩说明会平均问答次数年份变化&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/output_6_0.svg" alt="svg"  />
</p>
<p><br><br></p>
<h2 id="相关文献">相关文献</h2>
<p>许帅,邵帅,何贤杰.业绩说明会前瞻性信息对分析师盈余预测准确性的影响——信口雌黄还是言而有征[J].中国管理科学:1-15.</p>
<blockquote>
<p>摘要:本文以2007—2020年上市公司业绩说明会为背景，研究前瞻性信息披露对分析师预测的影响，发现业绩说明会中的前瞻性信息可以显著提升分析师盈余预测准确性。公司的信息不对称程度越高，前瞻性信息对分析师预测准确性提升越多。分析师专长工作经验越丰富，具备更强的信息捕捉能力，可以更好地吸收与理解业绩说明会中的前瞻性信息，做出更准确的预测。进一步，本文对前瞻性信息影响分析师预测的路径进行了讨论，认为前瞻性信息可能通过吸引分析师和机构投资者调研，增进分析师对上市公司经营状况的了解，进而提升盈余预测准确性。此外，本文发现，前瞻性信息中业绩相关类信息因具有更高的可信度，且与盈余因子直接相关，能够显著提升分析师盈余预测准确性。本研究为管理层披露与分析师的互动研究提供了增量证据，研究结果支持了业绩说明会有效性，对未来监管部门制定相关信息披露政策提供依据和建议。</p>
</blockquote>
<br>
<p>卞世博,管之凡,阎志鹏.答非所问与市场反应:基于业绩说明会的研究[J].管理科学学报,2021,24(04):109-126.</p>
<blockquote>
<p>摘要:对上市公司业绩说明会中投资者与管理层问答互动中管理层答非所问的现象进行了研究.本文以中小板和创业板上市公司召开的业绩说明会作为研究样本,利用文本分析方法对业绩说明会中管理层在回答投资者提问时答非所问的程度进行度量,进而实证分析了管理层的答非所问与市场反应和公司未来业绩表现之间的可能关联.结果发现:在控制其它因素之后,管理层的答非所问与市场反应之间呈现显著的负相关关系,即公司管理层的答非所问程度越高,随后公司股票的市场表现则就会越差,并且对于那些低分析师关注的公司尤为明显;而在公司未来业绩表现方面,管理层答非所问的程度越高,则公司未来的业绩表现则会越差.</p>
</blockquote>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>CAR2023 | 文本分析在会计中的应用</title>
      <link>https://textdata.cn/blog/2023-08-26-text-analysis-in-accounting/</link>
      <pubDate>Sat, 26 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-08-26-text-analysis-in-accounting/</guid>
      <description>&lt;h2 id=&#34;一文本分析在会计领域中的应用&#34;&gt;一、文本分析在会计领域中的应用&lt;/h2&gt;
&lt;p&gt;Bochkay, Khrystyna, Stephen V. Brown, Andrew J. Leone, and Jennifer Wu Tucker. &amp;ldquo;Textual analysis in accounting: What&amp;rsquo;s next?.&amp;rdquo; &lt;em&gt;Contemporary accounting research&lt;/em&gt; 40, no. 2 (2023): 765-805.&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;11-摘要&#34;&gt;1.1 摘要&lt;/h3&gt;
&lt;p&gt;自然语言是商务沟通的重要形式。 文本分析是指用自然语言处理（NLP）技术处理文本数据，从而得到某些感兴趣的测量值(信息)。 我们调查了顶级会计期刊的出版物，并描述了会计文本分析的趋势和现状。 我们将可用的 NLP 方法组织在一个统一的框架中。 会计研究者经常使用文本分析来衡量披露情绪、可读性和披露数量； 比较披露信息以确定相似性或差异；识别前瞻性信息； 并检测主题。 对于每一项任务，我们都解释了传统方法和基于机器学习（尤其是深度学习）的新方法。 我们讨论如何建立基于文本的测量的构造有效性以及研究人员在实施 NLP 模型时面临的典型决策。 最后，我们讨论了未来研究的机会。 我们的结论是：(i) 文本分析已发展成为一种重要的研究方法，(ii) 会计研究人员应该增加对机器学习（尤其是深度学习）的了解和使用，以进行文本分析。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;12-发文趋势&#34;&gt;1.2 发文趋势&lt;/h3&gt;
&lt;p&gt;会计顶刊文本分析发文量如下图&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Accounting Review(TAR),&lt;/p&gt;
&lt;p&gt;Journal of Accounting Research(JAR),&lt;/p&gt;
&lt;p&gt;Journal of Accounting andEconomics(JAE),&lt;/p&gt;
&lt;p&gt;Contemporary Accounting Research(CAR),&lt;/p&gt;
&lt;p&gt;Review of Accounting Studies(RAST),&lt;/p&gt;
&lt;p&gt;Accounting, Organizations,and Society(AOS),&lt;/p&gt;
&lt;p&gt;ManagementScience(MS).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/fig-1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/fig-2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;13-数据源所用指标&#34;&gt;1.3 数据源&amp;amp;所用指标&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/fig-3.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二文本分析操作方法&#34;&gt;二、文本分析操作方法&lt;/h2&gt;
&lt;p&gt;文本分析各方法指南&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据获取&amp;amp;预处理步骤&lt;/li&gt;
&lt;li&gt;词典选择(构建)步骤&lt;/li&gt;
&lt;li&gt;监督机器学习步骤&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;21-数据获取预处理&#34;&gt;2.1 数据获取&amp;amp;预处理&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;数据获取&lt;/strong&gt; 使用人工手动或爬虫从EDGAR、公司网站、社交媒体等数据源采集下载&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据清洗&lt;/strong&gt;  剔除HTML中的标签、非文本字符、特殊字符(如&amp;amp; ￥ $ 等)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分词&lt;/strong&gt; 将文本转为颗粒度为词语的成分&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文档筛选&lt;/strong&gt;  字符数太短的文档删除掉&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;停用词&lt;/strong&gt; 剔除文本中的停用词，如(中文如“的他呢了地”，英文如 the、in、a)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;合并同类项(stemming&amp;amp;lemmatization)&lt;/strong&gt;  文本中出现的increasing, increases, and increased，都整理为increase。&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-词典选择构建步骤&#34;&gt;2.2 词典选择(构建)步骤&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;选择词典&lt;/strong&gt; 选择符合研究目的的词典，如做文本的情感分析，可以选择用积极词典和消极词典。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;词频统计&lt;/strong&gt;  统计词语是否出现，还是统计词语出现次数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;词语权重&lt;/strong&gt;  确定所有计数是否具有相同的权重，或者某些单词或短语应获得更高或更低的权重（例如，更常见的单词获得更低的权重）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;词典验证&lt;/strong&gt; 将字典在识别相关内容方面的表现与人工注释者进行比较。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;确定指标&lt;/strong&gt; 确定最终感兴趣变量的标量（例如，文档中的总单词数）&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-监督机器学习步骤&#34;&gt;2.3 监督机器学习步骤&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;结果变量&lt;/strong&gt;  决定如何表示感兴趣的变量：(i) 连续变量或 (ii) 分类变量&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;标注数据集&lt;/strong&gt;  收集带标注信息的样本数据（例如，带标签的单词、句子、段落或文章）。标注平台（例如，Prodigy、Amazon Mechanical Turk、TagEditor、SMART 和 piaf）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分为训练集、测试集&lt;/strong&gt;  将带标注的数据集拆分为子样本以进行训练、验证和测试。 确保感兴趣的变量的每个类别都有很好的代表性&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型选择&lt;/strong&gt;  如果采用深度学习模型，请决定模型（例如 BERT）以及是否对模型进行微调。 如果使用传统机器学习，请选择特定模型（例如 NB、SVM 或 RF）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文本特征工程&lt;/strong&gt; 使用词袋法或者词嵌入&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估模型&lt;/strong&gt;  确定评估模型性能的指标。 选项包括准确度、精确度、召回率、F 分数和 ROC-AUC&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型拟合&lt;/strong&gt;  使用带注释的数据拟合模型，检查验证数据上的模型性能，并确定是否需要更多带注释的示例。 这是一个迭代的过程&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;确定指标&lt;/strong&gt; 确定最终感兴趣变量的标量（例如，文档中的总单词数）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一文本分析在会计领域中的应用">一、文本分析在会计领域中的应用</h2>
<p>Bochkay, Khrystyna, Stephen V. Brown, Andrew J. Leone, and Jennifer Wu Tucker. &ldquo;Textual analysis in accounting: What&rsquo;s next?.&rdquo; <em>Contemporary accounting research</em> 40, no. 2 (2023): 765-805.</p>
<br>
<h3 id="11-摘要">1.1 摘要</h3>
<p>自然语言是商务沟通的重要形式。 文本分析是指用自然语言处理（NLP）技术处理文本数据，从而得到某些感兴趣的测量值(信息)。 我们调查了顶级会计期刊的出版物，并描述了会计文本分析的趋势和现状。 我们将可用的 NLP 方法组织在一个统一的框架中。 会计研究者经常使用文本分析来衡量披露情绪、可读性和披露数量； 比较披露信息以确定相似性或差异；识别前瞻性信息； 并检测主题。 对于每一项任务，我们都解释了传统方法和基于机器学习（尤其是深度学习）的新方法。 我们讨论如何建立基于文本的测量的构造有效性以及研究人员在实施 NLP 模型时面临的典型决策。 最后，我们讨论了未来研究的机会。 我们的结论是：(i) 文本分析已发展成为一种重要的研究方法，(ii) 会计研究人员应该增加对机器学习（尤其是深度学习）的了解和使用，以进行文本分析。</p>
<br>
<h3 id="12-发文趋势">1.2 发文趋势</h3>
<p>会计顶刊文本分析发文量如下图</p>
<blockquote>
<p>The Accounting Review(TAR),</p>
<p>Journal of Accounting Research(JAR),</p>
<p>Journal of Accounting andEconomics(JAE),</p>
<p>Contemporary Accounting Research(CAR),</p>
<p>Review of Accounting Studies(RAST),</p>
<p>Accounting, Organizations,and Society(AOS),</p>
<p>ManagementScience(MS).</p>
</blockquote>
<p><img loading="lazy" src="img/fig-1.png" alt=""  />
</p>
<p><img loading="lazy" src="img/fig-2.png" alt=""  />
</p>
<br>
<h3 id="13-数据源所用指标">1.3 数据源&amp;所用指标</h3>
<p><img loading="lazy" src="img/fig-3.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二文本分析操作方法">二、文本分析操作方法</h2>
<p>文本分析各方法指南</p>
<ol>
<li>数据获取&amp;预处理步骤</li>
<li>词典选择(构建)步骤</li>
<li>监督机器学习步骤</li>
</ol>
<h3 id="21-数据获取预处理">2.1 数据获取&amp;预处理</h3>
<ol>
<li><strong>数据获取</strong> 使用人工手动或爬虫从EDGAR、公司网站、社交媒体等数据源采集下载</li>
<li><strong>数据清洗</strong>  剔除HTML中的标签、非文本字符、特殊字符(如&amp; ￥ $ 等)</li>
<li><strong>分词</strong> 将文本转为颗粒度为词语的成分</li>
<li><strong>文档筛选</strong>  字符数太短的文档删除掉</li>
<li><strong>停用词</strong> 剔除文本中的停用词，如(中文如“的他呢了地”，英文如 the、in、a)</li>
<li><strong>合并同类项(stemming&amp;lemmatization)</strong>  文本中出现的increasing, increases, and increased，都整理为increase。</li>
</ol>
<br>
<h3 id="22-词典选择构建步骤">2.2 词典选择(构建)步骤</h3>
<ol>
<li><strong>选择词典</strong> 选择符合研究目的的词典，如做文本的情感分析，可以选择用积极词典和消极词典。</li>
<li><strong>词频统计</strong>  统计词语是否出现，还是统计词语出现次数</li>
<li><strong>词语权重</strong>  确定所有计数是否具有相同的权重，或者某些单词或短语应获得更高或更低的权重（例如，更常见的单词获得更低的权重）。</li>
<li><strong>词典验证</strong> 将字典在识别相关内容方面的表现与人工注释者进行比较。</li>
<li><strong>确定指标</strong> 确定最终感兴趣变量的标量（例如，文档中的总单词数）</li>
</ol>
<br>
<h3 id="23-监督机器学习步骤">2.3 监督机器学习步骤</h3>
<ol>
<li><strong>结果变量</strong>  决定如何表示感兴趣的变量：(i) 连续变量或 (ii) 分类变量</li>
<li><strong>标注数据集</strong>  收集带标注信息的样本数据（例如，带标签的单词、句子、段落或文章）。标注平台（例如，Prodigy、Amazon Mechanical Turk、TagEditor、SMART 和 piaf）</li>
<li><strong>分为训练集、测试集</strong>  将带标注的数据集拆分为子样本以进行训练、验证和测试。 确保感兴趣的变量的每个类别都有很好的代表性</li>
<li><strong>模型选择</strong>  如果采用深度学习模型，请决定模型（例如 BERT）以及是否对模型进行微调。 如果使用传统机器学习，请选择特定模型（例如 NB、SVM 或 RF）</li>
<li><strong>文本特征工程</strong> 使用词袋法或者词嵌入</li>
<li><strong>评估模型</strong>  确定评估模型性能的指标。 选项包括准确度、精确度、召回率、F 分数和 ROC-AUC</li>
<li><strong>模型拟合</strong>  使用带注释的数据拟合模型，检查验证数据上的模型性能，并确定是否需要更多带注释的示例。 这是一个迭代的过程</li>
<li><strong>确定指标</strong> 确定最终感兴趣变量的标量（例如，文档中的总单词数）</li>
</ol>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Pandas库 | 对高管数据xlsx中的简介字段做文本分析</title>
      <link>https://textdata.cn/blog/2023-08-07-using-str-contains-method-to-judge-some-specific-content-in-excel/</link>
      <pubDate>Mon, 07 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-08-07-using-str-contains-method-to-judge-some-specific-content-in-excel/</guid>
      <description>&lt;h2 id=&#34;一高管数据集&#34;&gt;一、高管数据集&lt;/h2&gt;
&lt;h3 id=&#34;11-介绍&#34;&gt;1.1 介绍&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-11-25-senior-manager-resume-dataset/&#34;&gt;数据集 | 90w条中国上市公司高管数据&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;90w 条中国上市公司高管简历，数据源-新浪财经，统计的日期范围&lt;strong&gt;1990-2021&lt;/strong&gt;年。&lt;/p&gt;
&lt;h3 id=&#34;12-字段&#34;&gt;1.2 字段&lt;/h3&gt;
&lt;p&gt;数据集的字段含，大多是从「个人简历」中计算衍生出来的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- ID
- 姓名
- 证券代码
- 统计截止日期
- 个人简历
- 国籍
- 籍贯
- 籍贯所在地区代码
- 出生地
- 出生地所在地区代码
- 性别
- 年龄
- 毕业院校
- 学历  1=中专及中专以下； 2=大专； 3=本科； 4=硕士研究生； 5=博士研究生； 6=其他（以其他形式公布的学历，如荣誉博士、函授等）； 7=MBA/EMBA
- 专业
- 职称
- 是否领取薪酬
- 报告期报酬总额
- 年末持股数
- 是否高管团队成员
- 是否董事会成员
- 是否独立董事
- 是否兼任董事长和CEO
- 是否监事
- 具体职务
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;13-应用价值&#34;&gt;1.3 应用价值&lt;/h3&gt;
&lt;p&gt;这里粘贴部分应用高管数据论文&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;何瑛,于文蕾,戴逸驰,王砚羽.高管职业经历与企业创新[J].管理世界,2019,35(11):174-192.&lt;/li&gt;
&lt;li&gt;杨林,和欣,顾红芳.高管团队经验、动态能力与企业战略突变：管理自主权的调节效应[J].管理世界,2020,36(06):168-188+201+252.&lt;/li&gt;
&lt;li&gt;周楷唐,麻志明,吴联生.高管学术经历与公司债务融资成本[J].经济研究,2017,52(07):169-183.&lt;/li&gt;
&lt;li&gt;陆瑶,张叶青,黎波,赵浩宇.高管个人特征与公司业绩——基于机器学习的经验证据[J].管理科学学报,2020,23(02):120-140.&lt;/li&gt;
&lt;li&gt;柳光强,孔高文.高管经管教育背景与企业内部薪酬差距[J].会计研究,2021,(03):110-121.&lt;/li&gt;
&lt;li&gt;郑建明,孙诗璐,李金甜.高管文化背景与企业债务成本——基于劳模文化的视角[J].会计研究,2021,(03):137-145.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二代码案例&#34;&gt;二、代码案例&lt;/h2&gt;
&lt;p&gt;用Python实现以下五个技术难题，主要对高管简介进行操作&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;读取xlsx文件（90w高管数据）&lt;/li&gt;
&lt;li&gt;简介文本中是否含指定词语(例如找出有【清华大学】求学经历的高管)&lt;/li&gt;
&lt;li&gt;大学高管数量排行榜&lt;/li&gt;
&lt;li&gt;统计文本中指定词语出现次数(例如统计每位高管内【大学】出现次数)&lt;/li&gt;
&lt;li&gt;找出每位高管的出生年份(用正则表达式)&lt;/li&gt;
&lt;li&gt;统计每位高管经历的时间点个数
&amp;hellip;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;21-导入数据&#34;&gt;2.1 导入数据&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_excel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;高管数据.xlsx&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#剔除「个人简历」字段中的缺失值&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-简介文本长度&#34;&gt;2.2 简介文本长度&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#新增一个字段length，将简介文本长度保存到length中&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#df[&amp;#39;length&amp;#39;] = df[&amp;#39;个人简历&amp;#39;].str.len()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;0         161
1         154
2         395
3         306
4         335
         ... 
900882     40
900883     54
900884     71
900885     41
900886     62
Name: 个人简历, Length: 736970, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-简介文本中是否含指定词语&#34;&gt;2.3 简介文本中是否含指定词语&lt;/h3&gt;
&lt;p&gt;例如找出有【清华大学】求学经历的高管,这里直接使用**Series.str.contains()**方法来直接搜某字段(Series)是否含某个词&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;len(df[df[&#39;个人简历&#39;].str.contains(&#39;清华大学&#39;)])&lt;/code&gt; 保留有「清华大学」学习经历的高管&lt;/li&gt;
&lt;li&gt;&lt;code&gt;len(df[df[&#39;个人简历&#39;].str.contains(&#39;北京大学&#39;)])&lt;/code&gt; 保留有「北京大学」学习经历的高管&lt;/li&gt;
&lt;li&gt;&lt;code&gt;len(df[df[&#39;个人简历&#39;].str.contains(&#39;清华大学|北京大学&#39;)])&lt;/code&gt; 保留有「清华大学」或「北京大学」学习经历的高管&lt;/li&gt;
&lt;li&gt;&lt;code&gt;len(df[df[&#39;个人简历&#39;].str.contains(&#39;清华大学&#39;) &amp;amp; df[&#39;个人简历&#39;].str.contains(&#39;北京大学&#39;)])&lt;/code&gt; 保留同时有「清华大学」和「北京大学」学习经历的高管&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第三个(北大清华)表达式的数量应该是最多的(前两者之和)， 第四个表达式是最少。 注意, 逻辑【或|】【且&amp;amp;】可以有任意多个&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#统计有【清华大学】学习经历的高管人数&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;清华大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;10377
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;北京大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;8709
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;清华大学|北京大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;18647
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;清华大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;北京大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;439
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h3 id=&#34;24-大学高管数量排行榜&#34;&gt;2.4 大学高管数量排行榜&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#测试列表(凭记忆手动输入的大学，各位可以自己设计测试列表)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;test_universitys&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;清华大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;北京大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;中国人民大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;浙江大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;s1&#34;&gt;&amp;#39;上海交通大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;西安交通大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;同济大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;南开大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;天津大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;s1&#34;&gt;&amp;#39;武汉大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;华中科技大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;中国科学技术大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;南京大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;s1&#34;&gt;&amp;#39;中山大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;中南大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;四川大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;重庆大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;兰州大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;湖南大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;s1&#34;&gt;&amp;#39;山东大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;吉林大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;大连理工大大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;东北大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;北京航空航天大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;中国地质大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;


&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;大学高管人数排行&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;uni_infos&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;university&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;test_universitys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;num&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;university&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;uni_infos&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;university&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    
&lt;span class=&#34;n&#34;&gt;uni_infos&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;sorted&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uni_infos&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reverse&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;uni_infos&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;大学高管人数排行

[(&#39;清华大学&#39;, 10377),
 (&#39;北京大学&#39;, 8709),
 (&#39;中国人民大学&#39;, 7012),
 (&#39;浙江大学&#39;, 5816),
 (&#39;中山大学&#39;, 4065),
 (&#39;上海交通大学&#39;, 3844),
 (&#39;武汉大学&#39;, 3578),
 (&#39;南京大学&#39;, 3272),
 (&#39;西安交通大学&#39;, 2972),
 (&#39;南开大学&#39;, 2716),
 (&#39;湖南大学&#39;, 2502),
 (&#39;华中科技大学&#39;, 2356),
 (&#39;同济大学&#39;, 2089),
 (&#39;吉林大学&#39;, 2044),
 (&#39;四川大学&#39;, 1934),
 (&#39;山东大学&#39;, 1847),
 (&#39;中南大学&#39;, 1615),
 (&#39;天津大学&#39;, 1598),
 (&#39;重庆大学&#39;, 1440),
 (&#39;北京航空航天大学&#39;, 1334),
 (&#39;东北大学&#39;, 1241),
 (&#39;中国科学技术大学&#39;, 842),
 (&#39;兰州大学&#39;, 745),
 (&#39;中国地质大学&#39;, 437),
 (&#39;大连理工大大学&#39;, 0)]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h3 id=&#34;25-统计文本中指定词语出现次数&#34;&gt;2.5 统计文本中指定词语出现次数&lt;/h3&gt;
&lt;p&gt;例如统计每位高管内【大学】出现次数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0         0
1         2
2         0
3         0
4         0
         ..
900882    0
900883    0
900884    0
900885    0
900886    0
Name: 个人简历, Length: 736970, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;高管总人数: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#简历中无「大学」字眼&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;无大学经历高管人数:&amp;#39;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#简历中有「大学」字眼&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;有大学经历高管人数:&amp;#39;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;高管总人数:  736970
无大学经历高管人数: 515172
有大学经历高管人数: 221798
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#有些企业单位名字中带有「大学」，但这类企业非常少。&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#「大学」词语出现次数可以近似看做学习经历次数&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#如此， 1可以看做本科学历，2看做研究生学历， 3看做博士学历&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;大学&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value_counts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;normalize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;bar&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;output_14_1.png&#34; alt=&#34;png&#34;  /&gt;

​&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;26-找出每位高管的出生年份用正则表达式&#34;&gt;2.6 找出每位高管的出生年份(用正则表达式)&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;\d&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{4}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0                                                    [1969]
1                      [1965, 1984, 1986, 1990, 1994, 1995]
2         [1972, 1998, 1999, 2000, 2015, 2002, 2016, 200...
3         [1960, 1982, 1989, 1990, 1991, 1991, 2002, 200...
4         [1962, 2009, 1985, 1996, 1996, 2008, 1993, 200...
                                ...                        
900882                                                   []
900883                                                   []
900884                                                   []
900885                                                   []
900886                                                   []
Name: 个人简历, Length: 736970, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;birth_year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;#返回出生年份&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;#没有年份的，返回0&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
    
    
&lt;span class=&#34;c1&#34;&gt;#高管出生年份&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;\d&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{4}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;birth_year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0         1969
1         1965
2         1972
3         1960
4         1962
          ... 
900882       0
900883       0
900884       0
900885       0
900886       0
Name: 个人简历, Length: 736970, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#高管时间点个数(感觉可以看做经历的个数)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;个人简历&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;\d&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{4}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0          1
1          6
2         10
3         10
4          8
          ..
900882     0
900883     0
900884     0
900885     0
900886     0
Name: 个人简历, Length: 736970, dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;获取代码&#34;&gt;获取代码&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;top-ceo.ipynb&#34;&gt;&lt;strong&gt;点击下载ipynb代码文件&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一高管数据集">一、高管数据集</h2>
<h3 id="11-介绍">1.1 介绍</h3>
<p><a href="https://textdata.cn/blog/2022-11-25-senior-manager-resume-dataset/">数据集 | 90w条中国上市公司高管数据</a></p>
<p>90w 条中国上市公司高管简历，数据源-新浪财经，统计的日期范围<strong>1990-2021</strong>年。</p>
<h3 id="12-字段">1.2 字段</h3>
<p>数据集的字段含，大多是从「个人简历」中计算衍生出来的。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- ID
- 姓名
- 证券代码
- 统计截止日期
- 个人简历
- 国籍
- 籍贯
- 籍贯所在地区代码
- 出生地
- 出生地所在地区代码
- 性别
- 年龄
- 毕业院校
- 学历  1=中专及中专以下； 2=大专； 3=本科； 4=硕士研究生； 5=博士研究生； 6=其他（以其他形式公布的学历，如荣誉博士、函授等）； 7=MBA/EMBA
- 专业
- 职称
- 是否领取薪酬
- 报告期报酬总额
- 年末持股数
- 是否高管团队成员
- 是否董事会成员
- 是否独立董事
- 是否兼任董事长和CEO
- 是否监事
- 具体职务
</code></pre></div><br>
<h3 id="13-应用价值">1.3 应用价值</h3>
<p>这里粘贴部分应用高管数据论文</p>
<ul>
<li>何瑛,于文蕾,戴逸驰,王砚羽.高管职业经历与企业创新[J].管理世界,2019,35(11):174-192.</li>
<li>杨林,和欣,顾红芳.高管团队经验、动态能力与企业战略突变：管理自主权的调节效应[J].管理世界,2020,36(06):168-188+201+252.</li>
<li>周楷唐,麻志明,吴联生.高管学术经历与公司债务融资成本[J].经济研究,2017,52(07):169-183.</li>
<li>陆瑶,张叶青,黎波,赵浩宇.高管个人特征与公司业绩——基于机器学习的经验证据[J].管理科学学报,2020,23(02):120-140.</li>
<li>柳光强,孔高文.高管经管教育背景与企业内部薪酬差距[J].会计研究,2021,(03):110-121.</li>
<li>郑建明,孙诗璐,李金甜.高管文化背景与企业债务成本——基于劳模文化的视角[J].会计研究,2021,(03):137-145.</li>
</ul>
<p><br><br></p>
<h2 id="二代码案例">二、代码案例</h2>
<p>用Python实现以下五个技术难题，主要对高管简介进行操作</p>
<ol>
<li>读取xlsx文件（90w高管数据）</li>
<li>简介文本中是否含指定词语(例如找出有【清华大学】求学经历的高管)</li>
<li>大学高管数量排行榜</li>
<li>统计文本中指定词语出现次数(例如统计每位高管内【大学】出现次数)</li>
<li>找出每位高管的出生年份(用正则表达式)</li>
<li>统计每位高管经历的时间点个数
&hellip;</li>
</ol>
<h3 id="21-导入数据">2.1 导入数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;高管数据.xlsx&#39;</span><span class="p">)</span>
<span class="c1">#剔除「个人简历」字段中的缺失值</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><br>
<h3 id="22-简介文本长度">2.2 简介文本长度</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len</span><span class="p">()</span>

<span class="c1">#新增一个字段length，将简介文本长度保存到length中</span>
<span class="c1">#df[&#39;length&#39;] = df[&#39;个人简历&#39;].str.len()</span>
</code></pre></div><pre><code>0         161
1         154
2         395
3         306
4         335
         ... 
900882     40
900883     54
900884     71
900885     41
900886     62
Name: 个人简历, Length: 736970, dtype: int64
</code></pre>
<br>
<h3 id="23-简介文本中是否含指定词语">2.3 简介文本中是否含指定词语</h3>
<p>例如找出有【清华大学】求学经历的高管,这里直接使用**Series.str.contains()**方法来直接搜某字段(Series)是否含某个词</p>
<ul>
<li><code>len(df[df['个人简历'].str.contains('清华大学')])</code> 保留有「清华大学」学习经历的高管</li>
<li><code>len(df[df['个人简历'].str.contains('北京大学')])</code> 保留有「北京大学」学习经历的高管</li>
<li><code>len(df[df['个人简历'].str.contains('清华大学|北京大学')])</code> 保留有「清华大学」或「北京大学」学习经历的高管</li>
<li><code>len(df[df['个人简历'].str.contains('清华大学') &amp; df['个人简历'].str.contains('北京大学')])</code> 保留同时有「清华大学」和「北京大学」学习经历的高管</li>
</ul>
<p>第三个(北大清华)表达式的数量应该是最多的(前两者之和)， 第四个表达式是最少。 注意, 逻辑【或|】【且&amp;】可以有任意多个</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#统计有【清华大学】学习经历的高管人数</span>
<span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;清华大学&#39;</span><span class="p">)])</span>
</code></pre></div><p>Run</p>
<pre><code>10377
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;北京大学&#39;</span><span class="p">)])</span>
</code></pre></div><p>Run</p>
<pre><code>8709
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;清华大学|北京大学&#39;</span><span class="p">)])</span>
</code></pre></div><p>Run</p>
<pre><code>18647
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;清华大学&#39;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;北京大学&#39;</span><span class="p">)])</span>
</code></pre></div><p>Run</p>
<pre><code>439
</code></pre>
<br>
<h3 id="24-大学高管数量排行榜">2.4 大学高管数量排行榜</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#测试列表(凭记忆手动输入的大学，各位可以自己设计测试列表)</span>
<span class="n">test_universitys</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;清华大学&#39;</span><span class="p">,</span> <span class="s1">&#39;北京大学&#39;</span><span class="p">,</span> <span class="s1">&#39;中国人民大学&#39;</span><span class="p">,</span> <span class="s1">&#39;浙江大学&#39;</span><span class="p">,</span> 
                    <span class="s1">&#39;上海交通大学&#39;</span><span class="p">,</span> <span class="s1">&#39;西安交通大学&#39;</span><span class="p">,</span> <span class="s1">&#39;同济大学&#39;</span><span class="p">,</span> <span class="s1">&#39;南开大学&#39;</span><span class="p">,</span> <span class="s1">&#39;天津大学&#39;</span><span class="p">,</span> 
                    <span class="s1">&#39;武汉大学&#39;</span><span class="p">,</span> <span class="s1">&#39;华中科技大学&#39;</span><span class="p">,</span> <span class="s1">&#39;中国科学技术大学&#39;</span><span class="p">,</span> <span class="s1">&#39;南京大学&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;中山大学&#39;</span><span class="p">,</span> <span class="s1">&#39;中南大学&#39;</span><span class="p">,</span> <span class="s1">&#39;四川大学&#39;</span><span class="p">,</span> <span class="s1">&#39;重庆大学&#39;</span><span class="p">,</span> <span class="s1">&#39;兰州大学&#39;</span><span class="p">,</span> <span class="s1">&#39;湖南大学&#39;</span><span class="p">,</span> 
                    <span class="s1">&#39;山东大学&#39;</span><span class="p">,</span> <span class="s1">&#39;吉林大学&#39;</span><span class="p">,</span> <span class="s1">&#39;大连理工大大学&#39;</span><span class="p">,</span> <span class="s1">&#39;东北大学&#39;</span><span class="p">,</span> <span class="s1">&#39;北京航空航天大学&#39;</span><span class="p">,</span> <span class="s1">&#39;中国地质大学&#39;</span><span class="p">]</span>


<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;大学高管人数排行&#39;</span><span class="p">)</span>

<span class="n">uni_infos</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">university</span> <span class="ow">in</span> <span class="n">test_universitys</span><span class="p">:</span>
    <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">university</span><span class="p">)])</span>
    <span class="n">uni_infos</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">university</span><span class="p">,</span> <span class="n">num</span><span class="p">))</span>
    
<span class="n">uni_infos</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">uni_infos</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">uni_infos</span>
</code></pre></div><p>Run</p>
<pre><code>大学高管人数排行

[('清华大学', 10377),
 ('北京大学', 8709),
 ('中国人民大学', 7012),
 ('浙江大学', 5816),
 ('中山大学', 4065),
 ('上海交通大学', 3844),
 ('武汉大学', 3578),
 ('南京大学', 3272),
 ('西安交通大学', 2972),
 ('南开大学', 2716),
 ('湖南大学', 2502),
 ('华中科技大学', 2356),
 ('同济大学', 2089),
 ('吉林大学', 2044),
 ('四川大学', 1934),
 ('山东大学', 1847),
 ('中南大学', 1615),
 ('天津大学', 1598),
 ('重庆大学', 1440),
 ('北京航空航天大学', 1334),
 ('东北大学', 1241),
 ('中国科学技术大学', 842),
 ('兰州大学', 745),
 ('中国地质大学', 437),
 ('大连理工大大学', 0)]
</code></pre>
<br>
<h3 id="25-统计文本中指定词语出现次数">2.5 统计文本中指定词语出现次数</h3>
<p>例如统计每位高管内【大学】出现次数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;大学&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>0         0
1         2
2         0
3         0
4         0
         ..
900882    0
900883    0
900884    0
900885    0
900886    0
Name: 个人简历, Length: 736970, dtype: int64
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;高管总人数: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="c1">#简历中无「大学」字眼</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;无大学经历高管人数:&#39;</span> <span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;大学&#39;</span><span class="p">)</span><span class="o">==</span><span class="mi">0</span><span class="p">]))</span>
<span class="c1">#简历中有「大学」字眼</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;有大学经历高管人数:&#39;</span> <span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;大学&#39;</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>高管总人数:  736970
无大学经历高管人数: 515172
有大学经历高管人数: 221798
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#有些企业单位名字中带有「大学」，但这类企业非常少。</span>
<span class="c1">#「大学」词语出现次数可以近似看做学习经历次数</span>
<span class="c1">#如此， 1可以看做本科学历，2看做研究生学历， 3看做博士学历</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;大学&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="output_14_1.png" alt="png"  />

​</p>
<br>
<h3 id="26-找出每位高管的出生年份用正则表达式">2.6 找出每位高管的出生年份(用正则表达式)</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;\d</span><span class="si">{4}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>0                                                    [1969]
1                      [1965, 1984, 1986, 1990, 1994, 1995]
2         [1972, 1998, 1999, 2000, 2015, 2002, 2016, 200...
3         [1960, 1982, 1989, 1990, 1991, 1991, 2002, 200...
4         [1962, 2009, 1985, 1996, 1996, 2008, 1993, 200...
                                ...                        
900882                                                   []
900883                                                   []
900884                                                   []
900885                                                   []
900886                                                   []
Name: 个人简历, Length: 736970, dtype: object
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">birth_year</span><span class="p">(</span><span class="n">years</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1">#返回出生年份</span>
        <span class="k">return</span> <span class="n">years</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="c1">#没有年份的，返回0</span>
        <span class="k">return</span> <span class="mi">0</span>
    
    
<span class="c1">#高管出生年份</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;\d</span><span class="si">{4}</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">birth_year</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>0         1969
1         1965
2         1972
3         1960
4         1962
          ... 
900882       0
900883       0
900884       0
900885       0
900886       0
Name: 个人简历, Length: 736970, dtype: object
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#高管时间点个数(感觉可以看做经历的个数)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;个人简历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;\d</span><span class="si">{4}</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">ys</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">ys</span><span class="p">)))</span>
</code></pre></div><p>Run</p>
<pre><code>0          1
1          6
2         10
3         10
4          8
          ..
900882     0
900883     0
900884     0
900885     0
900886     0
Name: 个人简历, Length: 736970, dtype: int64
</code></pre>
<p><br><br></p>
<h2 id="获取代码">获取代码</h2>
<p><a href="top-ceo.ipynb"><strong>点击下载ipynb代码文件</strong></a></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>B站 | &#34;高铁互殴&#34;视频词云图绘制</title>
      <link>https://textdata.cn/blog/2023-05-11-bilibili-dongbei-big-brother/</link>
      <pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-05-11-bilibili-dongbei-big-brother/</guid>
      <description>&lt;h2 id=&#34;一高铁互殴&#34;&gt;一、“高铁互殴”&lt;/h2&gt;
&lt;p&gt;最近「高铁互殴」是个热门话题， 身处局外， 信息是不全面的，没法做到客观公正。但看到大哥这个视频， 我自己是怕麻烦，不敢挺身而出的，所以很佩服东北大哥挺身而出这一行为，至于互殴双方对错与否不做评论。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/screen.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;5月2日四川成都，一名女孩独自乘坐高铁从老家到成都东，她座位后的三个座位一共坐了两个大人三个小孩，孩子在玩游戏时多次撞到女孩的椅背。女孩称自己忍了很多次，也回头进行劝说。孩子的妈妈对着女孩嘶吼说：他们是小孩子，你至于吗？孩子爸爸开始辱骂女孩，女孩回嘴时被孩子妈妈扇了一个耳光，女孩气不过回了手。一名东北大哥看不下去了，起身对双方进行劝说。事后女孩称对于东北大哥的热心出手，也是非常感谢。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二数据采集&#34;&gt;二、数据采集&lt;/h2&gt;
&lt;p&gt;今天就用这个代码，采集视频 &lt;a href=&#34;https://www.bilibili.com/video/BV1ys4y1g7qt/&#34;&gt;https://www.bilibili.com/video/BV1ys4y1g7qt/&lt;/a&gt; 的评论和弹幕。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;get_video_comments&lt;/strong&gt;(bv, max_page=10, speed=1, encoding=&amp;lsquo;utf-8&amp;rsquo;) 获取评论&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;get_video_danmaku&lt;/strong&gt;(bv, encoding=&amp;lsquo;utf-8&amp;rsquo;) 获取弹幕&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这两函数在 &lt;a href=&#34;https://textdata.cn/blog/2023-04-23-data-collector-for-bilibili-danmu/&#34;&gt;网络爬虫(付费) | 使用Python采集B站弹幕和评论数据&lt;/a&gt; 内可以下载到的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#每页有20个评论。据此计算页码数&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;get_video_comments&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;BV1ys4y1g7qt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                   &lt;span class=&#34;n&#34;&gt;max_page&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;250&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                   &lt;span class=&#34;n&#34;&gt;speed&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                   &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 

&lt;span class=&#34;n&#34;&gt;get_video_danmaku&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;BV1ys4y1g7qt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;三数据分析&#34;&gt;三、数据分析&lt;/h2&gt;
&lt;p&gt;爬虫函数运行结束后，
&lt;img loading=&#34;lazy&#34; src=&#34;img/data.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;以评论数据为例，&lt;/p&gt;
&lt;h3 id=&#34;31-导入数据&#34;&gt;3.1 导入数据&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;comments_BV1ys4y1g7qt.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#记录数&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;2585
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-词频统计&#34;&gt;3.2 词频统计&lt;/h3&gt;
&lt;p&gt;词频统计&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;jieba&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#pyecharts版本2.0.1&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#汇总文本&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;comment&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tolist&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#清洗数据&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;[&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\u4e00&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\u9fa5&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;]+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#分词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;jieba&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lcut&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#词频统计&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wordfreqs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;freq&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;wordfreqs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;freq&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    
&lt;span class=&#34;c1&#34;&gt;#词频排序&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wordfreqs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;sorted&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wordfreqs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reverse&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#显示前20个&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wordfreqs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;    [(&amp;#39;大哥&amp;#39;, &amp;#39;766&amp;#39;),
     (&amp;#39;东北&amp;#39;, &amp;#39;519&amp;#39;),
     (&amp;#39;孩子&amp;#39;, &amp;#39;444&amp;#39;),
     (&amp;#39;这种&amp;#39;, &amp;#39;254&amp;#39;),
     (&amp;#39;一个&amp;#39;, &amp;#39;200&amp;#39;),
     (&amp;#39;小孩&amp;#39;, &amp;#39;199&amp;#39;),
     (&amp;#39;就是&amp;#39;, &amp;#39;195&amp;#39;),
     (&amp;#39;自己&amp;#39;, &amp;#39;175&amp;#39;),
     (&amp;#39;父母&amp;#39;, &amp;#39;155&amp;#39;),
     (&amp;#39;家长&amp;#39;, &amp;#39;154&amp;#39;),
     (&amp;#39;不是&amp;#39;, &amp;#39;148&amp;#39;),
     (&amp;#39;真的&amp;#39;, &amp;#39;146&amp;#39;),
     (&amp;#39;支持&amp;#39;, &amp;#39;136&amp;#39;),
     (&amp;#39;什么&amp;#39;, &amp;#39;133&amp;#39;),
     (&amp;#39;这样&amp;#39;, &amp;#39;127&amp;#39;),
     (&amp;#39;没有&amp;#39;, &amp;#39;126&amp;#39;),
     (&amp;#39;这个&amp;#39;, &amp;#39;125&amp;#39;),
     (&amp;#39;他们&amp;#39;, &amp;#39;120&amp;#39;),
     (&amp;#39;直接&amp;#39;, &amp;#39;112&amp;#39;),
     (&amp;#39;遇到&amp;#39;, &amp;#39;102&amp;#39;)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;33-保存到xlsx中&#34;&gt;3.3 保存到xlsx中&lt;/h3&gt;
&lt;p&gt;词频统计保存到xlsx中
&lt;img loading=&#34;lazy&#34; src=&#34;img/%e4%bf%9d%e5%ad%98%e5%88%b0xlsx%e4%b8%ad.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wordfreqs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;word&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;freq&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#df2.to_csv(&amp;#39;高铁互殴-词频统计.csv&amp;#39;, index=False)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_excel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;高铁互殴-词频统计.xlsx&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#显示前5个词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-词云图&#34;&gt;3.2 词云图&lt;/h3&gt;
&lt;p&gt;使用pyecharts(版本号2.0.1)绘制词云图，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pyecharts.options&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;opts&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pyecharts.charts&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;WordCloud&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#绘制词云图&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wordfreqs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wordfreqs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;WordCloud&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;series_name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data_pair&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wordfreqs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word_size_range&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_global_opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;title_opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TitleOpts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;高铁互殴&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title_textstyle_opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TextStyleOpts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;font_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;23&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
                             &lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;tooltip_opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TooltipOpts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;is_show&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;render&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;高铁互殴.html&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#存储位置&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/%e9%ab%98%e9%93%81%e4%ba%92%e6%ae%b4.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一高铁互殴">一、“高铁互殴”</h2>
<p>最近「高铁互殴」是个热门话题， 身处局外， 信息是不全面的，没法做到客观公正。但看到大哥这个视频， 我自己是怕麻烦，不敢挺身而出的，所以很佩服东北大哥挺身而出这一行为，至于互殴双方对错与否不做评论。</p>
<p><img loading="lazy" src="img/screen.png" alt=""  />
</p>
<blockquote>
<p>5月2日四川成都，一名女孩独自乘坐高铁从老家到成都东，她座位后的三个座位一共坐了两个大人三个小孩，孩子在玩游戏时多次撞到女孩的椅背。女孩称自己忍了很多次，也回头进行劝说。孩子的妈妈对着女孩嘶吼说：他们是小孩子，你至于吗？孩子爸爸开始辱骂女孩，女孩回嘴时被孩子妈妈扇了一个耳光，女孩气不过回了手。一名东北大哥看不下去了，起身对双方进行劝说。事后女孩称对于东北大哥的热心出手，也是非常感谢。</p>
</blockquote>
<p><br><br></p>
<h2 id="二数据采集">二、数据采集</h2>
<p>今天就用这个代码，采集视频 <a href="https://www.bilibili.com/video/BV1ys4y1g7qt/">https://www.bilibili.com/video/BV1ys4y1g7qt/</a> 的评论和弹幕。</p>
<ul>
<li><strong>get_video_comments</strong>(bv, max_page=10, speed=1, encoding=&lsquo;utf-8&rsquo;) 获取评论</li>
<li><strong>get_video_danmaku</strong>(bv, encoding=&lsquo;utf-8&rsquo;) 获取弹幕</li>
</ul>
<p>这两函数在 <a href="https://textdata.cn/blog/2023-04-23-data-collector-for-bilibili-danmu/">网络爬虫(付费) | 使用Python采集B站弹幕和评论数据</a> 内可以下载到的。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#每页有20个评论。据此计算页码数</span>
<span class="n">get_video_comments</span><span class="p">(</span><span class="n">bv</span><span class="o">=</span><span class="s1">&#39;BV1ys4y1g7qt&#39;</span><span class="p">,</span> 
                   <span class="n">max_page</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> 
                   <span class="n">speed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                   <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> 

<span class="n">get_video_danmaku</span><span class="p">(</span><span class="n">bv</span><span class="o">=</span><span class="s1">&#39;BV1ys4y1g7qt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> 
</code></pre></div><br>
<br>
<h2 id="三数据分析">三、数据分析</h2>
<p>爬虫函数运行结束后，
<img loading="lazy" src="img/data.png" alt=""  />
</p>
<p>以评论数据为例，</p>
<h3 id="31-导入数据">3.1 导入数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;comments_BV1ys4y1g7qt.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#记录数</span>
<span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><pre><code>2585
</code></pre>
<br>
<h3 id="32-词频统计">3.2 词频统计</h3>
<p>词频统计</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="c1">#pyecharts版本2.0.1</span>

<span class="c1">#汇总文本</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;comment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="c1">#清洗数据</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]+&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span>

<span class="c1">#分词</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">]</span>

<span class="c1">#词频统计</span>
<span class="n">wordfreqs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
    <span class="n">freq</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="n">wordfreqs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">word</span><span class="p">,</span> <span class="n">freq</span><span class="p">))</span>
    
<span class="c1">#词频排序</span>
<span class="n">wordfreqs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">wordfreqs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">#显示前20个</span>
<span class="n">wordfreqs</span><span class="p">[:</span><span class="mi">20</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [(&#39;大哥&#39;, &#39;766&#39;),
     (&#39;东北&#39;, &#39;519&#39;),
     (&#39;孩子&#39;, &#39;444&#39;),
     (&#39;这种&#39;, &#39;254&#39;),
     (&#39;一个&#39;, &#39;200&#39;),
     (&#39;小孩&#39;, &#39;199&#39;),
     (&#39;就是&#39;, &#39;195&#39;),
     (&#39;自己&#39;, &#39;175&#39;),
     (&#39;父母&#39;, &#39;155&#39;),
     (&#39;家长&#39;, &#39;154&#39;),
     (&#39;不是&#39;, &#39;148&#39;),
     (&#39;真的&#39;, &#39;146&#39;),
     (&#39;支持&#39;, &#39;136&#39;),
     (&#39;什么&#39;, &#39;133&#39;),
     (&#39;这样&#39;, &#39;127&#39;),
     (&#39;没有&#39;, &#39;126&#39;),
     (&#39;这个&#39;, &#39;125&#39;),
     (&#39;他们&#39;, &#39;120&#39;),
     (&#39;直接&#39;, &#39;112&#39;),
     (&#39;遇到&#39;, &#39;102&#39;)]
</code></pre></div><br>
<h3 id="33-保存到xlsx中">3.3 保存到xlsx中</h3>
<p>词频统计保存到xlsx中
<img loading="lazy" src="img/%e4%bf%9d%e5%ad%98%e5%88%b0xlsx%e4%b8%ad.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">wordfreqs</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;word&#39;</span><span class="p">,</span> <span class="s1">&#39;freq&#39;</span><span class="p">])</span>

<span class="c1">#df2.to_csv(&#39;高铁互殴-词频统计.csv&#39;, index=False)</span>
<span class="n">df2</span><span class="o">.</span><span class="n">to_excel</span><span class="p">(</span><span class="s1">&#39;高铁互殴-词频统计.xlsx&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1">#显示前5个词</span>
<span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<h3 id="32-词云图">3.2 词云图</h3>
<p>使用pyecharts(版本号2.0.1)绘制词云图，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>

<span class="c1">#绘制词云图</span>
<span class="n">wordfreqs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">f</span><span class="p">))</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">f</span> <span class="ow">in</span> <span class="n">wordfreqs</span><span class="p">]</span>
<span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
<span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">wordfreqs</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
<span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
    <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;高铁互殴&#34;</span><span class="p">,</span> <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
                             <span class="p">),</span>
    <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">wc</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s2">&#34;高铁互殴.html&#34;</span><span class="p">)</span>  <span class="c1">#存储位置</span>
</code></pre></div><p><img loading="lazy" src="img/%e9%ab%98%e9%93%81%e4%ba%92%e6%ae%b4.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>tomotopy | 速度最快的LDA主题模型</title>
      <link>https://textdata.cn/blog/2023-04-25-tomotopy_is_the_fastest_topic_model/</link>
      <pubDate>Tue, 25 Apr 2023 11:43:10 +0600</pubDate>
      
      <guid>/blog/2023-04-25-tomotopy_is_the_fastest_topic_model/</guid>
      <description>接近C的速度，比市面的sklearn、gensim快十几倍</description>
      <content:encoded><![CDATA[<h2 id="代码下载tomotopy_codezip"><a href="tomotopy_code.zip">代码下载</a></h2>
<br>
<h2 id="tomotopy简介">tomotopy简介？</h2>
<p>tomotopy 是 tomoto（主题建模工具）的 Python 扩展，它是用 C++ 编写的基于 Gibbs 采样的主题模型库。支持的主题模型包括 LDA、DMR、HDP、MG-LDA、PA 和 HPA， 利用现代 CPU 的矢量化来最大化速度。</p>
<p><a href="https://github.com/bab2min/tomotopy">https://github.com/bab2min/tomotopy</a></p>
<p><strong>下图中同样的数据集， tomotopy迭代200次，gensim迭代10次的情况下， tomotopy与gensim耗时对比图，由此可见tomotopy训练主题模型速度之快。</strong>
<img loading="lazy" src="img/TomotopyVsGensim.png" alt=""  />
</p>
<p>当前版本的 tomotopy 支持的主题模型包括</p>
<ul>
<li>潜在狄利克雷分配（LDAModel）</li>
<li>标记的 LDA（LLDA 模型）</li>
<li>部分标记的 LDA（PLDA 模型）</li>
<li>监督LDA（SLDA模型）</li>
<li>Dirichlet 多项回归 (DMRModel)</li>
<li>广义狄利克雷多项回归 (GDMRModel)</li>
<li>分层狄利克雷过程 (HDPModel)</li>
<li>分层LDA（HLDA模型）</li>
<li>多粒 LDA（MGLDA 模型）</li>
<li>弹珠盘分配（PAModel）</li>
<li>分层 PA (HPAModel)</li>
<li>相关主题模型（CTModel）</li>
<li>动态主题模型 (DTModel)</li>
<li>基于伪文档的主题模型（PTModel）。</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">tomotopy</span><span class="o">==</span><span class="mf">0.12.2</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">pyLDAvis</span><span class="o">==</span><span class="mf">3.3.1</span>  
</code></pre></div><p>目前，tomotopy 可以利用 AVX2、AVX 或 SSE2 SIMD 指令集来最大程度利用PC的性能。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>

<span class="n">tp</span><span class="o">.</span><span class="n">isa</span>
</code></pre></div><p>Run</p>
<pre><code>'avx2'
</code></pre>
<p>如果 tp.isa 返回 None，则训练过程可能需要很长时间。</p>
<br>
<h2 id="1-导入数据">1. 导入数据</h2>
<p>准备一个自己很熟悉的数据disaster_news.csv，一共有332条，话题数K=5，（正常情况下K是需要探索的）。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;disaster_news.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<h2 id="2-整理数据">2. 整理数据</h2>
<p>分词、去除停用词,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;stopwords.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">segment</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">words</span>

<span class="n">test</span> <span class="o">=</span> <span class="s2">&#34;云南永善县级地震已致人伤间民房受损中新网月日电据云南昭通市防震减灾局官方网站消息截至日时云南昭通永善县级地震已造成人受伤其中重伤人轻伤人已全部送医院救治民房受损户间倒塌户间个乡镇所学校不同程度受损目前被损毁电力交通通讯设施已全部抢通修复当地已调拨帐篷顶紧急转移万人月日时分云南昭通永善县发生里氏级地震震源深度公里当地震感强烈此外成都等四川多地也有明显震感&#34;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">segment</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
</code></pre></div><pre><code>['云南', '永善县', '级', '地震', '已致', '伤间', '民房', '受损', '中新网', '日电', '云南', '昭通市', '防震', '减灾', '局', '官方网站', '消息', '日时', '云南', '昭通', '永善县', '级', '地震', '造成', '受伤', '重伤', '轻伤', '送', '医院', '救治', '民房', '受损', '户间', '倒塌', '户间', '乡镇', '学校', '不同', '程度', '受损', '目前', '损毁', '电力', '交通', '通讯', '设施', '抢通', '修复', '调拨', '帐篷', '顶', '紧急', '转移', '万人', '时分', '云南', '昭通', '永善县', '发生', '里氏', '级', '地震', '震源', '深度', '公里', '震感', '强烈', '成都', '四川', '多地', '明显', '震感']
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;words&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">segment</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<h2 id="3-找到最佳k">3. 找到最佳K</h2>
<p>绘制Coherence曲线是一种常见的方法，用于选择主题数（k）的最佳值。这可以帮助您确定在哪个主题数下主题模型的性能最佳。以下是一般步骤以及如何解释Coherence曲线来找出最佳的k：</p>
<ol>
<li>创建主题模型：首先，您需要使用不同的k值来创建一系列主题模型，每个模型都有不同数量的主题（k）。这可以通过循环遍历k值并训练主题模型来完成。</li>
<li>计算Coherence：对于每个k值，使用tomotopy.coherence.Coherence类计算主题模型的一致性度量（比如C_V、UMass等）。这将为每个k值生成一个一致性得分。</li>
<li>绘制Coherence曲线：将k值（主题数）作为x轴，一致性得分作为y轴，绘制Coherence曲线。得到一个k和一致性得分之间的关系图。</li>
<li>寻找拐点：观察Coherence曲线，通常会看到一条曲线在某个k值附近达到峰值，然后开始下降。这个峰值对应的k值通常被认为是最佳的主题数。这是因为在这个k值下，主题模型的主题在文本中的一致性较高。</li>
<li>选择最佳k：根据Coherence曲线上的峰值，选择最佳的k值作为主题模型的最终主题数。</li>
<li>模型评估：一旦选择了最佳k值，您可以使用该值来训练最终的主题模型，并在任务中进行评估。</li>
</ol>
<p>Coherence曲线上的峰值通常对应于最佳的主题数，因为在这个点上主题之间的关联度较高。然而，需要谨慎选择，因为有时候峰值可能不明显，或者可能有多个相似的峰值。您可以使用这个方法来帮助确定最佳的主题数，但最终的决策可能还需要结合领域知识和任务需求来做出。</p>
<p>tomotopy每次运行得到的图形状不一样，为了保证运行结果具有可比性，设置了随机种子seed为1，你也可以根据需要改为自己需要的随机状态(这里有点像炼丹)。经过运行发现k=5比较合适（跑出了我的预判）。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">find_k</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">min_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_k</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="c1">#min_df 词语最少出现在2个文档中</span>
    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_k</span><span class="p">,</span> <span class="n">max_k</span><span class="p">):</span>
        <span class="c1">#seed随机种子，保证在大邓这里运行结果与你运行的结果一样</span>
        <span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="n">min_df</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">words</span><span class="p">:</span>
                <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
        <span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
        <span class="n">coh</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">coherence</span><span class="o">.</span><span class="n">Coherence</span><span class="p">(</span><span class="n">mdl</span><span class="p">)</span>
        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">coh</span><span class="o">.</span><span class="n">get_score</span><span class="p">())</span>

    <span class="c1">#x = list(range(min_k, max_k - 1))  # 区间最右侧的值。注意：不能大于max_k</span>
    <span class="c1">#print(x)</span>
    <span class="c1">#print()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">min_k</span><span class="p">,</span> <span class="n">max_k</span><span class="p">),</span> <span class="n">scores</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;number of topics&#34;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;coherence&#34;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    
<span class="n">find_k</span><span class="p">(</span><span class="n">docs</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;words&#39;</span><span class="p">],</span> <span class="n">min_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_11_0.png" alt="png"  />
</p>
<br>
<h2 id="4-训练lda">4. 训练lda</h2>
<p>使用tomotopy的LDA模型， 话题数K=5</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">tomotopy</span> <span class="k">as</span> <span class="nn">tp</span>

<span class="c1">#初始化LDA</span>
<span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">555</span><span class="p">)</span>
<span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;words&#39;</span><span class="p">]:</span>
    <span class="c1">#确认words 是 非空词语列表</span>
    <span class="k">if</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">words</span><span class="p">)</span>

<span class="c1">#训练</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1">#查看每个topic feature words</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Top 10 words of topic #</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">get_topic_words</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    Top 10 words of topic #0
    [(&#39;一辆&#39;, 0.02751251682639122), (&#39;事故&#39;, 0.021704642102122307), (&#39;记者&#39;, 0.018342189490795135), (&#39;死亡&#39;, 0.01650812290608883), (&#39;造成&#39;, 0.014062701724469662), (&#39;人员&#39;, 0.013909862376749516), (&#39;现场&#39;, 0.013451346196234226), (&#39;受伤&#39;, 0.012687151320278645), (&#39;相撞&#39;, 0.011922957375645638), (&#39;货车&#39;, 0.011922957375645638)]


​    
​    Top 10 words of topic #1
​    [(&#39;学生&#39;, 0.02709135226905346), (&#39;食物中毒&#39;, 0.02498047426342964), (&#39;出现&#39;, 0.019175563007593155), (&#39;医院&#39;, 0.016185153275728226), (&#39;事件&#39;, 0.013546556234359741), (&#39;调查&#39;, 0.013194743543863297), (&#39;年月日&#39;, 0.012842929922044277), (&#39;治疗&#39;, 0.012667023576796055), (&#39;症状&#39;, 0.011787491850554943), (&#39;名&#39;, 0.011259771883487701)]


​    
​    Top 10 words of topic #2
​    [(&#39;现场&#39;, 0.018848909065127373), (&#39;发生&#39;, 0.01677251048386097), (&#39;医院&#39;, 0.015015557408332825), (&#39;起火&#39;, 0.014216942712664604), (&#39;原因&#39;, 0.012140544131398201), (&#39;目前&#39;, 0.012140544131398201), (&#39;救治&#39;, 0.01150165218859911), (&#39;进行&#39;, 0.011022482998669147), (&#39;名&#39;, 0.009425252676010132), (&#39;火势&#39;, 0.009265529923141003)]


​    
​    Top 10 words of topic #3
​    [(&#39;发生&#39;, 0.03348556533455849), (&#39;爆炸&#39;, 0.022389251738786697), (&#39;造成&#39;, 0.019663840532302856), (&#39;死亡&#39;, 0.01713310182094574), (&#39;受伤&#39;, 0.016938429325819016), (&#39;年月日&#39;, 0.016354413703083992), (&#39;轿车&#39;, 0.012655640952289104), (&#39;警方&#39;, 0.012460969388484955), (&#39;袭击&#39;, 0.012266295962035656), (&#39;事件&#39;, 0.011487606912851334)]


​    
​    Top 10 words of topic #4
​    [(&#39;地震&#39;, 0.047826822847127914), (&#39;发生&#39;, 0.03555167838931084), (&#39;火灾&#39;, 0.03140682727098465), (&#39;时分&#39;, 0.020885275676846504), (&#39;级&#39;, 0.015783920884132385), (&#39;时间&#39;, 0.013870910741388798), (&#39;公里&#39;, 0.013711493462324142), (&#39;人员伤亡&#39;, 0.013073823414742947), (&#39;记者&#39;, 0.013073823414742947), (&#39;震感&#39;, 0.012276736088097095)]
</code></pre></div><br>
<p>查看话题模型信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mdl</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mysql" data-lang="mysql"><span class="w">
</span><span class="w">    </span><span class="o">&lt;</span><span class="n">Basic</span><span class="w"> </span><span class="n">Info</span><span class="o">&gt;</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="nf">LDAModel</span><span class="w"> </span><span class="p">(</span><span class="n">current</span><span class="w"> </span><span class="n">version</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">12</span><span class="p">.</span><span class="mi">2</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="mi">332</span><span class="w"> </span><span class="n">docs</span><span class="p">,</span><span class="w"> </span><span class="mi">29749</span><span class="w"> </span><span class="n">words</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">Total</span><span class="w"> </span><span class="n">Vocabs</span><span class="p">:</span><span class="w"> </span><span class="mi">8428</span><span class="p">,</span><span class="w"> </span><span class="n">Used</span><span class="w"> </span><span class="n">Vocabs</span><span class="p">:</span><span class="w"> </span><span class="mi">2984</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">Entropy</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">words</span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="p">.</span><span class="mi">10665</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">Entropy</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">term</span><span class="o">-</span><span class="n">weighted</span><span class="w"> </span><span class="n">words</span><span class="p">:</span><span class="w"> </span><span class="mi">7</span><span class="p">.</span><span class="mi">10665</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">Removed</span><span class="w"> </span><span class="n">Vocabs</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">NA</span><span class="o">&gt;</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w">
</span><span class="w">    </span><span class="o">&lt;</span><span class="n">Training</span><span class="w"> </span><span class="n">Info</span><span class="o">&gt;</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">Iterations</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="n">Burn</span><span class="o">-</span><span class="k">in</span><span class="w"> </span><span class="n">steps</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">Optimization</span><span class="w"> </span><span class="k">Interval</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span><span class="w"> </span><span class="n">per</span><span class="w"> </span><span class="n">word</span><span class="p">:</span><span class="w"> </span><span class="o">-</span><span class="mi">7</span><span class="p">.</span><span class="mi">79934</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w">
</span><span class="w">    </span><span class="o">&lt;</span><span class="n">Initial</span><span class="w"> </span><span class="n">Parameters</span><span class="o">&gt;</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">tw</span><span class="p">:</span><span class="w"> </span><span class="n">TermWeight</span><span class="p">.</span><span class="n">ONE</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">min_cf</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">(</span><span class="n">minimum</span><span class="w"> </span><span class="n">collection</span><span class="w"> </span><span class="n">frequency</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">words</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">min_df</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="p">(</span><span class="n">minimum</span><span class="w"> </span><span class="n">document</span><span class="w"> </span><span class="n">frequency</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">words</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">rm_top</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">(</span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">top</span><span class="w"> </span><span class="n">words</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">removed</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">k</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="p">(</span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">topics</span><span class="w"> </span><span class="k">between</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="mi">32767</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">alpha</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="p">(</span><span class="n">hyperparameter</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Dirichlet</span><span class="w"> </span><span class="n">distribution</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">document</span><span class="o">-</span><span class="n">topic</span><span class="p">,</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">single</span><span class="w"> </span><span class="o">`</span><span class="kt">float</span><span class="o">`</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">case</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">symmetric</span><span class="w"> </span><span class="n">prior</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">`</span><span class="n">k</span><span class="o">`</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">`</span><span class="kt">float</span><span class="o">`</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">case</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">asymmetric</span><span class="w"> </span><span class="n">prior</span><span class="p">.)</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">eta</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">01</span><span class="w"> </span><span class="p">(</span><span class="n">hyperparameter</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Dirichlet</span><span class="w"> </span><span class="n">distribution</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">topic</span><span class="o">-</span><span class="n">word</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">seed</span><span class="p">:</span><span class="w"> </span><span class="mi">555</span><span class="w"> </span><span class="p">(</span><span class="n">random</span><span class="w"> </span><span class="n">seed</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="n">trained</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">12</span><span class="p">.</span><span class="mi">2</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w">
</span><span class="w">    </span><span class="o">&lt;</span><span class="n">Parameters</span><span class="o">&gt;</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="nf">alpha</span><span class="w"> </span><span class="p">(</span><span class="n">Dirichlet</span><span class="w"> </span><span class="n">prior</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">per</span><span class="o">-</span><span class="n">document</span><span class="w"> </span><span class="n">topic</span><span class="w"> </span><span class="n">distributions</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w">  </span><span class="p">[</span><span class="mi">0</span><span class="p">.</span><span class="mi">7143365</span><span class="w">  </span><span class="mi">0</span><span class="p">.</span><span class="mi">6852513</span><span class="w">  </span><span class="mi">0</span><span class="p">.</span><span class="mi">75089616</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">6204677</span><span class="w">  </span><span class="mi">0</span><span class="p">.</span><span class="mi">7040125</span><span class="w"> </span><span class="p">]</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="nf">eta</span><span class="w"> </span><span class="p">(</span><span class="n">Dirichlet</span><span class="w"> </span><span class="n">prior</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">per</span><span class="o">-</span><span class="n">topic</span><span class="w"> </span><span class="n">word</span><span class="w"> </span><span class="n">distribution</span><span class="p">)</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w">  </span><span class="mi">0</span><span class="p">.</span><span class="mi">01</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w">
</span><span class="w">    </span><span class="o">&lt;</span><span class="n">Topics</span><span class="o">&gt;</span><span class="w">
</span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="c1">#0 (6513) : 一辆 事故 记者 死亡 造成
</span><span class="c1"></span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="c1">#1 (5655) : 学生 食物中毒 出现 医院 事件
</span><span class="c1"></span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="c1">#2 (6231) : 现场 发生 医院 起火 原因
</span><span class="c1"></span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="c1">#3 (5107) : 发生 爆炸 造成 死亡 受伤
</span><span class="c1"></span><span class="w">    </span><span class="o">|</span><span class="w"> </span><span class="c1">#4 (6243) : 地震 发生 火灾 时分 级
</span></code></pre></div><h3 id="topic解读">topic解读</h3>
<p>根据每个话题top10的特征词，5个话题解读为</p>
<ul>
<li>交通事故| #0 (6513) : 一辆 事故 记者 死亡 造成</li>
<li>食品安全| #1 (5655) : 学生 食物中毒 出现 医院 事件</li>
<li>火灾新闻| #2 (6231) : 现场 发生 医院 起火 原因</li>
<li>恐怖袭击| #3 (5107) : 发生 爆炸 造成 死亡 受伤</li>
<li>地震灾害| #4 (6243) : 地震 发生 火灾 时分 级</li>
</ul>
<br>
<h2 id="5-可视化">5. 可视化</h2>
<p>使用pyLDAvis</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pyLDAvis</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">Warning</span><span class="p">)</span>

<span class="c1">#在notebook显示</span>
<span class="n">pyLDAvis</span><span class="o">.</span><span class="n">enable_notebook</span><span class="p">()</span>

<span class="c1">#获取pyldavis需要的参数</span>
<span class="n">topic_term_dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">mdl</span><span class="o">.</span><span class="n">get_topic_word_dist</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">k</span><span class="p">)])</span>
<span class="n">doc_topic_dists</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">get_topic_dist</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">])</span>
<span class="n">doc_topic_dists</span> <span class="o">/=</span> <span class="n">doc_topic_dists</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">doc_lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="o">.</span><span class="n">words</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">mdl</span><span class="o">.</span><span class="n">docs</span><span class="p">])</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">mdl</span><span class="o">.</span><span class="n">used_vocabs</span><span class="p">)</span>
<span class="n">term_frequency</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">used_vocab_freq</span>


<span class="n">prepared_data</span> <span class="o">=</span> <span class="n">pyLDAvis</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
    <span class="n">topic_term_dists</span><span class="p">,</span> 
    <span class="n">doc_topic_dists</span><span class="p">,</span> 
    <span class="n">doc_lengths</span><span class="p">,</span> 
    <span class="n">vocab</span><span class="p">,</span> 
    <span class="n">term_frequency</span><span class="p">,</span>
    <span class="n">start_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># tomotopy话题id从0开始，pyLDAvis话题id从1开始</span>
    <span class="n">sort_topics</span><span class="o">=</span><span class="kc">False</span> <span class="c1">#注意：否则pyLDAvis与tomotopy内的话题无法一一对应。 </span>
<span class="p">)</span>


<span class="c1">#可视化结果存到html文件中</span>
<span class="c1">#pyLDAvis.save_html(prepared_data, &#39;ldavis.html&#39;)</span>

<span class="c1">#notebook中显示</span>
<span class="n">pyLDAvis</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">prepared_data</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/ldavis.png" alt=""  />
</p>
<br>
<h2 id="6-预测">6. 预测</h2>
<p>预测某文档的话题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">jieba</span>
<span class="n">stopwords</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;stopwords.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1">#预测</span>
<span class="n">doc</span> <span class="o">=</span> <span class="s1">&#39;云南永善县级地震已致伤间民房受损中新网日电云南昭通市防震减灾局官方网站消息日时云南昭通永善县级地震造成受伤重伤轻伤送医院救治民房受损户间倒塌户间乡镇学校不同程度受损目前损毁电力交通通讯设施抢通修复调拨帐篷顶紧急转移万人时分云南昭通永善县发生里氏级地震震源深度公里震感强烈成都四川多地明显震感&#39;</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>

<span class="c1">#构造tomotopy需要的数据</span>
<span class="n">doc_inst</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">make_doc</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">words</span><span class="p">)</span>
<span class="n">topic_dist</span><span class="p">,</span> <span class="n">ll</span> <span class="o">=</span> <span class="n">mdl</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">doc_inst</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Topic Distribution for Unseen Docs: &#34;</span><span class="p">,</span> <span class="n">topic_dist</span><span class="p">)</span>

</code></pre></div><pre><code>Topic Distribution for Unseen Docs:  [0.11645161 0.10240361 0.5342029  0.03622254 0.21071935]
</code></pre>
<p>列表长度为5， 列表第三个数值(topic #2)数值最大，该文本最大的可能性是topic #2</p>
<br>
<h2 id="补充-指定主题特征词">补充: 指定主题特征词</h2>
<p>如果对数据比较了解，已经知道有一些主题，可以把比较明显的词语分配给指定的topic_id。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mdl</span> <span class="o">=</span> <span class="n">tp</span><span class="o">.</span><span class="n">LDAModel</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">555</span><span class="p">)</span>

<span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;words&#39;</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">mdl</span><span class="o">.</span><span class="n">add_doc</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="c1">#把word相撞 分配给topic_0, 权重设置为1， 其他topic权重设置为0.1</span>
<span class="c1">#注意这里的range(5) 5是对应的k值</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">set_word_prior</span><span class="p">(</span><span class="s1">&#39;相撞&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
<span class="c1">#把word地震 分配给topic_1, 权重设置为1， 其他topic权重设置为0.1</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">set_word_prior</span><span class="p">(</span><span class="s1">&#39;地震&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
<span class="c1">#把word火灾 分配给topic_2, 权重设置为1， 其他topic权重设置为0.1</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">set_word_prior</span><span class="p">(</span><span class="s1">&#39;火灾&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
<span class="c1">#把word中毒 分配给topic_3, 权重设置为1， 其他topic权重设置为0.1</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">set_word_prior</span><span class="p">(</span><span class="s1">&#39;中毒&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">3</span> <span class="k">else</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
<span class="c1">#把word袭击 分配给topic_4, 权重设置为1， 其他topic权重设置为0.1</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">set_word_prior</span><span class="p">(</span><span class="s1">&#39;袭击&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">4</span> <span class="k">else</span> <span class="mf">0.1</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>

<span class="n">mdl</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">mdl</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>  
</code></pre></div><pre><code>&lt;Basic Info&gt;
| LDAModel (current version: 0.12.2)
| 332 docs, 29749 words
| Total Vocabs: 8428, Used Vocabs: 2984
| Entropy of words: 7.10665
| Entropy of term-weighted words: 7.10665
| Removed Vocabs: &lt;NA&gt;
|
&lt;Training Info&gt;
| Iterations: 10, Burn-in steps: 0
| Optimization Interval: 10
| Log-likelihood per word: -7.72251
|
&lt;Initial Parameters&gt;
| tw: TermWeight.ONE
| min_cf: 0 (minimum collection frequency of words)
| min_df: 2 (minimum document frequency of words)
| rm_top: 0 (the number of top words to be removed)
| k: 5 (the number of topics between 1 ~ 32767)
| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)
| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)
| seed: 555 (random seed)
| trained in version 0.12.2
|
&lt;Parameters&gt;
| alpha (Dirichlet prior on the per-document topic distributions)
|  [0.7106193  0.60264444 0.5734784  0.71375024 0.6234263 ]
| eta (Dirichlet prior on the per-topic word distribution)
|  0.01
|
&lt;Topics&gt;
| #0 (6599) : 一辆 事故 死亡 发生 造成
| #1 (6087) : 地震 发生 级 公里 年月日
| #2 (5892) : 火灾 发生 现场 大火 起火
| #3 (6402) : 医院 学生 食物中毒 出现 名
| #4 (4769) : 事件 发生 袭击 人员 工作
|
</code></pre>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本分析 | 词典法的两种代码实现</title>
      <link>https://textdata.cn/blog/2023-04-17-two-method-for-liwc/</link>
      <pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-04-17-two-method-for-liwc/</guid>
      <description>但这周末，我使用1.4G的mda数据集， 5w条记录。尝试计算某类词的出现次数， 该词典含几百个词。在我的96G内存的macbook中，运行了十几个小时都没结果。于是同一个问题，本文分享了两种实现方法。一般情况下，使用「方法一」即可。当第一种方法运行不出结果，可以尝试「方法二」。</description>
      <content:encoded><![CDATA[<h2 id="一问题描述">一、问题描述</h2>
<p>对csv、xlsx等类型做词典法分析，经常用到apply方法，但是之前分享到案例数据量比较小， 词典都是几个词， 一般都能运行出结果。</p>
<blockquote>
<p><a href="https://textdata.cn/blog/liwc_python_text_mining/">liwc其实就是一种词典法</a></p>
</blockquote>
<p>但这周末，我使用1.4G的mda数据集， 5w条记录。尝试计算某类词的出现次数， 该词典含几百个词。在我的96G内存的macbook中，运行了十几个小时都没结果。</p>
<p>于是同一个问题，本文分享了两种实现方法。<strong>一般情况下，使用「方法一」即可。当第一种方法运行不出结果，可以尝试「方法二」。</strong></p>
<p><br><br></p>
<h2 id="二方法一">二、方法一</h2>
<h3 id="21-读取数据">2.1 读取数据</h3>
<p>导入含5000条记录的mda数据 test_mda.csv ，这里声明格式，防止年份和股票代码被识别为数字。</p>
<blockquote>
<p>如果想要完整的mda数据，可以前往购买[]</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python">
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#导入1000条测试数据</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;test_mda.csv&#39;</span><span class="p">,</span> <span class="n">converters</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;year&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">})</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><pre><code>5000
</code></pre>
<br>
<h3 id="22-准备词典">2.2 准备词典</h3>
<p>为了节约时间，也构造了只有几个词语的词典。</p>
<blockquote>
<p>自己随机手写的词典，如果需要应用的自己的研究中, 请将这几个概念词典扩充的完备一些，词汇量尽可能多一些。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#创新</span>
<span class="n">inovation_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;科技&#39;</span><span class="p">,</span> <span class="s1">&#39;研发&#39;</span><span class="p">,</span> <span class="s1">&#39;高校&#39;</span><span class="p">,</span> <span class="s1">&#39;技术&#39;</span><span class="p">,</span> <span class="s1">&#39;科学&#39;</span><span class="p">,</span> <span class="s1">&#39;理论&#39;</span><span class="p">,</span> <span class="s1">&#39;专利&#39;</span><span class="p">,</span> <span class="s1">&#39;攻克&#39;</span><span class="p">,</span> <span class="s1">&#39;改良&#39;</span><span class="p">,</span> <span class="s1">&#39;工艺&#39;</span><span class="p">,</span> <span class="s1">&#39;前沿&#39;</span><span class="p">,</span> <span class="s1">&#39;尖端&#39;</span><span class="p">]</span>

<span class="c1">#环保</span>
<span class="n">green_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;绿色&#39;</span><span class="p">,</span> <span class="s1">&#39;节能&#39;</span><span class="p">,</span> <span class="s1">&#39;低碳&#39;</span><span class="p">,</span> <span class="s1">&#39;环保&#39;</span><span class="p">,</span> <span class="s1">&#39;环境友好&#39;</span><span class="p">,</span> <span class="s1">&#39;无污染&#39;</span><span class="p">]</span>

<span class="c1">#短视主义</span>
<span class="n">push_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;加快&#39;</span><span class="p">,</span> <span class="s1">&#39;尽快&#39;</span><span class="p">,</span> <span class="s1">&#39;抓紧&#39;</span><span class="p">,</span> <span class="s1">&#39;月底&#39;</span><span class="p">,</span> <span class="s1">&#39;年底&#39;</span><span class="p">,</span> <span class="s1">&#39;争取&#39;</span><span class="p">,</span> <span class="s1">&#39;马上&#39;</span><span class="p">,</span> <span class="s1">&#39;立刻&#39;</span><span class="p">,</span> <span class="s1">&#39;年内&#39;</span><span class="p">,</span> <span class="s1">&#39;数月&#39;</span><span class="p">,</span> <span class="s1">&#39;数年&#39;</span><span class="p">]</span>

<span class="c1">#模棱两可</span>

<span class="n">prob_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;可能&#39;</span><span class="p">,</span> <span class="s1">&#39;大概&#39;</span><span class="p">,</span> <span class="s1">&#39;左右&#39;</span><span class="p">,</span> <span class="s1">&#39;估计&#39;</span><span class="p">,</span> <span class="s1">&#39;大约&#39;</span><span class="p">]</span>
</code></pre></div><br>
<h3 id="23-设计函数">2.3 设计函数</h3>
<p>该函数能实现对 inovation_words 和  green_words两类词的词频统计;</p>
<p>返回的结果包括总词频、green词频、 inovation词频。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#创新</span>
<span class="n">inovation_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;科技&#39;</span><span class="p">,</span> <span class="s1">&#39;研发&#39;</span><span class="p">,</span> <span class="s1">&#39;高校&#39;</span><span class="p">,</span> <span class="s1">&#39;技术&#39;</span><span class="p">,</span> <span class="s1">&#39;科学&#39;</span><span class="p">,</span> <span class="s1">&#39;理论&#39;</span><span class="p">,</span> <span class="s1">&#39;专利&#39;</span><span class="p">,</span> <span class="s1">&#39;攻克&#39;</span><span class="p">,</span> <span class="s1">&#39;改良&#39;</span><span class="p">,</span> <span class="s1">&#39;工艺&#39;</span><span class="p">,</span> <span class="s1">&#39;前沿&#39;</span><span class="p">,</span> <span class="s1">&#39;尖端&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">]</span>
<span class="c1">#环保</span>
<span class="n">green_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;绿色&#39;</span><span class="p">,</span> <span class="s1">&#39;节能&#39;</span><span class="p">,</span> <span class="s1">&#39;低碳&#39;</span><span class="p">,</span> <span class="s1">&#39;环保&#39;</span><span class="p">,</span> <span class="s1">&#39;环境友好&#39;</span><span class="p">,</span> <span class="s1">&#39;无污染&#39;</span><span class="p">]</span>
<span class="c1">#短视主义</span>
<span class="n">push_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;加快&#39;</span><span class="p">,</span> <span class="s1">&#39;尽快&#39;</span><span class="p">,</span> <span class="s1">&#39;抓紧&#39;</span><span class="p">,</span> <span class="s1">&#39;月底&#39;</span><span class="p">,</span> <span class="s1">&#39;年底&#39;</span><span class="p">,</span> <span class="s1">&#39;争取&#39;</span><span class="p">,</span> <span class="s1">&#39;马上&#39;</span><span class="p">,</span> <span class="s1">&#39;立刻&#39;</span><span class="p">,</span> <span class="s1">&#39;年内&#39;</span><span class="p">,</span> <span class="s1">&#39;数月&#39;</span><span class="p">,</span> <span class="s1">&#39;数年&#39;</span><span class="p">]</span>
<span class="c1">#模棱两可</span>
<span class="n">prob_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;可能&#39;</span><span class="p">,</span> <span class="s1">&#39;大概&#39;</span><span class="p">,</span> <span class="s1">&#39;左右&#39;</span><span class="p">,</span> <span class="s1">&#39;估计&#39;</span><span class="p">,</span> <span class="s1">&#39;大约&#39;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">analysis_info</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">inovation_num</span><span class="p">,</span> <span class="n">green_num</span><span class="p">,</span> <span class="n">push_num</span><span class="p">,</span> <span class="n">prob_num</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">push_num</span> 
    
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">inovation_words</span><span class="p">:</span>
        <span class="n">inovation_num</span> <span class="o">=</span> <span class="n">inovation_num</span> <span class="o">+</span> <span class="n">words</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">green_words</span><span class="p">:</span>
        <span class="n">green_num</span> <span class="o">=</span> <span class="n">green_num</span> <span class="o">+</span> <span class="n">words</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">push_words</span><span class="p">:</span>
        <span class="n">push_num</span> <span class="o">=</span> <span class="n">push_num</span> <span class="o">+</span> <span class="n">words</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        
    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">prob_words</span><span class="p">:</span>
        <span class="n">prob_num</span> <span class="o">=</span> <span class="n">prob_num</span> <span class="o">+</span> <span class="n">words</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        
    <span class="n">res</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;words&#39;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span>
        <span class="s1">&#39;inovation&#39;</span><span class="p">:</span> <span class="n">inovation_num</span><span class="p">,</span>
        <span class="s1">&#39;green&#39;</span><span class="p">:</span> <span class="n">green_num</span><span class="p">,</span>
        <span class="s1">&#39;push&#39;</span><span class="p">:</span> <span class="n">push_num</span><span class="p">,</span>
        <span class="s1">&#39;prob&#39;</span><span class="p">:</span> <span class="n">prob_num</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>

<span class="n">analysis_info</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s1">&#39;2022年是公司规范运作，坚持科技创新，保持持续发展。&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>words        15
inovation     2
green         0
push          0
prob          0
dtype: int64
</code></pre>
<br>
<h3 id="24-批量计算">2.4 批量计算</h3>
<p>选中text列，对该列批量运行 analysis_info 。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">time</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="n">info_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">analysis_info</span><span class="p">)</span>
<span class="n">res_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">info_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;耗时 </span><span class="si">{}</span><span class="s1"> s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)))</span>

<span class="n">res_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">耗时 112 s
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三方法二">三、方法二</h2>
<p>将中文分词后， 使用bag-of-words构造词语词频矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;new_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">text</span><span class="p">:</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">text</span><span class="p">)))</span>

<span class="n">vectorize</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">dtm</span> <span class="o">=</span> <span class="n">vectorize</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">new_text</span><span class="p">)</span>
<span class="n">bagofword_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dtm</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> 
                            <span class="n">columns</span><span class="o">=</span><span class="n">vectorize</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span> 


<span class="c1">#创新</span>
<span class="n">inovation_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;科技&#39;</span><span class="p">,</span> <span class="s1">&#39;研发&#39;</span><span class="p">,</span> <span class="s1">&#39;高校&#39;</span><span class="p">,</span> <span class="s1">&#39;技术&#39;</span><span class="p">,</span> <span class="s1">&#39;科学&#39;</span><span class="p">,</span> <span class="s1">&#39;理论&#39;</span><span class="p">,</span> <span class="s1">&#39;专利&#39;</span><span class="p">,</span> <span class="s1">&#39;攻克&#39;</span><span class="p">,</span> <span class="s1">&#39;改良&#39;</span><span class="p">,</span> <span class="s1">&#39;工艺&#39;</span><span class="p">,</span> <span class="s1">&#39;前沿&#39;</span><span class="p">,</span> <span class="s1">&#39;尖端&#39;</span><span class="p">]</span>
<span class="c1">#环保</span>
<span class="n">green_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;绿色&#39;</span><span class="p">,</span> <span class="s1">&#39;节能&#39;</span><span class="p">,</span> <span class="s1">&#39;低碳&#39;</span><span class="p">,</span> <span class="s1">&#39;环保&#39;</span><span class="p">,</span> <span class="s1">&#39;环境友好&#39;</span><span class="p">,</span> <span class="s1">&#39;无污染&#39;</span><span class="p">]</span>
<span class="c1">#短视主义</span>
<span class="n">push_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;加快&#39;</span><span class="p">,</span> <span class="s1">&#39;尽快&#39;</span><span class="p">,</span> <span class="s1">&#39;抓紧&#39;</span><span class="p">,</span> <span class="s1">&#39;月底&#39;</span><span class="p">,</span> <span class="s1">&#39;年底&#39;</span><span class="p">,</span> <span class="s1">&#39;争取&#39;</span><span class="p">,</span> <span class="s1">&#39;马上&#39;</span><span class="p">,</span> <span class="s1">&#39;立刻&#39;</span><span class="p">,</span> <span class="s1">&#39;年内&#39;</span><span class="p">,</span> <span class="s1">&#39;数月&#39;</span><span class="p">,</span> <span class="s1">&#39;数年&#39;</span><span class="p">]</span>
<span class="c1">#模棱两可</span>
<span class="n">prob_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;可能&#39;</span><span class="p">,</span> <span class="s1">&#39;大概&#39;</span><span class="p">,</span> <span class="s1">&#39;左右&#39;</span><span class="p">,</span> <span class="s1">&#39;估计&#39;</span><span class="p">,</span> <span class="s1">&#39;大约&#39;</span><span class="p">]</span>

<span class="n">INOVATION_WORDS</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">inovation_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">bagofword_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">GREEN_WORDS</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">green_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">bagofword_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">PUSH_WORDS</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">push_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">bagofword_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">PROB_WORDS</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">prob_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">bagofword_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;year&#39;</span><span class="p">:</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span>
        <span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;code&#39;</span><span class="p">],</span>
        <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span>
        
        <span class="s1">&#39;inovation&#39;</span><span class="p">:</span> <span class="n">bagofword_df</span><span class="p">[</span><span class="n">INOVATION_WORDS</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;green&#39;</span><span class="p">:</span> <span class="n">bagofword_df</span><span class="p">[</span><span class="n">GREEN_WORDS</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;push&#39;</span><span class="p">:</span> <span class="n">bagofword_df</span><span class="p">[</span><span class="n">PUSH_WORDS</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;prob&#39;</span><span class="p">:</span> <span class="n">bagofword_df</span><span class="p">[</span><span class="n">PROB_WORDS</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;words&#39;</span><span class="p">:</span> <span class="n">bagofword_df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">sum</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">}</span>


<span class="n">res_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;耗时 </span><span class="si">{}</span><span class="s1"> s&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)))</span>

<span class="n">res_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">耗时 164 s
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四讨论">四、讨论</h2>
<p>analysis_info(text) 函数内含有4个for循环，for循环是效率很低的操作。 随着所要计算的词典数n的增加， 方法一时间会随着n的增长而线性增长。</p>
<p>而方法二， 最费时间的瓶颈是将文本转化为数字(比较费时间)，后续的计算均为向量化(矩阵化)的数值计算，随着词典数n的增加， 所消耗的时间会越来越短。</p>
<p><strong>一般情况下，使用「方法一」即可。当第一种方法运行不出结果，可以尝试「方法二」。</strong></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>不要当真 | 词云图看婚姻的本质是什么</title>
      <link>https://textdata.cn/blog/2023-04-14-what-is-the-nature-of-marriage/</link>
      <pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-04-14-what-is-the-nature-of-marriage/</guid>
      <description>&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;今天在知乎刷到「婚姻的本质是什么?」。&lt;/p&gt;
&lt;p&gt;绝大多数回答都是跟私有产权、生产关系相关，直到有一个回答&lt;strong&gt;整个婚姻法里也没有关于爱情的只言片语，从头到尾就一个字，钱，所以婚姻本质上就是契约&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;一下子想到用词频统计试一下， 看看民法典婚姻家庭部分和非婚姻家庭部分两部分内容的侧重点分别是什么。&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;数据准备&#34;&gt;数据准备&lt;/h2&gt;
&lt;p&gt;民法典章节目录如下， 把「第五编　婚姻家庭」单独拿出来做「&lt;a href=&#34;marital-relationship.txt&#34;&gt;婚姻家庭内容&lt;/a&gt;」， 剩下的内容做「&lt;a href=&#34;non-marital-relationship.txt&#34;&gt;非婚姻家庭内容&lt;/a&gt;」。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 第一编　总则
  - 第一章　基本规定
  - 第二章　自然人
  - 第三章　法人
  - 第四章　非法人组织
  - 第五章　民事权利
  - 第六章　民事法律行为
  - 第七章　代理
  - 第八章　民事责任
  - 第九章　诉讼时效
  - 第十章　期间计算
  
- 第二编　物权
  - 第一分编　通则
     - 第一章　一般规定
     - 第二章　物权的设立、变更、转让和消灭
     - 第三章　物权的保护
  - 第二分编　所有权
  - 第三分编　用益物权
  - 第四分编　担保物权
  - 第五分编　占有

- 第三编　合同
  - 第一分编　通则 
  - 第二分编　典型合同
  - 第三分编　准合同

- 第四编　人格权
    - 第一章　一般规定
    - 第二章　生命权、身体权和健康权
    - 第三章　姓名权和名称权
    - 第四章　肖像权
    - 第五章　名誉权和荣誉权
    - 第六章　隐私权和个人信息保护

- 第五编　婚姻家庭
   - 第一章　一般规定
   - 第二章　结婚
   - 第三章　家庭关系
       - 第一节　夫妻关系
       - 第二节　父母子女关系和其他近亲属关系
   - 第四章　离婚
   - 第五章　收养
      - 第一节　收养关系的成立
      - 第二节　收养的效力
      - 第三节　收养关系的解除
      
- 第六编　继承
   - 第一章　一般规定
   - 第二章　法定继承
   - 第三章　遗嘱继承和遗赠
   - 第四章　遗产的处理

- 第七编　侵权责任
   - 第一章　一般规定
   - 第二章　损害赔偿
   - 第三章　责任主体的特殊规定
   - 第四章　产品责任
   - 第五章　机动车交通事故责任
   - 第六章　医疗损害责任
   - 第七章　环境污染和生态破坏责任
   - 第八章　高度危险责任
   - 第九章　饲养动物损害责任
   - 第十章　建筑物和物件损害责任
- 附则
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h2 id=&#34;读取数据&#34;&gt;读取数据&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#第五编　婚姻家庭&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;marital_relationship_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;marital-relationship.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#不含【第五编　婚姻家庭】其余部分&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;non_marital_relationship_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;non-marital-relationship.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;marital_relationship_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;500&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;&#39;第五编\u3000婚姻家庭\n第一章\u3000一般规定\n第一千零四十条\u3000本编调整因婚姻家庭产生的民事关系。\n第一千零四十一条\u3000婚姻家庭受国家保护。\n实行婚姻自由、一夫一妻、男女平等的婚姻制度。\n保护妇女、未成年人、老年人、残疾人的合法权益。\n第一千零四十二条\u3000禁止包办、买卖婚姻和其他干涉婚姻自由的行为。禁止借婚姻索取财物。\n禁止重婚。禁止有配偶者与他人同居。\n禁止家庭暴力。禁止家庭成员间的虐待和遗弃。\n第一千零四十三条\u3000家庭应当树立优良家风，弘扬家庭美德，重视家庭文明建设。\n夫妻应当互相忠实，互相尊重，互相关爱；家庭成员应当敬老爱幼，互相帮助，维护平等、和睦、文明的婚姻家庭关系。\n第一千零四十四条\u3000收养应当遵循最有利于被收养人的原则，保障被收养人和收养人的合法权益。\n禁止借收养名义买卖未成年人。\n第一千零四十五条\u3000亲属包括配偶、血亲和姻亲。\n配偶、父母、子女、兄弟姐妹、祖父母、外祖父母、孙子女、外孙子女为近亲属。\n配偶、父母、子女和其他共同生活的近亲属为家庭成员。\n第二章\u3000结婚\n第一千零四十六条\u3000结婚应当男女双方完全自愿，禁止任何一方对另一方加以强迫，禁止任何组织或者个人加以干涉。\n第一千零四十七条\u3000结婚&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__version__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#词频统计&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;civil_code_word_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;term_freq&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;civil_code_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;marital_relationship_word_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;term_freq&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;marital_relationship_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;non_marital_relationship_word_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;term_freq&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;non_marital_relationship_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h2 id=&#34;词云图&#34;&gt;词云图&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pyecharts.options&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;opts&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pyecharts.charts&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;WordCloud&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;random&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;plot_wordcloud&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wordcounts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;wordcounts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wordcounts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;items&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()]&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;wc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;WordCloud&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;wc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;series_name&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data_pair&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wordcounts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word_size_range&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;wc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_global_opts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;title_opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TitleOpts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title_textstyle_opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TextStyleOpts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;font_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;23&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
                                 &lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;tooltip_opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;opts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TooltipOpts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;is_show&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;render_notebook&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;plot_wordcloud&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wordcounts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;marital_relationship_word_count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
               &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;民法典-婚姻家庭部分&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/marriage.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;plot_wordcloud&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wordcounts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;non_marital_relationship_word_count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
               &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;民法典-非婚姻部分的&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/non-marriage.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;不可当真的实验&#34;&gt;不可当真的实验&lt;/h2&gt;
&lt;p&gt;这里仅仅仅是一个实验，结果不可当真。毕竟使用性质的不同数据做分析，得出的结论可能会完全相反。&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="背景">背景</h2>
<p>今天在知乎刷到「婚姻的本质是什么?」。</p>
<p>绝大多数回答都是跟私有产权、生产关系相关，直到有一个回答<strong>整个婚姻法里也没有关于爱情的只言片语，从头到尾就一个字，钱，所以婚姻本质上就是契约</strong>。</p>
<p>一下子想到用词频统计试一下， 看看民法典婚姻家庭部分和非婚姻家庭部分两部分内容的侧重点分别是什么。</p>
<br>
<h2 id="数据准备">数据准备</h2>
<p>民法典章节目录如下， 把「第五编　婚姻家庭」单独拿出来做「<a href="marital-relationship.txt">婚姻家庭内容</a>」， 剩下的内容做「<a href="non-marital-relationship.txt">非婚姻家庭内容</a>」。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 第一编　总则
  - 第一章　基本规定
  - 第二章　自然人
  - 第三章　法人
  - 第四章　非法人组织
  - 第五章　民事权利
  - 第六章　民事法律行为
  - 第七章　代理
  - 第八章　民事责任
  - 第九章　诉讼时效
  - 第十章　期间计算
  
- 第二编　物权
  - 第一分编　通则
     - 第一章　一般规定
     - 第二章　物权的设立、变更、转让和消灭
     - 第三章　物权的保护
  - 第二分编　所有权
  - 第三分编　用益物权
  - 第四分编　担保物权
  - 第五分编　占有

- 第三编　合同
  - 第一分编　通则 
  - 第二分编　典型合同
  - 第三分编　准合同

- 第四编　人格权
    - 第一章　一般规定
    - 第二章　生命权、身体权和健康权
    - 第三章　姓名权和名称权
    - 第四章　肖像权
    - 第五章　名誉权和荣誉权
    - 第六章　隐私权和个人信息保护

- 第五编　婚姻家庭
   - 第一章　一般规定
   - 第二章　结婚
   - 第三章　家庭关系
       - 第一节　夫妻关系
       - 第二节　父母子女关系和其他近亲属关系
   - 第四章　离婚
   - 第五章　收养
      - 第一节　收养关系的成立
      - 第二节　收养的效力
      - 第三节　收养关系的解除
      
- 第六编　继承
   - 第一章　一般规定
   - 第二章　法定继承
   - 第三章　遗嘱继承和遗赠
   - 第四章　遗产的处理

- 第七编　侵权责任
   - 第一章　一般规定
   - 第二章　损害赔偿
   - 第三章　责任主体的特殊规定
   - 第四章　产品责任
   - 第五章　机动车交通事故责任
   - 第六章　医疗损害责任
   - 第七章　环境污染和生态破坏责任
   - 第八章　高度危险责任
   - 第九章　饲养动物损害责任
   - 第十章　建筑物和物件损害责任
- 附则
</code></pre></div><br>
<h2 id="读取数据">读取数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#第五编　婚姻家庭</span>
<span class="n">marital_relationship_text</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;marital-relationship.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="c1">#不含【第五编　婚姻家庭】其余部分</span>
<span class="n">non_marital_relationship_text</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;non-marital-relationship.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">marital_relationship_text</span><span class="p">[:</span><span class="mi">500</span><span class="p">]</span>
</code></pre></div><pre><code>'第五编\u3000婚姻家庭\n第一章\u3000一般规定\n第一千零四十条\u3000本编调整因婚姻家庭产生的民事关系。\n第一千零四十一条\u3000婚姻家庭受国家保护。\n实行婚姻自由、一夫一妻、男女平等的婚姻制度。\n保护妇女、未成年人、老年人、残疾人的合法权益。\n第一千零四十二条\u3000禁止包办、买卖婚姻和其他干涉婚姻自由的行为。禁止借婚姻索取财物。\n禁止重婚。禁止有配偶者与他人同居。\n禁止家庭暴力。禁止家庭成员间的虐待和遗弃。\n第一千零四十三条\u3000家庭应当树立优良家风，弘扬家庭美德，重视家庭文明建设。\n夫妻应当互相忠实，互相尊重，互相关爱；家庭成员应当敬老爱幼，互相帮助，维护平等、和睦、文明的婚姻家庭关系。\n第一千零四十四条\u3000收养应当遵循最有利于被收养人的原则，保障被收养人和收养人的合法权益。\n禁止借收养名义买卖未成年人。\n第一千零四十五条\u3000亲属包括配偶、血亲和姻亲。\n配偶、父母、子女、兄弟姐妹、祖父母、外祖父母、孙子女、外孙子女为近亲属。\n配偶、父母、子女和其他共同生活的近亲属为家庭成员。\n第二章\u3000结婚\n第一千零四十六条\u3000结婚应当男女双方完全自愿，禁止任何一方对另一方加以强迫，禁止任何组织或者个人加以干涉。\n第一千零四十七条\u3000结婚'
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="c1">#词频统计</span>
<span class="n">civil_code_word_count</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">civil_code_text</span><span class="p">)</span>
<span class="n">marital_relationship_word_count</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">marital_relationship_text</span><span class="p">)</span>
<span class="n">non_marital_relationship_word_count</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">non_marital_relationship_text</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="词云图">词云图</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
<span class="kn">import</span> <span class="nn">random</span>


<span class="k">def</span> <span class="nf">plot_wordcloud</span><span class="p">(</span><span class="n">wordcounts</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="n">wordcounts</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">f</span><span class="p">))</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">f</span> <span class="ow">in</span> <span class="n">wordcounts</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
    <span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">wordcounts</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
        <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
                                 <span class="p">),</span>
        <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">wc</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>


<span class="n">plot_wordcloud</span><span class="p">(</span><span class="n">wordcounts</span><span class="o">=</span><span class="n">marital_relationship_word_count</span><span class="p">,</span> 
               <span class="n">title</span><span class="o">=</span><span class="s1">&#39;民法典-婚姻家庭部分&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/marriage.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">plot_wordcloud</span><span class="p">(</span><span class="n">wordcounts</span><span class="o">=</span><span class="n">non_marital_relationship_word_count</span><span class="p">,</span> 
               <span class="n">title</span><span class="o">=</span><span class="s1">&#39;民法典-非婚姻部分的&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/non-marriage.png" alt=""  />
</p>
<br>
<h2 id="不可当真的实验">不可当真的实验</h2>
<p>这里仅仅仅是一个实验，结果不可当真。毕竟使用性质的不同数据做分析，得出的结论可能会完全相反。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>叙事经济学：揭示经济中的叙事</title>
      <link>https://textdata.cn/blog/2023-04-09-narrative-economic-method/</link>
      <pubDate>Sat, 08 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-04-09-narrative-economic-method/</guid>
      <description>在叙事经济学中，**叙事是指人们在经验、文化和信仰的基础上，通过描述和解释社会事件及其背后的意义，构建一个意义上的世界**。叙事作为一个社会现象，是人们对现实的理解、描述和解释，不仅仅包括人类生活中的日常对话，也包括文学、历史、政治、经济等领域的叙事。人们通过叙事来传递知识、价值观念、历史、文化等方面的信息，同时也对叙事本身进行评价和改变。</description>
      <content:encoded><![CDATA[<p>昨天分享了[管理世界 | 政府与市场心理因素的经济影响及其测度]((<a href="https://textdata.cn/blog/2023-04-08-measurement_of_psychological_factors_and_their_economic_impact">https://textdata.cn/blog/2023-04-08-measurement_of_psychological_factors_and_their_economic_impact</a>)， 感觉使用非结构数据，尤其是文本数据开展经济研究，可以阅读《叙事经济学》。叙事经济学(Narrative Economics)是2019年诺贝尔经济学奖得主罗伯特·希勒(Robert Shiller)的新作，这本书主要探讨了经济学中的叙事现象及其对经济发展的影响。</p>
<p><img loading="lazy" src="img/fields.png" alt=""  />
</p>
<p>读这本书的过程中，大邓脑子里蹦出了**「历史不会重演，但总是惊人的相似」**。 我们普通人透过历史的迷雾，看到的相似性其实是一种故事性的相似。学习历史， 可以学到智慧，所以使用叙事学研究经济问题，也是一种很智慧的合理的方法。</p>
<p>本文将从叙事的定义入手，介绍叙事经济学的研究内容及其价值，进而探讨如何开展叙事研究，最后结合Python文本分析，阐述叙事经济学与文本分析的关系。</p>
<br>
<h2 id="一什么是叙事">一、什么是叙事？</h2>
<p>在叙事经济学中，<strong>叙事是指人们在经验、文化和信仰的基础上，通过描述和解释社会事件及其背后的意义，构建一个意义上的世界</strong>。叙事作为一个社会现象，是人们对现实的理解、描述和解释，不仅仅包括人类生活中的日常对话，也包括文学、历史、政治、经济等领域的叙事。人们通过叙事来传递知识、价值观念、历史、文化等方面的信息，同时也对叙事本身进行评价和改变。</p>
<br>
<h2 id="二叙事经济学的研究内容及其价值">二、叙事经济学的研究内容及其价值</h2>
<p>叙事经济学主要关注经济领域中的叙事现象及其对经济发展的影响。书中列举了许多历史事件，如股市崩盘、经济危机、金融泡沫等，分析了这些事件发生时的社会叙事，以及叙事如何影响人们的行为和经济发展。叙事经济学的研究内容主要包括以下几个方面：</p>
<h3 id="1-叙事的构建与演变">1. 叙事的构建与演变</h3>
<p>叙事是社会历史的产物，它们不是独立存在的，而是构成一个相互联系、相互作用的叙事网络。叙事的演变往往具有惯性和路径依赖性，即过去的叙事会影响人们对现实的认知和解释，进而影响未来的叙事构建和演变。</p>
<h3 id="2-叙事的传播与影响">2. 叙事的传播与影响</h3>
<p>叙事的传播和影响具有复杂性和非线性性，受到多种因素的影响，包括媒体、政治、经济等方面。叙事可以影响人们的行为和决策，进而对经济产生重要的影响。</p>
<h3 id="3-叙事的评价与变革">3. 叙事的评价与变革</h3>
<p>叙事的评价和变革是人们对叙事的反思和改进，是推动叙事演变和发展的重要力量。叙事的评价和变革可以通过媒体、政治、经济等多种渠道进行，也可以通过个体的反思和探索实现。</p>
<p>叙事经济学的研究价值在于揭示了经济领域中的叙事现象，对经济发展的影响进行了深入的分析和探讨。这种研究方法超越了传统经济学中对冷静理性的假设，更加注重人的主观认知和情感因素对经济行为的影响。通过叙事经济学的研究，我们可以更好地理解人们的行为和决策，为经济政策的制定和实施提供更加全面和深入的思考。</p>
<br>
<h2 id="四与叙事相关的学科">四、与叙事相关的学科</h2>
<p>叙事作为一种人类基本的思维方式和交流形式，涉及到多个学科的研究和应用。以下是一些与叙事相关的学科：</p>
<ul>
<li>叙事学：研究叙事的语言、结构和功能，探讨叙事如何构建和传递意义。</li>
<li>心理学：研究叙事对个体认知、情感和行为的影响，探讨叙事与个体心理机制的关系。</li>
<li>文学学：研究文学作品中的叙事结构和技巧，探讨叙事在文学中的艺术表达和意义。</li>
<li>社会学：研究叙事在社会和文化中的作用和意义，探讨叙事与社会结构和变革的关系。</li>
<li>历史学：研究历史事件和过程中的叙事构建和演变，探讨叙事对历史认知和评价的影响。</li>
<li>经济学：研究经济领域中的叙事现象和影响，探讨叙事对经济决策和行为的影响。</li>
<li>认知科学：研究叙事构建和理解的认知机制，探讨叙事与个体认知和思维的关系。</li>
</ul>
<p>这些学科之间有着密切的联系和互动，可以结合起来进行研究。例如，结合心理学和叙事学的研究，可以探讨叙事对个体情感和行为的影响机制；结合经济学和叙事学的研究，可以探讨叙事对经济决策和行为的影响因素和机制。同时，也可以运用机器学习、自然语言处理、网络分析等技术，对大量文本数据进行处理和分析，揭示叙事中的关键词、主题和情感等信息，更深入地探讨叙事的影响和作用。</p>
<br>
<h2 id="三python文本分析与叙事的关系">三、Python文本分析与叙事的关系</h2>
<p>Python文本分析是一种常用的文本数据处理和分析工具，可以对大量文本数据进行快速的处理和分析。在叙事经济学的研究中，Python文本分析可以应用于以下方面：</p>
<ol>
<li>
<p>词频统计</p>
</li>
<li>
<p>叙事内容分析
通过Python文本分析工具，可以对大量文本数据进行词频统计（趋势分析）、主题分析、情感分析等处理，揭示叙事中的关键词、主题和情感等信息。这些信息可以帮助研究者深入了解叙事内容和叙事演变过程，进而探讨叙事对经济发展的影响。下图是文中的词频统计的趋势图，从中可以看到<strong>大萧条</strong>这个词的使用趋势。
<img loading="lazy" src="img/trends1.png" alt=""  />
</p>
</li>
<li>
<p>叙事传播分析
通过Python文本分析工具，可以对社交媒体平台、新闻报道等文本数据进行采集和处理，了解叙事在网络中的传播路径和影响程度，进而探讨叙事对人们行为和决策的影响。这些分析结果可以帮助研究者更好地理解叙事的传播机制和影响因素，为叙事经济学的研究提供数据支持。</p>
</li>
<li>
<p>叙事评价与变革分析
通过Python文本分析工具，可以对叙事内容的变化和评价进行跟踪和分析，了解叙事的评价和变革过程，进而探讨叙事对经济发展的影响。这些分析结果可以帮助研究者更好地理解叙事的演变和发展趋势，为叙事经济学的研究提供理论支持。</p>
</li>
</ol>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理世界 | 政府与市场心理因素的经济影响及其测度</title>
      <link>https://textdata.cn/blog/2023-04-08-measurement_of_psychological_factors_and_their_economic_impact/</link>
      <pubDate>Fri, 07 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-04-08-measurement_of_psychological_factors_and_their_economic_impact/</guid>
      <description>政府与市场关系是经济学的世界性难题，也是中国特色社会主义市场经济的核心问题。在政府政策制定与实施过程中，经济主体会基于自己掌握的信息和认知能力，学习、解读政策含义，形成对政策影响的预期，并基于自身利益最大化原则做出最优决策，从而影响宏观经济运行。因此，研究政府政策背景下各类经济主体的预期等心理因素的产生原因与形成过程，以及经济主体心理因素对经济运行与政策效应的影响机制，是深刻认识政府与市场关系的一个重要视角。本文提出利用人工智能特别是机器学习方法，从海量非结构化大数据提取政府政策变化与各类经济主体对政策变化的反应等信息，从理论和实证两个层面分析经济政策与经济主体的互动关系，以及经济主体心理因素如何影响经济运行与政策效应，并探讨发展非结构化大数据计量经济学，推动形成分析经济政策背景下经济主体心理因素及其影响的实证研究范式，以深入研究政府与市场关系。</description>
      <content:encoded><![CDATA[<h2 id="本文益处">本文益处</h2>
<p>阅读此文可以对以下几个方面有新的收获</p>
<ul>
<li>经济主体预期理论</li>
<li>测度经济主体心理因素方法论基础</li>
<li>测度经济政策不确定性、央行沟通与预期管理、通胀预期、预期冲击、经济主体信心与情绪等方面的应用</li>
<li>非结构化大数据测度心理因素及其影响所面临的主要困难与问题</li>
<li>非结构化大数据实证研究主要方法论的发展趋势和研究重点</li>
<li>&hellip;&hellip;</li>
</ul>
<p>洪永淼,刘俸奇,薛涧坡. <strong>政府与市场心理因素的经济影响及其测度</strong>[J].管理世界,2023,39(03):30-51.</p>
<p>摘要: 政府与市场关系是经济学的世界性难题，也是中国特色社会主义市场经济的核心问题。在政府政策制定与实施过程中，<strong>经济主体会基于自己掌握的信息和认知能力，学习、解读政策含义</strong>，形成对政策影响的预期，并基于自身利益最大化原则做出最优决策，从而影响宏观经济运行。<strong>因此，研究政府政策背景下各类经济主体的预期等心理因素的产生原因与形成过程，以及经济主体心理因素对经济运行与政策效应的影响机制，是深刻认识政府与市场关系的一个重要视角。本文提出利用人工智能特别是机器学习方法，从海量非结构化大数据提取政府政策变化与各类经济主体对政策变化的反应等信息，从理论和实证两个层面分析经济政策与经济主体的互动关系，以及经济主体心理因素如何影响经济运行与政策效应，并探讨发展非结构化大数据计量经济学，推动形成分析经济政策背景下经济主体心理因素及其影响的实证研究范式，以深入研究政府与市场关系</strong>。</p>
<p>关键词：政府与市场关系 心理因素 文本数据 非结构化大数据 政策传导机制</p>
<br>
<h2 id="一引言">一、引言</h2>
<p>本文提出一个基于政府政策背景下 济主体心理因素的产生、形成与变化来研究政府与市场关系的研究思路。 <strong>经济政策的落脚点是经济主体，通过影响经济主体决策发挥作用。 而经济主体具有主观意识与前瞻性思维，在政府制定、实施政策过程中，经济 主体会基于自己掌握的信息和认知能力，学习、解读政策含义，形成对政策影响的预期，并基于自身利益最大化原则，做出最优决策</strong> 。 在政府政策的传导机制中，经济主体的预期以及由预期衍生的情绪等心理因素发挥 着重要作用。 在社会经济活动中，心理因素比比皆是，包括福利经济学的幸福感、市场营销学的客户满意度、 金融学的投资者情绪、微观经济学的企业家精神与风险偏好、宏观经济学的政策不确定性、社会学的社会信任 与社会声誉、新闻学的社会舆情与媒体关注度、教育学的学生学习压力、政治学的群体政治倾向，以及人文学的文化因素（文化是长期实践中形成的比较稳定的社会或群体心理，如企业文化）等。 在金融市场中，流动性 陷阱、羊群效应、资产与房地产泡沫、金融传染病等现象，都与投资者情绪特别是与偏离经济基本面的乐观或 悲观情绪相关（席勒，2001）。 <strong>马克思主义认为，社会存在决定社会意识，同时社会意识对社会存在具有反作用。 经济主体心理因素是对客观经济现实的主观反映，由客观因素决定。 另一方面，经济主体心理因素会影 响经济主体决策，从而影响政府政策效应与宏观经济运行</strong>。</p>
<p>长期以来，由于测度心理因素所面临的困难与挑战，研究者大多只能采用定性分析方法研究 经济主体心理因素及其影响。 随着大数据革命和数字经济时代的来临，各级政府与各类经济主体在数字经济活动中产生海量经济大数据。 大数据特别是非结构化大数据，如文本、图像、音频、视频等数据，包含传统结构化数据没有的信息，包括政府政策变化（如语调变化）与各类经济主体对政策变化的反应等。 使用人工智能特别是机器学习技术，可从非结构化大数据中提取各类经济主体心理信息，构建心理变量，将原来的定性分析转变为符合现代经济学实证研究范式的定量分析。 由于非结构化大数据包含政府政策变化、重要事件冲击以及 各类经济主体对政策变化与重要事件冲击的反应等信息，经济学家可基于非结构化大数据从经济主体心理因 素视角研究政府与市场关系。 <strong>近年来，随着自然语言处理技术的迅速发展，利用文本数据测度经济主体心理因素及其影响，已成为经济学、金融学、会计学、社会学以及政治学等学科的一个热门领域</strong>（格里默、斯图尔特， 2013；埃文斯、阿希维斯，2016；洛克伦、麦克唐纳，2016；根茨科等，2019）。 <strong>本文的主要目的是提出基于非结构化大数据提取政府政策变化与各类经济主体对政策变化反应等信息的研究思路，分析政府政策变化如何影响各类经济主体的预期等心理因素，从而影响经济运行与政策效应，探讨如何发展非结构化大数据计量经济学， 推动形成分析政府政策背景下经济主体心理因素及其影响的实证研究范式，以深入研究政府与市场关系</strong>。</p>
<br>
<h2 id="二预期情绪与经济政策理论视角">二、预期、情绪与经济政策：理论视角</h2>
<p>预期在经济活动中发挥着重要作用，预期管理是宏观经济管理的一个重要手段。</p>
<h3 id="21-预期与预期形成机制">2.1 预期与预期形成机制</h3>
<p>经济学与自然科学的一个根本区别在于，经济主体决策具有<strong>前瞻性</strong>（埃文斯、洪卡波希亚，2001）。 宏观经济学中，预期发挥着关键性作用。 虽然经济学家对预期重要性有一定共识，但对预期形成机制却有不同理解。 目前，宏观经济学有 3 种主要假设解释预期的形成机制，分别是适应性预期、理性预期以及适应性学习。</p>
<p><strong>适应性预期</strong>可追溯至计量经济学先驱者之一费雪（费雪，1911），在 20 世纪 60~70 年代流行于宏观经济学， <strong>其核心思想是，经济主体利用一部分历史信息预测经济未来趋势，属于后顾行为。 在适应性预期假设下，未来的政策变化不一定能够影响经济主体当前的行为决策，因此宏观经济学模型的预测能力也极为有限</strong>。</p>
<p><strong>理性预期概念</strong>由穆斯（1961）正式提出，假设经济主体能够充分利用所获得的全部信息对未来进行预期， 其主观期望与客观的数学期望相一致，不存在系统性预期偏差，且具有前瞻性。 1995 年诺贝尔经济学奖获得者卢卡斯（1976）指出，<strong>不确定性市场条件下经济主体的理性预期对政策效应有重要影响</strong>。 例如，理性预期的 经济主体能够预见政府货币政策对未来经济的影响，形成对货币政策的正确预期，并且根据预期提前做出反应，最终会导致货币政策失效，即货币是中性的。 这是著名的“卢卡斯批判”。 此后，以理性预期为基础的动态随机一般均衡模型成为现代宏观经济学政策分析的一种主流方法。</p>
<p><strong>虽然理性预期理论广泛应用于政策分析，但其没有考虑经济主体形成预期的动态过程，也没有考虑经济主体在收集、处理信息以及认知能力等方面的局限性</strong>。 首先，<strong>理性预期假设经济主体不存在系统性认知偏差</strong>， 这种理想状态需要长期反复试错才能实现。 理性预期可被理解为经济运行过程中的一种长期均衡状态。 事实上，实证研究表明，货币政策并不是中性的（克里斯蒂诺等，1999）。 其次，理性预期理论存在某些缺陷（阿克 洛夫等，2000；席勒，2014），例如<strong>理性预期假设较为严苛，要求经济主体具备同质性，信念一致且具有充分信息，完全了解模型的结构、外生参数以及政策实施前后的经济特征</strong>。 但是，神经经济学研究揭示了经济主体认知水平的有限性（卡默勒等，2005），且经济主体异质性是一个普遍现象（李戎等，2022）。 例如，中国目前有超 过 1.6 亿个市场主体，其中，国有企业、民营企业、个体工商业者、港澳台企业、外资企业存在巨大异质性。</p>
<p>基于上述原因，宏观经济学在理性预期基础上提出一个新的预期理论，即<strong>适应性学习</strong>（萨金特，1993， 1999；埃文斯、洪卡波希亚，2001），假设经济主体能够感知长期均衡状态下的经济运行规则，利用其所有可得信息采用最小二乘法估计上述规则，并随数据集的更新不断改进预期效果。 由于经济主体在适应性学习过程 中利用了其所有信息，经济政策能够通过不断改变市场预期而影响经济运行（埃文斯等，2009）。</p>
<p><strong>适应性学习可视为有限理性的一种表现形式</strong>。 1978 年诺贝尔经济学奖获得者西蒙（1982）认为经济主体获取信息存在成本，且经济主体处理信息的能力有限，不能充分利用其所获得的信息。 在有限理性基础上， 2011 年诺贝尔经济学奖获得者西姆斯（2003）提出<strong>理性疏忽概念</strong>，认为经济主体在获得、吸收与处理信息时需要一定成本，只能从完全信息中选取部分形成预期。 由于获得的信息有限，经济主体不会对政策信号给予足够关注，因此政府通过市场预期影响经济运行的效果受到限制。 除了理性疏忽外，涉及有限理性的理论还有不完全信息理论与粘性预期理论等（安吉雷托斯、连，2018）。 随着计算技术的发展，有限理性由于更贴近现实而在财政与货币政策领域得到广泛应用，成为当前宏观经济学的一个研究热点。</p>
<br>
<h3 id="22-预期与经济波动">2.2 预期与经济波动</h3>
<p>政府释放的政策信号会影响经济主体预期，产生乐观或悲观情绪，继而影响其决策（凯恩斯，1936）。 经典文献认为经济主体预期可分为两部分，一部分是与宏观经济基本面相关的<strong>理性估算</strong>，另一部分是与宏观经济基本面无关的“动物精神”或<strong>情绪因素</strong>（巴斯基、西姆斯，2012）。 本节描述经济政策消息冲击、经济政策不确定性冲击与经济政策情绪冲击影响经济主体心理的形成机制以及经济波动的传导渠道。 其中，经济政策消息冲击和经济政策不确定性冲击与经济基本面相关，而经济政策情绪冲击与经济基本面无关，属于非理性因素。</p>
<br>
<ol>
<li><strong>经济政策消息冲击</strong></li>
</ol>
<p>“<strong>消息冲击</strong>”（也称“<strong>预期冲击</strong>”）的概念最早由庇古（1927）提出。 庇古指出，当公众获得未来经济景气的消息并形成乐观预期时，为应对总需求增加，公众会提前进行资本积累，从而造成当前经济繁荣，这被称为“<strong>庇古周期</strong>”。 与传统实际冲击的区别在于，消息冲击不会影响当前经济基本面，但有可能改变未来经济基本面。</p>
<p>经济政策消息冲击产生的客观原因是经济政策存在内部时滞与外部时滞（利珀等，2013）。 内部时滞是指经济政策变动从提出草案到确定实施之间存在时滞。</p>
<p>理性预期是经济政策消息冲击形成的理论基础。 经济主体信息集包含了其在当前可获得的全部信息。 在经济政策实施之前，经济主体就捕获到了未来经济政策变动的消息，对其当前行为做出相应调整，从而引起 经济波动。 实证研究表明，预期冲击是产生经济波动的重要原因，甚至比经济政策实际冲击更重要（施密特· 格罗赫、乌里韦，2012）。 利好的经济政策消息冲击会引起当前经济扩张，这为政府利用预期调控经济提供理论基础与经验支撑。</p>
<br>
<ol start="2">
<li><strong>经济政策不确定性冲击</strong></li>
</ol>
<p><strong>经济政策不确定性</strong>是指经济主体无法预见未来政策发生变化的可能性（奈特，1921），这是经济主体形成预期的重要因素。 经济政策不确定性包括经济政策能否出台、何时出台、执行力度以及执行效果等（居伦、伊昂，2016）。 贝克等（2016）构建的经济政策不确定性指数将经济政策不确定性细化为政策制定者、政策制定时 间与政策内容变化的不确定性。</p>
<p>经济政策的模糊性或噪音可用来刻画经济政策不确定性。 假设经济主体获得的经济政策变化的信息集包含政策变化消息与政策噪音两部分，其中政策噪音的方差测度经济政策不确定性的大小。 经济主体感知到的经济政策不确定性程度随噪音方差的增大而提高。 当噪音方差趋于 0，即不存在经济政策不确定性时，经济主体将完全依照获得的经济政策变化信息做出决策；当噪音方差趋于无穷大时，经济主体认为接收到的经济 政策变化信息都是噪音，将不会做出任何反应。 因此，经济政策不确定性直接作用于经济主体信息集，影响其预期形成，从而影响经济主体决策（费沃、皮特伦蒂，2016）。 为了抑制经济政策不确定性的消极影响，里科等 （2016）认为政府与经济主体之间应该加强政策沟通协调。</p>
<br>
<ol start="3">
<li><strong>经济政策情绪冲击</strong></li>
</ol>
<p>经济政策情绪冲击是指与未来政策基本面无关的乐观或悲观情绪冲击。 这一思想最早源于凯恩斯的“动物精神”，认为经济主体行为与经济基本面无关，乐观与悲观情绪是驱动经济波动的主要力量。 经济主体许多情绪是从其预期衍生出来的。 例如，由于宣布某个重大利好政策，经济主体改善了对未来经济增长的预期，由此产生一种乐观情绪。 反之，如果出现一个重大负面冲击，经济主体将下调对未来经济增长的预期，由此产生一种悲观情绪。 一个人的情绪也可能受其他因素影响，如受其他经济主体的情绪传染（席勒，2001，2019）。</p>
<p><strong>“动物精神”是非理性心理因素，但现代宏观经济学尝试从理性预期视角讨论经济政策情绪冲击，并使用“预期自我实现”概念刻画情绪冲击对政策效应的影响</strong>。 例如，施密特·格罗赫和乌里韦（1997）分析了平衡预算法则下征收个人所得税产生“预期自我实现”均衡的可能性。 当政府财政支出总量不变时，经济主体减税预 期会形成乐观情绪，提高劳动供给，产生扩张效应。 由于经济总量增加而政府支出规模不变，政府减税成为可 能，从而实现了经济主体最初的减税预期。 情绪冲击还应用于货币政策研究（黄等，2009）。 但鲜有理论刻画政府政策提升经济主体信心的作用机制，吉马良斯等（2016）通过理论模型说明政府财政支出增加会引发经济 主体对社会投资的乐观情绪而提振信心，实现经济扩张，为财政政策提振经济主体信心奠定微观理论基础。</p>
<br>
<h3 id="23-情感因素">2.3 情感因素</h3>
<p>经济主体情感因素也会影响其决策。 情感因素包括人天生拥有的攀比、妒忌与信任等本能心理因素。 <strong>新古典经济学认为经济主体是完全理性，忽视了由人性产生的复杂情感体验（卡默勒等，2005）。 神经经济学、认知经济学与基因经济学证实经济主体情感因素的存在</strong>。 例如，多曼等（2011）利用核磁共振神经影像技术，比 较经济主体血氧水平变化来研究经济主体是否存在攀比效应。 布鲁尼洛等（2020）发现，经济主体的基因会影 响其性格等心理因素，从而影响经济主体行为。</p>
<p><strong>经济政策如何通过经济主体情感因素影响宏观经济是当前一个研究热点</strong>。 在现代经济中，服务业特别是消费性服务业是经济主体情感产生、交流与传递的过程，情感因素对服务业的发展有重要影响。 <strong>传统政策分析建立在理性预期基础上，但随着行为经济学与实验经济学提供日益丰富的实证证据，越来越多的学者关注经济主体情感因素对经济政策传导机制的影响。 行为经济学通过洞察经济主体心理活动分析经济主体的行为决策，为经济学理论创新提供心理学证据</strong>。 经济政策与经济主体心理因素相互结合，催生了行为财政学与行为货币经济学，奠定了宏观经济政策分析的微观行为基础，为研究政府与市场关系提供了全新分析框架。</p>
<p><strong>行为财政学</strong>最早由麦卡弗里和斯莱姆罗德（2006）提出，将传统经济学基本原理的“理性”部分与行为经济学的“非理性”部分（或心理因素）结合起来，研究财政政策传导机制。 财政支出可通过经济主体同群效应这一 外部性因素影响宏观经济运行。 <strong>同群效应</strong>是指经济主体效用函数不仅取决于自身条件，还受其所处社会地位 （用消费、收入或工作时间等衡量）的影响。 在消费层面，政府支出可通过经济主体之间的攀比效应对产出产生扩张效应（黄等，2022b）。 具体机制是，政府支出增加能够提高社会平均财富，具有攀比心理的人会增加工作时间来追赶其他人的消费水平，从而推动产出扩张。 同群效应的另一表现形式是经济主体工作时间的“内 卷”，费沃等（2013）发现政府支出增加能够提高社会平均工作时间，由于存在“内卷”心理，个体观测到别人增 加工作量后也将努力工作，最终促进产出扩张。</p>
<p>在财政收入端，现有文献认为存在“<strong>税收遵从之谜</strong>”，即传统理性经济人假设认为经济主体依法纳税程度会随着税务机关逃税处罚力度加大而增强，但实证研究发现两者存在负相关。 这涉及经济主体心理因素的作用机制：当处罚力度过大时，经济主体会感受到不公平或出现逆反心理，从而产生税收不遵从行为（温策尔， 2003）。 同样，政府可利用经济主体心理因素增强其税收遵从能力，丹尼夫等（2021）发现，税务部门采用威慑手段可增强征税能力。 此外，财政幻觉理论认为，由于经济主体存在认知偏差，经济主体感受到的纳税负担会 小于实际纳税负担，从而降低其逃税动机，减少规避政府税收的影子经济规模（布恩等，2012）。</p>
<p><strong>行为货币经济学</strong>由罗瑟利（2015）提出，将心理要素纳入传统货币政策分析框架中，探讨货币政策通过心理因素发挥作用的传导机制。 例如，信任对货币政策效应具有重要影响。 布尔西安和法亚（2018）发现，经济主体对央行信任下降会恶化货币政策实施效果。 史蒂文森和沃尔弗斯（2011）和李新荣等（2014）发现，经济主 体对中央银行的信任程度会影响通胀预期。 当经济繁荣时，经济主体对央行有较强的信任度，从而产生较低的通胀预期。</p>
<p>经济主体心理因素也可解释现代货币政策理论中的悖论。 德尔内格罗等（2015）发现新凯恩理论存在 “<strong>前瞻性指引之谜</strong>”，即央行发布未来名义利率变化会引起产出与通胀波动，基于理论模型估算的波动率大于实际数据的波动率。 为解决上述难题，艾劳多（2020）将经济主体面临的诱惑和自控力等心理因素加入效用函 数，发现修正后的新凯恩斯理论结果与经验证据一致，较好解释了传统理论模型存在的“前瞻性指引之谜”。</p>
<p><br><br></p>
<h2 id="三经济学心理因素测度非结构化大数据视角">三、经济学心理因素测度：非结构化大数据视角</h2>
<p>经济学理论分析，特别是经济学建模，是理解经济主体心理因素及其影响的理论基础。 <strong>第二节的各种预期理论为人们理解政策变化如何影响经济主体预期，以及经济主体预期如何影响政策效应提供了学理基础。 更一般地，经济主体的心理活动，如预期、信心、情绪、情感等的产生、形成与变化，具有规律性，可通过经济学建模，包括使用数学模型，结合经济学与心理学理论进行研究</strong>。 例如，人工神经网络模型就是根据认知科学关 于人脑认知理论而提出的一个模仿人脑认知过程的数学模型或非参数模型，可用于近似任何未知平滑函数， 具有所谓泛逼近性质（怀特，1992），已成为机器学习的一个重要方法。 反过来，可借助数学模型研究经济主体的心理活动规律。 例如，人工智能特别是机器学习可用于研究经济主体预期的产生、形成与演变过程。 相对于宏观经济学中基于最小二乘法估计的适应性学习机制，基于机器学习的适应性学习不必假设经济主体能够感知均衡状态下的经济运行规则，机器学习能够充分利用经济主体所获得的信息，捕捉预期形成与变化过程中各种线性与非线性动态特征，最终趋近未知的经济运行规则。</p>
<p><strong>任何经济理论都建立在一定的假设基础之上，这些假设是否与经济现实相吻合、经济理论能否解释经济现象，都是需要回答的重要问题。 解决这些问题的关键是通过基于数据的实证研究，检验从经济理论推导出的可实证的重要预测或结论是否正确</strong>。 从实证研究视角分析政策变化如何影响经济主体预期等心理因素从而传导到经济运行的一个关键问题是，如何测度反映政府与市场互动关系的经济主体心理因素。 长期以来， 心理因素测度一直面临诸多挑战与困难，甚至有人质疑心理因素是否可以测度。 例如，经济统计学鼻祖威廉· 配第认为，经济主体心理因素是不可测度的，他在 1672 年出版的代表作《政治算术》中说：“ 至于那些从某些人 的容易变动的思想、意见、胃口和情绪为依据的原因，则留待别人去研究。 这里我敢明白地、老实说，以这些因 素（容易变动的思想等等）为依据（即使这些因素可以叫做依据）的原因是不能谈得透彻的。”（配第，2014）。</p>
<p>事实上，现代心理学与实验经济学对经济主体心理因素能否测度的问题，已提供了肯定的答案。 <strong>现代心理学有一门方法论学科叫心理计量学，该学科依据心理学理论，使用科学方法与工具测度人的能力、人格以及心理健康等心理特征与行为。 常用方法包括观察法、访谈法、问卷法、实验法、心理物理法等</strong>。 心理测度著名的例子包括对智商和情商的测度。 但是，心理学与实验经济学对心理因素及其影响的测度，大多通过实验室、 观察访谈或统计调查等方式，其研究的深度、广度与抽样频率不可避免受到各种限制与约束。 例如，经济主体幸福感是福利经济学一个非常重要的心理变量。 人们常采用问卷调查形式评估调查对象对于家庭、健康、安 全、价值观、自由、幸福以及生活满意度等多方面的“<strong>表述性偏好</strong>”，以此构建指数测度幸福感（班杰明等， 2014）。 美国家庭调查数据库通过问卷调查测度经济主体幸福感。 受访者以七分制回答问题：“你认为这些天的情况如何？”，其中数值“1”定义为“非常不开心”，数值“7”为“非常开心”，中间取值没有明确定义。 中国家庭追踪调查也包含有关个人幸福感的问卷问题。 <strong>但这种自我报告的幸福感指数无法有效进行跨群体比较（邦 德、兰格，2014），并且由于幸福感测度结果还依赖于回答者（包括其所处环境）以及问题的措辞，因此自我报告的幸福感指数的真实性存在一定质疑</strong>（迪顿、斯通，2013）。</p>
<p>随着互联网、移动互联网、物联网以及数字经济的蓬勃发展，各级政府与各类经济主体在数字经济活动中产生海量大数据，这些经济大数据包含政府政策变化、重要事件冲击以及各类经济主体对政策变化与重要事 件冲击的反应等信息。 人工智能技术的发展和大数据可获得性的提升，为实证研究政府与市场关系提供了一 个可行方法。 大数据，特别是文本、图像、音频与视频等非结构化大数据，包含很多传统结构化数据没有的信息，如政府政策的语调变化与经济主体的预期、信心、情绪、情感等，特别是，<strong>语言是人类进行信息传递、情感交流与沟通的最主要工具，因此文本数据包含大量各类经济主体的心理信息，可通过自然语言处理技术提取</strong>。 文本数据以及其他非结构化大数据的来源主要是社交数字平台、电商平台、上市公司财务报告、新闻报纸与其他新闻媒介、各级政府工作报告、会议纪要、公告、声明、谈话，以及政府领导人公开发表的文章等。 文本数据可分为正式文本数据与非正式文本数据。 正式文本数据主要包括政府工作报告、政策文件、官方新闻媒体报道与上市公司披露文件等官方文本；非正式文本数据包括社交网络文本（如微博、微信公众号等）与网络论坛 文本（如股吧）等个人文本。 在形式上，正式文本具有内容规范、表述相对客观以及信息密度高的特点；非正式文本信息密度较低，但包含大量口语与俚语等非正式语言，提供额外信息。 <strong>从文本数据中构建心理变量后，可代入计量经济学模型中，定量研究其经济影响。 这样，原来只能进行定性分析的问题，可转化为较严谨的定量实证研究。 这种研究范式通常称为文本回归</strong>。</p>
<p>与观察访谈、统计调查以及实验方法相比，基于海量非结构化大数据的心理因素测度具有更广泛的样本代表性，可在较大程度上减少样本选择偏差。 非结构化大数据具有高频性或实时性，可构建心理变量时间序列数据，甚至高频时间序列数据，用来刻画经济主体心理变量的动态特征。 海量微观行为大数据还包含经济主体之间互动关系信息，有助于研究情绪等心理因素如何在经济主体中相互传染。 但是，大数据不是统计调查数据，而是数字经济活动的数字化记录或观测数据。 大数据特别是非结构化数据种类繁多，结构复杂，来源不一。 因此，从这些非结构化大数据中提取经济主体心理信息并对其进行实证建模分析，在方法与技术上具有相当挑战性。 在第四节和第五节中，我们先介绍目前经济学、金融学、会计学等学科使用文本数据测度经济主体心理因素及其影响的实证研究成果，然后讨论如何构建基于非结构化大数据分析政府政策变化与经济主体心理因素互动关系的实证研究范式。</p>
<p><br><br></p>
<h2 id="四文本数据与经济主体心理因素测度实证视角">四、文本数据与经济主体心理因素测度：实证视角</h2>
<p><strong>文本数据分析是当前经济学、金融学、会计学等学科的一个主流实证研究范式，其中自然语言处理是最主要的文本数据信息提取技术，常用方法包括词典法、主题模型、词向量法等</strong>。 具体步骤为：</p>
<ol>
<li>使用分词技术对文本数据进行分词处理，生成文本矩阵，通常采用独热表示法将上述矩阵转化为高维稀疏数据矩 阵，然后再使用词嵌入技术进行降维处理，最终将非结构化数据转化为数据矩阵。</li>
<li>采用统计方法将 数据矩阵转化为目标信息，根据是否存在训练集，可分为词典法或主题分类模型等无监督学习方法，以及包含深度神经网络与卷积神经网络等新兴深度学习技术的有监督学习方法（沈艳等，2019）。</li>
</ol>
<p>在这一节，我们讨论如何基于文本数据测度 <strong>经济政策不确定性、央行沟通与预期管理、通胀预期、预期冲击、以及经济主体信心与情绪</strong>。</p>
<br>
<h3 id="41-经济政策不确定性的测度">4.1 经济政策不确定性的测度</h3>
<p><strong>经济政策不确定性</strong>是研究政府与市场关系的一个重要内容。 相关实证研究涉及财政政策不确定性、货币政策不确定性、贸易政策不确定性等。 在现实经济中，政策不确定性在不同程度上与不同范围内一直存在，影响经济主体行为决策与经济运行。 <strong>传统方法基于时间序列计量经济学模型，将经济政策波动性视为不确定 性。 由于经济政策与产出等变量相互影响，基于时间序列模型的经济政策不确定性包含宏观经济不确定性， 因此，这种方法存在内生性问题</strong>（凯利等，2016）。 为解决此问题，有学者将非经济虚拟变量作为代理变量测度经济政策不确定性，例如朱利欧和约克（2012）采用选举变量测度经济政策不确定性，但其缺点是所测度的政策不确定性时变性较差。 除计量经济学建模方法外，有文献基于调查数据构建经济政策不确定性指数，但这种指数仅能刻画某一政策不确定性，或政策不确定性的某一方面，不能全面刻画经济政策不确定性。 此外，接受调查人员可能存在样本选择偏差或系统性预测误差，导致政策不确定性测度的主观性较大（朱拉多等， 2015）。</p>
<p><strong>文本数据分析能够捕捉经济主体对经济政策不确定性的真实感知，已成为测度经济政策不确定性的主流方法</strong>。 贝克等（2016）在这方面做出开创性贡献，他们选取美国最具影响力的 10 家报纸，统计了每月包含经济、政策与不确定性等关键词频数，构建了美国经济政策不确定性指数。 随后，黄和陆（2020）采用中国大陆报纸测度中国经济政策不确定性月度指数。 陈和钟（2019）开发了新的机器学习算法（包括单词嵌入、多层感知 器和递归神经网络等），根据《人民日报》中文文本数据构建了中国政策变化指数。</p>
<p>贝克等（2016）的方法可用于构建各种分类政策不确定性指数。 例如，赫斯特德等（2020）基于《华盛顿邮报》《华尔街日报》以及《纽约时报》测度了美国货币政策不确定性。 汉德利和利茂（2017）以及卡尔达拉等 （2020）构建了美国贸易政策不确定性指数。 李和徐（2019）测度了美国地方税收政策不确定性指数。 朱军 （2017）基于汉语语境特征分析中国财政政策不确定性。 陈英楠等（2022）利用中国六家主流报纸构建了房地产政策不确定性指数。 部分学者还从经济主体主观感受角度构建政策不确定性指数。 邦坦皮等（2016）基于经济主体在其谷歌搜索时使用与经济政策不确定性相关的词汇量构建了美国政策不确定性指数。 聂辉华等 （2020）利用中国 A 股上市公司年报测度了中国企业对经济政策不确定性的主观感受。 本古里亚等（2022）基 于中国上市公司年报构建了中国企业视角下贸易政策不确定性指数。</p>
<br>
<h3 id="42-央行沟通与预期管理">4.2 央行沟通与预期管理</h3>
<p><strong>在现实经济中，中央政府与各级地方政府之间、不同政府部门之间以及政府与各类经济主体之间的信息传递经常遇到障碍，导致政策在从中央到地方政府传导过程中，被曲解或层层加码，或者在从政府向经济主体传导过程中，引起经济主体过度反应</strong>。 例如，2021 年，政府有关部门相继出台了一些反垄断措施，规范数字经 济平台、资本市场以及文化市场有序发展，以保障国家数据安全与经济安全，保护劳动者、消费者、中小企业等社会弱势群体的合法权益，同时还出台促进共同富裕的远景目标与政策措施。 这些政策在方向上是正确的， 但却引起了部分市场主体的不安情绪，导致国内外市场波动。 这在一定程度上反映了政策不确定性的客观存 在，同时也反映了政府与市场在政策沟通过程中存在堵点，有待化解、改进（洪永淼，2021）。 在这方面，一些国家央行在出台前瞻性货币政策时，通常会与市场主体密切沟通，尽量减少市场波动，这值得借鉴与学习。</p>
<p>央行传统预期管理注重实际政策措施，而现代预期管理则强调央行与经济主体的沟通交流（莫里斯、茜 恩，2018）。 <strong>2008 年金融危机后，受零利率下限约束，以调节名义利率为基础的通胀预期管理失效，央行无法通过利率渠道影响消费与投资，央行沟通便成为一个重要的新型货币政策工具</strong>（伯南克，2020）。 所谓央行沟通是指央行向经济主体公布货币政策目标与规则、经济展望以及货币政策走向等相关信息的过程（布林德等， 2008）。 央行沟通可释放货币政策变化消息，降低经济主体所感知的货币政策变化中的噪音成分，促使经济主体形成比较一致的预期，这已成为央行通过预期调节经济运行的一个重要方式，甚至比实际货币政策操作还重要（汉森、麦克马洪，2016）。 央行沟通通过释放经济展望与前瞻性指引等信息改变经济主体预期，因此央行沟通文本数据包含经济主体关于货币政策预期的丰富信息。 <strong>评估央行沟通效果已成为当前宏观经济学与金融学研究的一个热点</strong>，也为丰富货币政策预期管理理论提供实证基础。 虽然中国尚未受零利率下限的影响， 但央行沟通在引导中国经济主体预期方面也发挥着重要作用。 易纲在《人民日报》发文指出，建设现代中央银 行制度要使货币政策沟通制度化，有效管理和引导市场预期（易纲，2022）。</p>
<p>目前，中国央行沟通语料库包括口头沟通与书面沟通。 口头沟通主要包括央行领导人讲话、采访、新闻发 布会、窗口指导等；书面沟通主要包括每季度定期发布的《货币政策执行报告》以及《金融稳定报告》和央行货 币政策委员会的会议纪要等。 书面沟通相较于口头沟通更为正式，沟通也更加常态化。 <strong>如何从口头与书面沟通文本数据获得经济主体所感知的政策语调是现有研究的难点</strong>，林建浩等（2021）根据中国央行沟通特有的表达习惯，将自建词典与通用词典相结合，采用栅栏分布式多项回归模型解决文本数据的高维性和稀疏性问题， 测度中国央行沟通指数。 <strong>目前测度央行沟通的方法有叙事法、主观赋值法、措辞提取法以及机器学习方法</strong>。 与前 3 种方法相比，机器学习方法在提取央行沟通信息方面具有明显优势，这种方法可构建文本情绪、文本相似性以及文本可读性等指数。 在<strong>文本情绪</strong>方面，现有文献通常使用哈佛大学通用调查词典（第四版）等特定词典构建美联储沟通情绪指数（杰加迪西、吴，2015；施默林、瓦格纳，2019），以及测度欧洲央行货币政策立场（皮 考特、雷诺，2017），而姜富伟等（2021a）基于中文文本分词方法与情绪词典构建中国央行沟通情绪指数。 <strong>文本相似度</strong>是指不同的央行沟通文本数据在遣词造句或表达含义上的相近程度，可反映政策发布者信息公布的谨慎程度。 现有文献大多使用词频向量的余弦相似性测度文本相似度（埃尔曼、塔勒米，2020）。 <strong>文本可读性</strong>是指经济主体阅读与理解央行沟通文本的难易程度。 洛克伦和麦克唐纳（2014）使用弗莱什易读指数刻画了美 联储货币政策文本可读性。 费拉拉和安吉诺（2022）使用有监督与无监督的机器学习方法从欧洲央行演讲与推文等文本中提取主题信息，也采用弗莱什易读指数系统刻画欧洲<strong>央行沟通清晰度</strong>。</p>
<br>
<h3 id="43-通胀预期的测度">4.3 通胀预期的测度</h3>
<p>通胀预期在现代宏观经济学特别是货币政策研究中占有重要地位，影响经济主体投资决策和政府政策制定。 在新凯恩斯理论中，央行货币政策规则从盯住当前市场的通货膨胀率，逐渐发展为包含通胀预期的<strong>前瞻性规则</strong>（黄等，2009）。 因此，精准测度并管理通胀预期决定了货币政策的有效性。 由于传统计量经济学模型 可能包含过多噪音，通过构建计量经济学模型提取金融市场有效信息来预测通胀的效果并不理想（鲍尔、麦卡 锡，2015）。 大型机构关于通胀预期的调研数据似乎是更好选择。 现有研究表明基于调查数据的通胀预期预测结果优于计量经济学模型的预测（安等，2007；福斯特、赖特，2013），但调查数据本身存在不少缺点。 例如， 受成本限制，调查群体仅包含市场中的小部分个体且抽样频率过低。 此外，调查获得的专家与居民两组通胀预期数据在预测未来通胀能力方面差异较大，央行在制定货币政策时需要对两组通胀预期数据赋予不同权重 （利齐亚克、盛，2021）。</p>
<p><strong>电话、报纸与网络等媒体是经济主体理解政策与市场的重要中介，经济主体通常从媒体间接获得通胀预期信息，而不是直接在市场中花费大量时间追踪信息（卡罗尔，2003）。 因此可从文本数据构建通胀预期数据</strong>。 例如，加布里埃良等（2019）基于在线新闻数据构建英国日度通胀预期数据，比较准确反映通胀预期。 这种方法可推广到测度其他国家的通胀预期。 例如，安基利科等（2022）基于意大利推特数据，采用机器学习方法构建日度通胀预期数据；中岛等（2022）通过日本内阁办公室经济调查文本数据，采用机器学习方法构建了 日本价格情绪指数。 目前，鲜有研究基于文本数据构建中国通胀预期数据。</p>
<br>
<h3 id="44-经济政策预期冲击的识别">4.4 经济政策预期冲击的识别</h3>
<p>财政政策与货币政策预期冲击或消息冲击对宏观经济波动有重要影响。 <strong>由于计量经济学模型的信息集通常小于经济主体所获得的信息集，传统向量自回归模型无法识别预期冲击对经济波动的影响（杨，2005；利珀等， 2013），因此，扩展计量经济学模型信息集成为研究经济政策预期冲击的一个重要问题，其中一个办法是寻找预期的代理变量</strong>。 传统方法分别将金融市场信息（与政府支出相关行业股价）和调查数据作为经济政策预期的代理变量，但这两种测度方法均存在不足之处。 金融市场信息存在较大噪音，且仅反映金融市场参与者对经济政策的预期。 调查数据仅覆盖小样本群体，且抽样频率较低。 因此，采用叙事法获得经济主体对经济政策的预期 是一个较好选择。 在政府收入消息冲击方面，罗默和罗默（2010）利用美国总统演讲与国会记录中税收变化信息，构建税收消息变量，考察税收变化对宏观经济波动的影响。 在政府支出消息冲击方面，雷米（2011）利用商业报刊报道的国防支出数据，发现国防预期支出扩张可增加宏观经济产出与劳动力投入，降低消费与投资。</p>
<p>主流文献认为，罗默和罗默（2010）与雷米（2011）的叙事方法能够有效识别财政支出预期冲击。 但是，<strong>贾西姆等（2022）认为叙事法构建的指标存在明显不足，例如，没有充分考虑消息包含的噪音成分，且经济主体预期可能随时间发生变化等，因此，采用文本数据分析可能是更好选择</strong>。 拉森和托斯鲁德（2019）使用潜在狄利克雷分配主题模型，构建了挪威全要素生产率消息冲击数据，发现全要素生产率消息冲击带来产出扩张。 贾西姆等（2022）采用半监督文本分析方法准确区分了增税与减税新闻，通过美国总统演讲文本中关于税收变化消息的重复度确定税收消息是否实现，他们发现税收变化消息对产出具有显著的延迟性影响。 这是未来研经济政策预期冲击如何影响宏观经济波动的一个重要方向。</p>
<br>
<h3 id="45-经济主体信心情绪与情感的度量">4.5 经济主体信心、情绪与情感的度量</h3>
<p>现有文献大多采用调查数据测度经济主体信心，但调查数据仅反映部分群体信心指数，而且由于调查成本较高、抽样频率低，调查数据不能充分、及时地反映经济主体信心变化趋势。 相比之下，<strong>文本数据可更好测度经济主体信心与情绪</strong>。</p>
<p>经济主体情绪包含正面与负面、乐观与悲观、积极与消极、看涨与看跌等心理因素，文本数据分析也常用“语调”刻画情绪。 由于文本数据规模大、频率高，测度情绪已成为文本数据在金融学的一个重要应用（姜富伟等，2021b）。 目前，测度情绪的方法主要有以词典法为主的无监督学习和以机器学习为主的有监督学习。 词典法是依据事先给定的情绪词典，通过分析文本数据中积极与消极词汇出现的频数来测度经济主体情绪。 由于词典法依赖各个字典所选择的单词，且仅包含与情感相关的词汇，因此可能无法捕获全部情感信息。 近年来，机器学习越来越受到研究者的青睐（根茨科等，2019）。 有监督的机器学习方法把情感测度视为一个文本分类问题，将有标签的文本数据集划分为训练集与测试集，利用训练集来训练模型，最后将训练模型用于测试集进行预测。 主要机器学习方法包含基于概率论的朴素贝叶斯分类器（布尔迈尔、泽希纳，2021）和支持向量机（李等，2019）等。 对没有预定词典的文本数据而言，机器学习方法是较好的选择，但是与词典法相比，机器学习的训练集往往需要通过人工选择，因此耗时成本较大且分类结果因人而异，导致研究结论可复制性不高。</p>
<p>经济主体情绪显著影响宏观经济运行。 张成思等（2021）采用中国 A 股上市公司年报测度了中国企业宏观经济感知程度，分析表明，当央行实施积极货币政策时，对宏观经济感知更乐观的企业会积极响应政策刺激，增加投融资行为。 范小云等（2022）使用中国 32 家知名报刊 175 万条新闻数据，将所开发的词典与机器学习方法相结合，提出一种混合式情绪分析方法，发现新闻媒体情绪可通过提振经济主体信心来扩张消费与产出。 夏皮诺等 （2022）基于美国 16 家主要报纸近 24 万条经济金融新闻文本数据，将现有词典与专门构建的新词典相结合测度美 国市场情绪，发现其构建的美国市场情绪指数与经济周期和经济事件显著相关，基于文本数据的高频情绪指数能 预测美国市场情绪调查数据，且乐观情绪冲击能引起产出与消费的显著扩张。 现有文献也发现基于文本数据得到的经济主体情绪指数可改进宏观经济预测。 例如，阿吉拉尔等（2021）基于西班牙报纸构建了日度市场情绪指 数，该指数与经济信心调查数据高度相关，但在预测 GDP 变化时优于调查数据。 孔索莱等（2021）从意大利与西班牙经济新闻中构建新闻媒体情绪指数，发现这一指数有助于预测意大利与西班牙主权债券收益率利差。</p>
<p>除了与预期相关的信心与情绪等心理因素，<strong>行为财政学与行为货币经济学理论表明，情感因素（如攀比、 嫉妒、内卷等）对经济主体决策同样重要</strong>。 在行为财政学中，为提高经济主体税收遵从度，增强国家税收征收与公共服务供给能力，现有文献从经济政策威慑性视角研究政策宣传的有效传导渠道。 例如，为了测度税法 对经济主体的威慑性，毛捷等（2022）通过中国知网报刊获取 2.15 万条税法宣传报道，借助机器学习方法研究威胁性宣传效果，即通过曝光违法犯罪行为达到警示与宣传效果，他们发现税法宣传可降低经济主体侥幸心理，提升其税收遵从意愿。 在行为货币经济学中，理论模型通常认为经济主体对经济政策的信任是货币政策 发挥作用的重要因素，但如何测度经济主体信任度是一个实证研究难点。 余等（2021）基于推特评论文本数据，采用深度学习方法测度新闻可信性，发现金融报刊的新闻可信度对股票价格有正向影响。</p>
<p>鲜有文献测度经济主体的攀比、嫉妒和内卷等情感因素，主要是在测度方法上存在一定困难。 例如，在研究经济主体消费是否存在<strong>同群效应</strong>时，需要寻找周围群体消费水平的代理变量，目前国内研究基于调查数据构建区县层面同群组，代表周围群体消费水平（宋泽、邹红，2021），美国学者则构建了州层面同群组（伯特兰、 莫尔斯，2016），如此宏观层面的划分很难准确测度周围群体消费水平对经济主体消费的影响。 但随着网络技术的发展与普及，可直接从微博与贴吧等文本数据提取经济主体对周围群体消费变化的态度与看法等信息， 用于研究经济主体是否存在同群效应。</p>
<p><br><br></p>
<h2 id="五问题挑战与展望方法论视角">五、问题、挑战与展望：方法论视角</h2>
<p>目前，经济学、金融学与会计学等学科在利用文本数据测度经济主体心理因素及其影响方面，已获得一系列重要实证发现，凝炼了一些经验典型特征事实，为深刻理解政府政策背景下经济主体心理因素的重要作用提供了很多洞见。 同时，基于非结构化大数据的经济学实证研究尚处在初级阶段，还存在不少困难与问题，需要进一步改进、完善基于非结构化大数据的实证研究，特别是发展新的研究方法与分析工具，形成一个系统的研究范式。</p>
<h3 id="51-非结构化大数据实证研究面临的困难">5.1 非结构化大数据实证研究面临的困难</h3>
<h4 id="511-因果推断问题">5.1.1 因果推断问题</h4>
<p>经济学研究最主要目的是识别经济因果关系，揭示经济运行规律。 因果关系是指在其他因素保持不变的条件下，通过改变某个变量（如政策变量或心理变量），观测结果变量是否随之变化。 <strong>一个经济理论是否具有深刻解释力取决于它能否揭示可被验证的经济因果关系。 目前，文本数据实证研究发现，经济主体心理因素对宏观经济与金融市场有重要影响，但这种影响大多体现为相关性，而不是因果关系</strong>。 例如，贝克等（2016）发现，他们构建的美国经济不确定性指数与美国就业和产出变化存在负相关关系，与美国股票市场波动程度则呈现正相关关系，但这些相关关系不能被解释为因果关系。 推动政府政策背景下经济主体心理因素与经济变量之间的因果分析是未来政府与市场关系研究的一个重点。 在实验科学中，可通过实验手段控制其他因素不变，识别并测度所关注变量之间的因果关系。 但在经济学与社会科学中，很多情形难以实现可控实验，导致推断心理变量与经济变量的因果关系面临很大困难。 经济主体心理因素的因果分析的难点首先在于，海量经济大数据本质上是非实验性观察数据，很难甚至不可能控制其他因素保持不变。 更重要的是，经济主体决策时的前瞻性思维，具有“反身性”特征，容易形成双向、 复杂的互动关系。 <strong>因此，在研究经济主体心理因素的因果关系时，需要与有关经济政策传导机制的经济理论结合起来，借助经济学理论甚至心理学理论指导</strong>。 此外，还需要应用计量经济学、统计学与机器学习的新方法、新工具帮助推断因果关系。 例如，众所周知，机器学习样本外预测比很多传统计量经济模型要精准得多， 机器学习精准预测能力不是基于经济学因果关系，而是基于大数据中的变量特征与变量之间的统计关系（如 相关关系或预测关系）。 但是，机器学习可用来改进基于观测数据的因果关系推断。 例如，虚拟事实估计是使用观测数据识别与测度经济因果关系的一个重要方法（珀尔，2009；瓦里安，2016）。 通过挖掘大数据中变量之间的相关性或预测关系，机器学习可精准估计或预测虚拟事实结果，即假设经济政策没有实施时经济运行的潜在结果。 然后对比虚拟事实结果与实际结果的差别，便可识别经济政策或心理因素与经济结果之间的因果关系，并精准测度政策效应或心理因素影响的大小。</p>
<br>
<h4 id="512-经济主体的异质性与社会性问题">5.1.2 经济主体的异质性与社会性问题</h4>
<p><strong>现有文本数据实证研究大多假设经济主体是同质且独立的个体，通过简单加总得到宏观心理变量的测度，然后应用计量经济学方法推断宏观心理变量与宏观经济变量间的数量关系。 这种研究缺乏微观经济理论基础，且存在由于经济主体异质性而导致信息失真的可能性。 现实经济中的经济主体通常呈现显著的异质性特征</strong>，例如，在中国市场主体中，企业分为国有企业与非国有企业，非国有企业又分为民营企业与外资企业，此 外还有混合所有制企业与合资企业，以及大量个体工商业者。 这些企业具有明显不同的组织结构和行为方式，表现出巨大异质性。 地方政府之间在经济社会治理方面也存在显著差异。 在政策从中央政府向地方政府 传导过程中，地方政府由于认知、理解以及实际困难等各种原因，对中央政府政策的执行与响应程度存在差别。 此外，不同经济主体对同一政策的反应不一样，同一政策对不同经济主体的影响也是不一样的。 <strong>文本等非结构化大数据可提供各类异质性地方政府与经济主体的动态行为信息，从而更准确测度政策背景下经济主体心理因素及其影响，特别是经济政策对不同行业、不同地区、不同群体的动态分布效应</strong>。</p>
<p>另一方面，互联网等信息技术的快速进步与广泛应用，使经济主体之间结成复杂社会网络，这种网络会强化产生共情、情绪传染等现象的趋势，通过乘数效应放大或者收缩经济政策效应或重要事件冲击的影响。 2021 年初，美国股票市场大量散户在一家叫做“游戏驿站”的美国游戏零售公司股票交易中，抱团与对冲基金激烈博弈，他们受社交网络平台以及美国主流媒体报道传播的情绪感染，形成一股强大的乐观情绪，使该公司股价在不到一个月上涨 20 倍，迫使做空的机构投资者不断爆仓，充分显示了社交网络平台情绪传染的巨大威力。 海量经济行为大数据包含大量微观经济主体互动关系信息，可用于研究政府与经济主体以及各类经济主体之间的情绪传染机制，因此在研究经济主体心理因素时，不仅要注意经济主体异质性的影响，还要考虑经济主体社会性的影响，重视宏观经济心理因素的微观行为基础。</p>
<br>
<h4 id="513-经济主体心理因素的测度误差问题">5.1.3 经济主体心理因素的测度误差问题</h4>
<p>从非结构化大数据提取信息，主要依靠人工智能与机器学习技术，如分析文本数据的自然语言处理技术， 分析图像与视频数据的计算机视觉技术，以及分析音频数据的计算机语音识别技术等。 <strong>从统计学角度看，这些非结构化数据分析技术，本质上都是统计模型，这些模型大多是误设模型，因此所构建的心理变量存在不同程度的测度误差</strong>，虽然这些测度误差随着人工智能与机器学习技术的进步而不断减少。 目前实证研究在使用文本数据测度经济主体情绪时，<strong>大多使用词典法，通过关键词频数测度心理变量，这种方法显然没有将词与词的关系考虑在内，这将不可避免产生信息遗漏与测度误差</strong>。 从计量经济学视角看，心理变量测度误差的存在， 对计量经济学推断将造成一定影响。 例如，当解释变量包含测度误差时，普通最小二乘法估计将产生偏差，需要使用工具变量法（如二阶段最小二乘法）才能获得一致估计。 <strong>事实上，基于自然语言处理技术构建的心理变量的测度误差，并不满足统计意义上的独立同分布假设，可能还包含着重要的心理遗漏信息</strong>。 但是，目前基于文本数据的实证研究都没有考虑这些问题及其产生的影响。</p>
<p>利用文本数据特别是中文文本数据进行定量分析的技术难度较高。 英文文本数据的信息提取在现有研究文献中已有标准方法，其经济类管理类词典也已比较完善。 常用英文词典有哈佛大学通用调查词典、亨利词典（亨利，2008）、文辞乐观与悲观词汇词典和洛克伦-麦克唐纳词典（洛克伦、麦克唐纳，2011）等（唐国豪等， 2016）。 不同英文词典的应用范围和功能有所差异，例如哈佛大学通用调查词典（第四版）主要涉及心理学与社会学领域的正面和负面词汇，亨利词典是第一本金融文本词典，但其词汇量较为匮乏，而洛克伦-麦克唐纳 词典涵盖的金融情绪词汇较为全面且准确。 由于中英文语言结构差异，中文文本数据需要相对复杂的处理方 法。 中文文本数据分析刚起步不久，在经济学、管理学领域还缺乏比较全面、完善的词典。 中文文本数据的分词位置会影响对文本真实含义的理解，一些关键词的词性（如名词与动词）随着上下文语境的变化而变化，一些常用词的语义还会随着时代变化而产生截然不同的理解，特别是在互联网时代，中文语言进化速度加快，完 全相同的词汇，其含义可能在短短几年内便发生巨大变化，大量新的网络语言不断涌现，具有强烈的时代特征，无法按照常规中文语言含义进行分析。 此外，不同的中文文本来源在叙述同一事情或现象时经常采用不 同的词汇与叙事方式，使用同一词典可能会产生严重的测度偏差。 所有这些差异与困难，意味着不能照搬照抄英文文本数据分析方法，必须根据中文文本结构特点，开发适合中文文本数据的分析方法与技术。 <strong>如何从海量中文文本数据中提取经济主体心理因素等信息是中国经济学一个重要的学术前沿问题</strong>。 在这方面，已取得一定进展。 王靖一和黄益平（2018）、姚加权等（2021）、姜富伟等（2021b）、范小云等（2022）等开发了中文文 本情绪词典。 针对词典法与传统机器学习方法没有考虑语言结构、没有关注文档整体信息等缺陷，范等 （2022）提出因子增强正则化预测模型，这个方法考虑了词汇之间的逻辑关系，能够提取文本的隐藏主题，从而弥补现有研究的不足。 也有文献引入自然语言前沿处理方法提升数据信息分析质量，例如黄等（2022a）拓展目前自然语言处理领域流行的 BERT 模型，构建能充分利用财务文本上下文信息的 FinBERT 模型，并证明其在情绪分类方面显著优于洛克伦-麦克唐纳词典以及其他传统机器学习技术。</p>
<br>
<h3 id="52-构建非结构化大数据实证研">5.2 构建非结构化大数据实证研</h3>
<h4 id="521-充分利用各种类型的非结构化经济大数据">5.2.1 充分利用各种类型的非结构化经济大数据</h4>
<p><strong>目前有关经济主体心理因素的研究主要基于文本数据，相对较少使用其他非结构化数据，例如图像、音频、视频数据等</strong>。 事实上，其他非结构化大数据也包含丰富的关于各类经济主体的心理信息，例如人们常说 “一图胜千言”，包括社交网络平台上各种常见的“表情包”。 随着计算机视觉与人工视觉网络等机器学习技术 的发展，已有政治学学者从图像数据中挖掘经济主体信息。 例如，王等（2016）利用推特<strong>个人资料照片</strong>分析了美国总统选举中民主党与共和党总统候选人支持者的人口构成。 查克拉波蒂（2017）使用上述方法分析美国社会与政治活动发起人的特征。 在经济学领域，阿罗米和克莱门茨（2021）基于政策制定者、交易员、公司经理 等在媒体上发布的照片构建他们的面部表情指数与情绪指数。 奥贝德和普图通（2022）采用卷积神经网络方法对《华尔街日报》照片进行分类，依据每日新闻照片传递的悲观情绪图像所占比例，构建了投资者情绪指数。 <strong>图像信息挖掘技术在管理学中也有应用</strong>（布兰克斯普尔等，2017）。 目前，基于音频与视频数据的实证研 究大多集中在技术层面，政策应用还不多见。 例如，埃德曼斯等（2022）根据音乐平台的热门音乐所传递的正面情感测度投资者情绪。 库雷西和阿格沃尔（2022）基于视频数据提出一种利用卷积神经网络与深度学习技术测度经济主体情绪的方法。 常和彭（2022）基于抖音国际版产生的流行短视频，收集了每日 500 个最受欢迎短视频数据，并应用深度视觉音频注意力网络技术构建了各个国家的股票市场情绪指数。 <strong>上述研究仅使用单模态数据，即对非结构化大数据进行单独分析，没有考虑融合文本、图像、音频或视频等多模态数据蕴含的更 为丰富信息</strong>。 例如，经济主体在微博等社交媒体传达情绪时，往往会通过文字搭配图片方式发表评论；视频数据包含文字、音频和图像等各类非结构化数据。 分析多模态数据的难点在于如何融合不同类型非结构化大数据，巴尔特鲁沙炎等（2018）对多模态数据融合（特征融合、决策融合和混合融合）进行综述，为使用多模态数据 测度经济主体心理因素提供参考。 <strong>除图像、音频、视频等非结构化大数据外，一些学者利用卫星遥感技术测度夜间灯光、天气、地形、农业以 及城市用地、种植作物、建筑类型与自然资源等图像，并应用于经济学分析</strong>（唐纳森、斯托里加德，2016）。 作为 一种最有代表性的遥感数据，夜间灯光数据能够比较客观反映人类社会生产生活状况，一些学者因此将夜间 灯光数据作为地方经济活动的代理变量，使用夜间灯光数据估计、调整与修正国民生产总值（亨德森等， 2012），或研究各种经济问题（王贤彬、黄亮雄，2018；吉布森等，2020）。 李等（2022）使用海事卫星遥感数据测度公海上国际贸易货船二氧化碳排放量，填补了全球碳排放测度研究的一个空白。 陈等（2022）利用城际运输卡车物流遥感数据定量评估上海新冠肺炎疫情管控政策对上海乃至长三角地区经济的影响。 毫无疑问，各种非结构化大数据包含很多结构化数据所没有的信息，这些信息远没有充分挖掘出来，需要整合、集成并综合利用各种不同类型的非结构化大数据。</p>
<br>
<h4 id="522-系统构建政府政策数据库与各类经济主体心理数据库">5.2.2 系统构建政府政策数据库与各类经济主体心理数据库</h4>
<p>与结构化大数据相比，非结构化大数据包含更丰富的各级政府政策变化、各种重要事件冲击以及各类经济主体对政府政策变化与重要事件的反应等信息，因此<strong>在研究政府与市场关系时，必须重视非结构化经济大数据库建设。 非结构化大数据种类繁多、结构复杂、来源不同，收集、清洗、处理、整合、分析各种非结构化大数据，并将其转化为结构化数据，是一项非常艰巨但又非常重要的基础设施建设工程</strong>。 非结构化大数据是高维或超高维数据，将非结构化大数据转化为结构化数据，特别是构建经济政策变量与经济主体心理变量的结构化数据，本质上是一个降维问题。 应该加强与计算机科学和<strong>计量语言学</strong>的交叉融合，将这些学科用于非结构化数据分析的方法与工具，如自然语言处理技术、计算机视觉技术、计算机语言技术、计量语言学模型等，应用并普及到经济学实证研究中，以加快构建系统性、结构化政府政策数据库与经济主体心理数据库。</p>
<p>除各种非结构化大数据外，还应重视使用其他方式与渠道，例如采用统计调查方法搜集与整理经济主体心理数据。 例如，美国统计学会与美国国家经济研究局于 1968 年开创的，1990 年由美联储费城分行接手的 “<strong>美国专业预测者调查</strong>”，按季度发布专业预测者对美国主要宏观经济变量的预测结果。 里科等（2016）根据该 调查数据，通过考察不同个体对美国政府支出的预测差异来测度未来政府支出变化不确定性。 从 20 世纪 40 年代开始，美国密歇根大学调查中心基于对 500~600 名成年人的原始调查数据，编制并于每月第 10 天发布“<strong>密歇根消费者信心指数</strong>”，反映美国消费者对当前以及未来经济的信心强弱，成为预测美国经济走势与消费趋向的一个先行指标。 美国管理与组织实践调查是美国第一次大规模管理实践调查，涵盖超过 1 万家公司的 3 万 个工厂，调查企业管理层对当年与未来一年销售量的主观概率分布预测。 布卢姆等（2022）基于该调查数据， 测度企业管理层真实感受到的销售不确定性，即<strong>企业主观不确定性</strong>。 在中国，国家统计局编制的<strong>中国消费者信心指数与企业家信心指数</strong>，通过对城市消费者、法人企业以及依照法人单位进行统计的产业活动单位负责人进行问卷调查，以指数形式综合反映经济主体对宏观经济环境的主观感受与信心。 此外，中国国家统计局 中国经济景气监测中心的“<strong>中国百名经济学家信心调查</strong>”，中国人民银行的未来物价预期指数、未来就业预期 指数与企业家信心，中国银联的银行卡消费信心指数等，都是通过统计调查获得的中国经济主体预期数据。</p>
<br>
<h4 id="523-大力发展非结构化大数据计量经济学">5.2.3 大力发展非结构化大数据计量经济学</h4>
<p>除政府政策数据库与经济主体心理数据库建设外，非结构化大数据的分析、变量构建、模型设定、估计、推断 与预测等具有较高技术门槛，需要大力发展非结构化大数据计量经济学，系统性形成非结构化大数据信息提取、建模、估计、检验、预测的计量经济学理论、方法与工具。 大数据与人工智能特别是机器学习的发展给计量经济学带来了许多机遇与挑战（瓦里安，2014；洪永淼、汪寿阳，2021a，2021b）。 <strong>近年来，结构化大数据计量经济学理论与方法创新取得了巨大进展，包括高频与时变时间序列建模与预测、实时预测、高维计量经济学建模、新型结 构化数据（如函数数据、区间数据、矩阵数据、符号数据等）建模，以及基于机器学习的因果推断（阿西，2019）等</strong>。 相比之下，非结构化大数据计量经济学理论与方法创新则处于起步阶段。 一方面，计算机科学关于各种非结构化大数据的分析技术突飞猛进，但这些先进技术尚未广泛应用于分析非结构化经济大数据，在经济学领域存在明显的“应用时滞”。 另一方面，除构建非结构化大数据库外，还需要发展适合于分析非结构化大数据的计量经济学理论与方法，并与计算机科学和计量语言学关于非结构化大数据的分析方法与技术有机融合起来。</p>
<p>毫无疑问，在非结构化大数据被转化为结构化数据之后，很多现有计量经济学理论与方法可发挥重要作用。 但是，<strong>基于非结构化大数据构建而成的结构化数据，需要深入研究其经济含义与统计性质。 例如，基于非结构化大数据构建而成的经济主体情绪变量时间序列数据，是否为非平稳随机过程？有无存在结构性变化？ 是具有结构突变还是平稳结构变化？是短记忆过程还是长记忆过程？研究这些问题对理解情绪指数的心理学、经济学含义及其计量经济学建模与推断具有重要意义</strong>。 例如，如果所构建的时间序列情绪变量是计量经 济学意义上的非平稳一阶单整 I（1）过程，则可能会出现时间序列计量经济学中所谓的伪回归现象；而如果时 间序列情绪变量是一个局部平稳过程，则需要发展新的计量经济学建模、估计、推断与预测理论与方法。 王等 （2022）使用具有时变波动性的随机游走模型刻画中国股市投资者情绪指数，并发现投资者情绪对股票定价没有长期影响。 也有研究发现，美国投资者情绪指数具有显著的结构突变特征（姚，2022）。 此外，由于经济主体情绪是不可观测的潜在变量，因此计量经济学的潜在因子模型，包括高维时变因子模型，适合于对经济主体情绪进行建模。 新型数据需要新的分析方法与工具。 随着非结构化数据可获得性的提升与广泛使用，迫切需要创新非结构化大数据计量经济学理论与方法。</p>
<br>
<h4 id="524-促进跨学科系统性交叉研究">5.2.4 促进跨学科系统性交叉研究</h4>
<p>基于非结构化大数据的经济主体心理变量测度与建模涉及多学科理论与方法，包括经济学、心理学、社会学、社会心理学、语言学、认知科学、计量经济学、统计学、数据科学、计算机科学、计量语言学、复杂性科学等。 其中，</p>
<ul>
<li><strong>心理学、社会心理学、认知科学与语言学有助于理解经济主体心理因素及其变化规律</strong>。</li>
<li><strong>计量经济学、统计 学、数据科学、计算机科学、计量语言学以及复杂性科学能够为提取非结构化大数据中的心理信息提供方法论指导与技术支撑</strong>。</li>
<li><strong>经济学、心理学、社会学、社会心理学则为经济主体心理变量的经济学解释提供理论指导</strong>。</li>
</ul>
<p>应打破学科壁垒，促进不同学科的交叉融合，推动经济学与其他社会科学（包括心理学、社会学、社会心理 学、政治学、法学、历史学、文化学等）的跨学科系统性研究。</p>
<p><br><br></p>
<h2 id="六结论">六、结论</h2>
<p><strong>在研究政府与市场关系过程中，需要重视各类经济主体心理因素，如预期、信念、信心、情绪、情感等。 这些心理因素的产生、形成与变化有其客观原因。 由于其“反身性”特征，经济主体心理因素对政府政策传导机制有重要影响。 经济主体通过学习、领会、解读政策含义，形成对政策的预期，然后根据自身利益最大化原则做出最优决策，从而影响宏观经济运行。 因此，测度经济主体心理因素及其影响是研究政府政策传递机制的一个重要思路</strong>。</p>
<p><strong>如何科学测度经济主体心理因素在方法论上面临巨大困难与挑战</strong>。 虽然现代心理学与实验经济学为定量分析心理因素提供了一些分析工具，例如观察访谈、统计调查以及实验方法等，但其研究的深度、广度与抽样频率受到各种限制与约束。 人工智能技术的发展与非结构化大数据可获得性的提升，为测度经济主体心理因素及其影响，提供了一个可行方法。 <strong>非结构化大数据包含大量关于政府政策变化（如语调变化）、重要事件冲击以及经济主体对政策变化与重要事件冲击的反应等信息，特别是语言是人类表达思想与情感，进行沟通交流的最主要工具，可从文本等数据中提取、测度各类经济主体心理因素及其影响</strong>。 这样，原来的定性分析可转变为严谨的定量实证分析，从而打破经济学中定性分析与定量分析的界限，并使经济学与其他社会科学跨学科系统性研究成为可能。 由于中国巨大的人口规模与经济规模，新的信息技术及其应用层出不穷，数字经济发展具有巨大潜力与规模优势，中国即将成为全球最大的数据生产国。 大数据资源中，相当大一部分是非结构化大数据，这些数据包含许多传统结构化数据无法反映的重要信息，特别是各种政府政策变化与重要事件冲击、以及各类经济主体对政策变化与重要事件冲击的反应等信息。 中国庞大的数据资源与政府的重要作用，使中国拥有全球最大且独一无二的“政策数据库”这一富矿，在研究政府与市场关系方面具有天然优势。 <strong>我们应该积极推动学科交 叉，大胆借鉴心理学、社会心理学、认知科学、计算机科学、计量语言学以及复杂性科学等理论与方法，发展新的研究方法与工具，形成新的研究范式，构建具有深厚学理基础关于政府与市场关系的原创性自主经济理论， 以指导中国式现代化建设与全球化实践</strong> ① 。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>语言相对性论 | 语言是否决定/影响人的思维和认知</title>
      <link>https://textdata.cn/blog/2023-04-07-sapir-whorf-hypothesis/</link>
      <pubDate>Fri, 07 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-04-07-sapir-whorf-hypothesis/</guid>
      <description>语言相对性论 (Linguisitic Relativity)也叫萨丕尔-沃尔夫假说 (Whorfian Hypothesis)， 该假说认为不同的语言能否决定母语者的思维。语言不仅反映了我们对世界的认知，还会影响我们对世界的感知和思考方式。对于用某个单词表示的某个事物，使用不同语言的人会有不同的理解，而这种看待现实世界的方式之所以不同，证实由于语言本身的差异。 与该假说形成息息相关的学者依次有洪堡特、鲍阿斯、萨丕尔、沃尔夫等。</description>
      <content:encoded><![CDATA[<h2 id="语言相对性论">语言相对性论</h2>
<p><strong>语言相对性论</strong> (Linguisitic Relativity)也叫萨丕尔-沃尔夫假说 (Whorfian Hypothesis)， 该假说认为不同的语言能否决定母语者的思维。语言不仅反映了我们对世界的认知，还会影响我们对世界的感知和思考方式。对于用某个单词表示的某个事物，使用不同语言的人会有不同的理解，而这种看待现实世界的方式之所以不同，证实由于语言本身的差异。 与该假说形成息息相关的学者依次有: 洪堡特、鲍阿斯、萨丕尔、沃尔夫等。</p>
<p>最早提出该猜想的是十九世纪德国思想家威廉·冯·洪堡特， 在《<strong>论人类语言结构的差异及对人类精神发展的影响</strong>》中提出，<strong>语言之间在语法结构上的差异很可能会影响到以这种语言为母语的人的思维方式，而某个群体在社会生活中所强调的方面也会通过语言的结构体现出来，语言结构和人的精神发展之间有着互相强化的作用</strong>。他主要研究印欧语系各种语言，以及一些印欧语系的语言，讨论屈折语对人类思维的影响。</p>
<p>之后，随着语言学和人类学的进一步发展，这一领域研究的前沿阵地转移到了美国， 由美国语言学家和人类学家爱德华·萨丕尔、他的老师弗兰茨·鲍阿斯以及他的学生本杰明·沃尔夫进行更深刻的讨论。鲍阿斯不是萨丕尔-沃尔夫假说的真正创始人，但他对该假说的贡献不可忽视。<strong>鲍阿斯首先把人类学划分为四个领域: 考古学、文化人咧学、物质人类学、语言学</strong>。他还是人类学研究中语言相对论的坚定倡导者。相对论为萨丕尔、沃尔夫及美国其他描写注意者提供了语言学研究的方法论，它为沃尔夫假说提供了坚定的方法表达基础。</p>
<p>萨丕尔认为，因为两种语言之间的完美的对译是不存在的，所以语言之间必然存在差异，表达现实的方式也必然有差异，所以不同群体所生活的“现实”其实是不一样的。他认为语言的不同仅仅是表达共同经验(经历)的表达模式不同，而不是经验本身的不同。语言的差异不是各自分类体内容的不同，而是各自系统的、形式的排列不同；</p>
<p><strong>沃尔夫则一直试图用不同语言中表达某一概念的词汇的多寡来证明不同母语的人对世界的认知是不同的。由于萨丕尔和沃尔夫师徒二人在这方面的讨论相对比较集中，特别是沃尔夫句出来的一些例子有很大的影响范围，所以后来很多学者把这种“语言决定思维”的理论假说称之为“萨丕尔-沃尔夫假说”(Sapir-Whorf Hypothesis)</strong> 。 围绕着萨丕尔-沃尔夫假说，学界其实有强假设、弱假设两种版本的讨论和争议，至今仍未有定论。</p>
<br>
<h2 id="强假设">强假设</h2>
<p>一般在提到萨丕尔-沃尔夫假说的时候， 我们会在之下区分两个强弱力量不同的假说。强假说认为，<strong>语言完全限制住了思维发展的可能， 一旦我们接受了母语之后，整个大脑就被语言的结构、词汇、含义表达方式进行了塑形，人的思维很难再改变</strong> 。</p>
<p>沃尔夫在爱斯基摩人能用很多不同的名称来描述雪（迎面过来的雪、漂浮的雪、可以用来建雪屋的雪&hellip;&hellip;）这些词汇是的他们能够“看到”的&ndash;活血说可以辨识的&ndash;雪的种类，远远超过我们这些生活在温带的人。基于同样的道理，菲律宾群岛上的哈诺人，其语言中拥有92种描述大米的不同类别和状态的词。一些研究发现，不同语言对颜色、时间和空间等概念的表达方式影响了说这种语言的人在这些领域的认知方式 。例如，某些语言对颜色的划分不同，可能导致说这种语言的人在颜色辨别上的表现也有所不同（Roberson, D., &amp; Hanley, J. R., 2015）。</p>
<blockquote>
<p>这里需要注意，从沃尔夫最初提到的4种， 后来发生了以讹传讹， 雪变成十几种，几十种雪。。。。</p>
</blockquote>
<p><strong>沃尔夫假设认为：外在世界的真实，通过某些内部表征过程，被转译成某种符合长期认知结构的知觉。而信息在大脑中被结构化的途径之一，显然与我们每个人发展出的特定的语言编码有关。随着语言的不同， 对真实世界的语言编码也会不同</strong>。</p>
<p><strong>某种经验对我们越重要，则我们的语言基于表达这种经验的方式也就越多&ndash;换言之，语言决定了我们的感知。特定语言编码的发生发展，有赖于特定的文化需要</strong>；某个语言群体中的成员对改语言汇中特定编码的学习，其实也正是对该种文化中重要价值的学习过程，而这些重要的文化价值中的一部分则可能与种族的生存密切相关。语言编码发展的结果，还可能进一步决定何种信息得到编码、转换和记忆。</p>
<br>
<h2 id="弱假设">弱假设</h2>
<p>弱假设则认为语言中的结构和我们平时习以为常的语言用法，可能会对我们的一些思维方式和日常行为产生一定的影响，就想我们平常经常使用的东西通话的隐喻，比如“人生就是一场旅程(life is journey)”， 那么在实际生活中， 我们就真的会把人生的不同阶段当成旅途中的不同驿站来加以认知。</p>
<br>
<h2 id="总结">总结</h2>
<p>将近200多年后的今天， 当年洪堡特提出的“语言结构决定人类思维”的理论，也就是“萨丕尔-沃尔夫”强假设，已经基本被否定了。 名词的性别并不能决定着名词所代表的事物真的拥有性别，语言里缺失左和右的词汇并不影响代表着母语者就无法区分左右、进行旋转。</p>
<p>但是，<strong>萨丕尔-沃尔夫弱假设，也就是“语言使用影响人类认知”这一点，有一定的合理性，假说提出半个世界以来，仍有语言学、心理学等领域研究从不同角度开展研究，结果多数也支持了弱假设。</strong>  最经典的实验当属相对位置的表述和颜色的辨认，母语中强调 “东西南北”的人更倾向于使用绝对方向描述，而母语里经常使用相对方向的人则会以“前后左右”作为描述的基准； 人们在为颜色命名的时候， 往往会对母语里有相应单词的颜色做出更快的判断，而蓝和绿的接线是最难判断的，因为在很多语言里，他们会用同一个单词泛指这两种颜色。这么看来，母语的词汇上、结构上存在的区别，的确会在某方面影响人类的认知。</p>
<p>还有一些研究发现，不同语言对颜色、时间和空间等概念的表达方式影响了说这种语言的人在这些领域的认知方式。例如，Roberson（2010）某些语言对颜色的划分不同，可能导致说这种语言的人在颜色辨别上的表现也有所不同。斯坦福大学心理系的Boroditsky团队， 进行了一系列英语与汉语母语者的对比研究， 例如，她认为英语的时间范畴是水平的，而汉语的时间范畴是垂直的(比如 “接下来”，“下一个界面”)， 因此她设计了实验，将两个时间点分别以垂直和水平的方式呈现给被试，让被试判断哪个时间点更早。结果发现，当两个时间点是以垂直位置呈现(上下)， 汉语母语者的反应时间更短，反应正确率更高；当两个时间点以水平位置呈现(左右)， 英语母语者反应时间更短， 反应正确率更高。这些实证研究都从自然科学的角度对语言和思维的关系进行了验证。</p>
<br>
<h2 id="参考文献">参考文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Athanasopoulos, Panos, and Aina Casaponsa. &#34;The Whorfian brain: Neuroscientific approaches to linguistic relativity.&#34; Cognitive Neuropsychology 37, no. 5-6 (2020): 393-412.
Boroditsky, Lera. &#34;Does language shape thought?: Mandarin and English speakers&#39; conceptions of time.&#34; Cognitive psychology 43, no. 1 (2001): 1-22.
Roberson, Debi, and J. Richard Hanley. &#34;An account of the relationship between language and thought in the color domain.&#34; Words and the Mind (2010): 183.
</code></pre></div><br>
## 广而告之
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>可视化 | 睡前消息的科学社会、科学技术、社会化抚养话题可视化</title>
      <link>https://textdata.cn/blog/2023-03-22-bedtime-topic_model_visualization/</link>
      <pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-22-bedtime-topic_model_visualization/</guid>
      <description>睡前消息是我最喜欢看的节目， 基本上隔两天不看睡不踏实。本次分享，不涉及观点之争，纯属技术玩乐。</description>
      <content:encoded><![CDATA[<p>我的信仰是科学的唯物史观，虽然一直觉得爷爷是伟人，但是没有系统钻研小本本，所以似懂非懂，好在有睡前消息这个节目，可以时不时的聆听到半个世纪前的伟人智慧。</p>
<p><img loading="lazy" src="img/df4.png" alt=""  />
</p>
<p><img loading="lazy" src="img/%e7%a7%91%e5%ad%a6%e7%a4%be%e4%bc%9a.png" alt=""  />
</p>
<br>
<h2 id="一读取数据">一、读取数据</h2>
<p>睡前消息是我最喜欢看的节目， 基本上隔两天不看睡不踏实。本次分享，不涉及观点之争，纯属技术玩乐。</p>
<p><a href="https://textdata.cn/blog/2023-03-06-bedtime-news-datasets/">数据集 | 马前卒工作室睡前消息文稿汇总</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="n">bed_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;bedtime_news.csv&#39;</span><span class="p">,</span> <span class="n">converters</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">})</span>
<span class="n">bed_df</span><span class="o">.</span><span class="n">date</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">bed_df</span><span class="o">.</span><span class="n">date</span><span class="p">)</span>
<span class="n">bed_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<Br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#数据集起始日期</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">bed_df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">())[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">bed_df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">())[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2019-07-12
2022-11-29
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#数据集含有的节目数</span>
<span class="nb">len</span><span class="p">(</span><span class="n">bed_df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">522
</code></pre></div><br>
<br>
<h2 id="二准备工作">二、准备工作</h2>
<h3 id="21-自定义词典">2.1 自定义词典</h3>
<ul>
<li>科学、技术</li>
<li>社会化抚养</li>
<li>债务、独山县</li>
<li>中医、以岭药业</li>
</ul>
<p>将感兴趣的词加入到jieba自定义词典中，防止被错分。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">diywords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;科学&#39;</span><span class="p">,</span> <span class="s1">&#39;技术&#39;</span><span class="p">,</span> <span class="s1">&#39;社会化抚养&#39;</span><span class="p">,</span> <span class="s1">&#39;债务&#39;</span><span class="p">,</span> <span class="s1">&#39;独山县&#39;</span><span class="p">,</span> <span class="s1">&#39;毛选&#39;</span><span class="p">,</span> <span class="s1">&#39;唯物&#39;</span><span class="p">,</span> <span class="s1">&#39;社会实验&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">diywords</span><span class="p">:</span>
    <span class="n">jieba</span><span class="o">.</span><span class="n">add_word</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="22-word_in_context">2.2 word_in_context</h3>
<p>在这里定义了一个word_in_context函数，可以查看某些关键词上下文环境。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>


<span class="k">def</span> <span class="nf">word_in_context</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">lang</span><span class="o">==</span><span class="s1">&#39;chinese&#39;</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&#34;你应该安装nltk和对应的nltk_data, 请看B站https://www.bilibili.com/video/BV14A411i7DB&#34;</span><span class="p">)</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">]</span>
    <span class="n">kw_idxss</span> <span class="o">=</span> <span class="p">[[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">keyword</span><span class="p">]</span> <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">]</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">keyword</span><span class="p">,</span> <span class="n">kw_idxs</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">keywords</span><span class="p">,</span> <span class="n">kw_idxss</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">kw_idxs</span><span class="p">:</span>
            <span class="n">half</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span> <span class="o">-</span> <span class="n">half</span><span class="p">)</span>
            <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">half</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">row</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;keyword&#39;</span><span class="p">:</span> <span class="n">keyword</span><span class="p">,</span> 
                   <span class="s1">&#39;context&#39;</span><span class="p">:</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">start</span><span class="p">:</span> <span class="n">end</span><span class="p">])</span> <span class="k">if</span> <span class="n">lang</span><span class="o">==</span><span class="s1">&#39;chinese&#39;</span> <span class="k">else</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">start</span><span class="p">:</span> <span class="n">end</span><span class="p">])</span>
                      <span class="p">}</span>
            <span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>


<span class="c1">#测试【打算】前后上下文5个单词</span>
<span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s1">&#39;根本没打算过真抓到人，当然也不打算付钱。转发推送：还有一个话题&#39;</span><span class="p">,</span> 
                <span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;打算&#39;</span><span class="p">],</span> 
                <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> 
                <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="23-词云图">2.3 词云图</h3>
<p>pip install pyecharts==2.0.1</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">plot_wordcloud</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">title</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
    <span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;context&#39;</span><span class="p">])</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]+&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">wordfreqs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
        <span class="n">freq</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="n">wordfreqs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">word</span><span class="p">,</span> <span class="n">freq</span><span class="p">))</span>
    <span class="n">wordfreqs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">wordfreqs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">wordfreqs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">f</span><span class="p">))</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">f</span> <span class="ow">in</span> <span class="n">wordfreqs</span><span class="p">]</span>
    <span class="n">cloud</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
    <span class="n">cloud</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">wordfreqs</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
    <span class="n">cloud</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span><span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> 
                                                    <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">23</span><span class="p">)),</span>
                          <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">cloud</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">.html&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">cloud</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div><p><br><br></p>
<h2 id="三话题分析">三、话题分析</h2>
<p>相比LDA机器学习算法的晦涩难懂，其实可以用word_in_context对指定关键词进行定位和分析，数据处理的过程清晰透明。</p>
<h3 id="31-topic-社会化抚养">3.1 Topic-社会化抚养</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dfs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">bed_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> 
                            <span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;社会化抚养&#39;</span><span class="p">],</span> 
                            <span class="n">window</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span>
    <span class="n">dfs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    
<span class="n">topic_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dfs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">topic_df</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#涉及该主题的节目数</span>
<span class="nb">len</span><span class="p">(</span><span class="n">topic_df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">65
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#占比</span>
<span class="nb">len</span><span class="p">(</span><span class="n">topic_df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">bed_df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.12452107279693486
</code></pre></div><br>
<p>在咱们这个数据集中，睡前消息500多期节目中，有65期谈及社会化抚养的，比例12%。</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">plot_wordcloud</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">topic_df</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;社会化抚养&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/%e7%a4%be%e4%bc%9a%e5%8c%96%e6%8a%9a%e5%85%bb.png" alt=""  />
</p>
<br>
<h3 id="32-topic-科学技术">3.2 Topic-科学技术</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dfs2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">bed_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> 
                            <span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;科学&#39;</span><span class="p">,</span> <span class="s1">&#39;技术&#39;</span><span class="p">],</span> 
                            <span class="n">window</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span>
    <span class="n">dfs2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    
<span class="n">topic_df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dfs2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">topic_df2</span>
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#涉及该主题的节目数</span>
<span class="nb">len</span><span class="p">(</span><span class="n">topic_df2</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">411
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#占比</span>
<span class="nb">len</span><span class="p">(</span><span class="n">topic_df2</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">bed_df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.7873563218390804
</code></pre></div><br>
<p>在咱们这个数据集中，睡前消息500多期节目中，有411期谈及社会化抚养的，比例79%。</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">plot_wordcloud</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">topic_df</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;科学技术&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/%e7%a7%91%e5%ad%a6%e6%8a%80%e6%9c%af.png" alt=""  />
</p>
<br>
<h2 id="33-科学社会">3.3 科学社会</h2>
<p>面对社会问题，睡前消息倡导科学社会实验， 也喜欢讲毛选语录， 两者所涉及的是唯物的观点，科学社会的观点。放在一起试试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dfs3</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">bed_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> 
                            <span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;毛选&#39;</span><span class="p">,</span> <span class="s1">&#39;唯物&#39;</span><span class="p">,</span> <span class="s1">&#39;社会实验&#39;</span><span class="p">],</span> 
                            <span class="n">window</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span>
    <span class="n">dfs3</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    
<span class="n">topic_df3</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dfs3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">topic_df3</span>
</code></pre></div><p><img loading="lazy" src="img/df4.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#涉及该主题的节目数</span>
<span class="nb">len</span><span class="p">(</span><span class="n">topic_df3</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">14
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">topic_df3</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">bed_df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.02681992337164751
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">plot_wordcloud</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">topic_df3</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;科学社会&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/%e7%a7%91%e5%ad%a6%e7%a4%be%e4%bc%9a.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Nature | 通用中英文六维语义情感词典</title>
      <link>https://textdata.cn/blog/2023-03-20-nature-six-semantic-dimension-database/</link>
      <pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-20-nature-six-semantic-dimension-database/</guid>
      <description>来自心理学和认知神经科学的证据表明，人类大脑的语义系统包含几个特定的子系统，每个子系统都代表语义信息的特定维度。对这些不同语义维度上的词语评分可以帮助研究语义维度对语言处理的行为和神经影响，并根据人类认知系统的语义空间建立语言含义的计算表示。现有的语义评分数据库提供了数百到数千个词语的评分，但这无法支持对自然文本或语音的全面语义分析。本文报告了一个大型数据库——六维语义数据库（SSDD， 后文「数据库」均用「词典」代替），其中包含对 17,940个常用汉语词语在六个主要语义维度上的主观评分：视觉、运动、社交、情感、时间和空间。此外，使用计算模型学习主观评分和词嵌入之间的映射关系，我们在SSDD中包括了1,427,992个汉语和1,515,633个英语词语的估计语义评分。SSDD将有助于自然语言处理、文本分析和大脑中的语义表示研究。</description>
      <content:encoded><![CDATA[<h2 id="应用价值">应用价值</h2>
<p>对于大量散落在网络中的文本数据， 可以度量用户在视觉、运动、社交、情感、时间和空间等维度上心理、认知、抽象层面的信息。</p>
<br>
<p>Wang, S., Zhang, Y., Shi, W. et al. A large dataset of semantic ratings and its computational extension. Sci Data 10, 106 (2023). <a href="https://doi.org/10.1038/s41597-023-01995-6">https://doi.org/10.1038/s41597-023-01995-6</a></p>
<p><img loading="lazy" src="img/cover.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="一摘要">一、摘要</h2>
<p>来自心理学和认知神经科学的证据表明，人类大脑的语义系统包含几个特定的子系统，每个子系统都代表语义信息的特定维度。对这些不同语义维度上的词语评分可以帮助研究语义维度对语言处理的行为和神经影响，并根据人类认知系统的语义空间建立语言含义的计算表示。现有的语义评分数据库提供了数百到数千个词语的评分，但这无法支持对自然文本或语音的全面语义分析。本文报告了一个大型数据库——<strong>六维语义数据库</strong>（SSDD， 后文「数据库」均用「词典」代替），其中包含对 <strong>17,940</strong> 个常用汉语词语在六个主要语义维度上的主观评分：<strong>视觉、运动、社交、情感、时间和空间</strong>。此外，使用计算模型学习主观评分和词嵌入之间的映射关系，我们在SSDD中包括了1,427,992个汉语和1,515,633个英语词语的估计语义评分。SSDD将有助于自然语言处理、文本分析和大脑中的语义表示研究。</p>
<p><br><br></p>
<h2 id="二背景">二、背景</h2>
<p>大量行为和神经证据表明，单词语义表示分布在不同的神经子系统中， 每个子系统代表着特定的语义信息维度。这些语义子系统和维度为人类语义系统的组织提供了重要线索。为了研究单词在人脑中的 <strong>意义表示</strong> 和 <strong>信息加工</strong>, 许多研究基于心理和神经生物学可行的语义维度，通过人员标准构建了单词的词典。与现有NLP领域的embeddings相比， 基于特定意义维度所构建的词典可以在经验语义纬度上提供量化的评分， 使得研究者可以调查语义维度对语言处理的行为和神经影响， 并建立语言含义的(表示)计算。</p>
<p>然而现有的词典只包含数百、数钱个词，不足以支持自然文本或语音的全面语义分析。该研究提供了一个大型的语义评分语义词典， 名为<strong>六维语义</strong>（Six Semantic Dimension Database，SSDD）词典， <strong>每个中英文词语含六个维度的得分(可以理解为效价valence),分别是视觉、运动、社交、情感、时间、空间</strong>。 其中视觉和运动维度反映了感觉运动体验对语义表示的影响。感觉和运动维度可能是最常研究的语义维度之一，它们对于对象和动作概念的重要性已经得到了很好的确认。在与语义表示相关的多个感官维度中，我们选择了视觉维度，因为视觉是主导的感觉模态。视觉和运动语义对认知处理的行为和神经影响已经被许多研究证实。社交和情感维度反映了社会情感体验对语义表示的影响。<strong>这些维度具有可分离的神经相关性，并且对于心理和抽象概念的表示尤其重要</strong>。Huth等人采用数据驱动方法研究了大脑中的语义表示组织，并发现社交情感和感官运动语义与最重要的数据驱动语义维度的两端相关联。因此，社交和情感维度可以作为视觉和运动维度的重要补充，以反映语义表示。时间和空间维度对于事件和情境的表示尤为重要。神经心理学和神经影像研究也表明这些维度具有可分离的神经相关性。Binder等人对经验语义属性进行了全面的综述，反映了六个维度的代表性。Binder等人总结了属于14个领域的65个语义维度，其中超过2/3的维度属于视觉、运动、社交、情感、时间和空间领域。SSDD将这六个领域作为粗粒度语义维度，并为每个维度提供了一般评分。</p>
<p><br><br></p>
<h2 id="三构建方法">三、构建方法</h2>
<h3 id="31-标准者被试人员">3.1 标准者(被试人员)</h3>
<p>该研究找了85位心理、神经都正常的本硕中国学生， 通过数据质量评估，最终保留了80位学生的数据标注结果。</p>
<br>
<h3 id="32-待标注的17940中文词">3.2 待标注的17940中文词</h3>
<p>待标注中文词，一共有17940个， 是由三种数据源筛选得来</p>
<ol>
<li>中文维基百科12814高频词</li>
<li>fMRI领域研究(发表&amp;未发表)的4915个中文词</li>
<li>最后一组项目是来自Binder等人和Tamir等人的语义评分实验中英文刺激词的211个汉语翻译。</li>
</ol>
<br>
<h3 id="33-标注过程">3.3 标注过程</h3>
<ul>
<li>该研究对17980个词进行了6个标注实验的， 每个实验聚焦于一个语义维度（视觉、运动、社交、情感、时间、空间）。</li>
<li>每个标注实验会分成18个session，每个session含1000个词(最后一个session有940个词)</li>
<li>标准过程使用问卷星</li>
<li>情绪维度标注的时候，使用13-point scale标准标注(-6表示非常负面， 0表示中性， 6表示非常积极)</li>
<li>剩下的5维（视觉、运动、社交、时间、空间）使用7-point scale标准标注(1表示非常低， 7表示非常高)</li>
<li>每次标注前， 标注者需要阅读标注指南，指南会含有一些语义例子。</li>
<li>为了控制标准数据质量， 保证每位标注者与所有标准者的相关性大于0.5，最终拒绝了28个session大概0.87%的数据量。</li>
</ul>
<br>
<h3 id="34-扩充词典">3.4 扩充词典</h3>
<p>标准的17940个中文词的六维度数据，可以认为是标准数据。用机器学习方法，想办法扩充词典。</p>
<p>该团队检验了语义上下文不敏感的词嵌入算法(word2vec/Glove)和 对上下文语义敏感的嵌入算法(GPT2, BERT ERNIE, and MacBERT) ，让这6类嵌入模型分别预测， 确定下表现效果较好的Word2vec和MacBERT算法。</p>
<p>使用Word2vec和MacBERT预测剩下所有的中文词，共扩展出1427992个中文词。</p>
<p>人类在视觉、运动、社交、情感、时间、空间六个维度上是共通的，结合语言嵌入模型可以在不同语言中进行语义空间对齐，该研究根据英文嵌入语言模型，也预测出了1,515,633个词。</p>
<br>
<br>
<h2 id="四ssdd">四、SSDD</h2>
<p>SSDD包含两个数据集：</p>
<ul>
<li>第一个是17,940个常用汉语词语在六个语义维度上的主观评分。</li>
<li>第二个是主观评分数据的计算扩展。我们将主观评分与计算模型相结合，然后估算出1,427,992个汉语和1,515,633个英语单词的语义评分。</li>
</ul>
<p>该研究标准、训练的源代码数据均已开源，https://osf.io/n5vke/</p>
<p><img loading="lazy" src="img/osf.png" alt=""  />
</p>
<p>由于数据量太大， 这里只给大家读取并显示17940个标注的6维语义数据, 其实对于经管社科研究， 标注的 <a href="Rated_semantic_dimensions.csv">17940个词</a> 已经是很大的情感词典了。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Rated_semantic_dimensions.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#词汇量</span>
<span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">17940
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 计算相关性矩阵</span>
<span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">corr_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">mark</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">cell</span><span class="p">:</span> <span class="s1">&#39;background-color: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="s1">&#39;red&#39;</span> <span class="k">if</span> <span class="n">cell</span> <span class="o">&gt;</span> <span class="mf">0.4</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
<span class="n">color_matrix</span> <span class="o">=</span> <span class="n">corr_df</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="n">mark</span><span class="p">)</span>
<span class="n">color_matrix</span>
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<p>相关性最高的单元格是Motor与Vision， 6个维度相关性均小于0.5 ， 六维的选择是很合理。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>word_in_context | 查看某类词的上下文，更好的理解文本数据</title>
      <link>https://textdata.cn/blog/2023-03-19-word-in-context/</link>
      <pubDate>Sun, 19 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-19-word-in-context/</guid>
      <description>通过一个单词所处的语境，我们可以了解该单词的含义。**该谚语源于英国语言学家 J.R. Firth 的理论，他认为单词的含义是由其周围的语境和与之相伴的其他单词所决定的，因此我们需要通过单词出现的上下文来理解其含义。这一理论在语言学、自然语言处理等领域有着广泛的应用。之前分享过 [ 使用正则表达式、文本向量化、线性回归算法从md&amp;amp;a数据中计算 「企业融资约束指标」 ]， 使用的是正则表达式识别融资约束文本。但是正则表达式设计十分复杂且有难度，在此之前，如果能够查看某些融资关键词附近上下文， 可帮助研究者更全面地了解数据集中关键词的使用情况和语境，更好的设计正则表达式，亦或许意外找出新的有价值的线索。</description>
      <content:encoded><![CDATA[<p>Firth（1957）有一句名言，理解一个词要从ta身边入手。</p>
<blockquote>
<p>You shall know a word by the company it keeps</p>
</blockquote>
<p>通过一个单词所处的语境，我们可以了解该单词的含义。<strong>该谚语源于英国语言学家 J.R. Firth 的理论，他认为单词的含义是由其周围的语境和与之相伴的其他单词所决定的，因此我们需要通过单词出现的上下文来理解其含义。这一理论在语言学、自然语言处理等领域有着广泛的应用</strong>。之前分享过</p>
<p><img loading="lazy" src="img/39faq-firth_words.png" alt=""  />
</p>
<p>之前分享过 <a href="https://textdata.cn/blog/2023-12-31-using-regex-to-compute-the-financial_constraints"> 使用正则表达式、文本向量化、线性回归算法从md&amp;a数据中计算 「企业融资约束指标」 </a>， 使用的是正则表达式识别融资约束文本。但是正则表达式设计十分复杂且有难度，在此之前，如果能够查看某些融资关键词附近上下文， 可帮助研究者更全面地了解数据集中关键词的使用情况和语境，更好的设计正则表达式，亦或许意外找出新的有价值的线索。</p>
<br>
<h2 id="代码">代码</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="k">def</span> <span class="nf">word_in_context</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    Given text and keywords, the task is to find the text where the keyword appears
</span><span class="s2">    Args:
</span><span class="s2">        text (str): input document, string format
</span><span class="s2">        keywords (list): keywords
</span><span class="s2">        window (int): return the text where the keyword appears, default is 3, meaning return 3 word.
</span><span class="s2">        lang (str, optional): setting the lang, only support chinese and english. Defaults to &#39;chinese&#39;.
</span><span class="s2">
</span><span class="s2">    Returns:
</span><span class="s2">        list contains multiple dictionaries, where each dictionary contains the sentence, keyword, and the sentence where the keyword appears
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="k">if</span> <span class="n">lang</span><span class="o">==</span><span class="s1">&#39;chinese&#39;</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&#34;你应该安装nltk和对应的nltk_data, 请看B站https://www.bilibili.com/video/BV14A411i7DB&#34;</span><span class="p">)</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">]</span>
    <span class="n">kw_idxss</span> <span class="o">=</span> <span class="p">[[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">keyword</span><span class="p">]</span> <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">]</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">keyword</span><span class="p">,</span> <span class="n">kw_idxs</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">keywords</span><span class="p">,</span> <span class="n">kw_idxss</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">kw_idxs</span><span class="p">:</span>
            <span class="n">half</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span> <span class="o">-</span> <span class="n">half</span><span class="p">)</span>
            <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">half</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">row</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;keyword&#39;</span><span class="p">:</span> <span class="n">keyword</span><span class="p">,</span> 
                   <span class="s1">&#39;context&#39;</span><span class="p">:</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">start</span><span class="p">:</span> <span class="n">end</span><span class="p">])</span> <span class="k">if</span> <span class="n">lang</span><span class="o">==</span><span class="s1">&#39;chinese&#39;</span> <span class="k">else</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">start</span><span class="p">:</span> <span class="n">end</span><span class="p">])</span>
                      <span class="p">}</span>
            <span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>


</code></pre></div><p><br><br></p>
<h2 id="练习">练习</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#测试代码，假设zh_text是年报文本，从找找出丝网词相关词的上下文</span>
<span class="n">zh_text</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">【插入一条自家广告】大邓自己家的家，
</span><span class="s2">安平县多隆丝网制品，生产销售不锈钢轧花网、
</span><span class="s2">电焊网、石笼网、刀片刺绳、冲孔网等丝网制品。
</span><span class="s2">联系人 邓颖静 0318-7686899
</span><span class="s2">
</span><span class="s2">人生苦短，我学Python
</span><span class="s2">在社科中，可以用Python做文本分析
</span><span class="s2">Python是一门功能强大的编程语言，广泛应用在经管社科领域。
</span><span class="s2">可以做网络爬虫、文本分析、LDA话题模型、相似度分析等。
</span><span class="s2">
</span><span class="s2">今年经济不景气，形势异常严峻。
</span><span class="s2">由于疫情不景气，静默管理， 产品积压， 公司经营困难。
</span><span class="s2">保就业促就业，任务十分艰巨。
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="c1">#【产品词】上下文</span>
<span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">zh_text</span><span class="p">,</span> 
                <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;石笼&#39;</span><span class="p">],</span> 
                <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><table>
<thead>
<tr>
<th>keyword</th>
<th>context</th>
</tr>
</thead>
<tbody>
<tr>
<td>石笼</td>
<td>电焊网、石笼网、刀片刺绳</td>
</tr>
</tbody>
</table>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#【经营】上下文</span>
<span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">zh_text</span><span class="p">,</span> 
                <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;经营&#39;</span><span class="p">],</span> 
                <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><table>
<thead>
<tr>
<th>keyword</th>
<th>context</th>
</tr>
</thead>
<tbody>
<tr>
<td>经营</td>
<td>&gt;积压， 公司经营困难。\n保</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#【Python】上下文</span>
<span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">zh_text</span><span class="p">,</span> 
                <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;python&#39;</span><span class="p">],</span> 
                <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><table>
<thead>
<tr>
<th>keyword</th>
<th>context</th>
</tr>
</thead>
<tbody>
<tr>
<td>python</td>
<td>人生苦短，我学python\n在社科中</td>
</tr>
<tr>
<td>python</td>
<td>中，可以用python做文本分析\n</td>
</tr>
<tr>
<td>python</td>
<td>做文本分析\npython是一门功能强大的</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析</title>
      <link>https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/</link>
      <pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/</guid>
      <description>&lt;h2 id=&#34;一数据集介绍&#34;&gt;一、数据集介绍&lt;/h2&gt;
&lt;p&gt;2001-2022年A股年报数据集，含 4 个文件，约 11G。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 管理层讨论与分析txt.zip
- 年报txt.zip
- A01-22.csv.gz
- mda01-22.csv.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/a-mda.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;注意&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;zip文件夹是原始数据， 解压后内部为 txt 文件。&lt;/li&gt;
&lt;li&gt;gz文件为汇总数据， 解压后是csv文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二年报数据&#34;&gt;二、年报数据&lt;/h2&gt;
&lt;p&gt;2001-2022年年报数据。数据中只有year、code、text三个字段， 如果想增加诸如公司简称、行业等信息， 可以使用 &lt;a href=&#34;https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/&#34;&gt;&lt;strong&gt;数据集 | A股上市公司基本信息&lt;/strong&gt;&lt;/a&gt;   进行并表。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;anual_report_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;A01-22.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;anual_report_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;年报记录数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;anual_report_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;55856
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;上市公司总数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;anual_report_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;code&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nunique&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;5357
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;三mda数据&#34;&gt;三、MD&amp;amp;A数据&lt;/h2&gt;
&lt;p&gt;2001-2022年MD&amp;amp;A数据， 数据中只有year、code、text三个字段， 如果想增加诸如公司简称、行业等信息， 可以使用 &lt;a href=&#34;https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/&#34;&gt;&lt;strong&gt;数据集 | A股上市公司基本信息&lt;/strong&gt;&lt;/a&gt;   进行并表。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mda_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mda01-22.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mda_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;55439
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;上市公司总数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mda_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;code&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nunique&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;5355
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四相关内容&#34;&gt;四、相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-01-21-hk-stock-market-anual-report/&#34;&gt;&lt;strong&gt;数据集 | 港股年报文本数据集(2007 ~ 2023.12)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-01-18-neeq-china-listed-on-nation-equities-exchange-and-quotation-system-anunal-year-report/&#34;&gt;&lt;strong&gt;数据集(付费) | 三板上市公司年报2002-2023.12&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-01-14-usa-sec-10k-report-dataset/&#34;&gt;&lt;strong&gt;数据集 | 美股年报10-K、20-F数据(2000-2023.12)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/&#34;&gt;&lt;strong&gt;词向量(付费) | 使用MD&amp;amp;A2001-2022语料训练Word2Vec模型&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-01-06-mda_informative_content/&#34;&gt;中国工业经济 | MD&amp;amp;A信息含量指标构建代码实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-01-13-information-content-of-critical-audit/&#34;&gt;金融研究 | 使用Python构建「关键审计事项信息含量」&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-09-08-earnings-communication-conference-forward-looking-statements-information/&#34;&gt;中国管理科学 | 使用业绩说明会文本数据测量上市公司前瞻性信息&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/&#34;&gt;&lt;strong&gt;数据集 | A股上市公司基本信息&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四获取数据&#34;&gt;四、获取数据&lt;/h2&gt;
&lt;p&gt;数据集 100 元，&lt;strong&gt;加微信 372335839， 备注「姓名-学校-专业」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一数据集介绍">一、数据集介绍</h2>
<p>2001-2022年A股年报数据集，含 4 个文件，约 11G。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 管理层讨论与分析txt.zip
- 年报txt.zip
- A01-22.csv.gz
- mda01-22.csv.gz
</code></pre></div><p><img loading="lazy" src="img/a-mda.png" alt=""  />
</p>
<br>
<p>注意</p>
<ul>
<li>zip文件夹是原始数据， 解压后内部为 txt 文件。</li>
<li>gz文件为汇总数据， 解压后是csv文件。</li>
</ul>
<p><br><br></p>
<h2 id="二年报数据">二、年报数据</h2>
<p>2001-2022年年报数据。数据中只有year、code、text三个字段， 如果想增加诸如公司简称、行业等信息， 可以使用 <a href="https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/"><strong>数据集 | A股上市公司基本信息</strong></a>   进行并表。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">anual_report_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;A01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">anual_report_df</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<p>年报记录数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">anual_report_df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">55856
</code></pre></div><br>
<p>上市公司总数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">anual_report_df</span><span class="o">.</span><span class="n">code</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">5357
</code></pre></div><br>
<br>
<h2 id="三mda数据">三、MD&amp;A数据</h2>
<p>2001-2022年MD&amp;A数据， 数据中只有year、code、text三个字段， 如果想增加诸如公司简称、行业等信息， 可以使用 <a href="https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/"><strong>数据集 | A股上市公司基本信息</strong></a>   进行并表。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mda_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;mda01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="n">mda_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">mda_df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">55439
</code></pre></div><br>
<p>上市公司总数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mda_df</span><span class="o">.</span><span class="n">code</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">5355
</code></pre></div><p><br><br></p>
<h2 id="四相关内容">四、相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-01-21-hk-stock-market-anual-report/"><strong>数据集 | 港股年报文本数据集(2007 ~ 2023.12)</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-18-neeq-china-listed-on-nation-equities-exchange-and-quotation-system-anunal-year-report/"><strong>数据集(付费) | 三板上市公司年报2002-2023.12</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-01-14-usa-sec-10k-report-dataset/"><strong>数据集 | 美股年报10-K、20-F数据(2000-2023.12)</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"><strong>词向量(付费) | 使用MD&amp;A2001-2022语料训练Word2Vec模型</strong></a></li>
<li><a href="https://textdata.cn/blog/2023-01-06-mda_informative_content/">中国工业经济 | MD&amp;A信息含量指标构建代码实现</a></li>
<li><a href="https://textdata.cn/blog/2023-01-13-information-content-of-critical-audit/">金融研究 | 使用Python构建「关键审计事项信息含量」</a></li>
<li><a href="https://textdata.cn/blog/2023-09-08-earnings-communication-conference-forward-looking-statements-information/">中国管理科学 | 使用业绩说明会文本数据测量上市公司前瞻性信息</a></li>
<li><a href="https://textdata.cn/blog/2024-04-16-china-listed-company-information-dataset/"><strong>数据集 | A股上市公司基本信息</strong></a></li>
</ul>
<p><br><br></p>
<h2 id="四获取数据">四、获取数据</h2>
<p>数据集 100 元，<strong>加微信 372335839， 备注「姓名-学校-专业」</strong>。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</title>
      <link>https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/</link>
      <pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/</guid>
      <description>Literally, **word embedding (Embeddings)** is the use of dense vectors to represent the semantics of a word. **Scholars have shown that by comparing the distance between these word vectors, we can understand how &amp;#34;humans&amp;#34; understand the meaning of words**. So, if we have a corpus comparing the distance between &amp;#34;taxes&amp;#34; and social groups (&amp;#34;conservatives&amp;#34;, &amp;#34;socialists&amp;#34;), semantically, &amp;#34;taxes&amp;#34; should be farther away from &amp;#34;socialists&amp;#34;, after all The money collected is for the service of the general public and has elements of socialism. In the word embedding space, word vectors contain rich information, such as analogies. Spain is to Madrid what Germany is to Berlin and France to Paris.字面上，**词嵌入(Embeddings)**是使用稠密向量表示一个词语的语义。**学者们已经表明，通过比较这些词向量之间的距离，我们可以了解“人类”如何理解单词的含义**。因此，如果我们有一个语料库，比较“税收” 与 社会团体(“保守派”、“社会主义者”)  之间的距离， 按照语义，“税收”应该距离 “社会主义者” 跟多一些，毕竟收上来的钱是为了社会大众服务，有社会主义的成分。在词嵌入空间中，词向量含有丰富的信息，例如可以做类比。西班牙之于马德里， 正如德国至于柏林、法国之于巴黎。&amp;#34;</description>
      <content:encoded><![CDATA[<p>Rodriguez, Pedro L., and Arthur Spirling. &ldquo;<strong>Word embeddings: What works, what doesn’t, and how to tell the difference for applied research</strong>.&rdquo; <em>The Journal of Politics</em> 84, no. 1 (2022): 101-115.</p>
<p>论文作者是政治学领域的， 但是词嵌入方法在社科研究是通用的，大邓觉得挺有用的，使用chatGPT进行翻译+大邓整理， 耗时约2个小时。</p>
<br>
<h2 id="1-什么是词嵌入">1. 什么是词嵌入？</h2>
<p>字面上，**词嵌入(Embeddings)**是使用稠密向量表示一个词语的语义。<strong>学者们已经表明，通过比较这些词向量之间的距离，我们可以了解“人类”如何理解单词的含义</strong>。因此，如果我们有一个语料库，比较“税收” 与 社会团体(“保守派”、“社会主义者”)  之间的距离， 按照语义，“税收”应该距离 “社会主义者” 跟多一些，毕竟收上来的钱是为了社会大众服务，有社会主义的成分。在词嵌入空间中，词向量含有丰富的信息，例如可以做类比。西班牙之于马德里， 正如德国至于柏林、法国之于巴黎。</p>
<p><img loading="lazy" src="img/word-embeddings.png" alt=""  />
</p>
<br>
<h2 id="2-我没有听说过它们这是一个新的想法吗">2. 我没有听说过它们。这是一个新的想法吗？</h2>
<p>不是。这个想法相当古老，至少可以追溯到20世纪50年代。如果非要说「新」，变化的只是相比过去，现在可以更快、更容易获取嵌入。</p>
<br>
<h2 id="3-好的那么这个旧的想法是什么">3. 好的。那么这个“旧”的想法是什么？</h2>
<p><img loading="lazy" src="img/firth_words.png" alt=""  />
</p>
<p>Firth（1957）有一个人人引用的谚语：</p>
<blockquote>
<p>You shall know a word by the company it keeps</p>
</blockquote>
<p>通过一个单词所处的语境，我们可以了解该单词的含义。 <strong>该谚语源于英国语言学家 J.R. Firth 的理论，他认为单词的含义是由其周围的语境和与之相伴的其他单词所决定的，因此我们需要通过单词出现的上下文来理解其含义。这一理论在语言学、自然语言处理等领域有着广泛的应用</strong>。</p>
<p>简而言之，这就是所谓的 “<strong>分布假设</strong>” 。字面上，这个想法是，出现在类似的 <strong>上下文Context</strong> 中的单词可能意味着相似的事物。如果“<strong>咖啡</strong>”和“茶”总是接近于“<strong>杯子</strong>”，那么我们可能会推断出“咖啡”和“<strong>茶</strong>”是相似的。</p>
<p>利用这一洞见的模型有时被称为 <strong>分布语义模型（distributional semantic models， DSMs）</strong>。</p>
<br>
<h2 id="4-在这种情况下上下文字面上是什么意思">4. 在这种情况下，“上下文”字面上是什么意思？</h2>
<p>在嵌入文献中，上下文通常是一个局部的对称窗口，围绕一个词展开。因此，假设我们的句子是：</p>
<blockquote>
<p>&ldquo;We then heard some nice, relaxing <strong>music</strong> that gently worked to a crescendo.&rdquo;</p>
<p>“然后我们听到一些美妙、轻松的音乐，它慢慢地推向高潮。”</p>
</blockquote>
<p>这里以 <strong>music</strong>  为中心的一个对称窗口可以是 <strong>(nice, relaxing)</strong>  和  <strong>(that, gently)</strong>。</p>
<p>三个词的窗口可以是 <strong>(some, nice, relaxing)</strong>  和  <strong>(&ldquo;that, gently, worked)</strong> 。窗口内music前后的词就是ta的上下文。</p>
<br>
<h2 id="5-所以所有的dsms都使用本地窗口吗">5. 所以所有的DSMs都使用本地窗口吗？</h2>
<p>不是的。 <strong>分布语义模型</strong>（DSMs）包括像隐含狄利克雷分配（LDA）这样的东西，政治学通常用于“主题模型”。但是它们通常不使用本地窗口。</p>
<br>
<h2 id="6-我明白了所以嵌入模型没有做词袋假设">6. 我明白了！所以嵌入模型没有做“词袋”假设？</h2>
<p>好吧，这要看你的意思是什么。显然，本地窗口在某种意义上有助于考虑词序。但是在窗口内，模型通常将其视为词袋（即无序的）。</p>
<p><img loading="lazy" src="img/%e8%af%8d%e8%a2%8b%e6%b3%95%e4%b8%8e%e8%af%8d%e5%b5%8c%e5%85%a5.png" alt=""  />
</p>
<br>
<h2 id="7-嵌入向量与我在文本数据课程中学习的向量空间模型有关系吗">7. 嵌入向量与我在文本数据课程中学习的“向量空间”模型有关系吗？</h2>
<p><strong>有也没有</strong>。在典型的向量空间模型中，每个文档都是一个实值向量（通常是计数）。因此，“dog eat dog world”可能表示为<code>[2, 1, 1]</code>，其中第一个元素表示“dog”，第二个表示“eat”，第三个表示“world”等等。在词嵌入中，每个单词都有自己的向量，而这些向量是由模型学习的。它与向量空间模型有关，因为单词在多维空间中以向量形式表示。</p>
<br>
<h2 id="8-听起来很有趣但是使用单词嵌入有什么好处">8. 听起来很有趣。但是使用单词嵌入有什么好处？</h2>
<p>事实证明，以这种方式表示单词对于许多“下游”的自然语言处理和机器学习任务是有帮助的。例如，词性标注：嵌入可以帮助我们区分单词在给定上下文中使用的“意义”。更普遍地，<strong>了解概念之间的关系可能是有用的</strong>：如果我们知道在我们的语料库中，“雨伞”比“晒霜”更接近于“雨衣”，我们可能想要向那些寻找雨衣而不是晒霜的人推广雨伞。 在政治学中的一个自然应用是建立词典：如果“共和党”在嵌入空间中靠近“保守派”和“新保守主义者”，那么这可能告诉我们，我们应该将所有这些都视为美国政治中右翼意识形态的例子。</p>
<p>有一个经典的关于嵌入的运算公式， 假设我们有以下单词的嵌入向量：“国王”，“女王”，“男人”，“女人”。对于某些规范，事实证明，大致上有： “国王”-“男人”+“女人”=“女王”。也就是说，“国王”类似于“女王”，就像“男人”类似于“女人”一样。</p>
<p><img loading="lazy" src="img/word2vec-king-queen.png" alt=""  />
</p>
<p><img loading="lazy" src="img/word2vec-semantic-queen-king.jpeg" alt=""  />
</p>
<br>
<h2 id="9-你已经说服我了那么如何获得这些嵌入">9. 你已经说服我了。那么如何获得这些嵌入？</h2>
<p>你需要一个模型。有很多很多选择，从2000年初期的  **神经neural ** 网络模型开始。</p>
<br>
<h2 id="10-哇神经听起来非常复杂啊">10. 哇，“神经”听起来非常复杂啊？</h2>
<p>不，实际上不是。这些模型也已经存在很长时间了（至少自1990年代末/2000年代初以来）。重申一下，它们只是因为现在算力最近变得快速，具有可扩展性。</p>
<br>
<h2 id="11我应该使用哪种模型">11.我应该使用哪种模型？</h2>
<p>由你自己决定，但最受欢迎的是：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>年份</th>
<th>资料</th>
</tr>
</thead>
<tbody>
<tr>
<td>Word2Vec</td>
<td>2013</td>
<td>论文 <a href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a><br>代码 <a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a></td>
</tr>
<tr>
<td>Glove</td>
<td>2014</td>
<td>网站 <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></td>
</tr>
</tbody>
</table>
<br>
<h2 id="12-什么是-word2vec">12. 什么是 Word2Vec？</h2>
<p>它是一种实现 <strong>词嵌入</strong> 的方法，有两种不同的类型（称为“体系结构”）：</p>
<ol>
<li><strong>连续词袋（CBOW）</strong>。这假设您想要在给定上下文词（来自本地窗口）的情况下预测目标词（上面的“音乐”），<strong>这有点像做英文完形填空</strong>。</li>
<li><strong>Skip-gram</strong>。这假设您想要预测给定特定单词（在我们的例子中为 <strong>music</strong> ）的上下文词（在 <strong>music</strong> 周围的本地窗口中的内容）。</li>
</ol>
<br>
<h2 id="13-word2vec如何拟合数据">13. Word2Vec如何拟合数据？</h2>
<p>它通过单词与单词之间进行，尝试预测目标单词或上下文，具体取决于所需的体系结构。最终，它使用神经网络模型来完成这个任务。</p>
<br>
<h2 id="14-word2vec-是深度学习吗">14. Word2Vec 是深度学习吗？</h2>
<p>不是的。word2vec 的神经网络只有一层，所以它不是真正的 <strong>深度学习Deep Learning</strong> ， 而是“浅层学习”。</p>
<br>
<h2 id="15-什么是-glove-">15. 什么是 GloVe ？</h2>
<p><strong>GloVe</strong> 的全称是 Global Vectors for Word Representation， 它也是一种生成 <strong>词嵌入</strong> 的方法，字面上表示用于单词表示的全局向量。<br></p>
<h2 id="16-glove-如何拟合数据">16. GloVe 如何拟合数据？</h2>
<p>它使用 “全局”（聚合）共现计数。请注意，<code>Word2Vec</code>不会这样做：它按单词进行处理，从不将整个语料库视为一个整体。</p>
<br>
<h2 id="17-那么-glove-不是深度学习">17. 那么 GloVe 不是深度学习？</h2>
<p>不是，但<code>Word2Vec</code>也不是（参见上面）。</p>
<br>
<h2 id="17-哪个更好-glove-还是-word2vec">17. 哪个更好？ GloVe 还是 Word2Vec？</h2>
<p>没有。首先，从根本上讲，它们在所做的事情上是基本相似的。<strong>有一些证据表明，在某些任务，GloVe 更稳定，表现更好</strong>。</p>
<p>在我们的研究中，我们发现，“开箱即用”，相对于<code>Word2Vec</code>（skipgram），<code>GloVe</code>最初在建议提示词的良好最近邻方面表现更好（请参见下面的#27）。但是，一旦我们将<code>Word2Vec</code>词汇子集排除了非常罕见的单词，就在人类编码器偏好方面表现几乎相同。</p>
<br>
<h2 id="18-我有一个语料库如何使用这些模型">18. 我有一个语料库，如何使用这些模型？</h2>
<p>你使用 word2vec 或 glove 代码， 并将文本数据(语料)导入到代码中， 运行得到词嵌入模型文件。大邓的 cntext库支持两种算法的实现，需要注意的是， glove训练速度较慢， 而斯坦福大学训练Glove使用的是C语言代码。</p>
<ul>
<li>如果担心Glove速度，就用斯坦福的代码。</li>
<li>如果想简单点，不考虑速度，可以考虑大邓整理出的cntext库</li>
</ul>
<p><a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></p>
<br>
<h2 id="19-嗯听起来很复杂还有其他选择吗">19. 嗯。听起来很复杂。还有其他选择吗？</h2>
<p>很多时候自己训练的词嵌入模型耗时耗力， 其实如果你研究对象所涉及的数据与已有研究机构使用的语料是类似的， 也可以使用研究机构公开出的  <strong>预先训练好的词嵌入模型</strong> 。</p>
<br>
<h2 id="20-如果我的语料库与维基百科完全不同怎么办">20. 如果我的语料库与维基百科完全不同怎么办？</h2>
<p>你可以将 word2vec 或 glove 拟合到你的文本数据中。但请注意，拟合的数据越多，词嵌入模型任务表现的越好）。因此，你可能需要相当多的数据：几百M 几个G 甚至更多，而不是区区 50 份宣言文档。</p>
<br>
<h2 id="21--好的本地训练还是使用预训练模型还有哪些决策需要做">21.  好的，本地训练还是使用预训练模型。还有哪些决策需要做？</h2>
<p>回到 <strong>FAQ 18</strong>，你需要做出决策的关键参数包括：</p>
<ul>
<li><strong>窗口大小</strong>：也就是你的单词周围的（对称的）窗口大小。例如2、4、6、10等。</li>
<li><strong>嵌入向量长度</strong>：代表单词的向量大小。例如50、100、300、450等。</li>
</ul>
<p>需要注意的是，即使你使用 <strong>预训练的词嵌入模型</strong>，你也需要做出这些决策，因为你将根据需求下载不同的词嵌入预训练模型。</p>
<p><img loading="lazy" src="img/%e6%a8%a1%e5%9e%8b%e5%86%b3%e7%ad%96.png" alt=""  />
</p>
<br>
<h2 id="22-最佳的窗口大小如何确定">22. 最佳的窗口大小如何确定？</h2>
<p>对社会科学而言，窗口大小并没有固定的说法。 总体上呢，我们知道更大的窗口（&gt;2）捕捉更多主题关系（例如“Obama-President”），而更小的窗口（&lt;2）则捕捉词汇前后的语法关系（例如“jumps-jumping”）。</p>
<br>
<h2 id="23--最佳的词嵌入向量长度维度数如何确定">23.  最佳的词嵌入向量长度(维度数)如何确定？</h2>
<p>对社会科学而言，<strong>维度数</strong> 也没有固定的说法。 总体上， 我们知道更大的维度数可以捕捉更多词汇语义信息。 但是维度数过大也可能不是最优的，因为这会导致冗余（即有很多维度没有作用，可能只是捕捉噪声）。但是太小也不好：极限情况下，可能无法区分单词。常见的维度数如50， 100， 200维</p>
<br>
<h2 id="24-好的还有什么需要不需要了解的">24. 好的，还有什么需要（不需要）了解的？</h2>
<p>词嵌入的向量并不稳定。如果你在本地拟合你的语料库，每次你会得到不同的结果。这是由于模型的性质（<strong>机器学习类算法大多是要进行模型随机初始化，这导致每次得到的模型会有差异</strong>），这基本上是一个事实。  <strong>缓解这种情况的一种方法是多次估计相同的模型并在所需的距离度量上进行聚合</strong>。</p>
<br>
<h2 id="25-还有其他需要注意的事项吗">25. 还有其他需要注意的事项吗？</h2>
<p>是的，还有一些与技术模型拟合相关的超参数可以选择（例如收敛阈值、迭代次数）。但是我们不会深入讨论这些问题，因为许多这些问题都有适当的默认值。</p>
<br>
<h2 id="26-好的那么对于我的政治科学案例我该怎么做呢">26. 好的。那么对于我的「政治科学案例」，我该怎么做呢？</h2>
<p>很高兴你问到这个问题。我们的论文涵盖了政治科学中几个语料库的这些问题，我们认为这些语料库在实际研究中是有一定代表性的。我们评估了在超参数选择之间变化时“性能”如何变化以及它如何影响稳定性。</p>
<br>
<h2 id="27-我们如何评估政治科学中的嵌入性能">27. 我们如何评估政治科学中的嵌入“性能”？</h2>
<p>我们使用了四个工具：</p>
<ol>
<li>技术标准：本质上是“适合度”（作为窗口大小的函数），加上计算时间。</li>
<li>稳定性： 同一模型的不同运行在关键词的最近邻居方面的相关性如何。</li>
<li>查询搜索排名相似性： 两个不同模型的查询单词的最近邻居有多相似。</li>
<li>图灵测试：请人类比较嵌入模型生成的最近邻居和（不同、独立的）人类编码器生成的最近邻居。</li>
</ol>
<br>
<h2 id="28-在上下文中什么是最近邻查询单词是什么">28. 在上下文中，什么是最近邻？查询单词是什么？</h2>
<p>无论规范如何，<strong>词嵌入模型</strong> 都会为词汇表中的任何单词提供向量表示。 对于给定的单词的向量——我们可以使用一定的算法(余弦相似性最大)找到距离ta最近的n个词，这些是它的最近邻，彼此语义会比较紧密相似。</p>
<p>显然，我们不会检查词汇表中的每个单词，而是某些与政治科学有关的单词。对我们而言，这些单词与以下内容相关：</p>
<ul>
<li>大的议题，如： <code>民主</code>，<code>自由</code>，<code>平等</code>，<code>正义</code>。</li>
<li>美国的政策辩论：<code>移民</code>，<code>堕胎</code>，<code>福利</code>，<code>税收</code>。</li>
<li>党派： <code>共和党</code>，<code>民主党</code></li>
</ul>
<p>我们也可以选择其他单词，但我们认为这些是一个不错的开始。</p>
<br>
<h2 id="29-我们的语料库是什么">29. 我们的语料库是什么？</h2>
<p>我们的重点语料库是第102-111届国会的 <strong>Congressional Record</strong> 记录。它是美式英语，大约有140万个文档。 但我们还有其他语言的语料库（见下文）。</p>
<br>
<h2 id="30-我们尝试了哪些参数组合">30. 我们尝试了哪些参数组合？</h2>
<ul>
<li>窗口大小：1、6、12、24、48</li>
<li>嵌入维度：50、100、200、300、450</li>
</ul>
<br>
<h2 id="31-我们使用哪个模型">31. 我们使用哪个模型？</h2>
<p>我们同时使用 <strong>GloVe</strong> 和 <strong>Word2Vec</strong>。但是在文章的前两个部分，我们将 <strong>GloVe</strong> 视为一种基准线。 然后在论文的后面，我们将 <strong>GloVe</strong> 的性能与 <strong>Word2Vec</strong> 进行比较，以人类编码者的喜好为衡量标准。在实践中，由于模型使用的词汇不同，这种比较稍微有些复杂。</p>
<br>
<h2 id="32-图灵测试是如何实际工作的">32. 图灵测试是如何实际工作的？</h2>
<p>我们给人类编码者一组查询或提示词（<strong>来自FAQ 28</strong>）。然后告诉他们编写（十个）与这些单词相似，意义相近，且可能会出现在语料库中等等的单词。从“机器”方面，我们提取模型建议的十个最近邻。最后，我们要求（不同的，独立的）人类编码者比较“人类”的最近邻和“机器”的最近邻（我们不告诉他们哪个是哪个）。他们只需要告诉我们哪一个更适合所述提示单词。</p>
<p>这是一种近似的“图灵测试”，因为我们试图找出机器（模型）是否能够很好地模仿人类的能力。</p>
<br>
<h2 id="33-基于技术标准的结果如何">33. 基于技术标准的结果如何？</h2>
<p>首先，请注意，要评估这一点，我们参考的是 <strong>GloVe</strong> 本地拟合我们的***Congressional Record ***语料库。</p>
<p><strong>更大的窗口和更多的嵌入维度总是改善模型拟合——但是二者都存在递减的回报</strong>。毫无疑问，使用非常小的<strong>嵌入维度数</strong>（&lt;100）对拟合是不好的。并不奇怪，更大的模型——更大的窗口，更长的向量——更需要时间来拟合。但即使是最大的模型也不那么糟糕：拟合最大的模型需要大约3个小时左右。</p>
<p>在交换拟合与时间方面，使用<strong>标准选项6-300</strong>（即窗口大小为6，嵌入维度为300）是一个合理的选择。</p>
<br>
<h2 id="34-稳定性的结果是什么">34. 稳定性的结果是什么？</h2>
<p>在其他条件相同的情况下，较大窗口size的模型更加稳定，但是在窗口size过大，尤其是在维度数很大的模型中，会出现一些性能下降。较少的维度数（其他条件相同）似乎也不太稳定。<strong>少量的维度数和小的窗口大小会产生特别高的方差结果</strong>。</p>
<br>
<h2 id="35-查询排名的相关性如何">35. 查询排名的相关性如何？</h2>
<p>首先，无论查看关键政治单词还是随机单词，所有预训练模型都显示出高相关性（通常&gt; 0.6）。也就是说，它们之间实际上没有太多“实质性”差异。 预训练模型和本地模型在这方面，相关性也很高（&gt; 0.5），尤其是对于我们的政治单词。</p>
<p><strong>这表明使用现成的预训练嵌入模型 与使用本地拟合的模型能产生近似的结论。</strong></p>
<br>
<h2 id="36-人类更喜欢哪一个人类还是机器">36. 人类更喜欢哪一个：人类还是机器？</h2>
<p>令人惊讶的是，在我们的政治查询集合中，并没有明显的优胜者。 在自己电脑本地训练出的词嵌入模型通常可以达到人类表现约70%。预训练嵌入模型的表现更好：它们可以达到高达87%的人类表现。</p>
<br>
<h2 id="37-这些发现是否具有普适性还是只适用于这个语料库">37. 这些发现是否具有普适性？还是只适用于这个语料库？</h2>
<p>我们在几个语料库上进行了相同的实验（不包括 Turing 测试），结果与上述结果大致相似。也就是说，我们认为这些结果并不是源于 <em>Congressional Record</em> 这个语料库本身的原因。</p>
<p>我们看过的其他语料库包括：</p>
<table>
<thead>
<tr>
<th>语料库</th>
<th>语言</th>
<th>年份</th>
<th>数据量</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hansard</td>
<td>英国英语</td>
<td>1935-2013年</td>
<td>约4.5M个文档</td>
</tr>
<tr>
<td>State of the Union国情咨文</td>
<td>美国英语</td>
<td>1790-2018年</td>
<td>小语料库</td>
</tr>
<tr>
<td>Cortes Generales</td>
<td>西班牙语</td>
<td>1993-2018年</td>
<td>约1.3M个文档</td>
</tr>
<tr>
<td>Deutscher Bundestag</td>
<td>德语</td>
<td>1998-2018年</td>
<td>约1.2M个文档</td>
</tr>
</tbody>
</table>
<br>
<h2 id="38-最终结论在应用设置中研究人员应该怎么做">38. 最终结论：在应用设置中，研究人员应该怎么做？</h2>
<p>对于政治学而言，<strong>预训练词嵌入模型</strong> 的标准设定是 <strong>6-300 GloVe模型</strong>（窗口大小=6，维数=300）的表现与其他任何方法相比都差不多。这些模型与您进行的其他事情相当高度相关。更重要的是，人类喜欢这些结果——尤其是与本地训练模型相比，它甚至接近人类生成的结果。显然，使用预训练模型更便宜。但我们的经验是，这可能本身并不足以说服人，因为对于大约100万个文档的语料库而言，本地训练处这些模型是相当快速。</p>
<p>您可能会通过进行本地拟合来在实质上获得一些收益，但从我们的人类验证来看，这些收益并不明显。但是，您可能正在优化一些标准，而我们并不知道这些标准是什么。<strong>如果您必须进行本地拟合，则要避免小窗口和小维数，特别是在结合使用时</strong>。</p>
<br>
<h2 id="39-最后的想法是">39. 最后的想法是</h2>
<p>我们没有研究词嵌入是否本身相对于其他算法（如词袋法）代表着更好的东西。粗略地说，仅仅因为词嵌入模型现在非常流行，就不意味着它们应该取代您正在进行的主题模型或其他模型。</p>
<p>这是一个发展迅速的领域。更复杂，表现更好的词嵌入模型版本将会（已经）出现。因此，我们认为我们的关键贡献是图灵测试的安排：我们推测，在政治科学中，受监督的问题将继续相对较少，因此学者们应该确保他们的文档和单词的模型表示确实与有用的东西相符。</p>
<p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 历史文本中的语言积极性反映了动态的环境和心理因素(含Python代码)</title>
      <link>https://textdata.cn/blog/2023-03-13-linguistic-positivity-in-historical-texts-reflects-dynamic-environmental-and-psychological-factors/</link>
      <pubDate>Mon, 13 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-13-linguistic-positivity-in-historical-texts-reflects-dynamic-environmental-and-psychological-factors/</guid>
      <description>Linguistic positivity in historical texts reflects dynamic environmental and psychological factors历史文本中的语言积极性反映了动态的环境和心理因素</description>
      <content:encoded><![CDATA[<p>分享一篇研究语言积极倾向的论文，论文中使用Python工具进行词频统计，大大降低了人工成本，提高了科研效率。</p>
<p>该论文发表在PNAS：</p>
<blockquote>
<p>Iliev, R., Hoover, J., Dehghani, M. and Axelrod, R., 2016. <strong>Linguistic positivity in historical texts reflects dynamic environmental and psychological factors</strong>. <em>Proceedings of the National Academy of Sciences</em>, <em>113</em>(49), pp.E7871-E7879.</p>
</blockquote>
<p><br><br></p>
<h2 id="一研究背景">一、研究背景</h2>
<p>作者指出：当前关于 <strong>LPB(language positivity bias)作用机制</strong> 的驱动因素有普遍的普遍的认知偏差、情感状态、客观环境和社会规范，但是究竟是哪种机制驱动LPB，以及这种影响是否可能是由于这些或其他因素子集之间的相互作用驱动的，尚无研究讨论。造成这种不确定性的一个原因是，以前对 LPB 的调查采用了语言的共时方法，并且无法提供任何洞察力来了解 LPB 在给定语言中是否或在何种程度上具有跨时间和上下文的稳定。在该研究中，作者采用了一种将 LPB 视为动态现象的方法。具体来说，使用两个带时间戳的美国英语语料库，研究 LPB 中的经度变化作为主观、客观和社会因素的函数。这种方法可以研究 LPB 的历时变化，这是一个未探索的影响维度，而且还可以在先前提出的 LPB 解释之间进行裁决。</p>
<p><br><br></p>
<h2 id="二研究过程">二、研究过程</h2>
<p>提出五个假设：</p>
<ol>
<li>
<p>LPB没有线性趋势</p>
</li>
<li>
<p>LPB 随着时间的推移而增加。</p>
</li>
<li>
<p>LPB 随着时间的推移而减少。</p>
</li>
<li>
<p>LPB 的变化来预测，环境恶化将与LPB的降低相关联。</p>
</li>
<li>
<p>LPB的变化将通过集体影响的变化来预测，因此国家层面幸福感的下降将与LPB的下降相关。</p>
</li>
</ol>
<p>开展四个研究：</p>
<ol>
<li>
<p>线性趋势</p>
</li>
<li>
<p>战争伤亡人数</p>
</li>
<li>
<p>经济苦难</p>
</li>
<li>
<p>主观幸福</p>
</li>
</ol>
<p><br><br></p>
<h2 id="三数据来源">三、数据来源</h2>
<ol>
<li><strong>情感词典</strong>：使用了语言查询和字数统计 (LIWC) 词典 (56) 中的正面和负面情绪词类别，其中包含 907 个词和词干。正面类别有 408 个条目，负面类别有 499 个条目。</li>
<li><strong>战争伤亡人数</strong>：使用了来自美国退伍军人事务部 (57) 的情况说明书中的数据，并计算了过去两个世纪美国参与的战争中美国军人的平均伤亡人数。</li>
<li><strong>痛苦指数</strong>：使用了来自 <a href="http://www.miseryindex.us">www.miseryindex.us</a> 的数据，其中包含 1948 年至 2015 年期间美国的苦难指数指标。</li>
<li><strong>幸福指数</strong>：使用了世界幸福数据库中有关美国幸福感的调查数据。</li>
</ol>
<p><br><br></p>
<h2 id="四研究发现">四、研究发现：</h2>
<ol>
<li>美式英语中情感词的使用随着时间的推移而减少。</li>
<li>发现了 LPB 纵向下降趋势的令人信服的证据。这种趋势在谷歌 Ngrams 语料库中非常强烈，在纽约时报语料库中略显重要。</li>
<li>发现 LPB 也随着战争的伤亡人数而减少。</li>
<li>LPB 可以通过客观环境的不太极端的测量来预测。在控制时间后，我们发现痛苦指数较高的年份在两个语料库中的 LPB 水平往往较低。</li>
<li>发现 LPB 的短期波动随全国幸福指数的变化而变化。</li>
<li>这些结果进一步证实了LPB 不能简单地解释为普遍认知机制的功能。</li>
</ol>
<p><br><br></p>
<h2 id="五代码">五、代码</h2>
<p>为简化学习难度，论文细节部分不展开。本文主要给大家展示用Python做情感词频统计、词频历时折线图这两部分内容。先安装需要的包</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">
!pip3 install pyecharts==1.6.2
!pip3 install pyecharts-javascripthon==0.0.6              
!pip3 install pyecharts-jupyter-installer==0.0.3              
!pip3 install pyecharts-snapshot==0.2.0 
</code></pre></div><br>
<h3 id="51-实验数据集">5.1 实验数据集</h3>
<p>美国政治在其年轻的生命周期中经历了许多意识形态的动荡。这些动荡被称为政治时代。“政治时代是指在历史和政治科学中使用的一种美国政治模式，用于分类存在于美国的政党制度。” 数据集作者创建这个数据集的原因是为了正确讨论过去的政治事件，了解各个时期的政治平台和氛围是很重要的。例如，民主党和共和党的政治理念在不同的时期发生了巨大的变化，党派之间的理念也在不断转变、涌现和消失。截至目前为止，美国政治中已经公认了6个政治时代，分别是：</p>
<ol>
<li>第一党派制度（1792年至1824年）</li>
<li>第二党派制度（1828年至1854年）</li>
<li>第三党派制度（1854年至1895年）</li>
<li>第四党派制度（1896年至1932年）</li>
<li>第五党派制度（1932年至1964年）</li>
<li>第六党派制度（1964年至今）</li>
</ol>
<p>我将总统同一个年度的演讲汇总到一起，最终得到  <a href="yearly_american_speech_dataset.csv">yearly_american_speech_dataset.csv</a> 。
<br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;yearly_american_speech_dataset.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<h3 id="52-小实验">5.2 小实验</h3>
<p>写代码讲究先把任务抽象化， 将大问题拆解成可组装的小问题。即先小后大，先局部后整体。这里推荐用Python中的cntext库，该库文档清晰，代码简洁。先安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">!pip3 install scikit-learn==1.0
!pip3 install cntext==1.8.4
</code></pre></div><br>
<p>最简单的情感分析，即分析一句话 text 的正负面情感词出现的词频。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;What a sunny day!&#39;</span>

<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;sunny&#39;</span><span class="p">,</span> <span class="s1">&#39;good&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;bad&#39;</span><span class="p">,</span> <span class="s1">&#39;terrible&#39;</span><span class="p">]}</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>{'pos_num': 1,
 'neg_num': 0,
 'stopword_num': 1,
 'word_num': 5,
 'sentence_num': 1}
</code></pre>
<br>
<p>根据自定义的词典diciton， 可以看出 <strong>What a sunny day!</strong> 中有1个pos类词，0个neg类词。</p>
<br>
<h3 id="53-找个公开的情感词典">5.3 找个公开的情感词典</h3>
<p>计算文本中正负面情感词出现次数， 需要有情感词词表。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看cntext内置的词典</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><pre><code>['DUTIR.pkl',
 'HOWNET.pkl',
 'Chinese_Loughran_McDonald_Financial_Sentiment.pkl',
 'sentiws.pkl',
 'ChineseFinancialFormalUnformalSentiment.pkl',
 'Chinese_Digitalization.pkl',
 'ANEW.pkl',
 'LSD2015.pkl',
 'NRC.pkl',
 'geninqposneg.pkl',
 'HuLiu.pkl',
 'Loughran_McDonald_Financial_Sentiment.pkl',
 'AFINN.pkl',
 'ADV_CONJ.pkl',
 'STOPWORDS.pkl',
 'Concreteness.pkl',
 'ChineseEmoBank.pkl']
</code></pre>
<p>查看NRC词典的信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#NRC词典</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NRC词典描述: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;Desc&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NRC参考文献: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;Refer&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NRC词典内词表有: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Pos词表前10个词: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;positive&#39;</span><span class="p">][:</span><span class="mi">10</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Neg词表前10个词: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;negative&#39;</span><span class="p">][:</span><span class="mi">10</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>NRC词典描述:  The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing.

NRC参考文献:  Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

NRC词典内词表有:  dict_keys(['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust'])

Pos词表前10个词:  ['abba', 'ability', 'abovementioned', 'absolute', 'absolution', 'absorbed', 'abundance', 'abundant', 'academic', 'academy']

Neg词表前10个词:  ['abandon', 'abandoned', 'abandonment', 'abduction', 'aberrant', 'aberration', 'abhor', 'abhorrent', 'abject', 'abnormal']
</code></pre>
<br>
<h3 id="54-情感词频统计">5.4 情感词频统计</h3>
<p>使用cntext设计一个函数，将计算得到的文本词数/正(负)面词出现次数, 得到情感词在文中的占比，方便后续的可视化绘图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">emotion_analysis</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;positive&#39;</span><span class="p">],</span>
               <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;negative&#39;</span><span class="p">],</span>
               <span class="s1">&#39;anger&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;anger&#39;</span><span class="p">],</span>
               <span class="s1">&#39;anticipation&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;anticipation&#39;</span><span class="p">],</span>
               <span class="s1">&#39;disgust&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;disgust&#39;</span><span class="p">],</span>
               <span class="s1">&#39;fear&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;fear&#39;</span><span class="p">],</span>
               <span class="s1">&#39;joy&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;joy&#39;</span><span class="p">],</span>
               <span class="s1">&#39;sadness&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;sadness&#39;</span><span class="p">],</span>
               <span class="s1">&#39;surprise&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;surprise&#39;</span><span class="p">],</span>
               <span class="s1">&#39;trust&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;trust&#39;</span><span class="p">]}</span>
    
    <span class="n">res</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
                       <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span>
                       <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>


<span class="c1">#实验ok</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;What a sunny day!&#39;</span>
<span class="n">emotion_analysis</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>pos_num             1
neg_num             0
anger_num           0
anticipation_num    1
disgust_num         0
fear_num            0
joy_num             1
sadness_num         0
surprise_num        1
trust_num           0
stopword_num        1
word_num            5
sentence_num        1
dtype: int64
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Transcript&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">emotion_analysis</span><span class="p">)</span>
<span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
<br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div><p>Run</p>
<pre><code>Index(['pos_num', 'neg_num', 'anger_num', 'anticipation_num', 'disgust_num',
       'fear_num', 'joy_num', 'sadness_num', 'surprise_num', 'trust_num',
       'stopword_num', 'word_num', 'sentence_num'],
      dtype='object')
</code></pre>
<br>
<p>计算情感、情绪的词频(占比)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;pos_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">grouped_df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;neg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;neg_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;anger&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;anger_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;anticipation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;anticipation_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;disgust&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;disgust_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;fear&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;fear_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;joy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;joy_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;sadness&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;sadness_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;surprise&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;surprise_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;trust&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;trust_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;SentiScore&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;pos_num&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;neg_num&#39;</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;pos_num&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;neg_num&#39;</span><span class="p">])</span>
</code></pre></div><br>
<p>合并数据数据</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">res_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">df2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">res_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="六可视化">六、可视化</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">res_df</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div><p>Run</p>
<pre><code>Index(['pos_num', 'neg_num', 'anger_num', 'anticipation_num', 'disgust_num',
       'fear_num', 'joy_num', 'sadness_num', 'surprise_num', 'trust_num',
       'stopword_num', 'word_num', 'sentence_num', 'pos', 'neg', 'anger',
       'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise',
       'trust'],
      dtype='object')
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pyecharts</span> <span class="kn">import</span> <span class="n">options</span> <span class="k">as</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">Line</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>


<span class="c1"># 创建折线图对象</span>
<span class="n">line_chart1</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>


<span class="c1"># 添加 x 轴和 y 轴数据</span>
<span class="n">line_chart1</span><span class="o">.</span><span class="n">add_xaxis</span><span class="p">(</span><span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">line_chart1</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;Sentiment Score&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;SentiScore&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                     <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>


<span class="c1"># 配置图表选项</span>
<span class="n">line_chart1</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
    <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;美国总统演讲情感得分年度历时可视化&#34;</span><span class="p">),</span>
    <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">trigger</span><span class="o">=</span><span class="s2">&#34;axis&#34;</span><span class="p">,</span> <span class="n">axis_pointer_type</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">),</span>
    <span class="n">legend_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">LegendOpts</span><span class="p">(</span><span class="n">pos_right</span><span class="o">=</span><span class="s2">&#34;right&#34;</span><span class="p">,</span>
                                <span class="n">orient</span><span class="o">=</span><span class="s2">&#34;vertical&#34;</span><span class="p">,</span>
                                <span class="n">pos_top</span><span class="o">=</span><span class="s2">&#34;center&#34;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 显示图表</span>
<span class="n">line_chart1</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div><p><a href="line_chart1.html"><img loading="lazy" src="img/%e5%9b%be1.png" alt=""  />
</a></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pyecharts</span> <span class="kn">import</span> <span class="n">options</span> <span class="k">as</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">Line</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>


<span class="c1"># 创建折线图对象</span>
<span class="n">line_chart2</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>


<span class="c1"># 添加 x 轴和 y 轴数据</span>
<span class="n">line_chart2</span><span class="o">.</span><span class="n">add_xaxis</span><span class="p">(</span><span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="n">line_chart2</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;Positive&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart2</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;Negative&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;neg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># 配置图表选项</span>
<span class="n">line_chart2</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
    <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;美国总统演讲正、负面情感年度历时可视化&#34;</span><span class="p">),</span>
    <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">trigger</span><span class="o">=</span><span class="s2">&#34;axis&#34;</span><span class="p">,</span> <span class="n">axis_pointer_type</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">),</span>
    <span class="n">legend_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">LegendOpts</span><span class="p">(</span><span class="n">pos_right</span><span class="o">=</span><span class="s2">&#34;right&#34;</span><span class="p">,</span>
                                <span class="n">orient</span><span class="o">=</span><span class="s2">&#34;vertical&#34;</span><span class="p">,</span>
                                <span class="n">pos_top</span><span class="o">=</span><span class="s2">&#34;center&#34;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 显示图表</span>
<span class="n">line_chart2</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div><p><a href="line_chart2.html"><img loading="lazy" src="img/%e5%9b%be2.png" alt=""  />
</a><br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pyecharts</span> <span class="kn">import</span> <span class="n">options</span> <span class="k">as</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">Line</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>


<span class="c1"># 创建折线图对象</span>
<span class="n">line_chart3</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>

<span class="c1"># 添加 x 轴和 y 轴数据</span>
<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_xaxis</span><span class="p">(</span><span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;anger&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;anger&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;anticipation&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;anticipation&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;disgust&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;disgust&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;fear&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;fear&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;joy&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;joy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;sadness&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;sadness&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;surprise&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;surprise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;trust&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;trust&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>


<span class="c1"># 配置图表选项</span>
<span class="n">line_chart3</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
    <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;美国总统8类情绪词用量年度历时可视化&#34;</span><span class="p">),</span>
    <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">trigger</span><span class="o">=</span><span class="s2">&#34;axis&#34;</span><span class="p">,</span> <span class="n">axis_pointer_type</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">),</span>
    <span class="n">legend_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">LegendOpts</span><span class="p">(</span><span class="n">pos_right</span><span class="o">=</span><span class="s2">&#34;right&#34;</span><span class="p">,</span>
                                <span class="n">orient</span><span class="o">=</span><span class="s2">&#34;vertical&#34;</span><span class="p">,</span>
                                <span class="n">pos_top</span><span class="o">=</span><span class="s2">&#34;center&#34;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 显示图表</span>
<span class="n">line_chart3</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div><p><a href="line_chart3.html"><img loading="lazy" src="img/%e5%9b%be3.png" alt=""  />
</a>
<a href="line_chart3.html"><img loading="lazy" src="img/%e5%9b%be4.png" alt=""  />
</a></p>
<p>最后一张图中trust指标在08年前后几年是下降趋势，可能的原因是， 那个阶段正是08年金融危机，美国政府为了救华尔街， 用公民腰包里的钱补贴华尔街金融巨鳄。信任下降。</p>
<p><br><br></p>
<h2 id="七保存">七、保存</h2>
<p>情感计算过程得到的 res_df 和 可视化结果line_chart 建议都保存起来， 方便下次可以快速进入可视化阶段。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">res_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;sentiment_anlysis_result.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">line_chart1</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;line_chart1.html&#39;</span><span class="p">)</span>
<span class="n">line_chart2</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;line_chart2.html&#39;</span><span class="p">)</span>
<span class="n">line_chart3</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;line_chart3.html&#39;</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="资料下载">资料下载</h2>
<ul>
<li><a href="code.ipynb">代码</a></li>
<li><a href="yearly_american_speech_dataset.csv">数据</a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>基于词嵌入技术的心理学研究: 方法及应用</title>
      <link>https://textdata.cn/blog/2023-03-10-psychological-research-with-word-embeddings/</link>
      <pubDate>Fri, 10 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-10-psychological-research-with-word-embeddings/</guid>
      <description>词嵌入是自然语言处理的一项基础技术。 其核心理念是根据大规模语料中词语和上下文的联系, 使用神经网络等机器学习算法自动提取有限维度的语义特征, 将每个词表示为一个低维稠密的数值向量(词向 量), 以用于后续分析。 心理学研究中, 词向量及其衍生的各种语义联系指标可用于探究人类的语义加工、认知判断、发散思维、社会偏见与刻板印象、社会与文化心理变迁等各类问题。 未来, 基于词嵌入技术的心理 学研究需要区分心理的内隐和外显成分, 深化拓展动态词向量和大型预训练语言模型(如 GPT、BERT)的应用, 并在时间和空间维度建立细粒度词向量数据库, 更多开展基于词嵌入的社会变迁和跨文化研究。 As a fundamental technique in natural language processing (NLP), word embedding quantifies a word as a low-dimensional, dense, and continuous numeric vector (i.e., word vector). Word embeddings can be obtained by using machine learning algorithms such as neural networks to predict the surrounding words given a word or vice versa (Word2Vec and FastText) or by predicting the probability of co-occurrence of multiple words (GloVe) in large-scale text corpora. Theoretically, the dimensions of a word vector reflect the pattern of how the word can be predicted in contexts; however, they also connote substantial semantic information of the word. Therefore, word embeddings can be used to analyze semantic meanings of text. In recent years, word embeddings have been increasingly applied to study human psychology, including human semantic processing, cognitive judgment, divergent thinking, social biases and stereotypes, and sociocultural changes at the societal or population level. Future research using word embeddings should (1) distinguish between implicit and explicit components of social cognition, (2) train fine-grained word vectors in terms of time and region to facilitate cross-temporal and cross-cultural research, and (3) apply contextualized word embeddings and large pre-trained language models such as GPT and BERT. To enhance the application of word embeddings in psychology。</description>
      <content:encoded><![CDATA[<p><a href="https://psychbruce.github.io/">包寒吴霜博客 https://psychbruce.github.io/</a></p>
<p><img loading="lazy" src="img/%e5%8c%85%e5%af%92%e5%90%b4%e9%9c%9c.png" alt=""  />
</p>
<br>
<p><img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-01.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-02.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-03.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-04.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-05.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-06.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-07.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-08.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-09.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-10.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-11.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-12.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-13.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>可视化 | 词嵌入模型用于计算社科领域刻板印象等信息（含代码）</title>
      <link>https://textdata.cn/blog/2023-03-03-extracts-cognitive-information-and-visualization-with-embedings/</link>
      <pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-03-extracts-cognitive-information-and-visualization-with-embedings/</guid>
      <description>语言的文字反映了人类思想的结构，使我们能够在个人之间传递思想， 而使用大规模语料训练得来的词嵌入模型蕴含着这类信息。 英文的词嵌入在社会科学中的应用教程较多，大家可以谷歌查询，我主要想丰富中文数据的教程。The words of language reflect the structure of human thought, allowing us to transfer thoughts between individuals, and word embedding models trained using large-scale corpora contain this information. There are many application tutorials of English word embedding in social science. You can search it on Google. I mainly want to enrich the tutorials of Chinese data.</description>
      <content:encoded><![CDATA[<iframe
    src="//player.bilibili.com/player.html?bvid=BV1CY4y11712&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<p>语言的文字反映了人类思想的结构，使我们能够在个人之间传递思想， 而使用大规模语料训练得来的词嵌入模型往往蕴含着这类信息。</p>
<br>
<h3 id="11-国内外社科方面的应用">1.1 国内外社科方面的应用</h3>
<p><strong>在国内社科领域， 应用词嵌入 主要用来做情感分析</strong>，大致的算法(思路)是</p>
<ol>
<li>训练词向量模型</li>
<li>根据词向量cosine或欧几里得距离，度量词语的相似性，进而扩展某种概念词典</li>
<li>检查扩充的概念词典，剔除无效词后。</li>
<li>使用整理好的概念词典，统计文本中出现该概念的词频，当做该概念的代理指标。</li>
</ol>
<p>但词嵌入在国外社科领域， 不用传统方法，使用文本数据，也能做出 **刻板印象、性别种族歧视、词语百年来语义变迁、女性高管就职后公司内性别观念变化、测量创新力(发散思维)**等议题的实证研究。</p>
<p>下图是「阶级财富性别与运动」，摘自2019年文化几何学这篇论文。</p>
<p><img loading="lazy" src="img/sport_class_fortune.png" alt=""  />
</p>
<blockquote>
<p>Kozlowski, Austin C., Matt Taddy, and James A. Evans. &ldquo;The geometry of culture: Analyzing the meanings of class through word embeddings.&rdquo; American Sociological Review 84, no. 5 (2019): 905-949.</p>
</blockquote>
<p>本文主要内容是实现这类文化几何学图的中文可视化。</p>
<br>
<h3 id="12-之前分享过的资料">1.2 之前分享过的资料</h3>
<p>之前大邓分享过的词嵌入稍有涉及，感兴趣的可以阅读我之前分享的文章</p>
<ul>
<li><a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></li>
<li><a href="https://textdata.cn/blog/2022-10-16-python-word-mover-s-distance/">Python | 词移距离(Word Mover’s Distance)</a></li>
<li><a href="https://textdata.cn/blog/wordbias/">WordBias库 | 发现偏见(刻板印象)的交互式工具</a></li>
<li><a href="https://textdata.cn/blog/embeddingsandattitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
<li><a href="https://textdata.cn/blog/whatlies_word2vec/">whatlies库 | 可视化词向量</a></li>
<li><a href="https://textdata.cn/blog//2022-11-14-pnas_naming_unrelated_words_predicts_creativity/">PNAS | 使用语义距离测量一个人的创新力(发散思维)得分</a></li>
<li><a href="https://textdata.cn/blog/embeddings_resource_usage_method/">中文词向量资源汇总 &amp; 使用方法</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
</ul>
<br>
<p>但可视化分享的不多，<strong>本文将用公开的中文预训练模型，验证可视化中文圈的群体记忆、刻板印象、偏见等信息</strong>。在此先放一张论文中两个截图， <strong>语义概念向量</strong> 一般是由语义相反的两组词构成。</p>
<p><img loading="lazy" src="img/size.png" alt=""  />

<img loading="lazy" src="img/%e4%ba%8c%e5%85%83%e6%a6%82%e5%bf%b5%e7%bb%84.png" alt=""  />
</p>
<blockquote>
<p>Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. <strong>Semantic projection recovers rich human knowledge of multiple object features from word embeddings</strong>. <em>Nature Human Behaviour</em>, pp.1-13.</p>
</blockquote>
<br>
<h3 id="概念向量的计算方法">概念向量的计算方法</h3>
<ul>
<li>二维坐标系下，点和向量都可以用二维数组(m, n)表示。同理，在n维空间中，点和向量都是n维数组。</li>
<li>将多个近义的词向量， 通过平均法创建出一个 <strong>均值端点</strong>。</li>
<li>语义完全相反的两个<strong>均值端点</strong>， 通过减法操作， 得到 <strong>概念向量</strong></li>
</ul>
<br>
<h3 id="为啥每个端点向量用多个词计算">为啥每个端点向量用多个词计算？</h3>
<p>单个词变动较大， 为了保证语义的稳定性，最好是找一组词构成概念的一个端点。</p>
<p><br><br></p>
<h2 id="二准备工作">二、准备工作</h2>
<p>下载预训练模型，可以查看这篇文章获取</p>
<p><a href="https://textdata.cn/blog/embeddings_resource_usage_method/">中文词向量资源汇总 &amp; 使用方法</a></p>
<p>之后安装好本节需要的python包</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">whatlies</span><span class="o">==</span><span class="mf">0.7.0</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">matplotlib_inline</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">gensim</span><span class="o">==</span><span class="mf">4.2.0</span>
</code></pre></div><p><br><br></p>
<h2 id="三导入预训练模型">三、导入预训练模型</h2>
<p>使用 gensim 库导入预训练模型，这里我本地保留的是预训练模型是word2vec中的sgns算法训练出来的。 导入后的数据是 KeyedVectors 类型的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1"># 微博 sgns.weibo.word.bz2 为例  </span>
<span class="n">weibo_wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.weibo.word.bz2&#39;</span><span class="p">,</span> 
                                             <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                             <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># 知乎 sgns.renmin.word.bz2</span>
<span class="n">zhihu_wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.zhihu.word.bz2&#39;</span><span class="p">,</span> 
                                              <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                              <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># 中文维基 </span>
<span class="n">wiki_wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.wiki.word.bz2&#39;</span><span class="p">,</span> 
                                              <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                              <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="31-预训练模型的词汇量">3.1 预训练模型的词汇量</h3>
<p>weibo_wv、zhihu_wv、wiki_wv是KeyedVectors类型的数据，可以直接查看词汇量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;预训练模型词汇量&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;微博: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">weibo_wv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;知乎: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">zhihu_wv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;中文维基: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">wiki_wv</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">预训练模型词汇量

微博:  195202
知乎:  259949
中文维基:  352277
</code></pre></div><br>
<h3 id="32-通用词">3.2 通用词</h3>
<p>使用不同数据集训练，得到的语言模型所含词语会有差异。这里我们查看通用词一共有多少</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wiki_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">wiki_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
<span class="n">zhihu_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
<span class="n">weibo_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">weibo_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>

<span class="c1">#交集</span>
<span class="n">common_vocab</span> <span class="o">=</span> <span class="n">wiki_vocab</span> <span class="o">&amp;</span> <span class="n">zhihu_vocab</span> <span class="o">&amp;</span>  <span class="n">weibo_vocab</span><span class="c1"># intersection</span>

<span class="nb">len</span><span class="p">(</span><span class="n">common_vocab</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">118539
</code></pre></div><br>
<h3 id="33-提取某个词的向量">3.3 提取某个词的向量</h3>
<p>以维基百科为例， 查看「幸福」的词向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#只显示向量的前20个数字</span>
<span class="n">wiki_wv</span><span class="p">[</span><span class="s1">&#39;幸福&#39;</span><span class="p">][:</span><span class="mi">20</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 0.159344,  0.280468, -0.236876, -0.198076, -0.170838,  0.027264,
           -0.349646,  0.289169, -0.421038, -0.470539,  0.247534,  0.112968,
            0.355498,  0.479956,  0.093291,  0.081054, -0.046995, -0.624586,
            0.568242,  0.16665 ], dtype=float32)
</code></pre></div><br>
<h3 id="34-查看词向量的维度">3.4 查看词向量的维度</h3>
<p>查看向量的长度（维度），以「幸福」为例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;预训练模型维度数&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;微博: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">weibo_wv</span><span class="p">[</span><span class="s2">&#34;幸福&#34;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;知乎: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">zhihu_wv</span><span class="p">[</span><span class="s2">&#34;幸福&#34;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;中文维基: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">wiki_wv</span><span class="p">[</span><span class="s2">&#34;幸福&#34;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">预训练模型维度数
微博:  300
知乎:  300
中文维基:  300
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#单个词向量的尺寸</span>
<span class="n">wiki_wv</span><span class="p">[</span><span class="s1">&#39;幸福&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(300,)
</code></pre></div><br>
<h3 id="35-计算多个词向量的均值向量">3.5 计算多个词向量的均值向量</h3>
<p>先看一下多个词提取后得到的数据形状</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(4, 300)
</code></pre></div><br>
<p>4个词，每个词都是300维的词向量。如果计算4个词向量的均值向量，返回的尺寸应该是 (300,)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">male_vector</span> <span class="o">=</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">male_vector</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(300,)
</code></pre></div><br>
<h3 id="36-最相似的词">3.6 最相似的词</h3>
<p>网上的教程经常分享最相似的词，这里我们也实验一下。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wiki_wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;于社会&#39;, 0.6210986971855164),
(&#39;社会上&#39;, 0.5995474457740784),
(&#39;社会关系&#39;, 0.5894029140472412),
(&#39;各阶层&#39;, 0.5799717903137207),
(&#39;社会制度&#39;, 0.5777087211608887),
(&#39;社会变迁&#39;, 0.5756841897964478),
(&#39;令社会&#39;, 0.575627326965332),
(&#39;社会变革&#39;, 0.5755838751792908),
(&#39;思想观念&#39;, 0.5752044916152954),
(&#39;社会存在&#39;, 0.573627769947052)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">weibo_wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;金钱至上&#39;, 0.5815222859382629),
(&#39;各阶层&#39;, 0.5668456554412842),
(&#39;福利制度&#39;, 0.5532322525978088),
(&#39;官与民&#39;, 0.5526734590530396),
(&#39;高考制度&#39;, 0.5515810251235962),
(&#39;资源分配&#39;, 0.5500271916389465),
(&#39;功利主义&#39;, 0.5484314560890198),
(&#39;分级制&#39;, 0.5450907349586487),
(&#39;功利化&#39;, 0.5432640910148621),
(&#39;法制建设&#39;, 0.5420899391174316)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;社会存在&#39;, 0.6277482509613037),
 (&#39;社会生活&#39;, 0.613935649394989),
(&#39;社会群体&#39;, 0.6123108863830566),
(&#39;社会意识&#39;, 0.6055717468261719),
(&#39;物欲横流&#39;, 0.6041101217269897),
(&#39;民主决策&#39;, 0.602908194065094),
(&#39;阶级分化&#39;, 0.59609454870224),
(&#39;社会上&#39;, 0.5932644605636597),
(&#39;于社会&#39;, 0.5919737219810486),
(&#39;法制化&#39;, 0.5820874571800232)]
</code></pre></div><p><br><br></p>
<h2 id="四-可视化">四、 可视化</h2>
<p>为了让中文可以在matplotlib正常显示， 需要先运行下方代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>

<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>

<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># 如果是其他系统，可以使用系统默认字体</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>
</code></pre></div><br>
<h3 id="41-运动的贫富和性别属性">4.1 运动的贫富和性别属性</h3>
<p>先看一个最难的例子， 后面的例子都是围绕ta展开的。</p>
<ul>
<li><strong>性别向量</strong> 由 <strong>男性均值端点向量</strong> 和 <strong>女性均值端点向量</strong> 计算得来</li>
<li><strong>贫富向量</strong> 由 <strong>富裕均值端点向量</strong> 和 <strong>贫穷均值端点向量</strong> 计算得来</li>
</ul>
<p>需要注意， 不论是 <strong>性别向量</strong>、<strong>贫富向量</strong> 还是运动词的词向量，都是 300维的向量。 如果在低维空间，例如2维坐标轴中可视化，需要做投影操作。这里需要一点大学线性代数的点乘知识。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 获取需要绘制的单词列表</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;足球&#39;</span><span class="p">,</span> <span class="s1">&#39;拳击&#39;</span><span class="p">,</span> <span class="s1">&#39;高尔夫&#39;</span><span class="p">,</span> <span class="s1">&#39;棒球&#39;</span><span class="p">,</span> <span class="s1">&#39;芭蕾&#39;</span><span class="p">]</span>

<span class="c1"># 获取词向量，并转换为 NumPy 数组</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">wiki_wv</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>

<span class="c1"># 选择两个词向量作为新坐标系的 x 轴和 y 轴</span>
<span class="n">x_axis</span> <span class="o">=</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span>  <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_axis</span> <span class="o">=</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;拮据&#39;</span><span class="p">,</span> <span class="s1">&#39;囊中羞涩&#39;</span><span class="p">,</span> <span class="s1">&#39;困难&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;贵气&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 计算每个词向量在新坐标系中的投影</span>
<span class="n">x_coords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">x_axis</span><span class="p">)</span>
<span class="n">y_coords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">y_axis</span><span class="p">)</span>

<span class="c1"># 绘制图形</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">,</span> <span class="n">y_coords</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="p">(</span><span class="n">x_coords</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_coords</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="c1"># 绘制 x 轴和 y 轴的十字线</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;贫富(贫下富上)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;运动的贫富和性别属性&#39;</span><span class="p">)</span>
<span class="c1">#plt.show()</span>


<span class="c1">#保存</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/运动的贫富和性别属性.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_25_0.png" alt="svg"  />
</p>
<br>
<h3 id="42-使用whatlies处理数据">4.2 使用whatlies处理数据</h3>
<p>上面的可视化代码太长了，使用whatlies可以简化代码量。我们把 KeyedVectors类 转为 EmbeddingSet类，这里就可以更容易的把点显示为带箭头的向量。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">whatlies</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">EmbeddingSet</span>

<span class="c1"># load vectors as whatlies EmbeddingSet</span>
<span class="n">wiki_emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="o">.</span><span class="n">from_names_X</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="n">wiki_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">,</span> 
                                     <span class="n">X</span><span class="o">=</span><span class="n">wiki_wv</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>

<span class="n">weibo_emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="o">.</span><span class="n">from_names_X</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="n">weibo_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">,</span>
                                      <span class="n">X</span> <span class="o">=</span> <span class="n">weibo_wv</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>

<span class="n">zhihu_emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="o">.</span><span class="n">from_names_X</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">,</span> 
                                      <span class="n">X</span><span class="o">=</span><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>

</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># list similar words, n=10</span>
<span class="n">wiki_emb</span><span class="o">.</span><span class="n">score_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(Emb[社会], 5.960464477539063e-08),
(Emb[于社会], 0.3789013624191284),
(Emb[社会上], 0.4004524350166321),
(Emb[社会关系], 0.410597026348114),
(Emb[各阶层], 0.42002809047698975),
(Emb[社会制度], 0.4222911596298218),
(Emb[社会变迁], 0.42431581020355225),
(Emb[令社会], 0.42437267303466797),
(Emb[社会变革], 0.424416184425354),
(Emb[思想观念], 0.4247954487800598)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">weibo_emb</span><span class="o">.</span><span class="n">score_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(Emb[社会], 0.0),
(Emb[金钱至上], 0.41847753524780273),
(Emb[各阶层], 0.43315428495407104),
(Emb[福利制度], 0.4467676877975464),
(Emb[官与民], 0.4473266005516052),
(Emb[高考制度], 0.4484187364578247),
(Emb[资源分配], 0.44997286796569824),
(Emb[功利主义], 0.4515683650970459),
(Emb[分级制], 0.45490920543670654),
(Emb[功利化], 0.4567357897758484)]
</code></pre></div><br>
<h3 id="43-whatlies默认可视化">4.3 whatlies默认可视化</h3>
<p>使用whatlies默认的效果绘制如下，但需要注意， 这里的Dimension0和Dimension1的含义是未知的。所以除了可视化， 含义解读起来比较困难。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># the default X and Y axes are the first two dimensions of the embedding vectors</span>
<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;马&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> 
                   <span class="n">color</span><span class="o">=</span><span class="s2">&#34;purple&#34;</span><span class="p">)</span>

<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;鲨鱼&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;blue&#34;</span><span class="p">)</span>

<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;鸟类&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;green&#34;</span><span class="p">)</span>
<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;人&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">)</span>
<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;蛇&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/output_30_0.svg" alt="svg"  />
</p>
<br>
<h3 id="44-使用端点向量当基向量">4.4 使用端点向量当基向量</h3>
<p>使用端点向量当基向量，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;美国&#34;</span><span class="p">,</span> <span class="s2">&#34;中国&#34;</span><span class="p">,</span> <span class="s2">&#34;俄罗斯&#34;</span><span class="p">,</span> <span class="s2">&#34;韩国&#34;</span><span class="p">]</span>

<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span><span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;弱小&#34;</span><span class="p">],</span> 
          <span class="n">y_axis</span><span class="o">=</span><span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;强大&#34;</span><span class="p">],</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">])</span>

<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;国家强弱&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/国家强弱.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_32_0.svg" alt="svg"  />
</p>
<br>
<p>按照我们的理解， 强大与弱小是方向相反的。但是如果将这两个词分别做基向量。如图所示，就体现不出方向。</p>
<p>同时，因为给定有意义的基向量作为坐标轴向量， 坐标轴含有了意义，可视化的结果可以看出语义信息的亲疏远近。</p>
<p>可以看到， 中美俄是大国强国，韩国是小国军事弱国。</p>
<br>
<h3 id="45-使用概念向量当做基向量">4.5 使用概念向量当做基向量</h3>
<p>当使用概念向量做基向量， 我们就能保留住词语之间的正反方向。避免 4.4 反义词之间无法体现方向性信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#待考察词</span>
<span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s1">&#39;足球&#39;</span><span class="p">,</span> <span class="s2">&#34;斗殴&#34;</span><span class="p">,</span> <span class="s1">&#39;高尔夫&#39;</span><span class="p">,</span> <span class="s1">&#39;篮球&#39;</span><span class="p">,</span> <span class="s1">&#39;芭蕾&#39;</span><span class="p">,</span> <span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;乒乓球&#39;</span><span class="p">,</span> <span class="s1">&#39;举重&#39;</span><span class="p">]</span>

<span class="c1">#性别概念向量</span>
<span class="n">sex_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>
<span class="c1">#贫富概念向量</span>
<span class="n">disparity_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;贵气&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;拮据&#39;</span><span class="p">,</span> <span class="s1">&#39;囊中羞涩&#39;</span><span class="p">,</span> <span class="s1">&#39;困难&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> 

<span class="c1">#概念向量 做 基向量</span>
<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span> <span class="n">sex_vector</span><span class="p">,</span> 
          <span class="n">y_axis</span><span class="o">=</span> <span class="n">disparity_vector</span><span class="p">,</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">])</span>


<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;运动中体现的贫富与性别信息&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;贫富(贫下富上)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/运动中体现的贫富与性别信息.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_34_0.svg" alt="svg"  />
</p>
<br>
<p>刚刚的图中加入了<code>男、女、贫穷、富裕</code>四个词，是为了帮助我们识别出方向来的，判断横纵坐标的含义和方向性。现在我们可以去掉这四个词，绘制更美观的图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s1">&#39;足球&#39;</span><span class="p">,</span> <span class="s1">&#39;高尔夫&#39;</span><span class="p">,</span> <span class="s1">&#39;篮球&#39;</span><span class="p">,</span> <span class="s1">&#39;芭蕾&#39;</span><span class="p">,</span> <span class="s1">&#39;乒乓球&#39;</span><span class="p">,</span> <span class="s1">&#39;举重&#39;</span><span class="p">,</span> <span class="s1">&#39;徒步&#39;</span><span class="p">]</span>

<span class="n">sex_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> 
<span class="n">disparity_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;贵气&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;拮据&#39;</span><span class="p">,</span> <span class="s1">&#39;囊中羞涩&#39;</span><span class="p">,</span> <span class="s1">&#39;困难&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> 

<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span> <span class="n">sex_vector</span><span class="p">,</span> 
          <span class="n">y_axis</span><span class="o">=</span> <span class="n">disparity_vector</span><span class="p">,</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">])</span>

<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;运动中体现的贫富与性别信息&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="c1">#plt.axis(&#39;off&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;贫富(贫下富上)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/运动中体现的贫富与性别信息.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_36_0.png" alt="svg"  />
</p>
<br>
<p>从上图可以看出， 在wiki百科中记录下的， 我们对不同运动是存在贫富、性别化的信息。这些信息根据研究场景，解读为<strong>刻板印象、态度偏好、文化记忆</strong>等。 我们再看一个例子， 把中国动物(含神兽)分别在性别维度和尺寸维度可视化。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s1">&#39;虎&#39;</span><span class="p">,</span> <span class="s1">&#39;龙&#39;</span><span class="p">,</span> <span class="s1">&#39;猫&#39;</span><span class="p">,</span> <span class="s1">&#39;燕子&#39;</span><span class="p">,</span> <span class="s1">&#39;蝴蝶&#39;</span><span class="p">]</span>

<span class="n">sex_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span><span class="o">-</span><span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>
<span class="n">size_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;大&#39;</span><span class="p">,</span> <span class="s1">&#39;庞&#39;</span><span class="p">,</span> <span class="s1">&#39;巨&#39;</span><span class="p">,</span> <span class="s1">&#39;高&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;矮&#39;</span><span class="p">,</span> <span class="s1">&#39;小&#39;</span><span class="p">,</span> <span class="s1">&#39;微&#39;</span><span class="p">,</span> <span class="s1">&#39;毫&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>


<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span> <span class="n">sex_vector</span><span class="p">,</span> 
          <span class="n">y_axis</span><span class="o">=</span> <span class="n">size_vector</span><span class="p">,</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">,</span> <span class="s2">&#34;yellow&#34;</span><span class="p">,</span> <span class="s2">&#34;grey&#34;</span><span class="p">])</span>

<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;文化中动物词蕴含的性别化和尺寸信息&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="c1">#plt.axis(&#39;off&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;尺寸(下小上大)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/文化中动物词蕴含的性别化和尺寸信息.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_38_0.svg" alt="svg"  />
</p>
<p><br><br></p>
<h2 id="代码获取">代码获取</h2>
<p>公众号:  大邓和他的Python， 同日期推文， 付费阅读获取全文教程、数据、代码~</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>simpleT5 库 | 根据英文摘要内容生成标题</title>
      <link>https://textdata.cn/blog/2023-02-23-simplet5-one-line-summary/</link>
      <pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-02-23-simplet5-one-line-summary/</guid>
      <description>T5（Text-to-Text Transfer Transformer）是一种基于 Transformer 架构的自然语言处理模型，由 Google Brain 团队开发。T5 模型采用了 encoder-decoder 架构，其中 encoder 将输入文本编码为向量，decoder 则从该向量生成目标文本。T5 模型的特点是将所有自然语言处理任务都视为“从输入文本到输出文本”的转换问题，它可以通过在任务之间共享模型参数和预训练模型来轻松地应用于各种 NLP 任务，如**文本分类、命名实体识别、文本摘要、问答系统**等。 与其他 NLP 模型不同的是，T5 模型使用了一种称为“text-to-text”方法的统一输入输出架构，使得所有 NLP 任务都能转化为文本转换问题，从而使得模型训练更加高效。</description>
      <content:encoded><![CDATA[<p>simpleT5 是基于 PyTorch 实现的 T5 模型库，旨在为用户提供一种简单、易用、可定制的 T5 模型工具。T5（Text-to-Text Transfer Transformer）是一种基于 Transformer 架构的自然语言处理模型，由 Google Brain 团队开发。T5 模型采用了 encoder-decoder 架构，其中 encoder 将输入文本编码为向量，decoder 则从该向量生成目标文本。</p>
<p><img loading="lazy" src="img/new_text_to_text.jpg" alt=""  />
</p>
<p>simpleT5 的设计目标是尽可能地减少 T5 模型的使用门槛，以方便用户在自然语言处理任务中快速应用 T5 模型，从而节省大量的模型开发时间和成本。</p>
<p>simpleT5 提供了一个简单的 API 接口，用户只需要提供输入文本和模型参数，即可轻松地使用 T5 模型进行文本转换任务，如<strong>文本摘要、机器翻译、对话系统</strong>等。simpleT5 还提供了一些预训练模型，包括 T5-small、T5-base 和 T5-large 等不同规模的模型，用户可以根据任务需求选择合适的模型。</p>
<p>除此之外，simpleT5 还提供了一些有用的工具和功能，如文本预处理、数据集加载、训练日志记录等，以帮助用户更轻松地进行模型训练和调试。simpleT5 的开发者们还提供了详细的文档和示例代码，以帮助用户更快地上手使用该库。</p>
<p>总之，simpleT5 为用户提供了一种快速、方便、可定制的 T5 模型工具，可以帮助用户在自然语言处理任务中更加高效地应用 T5 模型，节省大量的开发时间和成本。</p>
<p><br><br></p>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="n">simplet5</span>
</code></pre></div><p><br><br></p>
<h2 id="快速上手">快速上手</h2>
<p>t5模型有很多，如下图，今天以huggingface中公开的模型 <strong>snrspeaks/t5-one-line-summary为例， 展示 「根据传入的摘要内容生成对应的标题」。</strong></p>
<p><img loading="lazy" src="img/t5-models.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># pip install --upgrade simplet5</span>
<span class="kn">from</span> <span class="nn">simplet5</span> <span class="kn">import</span> <span class="n">SimpleT5</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleT5</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&#34;t5&#34;</span><span class="p">,</span><span class="s2">&#34;snrspeaks/t5-one-line-summary&#34;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    Global seed set to 42
    Downloading:   100%|          | 0.00/1.36k [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/850M [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/1.84k [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/773k [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/1.32M [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/1.74k [00:00&lt;?, ?B/s]
</code></pre></div><br>
<p>根据英文摘要生成标题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">abstract</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production 
</span><span class="s2">machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and 
</span><span class="s2">handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a 
</span><span class="s2">set of novel high-level, declarative abstractions. Overton&#39;s vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. 
</span><span class="s2">In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, 
</span><span class="s2">Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, 
</span><span class="s2">Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">abstract</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['Overton: Building, Deploying, and Monitoring Deep Machine Learning Systems']
</code></pre>
<br>
<p>根据摘要生成多个标题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">abstract</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production 
</span><span class="s2">machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and 
</span><span class="s2">handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a 
</span><span class="s2">set of novel high-level, declarative abstractions. Overton&#39;s vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. 
</span><span class="s2">In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, 
</span><span class="s2">Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, 
</span><span class="s2">Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="c1">#根据摘要生成5个标题</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">abstract</span><span class="p">,</span> 
              <span class="n">num_return_sequences</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> 
              <span class="n">num_beams</span><span class="o">=</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<pre><code>['Overton: Building, Deploying, and Monitoring Deep Machine Learning Systems',
 'Overton: Building, Deployment, and Improving Production Machine Learning Systems',
 'Overton: Building, Deploying, and Monitoring Machine Learning Systems for Engineers',
 'Overton: Building, Deploying, and Monitoring Machine Learning Systems',
 'Overton: Building, Deployment, and Monitoring Deep Machine Learning Systems']
</code></pre>
<p><br><br></p>
<h2 id="simplet5微调">simpleT5微调</h2>
<p>在 T5 模型的预训练阶段，它使用了巨大的文本语料库进行无监督的训练，以学习将输入文本转换为输出文本的能力。</p>
<p>预训练阶段结束后，T5 模型可以通过微调或迁移学习的方式用于各种下游 NLP 任务中，以实现最先进的性能表现。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">path</span> <span class="o">=</span> <span class="s2">&#34;https://raw.githubusercontent.com/Shivanandroy/T5-Finetuning-PyTorch/main/data/news_summary.csv&#34;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># simple5库传入的数据是DataFrames，必须含 &#34;source_text&#34; 和 &#34;target_text&#34;这两个字段。</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;headlines&#34;</span><span class="p">:</span><span class="s2">&#34;target_text&#34;</span><span class="p">,</span> <span class="s2">&#34;text&#34;</span><span class="p">:</span><span class="s2">&#34;source_text&#34;</span><span class="p">})</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;source_text&#39;</span><span class="p">,</span> <span class="s1">&#39;target_text&#39;</span><span class="p">]]</span>

<span class="c1"># T5 模型微调时候，source_text 数据都加入了前缀关键词summarise， 告诉 T5模型要做总结类任务的微调。</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;source_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;summarize: &#34;</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;source_text&#39;</span><span class="p">]</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<p>查看数据的形状</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_df</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">((78720, 2), (19681, 2))
</code></pre></div><br>
<p>开始进行 T5 模型的微调</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">simplet5</span> <span class="kn">import</span> <span class="n">SimpleT5</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleT5</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s2">&#34;t5&#34;</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&#34;t5-base&#34;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_df</span><span class="o">=</span><span class="n">train_df</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span>
            <span class="n">eval_df</span><span class="o">=</span><span class="n">test_df</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> 
            <span class="n">source_max_token_len</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> 
            <span class="n">target_max_token_len</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Downloading: 100%
792k/792k [00:36&lt;00:00, 21.6kB/s]

Downloading: 100%
1.39M/1.39M [00:02&lt;00:00, 641kB/s]

Downloading: 100%
1.20k/1.20k [00:00&lt;00:00, 3.50kB/s]

Downloading: 100%
892M/892M [00:32&lt;00:00, 27.4MB/s]

GPU available: True, used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M 
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Validation sanity check: 0%
0/2 [22:52&lt;?, ?it/s]
Global seed set to 42
Epoch 2: 100%
638/638 [04:07&lt;00:00, 2.57it/s, loss=1.02, v_num=0, val_loss=1.200, train_loss=1.130]
Validating: 100%
13/13 [00:01&lt;00:00, 7.43it/s]
Validating: 100%
13/13 [00:01&lt;00:00, 7.29it/s]
Validating: 100%
13/13 [00:01&lt;00:00, 7.30it/s]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># let&#39;s load the trained model for inferencing:</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&#34;t5&#34;</span><span class="p">,</span><span class="s2">&#34;outputs/SimpleT5-epoch-2-train-loss-0.9478&#34;</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">text_to_summarize</span><span class="o">=</span><span class="s2">&#34;&#34;&#34;summarize: Rahul Gandhi has replied to Goa CM Manohar Parrikar&#39;s letter, 
</span><span class="s2">which accused the Congress President of using his &#34;visit to an ailing man for political gains&#34;. 
</span><span class="s2">&#34;He&#39;s under immense pressure from the PM after our meeting and needs to demonstrate his loyalty by attacking me,&#34; 
</span><span class="s2">Gandhi wrote in his letter. Parrikar had clarified he didn&#39;t discuss Rafale deal with Rahul.
</span><span class="s2">&#34;&#34;&#34;</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">text_to_summarize</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;Rahul responds to Goa CM accusing him of using visit for political gain&#39;]
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>正则表达式 | 词频统计、情感分析、融资约束</title>
      <link>https://textdata.cn/blog/2023-02-18-regex-expression-examples/</link>
      <pubDate>Mon, 13 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-02-18-regex-expression-examples/</guid>
      <description>&lt;h2 id=&#34;一统计某类词出现次数&#34;&gt;一、统计某类词出现次数&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 定义词语列表&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;word_list&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;融资&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;资金&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;筹资&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;投资&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;投入资金&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 将词语列表转换为正则表达式模式&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;|&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 定义待匹配的中文文本&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;今年公司的融资计划受到了资金压力的影响，筹资难度较大。&amp;#34;&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 使用 re.findall() 方法统计出现次数&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 打印结果&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;3
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h3 id=&#34;注意&#34;&gt;注意：&lt;/h3&gt;
&lt;p&gt;使用正则表达式统计某类词的出现次数的效率取决于正则表达式的复杂度和文本的大小。对于较小的文本和简单的正则表达式，使用正则表达式可以提供一个简单的解决方案。但是对于较大的文本和复杂的正则表达式，可能会导致性能问题。&lt;/p&gt;
&lt;p&gt;如果需要更快的实现方法，可以考虑使用字符串查找和替换函数，例如Python中的str.count()方法，该方法可以计算一个字符串中出现子字符串的次数。通过遍历所有需要查找的词汇并分别计算其出现次数，可以得出所有词汇的出现次数。这种方法可以避免正则表达式的匹配成本，并且通常比使用正则表达式更快。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 定义词语列表&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;融资&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;资金&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;筹资&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;投资&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;投入资金&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 定义待匹配的中文文本&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;今年公司的融资计划受到了资金压力的影响，筹资难度较大。&amp;#34;&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#words类词 出现次数&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h2 id=&#34;二-情感分析&#34;&gt;二、 情感分析&lt;/h2&gt;
&lt;p&gt;正则表达式也可以用于情感分析。下面是一个使用正则表达式实现情感分析的示例代码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先定义了一个包含积极情感词和消极情感词的列表。&lt;/li&gt;
&lt;li&gt;然后，使用正则表达式识别文本中出现的积极情感词和消极情感词，并计算它们出现的次数。&lt;/li&gt;
&lt;li&gt;最后，通过比较积极词和消极词的出现次数来确定情感分析结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 情感词列表，这里只列出了一些示例&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;positive_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;喜欢&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;赞&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;棒&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;好&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;美&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;赏心悦目&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;negative_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;讨厌&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;踩&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;差&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;烂&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;丑&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;不满意&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 使用正则表达式识别文本中的情感词&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sentiment_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;positive_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;|&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;positive_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;negative_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;|&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;negative_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;positive_count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;negative_count&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 测试示例&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;我很喜欢这个餐厅，菜品棒极了！&amp;#39;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;这部电影很烂，演员演得太差了。&amp;#39;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;这个景点很美，值得去看看。&amp;#39;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text4&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;这家店的服务太差了，不满意。&amp;#39;&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sentiment_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 输出 2&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sentiment_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 输出 -2&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sentiment_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 输出 1&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sentiment_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 输出 -1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;2
-2
1
-2
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;nouns&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;GDP&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;CPI&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;PPI&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;利率&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;政策&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pos_adj&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;增长&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;提高&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;优化&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;neg_adj&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;下降&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;降低&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;恶化&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sentiment_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;noun&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nouns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;noun&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;match&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adj&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pos_adj&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adj&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adj&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;neg_adj&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;adj&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;中国GDP增长预期下调，影响市场情绪&amp;#39;&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sentiment_analysis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h2 id=&#34;三融资约束&#34;&gt;三、融资约束&lt;/h2&gt;
&lt;h3 id=&#34;31-识别融资约束句子&#34;&gt;3.1 识别融资约束句子&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;姜付秀,王运通,田园,吴恺.多个大股东与企业融资约束——基于文本分析的经验证据[J].管理世界,2017,(12):61-74.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;摘要: 本文采用文本分析方法构建了融资约束指标,在此基础上,实证检验了多个大股东对企业融资约束的影响以及相应的作用机理。我们发现,多个大股东的公司有着较低的融资约束水平。该结论在控制内生性情况下依然成立。中介效应模型的检验结果表明,其他大股东通过抑制控股股东的掏空行为降低了企业融资约束。进一步的研究结果表明,在其他大股东具有较强的监督动机和监督能力(大股东数量更多、持股数量之和更大、大股东之间不容易合谋)、及更好的外部环境(信息环境、法律环境)时,公司的融资约束水平更低,这些发现在逻辑上为其他大股东的监督假说提供证据支持的同时,也表明大股东发挥监督作用降低企业融资约束需要一定条件。本文为完善中国情景下的融资约束指标构建、更好度量中国企业融资约束提供了有益参考;同时,为股权结构安排的经济后果提供了新的证据支持。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 融资不足情况：&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;regex1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;(?:融资|资金|筹资)[^。]{0,6}?(?:难以|不能|无法|不足以)[^。]*&amp;#34;&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 融资成本或压力过大情况：&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;regex2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;(?:融资|资金|筹资)[^。]{0,6}?(?:成本|压力|难度)[^。]{0,4}?(?:升|增|高|大)[^。]*&amp;#34;&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 使用“或”运算符, 将这些正则表达式组合成一个大的正则表达式&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;(&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;regex1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;)|(&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;regex2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;)&amp;#34;&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;这是一段融资方面的文本，其中提到了资金不足的情况，还提到了融资成本过高的问题。&amp;#34;&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;[(&#39;&#39;, &#39;融资成本过高的问题&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;32-出现融资约束句子的数量&#34;&gt;3.2 出现融资约束句子的数量&lt;/h3&gt;
&lt;p&gt;pattern可以应用的A股上市公司md&amp;amp;a部分的融资压力句子的识别，其实稍微改动就可以测量md&amp;amp;a中企业关于融资压力句子的出现次数。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;finance_constrain&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# 融资不足情况：&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;regex1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;(?:融资|资金|筹资)[^。]{0,6}?(?:难以|不能|无法|不足以)[^。]*&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# 融资成本或压力过大情况：&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;regex2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;(?:融资|资金|筹资)[^。]{0,6}?(?:成本|压力|难度)[^。]{0,4}?(?:升|增|高|大)[^。]*&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# 使用“或”运算符, 将这些正则表达式组合成一个大的正则表达式&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;(&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;regex1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;)|(&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;regex2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;)&amp;#34;&lt;/span&gt;


    &lt;span class=&#34;c1&#34;&gt;# 将 pattern 编译成正则表达式对象&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;regex&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;compile&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pattern&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;


    &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;line&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;match&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;regex&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;line&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;match&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;支持开票 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一统计某类词出现次数">一、统计某类词出现次数</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>

<span class="c1"># 定义词语列表</span>
<span class="n">word_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;融资&#39;</span><span class="p">,</span> <span class="s1">&#39;资金&#39;</span><span class="p">,</span> <span class="s1">&#39;筹资&#39;</span><span class="p">,</span> <span class="s1">&#39;投资&#39;</span><span class="p">,</span> <span class="s1">&#39;投入资金&#39;</span><span class="p">]</span>

<span class="c1"># 将词语列表转换为正则表达式模式</span>
<span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;|&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">word_list</span><span class="p">)</span>

<span class="c1"># 定义待匹配的中文文本</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&#34;今年公司的融资计划受到了资金压力的影响，筹资难度较大。&#34;</span>

<span class="c1"># 使用 re.findall() 方法统计出现次数</span>
<span class="n">count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span>

<span class="c1"># 打印结果</span>
<span class="nb">print</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
</code></pre></div><pre><code>3
</code></pre>
<br>
<h3 id="注意">注意：</h3>
<p>使用正则表达式统计某类词的出现次数的效率取决于正则表达式的复杂度和文本的大小。对于较小的文本和简单的正则表达式，使用正则表达式可以提供一个简单的解决方案。但是对于较大的文本和复杂的正则表达式，可能会导致性能问题。</p>
<p>如果需要更快的实现方法，可以考虑使用字符串查找和替换函数，例如Python中的str.count()方法，该方法可以计算一个字符串中出现子字符串的次数。通过遍历所有需要查找的词汇并分别计算其出现次数，可以得出所有词汇的出现次数。这种方法可以避免正则表达式的匹配成本，并且通常比使用正则表达式更快。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 定义词语列表</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;融资&#39;</span><span class="p">,</span> <span class="s1">&#39;资金&#39;</span><span class="p">,</span> <span class="s1">&#39;筹资&#39;</span><span class="p">,</span> <span class="s1">&#39;投资&#39;</span><span class="p">,</span> <span class="s1">&#39;投入资金&#39;</span><span class="p">]</span>

<span class="c1"># 定义待匹配的中文文本</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&#34;今年公司的融资计划受到了资金压力的影响，筹资难度较大。&#34;</span>

<span class="c1">#words类词 出现次数</span>
<span class="n">count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="二-情感分析">二、 情感分析</h2>
<p>正则表达式也可以用于情感分析。下面是一个使用正则表达式实现情感分析的示例代码：</p>
<ul>
<li>首先定义了一个包含积极情感词和消极情感词的列表。</li>
<li>然后，使用正则表达式识别文本中出现的积极情感词和消极情感词，并计算它们出现的次数。</li>
<li>最后，通过比较积极词和消极词的出现次数来确定情感分析结果。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>

<span class="c1"># 情感词列表，这里只列出了一些示例</span>
<span class="n">positive_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;喜欢&#39;</span><span class="p">,</span> <span class="s1">&#39;赞&#39;</span><span class="p">,</span> <span class="s1">&#39;棒&#39;</span><span class="p">,</span> <span class="s1">&#39;好&#39;</span><span class="p">,</span> <span class="s1">&#39;美&#39;</span><span class="p">,</span> <span class="s1">&#39;赏心悦目&#39;</span><span class="p">]</span>
<span class="n">negative_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;讨厌&#39;</span><span class="p">,</span> <span class="s1">&#39;踩&#39;</span><span class="p">,</span> <span class="s1">&#39;差&#39;</span><span class="p">,</span> <span class="s1">&#39;烂&#39;</span><span class="p">,</span> <span class="s1">&#39;丑&#39;</span><span class="p">,</span> <span class="s1">&#39;不满意&#39;</span><span class="p">]</span>

<span class="c1"># 使用正则表达式识别文本中的情感词</span>
<span class="k">def</span> <span class="nf">sentiment_analysis</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">positive_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;|&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">positive_words</span><span class="p">),</span> <span class="n">text</span><span class="p">))</span>
    <span class="n">negative_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;|&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">negative_words</span><span class="p">),</span> <span class="n">text</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">positive_count</span> <span class="o">-</span> <span class="n">negative_count</span>

<span class="c1"># 测试示例</span>
<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;我很喜欢这个餐厅，菜品棒极了！&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;这部电影很烂，演员演得太差了。&#39;</span>
<span class="n">text3</span> <span class="o">=</span> <span class="s1">&#39;这个景点很美，值得去看看。&#39;</span>
<span class="n">text4</span> <span class="o">=</span> <span class="s1">&#39;这家店的服务太差了，不满意。&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment_analysis</span><span class="p">(</span><span class="n">text1</span><span class="p">))</span>  <span class="c1"># 输出 2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment_analysis</span><span class="p">(</span><span class="n">text2</span><span class="p">))</span>  <span class="c1"># 输出 -2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment_analysis</span><span class="p">(</span><span class="n">text3</span><span class="p">))</span>  <span class="c1"># 输出 1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment_analysis</span><span class="p">(</span><span class="n">text4</span><span class="p">))</span>  <span class="c1"># 输出 -1</span>
</code></pre></div><pre><code>2
-2
1
-2
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>

<span class="n">nouns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;GDP&#39;</span><span class="p">,</span> <span class="s1">&#39;CPI&#39;</span><span class="p">,</span> <span class="s1">&#39;PPI&#39;</span><span class="p">,</span> <span class="s1">&#39;利率&#39;</span><span class="p">,</span> <span class="s1">&#39;政策&#39;</span><span class="p">]</span>
<span class="n">pos_adj</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;增长&#39;</span><span class="p">,</span> <span class="s1">&#39;提高&#39;</span><span class="p">,</span> <span class="s1">&#39;优化&#39;</span><span class="p">]</span>
<span class="n">neg_adj</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;下降&#39;</span><span class="p">,</span> <span class="s1">&#39;降低&#39;</span><span class="p">,</span> <span class="s1">&#39;恶化&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">sentiment_analysis</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">noun</span> <span class="ow">in</span> <span class="n">nouns</span><span class="p">:</span>
        <span class="n">matches</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">noun</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">match</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">adj</span> <span class="ow">in</span> <span class="n">pos_adj</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">adj</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
                    <span class="n">score</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">for</span> <span class="n">adj</span> <span class="ow">in</span> <span class="n">neg_adj</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">adj</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
                    <span class="n">score</span> <span class="o">-=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">score</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;中国GDP增长预期下调，影响市场情绪&#39;</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">sentiment_analysis</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

</code></pre></div><pre><code>1
</code></pre>
<br>
<h2 id="三融资约束">三、融资约束</h2>
<h3 id="31-识别融资约束句子">3.1 识别融资约束句子</h3>
<p><strong>姜付秀,王运通,田园,吴恺.多个大股东与企业融资约束——基于文本分析的经验证据[J].管理世界,2017,(12):61-74.</strong></p>
<p>摘要: 本文采用文本分析方法构建了融资约束指标,在此基础上,实证检验了多个大股东对企业融资约束的影响以及相应的作用机理。我们发现,多个大股东的公司有着较低的融资约束水平。该结论在控制内生性情况下依然成立。中介效应模型的检验结果表明,其他大股东通过抑制控股股东的掏空行为降低了企业融资约束。进一步的研究结果表明,在其他大股东具有较强的监督动机和监督能力(大股东数量更多、持股数量之和更大、大股东之间不容易合谋)、及更好的外部环境(信息环境、法律环境)时,公司的融资约束水平更低,这些发现在逻辑上为其他大股东的监督假说提供证据支持的同时,也表明大股东发挥监督作用降低企业融资约束需要一定条件。本文为完善中国情景下的融资约束指标构建、更好度量中国企业融资约束提供了有益参考;同时,为股权结构安排的经济后果提供了新的证据支持。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>

<span class="c1"># 融资不足情况：</span>
<span class="n">regex1</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(?:融资|资金|筹资)[^。]{0,6}?(?:难以|不能|无法|不足以)[^。]*&#34;</span>
<span class="c1"># 融资成本或压力过大情况：</span>
<span class="n">regex2</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(?:融资|资金|筹资)[^。]{0,6}?(?:成本|压力|难度)[^。]{0,4}?(?:升|增|高|大)[^。]*&#34;</span>
<span class="c1"># 使用“或”运算符, 将这些正则表达式组合成一个大的正则表达式</span>
<span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(&#34;</span> <span class="o">+</span> <span class="n">regex1</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&#34;)|(&#34;</span> <span class="o">+</span> <span class="n">regex2</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&#34;)&#34;</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">&#34;这是一段融资方面的文本，其中提到了资金不足的情况，还提到了融资成本过高的问题。&#34;</span>

<span class="n">matches</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">matches</span><span class="p">)</span>
</code></pre></div><pre><code>[('', '融资成本过高的问题')]
</code></pre>
<h3 id="32-出现融资约束句子的数量">3.2 出现融资约束句子的数量</h3>
<p>pattern可以应用的A股上市公司md&amp;a部分的融资压力句子的识别，其实稍微改动就可以测量md&amp;a中企业关于融资压力句子的出现次数。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>


<span class="k">def</span> <span class="nf">finance_constrain</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># 融资不足情况：</span>
    <span class="n">regex1</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(?:融资|资金|筹资)[^。]{0,6}?(?:难以|不能|无法|不足以)[^。]*&#34;</span>
    <span class="c1"># 融资成本或压力过大情况：</span>
    <span class="n">regex2</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(?:融资|资金|筹资)[^。]{0,6}?(?:成本|压力|难度)[^。]{0,4}?(?:升|增|高|大)[^。]*&#34;</span>
    <span class="c1"># 使用“或”运算符, 将这些正则表达式组合成一个大的正则表达式</span>
    <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&#34;(&#34;</span> <span class="o">+</span> <span class="n">regex1</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&#34;)|(&#34;</span> <span class="o">+</span> <span class="n">regex2</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&#34;)&#34;</span>


    <span class="c1"># 将 pattern 编译成正则表达式对象</span>
    <span class="n">regex</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">pattern</span><span class="p">)</span>


    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">match</span> <span class="o">=</span> <span class="n">regex</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">match</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">count</span>
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 大数据时代的「计算文化心理学」</title>
      <link>https://textdata.cn/blog/2023-02-13-computing-cultural-psychology-with-big-data/</link>
      <pubDate>Mon, 13 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-02-13-computing-cultural-psychology-with-big-data/</guid>
      <description>随着大数据技术与文化心理学的融合发展, 计算文化心理学作为一门新兴交叉学科逐渐兴起, 大尺 度、近乎全样本的文化心理分析真正得以实现。计算文化心理学关注的文化变量主要围绕个人主义/集体主义 这一文化心理学使用最为广泛的维度展开, 分析方法包括特征词典、机器学习、社会网络分析、仿真模拟等, 分 析思路包括时间维度上的文化变迁效应以及空间维度上的文化地理效应。 当然, 计算文化心理学在为传统文 化心理研究提供新方法、新范式的同时, 也存在解码失真、样本偏差、词语多义性、隐私风险等局限, 未来研 究应重视变量理论解释、文化动态演化分析、学科深度整合、生态效度等问题。</description>
      <content:encoded><![CDATA[<h2 id="推荐理由">推荐理由</h2>
<p><a href="https://textdata.cn/blog/management_python_course/"><strong>「课程 | Python实证指标构建与文本分析」</strong></a> 一直是面向经管、心理学、社会学等计算社会科学的一门综合性课程。 希望能学技术的同时，也多了解技术应用背后的方法论、认识论，大数据导向的研究必能事半功倍。</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">吴胜涛,茅云云,吴舒涵,冯健仁,张庆鹏,谢天,陈浩,朱廷劭.基于大数据的文化心理分析[J].心理科学进展:1-13.
</code></pre></div><p><strong>摘要</strong>: 随着大数据技术与文化心理学的融合发展, 计算文化心理学作为一门新兴交叉学科逐渐兴起, 大尺度、近乎全样本的文化心理分析真正得以实现。计算文化心理学关注的文化变量主要围绕个人主义/集体主义这一文化心理学使用最为广泛的维度展开, 分析方法包括特征词典、机器学习、社会网络分析、仿真模拟等, 分析思路包括时间维度上的文化变迁效应以及空间维度上的文化地理效应。 当然, 计算文化心理学在为传统文 化心理研究提供新方法、新范式的同时, 也存在解码失真、样本偏差、词语多义性、隐私风险等局限, 未来研 究应重视变量理论解释、文化动态演化分析、学科深度整合、生态效度等问题。</p>
<p><strong>关键词: 大数据, 文化, 计算文化心理学, 文化变迁, 文化地理</strong></p>
<p><br><br></p>
<h2 id="一大数据文化心理学">一、大数据文化心理学</h2>
<p>近 10 年的大量文献来看, <strong>大数据技术已经成为当前哲学、自然科学和 社会科学共同关注的重要问题,  为人文与社会科学提供了继实验、理论、仿真之后的第四种研究范式,  并催生了计算社会科学</strong>的诞生(Gray, 2009; Lazer et al., 2009)。</p>
<p>随着大数据时代的到来, 传统文化心理学面临诸多挑战, 但也迎来了范式变革的机遇。</p>
<p>首先, 互联网将人与人之间的距离拉近, 使得不同文化背景的人主动或被动地汇聚在一个时空压缩的“地球村”里, 全球范围的文化互动、变迁成为常态, 但由于研究资料受限、分析方法滞后, <strong>传统文化心理学在进行大尺度时间、空间分析时费时费力。幸运的是, 信息技术的发展使得不同时间、空间的文化符号及其互动过程留下了海量的数据印记, 这为研究者分析不同时间、不同空间下的文化心理特征提供了无可比拟的大规模生态数据库。</strong> 进而, 大数据技术赋能研究者们方便地抓取网络生态数据, 研究文化动态演化及节点事件效应(Park, Baek et al., 2014; Wu, Zhou et al., 2018), 使得时间趋势和空间地理上的大尺度分析真正成为可能, 文化心理学的研究问题也得以拓展。</p>
<p>其次, 大数据时代文化符号生产快速增长, 但是基于纸笔测验和行为实验的传统文化心理学 研究往往是滞后的, 且样本量较小、代表性不够, 远远不能满足研究者和决策者的需要。 而<strong>基于大数据的文化心理学研究能够以相对较低的时间、 人力成本实现近乎全样本的数据进行自动抓取、 实时分析乃至实验操控, 并从海量数据中迅速锁定、预测某一群体的行为特征, 进而实现宏观群 体水平的文化心理分析、预测和改变。</strong></p>
<p><strong>再者, 大数据时代的文化互动日趋个体化和复杂化, 而传统的文化心理学容易忽视文化成员在个体水平及复杂情景中的差异, 仅通过抽样追 求简单因果关系, 却难以把握文化心理、行为间的复杂相关性</strong>。也就是说, 在没有搞清楚“是什么” 的情况下就去分析“为什么” (顾肃, 2021)。 巨大、 连续的数据存储和模型分析, 以一种细粒度和大规模乃至全球范围的方式实时研究个体行为, 能够根据个体和情景变异进行适应性的调整和持续、实时的纠正;  同时, 庞大数据量补偿了精确性上的损失, 这有助于分辨出最合适的相关关系, 捕捉小数据测量时没有的新价值(Lazer et al., 2009)。尽管基于大数据技术的词频、预测模型或社会网络分析并非新鲜事物, 但将其应用于文化心理学研究能够从整体上把握对象, 促进从因果思维向相关性思维转变, 进而有利于全面、客观地揭示隐藏在数据背后的文化意涵和规律(陈云松, 2015)。</p>
<p><strong>最后, 更为重要的是, 大数据时代的文化形态也发生了深刻变化。</strong> 随着互联网的快速发展与网民的大量增加, 基于网络的独特文化现象层出 不穷, 一些经典的文化行为特征在网络上的表现也有所不同, 但传统文化心理学对此知之甚少。 例如, 研究发现相对于其他社会环境, 网络环境 下人们的情绪表达会更极端, 而且网络使用的时间越长, 极端情绪越明显(King, 2001)。 此外, 由于线上与线下任务的认知加工过程不同, 人们在 一些经典的文化认知表现上也存在差异。 例如, 相对于认知负荷较大的线下记忆任务, 低认知负荷的线上浏览任务中被试有更多的认知资源来处理外部信息, 因此在社会行为推理上会表现出较小的对应偏差, 基于国别的文化差异被基于线上 和线下的文化差异所代替(Miyamoto &amp; Kitayama, 2002)。 尤其值得注意的是, 移动互联网本身就是一种带有偏差的文化产品, 例如具有相应文化倾向(如高个人主义、低不确定性回避)的用户也更能 体验到技术−文化匹配后的愉悦感和效用感(Lee et al., 2007)。 因此我们不能仅满足于将研究线下文化现象的思维来简单套用到线上行为的文化心理研究中。 总之, 随着计算机和大数据技术的深入发展, 人类社会及其文化形态正经历着深刻的变革。 <strong>文化符号的生产和互动日趋复杂, 文化成员及其社 会网络更是留下了海量的文本与行为足迹。 这使得文化必须被快速地描述、预测甚至改变, “可计算”的文化符号及其互动过程也逐渐成为文化心理学的研究对象。 接下来, 我们将主要对计算文化心理学的产生过程、主要变量和分析技术进行回顾, 并介绍该技术在相关文化变迁、文化地理效应的应用。</strong></p>
<p><br><br></p>
<h2 id="二计算文化心理学">二、计算文化心理学</h2>
<p>在计算文化心理学正式产生以前, 文化学者就受到计算机技术的启发, 提出了**「文化计算」**概 念, 强调要发挥计算机的独特优势, 以便多维度 展现文化及其与人的互动(Tosa et al., 2005)。通过 <strong>文化计算</strong>, 研究者首先在艺术领域实现了文化的数据化呈现和翻译(Tosa et al., 2019)。 进而, <strong>文化计算把计算及相关技术应用到越来越多的文化领域, 探索其发展规律、提示其内在联系并对其进行量化分析与展示的科学</strong>(赵海英 等, 2016)。 如 Michel 等人(2011)通过对谷歌图书项目的数据进行分析, 发现了特定关键词使用频率的时间变化规律, 并由此推导出人类文化发展的趋势, 如语法演变、集体记忆和流行病学的变化趋势等; 吴育锋等人(2018)通过“文心” (中文心理分析系统) 对小说《平凡的世界》中的对话文本进行文学智能分析, 发现其计算出的小说人物性格与以往文艺研究结果一致, 并发现了不同时代人物呈现出性格变迁的趋势(如新时代青年的性格更加开放、 外向);</p>
<p>Neuman 等人(2012)在 <strong>「文化计算」</strong> 的基础上正式提出了**「计算文化心理学」**”的概念, 即通过计算工具和大数据资料库来研究文化符号及其互动过程的心理学分支。 值得注意的是, 虽然文化计算和基于计算的文化心理学有很多相似之处, 但是二者也有区别：</p>
<ul>
<li>前者侧重文化载体的呈现, 以及 文化基因的提取;</li>
<li>后者除了一般意义上的文化计算, 还注重分析人与文化环境的交互关系, 并且文化载体不仅是传统艺术文化, 也包含网络社交 媒体等。</li>
</ul>
<p><br><br></p>
<h2 id="三文化心理变量的提取">三、文化心理变量的提取</h2>
<p>为了理解文化现象, 心理学者需要首先提取文化心理变量, 进而理解影响其成员思维的符号系统和互动机制。<strong>语言是被分享的文化系统(王斌, 2012), 因此我们可以把语言看做是理解文化的窗口以及提取文化心理变量的来源(罗伯特·怀尔 等, 2017)。 语言的本质是思想、话语的直接现实和展示(吴美川, 2020), 其在计算文化心理学领域的语言载体主要包括书籍、社会媒体数据、档案数据、网络音频等</strong>。</p>
<p>针对语言文化载体, 研究者主要提取的文化心理变量包括<strong>个人主义/集体主义、个性解放/自我表达、文化松紧度</strong>等, 详见表 1。</p>
<p><img loading="lazy" src="img/%e6%96%87%e5%8c%96%e5%bf%83%e7%90%86%e5%8f%98%e9%87%8f%e7%a0%94%e7%a9%b6%e8%b7%9d%e7%a6%bb.png" alt=""  />
</p>
<p>第一, 这些变量本身具有重要研究价值, 如个人主义/集体 主义是文化心理学中使用最为广泛的文化维度, 尤其对社会文化变迁和跨文化比较研究具有重要 意义; 第二, 社会现实的需求, 如新冠疫情的全球大流行显著增加了规范遵从的强度, 这需要研 究者及时分析其文化适应后果及政策干预策略; 第三, 此类文化变量的可操作性强, 便于用大数 据方法来进行抓取和计算。</p>
<p>此外, 计算文化心理学研究也对一些文化变量进行了操作化和创造性的发展。 首先, 考虑到大数据的语料库特点, 选用一些便于操作的语言 特征作为经典文化心理学变量的指标, 例如用第一人称单数的使用频率作为个人主义的指标(Twenge et al., 2013); 其次, 将个体的行为印记(如@他人、 成组人群比例)作为文化心理变量的指标, 是对以往基于主观报告的文化心理测量的重要发展(Wu, Li et al., 2018); 此外, 根据新媒体的特点, 采用表情包等作为文化相关情绪表达规范的指标也是一种有益的创新(Koda &amp; Ishida, 2006)。</p>
<p><br><br></p>
<h2 id="四文化心理变量的计算方法">四、文化心理变量的计算方法</h2>
<p>目前采用大数据技术进行文化心理分析主要 有 4 种方法：</p>
<ol>
<li>
<p>基于<strong>特征词典的频次分析</strong>, 如通过对数字、表情和语言等方面的关键特征分析, 研究个体或群体的文化价值观, 这是一种简单有效但相对粗糙的分析(李国杰, 程学旗, 2012)。</p>
</li>
<li>
<p>基于<strong>机器学习的模型预测或词嵌入联想测验</strong>, 如通过机器学习模型对用户的个性、价值观和意识 形态进行识别, 这种方法更精确, 对心理特征的 把握也更全面(Kosinski et al., 2015); 词嵌入联想 测验以词嵌入向量特征代替传统的词频特征, 将 靶词和属性词向量的余弦相似度作为语义关联度的指标, 是社会态度与价值观研究的新方法 (Caliskan et al., 2017; Hamamura et al., 2021)。</p>
</li>
<li>
<p><strong>社会网络分析</strong>, 如通过对网络搜索、网友互粉 等数据的分析, 研究不同文化群体的行为和决策风格, 这种方法目前在文化心理学领域还相对较 少, 但具有重要的社会实践价值(Wu, Li et al., 2018);</p>
</li>
<li>
<p><strong>社会仿真模型</strong>, 因为很多社会事 件是无法在现实中进行实验的, 所以采用仿真模拟的办法来研究某一特定的系统和策略, 从而达到分析社会事件的目的(刘婷婷 等, 2016)。 当然, 这 4 种技术并不是孤立的, 研究者也会根据研究 问题采取一种或多种技术。</p>
</li>
</ol>
<blockquote>
<p>特征词典的频次分析、机器学习的模型预测或词嵌入联想测验等技术方法在课程<a href="https://textdata.cn/blog/management_python_course/"><strong>「课程 | Python实证指标构建与文本分析」</strong></a> 中均有丰富的案例、代码、实战。</p>
</blockquote>
<br>
<p>总之, 计算文化心理学致力于应用大数据思维、大数据资源和计算工具来研究文化系统背后的意义制造与互动过程, 它不仅为文化心理分析 提供了新的工具, 也为理解人类思维本质和文化 形态提供了新的学科范式(Neuman, 2014)。<strong>围绕着“集体主义/个人主义”这一被最广泛讨论的文化心理变量, 「计算文化心理学」可以在时间、空间两个维度方面有如下应用</strong>：</p>
<ul>
<li>在时间维度上, 分别从长时程的宏观历史发展和短时程的经济波动转型两个角度介绍计算文化心理学的应用;</li>
<li>在空间维度上, 主要进行生态地理分析, 如文化在特定政治和经济框架内的地区差异, 不同地区生态或户籍制度造成的个人−环境匹配与价值失调。 分析框架见图 1。</li>
</ul>
<p><img loading="lazy" src="img/%e8%ae%a1%e7%ae%97%e6%96%87%e5%8c%96%e5%bf%83%e7%90%86%e5%ad%a6%e7%9a%84%e5%88%86%e6%9e%90%e6%a1%86%e6%9e%b6.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="五不足展望">五、不足&amp;展望</h2>
<p>虽然计算文化心理学已经崭露头角, 相对于 传统文化心理学研究方法和范式具有无法比拟的优势, 但利用大数据进行文化心理分析仍然存在一些局限。</p>
<h3 id="51-技术自身带来的局限">5.1 技术自身带来的局限</h3>
<p><strong>首先, 很多研究者忽视了同一变量在不同时代、不同国家的意义分歧, 造成解码失真</strong>。 心理学者检验文化变迁的常用方法是寻找作者认为对 文化敏感的词汇并探索其使用频率的时间变化, 比如通过追踪第一人称单、复数代词的使用频率 来分析集体主义、个人主义的时间变迁效应 (Twenge et al., 2013; Yu et al., 2016)。 然而, 语言 总是随着时间演变, 同一个词义并不具有稳定性, 其不可避免地受到经济、政治或者文化因素的影响。</p>
<p><strong>假定大数据背后的文化成员能代表整个文化, 容易将带有偏差的文化成员样本视为研究整体</strong>。 例如, 很多研究者使用当下非常流行的语料库——Google Ngram Viewer 来研究集体主义/个人主 义, 并把结论推广至整个国家层面(Twenge et al., 2013)。 但实际上, 该语料库并不能代表无差别的文化整体, 相反, 它仅仅是突出了特定子群体的文化, 如 Google Ngram Viewer 仅代表书面文化的使用者, 社交媒体数据只包含社交媒体用户, 而 忽视了没有机会接触书籍和社交媒体的文化成员 (Pettit, 2016)。 所以, <strong>研究者要特别注意, 诸如图书语料库、社交媒体分析等并不是基于无偏样本, 因此在使用该类分析材料时要考虑结论的可推广 性和普遍适用性问题。</strong></p>
<p>第三, 研究者对量表或题项进行赋值、使得变量可计量, 进而将文化心理与行为转换为一种符号系统, 但是<strong>大数据分析中的频次分析提取的 关键词往往具有多义性, 如果研究者只是摘取了某个含义并将其纳入特定理论范畴, 则难免会出 现选择性谬误</strong>。 同时, 用于处理数据的软件还会 进一步固化这一测量误差。例如在 LIWC 软件中, 因 为 “great” 属 于积极情感范畴 , 所 以 “a great amount of rain”这一无关表达将被归类为积极情 感表达。 这意味着, 即便大数据分析技术的飞速发展, 但其得出的结果仍不够精确。</p>
<p>第四, 存在文化被监控和被操纵、以及侵犯个人隐私的风险。 大数据以其海量存储和智能计算优势来提取个人和特定群体的文化和性格数据, 进而实施与文化−性格相匹配的精准操纵, 甚至攻击。 这些做法明显的弊端就是数据隐私性和国 家区域安全难以保证, 容易被不法分子利用。 因 此, 为了避免个体数据及其分析结果被滥用, 全球研究者应该将研究伦理置于大数据分析的首位, 共同谋求相应的解决措施和共通的数据法律来解 决个人信息及国家或地区文化心理分析的不当使 用问题 (Kosinski et al., 2015)。</p>
<br>
<h3 id="52-技术带来的机会">5.2 技术带来的机会</h3>
<p><strong>我们不能忽视它在未来的巨大发展空间， 应从理论高度对大数据测量指标进行准确定义和解释</strong>。 大数据时代, 出现了重视数据驱动、而轻视理论的现象(维克托 • 迈尔-舍恩伯格, 肯尼斯 • 库克耶, 2013)。 但实际上大数据只是在数据采集中提供了一个发现或检验科学理论的新方法, 不能在知识生产环节中取代理论知识的地位。 理论在大数据研究中是重要的, 一方面大数据分析的算法模型以及对数据的解释需要理论的支撑(彭理强, 2019), 另一方面我们需要运用数据的意向性并由此对数据进行分类, 而不同的意向性和不同的分类体系是由理论概念决定的(齐磊磊, 2015)。 因此, <strong>在未来的研究中, 研究者的设计和经验数据的使用应在理论指引下进行, 尤其在指标维度的构建上要重视理论术语的指称和实体之间的关系</strong>(贾向桐, 2019)。</p>
<p><strong>其次, 利用多种大数据语料库, 进行更广泛的文化动态演化分析</strong>。 过往的研究容易把文化看做静态的单一实体, 并且将不同文化之间的异同 归因于其背后的国家/民族中根深蒂固的特质, 即轻易对群体进行本质化。 然而, 我们需要意识到文化知识是动态发展的过程, 文化的相关潜质或 因果连结存在于特定共享文化知识的激活, 并且 共享文化会随着社会政治和其他类型的社会变化 不断地产生和演化(康萤仪 等, 2010)。 传统的文 化心理学研究受制于小样本并容易忽视微弱的相 关关系, 因此在条件允许的情况下, 未来需要依 赖多种大数据语料库开展更广泛的纵向大尺度研 究, 探索文化符号串联及其背后机制的研究线 索。例如, 研究者可以使用历史档案数据库, 来研 究共享文化从何而来、其分布与使用的频率如何、 共享的边界是什么、共享文化被激活后产生了什么后果等, 进而促进共同的和个体的经验。</p>
<p><strong>第三, 推动文化心理学与计算机、传播学、 历史学等学科的深度整合</strong>。 文化心理学的力量在于其方法论上的多元化和多学科基础(余霞, 钟年, 2019; Cohen, 2019)。 但是, 跨学科也使得学科互补的同时充满文化冲突, 例如不同学科会由于不同范式而在考虑问题的优先级等方面存在根本性 差异, 一旦学科之间产生冲突, 问题或许就会贯穿任何主题和学科的边界(Popper, 1963)。 然而, <strong>当计算思维、数据资源和计算工具被采用后, 就可以从方法论层面加速对研究问题的解决</strong>。 在未来研究中, 研究者可以充分利用计算文化心理学固有的学科多样性和数据多源化, 尝试通过“计 算”去更好地整合文化心理学背后的多个学科视 角及其变量维度, 使大家共同聚焦某个问题。 比如, 为了描绘复杂的文化现象共同将目光聚焦于 背后多源数据的处理、共享数据库的建成、理论与数据的结合等学科问题, 不断推进计算文化心理学的整体发展。</p>
<p><strong>最后, 利用大数据的“场景功能”, 提升文化心理学研究的生态效度。</strong> 一直以来, 由于实验情境缺乏生态效度, 心理学赖以成为“科学”的实验研究备受质疑; 而在大数据背景下, 研究者对已有数据的挖掘完全能够在自然情境下生成实验变量, 在大数据中挑选情境便是实验方式(喻丰 等, 2015)。 在未来研究中, 研究者不仅可以聚焦小场景, 研究个人经验与近端影响源之间的关系, 而且可以探索大场景中更远端影响源与个人经验之间的相互作用, 从而分析不同时间、空间的文化 差异。值得注意的是, 当场景被扩大和拉长, 这些真实的场景可能体现了个人行为与动机的绝大部 分变异, 这体现了自然情境研究的“深度背景化”, 有助于提升文化等背景特征的解释水平。</p>
<p><br><br></p>
<h2 id="六总结">六、总结</h2>
<p>总之, 计算机和大数据技术深刻影响了文化符号的生产、互动方式以及文化形态本身, 并促成了计算文化心理学的产生和发展。 通过回顾计 算文化心理学的产生过程, 以及将大数据分析方法应用到文化变迁、文化地理效应等多个领域的实证研究, 有助于我们理解计算文化心理学的学 科优势、局限及未来发展方向, 并对文化心理研 究的思路拓展和方法革新具有启发意义。</p>
<p>尤为重要的是, 大数据为大尺度的文化分析提供了可能。 文化作为一种宏观且多变的现象, 任何一种文化理论都需要从越大规模的时间和空 间维度来进行检验和评判, 基于大数据的计算文化心理学能够为研究者提供数以亿计的数据以及 强有力的分析工具; 同时, 大数据改变了传统文 化心理学的研究范式, 更产生了诸多新的文化现 象, 这蕴含了整个学科思维方式和学科体系的变 革(喻丰 等, 2015)。当然, 在看到大数据为传统文 化心理研究提供新方法、新范式的同时, 我们也不能忽视其研究局限, 以及潜在的伦理和安全风险。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>NiceGUI库 | 简单易懂的Web GUI开发包； 可开发数据标注工具、心理学实验工具等</title>
      <link>https://textdata.cn/blog/2023-01-21-create-brower-data-label-tools-with-nicegui/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-01-21-create-brower-data-label-tools-with-nicegui/</guid>
      <description>简单易懂的Web GUI开发包； 可开发数据标注工具、心理学实验工具等</description>
      <content:encoded><![CDATA[<p>分享理由， 发挥想象力， 可以使用该工具开发</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 浏览器页面端的数据标注程序
- Web页面心理学实验程序
- ...
</code></pre></div><br>
<h2 id="一nicegui">一、NiceGUI</h2>
<p><a href="https://github.com/zauberzeug/nicegui">NiceGUI</a>  为您处理所有 Web 开发细节， 你只需专注于编写 Python 代码。 易用的Python UI框架，显示在Web浏览器里，可创建按钮，对话框，markdown，3D场景，绘图等</p>
<p><img loading="lazy" src="img/1.png" alt=""  />
</p>
<br>
<h2 id="二功能特性">二、功能特性</h2>
<p><img loading="lazy" src="img/2.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 交互
  - 按钮、滚动条、输入框
  - 提醒、对话、菜单
  - 键盘输入
  - ...

- 样式设计
  - 自定义颜色主题
  - 自定义CSS
  - 现代设计风格素材
  - 内置Taiwind

- 布局
  - 导航栏、选项卡、面板、...
  - 用行、列和卡片分组
  - HTML 和降价元素
  - 默认Flex

- 代码
  - 实时
  - 代码更改时隐式重新加载
  - 直接的数据绑定
  - 在 Python 内执行 javascript

- 可视化
  - 图表、图表和表格
  - 3D场景
  - 进度条
  - 用于数据刷新的内置定时器

...
</code></pre></div><br>
<h2 id="三快速上手">三、快速上手</h2>
<p><a href="https://nicegui.io/">https://nicegui.io/</a></p>
<p><img loading="lazy" src="img/3.png" alt=""  />

<img loading="lazy" src="img/4.png" alt=""  />

<img loading="lazy" src="img/5.png" alt=""  />

<img loading="lazy" src="img/6.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>R语言 | 绘制文本数据情感历时趋势图</title>
      <link>https://textdata.cn/blog/2023-01-20-visualization-of-sentiment-analysis-of-historical-text-data-with-r/</link>
      <pubDate>Fri, 20 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-01-20-visualization-of-sentiment-analysis-of-historical-text-data-with-r/</guid>
      <description>使用R语言，基于卫报数据、LSD2015词典，绘制文本数据情感历时趋势图</description>
      <content:encoded><![CDATA[<p>使用R语言绘制文本数据情感历时趋势图， 实现步骤</p>
<ol>
<li>导入数据、</li>
<li>保留涉及政府的内容； 确定政府关键词，保留政府关键词出现位置窗口距离内的所有词语</li>
<li>情感计算，使用LSD2015中的正、负两类词表， 统计正、负情感词出现次数</li>
<li>绘制出卫报中涉及政府情感的历时可视化图</li>
</ol>
<br>
<h2 id="1-导入数据">1. 导入数据</h2>
<p>该语料库包含 2012 年至 2016 年的 6,000 篇卫报新闻文章。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-R" data-lang="R"><span class="nf">library</span><span class="p">(</span><span class="n">quanteda</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">quanteda.corpora</span><span class="p">)</span>

<span class="n">corp_news</span> <span class="o">&lt;-</span> <span class="nf">readRDS</span><span class="p">(</span><span class="s">&#34;data/data_corpus_guardian.RDS&#34;</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="2-保留涉及政府的内容">2. 保留涉及政府的内容</h2>
<p>确定政府关键词，保留政府关键词出现位置窗口距离内的所有词语</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-R" data-lang="R"><span class="c1"># 分词</span>
<span class="n">toks_news</span> <span class="o">&lt;-</span> <span class="nf">tokens</span><span class="p">(</span><span class="n">corp_news</span><span class="p">,</span> <span class="n">remove_punct</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>

<span class="c1">#政府相关词</span>
<span class="n">gov</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="s">&#34;government&#34;</span><span class="p">,</span> <span class="s">&#34;cabinet&#34;</span><span class="p">,</span> <span class="s">&#34;prime minister&#34;</span><span class="p">)</span>

<span class="c1"># 政府gov词出现位置前后10个词都保留</span>
<span class="c1"># note: 使用 phrase() 构建词组</span>
<span class="n">toks_gov</span> <span class="o">&lt;-</span> <span class="nf">tokens_keep</span><span class="p">(</span><span class="n">toks_news</span><span class="p">,</span> <span class="n">pattern</span> <span class="o">=</span> <span class="nf">phrase</span><span class="p">(</span><span class="n">gov</span><span class="p">),</span> <span class="n">window</span> <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="3-情感计算">3. 情感计算</h2>
<p>使用 data_dictionary_LSD2015 情感词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-R" data-lang="R"><span class="nf">lengths</span><span class="p">(</span><span class="n">data_dictionary_LSD2015</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">negative     positive neg_positive neg_negative 
2858         1709         1721         2860 
</code></pre></div><p>data_dictionary_LSD2015词典中有四个词表，只使用前两个，即 negative 和 positive</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-R" data-lang="R"><span class="c1"># 只使用 negative 和 positive </span>
<span class="n">data_dictionary_LSD2015_pos_neg</span> <span class="o">&lt;-</span> <span class="n">data_dictionary_LSD2015[1</span><span class="o">:</span><span class="m">2</span><span class="n">]</span>

<span class="c1"># toks_gov中属于positive词表的词语出现几次， positive就是几；同理，negative；</span>
<span class="n">toks_gov_lsd</span> <span class="o">&lt;-</span> <span class="nf">tokens_lookup</span><span class="p">(</span><span class="n">toks_gov</span><span class="p">,</span> 
                              <span class="n">dictionary</span> <span class="o">=</span> <span class="n">data_dictionary_LSD2015_pos_neg</span><span class="p">)</span>

<span class="c1"># 构建文档特征矩阵， 并将date进行分组</span>
<span class="n">dfmat_gov_lsd</span> <span class="o">&lt;-</span> <span class="nf">dfm</span><span class="p">(</span><span class="n">toks_gov_lsd</span><span class="p">)</span> <span class="o">%&gt;%</span> 
  <span class="nf">dfm_group</span><span class="p">(</span><span class="n">groups</span> <span class="o">=</span> <span class="n">date</span><span class="p">)</span>

<span class="n">dfmat_gov_lsd</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Document-feature matrix of: 1,453 documents, 2 features (36.89% sparse) and 3 docvars.
            features
docs         negative positive
  2012-01-02        2        0
  2012-01-04        0        0
  2012-01-05        0        1
  2012-01-06        0        0
  2012-01-07        1        0
  2012-01-11        4        4
[ reached max_ndoc ... 1,447 more documents ]
</code></pre></div><br>
<h2 id="4-可视化">4. 可视化</h2>
<p>图表详情</p>
<ul>
<li>x轴日期</li>
<li>y轴词频</li>
<li>图中的柱为每天中的正、负词出现次数</li>
</ul>
<p>由此绘制出卫报中涉及政府情感的历时可视化图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-R" data-lang="R"><span class="nf">matplot</span><span class="p">(</span><span class="n">dfmat_gov_lsd</span><span class="o">$</span><span class="n">date</span><span class="p">,</span> <span class="n">dfmat_gov_lsd</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&#34;l&#34;</span><span class="p">,</span> <span class="n">lty</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">,</span>
        <span class="n">ylab</span> <span class="o">=</span> <span class="s">&#34;Frequency&#34;</span><span class="p">,</span> <span class="n">xlab</span> <span class="o">=</span> <span class="s">&#34;&#34;</span><span class="p">)</span>
<span class="nf">grid</span><span class="p">()</span>
<span class="nf">legend</span><span class="p">(</span><span class="s">&#34;topleft&#34;</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">,</span> <span class="n">legend</span> <span class="o">=</span> <span class="nf">colnames</span><span class="p">(</span><span class="n">dfmat_gov_lsd</span><span class="p">),</span> <span class="n">lty</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">bg</span> <span class="o">=</span> <span class="s">&#34;white&#34;</span><span class="p">)</span>
</code></pre></div><br>
<p>通过获取正面词和负面词的频率之间的差异来计算每日情绪分数。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-R" data-lang="R"><span class="nf">plot</span><span class="p">(</span><span class="n">dfmat_gov_lsd</span><span class="o">$</span><span class="n">date</span><span class="p">,</span> <span class="n">dfmat_gov_lsd[</span><span class="p">,</span><span class="s">&#34;positive&#34;</span><span class="n">]</span> <span class="o">-</span> <span class="n">dfmat_gov_lsd[</span><span class="p">,</span><span class="s">&#34;negative&#34;</span><span class="n">]</span><span class="p">,</span> 
     <span class="n">type</span> <span class="o">=</span> <span class="s">&#34;l&#34;</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&#34;Sentiment&#34;</span><span class="p">,</span> <span class="n">xlab</span> <span class="o">=</span> <span class="s">&#34;&#34;</span><span class="p">)</span>
<span class="nf">grid</span><span class="p">()</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">h</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">lty</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
</code></pre></div><br>
<p>上面两幅图太粗糙了， 可以应用内核平滑处理下情感值，更清楚地显示卫报中涉及政府的情感变化趋势。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-R" data-lang="R"><span class="n">dat_smooth</span> <span class="o">&lt;-</span> <span class="nf">ksmooth</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">dfmat_gov_lsd</span><span class="o">$</span><span class="n">date</span><span class="p">,</span> 
                      <span class="n">y</span> <span class="o">=</span> <span class="n">dfmat_gov_lsd[</span><span class="p">,</span><span class="s">&#34;positive&#34;</span><span class="n">]</span> <span class="o">-</span> <span class="n">dfmat_gov_lsd[</span><span class="p">,</span><span class="s">&#34;negative&#34;</span><span class="n">]</span><span class="p">,</span>
                      <span class="n">kernel</span> <span class="o">=</span> <span class="s">&#34;normal&#34;</span><span class="p">,</span> <span class="n">bandwidth</span> <span class="o">=</span> <span class="m">30</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">dat_smooth</span><span class="o">$</span><span class="n">x</span><span class="p">,</span> <span class="n">dat_smooth</span><span class="o">$</span><span class="n">y</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&#34;l&#34;</span><span class="p">,</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&#34;Sentiment&#34;</span><span class="p">,</span> <span class="n">xlab</span> <span class="o">=</span> <span class="s">&#34;&#34;</span><span class="p">)</span>
<span class="nf">grid</span><span class="p">()</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">h</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">lty</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="资料出处">资料出处</h2>
<ul>
<li><a href="https://tutorials.quanteda.io/advanced-operations/targeted-dictionary-analysis/">https://tutorials.quanteda.io/advanced-operations/targeted-dictionary-analysis/</a></li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Rath | 自动化数据分析工具</title>
      <link>https://textdata.cn/blog/2023-01-18-rath-next-generation-business-intelligence/</link>
      <pubDate>Wed, 18 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-01-18-rath-next-generation-business-intelligence/</guid>
      <description>未来的数据分析场景会是怎样的？RATH借助自动化的数据分析、智能可视化叙事、因果发现与文本挖掘帮助你以前所未有的方式挖掘数据中的价值。</description>
      <content:encoded><![CDATA[<div align="center">
  <br>
  <p align="center">
    <img src="https://camo.githubusercontent.com/53b952ee2dce0e37b0357d94965630b98e729027fb979911b377fa70501471e4/68747470733a2f2f6b616e61726965732e636e2f6173736574732f6b616e61726965732d6c6f676f2e706e67" alt="RATH, the automated exploratory Data Analysis co-pilot" width="120">
   </p>
   <h1 style="font-size:55px">RATH</h1>
  <strong>次世代开源智能数据分析与可视化应用</strong>
  <br></br>
  未来的数据分析场景会是怎样的？RATH借助自动化的数据分析、智能可视化叙事、因果发现与文本挖掘帮助你以前所未有的方式挖掘数据中的价值。
</div>
<br>
<div id="header" align="center">
  <a href="https://www.gnu.org/licenses/agpl-3.0.en.html">
    <img src="https://img.shields.io/badge/license-AGPL-brightgreen?style=flat-square" alt="AGPL License">
  </a>
  <a href="https://www.gnu.org/licenses/agpl-3.0.en.html">
    <img src="https://badgen.net/github/stars/kanaries/rath?style=flat-square" alt="RATH GitHub Stars">
  </a>
  <a href="https://www.gnu.org/licenses/agpl-3.0.en.html">
    <img src="https://badgen.net/github/forks/kanaries/rath?style=flat-square" alt="RATH GitHub Forks">
  </a>
    <a href="https://www.gnu.org/licenses/agpl-3.0.en.html">
    <img src="https://img.shields.io/github/workflow/status/kanaries/rath/Rath%20Auto%20Build?style=flat-square" alt="RATH GitHub Forks">
  </a>
  </a>
    <a href="https://www.gnu.org/licenses/agpl-3.0.en.html">
    <img src="https://img.shields.io/npm/v/@kanaries/graphic-walker/latest?label=%40kanaries%2Fgraphic-walker&style=flat-square" alt="RATH GitHub Forks">
  </a>
</div>
<br>
<h2 id="欢迎">欢迎</h2>
<p><strong>欢迎使用<a href="https://kanaries.cn">RATH</a>!</strong></p>
<p>RATH 不仅仅是数据分析和可视化工具（如 Tableau）的开源替代品，还是次世代的数据分析应用的雏形。主要功能包括：</p>
<ul>
<li>支持主流数据库导入</li>
<li>自动生成多维数据并可视化</li>
<li>自动发现数据规律，揭示数据的内在联系和因果关系</li>
<li>因果发现与推断，帮你挖掘更深层次的变量关系。</li>
<li>根据你关心的文本片段，自动理解你想要进行的数据转化操作，并帮你生成转化选项。</li>
<li>使用增强分析引擎自动化你的探索性数据分析（EDA）流程</li>
<li>数据绘板，使用绘画的方式玩数据分析。</li>
</ul>
<p><img loading="lazy" src="img/feature-demo.gif" alt=""  />
</p>
<br>
<h2 id="快速上手rath">快速上手RATH</h2>
<ul>
<li>🚀 在浏览器中<a href="https://rath.kanaries.net">立即尝试RATH</a></li>
<li>📖 阅读<a href="https://docs.kanaries.net">RATH 文档</a></li>
<li><a href="https://www.bilibili.com/video/BV1Pe4y1E7Y2/?share_source=copy_web&amp;vd_source=57ac992756e57aeb910c02693db35eac">绘板功能视频</a></li>
</ul>
<br>
<h2 id="联系我们">联系我们</h2>
<p>RATH是开源项目，离不开开源贡献者和关注者的支持。当你遇到问题，bug，疑惑，甚至有有趣的想法或建议，都可以联系我们：</p>
<ul>
<li>邮件: <a href="mailto:support@kanaries.org">support@kanaries.org</a></li>
<li>QQ群: 129132269</li>
<li>公众号: kanaries</li>
</ul>
<p>💪加入我们的社区，成为 RATH 大家庭的一部分！💪</p>
<br>
<h2 id="目录">目录</h2>
<p>| <a href="#why-use-rath">Why use RATH?</a> | <a href="#try-rath">Try RATH</a> | <a href="#feature-highlights">Feature highlights</a> | <a href="#walkthroughs">Walkthroughs</a> | <a href="#developer-documentation">Developer Documentation</a> | <a href="#project-status">Project Status</a> | <a href="#community">Community</a> | <a href="#contributions">Contributions</a> | <a href="#license-agpl">License (AGPL)</a> |</p>
<br>
<h2 id="启动rath">启动RATH</h2>
<p>你可以：</p>
<ul>
<li>无需代码知识，在浏览器中直接使用 <a href="https://rath.kanaries.cn/">RATH Cloud</a></li>
<li>下载 <a href="https://kanaries.cn/#/products">桌面版RATH</a></li>
<li>或者在本地部署RATH：</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">git clone https://github.com/Kanaries/Rath.git <span class="o">&amp;&amp;</span> <span class="nb">cd</span> Rath
yarn install
yarn workspace rath-client start
</code></pre></div><br>
<h2 id="功能特点">功能特点</h2>
<ul>
<li>
<p>自动化的探索分析 🚀 数据分析领域的Copilot</p>
<p>面对复杂多变的数据无从下手？发现数据问题，难以快速定位原因。RATH 提供全/半自动的探索分析能力，让机器替你在复杂多变的数据中完成挖掘探索工作。甚至只需一键即可生成动态数据报表。</p>
</li>
<li>
<p>多维可视化探索 🔭 RATH 可以自动化的识别一些数据中的高维复杂规律，并以多维可视化的形式呈现。</p>
<p>数据探索分析时，多维可视化分析往往能够揭示数据中的深层规律，带给分析人员更多的洞见。过去，需要分析人员具备一定的可视分析的专业知识，才能有效的运用高维可视化发现规律。RATH 则会帮你自动化完成这一工作，让你专注在业务问题本身。</p>
</li>
<li>
<p>基于图形语法的分析模块 👾 RATH 内置了基于图形语法的内置自助分析模块。</p>
<p>除了全自动化的分析体验，分析师有时会有着明确的分析目的，此时 RATH 会提供一个基于图形语法的自助分析模块，帮助分析师使用传统的分析方式完成自定义的分析。</p>
</li>
<li>
<p>无需担心冷启动问题 🤝 RATH 并不依赖于一些先验知识或是领域经验的输入</p>
<p>与一些其他的自动化技术不同，RATH 不依赖一些预定义的领域经验、人为标注。只需要最纯粹的数据源本身即可，RATH 会自己理解数据本身，这使得即使你给到RATH的是混淆加密的数据，RATH 仍然可以给到有效的分析结果。当然，如果你能告诉 RATH 更多的信息，RATH 会表现的更好，但通常情况下，RATH 无需这些信息便能给出洞察。</p>
</li>
</ul>
<br>
<h2 id="功能截图">功能截图</h2>
<h3 id="导入数据源">导入数据源</h3>
<p><img loading="lazy" src="img/import-data-from-selected-data-source.gif" alt=""  />
</p>
<h3 id="数据转化与清洗">数据转化与清洗</h3>
<p>导入数据后，你可以在RATH中快速了解数据的大致分布情况，RATH也会提供一些自动化清洗和转化的方法。</p>
<p>另外，RATH还内置了文本模式识别提取功能，对于一些文本字段，你可以在文本中选择你关心的部分，RATH会自动归纳文本特征，并为你匹配类似的特征，并提取生成新字段。
<img loading="lazy" src="img/text-pattern-selection-01.gif" alt=""  />
</p>
<p><img loading="lazy" src="img/view-statistics-data-source.gif" alt=""  />
</p>
<h3 id="一键全自动分析并生成可视化视图">一键全自动分析，并生成可视化视图</h3>
<p>在完全没有头绪时，点击自动分析，RATH就可以帮你完成对数据集的探索与挖掘，帮你发现数据中的规律，问题，并自动生成可视化。</p>
<p><img loading="lazy" src="img/one-click-automated-data-analysis-visualization.gif" alt=""  />
</p>
<h3 id="半自动探索">半自动探索</h3>
<p>结合了全自动和半自动的优点，每次都根据你当前关心的点，推荐下一步的分析建议，而不再是做全局的自动化。</p>
<p><img loading="lazy" src="img/rath-data-analysis-ai-copilot.gif" alt=""  />
</p>
<h3 id="自助分析-类tableau">自助分析 （类Tableau）</h3>
<p>一个传统的类tableau的拖拉拽分析的模块，适合有明确的分析目的和问题。</p>
<p><img loading="lazy" src="img/manually-explore-data-tableau-ui.gif" alt=""  />
</p>
<blockquote>
<p>自助分析同时也是一个独立的模块。你可以把它嵌入到你自己的APP内。更多参考位于<code>packages/graphic-walker/README.md</code>的README文档</p>
<p>安装方法：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash">yarn add @kanaries/graphic-walker
<span class="c1"># or</span>
npm i --save @kanaries/graphic-walker
</code></pre></div></blockquote>
<h3 id="数据绘板以绘画的方式完成数据分析工作流">数据绘板，以绘画的方式完成数据分析工作流</h3>
<p><a href="https://www.bilibili.com/video/BV1Pe4y1E7Y2/?share_source=copy_web&amp;vd_source=57ac992756e57aeb910c02693db35eac">数据绘板演示视频</a></p>
<p><img loading="lazy" src="img/data-analysis-paiting.gif" alt=""  />
</p>
<br>
<h2 id="支持数据库">支持数据库</h2>
<p><img loading="lazy" src="img/support_databases.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>近年《经济研究》中「文本分析」相关论文</title>
      <link>https://textdata.cn/blog/2023-01-16-papers-using-text-mining-tech-in-journal-of-economic-research/</link>
      <pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-01-16-papers-using-text-mining-tech-in-journal-of-economic-research/</guid>
      <description>使用机器学习、文本分析方法，发表在《经济研究》的相关论文</description>
      <content:encoded><![CDATA[<p>张叶青, 陆瑶, 李乐芸. <strong>大数据应用对中国企业市场价值的影响——来自中国上市公司年报文本分析的证据</strong>[J]. 经济研究, 2021, 56(12):18.</p>
<p>摘要：推进大数据与实体经济的深度融合成为中国新一轮的经济增长点.本文通过对A股上市公司的年报进行<strong>文本分析</strong>, <strong>构建了衡量公司层面&quot;大数据&quot;应用程度的指标</strong>,探讨了企业大数据应用的发展状况及决定因素,检验了大数据应用对公司市场价值的影响.研究发现:第一,规模较大,有形资产比例较低,盈利能力较强,以及所在地区市场化程度较高的公司更可能在生产经营过程中应用大数据;第二,大数据的应用可以显著提高公司的市场价值;第三,主要的影响机制在于大数据的应用显著提高了公司的生产效率和研发投入,而相关技术和人才供给的不足可能会阻碍大数据对市场价值的积极影响.本文结论对中国未来大数据相关的政策设计具有参考价值,为推动实体企业生产经营与大数据的高效融合提供了经验证据和指导建议.</p>
 <br> 
<p>李晓溪,杨国超,饶品贵.<strong>交易所问询函有监管作用吗?——基于并购重组报告书的文本分析</strong>[J].经济研究,2019,54(05):181-198.</p>
<p>摘要:在国务院大力强调优化兼并重组市场环境的形势下,2014年以来交易所广泛使用的并购问询函制度能否发挥监管作用,成为并购重组服务实体经济能力的重要影响因素。为此,本文研究交易所问询函是否降低并购重组信息不对称进而提升并购绩效。研究结果表明,交易所问询函能够识别并购重组中的潜在风险,表现为信息不对称程度较高、报告书信息披露质量较差的并购重组交易更可能收到问询函。进一步地,被问询样本在收到问询函之后的买卖价差、分析师盈余预测误差以及分析师乐观程度较低。 <strong>针对具体作用机制,本文采用文本分析法比较修订前后的并购重组报告书,发现新修订报告书中标的方历史信息和前瞻信息的内容均更多,且更详细,表明问询函制度通过改善信息披露缓解了并购交易的信息不对称问题</strong>。经济后果方面,本文发现信息披露改善较多的被问询样本重组成功的可能性更大,未来市场业绩也更好。本文研究不仅丰富了问询函经济后果的相关研究,也为问询监管政策缓解并购重组信息不对称提供了理论参考。</p>
 <br> 
<p>张成思, 孙宇辰, 阮睿. <strong>宏观经济感知,货币政策与微观企业投融资行为</strong>[J]. 经济研究, 2021, 56(10):17.</p>
<p>摘要：<strong>本文基于中国上市公司年报文本信息,首次构建了中国微观企业宏观经济感知指数</strong>,并通过一个三期理论模型阐释货币政策在不同宏观经济感知情形下如何影响微观企业投融资行为.实证分析表明,当央行实施积极货币政策时,对宏观经济感知更乐观的企业更积极地响应政策刺激,表现为投融资行为增加.进一步将宏观经济感知指数分解为预期指数和回顾指数,分析结果表明:宏观经济感知指数的影响主要由反映企业未来预期的宏观经济预期指数引致,而反映历史信息的宏观经济回顾指数则没有显著影响.区分企业所有制的结果还表明,持有积极宏观经济感知的民营企业仅在积极货币政策状态下增加投资和提高杠杆率,而宏观经济感知对国有企业投融资行为的作用则未受货币政策状态影响.</p>
<br> 
<p>林建浩, 陈良源, 罗子豪,等. <strong>央行沟通有助于改善宏观经济预测吗?——基于文本数据的高维稀疏建模</strong>[J]. 经济研究, 2021,56(03).</p>
<p>摘要： 宏观经济预测是宏观调控精准施策的重要前提,一直以来是方法论研究的前沿议题.随着央行沟通在预期管理中的频繁使用,其传达的信息受到普遍关注,<strong>本文致力于利用央行沟通文本进行宏观经济预测.首先生成符合央行沟通表达习惯的专用词典用于构建完整语料库,继而利用栅栏分布式多项回归模型从高维和稀疏的语料库中提取有效信息</strong>,得到央行沟通测度.基于152个指标构建基准动态因子模型,进一步引入央行沟通测度作为新的预测因子,结果显示央行沟通测度有助于提升模型样本内拟合效果.考察样本外预测效果,在不包括预测变量历史信息时,央行沟通测度能够使得不同期限的预测精度提高6.80％-16.65％;包含预测变量历史信息时则出现分化,在期限较短时,央行沟通未能提升预测精度,这是因为主要沟通信息与预测变量历史信息重叠;当期限较长时,预测精度有所提升,表明沟通中少量的前瞻性指引具有持续的预测能力.本文研究从预测角度验证了中国央行沟通在预期管理中的作用,并为进一步利用非结构化的文本大数据提升中国宏观经济实时预测能力提供了新思路.</p>
 <br> 
<p>曹廷求, 张光利. <strong>自愿性信息披露与股价崩盘风险:基于电话会议的研究</strong>[J]. 经济研究, 2020, 55(11):17.</p>
<p>摘要：<strong>电话会议已经成为中国上市企业自愿性信息披露的重要渠道</strong>,本文从股价崩盘风险的视角探讨了电话会议的信息披露效果.实证结果表明,电话会议能够显著降低企业股价崩盘风险,在控制反向因果和遗漏变量导致的内生性问题之后,该结论依然成立.电话会议影响股价崩盘风险的程度受到企业异质性和外部信息环境的影响,具体而言,我们发现当企业的信息披露质量越高,机构投资者持股比例越高,被分析师关注程度越高,投资者对企业信息的需求越强时,电话会议对企业股价崩盘风险的影响越强.另外,我们分别从<strong>电话会议信息含量</strong>,投资者反应周期,<strong>高管语言特征</strong>,会议主题的视角讨论了电话会议影响股价崩盘风险的机制.本文的研究对于全面认识电话会议在资本市场中的信息披露效果以及如何降低股价崩盘风险具有重要的理论和现实意义。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>金融研究 | 央行货币政策文本相似度计算与可视化</title>
      <link>https://textdata.cn/blog/2023-01-10-similarity_of_cental_bank_monetary_policy/</link>
      <pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-01-10-similarity_of_cental_bank_monetary_policy/</guid>
      <description>本文利用金融情感词典和文本分析技术,分析中国人民银行货币政策执行报告的**文本情绪、文本相似度和文本可读性**等多维文本信息,刻画央行货币政策执行报告的文本特征,探究货币政策报告的文本信息与宏观经济和股票市场的关系。**实证研究发现,货币政策报告的文本情绪的改善会引起显著为正的股票市场价格反应, 报告文本相似度的增加会引起股票市场波动性的显著降低, 报告可读性对公布后股票市场的波动性影响不显著**。货币政策报告文本情绪还与诸多宏观经济指标显著相关。进一步研究发现,引起股票市场显著反应的是报告文本情绪中反映货币政策指引的部分,而反映宏观经济历史状态的部分对股票市场的影响不显著。本文从文本大数据分析角度证明了我国央行沟通的有效性,对国内央行沟通相关研究形成了有益补充。This paper uses text analysis techniques to analyze 71 Monetary Policy Implementation Ｒeports （ hereinafter referred to as“the reports”） of PBOC，calculates the text sentiment （ tone） ，the similarity and readability and other text indicators of the reports，and explores the relationship between these text indicators and the macro economy and the stock market． Based on the Chinese financial sentiment dictionary developed by Jiang et al． （ 2020） ，this paper uses the sentiment unit method to calculate the tone of the reports． In addition，this paper uses TF － IDF weighted cosine similarity to characterize the similarity of the reports，and uses average sentence length to characterize the readability of the reports． The paper then uses correlation analysis to examine the relationship between the tone of the reports and macroeconomic indicators such as economic growth，inflation， and interest rates． With reference to Ehrmann and Fratzscher （ 2009） ，Zhang and Hu （ 2014） ，this paper adds tone，similarity and readability to the EGAＲCH model to explore whether textual indicators of the reports affect stock market returns and the volatility on the trading day after the release． Furthermore，this paper decomposes the content of the reports into two parts： economic and financial fundamentals and central bank policy guidelines，calculates the tone of the two parts and examines their impacts on the stock market respectively．</description>
      <content:encoded><![CDATA[<p>姜富伟,胡逸驰,黄楠.<strong><a href="%E5%A4%AE%E8%A1%8C%E8%B4%A7%E5%B8%81%E6%94%BF%E7%AD%96%E6%8A%A5%E5%91%8A%E6%96%87%E6%9C%AC%E4%BF%A1%E6%81%AF%E3%80%81%E5%AE%8F%E8%A7%82%E7%BB%8F%E6%B5%8E%E4%B8%8E%E8%82%A1%E7%A5%A8%E5%B8%82%E5%9C%BA_%E5%A7%9C%E5%AF%8C%E4%BC%9F.pdf">央行货币政策报告文本信息、宏观经济与股票市场</a></strong>[J].金融研究,2021,(06):95-113.</p>
<br>
<p>摘要:本文利用金融情感词典和文本分析技术,分析中国人民银行货币政策执行报告的<strong>文本情绪、文本相似度和文本可读性</strong>等多维文本信息,刻画央行货币政策执行报告的文本特征,探究货币政策报告的文本信息与宏观经济和股票市场的关系。<strong>实证研究发现,货币政策报告的文本情绪的改善会引起显著为正的股票市场价格反应, 报告文本相似度的增加会引起股票市场波动性的显著降低, 报告可读性对公布后股票市场的波动性影响不显著</strong>。货币政策报告文本情绪还与诸多宏观经济指标显著相关。进一步研究发现,引起股票市场显著反应的是报告文本情绪中反映货币政策指引的部分,而反映宏观经济历史状态的部分对股票市场的影响不显著。本文从文本大数据分析角度证明了我国央行沟通的有效性,对国内央行沟通相关研究形成了有益补充。</p>
<br>
<p>文文相似度很好用，下图是该论文中绘制的2001-2018年间的货币政策报告<strong>文本相似度</strong>。 <strong>前后相邻两个季度的货币政策文本相似度越高，说明政策相似性高，政策连贯性强(变化小)。如果相似度较低，则政策变动的风险较大，政策连贯性差(变化大)</strong>。</p>
<p><img loading="lazy" src="img/%e8%ae%ba%e6%96%87-%e6%8a%a5%e5%91%8a%e6%96%87%e6%9c%ac%e7%9b%b8%e4%bc%bc%e5%ba%a6.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="复现相似度">复现相似度</h2>
<p>本文只实现文本相似度的度量、文本相似度趋势的可视化。</p>
<ol>
<li>准备数据</li>
<li>相似度计算</li>
<li>可视化</li>
</ol>
<p><br><br></p>
<h2 id="1-准备数据">1. 准备数据</h2>
<p>首先先手动从 <strong>中国人民银行</strong> 下载货币政策报告。</p>
<p><img loading="lazy" src="img/pbc.png" alt=""  />
</p>
<p>下图是我下载好的报告</p>
<p><img loading="lazy" src="img/pdfs.png" alt=""  />
</p>
<p>之后将其整理到csv中</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">from</span> <span class="nn">pdfdocx</span> <span class="kn">import</span> <span class="n">read_pdf</span>


<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;pbc_reports.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvf</span><span class="p">:</span>
    <span class="c1">#年份、季度、报告文本</span>
    <span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;q&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

    <span class="n">pdfs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;data/</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">pf</span> <span class="ow">in</span> <span class="n">pdfs</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;year&#39;</span><span class="p">:</span>  <span class="n">pf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">4</span><span class="p">],</span>
            <span class="s1">&#39;q&#39;</span><span class="p">:</span>  <span class="n">pf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">5</span><span class="p">],</span>
            <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;data/2013-3.pdf&#39;</span><span class="p">),</span>
        <span class="p">}</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="2-读取数据">2. 读取数据</h2>
<p>下载pdf时，遗漏了货币政策报告日期数据，将 pbc_reports.csv 修改为 pbc_reports.xlsx ，增加了 date 字段。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;pbc_reports.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#让每一行含有前后两个季度的报告文本</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;text2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="3-计算相似度">3. 计算相似度</h2>
<p>水平(行)方向，计算每一行中的 text 与 text2 两者的文本相似度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="c1">#row 为 pd.Series 类型数据，类似于字段</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;text2&#39;</span><span class="p">])</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">sim</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="c1">#异常标记为1</span>
        <span class="k">return</span> <span class="mi">1</span>

<span class="c1">#计算结果存储到 similarity 字段中</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;similarity&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">row</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="4-绘制折线图">4. 绘制折线图</h2>
<p>这里为了方便，使用 pandas_bokeh 库。 注意: 绘图不限于Python，各位也可以用excel、R。</p>
<p>参数:</p>
<ul>
<li>kind 图表类型，折线图line</li>
<li>x 横轴字段</li>
<li>y 纵轴字段</li>
<li>xlabel 横轴标签</li>
<li>ylabel 纵轴标签</li>
<li>title 图标题</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas_bokeh</span>

<span class="n">pandas_bokeh</span><span class="o">.</span><span class="n">output_notebook</span><span class="p">()</span>

<span class="c1">#选择折线图line</span>
<span class="c1">#</span>
<span class="n">df</span><span class="o">.</span><span class="n">plot_bokeh</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s1">&#39;line&#39;</span><span class="p">,</span>
              <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span>
              <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;similarity&#39;</span><span class="p">,</span>
              <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;2001~2022央行货币政策相似度趋势&#39;</span><span class="p">,</span>
              <span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;报告发布日期&#39;</span><span class="p">,</span>
              <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;相似度&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/%e5%9b%be1.png" alt=""  />
</p>
<br>
<p>刚刚生成的图没有经过移动平滑处理，所以锯齿比较多。论文中使用三季度移动平均线处理了 similarity ，我在此将其命名为 ma3_similarity</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas_bokeh</span>

<span class="n">pandas_bokeh</span><span class="o">.</span><span class="n">output_notebook</span><span class="p">()</span>

<span class="c1">#三季度移动平均线</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;ma3_similarity&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;similarity&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">min_periods</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">df</span><span class="o">.</span><span class="n">plot_bokeh</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s1">&#39;line&#39;</span><span class="p">,</span>
              <span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span>
              <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;ma3_similarity&#39;</span><span class="p">,</span>
              <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;2001~2022央行货币政策相似度趋势&#39;</span><span class="p">,</span>
              <span class="n">xlabel</span> <span class="o">=</span> <span class="s1">&#39;报告发布日期&#39;</span><span class="p">,</span>
              <span class="n">ylabel</span> <span class="o">=</span> <span class="s1">&#39;相似度&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/%e5%9b%be2.png" alt=""  />
</p>
<p><br>基本复刻论文原图相似度的变化趋势</p>
<p><img loading="lazy" src="img/%e8%ae%ba%e6%96%87-%e6%8a%a5%e5%91%8a%e6%96%87%e6%9c%ac%e7%9b%b8%e4%bc%bc%e5%ba%a6.png" alt=""  />
</p>
<br>
<p>有了相似度变化序列数据， 可以使用 ruptures库， 找到政策变化的时间点， 参考 <a href="https://textdata.cn/blog/2023-11-26-using-ruptures-to-detect-change-point/">使用 Ruptures 识别时间序列数据中的变化点</a></p>
<p><br><br></p>
<h2 id="代码下载">代码下载</h2>
<ul>
<li>代码及视频讲解已经添加至 <a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a>  中，感兴趣的同学欢迎订阅该系列课，涵盖Python语法入门、数据采集、文本分析、机器学习等。</li>
<li>未订阅 <a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a> 的朋友们，可转发本文集赞30+， 加微信 <strong>372335839</strong> ， 备注「<strong>姓名-学校-专业-央行相似度</strong>」，获取本文数据及代码。</li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>12G数据集 |  23w条Kickstarter项目信息</title>
      <link>https://textdata.cn/blog/2022-12-04-kickstarters_dataset/</link>
      <pubDate>Sun, 04 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-12-04-kickstarters_dataset/</guid>
      <description>2016年3月写好的kickstarter爬虫，每月执行一次。截止2022年11月， 所有压缩文件累积11.42G。文末有数据获取方式</description>
      <content:encoded><![CDATA[<h2 id="kickstarter介绍">Kickstarter介绍</h2>
<p>Kickstarter于2009年4月在美国纽约成立，是一个专为具有创意方案的企业筹资的众筹网站平台。</p>
<p>kickstarter平台的运作方式相对来说比较简单而有效：该平台的用户一方是有创新意渴望进行创作和创造的人，另一方则是愿意为他们出资金的人，然后见证新发明新创作新产品的出现。kickstarter网站的创意性活动包括：<strong>音乐，网页设计，平面设计，动画，作家</strong>以及所有有能力创造以及影响他人的活动。</p>
<p><br><br></p>
<h2 id="12g数据集">12G数据集</h2>
<p><strong>2016年3月</strong> 写好的kickstarter爬虫，每月执行一次。截止<strong>2022年11月</strong>， 所有压缩文件累积11.42G。<strong>文末有数据获取方式</strong></p>
<p><img loading="lazy" src="img/kickstarter_datasets_dir_screen.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="参考论文">参考论文</h2>
<p>该数据集研究价值，可用于研究市场营销、创新创业、信息管理等， 部分使用kickstarter作为研究对象的论文。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.*管理世界*.2016;5:81-98.
[2]Dai, Hengchen and Dennis J. Zhang. “Prosocial Goal Pursuit in Crowdfunding: Evidence from Kickstarter.” Journal of Marketing Research 56 (2019): 498 - 517.
[3]Gafni, H., Marom, D.M., Robb, A.M., &amp; Sade, O. (2020). Gender Dynamics in Crowdfunding (Kickstarter): Evidence on Entrepreneurs, Backers, and Taste-Based Discrimination*. Review of Finance.
[4]Jensen, Lasse Skovgaard and Ali Gürcan Özkil. “Identifying challenges in crowdfunded product development: a review of Kickstarter projects.” Design Science 4 (2018): n. pag.
</code></pre></div><br>
<br>
<h2 id="查看数据">查看数据</h2>
<p>任意选择一个zip文件解压会得到json文件，注意 <strong>不同json文件不太一样，所以本文的代码可能要有调整。</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#读取任意一个zip解压得到的csv文件</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="s1">&#39;data/Kickstarter_2022-06-09T03_20_03_365Z.json&#39;</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">230346
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 选中projects字段</span>
<span class="n">projects</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">projects</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    0         {&#39;id&#39;: 947118202, &#39;photo&#39;: {&#39;key&#39;: &#39;assets/029...
    1         {&#39;id&#39;: 426094497, &#39;photo&#39;: {&#39;key&#39;: &#39;assets/029...
    2         {&#39;id&#39;: 44835253, &#39;photo&#39;: {&#39;key&#39;: &#39;assets/034/...
    3         {&#39;id&#39;: 1001767271, &#39;photo&#39;: {&#39;key&#39;: &#39;assets/03...
    4         {&#39;id&#39;: 1880345176, &#39;photo&#39;: {&#39;key&#39;: &#39;assets/03...
                                    ...                        
    230341    {&#39;id&#39;: 676753351, &#39;photo&#39;: {&#39;key&#39;: &#39;assets/012...
    230342    {&#39;id&#39;: 1579378115, &#39;photo&#39;: {&#39;key&#39;: &#39;assets/02...
    230343    {&#39;id&#39;: 1281094926, &#39;photo&#39;: {&#39;key&#39;: &#39;assets/02...
    230344    {&#39;id&#39;: 783009016, &#39;photo&#39;: {&#39;key&#39;: &#39;assets/012...
    230345    {&#39;id&#39;: 324368296, &#39;photo&#39;: {&#39;key&#39;: &#39;assets/012...
    Name: data, Length: 230346, dtype: object
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看第一行，data列</span>
<span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    {&#39;id&#39;: 947118202,
     &#39;photo&#39;: {&#39;key&#39;: &#39;assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png&#39;,
      &#39;full&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=560&amp;h=315&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=26209d432871ad2e9cca642527c291d9&#39;,
      &#39;ed&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=352&amp;h=198&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=db82255e6639d5951506e0f2ed4d7d8b&#39;,
      &#39;med&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=272&amp;h=153&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=f7b43116136000c8efa892bdbdd2d956&#39;,
      &#39;little&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=208&amp;h=117&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=a52e3c34066a020e040c517c614a8b36&#39;,
      &#39;small&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=160&amp;h=90&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=6c5f1c254119ffe914b50250f8e2899f&#39;,
      &#39;thumb&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=48&amp;h=27&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=0222f379ed51059eb73adc7436f07b1e&#39;,
      &#39;1024x576&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=1024&amp;h=576&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=d01546c5e88f3f47e0dddc48b5dce9df&#39;,
      &#39;1536x864&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=1552&amp;h=873&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=f47505da909a374642906e6d418474e7&#39;},
     &#39;name&#39;: &#39;Paint Rogue&#39;,
     &#39;blurb&#39;: &#39;Roguelike | Platformer | Shooter&#39;,
     &#39;goal&#39;: 5000,
     &#39;pledged&#39;: 5268.22,
     &#39;state&#39;: &#39;successful&#39;,
     &#39;slug&#39;: &#39;paint-rogue&#39;,
     &#39;disable_communication&#39;: False,
     &#39;country&#39;: &#39;AU&#39;,
     &#39;country_displayable_name&#39;: &#39;Australia&#39;,
     &#39;currency&#39;: &#39;AUD&#39;,
     &#39;currency_symbol&#39;: &#39;$&#39;,
     &#39;currency_trailing_code&#39;: True,
     &#39;deadline&#39;: 1594247312,
     &#39;state_changed_at&#39;: 1594247312,
     &#39;created_at&#39;: 1591152439,
     &#39;launched_at&#39;: 1591655312,
     &#39;staff_pick&#39;: False,
     &#39;is_starrable&#39;: False,
     &#39;backers_count&#39;: 42,
     &#39;static_usd_rate&#39;: 0.69681992,
     &#39;usd_pledged&#39;: &#39;3671.0006389424&#39;,
     &#39;converted_pledged_amount&#39;: 3657,
     &#39;fx_rate&#39;: 0.7200616400000001,
     &#39;usd_exchange_rate&#39;: 0.69423473,
     &#39;current_currency&#39;: &#39;USD&#39;,
     &#39;usd_type&#39;: &#39;international&#39;,
     &#39;creator&#39;: {&#39;id&#39;: 1018782761,
      &#39;name&#39;: &#39;Andrew Von Stieglitz&#39;,
      &#39;is_registered&#39;: None,
      &#39;is_email_verified&#39;: None,
      &#39;chosen_currency&#39;: None,
      &#39;is_superbacker&#39;: None,
      &#39;avatar&#39;: {&#39;thumb&#39;: &#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2&amp;w=40&amp;h=40&amp;fit=crop&amp;v=1554207776&amp;auto=format&amp;frame=1&amp;q=92&amp;s=bf4ce960e83b57310b93c40dda68e213&#39;,
       &#39;small&#39;: &#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2&amp;w=80&amp;h=80&amp;fit=crop&amp;v=1554207776&amp;auto=format&amp;frame=1&amp;q=92&amp;s=a862ab30490c90cd08186f448884142d&#39;,
       &#39;medium&#39;: &#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2&amp;w=160&amp;h=160&amp;fit=crop&amp;v=1554207776&amp;auto=format&amp;frame=1&amp;q=92&amp;s=38923ac11699d68a7aae93ce126b97b6&#39;},
      &#39;urls&#39;: {&#39;web&#39;: {&#39;user&#39;: &#39;https://www.kickstarter.com/profile/1018782761&#39;},
       &#39;api&#39;: {&#39;user&#39;: &#39;https://api.kickstarter.com/v1/users/1018782761?signature=1654832212.14f9df54b2643f080ad98cacb07314f94757d9c1&#39;}}},
     &#39;location&#39;: {&#39;id&#39;: 1105779,
      &#39;name&#39;: &#39;Sydney&#39;,
      &#39;slug&#39;: &#39;sydney-au&#39;,
      &#39;short_name&#39;: &#39;Sydney, AU&#39;,
      &#39;displayable_name&#39;: &#39;Sydney, AU&#39;,
      &#39;localized_name&#39;: &#39;Sydney&#39;,
      &#39;country&#39;: &#39;AU&#39;,
      &#39;state&#39;: &#39;NSW&#39;,
      &#39;type&#39;: &#39;Town&#39;,
      &#39;is_root&#39;: False,
      &#39;expanded_country&#39;: &#39;Australia&#39;,
      &#39;urls&#39;: {&#39;web&#39;: {&#39;discover&#39;: &#39;https://www.kickstarter.com/discover/places/sydney-au&#39;,
        &#39;location&#39;: &#39;https://www.kickstarter.com/locations/sydney-au&#39;},
       &#39;api&#39;: {&#39;nearby_projects&#39;: &#39;https://api.kickstarter.com/v1/discover?signature=1654814982.2fcf49a7b611d4414d14b1dbe41ac53623192e6a&amp;woe_id=1105779&#39;}}},
     &#39;category&#39;: {&#39;id&#39;: 35,
      &#39;name&#39;: &#39;Video Games&#39;,
      &#39;analytics_name&#39;: &#39;Video Games&#39;,
      &#39;slug&#39;: &#39;games/video games&#39;,
      &#39;position&#39;: 7,
      &#39;parent_id&#39;: 12,
      &#39;parent_name&#39;: &#39;Games&#39;,
      &#39;color&#39;: 51627,
      &#39;urls&#39;: {&#39;web&#39;: {&#39;discover&#39;: &#39;http://www.kickstarter.com/discover/categories/games/video%20games&#39;}}},
     &#39;profile&#39;: {&#39;id&#39;: 4007060,
      &#39;project_id&#39;: 4007060,
      &#39;state&#39;: &#39;active&#39;,
      &#39;state_changed_at&#39;: 1594267960,
      &#39;name&#39;: &#39;Paint Rogue&#39;,
      &#39;blurb&#39;: &#39;Roguelike | Platformer | Shooter&#39;,
      &#39;background_color&#39;: &#39;&#39;,
      &#39;text_color&#39;: &#39;ffffff&#39;,
      &#39;link_background_color&#39;: &#39;&#39;,
      &#39;link_text_color&#39;: &#39;&#39;,
      &#39;link_text&#39;: &#39;Follow along!&#39;,
      &#39;link_url&#39;: &#39;https://www.kickstarter.com/projects/1018782761/paint-rogue/&#39;,
      &#39;show_feature_image&#39;: True,
      &#39;background_image_opacity&#39;: 0.5700000000000001,
      &#39;background_image_attributes&#39;: {&#39;id&#39;: 29758105,
       &#39;image_urls&#39;: {&#39;default&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/758/105/971e42c0e19ca75fbae0943aa874c3c2_original.png?ixlib=rb-4.0.2&amp;w=1600&amp;fit=max&amp;v=1594267934&amp;auto=format&amp;frame=1&amp;q=92&amp;s=b91907c9e125e206a11a1bcef322c142&#39;,
        &#39;baseball_card&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/758/105/971e42c0e19ca75fbae0943aa874c3c2_original.png?ixlib=rb-4.0.2&amp;w=460&amp;fit=max&amp;v=1594267934&amp;auto=format&amp;frame=1&amp;q=92&amp;s=e7af2286d1f74a51672fbb6060ad43c8&#39;}},
      &#39;should_show_feature_image_section&#39;: False,
      &#39;feature_image_attributes&#39;: {&#39;image_urls&#39;: {&#39;default&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=1552&amp;h=873&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=f47505da909a374642906e6d418474e7&#39;,
        &#39;baseball_card&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=560&amp;h=315&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=26209d432871ad2e9cca642527c291d9&#39;}}},
     &#39;spotlight&#39;: True,
     &#39;urls&#39;: {&#39;web&#39;: {&#39;project&#39;: &#39;https://www.kickstarter.com/projects/1018782761/paint-rogue?ref=discovery_category_newest&#39;,
       &#39;rewards&#39;: &#39;https://www.kickstarter.com/projects/1018782761/paint-rogue/rewards&#39;}},
     &#39;source_url&#39;: &#39;https://www.kickstarter.com/discover/categories/games/video%20games&#39;}
</code></pre></div><p><br><br></p>
<h2 id="字段">字段</h2>
<p>以第一条为例，查看每条众筹项目数据中的字段，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</code></pre></div><p>Run，运行结果#为后期加入的字段解释</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    dict_keys([
    &#39;id&#39;, &#39;photo&#39;,  #id、图片链接
    &#39;name&#39;, &#39;blurb&#39;,  #项目名
    &#39;goal&#39;,   #项目筹资目标金额
    &#39;pledged&#39;, 
    &#39;state&#39;,  #项目状态
    &#39;slug&#39;,  
    &#39;disable_communication&#39;, 
    &#39;country&#39;, &#39;country_displayable_name&#39;,   #国家
    &#39;currency&#39;, &#39;currency_symbol&#39;, &#39;currency_trailing_code&#39;,  #货币
    &#39;deadline&#39;, &#39;state_changed_at&#39;,  #项目筹资截止时间(时间戳格式)
    &#39;created_at&#39;,  #项目创建时间(时间戳格式)
    &#39;launched_at&#39;,  #项目上架时间(时间戳格式)
    &#39;staff_pick&#39;, &#39;is_starrable&#39;, 
    &#39;backers_count&#39;,  #资助人数
    &#39;static_usd_rate&#39;, &#39;usd_pledged&#39;, &#39;converted_pledged_amount&#39;, &#39;fx_rate&#39;, &#39;usd_exchange_rate&#39;, &#39;current_currency&#39;, &#39;usd_type&#39;, 
    &#39;creator&#39;,  #项目发起人信息
    &#39;location&#39;,  #地址
    &#39;category&#39;,  #项目所属类目信息
    &#39;profile&#39;,  #项目基本信息
    &#39;spotlight&#39;, 
    &#39;urls&#39;,  #项目链接
    &#39;source_url&#39;])
</code></pre></div><br>
<p>以第一条数据为例，依次查看这几个字段的信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#众筹项目具名</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;项目名&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;name&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;项目链接</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;urls&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1">#众筹项目的目标总金额</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;目标总金额: </span><span class="si">{goal}{currency}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">goal</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;goal&#39;</span><span class="p">],</span> 
                                          <span class="n">currency</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;currency&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    项目名 Paint Rogue
    
    项目链接
     {&#39;web&#39;: {&#39;project&#39;: &#39;https://www.kickstarter.com/projects/1018782761/paint-rogue?ref=discovery_category_newest&#39;, &#39;rewards&#39;: &#39;https://www.kickstarter.com/projects/1018782761/paint-rogue/rewards&#39;}}
    
    目标总金额: 5000AUD
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#众筹项目发起人信息</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;项目发起人信息</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;creator&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;项目基本信息</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;profile&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1">#众筹项目坐标</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;地址: &#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;location&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1">#众筹项目货币</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;货币:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;currency&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1">#众筹项目所在国家</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;所在国家: &#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;country_displayable_name&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    项目发起人信息
     {&#39;id&#39;: 1018782761, &#39;name&#39;: &#39;Andrew Von Stieglitz&#39;, &#39;is_registered&#39;: None, &#39;is_email_verified&#39;: None, &#39;chosen_currency&#39;: None, &#39;is_superbacker&#39;: None, &#39;avatar&#39;: {&#39;thumb&#39;: &#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2&amp;w=40&amp;h=40&amp;fit=crop&amp;v=1554207776&amp;auto=format&amp;frame=1&amp;q=92&amp;s=bf4ce960e83b57310b93c40dda68e213&#39;, &#39;small&#39;: &#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2&amp;w=80&amp;h=80&amp;fit=crop&amp;v=1554207776&amp;auto=format&amp;frame=1&amp;q=92&amp;s=a862ab30490c90cd08186f448884142d&#39;, &#39;medium&#39;: &#39;https://ksr-ugc.imgix.net/assets/024/628/691/39e3fdc8db723302f544f7161e32c4b7_original.png?ixlib=rb-4.0.2&amp;w=160&amp;h=160&amp;fit=crop&amp;v=1554207776&amp;auto=format&amp;frame=1&amp;q=92&amp;s=38923ac11699d68a7aae93ce126b97b6&#39;}, &#39;urls&#39;: {&#39;web&#39;: {&#39;user&#39;: &#39;https://www.kickstarter.com/profile/1018782761&#39;}, &#39;api&#39;: {&#39;user&#39;: &#39;https://api.kickstarter.com/v1/users/1018782761?signature=1654832212.14f9df54b2643f080ad98cacb07314f94757d9c1&#39;}}}
    
    项目基本信息
     {&#39;id&#39;: 4007060, &#39;project_id&#39;: 4007060, &#39;state&#39;: &#39;active&#39;, &#39;state_changed_at&#39;: 1594267960, &#39;name&#39;: &#39;Paint Rogue&#39;, &#39;blurb&#39;: &#39;Roguelike | Platformer | Shooter&#39;, &#39;background_color&#39;: &#39;&#39;, &#39;text_color&#39;: &#39;ffffff&#39;, &#39;link_background_color&#39;: &#39;&#39;, &#39;link_text_color&#39;: &#39;&#39;, &#39;link_text&#39;: &#39;Follow along!&#39;, &#39;link_url&#39;: &#39;https://www.kickstarter.com/projects/1018782761/paint-rogue/&#39;, &#39;show_feature_image&#39;: True, &#39;background_image_opacity&#39;: 0.5700000000000001, &#39;background_image_attributes&#39;: {&#39;id&#39;: 29758105, &#39;image_urls&#39;: {&#39;default&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/758/105/971e42c0e19ca75fbae0943aa874c3c2_original.png?ixlib=rb-4.0.2&amp;w=1600&amp;fit=max&amp;v=1594267934&amp;auto=format&amp;frame=1&amp;q=92&amp;s=b91907c9e125e206a11a1bcef322c142&#39;, &#39;baseball_card&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/758/105/971e42c0e19ca75fbae0943aa874c3c2_original.png?ixlib=rb-4.0.2&amp;w=460&amp;fit=max&amp;v=1594267934&amp;auto=format&amp;frame=1&amp;q=92&amp;s=e7af2286d1f74a51672fbb6060ad43c8&#39;}}, &#39;should_show_feature_image_section&#39;: False, &#39;feature_image_attributes&#39;: {&#39;image_urls&#39;: {&#39;default&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=1552&amp;h=873&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=f47505da909a374642906e6d418474e7&#39;, &#39;baseball_card&#39;: &#39;https://ksr-ugc.imgix.net/assets/029/542/393/098b025d0c25cc15e5b5f673b9ec992a_original.png?ixlib=rb-4.0.2&amp;crop=faces&amp;w=560&amp;h=315&amp;fit=crop&amp;v=1592675400&amp;auto=format&amp;frame=1&amp;q=92&amp;s=26209d432871ad2e9cca642527c291d9&#39;}}}
    
    地址:  {&#39;id&#39;: 1105779, &#39;name&#39;: &#39;Sydney&#39;, &#39;slug&#39;: &#39;sydney-au&#39;, &#39;short_name&#39;: &#39;Sydney, AU&#39;, &#39;displayable_name&#39;: &#39;Sydney, AU&#39;, &#39;localized_name&#39;: &#39;Sydney&#39;, &#39;country&#39;: &#39;AU&#39;, &#39;state&#39;: &#39;NSW&#39;, &#39;type&#39;: &#39;Town&#39;, &#39;is_root&#39;: False, &#39;expanded_country&#39;: &#39;Australia&#39;, &#39;urls&#39;: {&#39;web&#39;: {&#39;discover&#39;: &#39;https://www.kickstarter.com/discover/places/sydney-au&#39;, &#39;location&#39;: &#39;https://www.kickstarter.com/locations/sydney-au&#39;}, &#39;api&#39;: {&#39;nearby_projects&#39;: &#39;https://api.kickstarter.com/v1/discover?signature=1654814982.2fcf49a7b611d4414d14b1dbe41ac53623192e6a&amp;woe_id=1105779&#39;}}}
    
    货币: AUD
    
    所在国家:  Australia
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#众筹项目创建时间</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;项目创建时间: &#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;created_at&#39;</span><span class="p">])</span>

<span class="c1">#众筹项目上架时间</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;项目上架时间: &#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;launched_at&#39;</span><span class="p">])</span>

<span class="c1">#众筹项目截止时间</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;项目截止时间: &#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;deadline&#39;</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    项目创建时间:  1591152439
    项目上架时间:  1591655312
    项目截止时间:  1594247312
</code></pre></div><br>
<h3 id="时间戳转日期">时间戳转日期</h3>
<p>1591152439是时间戳，以某时间点距1970之间的秒数作为时间。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#时间戳转日期</span>

<span class="kn">import</span> <span class="nn">datetime</span>

<span class="k">def</span> <span class="nf">timestamp2str</span><span class="p">(</span><span class="n">timestamp</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">fromtimestamp</span><span class="p">(</span><span class="n">timestamp</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">&#39;</span><span class="si">{year}</span><span class="s1">-</span><span class="si">{month}</span><span class="s1">-</span><span class="si">{day}</span><span class="s1"> </span><span class="si">{hour}</span><span class="s1">:</span><span class="si">{minute}</span><span class="s1">:</span><span class="si">{second}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">d</span><span class="o">.</span><span class="n">year</span><span class="p">,</span>
                                                                 <span class="n">month</span><span class="o">=</span><span class="n">d</span><span class="o">.</span><span class="n">month</span><span class="p">,</span>
                                                                 <span class="n">day</span><span class="o">=</span><span class="n">d</span><span class="o">.</span><span class="n">day</span><span class="p">,</span>
                                                                 <span class="n">hour</span><span class="o">=</span><span class="n">d</span><span class="o">.</span><span class="n">hour</span><span class="p">,</span>
                                                                 <span class="n">minute</span><span class="o">=</span><span class="n">d</span><span class="o">.</span><span class="n">minute</span><span class="p">,</span>
                                                                 <span class="n">second</span><span class="o">=</span><span class="n">d</span><span class="o">.</span><span class="n">second</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;创建时间&#39;</span><span class="p">,</span> <span class="n">timestamp2str</span><span class="p">(</span><span class="mi">1591152439</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;上架时间&#39;</span><span class="p">,</span> <span class="n">timestamp2str</span><span class="p">(</span><span class="mi">1591655312</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;截止时间&#39;</span><span class="p">,</span> <span class="n">timestamp2str</span><span class="p">(</span><span class="mi">1594247312</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">创建时间 2020-6-3 10:47:19
上架时间 2020-6-9 6:28:32
截止时间 2020-7-9 6:28:32
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#众筹项目产品 所属类目信息</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;众筹类目:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;category&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># 众筹类目根链接</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;众筹类目根链接:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;source_url&#39;</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    众筹类目: {&#39;id&#39;: 35, &#39;name&#39;: &#39;Video Games&#39;, &#39;analytics_name&#39;: &#39;Video Games&#39;, &#39;slug&#39;: &#39;games/video games&#39;, &#39;position&#39;: 7, &#39;parent_id&#39;: 12, &#39;parent_name&#39;: &#39;Games&#39;, &#39;color&#39;: 51627, &#39;urls&#39;: {&#39;web&#39;: {&#39;discover&#39;: &#39;http://www.kickstarter.com/discover/categories/games/video%20games&#39;}}}
    
    众筹类目根链接: https://www.kickstarter.com/discover/categories/games/video%20games
</code></pre></div><p><br><br></p>
<h2 id="数据获取方法">数据获取方法</h2>
<p>转发分享至朋友圈，集赞50+, 加微信 372335839 ， 备注「姓名-学校-专业-Kickstarter」</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>BERTopic | 使用推特数据构建动态主题模型</title>
      <link>https://textdata.cn/blog/2022-12-03-dynamic_topic_model_with_bertopic/</link>
      <pubDate>Sun, 04 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-12-03-dynamic_topic_model_with_bertopic/</guid>
      <description>在本文中将使用BERTopic库，对美国前总统Trump推特数据集，构建动态主题模型DTM(Dynamic Topic Modeling)，可视化文档数据集中不同主题随时间的演变(变迁)。</description>
      <content:encoded><![CDATA[<p>在本文中将使用 BERTopic 库，对美国前总统 Trump 推特数据集，构建动态主题模型 DTM(Dynamic Topic Modeling)，可视化文档数据集中不同主题随时间的演变(变迁)。<strong>文末有代码下载方式</strong></p>
<br>
<h2 id="安装">安装</h2>
<p>为保证代码可复现，保证你我电脑中 bertopic 版本一致，先查看大邓电脑的 bertopic 版本</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">bertopic</span>

<span class="c1">#本文bertopic版本</span>
<span class="n">bertopic</span><span class="o">.</span><span class="n">__version__</span>
</code></pre></div><p>Run</p>
<pre><code>'0.12.0'
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#推荐指定版本安装；</span>
<span class="c1">#!pip3 install bertopic==0.12.0</span>

<span class="c1">#不指定版本安装</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">bertopic</span>
</code></pre></div><br>
<h2 id="导入数据">导入数据</h2>
<p>这里准备了twitter账号 @realDonalTrump 中 2021年的推特数据，  点击下载 <a href="code.zip"><strong>数据及代码</strong></a>。</p>
<ul>
<li>我们只分析原推特，不分析每条推特的回复。</li>
<li>因为要分析推特随时间的主题变化，需要准备 <strong>推特</strong> 及对应的 <strong>推文时间</strong></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="c1"># 导入数据</span>
<span class="n">trump_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;trump_twitter_2021.csv&#39;</span><span class="p">)</span>
<span class="n">trump_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-html" data-lang="html"><span class="p">&lt;</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">style</span> <span class="na">scoped</span><span class="p">&gt;</span>
    <span class="p">.</span><span class="nc">dataframe</span> <span class="nt">tbody</span> <span class="nt">tr</span> <span class="nt">th</span><span class="p">:</span><span class="nd">only-of-type</span> <span class="p">{</span>
        <span class="k">vertical-align</span><span class="p">:</span> <span class="kc">middle</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="p">.</span><span class="nc">dataframe</span> <span class="nt">tbody</span> <span class="nt">tr</span> <span class="nt">th</span> <span class="p">{</span>
        <span class="k">vertical-align</span><span class="p">:</span> <span class="kc">top</span><span class="p">;</span>
    <span class="p">}</span>
    
    <span class="p">.</span><span class="nc">dataframe</span> <span class="nt">thead</span> <span class="nt">th</span> <span class="p">{</span>
        <span class="k">text-align</span><span class="p">:</span> <span class="kc">right</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">&lt;/</span><span class="nt">style</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">table</span> <span class="na">border</span><span class="o">=</span><span class="s">&#34;1&#34;</span> <span class="na">class</span><span class="o">=</span><span class="s">&#34;dataframe&#34;</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">thead</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span> <span class="na">style</span><span class="o">=</span><span class="s">&#34;text-align: right;&#34;</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>id<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>text<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>isRetweet<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>isDeleted<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>device<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>favorites<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>retweets<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>date<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>isFlagged<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">thead</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">tbody</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>0<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>98454970654916608<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>Republicans and Democrats have both created ou...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>TweetDeck<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>49<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>255<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2011-08-02 18:07:48<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>1<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1234653427789070336<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>I was thrilled to be back in the Great city of...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>Twitter for iPhone<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>73748<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>17404<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2020-03-03 01:34:50<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>2<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1218010753434820614<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>RT @CBS_Herridge: READ: Letter to surveillance...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>t<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>Twitter for iPhone<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>0<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>7396<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2020-01-17 03:22:47<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>3<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1304875170860015617<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>The Unsolicited Mail In Ballot Scam is a major...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>Twitter for iPhone<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>80527<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>23502<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2020-09-12 20:10:58<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>4<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1218159531554897920<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>RT @MZHemingway: Very friendly telling of even...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>t<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>Twitter for iPhone<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>0<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>9081<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2020-01-17 13:13:59<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>f<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">tbody</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">table</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</code></pre></div><br>
<h2 id="预处理">预处理</h2>
<ul>
<li>使用正则表达式 清除推文中的http链接</li>
<li>剔除@符</li>
<li>使用正则表达式 剔除 非英文字符</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>

<span class="c1">#预处理函数clean_text</span>
<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&#34;http\S+&#34;</span><span class="p">,</span> <span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">!=</span><span class="s1">&#39;@&#39;</span><span class="p">])</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&#34;[^a-zA-Z]+&#34;</span><span class="p">,</span> <span class="s2">&#34; &#34;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">text</span>


<span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;hello @Apple, https://apple.com 李John&#39;</span>
<span class="c1">#验证函数有效性</span>
<span class="n">clean_text</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">test_text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>'hello john'
</code></pre>
<br>
<ul>
<li>对text字段使用预处理函数 clean_text</li>
<li>只保留原推文</li>
<li>准备推特tweets和时间戳 timestamps</li>
</ul>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#清洗字段text</span>
<span class="n">trump_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trump_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>

<span class="c1">#只保留特朗普原推文(剔除特朗普的Retweet)</span>
<span class="c1">#推文内容不能为”“</span>
<span class="n">trump_df</span> <span class="o">=</span> <span class="n">trump_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">trump_df</span><span class="p">[</span><span class="s1">&#39;isRetweet&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&#34;f&#34;</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">trump_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&#34;&#34;</span><span class="p">),</span> <span class="p">:]</span>

<span class="c1">#准备tweets及对应的timestamps</span>
<span class="n">tweets</span> <span class="o">=</span> <span class="n">trump_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>
<span class="n">timestamps</span> <span class="o">=</span> <span class="n">trump_df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

<span class="n">tweets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<pre><code>'republicans and democrats have both created our economic problems '
</code></pre>
<br>
<h2 id="初始化bertopic">初始化BERTopic</h2>
<p>在模型初始化阶段，使用所有推文数据， 会忽略时间维度。 该步骤会把所有时间段中出现的主题都提前训练识别出来。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>

<span class="c1">#大邓这里，运行了不到1小时</span>
<span class="c1">#特朗普比较活跃，什么内容都会参与，所以这里设置一个话题数下限为35，话题数上限不设置</span>
<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">min_topic_size</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">tweets</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    Downloading:   0%|          | 0.00/1.18k [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/190 [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/10.6k [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/612 [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/116 [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/39.3k [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/349 [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/90.9M [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/53.0 [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/112 [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/466k [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/350 [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/13.2k [00:00&lt;?, ?B/s]
    Downloading:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]
    Batches:   0%|          | 0/1418 [00:00&lt;?, ?it/s]

    2022-12-04 22:04:02,964 - BERTopic - Transformed documents to Embeddings
    2022-12-04 22:05:13,606 - BERTopic - Reduced dimensionality
    2022-12-04 22:05:17,814 - BERTopic - Clustered reduced embeddings
</code></pre></div><br>
<p>抽取出所有的话题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">freq</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_info</span><span class="p">()</span>

<span class="c1">#话题总数</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">freq</span><span class="p">))</span>
<span class="n">freq</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    169
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-html" data-lang="html"><span class="p">&lt;</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">style</span> <span class="na">scoped</span><span class="p">&gt;</span>
    <span class="p">.</span><span class="nc">dataframe</span> <span class="nt">tbody</span> <span class="nt">tr</span> <span class="nt">th</span><span class="p">:</span><span class="nd">only-of-type</span> <span class="p">{</span>
        <span class="k">vertical-align</span><span class="p">:</span> <span class="kc">middle</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="p">.</span><span class="nc">dataframe</span> <span class="nt">tbody</span> <span class="nt">tr</span> <span class="nt">th</span> <span class="p">{</span>
        <span class="k">vertical-align</span><span class="p">:</span> <span class="kc">top</span><span class="p">;</span>
    <span class="p">}</span>
    
    <span class="p">.</span><span class="nc">dataframe</span> <span class="nt">thead</span> <span class="nt">th</span> <span class="p">{</span>
        <span class="k">text-align</span><span class="p">:</span> <span class="kc">right</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">&lt;/</span><span class="nt">style</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">table</span> <span class="na">border</span><span class="o">=</span><span class="s">&#34;1&#34;</span> <span class="na">class</span><span class="o">=</span><span class="s">&#34;dataframe&#34;</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">thead</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span> <span class="na">style</span><span class="o">=</span><span class="s">&#34;text-align: right;&#34;</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>Topic<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>Count<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>Name<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">thead</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">tbody</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>0<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>-1<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>15098<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>-1_the_to_is_of<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>1<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>0<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>3182<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>0_run_president_trump_donald<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>2<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1821<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1_crowd_carolina_join_thank<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>3<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1084<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2_golf_course_doral_scotland<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>4<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>3<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1030<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>3_border_wall_immigration_mexico<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>5<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>4<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>811<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>4_china_trade_tariffs_chinese<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>6<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>5<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>642<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>5_obamacare_healthcare_repeal_website<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>7<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>6<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>638<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>6_hillary_clinton_crooked_she<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>8<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>7<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>607<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>7_amp_it_you_to<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>9<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>8<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>562<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>8_media_fake_news_failing<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">tbody</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">table</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</code></pre></div><br>
<p>-1 意识是所有的离群点(异类)推文，应该被忽略掉。接下来让我们看一下 Topic-4 的特征词及其权重</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#topic-4的特征词及权重</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [(&#39;china&#39;, 0.05289416225000891),
     (&#39;tariffs&#39;, 0.024471004754487165),
     (&#39;trade&#39;, 0.02437576425026641),
     (&#39;chinese&#39;, 0.013643270667358017),
     (&#39;us&#39;, 0.011206804363719649),
     (&#39;farmers&#39;, 0.01113584970813823),
     (&#39;our&#39;, 0.010197907480148342),
     (&#39;deal&#39;, 0.010014612658730073),
     (&#39;we&#39;, 0.009043537683534882),
     (&#39;countries&#39;, 0.00901653033214627)]
</code></pre></div><br>
<p>在二维空间中使用  Intertopic Distance Map 可视化所有主题。该图可以让我们继续创建 DTM 前，判断主题数设置的是否充分够用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">fig</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">()</span>
<span class="n">fig</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/topics.png" alt=""  />
</p>
<p>渲染的可视化文件太大，这里感兴趣的可以 <a href="topics.html">点击查看动态效果图</a></p>
<br>
<h2 id="构建dtm">构建DTM</h2>
<p>在 构建动态主题模型 前， 不同时间段中出现的主题需要预先都训练好。</p>
<ul>
<li>docs 文档数据，对应于本文的 tweets</li>
<li>timestamps 时间戳，对应于本文的 timestamps</li>
<li>global_tuning 是否将某个主题在 时间t 的主题表示向量 与 其全局主题表示向量 进行平均</li>
<li>evolution_tuning 是否将某个主题在 时间t 的主题表示向量 与 该主题在时间t-1 的主题表示向量 进行平均</li>
<li>nr_bins 时间段内含有的时间戳(点)数量。在数千个不同的时间戳中提取主题在计算上是低效的, 可以合并 20 个时间戳为一个时间段</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topics_over_time</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">topics_over_time</span><span class="p">(</span><span class="n">docs</span><span class="o">=</span><span class="n">tweets</span><span class="p">,</span> 
                                                <span class="n">timestamps</span><span class="o">=</span><span class="n">timestamps</span><span class="p">,</span> 
                                                <span class="n">global_tuning</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                                <span class="n">evolution_tuning</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                                <span class="n">nr_bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">topics_over_time</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-html" data-lang="html"><span class="p">&lt;</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">style</span> <span class="na">scoped</span><span class="p">&gt;</span>
    <span class="p">.</span><span class="nc">dataframe</span> <span class="nt">tbody</span> <span class="nt">tr</span> <span class="nt">th</span><span class="p">:</span><span class="nd">only-of-type</span> <span class="p">{</span>
        <span class="k">vertical-align</span><span class="p">:</span> <span class="kc">middle</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="p">.</span><span class="nc">dataframe</span> <span class="nt">tbody</span> <span class="nt">tr</span> <span class="nt">th</span> <span class="p">{</span>
        <span class="k">vertical-align</span><span class="p">:</span> <span class="kc">top</span><span class="p">;</span>
    <span class="p">}</span>
    
    <span class="p">.</span><span class="nc">dataframe</span> <span class="nt">thead</span> <span class="nt">th</span> <span class="p">{</span>
        <span class="k">text-align</span><span class="p">:</span> <span class="kc">right</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">&lt;/</span><span class="nt">style</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">table</span> <span class="na">border</span><span class="o">=</span><span class="s">&#34;1&#34;</span> <span class="na">class</span><span class="o">=</span><span class="s">&#34;dataframe&#34;</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">thead</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span> <span class="na">style</span><span class="o">=</span><span class="s">&#34;text-align: right;&#34;</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>Topic<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>Words<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>Frequency<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>Timestamp<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>Name<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">thead</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">tbody</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>0<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>-1<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>donald, keychain, champion, trump, contest<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>20<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2009-04-30 12:30:07.596999936<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>-1_the_to_is_of<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>1<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>0<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>donald, execute, imagination, step, randal<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>9<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2009-04-30 12:30:07.596999936<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>0_run_president_trump_donald<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>2<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>begun, schedule, ahead, international, scotland<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2009-04-30 12:30:07.596999936<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2_golf_course_doral_scotland<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>3<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>3<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>cling, wallflower, persona, walls, rather<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2009-04-30 12:30:07.596999936<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>3_border_wall_immigration_mexico<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>4<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>10<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>independence, safe, here, enjoy, happy<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>1<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2009-04-30 12:30:07.596999936<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>10_veterans_honor_heroes_our<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>...<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>1880<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>162<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>ratings, fredo, frank, bad, based<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2020-06-09 07:29:57.849999872<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>162_ratings_machine_show_sided<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>1881<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>163<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>yes, no, way, absolutely,<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2020-06-09 07:29:57.849999872<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>163_yes_no_absolutely_way<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>1882<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>164<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>twitter, trending, section, trends, conservative<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>13<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2020-06-09 07:29:57.849999872<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>164_twitter_trending_conservative_sectio...<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>1883<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>165<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>york, eaten, hell, new, blasio<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>4<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2020-06-09 07:29:57.849999872<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>165_york_ny_new_wonerful<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">tr</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">th</span><span class="p">&gt;</span>1884<span class="p">&lt;/</span><span class="nt">th</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>167<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>mixing, courthouse, mocked, notes, prosecuted<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>3<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>2020-06-09 07:29:57.849999872<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
      <span class="p">&lt;</span><span class="nt">td</span><span class="p">&gt;</span>167_jury_judge_guilty_foreperson<span class="p">&lt;/</span><span class="nt">td</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">tr</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">tbody</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">table</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">p</span><span class="p">&gt;</span>1885 rows × 5 columns<span class="p">&lt;/</span><span class="nt">p</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</code></pre></div><br>
<h2 id="可视化dtm">可视化DTM</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#模型中一共有169个主题，这里显示前Top10的主题的演变</span>
<span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics_over_time</span><span class="p">(</span><span class="n">topics_over_time</span><span class="p">,</span> <span class="n">top_n_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/dtm.png" alt=""  />

渲染的可视化文件太大，这里感兴趣的可以 <a href="dtm.html">点击查看动态效果图</a></p>
<br>
<h3 id="获取本文代码">获取本文代码</h3>
<p>点击获取 <a href="code.zip"><strong>数据及代码</strong></a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>JM2022综述 | 黄金领域: 为营销研究(新洞察)采集网络数据</title>
      <link>https://textdata.cn/blog/2022-12-03-scraping-web-data-for-marketing-insights/</link>
      <pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-12-03-scraping-web-data-for-marketing-insights/</guid>
      <description>Journal of Marketing 2022年一篇关于营销领域网络爬虫的文献综述</description>
      <content:encoded><![CDATA[<p>Boegershausen, Johannes, Hannes Datta, Abhishek Borah, and Andrew Stephen. &ldquo;Fields of gold: Scraping web data for marketing insights.&rdquo; <em>Journal of Marketing</em> (2022).</p>
<p>本文是JM中少有的技术流综述文，阅读起来晦涩难懂，我们就大概知道怎么回事， 查看有没有自己感兴趣的研究(方法)即可。该文作者为该综述专门开发了一个 web-scraping.org 的网站,截图如下</p>
<p><img loading="lazy" src="img/01-web-scraping.png" alt=""  />

<img loading="lazy" src="img/02-web-scraping.png" alt=""  />
</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/KiyFyLEkqNk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<br>
<h2 id="摘要">摘要</h2>
<p>市场营销学者越来越多使用网络爬虫和API接口，从互联网收集数据。尽管网络数据得到广泛使用，但很少有学者关注收集过程中面临的各种挑战。<strong>研究人员如何确保采集的数据集是有效的？</strong> 虽然现有资源强调提取网络数据的技术细节，<strong>但作者提出了一种新的方法框架，重点是提高其有效性</strong>。特别是，该框架强调解决有效性问题， 需要在数据采集的三个阶段(<strong>选择数据源、设计数据收集和提取数据</strong>)联合考虑技术和法律/伦理问题。作者进一步审查了营销Top5期刊上300 篇使用网络数据的论文，并总结提出了如何使用网络数据促进营销研究。本文最后指出了未来研究的方向，高价值的网络数据源和新方法。</p>
<p><strong>Keywords：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- web scraping
- application programming interface, API
- crawling
- validity
- user-generated content
- social media
big data
</code></pre></div><br>
<h2 id="一网络数据的魅力">一、网络数据的魅力</h2>
<p>社会和商业生活的加速数字化创造了数量空前的消费者和企业行为数字痕迹。 每分钟，全球用户在 Google 上进行 570 万次搜索，进行 600 万次商业交易，并在 Instagram 上分享6.5万张照片（Statista 2021）。 由此产生的网络数据——规模庞大、形式多样，而且通常可以在互联网上公开访问——对于那些想要量化消费、深入了解企业行为并跟踪难以或昂贵地观察社会活动的营销学者来说，这是一个潜在的金矿 . 网络数据对营销研究的重要性反映在越来越多的有影响力的出版物中，涵盖消费者文化理论、消费者心理学、实证建模和营销策略等。</p>
<p><img loading="lazy" src="img/fig-1-increased-use-of-web-data-in-marketing.png" alt=""  />
</p>
<p>整理了 <strong>营销领域 top 5 期刊( JM、JMR、JCR、JCP、MS) 的 313 篇论文</strong> ，经过整理绘制图-1（Figure1）， 使用网络数据进行研究的量呈现快速上涨的趋势。使用网络数据的论文占比，从2010年的4%提升到2020年的15%。 者313篇论文，数据的获取方式统计</p>
<ul>
<li>**59% 的论文使用了 <strong>网络爬虫</strong> 采集数据</li>
<li>12% 的论文使用API收集数据</li>
<li>9% 的论文同时使用了网络爬虫和API</li>
<li>20% 使用人工从网站手动复制粘贴数据</li>
</ul>
<p><strong>使用 网络数据 的论文，平均被引用次数 7.55， 远高于 非网络数据 的 3.90</strong>。</p>
<br>
<p>使用网络数据做新研究，大致有4种实现路径</p>
<ol>
<li><strong>研究新现象，新场景</strong>
<ul>
<li>网络世界产生的不同于现实世界的情景，可以研究新现象</li>
</ul>
</li>
<li><strong>繁荣生态价值</strong>
<ul>
<li>比如，对亚马逊评论数据进行研究，研究发现可以帮助亚马逊平台进行改善。</li>
</ul>
</li>
<li><strong>促进方法论进步</strong>
<ul>
<li>文本、图片、音频、视频等</li>
</ul>
</li>
<li><strong>提高测量效果(快、准、好、全)</strong>
<ul>
<li>借助一些API，可以对已有的数据集增加新的信息量。</li>
<li>例如，日期数据，结合HolidayAPI，可以查看日期的节假日信息</li>
<li>给定日期和IP地址，使用Weather Underground可以查看天气信息</li>
</ul>
</li>
</ol>
<p><img loading="lazy" src="img/table-1-four-pathway-of-knowledge-creation-using-web-data.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二数据采集的方法框架">二、数据采集的方法框架</h2>
<p>在使用 **网络爬虫 和 API ** 自动收集网络数据时，研究人员通常会在 **研究有效性、技术可行性和法律/伦理风险 **1 三者间权衡利弊得失，研究人员如何解决这些权衡，通过增强或破坏 <strong>统计结论有效性、内部有效性、结构有效性和外部有效性</strong> 来塑造研究结果的可信度（Shadish、Cook 和 Campbell 2002）。</p>
<p><img loading="lazy" src="img/fig-2-methodological-framework-for-collecting-web-data.png" alt=""  />
</p>
<p>本文开发了一个方法框架，为使用 网络爬虫 和 API 自动收集网络数据提供指导。图 2（Figure 2） 涵盖三个关键阶段</p>
<ul>
<li><strong>数据源选择</strong></li>
<li><strong>设计方案</strong>
<ul>
<li>从网站中抽取哪些信息</li>
<li>采集频率，即 每天(周/月)重复运行一次爬虫，得到面板数据</li>
</ul>
</li>
<li><strong>执行数据采集</strong>
<ul>
<li>如何改善爬虫运行效率</li>
<li>如何处理原始信息，完整的保存为原始格式html、json，还是只抽取存储当前想要的字段</li>
</ul>
</li>
</ul>
<p>研究人员通常从一组广泛的潜在数据源开始，并根据三个关键考虑因素（有效性、技术可行性和法律/道德风险）剔除其中一些数据源。这三个考虑因素出现在倒金字塔的角落，底部的有效性强调其重要性。鉴于在收集最终数据集之前难以预测其确切特征，研究人员在设计、原型化和完善数据收集时经常重新考虑这些因素。未能解决技术或法律/伦理问题可能意味着网络数据无法有意义地告知研究问题。</p>
<h3 id="21-数据源面临的挑战解决办法">2.1 数据源面临的挑战(解决办法)</h3>
<ol>
<li>探索潜在网络数据源
<ul>
<li>由于网络资源在质量、稳定性和可检索性方面存在巨大差异，研究人员可能倾向于只考虑主要或熟悉的平台。 对数据世界的彻底探索允许令人信服的理论检验和识别可能难以以其他方式注意到的新颖的、新兴的营销现象。</li>
</ul>
</li>
<li>考虑网络爬虫的替代方案
<ul>
<li>由于网络抓取是最流行的网络数据提取方法，研究人员可能会忽视其他提取数据的方法。 API 提供了一种记录和授权的方式来获取许多来源的 Web 数据。 一些来源还提供现成的数据集。 使用此类替代方案可以节省时间并最大限度地减少法律风险。</li>
</ul>
</li>
<li>将数据与场景结合对应起来
<ul>
<li>Web 数据通常没有大量的文档。 尽早识别潜在相关的背景信息对于研究的相关性和有效性至关重要。
<img loading="lazy" src="img/table-2-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />
</li>
</ul>
</li>
</ol>
<br>
<h3 id="22-设计数据采集方案">2.2 设计数据采集方案</h3>
<ol>
<li>从页面抽取什么信息，从有效性、合法、技术可行性 三个方面论证。</li>
<li>如何进行数据抽样？</li>
<li>以什么频率(每天、周、月)进行数据采集</li>
</ol>
<p><img loading="lazy" src="img/table-3-1-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />

<img loading="lazy" src="img/table-3-2-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />
</p>
<br>
<h3 id="23-执行数据采集">2.3 执行数据采集</h3>
<ol>
<li>如何改善爬虫运行效率</li>
<li>如何监控数据质量</li>
<li>整理数据文档(记录)
<img loading="lazy" src="img/table-4-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />
</li>
</ol>
<br>
<h2 id="部分参考文献">部分参考文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]Allard, Thomas, Lea H. Dunn, and Katherine White. &#34;Negative reviews, positive impact: Consumer empathetic responding to unfair word of mouth.&#34; Journal of Marketing 84, no. 4 (2020): 86-108.
[2]Gao, Weihe, Li Ji, Yong Liu, and Qi Sun. &#34;Branding cultural products in international markets: a study of hollywood movies in China.&#34; Journal of Marketing 84, no. 3 (2020): 86-105.
[3]Reich, Taly, and Sam J. Maglio. &#34;Featuring mistakes: The persuasive impact of purchase mistakes in online reviews.&#34; Journal of Marketing 84, no. 1 (2020): 52-65.
[4]Lee, Jeffrey K., and Ann Kronrod. &#34;The strength of weak-tie consensus language.&#34; Journal of Marketing Research 57, no. 2 (2020): 353-374.
[5]Matz, Sandra C., Cristina Segalin, David Stillwell, Sandrine R. Müller, and Maarten W. Bos. &#34;Predicting the personal appeal of marketing images using computational methods.&#34; Journal of Consumer Psychology 29, no. 3 (2019): 370-390.
[6]Dai, Hengchen, and Dennis J. Zhang. &#34;Prosocial goal pursuit in crowdfunding: Evidence from kickstarter.&#34; Journal of Marketing Research 56, no. 3 (2019): 498-517.
[7]Luffarelli, Jonathan, Mudra Mukesh, and Ammara Mahmood. &#34;Let the logo do the talking: The influence of logo descriptiveness on brand equity.&#34; Journal of Marketing Research 56, no. 5 (2019): 862-878.
[8]Bond, Samuel D., Stephen X. He, and Wen Wen. &#34;Speaking for “free”: Word of mouth in free-and paid-product settings.&#34; Journal of Marketing Research 56, no. 2 (2019): 276-290.
[9]Han, Kyuhong, Jihye Jung, Vikas Mittal, Jinyong Daniel Zyung, and Hajo Adam. &#34;Political identity and financial risk taking: Insights from social dominance orientation.&#34; Journal of Marketing Research 56, no. 4 (2019): 581-601.
[10]Netzer, Oded, Alain Lemaire, and Michal Herzenstein. &#34;When words sweat: Identifying signals for loan default in the text of loan applications.&#34; Journal of Marketing Research 56, no. 6 (2019): 960-980.
[11]Toubia, Olivier, Garud Iyengar, Renée Bunnell, and Alain Lemaire. &#34;Extracting features of entertainment products: A guided latent dirichlet allocation approach informed by the psychology of media consumption.&#34; Journal of Marketing Research 56, no. 1 (2019): 18-36.
[12]Van Laer, Tom, Jennifer Edson Escalas, Stephan Ludwig, and Ellis A. Van Den Hende. &#34;What happens in Vegas stays on TripAdvisor? A theory and technique to understand narrativity in consumer reviews.&#34; Journal of Consumer Research 46, no. 2 (2019): 267-285.
[13]Zhong, Ning, and David A. Schweidel. &#34;Capturing changes in social media content: A multiple latent changepoint topic model.&#34; Marketing Science 39, no. 4 (2020): 827-846.
[14]Colicev, Anatoli, Ashwin Malshe, Koen Pauwels, and Peter O&#39;Connor. &#34;Improving consumer mindset metrics and shareholder value through social media: The different roles of owned and earned media.&#34; Journal of Marketing 82, no. 1 (2018): 37-56.
[15]Liu, Xuan, Savannah Wei Shi, Thales Teixeira, and Michel Wedel. &#34;Video content marketing: The making of clips.&#34; Journal of Marketing 82, no. 4 (2018): 86-101.
[16]Liu, Jia, and Olivier Toubia. &#34;A semantic approach for estimating consumer content preferences from online search queries.&#34; Marketing Science 37, no. 6 (2018): 930-952.
[17]Nam, Hyoryung, Yogesh V. Joshi, and P. K. Kannan. &#34;Harvesting brand information from social tags.&#34; Journal of Marketing 81, no. 4 (2017): 88-108.
[18]Packard, Grant, and Jonah Berger. &#34;How language shapes word of mouth&#39;s impact.&#34; Journal of Marketing Research 54, no. 4 (2017): 572-588.
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>社会学研究 | 社会计算驱动的社会科学研究方法</title>
      <link>https://textdata.cn/blog/2022-12-03-social-computing-methodology-about-big-data-and-artificial-intelligence/</link>
      <pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-12-03-social-computing-methodology-about-big-data-and-artificial-intelligence/</guid>
      <description>一篇关于计算社会学方法论的综述性论文</description>
      <content:encoded><![CDATA[<p><strong><a href="%E7%A4%BE%E4%BC%9A%E8%AE%A1%E7%AE%97%E9%A9%B1%E5%8A%A8%E7%9A%84%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95_%E5%91%A8%E6%B6%9B.pdf">周涛,高馨,罗家德.社会计算驱动的社会科学研究方法[J].社会学研究,2022,37(05):130-155+228-229.</a></strong></p>
<p><img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-01.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-02.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-03.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-04.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-05.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-06.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-07.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-08.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-09.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-10.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-11.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-12.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-13.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-14.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-15.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-16.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-17.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-18.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-19.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-20.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-21.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集(付费) | 90w条中国上市公司高管数据</title>
      <link>https://textdata.cn/blog/2022-11-25-senior-manager-resume-dataset/</link>
      <pubDate>Fri, 25 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-25-senior-manager-resume-dataset/</guid>
      <description>数据集 | 90w条中国上市公司高管数据</description>
      <content:encoded><![CDATA[<p>90w条中国上市公司高管简历，数据源-新浪财经，统计的日期范围1990-2021年。</p>
<p><br><br></p>
<h2 id="相关论文">相关论文</h2>
<p>这里粘贴部分应用高管数据论文</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 何瑛,于文蕾,戴逸驰,王砚羽.高管职业经历与企业创新[J].管理世界,2019,35(11):174-192.
- 杨林,和欣,顾红芳.高管团队经验、动态能力与企业战略突变：管理自主权的调节效应[J].管理世界,2020,36(06):168-188+201+252.
- 周楷唐,麻志明,吴联生.高管学术经历与公司债务融资成本[J].经济研究,2017,52(07):169-183.
- 陆瑶,张叶青,黎波,赵浩宇.高管个人特征与公司业绩——基于机器学习的经验证据[J].管理科学学报,2020,23(02):120-140.
- 柳光强,孔高文.高管经管教育背景与企业内部薪酬差距[J].会计研究,2021,(03):110-121.
- 郑建明,孙诗璐,李金甜.高管文化背景与企业债务成本——基于劳模文化的视角[J].会计研究,2021,(03):137-145.
</code></pre></div><p><br><br></p>
<h2 id="一数据集字段">一、数据集字段</h2>
<blockquote>
<p>数据集的字段含，大多是从「个人简历」中计算衍生出来的。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- ID
- 姓名
- 证券代码
- 统计截止日期
- 个人简历
- 国籍
- 籍贯
- 籍贯所在地区代码
- 出生地
- 出生地所在地区代码
- 性别
- 年龄
- 毕业院校
- 学历  1=中专及中专以下； 2=大专； 3=本科； 4=硕士研究生； 5=博士研究生； 6=其他（以其他形式公布的学历，如荣誉博士、函授等）； 7=MBA/EMBA
- 专业
- 职称
- 是否领取薪酬
- 报告期报酬总额
- 年末持股数
- 是否高管团队成员
- 是否董事会成员
- 是否独立董事
- 是否兼任董事长和CEO
- 是否监事
- 具体职务
</code></pre></div><p><br><br></p>
<h2 id="二实验代码">二、实验代码</h2>
<h3 id="21-读取数据">2.1 读取数据</h3>
<ul>
<li>数据文件 <code>高管数据.xlsx</code></li>
<li>强制某几个字段的数据类型</li>
<li>将字段 「统计截止日期」 转化为 datetime 类型</li>
</ul>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import pandas as pd

# 导入数据，
df = pd.read_excel(&#39;高管数据.xlsx&#39;, 
                   #保证这两个字段是字符串格式
                   converters={&#39;证券代码&#39;: str, 
                               &#39;ID&#39;: str})

#将字段「统计截止日期」 整理为datetime格式
df[&#39;统计截止日期&#39;] = pd.to_datetime(df[&#39;统计截止日期&#39;])
#显示前1条记录
df.head(1)
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<h3 id="22-字段">2.2 字段</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df.columns
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    Index([&#39;ID&#39;, &#39;姓名&#39;, &#39;证券代码&#39;, &#39;统计截止日期&#39;, &#39;个人简历&#39;, &#39;国籍&#39;, &#39;籍贯&#39;, &#39;籍贯所在地区代码&#39;, &#39;出生地&#39;,
           &#39;出生地所在地区代码&#39;, &#39;性别&#39;, &#39;年龄&#39;, &#39;毕业院校&#39;, &#39;学历&#39;, &#39;专业&#39;, &#39;职称&#39;, &#39;是否领取薪酬&#39;, &#39;报告期报酬总额&#39;,
           &#39;津贴&#39;, &#39;年末持股数&#39;, &#39;是否高管团队成员&#39;, &#39;是否董事会成员&#39;, &#39;是否独立董事&#39;, &#39;是否兼任董事长和CEO&#39;, &#39;是否监事&#39;,
           &#39;具体职务&#39;],
          dtype=&#39;object&#39;)
</code></pre></div><br>
<h3 id="23-记录数">2.3 记录数</h3>
<p>数据集记录数共</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">len(df)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    900887
</code></pre></div><br>
<h3 id="24-覆盖日期">2.4 覆盖日期</h3>
<p>数据统计日期范围自 1990年12月10日 至 2021年7月19日</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df[&#39;统计截止日期&#39;].sort_values()
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    900886   1990-12-10
    900884   1990-12-10
    900883   1990-12-10
    900882   1990-12-10
    900881   1990-12-10
                ...    
    59734    2021-07-19
    59733    2021-07-19
    59731    2021-07-19
    59736    2021-07-19
    59742    2021-07-19
    Name: 统计截止日期, Length: 900887, dtype: datetime64[ns]
</code></pre></div><br>
<p>数据集字段 有</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df.columns
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    Index([&#39;ID&#39;, &#39;姓名&#39;, &#39;证券代码&#39;, &#39;统计截止日期&#39;, &#39;个人简历&#39;, &#39;国籍&#39;, &#39;籍贯&#39;, &#39;籍贯所在地区代码&#39;, &#39;出生地&#39;,
           &#39;出生地所在地区代码&#39;, &#39;性别&#39;, &#39;年龄&#39;, &#39;毕业院校&#39;, &#39;学历&#39;, &#39;专业&#39;, &#39;职称&#39;, &#39;是否领取薪酬&#39;, &#39;报告期报酬总额&#39;,
           &#39;津贴&#39;, &#39;年末持股数&#39;, &#39;是否高管团队成员&#39;, &#39;是否董事会成员&#39;, &#39;是否独立董事&#39;, &#39;是否兼任董事长和CEO&#39;, &#39;是否监事&#39;,
           &#39;具体职务&#39;],
          dtype=&#39;object&#39;)
</code></pre></div><br>
<h3 id="25-按条件筛选记录">2.5 按条件筛选记录</h3>
<p>截止统计日期时大于90岁的高管 记录有</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df[df[&#39;年龄&#39;]&gt;90]
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三相关内容">三、相关内容</h2>
<p><strong>何瑛,于文蕾,戴逸驰,王砚羽.高管职业经历与企业创新[J].管理世界,2019,35(11):174-192.</strong></p>
<blockquote>
<p>摘要:管理的本质是一种实践,在某些情形下,<strong>阅历比简历更重要,丰富的职业经历有助于企业高管形成多元化的思维结构、广阔的管理视野、丰富的社会资源和过人的胆识,也是塑造复合型人才的重要路径</strong>。本文基于行为金融理论和高层梯队理论,手工搜集整理了2007～2016年中国沪深A股上市公司高管职业经历独特数据集,从<strong>职能部门、企业、行业、组织机构和地域类型</strong>五个维度构建了复合型职业经历的衡量指标——职业经历丰富度指数,对CEO职业经历与企业创新的影响因素和影响机理进行理论解释、数据分析和验证。研究结果表明:CEO职业经历越丰富,企业创新水平越高,<strong>其中跨企业经历对创新水平的影响最为显著</strong>,其次是跨行业经历和跨组织机构经历,跨职能部门经历和跨地域经历对企业创新水平的影响最小;影响因素方面,基于公司内外部治理的视角发现,市场化程度越低、企业融资约束程度越低时,CEO职业经历丰富度对企业创新水平的促进作用越明显,国有企业CEO职业经历丰富度对企业创新水平的促进作用更强,而股权制衡度对CEO职业经历丰富度与企业创新水平的调节作用不明显;影响机理方面,CEO复合型职业经历主要是通过丰富高管的社会网络资源以及增强高管的风险偏好倾向,从而提升企业的创新水平。本文的研究结论拓展了企业创新影响因素及高管职业经历经济后果领域的相关文献,将复合型人才的影响从国家宏观层面拓展到企业微观层面,为企业高层次人才的招聘和选拔提供新的证据支持。 中提到高管的创新</p>
</blockquote>
<p>高管，一般是有多个企业经历的， 如何将高管职业经历转化为可以计算和比较的 <strong>高管职业经历向量</strong> 呢？</p>
<p><a href="https://mp.weixin.qq.com/s/1bs8ZS4upx25C08f2uBjBA"><strong>如何用「图嵌入」将企业、高管职业经历表征为向量数据</strong></a> , 有了向量可以</p>
<ul>
<li>计算高管之间的相似度</li>
<li>企业高管团队异质性，计算高管向量之间的距离</li>
<li>&hellip;</li>
</ul>
<p><br><br></p>
<h2 id="四数据获取">四、数据获取</h2>
<p>数据集 50 元， 加微信 <strong>372335839</strong>, 备注【姓名-学校-专业-高管】获取本数据集。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>训练&amp;使用Glove语言模型， 可度量刻板印象等</title>
      <link>https://textdata.cn/blog/2022-11-22-glove-embeddings-model/</link>
      <pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-22-glove-embeddings-model/</guid>
      <description>训练&amp;amp;使用Glove语言模型， 可度量刻板印象等</description>
      <content:encoded><![CDATA[<p>Glove可以捕捉到词语在语料库中的全局语义信息和类比信息， 据此基于语义向量计算刻板印象、文化变迁等，Glove模型在计算社会科学中拥有很大的应用潜力。</p>
<p><img loading="lazy" src="img/wordpaths.png" alt=""  />
</p>
<p>训练Glove模型有两种实现方式</p>
<ol>
<li>C语言；  <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></li>
<li>Python语言；mittens、glove-python</li>
</ol>
<p><img loading="lazy" src="img/stanford%e8%ae%ad%e7%bb%83Glove.png" alt=""  />
</p>
<h2 id="方法比较">方法比较</h2>
<table>
<thead>
<tr>
<th style="text-align:left">方法</th>
<th style="text-align:left">优点</th>
<th style="text-align:left">缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">C语言</td>
<td style="text-align:left">速度快，现成的代码工具</td>
<td style="text-align:left">源代码仅支持英文, 需要付出较高的学习成本才能改动支持中文。 对文科生小白而言，门槛高</td>
</tr>
<tr>
<td style="text-align:left">Python语言</td>
<td style="text-align:left">mittens、glove-python等包语法简洁, 易上手</td>
<td style="text-align:left">对文科生还是有一定的门槛，代码运行速度慢</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td></td>
</tr>
</tbody>
</table>
<br>
<p>不考虑性能约束条件，更多地考虑易用性，大邓简化了Python代码，将其内置到了cntext库。</p>
<p>对词向量、词嵌入感兴趣的童鞋，可以阅读下列相关资料</p>
<ul>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
</ul>
<br>
<h2 id="glove代码">GloVe代码</h2>
<p>cntext支持中英文， 只需要7行代码，可完成导入数据、训练模型、保存结果。 这里以三体小说数据为例， 使用 <a href="santi.txt"><strong>data/santi.txt</strong></a> 。</p>
<p><strong>需要注意， santi.txt文件内文本是已经分词处理过的</strong>。这样可以在english这类西方语言模式下使用空格来区分词语的边界。</p>
<blockquote>
<p>如果使用英文数据，下面代码只需要更改数据文件的路径即可。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#设置语言和项目文件夹路径</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Glove</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="c1">#导入语料</span>
<span class="n">model</span><span class="o">.</span><span class="n">create_vocab</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/santi.txt&#39;</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">#构建词语共现矩阵</span>
<span class="n">model</span><span class="o">.</span><span class="n">cooccurrence_matrix</span><span class="p">()</span>
<span class="c1">#设置词嵌入模型的向量维度、迭代数</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_embeddings</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="c1">#存储模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;santi_glove_model&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Building prefix dict from the default dictionary ...
Step 1/4: ...Create vocabulary for Glove.

Dumping model to file cache C:\Users\Deng\AppData\Local\Temp\jieba.cache
Loading model cost 0.628 seconds.

Prefix dict has been built successfully.

Step 2/4: ...Create cooccurrence matrix.
Step 3/4: ...Train glove embeddings. 
             Note, this part takes a long time to run

Iteration 20: error 64925132.71550
Step 3/4: ... Finish! Use 316.91 s

Step 4/4: ... Save the glove embeddings to a txt file
</code></pre></div><br>
<h2 id="导入glove预训练模型">导入GloVe预训练模型</h2>
<p>训练好的GloVe模型是txt文件，可以使用gensim导入。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1"># 导入GloVe模型文件</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;output/Glove/santi_glove_model.txt&#39;</span><span class="p">,</span>  <span class="n">no_header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">#查看某词的词向量</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;宇宙&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 0.6618259 ,  0.60663235,  0.9849417 , -1.028956  ,  1.0711069 ,
       -0.8875306 , -0.52833366, -1.0125595 , -0.9628481 ,  1.0356479 ,
        0.8595257 ,  0.7454354 , -1.0468111 , -0.26285014, -1.0310447 ,
        0.9906805 ,  0.05825566, -0.85581344, -0.9932533 , -1.020438  ,
        1.0495061 , -0.6973389 ,  0.49099424, -0.80775315,  0.64256483,
        1.0157642 ,  1.0135043 , -1.0131834 ,  0.17376372,  0.89585054,
        0.30890268,  0.798895  ,  0.6653925 ,  0.908629  , -1.048273  ,
       -0.35683677,  0.06306187, -1.0267074 , -1.0494691 ,  0.42172813,
        0.24005401,  0.5934993 , -0.0696691 , -1.0360557 , -0.9797269 ,
        1.0205714 , -0.376359  , -1.0501183 ,  1.0415571 , -0.9312968 ],
      dtype=float32)
</code></pre></div><br>
<h2 id="模型的使用">模型的使用</h2>
<p>语料中所有的词语都是维度相同的向量，可以根据向量计算找近义词、反义词。可参考 之前分享的   <a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.</li>
<li>William L. Hamilton, Jure Leskovec, and Dan Jurafsky. ACL 2016. Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.</li>
<li>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.
<ul>
<li><a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></li>
</ul>
</li>
<li><a href="https://github.com/hiDaDeng/cntext">https://github.com/hiDaDeng/cntext</a></li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>FinBERT | 金融文本BERT模型，可情感分析、识别ESG和FLS类型</title>
      <link>https://textdata.cn/blog/2022-11-17-finbert-finance-bert-model/</link>
      <pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-17-finbert-finance-bert-model/</guid>
      <description>金融语言模型</description>
      <content:encoded><![CDATA[<h2 id="finbert介绍">FinBERT介绍</h2>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/uj4hm7Lr2Wo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<br>
<p>FinBERT， 是使用49亿词的英文金融语料库数据，生成的BERT预训练语言模型。语料库上大小为 49亿个词。</p>
<ul>
<li>公司报告 10-K 和 10-Q：25亿个词</li>
<li>电话会议记录：13亿个词</li>
<li>分析师报告：11亿个词</li>
</ul>
<p>FinBERT开发者在多个金融 NLP 任务上对 FinBERT 预训练模型进行了微调，均优于传统机器学习模型、深度学习模型和微调 BERT 模型。 所有经过微调的 FinBERT 模型都公开托管在 Huggingface 🤗。  目前支持包括<strong>情绪分析、ESG 分类、前瞻性陈述 (FLS) 分类</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Huang, Allen H., Hui Wang, and Yi Yang. &#34;FinBERT: A large language model for extracting information from financial text.&#34; Contemporary Accounting Research (2022).

摘要（翻译）: 我们开发了 FinBERT，这是一种适用于金融领域的最先进的大型语言模型。我们表明，FinBERT 结合了金融知识，可以更好地总结金融文本中的上下文信息。使用分析报告中研究人员标记的句子样本，我们证明 FinBERT 大大优于 Loughran 和 McDonald 词典以及其他机器学习算法，包括朴素贝叶斯、支持向量机、随机森林、卷积神经网络和长短期记忆，在情感分类中。我们的结果表明，FinBERT 擅长识别其他算法错误标记为中性的句子的正面或负面情绪，这可能是因为它使用了金融文本中的上下文信息。我们发现，FinBERT 优于其他算法，以及 Google 的原始双向编码器表示形式来自 transformers (BERT) 模型，当训练样本量较小且文本中包含一般文本中不常用的金融词时，这种优势尤为突出。 FinBERT 在识别与环境、社会和治理问题相关的讨论方面也优于其他模型。最后，我们表明，与 FinBERT 相比，其他方法低估了收益电话会议的文本信息量至少 18%。我们的结果对学术研究人员、投资专业人士和金融市场监管机构具有重要意义。
</code></pre></div><br>
<h3 id="finbert功能">FinBERT功能</h3>
<p>具体来说，FinBERT有以下内容：</p>
<ul>
<li><a href="https://huggingface.co/yiyanghkust/finbert-pretrain">FinBERT-Pretrained</a>： 针对大规模金融文本的预训练 FinBERT 模型。</li>
<li><a href="https://huggingface.co/yiyanghkust/finbert-tone">FinBERT-Sentiment</a>： 用于情感分类任务。</li>
<li><a href="https://huggingface.co/yiyanghkust/finbert-esg">FinBERT-ESG</a>： 用于 ESG 分类任务。</li>
<li><a href="https://huggingface.co/yiyanghkust/finbert-fls">FinBERT-FLS</a>： 用于前瞻性陈述（FLS）分类任务。</li>
</ul>
<br>
<h3 id="环境配置">环境配置</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install transformers==4.18.0
</code></pre></div><p>本次实验使用的transformers版本为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import transformers
transformers.__version__
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">4.18.0
</code></pre></div><br>
<h3 id="代码下载">代码下载</h3>
<p><a href="FinBERT.ipynb">点击下载</a></p>
<p><br><br></p>
<h2 id="一情感分析">一、情感分析</h2>
<p>金融文本情绪可以调动管理者、信息中介和投资者的观点和意见, 因此分析金融文本情感(情绪)是有价值的。 FinBERT-Sentiment 是一个 FinBERT 模型，它根据标准普尔 500 家公司的分析师报告中的 10,000 个手动注释的句子进行了Fine-tune(微调)。</p>
<blockquote>
<p>Fine-Tune微调 是 深度学习的一种语言处理技术，可以在前人（已有）的语言模型文件基础上加入少量新场景的文本数据进行更新训练，生成出新场景的语言模型。</p>
</blockquote>
<ul>
<li><strong>输入</strong>：金融文本。</li>
<li><strong>输出</strong>：Positive, Neutral or Negative.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1">#首次运行，因为会下载FinBERT模型，耗时会比较久</span>
<span class="n">senti_finbert</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-tone&#39;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">senti_tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-tone&#39;</span><span class="p">)</span>
<span class="n">senti_nlp</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&#34;text-classification&#34;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">senti_finbert</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">senti_tokenizer</span><span class="p">)</span>
</code></pre></div><p><br>使用3条测试文本进行测试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 待分析的文本数据</span>
<span class="n">senti_results</span> <span class="o">=</span> <span class="n">senti_nlp</span><span class="p">([</span><span class="s1">&#39;growth is strong and we have plenty of liquidity.&#39;</span><span class="p">,</span> 
                           <span class="s1">&#39;there is a shortage of capital, and we need extra financing.&#39;</span><span class="p">,</span>
                           <span class="s1">&#39;formulation patents might protect Vasotec to a limited extent.&#39;</span><span class="p">])</span>
<span class="n">senti_results</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [{&#39;label&#39;: &#39;Positive&#39;, &#39;score&#39;: 1.0},
     {&#39;label&#39;: &#39;Negative&#39;, &#39;score&#39;: 0.9952379465103149},
     {&#39;label&#39;: &#39;Neutral&#39;, &#39;score&#39;: 0.9979718327522278}]
</code></pre></div><p><br><br></p>
<h2 id="二esg分类">二、ESG分类</h2>
<p>ESG 分析可以帮助投资者确定企业的长期可持续性并识别相关风险。 FinBERT-ESG 是一个 FinBERT 模型，根据来自公司 ESG 报告和年度报告的 2,000 个手动注释句子进行微调。</p>
<ul>
<li><strong>输入</strong>：金融文本。</li>
<li><strong>输出</strong>：Environmental, Social, Governance or None.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">esg_finbert</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-esg&#39;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">esg_tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-esg&#39;</span><span class="p">)</span>
<span class="n">esg_nlp</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&#34;text-classification&#34;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">esg_finbert</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">esg_tokenizer</span><span class="p">)</span>
</code></pre></div><p><br>使用3条测试文本进行测试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">esg_results</span> <span class="o">=</span> <span class="n">esg_nlp</span><span class="p">([</span><span class="s1">&#39;Managing and working to mitigate the impact our operations have on the environment is a core element of our business.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;Rhonda has been volunteering for several years for a variety of charitable community programs.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;Cabot</span><span class="se">\&#39;</span><span class="s1">s annual statements are audited annually by an independent registered public accounting firm.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;As of December 31, 2012, the 2011 Term Loan had a principal balance of $492.5 million.&#39;</span><span class="p">])</span>

<span class="n">esg_results</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [{&#39;label&#39;: &#39;Environmental&#39;, &#39;score&#39;: 0.9805498719215393},
     {&#39;label&#39;: &#39;Social&#39;, &#39;score&#39;: 0.9906041026115417},
     {&#39;label&#39;: &#39;Governance&#39;, &#39;score&#39;: 0.6738430857658386},
     {&#39;label&#39;: &#39;None&#39;, &#39;score&#39;: 0.9960240125656128}]
</code></pre></div><p><br><br></p>
<h2 id="三fls识别">三、FLS识别</h2>
<p><strong>前瞻性陈述 (FLS)</strong> 告知投资者经理人对公司未来事件或结果的信念和意见。 从公司报告中识别前瞻性陈述可以帮助投资者进行财务分析。 FinBERT-FLS 是一个 FinBERT 模型，它基于罗素 3000 家公司年报的管理讨论和分析部分的 3,500 个手动注释的句子进行了微调。</p>
<ul>
<li><strong>输入</strong>：金融文本。</li>
<li><strong>输出</strong>：Specific-FLS(特定 FLS) , Non-specific FLS(非特定 FLS),  Not-FLS(非 FLS)。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">fls_finbert</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-fls&#39;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">fls_tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-fls&#39;</span><span class="p">)</span>

<span class="n">fls_nlp</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&#34;text-classification&#34;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">fls_finbert</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">fls_tokenizer</span><span class="p">)</span>
</code></pre></div><p><br> 使用3条测试文本进行测试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">fls_results</span> <span class="o">=</span> <span class="n">fls_nlp</span><span class="p">([</span><span class="s1">&#39;we expect the age of our fleet to enhance availability and reliability due to reduced downtime for repairs.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;on an equivalent unit of production basis, general and administrative expenses declined 24 percent from 1994 to $.67 per boe.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;we will continue to assess the need for a valuation allowance against deferred tax assets considering all available evidence obtained in future reporting periods.&#39;</span><span class="p">])</span>


<span class="n">fls_results</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [{&#39;label&#39;: &#39;Specific FLS&#39;, &#39;score&#39;: 0.7727874517440796},
     {&#39;label&#39;: &#39;Not FLS&#39;, &#39;score&#39;: 0.9905241131782532},
     {&#39;label&#39;: &#39;Non-specific FLS&#39;, &#39;score&#39;: 0.975904107093811}]
</code></pre></div><p><br><br></p>
<h2 id="文档及引用说明">文档及引用说明</h2>
<ul>
<li>
<p>文档github地址 <a href="https://github.com/yya518/FinBERT">https://github.com/yya518/FinBERT</a></p>
</li>
<li>
<p>作者博客: <a href="https://yya518.github.io/research">https://yya518.github.io/research</a></p>
</li>
</ul>
<br>
<p>Huang, Allen H., Hui Wang, and Yi Yang. &ldquo;FinBERT: A large language model for extracting information from financial text.&rdquo; <strong>Contemporary Accounting Research (2022)</strong>.</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>如何正确读入文本数据不乱码(解决文本乱码问题)</title>
      <link>https://textdata.cn/blog/2022-11-16-how-to-fix-string-unicode-decode-error/</link>
      <pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-16-how-to-fix-string-unicode-decode-error/</guid>
      <description>gb2312、gbk、utf-8都是什么鬼</description>
      <content:encoded><![CDATA[<h2 id="乱码由来">乱码由来</h2>
<p>相信大家使用Python读取txt、csv文件时，经常遇到UnicodeDecodeError错误，从字面意思看是 编码解码问题 。 关于编码解码相关知识推荐看B站柴知道制作的视频，了解</p>
<p>**锟斤拷�⊠是怎样炼成的&mdash;&mdash;中文显示&quot;⼊&quot;门指南【**柴知道】</p>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1cB4y177QR&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<h2 id="遇到乱码">遇到乱码</h2>
<p>在Python中，遇到文件读取乱码的可能性很大，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;twitter_sentiment.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>c:\users\thunderhit\appdata\local\programs\python\python37-32\lib\site-packages\pandas\io\parsers.py in parser_f(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)
...

pandas\_libs\parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()

pandas\_libs\parsers.pyx in pandas._libs.parsers.raise_parser_error()

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbd in position 10717: illegal multibyte sequence
</code></pre>
<p>可以看到报错, 遇到 <strong>UnicodeDecodeError</strong> 问题</p>
<br>
<h2 id="使用encoding参数">使用encoding参数</h2>
<p>常见的解决办法是在函数内加入encoding参数，中文最常见的编码方式有gbk、gb2312、utf-8。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#这里依次试验gbk、gb2312、utf-8</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;twitter_sentiment.csv&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>依然出现 <strong>UnicodeDecodeError</strong> 问题 。所以encoding遇到很不常见的编码方式。</p>
<br>
<h2 id="获取encoding参数">获取encoding参数</h2>
<p>先获取 twitter_sentiment.csv 文件的编码方式，再进行读取操作。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">chardet</span>

<span class="c1">#读取为二进制数据</span>
<span class="n">binary_data</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;twitter_sentiment.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="c1">#传给chardet.detect，稍等片刻</span>
<span class="n">chardet</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">binary_data</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>{'encoding': 'Windows-1252', 'confidence': 0.7291192008535122, 'language': ''}
</code></pre>
<p>由此得知该文件的编码方式为 <strong>Windows-1252</strong> ， 重新更改即可成功读入数据</p>
<br>
<h2 id="成功读取">成功读取</h2>
<p>获取了正确的编码 <code>encoding='Windows-1252'</code>， 就能正确的读入数据</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#这里依次试验gbk、gb2312、utf-8</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;twitter_sentiment.csv&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;Windows-1252&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:right"><strong>ItemID</strong></th>
<th style="text-align:right"><strong>Sentiment</strong></th>
<th style="text-align:right"><strong>SentimentText</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">is so sad for my APL frie...</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:right">2</td>
<td style="text-align:right">0</td>
<td style="text-align:right">I missed the New Moon trail...</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:right">3</td>
<td style="text-align:right">1</td>
<td style="text-align:right">omg its already 7:30 :O</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:right">4</td>
<td style="text-align:right">0</td>
<td style="text-align:right">.. Omgaga. Im sooo im gunna CRy. I'...</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">0</td>
<td style="text-align:right">i think mi bf is cheating on me!!! ...</td>
</tr>
</tbody>
</table>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 金融学文本大数据挖掘方法与研究进展</title>
      <link>https://textdata.cn/blog/2022-11-16-literature-review-textmining-in-finance-yao2020/</link>
      <pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-16-literature-review-textmining-in-finance-yao2020/</guid>
      <description>description用于SEO优化</description>
      <content:encoded><![CDATA[<blockquote>
<p>姚加权,张锟澎,罗平.金融学文本大数据挖掘方法与研究进展[J].经济学动态,2020(04):143-158.</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>在金融学领域的传统实证研究中，所用数据多局限于财务报表和股票市场数据等结构化数据。而在大数据时代，计算机技术的进步使得数据类型不断丰富，研究者开始将非结构化的文本大数据引入到金融学领域的研究中，其主要包括<strong>上市公司披露文本</strong>、<strong>财经媒体报道</strong>、<strong>社交网络文本</strong>、网络搜索指数以及 P2P 网络借贷文本等，并对 <strong>文本可读性</strong>、<strong>语气语调</strong>、<strong>相似性</strong> 以及 <strong>语义特征</strong> 展开研究。本文首先介绍了金融学领域文本大数据挖掘步骤和方法，描述了语料获取、预处理过程、文档表示以及文档的特征抽取；然后根据不同的文本信息来源，梳理了金融学文本大数据的研究进展；最后对未来金融学文本大数据的研究方法和研究内容进行了展望。</p>
<p>关键词：文本大数据 文本分析 机器学习 深度学习 数据挖掘</p>
<p><img loading="lazy" src="img/01.png" alt=""  />

<img loading="lazy" src="img/02.png" alt=""  />

<img loading="lazy" src="img/03.png" alt=""  />

<img loading="lazy" src="img/04.png" alt=""  />

<img loading="lazy" src="img/05.png" alt=""  />

<img loading="lazy" src="img/06.png" alt=""  />

<img loading="lazy" src="img/07.png" alt=""  />

<img loading="lazy" src="img/08.png" alt=""  />

<img loading="lazy" src="img/09.png" alt=""  />

<img loading="lazy" src="img/10.png" alt=""  />

<img loading="lazy" src="img/11.png" alt=""  />

<img loading="lazy" src="img/12.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 使用语义距离测量一个人的创新力(发散思维)得分</title>
      <link>https://textdata.cn/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/</link>
      <pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/</guid>
      <description>使用语义距离测量一个人的创新力(发散思维)得分</description>
      <content:encoded><![CDATA[<br>
<p>传统测量 <strong>被试者创造力</strong> 存在耗费时间、主观性太强、缺乏客观性，且所得到的分值是不稳定的，无法跨时间、文化、群体进行分值比较。该研究分析了创新力的两大理论，即联系理论和执行理论，即创新力是包含思维的广度和深度两方面。</p>
<ul>
<li><strong>联系理论(广度)</strong> 负责搜寻所有可能方案的集合，增加集合的规模，体现思维的广度。</li>
<li><strong>执行理论(深度)</strong> 负责寻找最佳方案，并将方案落实执行，体现思维的深度。</li>
</ul>
<p>结合Glove词嵌入技术，将每个词理解为一个技术或知识，两词语语义越相似，发散性越低。</p>
<p>文中让被试按照一定规则，随意填写10个名词，使用其中7个有效词语测量被试的创新力(发散性)思维。可以简单的把7个词理解为知识或者技术，7个词语会形成21种词语对(组合)。最后求均值可以测量出被试词语对的语义距离体现创新发散性的强度。<strong>文末含案例代码</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Olson, J.A., Nahas, J., Chmoulevitch, D., Cropper, S.J. and Webb, M.E., 2021. Naming unrelated words predicts creativity. Proceedings of the National Academy of Sciences, 118(25), p.e2022340118.
</code></pre></div><p><br><br></p>
<h2 id="一摘要">一、摘要</h2>
<p><strong>一些理论认为，有 创造力 的人能够产生更多 发散性 的想法。如果这是正确的，简单地让被试写 N 个不相关的单词，然后测量这N个词的语义距离， 作为 #发散思维 的客观衡量标准</strong>。为了验证这一假设，我们要求 8,914 名参与者说出 10 个彼此尽可能不同的单词。</p>
<p>然后计算算法估计单词之间的平均语义距离；<strong>相关词（例如 cat 和 dog）比不相关词（例如 cat 和 thimble）的距离更短。我们预测，产生更大语义距离的人也会在传统的创造力测量中得分更高</strong>。</p>
<p>在研究 1 中，我们发现语义距离与两个广泛使用的创造力测量（替代用途任务和桥接关联差距任务）之间存在中度至强相关性。在研究 2 中，参与者来自 98 个国家，语义距离仅因基本人口变量而略有不同。在一系列已知可预测创造力的问题上，语义距离与表现之间也存在正相关关系。</p>
<p>总体而言， <strong>语义距离</strong> 与已建立的 创造力测量 的相关性至少与这些测量彼此之间的相关性一样强。 因此，在我们所说的发散关联任务中命名不相关的词可以作为发散思维的简短、可靠和客观的衡量标准。</p>
<br>
<h2 id="二创新力理论">二、创新力理论</h2>
<p>想出 3 个尽可能不同的词。根据两种主要的创造力理论 (1, 2)，选择这些词依赖于产生 #远程联想 ，同时抑制 #常见联想 。</p>
<p>#联想理论 (Associative Theory)认为，有创造力的人具有语义记忆结构，可以更容易地链接远程元素 (3-6)。</p>
<p>#执行理论 (Executive Theory) 侧重于自上而下的注意力控制；创造性的解决方案来自于监测和抑制共同的联想 (2, 7)。</p>
<p>基于这些理论，我们假设 <strong>填写n个无关单词的任务</strong> 可以可靠地衡量 #语言创造力 。 <strong>创造力有两个主要的心理成分， 收敛思维和发散思维，它们在产生创意输出时协同工作</strong>。收敛性思维任务衡量评估多种刺激并得出最适当响应的能力，例如问题的最佳解决方案 (3, 8-10)。这些任务往往更容易得分，因为只有一小部分正确答案。<strong>相比之下，发散思维任务通常使用开放式问题来衡量一个人产生各种解决方案的能力</strong> (11-13)。它们通常需要更长的回答(文本)，因此更难客观评分。</p>
<br>
<h2 id="三创新力测量">三、创新力测量</h2>
<h3 id="31--替代用途任务">3.1  替代用途任务</h3>
<p>最常见的发散思维测量是 <strong>替代用途任务</strong> Alternative Uses Task (14, 15)，在该任务中，参与者生成常见物体的用途，例如回形针或鞋子。使用常用的评分方法 (16)，评分者然后根据三个组成部分来判断回答：</p>
<ul>
<li>灵活性，产生的不同用途类别的数量；</li>
<li>独创性，每次使用相对于样本的其余部分的稀有程度，这对创造力特别重要（17、18）；和</li>
<li>流畅度，一共产生了多少次使用。</li>
</ul>
<br>
<h3 id="32-离散联系任务">3.2 离散联系任务</h3>
<p>本研究作者开发了 <strong>离散联系任务</strong> (Divergent Association Task， DAT) 的网站， <strong>填写你想到的10个不相关词语， 创造力越丰富的人，填写的词语语义距离往往会更远</strong>。</p>
<p><a href="https://www.datcreativity.com/">https://www.datcreativity.com/</a></p>
<p><img loading="lazy" src="img/1_pnas_divergent_association_task_mainpage.png" alt=""  />
</p>
<h3 id="被试填写10个单词的规则">被试填写10个单词的规则</h3>
<ol>
<li>只能填写英文单词</li>
<li>只能是名词(如事情、物体、概念)</li>
<li>不能填 专有名词（例如，特定的人或地点）</li>
<li>不能填写 专业词（比如技术词）</li>
<li>自己思考这些词，不要只看周围环境的物体。</li>
</ol>
<h3 id="dat算法实现">DAT算法实现</h3>
<ol>
<li>使用Glove预训练模型</li>
<li>选前7个词(一共10个词)， 存在 21个词对（组合）</li>
<li>对21词对， 分别计算词向量的余弦距离，分别乘以100。最终求均值得到DAT得分。</li>
</ol>
<blockquote>
<p>下图是大邓第二次填写得到的DAT得分，第一次只超过了6%的人，这方法第一次准，再测就知道如何提高DAT得分。</p>
</blockquote>
<p><img loading="lazy" src="img/2_pnas_divergent_association_task_result.png" alt=""  />
</p>
<p>DAT得分范围0-200， 得分为0可能是7个有效词之间语义相同，而得分200可能是有效词之间彼此语义完全不相同。实践中，得分大多处于65~90之间，且很少超过100。</p>
<p><img loading="lazy" src="img/pnas_dat_score_low_median_high.jpg" alt=""  />
</p>
<blockquote>
<p>词嵌入技术可以把每个词转化为等长的向量，而不同词语共处于相同的语义空间中。常见的词嵌入技术有word2vec、Glove、flastText等，因为最近有学者在 <strong>替代用途任务</strong>(Alternative Uses Task）中用过Glove算法，本文采用Glove算法。本研究使用的Glove预训练模型来自Common Crawl Corpus项目，该项目拥有数十亿网页文本数据。</p>
<p>为了提供冗余， 只采用 被试者 填写的前7个词作为有效单词(DAT的被试需要填写10个词)。DAT得分是这些词之间的语义距离的平均值，具体计算方法， 7个词两两相关的组合有 42种组合， 选择其中最有可能的 21 个语义组合。</p>
</blockquote>
<br>
<h2 id="四实验">四、实验</h2>
<p>这种发散思维的操作化是基于创造力的联想和执行控制理论。 更高的分数将显示出更大的能力来利用更远程的关联 (3-5) 或抑制过度相关的关联 (2, 7)。</p>
<p>在研究 1 中，我们通过将 DAT 与其他两种创造力测量方法进行比较来检验这一假设：替代用途任务 (15) 和桥接关联差距任务 (36)。
<img loading="lazy" src="img/pnas_dat_aut_algo_valid_num.jpg" alt=""  />
</p>
<p>在研究 2 中，我们测试了这些分数如何随人口统计而变化，以及它们是否与更大数据集中与发散性思维相关的其他测量值相关 (9, 37)。 这些研究评估了语义距离是否可以作为发散思维的可靠指标。
<img loading="lazy" src="img/pnas_dat_gender_age.jpg" alt=""  />
</p>
<br>
<h2 id="五讨论">五、讨论</h2>
<p>研究结果表面， 让被试简单的填写10个不想管单词的任务可以作为 测量发散思维 的可靠衡量标准。在研究中， 将这项任务的表现与已有的两种创造力量表做了比较，具有很高的相关性。</p>
<p>总体而言支持了语义发散性，尽管这种联系背后的确切机制尚不清楚，但在创新力最主要的两个理论，即联想理论或执行理论 的联系网络中衡量网络的范围或效率。</p>
<p><strong>DAT算法表现稳定，方差不随人口统计特征变化出现显著性变化（研究2），可以在跨年龄、跨性别的情况下应用</strong>。</p>
<br>
<h3 id="51-dat的优点">5.1 DAT的优点</h3>
<ul>
<li>操作简单，快捷，客观，节约了大量的人力时间，又能保证客观性。</li>
<li>得分绝对，可比较，可以用于测量不同群体(种族、文化、性别、年龄)的创造力得分。</li>
<li>对被试友好，一般一两分钟即可完成。</li>
</ul>
<h3 id="52-dat的不足">5.2 DAT的不足</h3>
<ul>
<li>创造力有发散性和执行力，发散性负责搜选所有方案集合的规模，而执行力是从方案集中选出最优方案并将其执行。DAT测量的仅仅是发散性思维。</li>
<li>被试可能通过填写稀奇的词语提高DAT得分。</li>
<li>只有短短几分钟，被试可能很难短时间内了解实验规则。</li>
</ul>
<h3 id="53-未来展望">5.3 未来展望</h3>
<p>DAT得分取决于Glove模型、语料库(数据集), 更新词模型或语料库，被试的DAT得分会发生变化。为简单起见，本研究使用免费的预训练模型， 通过一些努力，未来研究者可以对不同时期，不同国家的语料库来训练Glove模型。随着特定单词关联或多或少的联系， 更新的模型将会自动考虑这些变化，这将允许DAT得分跨越文化跨越时代，进行创新力的比较。</p>
<p><br><br></p>
<h2 id="代码">代码</h2>
<p>代码的文档说明请点击 github仓库地址 <a href="https://github.com/jayolson/divergent-association-task">https://github.com/jayolson/divergent-association-task</a> 查看。这里仅粘贴作者源代码，源代码需要配置好才可运行。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">dat</span>

<span class="c1">## 从 https://nlp.stanford.edu/projects/glove/ 下载Glove模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">dat</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="s2">&#34;glove.840B.300d.txt&#34;</span><span class="p">,</span> <span class="s2">&#34;words.txt&#34;</span><span class="p">)</span>

<span class="c1"># 验证词语，如输入的是词组，代码会将其转为连线形式的单词</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="s2">&#34;cul de sac&#34;</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cul-de-sac
</code></pre></div><br>
<p>计算两个词语之间的语义距离</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;dog&#34;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;thimble&#34;</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.1983
0.8787
</code></pre></div><br>
<p>计算词对的DAT得分（语义cosine距离*100）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">([</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;dog&#34;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">([</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;thimble&#34;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">19.83
87.87
</code></pre></div><br>
<p>假设有三个人分别都填写10个词，选其前7个词作为有效词。有效词如下，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">low</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;arm&#34;</span><span class="p">,</span> <span class="s2">&#34;eyes&#34;</span><span class="p">,</span> <span class="s2">&#34;feet&#34;</span><span class="p">,</span> <span class="s2">&#34;hand&#34;</span><span class="p">,</span> <span class="s2">&#34;head&#34;</span><span class="p">,</span> <span class="s2">&#34;leg&#34;</span><span class="p">,</span> <span class="s2">&#34;body&#34;</span><span class="p">]</span>
<span class="n">average</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;bag&#34;</span><span class="p">,</span> <span class="s2">&#34;bee&#34;</span><span class="p">,</span> <span class="s2">&#34;burger&#34;</span><span class="p">,</span> <span class="s2">&#34;feast&#34;</span><span class="p">,</span> <span class="s2">&#34;office&#34;</span><span class="p">,</span> <span class="s2">&#34;shoes&#34;</span><span class="p">,</span> <span class="s2">&#34;tree&#34;</span><span class="p">]</span>
<span class="n">high</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;hippo&#34;</span><span class="p">,</span> <span class="s2">&#34;jumper&#34;</span><span class="p">,</span> <span class="s2">&#34;machinery&#34;</span><span class="p">,</span> <span class="s2">&#34;prickle&#34;</span><span class="p">,</span> <span class="s2">&#34;tickets&#34;</span><span class="p">,</span> <span class="s2">&#34;tomato&#34;</span><span class="p">,</span> <span class="s2">&#34;violin&#34;</span><span class="p">]</span>

<span class="c1"># Compute the DAT score (transformed average cosine distance of first 7 valid words)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">low</span><span class="p">))</span> <span class="c1"># 50</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">average</span><span class="p">))</span> <span class="c1"># 78</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">high</span><span class="p">))</span> <span class="c1"># 95</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">50
78
95
</code></pre></div><p>需要注意pnas作者公开的代码只能用在英文，且无法自己训练Glove模型。如果想基于自有数据集（中文、英文），训练自有Glove模型，需要学习</p>
<ul>
<li>如何训练Glove模型</li>
<li>如何导入训练好的Glove模型</li>
<li>如何计算中英文dat得分</li>
</ul>
<p>相关知识点已更新至我的录播课课程 <a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>魔搭 | 中文AI模型开源社区</title>
      <link>https://textdata.cn/blog/2022-11-09-chinese-modelscope-open-source/</link>
      <pubDate>Wed, 09 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-09-chinese-modelscope-open-source/</guid>
      <description>ModelScope社区成立于2022 年6月，是一个模型开源社区及创新平台，由阿里巴巴达摩院，联合CCF开源发展委员会，共同作为项目发起方。社区联合国内AI领域合作伙伴与高校机构，致力于通过开放的社区合作，构建深度学习相关的模型开源，并开源相关模型服务创新技术，推动模型应用生态的繁荣发展。</description>
      <content:encoded><![CDATA[<h2 id="关于modelscope">关于ModelScope</h2>
<p>ModelScope社区成立于 2022 年 6 月，是一个模型开源社区及创新平台，由阿里巴巴达摩院，联合CCF开源发展委员会，共同作为项目发起方。</p>
<blockquote>
<p>社区联合国内AI领域合作伙伴与高校机构，致力于通过开放的社区合作，构建深度学习相关的模型开源，并开源相关模型服务创新技术，推动模型应用生态的繁荣发展。</p>
</blockquote>
<p>期待ModelScope会有不一样的表现。</p>
<p>与ModelScope类似的网站有</p>
<ul>
<li>国际 huggingface是较早将AI模型开源的网站，用户群体庞大，社区内有丰富的数据集、模型，文档详实。</li>
<li>国内 百度飞桨是国内AI模型开源较好的网站，用户群体较大，更新活跃，但是文档质量。。。</li>
</ul>
<p>目前ModelScope刚刚上线不久，模型和数据集都不怎么多</p>
<p><img loading="lazy" src="img/model_scope_homepage.png" alt=""  />
</p>
<br>
<h2 id="heading"></h2>
<h1 id="名词解释"><strong>名词解释</strong></h1>
<p>ModelScope平台是以模型为中心的模型开源社区，与模型的使用相关，您需要先了解如下概念。</p>
<table>
<thead>
<tr>
<th><strong>基础概念</strong></th>
<th><strong>定义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>任务</td>
<td>任务（Task）指某一领域具体的应用，以用于完成特定场景的任务。例如图像分类、文本生成、语音识别等，您可根据任务的输入输出找到适合您的应用场景的任务类型，通过任务的筛选来查找您所需的模型。</td>
</tr>
<tr>
<td>模型</td>
<td>模型（Model）是指一个具体的模型实例，包括模型网络结构和相应参数。ModelScope平台提供丰富的模型信息供用户体验与使用。</td>
</tr>
<tr>
<td>模型库</td>
<td>模型库（Modelhub）是指对模型进行存储、版本管理和相关操作的模型服务，用户上传和共享的模型将存储至ModelScope的模型库中，同时用户也可在Model hub中创建属于自己的模型存储库，并沿用平台提供的模型库管理功能进行模型管理。</td>
</tr>
<tr>
<td>数据集</td>
<td>数据集（Dataset）是方便共享及访问的数据集合，可用于算法训练、测试、验证，通常以表格形式出现。按照模态可划分为文本、图像、音频、视频、多模态等。</td>
</tr>
<tr>
<td>数据集库</td>
<td>数据集库（Datasethub）用于集中管理数据，支持模型进行训练、预测等，使各类型数据具备易访问、易管理、易共享的特点。</td>
</tr>
<tr>
<td>ModelScope Library</td>
<td>ModelScope Library是ModelScope平台自研的一套Python Library框架，通过调用特定的方法，用户可以只写短短的几行代码，就可以完成模型的推理、训练和评估等任务，也可以在此基础上快速进行二次开发，实现自己的创新想法。</td>
</tr>
</tbody>
</table>
<br>
<h2 id="一模型探索">一、模型探索</h2>
<p>首先访问平台网址https://www.modelscope.cn/models， 您将看见平台上已有的所有公开模型，根据任务筛选或者关键词搜索可查找您感兴趣的模型。</p>
<p><img loading="lazy" src="img/1-model_explore.png" alt=""  />
</p>
<br>
<h2 id="二环境准备">二、环境准备</h2>
<h3 id="21-本地开发环境">2.1 本地开发环境</h3>
<p>如果您需要在本地运行模型，需要进行相应的环境安装准备，包括：</p>
<ul>
<li><strong>安装python环境</strong>。支持python3，不支持python2，建议3.7版本及以上。我们推荐您使用Anaconda进行安装。</li>
<li><strong>安装深度学习框架</strong>。ModelScope Library目前支持Tensorflow，Pytorch两大深度学习框架进行模型训练、推理。您可根据模型所需的框架选择适合的框架进行安装。</li>
<li><strong>安装ModelScope Library</strong>。我们提供两种安装方式，您可选择适合的方式进行安装。
<ul>
<li>pip安装。ModelScope提供了根据不同领域的安装包，您可根据对应的模型选择所需的安装包。</li>
<li>使用源码安装。</li>
<li>更完整的安装信息参考：环境安装指南。</li>
</ul>
</li>
</ul>
<h3 id="22-在线notebook">2.2 在线Notebook</h3>
<p>若您觉得本地安装较为复杂， ModelScope平台也提供在线的运行环境，您可直接在Notebook中运行，Notebook中提供官方镜像无需自主进行环境安装，更加方便快捷，推荐大家使用！</p>
<p>注意：该功能需要您登录后使用，新用户注册ModelScope账号并完成阿里云账号绑定后即可获得免费算力资源，详情请参阅免费额度说明 。</p>
<p><img loading="lazy" src="img/model_scode_free_online_notebook.png" alt=""  />
</p>
<p><img loading="lazy" src="img/model_scode_free_online_notebook-2.png" alt=""  />
</p>
<br>
<h2 id="三2分钟跑通模型推理">三、2分钟跑通模型推理</h2>
<p>若您准备好本地环境或者已经打开一个Notebook的预装环境实例，则根据下述代码可对该模型进行推理。 使用modelscope pipeline接口只需要两步，同样以上述中文分词模型（damo/nlp_structbert_word-segmentation_chinese-base）为例简单说明：</p>
<p>首先根据task实例化一个pipeline对象</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">modelscope.pipelines</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="n">word_segmentation</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;word-segmentation&#39;</span><span class="p">,</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;damo/nlp_structbert_word-segmentation_chinese-base&#39;</span><span class="p">)</span>
</code></pre></div><p>输入数据，拿到结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">input_str</span> <span class="o">=</span> <span class="s1">&#39;今天天气不错，适合出去游玩&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word_segmentation</span><span class="p">(</span><span class="n">input_str</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;output&#39;: &#39;今天 天气 不错 ， 适合 出去 游玩&#39;}
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>DomainWordsDict | 领域词库构建方法与68领域、916万级专业词库分享</title>
      <link>https://textdata.cn/blog/2022-11-07-domainwordsdict-liuhuanyong/</link>
      <pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-07-domainwordsdict-liuhuanyong/</guid>
      <description>不同的领域词库能够支持不同的文本分析任务，也是支撑领域NLP处理的必备资源。</description>
      <content:encoded><![CDATA[<p>不同的领域词库能够支持不同的文本分析任务，也是支撑领域NLP处理的必备资源。</p>
<p>因此，本文介绍一个涵盖68个领域、共计916万词的专业词典知识库，可用于文本分类、知识增强、领域词汇库扩充等自然语言处理应用。</p>
<p>在利用学习模型进行自然语言处理任务时候，领域词汇可以作为一项重要的领域特征加入到模型当中，可以提升领域性模型的性能。</p>
<p>地址：https://github.com/liuhuanyong/DomainWordsDict</p>
<br>
<h2 id="项目概述">项目概述</h2>
<p>DomainWordsDict, Chinese words dict that contains more than 68 domains, which can be used as text classification、knowledge enhance task。涵盖68个领域、共计916万词的专业词典知识库，可用于文本分类、知识增强、领域词汇库扩充等自然语言处理应用。在利用学习模型进行自然语言处理任务时候，领域词汇可以作为一项重要的领域特征加入到模型当中，可以提升领域性模型的性能。</p>
<br>
<h2 id="项目由来">项目由来</h2>
<p>1、领域性是自然语言处理中十分重要的一类问题，不同的领域之间在文本形式、用词、表达上都存在差异。而领域词汇作为一个领域的表示是用来区分领域的常规手段，例如，在没有标注语料进行有监督的领域文本分类中，利用领域关键词进行匹配、计数、排序的方式即可以完成这一任务。<br>
2、当前，纵观中文开放语言资源，并未有出现较大规模的领域性资源，如领域的wordembedding词向量、领域的关键词库。而这一资源在传统方法进行文本处理具有较大价值。</p>
<p>为了填补这一空白以及对领域性词库进行基础语言资源建设，本项目被提出。</p>
<br>
<h2 id="数据来源">数据来源</h2>
<p>通过对领域垂直网站的解析、领域文本的特征词提取，近几年来对领域词典的收集与整理，人工清洗等处理工作之后， 最终形成了数学科学、人力招聘、天文科学、餐饮食品、外语学习等共计68个领域，共计916万词的较大规模领域词汇库。</p>
<br>
<h2 id="数据介绍">数据介绍</h2>
<p>数据放在data文件夹下，共68个txt文件，每个文件以领域的名称命名。每个文件中的每一行包括两列(以tab符分开)，分别代表词语名称以及对应的权重。文件中的词语按照权重从大到小的方式排列，权重越高，该词对于领域的代表性或区分能力就越强。在使用的过程中，我们可以设定具体的权重域值在选用不同的词语来用于特定任务。</p>
<br>
<h2 id="词典样例">词典样例</h2>
<table>
<thead>
<tr>
<th style="text-align:left">序号</th>
<th style="text-align:left">领域</th>
<th>个数</th>
<th style="text-align:left">举例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">1</td>
<td style="text-align:left">数学科学</td>
<td>17,287</td>
<td style="text-align:left">定义域、值域、半群、悖论、本原多项式、闭包、变换群、边连通度、不变因子、差集、超滤子、存在量词、代数、代数闭域、单射</td>
</tr>
<tr>
<td style="text-align:left">2</td>
<td style="text-align:left">人力招聘</td>
<td>447,606</td>
<td style="text-align:left">销售代表、产品经理、销售经理、电话销售、阿里云、客户经理、销售精英、销售工程师、集团总部、销售主管、商务专员、客服专员、课程顾问、Manager、销售助理</td>
</tr>
<tr>
<td style="text-align:left">3</td>
<td style="text-align:left">天文科学</td>
<td>4,135</td>
<td style="text-align:left">天体化学、天体力学、白洞、本星系群、不规则星系、垂直圈、地面天文学、第一宇宙速度、动力学宇宙学、方位天文学、高能天体物理学、观测宇宙学、光学天文学、航海天文学、航天动力学</td>
</tr>
<tr>
<td style="text-align:left">4</td>
<td style="text-align:left">餐饮食品</td>
<td>201,163</td>
<td style="text-align:left">六堡茶、参龟汤、婆参扒大鸭、虾籽大乌参、红扒鱼肚、蛤什蟆汤、参女、五盖山米茶、皖西黄大茶、涌溪火青、夫妻肺片、普洱茶、六堡散茶、漆蜡妙食、鱼羊鲜</td>
</tr>
<tr>
<td style="text-align:left">5</td>
<td style="text-align:left">外语学习</td>
<td>1,150</td>
<td style="text-align:left">褒义、被动语态、比较级、贬义、表语、表语从句、宾补、宾格、宾语、宾语从句、并列复合句、并列句、并列连词、不定代词、不定冠词</td>
</tr>
<tr>
<td style="text-align:left">6</td>
<td style="text-align:left">电影影视</td>
<td>114,577</td>
<td style="text-align:left">小查和寇弟的游轮生活、邪斗邪、邪完再邪、和莎莫的五百天、比利曼蒂和死神的大反击、融入那芒芒的大海里、阿扎泽勒、圣棱的星光、他们也忒不仗义了、吉姆贾木许、神探伽俐略、仔表栖议甲弟、叶卡捷琳堡音乐戏剧剧院、锁叶和属于自己的春天、神奇四侠</td>
</tr>
<tr>
<td style="text-align:left">7</td>
<td style="text-align:left">环境科学</td>
<td>7,891</td>
<td style="text-align:left">层积云、热岛效应、单区电除尘器、逆温、卷层云、卷积云、卷云、低压槽、厄尔尼诺现象、副热带高压、高压脊、锢囚锋、积雨云、蒙古气旋、气候变化</td>
</tr>
<tr>
<td style="text-align:left">8</td>
<td style="text-align:left">钢铁冶金</td>
<td>89,114</td>
<td style="text-align:left">炉料、炉衬、梭车、偏心轮、炉渣流动性、锰矿、软熔带、炉顶、耐热钢、脱碳、除尘风机、连续采煤机、锚杆机、扒渣、齿轮钢</td>
</tr>
<tr>
<td style="text-align:left">9</td>
<td style="text-align:left">印刷印染</td>
<td>464</td>
<td style="text-align:left">包装薄膜、包装防伪、标识防伪、玻璃油墨、产品防伪、打样机、电晕笔、分切机、复合机、覆膜机、功能薄膜、挂历印刷、刮墨刀、海报印刷、降解薄膜</td>
</tr>
<tr>
<td style="text-align:left">10</td>
<td style="text-align:left">美容美发</td>
<td>9,662</td>
<td style="text-align:left">阿莎露、娜莎迪、内眦赘皮、重睑术、艾茜凯、洗得你净、兰芝、欧莱雅、虾青素、削刀式划剪、扛薄剪、碧欧泉、高丝、赫莲娜、护肤</td>
</tr>
<tr>
<td style="text-align:left">11</td>
<td style="text-align:left">法律诉讼</td>
<td>62,717</td>
<td style="text-align:left">行政处罚、送达、书证、行政诉讼法、行政复议、指定管辖、合议庭、第三人、二审、民事诉讼法、诉讼时效、合并审理、法定代表人、司法解释、刑事诉讼法</td>
</tr>
<tr>
<td style="text-align:left">12</td>
<td style="text-align:left">计算机业</td>
<td>55,037</td>
<td style="text-align:left">字符串、排序、标识符、队列、访问级别、局部变量、安全类、安全内核、安全识别、安全域、绑定、包过滤、保留字、备份与恢复、编辑程序</td>
</tr>
<tr>
<td style="text-align:left">13</td>
<td style="text-align:left">水利工程</td>
<td>30,584</td>
<td style="text-align:left">拦污栅、电动蝶阀、管件、检查井、启闭机、手动蝶阀、手动葫芦、消力池、闸室、帮扎、包箍、边墩、草袋、承插管、粗料石</td>
</tr>
<tr>
<td style="text-align:left">14</td>
<td style="text-align:left">手机数码</td>
<td>10,955</td>
<td style="text-align:left">阿尔卡特、金立、奥盛、天时达、大显、首信、多普达、萨基姆、斯达康、索尼爱立信、摩奇、奥乐、柏卡、爱立信、迪比特</td>
</tr>
<tr>
<td style="text-align:left">15</td>
<td style="text-align:left">音乐歌曲</td>
<td>8,276</td>
<td style="text-align:left">月亮代表我的心、倒带、搁浅、回到过去、简单爱、菊花台、蒲公英的约定、日不落、听妈妈的话、退后、夜曲、音高、蔡依林、陈好、陈慧琳</td>
</tr>
<tr>
<td style="text-align:left">16</td>
<td style="text-align:left">地产开发</td>
<td>14,708</td>
<td style="text-align:left">锦绣花园、东方家园、东方巴黎、和平小区、红河小区、华侨饭店、金河小区、凯旋城、临江花园、世纪花园、世纪嘉园、世茂滨江新城、银河小区、中山花园、丽景苑</td>
</tr>
<tr>
<td style="text-align:left">17</td>
<td style="text-align:left">汉语言学</td>
<td>32,8050</td>
<td style="text-align:left">长虫、背时、胰子、刺挠、旮旯、上该、要得、茅房、邋遢、落雨、二杆子、爹爹、宝气、日白、棒老二</td>
</tr>
<tr>
<td style="text-align:left">18</td>
<td style="text-align:left">医药医学</td>
<td>54,9008</td>
<td style="text-align:left">毒邪内闭证、参附注射液、邪盛正衰、夹蛇龟肉、夹蛇龟、直接盖髓术间接盖髓术干髓术、参附汤、盖革缪勒计数器、荜澄茄、大敦、南沙参、白虎加人参汤、桡尺近侧关节、归地参术汤、地骨皮</td>
</tr>
<tr>
<td style="text-align:left">19</td>
<td style="text-align:left">网络文学</td>
<td>95,331</td>
<td style="text-align:left">竺氏三姐弟、参神契、明道参神契、元婴、缩地术、朱苹、地行术、胖都都、似模似样、水晶血龙参、渡劫、盖运聪、和氏之璧、詹姆、沐王府</td>
</tr>
<tr>
<td style="text-align:left">20</td>
<td style="text-align:left">休闲活动</td>
<td>59,186</td>
<td style="text-align:left">说、哪府并哪县、佛挡杀佛、罢罢罢来休休休、万、番、拾玉镯、打金枝、军、吾乃江东小霸王孙伯符、丁广和、会、了、贾、得</td>
</tr>
<tr>
<td style="text-align:left">21</td>
<td style="text-align:left">交通运输</td>
<td>27,230</td>
<td style="text-align:left">信号机、北京铁路局、沈阳铁路局、车辆段、车务段、工务段、哈尔滨铁路局、岔心、列车长、蒸汽机车、济南铁路局、南昌铁路局、上海铁路局、郑州铁路局、abs防抱死刹车系统</td>
</tr>
<tr>
<td style="text-align:left">22</td>
<td style="text-align:left">矿业勘探</td>
<td>20,817</td>
<td style="text-align:left">捕收剂、矿车、带式输送机、化合水、精煤、炼焦煤、煤仓、煤泥、起泡剂、选煤厂、闭路破碎、粗磨、粗碎、二分器、翻车机</td>
</tr>
<tr>
<td style="text-align:left">23</td>
<td style="text-align:left">地点名称</td>
<td>1,338,275</td>
<td style="text-align:left">新村、和平村、胜利村、新建村、太平村、向阳村、团结村、新华村、东山村、前进村、劲霸男装、红旗村、东风村、以纯、光明村</td>
</tr>
<tr>
<td style="text-align:left">24</td>
<td style="text-align:left">船舶工程</td>
<td>5,424</td>
<td style="text-align:left">舱壁板、风暴扶手、锚链管、锚链筒、白昼信号灯、车钟、充放电板、电笛、舵杆、舵机、舵叶、发电机屏、海图灯、空气断路器、雷达应答器</td>
</tr>
<tr>
<td style="text-align:left">25</td>
<td style="text-align:left">敏感用词</td>
<td>13,595</td>
<td style="text-align:left">抢盐、AV、hz、sm、PK、PX、C4、usk、flg、GCD、gcd、GHB、rfa、sex、TND</td>
</tr>
<tr>
<td style="text-align:left">26</td>
<td style="text-align:left">旅游交通</td>
<td>52,848</td>
<td style="text-align:left">报国寺、本溪水洞、大佛寺、大明寺、独乐寺、夫子庙、观音山、广胜寺、寒山寺、黑龙潭、极乐寺、蠡园、隆兴寺、鲁迅故居、明孝陵</td>
</tr>
<tr>
<td style="text-align:left">27</td>
<td style="text-align:left">机械工程</td>
<td>9,164</td>
<td style="text-align:left">磨床、铣床、滚子链、键槽、蜗杆、蜗轮、镗床、脱碳、保持架、齿距、齿宽、传动链、大齿轮、导程、碟形弹簧</td>
</tr>
<tr>
<td style="text-align:left">28</td>
<td style="text-align:left">考古挖掘</td>
<td>5,713</td>
<td style="text-align:left">二里头文化、辛店文化、朱开沟文化、夏家店文化、彭头山文化、齐家文化、二里岗文化、石家河文化、贾湖骨笛、云纹铜禁、郑振香、半坡遗址、大明宫遗址、阿房宫遗址、汉长安城遗址</td>
</tr>
<tr>
<td style="text-align:left">29</td>
<td style="text-align:left">人文政治</td>
<td>13,189</td>
<td style="text-align:left">坚持改革开放、坚持和完善人民代表大会制度、建设社会主义法治国家、我代表中共中央、向香港特别行政区同胞、高举中国特色社会主义伟大旗帜、转变发展方式、多种所有制经济共同发展的基本经济制度、改革开放以来、开辟了中国特色社会主义道路、毛泽东思想、维护世界和平与促进共同发展、病有所医、劳有所得、老有所养</td>
</tr>
<tr>
<td style="text-align:left">30</td>
<td style="text-align:left">电力电气</td>
<td>50,429</td>
<td style="text-align:left">高压侧、厂用电率、模芯和模套对准中心调整、栅差、空侧、氢侧、裕度、奥科勃纶、掺的石英光纤、缩颈模、辗页辗页橡辗页塑、揭大盖、拉线模模孔光洁度模角等光学检测仪、铁芯、有功功率</td>
</tr>
<tr>
<td style="text-align:left">31</td>
<td style="text-align:left">网络游戏</td>
<td>52,2150</td>
<td style="text-align:left">虎窟佛调行、此佛彼佛、伽邪舍多链、伽那格毒手蛇使、长剑、藏千邪、吸血、金创药、破甲、布鞋、藏邪、长邪、长邪带、长邪护手、长邪戒</td>
</tr>
<tr>
<td style="text-align:left">32</td>
<td style="text-align:left">纺织服装</td>
<td>28,111</td>
<td style="text-align:left">平纹布、风衣、夹克、九分裤、里料、棉绳、耳仔、西裤、氨纶、蝙蝠袖、插肩袖、翻领、口袋、裤长、立领</td>
</tr>
<tr>
<td style="text-align:left">33</td>
<td style="text-align:left">办公文教</td>
<td>6,135</td>
<td style="text-align:left">硒鼓、李浩东、王嘉豪、彩喷纸、充电辊、磁辊、定影膜、定影组件、粉盒、分离爪、感光鼓、鼓芯、加热组件、墨粉、色带</td>
</tr>
<tr>
<td style="text-align:left">34</td>
<td style="text-align:left">组织机构</td>
<td>369,709</td>
<td style="text-align:left">酒店名称、软件学院、校医院、管理学院、北京工业大学、广东工业大学、广东外语外贸大学、广州大学、广州中医药大学、华南理工大学、华南师范大学、网络中心、广州美术学院、河北大学、华南农业大学</td>
</tr>
<tr>
<td style="text-align:left">35</td>
<td style="text-align:left">化学化工</td>
<td>40,316</td>
<td style="text-align:left">苛化度苛化作用苛化率、没食子酸、奎哪啶红、助色团、单分子反应、瞞、电子亲合势、阿伏伽德罗数、丁咯地尔、盐酸丁咯地尔、乙烯、磁量子数、副价、均裂、量子数</td>
</tr>
<tr>
<td style="text-align:left">36</td>
<td style="text-align:left">诗词歌赋</td>
<td>772,992</td>
<td style="text-align:left">泊思禅寺呈廖明略其地盖干越寺在琵琶洲上、送矰吴尉并属寄声吴交代尉比行余亦行追作、裴纶著作见期行日延宿所居既至裴已行因书寄、送常宁吴尉并属寄声吴交代尉比行余亦行追作、次韵追和钱穆父内翰勰赵伯坚大卿令铄游颍湖、蕃有诗谢萧伯和见访伯和和之节推丈见而同作、僮有弹鹭置池上者予解其缚纵之而不去盖不饮、与硕父沈弟伯仲晚行河堤硕父欲作小亭于其上、王虞部惠佳篇叙述昔与湘潭亡弟游从仍以亡弟、舟中咏落景余清晖轻桡弄溪渚之句盖孟浩然耶、自道场山至何山读故人洪舜俞内翰诗刻追和、龟胜寺枸杞大如椽陈日华发其根而枯堂犹以地、永丰祝子益和予诗见寄许见访以长句谢之且贤、次韵德美碧感旧之什且约胡广仲伯逢季丘来会、舟宿南尉岸下夜夕不寐思丁老小山戏成长韵</td>
</tr>
<tr>
<td style="text-align:left">37</td>
<td style="text-align:left">社会科学</td>
<td>19,231</td>
<td style="text-align:left">第三人、承包合同、出资、法定代表人、国有资产、合伙协议、原件、不当得利、法学家、国际经济法、国际私法、民事诉讼法、善意第三人、违法、刑法</td>
</tr>
<tr>
<td style="text-align:left">38</td>
<td style="text-align:left">军事情报</td>
<td>76,249</td>
<td style="text-align:left">狙击步枪、突击步枪、无情角斗士的邪纹护手、型潜艇、无情角斗士的邪纹长裤、阿史那弥射、级驱逐舰、胡庆余堂牌参参胶囊、步兵战车、无壳弹、级潜艇、自动步枪、莫折大提起义、勒伯勒东、叶卡捷琳堡号核潜艇</td>
</tr>
<tr>
<td style="text-align:left">39</td>
<td style="text-align:left">农林牧渔</td>
<td>38,611</td>
<td style="text-align:left">广叶参属、大参属、毒参属、佛肚苣苔属、革叶荠属、吡弗咯菌素、大苞鞘花属、大丁草属、大爪草属、女菀属、莎菀属、绿尾大蚕蛾、围绿单爪鳃金龟、咯菌腈、天山邪蒿属</td>
</tr>
<tr>
<td style="text-align:left">40</td>
<td style="text-align:left">文学名著</td>
<td>235,996</td>
<td style="text-align:left">种柳成行夹流水、皂盖朱轮别似空、豸角当邪触、太虚幻境、满铺着寂寞和黑暗、自惟朴且疏、张生马瘦衣且单、早寒风摵摵、浣溪沙、探春、生命也是这般的一瞥么、并仰空若思、叶尾娜、秦女休行、自弄还自罢</td>
</tr>
<tr>
<td style="text-align:left">41</td>
<td style="text-align:left">新番动漫</td>
<td>152,984</td>
<td style="text-align:left">醞、骗、苢、颯、黨、芭、賩、问、覫、鳧、莫、韧、颙、鋨、駩</td>
</tr>
<tr>
<td style="text-align:left">42</td>
<td style="text-align:left">网络用语</td>
<td>23,972</td>
<td style="text-align:left">冰天雪地掩面泪奔、好苦、举手、看好你哦、困揉眼睛、来呀挑衅、脸红掩面、列队、皿哪里跑、摸摸头、人击掌、呜呜呜、凹凸曼、拜托啦人、抱抱</td>
</tr>
<tr>
<td style="text-align:left">43</td>
<td style="text-align:left">市场购物</td>
<td>6,3732</td>
<td style="text-align:left">缪缪、艾拓、盖奇、杰恩万堡、卡莎布兰卡、娜尔思、阿莎琪、阿她琪、阿枝、艾盟、艾娜斯、艾茜芬、奥联金盟、奥倩、奥诗裳</td>
</tr>
<tr>
<td style="text-align:left">44</td>
<td style="text-align:left">电子工程</td>
<td>6,107</td>
<td style="text-align:left">传输线、电偶极子、介电损耗、居里温度、特征阻抗、插入损耗、分频器、椭圆极化、无源网络、线极化、有源网络、圆极化、驻波比、按比例缩小、暗电流</td>
</tr>
<tr>
<td style="text-align:left">45</td>
<td style="text-align:left">金融财经</td>
<td>605,698</td>
<td style="text-align:left">L.P.、Ltd.、LLC、数在校数、学生数在校数、Inc.、Limited、质押、LP、拓日新能、拓尔思、国债、中国平安、中国人寿、中信证券</td>
</tr>
<tr>
<td style="text-align:left">46</td>
<td style="text-align:left">古代历史</td>
<td>114,701</td>
<td style="text-align:left">大畜利貞不家食吉、恒星什宿度、虞陆张骆陆吾朱传、棍噶扎勒参、大庶长、琅邪、孙和、刺史、召陵之盟、巴而术阿而忒的斤、阿史那思摩、莫折大提、仆射、石重贵、陆景</td>
</tr>
<tr>
<td style="text-align:left">47</td>
<td style="text-align:left">世界哲学</td>
<td>20,627</td>
<td style="text-align:left">费尔巴哈、阶级性、经院哲学、拉布里奥拉、李卜克内西、两点论、空亡、纳甲、天刑、阿多诺、鲍威尔、伯恩施坦、布哈林、不可知论、布洛赫</td>
</tr>
<tr>
<td style="text-align:left">48</td>
<td style="text-align:left">通信工程</td>
<td>3,814</td>
<td style="text-align:left">单稽指令、未说娩量、中止的椎、副帧、视见区、无线网卡、集合差、工厂说瞄、侧音、数位叠加和、符号差、单地址指令、单地址信息、单赋值语言、单钮鼠标器</td>
</tr>
<tr>
<td style="text-align:left">49</td>
<td style="text-align:left">人物名称</td>
<td>1,572,202</td>
<td style="text-align:left">醞、骗、苢、颯、黨、芭、张鑫、賩、问、李娟、李莉、覫、刘佳、莫、鳧</td>
</tr>
<tr>
<td style="text-align:left">50</td>
<td style="text-align:left">世界宗教</td>
<td>132,295</td>
<td style="text-align:left">大阿阇黎佛智足、佛前佛后难、佛世差摩竭、东方亦有阿閦鞞佛、鞞侈遮罗那三般那、萨婆僧伽三摩地伽兰地、前佛后佛、第十八祖伽邪舍多、阿悉多伽那、佛不见身知是佛、拘那含牟尼佛、邻阿伽色、阿伽色、大须弥佛、大焰肩佛</td>
</tr>
<tr>
<td style="text-align:left">51</td>
<td style="text-align:left">地理测绘</td>
<td>53,610</td>
<td style="text-align:left">侧分泌说、均变说、夹石、底栖的、单钭的、单栅笔石、风棱石、溺谷、无结构腐殖体、大陆车阀说、底辟构造、陆间裂谷系、地柱说、出射角、红帘石</td>
</tr>
<tr>
<td style="text-align:left">52</td>
<td style="text-align:left">民间习俗</td>
<td>1,365</td>
<td style="text-align:left">八字官星太多、八字无比劫、八字无官星、八字有比劫、八字有官星、白虎持势、白腊金、背禄逐马、比肩劫财与地支、比肩劫财与天干、比劫、比劫帮身、比劫夺财、比劫克财、壁上土</td>
</tr>
<tr>
<td style="text-align:left">53</td>
<td style="text-align:left">书法艺术</td>
<td>28,266</td>
<td style="text-align:left">三击掌、拾玉镯、单弦、苏州弹词、打金枝、副净、慢板、四进士、武松打虎、北京琴书、二人转、河南坠子、湖北大鼓、天津时调、高凤翰</td>
</tr>
<tr>
<td style="text-align:left">54</td>
<td style="text-align:left">期货期权</td>
<td>1,300</td>
<td style="text-align:left">热卷、铜、铝、螺纹钢、锡、锌、镍、PTA、动力煤、菜粕、豆粕、硅铁、玉米淀粉、PVC</td>
</tr>
<tr>
<td style="text-align:left">55</td>
<td style="text-align:left">土木工程</td>
<td>56,720</td>
<td style="text-align:left">等参单元等参数单元等参元、填石、盖梁、拱脚、嵌岩桩、无侧限抗压强度、翼缘、抹角、似棱体、似棱体公式、单墩单墩、刚度比劲度比、内排水系统、伸缩缝、弯曲刚度抗弯劲度</td>
</tr>
<tr>
<td style="text-align:left">56</td>
<td style="text-align:left">安全工程</td>
<td>4,051</td>
<td style="text-align:left">安全性能、疏散时间、隔离栅、灭火器材、安全标志、安全标准、安全防护、安全功能、安全认证、安全设备、安全准则、不安全行为、不安全状态、地下开采、二氧化碳灭火器</td>
</tr>
<tr>
<td style="text-align:left">57</td>
<td style="text-align:left">材料包装</td>
<td>1,473</td>
<td style="text-align:left">焊接面、奥丽斯纹、百花纹、百家姓纹、白牛皮、宝石兰、本白、编织纹、彩胶、茶花纹、充皮纸、大鳄鱼纹、大玫瑰纹、灯笼纹、蝶影纹</td>
</tr>
<tr>
<td style="text-align:left">58</td>
<td style="text-align:left">教育教学</td>
<td>111,449</td>
<td style="text-align:left">旅游管理、江西师范大学、电子信息科学与技术、西南交通大学、郑州大学、教务处、见恶如探汤、麦比乌斯圈、3、西北大学、北方工业大学、北京第二外国语学院、北京工商大学、北京工业大学、北京化工大学</td>
</tr>
<tr>
<td style="text-align:left">59</td>
<td style="text-align:left">家居装饰</td>
<td>8,668</td>
<td style="text-align:left">车脚、搭脚仔凳、汉代陶柜、什景灯、折屏、转屏、眠之堡、松堡王国、阿里斯顿、九牧、林内、奥地雅、白夹竹、大边和抹头、大唐合盛</td>
</tr>
<tr>
<td style="text-align:left">60</td>
<td style="text-align:left">工业设计</td>
<td>7,150</td>
<td style="text-align:left">急流槽、弯沉、压实度、沉降缝、底基层、钢筋笼、钢筋砼、浆砌片石、路堑、路缘石、清表、松铺厚度、圆管涵、锥坡、产品设计</td>
</tr>
<tr>
<td style="text-align:left">61</td>
<td style="text-align:left">物理科学</td>
<td>12,989</td>
<td style="text-align:left">临界指数、布儒斯特角、产生算符、场点、狄拉克方程、对易、对易关系、反射光栅、夫琅禾费衍射、光阑、归一化、基态、角放大率、勒让德变换、洛伦兹变换</td>
</tr>
<tr>
<td style="text-align:left">62</td>
<td style="text-align:left">体育运动</td>
<td>48,602</td>
<td style="text-align:left">奥伦塞彭特、范那佛洛、彼得伯勒联、法恩伯勒、什鲁斯伯里城、大蒙特基奥、哈万特和滑铁卢村、索尔兹伯里城、奥吉贾奥佛埃、奥锡拉库扎、奥伦塞、奇彭纳姆城、艾思莫茨、班伯里联、佛特</td>
</tr>
<tr>
<td style="text-align:left">63</td>
<td style="text-align:left">航空航天</td>
<td>682</td>
<td style="text-align:left">副翼、起落架、襟翼、升降舵、油滤、那拉提、戀攀椀、挀栀愀渀最、猀栀愀渀最、被释放、不安全事件、部分功率、不工作、不亮、不能复位</td>
</tr>
<tr>
<td style="text-align:left">64</td>
<td style="text-align:left">建筑装潢</td>
<td>32,826</td>
<td style="text-align:left">夹景、侧脚、丁顺隔皮砌式、副景、屏石、栅顶、尾景、佛座、普柏枋、寻杖合角造、侧天窗、单材拱、单风道系统、盖瓦、和玺彩画</td>
</tr>
<tr>
<td style="text-align:left">65</td>
<td style="text-align:left">广告传媒</td>
<td>166</td>
<td style="text-align:left">编辑机、彩喷纸、充气模型、促销台、灯箱布、分支器、挂历、光发射机、广告板、广告机、广告牌、广告旗杆、广告条幅、光接收机、光学摄像机</td>
</tr>
<tr>
<td style="text-align:left">66</td>
<td style="text-align:left">汽车行业</td>
<td>10,294</td>
<td style="text-align:left">杂物箱、机油泵、倒车镜、倒车雷达、脚踏板、分离轴承、半轴、换挡杆、节温器、转向灯开关、保险杠、滚针轴承、活塞销、通气塞、油箱盖</td>
</tr>
<tr>
<td style="text-align:left">67</td>
<td style="text-align:left">管理科学</td>
<td>20,751</td>
<td style="text-align:left">不孕不育、预防成本、最低库存、最高库存、安全班前会、安全标志使用导则、安全色、班前会、班前会记录、搬运分析、班组建设、备品备件、闭环、闭环与关闭、必要动作</td>
</tr>
<tr>
<td style="text-align:left">68</td>
<td style="text-align:left">动植生物</td>
<td>314,030</td>
<td style="text-align:left">鳉、尾棘无壳侧鳃、无壳侧鳃属、单序波缘大参、单刺侧红糠虾、非洲侧颈龟属、六结侧颈龟、六峰侧颈龟、南美侧颈龟属、奥氏抖尾地雀、无角陶塞特羊、显脉大参、短梗大参、盖革氏离子计数、蓝无壳侧鳃</td>
</tr>
</tbody>
</table>
<br>
<h2 id="项目总结">项目总结</h2>
<p>1、本项目开放了一个涵盖68个领域，带有行业代表性权重的领域词库，规模达到了916万词，是目前开放词典资源中较大规模的一个，填补了一定的空缺。  <br>
2，领域词汇库的构建和开放，是一项基础、必要且重要的工作。可以通过领域开放文本进行挖掘，如基于垂直网站解析、文本特征词提取等诸多方法来实现。 <br>
3，关于领域词汇知识库的构建方法和理论，可以参考之前写的博客《领域词汇知识库的类型、可用资源与构建技术漫谈》：https://blog.csdn.net/lhy2014/article/details/103995629。 <br>
4，语言资源、经典词库的构建，与目前盛行的深度学习自然语言处理并行不悖。将已构建好的领域词库或者知识库融合到深度学习模型当中，是一个很好的前进方向。需要且必要地关注底层语义资源的建设。</p>
<br>
<h2 id="关于作者">关于作者</h2>
<p>刘焕勇，liuhuanyong，现任360人工智能研究院算法专家，前中科院软件所工程师，主要研究方向为知识图谱、事件图谱在实际业务中的落地应用。<br>
得语言者得天下，得语言资源者，分得天下，得语言逻辑者，争得天下。</p>
<ul>
<li>个人主页：https://liuhuanyong.github.io</li>
<li>个人公众号：老刘说NLP</li>
</ul>
<br> 
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>十万级 | 多领域因果事件对数据集对外开源</title>
      <link>https://textdata.cn/blog/2022-11-07-chinese-casual-text-datasets/</link>
      <pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-07-chinese-casual-text-datasets/</guid>
      <description>description用于SEO优化</description>
      <content:encoded><![CDATA[<h2 id="作者">作者</h2>
<p>刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。</p>
<p>就职于360人工智能研究院、曾就职于中国科学院软件研究所。</p>
<p>老刘说NLP，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。</p>
<br>
<p>开放文本中蕴含着大量的逻辑性知识，以刻画事物之间逻辑传导关系的逻辑类知识库是推动知识推理发展的重要基础。
因果抽取是一个十分有趣的话题，研发大规模逻辑推理知识库有助于支持实体或事件等传导驱动决策任务，而目前尚未有开源的因果事件对出现，为了弥补这一空缺，本文对外开源一个面向多领域的十万级因果事件对数据集，可以自行转成因果关系图谱，展开更多有趣实验，供大家一起参考。
地址：https://github.com/liuhuanyong/CausalDataset</p>
<h2 id="一因果抽取常用方法">一、因果抽取常用方法</h2>
<p>我们在《<strong>事件图谱技术：因果关系事件对抽取常用方法的解析与动手实践</strong>》中讲述了因果抽取的方法，从传统模式规则、语义分析、依存句法、序列标注四种方式进行实践，并配上实现项目进行讲解，这涵盖了当前因果事件抽取的常用方式。</p>
<p>地址： <a href="https://github.com/liuhuanyong/CausalityEventExtraction">https://github.com/liuhuanyong/CausalityEventExtraction</a></p>
<h3 id="11-基于模式匹配的因果事件对提取">1.1 基于模式匹配的因果事件对提取</h3>
<p>基于模式匹配的方式，是进行因果抽取的入门级以及兜底方式，充分利用好语言学知识，具有显式标记的因果关联词、因果表达句式进行归纳，并配以正则表达式实现，可以有效地提取出大量的因果事件对。</p>
<br>
<h3 id="12-基于语义角色的因果事件抽取">1.2 基于语义角色的因果事件抽取</h3>
<p>基于触发词模式匹配的方法无法捕捉因果事件之间的关联关系，因此可以借助依存句法分析以及语义角色标注的方式进行处理。</p>
<p>以因果关系触发词为核心动作，首先从语义角色方面找寻该触发词动作的实施对象和受事对象，将实施对象作为原因事件，将受事对象作为结果事件，并根据词性过滤事件；</p>
<br>
<h3 id="13-基于依存句法的因果事件抽取">1.3 基于依存句法的因果事件抽取</h3>
<p>由于自然语言处理的复杂性，LTP中未能对一些子句中的因果关系触发词进行语义角色标注，或者只标注了一部分，即A0和A1未同时被标注出来，因此利用依存句法分析来抽取此类情况下的因果事件对。</p>
<br> 
<h3 id="14-基于序列标注的因果抽取">1.4 基于序列标注的因果抽取</h3>
<p>针对基于规则的因果抽取模型中的不足，可以使用基于Bert微调的序列标注模型。在序列标签的设计上，模型的序列标签采用BIO标签体系，标签类型主要为cause、triger、effect。
为了能方便地根据标签结果进行因果三元组组合，在设计标签体系时也对单因果、多因果进行了区分，分别设置为multi-cause、multi-effect。</p>
<p><br><br></p>
<h2 id="二基于多领域文本数据集的因果事件对">二、基于多领域文本数据集的因果事件对</h2>
<p>为了得到多领域因果事件对，我们以清华大学开源的文本分类数据集THUnews，<strong>THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式</strong>。</p>
<p>其在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。满足了多领域性的需求。</p>
<p><strong>数据地址：http://thuctc.thunlp.org/#中文文本分类数据集THUCNews</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">训练因果抽取识别模型，最终去重得到了100,688条因果关系对，通过对频次进行统计，可以过滤出质量较高的因果对，下面显示了格式为原因事件@结构事件\t出现频次格式下的数据样例。
投资风险巨大@本金全部亏损 248
用户友好界面@模式帮助用户选择场景 38
政策消息面和技术面所有信息@交易者预期变 37
磨砂表面处理@触感更佳 31
加上F2大光圈和丰富手动功能@机器推出受到消费者广泛关注 26
金属材质设计@整体造型更具品质感 25
商务机型中并常见@上下边框显得厚 23
顶盖采用工程塑料制成配@笔记本外壳防滑耐磨 19
取消传统曲面过度@iPhone4底部扬声器变得硕大 17
准专业机型GRDIGITALII和GX200电子水平仪功能引进@使用R10拍摄高楼山水 16
镜头位移减震功能以及闪光灯控制系统@低光照下拍摄照片时噪 14
像素触摸式液晶屏幕@操控方面人性化 14
采用直线条形式边框风格@整体看上去大气 14
像素摄像头镶嵌屏幕上方@视频聊天方便 14
</code></pre></div><br>
<h3 id="21-关于地震相关的因果事件对">2.1 关于“地震”相关的因果事件对</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">日本东北部海域发生里氏大地震@重大人员伤亡和财产损失 6
日本东北部海域发生里氏地震@重大人员伤亡和财产损失 5
印尼西爪哇省附近印度洋海域发生里氏地震@人死亡人受伤 4
智利中南部城市康塞普西翁附近发生里氏强烈地震@重大人员伤亡 3
智利发生里氏地震@重大人员伤亡和财产损失 3
东部凡省发生强烈地震@死亡人数 3
上周五地震中受损核反应堆发生爆炸@核工业相关公司股票 3
日本大地震@金融市场动 3
最近地震和海啸灾害中复苏@日元汇率下跌 3
日本东北部大地震@全球关注 2
汶川地震期间捐款数目@高度关注 2
</code></pre></div><br>
<h3 id="22-与贬值相关的因果事件对">2.2 与“贬值”相关的因果事件对</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">虚拟道具贬值@广范围用户付费意愿越来越低 3
流动性过剩加剧@贬值趋势 3
日本核泄露事件@外资产贬值 3
全球性经济复苏以及贬值流动性过剩@全球商品价格出现暴涨 3
朝鲜进行货币贬值@市场经济瘫痪 2
欧洲主权债务危机深化和亚洲国家货币贬值@日本有警惕金融资本市场动荡 2
游戏公司滥发虚拟物品@玩家虚拟物品贬值 2
住房价格贬值@全球经济下滑形势演变成 2
中长期内贬值@资金撤离资产 2
持续贬值和人民币升值预期@中国内地成为资金洼地 2
韩元贬值@进口商品价格上升 2
货币大体上呈贬值趋势@国际油价名义价格走高 2
朱广沪时期大面积召人@国家队贬值 1
</code></pre></div><br>
<h3 id="23-与恋爱相关的因果事件对">2.3 与“恋爱”相关的因果事件对</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">恋爱观婚姻观@观众极大兴趣 2
恋爱问题@学生意外伤害事 2
人相知相惜@恋爱温度始终保持合适系数 1
持人大爆钱包@恋爱故事 1
来美丽密令恋爱线人电影@陆毅闪耀大银幕上 1
李成儒和小演员侯角恋爱往事@媒体关注 1
歌曲转换过渡上显得流畅@听起来实在如男女恋爱中不伦恋 1
抓紧时间南京谈恋爱@台上台下哄笑 1
公司安排工作@没时间恋爱 1
强打精神去面对@恋爱没有兴趣 1
</code></pre></div><br>
<br>
<h2 id="总结">总结</h2>
<p>本文以清华大学开源的文本分类数据集THUnews，对外开源了一个面向多领域的十万级因果事件对数据集，并介绍了常用技术方法。当然，数据的质量也有不足之处，规模不大，可以加以改善。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>小规模金融并购、投资事件图谱设计概述与数据构成解析</title>
      <link>https://textdata.cn/blog/2022-11-07-financial-invest-merge/</link>
      <pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-07-financial-invest-merge/</guid>
      <description>小规模金融并购、投资事件图谱设计概述与数据构成解析</description>
      <content:encoded><![CDATA[<h2 id="作者">作者</h2>
<p>刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。</p>
<p>就职于360人工智能研究院、曾就职于中国科学院软件研究所。</p>
<p>老刘说NLP，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。</p>
<br>
<p>事件图谱是当前的一个十分有趣的话题，我们在前面的事件图谱系列文章中对事件图谱进行了论述。</p>
<p>例如文章《技术思考：面向落地应用的事件类图谱划分、关键问题及其与知识图谱的对比辨析》、《事件图谱应用：智能金融与情报分析中的七大应用潜在场景概述》、《事件图谱技术：基于触发词的事件句识别方法与关键流程总结》等。</p>
<p>同样本着技术具像化的原则，为了让大家对具体事件图谱有个清晰的直观的认识，本文我们介绍一个自建的金融事件图谱，涵盖并购和投资两大类事件类型，从金融事件图谱设计概述、投资事件图谱数据介绍以及并购事件图谱数据介绍三个角度进行论述，供大家一起参考。</p>
<br>
<h2 id="一金融事件图谱设计概述">一、金融事件图谱设计概述</h2>
<p>事件知识图谱EKG（event knowledge graph）是当前事件类图谱的一种，在这里，我更倾向于认为这个图谱本身更倾向于为一个事件知识库，而非实体知识图谱。</p>
<p><strong>事件知识图谱的工作主要围绕事件知识本身进行展开，关注点在于事件内部信息，如ACE中的8大类事件，将这几类事件中的信息进行抽取和填充就能够得到一个以特定事件类型作为分类标准的事件知识库，如婚姻事件库、爆炸事件库等。</strong></p>
<p>而相对应的，领域事件图谱显得更为重要，金融领域作为一个需求较为明显的领域，其建模能力更具代表性，例如，我们可以对事件图谱进行本体定义：</p>
<table>
<thead>
<tr>
<th style="text-align:left"><strong>事件类型</strong></th>
<th style="text-align:left"><strong>事件要素</strong></th>
<th style="text-align:left"><strong>事件关系</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">投资事件</td>
<td style="text-align:left">融资方、投资方、金额、轮次、融资时间、所属行业</td>
<td style="text-align:left">顺承/时序</td>
</tr>
<tr>
<td style="text-align:left">并购事件</td>
<td style="text-align:left">并购方、被并购方、并购状态、所属行业、涉及股权、并购开始时间、并购结束时间、是否VC/PE支持</td>
<td style="text-align:left">顺承/时序</td>
</tr>
</tbody>
</table>
<p>在这样一个本体框架之下，我们要构建起一个事件图谱，可以有两种方式：</p>
<ul>
<li>
<p><strong>从已经结构化好的数据源中直接获取</strong>。例如，目前针对投融资领域已经出现了许多垂类网站，如投资界、IT橘子中直接获取，并做清洗。这种方式最为快捷，但受制于人，其中的数据有限，并存在字段不全的问题。当我们想建成一个实时动态的金融事件图谱库，在捕捉实时数据时，及时处理时候，就需要采用抽取的思路。</p>
</li>
<li>
<p><strong>基于模型的非结构化文本抽取</strong>。为了避免方法1带来的拿来主义缺陷，我们可以转换为标准的事件抽取任务，针对实时的实时新闻流，进行论元识别、事件要素抽取。</p>
</li>
</ul>
<p>例如，给定文本：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">8日消息，总部位于墨西哥的在线批发平台Miferia获得了700万美元种子轮融资，该轮融资由贝恩资本风险投资公司和Tiger Global共同领投。Miferia批发平台将墨西哥的独立零售店与化妆品、食品和饮料以及家居装饰等类别的品牌联系起来。该平台拥有来自500多个品牌的数千种产品，每周有30多个新品牌上线。（Latamlist）
</code></pre></div><p>我们可以从中检测出融资事件：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">8月8日消息，总部位于墨西哥的在线批发平台Miferia获得了700万美元种子轮融资，该轮融资由贝恩资本风险投资公司和Tiger Global共同领投。
</code></pre></div><p>并识别出一下结构化信息：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">融资方：Miferia
金额：700万美元
轮次：种子轮以及投资方贝恩资本风险投资公司、Tiger Global；
融资时间：8月8日、所属行业：在线批发等信息
</code></pre></div><p>下图展示了一个金融领域的一个典型投资领域事件图谱：</p>
<p>其中包括“君度德瑞、新余凯信投资、深圳市立德富盈投资等投资信濠光电20%股权”、“东莞中科中广基金（领投）、中广创投、紫宸创投等投资信濠光电5%股权”两个投资事件，每个投资事件由投资方、融资方、金额、日期、轮次几个事件要素构成，<strong>而若以一个融资方为中心进行融资历程的刻画，就可以根据日期发展的先后顺序，在两个事件之间形成一条边</strong>。</p>
<p><img loading="lazy" src="img/fin_edge_networks.png" alt=""  />
</p>
<p>需要注意的是，现在的事件抽取任务中，是不包含事件名称的抽取的，但如果要星辰恶搞事件图谱，就必须保证该事件的唯一性和友好性，可以使用md5值来表示，但并不直观，图中给出了一个较好的例子，用一个短句来表示。</p>
<br>
<h2 id="二投资事件图谱数据介绍">二、投资事件图谱数据介绍</h2>
<p>我们以投资界为数据源，通过解析整理，形成了9093条投资事件，包括融资方、投资方、金额、轮次、融资时间、所属行业共5个要素。</p>
<p><img loading="lazy" src="img/fin_extract_data.png" alt=""  />
</p>
<p>数据样例：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="p">{</span>
    <span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;苏州聚源铸芯创投基金（领投）、创世一期、高捷资本等投资英彼森&#34;</span><span class="p">,</span>
    <span class="s2">&#34;event_type&#34;</span><span class="p">:</span><span class="s2">&#34;投资事件&#34;</span><span class="p">,</span>
    <span class="s2">&#34;融资方&#34;</span><span class="p">:</span><span class="s2">&#34;英彼森半导体（珠海）有限公司&#34;</span><span class="p">,</span>
    <span class="s2">&#34;投资方&#34;</span><span class="p">:[</span>
        <span class="s2">&#34;聚源资本&#34;</span><span class="p">,</span>
        <span class="s2">&#34;高捷资本&#34;</span><span class="p">,</span>
        <span class="s2">&#34;创世伙伴&#34;</span><span class="p">,</span>
        <span class="s2">&#34;绿河投资&#34;</span><span class="p">,</span>
        <span class="s2">&#34;珠海科技创投&#34;</span>
    <span class="p">],</span>
    <span class="s2">&#34;金额&#34;</span><span class="p">:</span><span class="s2">&#34;RMB数亿&#34;</span><span class="p">,</span>
    <span class="s2">&#34;轮次&#34;</span><span class="p">:</span><span class="s2">&#34;A轮&#34;</span><span class="p">,</span>
    <span class="s2">&#34;融资时间&#34;</span><span class="p">:</span><span class="s2">&#34;2021年06月29日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;所属行业&#34;</span><span class="p">:</span><span class="s2">&#34;半导体及电子设备-半导体&#34;</span>
<span class="p">}</span>

<span class="p">{</span>
    <span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;Esta Investments、DD Asset Holdings、DST China EC XI等投资滴滴集团6.08%股权&#34;</span><span class="p">,</span>
    <span class="s2">&#34;event_type&#34;</span><span class="p">:</span><span class="s2">&#34;投资事件&#34;</span><span class="p">,</span>
    <span class="s2">&#34;融资方&#34;</span><span class="p">:</span><span class="s2">&#34;滴滴&#34;</span><span class="p">,</span>
    <span class="s2">&#34;投资方&#34;</span><span class="p">:[</span>
        <span class="s2">&#34;Esta Investments&#34;</span><span class="p">,</span>
        <span class="s2">&#34;腾讯投资&#34;</span><span class="p">,</span>
        <span class="s2">&#34;THL A11&#34;</span><span class="p">,</span>
        <span class="s2">&#34;纪源资本&#34;</span><span class="p">,</span>
        <span class="s2">&#34;数字天空技术&#34;</span>
    <span class="p">],</span>
    <span class="s2">&#34;金额&#34;</span><span class="p">:</span><span class="s2">&#34;USD7.5亿&#34;</span><span class="p">,</span>
    <span class="s2">&#34;轮次&#34;</span><span class="p">:</span><span class="s2">&#34;B轮&#34;</span><span class="p">,</span>
    <span class="s2">&#34;融资时间&#34;</span><span class="p">:</span><span class="s2">&#34;2014年12月02日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;所属行业&#34;</span><span class="p">:</span><span class="s2">&#34;电信及增值业务-无线互联网服务&#34;</span>
<span class="p">}</span>

</code></pre></div><br>
<h2 id="三并购事件图谱数据介绍">三、并购事件图谱数据介绍</h2>
<p>同样的，我们得到了3865条并购事件数据，包括并购方、被并购方、并购状态、所属行业、涉及股权、并购开始时间、并购结束时间以及是否VC/PE支持等事件要素。</p>
<p><img loading="lazy" src="img/fin_extract_data2.png" alt=""  />
</p>
<p>数据样例：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="p">{</span>
    <span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;友睦口腔收购友睦三九60%股权&#34;</span><span class="p">,</span>
    <span class="s2">&#34;event_type&#34;</span><span class="p">:</span><span class="s2">&#34;并购事件&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购方&#34;</span><span class="p">:</span><span class="s2">&#34;深圳市友睦口腔股份有限公司&#34;</span><span class="p">,</span>
    <span class="s2">&#34;被并购方&#34;</span><span class="p">:</span><span class="s2">&#34;深圳友睦三九口腔门诊部有限公司&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购状态&#34;</span><span class="p">:</span><span class="s2">&#34;已完成&#34;</span><span class="p">,</span>
    <span class="s2">&#34;所属行业&#34;</span><span class="p">:</span><span class="s2">&#34;生物技术/医疗健康-医疗服务&#34;</span><span class="p">,</span>
    <span class="s2">&#34;涉及股权&#34;</span><span class="p">:</span><span class="s2">&#34;60.00 %&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购开始时间&#34;</span><span class="p">:</span><span class="s2">&#34;2017年03月21日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购结束时间&#34;</span><span class="p">:</span><span class="s2">&#34;2017年03月21日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;是否VC/PE支持&#34;</span><span class="p">:</span><span class="s2">&#34;是&#34;</span>
<span class="p">}</span>

<span class="p">{</span>
    <span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;我享科技收购我享网络&#34;</span><span class="p">,</span>
    <span class="s2">&#34;event_type&#34;</span><span class="p">:</span><span class="s2">&#34;并购事件&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购方&#34;</span><span class="p">:</span><span class="s2">&#34;上海我享网络信息科技股份有限公司&#34;</span><span class="p">,</span>
    <span class="s2">&#34;被并购方&#34;</span><span class="p">:</span><span class="s2">&#34;上海我享网络科技有限公司&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购状态&#34;</span><span class="p">:</span><span class="s2">&#34;已完成&#34;</span><span class="p">,</span>
    <span class="s2">&#34;所属行业&#34;</span><span class="p">:</span><span class="s2">&#34;互联网-电子商务-C2C&#34;</span><span class="p">,</span>
    <span class="s2">&#34;涉及股权&#34;</span><span class="p">:</span><span class="s2">&#34;N/A&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购开始时间&#34;</span><span class="p">:</span><span class="s2">&#34;2017年03月01日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购结束时间&#34;</span><span class="p">:</span><span class="s2">&#34;2017年03月24日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;是否VC/PE支持&#34;</span><span class="p">:</span><span class="s2">&#34;是&#34;</span>
<span class="p">}</span>
</code></pre></div><br>
<h2 id="总结">总结</h2>
<p>本文我们介绍l额一个自建的金融事件图谱，涵盖并购和投资两大类事件类型，从金融事件图谱设计概述、投资事件图谱数据介绍以及并购事件图谱数据介绍三个角度进行论述，这对加深我们对事件图谱的具象化认识具有一定的意义。</p>
<p>关于具体的数据，可以关注 <strong>公众号：老刘说NLP</strong>，并加入技术社区，与技术社区的朋友一同分享获取。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>预训练词向量模型的方法、应用场景、变体延伸与实践总结</title>
      <link>https://textdata.cn/blog/2022-11-07-embeddings-theory-applicaiton-liuhuanyong/</link>
      <pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-07-embeddings-theory-applicaiton-liuhuanyong/</guid>
      <description>预训练词向量模型的方法、应用场景、变体延伸与实践总结</description>
      <content:encoded><![CDATA[<p><br><br></p>
<h2 id="关于作者">关于作者</h2>
<p>刘焕勇，liuhuanyong，现任360人工智能研究院算法专家，前中科院软件所工程师，主要研究方向为知识图谱、事件图谱在实际业务中的落地应用。<br>
得语言者得天下，得语言资源者，分得天下，得语言逻辑者，争得天下。</p>
<ul>
<li>个人主页：https://liuhuanyong.github.io</li>
<li>个人公众号：老刘说NLP</li>
</ul>
<br>
<p>当前，以预训练语言模型PLM+fintune的自然语言处理范式可谓十分火热，有大量的文章在宣传这类方法，包括梳理以NNLM为起点的整个预训练方法的发展史。</p>
<p>当前工业界，主要使用的预训练模型包括两种，一种是以wordvec为代表的预训练词向量，另一种是以BERT为代表的预训练语言模型。前者通常作为词语表示输入的初始化，后接NN/CNN/LSTM等编码层，后者既可以同样后接，也可以直接接上softmax/crf/span-pointer等进行解码。</p>
<p>本文，主要围绕预训练词向量模型这一主题，对当前预训练词向量模型的常用方法、评估与应用方式、领域的迁移变体、当前开放的训练工具和词向量文件进行介绍，并以word2vec和fasttext为例，展示一个词向量训练的案例，做到理论与实践相结合。</p>
<p><br><br></p>
<h2 id="一预训练词向量模型方法">一、预训练词向量模型方法</h2>
<p>自从进入2010年以来，神经语言模型就逐渐进入人们眼球，以NNLM为典型最初代表的神经网络模型，极大的推动了NLP这一领域的发展。</p>
<p>实际上，早期词向量的研究通常来源于语言模型，比如NNLM和RNNLM，其主要目的是语言模型，而词向量只是一个副产物。著名的harris分布式假说提供了一个局部统计信息的理论基础。</p>
<p>下面就选择其中三种典型进行介绍。</p>
<br>
<h3 id="11-word2vec">1.1 word2vec</h3>
<p>word2vec是2013年Google开源的一款用于词向量计算的工具，通过内置的语言模型训练目标，可以将中间层得到的向量权重矩阵进行抽离，形成每个词对应的向量化表示，包括CBOW、Skip-gram两种方式，前者通过周围词来预测中心词，后者以中心词来预测上下文。</p>
<p><img loading="lazy" src="img/1.png" alt=""  />
</p>
<p>经典的wordvec结构包括输入层、隐藏层和输出层，其计算流程为：</p>
<p>1、输入层存储上下文单词的onehot。假设单词向量空间dim为V，上下文单词个数为C。</p>
<p>2、所有onehot分别乘以共享的输入权重矩阵W。V*N矩阵，N为自己设定的数，初始化权重矩阵W 。</p>
<p>3、所得的向量 相加求平均作为隐层向量, size为1*N。</p>
<p>4、乘以输出权重矩阵W' N*V。</p>
<p>5、得到向量1*V，经过激活函数处理得到V-dim概率分布。</p>
<p>6、Hierarchical Softmax分类，概率最大的index所指示的单词为预测出的中间词与预测值的onehot做比较，根据误差更新权重矩阵。</p>
<p><img loading="lazy" src="img/2.png" alt=""  />
</p>
<p>这个W矩阵就是所有单词的word embedding，任何一个单词的onehot乘以这个矩阵都将得到自己的词向量。</p>
<p>通常，在训练词向量时候，会根据语料的大小来选择相应的训练方法。例如，针对小型的数据集，可以用CBOW算法，该方法对于很多分布式信息进行了平滑处理，将一整段上下文信息视为一个单一观察量，对于小型的数据集，这一处理是有帮助的。相比之下，大型数据集，可以用Skip-Gram模型，该方法将每个“上下文-目标词汇”的组合视为一个新观察量，这种做法在大型数据集中会更为有效。</p>
<br>
<h3 id="12-fasttext">1.2 fasttext</h3>
<p>fastText是Facebook于2016年开源的一个词向量计算和文本分类工具。将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。包括字符级n-gram特征的引入以及分层Softmax分类两种。</p>
<p>与CBOW一样，原本的fastText模型包括输入层、隐含层、输出层，输入都是多个经向量表示的单词，输出都是一个特定的目标，隐含层都是对多个词向量的叠加平均。不同的是，CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档，CBOW的输入单词被onehot编码过，fastText的输入特征是经embedding化的，CBOW的输出是目标词汇，fastText的输出是文档对应的类标。</p>
<p>而如果将该类标替换成中间目标词，那么就可以得到wordvec的升级版，即单纯的词向量模型。例如，word2vec把语料库中的每个单词当成原子的，它会为每个单词生成一个向量。这忽略了单词内部的形态特征。</p>
<p>fasttext使用了字符级别的n-grams来表示一个单词。对于单词“apple”，假设n的取值为3，则它的trigram有“&lt;ap”, “app”, “ppl”, “ple”, “le&gt;”，其中，&lt;表示前缀，&gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，可以用这5个trigram的向量叠加来表示“apple”的词向量。</p>
<p>因此，因为它们的n-gram可以和其它词共享，对于训练词库之外的单词，能够解决或者oov词，这也是在当前很多文本分类、推荐场景中会优先选用fastText作为训练方法。</p>
<br>
<h3 id="13-glove">1.3 Glove</h3>
<p>GloVe是斯坦福团队于2014年提出一个词向量方法，全名叫“Global Vectors”，直接利用全局的统计信息进行训练。</p>
<p>与上述两种方式靠滑动窗口来制造局部上下文不同，GloVe会用到全局的词语之间共现的统计信息，即词的出现次数，词对之间的共现概率，形成共现概率矩阵，并试图生成词向量来毕竟共现概率，利用Word2Vec的skip-gram算法的高性能来解决LDA的计算量复杂问题。</p>
<p>因此，我们可以发现，Glove需要事先统计共现概率，这也让其通常被认为是无监督学习，实际上glove还是有label的，即共现次数。与wordvec还有一处不同的是，损失函数是最小平方损失函数，权重可以做映射变换。</p>
<p><br><br></p>
<h2 id="二预训练词向量的训练参数">二、预训练词向量的训练参数</h2>
<p>词向量模型的超参数很多，不同的参数选择会取得不同的效果，并且，word2vec中有几个大家提的比较多的问题。以gensim-word2vec为例，包括以下参数：</p>
<ul>
<li>sentences： 可以是一个list，对于大语料集，可使用BrownCorpus,Text8Corpus或LineSentence构建；</li>
<li>sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法；</li>
<li>size： 特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百；</li>
<li>window： 表示当前词与预测词在一个句子中的最大距离是多少；</li>
<li>alpha: 学习速率；</li>
<li>seed： 用于随机数发生器。与初始化词向量有关；</li>
<li>min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5；</li>
<li>max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制；</li>
<li>sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)；workers参数控制训练的并行数；</li>
<li>hs: 如果为1则会采用hierarchical softmax技巧。如果设置为0（defaut），则negative sampling会被使用；</li>
<li>negative: 如果&gt;0,则会采用negativesamping，用于设置多少个noise words；</li>
<li>cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defaut）则采用均值。只有使用CBOW的时候才起作用；</li>
<li>hashfxn： hash函数来初始化权重。默认使用python的hash函数；</li>
<li>iter： 迭代次数，默认为5；</li>
<li>trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RUE_DISCARD,utis.RUE_KEEP或者utis.RUE_DEFAUT的函数；</li>
<li>sorted_vocab： 如果为1（defaut），则在分配word index 的时候会先对单词基于频率降序排序；</li>
<li>batch_words： 每一批的传递给线程的单词的数量，默认为10000。</li>
</ul>
<p>不过，如此多的参数不一定能跳得过来，因此通常会集中在以下常规参数：</p>
<p><img loading="lazy" src="img/3.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三预训练词向量的评估与应用">三、预训练词向量的评估与应用</h2>
<p>预训练词向量生产出来，需要进行性能的评估。这方面的方法包括基于评测集，或者基于具体业务使用，用业务的指标来进行评估。</p>
<h3 id="31-预训练词向量的评估">3.1 预训练词向量的评估</h3>
<p>学术上，词向量的质量通常由类比问题任务进行评估。如CA-translated包含了三个语义问题和134个中文词。CA8 是专门为中文语言设计的。它包含了 17813 个类比问题，覆盖了综合的词法和语义关联。</p>
<p>工业，则使用词向量来代替之前随机生成的词向量文件，来对自然语言处理中的文本/情感分类、实体识别、关系抽取等任务进行评估。</p>
<br>
<h3 id="32-预训练词向量的应用">3.2 预训练词向量的应用</h3>
<p>预训练词向量文件最大的价值在于解决了一个词语的初始化稠密表示，在解决当前以数值化为输入的深度或机器学习模型第一部的同时，还保留了一个词的区别性特征。</p>
<p>一方面，当前词向量可以用于近义词挖掘的重要来源，通过某个词，通过计算词与其他词之间的相似度，并设定阈值，可以迭代挖掘出大量的相关词【过程中要注意语义漂移】。而这个词，直接就可以用于当前的搜索查询扩展、领域词构建等场景。进一步的，在模型方面，还可以作为EDA数据增强工作中的重要补充。</p>
<p>另一方面，词向量可以用于当前无监督文本表示的重要方法，通过对文本进行分词，然后找到词语对应的向量，通过向量叠加的方式可以快速得到一个文本的向量表示，这一表示在诸如情感分析、句子相似度计算等任务中是实际有效的，基于文本表示，也可以进一步提升文本分类、聚类、相似query召回等使用场景性能，甚至很形象的成为了当前业务模型的baseline或者兜底模型。</p>
<p><br><br></p>
<h2 id="四预训练词向量的变体延伸">四、预训练词向量的变体延伸</h2>
<h3 id="41-gramembedding">4.1 gramEmbedding</h3>
<p>共现信息，是cbow以及skipgram的基础，其本质在于通过周围词来建模中心词或者用中心词来建模周围词。因此，通过构造不同的共现信息，可以得到不同类型的向量形式。这里取了个名字叫gramembedding，用于表示专指文本的一系列embedding变体。</p>
<p>例如，对于一个词来说，我们可以把词拆分为词word、n元序列ngram、汉字character，偏旁部首Radical，词性POS，依存关系dependency、拼音pinying。</p>
<p>单元的共现，我们同样可以进行组合，例如，构造word-word，word-ngram、ngran-ngram等，得到上下文特征（单词、n-gram、字符等）等不同粒度的词向量。</p>
<p>观察近几年的发展，词向量可以进一步分成偏旁部首向量、字符向量等。如香侬科技推出的glyce向量，引入汉字的字形特征。蚂蚁金服推出的cw2vec字符向量，将汉字拆解成偏旁、字件进行建模。</p>
<p><img loading="lazy" src="img/4.png" alt=""  />
</p>
<p>当ngram中的n为1时，可以得到字向量，n为2或者更多时，则可以得到词向量等。fasttext中，就是得到了ngram的向量，并进行加和，得到一个OOV词语的向量进行表示。</p>
<p>例如，基于skigram，分别设定词向量的维度及其他超参数，可以得到字向量,拼音向量，词向量，词性向量，通过上下文共现与PCA降维的方法可以得到依存向量。</p>
<p><img loading="lazy" src="img/5.png" alt=""  />
</p>
<p>从下面的结果可以看出，词和字向量的效果看起来还不错。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    ***********************字符向量************************
    token:刘
    (&#39;李&#39;, 0.7306396961212158),(&#39;陈&#39;, 0.7201231122016907)
    (&#39;赵&#39;, 0.6974461674690247),(&#39;杨&#39;, 0.6972213983535767)
    (&#39;吴&#39;, 0.6851627230644226),(&#39;徐&#39;, 0.6516467332839966)
    (&#39;郭&#39;, 0.6499480605125427),(&#39;蔡&#39;, 0.6175302267074585)
    (&#39;郑&#39;, 0.6092196106910706),(&#39;孙&#39;, 0.5950524210929871)
    token:丑
    (&#39;卯&#39;, 0.6074919700622559),(&#39;酉&#39;, 0.5910211801528931)
    (&#39;巳&#39;, 0.5581363439559937),(&#39;戌&#39;, 0.43932047486305237)
    (&#39;戊&#39;, 0.41449615359306335),(&#39;壬&#39;, 0.40456631779670715)
    (&#39;謤&#39;, 0.367109090089798),(&#39;绯&#39;, 0.3643313944339752),
    (&#39;寅&#39;, 0.36351141333580017),(&#39;旽&#39;, 0.3549465537071228)

    ***********************依存向量************************
    dependency rel:ATT
    (&#39;COO&#39;, 0.14239487051963806),(&#39;ADV&#39;, -0.16987691819667816)
    (&#39;RAD&#39;, -0.2357601821422577),(&#39;HED&#39;, -0.2401314228773117)
    (&#39;SBV&#39;, -0.25625932216644287),(&#39;WP&#39;, -0.27165737748146057)
    (&#39;LAD&#39;, -0.2902592420578003),(&#39;POB&#39;, -0.2990782558917999)
    (&#39;VOB&#39;, -0.37553706765174866),(&#39;IOB&#39;, -0.6669262647628784)
    dependency rel:POB
    (&#39;IOB&#39;, 0.16698899865150452),(&#39;DBL&#39;, 0.16678886115550995)
    (&#39;FOB&#39;, 0.1657436639070511),(&#39;CMP&#39;, 0.14784857630729675)
    (&#39;VOB&#39;, 0.1461176574230194),(&#39;SBV&#39;, 0.08011472970247269)
    (&#39;LAD&#39;, -0.022307466715574265),(&#39;WP&#39;, -0.022942926734685898)
    (&#39;HED&#39;, -0.037264980375766754),(&#39;RAD&#39;, -0.042251598089933395)

    ***********************拼音向量************************
    pinyin:wo
    (&#39;shei&#39;, 0.6129732131958008)(&#39;ta&#39;, 0.6081706285476685)
    (&#39;nin&#39;, 0.5819231867790222),(&#39;！&#39;, 0.5435523986816406)
    (&#39;……&#39;, 0.48428624868392944),(&#39;ai&#39;, 0.47832390666007996)
    (&#39;o&#39;, 0.4761071801185608),(&#39;。』&#39;, 0.4598163366317749)
    (&#39;...&#39;, 0.45207729935646057),(&#39;ni&#39;, 0.44975683093070984)
    pinyin:guo
    (&#39;dang&#39;, 0.3908974528312683),(&#39;yuan&#39;, 0.378823846578598)
    (&#39;zu&#39;, 0.35387369990348816),(&#39;hua&#39;, 0.3405681848526001)
    (&#39;zheng&#39;, 0.3355437219142914),(&#39;yi&#39;, 0.3333034813404083)
    (&#39;ren&#39;, 0.3194104731082916),(&#39;jun&#39;, 0.3187354505062103)
    (&#39;hui&#39;, 0.31342023611068726),(&#39;xin&#39;, 0.3096797466278076)

    ***********************词性向量************************
    word postag:a
    (&#39;d&#39;, 0.7203904986381531),(&#39;c&#39;, 0.6124969720840454)
    (&#39;v&#39;, 0.4963228106498718),(&#39;an&#39;, 0.4531499147415161)
    (&#39;uz&#39;, 0.4459834396839142),(&#39;ud&#39;, 0.42059916257858276)
    (&#39;r&#39;, 0.4090540111064911),(&#39;uj&#39;, 0.4061364233493805)
    (&#39;i&#39;, 0.38707998394966125),(&#39;l&#39;, 0.3551557660102844)
    word postag:n
    (&#39;b&#39;, 0.7030695676803589),(&#39;vn&#39;, 0.490166038274765)
    (&#39;p&#39;, 0.4858315885066986),(&#39;v&#39;, 0.4499088227748871)
    (&#39;nt&#39;, 0.44155171513557434),(&#39;f&#39;, 0.26609259843826294)
    (&#39;s&#39;, 0.2639649212360382),(&#39;l&#39;, 0.24365971982479095)
    (&#39;ns&#39;, 0.2278469204902649),(&#39;m&#39;, 0.202927365899086)
    ***********************词向量************************
    word:爱情
    (&#39;爱恋&#39;, 0.6931096315383911),(&#39;真爱&#39;, 0.6897798776626587)
    (&#39;婚姻&#39;, 0.6540514826774597),(&#39;浪漫爱情&#39;, 0.6535360813140869)
    (&#39;情感&#39;, 0.6501022577285767),(&#39;感情&#39;, 0.6403399705886841)
    (&#39;纯爱&#39;, 0.6394841074943542),(&#39;爱情故事&#39;, 0.6282097101211548)
    (&#39;校园爱情&#39;, 0.6078493595123291),(&#39;情爱&#39;, 0.5976818799972534)
    word:创新
    (&#39;技术创新&#39;, 0.7648976445198059),(&#39;不断创新&#39;, 0.7172579765319824)
    (&#39;创新型&#39;, 0.6573833227157593),(&#39;创新能力&#39;, 0.6533682942390442)
    (&#39;创新性&#39;, 0.6160774827003479),(&#39;革新&#39;, 0.6159394383430481)
    (&#39;人才培养&#39;, 0.6093565821647644),(&#39;开拓创新&#39;, 0.6015594601631165)
    (&#39;探索&#39;, 0.5987343788146973),(&#39;技术革新&#39;, 0.5949685573577881)
</code></pre></div><p>从上，也看到一些十分有趣的现象：</p>
<p>1）依存向量，依存向量中可以看出，ATT作为定中关系，在依存关系中属于定中结构，COO(联合)，ADV(状中)的相似度要比主谓SBV，动宾VOB的相似度要高。另外，作为介宾的POB，相似的有IOB，DBL，FOB，这些关系均与宾语成分相关</p>
<p>2）拼音向量，从wo，guo的拼音相似拼音来看，我们可以看到，这种相似的拼音更像是一种搭配， 很有意思，(词性参照jieba分词词性对照表)。</p>
<p>3）词性向量，从a，n的相似词性来看，也似乎更像是一种搭配现象，或许有更好的解释。</p>
<br>
<h3 id="42-domainembedding">4.2 DomainEmbedding</h3>
<p>为了更好的适配不同领域的任务，当前也有很多的公司或者任务会选择使用领域性的领域进行训练，以得到不同领域的词向量文件，这与当前各种领域的bert模型做法是类似的。当前出现了金融领域bert、法律领域的bert等。</p>
<p>代表性的，2018年推出的Chinese-Word-Vectors中提供了包含经过数十种用各领域语料（百度百科、维基百科、人民日报 1947-2017、知乎、微博、文学、金融、古汉语等）训练的词向量，涵盖各领域，且包含多种训练设置。</p>
<p><img loading="lazy" src="img/6.png" alt=""  />
</p>
<p>又如，当前PaddleNLP官方提供了61种可直接加载的预训练词向量，训练自多领域中英文语料、如百度百科、新闻语料、微博等，覆盖多种经典词向量模型（word2vec、glove、fastText）、涵盖不同维度、不同语料库大小。</p>
<br>
<h3 id="43-graphembdding">4.3 GraphEmbdding</h3>
<p>经典的deepwalk以及node2vec也是借鉴word2vec思想，学习图节点嵌入的方法。并且成为当前推荐系统中的一个重量级使用方法。</p>
<p><strong>1、Deepwalk</strong></p>
<p>通过对图中的节点进行随机游走（主要考虑深度优先遍历），形成节点之间的游走序列，并将其作为上下文，后面接入skipgram形成节点向量，从构造上来看，就是首先利用random walk来表示图结构，然后利用skip-gram模型来更新学习节点表示。</p>
<p>随机选取与其邻接的下一个结点，直至达到给定长度，这个长度作为一个参数进行指定，这个类似于word2vec中的window_size上下文窗口。</p>
<p><img loading="lazy" src="img/7.png" alt=""  />
</p>
<p><strong>2、node2vec</strong></p>
<p>node2vec综合考虑了广度优先遍历（用于捕捉局部信息）和深度优先遍历（用于捕捉全局信息）的游走，提出二阶随机游走思想，解决内容相似和结构相似的问题。</p>
<p><img loading="lazy" src="img/8.png" alt=""  />
</p>
<p>前者具有直接链接关系的两个节点，我们可以认为是内容相似的（例如两个灰色网站之间很有可能能够直接跳转，如图中的s1，s2等一阶邻居）、结构相似（例如周围邻居数量都很类似，如图中的s6和u节点，两个都有4个邻接，结构类似）。</p>
<p><img loading="lazy" src="img/9.png" alt=""  />
</p>
<p>具体实现思路也很简单：</p>
<p>我们从节点v转移到节点t，并且当前在节点t时，需要考虑下一个采样节点x。因此，可以设计一个节点到它的不同邻居的转移概率：</p>
<p><img loading="lazy" src="img/10.png" alt=""  />
</p>
<p>其中，每一步采样都会有三种状态，分别对应于上图的0，1，2三种情况：</p>
<ul>
<li><strong>1）0代表如果t和x相等，那么采样的概率为1/p；</strong></li>
<li><strong>2）1代表t与x相连，采样的概率为1；</strong></li>
<li>3）2代表t与x不相连，采样的概率为1/q**</li>
</ul>
<p>式子中的参数p作为返回参数，控制重新采样上一步已访问节点的概率。参数q，作为出入参数，控制采样的方向。</p>
<p>其中：</p>
<ul>
<li><strong>1）当q&gt;1时，接下来采样的节点倾向于节点t，偏向于广度优先；</strong></li>
<li><strong>2）当q&lt;1时，接下来采样的节点倾向于远离t，偏向于深度优先遍历。</strong></li>
<li><strong>3）当p&gt;max(q,1)时，接下来采样的节点很大概率不是之前已访问节点，这一方法使得采样偏向深度优先；</strong></li>
<li><strong>4）当p&lt;max(q,1)时，接下来采样的节点很大概率是之前已访问节点，这一方法使得采样偏向广度优先。</strong></li>
</ul>
<p>此外，在推荐场景中也有item2vec的类似延伸，例如协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达。</p>
<p><br><br></p>
<h2 id="五预训练词向量的动手实操">五、预训练词向量的动手实操</h2>
<p>纸上得来终觉浅，觉知此事要躬行，能够动手实践是加强对该概念理解的重要方式。预训练词向量，在流程上，应该包括全量训练和增量训练两种。前者可以在有大规模训练语料的情况下得到领域的向量，后者适用于小语料微调。
下面以gemsim中的wordvec和fasttext为例进行实践，大家可以看出其中的一些具体的步骤和结果。</p>
<h3 id="51-word2vec向量训练">5.1 word2vec向量训练</h3>
<h4 id="1构造训练语料">1、构造训练语料</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># coding = utf-8</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="n">cur</span> <span class="o">=</span> <span class="s1">&#39;/&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">Trainvec</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;lawsuit.json&#34;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;duanzi.txt&#34;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">build_corpus</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">train_path</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;train.txt&#34;</span><span class="p">),</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">json_obj</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">json_obj</span><span class="p">[</span><span class="s2">&#34;content&#34;</span><span class="p">]</span>
                <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
                <span class="n">cut_wds</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
                <span class="n">train_path</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cut_wds</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">train_path</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">build_update_corpus</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">train_path</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;update.txt&#34;</span><span class="p">),</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">cut_wds</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                <span class="n">train_path</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">cut_wds</span> <span class="k">if</span> <span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">train_path</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
	  <span class="n">handler</span> <span class="o">=</span> <span class="n">Trainvec</span><span class="p">()</span>
    <span class="c1">#handler.build_corpus()</span>
    <span class="n">handler</span><span class="o">.</span><span class="n">build_update_corpus</span><span class="p">()</span>
</code></pre></div><br>
<h4 id="2配置输入与输出路径">2、配置输入与输出路径</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">word2vec</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="n">cur</span> <span class="o">=</span> <span class="s1">&#39;/&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;train.txt&#34;</span><span class="p">)</span>
<span class="n">update_filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;update.txt&#34;</span><span class="p">)</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec.model&#34;</span>
<span class="n">model_update_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec_update.model&#34;</span>
<span class="n">model_vec_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec.bin&#34;</span>
<span class="n">model_update_vec_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec_update.bin&#34;</span>
</code></pre></div><br>
<h4 id="3全量数据预训练">3、全量数据预训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">full_train_embedding</span><span class="p">():</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">min_word_count</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">context</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">downsampling</span> <span class="o">=</span> <span class="mf">1e-3</span>
    <span class="c1"># 获取日志信息</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># 加载分词后的文本，使用的是Text8Corpus类</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Text8Corpus</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="c1"># 训练模型，部分参数如下</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
                              <span class="n">size</span><span class="o">=</span><span class="n">num_features</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_word_count</span><span class="p">,</span>
                              <span class="n">window</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="n">downsampling</span><span class="p">)</span>
    <span class="c1">#保存模型,除包含词-向量,还保存词频等训练所需信息</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1">#保存词向量文件,保存的模型仅包含词-向量信息</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div><p>在保存过程中，存在两种方式，保存模型,除包含词-向量,还保存词频等训练所需信息，保存词向量文件,保存的模型仅包含词-向量信息。所以我们可以看到，词向量文件，确实是word2vec模型的副产物。</p>
<br>
<h4 id="4增量数据预训练">4、增量数据预训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">增量训练</span><span class="err">，</span><span class="n">主要解决在新的文本上进行训练</span><span class="err">，</span><span class="n">也可以引入一些新的词</span><span class="err">，</span><span class="n">但这个时候</span><span class="err">，</span><span class="n">需要考虑到min_count这一过滤条件</span><span class="err">。</span>

<span class="k">def</span> <span class="nf">update_train_embedding</span><span class="p">():</span>
    <span class="c1"># 获取日志信息</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># 加载新的训练数据</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">LineSentence</span><span class="p">(</span><span class="n">update_filepath</span><span class="p">)</span>
    <span class="c1"># 加载旧模型</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1"># 更新词汇表</span>
    <span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># 训练数据</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">epochs</span><span class="p">)</span>  <span class="c1"># epoch=iter语料库的迭代次数；（默认为5）  total_examples:句子数。</span>
    <span class="c1"># 保存模型，是分成两个来训练</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_update_path</span><span class="p">)</span>
    <span class="c1"># 保存词向量文件</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_update_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div><br> 
<h4 id="5词向量结果测试">5、词向量结果测试</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">test_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s2">&#34;wordvec.model.bin&#34;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">while</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">wd</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&#34;enter an word:&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">wd</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">words</span>
</code></pre></div><p>通过运行，我们可以得到如下查询结果：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">enter an word:开心
[(&#39;高兴&#39;, 0.7237069606781006), (&#39;有缘&#39;, 0.7097823619842529), (&#39;开了花&#39;, 0.7021969556808472), (&#39;玩得&#39;, 0.6799882650375366), (&#39;快乐&#39;, 0.6698621511459351), (&#39;不亦乐乎&#39;, 0.668710470199585), (&#39;鉴宝&#39;, 0.6672042012214661), (&#39;越聊&#39;, 0.6671714782714844), (&#39;爱玩&#39;, 0.6659203767776489), (&#39;着迷&#39;, 0.6657696962356567)]
enter an word:混蛋
[(&#39;享福&#39;, 0.9413065910339355), (&#39;没良心&#39;, 0.9331107139587402), (&#39;怪不得&#39;, 0.9317291975021362), (&#39;养不活&#39;, 0.9283043742179871), (&#39;好惨&#39;, 0.9255991578102112), (&#39;看笑话&#39;, 0.9251411557197571), (&#39;逗我&#39;, 0.9232471585273743), (&#39;命苦&#39;, 0.9226915836334229), (&#39;别怪&#39;, 0.921725332736969), (&#39;我养&#39;, 0.9205465316772461)]
enter an word:巴嘎
KeyError: &#34;word &#39;巴嘎&#39; not in vocabulary&#34;
</code></pre></div><p>从上面我们可以看到，wordvec中对于词表外的词是无法查询的，为了缓解这一问题，可以通过训练时候的min_count参数调至1，以覆盖更多的词语，另一种则是进行增量训练。</p>
<br>
<h3 id="52-fasttext向量训练">5.2 fasttext向量训练</h3>
<p>与wordvec类似，fasttext也才用了类似的训练方法。</p>
<h4 id="1全量数据训练">1、全量数据训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">full_train_embedding</span><span class="p">():</span>
    <span class="n">feature_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">window_size</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">min_count</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">workers</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">corpus_file</span> <span class="o">=</span> <span class="n">datapath</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_count</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">workers</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="n">corpus_file</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
        <span class="n">corpus_file</span><span class="o">=</span><span class="n">corpus_file</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
        <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">total_words</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_total_words</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1">#保存词向量文件,保存的模型仅包含词-向量信息</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><br>
<h4 id="2增量数据训练">2、增量数据训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">update_train_embedding</span><span class="p">():</span>
    <span class="c1"># 获取日志信息</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># 加载新的训练数据</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">LineSentence</span><span class="p">(</span><span class="n">update_filepath</span><span class="p">)</span>
    <span class="c1"># 加载旧模型</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1"># 更新词汇表</span>
    <span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># 训练数据</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">epochs</span><span class="p">)</span>  <span class="c1"># epoch=iter语料库的迭代次数；（默认为5）  total_examples:句子数。</span>
    <span class="c1"># 保存模型，是分成两个来训练</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_update_path</span><span class="p">)</span>
    <span class="c1"># 保存词向量文件</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_update_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>
</code></pre></div><br>
<h4 id="3词向量结果测试">3、词向量结果测试</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">test_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="k">while</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">wd</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&#34;enter an word:&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">wd</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">return</span>
</code></pre></div><p>通过执行，我们会得到以下查询结果：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">enter an word:开心
[(&#39;开心果&#39;, 0.7953568696975708), (&#39;高兴&#39;, 0.7377268671989441), (&#39;郡县&#39;, 0.6981974244117737), (&#39;有缘&#39;, 0.6916821002960205), (&#39;折勾以&#39;, 0.687650203704834), (&#39;爱&#39;, 0.684776782989502), (&#39;愉快&#39;, 0.6840348243713379), (&#39;快乐&#39;, 0.676334023475647), (&#39;太高兴&#39;, 0.6728817224502563), (&#39;放心&#39;, 0.6692144274711609)]
enter an word:混蛋
[(&#39;侯希辰&#39;, 0.7582178115844727), (&#39;舐&#39;, 0.7578023672103882), (&#39;走眼&#39;, 0.7541716694831848), (&#39;有眼&#39;, 0.7511969804763794), (&#39;贺应勤&#39;, 0.7478049397468567), (&#39;罗敏&#39;, 0.747008204460144), (&#39;郭守桥&#39;, 0.7450246810913086), (&#39;熊芳琴&#39;, 0.7417726516723633), (&#39;找死&#39;, 0.741632342338562), (&#39;许身&#39;, 0.7414941787719727)]
enter an word:巴嘎
[(&#39;陈晓大爆&#39;, 0.3896751403808594), (&#39;董王勇&#39;, 0.36747634410858154), (&#39;李刚&#39;, 0.34988462924957275), (&#39;曾杰&#39;, 0.34452974796295166), (&#39;张文宾&#39;, 0.3370075821876526), (&#39;成浩&#39;, 0.3369928300380707), (&#39;刘晓静&#39;, 0.3348349630832672), (&#39;刘晓丹&#39;, 0.3348219394683838), (&#39;刘骏&#39;, 0.32817351818084717), (&#39;吴建明&#39;, 0.32765522599220276)]
</code></pre></div><p>与上面的wordvec无法处理OOV问题不同，对于八嘎这一词，fasttext依旧可以推断出来，关于这个中间步骤，我们可以作为单独一个问题来说明。</p>
<br>
<h4 id="4fasttext是如何解决oov问题的">4、fasttext是如何解决oov问题的</h4>
<p>通过对其源码进行阅读，可以发现fasttext针对OOV词的原始计算方式包括三个步骤，</p>
<ul>
<li>1）抽取出每个词的N-grams;</li>
<li>2）与预先存好的n-grams词库进行匹配;</li>
<li>3）将匹配到的n-gram向量进行平均，实现如下：</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.utils_any2vec</span> <span class="kn">import</span> <span class="n">_save_word2vec_format</span><span class="p">,</span> <span class="n">_load_word2vec_format</span><span class="p">,</span> <span class="n">_compute_ngrams</span><span class="p">,</span> <span class="n">_ft_hash</span>

<span class="k">def</span> <span class="nf">compute_ngrams</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">min_n</span><span class="p">,</span> <span class="n">max_n</span><span class="p">):</span>
    <span class="n">BOW</span><span class="p">,</span> <span class="n">EOW</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;&lt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&gt;&#39;</span><span class="p">)</span>  <span class="c1"># Used by FastText to attach to all words as prefix and suffix</span>
    <span class="n">extended_word</span> <span class="o">=</span> <span class="n">BOW</span> <span class="o">+</span> <span class="n">word</span> <span class="o">+</span> <span class="n">EOW</span>
    <span class="n">ngrams</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">ngram_length</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_n</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">extended_word</span><span class="p">),</span> <span class="n">max_n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">extended_word</span><span class="p">)</span> <span class="o">-</span> <span class="n">ngram_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">ngrams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">extended_word</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">ngram_length</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ngrams</span>

    <span class="k">def</span> <span class="nf">word_vec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">use_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">FastTextKeyedVectors</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">word_vec</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">use_norm</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># from gensim.models.fasttext import compute_ngrams</span>
            <span class="n">word_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vectors_ngrams</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">ngrams</span> <span class="o">=</span> <span class="n">_compute_ngrams</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_n</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">use_norm</span><span class="p">:</span>
                <span class="n">ngram_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors_ngrams_norm</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ngram_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors_ngrams</span>
            <span class="n">ngrams_found</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">:</span>
                <span class="n">ngram_hash</span> <span class="o">=</span> <span class="n">_ft_hash</span><span class="p">(</span><span class="n">ngram</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket</span>
                <span class="k">if</span> <span class="n">ngram_hash</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hash2index</span><span class="p">:</span>
                    <span class="n">word_vec</span> <span class="o">+=</span> <span class="n">ngram_weights</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hash2index</span><span class="p">[</span><span class="n">ngram_hash</span><span class="p">]]</span>
                    <span class="n">ngrams_found</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">word_vec</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">word_vec</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ngrams_found</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># No ngrams of the word are present in self.ngrams</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s1">&#39;all ngrams for word </span><span class="si">%s</span><span class="s1"> absent from model&#39;</span> <span class="o">%</span> <span class="n">word</span><span class="p">)</span>
</code></pre></div><p>例如，通过滑动窗口的方式，设定最短ngram和最长ngram，可以得到ngram集合。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&gt;&gt;&gt; from gensim.models.utils_any2vec import *
&gt;&gt;&gt; ngrams = compute_ngrams(&#39;好嗨哦&#39;,min_n = 1,max_n =3)
&gt;&gt;&gt; ngrams
[&#39;&lt;&#39;, &#39;好&#39;, &#39;嗨&#39;, &#39;哦&#39;, &#39;&gt;&#39;, &#39;&lt;好&#39;, &#39;好嗨&#39;, &#39;嗨哦&#39;, &#39;哦&gt;&#39;, &#39;&lt;好嗨&#39;, &#39;好嗨哦&#39;, &#39;嗨哦&gt;&#39;]
</code></pre></div><p>不过，可以看到的是，ngram中引入了“&lt;”和“&gt;”用于标记头和尾，这对于语言模型来说十分生动。</p>
<p><br><br></p>
<h2 id="六开源词向量训练工具与预训文件">六、开源词向量训练工具与预训文件</h2>
<p>不必重复造轮子，当前已经陆续出现了一些代表性的预训练词向量工具和词向量资源，我们可以充分利用好。</p>
<h3 id="61-开源词向量训练工具">6.1 开源词向量训练工具</h3>
<ul>
<li>ngram2vec： <a href="https://github.com/zhezhaoa/ngram2vec/">https://github.com/zhezhaoa/ngram2vec/</a></li>
<li>word2vec： <a href="https://github.com/svn2github/word2vec">https://github.com/svn2github/word2vec</a></li>
<li>fasttext： <a href="https://github.com/facebookresearch/fastText">https://github.com/facebookresearch/fastText</a></li>
<li>glove：https://github.com/stanfordnlp/GloVe</li>
</ul>
<br>
<h3 id="62-开源预训练词向量文件">6.2 开源预训练词向量文件</h3>
<ul>
<li><a href="https://github.com/Embedding/Chinese-Word-Vectors">https://github.com/Embedding/Chinese-Word-Vectors</a></li>
<li><a href="https://github.com/liuhuanyong/Word2Vector">https://github.com/liuhuanyong/Word2Vector</a></li>
<li><a href="https://github.com/liuhuanyong/ChineseEmbedding">https://github.com/liuhuanyong/ChineseEmbedding</a></li>
</ul>
<p><br><br></p>
<h2 id="七本文总结">七、本文总结</h2>
<p>本文，主要围绕预训练词向量模型这一主题，对当前预训练词向量模型的常用方法、评估与应用方式、领域的迁移变体、当前开放的训练工具和词向量文件进行介绍，并以word2vec和fasttext为例，展示一个词向量训练的案例，做到理论与实践相结合。</p>
<p>关于预训练词向量相关的文章目前已经有很多，关于更为细致的解读，可以参考其他材料。预训练词向量是bert出现之前，NLP处理业务问题的标配，绝对称得上是一个里程碑的事件，并且开创了“万物皆可embdding”的时代。</p>
<p>实际上，词向量的发展也在一定程度上验证了当前nlp的进步。</p>
<p>由最开始的基于one-hot、tf-idf、textrank等的bag-of-words，到LSA（SVD）、pLSA、LDA的主题模型词向量，再到word2vec、fastText、glove为代表的固定表征，最后到当前elmo、GPT、bert为代表的基于词向量的动态表征，都说明了语义建模中的动态属性和文本语境的多样性。</p>
<p>不过，我们需要认识的是，在此类词向量中，虽然其本质仍然是语言模型，但是它的目标不是语言模型本身，而是词向量，其所作的一系列优化，其专注于词向量本身，因此做了许多优化来提高计算效率。</p>
<p>例如，与NNLM相比，word2vec将词向量直接sum，不再拼接，并舍弃隐层；考虑到sofmax归一化需要遍历整个词汇表，采用hierarchical softmax 和negative sampling进行优化，前者生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；后者对每一个样本中每一个词都进行负例采样。</p>
<p>最后，以当前一个新的观点来结尾：</p>
<p>现在的预训练语言模型是下一代知识图谱，那么预训练词向量是什么？垫底型相关词库？大家可以想想。</p>
<p><br><br></p>
<h2 id="参考文献">参考文献</h2>
<ol>
<li><a href="https://baijiahao.baidu.com/sid=1600509930259553151">https://baijiahao.baidu.com/sid=1600509930259553151</a></li>
<li><a href="https://mp.weixin.qq.com/s/u8WZqlsIcGCU5BqPH23S4w">https://mp.weixin.qq.com/s/u8WZqlsIcGCU5BqPH23S4w</a></li>
<li><a href="https://www.jianshu.com/p/546d12898378/">https://www.jianshu.com/p/546d12898378/</a></li>
<li><a href="https://www.jianshu.com/p/471d9bfbd72f">https://www.jianshu.com/p/471d9bfbd72f</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32965521">https://zhuanlan.zhihu.com/p/32965521</a></li>
<li><a href="https://mp.weixin.qq.com/s/DvbvYppeuMaPn84SRAINcQ">https://mp.weixin.qq.com/s/DvbvYppeuMaPn84SRAINcQ</a></li>
</ol>
<br> 
<br> 
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>27G数据集 | 使用Python对27G招股说明书进行文本分析</title>
      <link>https://textdata.cn/blog/2022-11-02-27g-python-27g-a-share-market-prospectus/</link>
      <pubDate>Wed, 02 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-02-27g-python-27g-a-share-market-prospectus/</guid>
      <description>使用Python对27G招股说明书进行文本分析</description>
      <content:encoded><![CDATA[<h2 id="一招股说明书">一、招股说明书</h2>
<p>从淘宝店花70元买的， 一共有27G， 百度网盘分享很不方便，且格式比较乱， 有txt、pdf等， 汇总整理至csv中。</p>
<p>使用csv格式，只要定义相应的指标函数，就可以很方便得到相应文本变量。</p>
<br>
<h2 id="二导入数据">二、导入数据</h2>
<p>截止2022.01.01，共有3630家公司，含公司名、股票代码、日期、标题、报告文本5个字段。</p>
<blockquote>
<p>链接：https://pan.baidu.com/s/1pLZHDy0oXwcTiCakFb-KiA
提取码：e68l</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;招股说明书.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 招股记录数</span>
<span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>4630
</code></pre>
<br>
<h2 id="三定义指标函数">三、定义指标函数</h2>
<p>这里准备两个比较简单的指标，设计函数，可以理解为设计数据分析流水线某环节的输入和输出。</p>
<ul>
<li>报告长度</li>
<li>情感得分</li>
<li>其他指标&hellip;</li>
</ul>
<p>例如</p>
<h3 id="31-报告长度函数">3.1 报告长度函数</h3>
<ul>
<li>输入: 字符串</li>
<li>运算: 计算字符长度</li>
<li>输出: 数字</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">length</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;你好啊&#39;</span>
<span class="n">length</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>3
</code></pre>
<br>
<h3 id="32-某类词个数">3.2 某类词个数</h3>
<ul>
<li>输入: 字符串</li>
<li>运算: 使用某种词典(成熟的或自己开发)，计算文本中正面词个数、负面词个数、总词数</li>
<li>输出: 数字</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#使用已有词典或自定义词典</span>
<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;独家&#39;</span><span class="p">,</span> <span class="s1">&#39;进步&#39;</span><span class="p">,</span> <span class="s1">&#39;发展&#39;</span><span class="p">,</span> <span class="s1">&#39;稳定&#39;</span><span class="p">,</span> <span class="s1">&#39;卓越&#39;</span><span class="p">,</span> <span class="s1">&#39;提高&#39;</span><span class="p">,</span> <span class="s1">&#39;成功&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;丑闻&#39;</span><span class="p">,</span> <span class="s1">&#39;挪用&#39;</span><span class="p">,</span> <span class="s1">&#39;错过&#39;</span><span class="p">,</span> <span class="s1">&#39;不利&#39;</span><span class="p">,</span> <span class="s1">&#39;牺牲&#39;</span><span class="p">,</span> <span class="s1">&#39;干扰&#39;</span><span class="p">,</span> <span class="s1">&#39;过度&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;公司在市场竞争中，主动发挥技术优势，取得了长足的发展。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>{'pos_num': 1,
 'neg_num': 0,
 'stopword_num': 7,
 'word_num': 16,
 'sentence_num': 1}
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;独家&#39;</span><span class="p">,</span> <span class="s1">&#39;进步&#39;</span><span class="p">,</span> <span class="s1">&#39;发展&#39;</span><span class="p">,</span> <span class="s1">&#39;稳定&#39;</span><span class="p">,</span> <span class="s1">&#39;卓越&#39;</span><span class="p">,</span> <span class="s1">&#39;提高&#39;</span><span class="p">,</span> <span class="s1">&#39;成功&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;丑闻&#39;</span><span class="p">,</span> <span class="s1">&#39;挪用&#39;</span><span class="p">,</span> <span class="s1">&#39;错过&#39;</span><span class="p">,</span> <span class="s1">&#39;不利&#39;</span><span class="p">,</span> <span class="s1">&#39;牺牲&#39;</span><span class="p">,</span> <span class="s1">&#39;干扰&#39;</span><span class="p">,</span> <span class="s1">&#39;过度&#39;</span><span class="p">]}</span>

<span class="k">def</span> <span class="nf">pos</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#使用已有词典或自定义词典</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
                       <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
                       <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;pos_num&#39;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">neg</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#使用已有词典或自定义词典</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
                       <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
                       <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;neg_num&#39;</span><span class="p">]</span>
    


<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;公司在市场竞争中，主动发挥技术优势，取得了长足的发展。&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pos</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">neg</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<pre><code>1
0
</code></pre>
<br>
<h2 id="四批量运算">四、批量运算</h2>
<p>选中dataframe中某一列，使用apply应用某种计算函数。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#确保text这列所有的数据均为字符串</span>
<span class="c1">#如果不是字符串，强制转化为字符串</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>

<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Len&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Pos&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Neg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">neg</span><span class="p">)</span>


<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Senti&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Pos&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Neg&#39;</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Pos&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;Neg&#39;</span><span class="p">])</span>
<span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<h2 id="五保存">五、保存</h2>
<p>最后保存为csv、或xlsx，具体根据自己需要进行选择。</p>
<ul>
<li>df.to_csv()</li>
<li>df.to_excel()</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#df.to_csv(&#39;result.csv&#39;, encoding=&#39;utf-8&#39;, index=False)</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_excel</span><span class="p">(</span><span class="s1">&#39;result.xlsx&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用textstat库计算文本可读性</title>
      <link>https://textdata.cn/blog/2022-10-22-textstats-readability/</link>
      <pubDate>Sat, 22 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-22-textstats-readability/</guid>
      <description>使用textstat库计算文本可读性</description>
      <content:encoded><![CDATA[


<p>textstat可以对文本进行可读性计算，支持英文、德语、西班牙、意大利、荷兰语等。目前不支持中文，如做中文文本分析，可以考虑用cntext包。</p>
<p><a href="https://github.com/textstat/textstat" class="uri">https://github.com/textstat/textstat</a></p>
<p><br></p>
<div id="任务" class="section level2">
<h2>任务</h2>
<p>今天在本文中，将学习三个知识点。</p>
<ol style="list-style-type: decimal">
<li>读取csv数据文件</li>
<li>选中csv中某列文本数据，依次使用apply方法，计算FOG、ARI、CLI等。</li>
<li>综合三个可读性指标，构造可读性mean值</li>
</ol>
<p><br></p>
</div>
<div id="安装" class="section level2">
<h2>安装</h2>
<pre><code>pip3 install textstat</code></pre>
<p><br></p>
</div>
<div id="读取数据" class="section level2">
<h2>读取数据</h2>
<p><a href="data.csv">点击下载实验数据data.csv</a></p>
<pre class="python"><code>import pandas as pd
import textstat
#设置dataframe显示的宽度
pd.options.display.max_colwidth = 50

df = pd.read_csv(&#39;data.csv&#39;)
df</code></pre>
<pre><code>##                                                  doc
## 0  Playing games has always been thought to be im...
## 1  the development of well-balanced and creative ...
## 2  however, what part, if any, they should play i...
## 3  of adults has never been researched that deepl...
## 4  that playing games is every bit as important f...
## 5  as for children. Not only is taking time out t...
## 6  with our children and other adults valuable to...
## 7  interpersonal relationships but is also a wond...
## 8                       to release built up tension.
## 9  The language will be used for syllable calcula...</code></pre>
<p><br></p>
</div>
<div id="series批操作" class="section level2">
<h2>Series批操作</h2>
<p>使用apply方法对pd.Series类型的数据进行批操作</p>
<p>extstat库有丰富的可读性方法，这里任选2个作为 <strong>批操作函数</strong>。</p>
<ul>
<li><strong>Fog</strong> textstat.gunning_fog(text)</li>
<li><strong>Flesch</strong> textstat.flesch_reading_ease(text)</li>
</ul>
<pre class="python"><code>df[&#39;Fog&#39;] = df[&#39;doc&#39;].apply(textstat.gunning_fog)
df[&#39;Flesch&#39;] = df[&#39;doc&#39;].apply(textstat.flesch_reading_ease)

df.head()</code></pre>
<pre><code>##                                                  doc   Fog  Flesch
## 0  Playing games has always been thought to be im...  4.00   78.25
## 1  the development of well-balanced and creative ...  8.51   30.53
## 2  however, what part, if any, they should play i...  4.40   94.15
## 3  of adults has never been researched that deepl...  4.00   86.71
## 4  that playing games is every bit as important f...  4.00   78.25</code></pre>
<p><br></p>
</div>
<div id="dataframe均值" class="section level2">
<h2>DataFrame均值</h2>
<p>选中Fog、Flesch两列</p>
<pre class="python"><code>#查看df[[&#39;Fog&#39;, &#39;Flesch&#39;]]数据类型
type(df[[&#39;Fog&#39;, &#39;Flesch&#39;]])</code></pre>
<pre><code>## &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;</code></pre>
<pre class="python"><code>#对这两个指标水平方向进行均值
df[&#39;Mean&#39;] = df[[&#39;Fog&#39;, &#39;Flesch&#39;]].mean(axis=1)
df.head()</code></pre>
<pre><code>##                                                  doc   Fog  Flesch    Mean
## 0  Playing games has always been thought to be im...  4.00   78.25  41.125
## 1  the development of well-balanced and creative ...  8.51   30.53  19.520
## 2  however, what part, if any, they should play i...  4.40   94.15  49.275
## 3  of adults has never been researched that deepl...  4.00   86.71  45.355
## 4  that playing games is every bit as important f...  4.00   78.25  41.125</code></pre>
<p><br></p>
</div>
<div id="存储" class="section level2">
<h2>存储</h2>
<p>存储到可读性.csv中</p>
<pre class="python"><code>df.to_csv(&#39;可读性.csv&#39;, index=False)</code></pre>
<p><br></p>
</div>
<div id="广而告之" class="section level2">
<h2>广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>1.5G数据集 | 200万条Indiegogo众筹项目信息</title>
      <link>https://textdata.cn/blog/2022-12-08-indiegogo-dataset/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-12-08-indiegogo-dataset/</guid>
      <description>1.57G indiegogo-dataset.jpeg</description>
      <content:encoded><![CDATA[<h2 id="indiegogo">Indiegogo</h2>
<p>Indiegogo成立于2008年，全球最大的科创新品首发和众筹平台， 是美国最早的众筹平台之一。</p>
<p><br><br></p>
<h2 id="参考论文">参考论文</h2>
<p>该数据集研究价值，可用于研究市场营销、创新创业、信息管理等， 部分使用众筹数据集作为研究对象的论文。</p>
<blockquote>
<p>[1]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.<em>管理世界</em>.2016;5:81-98.
[2]Dai, Hengchen and Dennis J. Zhang. “Prosocial Goal Pursuit in Crowdfunding: Evidence from Kickstarter.” Journal of Marketing Research 56 (2019): 498 - 517.
[3]Gafni, H., Marom, D.M., Robb, A.M., &amp; Sade, O. (2020). Gender Dynamics in Crowdfunding (Kickstarter): Evidence on Entrepreneurs, Backers, and Taste-Based Discrimination*. Review of Finance.
[4]Jensen, Lasse Skovgaard and Ali Gürcan Özkil. “Identifying challenges in crowdfunded product development: a review of Kickstarter projects.” Design Science 4 (2018): n. pag.</p>
</blockquote>
<p><br><br></p>
<h2 id="indiegogo数据">Indiegogo数据</h2>
<p>2016年4月写好的Indiegogo爬虫，每月执行一次, 最新的数据 可以前往https://webrobots.io/indiegogo-dataset/</p>
<p><img loading="lazy" src="img/web_robot.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="原始数据">‘原始’数据</h2>
<p>Web Robot网上公开的的Indiegogo原始数据几十个 csv文件,</p>
<p><img loading="lazy" src="img/zips.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="整理">整理</h2>
<p>将上图的zip全部合并为一个 Indiegogo_dataset.csv , 该文件 1.57G 。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="n">dff</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Indiegogo_Dataset/Indiegogo_dataset.csv&#39;</span><span class="p">,</span> <span class="n">on_bad_lines</span><span class="o">=</span><span class="s1">&#39;skip&#39;</span><span class="p">)</span>
<span class="n">dff</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<p>数据集的字段有</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Index([&#39;bullet_point&#39;, 
       &#39;category&#39;, &#39;category_url&#39;,  #项目类目及url
       &#39;clickthrough_url&#39;, #进入当前项目经由的某url
       &#39;close_date&#39;,  #项目截止日期
       &#39;currency&#39;,  #货币
       &#39;funds_raised_amount&#39;,  #当前已筹集的资金
       &#39;funds_raised_percent&#39;, #筹集资金进度(当前筹资/项目目标金额)
       &#39;image_url&#39;,  #图片url
       &#39;is_indemand&#39;, 
       &#39;is_pre_launch&#39;, #是否为预演
       &#39;offered_by&#39;,  #项目发起人
       &#39;open_date&#39;, #项目开始日期
       &#39;perk_goal_percentage&#39;, &#39;perks_claimed&#39;, 
       &#39;price_offered&#39;, #众筹价
       &#39;price_retail&#39;, #零售价
       &#39;product_stage&#39;,  #产品阶段
       &#39;project_id&#39;, #项目id
       &#39;project_type&#39;, #项目类型
       &#39;source_url&#39;, #项目url
       &#39;tagline&#39;, &#39;tags&#39;, #标签
       &#39;title&#39; ], #项目标题
      dtype=&#39;object&#39;)
</code></pre></div><p><br><br></p>
<h2 id="数据获取">数据获取</h2>
<ul>
<li>原始数据
<ul>
<li><a href="https://webrobots.io/indiegogo-dataset/">https://webrobots.io/indiegogo-dataset/</a></li>
</ul>
</li>
<li>整理的1.57G csv,
<ul>
<li>链接: <a href="https://pan.baidu.com/s/1j3PtV4GbFsyhjmr0NLbnKg">https://pan.baidu.com/s/1j3PtV4GbFsyhjmr0NLbnKg</a> 提取码: vfyc</li>
</ul>
</li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>14G数据集 | 2007-2021年A股上市公司年度报告（txt文件）</title>
      <link>https://textdata.cn/blog/2022-10-21-2007-2021-a-share-reports-dataset/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-21-2007-2021-a-share-reports-dataset/</guid>
      <description>2007-2021年A股上市公司年度报告（txt文件）</description>
      <content:encoded><![CDATA[<p>2007-2021年A股上市公司年度报告, 整理不易，请转发分享。</p>
<br>
<h2 id="截图">截图</h2>
<p><img loading="lazy" src="img/07-21.png" alt=""  />

<img loading="lazy" src="img/2007.png" alt=""  />
</p>
<br>
<h2 id="获取">获取</h2>
<blockquote>
<p>链接: <a href="https://pan.baidu.com/s/1jw6VGGAN9cxROoqWN2X4vw">https://pan.baidu.com/s/1jw6VGGAN9cxROoqWN2X4vw</a> 提取码: g3cn</p>
</blockquote>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Python | 词移距离(Word Mover&#39;s Distance)</title>
      <link>https://textdata.cn/blog/2022-10-16-python-word-mover-s-distance/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-16-python-word-mover-s-distance/</guid>
      <description>词移距离可以为我们提供短文相似度计算，距离越小，两文档相似度越高。</description>
      <content:encoded><![CDATA[


<p>词嵌入方法（word2vec、glove等）可以将每个词的语义映射到n维空间，在n维空间中，词语间距离远近可以表征语义的远近。Kusner等人(2015)提出<strong>词移距离</strong>（word mover’s distance， 后文用WMD缩写代替）借助词语向量语义距离，实现两文档间的相似度计算，距离越小，相似度越高。在会计领域中的应用可以用来度量问答场景的答非所问的程度。</p>
<p><br></p>
<div id="wmd基础" class="section level2">
<h2>WMD基础</h2>
<p>有两个文档</p>
<pre><code>doc1 = &quot;Obama speaks to the media in Illinois&quot;
doc2 = &quot;The President greets the press in Chicago.&quot;</code></pre>
<p>词向量一般是高(n)维空间，这里把n压缩到2维空间，使用matplotlib绘图。两个文档中重要的词语彼此之间存在语义相似度，</p>
<pre class="python"><code># Image from https://vene.ro/images/wmd-obama.png
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
img = mpimg.imread(&#39;img/wmd-obama.png&#39;)
imgplot = plt.imshow(img)
plt.axis(&#39;off&#39;)</code></pre>
<pre><code>## (-0.5, 2397.5, 1327.5, -0.5)</code></pre>
<pre class="python"><code>plt.show()</code></pre>
<p><img src="/blog/2022-10-16-python-word-mover-s-distance/index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p><img src="img/wmd-obama.png" /></p>
<p><br></p>
</div>
<div id="计算wmd步骤" class="section level2">
<h2>计算WMD步骤</h2>
<ol style="list-style-type: decimal">
<li>剔除文档中停止词，如the、a等无信息量词</li>
<li>导入预训练好的词嵌入(word2vec)模型(网上资源比较多，如果数据量很大，也可以自己使用gensim训练自己的词向量)</li>
<li>计算WMD</li>
</ol>
<pre class="python"><code>from nltk.corpus import stopwords
from nltk import download
download(&#39;stopwords&#39;)  # Download stopwords list.
stop_words = stopwords.words(&#39;english&#39;)

def preprocess(sentence):
    return [w for w in sentence.lower().split() if w not in stop_words]


doc1 = &quot;Obama speaks to the media in Illinois&quot;
doc2 = &quot;The President greets the press in Chicago.&quot;

doc1 = preprocess(doc1)
doc2 = preprocess(doc2)

print(doc1)
print(doc2)</code></pre>
<p>Run</p>
<pre><code>[&#39;obama&#39;, &#39;speaks&#39;, &#39;media&#39;, &#39;illinois&#39;]
[&#39;president&#39;, &#39;greets&#39;, &#39;press&#39;, &#39;chicago.&#39;]</code></pre>
<p><br></p>
<blockquote>
<p>如果运行代码出现nltk问题，可以观看视频 <a href="https://www.bilibili.com/video/BV14A411i7DB" class="uri">https://www.bilibili.com/video/BV14A411i7DB</a></p>
</blockquote>
<p>下载谷歌新闻预训练模型(word2vec-google-news-300) ,这里可以使用我提供的百度网盘</p>
<blockquote>
<p>链接：<a href="https://pan.baidu.com/s/1yzGLcMsZl3u1zigTHLdc2Q" class="uri">https://pan.baidu.com/s/1yzGLcMsZl3u1zigTHLdc2Q</a>
提取码：l63f</p>
</blockquote>
<p>这里需要</p>
<pre class="python"><code>from gensim.models import KeyedVectors

w2v_model = KeyedVectors.load(&#39;GoogleNews-vectors-negative300.bin.gz&#39;)
wmd = w2v_model.wmdistance(doc1, doc2)
print(&#39;distance :{wmd}&#39;.format(wmd=wmd))</code></pre>
<p>Run</p>
<pre><code>distance :0.8867237050133944</code></pre>
<p><br></p>
</div>
<div id="参考文献" class="section level2">
<h2>参考文献</h2>
<ul>
<li>Kusner, Matt J., Yu Sun, Nicholas I. Kolkin and Kilian Q. Weinberger. “From Word Embeddings To Document Distances.” ICML (2015).</li>
<li><a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html" class="uri">https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html</a></li>
</ul>
<p><br></p>
</div>
<div id="广而告之" class="section level2">
<h2>广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 | 多语言对齐词向量预训练模型</title>
      <link>https://textdata.cn/blog/2022-10-16-aligned-word-vectors/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-16-aligned-word-vectors/</guid>
      <description>借助该预训练模型，应该能做可做跨文化对比分析</description>
      <content:encoded><![CDATA[<h2 id="介绍">介绍</h2>
<p>Facebook研究者使用 fastText 算法，对维基百科(44种语言)语料数据进行了训练，最终生成了 44 种语言的对齐词向量。</p>
<br>
<h2 id="用途">用途</h2>
<p>wiki数据集有个优点，即由于众人分享、翻译，将不同语言的百科词条进行了翻译整理。所以facebook使用wiki训练对齐词向量有助于提升翻译准确性。与此同时，因为翻译者处于不同的语言和文化背景下，词条及词条内容必然蕴含着语言所特有的文化信息线索，有可能有助于我们挖掘跨语言的文化差异。例如中文词条<code>护士</code>和 英文词条<code>nurse</code> ，可以借助对齐词向量，比较护士这个群体在性别、种族等语义上的差异。</p>
<p>之前分享过的内容</p>
<ul>
<li><a href="https://textdata.cn/blog/embeddingsandattitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展&mdash;&mdash;基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
<li><a href="https://textdata.cn/blog/literatureembeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
</ul>
<p>不过fastText算法认为词语有不同的大小划分层次，从大到小分别是词语、词缀、字符等，使用 Joulin 等人 (2018) 中描述的 RCSLS 方法进行比对。</p>
<table>
<thead>
<tr>
<th><strong>Code</strong></th>
<th><strong>en-es</strong></th>
<th><strong>es-en</strong></th>
<th><strong>en-fr</strong></th>
<th><strong>fr-en</strong></th>
<th><strong>en-de</strong></th>
<th><strong>de-en</strong></th>
<th><strong>en-ru</strong></th>
<th><strong>ru-en</strong></th>
<th><strong>en-zh</strong></th>
<th><strong>zh-en</strong></th>
<th><strong>avg</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Joulin et al. [<a href="https://arxiv.org/abs/1804.07745">1</a>]</td>
<td>84.1</td>
<td>86.3</td>
<td>83.3</td>
<td>84.1</td>
<td><strong>79.1</strong></td>
<td>76.3</td>
<td><strong>57.9</strong></td>
<td><strong>67.2</strong></td>
<td>45.9</td>
<td>46.4</td>
<td>71.1</td>
</tr>
<tr>
<td>This implementation (10 epochs)</td>
<td>84.2</td>
<td><strong>86.6</strong></td>
<td><strong>83.9</strong></td>
<td>84.7</td>
<td>78.3</td>
<td>76.6</td>
<td>57.6</td>
<td>66.7</td>
<td><strong>47.6</strong></td>
<td><strong>47.4</strong></td>
<td>71.4</td>
</tr>
<tr>
<td>This implementation (unsup. model selection)</td>
<td><strong>84.3</strong></td>
<td><strong>86.6</strong></td>
<td><strong>83.9</strong></td>
<td><strong>85.0</strong></td>
<td>78.7</td>
<td><strong>76.7</strong></td>
<td>57.6</td>
<td>67.1</td>
<td><strong>47.6</strong></td>
<td><strong>47.4</strong></td>
<td><strong>71.5</strong></td>
</tr>
</tbody>
</table>
<p>算法得出的词向量在西方，尤其是西欧语言之间进行语义对齐，效果可能更好。而中文、日语等汉字语言，是由偏旁部首组成，与西方字母语言还是存在一定差异。上表也可以看出中英语义对齐准确率47%， 而其他语言之间对齐准确率平均为71%。</p>
<br>
<h2 id="模型资源">模型资源</h2>
<p><a href="https://fasttext.cc/docs/en/aligned-vectors.html">https://fasttext.cc/docs/en/aligned-vectors.html</a></p>
<p>对齐预训练向量模型下载链接</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Afrikaans: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.af.align.vec"><em>text</em></a></td>
<td>Arabic: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ar.align.vec"><em>text</em></a></td>
<td>Bulgarian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bg.align.vec"><em>text</em></a></td>
<td>Bengali: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bn.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Bosnian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bs.align.vec"><em>text</em></a></td>
<td>Catalan: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ca.align.vec"><em>text</em></a></td>
<td>Czech: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.cs.align.vec"><em>text</em></a></td>
<td>Danish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.da.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>German: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec"><em>text</em></a></td>
<td>Greek: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.el.align.vec"><em>text</em></a></td>
<td>English: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.en.align.vec"><em>text</em></a></td>
<td>Spanish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.es.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Estonian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.et.align.vec"><em>text</em></a></td>
<td>Persian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fa.align.vec"><em>text</em></a></td>
<td>Finnish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fi.align.vec"><em>text</em></a></td>
<td>French: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fr.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Hebrew: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.he.align.vec"><em>text</em></a></td>
<td>Hindi: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hi.align.vec"><em>text</em></a></td>
<td>Croatian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hr.align.vec"><em>text</em></a></td>
<td>Hungarian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hu.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Indonesian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.id.align.vec"><em>text</em></a></td>
<td>Italian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.it.align.vec"><em>text</em></a></td>
<td>Korean: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ko.align.vec"><em>text</em></a></td>
<td>Lithuanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.lt.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Latvian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.lv.align.vec"><em>text</em></a></td>
<td>Macedonian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.mk.align.vec"><em>text</em></a></td>
<td>Malay: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ms.align.vec"><em>text</em></a></td>
<td>Dutch: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.nl.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Norwegian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.no.align.vec"><em>text</em></a></td>
<td>Polish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.pl.align.vec"><em>text</em></a></td>
<td>Portuguese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.pt.align.vec"><em>text</em></a></td>
<td>Romanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ro.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Russian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ru.align.vec"><em>text</em></a></td>
<td>Slovak: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sk.align.vec"><em>text</em></a></td>
<td>Slovenian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sl.align.vec"><em>text</em></a></td>
<td>Albanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sq.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Swedish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sv.align.vec"><em>text</em></a></td>
<td>Tamil: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ta.align.vec"><em>text</em></a></td>
<td>Thai: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.th.align.vec"><em>text</em></a></td>
<td>Tagalog: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.tl.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Turkish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.tr.align.vec"><em>text</em></a></td>
<td>Ukrainian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.uk.align.vec"><em>text</em></a></td>
<td>Vietnamese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.vi.align.vec"><em>text</em></a></td>
<td>Chinese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.zh.align.vec"><em>text</em></a></td>
</tr>
</tbody>
</table>
<br>
<h2 id="格式">格式</h2>
<p>词向量默认使用的fastText格式</p>
<ul>
<li>第一行给了词向量的维数</li>
<li>从第二行开始，每一行由词语及对应的词向量组成。</li>
<li>数值之间使用空格间隔</li>
</ul>
<br>
<h2 id="代码">代码</h2>
<h3 id="导入模型">导入模型</h3>
<p>使用gensim导入fastText方法训练出的 预训练语言模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1">#导入刚刚下载的预训练模型</span>
<span class="c1">#该词向量模型300维</span>
<span class="n">zh_w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;wiki.zh.align.vec&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1">#英文词向量模型5G，太大了。如果内存小于16G不要使用下面命令</span>
<span class="c1">#en_w2v_model = KeyedVectors.load_word2vec_format(&#39;wiki.en.align.vec&#39;, binary=False)</span>
</code></pre></div><p>一旦导入成功，就可以进行向量计算。这里仅进行简单演示</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取某词的词向量</span>
<span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>array([ 0.0733,  0.0782,  0.0188, -0.0027, -0.0052,...,  0.0586,  0.0166,
       -0.1401, -0.0545, -0.0125,  0.0373, -0.0681,  0.063 ],
      dtype=float32)
</code></pre>
<br>
<p>在中文中， 护士职业的主要从业者为女性，反应在词向量相似度上，如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">,</span> <span class="s1">&#39;女性&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">,</span> <span class="s1">&#39;男性&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<pre><code>0.4417011
0.378651
</code></pre>
<br>
<p>更多w2v_model用法可参考 <a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="文献">文献</h2>
<p>如果使用了facebook的预训练词向量，请引用以下两篇文献。</p>
<ul>
<li>Joulin, Armand, Piotr Bojanowski, Tomas Mikolov, Hervé Jégou, and Edouard Grave. &ldquo;Loss in translation: Learning bilingual word mapping with a retrieval criterion.&rdquo; arXiv preprint arXiv:1804.07745 (2018).</li>
<li>Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. &ldquo;Enriching word vectors with subword information.&rdquo; Transactions of the association for computational linguistics 5 (2017): 135-146.</li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>R语言 | 使用word2vec词向量模型</title>
      <link>https://textdata.cn/blog/2022-10-12-r-word2vec/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-12-r-word2vec/</guid>
      <description>R语言训练和使用词向量word2vec模型</description>
      <content:encoded><![CDATA[


<p>Python的gensim库可以训练和使用word2vec模型，R语言中也有与之对应的<code>word2vec包</code>。word2vec是词嵌入技术中最常用的一种技术，如果对词嵌入不太了解，可以阅读前文</p>
<ul>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
</ul>
<p>本文需要的R包</p>
<pre><code>install.packages(c(&quot;word2vec&quot;, &quot;jiebaR&quot;, &quot;tidyverse&quot;, &quot;readtext&quot;))</code></pre>
<p><br></p>
<div id="word2vec包常用函数" class="section level2">
<h2>word2vec包常用函数</h2>
<ul>
<li>word2vec 使用文本数据训练word2vec模型</li>
<li>as.matrix 获取词向量</li>
<li>doc2vec 获取文档向量</li>
<li>predict 获取</li>
<li>write.word2vec 保存word2vec模型至文件</li>
<li>read.word2vec 读取word2vec模型文件</li>
</ul>
<p><br></p>
</div>
<div id="准备数据" class="section level2">
<h2>准备数据</h2>
<p>原始数据是从网站下载的 <code>三体.txt</code>, 未分词处理，现在需要</p>
<ol style="list-style-type: decimal">
<li>读中文取txt数据</li>
<li>保留标点符号，进行分词处理</li>
<li>分词结果重新整理为类似英文(空格间隔词语的形式)字符串</li>
<li>结果存入新的txt</li>
</ol>
<pre class="r"><code>library(jiebaR)
library(tidyverse)
library(word2vec)


#导入数据
tri_body &lt;- readtext::readtext(&#39;data/三体.txt&#39;)$text 

#分词（保留标点符号）
tokenizer &lt;- worker(symbol=T)
tri_words &lt;- segment(tri_body, tokenizer)

# 整理为英文格式（词语之间加空格）
segmented_text &lt;- stringr::str_c(tri_words, collapse = &quot; &quot;) %&gt;% c()

#写入txt
readr::write_file(segmented_text, file=&#39;data/santi.txt&#39;)</code></pre>
<p><br></p>
</div>
<div id="训练word2vec模型" class="section level2">
<h2>训练word2vec模型</h2>
<pre><code>word2vec(
  x,
  type = c(&quot;cbow&quot;, &quot;skip-gram&quot;),
  dim = 50,
  window = ifelse(type == &quot;cbow&quot;, 5L, 10L),
  iter = 5L,
  lr = 0.05,
  min_count = 5L,
  split = c(&quot; \n,.-!?:;/\&quot;#$%&amp;&#39;()*+&lt;=&gt;@[]\\^_`{|}~\t\v\f\r&quot;, &quot;.\n?!&quot;),
  stopwords = character(),
  threads = 1L,
  ...
)</code></pre>
<ul>
<li>x 英文文本数据txt文件(中文数据txt文件是分词后的txt文件，空格间隔词语)</li>
<li>type 训练方式，默认CBOW</li>
<li>dim 词向量维度，默认50维</li>
<li>window 词向量窗口，默认5</li>
<li>iter 训练迭代次数，默认5</li>
<li>split 分词、分句对应的分隔符。</li>
<li>lr 学习率，默认0.05</li>
<li>min_count 词语在语料中至少要出现5次(低于5次的词语，训练好的结果中没有该词语）</li>
<li>stopwords 停用词表，默认空字符集</li>
<li>threads 并行加速，cpu核数，默认1。为了加速训练过程，可以使用 <code>parallel::detectCores()</code> 获得本电脑的核数</li>
</ul>
<pre class="r"><code>#训练10维的词向量模型
model &lt;- word2vec(x = &#39;data/santi.txt&#39;, 
                  dim = 10,  
                  iter = 20, 
                  split = c(&quot; &quot;,  &quot;。？！；&quot;),
                  threads = parallel::detectCores()) #并行，使用cpu多核加速

emb &lt;- as.matrix(model)

#显示6个词
head(emb)</code></pre>
<pre><code>##             [,1]       [,2]        [,3]        [,4]      [,5]        [,6]
## 煮   -1.02566934 -0.9271542 -0.42417252 -0.54280633 1.8847700  0.41640753
## 报   -0.83992052  1.9440031  0.09093992  0.83522910 1.7909089  0.72149992
## 悬空 -0.06369513 -1.3519955 -2.13137460 -0.06198586 0.6096401  1.32933748
## 略    1.74687469 -0.4278547 -0.33822438  1.08505321 2.0168977 -0.07693915
## 伏   -0.68947995 -1.4147453 -1.95522511 -0.39963767 0.5269030  0.30352208
## 石柱 -0.40561640 -1.3643234  0.30329546 -0.94012892 2.1579018  0.79654717
##            [,7]       [,8]       [,9]      [,10]
## 煮   -1.1708908 -0.7624418 -0.6275516  1.2417521
## 报    0.5235919  0.8448864 -0.2960095 -0.0773837
## 悬空  0.1527163 -0.1337370 -0.1646384  1.1892601
## 略   -0.3246748 -0.9813624  0.5045205  0.2771466
## 伏    0.3166684 -1.4238008 -1.0167172 -0.0976937
## 石柱  0.2237919  0.6933151  0.7412233 -0.7918702</code></pre>
<p><br></p>
</div>
<div id="查看某词的vector" class="section level2">
<h2>查看某词的vector</h2>
<p>查看词语 <code>汪淼</code> 的vector</p>
<pre class="r"><code>emb[&quot;汪淼&quot;,]</code></pre>
<pre><code>##  [1] -0.77559733 -0.90021265  0.66555792 -0.10277803  1.89924443 -0.88817298
##  [7] -1.32665634 -0.75938725 -0.09628224  1.18008399</code></pre>
<p>查看词语 <code>地球</code> 的vector</p>
<pre class="r"><code>emb[&quot;地球&quot;,]</code></pre>
<pre><code>##  [1]  0.29645494 -0.61688840  0.91209215 -0.64530188  0.62816381 -0.72807491
##  [7]  0.50655973  2.38137436  1.19238114 -0.09610342</code></pre>
<p><br></p>
</div>
<div id="predict" class="section level2">
<h2>predict()</h2>
<p>找到语料中，词语 <code>罗辑</code> 最相似的 20个词</p>
<pre class="r"><code>predict(model, &#39;罗辑&#39;, type=&#39;nearest&#39;, top_n = 20)</code></pre>
<pre><code>## $罗辑
##    term1    term2 similarity rank
## 1   罗辑     胡文  0.9744400    1
## 2   罗辑   申玉菲  0.9678891    2
## 3   罗辑   瓦季姆  0.9550550    3
## 4   罗辑 狄奥伦娜  0.9518393    4
## 5   罗辑     蓝西  0.9472395    5
## 6   罗辑     护士  0.9471439    6
## 7   罗辑   法扎兰  0.9458703    7
## 8   罗辑   白艾思  0.9451101    8
## 9   罗辑     坎特  0.9396626    9
## 10  罗辑     白蓉  0.9387447   10
## 11  罗辑   参谋长  0.9377206   11
## 12  罗辑   弗雷斯  0.9369408   12
## 13  罗辑   第一眼  0.9357565   13
## 14  罗辑     父亲  0.9350463   14
## 15  罗辑   多少次  0.9314436   15
## 16  罗辑     门去  0.9291503   16
## 17  罗辑     维德  0.9267251   17
## 18  罗辑     褐蚁  0.9203902   18
## 19  罗辑       刚  0.9200501   19
## 20  罗辑     吴岳  0.9191605   20</code></pre>
<p>查看均值向量（多个词向量中心的）的10个近义词</p>
<pre class="r"><code>vectors &lt;- emb[c(&quot;汪淼&quot;, &quot;罗辑&quot;, &quot;叶文洁&quot;), ]
centroid_vector &lt;- colMeans(vectors)

predict(model, centroid_vector, type = &quot;nearest&quot;, top_n = 10)</code></pre>
<pre><code>##        term similarity rank
## 1      罗辑  0.9185568    1
## 2  狄奥伦娜  0.9104245    2
## 3      文洁  0.9088279    3
## 4      汪淼  0.9054156    4
## 5    白艾思  0.9046930    5
## 6      张翔  0.9026827    6
## 7      尴尬  0.8952187    7
## 8      庄颜  0.8952166    8
## 9      皇帝  0.8949283    9
## 10     父亲  0.8915347   10</code></pre>
<p><br></p>
</div>
<div id="doc2vec" class="section level2">
<h2>doc2vec()</h2>
<ul>
<li>doc2vec(object, newdata, split = ” “)
<ul>
<li>object word2vec模型对象</li>
<li>newdata 文档列表(用空格间隔的字符串列表)</li>
<li>split 默认分隔符是空格</li>
</ul></li>
</ul>
<p>将文档转为向量</p>
<pre class="r"><code>docs &lt;- c(&quot;哦 ， 对不起 ， 汪 教授 。 这是 我们 史强 队长 。&quot;, 
          &quot; 丁仪 博士 ， 您 能否 把 杨冬 的 遗书 给 汪 教授 看 一下 ？ &quot;)

doc2vec(object=model, newdata = docs, split=&#39; &#39;)</code></pre>
<pre><code>##            [,1]       [,2]       [,3]     [,4]      [,5]       [,6]       [,7]
## [1,] -1.1769752 -0.1065619  0.1983950 1.734068 0.5478012 -0.8320528 -0.2387014
## [2,] -0.4827189  0.0664595 -0.2119484 1.895074 0.6729840 -0.3008853 -0.6857539
##            [,8]      [,9]      [,10]
## [1,] -0.5519856 -2.007002  0.4182127
## [2,] -0.5976922 -2.130454 -0.4653725</code></pre>
<p><br></p>
</div>
<div id="保存word2vec模型" class="section level2">
<h2>保存word2vec模型</h2>
<p>保存模型，一般有两个目的</p>
<ul>
<li>为了分享word2vec模型</li>
<li>避免反复训练模型，节约数据分析时间</li>
</ul>
<pre class="r"><code>word2vec::write.word2vec(x = model, 
                         #新建output文件夹，将模型存入output文件夹内
                         file = &quot;output/santi_word2vec.bin&quot;)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p><br></p>
</div>
<div id="导入预训练模型" class="section level2">
<h2>导入预训练模型</h2>
<p>导入 <code>output/santi_word2vec.bin</code> 的预训练word2vec模型</p>
<pre class="r"><code>pre_trained_model &lt;- word2vec::read.word2vec(file = &quot;output/santi_word2vec.bin&quot;)
pre_trained_emb &lt;- as.matrix(pre_trained_model)
head(pre_trained_emb)</code></pre>
<pre><code>##              [,1]       [,2]       [,3]       [,4]       [,5]        [,6]
## 回荡   -1.9563367 -0.3099073 -1.2969902 -0.5719763  1.1507142 -0.05515177
## 听证会  0.2756990  1.3702289 -1.3303705 -0.1827691  0.6622804 -1.92008448
## 纲领    0.4495552  1.9311246 -0.5812275 -0.1470096 -0.2678985 -0.01694358
## 很亮    0.3621844 -1.0048453  0.7036168 -2.0917876  0.6459805  1.18436253
## 秒      1.9033701  1.6510324 -0.2616904  0.3671210  1.0618066  0.06588747
## 杰森   -1.2904713 -1.2501229  0.3380587  0.8590797  1.6798494 -0.58775252
##              [,7]       [,8]       [,9]      [,10]
## 回荡    1.1082711 -0.2064489 -0.9264346 -0.7816723
## 听证会 -1.0952694  0.6120903 -0.1326561  0.7252344
## 纲领   -0.6097277  2.1051276 -0.2405726 -0.8808851
## 很亮    0.1964065 -1.3926132 -0.4042619 -0.1645472
## 秒     -0.8347995  0.2591044  0.3594093  1.1929117
## 杰森    0.4941484 -1.1393189 -0.4687541  0.9951217</code></pre>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>nlp-roadmap | 文本分析知识点思维脑图</title>
      <link>https://textdata.cn/blog/2022-10-08-nlp-roadmap/</link>
      <pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-08-nlp-roadmap/</guid>
      <description>文本分析(社科领域中与Python相关的)，对应于计算机领域是自然语言处理，两者范畴高度重叠，关系密切。但相关术语脉络比较庞杂，nlp-roadmap项目为我们梳理了相关概念，更有助于快速掌握文本分析。</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="img/title.png" alt=""  />
</p>
<p align="center"><img width="333" src="img/main.png" /></p>
<p>文本分析(社科领域中与Python相关的)，对应于计算机领域是自然语言处理，两者范畴高度重叠，关系密切。但相关术语脉络比较庞杂，nlp-roadmap项目为我们梳理了相关概念，更有助于快速掌握文本分析。</p>
<br>
<h2 id="相关知识">相关知识</h2>
<p>NLP最相关的几个知识大类有统计学、机器学习、文本挖掘、自然语言处理</p>
<br>
<h2 id="统计学">统计学</h2>
<p><img loading="lazy" src="img/prob.png" alt=""  />
</p>
<h2 id="机器学习">机器学习</h2>
<p><img loading="lazy" src="img/ml.png" alt=""  />
</p>
<h2 id="文本挖掘">文本挖掘</h2>
<p><img loading="lazy" src="img/textmining.png" alt=""  />
</p>
<h2 id="自然语言处理nlp">自然语言处理NLP</h2>
<p><img loading="lazy" src="img/nlp.png" alt=""  />
</p>
<br>
<h2 id="参考">参考</h2>
<p>[1] <a href="https://ratsgo.github.io/">ratsgo&rsquo;s blog for textmining</a>, <a href="https://github.com/ratsgo">ratsgo</a>/<a href="https://github.com/ratsgo/ratsgo.github.io">ratsgo.github.io</a></p>
<p>[2] (한국어) 텍스트 마이닝을 위한 공부거리들, <a href="https://github.com/lovit">lovit</a>/<a href="https://github.com/lovit/textmining-tutorial">textmining-tutorial</a></p>
<p>[3] <em>Christopher Bishop(2006). Pattern Recognition and Machine Learning</em></p>
<p>[4] <em>Young, T., Hazarika, D., Poria, S., &amp; Cambria, E. (2017). Recent Trends in Deep Learning Based Natural Language Processing. arXiv preprint arXiv:1708.02709.</em></p>
<p>[5] curated collection of papers for the nlp practitioner, <a href="https://github.com/mihail911">mihail911</a>/<a href="https://github.com/mihail911/nlp-library">nlp-library</a></p>
<p><strong>Acknowledgement</strong> to <a href="https://github.com/ratsgo">ratsgo</a>, <a href="https://github.com/lovit">lovit</a> for creating great posts and lectures.</p>
<br>
<h2 id="作者">作者</h2>
<ul>
<li>Tae Hwan Jung <a href="https://github.com/graykode">@graykode</a>, Kyung Hee Univ CE(Undergraduate).</li>
<li>Author Email : <a href="mailto:nlkey2022@gmail.com">nlkey2022@gmail.com</a></li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 | 企业社会责任报告数据集</title>
      <link>https://textdata.cn/blog/coporate_social_responsibility_datasets/</link>
      <pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/coporate_social_responsibility_datasets/</guid>
      <description>近年来，企业社会责任（csr)已成为全球学术界研究的热点。国内外各大顶刊都先后刊登了多篇关于csr的文章，比如《企业绿色创新实践如何破解“和谐共生”难题？》（发表于管理世界）、《负责任的国际投资：ESG与中国OFDI》（发表于经济研究）、《Is my company really doing good? Factors influencing employees&amp;#39; evaluation of the authenticity of their company&amp;#39;s corporate social responsibility engagement》（发表于JBR）等。这些文章核心变量的构建大都基于对企业社会责任报告的内容分析和挖掘。比如《企业绿色创新实践如何破解“和谐共生”难题？》的被解释变量（绿色创新）以及部分解释变量（二元合法性和伦理型领导）。可见，社会责任报告对于我们研究esg至关重要。因此，接下来小编就带大家爬取深交所上市公司历年的社会责任报告，希望能够给大家带来一些帮助。In recent years, corporate social responsibility (csr) has become a research hotspot in the global academic circles. Major top journals at home and abroad have successively published many articles on CSR, such as How can corporate green innovation practice solve the problem of harmonious symbiosis? (published in Governance World), Responsible International Investment: ESG and China&amp;#39;s OFDI (published in Economic Research), Is my company really doing good? Factors influencing employees&amp;#39; evaluation of the authenticity of their company&amp;#39;s corporate social responsibility engagement (published in JBR) et al. The construction of core variables in these articles is mostly based on the content analysis and mining of corporate social responsibility reports. For example, How can corporate green innovation practice solve the problem of harmonious symbiosis? 》The explained variable (green innovation) and part of the explanatory variables (dual legitimacy and ethical leadership). It can be seen that the social responsibility report is very important for us to study esg. Therefore, the next editor will take you to crawl the social responsibility reports of companies listed on the Shenzhen Stock Exchange over the years, hoping to bring you some help.</description>
      <content:encoded><![CDATA[<blockquote>
<p>作者:张延丰 哈工程在读博士</p>
</blockquote>
<p>近年来，企业社会责任（csr)已成为全球学术界研究的热点。国内外各大顶刊都先后刊登了多篇关于csr的文章，比如《企业绿色创新实践如何破解“和谐共生”难题？》（发表于管理世界）、《负责任的国际投资：ESG与中国OFDI》（发表于经济研究）、《Is my company really doing good? Factors influencing employees' evaluation of the authenticity of their company&rsquo;s corporate social responsibility engagement》（发表于JBR）等。这些文章核心变量的构建大都基于对企业社会责任报告的内容分析和挖掘。比如《企业绿色创新实践如何破解“和谐共生”难题？》的被解释变量（绿色创新）以及部分解释变量（二元合法性和伦理型领导）。可见，社会责任报告对于我们研究esg至关重要。因此，接下来小编就带大家爬取深交所上市公司历年的社会责任报告，希望能够给大家带来一些帮助。</p>
<br>
<h2 id="获取数据集">获取数据集</h2>
<p>采集4000多个pdf文件。经过数据清洗，将20G的pdf数据，汇总整理到170M的csv文件内。</p>
<p><img loading="lazy" src="img/datasets.png" alt=""  />
</p>
<p>数据整理不易，如需获取本数据集，请转发本文至朋友圈集赞满30+， 加微信【372335839】，备注【深圳ESG数据集】</p>
<img src="img/wechat.jpg" style="zoom:50%;" />
<p><br><br></p>
<h2 id="一构建网络爬虫">一、构建网络爬虫</h2>
<p>数据采集分为多个步骤</p>
<ol>
<li>找网址规律(GET or POST), 构造url参数</li>
<li>伪装请求，防止被封</li>
<li>构造csv，存储信心</li>
<li>执行整个爬虫</li>
</ol>
<h3 id="11-url">1.1 url</h3>
<p>打开X交所的 <a href="http://www.szse.cn/disclosure/listed/notice/">http://www.szse.cn/disclosure/listed/notice/</a> ，同时打开浏览器开发者工具network面板，在截图左侧输入框输入关键词 『社会责任报告』，按下回车。</p>
<p>此时开发者工具network面板出现很多网络交换信息， 点击检查发现下图</p>
<p><img loading="lazy" src="img/01-%e7%bd%91%e5%9d%80%e8%a7%84%e5%be%8b.png" alt=""  />
</p>
<p>发现该页面数据是<strong>POST</strong>请求，网址为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">http://www.szse.cn/api/disc/announcement/annList?random=random参数
</code></pre></div><h3 id="12-headers">1.2 headers</h3>
<p>同时也能发现伪装头参数，现将两个重要信息整理为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://www.szse.cn/api/disc/announcement/annList?random=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">())</span>

<span class="c1">#伪装头</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Accept&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json, text/javascript, */*; q=0.01&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Encoding&#39;</span><span class="p">:</span> <span class="s1">&#39;gzip, deflate&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;zh-CN,zh;q=0.9,en;q=0.8&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Content-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Host&#39;</span><span class="p">:</span> <span class="s1">&#39;www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Origin&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Proxy-Connection&#39;</span><span class="p">:</span> <span class="s1">&#39;close&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Referer&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn/disclosure/listed/fixed/index.html&#39;</span><span class="p">,</span>
           <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Request-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;ajax&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Requested-With&#39;</span><span class="p">:</span> <span class="s1">&#39;XMLHttpRequest&#39;</span><span class="p">}</span>
</code></pre></div><h3 id="13-data参数">1.3 data参数</h3>
<p>POST请求需要构造data参数，在开发者对应于payload, 整理为Python格式</p>
<p><img loading="lazy" src="img/02-payload.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">
</code></pre></div><p><br><br></p>
<h3 id="14-preview">1.4 preview</h3>
<p>看到左侧渲染后的数据，同时也能在开发者工具network面板看到肉眼背后的源数据。我们使用preview预览截图再次确认网址规律没有问题。</p>
<p><img loading="lazy" src="img/03-data-preview.jpg" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">keyword</span> <span class="o">=</span> <span class="s1">&#39;社会责任报告&#39;</span>
<span class="n">page</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#post方法参数</span>
<span class="n">payload</span> <span class="o">=</span><span class="p">{</span><span class="s2">&#34;seDate&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">,</span><span class="s2">&#34;&#34;</span><span class="p">],</span>
           <span class="s2">&#34;searchKey&#34;</span><span class="p">:</span> <span class="p">[</span><span class="n">keyword</span><span class="p">],</span>
           <span class="s2">&#34;channelCode&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;listedNotice_disc&#34;</span><span class="p">],</span>
           <span class="s2">&#34;pageSize&#34;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
           <span class="s2">&#34;pageNum&#34;</span><span class="p">:</span> <span class="n">page</span><span class="p">}</span>
</code></pre></div><h3 id="15-csv">1.5 csv</h3>
<p>现在已经把爬虫最重要的工作做完了，剩下的就是想办法构造出csv，并将数据存入csv。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#定义csv字段，存储PDF链接信息至data/esg_links.csv</span>
<span class="n">csvf</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/esg_links.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf_link&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">]</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
<span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pdf_link&#39;</span><span class="p">:</span> <span class="s1">&#39;测试pdf文件链接&#39;</span><span class="p">,</span>
             <span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="s1">&#39;测试股票代码&#39;</span><span class="p">,</span>
             <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;股票名称&#39;</span><span class="p">,</span>
             <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="s1">&#39;报告名称&#39;</span><span class="p">,</span>
             <span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="s1">&#39;发布日期&#39;</span><span class="p">,</span>
             <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="s1">&#39;pdf文件字节大小&#39;</span><span class="p">,</span>
             <span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="s1">&#39;数据id&#39;</span><span class="p">}</span>
             
<span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

<span class="c1">#关闭csv</span>
<span class="n">csvf</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div><p><br><br></p>
<h2 id="二爬虫代码完整">二、爬虫代码(完整)</h2>
<p>当你看到本文时，该完整代码很有可能会随着网站变化而失效。不要悲伤难过， 按照爬虫思路自己diy即可。如果没有爬虫基础，学习 <a href="https://www.bilibili.com/video/BV1AE411r7ph">大邓的B站爬虫视频</a> ，</p>
<p><img loading="lazy" src="img/%e5%a4%a7%e9%82%93%e7%88%ac%e8%99%ab.jpg" alt=""  />
</p>
<p>自己懂爬虫原理diy代码，比改别人的代码来的更容易。将前面的准备工作组织起来, 就形成了下面的完整代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">csv</span>

<span class="n">keyword</span> <span class="o">=</span> <span class="s2">&#34;社会责任报告&#34;</span>

<span class="c1">#定义csv字段，存储PDF链接信息至data/esg_links.csv</span>
<span class="n">csvf</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/esg_links.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf_link&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">]</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
<span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

<span class="c1">#伪装头</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Accept&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json, text/javascript, */*; q=0.01&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Encoding&#39;</span><span class="p">:</span> <span class="s1">&#39;gzip, deflate&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;zh-CN,zh;q=0.9,en;q=0.8&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Content-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Host&#39;</span><span class="p">:</span> <span class="s1">&#39;www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Origin&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Proxy-Connection&#39;</span><span class="p">:</span> <span class="s1">&#39;close&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Referer&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn/disclosure/listed/fixed/index.html&#39;</span><span class="p">,</span>
           <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Request-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;ajax&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Requested-With&#39;</span><span class="p">:</span> <span class="s1">&#39;XMLHttpRequest&#39;</span><span class="p">}</span>

<span class="n">page</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#post方法参数</span>
<span class="n">payload</span> <span class="o">=</span><span class="p">{</span><span class="s2">&#34;seDate&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">,</span><span class="s2">&#34;&#34;</span><span class="p">],</span>
           <span class="s2">&#34;searchKey&#34;</span><span class="p">:</span> <span class="p">[</span><span class="n">keyword</span><span class="p">],</span>
           <span class="s2">&#34;channelCode&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;listedNotice_disc&#34;</span><span class="p">],</span>
           <span class="s2">&#34;pageSize&#34;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
           <span class="s2">&#34;pageNum&#34;</span><span class="p">:</span> <span class="n">page</span><span class="p">}</span>

<span class="c1">#发起请求</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://www.szse.cn/api/disc/announcement/annList?random=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">())</span>
<span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span>
                     <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                     <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">))</span>

<span class="c1">#当data关键词有对应的非空列表，循环一直进行。</span>
<span class="k">while</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s1">&#39;data&#39;</span><span class="p">]:</span>
    <span class="n">payload</span><span class="p">[</span><span class="s1">&#39;pageNum&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">page</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span>
                     <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                     <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">))</span>
    
    <span class="n">esgs</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">esg</span> <span class="ow">in</span> <span class="n">esgs</span><span class="p">:</span>
        <span class="c1">#以字典样式写入csv</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pdf_link&#39;</span><span class="p">:</span> <span class="s1">&#39;http://disc.static.szse.cn/download&#39;</span><span class="o">+</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;attachPath&#39;</span><span class="p">],</span>
                <span class="c1">#为防止股票代码被exel等软件识别为数字，特转为字符串，并加sz标识。</span>
                <span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="s1">&#39;sz&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">esg</span><span class="p">[</span><span class="s1">&#39;secCode&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span> 
                <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;secName&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">],</span>
                <span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;publishTime&#39;</span><span class="p">],</span>
                <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;attachSize&#39;</span><span class="p">],</span>
                <span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]}</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="n">page</span> <span class="o">=</span> <span class="n">page</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="c1">#关闭csv</span>
<span class="n">csvf</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div><p><br><br></p>
<h2 id="三查看csv">三、查看csv</h2>
<p>使用pandas读取 <code>data/esg_links.csv</code>,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;esg_links.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/04-df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">4392
</code></pre></div><p>一共有4392条 「企业社会责任」 的报告数据。</p>
<p><br><br></p>
<h2 id="四批量下载">四、批量下载</h2>
<p>下载就简单多了， 直接使用定义好的爬虫代码。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>

<span class="k">def</span> <span class="nf">download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    下载多媒体及文件
</span><span class="s2">    url： 多媒体文件链接（结尾有文件格式名）
</span><span class="s2">    file: 存储文件的路径（结尾有文件格式名）
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="c1">#获取到二进制数据</span>
    <span class="n">binarydata</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">content</span>
    <span class="c1">#以二进制形式将数据流存入fname中</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">binarydata</span><span class="p">)</span> 
        

<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">link</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pdf_link&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span>
    <span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">link</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/</span><span class="si">{}</span><span class="s1">.pdf&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0 山河药辅：山河药辅2021年度社会责任报告
1 新 希 望：2021年企业社会责任报告（英文版）
2 天原股份：宜宾天原集团股份有限公司社会责任报告
3 五 粮 液：2021年度社会责任报告（英文版）
4 中兵红箭：2021年度社会责任报告	
......
......
148 苏宁环球：2021年社会责任报告
149 蓝色光标：2021年度企业社会责任报告
150 开尔新材：2021年度社会责任报告
151 中顺洁柔：2021年社会责任报告
......
......

4391 闽东电力：2006年度社会责任报告
4392  阳光发展：2006年度社会责任报告书
</code></pre></div><p>采集过程中，被封锁在所难免，所以记得每次停止采集的位置，在csv中删除该位置之前的数据。然后重新运行代码即可。</p>
<h3 id="注意">注意</h3>
<p>即时解决以上问题，可能遇到奇怪的问题。比如</p>
<p><img loading="lazy" src="img/07-error.png" alt=""  />
</p>
<p>检查发现相比其他几百kb的pdf，问题文件大小只有几kb。问题可能是被网站封锁或网络不稳定导致，标记好问题pdf的链接，重新批量下载一遍。</p>
<p><img loading="lazy" src="img/06-error.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="汇总至csv">汇总至csv</h2>
<p>很多企业社会责任报告是图片合成的，所以这里的pdf体积很大。将data文件夹中的4000多个pdf汇总至esg_data.csv中，能节约出电脑内存空间，也方便后续数据分析。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pdfdocx</span> <span class="kn">import</span> <span class="n">read_pdf</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">csv</span>

<span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#新建esg_data.csv，用于存储企业社会责任报告数据</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;esg_data.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvf</span><span class="p">:</span>
    <span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf_link&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">,</span> <span class="s1">&#39;report_content&#39;</span><span class="p">]</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

    <span class="n">files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
        <span class="n">record_of_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">file</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.pdf&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">record_of_df</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
        <span class="n">data</span><span class="p">[</span><span class="s1">&#39;report_content&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;data/</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file</span><span class="p">))</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div><p>最后，数据从20G的data文件夹(4000多个PDF)压缩为一个170M的esg_data.csv文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">esg_reports_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;esg_data.csv&#39;</span><span class="p">)</span>
<span class="n">esg_reports_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/08-esg_reports_df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">esg_reports_df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">4346
</code></pre></div><p><br><br></p>
<h2 id="五相关文献">五、相关文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]解学梅, &amp; 朱琪玮. (2021). 企业绿色创新实践如何破解 “和谐共生” 难题?. 管理世界, 37(1), 128-149.
[2]谢红军 &amp; 吕雪.(2022).负责任的国际投资：ESG与中国OFDI. 经济研究(03),83-99.
[3]Schaefer, S. D., Terlutter, R., &amp; Diehl, S. (2019). Is my company really doing good? Factors influencing employees&#39; evaluation of the authenticity of their company&#39;s corporate social responsibility engagement. Journal of business research, 101, 128-143.
</code></pre></div><p><br><br></p>
<h2 id="六其他广告">六、其他(广告)</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>EmoBank | 中文维度情感词典</title>
      <link>https://textdata.cn/blog/chinese_emobank/</link>
      <pubDate>Sat, 16 Jul 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/chinese_emobank/</guid>
      <description>中文情绪银行 (Chinese EmoBank)是由人工标注产生的 中文维度情感词典，含效价valence和唤醒度arousal两个维度。</description>
      <content:encoded><![CDATA[<h2 id="引言">引言</h2>
<p><strong>『中文情绪银行』</strong> (Chinese EmoBank)是由人工标注产生的 <strong>中文维度情感词典</strong>  ，含效价valence和唤醒度arousal两个维度。</p>
<ul>
<li>效价valence，可测量出文本中的积极/消极情感程度。</li>
<li>唤醒度arousal，可测量文本中平静/兴奋状态的程度。</li>
</ul>
<p>该词典包括</p>
<ul>
<li>CVAW(Chinese valence-arousal words)， 5512词</li>
<li>CVAP(Chinese valence-arousal phrases)， 含2998词组</li>
<li>语料CVAS(Chinese valence-arousal sentences) 含2582个单句</li>
<li>语料CVAT(Chinese valence-arousal texts）  2969个句子</li>
</ul>
<p>需要注意该词典是繁体中文词典，经过繁体转简体，已将CVAW嵌入到最新的cntext包。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install --upgrade cntext 
</code></pre></div><br>
<blockquote>
<p>本文图片来源于</p>
<p><a href="http://nlp.innobic.yzu.edu.tw/resources/ChineseEmoBank.html">http://nlp.innobic.yzu.edu.tw/resources/ChineseEmoBank.html</a></p>
</blockquote>
<br>
<h2 id="cvawchinese-valence-arousal-words">CVAW(Chinese valence-arousal words)</h2>
<p><img loading="lazy" src="img/cvaw.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th style="text-align:left">Word</th>
<th style="text-align:left">Valence_Mean</th>
<th style="text-align:left">Arousal_Mean</th>
<th style="text-align:left">Valence_SD</th>
<th style="text-align:left">Arousal_SD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">乏味</td>
<td style="text-align:left">3.4</td>
<td style="text-align:left">3.0</td>
<td style="text-align:left">0.800</td>
<td style="text-align:left">1.414</td>
</tr>
<tr>
<td style="text-align:left">放鬆</td>
<td style="text-align:left">6.2</td>
<td style="text-align:left">2.0</td>
<td style="text-align:left">0.748</td>
<td style="text-align:left">0.894</td>
</tr>
<tr>
<td style="text-align:left">勝利</td>
<td style="text-align:left">7.8</td>
<td style="text-align:left">7.2</td>
<td style="text-align:left">0.748</td>
<td style="text-align:left">1.166</td>
</tr>
<tr>
<td style="text-align:left">痛苦</td>
<td style="text-align:left">2.4</td>
<td style="text-align:left">6.8</td>
<td style="text-align:left">0.490</td>
<td style="text-align:left">0.748</td>
</tr>
</tbody>
</table>
<br>
<h2 id="cvapchinese-valence-arousal-phrases-">CVAP(Chinese valence-arousal phrases )</h2>
<p><img loading="lazy" src="img/cvap.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th style="text-align:left">Modifier Type</th>
<th style="text-align:left">Phrase</th>
<th style="text-align:left">Valence_Mean</th>
<th style="text-align:left">Arousal_Mean</th>
<th style="text-align:left">Valence_SD</th>
<th style="text-align:left">Arousal_SD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">deg</td>
<td style="text-align:left">十分有趣</td>
<td style="text-align:left">8.222</td>
<td style="text-align:left">7.063</td>
<td style="text-align:left">0.533</td>
<td style="text-align:left">0.390</td>
</tr>
<tr>
<td style="text-align:left">mod</td>
<td style="text-align:left">應該開心</td>
<td style="text-align:left">5.986</td>
<td style="text-align:left">5.350</td>
<td style="text-align:left">0.242</td>
<td style="text-align:left">0.456</td>
</tr>
<tr>
<td style="text-align:left">neg</td>
<td style="text-align:left">不喜歡</td>
<td style="text-align:left">3.033</td>
<td style="text-align:left">5.788</td>
<td style="text-align:left">0.481</td>
<td style="text-align:left">0.605</td>
</tr>
<tr>
<td style="text-align:left">neg_deg</td>
<td style="text-align:left">沒有太難過</td>
<td style="text-align:left">4.478</td>
<td style="text-align:left">4.675</td>
<td style="text-align:left">0.413</td>
<td style="text-align:left">0.538</td>
</tr>
</tbody>
</table>
<br>
<h2 id="cvaschinese-valence-arousal-sentences">CVAS(Chinese valence-arousal sentences)</h2>
<p><img loading="lazy" src="img/cvas.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th style="text-align:left">Text</th>
<th style="text-align:left">Valence_Mean</th>
<th style="text-align:left">Arousal_Mean</th>
<th style="text-align:left">Valence_SD</th>
<th style="text-align:left">Arousal_SD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">這是我觀賞過的最令人驚歎的演出。</td>
<td style="text-align:left">7.000</td>
<td style="text-align:left">7.750</td>
<td style="text-align:left">0.000</td>
<td style="text-align:left">0.433</td>
</tr>
<tr>
<td style="text-align:left">簡直是人生惡夢的開端。</td>
<td style="text-align:left">2.600</td>
<td style="text-align:left">6.750</td>
<td style="text-align:left">0.490</td>
<td style="text-align:left">0.829</td>
</tr>
<tr>
<td style="text-align:left">從小我經常覺得現實很無聊。</td>
<td style="text-align:left">3.667</td>
<td style="text-align:left">4.333</td>
<td style="text-align:left">0.471</td>
<td style="text-align:left">0.471</td>
</tr>
<tr>
<td style="text-align:left">過去他們很輕鬆地賺錢。</td>
<td style="text-align:left">5.667</td>
<td style="text-align:left">4.000</td>
<td style="text-align:left">1.247</td>
<td style="text-align:left">0.816</td>
</tr>
</tbody>
</table>
<br>
<h2 id="cvatchinese-valence-arousal-texts">CVAT(Chinese valence-arousal texts)</h2>
<p><img loading="lazy" src="img/cvat.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th style="text-align:left">Text</th>
<th style="text-align:left">Valence_Mean</th>
<th style="text-align:left">Arousal_Mean</th>
<th style="text-align:left">Valence_SD</th>
<th style="text-align:left">Arousal_SD</th>
<th style="text-align:left">Category</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">很多車主抱怨新車怠速抖動嚴重&mdash;-冷車時更嚴重。</td>
<td style="text-align:left">3.250</td>
<td style="text-align:left">5.667</td>
<td style="text-align:left">1.090</td>
<td style="text-align:left">1.054</td>
<td style="text-align:left">Car</td>
</tr>
<tr>
<td style="text-align:left">房間裏黴味，煙味撲鼻，沒有窗戶通風，骯髒的地毯上的斑斑點點的污蹟，令人觸目驚心。</td>
<td style="text-align:left">1.889</td>
<td style="text-align:left">6.875</td>
<td style="text-align:left">0.737</td>
<td style="text-align:left">0.927</td>
<td style="text-align:left">Hotel</td>
</tr>
<tr>
<td style="text-align:left">CPU顯卡也完全夠用，接口也非常齊全，總體來說很滿意！</td>
<td style="text-align:left">7.143</td>
<td style="text-align:left">5.000</td>
<td style="text-align:left">0.350</td>
<td style="text-align:left">0.816</td>
<td style="text-align:left">Laptop</td>
</tr>
<tr>
<td style="text-align:left">飛安帶來更多保障，也提供旅客更安心的服務品質。</td>
<td style="text-align:left">7.000</td>
<td style="text-align:left">4.222</td>
<td style="text-align:left">0.535</td>
<td style="text-align:left">1.133</td>
<td style="text-align:left">News</td>
</tr>
</tbody>
</table>
<br>
<h2 id="文献">文献</h2>
<p>如果用到Chinese EmoBank词典，请注明出处。</p>
<p>Lung-Hao Lee, Jian-Hong Li and Liang-Chih Yu, &ldquo;<a href="https://dl.acm.org/doi/pdf/10.1145/3489141">Chinese EmoBank: Building Valence-Arousal Resources for Dimensional Sentiment Analysis,</a>&rdquo; <em>ACM Trans. Asian and Low-Resource Language Information Processing</em>, vol. 21, no. 4, article 65, 2022.</p>
<p>Liang-Chih Yu, Lung-Hao Lee, Shuai Hao, Jin Wang, Yunchao He, Jun Hu, K. Robert Lai, and Xuejie Zhang. 2016. &ldquo;<a href="http://www.aclweb.org/anthology/N16-1066.pdf">Building Chinese affective resources in valence-arousal dimensions.</a> In <em>Proceedings of NAACL/HLT-16</em>, pages 540-545.</p>
<br>
<h2 id="代码">代码</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;ChineseEmoBank.pkl&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;Referer-1&#39;: &#39;Lee, Lung-Hao, Jian-Hong Li, and Liang-Chih Yu. &#34;Chinese EmoBank: Building Valence-Arousal Resources for Dimensional Sentiment Analysis.&#34; Transactions on Asian and Low-Resource Language Information Processing 21, no. 4 (2022): 1-18.&#39;,
 
 &#39;Referer-2&#39;: &#39;Liang-Chih Yu, Lung-Hao Lee, Shuai Hao, Jin Wang, Yunchao He, Jun Hu, K. Robert Lai, and Xuejie Zhang. 2016. &#34;Building Chinese affective resources in valence-arousal dimensions. In Proceedings of NAACL/HLT-16, pages 540-545.&#39;,
 
 &#39;Desc&#39;: &#39;Chinese Sentiment Dictionary, includes 「valence」「arousal」. In cntext, we only take single word into account, ignore phrase.&#39;,
 
 &#39;ChineseEmoBank&#39;:       word  valence  arousal
 0     不可思议      5.4      7.2
 1       不平      3.6      5.8
 2       不甘      3.2      6.4
 3       不安      3.8      5.4
 4       不利      3.6      5.6
 ...    ...      ...      ...
 5505    黏闷      2.8      5.6
 5506    黏腻      2.7      5.8
 5507    艳丽      5.8      4.5
 5508    苗条      6.7      3.8
 5509    修长      7.0      4.5
</code></pre></div><br>
<p>ChineseEmoBank的CVAW词典(Chinese valence-arousal words)原有 5512词，经过繁体转简体处理，得到5510个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;ChineseEmoBank.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;ChineseEmoBank&#39;</span><span class="p">]</span>
<span class="n">diction_df</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<p>测量一段文本的valence和arousal，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;很多车主抱怨新车怠速抖动严重---冷车时更严重。&#39;</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_weight</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Help on function sentiment_by_weight in module cntext.stats:

sentiment_by_weight(text, diction, params, lang=&#39;english&#39;)
    calculate the occurrences of each sentiment category words in text;
    the complex influence of intensity adverbs and negative words on emotion is not considered.
    :param text:  text sring
    :param diction:  sentiment dictionary dataframe with weight.；
    :param params:  set sentiment category weight, such as params=[&#39;valence&#39;, &#39;arousal&#39;]
    :param lang: &#34;chinese&#34; or &#34;english&#34;; default lang=&#34;english&#34;
    
    :return:
</code></pre></div><br>
<p>计算文本text中chinese_emobank词两维度的汇总得分，得到valence、arousal、word_num</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;很多车主抱怨新车怠速抖动严重---冷车时更严重。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_weight</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="p">,</span> 
                       <span class="n">diction</span> <span class="o">=</span> <span class="n">diction_df</span><span class="p">,</span>
                       <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;valence&#39;</span><span class="p">,</span> <span class="s1">&#39;arousal&#39;</span><span class="p">],</span>
                       <span class="n">lang</span> <span class="o">=</span> <span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;valence&#39;: 14.8, 
&#39;arousal&#39;: 24.8, 
&#39;word_num&#39;: 13}
</code></pre></div><ul>
<li>valence是句子中各个chinese_emobank词valence得分的加总。</li>
<li>arousal是句子中各个chinese_emobank词arousal得分的加总。</li>
<li>word_num是句子中的词语数(含标点符号)，短文本的情况下，word_num会不太准确，长文本情况下无限接近真实词语数。</li>
</ul>
<p>需要注意，文本越长，valence和arousal指标应该会越大。使用这两个指标时，需要结合word_num进行均值处理，即</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Valence = valence/word_num

Arousal = arousal/word_num
</code></pre></div><p>这里未做均值处理，尽量保留文本的原始信息。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>chinese-converter | 中文繁简互换Python库</title>
      <link>https://textdata.cn/blog/chinese_converter/</link>
      <pubDate>Mon, 11 Jul 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/chinese_converter/</guid>
      <description>中文繁简互换</description>
      <content:encoded><![CDATA[<p>网上有一些繁体中文资源不能直接利用，通过chinese-convertor库，我们可以进行中文繁简互换。</p>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install chinese-converter
</code></pre></div><br>
<h2 id="快速上手">快速上手</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">chinese_converter</span>

<span class="n">chinese_converter</span><span class="o">.</span><span class="n">to_traditional</span><span class="p">(</span><span class="s2">&#34;中国&#34;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">中國
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">chinese_converter</span><span class="o">.</span><span class="n">to_simplified</span><span class="p">(</span><span class="s2">&#34;中國&#34;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">中国
</code></pre></div><br>
<br>
<h2 id="长期征稿">长期征稿</h2>
<div style="text-align: center;">
<figure >
    <a href="https://textdata.cn/blog/call_for_paper/">
        <img src="/images/blog/call_for_paper.png" width="100%" />
    </a>
    <figcaption><small><i>点击了解投稿</i></small></figcaption>
</figure>
</div>
<br>
<h2 id="招募小伙伴">招募小伙伴</h2>
<div style="text-align: center;">
<figure >
    <a href="https://textdata.cn/blog/we_need_you/">
        <img src="/images/blog/we_need_you.png" width="100%" />
    </a>
    <figcaption><small><i>点击加入我们</i></small></figcaption>
</figure>
</div>
<br>
<h2 id="了解课程">了解课程</h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://textdata.cn/blog/management_python_course/">点击进入详情页</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>2022暑期工作坊 | Python实证指标构建与文本分析</title>
      <link>https://textdata.cn/blog/2022_summer_workshop/</link>
      <pubDate>Sun, 10 Jul 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/2022_summer_workshop/</guid>
      <description>在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：网络爬虫技术 解决如何从网络世界中高效地采集数据？文本分析技术 解决如何从杂乱的文本数据中抽取实证指标(情绪、不确定、态度、认知等变量)</description>
      <content:encoded><![CDATA[<h2 id="课程介绍">课程介绍</h2>
<p>在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！</p>
<p>在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：</p>
<ul>
<li>网络爬虫技术 解决如何从网络世界中高效地采集数据？</li>
<li>文本分析技术 解决如何从杂乱的文本数据中抽取实证指标(情绪、不确定、态度、认知等变量)？</li>
</ul>
<br>
<h3 id="授课方式">授课方式</h3>
<ul>
<li>线上直播（电脑端与手机端皆可播放，回放十天）。</li>
<li>开课前会建立讲师微信群并发布最新学习资料，群聊长期有效，助教全程跟随。</li>
<li>第一时段-在线讲座 2022.8.16~17  上午&amp;下午</li>
<li>第二时段-论文指导 2022.8.24 下午
<ul>
<li>Python实证指标构建与文本分析课程结束一周后</li>
<li>半天时间</li>
<li>学员利用一周的时间用Python收集、整理数据、文本分析，撰写一个初步的论文与老师交流，老师一对一地指导如何修改文本数据挖掘的实证论文。</li>
</ul>
</li>
</ul>
<h3 id="费用与优惠">费用与优惠</h3>
<ul>
<li>报名总费用2500元（包含在线讲座费用2000元、论文指导费用500元、课后长期答疑以及全部讲义以及代码等资料）</li>
<li>个人报名优惠：报名两科9折；三科8折；四科及以上7.5折；老朋友9折；凭本人学生证报名可再减200元/人。</li>
<li>团队报名优惠：三人成团及以上9折；五人成团及以上8折。</li>
<li>7月10日之前报名可享每人优惠100元。</li>
<li>各项优惠叠加不超过总价的7.5折。</li>
</ul>
<h3 id="报名方式">报名方式</h3>
<ul>
<li>从即日起可加老师微信咨询与报名。</li>
<li>17816181460（同微信）（汪老师）</li>
</ul>
<p><img loading="lazy" src="img/wechat.png" alt=""  />
</p>
<h3 id="缴费方式">缴费方式</h3>
<ul>
<li>扫码付款</li>
<li>添加汪老师微信获取,支持公务卡支付</li>
</ul>
<h3 id="对公转账">对公转账</h3>
<ul>
<li>单位：杭州国商智库信息技术服务有限公司</li>
<li>开户银行：中国银行杭州大学城支行</li>
<li>银行账户：6232636200100260588</li>
</ul>
<p><br><br></p>
<h2 id="内容安排">内容安排</h2>
<h3 id="一python语法入门">一、Python语法入门</h3>
<ul>
<li>Python跟英语一样是一门语言</li>
<li>数据类型之字符串</li>
<li>数据类型之列表元组集合</li>
<li>数据类型之字典</li>
<li>数据类型之布尔值、None</li>
<li>逻辑语句(if&amp;for&amp;tryexcept)</li>
<li>列表推导式</li>
<li>理解函数</li>
<li>常用的内置函数</li>
<li>os路径库</li>
<li>内置库csv文件库</li>
<li>常见错误汇总</li>
</ul>
<br>
<h3 id="二数据采集">二、数据采集</h3>
<ul>
<li>网络爬虫原理</li>
<li>寻找网址规律</li>
<li>获取网页-requests库</li>
<li>pyquery库解析html网页</li>
<li>案例 1：豆瓣小说</li>
<li>json库解析json网页</li>
<li>案例 2：豆瓣电影</li>
<li>案例 3：微博</li>
<li>案例 4： 批量下载文档、多媒体文件</li>
<li>案例 5：上市公司定期报告pdf批量下载</li>
<li>区分动态网站与静态网站</li>
</ul>
<br>
<h3 id="三文本分析入门">三、文本分析入门</h3>
<ul>
<li>文本分析在经管领域中的应用</li>
<li>读取文件中的数据(txt、pdf、docx、xlsx、csv)</li>
<li>数据清洗re库-从文本中抽取姓名、年龄、电话、数字等各种信息</li>
<li>案例 6：如何将多个文件中的数据整理到一个excel中</li>
<li>中文jieba分词</li>
<li>案例 7：词频统计、制作词云图</li>
<li>案例 8：共现法扩展情感词典</li>
<li>案例 9：词向量word2vec扩展情感词典</li>
<li>案例 10：中文情感分析(无权重词典法)</li>
<li>数据分析pandas库快速入门</li>
<li>案例 11：使用pandas对excel中的文本进行情感分析</li>
<li>案例 12: 计算地图中两点(经纬度)距离及方位角</li>
</ul>
<br>
<h3 id="四机器学习">四、机器学习</h3>
<ul>
<li>了解机器学习</li>
<li>理解特征工程</li>
<li>文本特征工程-将文本转化为机器可处理的数字向量</li>
<li>认识词袋法、one-hot、Tf-Idf、word2vec</li>
<li>案例 13：使用tf-idf进行情感分析（有权重词典法）</li>
<li>案例 14： 使用标注工具对文本数据进行标注</li>
<li>案例 15：在线评论文本分类</li>
<li>文本相似性计算</li>
<li>案例 16：使用文本相似性识别变化(政策连续性)</li>
<li>案例 17：Kmeans聚类算法</li>
<li>案例 18：LDA话题模型</li>
<li>案例 19: 识别图片中的文本</li>
<li>python爬虫、文本分析、机器学习等技术在论文中的应用赏析</li>
</ul>
<br>
<h3 id="五词嵌入与认知">五、词嵌入与认知</h3>
<ul>
<li>词嵌入</li>
<li>豆瓣影评-gensim导入词向量模型</li>
<li>认知偏见(刻板印象)</li>
<li>总结: 文本分析在经管领域中的应用概述</li>
</ul>
<p><br><br></p>
<h2 id="文本分析应用案例">文本分析应用案例</h2>
<p>参照两篇论文的摘要，可以通过场景化等的方式帮助我们迅速理解上面两个问题。摘要部分的加粗内容是论文用到的分析技术，在我们的课程中均有与之对应的知识点和代码。</p>
<p><strong>王伟,陈伟,祝效国,王洪伟.众筹融资成功率与语言风格的说服性——基于Kickstarter的实证研究[J].管理世界,2016(05):81-98.</strong></p>
<blockquote>
<p>摘要：众筹融资效果决定着众筹平台的兴衰。众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。</p>
<p>然后，借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。</p>
<p>最后，建立语言说服风格对项目筹资影响的计量模型，并对 Kickstarter 平台上的 128345 个项目进行实证分析。总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。</p>
</blockquote>
<br>
<p><a href="https://textdata.cn/blog/text_mining_in_2021_management_world/">胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.</a></p>
<blockquote>
<p>在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间 导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现，年报 MD&amp;A 中披露的“短期视域” 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴。</p>
</blockquote>
<br>
<p><strong>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</strong></p>
<blockquote>
<p>摘要：尽管移动应用程序市场的增长为移动应用程序开发人员创新提供了巨大的市场机会和经济诱因，但它也不可避免地刺激了模仿者开发盗版软件。原始应用的从业人员和开发人员声称，模仿者窃取了原始应用的想法和潜在需求，并呼吁应用平台对此类模仿者采取行动。令人惊讶的是，很少有严格的研究来分析模仿者是否以及如何影响原始应用的需求。</p>
<p>进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原始或模仿者并检测两种模仿者的方法：欺骗性和非欺骗性。</p>
<p>根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的5,141个开发人员的10,100个动作游戏应用程序样本中，模仿应用程序对原始应用程序需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原件的需求。</p>
<p>结果表明，从总体上讲，模仿者对原始移动应用程序需求的影响在统计上是微不足道的。我们的研究通过提供一种识别模仿者的方法，并提供模仿者对原始应用需求的影响的证据，为越来越多的移动应用消费文献做出了贡献。</p>
</blockquote>
<br>
<p><strong>Markowitz, D. M., &amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).</strong></p>
<blockquote>
<p>处理流畅性等元认知框架通常表明人们对简单和通用的语言的反应比复杂和技术性语言更有利。与复杂的信息相比，人们更容易处理简单和非技术性的信息，因此会更多地与目标进行互动。在涵盖 12 个现场样本（总 n = 1,064,533）的两项研究中，我们通过展示人们在付出时间和注意力时更多地使用非技术语言（例如，简单的在线语言往往会获得更多社交信息）来建立并复制这种越简单越好的现象订婚）。然而，人们在捐款时会对复杂的语言做出反应（例如，慈善捐赠活动和赠款摘要中的复杂语言往往会收到更多的钱）。这一证据表明，人们根据时间或金钱目标以不同的方式使用复杂语言的启发式方法。这些结果强调语言是社会和心理过程的镜头，以及大规模测量文本模式的计算方法。</p>
</blockquote>
<br>
<h2 id="文献汇总">文献汇总</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html
[2]沈艳,陈赟,黄卓．文本大数据分析在经济学和金融学中的应用：一个文献综述[EB/OL].http://www.ccer.pku.edu.cn/yjcg/tlg/242968.htm,2018-11-19
[3]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.*管理世界*.2016;5:81-98.
[4]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[5]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
[6]Kenneth Benoit. July 16, 2019. “Text as Data: An Overview.” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. Handbook of Research Methods in Political Science and International Relations. Thousand Oaks: Sage.
[7]Loughran T, McDonald B. Textual analysis in accounting and finance: A survey[J]. *Journal of Accounting Research*, 2016, 54(4): 1187-1230. Author links open overlay panelComputational socioeconomics
[8]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &#34;Uniting the tribes: Using text for marketing insight.&#34; *Journal of Marketing* 84, no. 1 (2020): 1-25.
[9]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. &#34;A review of best practice recommendations for text analysis in R (and a user-friendly app).&#34; *Journal of Business and Psychology* 33, no. 4 (2018): 445-459.
[10]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. &#34;Lazy prices.&#34; *The Journal of Finance* 75, no. 3 (2020): 1371-1415.
[11]孟庆斌, 杨俊华, 鲁冰. 管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J]. *中国工业经济*, 2017 (12): 132-150.
[12]Wang, Quan, Beibei Li, and Param Vir Singh. &#34;Copycats vs. Original Mobile Apps: A Machine Learning Copycat-Detection Method and Empirical Analysis.&#34; *Information Systems Research* 29.2 (2018): 273-291.
[13]Hoberg, Gerard, and Gordon Phillips. 2016, Text-based network industries and endogenous product differentiation,?*Journal of Political Economy* 124, 1423-1465
[14]Loughran, Tim, and Bill McDonald. &#34;When is a liability not a liability? Textual analysis, dictionaries, and 10‐Ks.&#34; *The Journal of Finance* 66, no. 1 (2011): 35-65.
[15]Fairclough, Norman. 2003. Analysing discourse: Textual analysis for social research (Psychology Press)
[16]Grimmer, Justin, and Brandon M Stewart. 2013, Text as data: The promise and pitfalls of automatic content analysis methods for political texts, *Political analysis*21, 267-297.
[17]Markowitz, D. M., &amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).
[18]Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” Journal of Consumer Research 47, no. 5 (2021): 787-806.
[19]Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Asent库 | 英文文本数据情感分析</title>
      <link>https://textdata.cn/blog/asent_sentiment_analysis/</link>
      <pubDate>Sun, 10 Jul 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/asent_sentiment_analysis/</guid>
      <description>使用Python做英文情感分析，考虑否定词、程度副词对情感词的修饰作用。</description>
      <content:encoded><![CDATA[<p>Asent 是一个新的Python情感分析库， 依据情感词典，按照一定的规则，可用于评判词语、句子、文档的情感信息(正、负)。</p>
<p>目前与情感有关的规则有</p>
<ul>
<li>否定（即“不高兴”）</li>
<li>加强词（“非常高兴”）</li>
<li>对比共轭（即“但是”）</li>
<li>其他强调标记，如感叹号、大小写和问号。</li>
</ul>
<p>Asent目前仅支持<code>英语、丹麦、挪威、瑞典4种语言</code>。</p>
<br>
<h2 id="安装配置">安装配置</h2>
<p>学习课程之前，需要先下载并配置spacy模型， <a href="https://github.com/explosion/spacy-models/releases">https://github.com/explosion/spacy-models/releases</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="n">spacy</span><span class="o">==</span><span class="mf">3.2.0</span>
<span class="n">pip3</span> <span class="n">install</span> <span class="n">asent</span><span class="o">==</span><span class="mf">0.4.2</span>

<span class="c1">#下载en_core_web_lg-3.3.0-py3-none-any.whl到桌面</span>
<span class="c1">#下载链接: https://pan.baidu.com/s/13hFWFjy9uRxzC-9lqrp7SQ 提取码: em8l </span>

<span class="c1">#然后使用如下安装命令</span>
<span class="n">pip3</span> <span class="n">install</span> <span class="n">Desktop</span><span class="o">/</span><span class="n">en_core_web_lg</span><span class="o">-</span><span class="mf">3.2.0</span><span class="o">-</span><span class="n">py3</span><span class="o">-</span><span class="n">none</span><span class="o">-</span><span class="nb">any</span><span class="o">.</span><span class="n">whl</span>
</code></pre></div><br>
<h2 id="快速上手">快速上手</h2>
<p>以下将带您逐步了解情绪是如何计算的。</p>
<p>首先，我们需要一个 spaCy 管道，并且我们需要向其中添加 asent 管道。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">asent</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># load spacy pipeline</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;en_core_web_lg&#34;</span><span class="p">)</span>

<span class="c1"># add the rule-based sentiment model</span>
<span class="n">nlp</span><span class="o">.</span><span class="n">add_pipe</span><span class="p">(</span><span class="s2">&#34;asent_en_v1&#34;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>&lt;asent.component.Asent at 0x7fd6b3243130&gt;
</code></pre>
<br>
<h2 id="效价和极性">效价和极性</h2>
<p>如下所示， token的效价信息来自于人工标注的词典。例如<code>I am not very happy</code>中词语<code>happy</code>的人类情感评分是2.7。</p>
<p><img loading="lazy" src="img/token_polarity.png" alt=""  />
</p>
<p>首先我们查看每个词语对应的效价。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&#34;I am not very happy.&#34;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="s2">&#34;</span><span class="se">\t</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">valence</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>I 	 0.0
am 	 0.0
not 	 0.0
very 	 0.0
happy 	 2.7
. 	 0.0
</code></pre>
<p>在该语境中， <code>happy</code>前面有否定词not修饰，所以情感极性方面应该被看做消极的。一般否定词和副词可以将形容词的情感进行反转和放大(缩小)。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>polarity=0.0 token=I span=I
polarity=0.0 token=am span=am
polarity=0.0 token=not span=not
polarity=0.0 token=very span=very
polarity=-2.215 token=happy span=not very happy
polarity=0.0 token=. span=.
</code></pre>
<p>注意到， 词语在<code>happy</code>拥有-2.215的极性分，该分是由<code>not very happy</code>确定的。</p>
<br>
<h2 id="可视化">可视化</h2>
<p>asent拥有多种情感极性可视化的方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">asent</span><span class="o">.</span><span class="n">visualize</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&#34;prediction&#34;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/fig1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">asent</span><span class="o">.</span><span class="n">visualize</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&#34;analysis&#34;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/fig2.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">sents</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="p">)</span>
</code></pre></div><pre><code>neg=0.391 neu=0.609 pos=0.0 compound=-0.4964 span=I am not very happy.
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">doc</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span>
</code></pre></div><pre><code>DocPolarityOutput(neg=0.391, neu=0.609, pos=0.0, compound=-0.4964)
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&#34;I am not very happy.I am very very happy.It is awesome!!&#34;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;doc2情感极性信息: &#39;</span><span class="p">,</span> <span class="n">doc2</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;doc2情感得分:&#39;</span><span class="p">,</span> <span class="n">doc2</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="o">.</span><span class="n">compound</span><span class="p">)</span>
</code></pre></div><pre><code>doc2情感极性信息:  neg=0.13 neu=0.536 pos=0.333 compound=0.2794

doc2情感得分: 0.279353567721562
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#每个句子的情感极性信息</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">doc2</span><span class="o">.</span><span class="n">sents</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="p">)</span>
</code></pre></div><pre><code>neg=0.391 neu=0.609 pos=0.0 compound=-0.4964 span=I am not very happy.
neg=0.0 neu=0.539 pos=0.461 compound=0.6453 span=I am very very happy.
neg=0.0 neu=0.461 pos=0.539 compound=0.6892 span=It is awesome!!
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#每个句子的情感得分</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">doc2</span><span class="o">.</span><span class="n">sents</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">_</span><span class="o">.</span><span class="n">polarity</span><span class="o">.</span><span class="n">compound</span><span class="p">)</span>
</code></pre></div><pre><code>-0.4964238981617178
0.6452764659402158
0.689208135386188
</code></pre>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>视频分享| Python数据挖掘与金融科技 </title>
      <link>https://textdata.cn/blog/fintech_quant_with_python/</link>
      <pubDate>Fri, 24 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/fintech_quant_with_python/</guid>
      <description>应云南大学管理学院邀请，参加第一届中国研究生金融科技创新大赛讲座。做Python文本数据挖掘在金融科技中的应用。</description>
      <content:encoded><![CDATA[<blockquote>
<p>第一届中国研究生金融科技创新大赛讲座</p>
<p>2022/06/24 13:43</p>
<p>录制文件：https://dwz.win/ayS8</p>
</blockquote>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1xW4y1r7L1&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<h1 id="另类数据与投资算法">另类数据与投资算法</h1>
<blockquote>
<p>信息通信技术的创新、互联网和移动终端的普及，产生了了大量的区别于 传统财务数据的新型数据，这类非财务数据具有数据量大、实时性高、颗粒度细及“原始”等特点，影响着资本市场，在投资领域的应用受到了越来越多的关注。投资者可以用<strong>较低的成本</strong>获取大量的数据和信息，对这类信息进行筛选、分析，辅助制定投资决策。</p>
<p>能否选择一种尚未在资本市场广泛使用的另类数据，利用合适的算法把该数据应用于 A 股市场投资当中，并寻找合适的算法解决方案，研究其在投资中的价值，并构建出可行性的投资方案？</p>
</blockquote>
<br>
<p><img loading="lazy" src="img/unstructrueddata.png" alt=""  />
</p>
<br>
<br>
<h2 id="另类数据alternative-data">另类数据alternative data</h2>
<p>大数据思维， 快、多、大、异。</p>
<p>另类大数据产生的更多更快，与传统指标相关性小，能提供更多的信息增益。</p>
<p>另类数据alternative data主要包含以下三种:</p>
<table>
<thead>
<tr>
<th>另类数据</th>
<th>包括</th>
<th>结构化</th>
<th>类型</th>
<th>python技术</th>
</tr>
</thead>
<tbody>
<tr>
<td>个人产生的数据</td>
<td>社交媒体帖子、产品评论、互联网搜索趋势等</td>
<td>非结构</td>
<td>网页</td>
<td>爬虫</td>
</tr>
<tr>
<td>由业务流程产生的数据</td>
<td>公司工商数据、专利数据、尾气数据、招聘数据、商业交易、事件数据、招标数据、阿里巴巴、京东、美团等电商平台数据、app排行榜、直播和搜索指数数据等</td>
<td>结构化</td>
<td>数字</td>
<td>爬虫</td>
</tr>
<tr>
<td>传感器产生的数据</td>
<td>卫星图像数据、行人和车辆流量、船舶位置等，地图数据。</td>
<td>非结构</td>
<td>图像</td>
<td>图片分析</td>
</tr>
<tr>
<td>第三方数据</td>
<td>分析师研报情感数据、一致性预期。</td>
<td>结构</td>
<td>数字</td>
<td>付费</td>
</tr>
</tbody>
</table>
<p>国内提供另类数据的开源网站有:</p>
<ul>
<li><a href="https://tushare.pro/">tushare</a> 付费</li>
<li><a href="https://www.akshare.xyz/">akshare</a> 免费</li>
</ul>
<br>
<br>
<h2 id="文本">文本</h2>
<p>文化研究之父斯图亚特·霍尔（Stuart Hall）在《电视话语中的编码和解码》（<em>Encoding and decoding inthe television discourse</em>）一文中提出了“<strong>编码解码</strong>”理论。</p>
<ul>
<li><strong>编码（encoding）</strong>，信息传播者将所传递的讯息、意图或观点，转化为具有特定规则的代码。</li>
<li><strong>解码（decoding）</strong>，信息接受者，将上述代码按特定规则进行解读。</li>
</ul>
<p>信息传播学的编码解码理论</p>
<p><img loading="lazy" src="img/SenderReceiver.png" alt=""  />
<img loading="lazy" src="img/consumer_org_society.png" alt=""  />
</p>
<br>
<table>
<thead>
<tr>
<th>角度</th>
<th>解释</th>
<th>难度</th>
<th>python库</th>
</tr>
</thead>
<tbody>
<tr>
<td>信息检索</td>
<td>新闻咨询中是否出现某类信息(某类词)</td>
<td>低</td>
<td>re、jieba</td>
</tr>
<tr>
<td><strong>情感分析</strong></td>
<td>文本中正面词与负面词含量的对比</td>
<td>低</td>
<td>jieba、nltk</td>
</tr>
<tr>
<td>文本相似度</td>
<td>两文本向量化后的cosine余弦值的</td>
<td>中</td>
<td>jieba、scikit-learn</td>
</tr>
<tr>
<td>文本分类</td>
<td>标注数据，使用文本数据做类别预测(利好、利空)</td>
<td>中</td>
<td>scikit-learn</td>
</tr>
<tr>
<td>词向量</td>
<td>- 不同主体对同一概念的认知(偏见、刻板印象)等。<br><br />- 同一主体对不同概念的认知。</td>
<td>高</td>
<td>gensim</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="文本相似度提前预警股价暴跌">文本相似度提前预警股价暴跌。</h3>
<blockquote>
<p>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. &ldquo;Lazy prices.&rdquo; <em>The Journal of Finance</em> 75, no. 3 (2020): 1371-1415.</p>
</blockquote>
<p><img loading="lazy" src="img/%e8%82%a1%e4%bb%b7%e6%b3%a2%e5%8a%a8.png" alt=""  />
</p>
<p><img loading="lazy" src="img/%e7%9b%b8%e4%bc%bc%e5%ba%a6%e8%af%86%e5%88%ab.png" alt=""  />
</p>
<br>
<h2 id="图片">图片</h2>
<p>OCR图像识别，识别有没有、有多少。</p>
<ul>
<li>停车场汽车停放量，识别有多少量车，预测沃尔玛等商超的经营情况</li>
</ul>
<p><img loading="lazy" src="img/%e9%99%88%e7%a1%95-%e9%81%a5%e6%84%9f-%e8%b4%ab%e7%a9%b7%e5%9c%b0%e5%9b%be.png" alt=""  />
</p>
<br>
<h2 id="音频视频">音频、视频</h2>
<ul>
<li>演讲音频转文本，用到文本分析，度量讲话的语气语调。</li>
</ul>
<br>
<br>
<h2 id="金融资讯舆情分析">金融资讯舆情分析</h2>
<blockquote>
<p>新闻舆情作为金融投资市场上的重要信息可以及时披露上市公司的经营状 况或股价异动情况，常常可作为投资决策的重要参考，但市场中海量的舆情信息难以通过人工的方式逐一分析，往往只能主观挑选某些个人认为比较重要的 新闻媒体进行舆情的跟踪，并忽略和抛弃其他新闻媒体的舆情信息，这极有可能遗漏掉一部分有价值的重要信息。</p>
<p>请各参赛队伍根据赛方提供的上市公司新闻资讯数据，利用深度学习、自然语言处理算法进行建模分析，<strong>及时、准确地</strong>判断新闻资讯的 <strong>舆情倾向</strong>（利好、中性、利空等）</p>
</blockquote>
<p>新闻中的可以挖掘的金融指标</p>
<ul>
<li>
<p>分析师情绪    买在分歧，卖在一致。</p>
</li>
<li>
<p>新闻情绪  机构、媒体、散户。</p>
</li>
</ul>
<h2 id="测度算法">测度算法</h2>
<p>使用文本分析对咨询中的舆情倾向（利好、中性、利空等）进行分析。</p>
<table>
<thead>
<tr>
<th>算法</th>
<th>功能</th>
<th>类比</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>词典法</td>
<td>把文档转为某个数。<br>例如政府工作报告中提到&quot;创新&quot;、&ldquo;创业&quot;的个数。</td>
<td>原子</td>
<td></td>
</tr>
<tr>
<td>机器学习</td>
<td>把 文档 转为 vector</td>
<td>分子</td>
<td></td>
</tr>
<tr>
<td>词嵌入</td>
<td>比机器学习更深入彻底，将word看做vector。工程师，含有<code>男性、技术、高薪。。。</code></td>
<td>夸克</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img loading="lazy" src="img/%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%a4%a7%e8%87%b4%e6%a1%86%e6%9e%b6.png" alt=""  />
</p>
<br>
<h2 id="需要的技术">需要的技术</h2>
<ol>
<li>
<p>词典法-构造金融情感词典</p>
<ul>
<li>
<p>共现法，上下文共同出现。</p>
</li>
<li>
<p>词向量法</p>
</li>
</ul>
</li>
<li>
<p>ML做文本分类</p>
</li>
</ol>
<br>
<br>
<h2 id="构造金融词典">构造金融词典</h2>
<h3 id="共现法">共现法</h3>
<p>物以类聚，词以群分。近义词更容易出现在同一个上下文中。</p>
<p>以「利好」「利空」为例</p>
<ol>
<li>人工选定「利好」「利空」初始词</li>
<li>构建语料内的词语共现矩阵</li>
<li>得到与「利好」「利空」共现得分较高的前n个候选词</li>
<li>分别输出到txt内</li>
<li>人工筛查剔除</li>
</ol>
<h3 id="词向量">词向量</h3>
<p><img loading="lazy" src="img/word2vec.png" alt=""  />
</p>
<p>以「利好」「利空」为例</p>
<ol>
<li>人工选定「利好」「利空」初始词</li>
<li>训练语料内的词向量模型</li>
<li>得到与「利好」「利空」向量相似度较高的前n个候选词</li>
<li>分别输出到txt内</li>
<li>人工筛选剔除</li>
</ol>
<br>
<br>
<h2 id="ml做预测利好1利空0步骤">ML做预测(利好1、利空0)步骤</h2>
<ol>
<li>&hellip;(标注数据)</li>
<li>导入数据</li>
<li>数据清洗(剔除停用词，杂乱字符等)</li>
<li>特征工程（文本转化为向量）</li>
<li>将数据分为训练集和测试集</li>
<li>选择某种ML算法训练模型</li>
<li>评价模型</li>
</ol>
<p><br><br></p>
<h2 id="ml算法">ML算法</h2>
<p>机器学习算法分为 <strong>监督式</strong> 和 <strong>非监督式</strong>。本节特指监督式，即同时含有x1, x2,&hellip;xn和y.</p>
<p>ML训练出的模型，实际上是通过数据，学习 y=f(x1, x2, &hellip;xn)中的 f。</p>
<p><img loading="lazy" src="img/ML/%e5%a4%9a%e5%85%83%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th>监督学习算法</th>
<th>代码导入方法</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model">回归</a></td>
<td>from sklearn.linear_model import LinearRegression<br><br>from sklearn.linear_model import LogisticRegression<br>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors">K近邻</a></td>
<td>from sklearn.neighbors import KNeighborsClassifier<br>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm">支持向量机</a></td>
<td>from sklearn.svm import SVC<br/>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree">决策树</a></td>
<td>from sklearn.tree import DecisionTreeClassifier<br/>&hellip;</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h1 id="投保反欺诈模型">投保反欺诈模型</h1>
<p>机器学习可以根据丰富的数据和监控模型，对数据进行多重处理分析，建立实时反欺诈规则和模型，结合当前用户特征，实时识别用户欺诈行为。</p>
<p>请参赛队伍在<strong>了解投保信息收集的基础上</strong>，基于<strong>机器学习技术</strong>，对投保过程中的信息进行收集和分析，从数据中提取客户多维度异常模式，探索大数据反欺诈规则，实现异常识别功能，提前检测投保人在交易过程中是否有欺诈行为，识别可能的欺诈行为，减少欺诈损害。</p>
<blockquote>
<p>了解投保信息收集的基础上&ndash;&gt;提取新的x</p>
</blockquote>
<h2 id="ml做预测步骤">ML做预测步骤</h2>
<ol>
<li>&hellip;(标注数据)</li>
<li>导入数据</li>
<li>数据清洗(剔除停用词，杂乱字符等)</li>
<li>特征工程（构造并加入新的x）</li>
<li>将数据分为训练集和测试集</li>
<li>选择某种ML算法训练模型</li>
<li>评价模型</li>
</ol>
<br>
<table>
<thead>
<tr>
<th>监督学习算法</th>
<th>代码导入方法</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model">回归</a></td>
<td>from sklearn.linear_model import LinearRegression<br><br>from sklearn.linear_model import LogisticRegression<br>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors">K近邻</a></td>
<td>from sklearn.neighbors import KNeighborsClassifier<br>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm">支持向量机</a></td>
<td>from sklearn.svm import SVC<br/>&hellip;</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree">决策树</a></td>
<td>from sklearn.tree import DecisionTreeClassifier<br/>&hellip;</td>
</tr>
</tbody>
</table>
<br>
<h2 id="kaggle代码httpswwwkagglecomcodeahmedmsoliman94-accuracy"><a href="https://www.kaggle.com/code/ahmedmsoliman/94-accuracy">Kaggle代码</a></h2>
<p><br><br></p>
<h1 id="公募产品个性化推荐系统">公募产品个性化推荐系统</h1>
<blockquote>
<p>在客户需求升级和金融市场的竞争环境下，数字化运营将是未来金融机构核心竞争力的来源，是构筑差异化优势的重要手段。</p>
<p>请参赛队伍结合金融行业的数字化运营需求，根据赛方提供的公募基金资讯数据、风险等级数据、用户行为点击序列、公募产品详情页的停留时长、公募产品的自选收藏等行为数据集，运用机器学习、深度学习、推荐算法等科技手段，分析预测用户的下一个兴趣点，在满足风险合规的条件下为合适的用户找到合适的产品。</p>
</blockquote>
<br>
<h2 id="方法论基础">方法论基础</h2>
<p>假设:  相似的人 喜欢做 相似的事情</p>
<p>有三种推荐算法</p>
<table>
<thead>
<tr>
<th>推荐系统算法思想</th>
<th>解释</th>
<th>特征向量化</th>
</tr>
</thead>
<tbody>
<tr>
<td>Demographic Filtering</td>
<td>相似人口特征的人 喜欢 相似的事(物)</td>
<td>将人向量化。[age、gendre、salary、consume、地理、、、]</td>
</tr>
<tr>
<td><a href="https://www.kaggle.com/code/muhammadhananasghar/imdb-movies-content-based-recomendation-system">Content Based Filtering</a></td>
<td>如果一个人喜欢某个特定事(物)，他或她也会喜欢与它相似的项目。</td>
<td>将事物向量化</td>
</tr>
<tr>
<td><a href="https://www.kaggle.com/code/omarkhald/recommendation-system-collaborative-filter">Collaborative Filtering</a> 协同(联合)</td>
<td>人与事(物) 的 配对匹配 存在模式</td>
<td>用户-评价-矩阵</td>
</tr>
</tbody>
</table>
<br>
<h2 id="collaborative-filtering--user-item-matrix">Collaborative Filtering | user-item-matrix</h2>
<p>以用户影评为例，挖掘构造出用户、产品的特点(特征向量）。</p>
<p><img loading="lazy" src="img/03-%e9%a2%84%e6%b5%8b%e7%94%a8%e6%88%b7%e5%af%b9%e7%94%b5%e5%bd%b1%e7%9a%84%e5%96%9c%e5%a5%bd.png" alt=""  />
</p>
<p><img loading="lazy" src="img/04-%e7%94%a8%e6%88%b7%e8%af%84%e4%bb%b7%e7%9f%a9%e9%98%b5.png" alt=""  />
</p>
<p><img loading="lazy" src="img/06-%e9%a2%84%e6%b5%8b%e8%ae%a1%e7%ae%97.png" alt=""  />
</p>
<p><img loading="lazy" src="img/07-%e7%94%a8%e6%88%b7%e7%9b%b8%e4%bc%bc%e5%ba%a6.png" alt=""  />
</p>
<br>
<h2 id="冷启动问题">冷启动问题</h2>
<p>如果某个用户，没有任何影评数据，如何预测该用户的偏好？</p>
<p>思路: 依然假设物以类聚，人以群分。</p>
<p>公募基金公司  有历史记录</p>
<table>
<thead>
<tr>
<th>user</th>
<th>类型</th>
<th>个人风险偏好考试</th>
<th>金额</th>
</tr>
</thead>
<tbody>
<tr>
<td>User1 (age/gender/edu/addr/intro)</td>
<td>债券</td>
<td>保守</td>
<td>5000</td>
</tr>
<tr>
<td>User2 (age/gender/edu/addr/intro)</td>
<td>股票</td>
<td>激进</td>
<td>10000</td>
</tr>
<tr>
<td>&hellip;</td>
<td>..</td>
<td>..</td>
<td>..</td>
</tr>
</tbody>
</table>
<br>
<br>
<h1 id="本文之外">本文之外</h1>
<h2 id="长期征稿">长期征稿</h2>
<div style="text-align: center;">
<figure >
    <a href="https://textdata.cn/blog/call_for_paper/">
        <img src="/images/blog/call_for_paper.png" width="100%" />
    </a>
    <figcaption><small><i>点击了解投稿</i></small></figcaption>
</figure>
</div>
<br>
<h2 id="招募小伙伴">招募小伙伴</h2>
<div style="text-align: center;">
<figure >
    <a href="https://textdata.cn/blog/we_need_you/">
        <img src="/images/blog/we_need_you.png" width="100%" />
    </a>
    <figcaption><small><i>点击加入我们</i></small></figcaption>
</figure>
</div>
<h2 id="文本分析视频课">文本分析视频课</h2>
<p>想轻松而快捷的深刻了解一个领域，看视频(直播)学习是一个不错的方式。</p>
<ul>
<li>
<p>大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与<a href="https://textdata.cn/blog/2022-05-workshop/7-Python.html">直播课</a>。</p>
</li>
<li>
<p>如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的<a href="https://textdata.cn/blog/management_python_course">录播课</a>。</p>
</li>
<li>
<p>如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读<a href="https://textdata.cn/blog/paid_for_service">有偿说明</a></p>
</li>
</ul>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://textdata.cn/blog/management_python_course/">点击进入详情页</a></p>
<br>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理世界 | 使用LM中文金融词典对年报进行语调分析</title>
      <link>https://textdata.cn/blog/manager_tone_analysis_with_lm/</link>
      <pubDate>Wed, 22 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/manager_tone_analysis_with_lm/</guid>
      <description>使用LM中文金融词典对年报进行语调分析</description>
      <content:encoded><![CDATA[<p>今天分享的这篇论文通过 <strong>有道翻译</strong> 这一简单有效的的方式汉化了 LM英文金融词典，并使用 <strong>数词语数个数</strong>  的方式构造了管理层语调这个指标。</p>
<h2 id="文献">文献</h2>
<p><strong>曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.</strong></p>
<p>本文代码实现的视频讲解已更新至付费课 <a href="http://mp.weixin.qq.com/s?__biz=MzI1MTE2ODg4MA==&amp;mid=2650082457&amp;idx=2&amp;sn=de680696e2595e8f4dc894e283436819&amp;chksm=f1f6bd86c6813490e8dc413eaf8446f176c41047cd8f5fc4faff14f12737ff5d68903396e8ec&amp;scene=21#wechat_redirect"><strong>Python实证指标构建与文本分析</strong></a> 中。</p>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<h3 id="本文代码">本文代码</h3>
<p><a href="manager_tone_analysis_with_lm.zip">点击下载</a></p>
<h3 id="摘要">摘要</h3>
<p>基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在**「口是心非」** 的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,<strong>年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与  「年报语调」 相关</strong>。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。</p>
<h3 id="关键词">关键词</h3>
<p>年报; 语调管理; 内部人交易; 信息不对称;</p>
<h2 id="代码">代码</h2>
<ul>
<li>年报数据 <code>data/reports.csv</code></li>
<li>LM金融词典</li>
<li>需要更新cntext库至于1.7.3及以上版本</li>
</ul>
<p><br><br></p>
<h2 id="语调指标">语调指标</h2>
<ul>
<li>算法1 <code>该年报内 「积极词汇数 与消极词汇数 之差」 占 「年报总词汇数」 的比例；</code></li>
<li>算法2 <code>（积极词汇数-消极词汇数）/（ 积极词汇数+消极词汇数）</code></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/reports.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>company</th>
      <th>year</th>
      <th>content</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>四川长虹</td>
      <td>2017</td>
      <td>2017 年，面对复杂多变的外部环境和多重叠加的困难挑战，公司聚焦用户与产品，强化消费洞...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>江苏吴中</td>
      <td>2014</td>
      <td>2014 年，正值公司成立二十周年，上市十五周年，在董事会带领下，公司经营管理团队与全体...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>联美控股</td>
      <td>2017</td>
      <td>报告期内，公司实现营业收入 2,376,375,380.44 元，同比增长 16.24%，营...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>华海药业</td>
      <td>2016</td>
      <td>第三节\t公司业务概要\n\n一、 报告期内公司所从事的主要业务、经营模式及行业情况说明\n...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>江泉实业</td>
      <td>2014</td>
      <td>报告期内，全球经济形势复杂多变、复苏进程缓慢；我国宏观经济进入增速放缓、结构调整加剧的新...</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>500
</code></pre>
<p><br><br></p>
<h2 id="cntext">cntext</h2>
<p>安装cntext</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">cntext</span>
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    1.9
</code></pre></div><br>
<p>查看内置词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [&#39;DUTIR.pkl&#39;,
     &#39;HOWNET.pkl&#39;,
     &#39;Chinese_Loughran_McDonald_Financial_Sentiment.pkl&#39;,
     &#39;sentiws.pkl&#39;,
     &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
     &#39;ANEW.pkl&#39;,
     &#39;LSD2015.pkl&#39;,
     &#39;NRC.pkl&#39;,
     &#39;geninqposneg.pkl&#39;,
     &#39;HuLiu.pkl&#39;,
     &#39;Loughran_McDonald_Financial_Sentiment.pkl&#39;,
     &#39;AFINN.pkl&#39;,
     &#39;ADV_CONJ.pkl&#39;,
     &#39;STOPWORDS.pkl&#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">clm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;Chinese_Loughran_McDonald_Financial_Sentiment.pkl&#39;</span><span class="p">)</span>

<span class="c1">#print(clm)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;clm关键词: &#39;</span><span class="p">,</span><span class="n">clm</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Desc: &#39;</span><span class="p">,</span> <span class="n">clm</span><span class="p">[</span><span class="s1">&#39;Desc&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Referer: &#39;</span><span class="p">,</span> <span class="n">clm</span><span class="p">[</span><span class="s1">&#39;Referer&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Chinese_Loughran_McDonald_Financial_Sentiment词典含2类情感词</span><span class="se">\n</span><span class="s1">，分别是&#39;</span><span class="p">,</span> <span class="n">clm</span><span class="p">[</span><span class="s1">&#39;Chinese_Loughran_McDonald_Financial_Sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    clm关键词:  dict_keys([&#39;Chinese_Loughran_McDonald_Financial_Sentiment&#39;, &#39;Desc&#39;, &#39;Referer&#39;])
    
    Desc:  参照该论文，cntext库使用百度翻译、有道翻译对LM词典进行汉化处理。原文使用的有道翻译、金山词霸。
    
    Referer:  曾庆生, 周波, 张程, and 陈信元. &#34;年报语调与内部人交易: 表里如一还是口是心非?.&#34; 管理世界 34, no. 09 (2018): 143-160.
    
    Chinese_Loughran_McDonald_Financial_Sentiment词典含2类情感词
    ，分别是 dict_keys([&#39;negative&#39;, &#39;positive&#39;])
</code></pre></div><p><br><br></p>
<h2 id="cntexthttpsgithubcomhidadengcntext语调的实现"><a href="https://github.com/hiDaDeng/cntext">cntext</a>语调的实现</h2>
<ul>
<li>算法1 <code>该年报内 「积极词汇数 与消极词汇数 之差」 占 「年报总词汇数」 的比例；</code></li>
<li>算法2 <code>（积极词汇数-消极词汇数）/（ 积极词汇数+消极词汇数）</code></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    {&#39;pos_num&#39;: 3,
     &#39;neg_num&#39;: 0,
     &#39;stopword_num&#39;: 8,
     &#39;word_num&#39;: 14,
     &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#cntext1.x版</span>
<span class="n">diction</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;Chinese_Loughran_McDonald_Financial_Sentiment.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;Chinese_Loughran_McDonald_Financial_Sentiment&#39;</span><span class="p">]</span>

<span class="c1">#cntext2.x版代码</span>
<span class="c1">#diction = ct.read_yaml_dict(&#39;en_common_LoughranMcDonald.yaml&#39;)[&#39;Dictionary&#39;]</span>


<span class="k">def</span> <span class="nf">tone</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
                       <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span>
                       <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
  
<span class="c1">#第一个年报的语调</span>
<span class="n">tone</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    negative_num      67
    positive_num      76
    stopword_num     462
    word_num        1831
    sentence_num      52
    dtype: int64
</code></pre></div><br>
<p>选中文本列content， 对content整体实施tone计算，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#计算tone语调</span>
<span class="n">tone_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">tone</span><span class="p">)</span>
<span class="n">tone_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>negative_num</th>
      <th>positive_num</th>
      <th>stopword_num</th>
      <th>word_num</th>
      <th>sentence_num</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>67</td>
      <td>76</td>
      <td>462</td>
      <td>1831</td>
      <td>52</td>
    </tr>
    <tr>
      <th>1</th>
      <td>32</td>
      <td>59</td>
      <td>372</td>
      <td>1266</td>
      <td>34</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18</td>
      <td>33</td>
      <td>178</td>
      <td>816</td>
      <td>19</td>
    </tr>
    <tr>
      <th>3</th>
      <td>81</td>
      <td>114</td>
      <td>1055</td>
      <td>3619</td>
      <td>90</td>
    </tr>
    <tr>
      <th>4</th>
      <td>27</td>
      <td>17</td>
      <td>134</td>
      <td>453</td>
      <td>16</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<ul>
<li>算法1 <code>该年报内 「积极词汇数 与消极词汇数 之差」 占 「年报总词汇数」 的比例；</code></li>
<li>算法2 <code>（积极词汇数-消极词汇数）/（ 积极词汇数+消极词汇数）</code></li>
</ul>
<p>将得到的正、负面、总词数分别按照算法1和算法2进行计算。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 算法1</span>
<span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;tone1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;positive_num&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;negative_num&#39;</span><span class="p">])</span><span class="o">/</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="c1">#tone_df[&#39;tone1&#39;] = (tone_df[&#39;positive_num&#39;]-tone_df[&#39;negative_num&#39;])/(tone_df[&#39;word_num&#39;]+1)</span>

<span class="c1">#算法2</span>
<span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;tone2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;positive_num&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;negative_num&#39;</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;positive_num&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">tone_df</span><span class="p">[</span><span class="s1">&#39;negative_num&#39;</span><span class="p">])</span>
<span class="c1">#tone_df[&#39;tone2&#39;] = (tone_df[&#39;positive_num&#39;]-tone_df[&#39;negative_num&#39;])/(tone_df[&#39;positive_num&#39;]+tone_df[&#39;negative_num&#39;]+1)</span>

<span class="n">tone_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

</code></pre></div><p>Run</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>negative_num</th>
      <th>positive_num</th>
      <th>stopword_num</th>
      <th>word_num</th>
      <th>sentence_num</th>
      <th>tone1</th>
      <th>tone2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>67</td>
      <td>76</td>
      <td>462</td>
      <td>1831</td>
      <td>52</td>
      <td>0.004915</td>
      <td>0.062937</td>
    </tr>
    <tr>
      <th>1</th>
      <td>32</td>
      <td>59</td>
      <td>372</td>
      <td>1266</td>
      <td>34</td>
      <td>0.021327</td>
      <td>0.296703</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18</td>
      <td>33</td>
      <td>178</td>
      <td>816</td>
      <td>19</td>
      <td>0.018382</td>
      <td>0.294118</td>
    </tr>
    <tr>
      <th>3</th>
      <td>81</td>
      <td>114</td>
      <td>1055</td>
      <td>3619</td>
      <td>90</td>
      <td>0.009119</td>
      <td>0.169231</td>
    </tr>
    <tr>
      <th>4</th>
      <td>27</td>
      <td>17</td>
      <td>134</td>
      <td>453</td>
      <td>16</td>
      <td>-0.022075</td>
      <td>-0.227273</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<p>将结果存储到xlsx中</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">tone_df</span><span class="o">.</span><span class="n">to_excel</span><span class="p">(</span><span class="s1">&#39;output/管理层-语调分析.xlsx&#39;</span><span class="p">)</span>
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 | 关于DUTIR被污染解决办法</title>
      <link>https://textdata.cn/blog/fixed_dutir_bug/</link>
      <pubDate>Mon, 20 Jun 2022 12:40:10 +0600</pubDate>
      
      <guid>/blog/fixed_dutir_bug/</guid>
      <description>实在抱歉，大邓的粗心导致词典DUTIR被污染。大家如果使用cntext中的DUTIR，麻烦更新至1.7.2版本。</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="img/dutir%e9%97%ae%e9%a2%98.png" alt=""  />
</p>
<h2 id="本文资料">本文资料</h2>
<p><a href="DUTIR%E9%97%AE%E9%A2%98.zip">点击下载本文资料</a></p>
<br>
<h2 id="词典污染了">词典污染了</h2>
<p>实在抱歉，大邓的粗心导致词典DUTIR被污染。大家如果使用cntext中的DUTIR，麻烦更新至1.7.2版本。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="n">dutir</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)</span>
<span class="n">dutir</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1.7.1

{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, &#39;伤害&#39;,...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...],
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],   
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,...]},

 &#39;Desc&#39;: &#39;大连理工大学情感本体库，细粒度情感词典。含七大类情绪，依次是哀, 好, 惊, 惧, 乐, 怒, 恶&#39;,

 &#39;Referer&#39;: &#39;徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.&#39;}
</code></pre></div><p>七大类情绪有:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    dict_keys([&#39;哀&#39;, &#39;好&#39;, &#39;惊&#39;, &#39;惧&#39;, &#39;乐&#39;, &#39;怒&#39;, &#39;恶&#39;])
</code></pre></div><p>下面是Bug!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">if</span> <span class="s1">&#39;开心&#39;</span> <span class="ow">in</span> <span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">][</span><span class="n">key</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;「开心」出现在情绪【</span><span class="si">{}</span><span class="s1">】词表中&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    「开心」出现在情绪【乐】词表中
    「开心」出现在情绪【恶】词表中
</code></pre></div><p>词语「开心」同时出现在情绪【乐】和【恶】</p>
<br>
<h2 id="dutir词典">DUTIR词典</h2>
<p>在网上找到大连理工大学情感本体文献、词典xlsx文件。</p>
<p><img loading="lazy" src="img/dutir%e8%b5%84%e6%96%99.png" alt=""  />
</p>
<p>制作方法，把 21 种小情绪汇总到喜怒哀乐等七大类情绪中。词典被污染，很可能是我汇总过程中出的问题。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;大连理工大学中文情感词汇本体.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>词语</th>
      <th>词性种类</th>
      <th>词义数</th>
      <th>词义序号</th>
      <th>情感分类</th>
      <th>强度</th>
      <th>极性</th>
      <th>辅助情感分类</th>
      <th>强度.1</th>
      <th>极性.1</th>
      <th>Unnamed: 10</th>
      <th>Unnamed: 11</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>脏乱</td>
      <td>adj</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NN</td>
      <td>7</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>糟报</td>
      <td>adj</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NN</td>
      <td>5</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>早衰</td>
      <td>adj</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NE</td>
      <td>5</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>责备</td>
      <td>verb</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NN</td>
      <td>5</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>贼眼</td>
      <td>noun</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NN</td>
      <td>5</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<h2 id="汇总七类情绪">汇总七类情绪</h2>
<p><img loading="lazy" src="img/dutir%e4%b8%83%e5%a4%a7%e7%b1%bb.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#乐</span>
<span class="n">le_cates</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PA&#39;</span><span class="p">,</span> <span class="s1">&#39;PE&#39;</span><span class="p">]</span>
<span class="c1">#好</span>
<span class="n">hao_cates</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PD&#39;</span><span class="p">,</span> <span class="s1">&#39;PH&#39;</span><span class="p">,</span> <span class="s1">&#39;PG&#39;</span><span class="p">,</span> <span class="s1">&#39;PB&#39;</span><span class="p">,</span> <span class="s1">&#39;PK&#39;</span><span class="p">]</span>
<span class="c1"># 怒</span>
<span class="n">nu_cates</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NA&#39;</span><span class="p">]</span>
<span class="c1">#哀</span>
<span class="n">ai_cates</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NB&#39;</span><span class="p">,</span> <span class="s1">&#39;NJ&#39;</span><span class="p">,</span> <span class="s1">&#39;NH&#39;</span><span class="p">,</span> <span class="s1">&#39;PF&#39;</span><span class="p">]</span>
<span class="c1"># 惧</span>
<span class="n">ju_cates</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NI&#39;</span><span class="p">,</span> <span class="s1">&#39;NC&#39;</span><span class="p">,</span> <span class="s1">&#39;NG&#39;</span><span class="p">]</span>
<span class="c1"># 恶</span>
<span class="n">wu_cates</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NE&#39;</span><span class="p">,</span> <span class="s1">&#39;ND&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">,</span> <span class="s1">&#39;NK&#39;</span><span class="p">,</span> <span class="s1">&#39;NL&#39;</span><span class="p">]</span>
<span class="c1"># 惊</span>
<span class="n">jing_cates</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PC&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">emotion</span><span class="p">(</span><span class="n">cates</span><span class="p">):</span>
    <span class="n">dfs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">cate</span> <span class="ow">in</span> <span class="n">cates</span><span class="p">:</span>
        <span class="n">sdf</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;情感分类&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">cate</span><span class="p">]</span>
        <span class="n">dfs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sdf</span><span class="p">)</span>
    <span class="n">res_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dfs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;词语&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="c1"># 情绪【乐】的词语有：</span>
<span class="n">le_words</span> <span class="o">=</span> <span class="n">emotion</span><span class="p">(</span><span class="n">cates</span><span class="o">=</span><span class="n">le_cates</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">le_words</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [&#39;瑞雪&#39;, &#39;神采&#39;, &#39;喜人&#39;, &#39;怡悦&#39;, &#39;进益&#39;, &#39;奏凯&#39;, &#39;鸾凤和鸣&#39;, &#39;特等&#39;, &#39;欢快&#39;, &#39;如意&#39;]
</code></pre></div><br>
<h2 id="制作dutirpkl">制作DUTIR.pkl</h2>
<p>将DUTIR介绍、文献出处、对应的词典汇总到字典，并制作生成DUTIR.pkl文件</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dutir</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;乐&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">senti</span><span class="p">(</span><span class="n">cates</span><span class="o">=</span><span class="n">ju_cates</span><span class="p">)</span>
<span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;好&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">senti</span><span class="p">(</span><span class="n">cates</span><span class="o">=</span><span class="n">hao_cates</span><span class="p">)</span>
<span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;怒&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">senti</span><span class="p">(</span><span class="n">cates</span><span class="o">=</span><span class="n">nu_cates</span><span class="p">)</span>
<span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;哀&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">senti</span><span class="p">(</span><span class="n">cates</span><span class="o">=</span><span class="n">ai_cates</span><span class="p">)</span>
<span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;惧&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">senti</span><span class="p">(</span><span class="n">cates</span><span class="o">=</span><span class="n">ju_cates</span><span class="p">)</span>
<span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;恶&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">senti</span><span class="p">(</span><span class="n">cates</span><span class="o">=</span><span class="n">e_cates</span><span class="p">)</span>
<span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;惊&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">senti</span><span class="p">(</span><span class="n">cates</span><span class="o">=</span><span class="n">jing_cates</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">:</span> <span class="n">dutir</span><span class="p">,</span>
        <span class="s1">&#39;Desc&#39;</span><span class="p">:</span> <span class="s1">&#39;大连理工大学情感本体库，细粒度情感词典。含七大类情绪，依次是哀, 好, 惊, 惧, 乐, 怒, 恶&#39;</span><span class="p">,</span>
        <span class="s1">&#39;Referer&#39;</span><span class="p">:</span> <span class="s1">&#39;徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.&#39;</span><span class="p">}</span>


<span class="kn">import</span> <span class="nn">pickle</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="更新cntext">更新cntext</h2>
<p>解决DUTIR词典问题， 需更新至1.7.9版本。</p>
<p><strong>pip3 install cntext==1.7.9</strong></p>
<p>现在我们检查下刚刚的问题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="n">dutir</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">if</span> <span class="s1">&#39;开心&#39;</span> <span class="ow">in</span> <span class="n">dutir</span><span class="p">[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">][</span><span class="n">key</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;「开心」只出现在情绪【</span><span class="si">{}</span><span class="s1">】词表中&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    1.7.9
    「开心」只出现在情绪【恶】词表中
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>近年《管理世界》《管理科学学报》《金融研究》使用文本分析论文</title>
      <link>https://textdata.cn/blog/research_with_tm_in_chinese_top_ms_journal/</link>
      <pubDate>Fri, 17 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/research_with_tm_in_chinese_top_ms_journal/</guid>
      <description>近年《管理世界》《管理科学学报》期刊中使用文本分析论文汇总</description>
      <content:encoded><![CDATA[<h2 id="管理世界">管理世界</h2>
<p><strong>曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.DOI:10.19744/j.cnki.11-1235/f.2018.09.012.</strong></p>
<p><a href="https://textdata.cn/blog/manager_tone_analysis_with_lm/">本文代码实现</a></p>
<blockquote>
<p><strong>摘要</strong>: 基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在&quot;口是心非&quot;的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与年报语调相关。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。</p>
<p>**关键词：**年报; 语调管理; 内部人交易; 信息不对称;</p>
</blockquote>
<br>
<p><strong>洪永淼,汪寿阳.大数据如何改变经济学研究范式？[J].管理世界,2021,37(10):40-55+72+56.DOI:10.19744/j.cnki.11-1235/f.2021.0153.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文首先从经济学视角探讨大数据给经济学实证研究所带来的范式变革,包括从理性经济人到非完全理性经济人,从孤立的经济人到互相关联的社会经济人,从代表性经济人到异质性经济主体,以及从经济分析到经济社会活动的系统分析。然后,从方法论视角讨论大数据给经济学实证研究方法所带来的变革,包括从模型驱动到数据驱动,从参数不确定性到模型不确定性,从无偏估计到有偏估计,从低维建模到高维建模,从低频数据到高频甚至实时数据,从结构化数据到非结构化数据,从传统结构化数据到新型结构化数据,以及从人工分析到智能分析等。大数据引起的经济学研究范式与研究方法变革,正在深刻重塑经济学发展方向,不但加强了经济学实证研究范式的趋势,而且还进一步突破了现代西方经济学的一些基本假设的局限性,使经济学研究日益呈现出科学化、严谨化、精细化、多元化(跨学科)与系统化的趋势,并且与社会科学其他领域在方法论上日益趋同。中国大数据资源,为从中国经济实践中总结经济发展规律,从中国特殊性中凝练可复制的经济发展模式,从而构建具有深厚学理基础的原创性中国经济理论体系,提供了一个得天独厚的&quot;富矿&quot;。</p>
<p><strong>关键词：</strong>	大数据;文本分析;机器学习;研究范式;研究方法;反身性;</p>
</blockquote>
<br>
<p><strong>张宗新,吴钊颖.媒体情绪传染与分析师乐观偏差——基于机器学习文本分析方法的经验证据[J].管理世界,2021,37(01):170-185+11+20-22.DOI:10.19744/j.cnki.11-1235/f.2021.0011.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文利用2013～2017年上市公司的百度新闻报道作为文本,运用机器学习文本分析方法测算情绪倾向得分,考察了媒体情绪对分析师预测行为的影响及其传染机制与风险后果。研究发现:(1)媒体乐观情绪会显著正向影响分析师盈利预测的乐观偏差度;(2)媒体情绪通过&quot;分析师有限关注&quot;与&quot;投资者情绪&quot;两条路径来影响分析师预测的乐观倾向;(3)分析师乐观情绪和媒体乐观情绪均会加剧股价波动及尾部风险,且分析师乐观情绪是媒体情绪影响股价波动的传导路径;(4)明星分析师与非明星分析师均会受到媒体情绪的感染,前者理性程度相对更高但其行为对股价波动冲击更为明显。本研究对于规范媒体行为,矫正分析师过度乐观偏差,合理引导理性投资具有重要意义。</p>
<p><strong>关键词：</strong>	媒体报道情绪;分析师乐观偏差;股价波动;有限理性;</p>
</blockquote>
<br>
<p><strong>胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.DOI:10.19744/j.cnki.11-1235/f.2021.0070.</strong></p>
<blockquote>
<p>**摘要：**在可持续发展战略导向下,秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基石。然而,作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间导向理论,提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系,并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现,年报MD&amp;A中披露的&quot;短期视域&quot;语言能够反映管理者内在的短视主义特质,管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时,管理者短视主义对这些长期投资的负向影响越易受到抑制。最终,管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析,对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时,本文将文本分析和机器学习方法引入管理者短视主义的研究,为未来该领域的研究提供了参考和借鉴。</p>
<p><strong>关键词：</strong> 管理者短视;长期投资;文本分析;机器学习;</p>
</blockquote>
<br>
<p><strong>底璐璐,罗勇根,江伟,陈灿.客户年报语调具有供应链传染效应吗？——企业现金持有的视角[J].管理世界,2020,36(08):148-163.DOI:10.19744/j.cnki.11-1235/f.2020.0124.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 利用我国供应商企业前五名上市客户及其管理层语调的文本数据,本文考察了跨企业关系情形下客户年报语调对供应商企业现金持有决策的影响。研究结果发现,客户的年报语调越消极,供应商企业则会持有更多的现金,表明客户年报净负面语调在供应链上存在传染效应。进一步的研究发现,非国有性质、相对议价能力较低的供应商企业现金持有与客户年报净负面语调的正相关关系分别显著强于国有性质、相对议价能力较高的供应商企业。此外,当客户融资融券程度较高时,客户年报净负面语调对供应商企业现金持有的正向影响会有所增强。本文的研究不仅在考察跨企业情形下企业现金持有的影响因素以及客户文本信息的经济后果两个方面弥补了国内外现有研究的不足,而且对于企业如何进行现金持有决策提供了一定的经验证据与参考,这对于管理供应链相关风险,推动我国企业的供应链整合进而提升我国企业的全球竞争力具有重要的启示意义。</p>
<p><strong>关键词：</strong>	年报语调;现金持有;供应链传染;文本分析;</p>
</blockquote>
<br>
<p><strong>林晚发,赵仲匡,宋敏.管理层讨论与分析的语调操纵及其债券市场反应[J].管理世界,2022,38(01):164-180.DOI:10.19744/j.cnki.11-1235/f.2022.0012.</strong></p>
<blockquote>
<p>**摘要: **本文研究了管理层讨论与分析（MD&amp;A）语调的操纵行为及其债券市场反应。研究发现,MD&amp;A异常积极语调与预警Z值负相关,债务重组正相关,这表明MD&amp;A异常积极语调暗示了企业较高的未来风险,这与语调的信息增量解释相悖,因此MD&amp;A异常积极语调更可能是操纵的结果。进一步研究发现,MD&amp;A异常积极语调越大,债券信用评级越高,且该正向关系在与评级机构利益冲突大、信息透明度低的公司子样本中更显著;此外,债券投资者能够识别语调操纵行为,但随着债券市场公众投资者的参与,MD&amp;A异常积极语调与债券信用利差之间呈现出一定的负向关系,且这种负向关系在信息透明度低的企业组中更加显著。本文较早使用中国资本市场数据度量了MD&amp;A异常积极语调,且证实这种异常语调是管理层操纵的结果,并探讨了MD&amp;A语调操纵对于债券市场信息效率的影响,相关结论对于完善MD&amp;A文本信息披露监管法规、提高评级机构独立性以及提升债券市场信息效率具有重要启示。</p>
<p><strong>关键词：</strong></p>
<p>MD＆A语调操纵; 利益冲突; 债券信用评级; 债券信用利差;</p>
</blockquote>
<p><br><br></p>
<h2 id="管理科学学报">管理科学学报</h2>
<p><strong>马长峰, 陈志娟, 张顺明. 基于文本大数据分析的会计和金融研究综述[J]. 管理科学学报, 2020, 23(9):12..</strong></p>
<blockquote>
<p>**摘要：**作为一种非结构化数据,文本大数据最近十年深刻影响会计学和金融学研究.这种影响体现在两类文献:第一类以信息为中心,将文本分析技术用于信息的品质(可读性)和数量(文本信息含量),信息披露和市场异象等方面的研究;第二类与信息无关,主要是利用文本大数据分析技术构建全新指标,例如基于文本分析的公司竞争力,创新和经济政策不确定性等新变量,梳理上述文献研究脉络,揭示文本分析技术的优缺点,并且指出在会计和金融领域应用文本大数据技术的研究面临的挑战和机遇。</p>
<p>**关键词：**可读性; 信息; 欺诈; 创新; 经济政策不确定性</p>
</blockquote>
<br>
<p><strong>杨晓兰,王伟超,高媚.股市政策对股票市场的影响——基于投资者社会互动的视角[J].管理科学学报,2020,v.23;No.187(01):15-32.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文将影响股市的政策分为五类,检验股市的政策效应;并以新浪财经博客为投资者之间社会互动的媒介,利用文本挖掘技术和社会网络研究方法,构建反映投资者之间社会互动程度、情绪属性以及社会网络中心程度的变量,探讨社会互动对股市政策效应的影响.实证研究表明,舆论导向政策对股市收益率存在显著的正向影响;证券供给需求性政策、货币政策显著提高股市波动率,市场创新与市场交易制度显著降低市场波动率.同时,投资者对专业性政策的解读显著依赖于社会互动,社会互动会放大货币政策对股市收益率的正向影响,加剧证券供给需求性政策对股市波动的影响,平缓市场创新与市场交易制度对股市波动的影响,而不影响舆论导向政策对股市产生的效应.</p>
<p><strong>关键词：</strong>	政策;社交网络;社会互动;股票市场;文本挖掘;</p>
</blockquote>
<br>
<p><strong>赵子夜,杨庆,杨楠.言多必失?管理层报告的样板化及其经济后果[J].管理科学学报,2019,22(03):53-70.</strong></p>
<blockquote>
<p>**摘要：**样板化报告在古今中外都有广泛的运用,但报告者面临两难:一方面,样板化有利于规避披露风险;但另一方面,样板化又不利于传递内部信息.那么,投资者如何评价中国上市公司的报告的样板化程度?以中国上市公司的管理层讨论与分析的文字为样本,用公司t期和t-1期报告的纵向文本相似度以及本公司和其他公司同期的报告的平均横向相似度来衡量样板化的水平,并考察了其经济后果.检验结果表明,纵向样板化的经济后果呈现相机抉择性,当公司财务风险高（亏损、微利或者被出具非标准审计意见）时,信息效应占优,样板化的报告引发负面的市场评价,而当公司财务风险较低,风险效应占优,样板化的报告则引发市场的好评.另一方面,报告横向样板化则引起了整体的负面评价.在调节效应方面,纵向样板化的经济后果受公司创新、特质信息、董事长权力和停牌次数的影响,横向样板化的经济后果则受到公司独立董事的社会网络位置的影响.综合结果表明,公司管理层讨论与分析的横向样板化,以及在高财务风险条件下的纵向样本化都会因信息披露不足而引起负面的经济后果.</p>
<p><strong>关键词：</strong> 管理层报告;样板化;文本分析;经济后果;</p>
</blockquote>
<br>
<p><strong>卞世博, 管之凡, 阎志鹏. 答非所问与市场反应:基于业绩说明会的研究[J]. 管理科学学报, 2021, 24(4):18.</strong></p>
<blockquote>
<p><strong>摘要:</strong> 对上市公司业绩说明会中投资者与管理层问答互动中管理层答非所问的现象进行了研究.本文以中小板和创业板上市公司召开的业绩说明会作为研究样本,利用文本分析方法对业绩说明会中管理层在回答投资者提问时答非所问的程度进行度量,进而实证分析了管理层的答非所问与市场反应和公司未来业绩表现之间的可能关联.结果 发现:在控制其它因素之后,管理层的答非所问与市场反应之间呈现显著的负相关关系,即公司管理层的答非所问程度越高,随后公司股票的市场表现则就会越差,并且对于那些低分析师关注的公司尤为明显;而在公司未来业绩表现方面,管理层答非所问的程度越高,则公司未来的业绩表现则会越差.。</p>
<p>**关键词：**业绩说明会; 答非所问; 市场反应; 未来业绩</p>
</blockquote>
<br>
<p><strong>逯东, 宋昕倍. 媒体报道,上市公司年报可读性与融资约束[J]. 管理科学学报, 2021, 24(12):17..</strong></p>
<blockquote>
<p>**摘要：**采用文本分析方法,深入考察了上市公司年报可读性与融资约束的关系,并考虑媒体报道这一外部信息的调节效应研究发现,上市公司的年报可读性越低,其面临的融资约束越高;媒体报道的增多可以弱化年报可读性与融资约束的关系,且媒体报道情绪越正向,其调节作用越显著.进一步分析发现:机构投资者持股比例较高能减弱年报可读性和融资约束的关系;当年报可读性较低时,媒体报道的信息效应更为显著;只有官方媒体和地方媒体的报道数量与正向报道情绪能够显著缓解年报可读性低带来的融资约束;同时,较低的年报可读性是通过提高融资成本路径来加大公司的融资约束,且使得公司未来的融资方式呈现出内部融资增加,外部融资减少的特点.从融资约束角度拓展了关于财务报告文本信息披露质量的研究,并揭示了媒体报道如何有效改善内部信息披露不足的作用机理,为企业如何通过改善内,外部的信息环境来缓解自身的融资困境提供了理论依据。</p>
<p>**关键词：**年报可读性；融资约束；媒体报道；文本分析</p>
</blockquote>
<br>
<p><strong>姚加权, 冯绪, 王赞钧,等. 语调,情绪及市场影响:基于金融情绪词典[J]. 管理科学学报, 2021, 24(5):21.</strong></p>
<blockquote>
<p>**摘要：**金融文本的语调与情绪含有上市公司管理层以及个体投资者表达的情感信息,并对股票市场产生影响.通过词典重组和深度学习算法构建了适用于正式文本与非正式文本的金融领域中文情绪词典,并基于词典构建了上市公司的年报语调和社交媒体情绪指标.构建的年报语调指标和社交媒体情绪指标能有效地预测上市公司股票的收益率,成交量,波动率和非预期盈余等市场因素,并优于基于其他广泛使用情绪词典构建的指标.此外,年报语调指标和社交媒体情绪指标对上市公司的股价崩盘风险具有显著的预测作用.为文本大数据在金融市场的应用提供了分析工具,也为大数据时代的金融市场预测和监管等活动提供了决策支持。</p>
<p>**关键词：**情绪词典；语调；投资者情绪；市场影响</p>
</blockquote>
<br>
<p><strong>姜富伟, 马甜, 张宏伟. 高风险低收益? 基于机器学习的动态CAPM模型解释[J]. 管理科学学报, 2021.</strong></p>
<blockquote>
<p>**摘要：**我国股票市场存在高风险股票反而伴随较低收益的低风险定价异象,这有悖于传统资产定价理论.本文使用宏观经济和微观企业特征构建了六百多个变量的宏微观混合大数据集,并结合多种经典机器学习算法开发了基于大数据和机器学习的智能动态CAPM模型,检验了时变系统性风险对我国股市收益解释能力.实证结果表明:本文的智能动态CAPM定价模型能够显著解释我国股市低风险定价异象;随机森林等非线性机器学习算法表现最佳;影响股票时变系统风险的主要因素是市场类因子,基本面因子居次.本文对于我国股市系统性风险测度,动态资产定价模型构建和金融与大数据和人工智能融合创新有重要理论与实践指导意义.</p>
<p>**关键词：**系统性风险; 动态CAPM; 机器学习; 金融大数据</p>
</blockquote>
<br>
<p><strong>陆瑶, 张叶青, 黎波,等. 高管个人特征与公司业绩——基于机器学习的经验证据[J]. 管理科学学报, 2020, 23(2):21.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 在目前的公司治理文献中,大部分的高管特征研究一方面仅关注单一的高管特征与公司业绩之间的关联,缺乏全面的高管特征分析;另一方面主要围绕因果推断进行研究,缺乏从预测能力出发的系统定量的结论.本文首次采用机器学习算法中的Boosting回归树,全面考察了多维度高管特征对公司业绩的预测性.以我国2008年～2016年的上市公司为样本,研究了高管的多维个人特征是否能预测公司业绩,并进一步分析了对公司业绩预测能力较强的高管个人特征及其预测模式.研究发现:1)整体而言,在我国公司CEO和董事长的特征对公司业绩的预测能力较弱;2)在众多高管个人特征之中,高管持股比例和年龄对公司业绩的预测能力较强;3)高管持股比例和年龄与公司业绩之间的关联都呈现出非线性的特点,与以往的理论较为吻合.本研究不仅利用机器学习方法从一个更为全面的视角对中国的高管特征进行了研究,也为公司高管聘任和激励机制设计等方面提供了有益的启发.</p>
<p>**关键词：**机器学习；Boosting回归树；公司治理；公司业绩</p>
</blockquote>
<br>
<p><strong>吴武清, 赵越, 闫嘉文,等. 分析师文本语调会影响股价同步性吗?&ndash;基于利益相关者行为的中介效应检验[J]. 管理科学学报, 2020, 23(9):19.</strong></p>
<blockquote>
<p>**摘要：**文章考察了分析师研究报告的文本语调对股价同步性的影响与作用机制.首先爬取2006年至2018年中国A股上市公司377644份分析师研究报告,随机选出10434句文本并人工分为积极,中性,消极三类形成语料库,以此训练11种机器学习方法并比较各方法的预测准确性,最终选择朴素贝叶斯方法估计出分析师研究报告的文本语调.实证分析发现,分析师积极的文本语调显著降低了所追踪公司的股价同步性.这一结果与已有多数研究结论不同,但在做空机制欠发达的中国资本市场,个体选择性知觉理论为此提供了很好的解释.进一步地,中介效应检验结果表明,分析师积极的文本语调通过激励公司发布更多公告,引导机构投资者买入和吸引其他分析师发布研究报告,显著降低了股价同步性.该研究对于投资者关注研报语调指标,上市公司加强信息披露,政府部门完善资本市场制度均具有重要启示。</p>
<p>**关键词：**分析师文本语调; 股价同步性; 朴素贝叶斯; 选择性知觉; 中介效应</p>
</blockquote>
<br>
<p><strong>刘冠男, 曲金铭, 李小琳,等. 基于深度强化学习的救护车动态重定位调度研究[J]. 管理科学学报, 2020, 23(2):15.</strong></p>
<blockquote>
<p>**摘要：**救护车是挽救患者生命的重要医疗资源,合理调配有限的救护车资源可以降低呼叫响应时间,提高医疗服务水平.本文面向救护车动态重定位调度问题,提出了一种基于强化学习的调度策略结构.为解决传统强化学习所面临的高维状态空间的挑战,本文基于深度Q值网络(DQN)方法,提出了一种考虑多种调度交互因子的算法RedCon-DQN,以在给定环境状态下得到最优的重定位调度策略.在此基础上,本文还提出了急救网络弹性概念,以评估各站点对全局救护优化目标的影响力.最后,基于南京市2016年～2017年的实际救护车呼叫及响应数据,构造了环境交互模拟器.在模拟器中通过大规模数据实验,验证了模型得到的调度策略相比已有方法的优越性,并分析了不同时段下调度策略的有效性及其特点.</p>
<p>**关键词：**强化学习; DQN; 救护车调度; 重定位</p>
</blockquote>
<br>
<p><strong>黄丽华, 何晓, 卢向华. 企业在线社群内容组合策略的影响研究[J]. 管理科学学报, 2020, 23(2):15..</strong></p>
<blockquote>
<p>**摘要：**现代企业通过建立在线社群实现与消费者的互动,希望在向消费者提供服务的同时进行更好的营销,然而如何提供在线社群中的营销与服务内容却是一大难题.本文在营销—服务二元理论的基础上,提出了在线社群内容二元性的平衡维度与结合维度概念,并研究平衡维度与结合维度如何影响销售业绩与消费者的满意度.结合机器学习方法,本文发现,平衡维度对消费者满意度和销售绩效有提高作用,但是,结合维度对消费者满意度及企业绩效的影响呈倒U型;另外,企业员工的技能水平对内容二元性策略的效果有着显著的调节作用.研究结论对企业理解在线社群中的营销内容与服务内容之间的二元关系,以及内容提供策略的价值机制有重要的指导意义。</p>
<p>**关键词：**在线社群; 内容二元性; 销售绩效; 消费者满意度</p>
</blockquote>
<br>
<p><strong>部慧,解峥,李佳鸿,吴俊杰.基于股评的投资者情绪对股票市场的影响[J].管理科学学报,2018,v.21;No.166(04):86-101.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 探讨投资者情绪对我国股票市场的影响.为刻画投资者情绪,基于东方财富网股吧帖文与朴素贝叶斯方法,提出融合股评看涨看跌预期和投资者关注程度的投资者情绪度量指标.进一步,利用Granger因果检验、瞬时Granger因果检验、跨期回归分析等方法,探讨了投资者情绪对我国股票收益率、交易量和波动性是否具有预测能力及影响.实证结果揭示:虽然投资者情绪对股票市场收益率、交易量和波动性均无预测能力,但投资者情绪对股票收益率和交易量有当期影响;开盘前非交易时段的股评情绪对开盘价具有预测力,开盘后交易时段的股评情绪对收盘价和日交易量具有更显著的影响.此外,股票收益率是投资者情绪的Granger原因,即投资者情绪的形成依赖于前期市场收益率.这些实证结果为深入理解参与股吧评论的交易者的行为以及行为对市场产生的影响提供了证据.</p>
<p><strong>关键词:</strong> 	投资者情绪; 噪声交易者; 文本挖掘; Granger因果检验;</p>
</blockquote>
<p><br><br></p>
<p>##金融研究</p>
<p>姜富伟, 胡逸驰, &amp; 黄楠. (2021). 央行货币政策报告文本信息, 宏观经济与股票市场. <em>金融研究</em>, <em>492</em>(6), 95-113.</p>
<blockquote>
<p>**摘要: ** 本文利用金融情感词典和文本分析技术,分析中国人民银行货币政策执行报告的文本情绪、文本相似度和文本可读性等多维文本信息,刻画央行货币政策执行报告的文本特征,探究货币政策报告的文本信息与宏观经济和股票市场的关系。实证研究发现,货币政策报告的文本情绪的改善会引起显著为正的股票市场价格反应,报告文本相似度的增加会引起股票市场波动性的显著降低,报告可读性对公布后股票市场的波动性影响不显著。货币政策报告文本情绪还与诸多宏观经济指标显著相关。进一步研究发现,引起股票市场显著反应的是报告文本情绪中反映货币政策指引的部分,而反映宏观经济历史状态的部分对股票市场的影响不显著。本文从文本大数据分析角度证明了我国央行沟通的有效性,对国内央行沟通相关研究形成了有益补充。</p>
<p><strong>关键词:</strong>  文本情绪分析  中央银行沟通  股票市场  宏观经济</p>
</blockquote>
<br>
<p>孙彤, 薛爽, &amp; 崔庆慧. (2021). 企业家前台化影响企业价值吗?——基于新浪微博的实证证据. 金融研究, 491(5), 189-206.</p>
<blockquote>
<p><strong>摘要:</strong> 互联网时代信息传递成本和沟通成本显著降低。微博作为自媒体的主要代表之一,为企业家从企业的幕后走向台前提供了一条便捷的途径。本文以信息传递理论为基础,利用新浪微博数据,检验了企业家前台化行为对企业价值的影响。实证结果表明:(1)企业家发布微博这一前台化行为有助于提升企业价值。从对价值影响的路径看,企业家微博发布后,企业经营活动现金流增加,系统性风险降低;(2)针对企业家微博进行文本分析,发现企业家微博中个性化微博比例越高、“艾特”人数越多或者微博内容中正向语调比例越高,对企业价值的正向影响越显著;(3)相对于信息不对称程度较低的企业,信息不对称程度较高的企业中企业家更倾向于发布微博。上述实证结果说明自媒体对缓解企业、企业家与投资者之间的信息不对称具有一定作用,为企业家前台化决策及路径选择提供了参考。</p>
<p><strong>关键词:</strong>  企业家  前台化  微博  企业价值  信息传递</p>
</blockquote>
<br>
<p>阮睿, 孙宇辰, 唐悦, &amp; 聂辉华. (2021). 资本市场开放能否提高企业信息披露质量?——基于 “沪港通” 和年报文本挖掘的分析. 金融研究, 488(2), 188-206.</p>
<blockquote>
<p><strong>摘要:</strong> 提高信息披露质量对于改善上市公司治理结构和保护股东权益具有重要意义。本文利用2014年开通的“沪港通”机制这一准自然实验,研究资本市场开放是否提高了企业的信息披露质量。从2010-2019年A股上市公司年报文本中提炼可读性指标衡量信息披露质量,使用匹配和双重差分方法进行实证研究,发现“沪港通”机制实施以后,标的公司(纳入“沪港通”的A股上市公司)的信息披露质量显著提高。这一结论对不同的估计方法、样本区间及控制变量组均保持稳健。异质性分析表明,对于盈余操纵水平较高、股价信息含量较低的企业,资本市场开放能够更好地改善其信息披露质量。本文丰富了资本市场开放对企业行为和绩效影响的实证研究,为继续推进资本市场开放政策提供了理论依据。</p>
<p><strong>关键词:</strong>  沪港通  资本市场开放  信息披露  文本分析</p>
</blockquote>
<br>
<p>李哲, &amp; 王文翰. (2021). “多言寡行” 的环境责任表现能否影响银行信贷获取——基于 “言” 和 “行” 双维度的文本分析. 金融研究, 498(12), 116-132.</p>
<blockquote>
<p><strong>摘要:</strong> 基于我国推行绿色信贷的政策背景,本文考察了企业“多言寡行”的环境责任表现能否影响银行的信贷决策。研究发现:(1)从总体来看,“多言寡行”的环境责任表现有助于企业获取更多的银行借款。(2)相比于长期银行借款,“多言寡行”对于短期银行借款的正向影响更为明显。(3)《关于构建绿色金融体系的指导意见》的出台抑制了“多言寡行”对银行借款的正向影响。(4)进一步分析发现,相比于环境责任表现“少言多行”以及“少言寡行”的企业,企业“多言寡行”的环境责任表现对于银行的信贷资源具有显著的正向影响;“多言寡行”对银行借款的正向影响在无背景关联、价值较低以及市场环境更差的企业中更为明显。本文有助于信贷机构认识到绿色信贷政策面临的执行风险,为确保绿色信贷的健康发展提供了新的决策参考。
<strong>关键词:</strong>  环境责任表现  绿色金融  绿色信贷  文本分析</p>
</blockquote>
<br>
<p>潘健平, 潘越, &amp; 马奕涵. (2019). 以 “合” 为贵? 合作文化与企业创新. 金融研究, 463(1), 148-167.</p>
<blockquote>
<p><strong>摘要:</strong> 本文以2006-2015年沪深A股非金融上市公司为样本,基于上市公司网站对于企业文化的叙述和年报董事会报告两份本文,采用文本分析方法,构建两个度量企业合作文化强弱的指标,并研究企业合作文化对企业创新产出和创新效率的影响。研究发现,企业文化越强调合作,企业的创新产出越多,创新效率越高。这一结论在采用增加控制变量、利用水稻播种面积作为工具变量以及以董事长的非正常离职事件为冲击进行PSM-DID等多种方法后仍然稳健。渠道检验的结果显示,合作文化是通过提高企业内部员工的凝聚力和促进企业的“产学研”合作这两种渠道来促进企业创新。进一步的研究表明,合作文化的促进作用在竞争性行业以及地区信任程度和产业集群程度较高的地区中尤为显著。本文不仅从微观层面揭示企业文化对公司财务行为的影响机理,丰富和补充了当前方兴未艾的“文化与金融”研究,而且为国家制定建设社会主义文化强国的方针战略提供理论基础和实证支持。</p>
<p><strong>关键词:</strong>  合作  企业文化  企业创新</p>
</blockquote>
<br>
<p>彭红枫, &amp; 林川. (2018). 言之有物: 网络借贷中语言有用吗?——来自人人贷借款描述的经验证据. 金融研究, 461(11), 133-153.</p>
<blockquote>
<p><strong>摘要:</strong> 本文以“人人贷”平台的388522条借款标的为样本,基于借款描述文本构造P2P网络借贷词典,并探究文本中六种类型词语比重对网络借贷行为的影响,实证结果表明:首先,各类词语比重发出的信号对贷款人的投资决策有显著影响,积极类词语和金融类词语比重与借款成功率呈正相关,消极类词语比重、强语气词语比重和弱语气词语比重均与借款成功率呈负相关关系;其次,不同年龄层次和不同收入水平的借款人提供的描述性文本中词语信号对贷款人行为的影响存在较大差异,而性别差异和学历高低基本不影响词语信号作用的发挥;最后,各类词语比重发出的质量信号是部分有效的,金融类词语比重发出的信号有效且被投资者正确识别,强语气词语比重发出的信号同样有效却未被投资者准确识别,其他类别词语比重不是有效质量信号。</p>
<p><strong>关键词:</strong>  网络借贷  文本分析  信号理论</p>
</blockquote>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>长期征稿</title>
      <link>https://textdata.cn/blog/call_for_paper/</link>
      <pubDate>Fri, 17 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/call_for_paper/</guid>
      <description>欢迎向我们提供Python/R技术、文本(数据)分析、经管科研(含Python或R)等内容的稿件</description>
      <content:encoded><![CDATA[<ul>
<li></li>
</ul>
<h2 id="引言">引言</h2>
<p>总有一些你不认识的人，知道你想知道的东西。『大邓和他的Python』 或许可以成为一座桥梁，在大数据时代，促使不同背景、不同方向的学者学术灵感相互碰撞，迸发出更多的可能性。</p>
<p>『大邓和他的Python』 鼓励分享 Python/R技术、文本(数据)分析、经管科研(含Python或R)等内容。目的只有一个，让数据科学在社会科学中更接地气。</p>
  <br>
<h2 id="内容选题">内容选题</h2>
<p>未来公众号的选题内容规划</p>
<ol>
<li>网络爬虫(数据采集)</li>
<li>文本、音频、视频、文件等数据处理</li>
<li>机器学习、自然语言处理</li>
<li>经管、社科领域，借助数据挖掘的研究和技术</li>
<li>Python相关技术分享</li>
<li>其他(待定)</li>
</ol>
  <br>
<h2 id="稿件要求">稿件要求</h2>
<ul>
<li>
<p>文章确系个人原创作品，未曾在公开渠道发表，
如为其他平台已发表或待发表的文章，请明确标注</p>
</li>
<li>
<p>稿件建议以 markdown 格式撰写，
示例链接: <a href="https://pan.baidu.com/s/1ZpvWhrGGbah71YbkW-7pjg">https://pan.baidu.com/s/1ZpvWhrGGbah71YbkW-7pjg</a> 提取码: upuc</p>
</li>
<li>
<p>投递邮件发送至 <a href="mailto:thunderhit@qq.com">thunderhit@qq.com</a></p>
</li>
</ul>
  <br>
<h2 id="作者福利">作者福利</h2>
<ul>
<li>
<p>尊重原作者署名权，并将为每篇被采纳的原创首发稿件，
提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算。</p>
</li>
<li>
<p>如作者内容分享成体系，文稿质量高，公众号可组织付费直播课。</p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>欢迎各位向cntext库分享情感词典</title>
      <link>https://textdata.cn/blog/share_your_dict_to_cntext/</link>
      <pubDate>Sun, 12 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/share_your_dict_to_cntext/</guid>
      <description>随着cntext内置词典丰富了，使用cntext做文本分析会更好用、更易用。</description>
      <content:encoded><![CDATA[<p>前几天刚刚分享 <a href="https://textdata.cn/blog/liwc_python_text_mining/">LIWC vs Python  | 文本分析之词典词频法略讲(含代码)</a>，<strong>借鉴LIWC，我觉得中文也需要有社科类的中文情感词典库，如果能汇聚已发表论文中的中文情感词典，如用户生成内容UGC那样，那么中文文本分析也会变的容易</strong>。下图是LIWC用户分享词典界面。</p>
<h2 id="liwc用户分享词典">LIWC用户分享词典</h2>
<p>没有购买LIWC是看不到截图中的「USER-CREATED LIWC DICTIONARIES」。涉及版权，英文词典文件不作分享，一起尊重知识。</p>
<p><img loading="lazy" src="liwc-ugc.png" alt=""  />
</p>
<p><strong>中文领域有很多发表出来的各研究领域的情感词典，如果有词典推荐，欢迎thunderhit@qq.com联系我，我可以将词典整理为cntext内置格式。</strong></p>
<br>
<p><strong>假设cntext内置词典丰富了，使用cntext做如下文本分析操作。</strong></p>
<br>
<h2 id="案例-cntext操作">案例: cntext操作</h2>
<h3 id="cntext内置词典">cntext内置词典</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#cntext版本</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;cntext版本: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>

<span class="c1">#查看cntext内置词典</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&#39;cntext版本: 1.7.1&#39;

[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;,
 &#39;concreteness.pkl&#39;]
</code></pre></div><br>
<h3 id="导入内置pkl词典">导入内置pkl词典</h3>
<p>cntext内词典正在规范化，理想的规范词典应该含有词语列表、Desc简介和Referer参考文献三部分。例如，大连理工大学情感本体库词典DUTIR.pkl</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dutir</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)</span>
<span class="n">dutir</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, &#39;伤害&#39;,...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...],
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,...]},
 
 &#39;Desc&#39;: &#39;大连理工大学情感本体库，细粒度情感词典。含七大类情绪，依次是哀, 好, 惊, 惧, 乐, 怒, 恶&#39;,
 
 &#39;Referer&#39;: &#39;徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.&#39;}
</code></pre></div><p>dutir返回了</p>
<ul>
<li>词典数据</li>
<li>Desc词典介绍</li>
<li>Referer词典文献出处</li>
</ul>
<br>
<br>
<h3 id="用cntext做情感计算">用cntext做情感计算</h3>
<p>情感分析，统计文本中某类词出现个数，使用cntext.sentiment函数即可实现。</p>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text:  文本字符串</li>
<li>diction:  情感词典</li>
<li>lang: 语言类型，&ldquo;chinese&rdquo; or &ldquo;english&rdquo;; 默认lang=&ldquo;chinese&rdquo;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#自定义词典</span>
<span class="n">diy_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="c1">#cntext内置词典-DUTIR</span>
<span class="n">dutir</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">]</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="c1">#使用diy_dict做情感分析</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
                   <span class="n">diction</span><span class="o">=</span><span class="n">diy_dict</span><span class="p">,</span> 
                   <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
<span class="c1">#使用DUTIR做情感分析    </span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
                   <span class="n">diction</span><span class="o">=</span><span class="n">dutir</span><span class="p">,</span> 
                   <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
 
 
 {&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<h2 id="liwc用户分享词典-1">LIWC用户分享词典</h2>
<p>以下内容整理自LIWC网站，我添加了doi及中文翻译。由于没有阅读每个词典对应的文献，词典简介翻译可能会有差错。</p>
<p>以下词典仅仅是介绍，有疑惑的可以点击doi，找到对应论文进行理解。</p>
<p><strong>由于版权问题，词典文件资源不作分享</strong>。</p>
<p><img loading="lazy" src="liwc-ugc.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th>Dictionary</th>
<th>Desc</th>
<th>Author</th>
<th>Date</th>
<th>DOI</th>
</tr>
</thead>
<tbody>
<tr>
<td>Absolutist</td>
<td>Measure absolutist thinking in texts (eg, always, never)衡量文本中的绝对主义思维（例如，always、never）</td>
<td>Al-Mosaiwi &amp; Johnstone</td>
<td>2018</td>
<td><a href="https://doi.org/10.1177/2167702617747074">https://doi.org/10.1177/2167702617747074</a></td>
</tr>
<tr>
<td>Age_Stereotypes</td>
<td>Reflects eight broadly-defined stereotypes identified in past research as descriptive of older adults,such as <code>impaired, despondent, shrew,        recluse, vulnerable, golden, grandparent, conservative</code><br>反映过去研究中确定的八种广泛定义的刻板印象(用于描述老年人)，例如“受损、沮丧、泼妇、隐士、脆弱、黄金、祖父母、保守”</td>
<td>Jessica Remedios</td>
<td>2010</td>
<td><a href="https://doi.org/10.1080/15298860903054175">https://doi.org/10.1080/15298860903054175</a></td>
</tr>
<tr>
<td>Agitation&amp;Dejection</td>
<td>Based on studies linking promotion versus prevention focus with the emotions “Agitation” and “Dejection”<br>基于将促进与预防重点与情绪“激动”和“沮丧”联系起来的研究</td>
<td>Johnsen et al.</td>
<td>2014</td>
<td><a href="https://doi.org/10.2147/PRBM.S54947">https://doi.org/10.2147/PRBM.S54947</a></td>
</tr>
<tr>
<td>Behavioral_Activation</td>
<td>Captures linguistic indicators of planning and participation in enjoyable activities<br>捕捉规划和参与愉快活动的语言指标</td>
<td>Burkhardt et al.</td>
<td>2021</td>
<td><a href="https://doi.org/10.2196/28244">https://doi.org/10.2196/28244</a></td>
</tr>
<tr>
<td>Big_Two</td>
<td>Measure the degree to which a person is thinking in terms of <strong>Agency/Communion</strong>.<br>衡量一个人在<strong>机构/交流</strong>方面的思考程度。</td>
<td>Pietraszkiewicz et al.</td>
<td>2019</td>
<td><a href="https://doi.org/10.1002/ejsp.2561">https://doi.org/10.1002/ejsp.2561</a></td>
</tr>
<tr>
<td>Brand_Personality</td>
<td>Assesses Aaker’s five brand personality dimensions as well as 42 personality trait norms<br>评估 Aaker 的五个品牌个性维度以及 42 个个性特征规范</td>
<td>Opoku et al.</td>
<td>2008</td>
<td><a href="https://doi.org/10.1080/08841240802100386">https://doi.org/10.1080/08841240802100386</a></td>
</tr>
<tr>
<td>Controversial_Terms</td>
<td>A lexicon of terms that range in their degree of controversiality, particularly in terms of their use in the media.<br>具有争议程度的术语词典，特别是在媒体中的使用方面。</td>
<td>Mejova et al.</td>
<td>2014</td>
<td><a href="http://arxiv.org/abs/1409.8152">http://arxiv.org/abs/1409.8152</a></td>
</tr>
<tr>
<td>Corporate_Social_Responsibility</td>
<td>Reveals four dimensions of corporate social responsibility<br>揭示企业社会责任的四个维度</td>
<td>Nadra Pencle &amp; Irina Mălăescu</td>
<td>2016</td>
<td><a href="https://doi.org/10.2308/jeta-51615">https://doi.org/10.2308/jeta-51615</a></td>
</tr>
<tr>
<td>Cost_Benefit</td>
<td>Measures language related to perceived costs and benefits that result from a decision or behavior.<br>衡量与决策或行为导致的感知成本和收益相关的语言。</td>
<td>Michael McCullough</td>
<td>2006</td>
<td><a href="https://doi.org/10.1037/0022-006X.74.5.887">https://doi.org/10.1037/0022-006X.74.5.887</a></td>
</tr>
<tr>
<td>Creativity&amp;Innovation</td>
<td>Language describing creation and/or innovation<br>描述创造和/或创新的语言</td>
<td>Neufeld and Gaucher</td>
<td>2017</td>
<td></td>
</tr>
<tr>
<td>Crovitz_Innovator_Identification</td>
<td>Identify “innovators” and “non-innovators” using Hebert F. Crovitz’s 42 relational words<br>使用 Hebert F. Crovitz 的 42 个相关词识别“创新者”和“非创新者”</td>
<td>Greco et al.</td>
<td>2021</td>
<td><a href="https://doi.org/10.1007/s11135-020-01038-x">https://doi.org/10.1007/s11135-020-01038-x</a></td>
</tr>
<tr>
<td>extended_Moral_Foundations_Dictionary(eMFD)</td>
<td>The eMFD, unlike previous methods, is constructed from text annotations generated by a large sample of human coders.<br>与以前的方法不同，eMFD 是由大量人类编码人员生成的文本注释构成的。</td>
<td>Hopp et al.</td>
<td>2021</td>
<td><a href="https://doi.org/10.3758/s13428-020-01433-0">https://doi.org/10.3758/s13428-020-01433-0</a></td>
</tr>
<tr>
<td>Foresight</td>
<td>Measures the degree to which anticipation/foresight occurs. That is, words pointing to indicate where things are heading (often on the basis of recurrent behaviors).<br>衡量预期/预见发生的程度。 也就是说，指向事物前进方向的词语（通常基于反复出现的行为）。</td>
<td>Robert Hogenraad</td>
<td>2020</td>
<td><a href="https://doi.org/10.1007/s11135-020-01071-w">https://doi.org/10.1007/s11135-020-01071-w</a></td>
</tr>
<tr>
<td>Imagination</td>
<td>Digital lexicon of 627 entries relative to imagination and transfiguration, i.e., words pointing to the unbelievable and whatever is beyond the real.<br>与想象和变形相关的 627 个条目的数字词典，即指向令人难以置信的事物和超越真实事物的词语。</td>
<td>Robert Hogenraad</td>
<td>2019</td>
<td><a href="https://doi.org/10.1007/s11135-018-0813-7">https://doi.org/10.1007/s11135-018-0813-7</a></td>
</tr>
<tr>
<td>Global_Citizen</td>
<td>A dictionary to assess language usage related to global citizenship<br>用于评估与全球公民相关的语言使用情况的词典</td>
<td>Stephen Reysen et al.</td>
<td>2014</td>
<td><a href="https://doi.org/10.4018/ijcbpl.2014100101">https://doi.org/10.4018/ijcbpl.2014100101</a></td>
</tr>
<tr>
<td>Grant_Evaluation</td>
<td>Captures categories relevant to scientific grant review (ability, achievement, agentic, research, standout, pos eval, neg eval)<br>捕获与科学资助审查相关的类别（能力、成就、代理、研究、杰出、正面、负面）</td>
<td>Kaatz et al.</td>
<td>2015</td>
<td><a href="https://doi.org/10.1097/ACM.0000000000000442">https://doi.org/10.1097/ACM.0000000000000442</a></td>
</tr>
<tr>
<td>Home_Perceptions</td>
<td>Calculates the frequency of words describing clutter, a sense of the home as unfinished, restful words, and nature words<br>计算描述杂乱、未完成的家感、宁静的词和自然词的频率</td>
<td>Saxbe &amp; Repetti</td>
<td>2022-01-01</td>
<td><a href="https://doi.org/10.1177/0146167209352864">https://doi.org/10.1177/0146167209352864</a></td>
</tr>
<tr>
<td>Invective Dictionary</td>
<td>Use this dictionary to detect invective language in narrative<br/></td>
<td>A. T. Panter</td>
<td>2022-01-01</td>
<td></td>
</tr>
<tr>
<td>Linguistic_Category_Model</td>
<td>A computerized LCM analysis method<br/>使用这本词典检测叙事中的谩骂语言</td>
<td>Yi-Tai Seih</td>
<td>2017</td>
<td><a href="https://doi.org/10.1177/0261927X16657855">https://doi.org/10.1177/0261927X16657855</a></td>
</tr>
<tr>
<td>Loughran_McDonald_Financial_Sentiment</td>
<td>Dictionary for measuring positive and negative sentiment specifically in financial texts.This is the 2018 version of the dictionary.<br/>专门用于衡量金融文本中正面和负面情绪的字典。这是 2018 年版的字典。</td>
<td>Loughran &amp; McDonald</td>
<td>2011</td>
<td><a href="https://doi.org/10.1111/j.1540-6261.2010.01625.x">https://doi.org/10.1111/j.1540-6261.2010.01625.x</a></td>
</tr>
<tr>
<td>Masculine_and_Feminine</td>
<td>List of masculine and feminine words from Gaucher et al. (2011)<br/>Gaucher 等人的男性化和女性化词列表。 (2011)</td>
<td>Maureen McCusker</td>
<td>2011</td>
<td><a href="https://doi.org/10.1037/a0022530">https://doi.org/10.1037/a0022530</a></td>
</tr>
<tr>
<td>Mindfulness</td>
<td>Two categories of mindfulness language describing the mindfulness state and the more encompassing “mindfulness journey”<br/>描述正念状态的两类正念语言和更全面的“正念之旅”</td>
<td>Collins et al.</td>
<td>2009</td>
<td><a href="https://doi.org/10.1037/a0017579">https://doi.org/10.1037/a0017579</a></td>
</tr>
<tr>
<td>Mind_Perception</td>
<td>Measures linguistic use of mind perception (words related to “agency” and “experience”) in naturalistic settings<br/>在自然主义环境中测量心理感知（与“agency”和“experience”相关的词）的语言使用</td>
<td>Schweitzer &amp; Waytz</td>
<td>2020</td>
<td><a href="https://doi.org/10.1037/xge0001013">https://doi.org/10.1037/xge0001013</a></td>
</tr>
<tr>
<td>Moral_Foundations_v2.0</td>
<td>An updated version of the Moral Foundations Dictionary that is recommended over the original by its creators.<br/>道德词典的更新版本，由其创建者推荐。</td>
<td>Jeremy Frimer</td>
<td>2019</td>
<td><a href="https://doi.org/10.1016/j.jrp.2019.103906">https://doi.org/10.1016/j.jrp.2019.103906</a></td>
</tr>
<tr>
<td>Moral_Justification</td>
<td>Measures variation in justification content (deontological, consequentialist, or emotive) as a function of moral foundations<br/>衡量辩护内容（道义论、后果论或情感论）随道德基础的变化</td>
<td>Wheeler &amp; Laham</td>
<td>2016</td>
<td><a href="https://doi.org/10.1177/0146167216653374">https://doi.org/10.1177/0146167216653374</a></td>
</tr>
<tr>
<td>Personal_Values_Dictionary</td>
<td>Measures the 10 Schwartz Values (and 4 higher-order value dimensions).<br/>测量 10 个 Schwartz 值（和 4 个高阶值维度）。</td>
<td>Ponizovskiy et al.</td>
<td>2020</td>
<td><a href="https://doi.org/10.1002/per.2294">https://doi.org/10.1002/per.2294</a></td>
</tr>
<tr>
<td>Prosocial_Words</td>
<td>Calculates the density of prosocial words in anything that a person says<br/>计算一个人所说的任何内容中亲社会词的密度</td>
<td>Jeremy Frimer</td>
<td>2022-01-01</td>
<td><a href="https://doi.org/10.1073/pnas.1500355112">https://doi.org/10.1073/pnas.1500355112</a></td>
</tr>
<tr>
<td>Regulatory_Mode</td>
<td>Locomotion and Assessment States of Goal Pursuit<br/>目标追求的运动和评估状态</td>
<td>Dana Kanze, Mark A. Conley, and E. Tory Higgins</td>
<td>2019</td>
<td><a href="https://doi.org/10.1016/j.obhdp.2019.04.002">https://doi.org/10.1016/j.obhdp.2019.04.002</a></td>
</tr>
<tr>
<td>Security_Language</td>
<td>Provides a reference for the comparative study of security-related linguistic repertoires in political texts (speeches, policy documents, etc.).<br/>为政治文本（演讲、政策文件等）中与安全相关的语言库的比较研究提供参考。</td>
<td>Stephane Baele &amp; Olivier Sterck</td>
<td>2014</td>
<td><a href="https://doi.org/10.1111/1467-9248.12147">https://doi.org/10.1111/1467-9248.12147</a></td>
</tr>
<tr>
<td>Self-Care</td>
<td>Measures the degree to which self-care words are used (e.g., diet, yoga)<br/>衡量自我保健词的使用程度（例如，饮食、瑜伽）</td>
<td>Xunyi Wang et al.</td>
<td>2018</td>
<td><a href="https://doi.org/10.1093/jamia/ocy012">https://doi.org/10.1093/jamia/ocy012</a></td>
</tr>
<tr>
<td>Stereotype_Content</td>
<td>A stereotype content dictionary, made using a semi-automated method, to capture the Stereotype Content Model in text<br/>使用半自动化方法制作的刻板印象内容字典，用于捕获文本中的刻板印象内容模型</td>
<td>Nicolas et al.</td>
<td>2022-01-01</td>
<td><a href="https://doi.org/10.1002/ejsp.2724">https://doi.org/10.1002/ejsp.2724</a></td>
</tr>
<tr>
<td>Stress</td>
<td>A dictionary used to measure psychological stress. Created based on the LIWC2007 English Dictionary.<br/>用来测量心理压力的字典。 根据 LIWC2007 英语词典创建。</td>
<td>Wei Wang et al.</td>
<td>2022-01-01</td>
<td><a href="https://doi.org/10.1111/apps.12065">https://doi.org/10.1111/apps.12065</a></td>
</tr>
<tr>
<td>Well_Being</td>
<td>Words that might indicate the presence of purpose or meaning<br/>可能表明存在目的或意义的词</td>
<td>Ratner et al.</td>
<td>2019</td>
<td><a href="https://doi.org/10.1080/10888691.2019.1659140">https://doi.org/10.1080/10888691.2019.1659140</a></td>
</tr>
</tbody>
</table>
<br>
<h2 id="分享词典">分享词典</h2>
<p><strong>中文领域有很多发表出来的各研究领域的情感词典，如果有词典推荐，欢迎thunderhit@qq.com联系我，我会将词典整理为cntext内置格式。</strong></p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本分析 | 中国企业高管团队创新注意力</title>
      <link>https://textdata.cn/blog/how_chinese_tmtai_impact_corporate_inovation/</link>
      <pubDate>Thu, 09 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/how_chinese_tmtai_impact_corporate_inovation/</guid>
      <description>How does TMT attention to innovation of Chinese firms influence firm innovation activities? A study on the moderating role of corporate governance.</description>
      <content:encoded><![CDATA[<h2 id="代码下载">代码下载</h2>
<p><a href="%E9%AB%98%E7%AE%A1%E5%88%9B%E6%96%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BB%A3%E7%A0%81.zip">点击下载本文代码</a></p>
<br>
<h2 id="title">Title</h2>
<p>中国企业高管团队创新注意力 对 如何影响企业创新活动？ 公司治理的调节作用研究</p>
<blockquote>
<p>Chen, Shouming, Miao Bu, Sibin Wu, and Xin Liang. &ldquo;How does TMT attention to innovation of Chinese firms influence firm innovation activities? A study on the moderating role of corporate governance.&rdquo; <em>Journal of Business Research</em> 68, no. 5 (2015): 1127-1135.</p>
</blockquote>
<br>
<h2 id="摘要">摘要</h2>
<p>本文借鉴高层梯队理论，探讨了高管团队创新注意力对中国企业创新活动的影响。本文预测 <strong>高管团队创新注意力</strong> 对企业创新活动的影响受到公司治理特征的调节作用。进一步利用从2006年至2011年6年间394家中国制造企业收集的1747个公司年度观察数据，实证检验上述假设。</p>
<p>研究结果表明:企业高管团队创新注意力与企业专利申请之间存在正相关关系，且当企业为民营企业、董事会规模较大或独立董事较少时，该正相关关系更强。</p>
<br>
<h2 id="tmtai指标构建">TMTAI指标构建</h2>
<p><strong>高管团队创新注意力TMTAI</strong>（ TMT attention to innovation）：利用6个创新相关的关键词对该指标进行测量，可以使用词典词频法， 计算TMTAI词在年报中的词频。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">innovations</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;知识产权&#39;</span><span class="p">,</span> <span class="s1">&#39;自主创新&#39;</span><span class="p">,</span> <span class="s1">&#39;专利保护&#39;</span><span class="p">,</span> <span class="s1">&#39;专利侵权&#39;</span><span class="p">,</span> <span class="s1">&#39;技术创新&#39;</span><span class="p">,</span> <span class="s1">&#39;核心技术&#39;</span><span class="p">]</span>
</code></pre></div><br>
<h2 id="待分析的数据">待分析的数据</h2>
<p><strong>原论文使用2006-2011年中国上市企业年报</strong>， 这里我自己随便找了点年报数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;reports.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
 <br>
<p>写代码，一定秉承先简单，再复杂，先局部后整体。只要在具体的局部成功了，就可以推而广之。</p>
<p>那么我们拿出一条文本，对一条文本做高管团队创新注意力tmtai词语的计算</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">tmtai_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;知识产权&#39;</span><span class="p">,</span> <span class="s1">&#39;自主创新&#39;</span><span class="p">,</span> <span class="s1">&#39;专利保护&#39;</span><span class="p">,</span> <span class="s1">&#39;专利侵权&#39;</span><span class="p">,</span> <span class="s1">&#39;技术创新&#39;</span><span class="p">,</span> <span class="s1">&#39;核心技术&#39;</span><span class="p">]</span>
<span class="n">tmtai_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;tmtai&#39;</span><span class="p">:</span> <span class="n">tmtai_words</span><span class="p">}</span>

<span class="c1">#加入自定义词典，放置文本被错分</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tmtai_words</span><span class="p">:</span>
    <span class="n">jieba</span><span class="o">.</span><span class="n">add_word</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    
<span class="c1"># 我瞎编的</span>
<span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;我们公司尊重知识产权，但也要避免专利侵权，在下一阶段会加强自主创新，培育核心技术&#39;</span>


<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">test_text</span><span class="p">,</span>
                       <span class="n">diction</span><span class="o">=</span><span class="n">tmtai_dict</span><span class="p">,</span>
                       <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">tmtai_num        4
stopword_num     9
word_num        19
sentence_num     1
dtype: int64
</code></pre></div><p>实验成功，接下来就可以推广到所有<strong>text</strong>这一列</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">tmtai_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;知识产权&#39;</span><span class="p">,</span> <span class="s1">&#39;自主创新&#39;</span><span class="p">,</span> <span class="s1">&#39;专利保护&#39;</span><span class="p">,</span> <span class="s1">&#39;专利侵权&#39;</span><span class="p">,</span> <span class="s1">&#39;技术创新&#39;</span><span class="p">,</span> <span class="s1">&#39;核心技术&#39;</span><span class="p">]</span>
<span class="n">tmtai_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;tmtai&#39;</span><span class="p">:</span> <span class="n">tmtai_words</span><span class="p">}</span>

<span class="c1">#加入自定义词典，放置文本被错分</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tmtai_words</span><span class="p">:</span>
    <span class="n">jieba</span><span class="o">.</span><span class="n">add_word</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">tmtai_count</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
                                  <span class="n">diction</span><span class="o">=</span><span class="n">tmtai_dict</span><span class="p">,</span>
                                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">))</span>

<span class="c1">#选中text这列，统计其中每条文本中tmtai词出现次数</span>
<span class="n">tdf</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">tmtai_count</span><span class="p">)</span>
<span class="n">tdf</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#合并新旧两个dataframe</span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">tdf</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#tmtai指标是 词频，因此需要tmtai_num/word_num</span>
<span class="n">result_df</span><span class="p">[</span><span class="s1">&#39;tmtai_score&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result_df</span><span class="p">[</span><span class="s1">&#39;tmtai_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">result_df</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">result_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<p>查看结果，最后一列出现了我们感兴趣的 tmtai_score 指标</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">result_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df4.png" alt=""  />
</p>
<p>tmtai_score平均分</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#500家公司tmtai指标平均值</span>
<span class="n">result_df</span><span class="p">[</span><span class="s1">&#39;tmtai_score&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.0004088675846679111
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>LIWC vs Python  | 文本分析之词典词频法略讲(含代码)</title>
      <link>https://textdata.cn/blog/liwc_python_text_mining/</link>
      <pubDate>Wed, 08 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/liwc_python_text_mining/</guid>
      <description>语言查询和词数统计 (LIWC「Linguistic Inquiry and Word Count」) 数十年的实证研究——尤其是使用 LIWC 作为科学工具的研究——也为我们提供了理解、解释和量化心理、社会和行为现象的专业方法。LIWC-22 带有 100 多个内置字典，用于捕捉人们的社会和心理状态。每本词典都包含一系列单词、词干、表情符号和其他特定的语言结构，这些结构已被识别为反映感兴趣的心理类别。例如，「认知过程cognitive processes」词典包括 1,000 多个条目，这些条目反映了一个人何时通过一般和更具体的方式积极处理信息。 「从属关系affiliation」词典包括超过 350 个条目，这些条目反映了一个人与他人联系的需要，其中包括 「community」 和 「together」 等词。 LIWC 读取给定文本并将文本中的每个单词与字典单词列表进行比较，并计算文本中与每个字典类别匹配的总单词的百分比。例如，如果 LIWC 使用内置的 LIWC-22 词典分析包含 1000 个单词的单个语音，它可能会发现其中 50 个单词与积极情绪有关，10 个单词与从属关系有关。 LIWC 会将这些数字转换为百分比：5.0% 的积极情绪和 1.0% 的从属关系。</description>
      <content:encoded><![CDATA[<blockquote>
<p>本文原理介绍翻译自  <a href="https://www.liwc.app/help/howitworks">https://www.liwc.app/help/howitworks</a></p>
<p>对比及Python代码主要是自创作</p>
</blockquote>
<p><img loading="lazy" src="img/LIWC.png" alt=""  />
</p>
<p>LIWC是一种付费的文本分析软件，在学界知名度挺高的。今天翻译了LIWC: how it works <a href="https://www.liwc.app/help/howitworks">https://www.liwc.app/help/howitworks</a>  ，通过LIWC来侧面加深对 <strong>词典情感分析</strong> 的理解。</p>
<h2 id="词频可靠的指标">词频：可靠的指标</h2>
<p><strong>语言查询和词数统计 (LIWC「Linguistic Inquiry and Word Count」) 的核心逻辑来自数十年的科学研究表明，人们的语言可以提供极其丰富心理状态信息，包括情绪、思维方式和社会关注点</strong>。有时，这些见解是相当明显和直截了当的。例如，如果某人使用了很多像 <strong>「happy、excited、elated」</strong> 这样的词，他们可能会感到快乐，我们可以使用这些信息来可靠地估计他们当前的情绪状态。然而，<strong>言语行为和心理之间的关系往往不那么明显</strong>。例如，更自信、社会地位更高的人倾向于使用相对较高的 「you」 词，而使用 「me」词的频率相对较低。在这里，数十年的实证研究——尤其是使用 LIWC 作为科学工具的研究——也为我们提供了理解、解释和量化心理、社会和行为现象的专业方法。</p>
<p>但作为算法，实际上主要的计算方法是词频。而这点，借助Python可以完成从数据清洗到数据分析全部过程。</p>
<br>
<h2 id="丰富的词典">丰富的词典</h2>
<p>LIWC-22 带有 100 多个内置字典，用于捕捉人们的社会和心理状态。每本词典都包含一系列单词、词干、表情符号和其他特定的语言结构，这些结构已被识别为反映感兴趣的心理类别。例如，「认知过程cognitive processes」词典包括 1,000 多个条目，这些条目反映了一个人何时通过一般和更具体的方式积极处理信息。 「从属关系affiliation」词典包括超过 350 个条目，这些条目反映了一个人与他人联系的需要，其中包括 「community」 和 「together」 等词。</p>
<p>LIWC 读取给定文本并将文本中的每个单词与字典单词列表进行比较，并计算文本中与每个字典类别匹配的总单词的百分比。例如，如果 LIWC 使用内置的 LIWC-22 词典分析包含 1000 个单词的单个语音，它可能会发现其中 50 个单词与积极情绪有关，10 个单词与从属关系有关。 LIWC 会将这些数字转换为百分比：5.0% 的积极情绪和 1.0% 的从属关系。</p>
<p>请注意，许多 LIWC-22 类别是按层次结构组织的。根据定义，所有愤怒的词都被归类为负面情绪词，而负面情绪词又被归类为情绪词。另请注意，同一个词可能会被分类在多个字典中。例如，「celebrate」一词在积极情绪和成就词典中都有。</p>
<p>下图是liwc用户上传分享的自定义词典，目前有77个。好像需要购买liwc服务，才能下载里面的文件</p>
<p><img loading="lazy" src="img/liwc_user_diy_dict.png" alt=""  />
</p>
<br>
<h2 id="文本越长越好">文本越长越好</h2>
<p>不要忘记，LIWC 和所有文本分析工具一样，是一种相对粗糙的工具。它有时会在识别和计算单个单词时出错。考虑一下「mad」这个词——一个在愤怒词典中被计算在内的词。通常，今天，「mad」这个词确实反映了某种程度的愤怒。然而，有时它表达了喜悦（「he&rsquo;s mad for her.」）或精神不稳定（「mad as hatter」）。幸运的是，这很少成为问题，因为 LIWC 利用了语言使用的概率模型。是的，在给定的句子中，「mad」这个词可能被用来表达积极的情绪。然而，如果作者实际上正在经历积极情绪，他们通常会倾向于使用一个以上的积极情绪词，并且很可能很少使用其他愤怒词，这应该会导致积极情绪得分高而愤怒得分低。要记住的重要一点是，您分析的单词越多，结果就越值得信赖。 10,000 字的文本比 100 字的文本产生的结果可靠得多。任何少于 25-50 个单词的文本都应该以一定的怀疑态度来看待。</p>
<br>
<p>至此翻译结束</p>
<h2 id="简单对比python与liwc">简单对比：Python与LIWC</h2>
<table>
<thead>
<tr>
<th>工具</th>
<th>简介</th>
<th>算法</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody>
<tr>
<td>Python</td>
<td>编程语言</td>
<td>词频(典)法、词嵌入法</td>
<td>接近全能, 可以用Python搞定从数据采集、清洗、分析全流程<br><br/>可以把最新前沿应用到自己研究中 (nature、science、pnas相关文本分析方法的论文会大多会开源自己的Python代码)。</td>
<td>有一定的学习门槛<br></td>
</tr>
<tr>
<td>LIWC</td>
<td>软件</td>
<td>主要是词典法</td>
<td>学界认可<br><br>内置丰富的词典,  拿来即用。</td>
<td>不够灵活， 对中文支持不友好，内置词典几乎全是西方语言。</td>
</tr>
</tbody>
</table>
<br>
<h2 id="考虑数据清洗">考虑数据清洗</h2>
<p>综合来看，如果只使用 <strong>词频(词典)法</strong> 统计某一构念相关词语在文中出现的占比， LIWC 较 Python和R等编程语言有微弱优势。这里需要说明一下，完整的文本(数据)分析包含采集、清洗、分析。其中清洗部分工作量是最大的，数据科学家有个形象的统计，认为清洗占整个数据分析工作量的70%左右。</p>
<p>LIWC的上游环节往往需要借助Python和R等其他语言对原始数据做数据清洗和整理。</p>
<p>如果数据分析的代码量一共有100行，那么清洗的代码可能有70行，数据分析的代码只需再写30行。为了数据清洗任务，你可能不得不学Python，之后可再用LIWC；也可以  LIWC&amp;Python一起用。</p>
<br>
<h2 id="好消息">好消息</h2>
<p>大家可能觉得 <strong>词频(词典)法</strong> 算法过于粗暴， 通过对LIWC工作原理了解，我们知道LIWC软件底层算法也是词频(词典)法。</p>
<p>现在大家应该对 词频(词典)法 有了新的认识，更加有理论自信，技术自信。而Python对这种算法的运行其实很擅长的，</p>
<p>cntext是我一直在开发更新的一个包，一直想将常见的文本分析代码工作量压缩至 个位行数。</p>
<p>功能模块含</p>
<ul>
<li>
<p>stats 文本统计指标</p>
<ul>
<li>词频统计</li>
<li>可读性</li>
<li>内置pkl词典</li>
<li>情感分析</li>
</ul>
</li>
<li>
<p>dictionary构建词表(典)</p>
<ul>
<li>Sopmi 互信息扩充词典法</li>
<li>W2Vmodels 词向量扩充词典法</li>
<li>Glove Glove词向量模型</li>
</ul>
</li>
<li>
<p>similarity 文本相似度</p>
</li>
<li>
<p>cos相似度</p>
</li>
<li>
<p>jaccard相似度</p>
</li>
<li>
<p>编辑距离相似度</p>
</li>
<li>
<p>mind 计算文本中的认知方向（态度、偏见）</p>
</li>
</ul>
<p>比如对一条测试数据test_text， 使用 <strong>词频(词典)法</strong> 做情感分析，代码量不到5行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 自定义情感词典</span>
<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span> <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">]}</span>

<span class="c1"># 测试数据</span>
<span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="c1"># 情感计算</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">test_text</span><span class="p">,</span>  <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<p>即时对一个csv或excel文件，某一列文本做情感分析，代码量不超过10行。我们先看一下数据</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;test_sentiment_texts.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/res1.png" alt=""  />
</p>
<p>对text列做情感分析，使用自定义情感词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入自定义情感词典</span>
<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span> <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">]}</span>

<span class="c1"># 情感计算</span>
<span class="k">def</span> <span class="nf">diy_senti</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>  <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">))</span>

<span class="c1">#读取数据</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;test_sentiment_texts.csv&#39;</span><span class="p">)</span>

<span class="c1">#选中text列，对该列进行情感计算，得到dataframe</span>
<span class="n">senti_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">diy_senti</span><span class="p">)</span>

<span class="c1">#将df和senti_df两个dataframe合并</span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">senti_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1">#存储 &amp; 显示结果</span>
<span class="n">result_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;result_of_sentiment_texts.csv&#39;</span><span class="p">)</span>
<span class="n">result_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/res2.png" alt=""  />
</p>
<br>
<h2 id="本文代码">本文代码</h2>
<p><a href="liwc_python_text_mining.zip">点击下载</a></p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>招募小伙伴</title>
      <link>https://textdata.cn/blog/we_need_you/</link>
      <pubDate>Wed, 08 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/we_need_you/</guid>
      <description>一人行快，众人行远。聚集热爱编程(Python、R)、数据挖掘技术、社科(经管)科研的小伙伴，一起做技术分享。</description>
      <content:encoded><![CDATA[<p>截止今日，「公众号: 大邓和他的Python」聚集了27000位Python爱好者。首先要感谢大家的信任和支持！</p>
<h2 id="引言">引言</h2>
<p>数据挖掘在社科、经管等领域中的应用已成为潮流和趋势。数据采集、数据分析、文本编码(清洗)、机器学习、深度学习等，借助Python一门语言可以全部搞定。互联网时代下，海量的、不规则的数据散落在各处，等待着大家去整理去探索。科学研究的进展离不开测量方法和工具的革新，在大数据时代，掌握Python会让我们的实证研究选题更广、更深、更新。</p>
<p>随着Python技术社区发展，针对不规则数据的处理，如网页文本、报告pdf、图片、音频、视频， 技术的可行性和有用性越来越高，学习Python的价值也在越来越大。大邓一直在学习和分享Python，如果要学习高质量的内容，还得翻出去查看英文社区资料。</p>
<p><strong>一人行快，众人行远。聚集热爱编程(Python、R)、数据挖掘技术、社科(经管)科研的小伙伴，一起做技术分享。</strong></p>
<br>
<h2 id="内容规划">内容规划</h2>
<p>未来公众号的选题内容规划</p>
<ol>
<li>网络爬虫(数据采集)</li>
<li>文本、音频、视频、文件等数据处理</li>
<li>机器学习、自然语言处理</li>
<li>经管、社科领域，借助数据挖掘的研究和技术</li>
<li>Python相关技术分享</li>
<li>其他(待定)</li>
</ol>
<br>
<h2 id="招募小伙伴">招募小伙伴</h2>
<h3 id="小伙伴气质">小伙伴气质</h3>
<ol>
<li>
<p>有Python、R基础</p>
</li>
<li>
<p>对数据分析感兴趣</p>
</li>
<li>
<p>高校社科(经管)专业 在读硕博</p>
</li>
</ol>
<br>
<h3 id="工作内容">工作内容</h3>
<p><strong>结合小伙伴兴趣、特长，划定工作内容：</strong></p>
<ol>
<li>
<p>技术：python或R相关资料收集、选题、代码整理。</p>
</li>
<li>
<p>运营：新媒体运营、社群、内容运营</p>
</li>
</ol>
<br>
<h2 id="福利">福利</h2>
<ul>
<li>
<p>尊重原作者署名权，并将为每篇被采纳的原创首发稿件，
提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算。</p>
</li>
<li>
<p>如作者内容分享成体系，文稿质量高，公众号可组织付费直播课。</p>
</li>
</ul>
<blockquote>
<p>根据小伙伴的工作绩效，大邓会从公众号产生的收益中，发放一定的补贴。</p>
</blockquote>
<br>
<h2 id="参与流程">参与流程</h2>
<ol>
<li>
<p>准备个人word简历：个人信息、科研经历、兴趣爱好、考/保研经历、技术分享等</p>
</li>
<li>
<p>投递个人简历至 <a href="mailto:thunderhit@qq.com">thunderhit@qq.com</a></p>
</li>
</ol>
]]></content:encoded>
    </item>
    
    <item>
      <title>NLP资源 | 汽车、金融等9大领域预训练词向量模型下载资源</title>
      <link>https://textdata.cn/blog/pretained_nlp_models/</link>
      <pubDate>Wed, 25 May 2022 10:43:10 +0600</pubDate>
      
      <guid>/blog/pretained_nlp_models/</guid>
      <description>本文主要开放汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等9大领域预训练词向量，以及字符、依存、拼音与词性4类预训练向量地址，供大家一起使用</description>
      <content:encoded><![CDATA[<p>在前面的文章中，我们介绍了关于词向量的一些基础理论和训练方法，<strong>本文主要开放汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等9大领域预训练词向量，以及字符、依存、拼音与词性4类预训练向量地址，供大家一起使用</strong>。</p>
<h2 id="一汽车房产等9大领域预训练词向量">一、汽车、房产等9大领域预训练词向量</h2>
<p>通过收集多文本分类语料库，对汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等多个领域文本进行词向量训练，得到了如下预训练词向量的结果：</p>
<table>
<thead>
<tr>
<th>领域类型</th>
<th>模型类型</th>
<th>关键词集合</th>
<th>维度数</th>
</tr>
</thead>
<tbody>
<tr>
<td>汽车</td>
<td>word_vector_auto.model.bin</td>
<td>117,510</td>
<td>200</td>
</tr>
<tr>
<td>房产</td>
<td>word_vector_house.model.bin</td>
<td>145,287</td>
<td>200</td>
</tr>
<tr>
<td>教育</td>
<td>word_vector_edu.model.bin</td>
<td>242,874</td>
<td>200</td>
</tr>
<tr>
<td>社会</td>
<td>word_vector_society.model.bin</td>
<td>221,395</td>
<td>200</td>
</tr>
<tr>
<td>娱乐</td>
<td>word_vector_ent.model.bin</td>
<td>230,665</td>
<td>200</td>
</tr>
<tr>
<td>体育</td>
<td>word_vector_sports.model.bin</td>
<td>95724</td>
<td>200</td>
</tr>
<tr>
<td>金融</td>
<td>word_vector_finance.model.bin</td>
<td>284035</td>
<td>200</td>
</tr>
<tr>
<td>科技</td>
<td>word_vector_tech.model.bin</td>
<td>108188</td>
<td>200</td>
</tr>
<tr>
<td>游戏</td>
<td>word_vector_games.model.bin</td>
<td>100821</td>
<td>200</td>
</tr>
</tbody>
</table>
<p><strong>开放地址：</strong></p>
<p><a href="https://pan.baidu.com/s/1jEHFoAmVXlB67Q28-CeTvw">https://pan.baidu.com/s/1jEHFoAmVXlB67Q28-CeTvw</a> 密码: 1pa6</p>
<h2 id="二预训练字符依存拼音与词性向量">二、预训练字符、依存、拼音与词性向量</h2>
<p>通过对字符、依存、拼音与词性进行切分，使用同样的方式，可以得到相应的预训练词向量。</p>
<table>
<thead>
<tr>
<th>向量名称</th>
<th style="text-align:center">向量含义</th>
<th style="text-align:center">词数</th>
<th style="text-align:center">维度</th>
<th style="text-align:center">例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>de_vec_10</td>
<td style="text-align:center">依存关系向量</td>
<td style="text-align:center">13</td>
<td style="text-align:center">10</td>
<td style="text-align:center">SBV, ATT</td>
</tr>
<tr>
<td>pinyin_vec_300</td>
<td style="text-align:center">汉语拼音向量</td>
<td style="text-align:center">146242</td>
<td style="text-align:center">300</td>
<td style="text-align:center">ni, hao</td>
</tr>
<tr>
<td>postag_vec_30</td>
<td style="text-align:center">汉语词性向量</td>
<td style="text-align:center">59</td>
<td style="text-align:center">300</td>
<td style="text-align:center">n,v,a,d</td>
</tr>
<tr>
<td>token_vec_300</td>
<td style="text-align:center">汉语字向量</td>
<td style="text-align:center">20029</td>
<td style="text-align:center">300</td>
<td style="text-align:center">刘,焕,勇</td>
</tr>
<tr>
<td>word_vec_300</td>
<td style="text-align:center">汉语词向量</td>
<td style="text-align:center">673266</td>
<td style="text-align:center">300</td>
<td style="text-align:center">刘焕勇</td>
</tr>
</tbody>
</table>
<p><strong>开放地址：</strong></p>
<p><a href="https://github.com/liuhuanyong/ChineseEmbedding">https://github.com/liuhuanyong/ChineseEmbedding</a></p>
<p><strong>向量效果：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> 
 ***********************字符向量************************
    token:刘
    (&#39;李&#39;, 0.7306396961212158),(&#39;陈&#39;, 0.7201231122016907)
    (&#39;赵&#39;, 0.6974461674690247),(&#39;杨&#39;, 0.6972213983535767)
    (&#39;吴&#39;, 0.6851627230644226),(&#39;徐&#39;, 0.6516467332839966)
    (&#39;郭&#39;, 0.6499480605125427),(&#39;蔡&#39;, 0.6175302267074585)
    (&#39;郑&#39;, 0.6092196106910706),(&#39;孙&#39;, 0.5950524210929871)
    token:丑
    (&#39;卯&#39;, 0.6074919700622559),(&#39;酉&#39;, 0.5910211801528931)
    (&#39;巳&#39;, 0.5581363439559937),(&#39;戌&#39;, 0.43932047486305237)
    (&#39;戊&#39;, 0.41449615359306335),(&#39;壬&#39;, 0.40456631779670715)
    (&#39;謤&#39;, 0.367109090089798),(&#39;绯&#39;, 0.3643313944339752),
    (&#39;寅&#39;, 0.36351141333580017),(&#39;旽&#39;, 0.3549465537071228)


***********************依存向量************************
    dependency rel:ATT
    (&#39;COO&#39;, 0.14239487051963806),(&#39;ADV&#39;, -0.16987691819667816)
    (&#39;RAD&#39;, -0.2357601821422577),(&#39;HED&#39;, -0.2401314228773117)
    (&#39;SBV&#39;, -0.25625932216644287),(&#39;WP&#39;, -0.27165737748146057)
    (&#39;LAD&#39;, -0.2902592420578003),(&#39;POB&#39;, -0.2990782558917999)
    (&#39;VOB&#39;, -0.37553706765174866),(&#39;IOB&#39;, -0.6669262647628784)
    dependency rel:POB
    (&#39;IOB&#39;, 0.16698899865150452),(&#39;DBL&#39;, 0.16678886115550995)
    (&#39;FOB&#39;, 0.1657436639070511),(&#39;CMP&#39;, 0.14784857630729675)
    (&#39;VOB&#39;, 0.1461176574230194),(&#39;SBV&#39;, 0.08011472970247269)
    (&#39;LAD&#39;, -0.022307466715574265),(&#39;WP&#39;, -0.022942926734685898)
    (&#39;HED&#39;, -0.037264980375766754),(&#39;RAD&#39;, -0.042251598089933395)

  
  ***********************拼音向量************************
    pinyin:wo
    (&#39;shei&#39;, 0.6129732131958008)(&#39;ta&#39;, 0.6081706285476685)
    (&#39;nin&#39;, 0.5819231867790222),(&#39;！&#39;, 0.5435523986816406)
    (&#39;……&#39;, 0.48428624868392944),(&#39;ai&#39;, 0.47832390666007996)
    (&#39;o&#39;, 0.4761071801185608),(&#39;。』&#39;, 0.4598163366317749)
    (&#39;...&#39;, 0.45207729935646057),(&#39;ni&#39;, 0.44975683093070984)
    pinyin:guo
    (&#39;dang&#39;, 0.3908974528312683),(&#39;yuan&#39;, 0.378823846578598)
    (&#39;zu&#39;, 0.35387369990348816),(&#39;hua&#39;, 0.3405681848526001)
    (&#39;zheng&#39;, 0.3355437219142914),(&#39;yi&#39;, 0.3333034813404083)
    (&#39;ren&#39;, 0.3194104731082916),(&#39;jun&#39;, 0.3187354505062103)
    (&#39;hui&#39;, 0.31342023611068726),(&#39;xin&#39;, 0.3096797466278076)

   
   ***********************词性向量************************
    word postag:a
    (&#39;d&#39;, 0.7203904986381531),(&#39;c&#39;, 0.6124969720840454)
    (&#39;v&#39;, 0.4963228106498718),(&#39;an&#39;, 0.4531499147415161)
    (&#39;uz&#39;, 0.4459834396839142),(&#39;ud&#39;, 0.42059916257858276)
    (&#39;r&#39;, 0.4090540111064911),(&#39;uj&#39;, 0.4061364233493805)
    (&#39;i&#39;, 0.38707998394966125),(&#39;l&#39;, 0.3551557660102844)
    word postag:n
    (&#39;b&#39;, 0.7030695676803589),(&#39;vn&#39;, 0.490166038274765)
    (&#39;p&#39;, 0.4858315885066986),(&#39;v&#39;, 0.4499088227748871)
    (&#39;nt&#39;, 0.44155171513557434),(&#39;f&#39;, 0.26609259843826294)
    (&#39;s&#39;, 0.2639649212360382),(&#39;l&#39;, 0.24365971982479095)
    (&#39;ns&#39;, 0.2278469204902649),(&#39;m&#39;, 0.202927365899086)
    
    ***********************词向量************************
    word:爱情
    (&#39;爱恋&#39;, 0.6931096315383911),(&#39;真爱&#39;, 0.6897798776626587)
    (&#39;婚姻&#39;, 0.6540514826774597),(&#39;浪漫爱情&#39;, 0.6535360813140869)
    (&#39;情感&#39;, 0.6501022577285767),(&#39;感情&#39;, 0.6403399705886841)
    (&#39;纯爱&#39;, 0.6394841074943542),(&#39;爱情故事&#39;, 0.6282097101211548)
    (&#39;校园爱情&#39;, 0.6078493595123291),(&#39;情爱&#39;, 0.5976818799972534)
    word:创新
    (&#39;技术创新&#39;, 0.7648976445198059),(&#39;不断创新&#39;, 0.7172579765319824)
    (&#39;创新型&#39;, 0.6573833227157593),(&#39;创新能力&#39;, 0.6533682942390442)
    (&#39;创新性&#39;, 0.6160774827003479),(&#39;革新&#39;, 0.6159394383430481)
    (&#39;人才培养&#39;, 0.6093565821647644),(&#39;开拓创新&#39;, 0.6015594601631165)
    (&#39;探索&#39;, 0.5987343788146973),(&#39;技术革新&#39;, 0.5949685573577881)
</code></pre></div><h2 id="关于作者">关于作者</h2>
<p>老刘，刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。</p>
<p>就职于360人工智能研究院、曾就职于中国科学院软件研究所。</p>
<p><strong>老刘说NLP</strong>，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Wordify | 发现和区分消费者词汇的工具</title>
      <link>https://textdata.cn/blog/jcr_wordify/</link>
      <pubDate>Sun, 15 May 2022 10:43:10 +0600</pubDate>
      
      <guid>/blog/jcr_wordify/</guid>
      <description>工具介绍； 在不同上下文中，消费者的单词使用如何变化</description>
      <content:encoded><![CDATA[<p>JCR2021一篇软件介绍，支持中英文在内的多种语言。</p>
<blockquote>
<p>Hovy, D., Melumad, S. and Inman, J.J., 2021. Wordify: a tool for discovering and differentiating consumer vocabularies. <em>Journal of Consumer Research</em>, <em>48</em>(3), pp.394-414.</p>
</blockquote>
<br>
<h2 id="摘要">摘要:</h2>
<p>这项工作介绍了一个免费易用的在线文本分析工具<strong>Wordify</strong>，用于了解 <strong>“在不同上下文中，消费者的单词使用如何变化”</strong>。Wordify 使用  <strong>随机逻辑回归</strong>  (RLR) 来识别最能区分来自不同预分类文本的用词差异，例如男性与女性撰写的帖子用词差异，或好评与差评的用词差异。我们提供了说明性示例，以展示该工具如何用于多种用途，例如 (1) 揭示消费者在智能手机和 PC 上撰写评论时使用的独特词汇，(2) 发现推文中使用的词语在假定的支持者和反对有争议的广告，以及 (3) 扩展基于字典的情绪测量工具的字典。我们凭经验表明，Wordify 的 RLR 算法在区分词汇方面比支持向量机和卡方选择器表现更好，同时在计算时间上具有显着优势。还讨论了 Wordify 与其他文本分析工具的结合使用，例如概率主题建模和情感分析，以更深入地了解语言在消费者行为中的作用。</p>
<p><strong>关键词</strong>：文本分析，自然语言处理，语言，情感分析</p>
<br>
<h2 id="本地wordify配置">本地wordify配置</h2>
<p>作者在github公开了wordify的代码，仓库地址 <a href="https://github.com/MilaNLProc/wordify-webapp-streamlit">https://github.com/MilaNLProc/wordify-webapp-streamlit</a></p>
<p>大致的使用步骤</p>
<ol>
<li>wordify要配置spacy语言模型，配置方法参照以前分享的<a href="https://textdata.cn/blog/spacy_industry_application/">spacy产业级自然语言处理包</a></li>
<li>到github仓库下载代码，解压至桌面</li>
<li>打开命令行, 执行命令<code>cd desktop/wordify-webapp-streamlit-main</code></li>
<li>命令行执行<code>pip3 install -r requirements.txt</code></li>
<li>命令行执行<code>streamlit run app.py</code>， 此时命令行中出现本地服务地址(类似于网站)，浏览器打开这个地址即可</li>
</ol>
<p><strong>本地配置比较有难度，建议使用在线版https://wordify.unibocconi.it/</strong></p>
<br>
<h2 id="在线展示网站">在线展示网站</h2>
<h3 id="网址">网址</h3>
<p><a href="https://wordify.unibocconi.it/">https://wordify.unibocconi.it/</a></p>
<p><img loading="lazy" src="img/wordify_streamilit_website.png" alt=""  />
</p>
<h3 id="使用方法">使用方法</h3>
<ul>
<li>
<p>表格文件需含两个字段名，分别为<strong>text</strong>和<strong>label</strong>, 中文数据需要先为用空格间隔词语的文本样式。<a href="test_chinese.xlsx">中文样例文件</a></p>
</li>
<li>
<p>表格文件支持csv、xlsx、tsv、parquet，10M以内。数据上传成功后，页面会发生变化</p>
</li>
<li>
<p>在线页面在运行时一定不要刷新，这样会中断数据分析的过程</p>
</li>
<li>
<p>Wordify 的性能取决于文件中各个文本的长度。</p>
</li>
</ul>
<p><img loading="lazy" src="img/wordify_chinese_process.png" alt=""  />
</p>
<br>
<h2 id="点评">点评</h2>
<p>以往的文本分析思路，大多无视<strong>混杂效应Confound</strong>，主要从文本中抽取一些变量，如情感值，用于后期计量建模，试图挖掘<strong>文本指标(如情感值)<strong>与</strong>Outcome</strong>之间的因果关系。</p>
<blockquote>
<p>混杂效应，例如研究推文正负面情感对网友点击行为的影响。</p>
<p>研究的机制可以简化为<strong>不同的文本情感&ndash;&gt;产生不同的网友点击</strong></p>
<p>但是有可能不全是情感影响了网友的点击，作者存在性别差异，女性比男性更容易表达积极文本信息，而且在互联网世界女性比男性可能更有吸引力。</p>
</blockquote>
<p>论文中没提及Confound效应，但粗略浏览下，wordify创新地考虑了confound场景，通过文本分析，看看不同群体用词的差异。</p>
<p>wordify的缺点本地版配置太难，网页版运行太慢。0.6M的中文数据，我等了20min，还是没有跑出结果，果断关闭在线网页。总之感觉没有文中说的那么易用，门槛还是太高了。有耐心的朋友，如果感兴趣，可以去试试。</p>
<p>如果研究考虑文本的confound效应，可以参考<strong>causalnlp包</strong>，虽然配置难，但是运行速度还是有保证。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 |  Python文本分析包更新</title>
      <link>https://textdata.cn/blog/cntext_tutorial/</link>
      <pubDate>Mon, 09 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_tutorial/</guid>
      <description>扩展词典、情感分析、可阅读性，内置9种情感词典，涵盖中英文</description>
      <content:encoded><![CDATA[<p><a href="https://github.com/hidadeng/cntext"><img loading="lazy" src="https://img.shields.io/badge/cntext-%e4%b8%ad%e6%96%87%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%ba%93-orange?style=for-the-badge&amp;logo=appveyor" alt=""  />
</a></p>
<p><a href="version1.2.md">旧版cntext入口</a></p>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> <strong>stats</strong>  文本统计指标
<ul>
<li><input checked="" disabled="" type="checkbox"> 词频统计</li>
<li><input checked="" disabled="" type="checkbox"> 可读性</li>
<li><input checked="" disabled="" type="checkbox"> 内置pkl词典</li>
<li><input checked="" disabled="" type="checkbox"> <strong>情感分析</strong></li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>dictionary</strong> 构建词表(典)
<ul>
<li><input checked="" disabled="" type="checkbox"> Sopmi 互信息扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> W2Vmodels 词向量扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> Glove Glove词嵌入模型</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>similarity</strong>   文本相似度
<ul>
<li><input checked="" disabled="" type="checkbox"> cos相似度</li>
<li><input checked="" disabled="" type="checkbox"> jaccard相似度</li>
<li><input checked="" disabled="" type="checkbox"> 编辑距离相似度</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>mind.py</strong> 计算文本中的认知方向（态度、偏见）</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="cntext_examples.zip">click to download</a></p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext
</code></pre></div><br>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">mind</span>
    <span class="nx">dictionary</span>
    <span class="nx">similarity</span>
    <span class="nx">stats</span>
</code></pre></div><br>
<h2 id="一stats">一、stats</h2>
<p>目前stats内置的函数有</p>
<ul>
<li><strong>readability</strong>  文本可读性</li>
<li><strong>term_freq</strong> 词频统计函数</li>
<li><strong>dict_pkl_list</strong>  获取cntext内置词典列表(pkl格式)</li>
<li><strong>load_pkl_dict</strong> 导入pkl词典文件</li>
<li><strong>sentiment</strong> 情感分析</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="11--readability">1.1  readability</h3>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p>readability(text, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text: 文本字符串数据</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p><strong>中文可读性</strong> 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>


<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 28.0,
 &#39;readability2&#39;: 0.15789473684210525,
 &#39;readability3&#39;: 14.078947368421053}
</code></pre></div><br>
<p>句子中的符号变更会影响结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.16666666666666666,
 &#39;readability3&#39;: 13.583333333333334}
</code></pre></div><p><br><br></p>
<h3 id="12--term_freq">1.2  term_freq</h3>
<p>词频统计函数，返回Counter类型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="13-dict_pkl_list">1.3 dict_pkl_list</h3>
<p>获取cntext内置词典列表(pkl格式)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取cntext内置词典列表(pkl格式)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;, 
 &#39;concreteness.pkl&#39;]
</code></pre></div><p>词典对应关系, 部分情感词典资料整理自 <a href="https://github.com/quanteda/quanteda.sentiment">quanteda.sentiment</a></p>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>DUTIR.pkl</td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td>HOWNET.pkl</td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>sentiws.pkl</td>
<td>SentimentWortschatz (SentiWS)</td>
<td>英文</td>
<td>正面词、负面词；<br>效价</td>
</tr>
<tr>
<td>ChineseFinancialFormalUnformalSentiment.pkl</td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td>ANEW.pkl</td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>词语效价信息</td>
</tr>
<tr>
<td>LSD2015.pkl</td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>NRC.pkl</td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td>geninqposneg.pkl</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HuLiu.pkl</td>
<td>Hu&amp;Liu (2004)正、负情感词典</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>AFINN.pkl</td>
<td>尼尔森 (2011) 的“新 ANEW”效价词表</td>
<td>英文</td>
<td>情感效价信息valence</td>
</tr>
<tr>
<td>LoughranMcDonald.pkl</td>
<td>会计金融LM词典</td>
<td>英文</td>
<td>金融领域正、负面情感词</td>
</tr>
<tr>
<td>ADV_CONJ.pkl</td>
<td>副词连词</td>
<td>中文</td>
<td></td>
</tr>
<tr>
<td>STOPWORDS.pkl</td>
<td></td>
<td>中、英</td>
<td>停用词</td>
</tr>
<tr>
<td>concreteness.pkl</td>
<td>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</td>
<td>English</td>
<td>word &amp; concreateness score</td>
</tr>
</tbody>
</table>
<h3 id="注意">注意:</h3>
<ul>
<li>
<p>如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</li>
<li>
<p>如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue</p>
</li>
</ul>
<br>
<h3 id="14-load_pkl_dict">1.4 load_pkl_dict</h3>
<p>导入pkl词典文件，返回字典样式数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入pkl词典文件,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, ...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...], 
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;, &#39;刿目怵心&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;, &#39;喜糖&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,]
           }
</code></pre></div><br>
<h3 id="15-sentiment">1.5 sentiment</h3>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)
使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>diction:  情感词字典；</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">],</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p>如果不适用pkl词典，可以自定义自己的词典，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<h3 id="16-sentiment_by_valence">1.6 sentiment_by_valence</h3>
<p>sentiment函数默认所有情感词权重均为1，只需要统计文本中情感词的个数，即可得到文本情感得分。</p>
<p>sentiment_by_valence(text, diction, lang=&lsquo;english&rsquo;)函数考虑了词语的效价(valence)</p>
<ul>
<li>text 待输入文本</li>
<li>diction 带效价的词典，DataFrame格式。</li>
<li>lang 语言类型&rsquo;chinese' 或 &lsquo;english&rsquo;，默认&rsquo;english'</li>
</ul>
<p>这里我们以文本具体性度量为例， <strong>concreteness.pkl</strong> 整理自 Brysbaert2014的文章。</p>
<blockquote>
<p>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># load the concreteness.pkl dictionary file</span>
<span class="n">concreteness_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;concreteness.pkl&#39;</span><span class="p">)</span>
<span class="n">concreteness_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">word</th>
<th style="text-align:right">valence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">roadsweeper</td>
<td style="text-align:right">4.85</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">traindriver</td>
<td style="text-align:right">4.54</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">tush</td>
<td style="text-align:right">4.45</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">hairdress</td>
<td style="text-align:right">3.93</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">pharmaceutics</td>
<td style="text-align:right">3.77</td>
</tr>
</tbody>
</table>
<br>
<p>先看一条文本的具体性度量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">reply</span> <span class="o">=</span> <span class="s2">&#34;I&#39;ll go look for that&#34;</span>

<span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                              <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                              <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">score</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1.85
</code></pre></div><br>
<p>很多条文本的具体性度量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">employee_replys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I&#39;ll go look for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that top&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go look for that t-shirt in grey&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt in grey&#34;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">reply</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">employee_replys</span><span class="p">):</span>
    <span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                                  <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    
    <span class="n">template</span> <span class="o">=</span> <span class="s2">&#34;Concreteness Score: </span><span class="si">{score:.2f}</span><span class="s2"> | Example-</span><span class="si">{idx}</span><span class="s2">: </span><span class="si">{exmaple}</span><span class="s2">&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="n">score</span><span class="p">,</span> 
                          <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> 
                          <span class="n">exmaple</span><span class="o">=</span><span class="n">reply</span><span class="p">))</span>
    
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Concreteness Score: 1.55 | Example-0: I&#39;ll go look for that
Concreteness Score: 1.55 | Example-1: I&#39;ll go search for that
Concreteness Score: 1.89 | Example-2: I&#39;ll go search for that top
Concreteness Score: 2.04 | Example-3: I&#39;ll go search for that t-shirt
Concreteness Score: 2.37 | Example-4: I&#39;ll go look for that t-shirt in grey
Concreteness Score: 2.37 | Example-5: I&#39;ll go search for that t-shirt in grey
</code></pre></div><br>
<p><br><br></p>
<h2 id="二dictionary">二、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="21-sopmi">2.1 SoPmi</h3>
<p>SoPmi 共现法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                   <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                   <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                   <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span>
                   <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 44.49 s
</code></pre></div><br>
<h3 id="22-w2vmodels">2.2 W2VModels</h3>
<p>W2VModels 词向量</p>
<p><strong>特别要注意代码需要设定lang语言参数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   174 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s

</code></pre></div><br>
<h3 id="需要注意">需要注意</h3>
<p>训练出的w2v模型可以后续中使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">model路径</span><span class="p">)</span>
<span class="c1">#找出word的词向量</span>
<span class="c1">#w2v_model.get_vector(word)</span>
<span class="c1">#更多w2_model方法查看</span>
<span class="c1">#help(w2_model)</span>
</code></pre></div><p>例如本代码，运行生成的结果路径<code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;technology&#39;, 0.689210832118988),
 (&#39;infrastructure&#39;, 0.669672966003418),
 (&#39;resources&#39;, 0.6695448160171509),
 (&#39;talent&#39;, 0.6627111434936523),
 (&#39;execution&#39;, 0.6549549102783203),
 (&#39;marketing&#39;, 0.6533523797988892),
 (&#39;merchandising&#39;, 0.6504817008972168),
 (&#39;diversification&#39;, 0.6479553580284119),
 (&#39;expertise&#39;, 0.6446896195411682),
 (&#39;digital&#39;, 0.6326863765716553)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.45616838, -0.7799563 ,  0.56367606, -0.8570078 ,  0.600359  ,
       -0.6588043 ,  0.31116748, -0.11956959, -0.47599426,  0.21840936,
       -0.02268819,  0.1832016 ,  0.24452794,  0.01084935, -1.4213187 ,
        0.22840202,  0.46387577,  1.198386  , -0.621511  , -0.51598716,
        0.13352732,  0.04140598, -0.23470387,  0.6402956 ,  0.20394802,
        0.10799981,  0.24908689, -1.0117126 , -2.3168423 , -0.0402851 ,
        1.6886286 ,  0.5357047 ,  0.22932841, -0.6094084 ,  0.4515793 ,
       -0.5900931 ,  1.8684244 , -0.21056202,  0.29313338, -0.221067  ,
       -0.9535679 ,  0.07325   , -0.15823542,  1.1477109 ,  0.6716076 ,
       -1.0096023 ,  0.10605699,  1.4148282 ,  0.24576302,  0.5740349 ,
        0.19984631,  0.53964925,  0.41962907,  0.41497853, -1.0322098 ,
        0.01090925,  0.54345983,  0.806317  ,  0.31737605, -0.7965337 ,
        0.9282971 , -0.8775608 , -0.26852605, -0.06743863,  0.42815775,
       -0.11774074, -0.17956367,  0.88813037, -0.46279573, -1.0841943 ,
       -0.06798118,  0.4493006 ,  0.71962464, -0.02876493,  1.0282255 ,
       -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922,
       -0.16753489,  0.14065655, -1.8609751 ,  0.03587054,  1.2792674 ,
        1.2732009 , -0.74120265, -0.98000383,  0.4521185 , -0.26387128,
        0.37045383,  0.3680011 ,  0.7197629 , -0.3570571 ,  0.8016917 ,
        0.39243212, -0.5027844 , -1.2106236 ,  0.6412354 , -0.878307  ],
      dtype=float32)
</code></pre></div><p><br><br></p>
<h3 id="23-co_occurrence_matrix">2.3 co_occurrence_matrix</h3>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence2.png" alt=""  />
</p>
<p><br><br></p>
<h3 id="24--glove">2.4  Glove</h3>
<p>构建Glove词嵌入模型，使用英文数据<code>data/brown_corpus.txt</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Glove</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">create_vocab</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/brown_corpus.txt&#39;</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">cooccurrence_matrix</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_embeddings</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4: ...Create vocabulary for Glove.
Step 2/4: ...Create cooccurrence matrix.
Step 3/4: ...Train glove embeddings. 
             Note, this part takes a long time to run
Step 3/4: ... Finish! Use 175.98 s
</code></pre></div><p>生成的Glove词嵌入文件位于<code>output/Glove</code> 。</p>
<p><br><br></p>
<h2 id="三similarity">三、similarity</h2>
<p>四种相似度计算函数</p>
<ul>
<li>cosine_sim(text1, text2)  cos余弦相似</li>
<li>jaccard_sim(text1, text2)     jaccard相似</li>
<li>minedit_sim(text1, text2)  最小编辑距离相似度；</li>
<li>simple_sim(text1, text2) 更改变动算法</li>
</ul>
<p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩啊&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.82
0.67
2.00
0.87
</code></pre></div><p><br><br></p>
<h2 id="四text2mind">四、Text2Mind</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<p>- <strong>多个对象在某概念的远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
<p>- 多个对象在某<strong>概念的分量(fen，一声)的多少</strong>， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
<p>这两种认知分别可以用向量距离、向量语义投影计算得来。</p>
<ul>
<li>tm.sematic_distance(words, c_words1, c_words2)  向量距离</li>
<li>tm.sematic_projection(words, c_words1, c_words2)  向量语义投影</li>
</ul>
<h3 id="41-tmsematic_distancewords-c_words1-c_words2">4.1 tm.sematic_distance(words, c_words1, c_words2)</h3>
<p>分别计算words与c_words1、c_words2语义距离，返回距离差值。</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果d1-d2&lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<p><strong>下载glove_w2v.6B.100d.txt</strong>链接: <a href="https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw">https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw</a> 提取码: 72l0</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#Note: this is a word2vec format model</span>
<span class="n">tm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Text2Mind</span><span class="p">(</span><span class="n">w2v_model_path</span><span class="o">=</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="n">engineers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">mans</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">womans</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">engineers</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">mans</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">womans</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><p>-0.38 意味着工程师更接近于男人，而不是女人。</p>
<br>
<h3 id="42-tmsematic_projectionwords-c_words1-c_words2">4.2 tm.sematic_projection(words, c_words1, c_words2)</h3>
<p><strong>语义投影</strong>，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，word越位于c_words2一侧。</p>
<p>下图是语义投影示例图，本文算法和图片均来自 &ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&rdquo;</p>
<p><img loading="lazy" src="img/Nature_Semantic_projection_recovering_human_knowledge_of.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">smalls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">bigs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">smalls</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">bigs</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p>在这几个动物尺寸的感知上，人类觉得老鼠体型是最小，马的体型是最大。</p>
<p><br><br></p>
<h2 id="引用说明">引用说明</h2>
<p>如果研究中使用cntext，请使用以下格式进行引用</p>
<h3 id="apalike">apalike</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Deng X., Nan P. (2022). cntext: a Python tool for text mining (version 1.7.9). DOI: 10.5281/zenodo.7063523 URL: https://github.com/hiDaDeng/cntext
</code></pre></div><h3 id="bibtex">bibtex</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@misc{YourReferenceHere,
author = {Deng, Xudong and Nan, Peng},
doi = {10.5281/zenodo.7063523},
month = {9},
title = {cntext: a Python tool for text mining},
url = {https://github.com/hiDaDeng/cntext},
year = {2022}
}
</code></pre></div><h3 id="endnote">endnote</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">%0 Generic
%A Deng, Xudong
%A Nan, Peng
%D 2022
%K text mining
%K text analysi
%K social science
%K management science
%K semantic analysis
%R 10.5281/zenodo.7063523
%T cntext: a Python tool for text mining
%U https://github.com/hiDaDeng/cntext
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>sentence-transformer库 | 句子语义向量化</title>
      <link>https://textdata.cn/blog/sentence-transformer-tutorial/</link>
      <pubDate>Mon, 09 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/sentence-transformer-tutorial/</guid>
      <description>使用sentence-transformer库BERT技术，将句子语义向量化</description>
      <content:encoded><![CDATA[<blockquote>
<p>内容摘自</p>
<p>刘焕勇博客: <a href="https://liuhuanyong.github.io/">https://liuhuanyong.github.io/</a></p>
<p>原文地址: <a href="https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q">https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q</a></p>
<p>公众号: 老刘说NLP</p>
</blockquote>
<p>word2vec、glove是两种静态的词向量模型，即每个词语只有一个固定的向量表示。但在不同语境中，词语的语义会发生变化，按道理词向量也应该动态调整。相比word2vec、glove生成的静态词向量， BERT是一种动态的技术，可以根据上下文情景，得到语义变化的词向量。</p>
<p>HuggingFace网站提供了简易可用的数据集、丰富的预训练语言模型， 通过sentence-transformer库，我们可以使用HuggingFace内的预训练模型，得到不同情景的文本的语义向量。</p>
<p>HuggingFace网站  <a href="https://huggingface.co/">https://huggingface.co/</a></p>
<p><img loading="lazy" src="img/HuggingFace.png" alt=""  />
</p>
<br>
<h2 id="动态句向量">动态句向量</h2>
<p>sentence-transformer框架提供了一种简便的方法来计算句子和段落的向量表示（也称为句子嵌入）</p>
<p><img loading="lazy" src="img/sentence-transformer.png" alt=""  />
</p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">sentence</span><span class="o">-</span><span class="n">transformers</span>
</code></pre></div><br>
<h2 id="代码">代码</h2>
<p><a href="sentence-transformer-tutorial.zip">click to download the code</a></p>
<p>使用huggingface中的distiluse-base-multilingual-cased与训练模型，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">util</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;distiluse-base-multilingual-cased&#39;</span><span class="p">)</span>
</code></pre></div><p>第一次运行上方的代码，需要运行一定的时间用于下载。下载完成后，我们使用同种语义的中英文句子，分别计算得到emb1和emb2两个句向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">emb1 = model.encode(&#39;Natural language processing is a hard task for human&#39;)

emb2 = model.encode(&#39;自然语言处理对于人类来说是个困难的任务&#39;)
emb1
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 2.58186590e-02,  4.65703346e-02,  4.25276496e-02, -1.67875513e-02,
        5.56012690e-02, -3.44308838e-02, -6.53978735e-02,  1.77450478e-02,
       -3.47155109e-02,  2.86140274e-02,  2.48657260e-02,  7.94188876e-04,
        5.09755425e-02, -1.76107027e-02, -1.04308855e-02,  7.61642214e-03,
        ...
        4.28482369e-02,  1.76657233e-02, -5.83355911e-02,  1.92921527e-03,
        2.81221420e-02,  5.24400780e-03,  2.10703332e-02,  7.96715263e-03,
       -6.80630878e-02, -2.05304120e-02, -2.43293475e-02, -1.87458862e-02],
      dtype=float32)
</code></pre></div><p>在distiluse-base-multilingual-cased这种模型中， 不同语言的同义句应该具有类似的语义，那么cos相似度应该是很大的。越接近于1越相似；越接近于0，越不相似。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">cos_sim</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">pytorch_cos_sim</span><span class="p">(</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb2</span><span class="p">)</span>
<span class="n">cos_sim</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[0.8960]])
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>SimpleTransformers库 | 使用BERT实现文本向量化</title>
      <link>https://textdata.cn/blog/simple_transformer/</link>
      <pubDate>Thu, 05 May 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/simple_transformer/</guid>
      <description>基于BERT预训练模型，对文本进行向量化</description>
      <content:encoded><![CDATA[<p><code>Simple Transformers</code> 库基于 HuggingFace 的 <a href="https://github.com/huggingface/transformers">Transformers</a> 库，可让您快速训练和评估 Transformer 模型， <strong>初始化</strong>、<strong>训练</strong>和<strong>评估</strong>模型只需要 3 行代码。</p>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install simpletransformers
</code></pre></div><p><strong>Simple Transformer</strong> 模型在构建时考虑了特定的自然语言处理 (NLP) 任务。 每个这样的模型都配备了旨在最适合它们打算执行的任务的特性和功能。 使用 Simple Transformers 模型的高级过程遵循相同的模式。</p>
<ol>
<li>初始化一个特定于任务的模型
2.用<code>train_model()</code>训练模型</li>
<li>使用 <code>eval_model()</code> 评估模型</li>
<li>使用 <code>predict()</code> 对（未标记的）数据进行预测</li>
</ol>
<p>但是，不同模型之间存在必要的差异，以确保它们非常适合其预期任务。 关键差异通常是输入/输出数据格式和任何任务特定功能/配置选项的差异。 这些都可以在每个任务的文档部分中找到。</p>
<p>当前实现的特定于任务的“Simple Transformer”模型及其任务如下所示。</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Model</th>
</tr>
</thead>
<tbody>
<tr>
<td>Binary and multi-class text classification文本二分类、多分类</td>
<td><code>ClassificationModel</code></td>
</tr>
<tr>
<td>Conversational AI (chatbot training)对话机器人训练</td>
<td><code>ConvAIModel</code></td>
</tr>
<tr>
<td>Language generation语言生成</td>
<td><code>LanguageGenerationModel</code></td>
</tr>
<tr>
<td>Language model training/fine-tuning语言模型训练、微调</td>
<td><code>LanguageModelingModel</code></td>
</tr>
<tr>
<td>Multi-label text classification多类别文本分类</td>
<td><code>MultiLabelClassificationModel</code></td>
</tr>
<tr>
<td>Multi-modal classification (text and image data combined)多模态分类</td>
<td><code>MultiModalClassificationModel</code></td>
</tr>
<tr>
<td>Named entity recognition命名实体识别</td>
<td><code>NERModel</code></td>
</tr>
<tr>
<td>Question answering问答</td>
<td><code>QuestionAnsweringModel</code></td>
</tr>
<tr>
<td>Regression回归</td>
<td><code>ClassificationModel</code></td>
</tr>
<tr>
<td>Sentence-pair classification句对分类</td>
<td><code>ClassificationModel</code></td>
</tr>
<tr>
<td><strong>Text Representation Generation文本表征生成</strong></td>
<td><strong>RepresentationModel</strong></td>
</tr>
<tr>
<td>Document Retrieval文档抽取</td>
<td><code>RetrievalModel</code></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>有关如何使用这些模型的更多信息，请参阅 <a href="https://simpletransformers.ai/">docs</a> 中的相关部分。</strong></li>
<li>示例脚本可以在 <a href="https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples">examples</a> 目录中找到。</li>
<li>有关项目的最新更改，请参阅 <a href="https://github.com/ThilinaRajapakse/simpletransformers/blob/master/CHANGELOG.md">Changelog</a>。</li>
</ul>
<h2 id="生成句子嵌入">生成句子嵌入</h2>
<p>使用huggingface网站https://huggingface.co/ 提供的模型</p>
<ul>
<li>英文模型 bert-base-uncased</li>
<li>中文模型 bert-base-chinese</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">simpletransformers.language_representation</span> <span class="kn">import</span> <span class="n">RepresentationModel</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Machine Learning and Deep Learning are part of AI&#34;</span><span class="p">,</span> 
             <span class="s2">&#34;Data Science will excel in future&#34;</span><span class="p">]</span> <span class="c1">#it should always be a list</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">RepresentationModel</span><span class="p">(</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&#34;bert&#34;</span><span class="p">,</span>
        <span class="n">model_name</span><span class="o">=</span><span class="s2">&#34;bert-base-uncased&#34;</span><span class="p">,</span> <span class="c1">#英文模型</span>
        <span class="n">use_cuda</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">sentence_vectors</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_sentences</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">combine_strategy</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sentence_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_vectors</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(2, 768)

array([[-0.10800573,  0.19615649, -0.10756102, ..., -0.26362818,
         0.56403756, -0.30985302],
       [ 0.0201617 , -0.19381572,  0.4360792 , ..., -0.2979438 ,
         0.04984972, -0.702381  ]], dtype=float32)
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 从符号到嵌入：计算社会科学的两种文本表示</title>
      <link>https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/</link>
      <pubDate>Mon, 25 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/from_sysbol_to_embeddings_in_computational_social_science/</guid>
      <description>如何有效地表示数据以挖掘我们想要的计算社会科学的含义？为了探索答案，我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示。How to efficiently represent data to mine the implications we want for computational social science? To explore the answer, we conduct a thorough review of data representations for text and the web in CSS, and we summarize existing representations into two schemes, symbol-based and embedding-based</description>
      <content:encoded><![CDATA[<p>B站看到大牛刘知远关于文本分析在计算社会科学领域应用的分享，解答了我对文本表示的疑惑，看完了能对文本的特征工程加深理解，同时也能更清晰未来如何借助计算机科学技术开展社会科学研究。</p>
<blockquote>
<p><strong>全文摘抄自</strong></p>
<p>Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.</p>
</blockquote>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1qi4y1Q7qj&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<h2 id="摘要">摘要</h2>
<p><strong>计算社会科学</strong>（CSS），旨在利用计算方法来解决社会科学问题，是一个新兴和快速发展的领域。 CSS 的研究是数据驱动的，并且显着受益于在线用户生成内容和社交网络的可用性，其中包含用于调查的富文本和网络数据。然而，这些大规模、多模态的数据也给研究人员带来了很大的挑战：<strong>如何有效地表示数据以挖掘我们想要的 CSS 含义</strong>？为了探索答案，<strong>我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示</strong>，并为每个方案介绍了一系列典型的方法。随后，我们基于对来自 6 个涉及 CSS 的顶级场所的 400 多篇研究文章的调查，展示了上述表示的应用。从这些应用程序的统计数据中，<strong>我们挖掘出每种表示的强度，并发现基于嵌入的表示在过去十年中出现并获得越来越多的关注的趋势</strong>。最后，我们讨论了几个关键挑战和未来方向的开放性问题。本调查旨在为 CSS 研究人员提供对数据表示的更深入理解和更明智的应用。</p>
<p><strong>关键词</strong>：计算社会科学；基于符号的表示；基于嵌入的表示；社交网络</p>
<br>
<h2 id="一计算社会学数据分析流程">一、计算社会学数据分析流程</h2>
<p>其中第二步，数据表示目前有两大类表示(特征工程)方法</p>
<ul>
<li><strong>基于符号的文本表示</strong>；符号可以是单词(或词组)，也可以是概念(如正面情感、负面情感)</li>
<li><strong>基于嵌入(分布式)的文本表示</strong>；相比于符号法，将词(词组)看做一个点。嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(king)-v(queen)约等于v(man)-v(woman)</li>
</ul>
<p><img loading="lazy" src="img/fig1.png" alt=""  />
</p>
<br>
<h2 id="二基于符号的文本表示">二、基于符号的文本表示</h2>
<p>基于符号的文本表示一般来说默认词语是不可分的符号，每个词能根据词频统计出现次数的多与少，或是否存在。</p>
<h3 id="21-词语层面">2.1 词语层面</h3>
<ul>
<li>
<p>基于词频表示</p>
<ul>
<li>是否出现，出现标位1，反之标位0。</li>
<li>出现多少，词语出现几次，标为几个。</li>
</ul>
</li>
<li>
<p>基于特征表示，如每个词带有权重(得分)</p>
</li>
<li>
<p>基于网络表示，如词语共现网络(矩阵)</p>
</li>
</ul>
<h3 id="22-句子层面">2.2 句子层面</h3>
<ul>
<li>
<p>基于词频的表示</p>
<ul>
<li>one-hot 将文本转为向量，向量中每个数，词语出现标位1，反之标位0</li>
<li>bag-of-words，将文本转为向量，向量中每个数，词语出现n次标记为n</li>
<li>n-grams，对词组的处理，将词组看做一个单词(整体)。</li>
<li>Tf-Idf ,该算法分为tf和idf两部分。其中tf与bag-of-words类似，考虑词语出现次数。而idf还考虑词语在语料中出现场景的稀缺性程度。</li>
</ul>
</li>
<li>
<p>基于语法特征，如句法依存关系，类似于英语语法，将句子分为主谓宾、动词、名词等。</p>
</li>
<li>
<p>词典法，如使用正、负情感词典，对文本数据进行情感分析，可以得到pos和neg的各自得分</p>
</li>
</ul>
<p><img loading="lazy" src="img/fig2.png" alt=""  />
</p>
<br>
<h2 id="三基于嵌入的文本表示">三、基于嵌入的文本表示</h2>
<h3 id="31词语层面">3.1词语层面</h3>
<p>嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(best)-v(good)约等于v(worst)-v(bad)</p>
<h3 id="32-句子层面">3.2 句子层面</h3>
<p>词语是向量，那么由词语组成的句子也会加权得到一个向量。含有相似话题或含义相近的句子在多维向量空间中会比较接近。</p>
<p><img loading="lazy" src="img/fig7.png" alt=""  />
</p>
<br>
<h2 id="四任务分类文本的用法">四、任务分类：文本的用法</h2>
<p><img loading="lazy" src="img/fig16.png" alt=""  />
</p>
<p>有了文本数据，刚刚解决了如何表示文本。接下来，需要明确，我们使用文本目的是为了做哪类分析，得到哪些信息。有8种常见的文本分析图式</p>
<ul>
<li>描述性。如随时间推移，词频的发展趋势是变大的</li>
<li>相关性。</li>
<li>聚类。如lda话题分析、k-means聚类</li>
<li>相似度。两个文档转为向量后，可以通过cosine计算相似度</li>
<li>分类。机器学习分类，判断某文本隶属于哪个类别</li>
<li>回归。例如根据文本，判断某件事发生的概率</li>
<li>语言模型。</li>
<li>排序。</li>
</ul>
<br>
<h2 id="五发文趋势-符号vs嵌入">五、发文趋势-符号vs嵌入</h2>
<p>基于上一节中对应用程序的介绍，可以观察到基于符号和基于嵌入的表示在 <strong>计算社会科学</strong>中都得到了相当大的采用。为了明确研究它们的覆盖范围，我们计算了每年使用两种表示中的一种或两种的作品数量，如图 17 所示。通过比较nature、science、pnas三大顶级期刊，我们可以发现使用<strong>基于嵌入表示</strong>的文章比例在过去几年中逐渐。这表明越来越多的 计算社会科学文章 已经考虑并受益于基于嵌入表示。</p>
<p>图 18 显示了在 计算机领域ACL、WWW 和 KDD 的会议上中，发现使用基于嵌入的表示的文章数量已大大超过使用基于符号的表示的文章数量。然而，与图 17 相比，计算机科学会议中基于嵌入的表示的数量与三个多学科期刊之间存在很大差距。</p>
<p><img loading="lazy" src="img/3_top_journals.png" alt=""  />
</p>
<p><img loading="lazy" src="img/nlp.png" alt=""  />
</p>
<p>总而言之，在过去十年中，基于嵌入的表示已经出现并在 计算社会科学 中发挥着越来越重要的作用。</p>
<br>
<h2 id="六趋势解读">六、趋势解读</h2>
<p>基于它们的内部机制和现有应用，对趋势解读，我们总结出以下三个关键点。</p>
<p>基于符号的表示因其明确性和可解释性而擅长描述和关系的任务。</p>
<p>基于符号的表示中的每个值都表示一定的人类可读的含义，因此我们可以直接使用它来观察数据的分布，以及提取对象之间的关系。例如，基于频率的词表示用于观察文化变化并捕捉新闻中提及次数与公司股票交易量之间的关系。虽然基于主题模型的表示和一些基于神经的表示在一定程度上具有实际意义，但它们对于社会科学研究人员来说仍然是模糊的并且不那么引人注目。</p>
<p>由于神经网络具有强大的拟合数据和提取深度语义的能力，基于嵌入的表示在预测（例如分类和回归）和相似性任务中表现更好。一方面，神经网络通过大规模神经元的连接实现高效的输入输出映射功能。另一方面，通过多层网络的构建，实现深层语义和抽象概念的提取。现有研究表明，深层捕获相对于浅层更抽象的特征。诸如社会偏见和道德化之类的抽象概念都可以通过基于嵌入的表示来很好地衡量。虽然我们提到基于符号的表示可以通过一些定义的符号来代表抽象概念，但这种表示仍然是部分和肤浅的，很难捕捉到它们的全貌。</p>
<p>基于嵌入的表示需要更少的人力。基于符号的表示通常需要大量的专家知识来定义研究对象的特征，这是劳动密集型的。此外，对于一些没有充分特征的抽象概念或对象，它们的表现将受到限制。与它们不同的是，基于嵌入的表示是从数据中自动提取的，几乎不需要人工干预，甚至可以补充人类知识。例如，可以使用神经网络来自动恢复丢失的巴比伦文本，这即使对专家来说也是具有挑战性的。此外，基于嵌入的表示可以在没有手动定义的情况下描述语言的复杂性和歧义性。</p>
<br> 
<h2 id="七未来展望">七、未来展望</h2>
<p>尽管在过去十年中出现了从符号到嵌入的趋势，但仍有许多挑战和悬而未决的问题有待探索。展望未来，我们列出了一些与计算社会科学 中的数据表示相关的基本和潜在的未来方向。</p>
<p>预训练的语言模型。近年来，预训练的语言模型受到了相当大的关注，并在处理文本数据方面取得了巨大的成功 [100, 240]。这些模型从百科全书和书籍等海量文本数据中学习丰富的语义信息，仅在下游任务中进行微调以实现有效的基于嵌入的表示。因此，对于 计算社会科学，我们可以借助预训练的语言模型获得更通用、更健壮的文本表示。与从传统神经网络模型中学习的表示相比，这些表示不仅可以更广泛、更准确地从文本中分析社会现象，而且还可以减少那些需要大量标记数据的任务的人工注释。</p>
<p>图神经网络。通过消息传递机制，图神经网络 [461] 可以同时有效地对网络拓扑和节点/边缘特征（例如文本信息）进行建模，从而提供一个统一的框架来利用来自异构来源的信息。 计算社会科学 中的许多场景需要处理社交网络以及个人特征。因此，图神经网络技术在 计算社会科学 研究中具有很大的应用潜力，可以学习融合文本和网络信息的表示。事实上，计算机科学中的各种应用，例如自然语言处理 [418] 和推荐系统 [439]，已经采用图神经网络进行建模。</p>
<p>设计为预测和相似性。基于嵌入的表示以丰富和深层次的语义而闻名，而基于符号的表示通常保留在部分和浅层语义中。同时，基于嵌入的表示擅长预测和相似性的任务。因此，为了充分利用嵌入中的强语义，鼓励 计算社会科学 研究人员尽可能将研究问题设计为预测或相似性任务。例如，我们可以将社会偏见问题设计为性别词和中性词嵌入之间的相似性度量 [59, 133]。此外，人类语言的复杂性可以设计为一项预测任务，它以语言模型为指标查看单词或句子的预测概率[155]。</p>
<p>可解释性。诚然，基于嵌入的方法的一个缺点是缺乏可解释性。这个问题会损害与道德、安全或隐私相关的决策关键系统的应用。尽管嵌入模型，尤其是神经网络模型的可解释性尚未完全解决，但计算机科学领域的研究人员已经做出了一些努力，以提高基于神经模型的可解释性 [16]。因此，利用基于嵌入的模型和可解释性分析方法进行有效和（部分）可解释的预测将是一个有趣的方向。</p>
<br>
<h2 id="结论">结论</h2>
<p>计算社会科学作为一个新兴且有前途的跨学科领域，近年来吸引了相当多的研究兴趣。 计算社会科学 研究中广泛使用两种主要类型的数据，即文本数据和网络数据。在本次调查中，我们首先将数据表示总结为基于符号和基于嵌入的表示，并在构建这些表示时进一步介绍典型的方法。之后，我们基于来自 6 个经典期刊和会议的 400 多篇高被引文献，对这两类表示的应用进行了全面回顾。根据对这些应用的统计，发现了 计算社会科学 中基于嵌入的文本和网络表示正在出现和增长的趋势，我们进一步讨论了其中的原因。最后，我们提出了 计算社会科学 中的四个挑战和未解决的问题，它们是需要探索的基本和潜在方向。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>中文词向量资源汇总 &amp; 使用方法</title>
      <link>https://textdata.cn/blog/embeddings_resource_usage_method/</link>
      <pubDate>Thu, 21 Apr 2022 15:40:10 +0600</pubDate>
      
      <guid>/blog/embeddings_resource_usage_method/</guid>
      <description>数十种中文词向量模型资源下载&amp;amp;使用方法。Dozens of Chinese word vector model resource downloads &amp;amp; usage methods</description>
      <content:encoded><![CDATA[<br>
<h2 id="项目地址">项目地址</h2>
<p><a href="https://github.com/Embedding/Chinese-Word-Vectors">https://github.com/Embedding/Chinese-Word-Vectors</a></p>
<p>Chinese-Word-Vectors项目提供超过100种中文词向量，其中包括不同的表示方式（稠密SGNS和稀疏PPMI）、不同的上下文特征（词、N元组、字等等）、以及不同的训练语料。获取预训练词向量非常方便，下载后即可用于下游任务。</p>
<br>
<h2 id="参考文献">参考文献</h2>
<p>如果使用了本项目的词向量和CA8数据集请进行如下引用：</p>
<p>Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, <a href="http://aclweb.org/anthology/P18-2023"><em>Analogical Reasoning on Chinese Morphological and Semantic Relations</em></a>, ACL 2018.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@InProceedings{P18-2023,
  author =  &#34;Li, Shen
    and Zhao, Zhe
    and Hu, Renfen
    and Li, Wensi
    and Liu, Tao
    and Du, Xiaoyong&#34;,
  title =   &#34;Analogical Reasoning on Chinese Morphological and Semantic Relations&#34;,
  booktitle =   &#34;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&#34;,
  year =  &#34;2018&#34;,
  publisher =   &#34;Association for Computational Linguistics&#34;,
  pages =   &#34;138--143&#34;,
  location =  &#34;Melbourne, Australia&#34;,
  url =   &#34;http://aclweb.org/anthology/P18-2023&#34;
}
</code></pre></div><br>
<h3 id="不同领域">不同领域</h3>
<p>下列词向量基于不同的表示方式、不同的上下文特征以及不同领域的语料训练而成。</p>
<table align="center">
    <tr align="center">
        <td colspan="5"><b>Word2vec / Skip-Gram with Negative Sampling (SGNS)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1IiIbQGJ_AooTj5s8aZYcvA">300d</a> / PWD: 5555</td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1AmXYWVgkxrG4GokevPtNgA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZKePwxwsDdzNrfkc6WKdGQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZBVVD4mUSUuXOxlZ3V71ZA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19wQrclyynOnco3JBvnI5pA">300d</td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/19sqMz-JAhhxh3o6ecvQxQw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1upPkA8KJnxTZBfjuNDtaeQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1BvKk2QjbtQMch7EISppW2A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19Vso_k79FZb5OZCWQPAnFQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1tUghuTno5yOvOx4LXA9-wg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13yVrXeGYkxdGW3P6juiQmA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1pUqyn7mnPcUmzxT64gGpSw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1svFOwFBKnnlsqrF1t99Lnw">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/1EhtsbDa3ekzZPODWNLHcXA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FcPHv7S4vUgnL7WeWf4_PA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13CAxY5ffRFuOcHZu8VmArw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sqvrUtGBAZ7YWEsGz41DRQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VGOs0RH7DXE5vRrtw6boQA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1OQ6fQLCgqT43WTwh5fh_lg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1_xogqF9kJT6tmQHSAYrYeg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Fo27Lv_0nz8FXg-xbOz14Q">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/1zbuUJEEEpZRNHxZ7Gezzmw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/11PWBcvruXEDvKf2TiIXntg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/10bhJpaXMCUK02nHvRAttqA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FHl_bQkYucvVk-j2KG4dxA">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1ciq8iXtcrHpu3ir_VhK0zg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Oa4CkPd8o2xd6LEAaa4gmg">300d</a> / PWD: z5b4</td>
      <td><a href="https://pan.baidu.com/s/1IG8IxNp2s7vVklz-vyZR9A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1SEOKrJYS14HpqIaQT462kA">300d</a> / PWD: yenb</td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1vPSeUsSiWYXEWAuokLR0qQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sS9E7sclvS_UZcBgHN7xLQ">300d</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合<br>Baidu Netdisk / Google Drive</td>
      <td>
        <a href="https://pan.baidu.com/s/1luy-GlTdqqvJ3j-A4FcIOw">300d</a><br>
        <a href="https://drive.google.com/open?id=1Zh9ZCEu8_eSQ-qkYVQufQDNKPC4mtEKR">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1oJol-GaRMk4-8Ejpzxo6Gw">300d</a><br>
        <a href="https://drive.google.com/open?id=1WUU9LnoAjs--1E_WqcghLJ-Pp8bb38oS">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1DjIGENlhRbsVyHW-caRePg">300d</a><br>
        <a href="https://drive.google.com/open?id=1aVAK0Z2E5DkdIH6-JHbiWSL5dbAcz6c3">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/14JP1gD7hcmsWdSpTvA3vKA">300d</a><br>
        <a href="https://drive.google.com/open?id=1kSAl4_AOg3_6ayU7KRM0Nk66uGdSZdnk">300d</a>
      </td>
    </tr>
</table>
<table align="center">
    <tr align="center">
        <td colspan="5"><b>Positive Pointwise Mutual Information (PPMI)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1_itcjrQawCwcURa7WZLPOA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1cEZzN1S2senwWSyHOnL7YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1KcfFdyO0-kE9S9CwzIisfw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FXYM3CY161_4QMgiH8vasQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1MGXRrc54nITPzQ7sfEUjMA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1mtxZna8UJ7xBIxhBFntumQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dDImpAx41V73Byl2julOGA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1bsBQHXFpxMHGBexYof1_rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/1NLr1K7aapU2sYBvzbVny5g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1LJl3Br0ccGDHP0XX2k3pVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1GQQXGMn1AHh-BlifT0JD2g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1Xm9Ec3O3rJ6ayrwVwonC7g">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1ECA51CZLp9_JB_me7YZ9-Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FO39ZYy1mStERf_b53Y_yQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1lLBFBk8nn3spFAvKY9IJ6A">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1f-dLQZlZo_-B5ZKcPIc6rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/10wtgdmrTsTrjpSDvI0KzOw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b6zjvhOIqTdACSSbriisVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1w24vCfgqcoJvPxsB5VrRvw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b9BPiDRhiEZ-6ybTcovrqQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VaUP3YJC0IZKTbJ-1_8HZg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1g39PKwT0kSmpneKOgXR5YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1d8Bsuak0fyXxQOVUiNr-2w">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1D5fteBX0Vy4czEqpxXjlrQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/15O2EbToOzjNSkzJwAOk_Ug">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/11Dqywn0hfMhysto7bZS1Dw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1wY-7mfV6nwDj_tru6W9h4Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1DMW-MgLApbQnWwDd-pT_qw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1HTHhlr8zvzhTwed7dO0sDg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1jAuGJBxKqgapt__urGsBOQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/173AJfCoAV0ZA8Z31tKBdTA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dFCxke_Su3lLsuwZr7co3A">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1NJ1Gc99oE0-GV0QxBqy-qw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1YGEgyXIbw0O4NtoM1ohjdA">Sparse</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
    </tr>
</table>
<p><sup>*</sup>由于古汉语中绝大部份词均为单字词，因此只需字向量。</p>
<br>
<h2 id="语料">语料</h2>
<p>项目花费了大量精力来收集了来自多个领域的语料。所有的文本数据均移除了html和xml标记，仅保留了纯文本。之后采用了<a href="https://github.com/hankcs/HanLP">HanLP(v_1.5.3)</a>对文本进行了分词。此外，我们将繁体中文用<a href="https://github.com/BYVoid/OpenCC">Open Chinese Convert (OpenCC)</a>转换为了简体中文。更详细的语料信息如下所示：</p>
<table align="center">
	<tr align="center">
		<td><b>语料</b></td>
		<td><b>大小</b></td>
		<td><b>词数量</b></td>
		<td><b>词汇量</b></td>
		<td><b>详情</b></td>
	</tr>
	<tr align="center">
		<td>Baidu Encyclopedia<br />百度百科</td>
		<td>4.1G</td>
		<td>745M</td>
		<td>5422K</td>
		<td>中文百科<br />https://baike.baidu.com/</td>
	</tr>
	<tr align="center">
		<td>Wikipedia_zh<br />中文维基百科</td>
		<td>1.3G</td>
		<td>223M</td>
		<td>2129K</td>
		<td>中文维基百科<br />https://dumps.wikimedia.org/</td>
	</tr>
	<tr align="center">
		<td>People's Daily News<br />人民日报</td>
		<td>3.9G</td>
		<td>668M</td>
		<td>1664K</td>
		<td>人民日报新闻数据(1946-2017)<br />http://data.people.com.cn/</td>
	</tr>
	<tr align="center">
		<td>Sogou News<br />搜狗新闻</td>
		<td>3.7G</td>
		<td>649M</td>
		<td>1226K</td>
		<td>Sogou labs的新闻数据<br />http://www.sogou.com/labs/</td>
	</tr>
  <tr align="center">
    <td>Financial News<br />金融新闻</td>
    <td>6.2G</td>
    <td>1055M</td>
    <td>2785K</td>
    <td>从多个网站收集到的金融新闻</td>
  </tr>
	<tr align="center">
		<td>Zhihu_QA<br />知乎问答</td>
		<td>2.1G</td>
		<td>384M</td>
		<td>1117K</td>
		<td>中文问答数据<br />https://www.zhihu.com/</td>
	</tr>
	<tr align="center">
		<td>Weibo<br />微博</td>
		<td>0.73G</td>
		<td>136M</td>
		<td>850K</td>
		<td>NLPIR Lab提供的微博数据<br />http://www.nlpir.org/wordpress/download/weibo.7z</td>
	</tr>
	<tr align="center">
		<td>Literature<br />文学作品</td>
		<td>0.93G</td>
		<td>177M</td>
		<td>702K</td>
		<td>8599篇现代文学作品</td>
	</tr>
	<tr align="center">
		<td>Mixed-large<br />综合</td>
		<td>22.6G</td>
    <td>4037M</td>
    <td>10653K</td>
		<td>上述所有数据的汇总</td>
	</tr>
  <tr align="center">
    <td>Complete Library in Four Sections<br />四库全书</td>
    <td>1.5G</td>
    <td>714M</td>
    <td>21.8K</td>
    <td>目前最大的古代文献汇总</td>
  </tr>
</table>
上述统计结果中，所有词都被计算在内，包括低频词。
<br>
<h2 id="导入模型代码">导入模型(代码)</h2>
<p>例如我下载了多个词模型，下载得到bz2结尾的文件名，例如<code>sgns.financial.bigram.bz2</code>。</p>
<p><img loading="lazy" src="models.png" alt=""  />
</p>
<p>使用方式</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1">#以金融sgns.financial.bigram.bz2为例</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.financial.bigram.bz2&#39;</span><span class="p">,</span> 
                                          <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                          <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>


<span class="n">model</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x7fe7fad79d60&gt;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;投资&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.084635,  0.890228, -0.23223 , -0.308985,  0.058241,  0.458777,
       -0.152547, -0.413471,  0.269701, -0.078043, -0.4155  ,  0.074735,
        0.35714 ,  0.103431,  0.601784, -0.390854,  0.814801, -0.122664,
       -1.076744,  0.516941, -0.293319, -0.310251, -0.407794,  0.003898,
       -0.210962,  0.378095, -0.345955, -0.223848,  0.700162,  0.207644,
        0.426249, -0.272832, -0.110305, -0.701062, -0.173407, -0.172121,
       -0.682592,  0.593414,  0.279591, -0.408284, -0.166693,  0.753402,
        0.037375,  0.141865, -0.246024, -0.108663, -0.225255, -0.856601,
        0.381026,  0.401248,  0.012108, -0.126305, -0.374255,  0.728795,
        0.219549, -0.354029, -0.353131,  0.064867,  0.49565 , -0.503267,
       -0.304075,  0.145036,  0.688948,  0.063382, -0.223243,  0.474251,
        0.80543 ,  0.683178,  0.118159,  0.408411, -0.020066,  0.009045,
       -0.135446, -0.069633,  0.206357,  0.482845, -0.075307,  0.06433 ,
       -0.112367,  0.011816,  0.87427 , -0.120287, -0.31036 ,  0.369985,
        0.560386, -0.215248,  0.389631,  0.042943, -0.319149,  0.951551,
       -0.335188,  0.642246, -0.55546 ,  0.322397,  0.659618, -0.213124,
        0.346696, -0.342239,  0.31479 ,  0.078533, -0.345148,  0.815577,
       -0.530134,  0.303419, -0.158916, -0.190564,  0.436046, -0.112251,
       -0.339966,  0.253645,  0.181076,  0.122875, -0.310951, -0.126253,
        1.641405,  0.357906,  0.165796,  0.398656, -0.330591,  0.20328 ,
       -0.077191, -0.421248, -0.078504, -0.734519,  0.146212,  0.535727,
        0.014134,  0.040322, -0.44809 , -0.758205, -0.151237,  0.248258,
       -0.319704,  0.656033, -0.518857,  0.932356, -1.01786 , -0.46354 ,
        0.160921, -0.243597,  0.106666, -0.03404 ,  0.010672,  0.260243,
        0.899813,  0.171735, -0.108209, -0.009843, -0.18113 ,  0.302494,
        0.187285,  0.064669, -0.502041, -0.724377, -0.294312, -0.522256,
        0.334543,  0.740455, -0.357653,  0.540747,  0.256146,  0.513839,
        1.116628, -0.626111,  0.505574,  0.089774, -0.381137, -0.282352,
       -0.457542,  0.198909,  0.313638,  0.560809,  0.25295 ,  0.878158,
       -0.289311, -0.629047,  0.011103,  0.041058, -0.291302, -0.014001,
       -0.027697, -0.445817, -0.070086,  0.159816, -0.120071,  1.280489,
       -0.108866,  0.01586 , -0.505574, -0.679772, -0.343165,  0.595633,
        0.438108, -0.364066, -0.393667,  0.442285,  0.24979 , -0.191607,
        0.425692,  0.535577, -0.480332, -0.737461,  0.588498, -0.380264,
        0.151292,  0.077519, -0.221384,  0.699436,  0.401642,  0.509026,
       -0.411141,  0.206719, -0.097051, -0.451834, -0.825617,  0.602984,
        0.2853  ,  0.46055 ,  0.96472 ,  0.322712, -0.373446,  0.207944,
        0.236688,  0.566523,  0.037644,  1.241091,  0.025682,  0.373211,
        0.097712, -0.195355,  0.264579, -0.072992, -0.121629,  0.041688,
        0.213666,  0.329652, -0.015182,  0.396307,  0.117955,  0.119577,
       -0.334761, -0.135917,  0.409983,  0.512367, -0.292204,  0.302897,
       -0.325733,  0.383173, -0.92419 , -0.377535, -0.059801, -0.606275,
       -0.240482,  0.054021, -0.581386, -0.555691,  0.158354,  0.103765,
        0.107681,  0.248877, -0.597925,  0.193332,  0.844085,  0.00584 ,
        0.041622, -0.111235,  0.617778,  0.234883, -0.09562 ,  0.408324,
       -0.107121,  0.717875,  0.674794,  0.127214, -0.178357,  0.331436,
        0.417898, -0.650833, -0.428309, -0.576132,  0.210533, -0.057879,
       -0.578397,  0.468586,  0.103365, -0.403216, -0.398776,  0.094514,
       -0.130387,  0.628187, -0.463082, -0.951649,  0.561544,  0.118903,
        0.448327, -0.171685, -0.672348,  0.069471,  0.556452, -0.335425],
      dtype=float32)
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">model.similar_by_key(&#39;投资&#39;)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;长期投资&#39;, 0.5135656595230103),
 (&#39;投资规模&#39;, 0.5089880228042603),
 (&#39;智百扬&#39;, 0.49565914273262024),
 (&#39;投资总额&#39;, 0.4955061078071594),
 (&#39;洛辉&#39;, 0.489188551902771),
 (&#39;337409&#39;, 0.48917514085769653),
 (&#39;洛盛&#39;, 0.4819018244743347),
 (&#39;洛腾&#39;, 0.4728960692882538),
 (&#39;394150&#39;, 0.4704836308956146),
 (&#39;投资额&#39;, 0.4685181975364685)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">similar_by_key</span><span class="p">(</span><span class="s1">&#39;风险&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;提示&#39;, 0.6549968123435974),
 (&#39;经营风险&#39;, 0.6316577792167664),
 (&#39;景气衰退&#39;, 0.544153094291687),
 (&#39;风险分析&#39;, 0.5439289212226868),
 (&#39;遇宏观&#39;, 0.5435716509819031),
 (&#39;信用风险&#39;, 0.5345730185508728),
 (&#39;承受能力&#39;, 0.5291797518730164),
 (&#39;防范&#39;, 0.5271924138069153),
 (&#39;系统性&#39;, 0.5178108811378479),
 (&#39;不确定性&#39;, 0.5173759460449219)]
</code></pre></div><p>向量运行效果还行，感兴趣的同学也可以根据自己的数据训练word2vec模型，训练及使用的办法参照文章</p>
<p><a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>将年报数据汇总至xlsx文件中</title>
      <link>https://textdata.cn/blog/sh_market/</link>
      <pubDate>Thu, 21 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/sh_market/</guid>
      <description>分散在各处的pdf很难分析，如何将pdf汇总至excel。本文将pdf汇总与excel分析结合。</description>
      <content:encoded><![CDATA[<h2 id="整理到csv中">整理到csv中</h2>
<p>将70G定期报告披露数据集下载</p>
<p>链接: <a href="https://pan.baidu.com/s/1oboFUswiAMdA_Wn3xCh6YQ">https://pan.baidu.com/s/1oboFUswiAMdA_Wn3xCh6YQ</a> 提取码: g7bd</p>
<p><img loading="lazy" src="img/sh_marketing.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pdfdocx</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">clean</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;\s&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>

<span class="c1">#文件夹列表</span>
<span class="n">dirs</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;reports&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;DS&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">d</span><span class="p">]</span>
<span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">dirs</span><span class="p">:</span>
    <span class="n">datas</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;reports/</span><span class="si">{d}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="n">di</span><span class="p">))</span> <span class="k">if</span> <span class="s1">&#39;z&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">file</span> <span class="o">=</span> <span class="s1">&#39;reports/</span><span class="si">{di}</span><span class="s1">/</span><span class="si">{f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">di</span><span class="o">=</span><span class="n">di</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>
            <span class="n">code</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">year</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;_(\d</span><span class="si">{4}</span><span class="s1">)_&#39;</span><span class="p">,</span> <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">clean</span><span class="p">(</span><span class="n">pdfdocx</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="n">file</span><span class="p">))</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="n">code</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">text</span><span class="p">,</span> <span class="s1">&#39;year&#39;</span><span class="p">:</span><span class="n">year</span><span class="p">}</span>
            <span class="n">datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">datas</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">])</span>
    <span class="c1">#将每家公司的年报导出到csv中</span>
    <span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;data.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
    
    
<span class="c1">#读取</span>
<span class="n">ndf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data.csv&#39;</span><span class="p">)</span>
<span class="c1">#去重</span>
<span class="n">ndf</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1">#清洗</span>
<span class="n">ndf</span> <span class="o">=</span> <span class="n">ndf</span><span class="p">[</span><span class="n">ndf</span><span class="o">.</span><span class="n">code</span><span class="o">=!=</span><span class="s1">&#39;code&#39;</span><span class="p">]</span>
<span class="c1">#导出到xlsx</span>
<span class="n">ndf</span><span class="o">.</span><span class="n">to_excel</span><span class="p">(</span><span class="s1">&#39;data.xlsx&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="导入数据">导入数据</h2>
<p>excel数据下载链接: <a href="https://pan.baidu.com/s/1r4YRyxb7bTsx-_ayT4GDKQ">https://pan.baidu.com/s/1r4YRyxb7bTsx-_ayT4GDKQ</a> 提取码: ew4v</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;data.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>code</th>
      <th>year</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>603859</td>
      <td>2017</td>
      <td>2017年半年度报告1/116公司代码：603859公司简称：能科股份能科节能技术股份有限公...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>603859</td>
      <td>2019</td>
      <td>2019年半年度报告1/141公司代码：603859公司简称：能科股份能科科技股份有限公司2...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>603859</td>
      <td>2018</td>
      <td>2018年半年度报告1/120公司代码：603859公司简称：能科股份能科科技股份有限公司2...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>601500</td>
      <td>2017</td>
      <td>2017年半年度报告1/114公司代码：601500公司简称：通用股份江苏通用科技股份有限公...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>601500</td>
      <td>2019</td>
      <td>2019年半年度报告1/140公司代码：601500公司简称：通用股份江苏通用科技股份有限公...</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<h3 id="查看数据量">查看数据量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><pre><code>16984
</code></pre>
<br>
<h3 id="公司数">公司数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">code</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
</code></pre></div><pre><code>1476
</code></pre>
<br>
<h3 id="含有的年份">含有的年份</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">year</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
</code></pre></div><pre><code>[2002,
 2003,
 2004,
 2005,
 2006,
 2007,
 2008,
 2009,
 2010,
 2011,
 2012,
 2013,
 2014,
 2015,
 2016,
 2017,
 2018,
 2019]
</code></pre>
<br>
<h3 id="每家公司年报数">每家公司年报数</h3>
<p>数据集中，平均每家公司的年报数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">avg</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">/</span><span class="n">df</span><span class="o">.</span><span class="n">code</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
<span class="nb">round</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div><pre><code>11.51
</code></pre>
<br>
<h2 id="说明">说明</h2>
<p>数据是19年获取的，数据不全，下载过程中有部分pdf是破损的文件。</p>
<p>大家可以尝试该数据集训练会计年报词向量，看看有没有有趣的应用。</p>
<p>本数据可作探索实验性质，如果想在会计领域深入挖掘，建议找更全更精准的数据集。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>豆瓣影评 | 探索词向量妙处</title>
      <link>https://textdata.cn/blog/douban_w2v/</link>
      <pubDate>Thu, 21 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/douban_w2v/</guid>
      <description>使用cntext训练、使用词向量。</description>
      <content:encoded><![CDATA[<p>本文要点</p>
<ul>
<li>读取csv</li>
<li>cntext训练词向量模型</li>
<li>cntext扩展pos、neg词典</li>
<li>导入词向量模型</li>
<li>运用词向量模型</li>
</ul>
<br>
<br>
<h2 id="代码下载">代码下载</h2>
<p>链接: <a href="https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg">https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg</a> 提取码: og9t</p>
<p><br><br></p>
<h2 id="一读取数据">一、读取数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;douban.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;电影  : </span><span class="si">{}</span><span class="s2"> 部&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Movie_Name_CN</span><span class="o">.</span><span class="n">nunique</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;评论  : </span><span class="si">{}</span><span class="s2"> 条&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)))</span>
</code></pre></div><pre><code>电影  : 28 部
评论  : 2125056 条
</code></pre>
<p><br><br></p>
<h2 id="二训练模型">二、训练模型</h2>
<p>使用 <a href="https://textdata.cn/blog/cntext_simplification/"><em><strong>cntext</strong></em></a> 库(版本号1.9， 免费公开版)训练词向量word2vec模型,这里我把csv数据整理为txt</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext</span> <span class="kn">import</span> <span class="n">W2VModels</span>
<span class="c1">#cntext版本号1.9</span>
<span class="c1">#pip install cntext==1.9</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#训练word2vec模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>  <span class="c1">#语料数据</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;douban.txt&#39;</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...预处理    语料 ...
Step 2/4:...训练   word2vec模型
            耗时   2001 s
        
</code></pre></div><p>cntext 可以用于扩展词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;pos.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;neg.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 3/4:...准备 每个seed在word2vec模型中的相似候选词...
Step 4/4 完成! 耗时 2302 s
Step 3/4:...准备 每个seed在word2vec模型中的相似候选词...
Step 4/4 完成! 耗时 2303 s
</code></pre></div><p>在代码所在文件夹内可以找到</p>
<ul>
<li>output/w2v_candi_words/w2v.model</li>
<li>新的  pos.txt</li>
<li>新的  neg.txt</li>
</ul>
<p>新的 <em><strong>pos.txt</strong></em> 是对 <em><strong>pos.txt</strong></em> 词典的扩展。</p>
<br>
<br>
<h2 id="三导入w2v模型">三、导入w2v模型</h2>
<p>有的时候数据量特别大，模型训练十分不易。</p>
<p>这时，保存已训练好的模型，不止下次不用再同样的数据再次训练，也可分享给其他人使用。</p>
<p>训练结束后，在代码所在文件夹内可以找到 <code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span>
</code></pre></div><pre><code>&lt;gensim.models.keyedvectors.KeyedVectors at 0x7face0574880&gt;
</code></pre>
<p><em><strong>w2v_models</strong></em> 数据类型为 <em><strong>KeyedVectors</strong></em> ， 在本文中使用 <em><strong>w2v_models</strong></em> 代指 <em><strong>KeyedVectors</strong></em></p>
<p><br><br></p>
<h2 id="四玩转词向量">四、玩转词向量</h2>
<p>用户级的数据(如在线评论)感觉生成的向量会准一些，<strong>词向量的方向，近义反义在向量中都有体现</strong>。</p>
<p><img loading="lazy" src="man-woman.png" alt=""  />
</p>
<p>例如本文使用的是28部电影的2125056条影评， 一般评论内容包含电影相关信息，如电影题材、是否值的观影等。</p>
<p>而在我们训练出模型w2v_models存在一些常用的方法</p>
<ul>
<li><em><strong>w2v_model.get_vector(key)</strong></em> 获取key的词向量</li>
<li><em><strong>w2v_model.most_similar_to_given(key1, keys_list)</strong></em>  从 keys_list 中获取与 key1 最相似的词</li>
<li><em><strong>w2v_model.n_similarity(ws1, ws2)</strong></em> 两组词 <em><strong>ws1</strong></em>,  <em><strong>ws2</strong></em>  的相似度</li>
<li><em><strong>w2v_model.closer_than(key1, key2)</strong></em> 更接近于 <em><strong>key1</strong></em> 的词向量(相比于 <em><strong>key2</strong></em> )</li>
<li><em><strong>w2v_model.most_similar(positive, negative)</strong></em> 找出与 <em><strong>positive</strong></em> 同方向，与 <em><strong>negative</strong></em> 反向相反的词。</li>
</ul>
<h3 id="41-get_vectorkey">4.1 get_vector(key)</h3>
<p><em><strong>w2v_model.get_vector(key)</strong></em>  获取key的词向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取某词语的向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>array([ 0.06488553,  0.74188954,  0.25468495,  0.89755714,  1.8139195 ,
       -0.6950082 ,  0.24339403, -1.2188634 ,  0.543618  , -0.9988698 ,
        0.27471313,  0.9325699 , -0.5860608 , -0.5081917 ,  1.6423215 ,
       -0.0490295 , -0.3927043 ,  0.659067  ,  0.03185922, -1.021391  ,
       -1.3214804 , -0.28208104, -0.7819419 , -0.30637202, -1.5944146 ,
       -0.12383854, -0.70463836,  0.45689437,  1.223081  , -1.9453759 ,
       -0.5538997 , -0.9750523 , -0.10031194, -0.9568689 ,  0.30341247,
        1.1102395 ,  0.667315  , -1.1600997 , -0.26674765, -0.55144155,
       -0.3246094 ,  0.82902473, -0.47339582, -0.9009957 ,  1.7722464 ,
        0.28959563, -0.03453476,  0.4786787 , -0.48074463, -0.23090109,
       -0.49390873,  0.71246386,  2.1557336 ,  2.4899387 , -0.51481706,
        0.5579966 , -0.6973235 , -1.1408254 ,  0.72495663, -1.0326954 ,
       -0.5455598 ,  0.98941576, -1.2155218 , -0.9088408 ,  1.9184568 ,
       -0.21800426, -1.2009395 ,  0.29684314,  1.3672423 , -2.269391  ,
        0.6188098 , -0.02714545, -0.44811317,  1.4397241 , -1.0594722 ,
       -0.08088647, -0.13015983, -0.99255013,  0.62044877,  2.5046496 ,
        0.4054545 , -0.38767585, -0.6956541 ,  0.22991426,  0.5928579 ,
       -0.12684819, -0.17408212,  0.25033692, -1.4419957 , -0.27390227,
        1.166638  , -0.00624323, -1.6046506 ,  2.1633575 , -0.395548  ,
       -1.1297956 , -3.1474566 ,  0.38729438, -2.0434535 , -1.5511289 ],
      dtype=float32)
</code></pre>
<br>
<h3 id="42-most_similar_to_givenkey1-keys_list">4.2 most_similar_to_given(key1, keys_list)</h3>
<p>从 keys_list 中获取与 key1 最相似的词。例如在212w影评中，从<code>'爱情', '悬疑', '飞船', '历史', '战争'</code>找出最接近<code>'太空'</code>，最后返回<code>'飞船'</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#从 `keys_list` 中获取与 `key1` 最相似的 `key`。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar_to_given</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;太空&#39;</span><span class="p">,</span> 
                                <span class="n">keys_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;爱情&#39;</span><span class="p">,</span> <span class="s1">&#39;悬疑&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;历史&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>'飞船'
</code></pre>
<br> 
<h3 id="43-w2v_modeln_similarityws1-ws2">4.3 w2v_model.n_similarity(ws1, ws2)</h3>
<p>两组词ws1, ws2 的相似度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="n">cosine_similarity</span><span class="p">([</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;理想&#39;</span><span class="p">)],</span>  
                  <span class="p">[</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;现实&#39;</span><span class="p">)])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div><pre><code>0.5371934
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cosine算法</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.5371934
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#计算两组键之间的余弦相似度。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;赞&#39;</span><span class="p">,</span> <span class="s1">&#39;推荐&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;无聊&#39;</span><span class="p">,</span> <span class="s1">&#39;尴尬&#39;</span><span class="p">,</span> <span class="s1">&#39;垃圾&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.35008422
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> <span class="s1">&#39;梦想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">,</span> <span class="s1">&#39;生活&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.48020104
</code></pre>
<br>
<h3 id="44-w2v_modelcloser_thankey1-key2">4.4 w2v_model.closer_than(key1, key2)</h3>
<p>更接近于 <em><strong>key1</strong></em> 的词向量(相比于key2)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取所有更接近 `key1` 的键，而不是 `key2` 。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">closer_than</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> 
                      <span class="n">key2</span><span class="o">=</span><span class="s1">&#39;现实&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>['梦想', '妥协', '追梦', '愿望', '骨感']
</code></pre>
<br>
<h3 id="45-w2v_modelmost_similarpositive-negative">4.5 w2v_model.most_similar(positive, negative)</h3>
<p>找出与 <em><strong>positive</strong></em> 同方向，与 <em><strong>negative</strong></em> 反向相反的词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;过瘾&#39;</span><span class="p">],</span>
                       <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;垃圾&#39;</span><span class="p">],</span>
                       <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><pre><code>[('激动人心', 0.6859163045883179),
 ('惊心动魄', 0.6767394542694092),
 ('带感', 0.6723690032958984),
 ('惊险刺激', 0.667783796787262),
 ('刺激', 0.6445038318634033),
 ('燃', 0.6429688930511475),
 ('爽快', 0.6287934184074402),
 ('带劲', 0.6254130005836487),
 ('爽', 0.624543309211731),
 ('酣畅淋漓', 0.6140543818473816)]
</code></pre>
<br>
<h3 id="46-类比king-manwomanqueen">4.6 类比king-man+woman~queen</h3>
<p><img loading="lazy" src="kingqueenformular.png" alt=""  />
</p>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。</p>
<p>这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p><em><strong>gender_direction_1 = vector(man)-vector(woman)</strong></em></p>
<p><em><strong>gender_direction_2 = vector(king)-vector(queen)</strong></em></p>
<p>那两个性别方向应该近似，假设这里将其  <em><strong>gender_direction_1 = gender_direction_2</strong></em> ，则对于公式中任意一个词，都可以由等式中的其他三个词经过运算得到。例如</p>
<p><em><strong>vector(queen) =  vector(king)-vector(man)+vector(woman)</strong></em></p>
<p>这里构造了一个情绪的公式，计算如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 开心 - 难过 ~=  享受 - d</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;开心&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;难过&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;享受&#39;</span><span class="p">)</span>

<span class="c1">#d = a-b+c</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
</code></pre></div><pre><code>[('享受', 0.7833479046821594),
 ('开心', 0.6825607419013977),
 ('愉快', 0.6298696994781494),
 ('娱乐', 0.6215130090713501),
 ('感官', 0.6085000038146973),
 ('图个', 0.6052624583244324),
 ('图一乐', 0.6039161682128906),
 ('休闲', 0.60273677110672),
 ('视觉享受', 0.6006160378456116),
 ('轻松愉快', 0.5961319804191589)]
</code></pre>
<p>很遗憾，<em><strong>d</strong></em> 没有运算出煎熬之类的词语，但好在都是形容词，而且是快乐居多的形容词，类别是对的，就是方向是反的。</p>
<br>
<h3 id="词向量总结">词向量总结</h3>
<p>需要注意的是经典的运算 <em><strong>king-man+woman~queen</strong></em> 来自 <em><strong>Glove</strong></em>模型，而不是本文使用的 <em><strong>Word2Vec</strong></em>模型。两者相同点，<em><strong>Glove</strong></em> 与 <em><strong>Word2Vec</strong></em> 均为词嵌入 <em><strong>embeddings</strong></em> 技术。区别在于 <em><strong>Glove</strong></em> 获取的词的全局语义空间，而 <em><strong>Word2Vec</strong></em> 一般是某个词前后n个词(例如前后5个词)范围内的语义。做概念四则运算，以后如可能，建议用 <em><strong>Glove</strong></em>。</p>
<p>此外，即时使用 <em><strong>Glove</strong></em>，尽量使用概念的词组均值向量。首先要训练数据要存在这些人类认知的线索。其次，认知概念往往不是由一个词决定的，可能需要相关的很多词。例如人类社会中的<code>雄雌(没有贬义，包含了男女在内的概念)</code>，</p>
<ul>
<li>雄性概念词有<code>他、男人、男孩、父亲、爷爷、爸爸、姥爷...</code></li>
<li>雌性概念词有<code>她、女人、女孩、母亲、奶奶、妈妈、姥姥...</code></li>
<li>国王概念词有<code>查理n世、乔治、路易...</code></li>
<li>女王概念词有<code>伊丽莎白n世、维多利亚女王、叶卡捷琳娜二世...</code></li>
</ul>
<p>或许改成概念向量四则运算，公式可能更容易成立。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>结构模型|DSGE|Stata实证前沿|空间计量|Python数据挖掘2022五一工作坊</title>
      <link>https://textdata.cn/blog/2022-05-workshop/</link>
      <pubDate>Mon, 11 Apr 2022 03:43:10 +0600</pubDate>
      
      <guid>/blog/2022-05-workshop/</guid>
      <description>为推动我国经济、统计等社会科学量化研究方法学习与应用，培养和训练社会科学相关领域的青年学者、硕博士研究生，促进社会科学相关领域研究方法科学化规范化，“结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘”五一工作坊为广大学者提供了一个高水平学术交流、研究方法普及与研究经验分享的平台。工作坊采用模块式教学方法，不仅侧重经济、统计等社会科学量化基本方法的介绍，而且更加注重研究设计与研究选题训练，注重理论实践相结合，培养学员社会科学量化分析研究的综合能力。</description>
      <content:encoded><![CDATA[<p>大家好，五一劳动节假期我们将迎来了新的一期“结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘”工作坊，欢迎大家报名参加。我们将分七次为大家介绍本次工作坊的详细内容，敬请期待。</p>
<br>
<h2 id="关于我们">关于我们</h2>
<p>为推动我国经济、统计等社会科学量化研究方法学习与应用，培养和训练社会科学相关领域的青年学者、硕博士研究生，促进社会科学相关领域研究方法科学化规范化，“<strong>结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘</strong>”五一工作坊为广大学者提供了一个高水平学术交流、研究方法普及与研究经验分享的平台。工作坊采用模块式教学方法，不仅侧重经济、统计等社会科学量化基本方法的介绍，而且更加注重研究设计与研究选题训练，注重理论实践相结合，培养学员社会科学量化分析研究的综合能力。</p>
<p><strong>结构模型又称为结构计量模型</strong>，是将经济学模型和统计模型结合，用于估计描述现实的深层参数，模拟现实世界，<strong>以便合理地评估政策效果的实证工具</strong>。结构模型通过建立引起因果关系的数据生成具体方式（机制）的模型来解决简化型中的问题。模型中明确地指明了一些重要的外部因素（如政策）是如何影响通过某些参数来影响参与人决策的，那么通过改变这些外部因素并结合现有数据所估计出来的参数，结构模型便可以提供一系列反事实推断，<strong>对政策的制定有重要的意义</strong>。政策评估需要建立在理解对政策不变的“深层”参数之上。在结构式方法中，理论和实证的联系是紧密的。由于其建模技术的优雅和深刻，不仅是当今经济政策评估领域的前沿，也是发展经济理论的有力武器，<strong>在世界顶级期刊中，采用结构模型建模的文章引起广泛关注和引用，为所在学科的理论发展和政策评估带来深刻影响</strong>。</p>
<p>实证研究过程中学者普遍面临<strong>数据获取、清洗和编码</strong>的两大问题。在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用计量分析方法对数据进行分析。但大数据时代，网络数据成为亟待挖掘的潜在宝藏，大量商业信息、社会信息以<strong>文本等非结构化、异构型数据格式</strong>存储于海量网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两大问题，即：<strong>①从网络世界中高效地采集数据  ②从纷杂的文本数据抽取文本信息</strong>。</p>
<p>在获取数据及文本信息后，需要使用计量方法对数据进行分析处理。Stata、ArcGIS、Matlab等软件功能日益强大，理论也与时俱进。前沿分析固然可能会给你的Paper加分，但不理解其理论依据，会导致前沿方法的滥用, 使你的研究大为失色。</p>
<p><strong>DSGE</strong>，全称是dynamic stochastic general equilibrium，即<strong>动态随机一般均衡模型</strong>。是目前在宏观经济学研究占重要地位（甚至是主导地位）的模型方法，主要用于讨论<strong>经济增长、经济周期</strong>以及讨论<strong>政策工具效果</strong>（财政和货币政策）。我们需要对DSGE的深入学习。</p>
<p>为此，本次五一工作坊特别邀请七位走在理论实证、数据分析前沿的学者，为广大热爱经济学的学生、青年教师，讲解当下前沿模型的核心思想，基于Matlab、ArcGIS、Stata、Python等软件的实践操作。在这个知识与思想碰撞的时代，<strong>我们将与您分享最前沿的思想与实践技巧</strong>！为您带来最前沿计量经济理论与扎实操作并重的高质量课程。</p>
<br>
<h2 id="工作坊详情页">工作坊详情页</h2>
<p>由<strong>刘文革</strong>老师总筹划、<strong>谢杰</strong>老师协调发起工作坊，工作坊由7位老师分讲。</p>
<p><strong>点击下方链接</strong>，进入课程详情页，<strong>每门课程费用2000元(邓建鹏老师课程1000元)</strong>。</p>
<ul>
<li><a href="1_structural_model_1.html"><font color=blue>结构模型(一) -邹建文(中南财经政法大学)</font></a></li>
<li><a href="2_structural_model_2.html"><font color=blue>结构模型(二) -邓建鹏(上海财经大学)</font></a></li>
<li><a href="3-DSGE.html"><font color=blue>DSGE-王文甫(四川大学)</font></a></li>
<li><a href="4-Stata1.html"><font color=blue>Stata实证前沿(一)-王非(中国人民大学)</font></a></li>
<li><a href="5-Stata2.html"><font color=blue>Stata实证前沿(二)-司继春(上海对外经贸大学)</font></a></li>
<li><a href="6-Geo.html"><font color=blue>空间计量-李光勤(安徽财经大学)</font></a></li>
<li><a href="7-Python.html"><font color=blue>Python数据挖掘-邓旭东(哈尔滨工业大学)</font></a></li>
</ul>
<br>
<h2 id="授课方式">授课方式</h2>
<ul>
<li><strong>时间</strong>
<ul>
<li>2022年<strong>五一</strong>期间（<strong>具体时间待定</strong>）</li>
<li>每天6小时（8:30 — 11:30；14:00 — 17:00）+ <strong>30分钟答疑</strong>（部分课程晚间18:30-21:30进行）</li>
</ul>
</li>
<li>地点: 小鹅通平台（<strong>线上直播</strong>）</li>
<li><strong>每门课程2000元，视频保留10天</strong>；<strong>邓建鹏老师课程1000元</strong></li>
</ul>
<p><br><br></p>
<h2 id="报名信息">报名信息</h2>
<p>全国高等院校及研究机构从事经济科学研究的青年师生。尤其适合那些希望掌握高级实证方法，提升量化研究设计能力和国家课题申报能力的研究者。</p>
<h3 id="费用">费用</h3>
<ul>
<li><strong>每门课程2000元(每位老师讲授一门)</strong>；<strong>邓建鹏老师课程1000元</strong></li>
</ul>
<h3 id="优惠政策">优惠政策</h3>
<ul>
<li><strong>个人报名优惠</strong>：报名两位老师的课程9折；三位老师的课程8折；四位及以上老师的课程7.5折；老学员9折；学生优惠200元/人。</li>
<li><strong>团队报名优惠</strong>：三人成团及以上9折；五人成团及以上8折。</li>
</ul>
<h3 id="报名时间">报名时间</h3>
<p>从即日起</p>
<h3 id="报名咨询">报名咨询</h3>
<ul>
<li>17816181460（同微信）（汪老师）</li>
</ul>
<img src="img/wechat.png" style="zoom:40%;" />
<h3 id="缴费信息">缴费信息</h3>
<ul>
<li>单位：杭州国商智库信息技术服务有限公司</li>
<li>开户银行： 中国银行杭州大学城支行</li>
<li>银行账户：6232636200100260588</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>JCR2021 | 计算文本的语言具体性</title>
      <link>https://textdata.cn/blog/jcr_concreteness_computation/</link>
      <pubDate>Thu, 07 Apr 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/jcr_concreteness_computation/</guid>
      <description>语言具体性描述了一个词在多大程度上是指一个实际的、有形的或“真实的”实体，以一种更具体、更熟悉、更容易被眼睛或心灵感知的方式描述对象和行为（即，可想象或生动；Brysbaert, Warriner, and Kuperman 2014; Semin and Fiedler 1988). 我找了三篇论文，简单分享一下。</description>
      <content:encoded><![CDATA[<p>前不久分享了一篇JCR2018的综述 <a href="https://textdata.cn/blog/automatetextanalysisinmarket/">营销研究中文本分析应用概述(含案例及代码) </a></p>
<p>最近看到一篇JCR2021的实证 <strong>语言具体性如何影响消费者态度</strong> ，研究者从一个现象, 即消费者可以通过感知店员的表达具体(例如，更多的名词而非代词)，判断店员是否用心倾听自己的需求。这有点像三十年前， 在服务态度不好的百货商场，店员往往爱答不理。</p>
<p><img loading="lazy" src="img/Concreteness_JCR_computation.png" alt=""  />
</p>
<p>大邓作为消费者，相比1、2、3三种表达方式，我会更喜欢会觉得4、5、6句子中使用**较多细节、具体词的店员。**而简短表达，代词过多，表明店员连嘴都懒得张口服务我，似乎对我们的切身需求关注不足，态度好不端正的感觉。文中对店员言语具体性表达给出了建议及原因，例子如下图。</p>
<p><img loading="lazy" src="img/Concreteness_JCR_Explaination.png" alt=""  />
</p>
<ul>
<li><a href="https://textdata.cn/blog/2022-05-workshop/">结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘|2022五一工作坊</a></li>
<li>想随时随地系统学习Python文本分析，可以选择
<ul>
<li><a href="https://textdata.cn/blog/management_python_course/"><strong>Python网络爬虫与文本分析 | 2021录播课(虽是录播，但章节更多一些)</strong></a>。</li>
</ul>
</li>
<li>更喜欢有互动感通过直播学习，可以考虑
<ul>
<li><a href="https://textdata.cn/blog/2022-05-workshop/7-Python.html"><strong>Python网络爬虫与文本分析 | 2022五一直播</strong></a></li>
</ul>
</li>
</ul>
<p>我找了三篇文本具体性的论文，<strong>文章结尾附有具体性的Python案例代码</strong>，希望能对大家有帮助。</p>
<h2 id="语言具体性">语言具体性</h2>
<p><strong>语言具体性Concreteness</strong>描述了一个词在多大程度上是指一个实际的、有形的或“真实的”实体，以一种更具体、更熟悉、更容易被眼睛或心灵感知的方式描述对象和行为（即，可想象或生动；Brysbaert, Warriner, and Kuperman 2014; Semin and Fiedler 1988). 我找了三篇文本具体性的论文，文章结尾附有具体性的Python案例代码，希望能对大家有帮助。</p>
<br>
<h2 id="具体性词典">具体性词典</h2>
<p>Brysbaert, Warriner, A. B., &amp; Kuperman, V. (2014) 找4000人，网络众包标注，开发了英文40000词的<strong>具体性词典</strong>。下图是对应的词典excel文件，字段Conc.M就是对应词语的具体性得分。</p>
<blockquote>
<p>中文具体性词典目前可以考虑用这个资源，含1600中文词词典，指标包括具体性、可成象性。</p>
<p>Wang, Ruiming, Shuting Huang, Yacong Zhou, and Zhenguang G. Cai. &ldquo;Chinese character handwriting: A large-scale behavioral study and a database.&rdquo; Behavior Research Methods 52 (2020): 82-96.</p>
</blockquote>
<p><img loading="lazy" src="img/Concreteness_Wordlist.png" alt=""  />
</p>
<br>
<h2 id="心理距离与语言具体性">心理距离与语言具体性</h2>
<p>Snefjella, Bryor, and Victor Kuperman(2015)挖掘了<strong>心理距离</strong>与<strong>语言具体性</strong>之间的<strong>数学关系</strong>， 第一次将心理距离看做连续性变量进行度量(而之前的研究几乎只把心理距离设置为高、低二分类变量)，计算过程使用了Brysbaert2014的语言具体性词典度量。</p>
<p>实验结果与我们认知相吻合，基本上心理距离越大， 具体性得分越小；反之，也成立。下面我列出在地理、时间、社会三个维度的量化可视化结果。</p>
<h3 id="地理维度">地理维度</h3>
<p><img loading="lazy" src="img/Concreteness_psychological_distance1.png" alt=""  />
</p>
<h3 id="时间维度">时间维度</h3>
<p><img loading="lazy" src="img/Concreteness_psychological_distance2.png" alt=""  />
</p>
<h3 id="社会维度">社会维度</h3>
<p><img loading="lazy" src="img/Social-Distance-Groups.png" alt=""  />
</p>
<p><img loading="lazy" src="img/Concreteness_psychological_distance3.png" alt=""  />
</p>
<br>
<h2 id="代码实现-以jcr为例">代码实现-以JCR为例</h2>
<p><img loading="lazy" src="img/Concreteness_JCR.png" alt=""  />
</p>
<p>消费者经常对客户服务感到沮丧。 但是语言的简单转变是否有助于提高客户满意度？ 我们认为，<strong>语言具体性linguistic concreteness</strong>——员工在与客户交谈时使用的词语的<strong>有形性tangibility、具体性speciﬁcity或可想象性imaginability</strong>——可以塑造消费者的态度和行为。 五项研究，包括对两个不同领域环境中超过 1,000 次真实消费者-员工互动的文本分析，表明当员工与他们具体交谈时，客户会更满意、更愿意购买和购买。 这是因为客户推断使用更具体语言的员工正在倾听（即关注并理解他们的需求)。 这些发现加深了对语言如何塑造消费者行为的理解，揭示了具体性影响人们感知的心理机制，并为管理者帮助提高客户满意度提供了一种直接的方法。</p>
<p>假设我们作为消费者，看到员工对同一个意思使用如下不同表达，</p>
<p><img loading="lazy" src="img/jcr_attitude.png" alt=""  />
</p>
<p>相比4、5、6这三种表达方式，会觉得句子中使用**较多代词的店员懒得说话(态度不端正)。**而使用较多名词和形容词，会表明店员关注我们的切身需求。这篇JCR就是从这个角度切入的研究。</p>
<br>
<p>JCR文中具体性计算说明</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">We computed a concreteness score for each conversational turn (averaging across all words in that turn) and for each conversational participant (averaging across all words over all their turns). Results were the same whether or not stop words commonly excluded from linguistics analyses (e.g., but, and) were included. We report results excluding stop words.
</code></pre></div><p>按照我的理解， 设计如下算法</p>
<ol>
<li>对文本(会话)使用nltk分词，得到词语列表</li>
<li>在具体性词典中查询对应的具体性得分</li>
<li>得到文本的具体性得分(句子所有词的具体性得分加总除以词数)</li>
</ol>
<h3 id="方法一">方法一</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="c1">#JCR文中使用的Paetzold2016的词典</span>
<span class="c1"># Paetzold2016文中的词典下载链接失效。这里使用Brysbaert2014的词典</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s2">&#34;Concreteness_ratings_Brysbaert_et_al_BRM.xlsx&#34;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="k">def</span> <span class="nf">query_concreteness</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    查询word的具体性得分
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;Word&#34;</span><span class="p">]</span><span class="o">==</span><span class="n">word</span><span class="p">][</span><span class="s1">&#39;Conc.M&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    
 

<span class="k">def</span> <span class="nf">concreteness_score</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    计算文本的具体性得分
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    
    <span class="k">try</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;你的电脑nltk没配置好，请观看视频https://www.bilibili.com/video/BV14A411i7DB&#39;</span><span class="p">)</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
        
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">+=</span> <span class="n">query_concreteness</span><span class="p">(</span><span class="n">word</span><span class="o">=</span><span class="n">word</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">score</span> <span class="o">+=</span> <span class="mi">0</span>
            
    <span class="k">return</span> <span class="n">score</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    
  
<span class="c1"># 案例</span>
<span class="n">employee_replys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I&#39;ll go look for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that top&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go look for that t-shirt in grey&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt in grey&#34;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">reply</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">employee_replys</span><span class="p">):</span>
    <span class="n">score</span><span class="o">=</span><span class="n">concreteness_score</span><span class="p">(</span><span class="n">reply</span><span class="p">)</span>
    <span class="n">template</span> <span class="o">=</span> <span class="s2">&#34;Concreteness Score: </span><span class="si">{score:.2f}</span><span class="s2"> | Example-</span><span class="si">{idx}</span><span class="s2">: </span><span class="si">{exmaple}</span><span class="s2">&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="n">score</span><span class="p">,</span> 
                          <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> 
                          <span class="n">exmaple</span><span class="o">=</span><span class="n">reply</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Concreteness Score: 1.55 | Example-0: I&#39;ll go look for that
Concreteness Score: 1.55 | Example-1: I&#39;ll go search for that
Concreteness Score: 1.89 | Example-2: I&#39;ll go search for that top
Concreteness Score: 2.04 | Example-3: I&#39;ll go search for that t-shirt
Concreteness Score: 2.37 | Example-4: I&#39;ll go look for that t-shirt in grey
Concreteness Score: 2.37 | Example-5: I&#39;ll go search for that t-shirt in grey
</code></pre></div><p>员工的表达越具体，具体性得分越高。</p>
<p>跟JCR中的得分不一样，但是案例的得分趋势是一致的。基本上从上至下，每个员工回复对应的具体性得分越来越高。</p>
<p><img loading="lazy" src="img/Concreteness_JCR_computation.png" alt=""  />
</p>
<br>
<h3 id="方法二">方法二</h3>
<p>cntext内置了效价情感分析函数和Concreteness词典，因此本任务实际上可以用cntext完成。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext==1.7.7
</code></pre></div><p>代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># load the concreteness.pkl dictionary file</span>
<span class="n">concreteness_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;Concreteness.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;Concreteness&#39;</span><span class="p">]</span>
<span class="n">concreteness_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">word</th>
<th style="text-align:right">valence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">roadsweeper</td>
<td style="text-align:right">4.85</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">traindriver</td>
<td style="text-align:right">4.54</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">tush</td>
<td style="text-align:right">4.45</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">hairdress</td>
<td style="text-align:right">3.93</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">pharmaceutics</td>
<td style="text-align:right">3.77</td>
</tr>
</tbody>
</table>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">reply</span> <span class="o">=</span> <span class="s2">&#34;I&#39;ll go look for that&#34;</span>

<span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                              <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                              <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">score</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1.85
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">employee_replys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I&#39;ll go look for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that top&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go look for that t-shirt in grey&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt in grey&#34;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">reply</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">employee_replys</span><span class="p">):</span>
    <span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                                  <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    
    <span class="n">template</span> <span class="o">=</span> <span class="s2">&#34;Concreteness Score: </span><span class="si">{score:.2f}</span><span class="s2"> | Example-</span><span class="si">{idx}</span><span class="s2">: </span><span class="si">{exmaple}</span><span class="s2">&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="n">score</span><span class="p">,</span> 
                          <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> 
                          <span class="n">exmaple</span><span class="o">=</span><span class="n">reply</span><span class="p">))</span>
    
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Concreteness Score: 1.55 | Example-0: I&#39;ll go look for that
Concreteness Score: 1.55 | Example-1: I&#39;ll go search for that
Concreteness Score: 1.89 | Example-2: I&#39;ll go search for that top
Concreteness Score: 2.04 | Example-3: I&#39;ll go search for that t-shirt
Concreteness Score: 2.37 | Example-4: I&#39;ll go look for that t-shirt in grey
Concreteness Score: 2.37 | Example-5: I&#39;ll go search for that t-shirt in grey
</code></pre></div><p><br><br></p>
<h2 id="代码获取">代码获取</h2>
<p><font color=blue><a href="JCR_Concreteness_Computation.zip">点击下载本文代码</a></font></p>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2022-05-workshop/">结构模型、DSGE、Stata实证前沿、空间计量、Python数据挖掘|2022五一工作坊</a></p>
</li>
<li>
<p>想随时随地系统学习Python文本分析，可以选择</p>
<ul>
<li><a href="https://textdata.cn/blog/management_python_course/"><strong>Python网络爬虫与文本分析 | 2021录播课(虽是录播，但章节更多一些)</strong></a>。</li>
</ul>
</li>
<li>
<p>更喜欢有互动感通过直播学习，可以考虑</p>
<ul>
<li><a href="https://textdata.cn/blog/2022-05-workshop/7-Python.html"><strong>Python网络爬虫与文本分析 | 2022五一直播</strong></a></li>
</ul>
</li>
</ul>
<br>
<h2 id="相关文献">相关文献</h2>
<p>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</p>
<p>Snefjella, Bryor, and Victor Kuperman. &ldquo;Concreteness and psychological distance in natural language use.&rdquo; <em>Psychological science</em> 26, no. 9 (2015): 1449-1460.</p>
<p>Paetzold, G. H., and L. Specia (2016), “Inferring Psycholinguistic Properties of Words,” in Proceedings of the North American Association for Computational Linguistics-Human Language Technologies 2016, 435–40.</p>
<p>Packard, Grant, and Jonah Berger. &ldquo;How concrete language shapes customer satisfaction.&rdquo; <em>Journal of Consumer Research</em> 47, no. 5 (2021): 787-806.</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</title>
      <link>https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/</link>
      <pubDate>Thu, 07 Apr 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/2022-04-07-word-embeddings-in-social-science/</guid>
      <description>在大数据时代的背景下，基于大数据的分析处理技术为以“数据驱动”的社会科学研究创造了新的发 展契机。其中，词嵌入(Word Embeddings)技术借势大数据浪潮，以其高效的词表征能力和强大的迁移学习 能力在文本分析领域受到越来越多的关注。不同于传统的文本分析路径，词嵌入技术不仅实现了对非结构 化文本数据的表征，还保留了丰富的语义信息，可以实现对跨时间、跨文化文本中深层次文化信息的挖掘， 极大丰富了传统的社会科学实证的研究方法。文章总结了词嵌入技术的基本原理及特点，系统地梳理了词 嵌入技术的六大应用主题：社会偏见、概念联想、语义演变、组织关系、文本情感和个体决策机制。随后， 文章归纳了词嵌入技术的基本应用流程。词嵌入技术还面临文本数据的选择、中文文本的分词处理、单词 语义信息的表征层次三种挑战，文章归纳了相应的应对思路与方法。最后，基于词嵌入技术的强大适应能 力，未来研究可以进一步关注该技术在管理领域的应用前景，包括政策效应评估、用户推荐系统、品牌管 理、企业关系管理、组织内部管理、中国传统智慧与管理问题六个方面。In the context of the era of big data, the analysis and processing technology based on big data has created new development opportunities for data-driven social science research. Among them, word embedding (Word Embeddings) technology takes advantage of the wave of big data, and has received more and more attention in the field of text analysis with its efficient word representation ability and powerful transfer learning ability. Different from the traditional text analysis path, the word embedding technology not only realizes the representation of unstructured text data, but also retains rich semantic information, which can realize the mining of deep cultural information in cross-time and cross-cultural texts, which greatly It enriches the traditional social science empirical research methods. This article summarizes the basic principles and characteristics of word embedding technology, and systematically sorts out six application themes of word embedding technology: social bias, concept association, semantic evolution, organizational relationship, text emotion, and individual decision-making mechanism. Subsequently, the article summarizes the basic application process of word embedding technology. Word embedding technology also faces three challenges: the selection of text data, the word segmentation processing of Chinese text, and the representation level of word semantic information. The article summarizes the corresponding countermeasures and methods. Finally, based on the strong adaptability of word embedding technology, future research can further focus on the application prospects of this technology in the field of management, including policy effect evaluation, user recommendation system, brand management, enterprise relationship management, organization internal management, traditional Chinese wisdom and management There are six aspects to the problem.</description>
      <content:encoded><![CDATA[<p>词嵌入技术是文本分析中技术含量较高，可从文本中测量出人类认知信息。而即时是有一定学习能力的人，当阅读大量文本很难察觉文中是否有内置(预置)的信息，如作者的偏见、态度、刻板印象，等人类复杂认知。词嵌入技术可以将这类难以察觉的线索挖掘、测量。</p>
<br>
<br>
<h2 id="文献">文献</h2>
<p>本文全文摘自</p>
<p>冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html</p>
<br>
<h3 id="摘要">摘要</h3>
<p>在大数据时代的背景下，基于大数据的分析处理技术为以“数据驱动”的社会科学研究创造了新的发 展契机。其中，词嵌入(Word Embeddings)技术借势大数据浪潮，以其高效的词表征能力和强大的迁移学习能力在文本分析领域受到越来越多的关注。不同于传统的文本分析路径，词嵌入技术不仅实现了对非结构化文本数据的表征，还保留了丰富的语义信息，可以实现对跨时间、跨文化文本中深层次文化信息的挖掘， 极大丰富了传统的社会科学实证的研究方法。文章总结了词嵌入技术的基本原理及特点，系统地梳理了词 嵌入技术的六大应用主题：社会偏见、概念联想、语义演变、组织关系、文本情感和个体决策机制。随后， 文章归纳了词嵌入技术的基本应用流程。词嵌入技术还面临文本数据的选择、中文文本的分词处理、单词 语义信息的表征层次三种挑战，文章归纳了相应的应对思路与方法。最后，基于词嵌入技术的强大适应能 力，未来研究可以进一步关注该技术在管理领域的应用前景，包括政策效应评估、用户推荐系统、品牌管 理、企业关系管理、组织内部管理、中国传统智慧与管理问题六个方面。</p>
<br>
<p><strong>关键词</strong>: 词嵌入；自然语言处理；文本分析；社会科学；管理领域应用</p>
<p><br><br></p>
<h2 id="引言">引言</h2>
<p>作为人类开展文化交流和情感沟通的基本载体，语言承担了重要的信息交换功能。借助 于各类语言表达形式，人们将诸如知觉、思维、态度和情感等复杂的心理活动转化成特定的 语言。 而作为语言的典型载体之一，文本既能够在个体层面上反映人们的内心活动，也能 够在组织和社会层面上反映集体文化。 因此，从文本内容挖掘个体深层次的心理活动和 人类社会的文化沿革是社会科学的基本研究路径。</p>
<p>长期以来，<strong>在社会科学尤其是管理学和心理学等领域，实证研究多以针对实验、问卷和结构化的二手数据的量化分析为主导，而对于非结构化的文本材料(如访谈记录)仍以质性分析为主</strong>。[5] 在大数据时代，“数据+行为+交叉学科”已成为社会科学发展的必然方向。而<strong>计算社会科学</strong>的兴起则为我们理解人类行为、探讨社会现象提供了新的研究素材、视角和手段[6] 。随着互联网技术的飞速发展，人们在网络上发表<strong>大量包含思维、情感、观点的文本信息，这些井喷式爆发的文本为“以数据驱动”的社会科学研究提供了可及的信息来源</strong>。若能对之加以利用，无疑将拓宽社会科学研究的方法。[7 - 9] 然而，社会科学领域的<strong>传统文本研究方法以人工编码为主，其时间投入过大、成本较高、客观性相对较弱等不足极大地限制了文本数据 [10] 在实证研究中的应用</strong>。 所幸的是，以自然语言处理(Natural Language Processing; NLP)为核心的计算机文本分析技术(Computerized Text Analysis)的发展为大数据文本在社会科学领域 中的应用带来了契机。 “词”作为文本的最小单元，是计算机进行文本分析的基础。在自然语言处理领域，“词” 主要以向量(vector)的形式表示。而<strong>词嵌入(Word Embeddings)技术，即是一种可以把高维词 向量映射进低维向量空间，以此来实现词义理解的计算机文本分析技术</strong>。相较于其他自然语 言技术，词嵌入技术不仅展现出了高效的学习能力，而且<strong>允许计算机从更高的意义单元(即 目标词的上下文)出发理解词义、刻画“词”之间的相对关系</strong>，因此逐渐成为了自然语言处理的重要工具，<strong>在管理学、心理学等社会科学领域取得了丰富的研究进展</strong>。相比于传统的、以人工编码和词频统计为主导的文本分析方式，词嵌入的独特优势在于：</p>
<ul>
<li>借助计算机分析技术，可以在短时间内、以较低成本，实现对大规模文本数据的高效处理；</li>
<li>在挖掘文本特征和理解文本内容时，更多地依赖文本自身的分布规律，具有较强的客观性，其背后“数据驱动”的分析逻辑也使这一技术在探索性研究中具有较大的应用优势；</li>
<li>面对<strong>跨时间、跨文化比较</strong>的研究话题、以及在<strong>挖掘社会学、行为学变量</strong>及变量关系等领域展现出广阔的应用前景。</li>
</ul>
<p>词嵌入技术已在社会科学领域得到了广泛的应用，主要包括：<strong>社会偏见 、概念联想[14] 、关系网络和判断机制</strong>[16] 等六大主题，大量研究见诸 Nature、Science、PNAS、Academy Management Journal、American Sociological Review、Management Science 等国际期刊。反观国内的社会科学领域，词嵌入方法的应用价值还未得到足够重视和讨论。据此，本文通过介绍词嵌入技术的基本原理、梳理国外社会科学领域对词嵌入的应用情况，以期帮助国内社会科学研究者了解该技术独特的应用价值，推动词嵌入技术在大数据时代背景下对我国社会科学研究方法的丰富和推动。具体而言，本文：</p>
<ul>
<li>梳理了词嵌入技术原理，以帮助学者深入了解词嵌入在文本分析方面的技术优势；</li>
<li>梳理了现有文献中社会科学研究者们利用这一技术的相关应用研究，展现了在面对实际问题时可以如何利用词嵌入技术进行实证分析，以帮助学者了解该技术的适用领域；</li>
<li>总结词嵌入技术的基本应用流程，提供方法指引；</li>
<li>归纳了词嵌入技术面临的三方面挑战 ——文本数据的选择、中文文本的分词处理、语义信息表征，并提出了相应的应对思路；</li>
<li>从政策效应 评估、用户推荐系统、品牌管理、企业关系管理、组织管理与中国传统智慧和管理问题这六个方面出发，探讨词嵌入技术在以管理为代表的社会科学研究中的应用潜力和价值，以期丰富大数据时代背景下我国的管理科学研究方法。</li>
</ul>
<p><br><br></p>
<h2 id="一词嵌入技术的基本原理">一、词嵌入技术的基本原理</h2>
<p>不同于基于词频统计的文本分析方法，词嵌入技术的核心特征在于从文本的全局语义信息出发对“词”进行表征学习， 即<strong>大规模利用文本中“词”的上下文信息，将文本词汇映射至高维向量空间以实现词的向量化表示，使得词向量之间既保留着“词”在语义层面的关联，又满足向量所适用的代数运算性质</strong>。 在此基础上，通过度量词向量之间的几何关系(即“距 离”)便能够刻画“词”在现实语义中的关系。 进一步地，我们利用词与词之间这种可被量化的语义关系来探讨社会科学领域下的概念之间的相似性或相关性，并由此反映特定的社会文化和认知现象，乃至刻画社会、心理变量与其它行为变量间的相关关系。因此，<strong>词嵌入技术的应用主要包含两大步骤，即首先利用词嵌入模型从文本数据中获得对“词”的向量表征，再计算词向量距离进行相关性分析。</strong></p>
<br>
<h3 id="1-1-词的向量表征">1. 1 “词”的向量表征</h3>
<p>“词”的向量化表征是计算机进行文本分析的基础，也是词嵌入技术的本质属性。纵观计算机文本分析的历史，词向量的表征方法主要经历了以下两个发展阶段：</p>
<h4 id="第一个阶段-假设词语之间相互独立br"><strong>第一个阶段： 假设词语之间相互独立</strong><br></h4>
<p>第一个阶段是从词典出发、基于词频统计规则对“词”的离散型表征。例如，热向量编码(one-hot vector)通过</p>
<p>建立基于目标文本(“猫很可爱，狗也很可爱”)的分词词典({“猫”: 0，“狗”: 1，“也”: 2，“很”: 3，“可爱”: 4})，将每个“词”都表示为一个向量，使其维度与词典长度相当，且每个元素取值为0 或 1。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">“猫”   = (1, 0, 0, 0, 0)
“狗”   = (0, 1, 0, 0, 0)
“也”   = (0, 0, 1, 0, 0)
“很”   = (0, 0, 0, 1, 0)
“可爱” = (0, 0, 0, 0, 1))
</code></pre></div><p>这一类词表征方法虽然简单直观，但是在面对大规模文本时，词典长度的激增易造成参数空间的“<strong>维度灾难</strong>”问题(Curse of Dimension)① 。并且，粗糙的信息表征思路<strong>忽视了“词”的频率、上下文以及“词”之间的关联，使得这一类词向量无法反映“词”的语义信息</strong>。</p>
<br>
<h4 id="第二阶段-认为词语之间有千丝万缕的联系br">第二阶段: 认为词语之间有千丝万缕的联系<br></h4>
<p>为了提升词向量的表征质量，Deerwester 等[20] 主张从更高的文本意义单元理解文本词汇的含义。由此，分布式表征(distributional representation)[18] 成为了第二阶段的词表征方法。<strong>分布式假设(distributional hypothesis)是分布式表征方法的理论支撑，也是词嵌入技术背后的核心逻辑基础——即上下文相似的“词”拥有相似的或相关的语义，它能够反映了人类的语言使用习惯，也符合人的现实认知逻辑。[16,18,21] 人们倾向于对具有相似或者相关特征的对象产生认知关联，体现在文本层面则是相近的语言表达或高度的共现频率，即相似的上下文语境</strong>。基于此，通过分析目标词与其上下文词汇之间的统计分布规律可以学习到目标词的众多文本信息，使得词表征结果囊括文本语境的特征。因此，分布式表征的思想被广泛应用于后续的语义学习中，成为了词嵌入技术的基本逻辑。</p>
<p>其中，较为出色且经典的是 Mikolov 等人在 2013 年提出的 <strong>Word2Vec 模型</strong>，[18] 它标志 着 词嵌入模型 的 正 式 诞 生 。除了 Word2Vec 词嵌入模型外， Pennington 等 [23] 提出了同样具有高效学习能力的 <strong>GloVe</strong>(Global Vectors for Word Representation)学习框架，通过对词共现矩阵的矩阵分解，实 现对“词”的表征。此外，为了提升训练速度、适应海量文本学习，后 多对于词的分布式表征的改进算法，包括：<strong>fastText 算法</strong>、 谷歌的 <strong>ELMo</strong>(Embedding from Language Models)语言模型[25] 和 <strong>BERT</strong>(Bidirectional Encoder Representation from Transform) 语言模型[26]等。</p>
<p>在将文本信息嵌入进每一个“词”之后，我们<strong>获得了“词”的向量表示，即在词向量空间中的位置，而词与词之间的语义关联可以通过向量空间中点与点的位置距离来反映</strong>。词嵌入技 [27,28] 术下的向量具有两项重要的几何性质——<strong>“聚类”(clustering)和“并行”(parallel)</strong>。 其中，</p>
<ul>
<li>“聚类”性质是指，现实语义相近的“词”在向量空间中的位置也相近。 例如， “挪威”与“瑞 典”的词向量更接近，而“意大利”和“德国”的词向量更接近。</li>
<li>而“并行”性质是指，向量空 间中的词向量之间满足基本的代数运算性质，且这种运算逻辑基本符合“词”的现实语义逻辑。</li>
</ul>
<p>[18] 例如，从语义逻辑来看，“国王(King)”和“王后(Queen)”的区别平行于“男人(Man)”和“女人 (Woman)”的区别，反映到对应词向量上即可以得到“King−Man+Woman=Queen”的代数形式。</p>
<p><img loading="lazy" src="img/kingqueen.png" alt=""  />
</p>
<p>综合以上内容可知，词嵌入虽然聚焦在“词”这一最小的文本单位上，但是看到的是丰富的全局文本语义信在“词”上的投射和体现。这不仅与传统的、基于词频的文本分析方法在逻辑上有着本质的区别，更能够为文本分析提供更深刻、更生动的洞察，构成了词嵌入文本分析技术在大数据时代的社会科学领域相关研究中的独特应用优势。</p>
<p><br><br></p>
<h3 id="12-词向量的距离计算">1.2 词向量的距离计算</h3>
<p>在词嵌入领域，词向量间的“距离”是词与词之间相关程度的度量指标，是分析概念之间的相关性的基础。词嵌入技术将文本中的“词”映射为 N 维欧式空间中的“点”，“词”在空间中 的位置坐标即用其对应的 N 维向量来标识。由于点的位置反映了词语的语义，因而点与点之间的空间距离即反映了词与词之间的语义相似性，对词向量进行特定的代数运算(如加减、 内积等)<strong>能够用以度量词与词之间、概念与概念之间乃至文档与文档之间的相关性</strong>。</p>
<h4 id="121-词与词之间的距离">1.2.1 词与词之间的距离</h4>
<p>设在 n 维语义空间中，单词 A 和 B 分别对应词向量 vA=vA1,&hellip;, vAn 和 vB=vB1,&hellip;, vBn， vA 与 vB 之间的距离计算方式主要有以下两种：</p>
<ul>
<li>余弦相似度(cosine similarity)：。余弦相似度衡量词向量 vA 和 vB 之间的向量夹角的余弦值，其取值范围为[-1,1]。余弦相似度取值为 0，则代表单词 A 和 B 之间不存在语义关系；而取值越靠近 1，表明单词 A 和 B 之间具有正相关性；反之，取值越 靠近-1，则表明单词 A 和 B 之间具有负相关性</li>
<li>欧式距离(euclidean distance)：欧式距离越小表明单词 A 和单词 B 在词向量空间中的位置越近，之间的语义关系越强；反之，欧式距离越大表明单 词 A 和 B 语义关系越弱。</li>
</ul>
<br>
<h4 id="122-概念与概念之间的距离">1.2.2 概念与概念之间的距离</h4>
<p>在词嵌入分析领域，一个概念是由一系列“相关词”组合而成，例如，“<strong>女性</strong>”概念可以通过“<strong>女人</strong>”、“女生”、“<strong>母亲</strong>”等名词来表达。<code>而在对比不同属性概念(如“女性”vs.“男性”与“智慧”)之间的相关性时，需要逐个计算概念间的相对距离(“女性”与“智慧”的距离 vs.“男性”与“智慧”的距离)</code>。Garg 等[13] 、Caliskan 等[17] 分别构建了以下两种相对距离的计算方法，并为众多后续研究所采用：</p>
<ul>
<li>
<p><strong>相对范数差函数</strong>(relative norm distance)：</p>
<ul>
<li>$$vm∈Mvm−vA−vm−vB $$</li>
</ul>
</li>
<li>
<p>该函数用于衡量两项目标词概念与某一项特征概念的相对距离。其中，M 代表特征概念(如“智慧”)，vm 为所属概念的相关词向量(如“聪明”)；vA 和 vB 分别代表两类目标词向量(如“男性”vs.“女性”)。该函数的含义为：在“男性”和“女性”两类群体中，哪一类群体与“智慧”这一概念更相关。若函数值为正，则代表“女性”与“智慧”更相近；若函数值为负，则代表“男性”与“智慧”更相近；若函数值靠近 0，则表明“智慧”不存在明显的性别偏向。</p>
</li>
<li>
<p><strong>词嵌入相关性检验</strong> (Word Embeddings Association Test; WEAT)：</p>
<ul>
<li>$$s(X,Y,A,B)=vx∈Xs(vx,A,B)−vy∈Ys(vy,A,B)$$</li>
<li>$$s(vw,A,B)=meanva∈Acos(vw,va)−meanvb∈Bcos(vw,vb)$$</li>
<li>该框架用于衡量两组目标词 X,Y(Target Words，如“男性”vs.“女性”)与两组属性词 A,B(Attribute Words，如“事业”vs.“家庭”)在语义上的相对距离差异，其中 vw 为所属概念的相关词向量(如在描述“男性”概念时，人们往往会使用“男生”、“父亲”、“男人”等词语)。s(vw,A,B)表示单词 vw 与两类属性词 A 和 B 的相对距离，正值代表其与 A 属性距离更近、语义更相关，反之则反；而 s(X,Y,A,B)则衡量了两项目标词 X,Y 和两项属性词 A,B 相对距离的差异，即在 “ 男性 ” 和 “ 女性 ” 两类群体中， 哪一类群体与 “ 事业 ” 或 “ 家庭 ” 的文化概念更相关。 若s(X,Y,A,B)为正值，则表明相比于“女性”，“男性”与“事业”的语义相关性更高，反之则表明“女性”与“事业”的语义相关性更高。此外，WEAT 框架还提供了相应结果的显著性检验方式以及效应量指标。</li>
</ul>
</li>
<li>
<p><strong>文档与文档之间的距离—词移距离</strong>(Word Mover’s Distance) minT≥0i,j=1nTij∙ci,j, s.t. jnTij=di , ∀iϵ1,&hellip;,n</p>
</li>
<li>
<p>除了概念间的相关性分析，我们可以通过文档间的相似性来探讨如文本主题、个体及组织之间的相关性问题，Kusner 等[31] 提出“词移距离”这一计算方法能够实现文档间的相似 性度量。词移距离即一个向量空间移动到另一向量空间所需的最小距离，通过对两个向量语 义空间中所有词向量间的欧式距离进行加权求和，以此来衡量两个文本间的相似性，如上式 所示。其中，c(i, j)为词向量间的欧式距离；Tij 为词向量之间的权重(由 TF-IDF② 计算加权值)。 函数值越大代表两个文本的相关程度越低，反之越高。</p>
</li>
</ul>
<p><br><br></p>
<h2 id="二词嵌入技术的优势">二、词嵌入技术的优势</h2>
<p>传统的社会科学研究通常需要借助科学实验、社会调查和人工编码等方法，依赖于专家学者的领域知识和实践直觉，存在主观性较强、耗时、耗资源的缺点。 另外，传统的社会科学研究局限于小样本数据和历史数据的不足，通常关注当下的、有限范围的社会情景， 难以进行跨时间、跨文化的分析。 反观，以词嵌入为代表的计算机化的文本分析：</p>
<ul>
<li>可处理大文本数据，不仅极大地节省人力和时间投入，而且可以拓宽现有社会科学研究的素材；</li>
<li>能够利用现有数据和先验知识改进学习算法，可拓展性和重复性强；</li>
<li>能依据文本内在的分布规律和领域知识，<strong>挖掘人们内隐层面的认知信息，结果更加客观真实</strong>；</li>
<li>能够从大规模文本中挖掘代表整体社会的认知，<strong>尤其擅长进行跨时间、跨文化的文本研究</strong>，结论不仅具有广泛的样本代表性，而且<strong>可以展示相关文化概念、思想观念等研究对象的纵向历时演化</strong>。 这些优点极大地丰富了社会科学的研究方法，拓展了社会科学的研究视野。传统的社会科学 研究方法与词嵌入技术的具体区别见表 1。</li>
</ul>
<br>
<p><strong>表 1 传统的社会科学研究与词嵌入技术的区别</strong></p>
<table>
<thead>
<tr>
<th>对比维度</th>
<th>传统的社会科学研究路径</th>
<th>基于词嵌入技术的社会科学研究路径</th>
</tr>
</thead>
<tbody>
<tr>
<td>研究工具</td>
<td>问卷、访谈、实验、案例分析等</td>
<td>Word2Vec、GloVe 等词嵌入模型，以及词向量、概念及文本的相关性计算</td>
</tr>
<tr>
<td>方法依据</td>
<td>基于实践经验和严格的理论推断，依赖于特定学科内专家学者的领域知识和实践直觉，是以人为中心的研究方法；围绕研究假设进行数据检验的分析路径</td>
<td>基于语言文本来理解文化概念和思想观念，综合利用社会科学理论、计算机科学等探讨社会、心理和行为层面的问题，是人智与计算机相结合的分析方法 ；不依赖严格的假设，利用数据挖掘展开探索性的研究</td>
</tr>
<tr>
<td>检验标准</td>
<td>大部分研究结论缺少严格客观的评断标准，主观性较强</td>
<td>有多项较为成熟的指标及评价流程，具体包括：检验词嵌入模型的训练效度 ( 模型在特定的测试任务集上的表现 ) 和检验研究结论的外部效度(将结论与其他社会调查数据、其他研究方法的结果展开对比)</td>
</tr>
<tr>
<td>数据来源</td>
<td>调研记录、实验数据、二手数据、文献等；受限于成本投入，数据来源较为单一且体量较小</td>
<td>数据来源广泛，能够熟练处理包括会议记录、网络文本、新闻书籍等非结构化的文本数据；在处理大规模、跨文化、跨时间的文本数据时有极大的优势</td>
</tr>
<tr>
<td>信息层次</td>
<td>以基于自我报告的外显认知为主，在获取被访者内隐认知时依赖于间接的方法设计；研究较大地依赖于样本选择，结论在跨时间、跨文化上的代表性有限</td>
<td>允许研究者直接挖掘文本所反映的内隐社会认知；研究较多从社会、集体层面的文本范围着手，结论具有较强的代表性和普适性</td>
</tr>
<tr>
<td>其他特点</td>
<td>应用过程中耗时长、成本较高；主观性较强；结论的可复现性较弱</td>
<td>针对大数据文本的无监督训练、时间人力投入小；客观性较强；结论的可复现性较强</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="三词嵌入技术在社会科学领域的应用话题">三、词嵌入技术在社会科学领域的应用话题</h2>
<h3 id="31-社会偏见刻板印象">3.1 社会偏见/刻板印象</h3>
<p><strong>文本语言能够反映人类对世界的认知和态度，基于词嵌入的文本分析方法可以有效地挖掘社会偏见和刻板印象</strong>。Garg 等[13] 采用词嵌入技术分析了来自纽约时报、谷歌新闻、谷歌图书及美国历史文本库(Corpus of Historical American English; COHA③ )的文本数据，揭示了 1900～1990 年间美国社会在性别和种族两大议题上的刻板印象及其历史变化。Garg 等[13] 首先以十年为单位将文本数据分为 9 份，并针对每一份文本数据使用词嵌入技术，以获得“词” 的向量表征。然后，他们整理了相应概念的单词列表，其中包括：与“性别”维度相关的“男性”词汇[如 he]和“女性”词汇[如 she]；“白人”词汇[如 Harris]、“亚裔”词汇[如 Huang]和“拉丁 裔”词汇[如 Ruiz]。进一步，他们构建了相对范数差(relative norm difference)函数，用以计算 一组词向量(如“男性”vs.“女性”)与目标词(如专业工作)的相对距离，以此度量社会刻板印象 的程度。Garg 等[13] 发现近百年间在美国社会的认知中始终存在着较为明显的性别偏见和种 族偏见。例如，“女性”往往和护士、保洁、舞者、秘书等职业联系更为紧密，而与工程师、 木匠、技术人员等职业更为疏远。此外，亚裔姓名和教授、科学家、化学家和工程师等学术职位联系更紧密，白人姓名往往与警察、统计学家、摄影家等职位联系更紧密。通过纵向分析概念间距离随时间推移的动态演化，Garg 等[13] 发现，这两类刻板印象随着时间呈减弱势 态，结合相关社会科学理论和历史事件，他们进一步提出 60、70 年代的美国民权运动是改善刻板印象的关键事件，而基于词嵌入的概念间相对距离也如实地反映了美国民权运动对于 国民性别、种族认知的深刻影响。 <br></p>
<p>作为文化的产物之一，歌曲也能反映社会认知中的偏见/刻板印象。Boghrati 和 Berger[12]  利用 Word2Vec 词嵌入技术，挖掘了自 1965 年至 2018 年的近 60 年间美国公告榜(Billboard) 上流行歌曲歌词文本中所隐含的性别偏见。一方面，他们在流行音乐中发现了显著的“<strong>厌女症(misogyny)”刻板印象(如图 2)，相比“男性”词汇，人们更少将“女性”词汇和“能力/ 成功/热情”等具有积极属性的词汇相关联</strong>。但另一方面，歌曲中所反映的性别偏见随时间的推移呈现逐渐减弱的趋势。进一步地，Boghrati 和 Berger[12] 通过控制歌曲创作者的性别因素，发现男性作词人是影响歌曲“厌女症”现象变化的关键因素。<br></p>
<p><img loading="lazy" src="img/musicSuccessGenderbias.png" alt=""  />
</p>
<p>为了检测词嵌入技术是否能够有效挖掘文本中的社会偏见，Caliskan 等[17] 对比了词嵌入模型和<strong>内隐联想测试</strong>(Implicit Association Test; IAT)④ ——<strong>一种社会科学领域中最常用的社会 偏见/刻板印象等内隐认知的测量方法</strong>—的差异。虽然 IAT 能够有效捕捉个体的社会偏见的内隐认知，但是该方法需要严格的实验环境、耗时较长且测量样本往往受到时间和空间的局限。Caliskan 等[17] 利用 GloVe 词嵌入模型构建了 <strong>WEAT(Word Embedding Association Test) 分析框架</strong>，并利用这一框架研究了 8 项内隐认知，如“科学-男性”和“艺术-女性”，并将其与 基于 IAT 的研究结果进行了对比。[17] 他们的研究表明，基于词嵌入技术的结论与基于 IAT 的结论具有高度且稳定的一致性。Caliskan 等[17] 进一步指出，在未来的内隐态度研究中，词嵌入技术不仅能够作为 IAT 测量方法的补充，更能够在样本代表性、研究成本和研究视野等方面展现出 IAT 所不具备的独特优势。<br></p>
<p>除了探讨<strong>社会偏见</strong>与刻板印象的跨时特征之外，词嵌入技术还适用于<strong>跨文化</strong>的对比分析。 Defranza 等 [33] 利用词嵌入技术探讨了不同地域在性别偏见方面的程度差异。 他们利用 fastText 模型和 WEAT 分析框架， 从 49 类不同语种的文本中分别挖掘不同社会文化下的性别偏见现象。 结果显示，当一个地区的性别语言(Gendered language)——即语言中更加区分词汇的性别属性和使用者的性别身份(如泰语、芬兰语)更强时，该地区的性别偏见更加明显(图 3)，具体表现为男性与积极属性词汇的关联性更强，而女性与积极属性的联系更弱。 这一结果在一定程度上验证了萨皮尔-沃夫假说(Sapir-Whorf hypothesis)，即<strong>语言能够塑造的思维和认知</strong>。<br></p>
<p><img loading="lazy" src="img/languageBiasGender.png" alt=""  />
</p>
<p><br><br></p>
<h3 id="32-文化认知">3.2 文化认知</h3>
<p><strong>历史无法复刻，但承载历史痕迹的文本资料能够帮助人们窥探特定时代背景下的文化内涵</strong>。Kozlowski 等[28] 利用词嵌入技术分析了自 1900～1999 年来 100 余年间公开发表的书籍， <strong>探讨了 20 世纪美国社会对七大等级文化(财富、道德、职业、性别、教育、品味、身份地位) 的共识和演变规律</strong>。首先，作者利用 Word2Vec 模型，构建了一系列标度等级文化维度(如“贫 穷-富有”、“道德-不道德”、“男性-女性”等)的词向量空间(如“性别-财富”、“职业-道德”、“品 味-职业”、“教育-地位”等)。进一步，作者将一系列目标词分别映射进相应的等级维度空间， 以此标度这些目标词的多元等级属性。例如，在被映射进如图 4 所展示的“性别—财富”等级 维度空间后，“volleyball(排球运动)”一词表现出更靠近“feminine(女性气质)”和“rich(富有)”的 等级特征。此外，标度不同等级维度的向量之间的夹角也具有一定的社会文化含义。例如， <strong>“教育”与“道德”和“品味”的相关性较强且保持历时的稳定性，但与“职业”这一维度的相关性 相对较弱，这说明教育水平的提升能够提升人的修养和品味，但对职位状态和等级(如“失业” 和“就业”)的影响较小</strong>。Kozlowski 等[28] 的研究结果表明，词向量不仅可以反映特定概念间的语义关联和相互作用，还能够用于探讨多元文化维度之间的语义结构，从而推动实现更高层次构念的表征。</p>
<p><img loading="lazy" src="img/americanClass.jpeg" alt=""  />
</p>
<p><strong>作为文化概念的关键形式，社会认知(social cognition)是人们对各种社会刺激的综合加工过程， 是人们的社会动机系统和社会情感系统形成变化的基础</strong> 。</p>
<p><strong>社会认知包括社会信念 (social belief)和社会态度(social attitude)</strong>。而根据内隐—外显双系统理论， 社会认知可进一步分为外显社会认知(explicit social cognition)和内隐社会认知(implicit social cognition) 两类</p>
<ul>
<li><strong>外显社会认知</strong>强调个体可以通过自省的方式(如问卷法)报告的社会认知</li>
<li><strong>内隐社会认知</strong>描述个体无法内省的、潜意识层面的社会认知。</li>
</ul>
<p>然而，受限于现有对<strong>内隐认知</strong>的挖掘手段，有关内隐信念 (Implicit belief，如“亚洲人很聪明”)和内隐态度(Implicit attitude，如“我喜欢亚洲人”)的关系，已有研究要么将其混为一谈，要么将其作为互不干涉的独立构念。为了厘清该问题，Kurdi 等[14] 利用基于预训练的 fastText 词嵌入技术，分析了内隐态度和内隐信念的关联，并对比其与外显态度和外显信念的差异。具体而言，基于被试自我报告的实验结果显示，个体的外显态度与外显信念存在方向上的不一致性，例如，亚裔群体常被白人给予负面的评价(外显态 度)，但在智商、能力方面被认为有突出优势(外显信念)；而基于词嵌入技术的分析结果则表 明内隐态度与内隐信念具有一致性，内隐态度驱使内隐信念的产生。[14 ] 例如，白人群体有 较高的自我评价(内隐态度)，也认为本群体的智商高于亚裔群体(内隐信念)。另外，Kurdi 等[14] 发现人们对同一类属性词的认知也存在差异，例如，人们倾向于认为“book-smart(有学 问的)”优于“street-smart(生存力强的)”。综合可知，词嵌入技术可以作为挖掘文化概念认知以及社会认知的有效工具。</p>
<br>
<br>
<h3 id="33-语义内涵演变">3.3 语义内涵演变</h3>
<p><strong>语言的涵义会随着时代发生改变，而词嵌入技术的一大突出优势即表现为处理跨时段的大文本数据。文本语言的运用具有系统规律性，[35] 词嵌入模型训练生成的词向量能够有效 表征词的语义信息，通过针对来自不同历史时段的文本训练词嵌入模，有助于学者在时间维度上分析词义演变</strong>。Hamilton 等[36] 通过测量目标词的向量表示的时间位移值，来描述历史文本中高频词汇与多义词汇的词义历时变化。例如，如图 5 所示，“gay”作为一个多义词，在 1900 年代的文本中和“cheerful(开心)”和“frolicsome(玩闹)”词义更接近，而到 1990 年代则与“homosexual(同性恋)”、“lesbian(女性同性恋)”等更接近。进一步，Hamilton 等[36] 依据词嵌入技术的分析结果，提出了两条语义演化法则：(1)一致性，高频词汇会保持词义相对一致的历时演化规律；(2)新颖性，多义词汇的语义演化会更加快速。</p>
<p><img loading="lazy" src="img/DiachronicWordEmbeddings.png" alt=""  />
</p>
<p><strong>除了对普遍意义上的词汇含义的演变规律进行探讨，某些具体词汇的演化情况同样引起了学者们的关注，尤其是那些能反映特定文化背景或时代发展的词汇</strong>。 Rodman 等[11] 基于1855～2016 年间的纽约时报、路透社报道、美联社报道三大新闻文本集，挖掘并追踪了美国一个多世纪以来围绕“平等”一词的词义演变，其中包含了使用环境、指代对象等。他们发现，在上世纪 50 年代之前，即美国民权运动前，“平等”的词义与“社会”话题相关的词汇的关联度整体较高，但随着历史发展呈现减弱趋势，这一结果与美国民权运动前普遍存在的社会不公平现象相符合(如种族歧视)。而自上世纪 70 年代，即民权运动基本结束以来，“社会”与“经济”、“教育”等概念下的词汇的关联程度不断增强，反映了本世纪以来美国社会对“教育公平”，“经济公平”等热门话题的高度关注。可见，基于词嵌入的文本分析能够敏锐地捕捉到社会文化演变的信息线索，并能为社会、文化等领域的运动发展提供预示。</p>
<p><br><br></p>
<h3 id="34-文本情感分析">3.4 文本情感分析</h3>
<p>文本信息不仅包含词义，还表达情感。现有部分词嵌入模型在关注语义语法层面的表征 的同时，还进一步关注了词的情感信息，尤其是某些单词具有相似的上下文环境但所含的情 感态度截然相反 ( 如 “good” 和 “bad”) 。 例如， Tang 等 [37] 提出了 情感嵌入模型 (Sentiment Embeddings)，该模型不仅可以如词嵌入模型一样反映词义信息，还能识别词所包含的情感 信息，进而可以对文本(如在线评论)中的情感信息做出推断。</p>
<p><strong>情感的丰富性和语言的灵活性使得单词在不同文本环境下会呈现出差异化的情感特征</strong>。 例如，“I’m gonna put something offensive to some people.”中的“offensive(冒犯)”一词在该句话 中带有消极负面的情绪；而“#FSU offensive coordinator Sanders coached for Tennessee 1st  [37] #BCS title game.”中的“offensive(矛盾)”一词则不含明显的情感信息。 Xiang 和 Zhou[38] 藉此 指出，在推断词的情感信息时加入对文本 主题(topic)的考量将有效提高情感推断的准确性。 Ren 等[39] 基于 Word2Vec 词嵌入模型，结合主题模型(Latent Dirichlet Allocation; LDA)的方法， 提出了主题增强的词嵌入模型(Topic-Enhanced Word Embeddings; TEWE)。作者使用支持向 量机(Support Vector Machine; SVM)作为文本情感分类器，发现 TEWE 模型在文本情感分类 任务有突出表现。 例如，该模型能更有效地区分含有负面情感态度的词汇(如 insane)与正 面情感词汇(如 sane)的差异。在此基础上，Xiong 等[40] 进一步考虑了文本情感信息的多元化 特征 ， 构建了多元层次情感词嵌入模型 (Multi-level Sentiment-enriched Word Embeddings; MSWE)。他们发现，在标注“积极[#happy; :-)]—消极[#angry; :-(]”的情感标签任务中，MSWE 情感嵌入模型能够实现 85.75%的分类水平，表明基于词的情感表征能够更有效地挖掘文本 背后的情感。</p>
<p><br><br></p>
<h3 id="35-组织关系分析">3.5 组织关系分析</h3>
<p><strong>词嵌入技术在挖掘社会内隐认知方面展现出强大的效力，因而词嵌入技术可用于挖掘不 同组织在价值观和意识形态层面的关联，以此作为组织关系的推断依据</strong>。在此思路基础上， Spirling 和 Rodriguez[41] 采用 GloVe 和 Word2Vec 的词嵌入模型，分析了美国共和党和民主党两大政党在其各自的公开发言稿中对部分政治议题的所表达的态度。例如，对于“堕胎”议题， 两政党的理解存在较大争议：民主党认为“堕胎”是一种自愿选择，而共和党认为“堕胎”与“绝 育”、“公平”的话题相关；对于“税收”议题，两政党的理解则存在更多共识。由此可见，词嵌入技术不仅可以帮助我们了解政党组织在哪些政治议题上存在冲突，还可进一步衡量不同党派的政治关系。</p>
<p>Rheault 和 Cochrane[42] 分析了英国、加拿大和美国 20 世纪以来的议会记录文本，并依 据政党派别关系构建了“党派嵌入”(Party Embeddings)模型。学者利用词嵌入模型量化了不同党派在不同年代的议会观点中与特定“意识形态”维度(如自由 vs.保守、北部 vs.南部)的相关性，从而对比不同党派组织的意识形态差异。从整体上看，美国民主党的意识形态更靠近自 由派思想(如“民权”、“种族”、“枪支管控”)，而美国共和党的意识形态更具保守派和南部 [42] 州色彩(如“官僚”、“果农”、“烟草”)，且两党的意识形态差异随着时间不断扩大。 同样地， 对于加拿大，新民主党派与联盟党在意识形态上的政治冲突最为严重，魁北克政团与联邦主 [42] 义政团在事关“联盟”的政治议题上观点不同。 Pomeroy 等[15] 利用 GloVe 词嵌入模型分析了各个国家在联合国论坛的公开演讲文本，并使用词移距离(Word Mover’s Distance)来量化国 家讲演文本间的总体相似性，以此来反映国家立场及偏好的相似性。 作者发现，基于词嵌入技术的分析结果能够很好地反映国家间的政治关系。例如，虽然土耳其和希腊两国在投 票议程中表现出态度相似性(即一致的国家关系)，但实则两国在当年发生了边境军事冲突， 而这一冲突能从两国的联合国讲演文本中捕捉到线索。本研究指出，有关词嵌入在主体网络 关系的应用集中于党派关系和国际关系的研究，未来研究还可以考虑分析其他情景下的主体关系，如社交网络关系、品牌竞争关系、组织内部关系等。</p>
<p><br><br></p>
<h3 id="36-个体的判断与决策心理">3.6 个体的判断与决策心理</h3>
<p><strong>决策结果和决策信息线索之间具有表征关系，因而词嵌入技术能够通过挖掘概念间的内在关联，在一定程度上揭示个体在决策任务中的思维过程和决策依据</strong>。Bhatia[16] 在自然语言处理的框架下，验证了以往决策研究中的相关性判断机制，即人们在进行判断性任务(如：“A 多大可能属于 B?”)时，会出于直觉性心理去思考问题与选项间的相关性或相似性，并以 作为判断依据。具体而言，作者综合 Word2Vec、CCA、GloVe 几项词嵌入技术，基于谷歌 新闻和 GigaWord 文本库⑤ 训练生成词向量。进一步，作者通过对句子中每个“词”的向量求 取平均值，分别对判断问题(如“在以下的两座德国城市中，哪一个人口最多？”)与选项(如“汉堡”和“科隆”)实现表征。作者依据两者间的语义相关性来预测答案选项的概率分布，并据此模拟一般决策者的选择。例如，针对上述问题，基于词嵌入模型的预测结果为“汉堡”，与被试的选择高度相似。此外，词嵌入模型在其它测试任务(如经典的“Linda 问题”⑥ )下也预测了 [16] 决策者的选择倾向， 这一现象与代表性启发理论(representativeness heuristic)——一种依赖人的相关性感知进行识别和判断的心理决策过程——相符。这说明词嵌入模型在很大程度上 能够解释人的相关性判断机制， 甚至对其中常见的认知偏差 ， 如 <strong>合取谬误</strong> (conjunction fallacy)⑦ 、<strong>基础概率忽略</strong>(base rate neglect)⑧ 也能够予以反映。[16] 实证结果表明，词嵌入技术为我们理解人的直觉性判断心理提供了信息参考，能够帮助实现相对精准的决策预测。</p>
<p>另外，个体的风险感知和风险判断也是个体决策研究中的重要组成部分。Bhatia[43] 利用词嵌入技术探讨了人们面对各类风险源时的风险评估机制。作者通过基于谷歌新闻文本的预 训练 Word2Vec 模型，量化了不同风险源(技术性风险源：“新兴技术”、“能源”等；活动性风 险源：“运动”、“职业”等)与相关概念的语义联系，进一步 揭示了人们进行风险评估时的知 识表征内容(即内隐联想)。例如，当评估药物风险时，人们在潜意识里会联想到“毒品(drug)”、 “无序(disorder)”等具有高风险含义的概念(如图 6a 的词云图)；而评估运动风险时，人们容易 联想到“碰撞(crash)”、“斗争(combat)”等风险事件(如图 6b)。作者弥补了以往有关风险评估 的研究方法中难以预测样本外数据(如新型风险源)的缺陷，展现了词嵌入技术在理解和预测个体判断决策机制中的应用优势。</p>
<blockquote>
<p>⑥ “Linda 问题”是指“琳达，31 岁，单身，一位直率又聪明的女士，主修哲学。在学生时代，它就对歧视 问题和社会公正问题较为关心，还参加了反核示威游行。请问琳达更有可能是下面哪种情况？”有两个选 项：“A.琳达是银行出纳；B.琳达是银行出纳，同时她还积极参与女权运动”。相比于 A 选项，B 选项所 塑造的女性形象更贴近问题所提供的信息，因而人们会倾向于选择 B 选项。</p>
<p>⑦合取谬误是指人们总是认为两个事件的联合出现比只出现其中一件事的可能性要大。以“Linda 问题”为例， 人们会更多地选择 B，虽然从实际概率角度来讲，B 选项的概率应低于 A 选项。</p>
<p>⑧基础概率忽略是人们在进行主观概率判断时倾向于使用当下的具体信息而忽略掉一般常识的现象。</p>
</blockquote>
<p><img loading="lazy" src="img/EmbeddingsThemes1.png" alt=""  />
<img loading="lazy" src="img/EmbeddingsThemes2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四词嵌入分析的基本流程">四、词嵌入分析的基本流程</h2>
<p>词向量的表征学习存在两条路径：一是采用本地化的训练模型(local-trained model)。二是使用预训练的词嵌入模型(pre-trained model)。针对第一条路径，通常需要经历如下预处理和模型训练步骤(见图 7)：</p>
<h3 id="41-选择合适的语料库corpus">4.1 选择合适的语料库(corpus)</h3>
<p>语料库是用于训练词嵌入模型的文本集，“词”的表征效果以及后续的相关性分析依赖于训练文本的规模、质量及其所处的语言环境。对文本语料的选择需严格依研究者的具体问题而定，使研究主题/情景与文本主题/情景相对应，[41] 进而推动单词间的语义关系聚焦于特定的领域和视角上。例如，<strong>以探讨社会文化现象的研究可以选择新闻时报、社交媒体动态等社会文本作为主要语料；探讨消费者心理及行为的研究则可以将线上评论等商业沟通文本作为语料；关注组织行为的研究则以会议 记录、公司年报等组织内部的官方文本为主</strong>。</p>
<p>文本数据的获取主要有以下三种途径：</p>
<ul>
<li>第一， 国内外由政府、企业及其他组织或个人提供的公开的、已初步整理规范的文本数据库。如， 人民日报文本集(1946 年至今) ⑬ 、谷歌图书(包含 1500 年～2012 年期间公开出版的书籍，约占人类历史所有出版书目总数的 6%) ⑭ 、亚马逊评论集(包含 1996 年～2018 年亚马逊平台用 户对近 30 个产品品类的超过 2 亿条评论) ⑮ 、维基百科数据库(包含来自 400 多万篇文章的近 19 亿个单词的维基百科全文) ⑯ 等。</li>
<li>第二，借助“爬虫”程序收集文本数据。根据研究需要， 研究者可以在特定的网站上爬取一定的文本。例如，众筹平台的项目申报文本材料、微博平台的历时推文、论坛用户间的互动文本、企业员工在 Glassdoor ⑰ 等职业资讯网的日志评论等文本。</li>
<li>第三，纸质版文本转换成电子文本。必要时，研究者还可以将纸质文本录入为电子文 本形式，如员工日记、会议记录、线下心理咨询文稿等。</li>
</ul>
<h3 id="42-语料预处理pre-processing">4.2 语料预处理(pre-processing)</h3>
<p>常规的预处理流程包括：</p>
<ul>
<li>删除与文本内容无关的标点符号、特殊字符(如：数字，空格符，分行符，“©”)和其它停用词(如：代词、连词)。</li>
<li>此外，中文文本的预处理中还需要对文本分词 (segmentation)，从而将语料处理成由“词”这一最小的文本分析单位所构成的列表(如将语 句“我很开心”分词为“我”/“很”/“开心”)。现今常用的中文分词工具有“Jieba”、“HanLP”、 “THULAC”、“TopWORDS”等。</li>
</ul>
<h3 id="43-模型训练">4.3 模型训练</h3>
<p>在预处理后的语料文本中训练词嵌入模型，最终实现文本词的向量表征。当前主流的词嵌入模型有 Word2Vec、GloVe、fastText 等， 而在 Python 环境下，大量与自然语言处理相关的成熟的开源工具包(如，Gensim)中提供了相关的算法模块，并允许研究者对相关参数(词向量的维度、单词上下文的观测窗口的大小 等)进行调整。</p>
<p>此外，基于词嵌入模型的迁移学习能力，也可直接使用预训练的词嵌入模型(如，谷歌 的 GloVe 和 BERT )，从而获得基于其它大型语料库充分训练得到的词向量表示，并根据自身的文本特征对模型或表征结果进行微调(finetune)。但无论采用何种词向量表征路径， 在正式的词向量相关性分析之前，都<strong>有必要对词嵌入模型的训练结果，即词向量的表征效度进行评估</strong>。<strong>常见的评估方式是通过与人工标注的词相关性评分进行对比，检验二者是否一致， 以此判断词嵌入模型是否能够捕捉一般化的语义关系</strong>。目前，已有大量成熟的针对“词对” (word pairs)相似性的人工标注的测试集，如 MEN-3000(英文)[47] 、Wordsim240/297(中文)[48 ] [12,41]。</p>
<p><strong>词嵌入模型训练完成后，可以进一步依据研究目的进行词向量间的“距离”的几何计算， 主要包括词列表构建、相关性计算、有效性检验和稳健性检验四个步骤</strong> 。</p>
<ul>
<li>(1)<strong>构建词列表</strong>。 在词嵌入的文本分析中，特定概念通常由一系列近义词或同属性词列表构成。例如，在 Garg 等[13] 的研究中，他们构建的“男性”概念词列表包括“male”、“men”、“father”、“brother”等 20 个单词。</li>
<li>(2)计算词向量的相关性。针对具体研究问题，衡量“词”或者概念之间的语义关联(即 词向量间的“距离”)，主要包括余弦相似度(cosine similarity)、欧式距离(euclidean distance)两 种基本的计算方法(详见上文第一部分内容)。</li>
<li>(3)<strong>有效性检验</strong>。针对词嵌入的分析结果，我们 有必要进行进一步的检验，以保证结论的可靠性以及方法的有效性。具体包括两类检验方法： ①与对应年代的相关社会调查数据 (如，社会职业性别占比调查、社会偏见大调查 ) [13,49-51] 进行比对，以检验词与词的相关程度、变化是否与相应的指标数据、社会事件相吻合； ②与其它研究方法进行对比，如内隐联想测试(IAT)、主题模型(LDA)，以检验词嵌入模型能 否重复已有研究结果。[11,14]</li>
<li>(4)稳健性检验。作为一种无监督的探索性分析方法，词嵌入的 分析结果会因文本、模型等因素的不同而产生差异。通过变换词嵌入模型、参数、文本语料 或相关性计算方法，以检验研究结果的一致性。稳健性程度越高则代表基于词嵌入分析方法 的结论可靠性越高。</li>
</ul>
<p><img loading="lazy" src="img/EmbeddingApplicationProcess.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="五词嵌入分析方法的挑战与应对">五、词嵌入分析方法的挑战与应对</h2>
<p>作为一种计算机化的文本分析方法，词嵌入技术在文本数据的预处理、文本表征效果等方面有一定的条件限制，因而该技术面临着以下几个方面的挑战。学界一直致力于词嵌入分析方法的完善，并就下述问题提出了针对性的应对方案和解决思路，详见表 3。</p>
<p><strong>(1)词嵌入技术的分析效果依赖于文本数据的体量、质量和语言环境</strong>。 <br></p>
<p>①通常来说，文本数据规模越大越有利于词嵌入学习和提取更充分的语义信息，[11] 而体量较小的文本，可能会限制词嵌入模型的训练效果，难以将文本的全局语义信息嵌入到单词上。[11,18] 对此，Rodman[11] 提出了两种解决思路：一是采用预训练的词嵌入模型。预训练词向量通常基于超大型文本数据训练而来， 使模型具备较好且广泛的语义表征能力。 二是采取“自举法”(Bootstrapping)⑱ 。通过该方法生成规模更大的文本数据集，并针对不同的抽样过程生成不同的词向量结果，随后求取其平均值以增强词向量的稳定性和有效性。<br></p>
<p>②除此以外，文本语料的选择应考虑到其依存的文本情景、社会文化环境等背景信息。文本所依存的语言环境、文化观念和观点立场在很大程度上影响着文本词汇的分布方式(即单词上下文)，因此利用不同文化背景的文本训练词嵌入模型，可能会产生不同的研究结论。Spirling 和 Rodriguez[41] 对美国国会议事文本的分析所示，不同的党派组织对同一政治 议题的理解存在“冲突”(如，“堕胎(abortion)” &amp; “福利(welfare)”)。再如，在研究组织员工的 幸福观时，企业的官方书面文本(更具指导性和应然性)以及员工的口述文本(更具真实性)可 能潜藏不同的结论。然而，如何权衡和选择合适的语料、如何处理不同文本下的结论不一致 等问题仍缺乏统一的解决标准。就学者的普遍实践来看，文本选择需要“有的放矢”，即依据 研究问题对文本的背景信息(如，表达视角、代表阶层、文本性质 )进行必要地分析和筛选， 在扩大语料库规模和类型的同时也要尽可能使之聚焦在同一视角和语境上。[11,28,41] <br><br></p>
<p><strong>(2)“词”是词嵌入分析的基本单位，中文文本的分析需要预先进行“分词”处理</strong>。<br>对于某些特定领域的文本而言，如专业学术文章、古代汉语文本等，由于其文本内容及结构与标准的训练语料存在较大差异，[46] 使得文本分词的过程存在一定困难。近年来，大量学者就优化文本预处理技术展开了探讨，例如，Deng 等[46] 开发了“TopWORDS”分词软件，在小型训练文本中实现了部分低频词的精确识别，亦能处理含有大量未知专业语汇的文本，该方法的有效性在古汉语文本的分词任务上得到了进一步验证。在突破文本预处理中的困难后，可以应用词嵌入的实践流程对文言文展开分析，探讨古代社会文背景下的社会科学课题，如围绕权力、阶层、性别、宗教等的社会规范以及其它社会价值观念的演变。</p>
<br>
<p><strong>(3)在由“词”构成的文本结构中，词与词之间的组合搭配能够创造出更加丰富且抽象的语义信息，这一类信息难以通过词向量间简单的结构化公式运算来体现。</strong> <br>词嵌入技术对文本中的“词”展开语义分析，所建构的是词与词之间的关联，侧重于表达“单词级别”的语义信息。因此，基于词向量的简单几何计算难以直接反映 “单词级别”以外的语义信息，[18,5 2 ] 如词组概念、段落含义、文本主题等。学界也在积极探索“组合式分布语义”的实现方法，即如何利用词表征的组合实现对短语、段落和文档的有效表示。[18,53 ] 在自然语言处理领域，以LDA 为代表的主题模型从整个语言系统分布中学习“词”的含义，侧重于建模词与文档的关系，体现的是词的主题信息。相比于词嵌入模型下的单词之间的关联，包含主题特征的词向量之间能够反映相对丰富的语义关联。鉴于词嵌入模型具有较强的扩展能力，大量学者针对词嵌入模型的算法和训练过程进行优化，即将有关文本整体特征的信息或其他的领域知识融入词嵌入的学习过程。例如，Liu 等[54] 基于 Word2Vec 词嵌入模型，并结合 LDA 算法，使词向量包含更多的主题特征，如 “apple(苹果)”在电子产品的背景信息下表示“苹果公司”，而在食品背景信息下表示“苹果”这 类水果。此外，词嵌入技术的基本原理在文本表征领域也得到了长足发展。例如， Le 和 Mikolov[55] 将 Word2Vec(skip-gram)的算法运用至句子和短文本的表征学习；词嵌入模型界的 新秀——BERT 模型，能够有效表征句子等“单词级别”以上的文本语义概念，推动了对更 高文本单位的关系层面的理解。<br></p>
<br>
<p><strong>(4)传统的社会科学研究方法具备词嵌入技术所无法提供的分析视角，尤其是相对于“文本细读法(close reading)”，词嵌入这一计算机化的分析方法难以捕捉更加细微的语义差别</strong>。<br></p>
<p>基于上下文分布来表示单词的方法也难以学习单词的细粒度语义， 例如，同义词、反义词、多义词、上下位词等词义的区分和表征还有待优化。据此，相关学者提出利用有监督的学习过程，在词嵌入的算法层面引入某些先验知识库(如，描述词义关联信息的“WordNet”语义网[5 7 ] )，帮助模型更好地捕捉单词多元的属性信息，从而有助于避免词嵌入表征词义的逻辑偏离实践认知。[19]</p>
<p><img loading="lazy" src="img/ChallengeEmbeddings.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="六词嵌入技术在管理学领域的应用展望">六、词嵌入技术在管理学领域的应用展望</h2>
<h3 id="61-政策效应评估">6.1 政策效应评估</h3>
<p>已有政策效应评估主要采用定量分析工具(如，双差分法)分析显性的数据指标(如，人均收入)，而对于政策的隐式效应(如，社会心理、文化认知)的判断还较为局限。本研究提出， 词嵌入技术可以结合因果推断，[5 8 ] 分析公共政策的有效性。具体而言，词嵌入技术可以研 究：①政策对社会文化认知的影响，如“2020 年禁塑令”实施对公民“环保”、“健康”等概念的 认知的影响；“2021 年惠游湖北”政策对武汉城市污名化的缓释作用等。②衡量政策创新性， 探讨政策带来的创新性影响。例如，Perren 和 Sapsed[5 9 ] 分析了英国在过去近 50 年间的议会 记录，发现在其实施“国家创新计划”之后，“创新”这一概念在社会文本的出现频率显著上升， 有关科技领域的词汇与“创新”一词的共现频率显著增加。未来研究可以利用词嵌入技术来挖 掘“创新”与具体领域，例如，“知识”、“科技”、“法律”的相关度，并比对其在政策实施节点 前后的变化。</p>
<br>
<h3 id="62-用户推荐系统">6.2 用户推荐系统</h3>
<p>以往的线上推荐系统多基于用户行为数据来判断个体偏好， 以矩阵分解 (Matrix Factorization)和协同过滤(Collaborative Filtering)为代表的技术被广泛应用于线上推荐场景。 然而相比上述几项技术，词嵌入能实现更高水平的用户偏好预测， [ 60 , 6 1 ] 这意味着文本数据 将是我们了解用户的重要渠道。现如今，电商评论和社交媒体的动态文本记录了大量消费者 认知、态度及其它表现个人特征的信息。利用词嵌入技术，平台能够挖掘用户对特定领域话 题的偏好、对产品的偏好和评价，以及用户之间的相似特征等。例如，通过计算“产品/品牌” 与积极、消极属性词汇的相对“距离”，[13] 来衡量个体用户的内隐偏好和真实的消费感受，并 据此展开产品推荐和广告投放。</p>
<br>
<h3 id="63-数智品牌管理">6.3 数“智”品牌管理</h3>
<p>鉴于词嵌入技术在挖掘社会内隐认知上的突出表现，因而能用于刻画企业—消费者关系， 辅助企业的品牌管理战略。 借助词嵌入的分析方法， 企业可以透过用户生成文本 (user-generated content)(如，社交媒体、网络论坛和线上评论)纵观消费者对企业品牌形象的 态度、评价，也可以用于挖掘影响消费者满意度的关键因素和市场潜在需求。[8, 6 2 ] 作为品牌 形象的内核，企业品牌个性(brand personality)及其历时演变同样也可以使用词嵌入的分析方 法对其进行挖掘，[12] 从不同时期的社会文本中测量相应的语义关系，即品牌与个性维度间 的相关性(如，品牌与“真诚型”vs.“粗犷型”)。在跨文化视角下，词嵌入技术能够帮助企业考 察不同文化背景下的市场对其品牌的认知差异，并据此助推企业品牌的形象定位与国际化进 程。[6 3 ] 再如，利用词嵌入技术能够帮助企业动态追踪新产品的市场评价，为企业评估产品 的市场表现提供新的分析工具。其它的相关话题，诸如品牌依恋(brand attachment)、品牌文 化(brand culture)和品牌联想(brand association)等研究也将受益于词嵌入的分析方法。</p>
<br>
<h3 id="64-企业关系管理">6.4 企业关系管理</h3>
<p>文本作为企业对外传达信息、价值观，以及企业间进行交流的重要载体之一，蕴含了大量的、足以表征企业特征的信息。以往从文本层面探讨企业关系的研究相对较少，而利用词嵌入的基本原理，未来研究可以考虑利用文本来刻画企业间的关系(如竞争、合作、信任等)， 进而更有效地描绘企业在网络中的嵌入式角色(embedded role)。例如，学界一直致力于研究 企业间关系网络的结构特点及其对企业绩效、企业间联合研发效率的潜在影响，探讨了社会 网络嵌入视角下的企业组织的合作范式。例如，基于知识理论视角，企业合作网络的形成及 演变动机在很大程度上取决于知识的互补性与相似性特征。[64 ] 未来研究可以利用文本刻画 企业的关系网络及其节点特征，探索企业的合作策略和市场战略。尤其在信息不对称的商业环境下，基于文本的社会关系分析能够为企业的战略伙伴选择、市场表现等提供新的分析路 径和信息参考。</p>
<br>
<h3 id="65-组织内部管理">6.5 组织内部管理</h3>
<p>在管理学领域，有关组织行为的研究大多依赖于问卷访谈和自然观察等形式的调查方法， 以及基于组织管理目标开展特定的田野实验。这些研究路径在理解和预测个体行为的过程中 存在较强的主观性和外显性，难以挖掘组织成员真实的内隐认知。此外，以往对组织场景内 的文本分析在很大程度上受限于专家学者的领域知识和实践经验，耗时、低效且准确率低。 本研究提出，词嵌入技术可以用于分析组织内成员的心理及行为规律，通过挖掘组织内的文本(如会议记录、员工评述、领导讲演文本)，揭示员工的内隐认知信息(如动机、信念、情绪)， 甚至包括领导力(leadership)、员工创新力、员工的组织支持感(organizational support)和企业文化等主题。</p>
<br>
<h3 id="66-中国传统智慧与管理问题">6.6 中国传统智慧与管理问题</h3>
<p>社会科学研究者不仅需要关注当下的社会情景，也需要从历史中洞察现象、以史为鉴。 中国社会文化背景下的众多管理问题、思想乃至组织行为领域的话题，均能够从历史事件中窥知和借鉴。例如，Huang 等[6 5 ] 基于《资治通鉴》这一古籍中的记载，探讨了中国家族式企 业内的领导—员工关系。他们以古代的皇帝与太子间的关继承案例作为样本，并在长时间的 人工阅读和变量编码后发现，家庭组织在权力转移的过程中，子女继任的可能性与其父母在 位者对其的压制行为存在“U 型”关系。同样地，词嵌入技术可以挖掘在位者对继任者的评价， 以此判断与继任可能性之间的关系。此外，通过对《二十四史》展开词嵌入分析，也能帮助学者了解中国各个朝代的管理层在应对人事、外部环境、组织治理等方面的管理思想与策略。 对此，本文展望利用词嵌入方法对中华古籍文本展开必要的分析，挖掘诸如组织领导风格、 组织文化、组织竞争力、管理者与下属间关系、人员激励政策等研究话题，进而探索中国本土的管理智慧和组织话题。[65]</p>
<p><br><br></p>
<h2 id="七相关内容">七、相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量  | 使用<strong>人民网领导留言板</strong>语料训练Word2Vec模型</a></li>
</ul>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PyPlutchik库 | 可视化文本的情绪轮(情绪指纹)</title>
      <link>https://textdata.cn/blog/pyplutchik_emotion_circle/</link>
      <pubDate>Sun, 03 Apr 2022 10:43:10 +0600</pubDate>
      
      <guid>/blog/pyplutchik_emotion_circle/</guid>
      <description>越来越多的社交网络学者， 为测量情绪， 基于**心理学家 Robert Plutchik** 提出的模型（通常简称为“**Plutchik轮**”，人类的情绪一共有8大类）制作了大量的情绪可视化作品。在某种程度上，Plutchik轮可以看做情绪指纹，例如不同的电影题材在8类情绪的分布是不一样的。</description>
      <content:encoded><![CDATA[<p>越来越多的社交网络学者， 为测量情绪， 基于<strong>心理学家 Robert Plutchik</strong> 提出的模型（通常简称为“<strong>Plutchik轮</strong>”，人类的情绪一共有8大类）制作了大量的情绪可视化作品。在某种程度上，Plutchik轮可以看做情绪指纹，例如不同的电影题材在8类情绪的分布是不一样的。</p>
<p><img loading="lazy" src="img/imdb_full.png" alt=""  />
</p>
<p>今天介绍 <strong>PyPlutchik</strong>，这是一个 Python 库，专门用于在文本或语料库中可视化 Plutchik 的情绪。 PyPlutchik 绘制 Plutchik 的花朵，每个情感花瓣的大小取决于语料库中检测到或注释了多少情感，也代表每个情感花瓣的三个强度程度。</p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install pyplutchik
</code></pre></div><br>
<h2 id="usage">Usage</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pyplutchik</span> <span class="kn">import</span> <span class="n">plutchik</span>

<span class="n">emotions</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;joy&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>
            <span class="s1">&#39;trust&#39;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
            <span class="s1">&#39;fear&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
            <span class="s1">&#39;surprise&#39;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
            <span class="s1">&#39;sadness&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
            <span class="s1">&#39;disgust&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
            <span class="s1">&#39;anger&#39;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
            <span class="s1">&#39;anticipation&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">}</span>
            
<span class="n">plutchik</span><span class="p">(</span><span class="n">emotions</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="功能特性">功能特性</h2>
<p>PyPlutchik 提供了一个即插即用的工具，用于在文本或语料库中定量表示 Plutchik 的情绪。 它尊重 Plutchik 轮中每个花瓣的原始颜色和空间位移。</p>
<p>在 Pyplutchik 中，用户可以只传递一个字典作为唯一参数，其中字典的键必须是 8 种基本情绪。 每个值必须是 ∈ [0, 1]。</p>
<p><img loading="lazy" src="img/example01.png" alt=""  />
</p>
<br>
<p>每类情绪存在三种强度，下表是Pyplutchik的8大类情绪三种强度汇总。</p>
<p>用户还可以给每个情绪传入一个长度是3的列表，依次表示情绪在低、中、高三种强度的程度，数值0 和 1 之间。</p>
<table>
<thead>
<tr>
<th>Lower intensity</th>
<th>Emotion</th>
<th>Higher intensity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Annoyance</td>
<td>Anger</td>
<td>Rage</td>
</tr>
<tr>
<td>Interest</td>
<td>Anticipation</td>
<td>Vigilance</td>
</tr>
<tr>
<td>Serenity</td>
<td>Joy</td>
<td>Ecstasy</td>
</tr>
<tr>
<td>Acceptance</td>
<td>Trust</td>
<td>Admiration</td>
</tr>
<tr>
<td>Apprehension</td>
<td>Fear</td>
<td>Terror</td>
</tr>
<tr>
<td>Distraction</td>
<td>Surprise</td>
<td>Amazement</td>
</tr>
<tr>
<td>Pensiveness</td>
<td>Sadness</td>
<td>Grief</td>
</tr>
<tr>
<td>Boredom</td>
<td>Disgust</td>
<td>Loathing</td>
</tr>
</tbody>
</table>
<p><img loading="lazy" src="img/example02.png" alt=""  />
</p>
<br>
<p>PyPlutchik 也可表征用户数据中的主要二元、次要二元、二元和相反的情绪。 它会自动从字典的关键字中了解用户想要显示什么样的二元组。</p>
<p><img loading="lazy" src="img/example03.png" alt=""  />
</p>
<br>
<h2 id="绘图技巧">绘图技巧</h2>
<p>可以专注于情绪子集，不会忽视其他情绪，将情绪列表作为参数“<strong>highlight_emotions</strong>”的值传递：</p>
<p><img loading="lazy" src="img/highlight_emotions.png" alt=""  />
</p>
<p>我们可以比较同一亚马逊语料库的不同子组，将我们的可视化并排放置，并仅突出显示愤怒、厌恶和恐惧的花瓣，以轻松发现这些负面情绪在 5 星评论中的表现如何低于 1 星评论 .</p>
<p><img loading="lazy" src="img/amazon.png" alt=""  />
</p>
<br>
<p>或者可以隐藏坐标、刻度和标签，只绘制花瓣，使用参数 <code>show_coordinates = False</code> 和 <code>show_ticklabels = False</code>。下图是imdb不同电影的情绪轮</p>
<p><img loading="lazy" src="img/imdb_full.png" alt=""  />
</p>
<br>
<h2 id="查看文档">查看文档</h2>
<p>有关所有参数的文档和示例库，请参见<a href="Documentation.html">文档</a></p>
<p><a href="PyPlutchik.zip"><strong>文档下载</strong></a></p>
<p>🔥 PyPlutchik 2.0 即将推出！ 新功能包括从文本中提取情感并检查非主题词典。 敬请期待……🔥</p>
<br>
<h2 id="说明">说明</h2>
<p>如果使用PyPlutchik，请在文献中说明，格式如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Semeraro A, Vilella S, Ruffo G (2021) PyPlutchik: Visualising and comparing emotion-annotated corpora PLOS ONE 16(9):e0256503.https://doi.org/10.1371/journal.pone.0256503
</code></pre></div><br>
<h2 id="semeraro的研究">Semeraro的研究</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Semeraro, Alfonso, Salvatore Vilella, Giancarlo Ruffo, and Massimo Stella. &#34;Writing about COVID-19 vaccines: Emotional profiling unravels how mainstream and alternative press framed AstraZeneca, Pfizer and vaccination campaigns.&#34; *arXiv preprint arXiv:2201.07538* (2022).
</code></pre></div><ul>
<li><strong>摘要</strong>: 自 2020 年 11 月宣布 COVID-19 疫苗以来，媒体和社交媒体对 COVID-19 疫苗进行了大量辩论。由于大多数研究都集中在社交媒体中的 COVID-19 虚假信息上，与其他来源相比，主流新闻媒体如何构建 COVID-19 叙述很少受到关注。为了填补这一空白，我们使用认知网络科学和自然语言处理来重建 5745 条关于 COVID-19 疫苗的新闻的随时间变化的语义和情感框架。我们的数据集涵盖了 8 个月内的 17 个网点，其中包括在 Facebook（500 万总股数）和 Twitter（20 万股总股数）上大量转发的意大利新闻文章。我们发现主流消息来源构建“疫苗/疫苗”的总体概念的方式始终具有高度的信任/预期和较少的厌恶。在替代来源构建 COVID-19 疫苗的方式中，这些情绪严重缺失。在疫苗的特定实例中发现了更多差异。另类新闻包括以强烈的悲伤来描述阿斯利康疫苗的标题，而主流标题中没有。与“阿斯利康”相比，主流新闻最初将“辉瑞”与副作用（例如“过敏”、“反应”、“发烧”）相关联更多。随着后一种疫苗的暂停，在 2021 年 3 月 15 日，我们发现了一种语义/情感转变：即使是主流文章标题都将“阿斯利康”框定为在语义上更丰富与副作用的负面关联，而“辉瑞”则经历了积极的效价转变，主要与其更高的效率有关。血栓形成与可怕的概念联想一起进入了疫苗的框架，而死亡这个词经历了情感转变，在替代标题中转向恐惧，在主流标题中失去了希望的内涵，缺乏预期。我们的发现揭示了媒体采用的围绕 COVID-19 疫苗的情感叙述的关键方面，强调了了解替代媒体和主流媒体如何报道疫苗接种新闻的必要性。</li>
<li><strong>Keywords</strong>: natural language processing, text analysis, complex networks, cognitive network science, COVID-19, COVID-19 vaccines
<img loading="lazy" src="img/figure2.png" alt=""  />

<img loading="lazy" src="img/figure3.png" alt=""  />

<img loading="lazy" src="img/figure4.png" alt=""  />
</li>
</ul>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>whatlies库 | 可视化词向量</title>
      <link>https://textdata.cn/blog/whatlies_word2vec/</link>
      <pubDate>Sat, 02 Apr 2022 16:40:10 +0600</pubDate>
      
      <guid>/blog/whatlies_word2vec/</guid>
      <description>词语之间可以比较亲疏远近</description>
      <content:encoded><![CDATA[<h2 id="代码下载">代码下载</h2>
<blockquote>
<p>链接:https://pan.baidu.com/s/1vJohEJ0pc6t4PBK04PiZbg  密码:t7a6</p>
</blockquote>
<br>
<h2 id="whatlies">whatlies</h2>
<p>可以与spacy语言模型结合，可视化词向量。安装zh_core_web_md、en_core_web_md和whatlies。具体文档可以查看https://github.com/RasaHQ/whatlies</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">zh_core_web_md</span><span class="o">-</span><span class="mf">3.0.0</span><span class="o">-</span><span class="n">py3</span><span class="o">-</span><span class="n">none</span><span class="o">-</span><span class="nb">any</span><span class="o">.</span><span class="n">whl</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">en_core_web_md</span><span class="o">-</span><span class="mf">3.0.0</span><span class="o">-</span><span class="n">py3</span><span class="o">-</span><span class="n">none</span><span class="o">-</span><span class="nb">any</span><span class="o">.</span><span class="n">whl</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">whatlies</span>
</code></pre></div><br>
<h2 id="快速上手">快速上手</h2>
<p>spacy模型中的词向量均为几十上百维度的词向量，通过压缩映射至二维空间后，横坐标man，纵坐标woman，就可以将词语的性别倾向可视化出来。</p>
<p>词向量语言模型会学习到人类的<strong>刻板印象</strong>，</p>
<p><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">大数据时代下社会科学研究方法的拓展—基于词嵌入技术的文本分析的应用</a></p>
<p><a href="https://textdata.cn/blog/embeddingsandattitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></p>
<p>例如nurse是女性，doctor是男性。</p>
<p>制作两维度画轴，其中以woman作纵轴，man作横轴。 nurse、queen一般更多的是女性从业者，因此更接近y轴。 king国王多为男性，所以更接近x轴。</p>
<p>至于动物，女性喜欢养猫，男性喜欢养狗，所以也能体现出词语的性别倾向。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">whatlies</span> <span class="kn">import</span> <span class="n">EmbeddingSet</span>
<span class="kn">from</span> <span class="nn">whatlies.language</span> <span class="kn">import</span> <span class="n">SpacyLanguage</span>

<span class="n">lang</span> <span class="o">=</span> <span class="n">SpacyLanguage</span><span class="p">(</span><span class="s2">&#34;en_core_web_md&#34;</span><span class="p">)</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;dog&#34;</span><span class="p">,</span> <span class="s2">&#34;fish&#34;</span><span class="p">,</span> <span class="s2">&#34;kitten&#34;</span><span class="p">,</span> <span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;woman&#34;</span><span class="p">,</span>
         <span class="s2">&#34;king&#34;</span><span class="p">,</span> <span class="s2">&#34;queen&#34;</span><span class="p">,</span> <span class="s2">&#34;doctor&#34;</span><span class="p">,</span> <span class="s2">&#34;nurse&#34;</span><span class="p">]</span>

<span class="n">emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">lang</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
<span class="n">emb</span><span class="o">.</span><span class="n">plot_interactive</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span><span class="n">emb</span><span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">],</span> <span class="n">y_axis</span><span class="o">=</span><span class="n">emb</span><span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">])</span>
</code></pre></div>
<figure >
    
        <img src="img/gif-zero.gif" width="100%" />
    
    
</figure>

<p>whatlies也可以对中文进行操作。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">whatlies</span> <span class="kn">import</span> <span class="n">EmbeddingSet</span>
<span class="kn">from</span> <span class="nn">whatlies.language</span> <span class="kn">import</span> <span class="n">SpacyLanguage</span>

<span class="n">zh_lang</span> <span class="o">=</span> <span class="n">SpacyLanguage</span><span class="p">(</span><span class="s2">&#34;zh_core_web_md&#34;</span><span class="p">)</span>

<span class="n">zh_words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;猫&#34;</span><span class="p">,</span> <span class="s2">&#34;狗&#34;</span><span class="p">,</span> <span class="s2">&#34;鱼&#34;</span><span class="p">,</span> <span class="s2">&#34;鲤鱼&#34;</span><span class="p">,</span> <span class="s2">&#34;男人&#34;</span><span class="p">,</span> <span class="s2">&#34;女人&#34;</span><span class="p">,</span>
         <span class="s2">&#34;国王&#34;</span><span class="p">,</span> <span class="s2">&#34;王后&#34;</span><span class="p">,</span> <span class="s2">&#34;医生&#34;</span><span class="p">,</span> <span class="s2">&#34;护士&#34;</span><span class="p">]</span>

<span class="n">zh_emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">zh_lang</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">zh_words</span><span class="p">])</span>
<span class="n">zh_emb</span><span class="o">.</span><span class="n">plot_interactive</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span><span class="n">zh_emb</span><span class="p">[</span><span class="s2">&#34;男人&#34;</span><span class="p">],</span> <span class="n">y_axis</span><span class="o">=</span><span class="n">zh_emb</span><span class="p">[</span><span class="s2">&#34;女人&#34;</span><span class="p">])</span>
</code></pre></div>
<figure >
    
        <img src="img/chinese.png" width="100%" />
    
    
</figure>

<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 |  Python文本分析包更新</title>
      <link>https://textdata.cn/blog/cntext_simplification/</link>
      <pubDate>Fri, 01 Apr 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_simplification/</guid>
      <description>扩展词典、情感分析、可阅读性，内置9种情感词典，涵盖中英文</description>
      <content:encoded><![CDATA[<p><a href="https://github.com/hidadeng/cntext"><img loading="lazy" src="https://img.shields.io/badge/cntext-%e4%b8%ad%e6%96%87%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%ba%93-orange?style=for-the-badge&amp;logo=appveyor" alt=""  />
</a></p>
<p><a href="version1.2.md">旧版cntext入口</a></p>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> <strong>stats</strong>  文本统计指标
<ul>
<li><input checked="" disabled="" type="checkbox"> 词频统计</li>
<li><input checked="" disabled="" type="checkbox"> 可读性</li>
<li><input checked="" disabled="" type="checkbox"> 内置pkl词典</li>
<li><input checked="" disabled="" type="checkbox"> <strong>情感分析</strong></li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>dictionary</strong> 构建词表(典)
<ul>
<li><input checked="" disabled="" type="checkbox"> Sopmi 互信息扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> W2Vmodels 词向量扩充词典法</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>similarity</strong>   文本相似度
<ul>
<li><input checked="" disabled="" type="checkbox"> cos相似度</li>
<li><input checked="" disabled="" type="checkbox"> jaccard相似度</li>
<li><input checked="" disabled="" type="checkbox"> 编辑距离相似度</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>mind.py</strong> 计算文本中的认知方向（态度、偏见）</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext
</code></pre></div><br>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="mf">1.8.4</span>

<span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">mind</span>
    <span class="nx">dictionary</span>
    <span class="nx">similarity</span>
    <span class="nx">stats</span>
</code></pre></div><br>
<h2 id="一stats">一、stats</h2>
<p>目前stats内置的函数有</p>
<ul>
<li><strong>readability</strong>  文本可读性</li>
<li><strong>term_freq</strong> 词频统计函数</li>
<li><strong>dict_pkl_list</strong>  获取cntext内置词典列表(pkl格式)</li>
<li><strong>load_pkl_dict</strong> 导入pkl词典文件</li>
<li><strong>sentiment</strong> 情感分析</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="11--readability">1.1  readability</h3>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p>readability(text, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text: 文本字符串数据</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p>**中文可读性 ** 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>


<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 28.0,
 &#39;readability2&#39;: 0.15789473684210525,
 &#39;readability3&#39;: 14.078947368421053}
</code></pre></div><br>
<p>句子中的符号变更会影响结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.16666666666666666,
 &#39;readability3&#39;: 13.583333333333334}
</code></pre></div><p><br><br></p>
<h3 id="12--term_freq">1.2  term_freq</h3>
<p>词频统计函数，返回Counter类型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="13-dict_pkl_list">1.3 dict_pkl_list</h3>
<p>获取cntext内置词典列表(pkl格式)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取cntext内置词典列表(pkl格式)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;]
</code></pre></div><p>词典对应关系, 部分情感词典资料整理自 <a href="https://github.com/quanteda/quanteda.sentiment">quanteda.sentiment</a></p>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>DUTIR.pkl</td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td>HOWNET.pkl</td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>sentiws.pkl</td>
<td>SentimentWortschatz (SentiWS)</td>
<td>英文</td>
<td>正面词、负面词；<br>效价</td>
</tr>
<tr>
<td>ChineseFinancialFormalUnformalSentiment.pkl</td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td>ANEW.pkl</td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>词语效价信息</td>
</tr>
<tr>
<td>LSD2015.pkl</td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>NRC.pkl</td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td>geninqposneg.pkl</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HuLiu.pkl</td>
<td>Hu&amp;Liu (2004)正、负情感词典</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>AFINN.pkl</td>
<td>尼尔森 (2011) 的“新 ANEW”效价词表</td>
<td>英文</td>
<td>情感效价信息valence</td>
</tr>
<tr>
<td>LoughranMcDonald.pkl</td>
<td>会计金融LM词典</td>
<td>英文</td>
<td>金融领域正、负面情感词</td>
</tr>
<tr>
<td>ADV_CONJ.pkl</td>
<td>副词连词</td>
<td>中文</td>
<td></td>
</tr>
<tr>
<td>STOPWORDS.pkl</td>
<td></td>
<td>中、英</td>
<td>停用词</td>
</tr>
</tbody>
</table>
<h3 id="注意">注意:</h3>
<ul>
<li>
<p>如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</li>
<li>
<p>如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue</p>
</li>
</ul>
<br>
<h3 id="14-load_pkl_dict">1.4 load_pkl_dict</h3>
<p>导入pkl词典文件，返回字典样式数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入pkl词典文件,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, ...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...], 
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;, &#39;刿目怵心&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;, &#39;喜糖&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,]
           }
</code></pre></div><br>
<h3 id="15-sentiment">1.5 sentiment</h3>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)
使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>diction:  情感词字典；</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">],</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p>如果不适用pkl词典，可以自定义自己的词典，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p><br><br></p>
<h2 id="二dictionary">二、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="21-sopmi-共现法">2.1 SoPmi 共现法</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                   <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                   <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                   <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 44.49 s
</code></pre></div><br>
<h3 id="22-w2vmodels-词向量">2.2 W2VModels 词向量</h3>
<p><strong>特别要注意代码需要设定lang语言参数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   174 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s

</code></pre></div><br>
<h3 id="需要注意">需要注意</h3>
<p>训练出的w2v模型可以后续中使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">model路径</span><span class="p">)</span>
<span class="c1">#找出word的词向量</span>
<span class="c1">#w2v_model.get_vector(word)</span>
<span class="c1">#更多w2_model方法查看</span>
<span class="c1">#help(w2_model)</span>
</code></pre></div><p>例如本代码，运行生成的结果路径<code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;technology&#39;, 0.689210832118988),
 (&#39;infrastructure&#39;, 0.669672966003418),
 (&#39;resources&#39;, 0.6695448160171509),
 (&#39;talent&#39;, 0.6627111434936523),
 (&#39;execution&#39;, 0.6549549102783203),
 (&#39;marketing&#39;, 0.6533523797988892),
 (&#39;merchandising&#39;, 0.6504817008972168),
 (&#39;diversification&#39;, 0.6479553580284119),
 (&#39;expertise&#39;, 0.6446896195411682),
 (&#39;digital&#39;, 0.6326863765716553)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.45616838, -0.7799563 ,  0.56367606, -0.8570078 ,  0.600359  ,
       -0.6588043 ,  0.31116748, -0.11956959, -0.47599426,  0.21840936,
       -0.02268819,  0.1832016 ,  0.24452794,  0.01084935, -1.4213187 ,
        0.22840202,  0.46387577,  1.198386  , -0.621511  , -0.51598716,
        0.13352732,  0.04140598, -0.23470387,  0.6402956 ,  0.20394802,
        0.10799981,  0.24908689, -1.0117126 , -2.3168423 , -0.0402851 ,
        1.6886286 ,  0.5357047 ,  0.22932841, -0.6094084 ,  0.4515793 ,
       -0.5900931 ,  1.8684244 , -0.21056202,  0.29313338, -0.221067  ,
       -0.9535679 ,  0.07325   , -0.15823542,  1.1477109 ,  0.6716076 ,
       -1.0096023 ,  0.10605699,  1.4148282 ,  0.24576302,  0.5740349 ,
        0.19984631,  0.53964925,  0.41962907,  0.41497853, -1.0322098 ,
        0.01090925,  0.54345983,  0.806317  ,  0.31737605, -0.7965337 ,
        0.9282971 , -0.8775608 , -0.26852605, -0.06743863,  0.42815775,
       -0.11774074, -0.17956367,  0.88813037, -0.46279573, -1.0841943 ,
       -0.06798118,  0.4493006 ,  0.71962464, -0.02876493,  1.0282255 ,
       -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922,
       -0.16753489,  0.14065655, -1.8609751 ,  0.03587054,  1.2792674 ,
        1.2732009 , -0.74120265, -0.98000383,  0.4521185 , -0.26387128,
        0.37045383,  0.3680011 ,  0.7197629 , -0.3570571 ,  0.8016917 ,
        0.39243212, -0.5027844 , -1.2106236 ,  0.6412354 , -0.878307  ],
      dtype=float32)
</code></pre></div><p><br><br></p>
<h2 id="23-co_occurrence_matrix">2.3 co_occurrence_matrix</h2>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三similarity">三、similarity</h2>
<p>四种相似度计算函数</p>
<ul>
<li>cosine_sim(text1, text2)  cos余弦相似</li>
<li>jaccard_sim(text1, text2)     jaccard相似</li>
<li>minedit_sim(text1, text2)  最小编辑距离相似度；</li>
<li>simple_sim(text1, text2) 更改变动算法</li>
</ul>
<p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩啊&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.82
0.67
2.00
0.87
</code></pre></div><p><br><br></p>
<h2 id="四text2mind">四、Text2Mind</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<p>- <strong>多个对象在某概念的远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
<p>- 多个对象在某<strong>概念的分量(fen，一声)的多少</strong>， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
<p>这两种认知分别可以用向量距离、向量语义投影计算得来。</p>
<ul>
<li>tm.sematic_distance(words, c_words1, c_words2)  向量距离</li>
<li>tm.sematic_projection(words, c_words1, c_words2)  向量语义投影</li>
</ul>
<h3 id="41-tmsematic_distancewords-c_words1-c_words2">4.1 tm.sematic_distance(words, c_words1, c_words2)</h3>
<p>分别计算words与c_words1、c_words2语义距离，返回距离差值。</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果d1-d2&lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<p><strong>下载glove_w2v.6B.100d.txt</strong>链接: <a href="https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw">https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw</a> 提取码: 72l0</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#Note: this is a word2vec format model</span>
<span class="n">tm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Text2Mind</span><span class="p">(</span><span class="n">w2v_model_path</span><span class="o">=</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="n">engineer</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">mans</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">womans</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">mans</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">womans</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><br>
<h3 id="42-tmsematic_projectionwords-c_words1-c_words2">4.2 tm.sematic_projection(words, c_words1, c_words2)</h3>
<p><strong>语义投影</strong>，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，word越位于c_words2一侧。</p>
<p>下图是语义投影示例图，本文算法和图片均来自 &ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&rdquo;</p>
<p><img loading="lazy" src="img/Nature_Semantic_projection_recovering_human_knowledge_of.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">smalls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">bigs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">smalls</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">bigs</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>doccano|为机器学习建模做数据标注</title>
      <link>https://textdata.cn/blog/doccano_text_anotation/</link>
      <pubDate>Mon, 28 Mar 2022 10:43:10 +0600</pubDate>
      
      <guid>/blog/doccano_text_anotation/</guid>
      <description>使用doccano，为机器学习建模做数据标注</description>
      <content:encoded><![CDATA[<h2 id="doccano">doccano</h2>
<p>doccano是开源的数据标注工具，可以简化数据标注的难度。需要注意，市面上的机器学习课程一般都默认数据已标注，在此基础上讲机器学习。</p>
<p><img loading="lazy" src="img/doccano.gif" alt=""  />
</p>
<p>您还可以将 doccano 与您的脚本集成，因为它将功能公开为API。 doccano API是在局域网内的网址链接，多台设备可打开浏览、标注。</p>
<br>
<h2 id="一标记流程">一、标记流程</h2>
<p>通过以下步骤开始和完成使用 doccano 的标签项目：</p>
<ol>
<li>安装doccano。</li>
<li>运行doccano。</li>
<li>设置标签项目。 选择标签项目的类型， 配置项目设置。</li>
<li>导入数据集。 您还可以导入带标签的数据集。</li>
<li>给项目添加<strong>标注人员</strong></li>
<li>给标志者定义标注工作文档说明</li>
<li>开始标记数据。</li>
<li>导出标记的数据集。</li>
</ol>
<br>
<h2 id="二配置环境">二、配置环境</h2>
<h3 id="21-安装">2.1 安装</h3>
<p>打开命令行（cmd、terminal）执行安装命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install doccano
</code></pre></div><br>
<h3 id="22-运行doccano">2.2 运行doccano</h3>
<p>在命令行（cmd、terminal）内依次执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">#在电脑第一次运行的时候初始化doccano
#只需设置一次，之后不用再运行该命令
doccano init

#创建用户名及密码；例如现在有一个主管admin，两个标注员tom和jack
#设置好用户，之后不用再运行该命令
doccano createuser --username admin --password pass
doccano createuser --username tom --password pass
doccano createuser --username jack --password pass

#开启doccano服务
doccano webserver
</code></pre></div><p>完成上述操作后，另打开一个新的命令行，执行下列命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">doccano task
</code></pre></div><br>
<h2 id="三案例">三、案例</h2>
<p>下面我们以外卖评论数据为例，对评论进行判断，标注为正、负面情感。<a href="data.csv">点击下载data.csv</a></p>
<br>
<h3 id="31-创建项目">3.1 创建项目</h3>
<p>先登录用户名和密码，这里的admin是超级用户(权限最大)</p>
<p><img loading="lazy" src="img/signin.png" alt=""  />
</p>
<p>为创建项目，如图点击<strong>Create</strong>按钮。 根据需要选择合适的项目类别,这里选择的<strong>Text  Classification，</strong></p>
<p><strong>填写项目信息</strong>，例如项目名情感标注，简介、标注类型</p>
<p><img loading="lazy" src="img/create_project.png" alt=""  />
</p>
<p>根据需要选择项目的功能需求，例如允许单标签，把数据打乱随机显示， 用户之间共享标注</p>
<p><img loading="lazy" src="img/create_project2.png" alt=""  />
</p>
<br>
<h3 id="32-上传数据">3.2 上传数据</h3>
<p>创建项目后，点击<strong>Dataset</strong>按钮，点击<strong>Import dataset</strong>导入数据。我这里准备的是csv文件，拥有review和label两个字段。</p>
<p><img loading="lazy" src="img/import_dataset1.png" alt=""  />
</p>
<p><img loading="lazy" src="img/import_dataset2.png" alt=""  />
</p>
<p><img loading="lazy" src="img/import_dataset3.png" alt=""  />
</p>
<br>
<h3 id="33-定义标签">3.3 定义标签</h3>
<p>点击左侧菜单中的“<strong>Labels</strong>”按钮来定义我们的标签。 我们应该看到标签编辑器页面。 在标签编辑器页面中，您可以通过指定标签文本、快捷键、背景颜色和文本颜色来创建标签。</p>
<p><img loading="lazy" src="img/define_labels.png" alt=""  />
</p>
<p><img loading="lazy" src="img/define_labels1.png" alt=""  />
</p>
<p>同理，可以定义负面neg标签。现在有了pos和neg两个标签。</p>
<p><img loading="lazy" src="img/define_labels3.png" alt=""  />
</p>
<br>
<h3 id="34-添加成员">3.4 添加成员</h3>
<p>点击左侧目录中的 <code>Members</code> 按钮，</p>
<p><img loading="lazy" src="img/select_members.png" alt=""  />
</p>
<p>然后，选择“<strong>Add</strong>”按钮以显示表单。 使用您要添加到项目中的用户名和角色填写此表单。 然后，选择“<strong>Save</strong>”按钮。</p>
<p><img loading="lazy" src="img/select_user.png" alt=""  />
</p>
<p>如果没有可供选择的成员，记得创建成员。形如<code>doccano createuser --username tom --password pass</code></p>
<br>
<h3 id="35-开始标注">3.5 开始标注</h3>
<p>接下来，我们准备标注文本数据。 只需点击导航栏中的“<strong>Start annotation</strong>”按钮，我们就可以开始对文档进行批注了。</p>
<p><img loading="lazy" src="img/annotation.png" alt=""  />
</p>
<br>
<h3 id="36-导出数据">3.6 导出数据</h3>
<p>在注释步骤之后，我们可以下载标注后的数据。 转到“<strong>Dataset</strong>”页面，然后单击“操作”菜单中的“<strong>Export dataset</strong>”按钮。 选择导出格式后，单击“<strong>Export</strong>”。 您应该看到以下屏幕：</p>
<p><img loading="lazy" src="img/export_dataset.png" alt=""  />
</p>
<p>到出的结果如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;all.csv&#39;</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/export_dataset2.png" alt=""  />
</p>
<br>
<h3 id="37-导出数据">3.7 导出数据</h3>
<p>对了，当标注过程不同阶段，还看查看标注工作量等可视化信息</p>
<p><img loading="lazy" src="img/vis.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>大邓整理的有用Python项目</title>
      <link>https://textdata.cn/blog/python_tools/</link>
      <pubDate>Fri, 07 Jan 2022 12:43:10 +0600</pubDate>
      
      <guid>/blog/python_tools/</guid>
      <description>涵盖网络爬虫、数据库、数据分析、机器学习、可视化、文本分析</description>
      <content:encoded><![CDATA[<h2 id="tool_kits">Tool_Kits</h2>
<p>工具箱大全,主要是Python项目。</p>
<p>涵盖：</p>
<ol>
<li>网络爬虫</li>
<li>数据库</li>
<li>数据分析</li>
<li>机器学习</li>
<li>可视化</li>
<li>文本分析</li>
<li>GUI</li>
<li>自动化办公</li>
<li>其他</li>
</ol>
<br>
<h2 id="网络爬虫">网络爬虫</h2>
<ul>
<li><a href="https://github.com/psf/requests">requests</a>  最好用的网络爬虫访问库</li>
<li><a href="https://github.com/hidadeng/smartscraper">smartscraper</a>  最简单的网络爬虫访问&amp;解析库</li>
<li><a href="https://github.com/hidadeng/weibo_crawler">weibo_crawler</a> 最简单的微博爬虫</li>
<li><a href="https://mp.weixin.qq.com/s/qL1uEk4j_ks3uhjINe-xyA">崔庆才大神发布的测试站点</a></li>
<li><a href="https://github.com/Gerapy/GerapyAutoExtractor">gerapy-auto-extractor</a> 爬虫页面智能解析库</li>
<li><a href="https://github.com/davidteather/TikTok-Api">TikTok-Api</a> 抖音国际站爬虫库</li>
<li><a href="https://github.com/tebelorg/RPA-Python">rpa</a> Python自动化操纵包</li>
<li><a href="https://github.com/celery/celery/">celery</a> 可以用于制作爬虫访问队列</li>
<li><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a> 最简单的网页解析库</li>
<li><a href="https://github.com/gawel/pyquery">pyquery</a>  最简洁网页解析库</li>
<li><a href="https://github.com/scrapy/scrapy">scrapy</a> 最流行的爬虫框架</li>
<li><a href="https://github.com/binux/pyspider">pyspider</a> 国人开发的爬虫框架</li>
<li><a href="https://github.com/SeleniumHQ/selenium/">selenium</a> 浏览器自动化测试框架，可以用于爬虫反爬</li>
<li><a href="https://github.com/microsoft/playwright">playwright</a> 微软开源的浏览器自动化测试框架</li>
<li><a href="https://github.com/imWildCat/scylla">scylla</a> 智能IP代理池，用于反爬</li>
<li><a href="https://github.com/hidadeng/shreport">shreport</a> 上海证券交易所上市公司定期报告下载</li>
<li><a href="https://github.com/codelucas/newspaper">newspaper</a> 新闻爬虫库，根据提供的url可以抽取出新闻标题、作者、关键词、总结，部分功能支持中文</li>
<li><a href="https://github.com/sml2h3/ddddocr">ddddocr</a>  通用验证码识别OCR pypi版</li>
</ul>
<br>
<h2 id="web">Web</h2>
<ul>
<li><a href="https://github.com/getpelican/pelican">pelican</a> Python静态网站生成库</li>
<li><a href="https://github.com/pallets/flask">flask</a> 可以开发网站、分享rest-api接口;流行度top2的web框架</li>
<li><a href="https://github.com/streamlit/streamlit">streamlit</a>、<a href="https://github.com/wang0618/PyWebIO">PyWebIO</a>对Python小白最友好的的web库</li>
<li><a href="https://github.com/tiangolo/fastapi">fastapi</a>  web框架，高性能，易于学习，快速编写代码；</li>
<li><a href="https://github.com/wang0618/PyWebIO">PyWebIO</a>  不需要编写HTML和JS代码，就可以构建简单的基于浏览器的GUI应用。</li>
<li><a href="https://github.com/mkdocs/mkdocs">mkdocs</a> 制作文档网站
<br></li>
</ul>
<h2 id="数据库">数据库</h2>
<ul>
<li><a href="https://github.com/PyMySQL/PyMySQL">PyMySQL</a></li>
<li><a href="https://docs.python.org/3/library/sqlite3.html">Sqlite3</a>  轻量级sql数据库(python内置库)</li>
<li><a href="https://github.com/mongodb/mongo-python-driver">pymongo</a> 非关系型MongoDB库</li>
<li><a href="https://github.com/redis/redis">redis</a>  Redis数据库</li>
<li><a href="https://github.com/py2neo-org/py2neo">py2neo</a>  对接Neo4J数据库的python库</li>
<li><a href="https://github.com/simonw/datasette">datasette</a> 探索和发布数据的开源多功能工具，主要面向数据记者、博物馆馆长、档案管理员、地方政府、科学家、研究人员以及任何拥有希望与世界分享数据的人。</li>
</ul>
<br>
<h2 id="数据分析">数据分析</h2>
<ul>
<li><a href="https://github.com/pandas-dev/pandas">pandas</a> 必须Python数据分析库，读取文件、预处理数据、分析、存储</li>
<li><a href="https://github.com/garrettj403/SciencePlots">SciencePlots</a>  科学绘图的Python工具包</li>
<li><a href="https://github.com/orchest/orchest">Orchest</a> 创建数据科学工作量的工具。Orchest是一款Web数据科学工具，可在文件系统上运行</li>
<li><a href="https://github.com/statsmodels/statsmodels">statsmodels</a> Python的统计计量统计库</li>
<li><a href="https://github.com/bashtage/linearmodels">linearmodels</a> 添加线性模型，包括statsmodels中缺少的工具变量和面板数据模型。</li>
<li><a href="https://github.com/streamlit/streamlit">streamlit</a> 快速搭建本地数据分析类Web应用</li>
<li><a href="https://github.com/modin-project/modin">modin</a> pandas加速库，接口语法与pandas高度一致</li>
<li><a href="https://github.com/dask/dask">dask</a>  pandas加速库，接口语法与pandas高度一致</li>
<li><a href="https://github.com/has2k1/plydata%5D">plydata</a>  pandas管道语法库</li>
<li><a href="https://github.com/networkx/networkx">networkx</a> 社交网络分析库</li>
</ul>
<br>
<p>​</p>
<h2 id="机器学习">机器学习</h2>
<ul>
<li><a href="https://github.com/VowpalWabbit/vowpal_wabbit">vowpal wabbit</a>  机器学习的前沿库</li>
<li><a href="https://github.com/scikit-learn/scikit-learn">scikit-learn</a> 机器学习必学库，支持有监督、无监督多种算法，含文本分析功能</li>
<li><a href="https://github.com/biolab/orange3">Orange3</a> 点击操作的机器学习分析<strong>软件</strong>， 可文本分析</li>
<li><a href="https://github.com/doccano/doccano">doccano</a> 文本数据标注工具</li>
<li><a href="https://github.com/heartexlabs/label-studio">label-studio</a>  最牛掰的文本数据标注工具</li>
</ul>
<h2 id="可视化">可视化</h2>
<ul>
<li><a href="https://github.com/streamlit/streamlit">streamlit</a> 快速搭建本地数据分析类Web应用</li>
<li><a href="https://github.com/matplotlib/matplotlib">matplotlib</a>  Python中最万能绘图库，很少有ta画不出来的图；但语法较难、静态图</li>
<li><a href="https://github.com/nschloe/matplotx">matplotx</a>  Matplotlib扩展库，可以提供更多样式，简化样式设定</li>
<li><a href="https://github.com/mwaskom/seaborn">seaborn</a> 基于matplotlib开发的简化版可视化库， 一般的图可以用ta绘制； 高度定制仍需要结合matplotlib进行样式定制；静态图</li>
<li><a href="https://github.com/has2k1/plotnine">plotnine</a>  ggplot2语法的Python可视化库， 可与<a href="https://github.com/has2k1/plydata%5D">plydata</a> 库结合使用</li>
<li><a href="https://github.com/pyecharts/pyecharts">pyecharts</a> 国人开发并封装的动态可视化图绘制库; 中文文档</li>
<li><a href="https://github.com/plotly/plotly.py">plotly</a> 动态可视化图绘制库</li>
<li><a href="https://github.com/bokeh/bokeh">bokeh</a> 动态可视化图绘制库</li>
<li><a href="https://github.com/garrettj403/SciencePlots">SciencePlots</a>  科研论文绘图，基于matplotlib</li>
<li><a href="https://github.com/datapane/datapane">datapane</a>  数据分析报告生成</li>
<li><a href="https://github.com/apache/superset">superset</a> 开源商务智能分析可视化库</li>
<li><a href="https://github.com/alfonsosemeraro/pyplutchik">pyplutchik</a> 文本可视化，可将文本情感信息按照plutchik轮样式可视化</li>
</ul>
<br> 
<h2 id="文本分析">文本分析</h2>
<ul>
<li><a href="https://github.com/nltk/nltk">nltk</a> 自然语言分析套件，对中文不友好</li>
<li><a href="https://github.com/shaypal5/skift">skift</a>  使用scikit-learn语法封装了fastText功能的包。</li>
<li><a href="https://github.com/andrewtavis/kwx">kwx</a>  Python 中基于 BERT、LDA 和 TFIDF 的关键字提取</li>
<li><a href="https://github.com/explosion/spaCy">spacy</a> 工业级自然语言模型库，支持中文</li>
<li><a href="https://github.com/fxsjy/jieba">jieba</a>  中文文本分词库</li>
<li><a href="https://github.com/isnowfy/snownlp">snownlp</a> 中文情感分析库</li>
<li><a href="https://github.com/RaRe-Technologies/gensim">gensim</a>  最好用、最全的话题模型</li>
<li><a href="https://github.com/hidadeng/cntext">cntext</a> 中文文本分析库，含词频统计、情感分析、可视化</li>
<li><a href="https://github.com/heartexlabs/label-studio">label-studio</a>  最牛掰的文本数据标注工具</li>
<li><a href="https://github.com/doccano/doccano">doccano</a> 文本数据标注工具</li>
<li><a href="https://github.com/shivam5992/textstat">textstat</a>  文本可读性计算包(算法全，但仅支持英文)</li>
<li><a href="https://github.com/jbesomi/texthero">texthero</a> 文本预处理、展示、可视化库，仅支持英文</li>
<li><a href="https://github.com/textpipe/textpipe">textpipe</a> 文本分析流水线</li>
<li><a href="https://github.com/davidmcclure/textplot">textplot</a> 词语网络图</li>
<li><a href="https://github.com/ryanjgallagher/shifterator">shifterator</a> 通过让您查看单词使用方式的变化，单词移位可以帮助您进行从根本上更可解释的情感，熵和散度分析。量化不同单词对两个文本差异做出的贡献，以及它们如何发挥作用。</li>
<li><a href="https://github.com/vi3k6i5/GuidedLDA">GuidedLDA</a> 半监督LDA主题模型</li>
<li><a href="https://github.com/gregversteeg/corex_topic">corex_topic</a>  层次非监督、半监督话题模型</li>
<li><a href="https://github.com/MaartenGr/BERTopic">BERTopic</a> BERT话题模型</li>
<li><a href="https://github.com/RasaHQ/whatlies">whatlies</a>  词向量可视化</li>
<li><a href="https://github.com/HLasse/TextDescriptives">TextDescriptives</a> 文本描述性统计,不支持中文</li>
<li><a href="https://github.com/hidadeng/pdfdocx">pdfdocx</a> pdf、docx读取库</li>
<li><a href="https://github.com/ocrmypdf/OCRmyPDF">OCRmyPDF</a>    为<strong>扫描的 PDF</strong> 文件添加了 OCR 文本层，允许对其进行搜索</li>
<li><a href="https://github.com/ddangelov/Top2Vec">Top2Vec</a>  主题建模和语义搜索的算法, 自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。 适用于短文本;</li>
<li><a href="https://github.com/jboynyc/textnets">TextNet</a>  textnet将文档集表示为文档和单词的网络,为文本分析与可视化提供了新的可能性。</li>
<li><a href="https://github.com/remram44/taguette">taguette</a> 免费开源的定性研究工具</li>
</ul>
<br>
<h2 id="gui窗体软件开发">GUI窗体软件开发</h2>
<ul>
<li><a href="https://wiki.python.org/moin/TkInter">tkinter</a> Python内置的gui库</li>
<li><a href="https://github.com/PySimpleGUI/PySimpleGUI">PySimpleGUI</a> 最简单的gui开发库</li>
<li><a href="https://doc.qt.io/qt.html#qtforpython">pyqt5、pyside</a> 最牛掰的gui软件开发库</li>
<li><a href="https://github.com/hoffstadt/DearPyGui">DearPyGui</a>  易于使用且功能强大的Python GUI框架，它提供了DearImGui的包装。</li>
<li><a href="https://github.com/pywebio/PyWebIO">PyWebIO</a>  快速构建 Web 应用的 Python 工具</li>
<li><a href="https://github.com/kivy/kivy">kivy</a>  star数高达14k的gui库
<br></li>
</ul>
<h2 id="自动化办公">自动化办公</h2>
<ul>
<li><a href="https://github.com/zhangyunhao116/zmail">zmail</a> 自动化收发邮件管理库</li>
<li><a href="https://github.com/pywinauto/pywinauto">pywinauto</a> Windows电脑自动化Python库</li>
<li><a href="https://github.com/Kozea/WeasyPrint">WeasyPrint</a>  自动化生产pdf报告</li>
<li><a href="https://github.com/jorisschellekens/ptext-release"></a> 对PDF文件读取、更改、添加信息</li>
<li><a href="https://github.com/SeleniumHQ/selenium/">selenium</a> 浏览器自动化框架，可以自动化点击浏览器，完成某些工作</li>
<li><a href="https://github.com/mkdocs/mkdocs/">mkdocx</a></li>
<li><a href="https://github.com/python-openxml/python-docx">python-docx</a>  创建、修改docx文件库</li>
<li><a href="https://github.com/scanny/python-pptx">python-ppt</a>  创建、修改ppt文件库</li>
<li><a href="https://openpyxl.readthedocs.io/en/stable/">openpyxl</a> xlsx文件库</li>
<li><a href="https://github.com/wang0618/PyWebIO">PyWebIO</a>  不需要编写HTML和JS代码，就可以构建简单的基于浏览器的GUI应用。</li>
</ul>
<br>
<h2 id="其他">其他</h2>
<ul>
<li><a href="https://github.com/hiresearch/hiresearch.github.io">hiresearch</a> 丢弃繁杂收藏夹，定义简洁办公的浏览器首页</li>
<li><a href="https://github.com/hakimel/reveal.js">reveal.js</a> 最流行的幻灯片</li>
<li><a href="https://github.com/slidevjs/slidev">slidev</a> 编程人员使用的幻灯片</li>
<li><a href="https://github.com/mkdocs/mkdocs">mkdocs</a> 制作文档网站</li>
<li><a href="https://github.com/mockoon/cli">mockoon</a>  帮我们快速搭建 API 服务图形化界面工具</li>
<li><a href="https://www.codepng.app/">codepng</a>  把代码转为美观的截图的website</li>
<li><a href="https://github.com/amphibian-dev/toad">toad</a> 金融风险评分卡；覆盖了建模全流程，从 EDA、特征工程、特征筛选 到 模型验证和评分卡转化</li>
<li><a href="https://github.com/salomonelli/best-resume-ever">best-resume-ever</a> Latex项目， 基于 Web 的简历模板，可以生成网页简历，然后用浏览器打印成 PDF 文件。</li>
<li><a href="https://github.com/vivjay30/pychorus">pychorus</a> 将音频文件中的高潮部分剪辑出来的python包</li>
<li><a href="https://github.com/imageio/imageio">imageio</a>  用于读取和写入图像数据的 Python 库；</li>
<li><a href="https://github.com/Textualize/rich">rich</a> 让命令行输出更美观简洁的Python包</li>
<li><a href="https://github.com/Textualize/textual">textual</a> rich作者开发的<strong>文本用户界面</strong>用户</li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 情侣分手3个月前就有预兆！聊天记录还能反映分手后遗症</title>
      <link>https://textdata.cn/blog/2022-01-02-pnas_love_separate/</link>
      <pubDate>Sun, 02 Jan 2022 12:43:10 +0600</pubDate>
      
      <guid>/blog/2022-01-02-pnas_love_separate/</guid>
      <description>女友提出分手，你是不是后悔没有早点察觉？这不，关于分手前的聊天记录的文本分析，科学家给出了答案。</description>
      <content:encoded><![CDATA[<blockquote>
<p>公众号-新智元</p>
<p>编辑: 桃子 小咸鱼</p>
</blockquote>
<p>女朋友提出分手，你是不是后悔没有早点察觉？</p>
<p><strong>这不，对于分手前的有关迹象，科学家给出了答案：聊天记录！</strong></p>
<br>
<p>近日，美国德克萨斯大学的研究人员发表的一篇论文表明，通过分析情侣的聊天记录，可以找到情侣即将分手的证据。</p>
<p>这篇 Language left behind on social media exposes the emotional and cognitive costs of a romantic breakup 已于12月20日发表在国际顶刊PNAS。</p>
<p><img loading="lazy" src="img/pnas_love_home.png" alt=""  />
</p>
<p><strong>分手前三个月，聊天便不正常了</strong></p>
<p>基于已有的数据，研究人员对6800名Reddit用户发布的1027541个帖子进行了文本分析。</p>
<p><img loading="lazy" src="img/pnas_love_table.png" alt=""  />
</p>
<p>这些帖子涵盖了用户在过去2年里的Reddit发帖数据，这些帖子的内容不仅仅与他们的感情关系有关，也涵盖了他们生活中各个方面的内容。</p>
 <br>
<h2 id="语言标记">语言标记</h2>
<p>表示「即将分手」的「<strong>语言标记</strong>」在分手发生前3个月出现的频次非常高，在分手的那一周达到峰值，并在6个月后恢复到正常基线。</p>
<p>在表示「即将分手」的「语言标记」中，出现次数比较多的是「我」、「我们」这类字眼，以及暗含认知过程（cognitive processing ）的词汇（常常表示抑郁、高度集中的注意力、探究意义等）。</p>
<p><img loading="lazy" src="img/pnas_love_line.png" alt=""  />
</p>
<p>分手前后Reddit用户语言模式的变化。第0周是每个用户在分手时公开披露分手的时间点。</p>
<p>此外，「语言标记」中有关「分析思考」（analytic thinking）的词汇的出现频次会下降，与「分析思考」相关的词汇往往包含更多以个人为中心和非正式的语言。</p>
<p>研究人员还发现，即使人们在与分手这类话题无关的群组中发帖，上述这种「语言标记」模式仍然存在。</p>
<p><strong>要注意了！</strong></p>
<p>与偶尔发一次帖的人相比，那些经常发布分手信息的人在分手一年之后，适应能力会变得很差。</p>
<p>83%的人都会以第一人称发送第一次分手的信息，并且这些帖子详细描述了分手过程，甚至导致分手的原因和分手的后果都有。</p>
<p><img loading="lazy" src="img/pnas_love_S1.png" alt=""  />
</p>
<p>例如，一位网友通过回忆来讲述自己分手的故事：</p>
<blockquote>
<p>“Hey breakups, going through a rough one this week. The girl I&rsquo;ve been seeing the last 7 months left me last Friday due to us not having common interests. Our relationship seemed just awesome and thought we were happy. We did get into a &ldquo;routine&rdquo; pretty fast and I was happy with it. Cook dinner, sex, watch movies together. I knew going into this with her she was a free spirited outdoorsy type and I am admittedly the opposite. But we sparked, and formed a relationship after a few months of talking. Things seemed great. We lived maybe 45 minutes away from each other and had different work schedules, so we only saw each other maybe 3 times a week, so it was sometimes hard to see each other&hellip;”</p>
</blockquote>
<br>
<h2 id="分析思考和认知过程">分析思考和认知过程</h2>
<p>尽管许多情侣关系解体模型都强调了分手过程中固有的认知思维，但没有一项研究能够实时跟踪认知过程。</p>
<p>同时，研究人员在研究认知过程的变化时面临着几个艰巨的挑战，包括如何识别和测量感兴趣的认知动态。</p>
<p>研究人员最近的工作确定了两种基于语言的通用思维模式：一是分析思考，另一是认知过程。</p>
<p><img loading="lazy" src="img/pnas_love_LIWC.png" alt=""  />
</p>
<p><strong>说话以「我」、「我们」为焦点</strong></p>
<p>回想下，排除第三者关系，女友和你提出分手前，是不是经常会说「我&hellip;.」</p>
<p>正如研究人员指出，当分手发生时，人们会向内去理解发生关系破裂的原因，这有时会让人陷入沉思和情绪困扰 。</p>
<p>在与抑郁、自杀、情绪剧变、消极和心理困扰相关的报告中，「我」这个词是最常见的。这表明 「我」的使用能够捕获内部的焦点和个人内部的关注。</p>
<p>同样，在情侣关系研究中，在分手之前、之中和之后查看「我」字的使用可能是一种不错的方法，来跟踪人们在整个分手过程中的对自我关注和调整。</p>
<p><img loading="lazy" src="img/pnas_love_LIWC.png" alt=""  />
</p>
<p>在分手期间，人们可能会深究对方的前任伴侣和两人间的关系。</p>
<p>「我们」一词，揭示了情侣间的关系承诺、继续关系的意图和解决问题的行为等等信息。</p>
<p>而情侣之间更多地使用「我们」一词突出了成功的浪漫关系背后靠的是情侣间的相互依存性。</p>
<p>但是，如果情侣间的关系变坏呢？</p>
<p>一些研究发现，经常分享分手故事的人，如果开始频繁地使用「我们」这个词，这种现象就预示着他和他伴侣间的关系会变得更差。</p>
<br>
<h2 id="分手后遗症">分手后遗症</h2>
<p>在人们的社交生活与其在线状态交织在一起的时代，研究分手和其他个人心情动荡出现了新的方法。</p>
<p>通过研究社交媒体帖子，研究人员已经发现了与人们情绪和心理状态相关的语言模式，例如抑郁症、创伤后应激障碍诊断和注意缺陷多动障碍症状。</p>
<p>通过对社交媒体平台中人们的语言进行分析，研究人员最终可以追踪人们在分手时不断演变的心理过程。</p>
<p>正如研究者指出，真正分手后会持续6个月的心理影响。</p>
<p><img loading="lazy" src="img/pnas_love_sig.png" alt=""  />
</p>
<p>分手后遗症，你有吗？</p>
<br>
<h2 id="数据下载httpsosfioa9qmxview_only07f3d732d9c04bcc9f6844c4e889c1e8"><a href="https://osf.io/a9qmx/?view_only=07f3d732d9c04bcc9f6844c4e889c1e8">数据下载</a></h2>
<p><a href="https://osf.io/a9qmx/?view_only=07f3d732d9c04bcc9f6844c4e889c1e8">4.8G,含代码</a></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>量化历史语言学-贝叶斯语言谱系分析</title>
      <link>https://textdata.cn/blog/quant_lang/</link>
      <pubDate>Sat, 01 Jan 2022 20:43:10 +0600</pubDate>
      
      <guid>/blog/quant_lang/</guid>
      <description>量化历史语言学，尤其是贝叶斯语言谱系分析的前世今生，以及可能的未来</description>
      <content:encoded><![CDATA[<blockquote>
<p>Author:小云哥哥</p>
<p>Src: <a href="https://zhuanlan.zhihu.com/p/386454664">https://zhuanlan.zhihu.com/p/386454664</a></p>
</blockquote>
<p>历史语言学家有两个基本任务。第一个任务是追溯相关语言的源头——所谓的“原始语言”，说得通俗一点就是推测一下祖先们是怎么说话的。但其实除非有时光机器，否则我们不可能知道祖先的发音，构拟原始语言的最终目的是使用一个自洽的系统去解释现代亲属语言的差异。这个任务是技术活儿，需要硬功夫，语言学家必须对这些语言的各方面都了如指掌，通过多年的时间真正理解这些语言的运作（尤其是音系和形态），而不是一上来就开始尬比较。第二任务是第一个任务的衍生产品。当我们能够解释亲属语言的差异以后，我们自然而然会发现有的语言差异较小，有的语言差异较大，我们会希望知道每种差异发生的时间顺序，从而推断出亲属语言是因循什么轨迹从原始语言中分化出来的。这就是语言的谱系。</p>
<p>虽然传统的历史语言学取得了非常大的成功，但是语言学家毕竟是人，他们用人力研究为数众多的语言、处理浩如烟海的语料，总会出现这样那样的问题。比如说我们会在不少著作中看到语言学家前后标准不一致，或者分析过程描述不清晰透明等现象。有时候这些问题并不是有意为之，而是因为人确实无法预估那么多的事情，出错在所难免。</p>
<p>于是，一部分语言学家开始认识到，我们需要一个机器协助的、量化的转变。人的大脑爱耍小聪明，更擅长处理复杂而特殊的个案，而机器更像一个奴隶，可以帮人类用统一的方法处理繁多和重复的工作。那么，历史语言学家的两个任务，机器可以协助我们解决哪一个呢？原始语言的构拟还是语言的谱系？</p>
<p>事实上，这两个任务都需要很多的小聪明。如果原始语言的构拟是简单的音位比较，那么机器也许可以很快做出来。但实际操作上要比这个绕很多的弯儿，需要语言学家综合知识的灵活运用。比如索绪尔的喉音理论，就需要对梵语动词变位的深入理解，从而比较不同变位模式的内在一致，并且对音变的类型学有融会贯通的了解。这一切的运作，可能在索绪尔的脑子里一秒钟就能形成，而机器则不可能在短时间内完成喉音的构拟。我们引入机器是为了提高效率，而不是降低效率。<strong>因此，语言学家把目光转向了第二个任务，语言的谱系</strong>。历史语言学确定谱系的唯一标准是共同创新，但辨认共同创新实际上也需要深厚的研究功底，机器很难按照人类的方式分析。一个比较可行的办法是偏离历史语言学的原则，使用统计学的方式，构造出在统计学意义上最可能的谱系。</p>
<p>在这篇文章中，我就用流水账的方式梳理一下机器协助的语言谱系分析的相关历史，尤其专注于贝叶斯谱系分析。因为是流水账，所以不会分小节，我也会省去所有赶客的公式和理论描述。</p>
<br>
<h2 id="语言谱系分析">语言谱系分析</h2>
<p>较早使用统计学处理语言谱系的研究可以追溯到十九世纪前叶。不过现代的尝试最值得提的是二十世纪二十年代开始的一系列操作。波兰人类学家Czekanowski[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_1">1]</a>在1928年收集了二十多个音系、形态和词汇上的特点，研究了包括立陶宛语、古教会斯拉夫语、哥特语、古爱尔兰语、拉丁语、希腊语、吠陀梵语、阿维斯陀语和亚美尼亚语相互之间的关系。他们得出的结果其中一个错误是认为哥特语与波罗地-斯拉夫语更为接近，而不是意大利-凯尔特语。1937年，加州大学的Kroeber和Chrétien[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_2">2]</a>在前人的基础上，添加了新的数据（主要是特征性的音变和形态变化方面的数据），使参与比较的特征达到了74个。下图是Kroeber and Chrétien (1937)的统计分析</p>

<figure >
    
        <img src="img/tab_occu.jpg" width="800" />
    
    
</figure>

<p>Kroeber和Chrétien就是通过判断每一个特征是否在各种语言中出现，列出矩阵计算出各语言的相似度。他们的结果，至少从这九种语言来看，基本上与历史语言学的结果相符。但由于数据本身的局限性，他们的方法并没有被大规模地使用，并且遭到了一些批评。我不知道他们的计算是否用了机器，但是从他们并不复杂的公式来看，可能是笔算的。</p>
<p>虽然这些早期的尝试寿命并不长，但是也为量化历史语言学定下了统计学的基调，尽管在数据选取上，名义上是使用了历史语言学的结论，但是并没有使用历史语言学的分类标准，而是把这些结论转化成可以用于统计学的数据。这也是从这以后，直至现在将尽一个世纪的趋势。</p>
<blockquote>
<p>La linguistique est la science statistique type ; les statisticiens le savent bien ; la plupart des linguistes l&rsquo;ignorent encore. (Guiraud 1959: 15[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_3">3]</a>)
语言学就是典型的统计科学；统计学家心里很清楚，大部分语言学家却不知道。</p>
</blockquote>
<p>&lt;比如&gt;</p>
<h2 id="基于词汇的语言谱系分析">基于词汇的语言谱系分析</h2>
<p>1950年代，有一个长得有点喜感的中年男人，叫Morris Swadesh。他是一个美国的语言学家。身为一个历史语言学家，他并不把关注的重点放在音系和形态的变化上，而是更专注于词汇。词汇相对于音系和形态，显然是更容易操作的东西，毕竟它们就像拼好的积木，能让人一眼就辨认出来。Morris Swadesh (1909-1967)</p>

<figure >
    
        <img src="img/Morris.jpg" width="800" />
    
    
</figure>

<p>Swadesh认为不同语言中词汇的重合度很可能与语系的演化有关。这点很符合我们的直觉，基因关系较远的语言中，非同源的词汇理应越多。而且，他还假设词汇系统是按一定的速率变化的，我们只要以这个速率为基础，然后比较亲属语言的同源词的多寡，就能得到语言的谱系，同时我们还可以算出亲属语言的分裂时间。<strong>Swadesh (1950)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_4">4]</a>认为词汇的变化速率是每过1000年，一种语言想对于原本形态的同源词就会降低到原来的85%。后来这个百分比又被改为81%。 这个数字大概是基于古英语和现代英语的词汇变化确定的</strong>。</p>
<h3 id="核心词汇">核心词汇</h3>
<p>我们不可能穷尽所有的词汇，所以就需要选取一些具有代表性的词汇来简化我们的研究。Swadesh整理出一份100词的词表，现在我们称为“<strong>核心词汇</strong>”或者“<strong>基本词汇</strong>”，包括身体部位、数字、颜色、基本动作等类别，这些词汇被认为是最不容易被借用的，有较大的概率是本土词汇。</p>
<h3 id="语言断代学词汇统计学">语言断代学（词汇统计学）</h3>
<p>这么一来，如果我们发现两种亲属语言在核心词汇表上有81%的同源词，那么我们就可以认为这两种语言的分化时间是1000年。如果它们有81%×81%=65.61%的同源词，那么它们的分化时间就是2000年。这个方法我们称为Glottochronology，汉语称作“<strong>语言断代学</strong>”，它也是“<strong>词汇统计学</strong>”（lexicostatistics）的最主要方法之一。</p>
<p>为了让故事更连续，我在这里删除了其它的研究方法，比如计算词汇间Levenshtein距离，有关这方面的内容，可以看<a href="https://www.zhihu.com/question/442752699/answer/1714008582">这个回答</a>。</p>
<p>总而言之，从Swadesh开始，量化历史语言学基本上就在词汇之间徘徊，人们开始想尽办法从词汇中找到语言发展的轨迹。当然， 也有从音系/形态上考虑的（Ringe et al 2002）[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_5">5]</a>，还有从类型学上考虑的（Dunn et al 2008）[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_6">6]</a>，但始终无法摆脱或者撼动以词汇为基础的大趋势。</p>
<p>语言断代学虽然在语言谱系分析的量化上取得了较大的进展，但最终仍被认为是失败的方法。这是因为它强制规定词汇有着固定的变化速率。这一基本假设从直觉上就不符合语言的发展历程，而且没有靠谱的研究去证明，反而很容易被证伪。比如说，我们使用语言年代学的模型，我们会得到格鲁吉亚语和明格列尔语的分化年代距今约1000年左右。但实际上，它们两个的分化年代要远早于公元四到五世纪（Bergsland and Vogt 1962）[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_7">7]</a>。Swadesh本人也觉得这个方法有问题。所以逐渐人们也就不再使用语言年代学了。</p>
<p>语言断代学最大的贡献不在于它得出的结论有多正确，而是让人们看到为语言分化断代的希望，通过语言的年代来研究人类史前史成为了可能，历史语言学不再是自娱自乐地谈论古人怎么说话，而一跃成为了人类历史研究中最重要的学科之一。</p>
<p>比如说，Gray and Jordan (2000)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_8">8]</a>使用简约分析（parsimony analysis）计算出南岛语的谱系，测试了有关南岛语系起源的两个假说，“快车假说（express-train）”和“岛屿纠缠假说（entangled bank）”。他们发现快车假说与南岛语谱系树惊人吻合，从而确认了南岛语是从台湾省起源，扩散到南部各个岛屿的。</p>
<p>因为有了成功的希望，所以尽管有很多语言学家对语言的断代嗤之以鼻，另一部分语言学家仍旧在探索着新的道路。我们在这里将跳过一些不太受欢迎的研究方法，比如Ringe et al (2002)和Nakhleh et al (2005)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_9">9]</a>的“完全谱系”（perfect phylogenies）。他们跟二十世纪二三十年代的那帮人类似，使用了音系和形态方面的语料来计算，当然他们的统计学方法要先进得多。只不过，他们处理语料的方式跟前人一样，基本上就是看哪一些特征在哪一些语言中存在，并没有具体到确切的实例。</p>
<h3 id="语言变化的时钟-宽松时钟">语言变化的时钟-宽松时钟</h3>
<p>谱系分析始终只是历史语言学家的其中一个任务，更多的历史语言学家喜欢智力游戏，在构拟上下的功夫比较多，对于机器处理的谱系分析的热情没那么高。正在这时，那一边厢，生物学家们正在努力地发展更有效的断代方法。美国亚利桑那大学的演化生物学家Michael Sanderson就是其中一个代表人物。他从1997到2002发表了一系列的论文[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_10">10]</a>[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_11">11]</a>，研究了一些已知的谱系树分支的年龄，认为DNA序列的发展确实是有既定的速率，这个速率是它们内在的“时钟”决定的，在不同的分支中，时钟走动的快慢是有区别的。如果我们把这个思想代入到语言学中，我们就知道，按照Swadesh的语言年代学的方法，词汇的发展被认为有统一的“时钟”，或者叫“分子时钟”（molecular clock），我们称为“严格时钟”（strict clock），而分子生物学的最新假设则是引入了“宽松时钟”（relaxed clock），换成语言学，则是认为词汇在不同语支的替换速率并不统一，而是各有各的速率。顺便一提，Swadesh的语言年代学比分子生物学中的“分子时钟”的提出（Zuckerkandl and Pauling 1965）[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_12">12]</a>要早十年左右，但是生物学家在接受新鲜事物和创新方面要比语言学家快得多。在贝叶斯谱系分析中，Thorne et al (1998)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_13">13]</a>和Drummond et al (2006)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_14">14]</a>等人都对严格时钟的框架进行了批评和测试，并发现宽松时钟确实可以更好地模拟真实的演化过程。</p>
<p>历史语言学最终还是再次向生物学靠拢了。2003年，Gray and Atkinson (2003)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_15">15]</a>在Nature上发表了一篇文章，他们使用了贝叶斯谱系分析计算出了印欧语的谱系树，并如同之前测试南岛民族的演化历程一样，这次他们也用谱系树测试了印欧语起源的两种假说，并表示语言的谱系支持原始印欧人是安纳托利亚的农民这一种看法。下图是Gray and Atkinson (2003)的印欧语谱系树</p>

<figure >
    
        <img src="img/Hcluster.jpg" width="800" />
    
    
</figure>

<br>
<h2 id="谱系分析算法">谱系分析算法</h2>
<p>那么，语言的贝叶斯谱系分析究竟是怎么进行的呢？就像我们之前说的，词汇仍旧是基础。我们选取一个核心词汇表，然后把词汇表中的词汇翻译成我们需要解决的诸语言。当然，这一切都是建立在我们有合理理由怀疑这些语言是同属一个语系的前提下，否则我们得到的结果就没有意义。把词汇翻译成各种语言以后，我们就开始辨别同源词。我们把每一个义项下的同源词找出来，并把它们配成对儿。下图是词源词典编辑工具Edictor界面下的同源词辨认</p>

<figure >
    
        <img src="img/alignLang.jpg" width="800" />
    
    
</figure>

<p>同一个义项下，不同的语言可能呈现不同的词汇。比如汉语中，表示“EAT”这个义项的词汇在粤语和闽南语中都是来自“食”的同源词（粤语：si̍k，闽南语chia̍h），而普通话中则用“吃”来替代。那么单凭这一个词，我们用人脑都可以算出来，粤语和闽南语应该归在一个分支下，而普通话则应该属于另一个分支。</p>
<p>我们就这样把数十甚至上百种语言的同源词都标记出来，并且把它们转化为机器可读的形式。那么什么样的形式机器才可读呢？机器是不会管你每个词是怎么发音的，它只想知道某两个词是不是同源词。所以你只需要告诉它哪些词是同源词，哪些不是，就可以了。如果两个词是同源词，那么就标记一个“1”，如果不是就标记一个“0”。所以你就要做一个像下图一样的东西，全是0和1，咱们看不懂，但是机器很容易看懂：</p>

<figure >
    
        <img src="img/binary.jpg" width="800" />
    
    
</figure>

<p>看到这里，大家就明白了。贝叶斯谱系分析的数据是“同源关系”，而不是同源词本身。我们把数据喂给电脑，接下来就让电脑处理吧。</p>
<h3 id="马尔可夫链蒙特卡洛">马尔可夫链蒙特卡洛</h3>
<p>很多传统语言学家诟病，电脑处理的这个部分不透明，像在黑盒里操作一样，不放心把一切交给程序。为了解除一部分疑虑，我在这里解释一下究竟机器是怎么算谱系树的，当然，为了不赶客，下文中不会出现深奥的东西。</p>
<p>机器在得到我们这些充满0和1的数据后，会开始使用贝叶斯定理，计算出一棵谱系树的可能性。它会先随机生成一棵谱系树，这棵谱系树正确反映语言谱系的概率可想而知是非常低的，但机器就会根据输入的数据，把这个概率算出来，先记下。然后它就会改变原树的形状，生成一棵新的树，再计算出这棵树正确反映语言谱系的概率算出来，与前一棵树的概率比较。如果前一棵树的概率比较小，那么我们就保留新的这棵树。如果前一棵树的概率较大，那么说明新树比旧树还要差，因此我们就会计算前后两个概率的比值（用新的概率除以旧的概率），得到的就是接受这棵新树的概率。然后机器会一直生成新的树，一直重复着相同的比较和计算，一般我们会让机器重复上千万次的计算，从而保证生成的每一棵树的概率达到一个较为稳定的值。这个过程有个名字，叫马尔可夫链蒙特卡洛（Markov chain Monte Carlo，MCMC）。大家可以看以下这篇文章，对其中的数学做了详细介绍：</p>
<p><a href="https://zhuanlan.zhihu.com/p/420214359">https://zhuanlan.zhihu.com/p/420214359</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/420214359">而今听雨：MCMC与贝叶斯推断简介：从入门到放弃111 赞同 · 16 评论文章</a></p>
<h3 id="共识树">共识树</h3>
<p>计算完了上千万次的树以后，还要进行一个步骤。就是我们需要把前边那些低概率的树删掉一点，或者说“烧掉”（burn-in），这样我们就可以排除掉那些比较糟糕的树。最终留下的带有稳定较高概率的树的集合，就是机器为我们输出的结果。所以，我们在众多有关贝叶斯谱系分析论文中看到的树，都不是一棵树，而是成千上万棵具有相近概率的树相互妥协的结果，我们称为“共识树”（consensus tree）。</p>
<p>机器计算出的每棵树的分支都有着不同的长度。这些长度跟每一个分支末端的语言年龄是成正比的。也就是说，单凭这些分支的长度，我们只能知道语言之间年龄的比值，而我们想知道的却是它们精确到年的真正年龄。这就需要我们找到一个参考点，或者一个称为prior的东西。Prior可以是对得出最佳谱系有利的任何参考数据，而针对语言谱系的年龄，最理想的prior就是语言被记录的时间。比如，我们知道书面藏语是1300多年前被记录的，那么我们就为书面藏语标记1300年的年龄。这样的信息越多，那么计算出来的年龄就会越准确。软件会结合分支的长度与我们给出的年龄信息，推算出其它语言的年龄。这样我们带有年龄的谱系树就产生了。</p>
<h3 id="densitree">Densitree</h3>
<p>即便有了年龄，共识树还是共识树，我们不能把它看作一棵单一的谱系树，这也是许多人看这类文章的误区。其实，除了这棵共识树，机器还能给我们提供另一种树，叫做Densitree。Densitree可以把所有谱系树中冲突的部分可视化，让我们看到究竟哪里出了问题。Densitree看起来还是很美观的， 是无数线条的集合。下图中展示的Sagart et al (2019)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_16">16]</a>汉藏语谱系的Densitree，显示了计算过程中出现的非树形结构。一个完美的树形结构中，每一种语言应该只被一条线连接，但是我们看到在这棵树上，有不少语言被深浅不一的线群连接了，比如比较严重的有Chepang、Tshangla、Dulong等语言。存在这一类非树状信息的一大原因在于我们没有完全正确地辨认同源词，而是被部分表面现象骗了，把借词也算成同源词，也提醒我们重新审视我们的同源词判定。Sagart et al (2019)汉藏语谱系的Densitree</p>

<figure >
    
        <img src="img/hcluster2.jpg" width="800" />
    
    
</figure>

<p>所以，我们除了看共识树以外，还要注意看densitree，densitree里有更多有用的信息。大部分人对于贝叶斯谱系分析，或者任何谱系分析的诟病都是基于最后的结论，极少注意到这些研究的数据结构和分析方法，甚至连结论都没有看全。因此，我呼吁大家除了看短短的正文，还要注意看文章的补充材料。</p>
<br>
<h2 id="贝叶斯谱系分析是不是语言学">贝叶斯谱系分析是不是语言学？</h2>
<p>语言学的贝叶斯谱系分析基本上就是如上述方式进行的，希望这样的描述足够通俗易懂。如果你们看懂了，你们可能会产生这一个疑问：究竟贝叶斯谱系分析跟传统历史语言学的结合有多紧密？这样子做出的语言谱系，究竟是不是语言学？</p>
<p>首先，我们应该明确，至少在语言学上，谱系分析的作用不是告诉我们确切的谱系，而是给我们一个有关语言谱系的参考，是辅助历史语言学研究的工具，而不能代替历史语言学本身。比如说，我们推测出的汉藏语系的谱系可以帮我们确立今后汉藏语系历史语言学研究的大方向，因为我们知道了哪些语言更可能属于同一分支，那么我们就可以根据这些线索和思路有针对性的研究。</p>
<p>贝叶斯语言谱系分析全过程中跟历史语言学有关的部分当然是前期的数据准备过程。这一过程需要历史语言学家判断同源词。如果研究对象是一个我们了解得比较深入的语系，比如印欧语系，我们判断同源词的标准当然是严格遵守历史语言学的原则的。但如果是像汉藏语系这种我们基本不了解的语系，判断同源词的时候很大程度上是靠猜测，有经验的语言学家比没有经验的一般人猜测的准确率自然会高出不少，但也不能完全保证准确。判断同源词的过程必须主要由人工处理。虽然现在也有不少判断同源词的工具和程序，但这些工具大部分基于词汇的相似性，但同源词、尤其是庞大语系下相距较远语言中的同源词往往不相似。比如拉丁语的duus和亚美尼亚语的erk是同源词，除非能把所有的音变告诉机器，否则机器是不可能把它们俩判断为同源词的。对于超级大的语系，判断同源词的工作可能长达数月，也需要好几个历史语言学家的商量与合作。做好同源词的数据后，我们就把一切复杂的计算交给电脑，等它算个几天，这一部分就脱离了传统语言学，进行纯粹统计学的计算了。</p>
<p>在得到谱系树之后，我们还可以进行后续的历史语言学研究，并把历史语言学的结论与贝叶斯谱系树进行比较。比如说，Birchall et al (2016)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_17">17]</a>就为Chapacruan语系的语言做了一个贝叶斯谱系分析，并同时使用音变创新手动得出了另一个谱系树，并对两棵树进行了比较研究，发现贝叶斯谱系分析得出的结论与手动做出的谱系树还是比较吻合的。又比如，在Sagart et al (2019)的汉藏语谱系发表后，项目成员又发表了一些后续的历史语言学研究与其遥相呼应，比如Lai et al (2020)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_18">18]</a>对西夏语谱系地位的研究，以及Jacques et Pellard (2021)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_19">19]</a>对羌缅语的分析。</p>
<p>虽然贝叶斯谱系分析的前前后后都少不了历史语言学的工作，但两者始终没有完全融合在一起，在整个研究中交集并不多，而是有着明显的分工。这一个弱点也经常被人们攻击。而且，贝叶斯谱系分析直到今天，都在使用语言学家们较为不喜欢的核心词汇作为基础，而词汇绝不能与生物学中的DNA序列相提并论，音系和形态才可以。</p>
<p>那么为什么我们坚持使用词汇呢？我在这里谈两个原因。</p>
<p>第一，词汇被认为可以涵盖历史语言学的大部分工作，并且容易操作。我们判断同源词的时候，自然要考虑到音变的规律性和对应关系，有时甚至要倒推形态，有时还要进行简单的构拟，这些工作都体现在同源词的判别中，因此我们选用词汇，并不是完全无视传统历史语言学，而是因为词汇的比较是传统历史语言学的“精华”。</p>
<p>第二，词汇的替换是可以无限进行下去的，而且词汇替换的速率已经被证明可以用一定的模型去模拟。而音变则是比较有方向性的，有的音变一旦发生，可能就没办法回头了，比如p &gt; f的音变很容易发生，而f &gt; p的音变则极少发生。另外，音变可以很快，也可以很慢，它们究竟能不能模拟也是一大问题。</p>
<p>因此，大部分语言学家在谱系分析时，都在如何更好地标记词汇上下功夫。以同源词关系为基础的谱系分析可以在较大的语系下取得成功，但如果我们要研究时间深度较浅的小分支，很可能就没那么得心应手了。</p>
<p>比如我们要研究官话的谱系，大部分官话的核心词汇都差不多，词汇替换的现象比较少，那么我们喂给机器的数据库可能大部分都是“1”，这样我们可能会得到许多平行的分支，而不是一棵有结构的树。用贝叶斯谱系分析做出的官话谱系，可能不会比白一平（2006）[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_20">20]</a>用最大简约法做出的官话谱系进步多少。再者，目前的贝叶斯分析也并非能对大语系完全掌控，比如说，Gray and Atkinson (2003)的印欧语谱系树最让人看不过眼的一点就是斯拉夫语的分类，大家可以自行上滑到他们的印欧语谱系树上，找找波兰语在哪里。</p>
<p>再举一个极端的例子，假设两种语言互相不能通话（发生了重要的音变），但所有核心词汇都是同源词，没有发生词汇替换，那么机器将认为这两种语言是同一种语言。尽管这样极端的情况在现实世界中不会发生，尽管每一种研究方法都有它的不足之处，但我们应该事先考虑到突发状况的解决办法。这就是未来我们需要解决的问题。</p>
<p>贝叶斯谱系分析在语言学上的应用已经差不多二十年了，但这二十年间，研究方法上的突破并不显著。人们当然知道这样的分析存在的问题，但是实际研究上，却很难去解决。比如在词汇替换中，有一种情况可能只有词汇的一部分被替换了，那么我们究竟是赋“1”呢，还是赋“0”呢？Hill and List (2017)[<a href="https://zhuanlan.zhihu.com/p/386454664#ref_21">21]</a>倒是提出了一个解决方案，他们开发了“部分同源词”（partial cognate）的标记方法，这种方法支持把一个词拆开，只标记同源的部分。如下图中，缅语支诸语言的“羽毛”一词，都可以分析称两个词素，我们可以把这两个词素拆开，分别标记同源关系。</p>

<figure >
    
        <img src="img/tokens.jpg" width="800" />
    
    
</figure>

<p>部分同源词的标记实际上已经向基于形态的谱系分析迈进了一步，虽然它并没有真正触及到复杂的形态变化，但至少在尝试为合成词的问题寻求合理的解决方法。部分同源词的标记通过实验证明是可行的，但是目前并没有很多真枪实弹的研究成果发表出来。</p>
<p>如果没有部分同源词的标记，贝叶斯谱系分析其实已经开始变得有点无聊，即便有了部分同源词标记，也并不能把它的有趣续命太久，毕竟这一步迈得也不大。当我们知道一个方法远没有达到理想的程度，但又不断原地打转时，我们就会自然而然地感到焦虑。</p>
<p>未来的贝叶斯谱系分析的重点必然在于我们处理数据的方式，如何融入更多的历史语言学原则是我们需要思考的。在上文中，我们已经提到，目前声称把语言学和贝叶斯谱系结合在一起的研究无非就是分别用贝叶斯做一个，再用语言学做一个，然后再进行定性的比较。这种方法是绝不能让人满意的。我们需要更加无缝的衔接。</p>
<p>另外，回归到1930年代或者Ringe et al (2002)和Nakhleh et al (2005)的“完全谱系”那种基于音系和形态特征的谱系分析似乎也是不可取的。因为这些研究对具体数据的处理完全不够，仅仅是从前人的作品里选出一部分可能对分类有用的特征进行计算，这中间仍会有许多不清晰的地方。</p>
<p>最理想的情景是从语料入手，自然地融入同源词判定以及音系、形态上的创新，让机器根据各语言创新的情况来计算出谱系树。这样不仅仅能大大增加研究的客观性和透明性——单纯的同源词判定的主观因素占比非常严重，而使用创新为依据可以让读者更直接地找出潜在的问题，而且可以让谱系分析有更强大的理论背景。</p>
<br>
<h2 id="最后">最后</h2>
<p>恰好，昨天（2021年7月6日），我在我们所的部门会议上谈到了这个问题，Gray and Atkinson (2003)的作者之一，Russell Gray，也是我们的部门主任，也谈了他的想法。他非常愿意看到新的贝叶斯谱系的方案，不过他承认即便在印欧语的谱系研究中，完全融合语言各层面的数据也是极难做到的。我解释道我不是想完全放弃以词汇的同源关系为基础的谱系分析，而是希望能通过音系和形态，去检验词汇同源关系所无法得到的细节。我的预感是，如果我们融入了音系/形态的创新，得出的结果中，非树状信号会大大减少，并帮我们检查同源词判定究竟在哪里出现了问题。</p>
<p>流水账就写到这里吧。我想大家在这篇流水账中看到的中心思想，是通过量化谱系分析的发展史，看到研究方法一步一步的变迁，以及它们遇到的困难和存在的问题。我们应该知道，评价这类研究的重点在于它们的方法，而不仅仅局限于结论——因为结论必然是有问题的，即便我们得到了一棵完美全对的谱系树，它仍旧是存在问题的，因为它并非完全基于历史语言学理论，而很大程度基于概率，它的完美只是概率问题（有时候太漂亮的结果，也是我们担心的来源之一）。只有对数据处理的方法不断地改进，才有可能把我们带向最真实的谱系分析。</p>
<p>谱系分析是历史语言学研究中的一个强大的辅助，尤其是它自带断代的特征，可以让我们更好地追溯人类的历史。因此今后的历史语言学家对机器协助的谱系研究会更加上心，争取让既有的历史语言学理论与新兴的技术更加默契地配合。这也是我的愿望列表上的一项，在今后数年的研究中会作出各种各样的尝试。</p>
<br>
<h2 id="参考">参考</h2>
<ol>
<li>Jan Czekanowski, Na Marginesie Recenzji P. K. Moszyiskiego o Ksigtce: Wstep do Historji Slowian. Lud, Series II, vol. VII (1928).</li>
<li>Kroeber, A., &amp; Chrétien, C. (1937). Quantitative Classification of Indo-European Languages. Language, 13(2), 83-103. doi:10.2307/408715</li>
<li>Guiraud, Pierre (1959), Problèmes et méthodes de la statistique linguistique, D. Reidel, Publishing Company, Dordrecht, Holland.</li>
<li>Swadesh, M. (1950). Salish internal relationships. International Journal of American Linguistics, 16(4), 157-167.</li>
<li>Ringe, D., Warnow, T., &amp; Taylor, A. (2002). Indo‐European and computational cladistics. Transactions of the philological society, 100(1), 59-129.</li>
<li>Dunn, M., Levinson, S. C., Lindström, E., Reesink, G., &amp; Terrill, A. (2008). Structural phylogeny in historical linguistics: Methodological explorations applied in Island Melanesia. Language, 710-759.</li>
<li>Bergsland, K., &amp; Vogt, H. (1962). On the validity of glottochronology. Current anthropology, 3(2), 115-153.</li>
<li>Gray, R. D., &amp; Jordan, F. M. Language trees support the express-train sequence of Austronesian expansion, 2000. Nature, 405, 1052.</li>
<li>Nakhleh, L., Ringe, D., &amp; Warnow, T. (2005). Perfect phylogenetic networks: A new methodology for reconstructing the evolutionary history of natural languages. Language, 382-420.</li>
<li>Sanderson, M. J. (1997). A nonparametric approach to estimating divergence times in the absence of rate constancy. Molecular biology and evolution, 14(12), 1218-1231.</li>
<li>Sanderson, M. 2002 Estimating absolute rates of evo- lution and divergence times: a penalized likelihood approach. Mol. Biol. Evol. 19, 101–109.</li>
<li>Zuckerkandl, E., &amp; Pauling, L. (1965). Evolutionary divergence and convergence in proteins. In Evolving genes and proteins (pp. 97-166). Academic Press.</li>
<li>Thorne, J. L., Kishino, H., &amp; Painter, I. S. (1998). Estimating the rate of evolution of the rate of molecular evolution. Molecular biology and evolution, 15(12), 1647-1657.</li>
<li>Drummond, A. J., Ho, S. Y. W., Phillips, M. J. &amp; Rambaut, A. 2006 Relaxed phylogenies and dating with confidence. PLoS Biol. 4, e88. 699 – 710. (doi:10.1371/ journal.pbio.0040088)</li>
<li>Gray, R. D., &amp; Atkinson, Q. D. (2003). Language-tree divergence times support the Anatolian theory of Indo-European origin. Nature, 426(6965), 435-439.</li>
<li>Sagart, L., Jacques, G., Lai, Y., Ryder, R. J., Thouzeau, V., Greenhill, S. J., &amp; List, J. M. (2019). Dated language phylogenies shed light on the ancestry of Sino-Tibetan. Proceedings of the National Academy of Sciences, 116(21), 10317-10322.</li>
<li>Birchall, J., Dunn, M., &amp; Greenhill, S. J. (2016). A combined comparative and phylogenetic analysis of the Chapacuran language family. International Journal of American Linguistics, 82(3), 255-284.</li>
<li>Lai, Yunfan., Gong, Xun., Gates, Jesse. P., &amp; Jacques, Guillaume. (2020). Tangut as a West Gyalrongic language. Folia Linguistica Historica, 54(s41), 171-203.</li>
<li>Jacques, G., &amp; Pellard, T. (2021). Phylogenies based on lexical innovations refute the Rung hypothesis. Diachronica, 38(1), 1-24.</li>
<li>Baxter, W. H. (2006). Mandarin dialect phylogeny. Cahiers de linguistique-Asie orientale, 35(1), 71-114.</li>
<li>Hill, N. W., &amp; List, J. M. (2017, September). Challenges of annotation and analysis in computer-assisted language comparison: A case study on Burmish languages. In Yearbook of the Poznan Linguistic Meeting (Vol. 3, No. 1, pp. 47-76). De Gruyter Open.</li>
</ol>
<h2 id="录播课">录播课</h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://textdata.cn/blog/management_python_course/">点击进入详情页</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用文本相似度可以识别变化的时间点</title>
      <link>https://textdata.cn/blog/text_sim/</link>
      <pubDate>Thu, 30 Dec 2021 12:43:10 +0600</pubDate>
      
      <guid>/blog/text_sim/</guid>
      <description>用漂亮国大统领报告(演讲)做政策连续性分析&amp;amp;可视化</description>
      <content:encoded><![CDATA[<p>使用文本相似度可以识别变化的时间点，先配置环境</p>
<h2 id="代码下载codezip"><a href="code.zip">代码下载</a></h2>
<br>
<h2 id="配置环境">配置环境</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">scikit</span><span class="o">-</span><span class="n">learn</span><span class="o">==</span><span class="mf">1.0</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">cntext</span><span class="o">==</span><span class="mf">1.2</span> 

<span class="c1"># 安装pyecharts可视化</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">pyecharts</span><span class="o">==</span><span class="mf">1.6.2</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">pyecharts</span><span class="o">-</span><span class="n">javascripthon</span><span class="o">==</span><span class="mf">0.0.6</span>              
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">pyecharts</span><span class="o">-</span><span class="n">jupyter</span><span class="o">-</span><span class="n">installer</span><span class="o">==</span><span class="mf">0.0.3</span>              
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">pyecharts</span><span class="o">-</span><span class="n">snapshot</span><span class="o">==</span><span class="mf">0.2.0</span> 
</code></pre></div><br>
<h2 id="1-查看数据">1. 查看数据</h2>
<p>本次使用sotu数据集，收集了从1790年至2018年国情咨文文本，这是漂亮国大统领每年发表的演讲，用于描述国家过去的成就和未来面临的挑战。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;sotu.csv&#39;</span><span class="p">)</span>
<span class="c1">#text2是text向下顺移1位的结果</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;text2&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#剔除空字符</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/df.png" width="800" />
    
    
</figure>

<p>两段文本的相似度可以通过cos计算</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.similarity</span> <span class="kn">import</span> <span class="n">similarity_score</span>

<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;Mr. Speaker, Mr. Vice President, members of&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;Thank you very much. Mr. Speaker, Mr. Vice&#39;</span>

<span class="n">similarity_score</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>{'Sim_Cosine': 0.4629100498862757,
 'Sim_Jaccard': 0.3,
 'Sim_MinEdit': 16,
 'Sim_Simple': 0.9619883040935673}
</code></pre>
<br>
<h2 id="2-相似度可视化">2. 相似度可视化</h2>
<p>如果把很多个相邻文本(有时间先后顺序)依次计算相似度，可以绘制出曲线，我们根据自己的领域知识，就可以看出变化的时间点。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.similarity</span> <span class="kn">import</span> <span class="n">similarity_score</span>
<span class="n">cosines</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">text1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">]</span>
    <span class="n">text2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;text2&#39;</span><span class="p">]</span>
    <span class="n">simi</span> <span class="o">=</span> <span class="n">similarity_score</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">)[</span><span class="s1">&#39;Sim_Cosine&#39;</span><span class="p">]</span>
    <span class="n">cosines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">simi</span><span class="p">)</span>
<span class="n">cosines</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[0.42767330405703097,
 0.39821498388325544,
 0.410744931596176,
 0.3844380358041578,
 0.4116242706522565,
 0.4169268094228332,
 0.4249719376001671,
 ....
 0.39065212923423315,
 0.3763764307701755,
 0.35307484669994105,
 0.4119319787659037,
 0.43053043053064594,
 0.45219743197249296,
 0.421723837550935,
 0.427904362863808]
</code></pre></div><p>紧接着</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">Line</span>
<span class="kn">from</span> <span class="nn">pyecharts</span> <span class="kn">import</span> <span class="n">options</span> <span class="k">as</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>

<span class="n">line</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>

<span class="n">line</span><span class="o">.</span><span class="n">add_xaxis</span><span class="p">(</span><span class="n">xaxis_data</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">])</span>

<span class="n">line</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s2">&#34;本期与上期的相似度&#34;</span><span class="p">,</span> 
               <span class="n">cosines</span><span class="p">,</span> 
               <span class="n">label_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">LabelOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="n">line</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span><span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;1790年至2018年国情咨文文本相似度&#34;</span><span class="p">))</span>

<span class="n">line</span><span class="o">.</span><span class="n">load_javascript</span><span class="p">()</span>
<span class="n">line</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;1790年至2018年国情咨文文本相似度可视化.html&#39;</span><span class="p">)</span>
<span class="n">line</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/%e7%9b%b8%e4%bc%bc%e5%ba%a6%e8%b6%8b%e5%8a%bf.png" width="800" />
    
    
</figure>

<p>注意，横坐标显示的是当年报告 与 前一年报告 的对比相似度</p>
<br>
<h2 id="3-图形解读">3. 图形解读</h2>
<p>相似度越低，说明本期与前期相比，文本变化较大，在本场景中可能是漂亮国在大幅度调整政策。</p>
<ol>
<li>在图中，我们最熟悉的时期是一战和二战，这个阶段在图中较长时间处于低位，漂亮国zf的政策处于战时状态。</li>
<li>漂亮国立国初期，相似度连线也长时间处于低位，体现了新国家正在探索为政之道。</li>
<li>漂亮国每4年选ju一次，那么换届年份，相似度也会比较低。</li>
</ol>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本数据: 展开、过滤和分块</title>
      <link>https://textdata.cn/blog/text_features_tutorial/</link>
      <pubDate>Tue, 28 Dec 2021 10:43:10 +0600</pubDate>
      
      <guid>/blog/text_features_tutorial/</guid>
      <description>TF-IDF刻画参与者信息的“新且熟悉” ,构建参与者文化网络</description>
      <content:encoded><![CDATA[<p>前段时间发现apachecn在github上翻译了一本和特征工程相关的书籍：《Feature Engineering for Machine Learning》，中文名为《面向机器学习的特征工程》。</p>
<p><a href="Feature_Engineering_for_Machine_Learning.pdf">Feature_Engineering_for_Machine_Learning.pdf</a></p>
<h2 id="三文本数据-展开过滤和分块httpfe4mlapachecnorgdocs3文本数据id三文本数据-展开过滤和分块"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E4%B8%89%E3%80%81%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE-%E5%B1%95%E5%BC%80%E3%80%81%E8%BF%87%E6%BB%A4%E5%92%8C%E5%88%86%E5%9D%97">三、文本数据: 展开、过滤和分块</a></h2>
<blockquote>
<p>译者：<a href="https://github.com/kkejili">@kkejili</a></p>
<p>校对者：<a href="https://github.com/KyrieHee">@HeYun</a></p>
</blockquote>
<p>如果让你来设计一个算法来分析以下段落，你会怎么做？</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Emma knocked on the door. No answer. She knocked again and waited. There was a large maple tree next to the house. Emma looked up the tree and saw a giant raven perched at the treetop. Under the afternoon sun, the raven gleamed magnificently. Its beak was hard and pointed, its claws sharp and strong. It looked regal and imposing. It reigned the tree it stood on. The raven was looking straight at Emma with its beady black eyes. Emma felt slightly intimidated. She took a step back from the door and tentatively said, “hello?” 复制ErrorOK!
</code></pre></div><p>该段包含很多信息。我们知道它谈到了到一个名叫Emma的人和一只乌鸦。这里有一座房子和一棵树，艾玛正想进屋，却看到了乌鸦。这只华丽的乌鸦注意到艾玛，她有点害怕，但正在尝试交流。</p>
<p>那么，这些信息的哪些部分是我们应该提取的显着特征？首先，提取主要角色艾玛和乌鸦的名字似乎是个好主意。接下来，注意房子，门和树的布置可能也很好。关于乌鸦的描述呢？Emma的行为呢，敲门，退后一步，打招呼呢？</p>
<p>本章介绍文本特征工程的基础知识。我们从词袋（bags of words）开始，这是基于字数统计的最简单的文本功能。一个非常相关的变换是 tf-idf，它本质上是一种特征缩放技术。它将被我在（下一篇）章节进行全面讨论。本章首先讨论文本特征提取，然后讨论如何过滤和清洗这些特征。</p>
<br>
<h2 id="bag-of-x把自然文本变成平面向量httpfe4mlapachecnorgdocs3文本数据idbag-of-x把自然文本变成平面向量"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=bag-of-x%EF%BC%9A%E6%8A%8A%E8%87%AA%E7%84%B6%E6%96%87%E6%9C%AC%E5%8F%98%E6%88%90%E5%B9%B3%E9%9D%A2%E5%90%91%E9%87%8F">Bag of X：把自然文本变成平面向量</a></h2>
<p>无论是构建机器学习模型还是特征工程，其结果应该是通俗易懂的。简单的事情很容易尝试，可解释的特征和模型相比于复杂的更易于调试。简单和可解释的功能并不总是会得到最精确的模型。但从简单开始就是一个好主意，仅在绝对必要时我们可以增加其复杂性。</p>
<p>对于文本数据，我们可以从称为 BOW 的字数统计开始。字数统计表中并没有特别费力来寻找<code>&quot;Emma&quot;</code>或乌鸦这样有趣的实体。但是这两个词在该段落中被重复提到，并且它们在这里的计数比诸如<code>&quot;hello&quot;</code>之类的随机词更高。对于此类简单的文档分类任务，字数统计通常比较适用。它也可用于信息检索，其目标是检索与输入文本相关的文档集。这两个任务都很好解释词级特征，因为某些特定词的存在可能是本文档主题内容的重要指标。</p>
<br>
<h2 id="词袋httpfe4mlapachecnorgdocs3文本数据id词袋"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E8%AF%8D%E8%A2%8B">词袋</a></h2>
<p>在词袋特征中，文本文档被转换成向量。（向量只是 n 个数字的集合。）向量包含词汇表中每个单词可能出现的数目。 如果单词<code>&quot;aardvark&quot;</code>在文档中出现三次，则该特征向量在与该单词对应的位置上的计数为 3。 如果词汇表中的单词没有出现在文档中，则计数为零。 例如，“这是一只小狗，它是非常可爱”的句子具有如图所示的 BOW 表示</p>

<figure >
    
        <img src="img/3-1.png" width="100%" />
    
    
</figure>

<p>图 3-1 转换词成向量描述图</p>
<p>BOW 将文本文档转换为平面向量。 它是“平面的”，因为它不包含任何原始的文本结构。 原文是一系列词语。但是词袋向量并没有序列；它只是记得每个单词在文本中出现多少次。 它不代表任何词层次结构的概念。 例如，“动物”的概念包括“狗”，“猫”，“乌鸦”等。但是在一个词袋表示中，这些词都是矢量的相同元素。</p>

<figure >
    
        <img src="img/3-2.png" width="100%" />
    
    
</figure>

<p>图 3-2 两个等效的词向量，向量中单词的排序不重要，只要它在数据集中的个数和文档中出现数量是一致的。</p>
<p>重要的是特征空间中数据的几何形状。 在一个词袋矢量中，每个单词成为矢量的一个维度。如果词汇表中有 n 个单词，则文档将成为n维空间中的一个点。 很难想象二维或三维以外的任何物体的几何形状，所以我们必须使用我们的想象力。 图3-3显示了我们的例句在对应于“小狗”和“可爱”两个维度的特征空间中的样子。</p>

<figure >
    
        <img src="img/3-3.png" width="100%" />
    
    
</figure>

<p>图 3-3 特征空间中文本文档的图示</p>

<figure >
    
        <img src="img/3-4.png" width="100%" />
    
    
</figure>

<p>图 3-4 三维特征空间</p>
<p>图 3-3 和图 3-4 描绘了特征空间中的数据向量。 坐标轴表示单个单词，它们是词袋表示下的特征，空间中的点表示数据点（文本文档）。 有时在数据空间中查看特征向量也是有益的。 特征向量包含每个数据点中特征的值。 轴表示单个数据点和点表示特征向量。 图 3-5 展示了一个例子。 通过对文本文档进行词袋特征化，一个特征是一个词，一个特征向量包含每个文档中这个词的计数。 这样，一个单词被表示为一个“一个词向量”。正如我们将在第 4 章中看到的那样，这些文档词向量来自词袋向量的转置矩阵。</p>

<figure >
    
        <img src="img/3-5.png" width="100%" />
    
    
</figure>

<br>
<h2 id="bag-of-n-gramhttpfe4mlapachecnorgdocs3文本数据idbag-of-n-gram"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=bag-of-n-gram">Bag-of-N-gram</a></h2>
<p>Bag-of-N-gram 或者 bag-of-ngram 是 BOW 的自然延伸。 n-gram 是 n 个有序的记号（token）。一个词基本上是一个 1-gram，也被称为一元模型。当它被标记后，计数机制可以将单个词进行计数，或将重叠序列计数为 n-gram。例如，<code>&quot;Emma knocked on the door&quot;</code>这句话会产生 n-gram，如<code>&quot;Emma knocked&quot;</code>，<code>&quot;knocked on&quot;</code>，<code>&quot;on the&quot;</code>，<code>&quot;the door&quot;</code>。 N-gram 保留了文本的更多原始序列结构，故 bag-of-ngram可以提供更多信息。但是，这是有代价的。理论上，用 k 个独特的词，可能有 k 个独立的 2-gram（也称为 bigram）。在实践中，并不是那么多，因为不是每个单词后都可以跟一个单词。尽管如此，通常有更多不同的 n-gram（n &gt; 1）比单词更多。这意味着词袋会更大并且有稀疏的特征空间。这也意味着 n-gram 计算，存储和建模的成本会变高。n 越大，信息越丰富，成本越高。</p>
<p>为了说明随着 n 增加 n-gram 的数量如何增加，我们来计算纽约时报文章数据集上的 n-gram。我们使用 Pandas 和 scikit-learn 中的<code>CountVectorizer</code>转换器来计算前 10,000 条评论的 n-gram。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">pandas</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">json</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span> 
<span class="c1"># Load the first 10,000 reviews </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json&#39;</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">js</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span> 
<span class="o">...</span> <span class="n">js</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()))</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">review_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">js</span><span class="p">)</span> 
<span class="c1"># Create feature transformers for unigram, bigram, and trigram. </span>
<span class="c1"># The default ignores single-character words, which is useful in practice because it trims </span>
<span class="c1"># uninformative words. But we explicitly include them in this example for illustration purposes. </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">bow_converter</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">token_pattern</span><span class="o">=</span><span class="s1">&#39;(?u)</span><span class="se">\\</span><span class="s1">b</span><span class="se">\\</span><span class="s1">w+</span><span class="se">\\</span><span class="s1">b&#39;</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">bigram_converter</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">token_pattern</span><span class="o">=</span><span class="s1">&#39;(?u)</span><span class="se">\\</span><span class="s1">b</span><span class="se">\\</span><span class="s1">w+</span><span class="se">\\</span><span class="s1">b&#39;</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">trigram_converter</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">token_pattern</span><span class="o">=</span><span class="s1">&#39;(?u)</span><span class="se">\\</span><span class="s1">b</span><span class="se">\\</span><span class="s1">w+</span><span class="se">\\</span><span class="s1">b&#39;</span><span class="p">)</span> 
<span class="c1"># Fit the transformers and look at vocabulary size </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">bow_converter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">review_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="n">bow_converter</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">bigram_converter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">review_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">bigram</span> <span class="o">=</span> <span class="n">bigram_converter</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">trigram_converter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">review_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">trigram</span> <span class="o">=</span> <span class="n">trigram_converter</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">bigram</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">trigram</span><span class="p">))</span> 
<span class="mi">26047</span> <span class="mi">346301</span> <span class="mi">847545</span> 
<span class="c1"># Sneak a peek at the ngram themselves</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">words</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span> 
<span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;00&#39;</span><span class="p">,</span> <span class="s1">&#39;000&#39;</span><span class="p">,</span> <span class="s1">&#39;0002&#39;</span><span class="p">,</span> <span class="s1">&#39;00am&#39;</span><span class="p">,</span> <span class="s1">&#39;00ish&#39;</span><span class="p">,</span> <span class="s1">&#39;00pm&#39;</span><span class="p">,</span> <span class="s1">&#39;01&#39;</span><span class="p">,</span> <span class="s1">&#39;01am&#39;</span><span class="p">,</span> <span class="s1">&#39;02&#39;</span><span class="p">]</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">bigram</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span> 
<span class="p">[</span><span class="s1">&#39;zucchinis at&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zucchinis took&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zucchinis we&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zuma over&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zuppa di&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zuppa toscana&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zuppe di&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zurich and&#39;</span><span class="p">,</span> 
<span class="s1">&#39;zz top&#39;</span><span class="p">,</span> 
<span class="s1">&#39;à la&#39;</span><span class="p">]</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">trigram</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span> 
<span class="p">[</span><span class="s1">&#39;0 10 definitely&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 2 also&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 25 per&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 3 miles&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 30 a&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 30 everything&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 30 lb&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 35 tip&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 5 curry&#39;</span><span class="p">,</span> 
<span class="s1">&#39;0 5 pork&#39;</span><span class="p">]</span> <span class="n">复制ErrorOK</span><span class="err">!</span>
</code></pre></div>
<figure >
    
        <img src="img/3-6.png" width="100%" />
    
    
</figure>

<p>图3-6 Number of unique n-gram in the first 10,000 reviews of the Yelp dataset</p>
<br>
<h3 id="过滤清洗特征httpfe4mlapachecnorgdocs3文本数据id过滤清洗特征"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E8%BF%87%E6%BB%A4%E6%B8%85%E6%B4%97%E7%89%B9%E5%BE%81">过滤清洗特征</a></h3>
<p>我们如何清晰地将信号从噪声中分离出来？ 通过过滤，使用原始标记化和计数来生成简单词表或 n-gram 列表的技术变得更加可用。 短语检测，我们将在下面讨论，可以看作是一个特别的 bigram 过滤器。 以下是执行过滤的几种方法。</p>
<br>
<h3 id="停用词httpfe4mlapachecnorgdocs3文本数据id停用词"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%81%9C%E7%94%A8%E8%AF%8D">停用词</a></h3>
<p>分类和检索通常不需要对文本有深入的理解。 例如，在<code>&quot;Emma knocked on the door&quot;</code>一句中，<code>&quot;on&quot;</code>和<code>&quot;the&quot;</code>这两个词没有包含很多信息。 代词、冠词和介词大部分时间并没有显示出其价值。流行的 Python NLP 软件包 NLTK 包含许多语言的语言学家定义的停用词列表。 （您将需要安装 NLTK 并运行<code>nltk.download()</code>来获取所有的好东西。）各种停用词列表也可以在网上找到。 例如，这里有一些来自英语停用词的示例词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Sample words from the nltk stopword list
a, about, above, am, an, been, didn’t, couldn’t, i’d, i’ll, itself, let’s, myself, our, they, through, when’s, whom, ... 复制ErrorOK!
</code></pre></div><p>请注意，该列表包含撇号，并且这些单词没有大写。 为了按原样使用它，标记化过程不得去掉撇号，并且这些词需要转换为小写。</p>
<br>
<h3 id="基于频率的过滤httpfe4mlapachecnorgdocs3文本数据id基于频率的过滤"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%9F%BA%E4%BA%8E%E9%A2%91%E7%8E%87%E7%9A%84%E8%BF%87%E6%BB%A4">基于频率的过滤</a></h3>
<p>停用词表是一种去除空洞特征常用词的方法。还有其他更统计的方法来理解“常用词”的概念。在搭配提取中，我们看到依赖于手动定义的方法，以及使用统计的方法。同样的想法也适用于文字过滤。我们也可以使用频率统计。</p>
<br>
<h3 id="高频词httpfe4mlapachecnorgdocs3文本数据id高频词"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E9%AB%98%E9%A2%91%E8%AF%8D">高频词</a></h3>
<p>频率统计对滤除语料库专用常用词以及通用停用词很有用。例如，纽约时报文章数据集中经常出现“纽约时报”和其中单个单词。“议院”这个词经常出现在加拿大议会辩论的Hansard语料库中的“众议院”一词中，这是一种用于统计机器翻译的流行数据集，因为它包含所有文档的英文和法文版本。这些词在普通语言中有意义，但不在语料库中。手动定义的停用词列表将捕获一般停用词，但不是语料库特定的停用词。</p>
<p>表 3-1 列出了 Yelp 评论数据集中最常用的 40 个单词。在这里，频率被认为是它们出现在文件（评论）中的数量，而不是它们在文件中的数量。正如我们所看到的，该列表涵盖了许多停用词。它也包含一些惊喜。<code>&quot;s&quot;</code>和<code>&quot;t&quot;</code>在列表中，因为我们使用撇号作为标记化分隔符，并且诸如<code>&quot;Mary's&quot;</code>或<code>&quot;did not&quot;</code>之类的词被解析为<code>&quot;Mary s&quot;</code>和<code>&quot;didn t&quot;</code>。词<code>&quot;good&quot;</code>，<code>&quot;food&quot;</code>和<code>&quot;great&quot;</code>分别出现在三分之一的评论中。但我们可能希望保留它们，因为它们对于情感分析或业务分类非常有用。</p>

<figure >
    
        <img src="img/biao.png" width="100%" />
    
    
</figure>

<p>最常用的单词最可以揭示问题，并突出显示通常有用的单词通常在该语料库中曾出现过多次。 例如，纽约时报语料库中最常见的词是“时代”。实际上，它有助于将基于频率的过滤与停用词列表结合起来。还有一个棘手的问题，即何处放置截止点。 不幸的是这里没有统一的答案。在大多数情况下截断还需手动确定，并且在数据集改变时可能需要重新检查。</p>
<br>
<h3 id="稀有词httpfe4mlapachecnorgdocs3文本数据id稀有词"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E7%A8%80%E6%9C%89%E8%AF%8D">稀有词</a></h3>
<p>根据任务的不同，可能还需要筛选出稀有词。对于统计模型而言，仅出现在一个或两个文档中的单词更像噪声而非有用信息。例如，假设任务是根据他们的 Yelp 评论对企业进行分类，并且单个评论包含<code>&quot;gobbledygook&quot;</code>这个词。基于这一个词，我们将如何说明这家企业是餐厅，美容院还是一间酒吧？即使我们知道在这种情况下的这种生意发生在酒吧，它也会对于其他包含<code>&quot;gobbledygook&quot;</code>这个词的评论来说，这可能是一个错误。</p>
<p>不仅稀有词不可靠，而且还会产生计算开销。这套 160 万个 Yelp 评论包含 357,481 个独特单词（用空格和标点符号表示），其中 189,915 只出现在一次评论中，41,162 次出现在两次评论中。超过 60% 的词汇很少发生。这是一种所谓的重尾分布，在现实世界的数据中非常普遍。许多统计机器学习模型的训练时间随着特征数量线性地变化，并且一些模型是二次的或更差的。稀有词汇会产生大量的计算和存储成本，而不会带来额外的收益。</p>
<p>根据字数统计，可以很容易地识别和修剪稀有词。或者，他们的计数可以汇总到一个特殊的垃圾箱中，可以作为附加功能。图3-7展示了一个短文档中的表示形式，该短文档包含一些常用单词和两个稀有词<code>&quot;gobbledygook&quot;</code>和<code>&quot;zylophant&quot;</code>。通常单词保留自己的计数，可以通过停用词列表或其他频率进一步过滤方法。这些难得的单词会失去他们的身份并被分组到垃圾桶功能中.</p>

<figure >
    
        <img src="img/3-7.png" width="100%" />
    
    
</figure>

<p>由于在计算整个语料库之前不会知道哪些词很少，因此需要收集垃圾桶功能作为后处理步骤。</p>
<p>由于本书是关于特征工程的，因此我们将重点放在特征上。但稀有概念也适用于数据点。如果文本文档很短，那么它可能不包含有用的信息，并且在训练模型时不应使用该信息。</p>
<p>应用此规则时必须谨慎。维基百科转储包含许多不完整的存根，可能安全过滤。另一方面，推文本身就很短，并且需要其他特征和建模技巧。</p>
<br>
<h3 id="词干解析stemminghttpfe4mlapachecnorgdocs3文本数据id词干解析stemming"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E8%AF%8D%E5%B9%B2%E8%A7%A3%E6%9E%90%EF%BC%88stemming%EF%BC%89">词干解析（Stemming）</a></h3>
<p>简单解析的一个问题是同一个单词的不同变体会被计算为单独的单词。例如，<code>&quot;flower&quot;</code>和<code>&quot;flowers&quot;</code>在技术上是不同的记号，<code>&quot;swimmer&quot;</code>，<code>&quot;swimming&quot;</code>和<code>&quot;swim&quot;</code>也是如此，尽管它们的含义非常接近。如果所有这些不同的变体都映射到同一个单词，那将会很好。</p>
<p>词干解析是一项 NLP 任务，试图将单词切分为基本的语言词干形式。有不同的方法。有些基于语言规则，其他基于观察统计。被称为词形化的算法的一个子类将词性标注和语言规则结合起来。</p>
<p>Porter stemmer 是英语中使用最广泛的免费词干工具。原来的程序是用 ANSI C 编写的，但是很多其他程序包已经封装它来提供对其他语言的访问。尽管其他语言的努力正在进行，但大多数词干工具专注于英语。</p>
<p>以下是通过 NLTK Python 包运行 Porter stemmer 的示例。正如我们所看到的，它处理了大量的情况，包括将<code>&quot;sixties&quot;</code>和<code>&quot;sixty&quot;</code>转变为同一根<code>&quot;sixti&quot;</code>。但这并不完美。单词<code>&quot;goes&quot;</code>映射到<code>&quot;goe&quot;</code>，而<code>&quot;go&quot;</code>映射到它自己。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">nltk</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">porter</span><span class="o">.</span><span class="n">PorterStemmer</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;flowers&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;lemon&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;zeroes&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;zero&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;stemmer&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;stem&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;sixties&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;sixti&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;sixty&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;sixty&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;goes&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;goe&#39;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;go&#39;</span><span class="p">)</span> 
<span class="sa">u</span><span class="s1">&#39;go&#39;</span> <span class="n">复制ErrorOK</span><span class="err">!</span>
</code></pre></div><p>词干解析的确有一个计算成本。 最终收益是否大于成本取决于应用程序。</p>
<br>
<h3 id="含义的原子从单词到-n-gram-到短语httpfe4mlapachecnorgdocs3文本数据id含义的原子从单词到-n-gram-到短语"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%90%AB%E4%B9%89%E7%9A%84%E5%8E%9F%E5%AD%90%EF%BC%9A%E4%BB%8E%E5%8D%95%E8%AF%8D%E5%88%B0-n-gram-%E5%88%B0%E7%9F%AD%E8%AF%AD">含义的原子：从单词到 N-gram 到短语</a></h3>
<p>词袋的概念很简单。但是，一台电脑怎么知道一个词是什么？文本文档以数字形式表示为一个字符串，基本上是一系列字符。也可能会遇到 JSON blob 或 HTML 页面形式的半结构化文本。但即使添加了标签和结构，基本单位仍然是一个字符串。如何将字符串转换为一系列的单词？这涉及解析和标记化的任务，我们将在下面讨论。</p>
<br>
<h3 id="解析和分词httpfe4mlapachecnorgdocs3文本数据id解析和分词"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E8%A7%A3%E6%9E%90%E5%92%8C%E5%88%86%E8%AF%8D">解析和分词</a></h3>
<p>当字符串包含的不仅仅是纯文本时，解析是必要的。例如，如果原始数据是网页，电子邮件或某种类型的日志，则它包含额外的结构。人们需要决定如何处理日志中的标记，页眉，页脚或无趣的部分。如果文档是网页，则解析器需要处理 URL。如果是电子邮件，则可能需要特殊字段，例如 From，To 和 Subject 需要被特别处理，否则，这些标题将作为最终计数中的普通单词统计，这可能没有用处。</p>
<p>解析后，文档的纯文本部分可以通过标记。这将字符串（一系列字符）转换为一系列记号。然后可以将每个记号计为一个单词。分词器需要知道哪些字符表示一个记号已经结束，另一个正在开始。空格字符通常是好的分隔符，正如标点符号一样。如果文本包含推文，则不应将井号（<code>#</code>）用作分隔符（也称为分隔符）。</p>
<p>有时，分析需要使用句子而不是整个文档。例如，n-gram 是一个句子的概括，不应超出句子范围。更复杂的文本特征化方法，如 word2vec 也适用于句子或段落。在这些情况下，需要首先将文档解析为句子，然后将每个句子进一步标记为单词。</p>
<br>
<h3 id="字符串对象httpfe4mlapachecnorgdocs3文本数据id字符串对象"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%AF%B9%E8%B1%A1">字符串对象</a></h3>
<p>字符串对象有各种编码，如 ASCII 或 Unicode。纯英文文本可以用 ASCII 编码。 一般语言需要 Unicode。 如果文档包含非 ASCII 字符，则确保分词器可以处理该特定编码。否则，结果将不正确。</p>
<br>
<h3 id="短语检测的搭配提取httpfe4mlapachecnorgdocs3文本数据id短语检测的搭配提取"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E7%9F%AD%E8%AF%AD%E6%A3%80%E6%B5%8B%E7%9A%84%E6%90%AD%E9%85%8D%E6%8F%90%E5%8F%96">短语检测的搭配提取</a></h3>
<p>连续的记号能立即被转化成词表和 n-gram。但从语义上讲，我们更习惯于理解短语，而不是 n-gram。在计算自然语言处理中，有用短语的概念被称为搭配。用 Manning 和 Schütze（1999：141）的话来说：“搭配是一个由两个或两个以上单词组成的表达，它们对应于某种常规的说话方式。”</p>
<p>搭配比其部分的总和更有意义。例如，<code>&quot;strong tea&quot;</code>具有超越<code>&quot;great physical strength&quot;</code>和<code>&quot;tea&quot;</code>的不同含义，因此被认为是搭配。另一方面，“可爱的小狗”这个短语恰恰意味着它的部分总和：“可爱”和“小狗”。因此，它不被视为搭配。</p>
<p>搭配不一定是连续的序列。<code>&quot;Emma knocked on the door&quot;</code>一词被认为包含搭配<code>&quot;knock door&quot;</code>，因此不是每一个搭配都是一个 n-gram。相反，并不是每个 n-gram 都被认为是一个有意义的搭配。</p>
<p>由于搭配不仅仅是其部分的总和，它们的含义也不能通过单个单词计数来充分表达。作为一种表现形式，词袋不足。袋子的 ngram 也是有问题的，因为它们捕获了太多无意义的序列（考虑<code>&quot;this is in the bag-of-ngram example&quot;</code>），而没有足够的有意义的序列。</p>
<p>搭配作为功能很有用。但是，如何从文本中发现并提取它们呢？一种方法是预先定义它们。如果我们努力尝试，我们可能会找到各种语言的全面成语列表，我们可以通过文本查看任何匹配。这将是非常昂贵的，但它会工作。如果语料库是非常特定领域的并且包含深奥的术语，那么这可能是首选的方法。但是这个列表需要大量的手动管理，并且需要不断更新语料库。例如，分析推文，博客和文章可能不太现实。</p>
<p>自从统计 NLP 过去二十年出现以来，人们越来越多地选择用于查找短语的统计方法。统计搭配提取方法不是建立固定的短语和惯用语言列表，而是依赖不断发展的数据来揭示当今流行的语言。</p>
<br>
<h3 id="基于频率的方法httpfe4mlapachecnorgdocs3文本数据id基于频率的方法"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%9F%BA%E4%BA%8E%E9%A2%91%E7%8E%87%E7%9A%84%E6%96%B9%E6%B3%95">基于频率的方法</a></h3>
<p>一个简单的黑魔法是频繁发生的 n-gram。这种方法的问题是最常发生的，这种可能不是最有用的。 表 3-2 显示了整个 Yelp 评论数据集中最流行的 bigram（<code>n=2</code>）。 正如我们所知的，按文件计数排列的最常见的十大常见术语是非常通用的术语，并不包含太多含义。</p>

<figure >
    
        <img src="img/biaod.png" width="100%" />
    
    
</figure>

<br>
<h3 id="用于搭配提取的假设检验httpfe4mlapachecnorgdocs3文本数据id用于搭配提取的假设检验"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E7%94%A8%E4%BA%8E%E6%90%AD%E9%85%8D%E6%8F%90%E5%8F%96%E7%9A%84%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C">用于搭配提取的假设检验</a></h3>
<p>原始流行度计数（Raw popularity count）是一个比较粗糙的方法。我们必须找到更聪慧的统计数据才能够轻松挑选出有意义的短语。关键的想法是看两个单词是否经常出现在一起。回答这个问题的统计机制被称为假设检验。</p>
<p>假设检验是将噪音数据归结为“是”或“否”的答案。它涉及将数据建模为从随机分布中抽取的样本。随机性意味着人们永远无法 100% 的确定答案；总会有异常的机会。所以答案附在概率上。例如，假设检验的结果可能是“这两个数据集来自同一分布，其概率为 95%”。对于假设检验的温和介绍，请参阅可汗学院关于假设检验和 p 值的教程。</p>
<p>在搭配提取的背景下，多年来已经提出了许多假设检验。最成功的方法之一是基于似然比检验（Dunning，1993）。对于给定的一对单词，该方法测试两个假设观察的数据集。假设 1（原假设）表示，词语 1 独立于词语 2 出现。另一种说法是说，看到词语1对我们是否看到词语2没有影响。假设 2（备选假设）说，看到词 1 改变了看到单词 2 的可能性。我们采用备选假设来暗示这两个单词形成一个共同的短语。因此，短语检测（也称为搭配提取）的似然比检验提出了以下问题：给定文本语料库中观察到的单词出现更可能是从两个单词彼此独立出现的模型中生成的，或者模型中两个词的概率纠缠？</p>
<p>这是有用的。让我们算一点。（数学非常精确和简洁地表达事物，但它确实需要与自然语言完全不同的分析器。）</p>

<figure >
    
        <img src="img/gongshi.png" width="100%" />
    
    
</figure>

<p>似然函数<code>L(Data; H)</code>表示在单词对的独立模型或非独立模型下观察数据集中词频的概率。为了计算这个概率，我们必须对如何生成数据做出另一个假设。最简单的数据生成模型是二项模型，其中对于数据集中的每个单词，我们抛出一个硬币，并且如果硬币朝上出现，我们插入我们的特殊单词，否则插入其他单词。在此策略下，特殊词的出现次数遵循二项分布。二项分布完全由词的总数，词的出现次数和词首概率决定。</p>
<p>似然比检验分析常用短语的算法收益如下。</p>
<ol>
<li>
<p>计算所有单体词的出现概率：<code>p(w)</code>。</p>
</li>
<li>
<p>计算所有唯一双元的条件成对词发生概率：<code>p(W2 × W1)</code></p>
</li>
<li>
<p>计算所有唯一的双对数似然比对数。</p>
</li>
<li>
<p>根据它们的似然比排序双字节。</p>
</li>
<li>
<p>以最小似然比值作为特征。</p>
<br>
</li>
</ol>
<h3 id="掌握似然比测试httpfe4mlapachecnorgdocs3文本数据id掌握似然比测试"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E6%8E%8C%E6%8F%A1%E4%BC%BC%E7%84%B6%E6%AF%94%E6%B5%8B%E8%AF%95">掌握似然比测试</a></h3>
<p>关键在于测试比较的不是概率参数本身，而是在这些参数（以及假设的数据生成模型）下观察数据的概率。可能性是统计学习的关键原则之一。但是在你看到它的前几次，这绝对是一个令人困惑的问题。一旦你确定了逻辑，它就变得直观了。</p>
<p>还有另一种基于点互信息的统计方法。但它对真实世界文本语料库中常见的罕见词很敏感。因此它不常用，我们不会在这里展示它。</p>
<p>请注意，搭配抽取的所有统计方法，无论是使用原始频率，假设测试还是点对点互信息，都是通过过滤候选词组列表来进行操作的。生成这种清单的最简单和最便宜的方法是计算 n-gram。它可能产生不连续的序列，但是它们计算成本颇高。在实践中，即使是连续 n-gram，人们也很少超过 bi-gram 或 tri-gram，因为即使在过滤之后，它们的数量也很多。为了生成更长的短语，还有其他方法，如分块或与词性标注相结合。</p>
<br>
<h3 id="分块chunking和词性标注part-of-speech-tagginghttpfe4mlapachecnorgdocs3文本数据id分块chunking和词性标注part-of-speech-tagging"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%88%86%E5%9D%97%EF%BC%88chunking%EF%BC%89%E5%92%8C%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%EF%BC%88part-of-speech-tagging%EF%BC%89">分块（Chunking）和词性标注（part-of-Speech Tagging）</a></h3>
<p>分块比 n-gram 要复杂一点，因为它基于词性，基于规则的模型形成了记号序列。</p>
<p>例如，我们可能最感兴趣的是在问题中找到所有名词短语，其中文本的实体，主题最为有趣。 为了找到这个，我们使用词性标记每个作品，然后检查该标记的邻域以查找词性分组或“块”。 定义单词到词类的模型通常是语言特定的。 几种开源 Python 库（如 NLTK，Spacy 和 TextBlob）具有多种语言模型。</p>
<p>为了说明 Python 中的几个库如何使用词性标注非常简单地进行分块，我们再次使用 Yelp 评论数据集。 我们将使用 spacy 和 TextBlob 来评估词类以找到名词短语。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">json</span> 
<span class="c1"># Load the first 10 reviews </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json&#39;</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">js</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span> 
<span class="n">js</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">readline</span><span class="p">()))</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">review_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">js</span><span class="p">)</span> 
<span class="c1">## First we&#39;ll walk through spaCy&#39;s functions </span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">spacy</span> 
<span class="c1"># preload the language model </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;en&#39;</span><span class="p">)</span> 
<span class="c1"># We can create a Pandas Series of spaCy nlp variables </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">doc_df</span> <span class="o">=</span> <span class="n">review_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">nlp</span><span class="p">)</span> 
<span class="c1"># spaCy gives you fine grained parts of speech using: (.pos_) </span>
<span class="c1"># and coarse grained parts of speech using: (.tag_) </span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">doc_df</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span> 
<span class="nb">print</span><span class="p">([</span><span class="n">doc</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">doc</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">doc</span><span class="o">.</span><span class="n">tag_</span><span class="p">])</span> 
<span class="n">Got</span> <span class="n">VERB</span> <span class="n">VBP</span> 
<span class="n">a</span> <span class="n">DET</span> <span class="n">DT</span> 
<span class="n">letter</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="ow">in</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">the</span> <span class="n">DET</span> <span class="n">DT</span> 
<span class="n">mail</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="n">last</span> <span class="n">ADJ</span> <span class="n">JJ</span> 
<span class="n">week</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="n">that</span> <span class="n">ADJ</span> <span class="n">WDT</span> 
<span class="n">said</span> <span class="n">VERB</span> <span class="n">VBD</span> 
<span class="n">Dr</span><span class="o">.</span> <span class="n">PROPN</span> <span class="n">NNP</span> 
<span class="n">Goldberg</span> <span class="n">PROPN</span> <span class="n">NNP</span> 
<span class="ow">is</span> <span class="n">VERB</span> <span class="n">VBZ</span> 
<span class="n">moving</span> <span class="n">VERB</span> <span class="n">VBG</span> 
<span class="n">to</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">Arizona</span> <span class="n">PROPN</span> <span class="n">NNP</span> 
<span class="n">to</span> <span class="n">PART</span> <span class="n">TO</span> 
<span class="n">take</span> <span class="n">VERB</span> <span class="n">VB</span> 
<span class="n">a</span> <span class="n">DET</span> <span class="n">DT</span> 
<span class="n">new</span> <span class="n">ADJ</span> <span class="n">JJ</span> 
<span class="n">position</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="n">there</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="ow">in</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">June</span> <span class="n">PROPN</span> <span class="n">NNP</span> 
<span class="o">.</span> <span class="n">PUNCT</span> <span class="o">.</span> 
<span class="n">SPACE</span> <span class="n">SP</span> 
<span class="n">He</span> <span class="n">PRON</span> <span class="n">PRP</span> 
<span class="n">will</span> <span class="n">VERB</span> <span class="n">MD</span> 
<span class="n">be</span> <span class="n">VERB</span> <span class="n">VB</span> 
<span class="n">missed</span> <span class="n">VERB</span> <span class="n">VBN</span> 
<span class="n">very</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="n">much</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="o">.</span> <span class="n">PUNCT</span> <span class="o">.</span> 
<span class="n">SPACE</span> <span class="n">SP</span> 
<span class="n">I</span> <span class="n">PRON</span> <span class="n">PRP</span> 
<span class="n">think</span> <span class="n">VERB</span> <span class="n">VBP</span> 
<span class="n">finding</span> <span class="n">VERB</span> <span class="n">VBG</span> 
<span class="n">a</span> <span class="n">DET</span> <span class="n">DT</span> 
<span class="n">new</span> <span class="n">ADJ</span> <span class="n">JJ</span> 
<span class="n">doctor</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="ow">in</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">NYC</span> <span class="n">PROPN</span> <span class="n">NNP</span> 
<span class="n">that</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">you</span> <span class="n">PRON</span> <span class="n">PRP</span> 
<span class="n">actually</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="n">like</span> <span class="n">INTJ</span> <span class="n">UH</span> 
<span class="n">might</span> <span class="n">VERB</span> <span class="n">MD</span> 
<span class="n">almost</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="n">be</span> <span class="n">VERB</span> <span class="n">VB</span> 
<span class="k">as</span> <span class="n">ADV</span> <span class="n">RB</span> 
<span class="n">awful</span> <span class="n">ADJ</span> <span class="n">JJ</span> 
<span class="k">as</span> <span class="n">ADP</span> <span class="n">IN</span> 
<span class="n">trying</span> <span class="n">VERB</span> <span class="n">VBG</span> 
<span class="n">to</span> <span class="n">PART</span> <span class="n">TO</span> 
<span class="n">find</span> <span class="n">VERB</span> <span class="n">VB</span> 
<span class="n">a</span> <span class="n">DET</span> <span class="n">DT</span> 
<span class="n">date</span> <span class="n">NOUN</span> <span class="n">NN</span> 
<span class="err">!</span> <span class="n">PUNCT</span> <span class="o">.</span> 
<span class="c1"># spaCy also does some basic noun chunking for us </span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">([</span><span class="n">chunk</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">doc_df</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">noun_chunks</span><span class="p">])</span> 
<span class="p">[</span><span class="n">a</span> <span class="n">letter</span><span class="p">,</span> <span class="n">the</span> <span class="n">mail</span><span class="p">,</span> <span class="n">Dr</span><span class="o">.</span> <span class="n">Goldberg</span><span class="p">,</span> <span class="n">Arizona</span><span class="p">,</span> <span class="n">a</span> <span class="n">new</span> <span class="n">position</span><span class="p">,</span> <span class="n">June</span><span class="p">,</span> <span class="n">He</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">a</span> <span class="n">new</span> <span class="n">doctor</span><span class="p">,</span> <span class="n">NYC</span><span class="p">,</span> <span class="n">you</span><span class="p">,</span> <span class="n">a</span> <span class="n">date</span><span class="p">]</span> 
<span class="c1">##### </span>
<span class="c1">## We can do the same feature transformations using Textblob </span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">textblob</span> <span class="kn">import</span> <span class="n">TextBlob</span> 
<span class="c1"># The default tagger in TextBlob uses the PatternTagger, which is fine for our example. </span>
<span class="c1"># You can also specify the NLTK tagger, which works better for incomplete sentences. </span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">blob_df</span> <span class="o">=</span> <span class="n">review_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">TextBlob</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">blob_df</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">tags</span> 
<span class="p">[(</span><span class="s1">&#39;Got&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;DT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;letter&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;the&#39;</span><span class="p">,</span> <span class="s1">&#39;DT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;mail&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;last&#39;</span><span class="p">,</span> <span class="s1">&#39;JJ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;week&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s1">&#39;WDT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;said&#39;</span><span class="p">,</span> <span class="s1">&#39;VBD&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;Dr.&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;Goldberg&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;VBZ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;moving&#39;</span><span class="p">,</span> <span class="s1">&#39;VBG&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;TO&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;Arizona&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;TO&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;take&#39;</span><span class="p">,</span> <span class="s1">&#39;VB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;DT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;new&#39;</span><span class="p">,</span> <span class="s1">&#39;JJ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;position&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;there&#39;</span><span class="p">,</span> <span class="s1">&#39;RB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;June&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;He&#39;</span><span class="p">,</span> <span class="s1">&#39;PRP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;will&#39;</span><span class="p">,</span> <span class="s1">&#39;MD&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;be&#39;</span><span class="p">,</span> <span class="s1">&#39;VB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;missed&#39;</span><span class="p">,</span> <span class="s1">&#39;VBN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;very&#39;</span><span class="p">,</span> <span class="s1">&#39;RB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;much&#39;</span><span class="p">,</span> <span class="s1">&#39;JJ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;PRP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;think&#39;</span><span class="p">,</span> <span class="s1">&#39;VBP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;finding&#39;</span><span class="p">,</span> <span class="s1">&#39;VBG&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;DT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;new&#39;</span><span class="p">,</span> <span class="s1">&#39;JJ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;doctor&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;NYC&#39;</span><span class="p">,</span> <span class="s1">&#39;NNP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;you&#39;</span><span class="p">,</span> <span class="s1">&#39;PRP&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;actually&#39;</span><span class="p">,</span> <span class="s1">&#39;RB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;like&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;might&#39;</span><span class="p">,</span> <span class="s1">&#39;MD&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;almost&#39;</span><span class="p">,</span> <span class="s1">&#39;RB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;be&#39;</span><span class="p">,</span> <span class="s1">&#39;VB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;as&#39;</span><span class="p">,</span> <span class="s1">&#39;RB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;awful&#39;</span><span class="p">,</span> <span class="s1">&#39;JJ&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;as&#39;</span><span class="p">,</span> <span class="s1">&#39;IN&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;trying&#39;</span><span class="p">,</span> <span class="s1">&#39;VBG&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;TO&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;find&#39;</span><span class="p">,</span> <span class="s1">&#39;VB&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;DT&#39;</span><span class="p">),</span> 
<span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">)]</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">([</span><span class="n">np</span> <span class="k">for</span> <span class="n">np</span> <span class="ow">in</span> <span class="n">blob_df</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">noun_phrases</span><span class="p">])</span> 
<span class="p">[</span><span class="s1">&#39;got&#39;</span><span class="p">,</span> <span class="s1">&#39;goldberg&#39;</span><span class="p">,</span> <span class="s1">&#39;arizona&#39;</span><span class="p">,</span> <span class="s1">&#39;new position&#39;</span><span class="p">,</span> <span class="s1">&#39;june&#39;</span><span class="p">,</span> <span class="s1">&#39;new doctor&#39;</span><span class="p">,</span> <span class="s1">&#39;nyc&#39;</span> <span class="n">复制ErrorOK</span><span class="err">!</span>
</code></pre></div><p>你可以看到每个库找到的名词短语有些不同。spacy 包含英语中的常见单词，如<code>&quot;a&quot;</code>和<code>&quot;the&quot;</code>，而 TextBlob 则删除这些单词。这反映了规则引擎的差异，它驱使每个库都认为是“名词短语”。 你也可以写你的词性关系来定义你正在寻找的块。使用 Python 进行自然语言处理可以深入了解从头开始用 Python 进行分块。</p>
<br>
<h2 id="总结httpfe4mlapachecnorgdocs3文本数据id总结"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E6%80%BB%E7%BB%93">总结</a></h2>
<p>词袋模型易于理解和计算，对分类和搜索任务很有用。但有时单个单词太简单，不足以将文本中的某些信息封装起来。为了解决这个问题，人们寄希望于比较长的序列。Bag-of-ngram 是 BOW 的自然概括，这个概念仍然容于理解，而且它的计算开销这就像 BOW 一样容易。</p>
<p>Bag of-ngram 生成更多不同的 ngram。它增加了特征存储成本，以及模型训练和预测阶段的计算成本。虽然数据点的数量保持不变，但特征空间的维度现在更大。因此数据密度更为稀疏。n 越高，存储和计算成本越高，数据越稀疏。由于这些原因，较长的 n-gram 并不总是会使模型精度的得到提高（或任何其他性能指标）。人们通常在<code>n = 2</code>或 3 时停止。较少的 n-gram 很少被使用。</p>
<p>防止稀疏性和成本增加的一种方法是过滤 n-gram 并保留最有意义的短语。这是搭配抽取的目标。理论上，搭配（或短语）可以在文本中形成非连续的标记序列。然而，在实践中，寻找非连续词组的计算成本要高得多并且没有太多的收益。因此搭配抽取通常从一个候选人名单中开始，并利用统计方法对他们进行过滤。</p>
<p>所有这些方法都将一系列文本标记转换为一组断开的计数。与一个序列相比，一个集合的结构要少得多；他们导致平面特征向量。</p>
<p>在本章中，我们用简单的语言描述文本特征化技术。这些技术将一段充满丰富语义结构的自然语言文本转化为一个简单的平面向量。我们讨论一些常用的过滤技术来降低向量维度。我们还引入了 ngram 和搭配抽取作为方法，在平面向量中添加更多的结构。下一章将详细介绍另一种常见的文本特征化技巧，称为 tf-idf。随后的章节将讨论更多方法将结构添加回平面向量。</p>
<br>
<h2 id="参考文献httpfe4mlapachecnorgdocs3文本数据id参考文献"><a href="http://fe4ml.apachecn.org/#/docs/3.%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE?id=%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">参考文献</a></h2>
<p>Dunning, Ted. 1993. “Accurate methods for the statistics of surprise and</p>
<p>coincidence.” ACM Journal of Computational Linguistics, special issue on using large corpora , 19:1 (61—74).</p>
<p>“Hypothesis Testing and p-Values.” Khan Academy, accessed May 31,</p>
<p>2016,https://www.khanacademy.org/math/probability/statistics-inferential/hypothesis-testing/v/hypothesis-testing-and-p-values.</p>
<p>Manning,Christopher D. and Hinrich Schütze. 1999. Foundations of StatisticalNatural Language Processing . Cambridge, Massachusettes: MIT Press.</p>
<p>Sometimes people call it the document “vector.” The vector extends from the original and ends at the specified point. For our purposes, “vector” and “point” are the same thing.</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 文本网络分析&amp;文化桥梁Python代码实现</title>
      <link>https://textdata.cn/blog/2021-12-28-pnas_culture_bridges/</link>
      <pubDate>Tue, 28 Dec 2021 09:43:10 +0600</pubDate>
      
      <guid>/blog/2021-12-28-pnas_culture_bridges/</guid>
      <description> PNAS2016这篇使用“自闭症谱系障碍ASD疾病的治病原因大讨论”做研究，文中使用TF-IDF刻画参与者信息的“新且熟悉” ,构建参与者文化网络。使用自动文本分析发现，如果组织方建立文化桥梁，在很少能一起讨论的议题领域内产生可连接的对话主题信息，这类信息不仅能引起多个受众的共鸣，而且还能让这些受众彼此进行对话，那么组织方更有可能激发新社交媒体受众的评论</description>
      <content:encoded><![CDATA[<h2 id="代码下载codezip"><a href="code.zip">代码下载</a></h2>
<p>现在一提到文本分析，除了词频统计、情感分析，就属话题分析最火，主流技术路线是使用LDA话题模型进行主题分析。但是LDA适合文档区分度大，文本档数较大。如果不满足这两点，LDA虽然能跑出模型，但是跑出的topic无法解读，没有意义。今天分享一个技术文，在看技术文之前，将技术文的背景文献稍微整理翻译了下，方便大家更好的理解textnets的应用场景。</p>
<p>网络分析通常用于描述人与人之间的关系——尤其是在社会科学中——但它也可以应用于词之间的关系。例如，网络关系可以通过文档中单个单词的共现来创建，或者可以使用双模式网络投影在文档之间创建关系。</p>
<p>基于网络的自动文本分析方法的优点是</p>
<ul>
<li>
<p>像社会群体一样，可以通过三元闭包更准确地测量词组的含义——或者任何两个词或术语相互的含义的原则如果将它们放在第三个词的上下文中，可以更准确地理解；</p>
</li>
<li>
<p><strong>文本网络可以应用于任何长度的文档</strong>，这与通常需要大量单词才能正常运行的主题模型不同。在简短的社交媒体文本变得普遍的时代，这是一个显着的优势。</p>
</li>
<li>
<p>最后，这种方法受益于<strong>社区检测</strong>跨学科文献的最新进展，可以说它提供了更准确的单词分组方法，这些方法受益于网络内观察到的聚类，而不是词袋模型。</p>
</li>
</ul>
<br>
<h2 id="背景-文化桥梁">背景-文化桥梁</h2>
<p>文化信息传递理论和公共审议和计算技术。</p>
<blockquote>
<p>Markowitz, D. M., &amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).</p>
</blockquote>
<p>由于每天光顾此类论坛的人数迅速增加，社交媒体为倡导组织塑造公共辩论提供了有力的机会。 然而，社会科学家还没有解释为什么一些<strong>议题发起者</strong>能成功发起大规模的广泛参与性(公开辩论/广泛对话)，而大多数其他组织却没做到。 本文使用自动文本分析发现，如果组织方建立<strong>文化桥梁</strong>，在很少能一起讨论的议题领域内产生可连接的对话主题信息，这类信息不仅能引起多个受众的共鸣，而且还能让这些受众彼此进行对话，那么组织方更有可能激发新社交媒体受众的评论。
在控制这些因素的情况下，建立实质性文化桥梁的组织， 其所发布信息， 比那些没有建立实质性文化桥梁的组织， 得到的评论数多 2.52 倍。</p>

<figure >
    
        <img src="img/large.jpg" />
    
    
</figure>

<p>社交网络分析通常用于描述个人之间的友谊或其他关系，但它也可通过参与者的消息或想法的类型来描述参与者之间的关系（如下图) 是“文化网络”中的一个小区域。</p>
<ul>
<li>每个节点描述一个参与议题公开对话的参与者</li>
<li>节点间的边代表那些在社交媒体倡导领域内讨论类似议题的人。</li>
</ul>
<p>PNAS2016这篇使用“<strong>自闭症谱系障碍ASD疾病的治病原因大讨论</strong>”做的数据分享，图中 t1 处的一类用户认为ASD致病可能跟疫苗有关，而另一类人可能认为ASD可能与遗传因素的有关。从图形看，t1这类议题发起方式，没有产生广泛参与性。而t2和t3，文化网络中因为文化桥梁的存在，产生了广泛参与性。</p>
<p><strong>假设的文化网络</strong>，其中节点代表参与有关议题的对话参与者，而节点之间的边则描述了其消息内容的相似性。议题广泛参与性，除了话题发起者影响力、话题投放资源等因素影响，还有一个因素就是发起的话题是否吸引了受众。对于参与者而言，最有吸引力的话题需要满足“<strong>新颖，且熟悉</strong>”。</p>
<p><strong>用TF-IDF刻画文化网络中的“新且熟悉”</strong>
在文本分析中有一个文本特征提取技术tf-idf</p>
<ul>
<li>tf指词语在某文档中出现的次数；从词语的角度，该值越大越熟悉</li>
<li>idf逆文档数，即词语出现在多少个文档中；从词语的角度，该值越小越新颖</li>
</ul>

<figure >
    
        <img src="img/large2.jpg" />
    
    
</figure>

<p>本教程将引导您完成使用文本网络分析和可视化数据所需的所有步骤。 在解决与使用文本网络相关的其他杂项问题之前，本教程首先介绍了一个独立的示例。</p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="n">textnets</span>
</code></pre></div><br>
<h2 id="1-查看数据">1. 查看数据</h2>
<p>pnas2016这篇的数据没有开源，通过文本构建文化网络、发现文化桥梁。这里使用一个特别特别小的新闻数据，关于人类第一次登月。如果我们使用<a href="https://github.com/jboynyc/textnets">textnets</a>，准备的数据需要有两个列</p>
<ul>
<li>议题参与者，类比报刊</li>
<li>议题参与者发布的内容，如评论等</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;test.csv&#39;</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">|    | Unnamed: 0        | headlines                                                                 |
|---:|:------------------|:--------------------------------------------------------------------------|
|  0 | The Guardian      | 3:56 am: Man Steps On to the Moon                                         |
|  1 | New York Times    | Men Walk on Moon -- Astronauts Land on Plain, Collect Rocks, Plant Flag   |
|  2 | Boston Globe      | Man Walks on Moon                                                         |
|  3 | Houston Chronicle | Armstrong and Aldrich &#34;Take One Small Step for Man&#34; on the Moon           |
|  4 | Washington Post   | The Eagle Has Landed -- Two Men Walk on the Moon                          |
|  5 | Chicago Tribune   | Giant Leap for Mankind -- Armstrong Takes 1st Step on Moon                |
|  6 | Los Angeles Times | Walk on Moon -- That\&#39;s One Small Step for Man, One Giant Leap for Mankind |
</code></pre></div><br>
<h2 id="2-导入corpus">2. 导入corpus</h2>
<p>使用textnets库的将数据导入为其特有的语料格式。从下方可以看到textnets可能会用spacy，如果要配置英文en_core_web_sm或中文zh_core_web_sm, 请查看该文 <a href="https://t.hk.uy/aCmr">https://t.hk.uy/aCmr</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">textnets</span> <span class="k">as</span> <span class="nn">tn</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="c1">#设置随机种子，保证代码可重复性</span>
<span class="n">tn</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&#34;seed&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">42</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">tn</span><span class="o">.</span><span class="n">Corpus</span><span class="o">.</span><span class="n">from_csv</span><span class="p">(</span><span class="s1">&#39;test.csv&#39;</span><span class="p">)</span>
<span class="n">corpus</span>
</code></pre></div>
<figure >
    
        <img src="img/corpus.png" width="100%" />
    
    
</figure>

<br>
<h2 id="3-构建网络">3. 构建网络</h2>
<p>需要注意的是corpus.tokenized()是textnets特有的分词方法，如果所处理的新闻是中文，需要提前分词去停用词整理为像英文数据格式，用空格间隔单词。</p>
<p>textnets提供了构建网络的方法</p>
<p>tn.Textnet(data, min_docs, connected, doc_attrs)</p>
<ul>
<li>data DataFrame类型, 三列，自己可以运行 corpus.tokenized() 查看样式</li>
<li>min_docs 一个词语存在于至少多少个文档中，默认为2。一个词至少出现在两个doc中，才会让两个doc产生连接</li>
<li>connected 仅保留网络的最大连接组件（默认值：False）</li>
<li>doc_attrs 文档节点的属性，字典的字典(双层嵌套字典)</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">t</span> <span class="o">=</span> <span class="n">tn</span><span class="o">.</span><span class="n">Textnet</span><span class="p">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">tokenized</span><span class="p">(),</span> <span class="n">min_docs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>使用所有默认参数， textnets 会帮我们删除英文停用词，词干化(合并同类词)，并删除标点符号、数字、URL 等。</p>
<p>但这里我们将破例将 min_docs 设置为1（因为数据只有几句话几十个单词，这里破例设置为1，正常这里至少是2）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">t</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>   <span class="c1">#标记节点名(单词、媒体)</span>
       <span class="n">show_clusters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1">#绘制簇的边界</span>
</code></pre></div>
<figure >
    
        <img src="img/output_10_0.svg" width="100%" />
    
    
</figure>

<p>show_clusters 使用 <strong>Leiden社区检测算法</strong>(Leiden community detection algorithm)找到了分区成簇，它似乎识别了<strong>同一主题</strong>(登月)下不同词之间的远近(相似的词在一个簇中，不同的词处于不同的簇中)。</p>
<p>你可能会疑惑：为什么网络图中的<strong>单词: moon</strong>会自己漂移？ 那是因为moon这个词在每个文档中只出现一次，所以每个文档moon的tf-idf得分为0。</p>
<p>让我们再次可视化相同的事情，但这次根据节点的 BiRank（二部网络的中心性度量）缩放节点，根据权重缩放边缘。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">t</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
       <span class="n">show_clusters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
       <span class="n">scale_nodes_by</span><span class="o">=</span><span class="s2">&#34;birank&#34;</span><span class="p">,</span>
       <span class="n">scale_edges_by</span><span class="o">=</span><span class="s2">&#34;weight&#34;</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_12_0.svg" width="100%" />
    
    
</figure>

<p>我们还可以只可视化报刊网络，不显示词语。这里设置node_type=&lsquo;doc&rsquo;</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#node_type有两种值， doc、term</span>
<span class="n">papers</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">project</span><span class="p">(</span><span class="n">node_type</span><span class="o">=</span><span class="s2">&#34;doc&#34;</span><span class="p">)</span>
<span class="n">papers</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_14_0.svg" width="100%" />
    
    
</figure>

<p>和之前的双向网络一样，我们可以看到Houston Chronicle、  Chicago Tribune、  Los Angeles Times更紧密地聚集在一起。</p>
<p>接下来，词网络：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">words</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">project</span><span class="p">(</span><span class="n">node_type</span><span class="o">=</span><span class="s2">&#34;term&#34;</span><span class="p">)</span>
<span class="n">words</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">show_clusters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_16_0.svg" width="100%" />
    
    
</figure>

<p>除了可视化之外，我们还可以使用<strong>社交网络指标</strong>分析我们的语料库。 例如，具有教高<strong>介数中心性betweenness centrality</strong>的文档可能将主题不同簇联系起来，起到文化桥梁的作用，从而刺激跨越符号鸿沟的交流(Bail,2016)。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">papers</span><span class="o">.</span><span class="n">top_betweenness</span><span class="p">()</span>
</code></pre></div><pre><code>Los Angeles Times    7.0
Boston Globe         0.0
Chicago Tribune      0.0
Houston Chronicle    0.0
New York Times       0.0
The Guardian         0.0
Washington Post      0.0
dtype: float64
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">words</span><span class="o">.</span><span class="n">top_betweenness</span><span class="p">()</span>
</code></pre></div><pre><code>walk         72.00
man          18.00
step         16.00
small        12.75
land          6.00
giant         6.00
leap          6.00
mankind       6.00
armstrong     3.25
plain         0.00
dtype: float64
</code></pre>
<p>这是因为New York Times在其标题中使用了“walk”一词，将“one small step”簇与“man on moon”簇联系起来。</p>
<p>我们可以再次生成词网络图，这次根据节点的中介中心性缩放节点，并使用“骨干提取”从网络中修剪边缘：cite:p<code>Serrano2009</code>。</p>
<p>我们还可以使用 color_clusters（而不是 show_clusters）根据节点的分区为节点着色。</p>
<p>我们可以过滤节点标签，只标记那些中间中心性betweenness centrality分数高于中位数的节点。 这在高阶网络中特别有用，其中标记每个节点会导致视觉混乱。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">words</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">scale_nodes_by</span><span class="o">=</span><span class="s2">&#34;betweenness&#34;</span><span class="p">,</span>
           <span class="n">color_clusters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
           <span class="n">edge_width</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="o">*</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="o">.</span><span class="n">edges</span><span class="p">[</span><span class="s2">&#34;weight&#34;</span><span class="p">]],</span>
           <span class="n">edge_opacity</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
           <span class="n">node_label_filter</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">n</span><span class="o">.</span><span class="n">betweenness</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">words</span><span class="o">.</span><span class="n">betweenness</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>
</code></pre></div>
<figure >
    
        <img src="img/output_21_0.svg" width="100%" />
    
    
</figure>

<h2 id="其他textnets案例资料">其他textnets案例资料</h2>
<p><a href="https://www.jboy.space/blog/enemies-foreign-and-partisan.html">https://www.jboy.space/blog/enemies-foreign-and-partisan.html</a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>神经网络textgenrnn库生成文本</title>
      <link>https://textdata.cn/blog/textgenrnn/</link>
      <pubDate>Tue, 28 Dec 2021 06:43:10 +0600</pubDate>
      
      <guid>/blog/textgenrnn/</guid>
      <description> 只需几行代码，即可在任何文本数据集上轻松训练您自己的任意大小和复杂性的文本生成神经网络。 </description>
      <content:encoded><![CDATA[<p>textgenrnn是在Keras/Tensorflow基础上搭建的Python包，特性:</p>
<ul>
<li>有现代的神经网络架构，使用注意力权重和嵌入Embedding来加速训练和提升模型质量</li>
<li>支持字符级别和单词级别的训练</li>
<li>可设置RNN尺寸、RNN层数、是否使用双向RNN</li>
<li>可支持对任意输入文本的训练，包括大文件</li>
<li>可以使用GPU训练，使用CPU生成文本</li>
<li>提供基于GPU的cuDNN，以加速模型训练</li>
<li>使用情景标签训练模型，更快的学习，产出更好的效果。</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="n">textgenrnn</span>
</code></pre></div><br>
<h2 id="快速上手">快速上手</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">textgenrnn</span> <span class="kn">import</span> <span class="n">textgenrnn</span>

<span class="n">textgen</span> <span class="o">=</span> <span class="n">textgenrnn</span><span class="p">()</span>
<span class="n">textgen</span><span class="o">.</span><span class="n">generate</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[Spoiler] Anyone else find this post and their person that was a little more than I really like the Star Wars in the fire or health and posting a personal house of the 2016 Letter for the game in a report of my backyard.
</code></pre></div><br>
<p>使用新文本训练新模型也很简单</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">textgen</span><span class="o">.</span><span class="n">train_from_file</span><span class="p">(</span><span class="s1">&#39;hacker_news_2000.txt&#39;</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">textgen</span><span class="o">.</span><span class="n">generate</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Project State Project Firefox
</code></pre></div><br>
<p>生成3个论文标题按照疯狂程度的增加顺序（temperature越高，生成算法偏离学习概率分布的程度越大)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">textgen.generate(3, temperature=1.0)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Why we got money “regular alter”

Urburg to Firefox acquires Nelf Multi Shamn

Kubernetes by Google’s Bern
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS|词汇熟悉度对线上参与和资金筹集的预测性效用</title>
      <link>https://textdata.cn/blog/2021-12-27-pnas_text_fluency/</link>
      <pubDate>Mon, 27 Dec 2021 10:43:10 +0600</pubDate>
      
      <guid>/blog/2021-12-27-pnas_text_fluency/</guid>
      <description>人们对简单和通用的语言的反应比复杂和技术语言更有利;本文提供了文本分析的新思路，使用LIWC测量术语流畅性、复杂词汇。</description>
      <content:encoded><![CDATA[<p>[<strong>论文下载The predictive utility of word familiarity for online engagements and funding.pdf</strong>](The predictive utility of word familiarity for online engagements and funding.pdf)</p>
<blockquote>
<p>Markowitz, D. M., &amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).</p>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>处理流畅性等元认知框架通常表明人们对简单和通用的语言的反应比复杂和技术语言更有利。与复杂的信息相比，人们更容易处理简单和非技术性的信息，因此会更多地与目标进行互动。在涵盖 12 个现场样本（总 n = 1,064,533）的两项研究中，我们通过展示人们在付出时间和注意力时更多地使用非技术语言（例如，简单的在线语言往往会获得更多社交信息）来建立并复制这种越简单越好的现象订婚）。然而，人们在捐款时会对复杂的语言做出反应（例如，慈善捐赠活动和赠款摘要中的复杂语言往往会收到更多的钱）。这一证据表明，人们根据时间或金钱目标以不同的方式使用复杂语言的启发式方法。这些结果强调语言是社会和心理过程的镜头，以及大规模测量文本模式的计算方法。</p>
<ul>
<li>processing fluency</li>
<li>field studies</li>
<li>automated text analysis</li>
<li>common words</li>
<li>jargon</li>
</ul>
<br>
## 研究背景-复杂词汇的负面效应
<p><strong>术语（jargon）</strong>，是复杂的、技术的、专业的语言，与日常语言相比，加工难度更大、更不流畅。许多关于<strong>加工流畅度</strong>（processing fluency）的研究都发现了使用术语的负面结果： 由于术语会给予人们不熟悉、加工困难的感觉，从而导致其较难理解。使用术语来描述手术过程的医生可能导致病人关于健康风险的错误估计；阅读了关于一项技术的复杂描述的人们（与阅读简单描述的人们相比）对该技术的理解更差并高估其风险。因此，不常用的、技术性的词汇通常不被看好，因为人们对其不熟悉而感觉较难加工，并给人们带来理解上的挑战。</p>
<p>然而，对于复杂词汇的影响，以往研究基本基于实验室结果，效应的强度、健壮性、对真实行为的预测性等仍不清楚。此外，以往大多数关于加工流畅度（processing fluency）的研究都依赖于人们的主观判断，即通过询问被试对于简单或复杂文本的感受来判断效应的大小。该研究则弥补了这两点不足，将加工流畅度操作性定义为词法流畅度（lexical fluency，即所用的词汇为通用词汇还是复杂词汇），并考察复杂词汇对人们在真实世界中行为的影响。</p>
<br>
<h2 id="工具性启发法">工具性启发法</h2>
<p>工具性启发法（instrumentality heuristic）认为，如果一个感觉很困难的经历是有助于达到特定目标的，人们会给予这个经历更高的评价。由此，如果工具性目标被激活，那么加工流畅性低的复杂文本，反而可能会被给予更高的评价。对此，该研究同时考察了复杂词汇对于线上参与度（社会参与度）和资金筹集的影响。</p>
<br>
<h2 id="研究假设">研究假设</h2>
<ul>
<li>假设一：没有工具性目标被激活时，人们更喜欢简单的语言，表现为更高的社会参与度</li>
<li>假设二：工具性目标被激活时，人们更喜欢复杂的语言，表现为更多的资金支持</li>
</ul>
<p>实验结果支持这两个假设：通用词汇与更多的线上支持（高社会参与度）相关，复杂词汇则与更多的资金支持相关。</p>
<br>
<h2 id="数据">数据</h2>
<h3 id="研究一的数据包括">研究一的数据包括：</h3>
<ol>
<li>
<p>来自左倾（纽约时报）、右倾（福克斯新闻）、中立（美联社）的新闻媒体的推特</p>
</li>
<li>
<p>随机选择来自上述三个组织的的记者/名人的个人推特</p>
</li>
<li>
<p>共和党政治家和特朗普手下的推特</p>
</li>
<li>
<p>Reddit文章标题</p>
</li>
<li>
<p>科学论文（来自PLoS One）的标题和、摘要</p>
</li>
<li>
<p>TED演讲标题、内容</p>
</li>
</ol>
<h3 id="研究二的数据包括">研究二的数据包括：</h3>
<ol>
<li>三个慈善平台</li>
</ol>
<p>  a) Kickstarter，主要是关于对创意项目的投资</p>
<p>  b) Indiegogo，主要是关于对创意项目和初创企业的投资</p>
<p>  c) GoFundMe，时要是关于生活事件的筹募（医疗、事故等）</p>
<ol start="2">
<li>NIH基金申请书的摘要</li>
</ol>
<br>
<h2 id="数据分析">数据分析</h2>
<p>自动文本分析工具：研究使用自动文本分析工具LIWC（Linguistic Inquiry and Word Count）来对文本进行分析。LIWC词典是一个经过专家和统计分析认证的工具，其包含了6400个代表“非正式、非专业”的英语单词。研究者把通用词汇的比例操作性定义为文本中LIWC词典中词汇的比例。</p>
<p>混合效应回归分析：使用混合效应回归分析的方法对数据进行分析。其中，回归模型中的控制变量主要有5类，分别是信息源（如新闻来源、演讲者、作者），时间（如年份、视频长度、发帖距今时间、发表时间），主题（如社会/政治等），金钱（如申请成功与否、货币类型）和投入程度（如出资人的数量、股份的数量）。</p>
<p>数据转换：</p>
<ol>
<li>
<p>研究一中，由于发表时间更长的信息更可能有更高的线上参与度，因此计算中所有参与度指标均除以了数据提取日期与发表日期之间的时间距离（数据提取-发表日期）。此外，对于考察的社会参与度指标，均进行了log转换。下文（表XX）中的点赞率、转发率等，均指代经过了上述转换后的点赞数、转发数等。</p>
</li>
<li>
<p>对研究一参与度相关指标求和时（如推特点赞率与转发率之和），对各指标标准化后再求和。</p>
</li>
<li>
<p>研究二中的因变量（各数据集中的所得资金数额）亦均进行了log转换。</p>
</li>
</ol>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>共词矩阵 | cntext更新至1.1</title>
      <link>https://textdata.cn/blog/cntext_upgrade/</link>
      <pubDate>Wed, 22 Dec 2021 13:43:10 +0600</pubDate>
      
      <guid>/blog/cntext_upgrade/</guid>
      <description>共现矩阵 </description>
      <content:encoded><![CDATA[<h2 id="cntext更新至11-版本">cntext更新至1.1 版本</h2>
<p>本次更新了共现矩阵的计算函数。</p>
<p>更新方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext --upgrade
</code></pre></div><p>或指定版本安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext==1.1
</code></pre></div><br>
<h2 id="co_occurrence_matrix">co_occurrence_matrix</h2>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.dictionary</span> <span class="kn">import</span> <span class="n">co_occurrence_matrix</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/co_occurrence1.png" width="100%" />
    
    
</figure>

<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/co_occurrence2.png" width="100%" />
    
    
</figure>

<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Phonemizer音素化 Python文本语音表征包</title>
      <link>https://textdata.cn/blog/phonemizer/</link>
      <pubDate>Mon, 20 Dec 2021 23:43:10 +0600</pubDate>
      
      <guid>/blog/phonemizer/</guid>
      <description> 音素是构成语音的基本声音，音节和单词建立在音节上。在与语音和语言处理相关的各种应用（例如文本到语音系统）中，将文本从其拼写形式转录为语音字母表是一项重要要求。 </description>
      <content:encoded><![CDATA[<h2 id="音素">音素</h2>
<p><strong>音素</strong>是构成语音的基本声音，音节和单词建立在音节上。在与<strong>语音</strong>和<strong>语言</strong>处理相关的各种应用（例如<strong>文本到语音系统</strong>）中，将文本从其拼写形式转录为语音字母表是一项重要要求。</p>
<p>Phonemizer 是一个精确寻址的 Python 包, 它将文本从其拼写表示转录为语音表示。 该包设计用户友好的，并公开了一个高级音素化函数， 支持大约100种不同的语言。phonemizer 使用的默认后端是 eSpeak
（Dunn &amp; Vitolins，2019 年），一种基于语言专业知识和手写转录规则的文本转语音软件。它将文本转录成国际音标，并支持一百多种语言。使用 MBROLA 声音（Tits &amp; Vitolins，2019），eSpeak 后端可用于大约 35 种语言，以 SAMPA 计算机可读语音字母表转录文本。</p>
<br>
<h2 id="安装">安装</h2>
<p>安装phonemizer前需要配置espeak-ng，</p>
<ul>
<li>win <a href="https://github.com/espeak-ng/espeak-ng/releases">https://github.com/espeak-ng/espeak-ng/releases</a>下载对应的msi文件点击安装</li>
<li>mac 首先配置好homebrew，之后命令行brew install espeak</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install phonemizer
</code></pre></div><h2 id="音素化phonemize">音素化phonemize</h2>
<p>from phonemizer import phonemize</p>
<p>phonemize(text, language=&lsquo;en-us&rsquo;, prepend_text=False, preserve_punctuation=False, with_stress=False, njobs=1)</p>
<ul>
<li>text 文本列表</li>
<li>language 语言。&ldquo;en-us&quot;美国英语， &ldquo;zh&quot;中文</li>
<li>prepend_text 输出结果保留输入的文本，默认False</li>
<li>preserve_punctuation 输出结果保留标点符号，默认False</li>
<li>with_stress 标记重读，默认False</li>
<li>njobs 并行运算核数，默认使用cpu的1个核。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">phonemizer</span> <span class="kn">import</span> <span class="n">phonemize</span>

<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;hello, my name is david&#39;</span><span class="p">,</span> 
        <span class="s1">&#39;nice to meet you!&#39;</span><span class="p">]</span>

<span class="c1"># Do this:</span>
<span class="n">phonemized</span> <span class="o">=</span>  <span class="n">phonemize</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="s1">&#39;en-us&#39;</span><span class="p">)</span>
<span class="n">phonemized</span>
</code></pre></div><p>Run</p>
<pre><code>['həloʊ maɪ neɪm ɪz deɪvɪd ', 'naɪs tə miːt juː ']
</code></pre>
<br>
<p>但上面的用法速度较慢， 更高效的写法应该为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">phonemizer.backend</span> <span class="kn">import</span> <span class="n">EspeakBackend</span>
<span class="n">backend</span> <span class="o">=</span> <span class="n">EspeakBackend</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="s1">&#39;en-us&#39;</span><span class="p">)</span>

<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;hello, my name is david&#39;</span><span class="p">,</span> 
         <span class="s1">&#39;nice to meet you!&#39;</span><span class="p">]</span>

<span class="n">phonemized</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">phonemize</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span> 
<span class="n">phonemized</span>
</code></pre></div><p>Run</p>
<pre><code>142 µs ± 851 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)
</code></pre>
<p>给每个单词构造音素，输出结果为字典样式</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">phonemizer.backend</span> <span class="kn">import</span> <span class="n">EspeakBackend</span>
<span class="kn">from</span> <span class="nn">phonemizer.punctuation</span> <span class="kn">import</span> <span class="n">Punctuation</span>
<span class="kn">from</span> <span class="nn">phonemizer.separator</span> <span class="kn">import</span> <span class="n">Separator</span>
<span class="n">backend</span> <span class="o">=</span> <span class="n">EspeakBackend</span><span class="p">(</span><span class="s1">&#39;en-us&#39;</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;It amused him to think that they were probably talking about him at that very moment&#39;</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)]</span>

<span class="c1"># 忽略词语边界，音素之间用空格间隔</span>
<span class="n">separator</span> <span class="o">=</span> <span class="n">Separator</span><span class="p">(</span><span class="n">phone</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">word</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">lexicon</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">backend</span><span class="o">.</span><span class="n">phonemize</span><span class="p">([</span><span class="n">word</span><span class="p">],</span> <span class="n">separator</span><span class="o">=</span><span class="n">separator</span><span class="p">,</span> <span class="n">strip</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
           <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">}</span>

<span class="n">lexicon</span>
</code></pre></div><p>Run</p>
<pre><code>{'it': 'ɪ t',
 'amused': 'ɐ m j uː s d',
 'him': 'h ɪ m',
 'to': 't uː',
 'think': 'θ ɪ ŋ k',
 'that': 'ð æ t',
 'they': 'ð eɪ',
 'were': 'w ɜː',
 'probably': 'p ɹ ɑː b ə b l i',
 'talking': 't ɔː k ɪ ŋ',
 'about': 'ɐ b aʊ t',
 'at': 'æ t',
 'very': 'v ɛ ɹ i',
 'moment': 'm oʊ m ə n t'}
</code></pre>
<br>
<h2 id="中文的音素化">中文的音素化</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text_zhs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;你好我的名字是大卫&#39;</span><span class="p">,</span> 
            <span class="s1">&#39;很高兴认识你&#39;</span><span class="p">]</span>

<span class="n">phonemized_zhs</span> <span class="o">=</span> <span class="n">phonemize</span><span class="p">(</span><span class="n">text_zhs</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="s1">&#39;zh&#39;</span><span class="p">)</span>
<span class="n">phonemized_zhs</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;ni2 xɑu2 wo2 tə1 miɜŋ tsi̪5 s.i.5 tɑ5 wei5 &#39;,
 &#39;xə2n kɑu5 ɕi5ŋ ʐə5n s.i.1 ni2 &#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">phonemizer.backend</span> <span class="kn">import</span> <span class="n">EspeakBackend</span>
<span class="kn">from</span> <span class="nn">phonemizer.punctuation</span> <span class="kn">import</span> <span class="n">Punctuation</span>
<span class="kn">from</span> <span class="nn">phonemizer.separator</span> <span class="kn">import</span> <span class="n">Separator</span>
<span class="kn">import</span> <span class="nn">re</span>


<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;想到他们可能在那个时候谈论他，他觉得好笑&#39;</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>


<span class="n">backend</span> <span class="o">=</span> <span class="n">EspeakBackend</span><span class="p">(</span><span class="s1">&#39;zh&#39;</span><span class="p">)</span>

<span class="n">separator</span> <span class="o">=</span> <span class="n">Separator</span><span class="p">(</span><span class="n">phone</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">word</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># 构建每个汉字一个对应的音素表达，输出结果为字典样式</span>
<span class="n">lexicon</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">backend</span><span class="o">.</span><span class="n">phonemize</span><span class="p">([</span><span class="n">word</span><span class="p">],</span> <span class="n">separator</span><span class="o">=</span><span class="n">separator</span><span class="p">,</span> <span class="n">strip</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
           <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">}</span>

<span class="n">lexicon</span>
</code></pre></div><p>Run</p>
<pre><code>{'想': 'ɕ iɑ2 ŋ ',
 '到': 't ɑu5 ',
 '他': 'th ɑ5 ',
 '们': 'm ə1 n ',
 '可': 'kh o2 ',
 '能': 'n əɜ ŋ ',
 '在': 'ts ai5 ',
 '那': 'n ɑ5 ',
 '个': 'k o1 ',
 '时': 's. i.ɜ ',
 '候': 'x ou5 ',
 '谈': 'th aɜ n ',
 '论': 'l uə5 n ',
 '觉': 'tɕ yɛɜ ',
 '得': 't ə1 ',
 '好': 'x ɑu2 ',
 '笑': 'ɕ j ɑu5 '}
</code></pre>
<br>
<h2 id="心理学相关概念">心理学相关概念</h2>
<p>这个包用起来比较简单，但是想到一个场景，说不定可以多个角度去分析文本。</p>
<p>经常看文本的时候，脑海里不自觉的读出声音，这种现象今天查了下叫做“听觉表象”，听觉表象产生于语言的视觉区和语言的运动区——角回和布洛卡区。通过听觉，听出相应的字的声音，我们就可以领会到这句话的意思。从常人的生长发育的过程，我们都是先牙牙学语，通过声音理解内容的含义，而后经过十数教育学会语言书面文字，掌握文字系统，此时我们的神经回路是“视觉(听觉)~记忆~理解”</p>
<p>对一个东西的“知觉”在心理学上叫做这个东西的“知觉表征”，相当于是把这个东西转码为了一个，用于后续在心理上对这个东西进行加工。这个心理符号的编码与这个东西本身的特征以及最初的感觉通道有关——一个真正的苹果常会被以视觉的方式编码，成为一个视觉知觉表征，可以简单理解成这个苹果的图像；一个词“苹果”常会被以听觉的方式编码，成为一个AVL单元，可以简单地理解成把“苹果”这两个字和“ping’guo”这个读音打包在一起的一个文件。</p>
<p>感觉代码被经过某些处理后储存在记忆当中，当有一天需要用的时候再被从记忆里提取出来。这个时候的提取，本身是一种建构，也就是与将感觉处理后存储起来的一个相反的过程——所以心理学上把我们所提取（建构）的这个代码就称为表象。表象和知觉是机能等价的（Neisser，1972），可以简单理解为表象就是是一种基于过去经验的知觉。这也解释了我们默读词汇的时候，这个AVL单元里“语音”的部分是哪里来的了——这是我们基于对自己声音的了解而建构的一种听觉表象。简单来说这个语音就是根据我们对自己声音的认识，来“想象”的读出来的声音。</p>
<blockquote>
<p>知乎回答-心理学哈士奇</p>
<p>Conrad R (1963). Acoustic confusions and memory span for words. Nature, 197: 1029-1030.</p>
<p>Neisser U (1972). Changing conception of imagery. In P W Sheehan (ED), The Function and Nature of Imagery. London: Achademic Press.
知乎回答-心理学哈士奇</p>
</blockquote>
<br>
<h2 id="引用格式">引用格式</h2>
<p>Bernard, M. and Titeux, H. (2021). Phonemizer: Text to phones transcription for multiple languages in python. Journal of Open Source Software, 6(68):3958.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-tex" data-lang="tex">@article<span class="nb">{</span>Bernard2021,
  doi = <span class="nb">{</span>10.21105/joss.03958<span class="nb">}</span>,
  url = <span class="nb">{</span>https://doi.org/10.21105/joss.03958<span class="nb">}</span>,
  year = <span class="nb">{</span>2021<span class="nb">}</span>,
  publisher = <span class="nb">{</span>The Open Journal<span class="nb">}</span>,
  volume = <span class="nb">{</span>6<span class="nb">}</span>,
  number = <span class="nb">{</span>68<span class="nb">}</span>,
  pages = <span class="nb">{</span>3958<span class="nb">}</span>,
  author = <span class="nb">{</span>Mathieu Bernard and Hadrien Titeux<span class="nb">}</span>,
  title = <span class="nb">{</span>Phonemizer: Text to Phones Transcription for Multiple Languages in Python<span class="nb">}</span>,
  journal = <span class="nb">{</span>Journal of Open Source Software<span class="nb">}</span>
<span class="nb">}</span>
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 历史语言记录揭示了近几十年来认知扭曲的激增</title>
      <link>https://textdata.cn/blog/2021-12-19-pnas_historical_language/</link>
      <pubDate>Sun, 19 Dec 2021 20:43:10 +0600</pubDate>
      
      <guid>/blog/2021-12-19-pnas_historical_language/</guid>
      <description> 患有抑郁症的人容易出现适应不良的思维模式，即认知扭曲，他们以过于消极和不准确的方式思考自己、世界和未来。 这些扭曲与个人情绪、行为和语言的显着变化有关。 **我们假设社会可以经历类似的集体心理变化，这些变化会反映在语言使用的历史记录中**。我们调查了过去 **125 年超 1400 万本书中认知扭曲（congnition disorder）**的文本标记的流行情况，并观察到自 1980 年代以来它们的流行程度激增，达到超过大萧条和两次世界大战的水平。 **这种模式似乎不是由词义、出版和写作标准或 Google 图书样本的变化驱动的**。 我们的研究结果发现，通过语言分析**最近的社会转向与认知扭曲和内化障碍相关**。</description>
      <content:encoded><![CDATA[<blockquote>
<p>Bollen, Johan, et al. &ldquo;Historical language records reveal a surge of cognitive distortions in recent decades.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 118.30 (2021).</p>
</blockquote>
<br>
<h2 id="摘要">摘要</h2>
<p>患有抑郁症的人容易出现适应不良的思维模式，即认知扭曲，他们以过于消极和不准确的方式思考自己、世界和未来。 这些扭曲与个人情绪、行为和语言的显着变化有关。 <strong>我们假设社会可以经历类似的集体心理变化，这些变化会反映在语言使用的历史记录中</strong>。我们调查了过去 <strong>125 年超 1400 万本书中认知扭曲（congnition disorder）<strong>的文本标记的流行情况，并观察到自 1980 年代以来它们的流行程度激增，达到超过大萧条和两次世界大战的水平。 <strong>这种模式似乎不是由词义、出版和写作标准或 Google 图书样本的变化驱动的</strong>。 我们的研究结果发现，通过语言分析</strong>最近的社会转向与认知扭曲和内化障碍相关</strong>。</p>
<br>
<h2 id="正文">正文</h2>
<p>抑郁症与独特且可识别的适应不良思维模式有关，称为<strong>认知扭曲</strong>，其中个人以不准确和过于消极的方式思考自己、未来和世界 (9-12)。例如，当个体用消极的、绝对主义的术语（例如，“I am a loser”）给自己贴上标签时，就会出现在抑郁症中看到的认知扭曲。他们可能会用二分法、极端的术语谈论未来事件（例如，“My meeting will be a complete disaster”）或对别人的心态做出毫无根据的假设（例如，“Everybody will think that I am a failure”）。</p>
<p>认知扭曲的类型通常区分许多部分重叠的类型，例如“灾难化”、“二分推理”、“否定积极的”、“情感推理”、“算命”、“标记和错误标记”、“放大和最小化”、“心理过滤”、“读心术”、“过度概括”、“个性化”和“应该陈述”。</p>
<p>**认知行为疗法 (cognitive-behavioral therapy，CBT) ** 是治疗抑郁症和其他内化障碍的黄金标准 (13)，其基础理论认为认知扭曲与内化障碍有关； 它们反映了环境压力下的负面情感和回避行为模式 (14, 15)。 <strong>语言与这种动态密切相关。 事实上，最近的研究表明，患有内化障碍的个体在他们的语言中表现出明显更高水平的认知扭曲 (16, 17)，以至于他们的患病率可能被用作抑郁症易感性的指标 (18, 19)。</strong></p>
<p>我们分析了过去 125 年中大量以英语、西班牙语和德语出版的超过 1400 万本书籍（谷歌图书）中的大量认知扭曲标记的流行情况。具体来说，我们正在研究由 CBT 专家、计算语言学家和双语母语人士组成的团队设计的数百个 1 到 5 个单词 (n-gram)、 标记的认知失真图式 (cognitive distortion schemata,CDS) 的纵向流行情况，以及由 CBT 专家小组外部验证，以捕捉 12 种认知扭曲的表达 (9)。 <strong>CDS n-gram</strong> 被设计为简短、明确和独立的语句，使用频率很高的术语表达特定认知扭曲类型的核心（图 1 和 SI 附录，表 S1-S3）。例如，3-gram 的“I am a”捕获了标签和错误标签失真，而不管其上下文或所涉及的精确标签（“女士”、“尊贵的人”、“失败者”等）。这些相同的 n-gram 在早期的研究中被证明显着更多。</p>
<br>
<h2 id="cds流行度测量">CDS流行度测量</h2>
<p><img loading="lazy" src="img/CDS_n-gram.png" alt=""  />
</p>
<p><strong>CDS n-gram</strong> 显示在灰色框内的示例，周围是合理的上下文词，这些词可能会有所不同，而不会影响 n-gram 是否标记给定类型的认知扭曲的表达（例如，<strong>读心术Mindreading、情感推理Emotiona lReasoning、标记Labeling和错误标记Mislabeling</strong>） . CDS 是由 CBT 专家、语言学家和母语使用者组成的团队设计的，用于捕捉特定认知扭曲类型的表达，而不管其特定的词汇上下文。 对于英语（美国）、西班牙语和德语，专家团队分别定义了 241、435 和 296 个 n-gram 来标记 12 种常见的认知扭曲类型。 请注意，我们的<strong>流行度测量只计算 CDS n-gram 的出现，而不管上下文（“每个人都在思考”、“仍然感觉”和“我是一个”）</strong>。 按失真类型提供的所有 CDS n-gram 的完整列表在SI Appendix, Tables S1–S3.</p>
<p><img loading="lazy" src="img/fig2.png" alt=""  />
</p>
<p>(A-C) 美国英语 (A)、西班牙语 (B) 和德语 (C) 从 1855 年到 2020 年 (125 y) 的 CDS n-gram 流行时间序列的中值 z 分数，其中添加了年份标记 对于重大历史事件。 在 20 世纪的大部分时间里，所有时间序列都显示出稳定或下降的水平，随后在过去的 30 年里认知扭曲急剧增加。</p>
<p>美国英语从 1899 年到 1978 年呈下降趋势，在 1914 年和 1940 年（第一次世界大战和第二次世界大战）以及特别是 1968 年出现小高峰。随后是 CDS 流行率从 1978 年开始激增，并持续到 2019 年。</p>
<p>对于西班牙语 我们发现从 1895 年到 1980 年代初期的稳定水平，在这一点上出现了一个趋势，即 CDS 患病率水平高于之前观察到的任何水平。</p>
<p>德国表现出稳定的 CDS 流行水平，除了第一次世界大战和第二次世界大战前后和之后的强劲高峰，直到 2007 年突然激增。</p>
<br>
<h2 id="data">Data</h2>
<p>研究数据谷歌已经开源，开源下载哦</p>
<p><a href="https://storage.googleapis.com/books/ngrams/books/datasetsv3.html">https://storage.googleapis.com/books/ngrams/books/datasetsv3.html</a></p>
<p><img loading="lazy" src="img/googlebooks.png" alt=""  />
</p>
<br>
<h2 id="cds-ngram词表">CDS ngram词表</h2>
<p>该论文CDS ngram词表</p>
<p><img loading="lazy" src="img/cdsngramlist.png" alt=""  />
</p>
<br>
<h2 id="代码">代码</h2>
<p>ngram代码实现</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">ngrams</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&#34;Historical language records reveal a surge of cognitive distortions in recent decades&#34;</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;分词结果: &#39;</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
<span class="c1">#2-gram</span>
<span class="n">two_grams</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tw</span><span class="p">)</span> <span class="k">for</span> <span class="n">tw</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;2-gram处理结果: &#39;</span><span class="p">,</span> <span class="n">two_grams</span><span class="p">)</span>
<span class="c1">#3-gram</span>
<span class="n">three_grams</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tw</span><span class="p">)</span> <span class="k">for</span> <span class="n">tw</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;3-gram处理结果: &#39;</span><span class="p">,</span> <span class="n">three_grams</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">分词结果:  [&#39;Historical&#39;, &#39;language&#39;, &#39;records&#39;, &#39;reveal&#39;, &#39;a&#39;, &#39;surge&#39;, &#39;of&#39;, &#39;cognitive&#39;, &#39;distortions&#39;, &#39;in&#39;, &#39;recent&#39;, &#39;decades&#39;]

2-gram处理结果:  [&#39;Historical language&#39;, &#39;language records&#39;, &#39;records reveal&#39;, &#39;reveal a&#39;, &#39;a surge&#39;, &#39;surge of&#39;, &#39;of cognitive&#39;, &#39;cognitive distortions&#39;, &#39;distortions in&#39;, &#39;in recent&#39;, &#39;recent decades&#39;]

3-gram处理结果:  [&#39;Historical language records&#39;, &#39;language records reveal&#39;, &#39;records reveal a&#39;, &#39;reveal a surge&#39;, &#39;a surge of&#39;, &#39;surge of cognitive&#39;, &#39;of cognitive distortions&#39;, &#39;cognitive distortions in&#39;, &#39;distortions in recent&#39;, &#39;in recent decades&#39;]

</code></pre></div><p>统计统计CDS-ngram与ngram频数，进而计算出CDS流行度。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Top2Vec|主题建模和语义搜索库</title>
      <link>https://textdata.cn/blog/top2vec_tutorial/</link>
      <pubDate>Mon, 13 Dec 2021 10:43:10 +0600</pubDate>
      
      <guid>/blog/top2vec_tutorial/</guid>
      <description>Python主题建模和语义搜索库</description>
      <content:encoded><![CDATA[<p>Top2Vec 是一种用于主题建模和语义搜索的算法。**我个人从理解代码和使用代码难度来看， 对于Python小白，BERTopic更适合直接用预训练词向量，而Top2Vec更适合对小规模数据训练词向量后做主题建模。**它自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。训练 Top2Vec 模型后，您可以：</p>
<ul>
<li>获取检测到的主题数。</li>
<li>获取话题。</li>
<li>获取主题大小。</li>
<li>获取分层主题。</li>
<li>按关键字搜索主题。</li>
<li>按主题搜索文档。</li>
<li>按关键字搜索文档。</li>
<li>找出相似的词。</li>
<li>查找类似的文档。</li>
<li>使用 RESTful-Top2Vec 公开模型</li>
<li>有关其工作原理的更多详细信息，请参阅论文。</li>
</ul>
<p><strong>亮点</strong></p>
<ul>
<li>自动查找主题数。</li>
<li>不需要停用词列表。</li>
<li>不需要词干/词形还原。</li>
<li>适用于短文本。</li>
<li>创建联合嵌入的主题、文档和词向量。</li>
<li>内置搜索功能。</li>
</ul>
<p><strong>它是如何工作的？</strong></p>
<p>该算法做出的假设是，许多语义相似的文档都表明了一个潜在的主题。</p>
<p>第一步是创建文档和词向量的联合嵌入。一旦文档和单词被嵌入到一个向量空间中，算法的目标就是找到密集的文档集群，然后确定哪些单词将这些文档吸引到一起。每个密集区域是一个主题，将文档吸引到密集区域的词就是主题词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">top2vec</span><span class="o">==</span><span class="mf">1.0.27</span>
</code></pre></div><h2 id="0-代码下载">0. 代码下载</h2>
<p><a href="top2vec_tutorial.zip">click to download code</a></p>
<p><br><br></p>
<h2 id="1-导入数据">1. 导入数据</h2>
<p>使用某灾难数据集，这里是存在标注的标签，但是我们假设不用label的，仅作为评判Top2vec运行效果的标准。<a href="cnews.csv">点击cnews.csv下载</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">top2vec</span> <span class="kn">import</span> <span class="n">Top2Vec</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;STOPWORDS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;STOPWORDS&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;cnews.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>时政    120
科技    106
时尚    106
财经    105
家居    103
教育     97
娱乐     96
体育     95
房产     87
游戏     85
Name: label, dtype: int64
</code></pre>
<p><br><br></p>
<h2 id="2-清洗数据">2. 清洗数据</h2>
<p>一般而言，作中文文本分析，需要把中文分词构造成类西方语言(空格间隔词语的文本)风格。在此期间，顺便将停用词剔除。其实在用top2vec时，不剔除停用词影响也不大。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleantext&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="3-训练模型">3. 训练模型</h2>
<p>Top2vec有一下四个常用参数</p>
<p><strong>Top2vec(documents, min_count, speed, workers)</strong></p>
<ul>
<li>documents: 文档列表</li>
<li>min_count: 词语最少出现次数。低于min_count的词不加入模型中</li>
<li>speed: 训练速度，参数默认&quot;learn&quot;
<ul>
<li>&ldquo;fast-learn&rdquo;  速度最快，训练效果最差</li>
<li>&ldquo;learn&rdquo;       速度，训练效果中等</li>
<li>&ldquo;deep-learn&rdquo;  速度最慢，训练效果最佳</li>
</ul>
</li>
<li>workers: 并行运行数，该值最大取值为电脑CPU的核数。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="n">Top2Vec</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleantext&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">(),</span> 
                <span class="n">min_count</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">speed</span><span class="o">=</span><span class="s2">&#34;deep-learn&#34;</span><span class="p">,</span>  
                <span class="n">workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>2021-12-14 20:21:10,318 - top2vec - INFO - Pre-processing documents for training
2021-12-14 20:21:10,871 - top2vec - INFO - Creating joint document/word embedding
2021-12-14 20:25:06,082 - top2vec - INFO - Creating lower dimension embedding of documents
2021-12-14 20:25:14,645 - top2vec - INFO - Finding dense areas of documents
2021-12-14 20:25:14,683 - top2vec - INFO - Finding topics
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 话题个数</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_num_topics</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>9
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 各话题数量</span>
<span class="n">topic_sizes</span><span class="p">,</span> <span class="n">topic_nums</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_sizes</span><span class="p">()</span>

<span class="p">{</span><span class="s2">&#34;topic_sizes&#34;</span><span class="p">:</span><span class="n">topic_sizes</span><span class="p">,</span> 
 <span class="s2">&#34;topic_ids&#34;</span><span class="p">:</span><span class="n">topic_nums</span><span class="p">}</span>
</code></pre></div><p>Run</p>
<pre><code>{'topic_sizes': array([361, 116, 107,  99,  97,  93,  82,  25,  20]),
 'topic_ids': array([0, 1, 2, 3, 4, 5, 6, 7, 8])}
</code></pre>
<p><br><br></p>
<h2 id="4-get_topics">4. get_topics</h2>
<p>用pyecharts词云图显示<strong>话题信息</strong>， 为了简化代码，将该功能封装为函数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gen_wordcloud</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    topic_words: 主题词列表
</span><span class="s2">    word_scores: 主题特征词的权重得分(词语表征主题的能力)
</span><span class="s2">    topic_id: 主题id
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
    <span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
    <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
    
    <span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">)]</span>

    <span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">88</span><span class="p">])</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
        <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;Topic_</span><span class="si">{topic_id}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">),</span> 
                                  <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">23</span><span class="p">)),</span>
        <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="n">display</span><span class="p">(</span><span class="n">wc</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">())</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topics</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>

<span class="k">for</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">):</span>
    <span class="n">gen_wordcloud</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/vis1.png" alt=""  />

<img loading="lazy" src="img/vis2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="5-get_documents_topics">5. get_documents_topics</h2>
<p>get_documents_topics(doc_ids, num_topics=1)</p>
<ul>
<li>doc_ids: 待查询文档id列表</li>
<li>num_topics: 返回某文档可能归属话题的个数</li>
</ul>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查第一条文档的</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_documents_topics</span><span class="p">(</span><span class="n">doc_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(array([0]),
 array([0.1384481], dtype=float32),
 array([[&#39;政府&#39;, &#39;经济&#39;, &#39;政策&#39;, &#39;建设&#39;, &#39;中方&#39;, &#39;发展&#39;, &#39;促进&#39;, &#39;部门&#39;, &#39;留学&#39;, &#39;学生&#39;, &#39;会议&#39;,
         &#39;我要&#39;, &#39;事务&#39;, &#39;日电&#39;, &#39;房价&#39;, &#39;教育&#39;, &#39;国务院&#39;, &#39;温家宝&#39;, &#39;留学生&#39;, &#39;人数&#39;, &#39;移民&#39;,
         &#39;会见&#39;, &#39;推动&#39;, &#39;申请者&#39;, &#39;申请&#39;, &#39;官员&#39;, &#39;住房&#39;, &#39;房屋&#39;, &#39;加强&#39;, &#39;中国政府&#39;, &#39;购房&#39;,
         &#39;国家&#39;, &#39;支付&#39;, &#39;楼市&#39;, &#39;外交部&#39;, &#39;接收&#39;, &#39;两国&#39;, &#39;原则&#39;, &#39;各地&#39;, &#39;总理&#39;, &#39;战略&#39;,
         &#39;和平&#39;, &#39;框架&#39;, &#39;评论&#39;, &#39;有序&#39;, &#39;装修&#39;, &#39;中国&#39;, &#39;就业&#39;, &#39;友好&#39;, &#39;人力资源&#39;]],
       dtype=&#39;&lt;U9&#39;),
 array([[0.3623712 , 0.36037514, 0.35219163, 0.35109183, 0.3499857 ,
         0.34666985, 0.3426961 , 0.34161803, 0.34010434, 0.3382269 ,
         0.33710504, 0.336056  , 0.33598724, 0.33488944, 0.3303768 ,
         0.32483265, 0.324798  , 0.32201332, 0.3174801 , 0.3153757 ,
         0.3152491 , 0.31338856, 0.31334093, 0.31244045, 0.31202242,
         0.30908576, 0.3086405 , 0.30838227, 0.30605763, 0.3053521 ,
         0.30474398, 0.30268514, 0.30253592, 0.30242488, 0.30227807,
         0.3017046 , 0.30116442, 0.30062813, 0.2996228 , 0.29806197,
         0.2972776 , 0.29709277, 0.29706252, 0.29584888, 0.29578486,
         0.29524648, 0.2944737 , 0.2939484 , 0.29286712, 0.29246706]],
       dtype=float32))
</code></pre></div><p><br><br></p>
<h2 id="6-search_topics">6. search_topics</h2>
<p>根据关键词搜索话题，查某词是否属于某话题，属于该主题的概率
search_topics(keywords, num_topics, keywords_neg=None)</p>
<ul>
<li>keywords: 关键词列表</li>
<li>num_topics: 返回话题个数，按照语义相似度从高到低排序</li>
<li>keywords_neg: 反义词列表</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gen_wordcloud2</span><span class="p">(</span><span class="n">query_word</span><span class="p">,</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">,</span> <span class="n">topic_probability</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    query_word: 待查询词
</span><span class="s2">    topic_words: 主题词列表
</span><span class="s2">    word_scores: 主题特征词的权重得分(词语表征主题的能力)
</span><span class="s2">    topic_id: 主题id
</span><span class="s2">    topic_probability: 主题概率
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
    <span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
    <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
    
    <span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">)]</span>

    <span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">88</span><span class="p">])</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;Word</span><span class="si">{query_word}</span><span class="se">\n</span><span class="s2">Topic_</span><span class="si">{topic_id}</span><span class="se">\n</span><span class="s2">Probability:</span><span class="si">{probability:.2f}</span><span class="s2">&#34;&#34;&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">query_word</span><span class="o">=</span><span class="n">query_word</span><span class="p">,</span>
                                                              <span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> 
                                                              <span class="n">probability</span><span class="o">=</span><span class="n">topic_probability</span><span class="p">)</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
        <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> 
                                  <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">18</span><span class="p">)),</span>
        <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="n">display</span><span class="p">(</span><span class="n">wc</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">())</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">query_word</span> <span class="o">=</span> <span class="s2">&#34;电影&#34;</span>
<span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_topics</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="n">query_word</span><span class="p">],</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_score</span><span class="p">,</span> <span class="n">topic_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">topic_score</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">:</span>
        <span class="n">gen_wordcloud2</span><span class="p">(</span><span class="n">query_word</span><span class="o">=</span><span class="n">query_word</span><span class="p">,</span> 
                       <span class="n">topic_words</span><span class="o">=</span><span class="n">topic_words</span><span class="p">,</span> 
                       <span class="n">word_scores</span><span class="o">=</span><span class="n">word_scores</span><span class="p">,</span> 
                       <span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> <span class="n">topic_probability</span><span class="o">=</span><span class="n">topic_score</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/vis5.png" alt=""  />
</p>
<br>
<h2 id="7-query_topics">7. query_topics</h2>
<p>根据一段文本寻找最符合该文本的话题
query_topics(query, num_topics)</p>
<ul>
<li>query: 查询文本，注意是用空格间隔词语的文本</li>
<li>num_topics: 返回的话题数</li>
</ul>
<p>返回话题特征词列表， 话题特征词权重， 话题概率， 话题id</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">querytext</span> <span class="o">=</span> <span class="s1">&#39;刘晓庆 55 岁 近日 颁奖礼 刘晓庆 一袭 宝蓝色 超低 胸 V 领 长裙 亮相 轻薄 蕾丝 奢华 皮草 艳丽 色彩 翠绿&#39;</span>
<span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">query_topics</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">querytext</span><span class="p">,</span> 
                                                                       <span class="n">num_topics</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;可能归属的话题有: &#39;</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;归属于该话题的概率&#39;</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">可能归属的话题有:  [1 4]
归属于该话题的概率 [0.32036728 0.1276904 ]
</code></pre></div><br>
<h2 id="8-search_documents_by_keywords">8. search_documents_by_keywords</h2>
<p>根据关键词，筛选文档</p>
<p>search_documents_by_keywords(keywords,
num_docs,
keywords_neg=None,
return_documents=True)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#文档， 语义相关性， 文档id</span>
<span class="n">docs</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">doc_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_keywords</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;搭配&#39;</span><span class="p">],</span> 
                                                         <span class="n">num_docs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                                                         <span class="n">keywords_neg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                                                         <span class="n">return_documents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">doc_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;----------&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Document: 106, Semantic similarity: 0.4943176805973053
白色 短裙 百变 休闲 感 要点 一定 敞开 衬衫 配合 牛仔裤 休闲 感 短裤 衬衫 短 敞开 显得 好好 穿 裤子 搭配 七分裤 遮住 臀部 长度 关键 尽量 选择 艳丽 颜色 带 出 青春 感 NO.3     白色 短裙 tips :   白色 短裙 + 粉色 上衣 这是 一套 减龄 百分百 搭配 白色 短裙 本来 清纯 粉色 上衣 搭配 更加 具有活力 tips :   白色 短裙 + 抹胸 + 外套 想要 性感 一点 就加 一件 抹胸 抹胸 胸前 构造 曲线 完美 再加 外套 保暖 得体 看似 简单 一款 搭配 其实 暗地里 偷偷地 修饰 身材
----------

Document: 870, Semantic similarity: 0.4483542740345001
组图 看达人 演绎 豹纹 军装 风 导语 懂得 潮流 总是 知道 适合 今冬 流行 亮点 太 军装 豹纹 类似 民族风情 想要 知道 搭配 快 看看 时尚 达 穿 军绿色 宽松 款 大衣 不失 俏皮 味道 高腰 设计 短裙 有效 提升 腰线 衬托出 修长 美腿 豹纹 今年 冬季 抢眼 搭配 元素 加上 驼色 针织衫 灰色 围巾 暖 棕色 手 挎包 整体 色调 统一 迷人 棕色 蓝色 结合能 眼前一亮 简洁 款式 依然 突显 独特 品味 宽松 针织 外套 衬托出 优美 身形 搭配 同样 沉闷 黑色 包包 性感 丝袜 装扮 依然 透露 出 迷人 气息 立领 衬衫 加上 深黄 高腰 裤 摩登 感 十足 随意 披上 外套 更显 慵懒 个性 法式 风情
----------

Document: 450, Semantic similarity: 0.4471719563007355
街 拍 爱 招摇过市 毛茸茸 ( 组图 ) 导语 皮草 每个 冬天 可能 丢弃 每个 需要 温暖 早些 相比 人造皮 草比 真皮 草 风头 更劲 时尚 环保 大牌 秀 场上 超模 一个个 穿着 人造皮 草 “ 招摇过市 ” 之后 街头 潮人 没有 理由 拒绝 外形 酷酷 这件 气场 皮草 单品 配合默契 摇滚 风 配饰 搭配 黑色 皮草 长 背心 更显 利落 酷酷 黑色 皮草 搭配 蓝色 衬衣 不同 感觉 加上 下半身 底裤 时髦 包包 颜色 提亮 整身 装扮 抹胸 式 皮草 特点 高贵典雅 适合 搭配 连衣裙 装饰 增添 时尚 美感 复古 圆点 连衣裙 搭配 宽松 棕色 皮草 衣 名媛 感觉 典雅 淑女 短款 黑色 皮草 搭配 贴身 仔裤 搭配 长靴 潇洒 帅气 茸茸 帽子 增添 不少 甜美 感
----------
</code></pre></div><p><br><br></p>
<h2 id="9-search_documents_by_topic">9. search_documents_by_topic</h2>
<p>根据指定的topic_id， 显示该主题前num_docs个文档，显示的文档是根据概率从高到低降序显示</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看topic4的前5条文档</span>
<span class="n">topic_id</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_topic</span><span class="p">(</span><span class="n">topic_num</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> <span class="n">num_docs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Document: 905, Semantic similarity: 0.4941929578781128
-----------
现场 阿联 第三节 未 亮相   奇才 连续 3 记 重扣 逆转 比分 新浪 体育讯 北京 时间 4 2 奇才 主场 迎战 联盟 垫底 骑士 奇才 新秀 后卫 约翰 - 沃尔因 一场 对阵 热火 比赛 斗殴 禁赛 一场 伤愈 复出 安德雷 - 布 莱切 回到 首发 阵容 奇才 本赛季 首场 客场 胜利 面对 骑士 取得 当时 奇才 115 - 110 击败 对手 上半场 骑士 命中率 达到 53.8% 奇才 仅 44% 骑士 希克 森 ( 16 ) 塞 申斯 ( 12 ) 得分 双 奇才 布 莱切 ( 22 ) 麦基 ( 12 ) 埃文斯 4 投 0 仅 抢下 1 篮板 易建联 上场 7 08 2 投 0 抢下 3 篮板 异地 再战 埃文斯 终于 得分 抢断 吉后 犯规 两罚 命中 吉 随即 突破 上篮 命中 回敬 球 杰 弗斯 三分 不进 吉 抢下 篮板 上篮 再进 一球 布 莱切 中投 命中 霍林斯 篮下 出手 不进 布 莱切 抢下 篮板 此后 克劳福德 连续 突破 先是 助攻 麦基 扣篮 盖掉 戴维斯 投篮 助攻 布 莱切 扣篮 戴维斯 运球 被断 布 莱切 传给 杰 弗斯 一记 暴扣 奇才 连续 3 次 颇具 气势 扣篮 连得 6 反超 骑士 1 骑士 请求 暂停 回到 比赛 吉 上篮 不进 麦基 低位 单打 不进 布 莱切 抢下 篮板 3 得手 骑士 进攻 24 违例 奇才 越战越勇 克劳福德 身体 失去 重心 情况 仍然 将球 投进 一个打 3 骑士 连续 吉 挺身而出 三分 命中 个人 已经 得到 10 此人 本赛季 短暂 效力 奇才 麦基 中投 不进 布 莱切 抢下 前场 篮板 将球 放进 麦基 防守 领到 犯规 希克 森两罚 命中 麦基 强攻 造成 霍林斯 犯规 两罚 一中 戴维斯 三分 不进 克劳福德 跑 投 命中 戴维斯 突分 霍林斯 暴扣 命中 回过头来 克劳福德 助攻 麦基扣 劲 爆 哈兰 高迪 中投 不进 克劳福德 投篮 偏出 布 莱切 3 报价 连续 抢 篮板 进攻 最后 犯规 两罚 一中 现在 已经 得到 32 18 篮板 布 莱切 底线 遭 报价 分球 埃文斯 三分 命中 霍林斯 篮下 重扣 奇才 请求 暂停 布 莱切 继续 得分 吉布森 上篮 命中 克劳福德 中投 不进 抢下 篮板 杰 弗斯 运球 突破 犯规 两罚 命中 易建联 节 没有 登场 第三节 比赛 结束 骑士 82 - 83 奇才 ( 草头 王 )
-----------

Document: 689, Semantic similarity: 0.4917592704296112
-----------
直击 康大 内线 一柱擎天   13 优势 到手 胜利在望 新浪 体育讯 北京 时间 4 5 ( 休斯敦 时间 4 4 ) 消息 NCAA   Final   4 总决赛 休斯敦 Reliant 球馆 举行 比赛 进入 最后 6 分钟 本场 表现 十分 亮眼 康涅狄格 内线 阿莱克斯 - 奥里 瓦基接 队友 直传 空切 篮下 扣篮 得分 打成 2 + 1 目前 已经 拿下 10 9 篮板 3 封盖 巴特勒 仍然 没 解决 进攻 端的 问题 下半场 23 投 仅仅 3 屡次 外线 空挡 出手 均 打铁 告终 仅仅 入账 8 目前 康大 已经 取得 13 优势 胜利在望 ( silencer )
-----------

Document: 425, Semantic similarity: 0.47035443782806396
-----------
今日 数据 趣谈 魔兽 悲情 似 张大帅   基德 焕发 第二 春 新浪 体育讯 北京 时间 4 17 NBA 季后赛 正式 开打 进行 4 场 比赛 以下 今日 比赛 诞生 有趣 数据 今日 首场 季后赛 芝加哥 公牛 第四节 剩 4 分钟 仍以 88 - 98 落后 接下来 打出 16 - 1 攻击 波 主场 一举 逆转 印第安纳 步行者 取胜 继 2004 之后 NBA 季后赛 舞台 再次出现 终场 前 4 分钟 落后 两位数 最终 翻盘 成功 案例 2004 5 9 西部 决赛 明尼苏达 森林狼 萨克拉门托 国王 比赛 森林狼 同样 终场 前 4 分钟 仍以 78 - 88 落后 接下来 打出 16 - 1 ( 惊人 相似 ) 最终 94 - 89 逆转 取胜 今天 公牛 逆转 步行者 比赛 德里克 - 罗斯 砍 39 罚球 21 投 19 2008 洛杉矶 湖人 对阵 犹他 爵士 一场 季后赛 科比 - 布莱恩特 创下 单场 罚球 23 投 21 季后赛 纪录 罗斯 位居 全场 三分球 9 次 出手 竟无一 命中 季后赛 历史 此前 两次 类似 案例 2008 奥兰多 魔术 对阵 多伦多 猛龙 一场 比赛 拉沙德 - 刘易斯 三分球 9 投 0 一次 熟知 1994 总决赛 第七场 约翰 - 斯塔克 斯 三分 线外 11 投 0 纽约 尼克斯 负于 休斯敦 火箭 冠军 擦肩而过 今天 亚特兰大 老鹰 客场 战胜 奥兰多 魔术 比赛 老鹰 五名 球员 得分 低于 13 — — 乔 - 约翰逊 ( 25 16 投 9 ) 贾马尔 - 克劳福德 ( 23 14 投 7 ) 艾尔 - 霍福德 ( 16 14 投 7 ) 约什 - 史密斯 ( 15 12 投 6 ) 科克 - 辛里奇 ( 13 10 投 6 ) 该队 过去 199 场 季后赛 尚属 首次 老鹰队 史上 一次 出现 这种 盛况 1966 4 14 131 - 127 战胜 洛杉矶 湖人 比赛 当时 书写 纪录 五人 里奇 - 古尔林 克里夫 - 哈根 泽尔莫 - 比蒂 比尔 - 布里奇斯 乔 - 考 德维尔 今天 负于 老鹰 比赛 德怀特 - 霍华德 ( 46 ) 贾 米尔 - 尼尔森 ( 27 ) 砍 73 队友 总共 仅 拿下 20 魔术 最终 93 - 103 负于 更为 均衡 对手 NBA 历史 8 支 球队 一场 季后赛 比赛 有过 两名 球员 联手 砍 全队 至少 75% 得分 1 队 取胜 追溯到 1950 4 9 当年 总决赛 第一场 比赛 乔治 - 麦肯 得到 37 吉姆 - 波 拉德 得到 14 率领 明尼阿波利斯 湖人 68 - 66 战胜 锡 拉丘兹 民族 ( 费城 76 前身 ) 7 队则 败北 得到 46 霍华德 抢下 19 篮板 常规 时间 取得 1975 4 19 布法罗 勇敢者 ( 洛杉矶 快船 前身 ) 战胜 华盛顿 子弹 ( 华盛顿 奇才 前身 ) 一场 季后赛 效力 勇敢者 鲍勃 - 麦卡 杜 同样 没有 加时赛 情况 砍 50 21 篮板 威尔特 - 张伯伦 一场 季后赛 常规 时间 砍 46 19 篮板 球队 却输 ( 事实上 张大帅 生涯 3 场 比赛 取得 数据 竟 败北 ) 刚 谢幕 本赛季 常规赛 杰森 - 基德 仅 两场 比赛 得分 达到 20 + 1 20 对阵 湖人 比赛 砍 赛季 最高 21 今天 达拉斯 小牛 主场 战胜 波特兰 开拓者 比赛 砍 24 命中 6 记 三分球 一场 季后赛 比赛 砍 20 + 得分 刷新 常规赛 创下 赛季 新高 NBA 历史 壮举 球员 如今 38 岁 基德 年龄 最大 成为 NBA 历史 一场 季后赛 比赛 单场 命中 6 记 三分球 年龄 最大 球员 此前 纪录 雷吉 - 米勒 2002 创下 当时 36 岁 今天 小牛 战胜 开拓者 比赛 德克 - 诺维茨基 第四节 13 次 罚球 出手 命中 追平 迈克尔 - 乔丹 纪录 1990 - 91 赛季 季后赛 一场 公牛 底特律 活塞 比赛 乔丹 单节 命中 13 次 罚球 率队 105 - 97 取胜 最终 公牛 获得 赛季 总冠军 今天 迈阿密 热火 战胜 费城 76 比赛 克里斯 - 波什 得到 25 12 篮板 勒布朗 - 詹姆斯 得到 21 14 篮板 他俩 队友 参加 首场 季后赛 前 一个 赛季 各为其主 接下来 赛季 并肩作战 季后赛 首场 比赛 砍 得分 20 + 篮板 10 + 组合 波什 詹姆斯 之前 无先例 ( 魑魅 )
-----------

Document: 155, Semantic similarity: 0.45704954862594604
-----------
现场 麦蒂 返场 销魂 跳投 两 连击   小拜 纳姆 单节 11 新浪 体育讯 北京 时间 4 6 华盛顿 奇才 主场 迎战 底特律 活塞 此前 球队 已经 客场 两连胜 若能 战胜 活塞 奇才 本赛季 首次 迎来 三连胜 异地 再战 埃文斯 中投 命中率 先 得分 拜纳姆 中投 不进 克劳福德 一人 带球 运 前场 对手 尚未 落位 情况 直接 出手 投篮 命中 这种 投篮 欠缺 考虑 根本 没有 战术 配合 全 个人 手感 遇到 防守 稍 一点 球队 沃尔 抢断 埃文斯 直接 暴扣 奇才 反超 4 活塞 请求 暂停 沃尔 报价 对手 拜纳姆 得到 机会 三分 出手 命中 布 莱切 上篮 得手 门罗 助攻 威尔 考克斯 扣篮 命中 埃文斯 三分 不进 拜纳姆 突破 上篮 命中 威尔 考克斯 拿布 莱切 没有 办法 运球 进攻 威尔 考克斯 只能 伸直 手臂 不断 滑步 被布 莱切 强投 命中 活塞 拜纳姆 发力 突破 上篮 命中 布 莱切 中投 不进 拜纳姆 卷土重来 造成 沃尔 犯规 两罚 命中 个人 已经 得到 11 门罗 抢断 布 莱切 普林斯 上篮 命中 活塞 反超 3 麦基 传球 失误 奇才 请求 暂停 威尔 考克斯 篮下 强打 奇才 反击 埃文斯 上篮 命中 普林斯 糟糕 状态 继续 中投 偏出 布 莱切 运球 单打 活塞 两名 内线 屡试不爽 造成 门罗 犯规 两罚 命中 汉密尔顿 中投 不进 威尔 考克斯 抢下 前场 篮板 直接 扣篮 命中 布 莱切 继续 发威 转身 摆脱 上篮 命中 拜纳姆 三分 偏出 球 砸 远 活塞 球员 退守 不及 克劳福德 轻松 上篮 命中 沃尔 中投 不进 拜纳姆 反击 遭 侵犯 两罚 命中 个人 单节 已经 得到 11 布 莱切 对手 包夹 中投 偏出 普林斯 跑 投 命中 活塞 反超 一分 克劳福德 中投 打铁 拜纳姆 没能 命中 三分 麦蒂 回到 赛场 塞拉芬 进攻 犯规 普林斯 中投 不进 门罗 补篮 命中 麦蒂断 球 直接 中投 命中 布 莱切 走步 麦蒂 假动作 点飞 克劳福德 投篮 再进 第三节 比赛 结束 活塞 81 - 78 奇才 ( 草头 王 )
-----------

Document: 254, Semantic similarity: 0.45255911350250244
-----------
奇才 vs 步行者 前瞻 走出 客场 阴影   斗狠 东部 老八 新浪 体育讯 北京 时间 4 7 奇才队 客场 挑战 东区 第八 步行者 目前 奇才 客场 战绩 3 胜 35 负 最近 客场 两连胜 奇才队 背靠背 作战 今天 主场 107 - 105 险胜 活塞 球队 一举 拿到 赛季 最长 三连胜 实际上 这是 奇才队 2007 - 08 赛季 以后 球队 第一个 赛季 三连胜 这场 比赛 奇才 惊人 获得 35 次 罚球 沃尔一人 包办 16 次 全场 得到 26 12 次 助攻 6 篮板 4 次 抢断 布 莱切 无疑 三连胜 第一 功臣 连胜 期间 场均 得到 29 15.3 篮板 克劳福德 同样 火爆 异常 一位 前锋 首发 埃文斯 表现 低估 活塞 比赛 埃文斯 13 投 9 中射下 20 沃尔 拿到 职业生涯 首个 三连胜 “ 联盟 留下 标签 一名 菜鸟 证明 一部分 很多 想 站 球场上 一分钟 全力以赴 ” 奇才 三连胜 对手 名副其实 鱼腩 球队 无论如何 三连胜 这支 弱旅 一个 不小 激励 尤其 伤病 满营 情况 目前 球队 6 可能 赛季 结束 前 无法 归队 包括 得分王 尼克 - 杨 约什 - 霍华德 拉沙德 - 刘易斯 布克 恩戴 耶 卡 蒂尔 - 马丁 步行者 35 胜 43 负 暂居 东部 第八 目前 东部 前七 已经 锁定 季后赛 剩下 第八名 悬念 步行者 领先 第九位 山猫 2.5 个胜场 领先 10 位 雄鹿 3.5 个胜场 剩下 4 场 比赛 情况 悬念 并不大 明天 山猫 雄鹿 迎战 强敌 ( 魔术 热火 ) 步行者 机会 扩大 领先 场次 优势 球队 头号 得分手 格兰杰 状态 过去 5 场 比赛 得分 20 以下 最近 三场 三分球 12 投 3 汉斯 布鲁在 过去 6 场 比赛 陷入 挣扎 场均 9.3 5.7 篮板 ( 之前 11 场 比赛 贡献 20.6 7.8 篮板 ) 一场 比赛 步行者 12 输给 黄蜂队 主教练 沃格尔 称之为 “ 惨痛 失败 ” 本赛季 两队 战成 2 - 1 步行者 赢下 最近 两次 交锋 两场 比赛 奇才 命中率 均 低于 40% 总 失误 高达 41 次 预计 两队 首发 奇才 沃尔 克劳福德 埃文斯 布 莱切 麦基 步行者 科 里森 格兰杰 乔治 汉斯 布鲁 希伯特 ( 木瓜 丁 )
-----------
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_keywords</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;搭配&#34;</span><span class="p">,</span> <span class="s2">&#34;高跟鞋&#34;</span><span class="p">],</span> <span class="n">num_docs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><pre><code>Document: 727, Semantic similarity: 0.5883481502532959
-----------
组图 冷气 办公室   连衣裙 配小 坎肩 美国 设计师 Diane   Von   Furstenberg 曾经 感觉 女人 穿 连衣裙 女人 找到 一件 适合 dream   dress 重要 无需 费神 搭配 单穿 连身 优雅 飞扬 裙摆 似乎 告诉 女 连衣裙 玩起 High   Fashion 变脸 游戏 DKNY 绿色 连衣裙   新品 未 定价 H &amp; M 黑色 外套   新品 未 定价 Agatha 配件 新品 未 定价 C . Banner 高跟鞋   新品 未 定价 低 V 领 连衣裙 秀出 属于 性感 更好 展现出 颈部 线条 搭配 修身 剪裁 西装 短款 皮手套 极具 欧美 明星 范儿 细 高跟鞋 更好 突出 双腿 长度 整体 显得 轻盈 不少 On &amp; on 米色 连衣裙   新品 未 定价 Asobio 针织 外套   RMB   449 Kookai 金色 腰带 Jc  
-----------

Document: 435, Semantic similarity: 0.5440454483032227
-----------
组图 秋冬 优雅 妖娆   女星 爱 裸 色系 导语 裸色 优雅 代名词 女星 近来 誓 裸色 进行 到底 无论是 徐若 ? 性感 乐基儿 气质 搭配 各色 礼服 赏心悦目 娇俏 款式 更是 大饱眼福 徐若 ? 飘逸 丝带    立刻 彰显 天王 嫂 贵妇 气质 袁咏仪 翻领 西装   气质 非凡 裸色 短款 紧身 西装 皮质 面料 彰显 个性 夹带 一点 蕾丝 装饰 女性 柔美 油然而生 搭配 碎花 蛋糕 裙 气质 非凡
-----------

Document: 870, Semantic similarity: 0.523485541343689
-----------
组图 看达人 演绎 豹纹 军装 风 导语 懂得 潮流 总是 知道 适合 今冬 流行 亮点 太 军装 豹纹 类似 民族风情 想要 知道 搭配 快 看看 时尚 达 穿 军绿色 宽松 款 大衣 不失 俏皮 味道 高腰 设计 短裙 有效 提升 腰线 衬托出 修长 美腿 豹纹 今年 冬季 抢眼 搭配 元素 加上 驼色 针织衫 灰色 围巾 暖 棕色 手 挎包 整体 色调 统一 迷人 棕色 蓝色 结合能 眼前一亮 简洁 款式 依然 突显 
-----------

Document: 522, Semantic similarity: 0.4756317138671875
-----------
女星 争当 蓝色妖姬 &amp; nbsp ; 英国 气质 女演员 瑞切尔 ・ 薇 兹 时尚 点评 英国 气质 女演员 瑞切尔 · 薇 兹 ( Rachel   Weisz )   美貌 非常 头脑 修身 印花 连衣裙 搭配 抢眼 棕红色 短 夹克 非常 好看 搭配 黑色 罗马 feel 高跟鞋 特别 有潮味 时尚 点评 身材 不算 瘦 女星 Lea   Michele 搭配 起来 非常 特色 一味 地瘦 风格 满是 褶皱 裙子 非常 修身 亮眼 颜色 非常
-----------

Document: 707, Semantic similarity: 0.47334203124046326
-----------
组图 黑丝 短裙 上阵   5 旬 女星 胜过 90 红星 导语 气温 越来越低 女星 不畏 严寒 纷纷 穿着 短裙 透视装 出席 活动 一番 比拼 不难 发现 气质 年轻 难得 厉害 一起 看看 刘晓庆 55 岁 近日 颁奖礼 刘晓庆 一袭 宝蓝色 超低 胸 V 领 长裙 亮相 轻薄 蕾丝 奢华 皮草 艳丽 色彩 翠绿 首饰 配上 短小 精炼 波波 头 瞬间 减龄 15 岁 张曼玉 46 岁 一向 气质 型 美女 著称 反倒 少 繁琐 修饰 刻意 打扮 超级 简单 Lanvin   for   H &amp; M 斜肩 礼裙 搭配 一双 皮质 手套 
-----------
</code></pre>
<p><br><br></p>
<h2 id="10-get_topic_hierarchy">10. get_topic_hierarchy</h2>
<p>对话题进行分类，需要</p>
<ol>
<li>先执行model.hierarchical_topic_reduction</li>
<li>再执行model.get_topic_hierarchy。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 将话题分为2类</span>
<span class="n">model</span><span class="o">.</span><span class="n">hierarchical_topic_reduction</span><span class="p">(</span><span class="n">num_topics</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_topic_hierarchy</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>[[7, 6, 1, 8, 5, 4, 3], [2, 0]]
</code></pre>
<p><br><br></p>
<h2 id="11-similar_words">11. similar_words</h2>
<p>查找相似词， 该方法其实也可以用于扩充词典。</p>
<p>similar_words(keywords, num_words, keywords_neg=None)</p>
<ul>
<li>keywords: 待查询关键词列表</li>
<li>num_words: 返回相似词个数</li>
<li>keywords_neg: 指定反义词列表</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查找【增进】的最相似的10个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">similar_words</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;增进&#34;</span><span class="p">],</span> 
                    <span class="n">num_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                    <span class="n">keywords_neg</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>(array(['两国关系', '两国', '温家宝', '王刚', '战略', '友好', '中欧', '政治', '会见', '人民'],
       dtype='&lt;U4'),
 array([0.50498132, 0.49835259, 0.4636392 , 0.45802986, 0.45299921,
        0.44836198, 0.43550295, 0.43471974, 0.43099192, 0.42711113]))
</code></pre>
<br>
<h2 id="12-save">12. save</h2>
<p>训练不易， 记得保存模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;随便起个名字.pkl&#39;</span><span class="p">)</span>
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 管理决策情境下大数据驱动的研究和应用挑战</title>
      <link>https://textdata.cn/blog/management_challenge_in_big_data_era/</link>
      <pubDate>Wed, 08 Dec 2021 12:42:10 +0600</pubDate>
      
      <guid>/blog/management_challenge_in_big_data_era/</guid>
      <description>在数字化生活背景下, 传统的管理变成或正在变成数据的管理, 传统的决策变成或正在变成基于数据分析的决策.从大数据的数据特征、问题特征和管理决策特征出发, 讨论管理决策研究和应用的范式转变.大数据驱动范式可以从外部嵌入、技术增强和使能创新三个角度来审视, 并体现出“数据驱动&#43;模型驱动”的“关联&#43;因果”含义.此外, 围绕大数据特征和重要研究方向, 阐述了全景式PAGE框架及其要素.</description>
      <content:encoded><![CDATA[<p><strong>摘  要：</strong></p>
<p>在数字化生活背景下, 传统的管理变成或正在变成数据的管理, 传统的决策变成或正在变成基于数据分析的决策.从大数据的数据特征、问题特征和管理决策特征出发, 讨论管理决策研究和应用的范式转变.大数据驱动范式可以从外部嵌入、技术增强和使能创新三个角度来审视, 并体现出“数据驱动+模型驱动”的“关联+因果”含义.此外, 围绕大数据特征和重要研究方向, 阐述了全景式PAGE框架及其要素.</p>
<p><strong>关键词</strong>： 大数据 ； 管理决策 ； 研究范式 ； 全景式 PAGE 框架</p>
<br>
<p>信息科技的飞速发展和深度融合开启了数字化生活的新篇章, 把人们带入了大数据 (big data) 时代.一方面, 随着各种感应探测技术、智能终端以及移动互联的广泛应用, 使得社会经济生活的方方面面以更细粒度的数据形式呈现, 进而整个社会的“像素”得到显著提升;另一方面, 社会“像素”的提升促进了数字“成像”的发展, 使得通过数据世界可以更清晰地描绘社会经济活动情境, 进而基于数据的商务分析 (business analytics, BA) 正在成为使能创新的核心竞争力.在此背景下, 传统的管理变成或正在变成数据的管理, 传统的决策变成或正在变成基于数据分析的决策.</p>
<p>近年来, 大数据成为学界、政界和业界持续关注的热点.在学术界, 早在2008年和2011年, 《Nature》与《Science》杂志分别从互联网技术、互联网经济学、超级计算、环境科学以及生物医药等多方面讨论大数据的处理与应用45此后, 大数据在各个学科领域包括医学、经济学、管理学以及公共管理等领域得到了广泛的探讨与研究6789同时, 大数据也引起世界各国高度重视, 美国、欧盟、澳大利亚以及日本等国部署了一系列大数据相关战略和关键领域。在产业界, 国内外大批知名企业掀起了技术产业创新浪潮, 通过收购与合作构建和提升大数据技术与应用能力, 布局和开拓相关的业态和市场.</p>
<p>我国政府对大数据高度重视并有一系列前瞻性洞见和部署.2015年十八届五中全会提出实施国家大数据战略  , 国务院发布《促进大数据发展行动纲要》  , 指出大数据是国家基础性战略资源, 旨在全面推进我国大数据发展和应用, 加快建设数据强国.2017年十九大报告进一步强调要推动互联网、大数据、人工智能和实体经济深度融合.通过国家需求、政策支持、产业结合以及企业研发等形式, 近些年来涌现出一大批重大规划和政产学研项目, 包括国家自然科学基金委员会 (NSFC) 于2015年9月启动的“大数据驱动的管理与决策研究”重大研究计划 (简称NSFC大数据重大研究计划, 参见附注)  .</p>
<p>大数据在给社会经济生活带来深刻变革的同时, 也对管理与决策研究带来一系列新的重要课题.从信息技术 (IT) 范畴来看, 可以从两个视角来认识大数据, 即大数据的“造”与“用”视角 (如图1所示) .这和产品的属性类似, 一方面, 人们关心产品是如何设计和制造出来的;另一方面, 人们关心产品是如何使用和有用的.大数据以IT的形式呈现, 通常可以概括为数据和系统 (包括算法、应用、平台等) .从造的视角出发, 涉及的主要问题包括大数据分析 (如画像、学习、推断等) 和大数据系统建设 (如体系、功能、集成等) .从用的视角出发, 涉及的主要问题包括大数据使用行为 (如采纳、影响、管理等) 和大数据使能创新 (如要素、价值、市场等) .</p>

<figure >
    
        <img src="img/1.jpg" width="800" />
    
    
</figure>

<p>值得一提的是, 大数据相关的研究不仅需要对相关领域的理论与应用进行探索和创新, 也需要对许多惯常的认识视角和方法论范式进行审视和发展.同时, 我国学者和研究人员也面临着“严谨 (rigor) 与相关 (relevance) ” (学术规范与实践影响) 和“世界与中国” (国际视野与中国根基) 既分野又统一的挑战, 当然应对这些挑战也为创新机遇开拓了广袤的空间.</p>
<br>
<br>
<h2 id="大数据特征">大数据特征</h2>
<p>概括说来, 大数据的特征可以从三个方面来描述:数据特征, 问题特征和管理决策特征, 分别刻画大数据具有的数据属性、大数据问题的特点、以及管理决策大数据问题的视角.</p>
<h3 id="11-大数据的数据特征">1.1 大数据的数据特征</h3>
<p>大数据作为数据, 具有体量大、多样性、 (价值) 密度低、速率高等属性特征 (即4V等特征）.第一, 数字化生活各要素的数据生成和交互加速了数据的海量积累, 使得数据规模剧增.体量大可以从超规模 (即超出传统规模) 和问题领域角度来理解, 因为规模是与问题领域相关, 而不是拘泥于统一量纲标准.例如, 市场营销领域的客户满意度调查的传统方式是问卷和访谈, 那么进一步考虑海量网上购物评论和社交媒体体验分享的用户生成内容 (user generated content, UGC) 就构成了一个大数据情境.第二, 数字化生活各要素的数据生成和交互丰富了数据类型, 使得数据多样性成为常态.多样性强调数据的多源异构和富媒体 (如文本、语音、图片、视频等) 特点.例如, 社交网络上的公众声音、智慧交通平台上的影像信息等均为富媒体形态且来源广泛.第三, 数字化生活各要素的数据生成和交互在加速海量积累的同时也减少了价值数据的占比, 使得价值发现的难度提升.价值密度低意味着数据挖掘和商务分析是大数据应用的关键.例如, 对于在线企业或服务平台来讲, 随着网络访问的增加和业务活动的扩展, 识别高价值的潜在用户变得相对困难, 也凸显出大数据分析的重要性.第四, 数字化生活各要素的数据生成和交互强化了流数据形态和即时性, 使得数据传输和交换速率显著升高.速率高对平滑流通和连续商务提出了更高要求.例如, 智能手机客户端应用软件 (Apps) 的使用需要在服务内容和效果方面 (包括相关内容的浏览、下载、上传、响应、展现等) 有良好的临场感和实时体验.</p>
<br>
<h3 id="12-大数据的问题特征">1.2 大数据的问题特征</h3>
<p>在各类研究和应用问题中, 有一类问题可以归为大数据问题.大数据问题应至少具有以下三个特点:粒度缩放、跨界关联和全局视图.首先, 粒度缩放是指问题要素的数据化, 并能够在不同粒度层级间进行缩放.这需要通过数据感知、连接和采集获得足够细的粒度性, 同时对于不同层级间的粒度转换具有分解和聚合能力.其次, 跨界关联是指问题的要素空间外拓.这需要扩展惯常的要素约束和领域视角, 强调“外部性”和“跨界”, 在问题要素空间中通过引入外部视角与传统视角联动, 将内部数据 (如个体自身、企业组织和行业等内部数据) 与外部数据 (如社会媒体内容等) 予以关联.最后, 全局视图是指问题定义与求解的全局性, 强调对相关情境的整体画像及其动态演化的把控和诠释.这需要基于数据分析和平台集成的全景式“成像”能力.</p>
<p>在数字化生活的背景下, 具有粒度缩放、跨界关联和全局视图特点的应用问题不断涌现, 进而激发了大量创新并催生了许多新模式、新业态.例如, 在医疗健康领域, 传统疾病诊疗中的病人就医关系正在被扩展为融合院外检测、干预、康复数据的新型诊疗模式.其中, 不仅涉及传统意义上的生化、影像和诊疗等医院内部数据, 也涉及医院外病人和社区相关的体征、体验、社会关系、环境等外部数据.这里, 需要获取相关生化组织、疾病、人、社区、环境等微观宏观粒度信息;同时进行视角拓展和关联, 包括从科室内外到医院内外的跨界融合;进而, 可以在全局层面进行更为有效的诊疗决策和管理.此外, 近年来发展迅速的新型医疗健康服务平台, 通过整合社会和行业资源, 连接医生、公众、医院以及相关上下游企业提供信息咨询、诊疗链入、健康指导等服务产品, 形成了一类新业态并呈现显著的大数据问题特征.再如, 在新型商务领域, 共享单车体现了大数据问题的粒度缩放、跨界关联和全局视图特点.通过车载传感器、定位系统以及智能手机终端等设备获得调度和管理需要的“人—车—路”粒度信息;同时, 打通导航、支付、通讯、商铺以及餐饮等诸多业务功能, 实现跨界联动;进而, 企业和平台可以从全局出发形成整体画像, 并优化布局和运作以做出相应的管理决策.</p>
<br>
<h3 id="13-大数据管理决策特征">1.3 大数据管理决策特征</h3>
<p>一般而言, 管理者在业务活动中通常有三个关注:发生了什么 (what) , 为什么发生 (why) 以及将发生什么 (will) .在大数据问题特征的情境下, 这三个关注可以从业务层面、数据层面和决策层面进行刻画, 进而形成管理决策大数据问题的特征框架 (如图2所示) .</p>

<figure >
    
        <img src="img/2.jpg" width="800" />
    
    
</figure>

<p>首先, 对于发生了什么 (what) 的关注, 业务层面需要反映业务的状态, 即已经发生或者正在发生的事件和活动 (如市场份额、交易现状、KPI表现等) ;数据层面需要体现业务环节的数据粒度, 即现有的数据能否足够支撑管理者对不同粒度层级的业务状态进行了解和把握 (如感知、采集、解析、融合等) ;决策层面需要构建问题的全局视角, 即定期整合汇总以及随需要素展现 (如:按时统计报表、实时信息查询等) .</p>
<p>接着, 对于为什么会发生 (why) 的关注, 业务层面需要反映业务及其要素之间的联系, 即业务特定状态的发生与哪些环节和要素有关联;数据层面需要体现不同业务数据路径的连接, 即不同粒度层级和跨界关联的业务数据是否有效融通, 并能够支持对数据的分析处理 (如多维、切分、回溯等) ;决策层面需要发现关联业务/要素之间的因果关系, 即厘清业务逻辑和状态转换机理.在此, 特别需要指出的是, 在很多情形下, 尤其在管理决策领域, 大数据需要既讲关联也讲因果.对于许多管理问题而言, 如果决策者对事件之间的因果关系没有准确的分析与判断, 则难以做出有效的决策, 当管理者面临重大决策时更是如此 (如投融资、进入新市场、业务转型、结构重组等) .</p>
<p>进而, 对于将发生什么 (will) 的关注, 业务层面需要反映业务发展轨迹, 即勾勒出由决策或变化导致的业务走向;数据层面需要体现数据的动态演化情况, 即对于相关事件进行不确定性动态建模并能够支持智能学习和推断 (如模拟、预测、人工智能等) ;决策层面需要提升前瞻性和风险洞见, 即获得决策情境映现和趋势预判能力.</p>
<br>
<h2 id="大数据驱动范式">大数据驱动范式</h2>
<p>系统化管理理论的产生及其发展, 包括行为理论、决策理论、权变理论和战略管理等理论体系和管理模型的研究[19], 在提炼管理思想、诠释管理模式和指导管理实践方面发挥了重要作用.长期以来, 管理学研究一直以模型驱动范式为领域主流.在模型驱动范式下, 研究者基于观察抽象和理论推演建立概念模型和关联假设, 再借助解析手段 (例如运筹学和博弈论等分析工具) 对模型进行求解和优化, 或利用相关数据 (包括仿真数据、调研数据、观测数据、系统记录数据等) 对假设进行统计检验.此外, 建立在归纳逻辑基础上的扎根理论等研究范式, 传统上强调从文献概括、实地调研、深度访谈中进行定性推演形成理论和认识.</p>
<p>但是, 在大数据背景下, 一些新的挑战正在涌现[20,21].这里, 以传统的行为模型或计量模型 (简称传统模型) 为例.第一, 传统模型基于观察抽象、理论推演以及经验提炼确定变量 (或构念) 组合, 以此构建变量关系和理论假设, 并通过数据实证进行模型检验.然而, 在大数据背景下, 常常需要检验大量的变量组合 (如指数级组合数) , 这就使得逐一构建传统模型并进行检验成为难以完成的任务.第二, 有些重要潜在影响因素和隐变量没有被意识到, 因而没有被考虑到传统模型的变量组合中, 这常常导致传统模型的假设与数据的适配性不强, 模型解释力不高.第三, 虽然知道有些影响因素和变量是重要的, 但是由于这些因素和变量在传统意义上不可测或不可获 (如文本、图像、语音等富媒体数据) , 难以容纳到传统模型变量组合中, 进而造成模型解释力不理想.第四, 当样本数据规模大幅增加时, 对一些变量的显著性检验有效性下降, 可能出现联系缺失或拟合过度等情形.</p>
<p>面对上述挑战, 数据驱动范式的优势不断凸显.概括说来, 数据驱动范式的作用有两个:一是直接发现特定变量关系模式, 形成问题解决方案;二是与模型驱动范式进行补充扩展, 形成融合范式.值得指出的是, 数据驱动范式发现的一类重要关系模式是关联 (association) 及其扩展形式 (如关联规则、层次关联、数量关联、时态关联、类关联、模式关联等) , 并广泛应用到许多领域 (如搜索、推荐、模式识别等) [22].然而, 许多管理决策情形不仅需要关联也需要因果, 这在一定程度上催生了融合范式及其应用.例如, 首先利用数据驱动范式的关联挖掘方法发现变量间的关联, 以缩减变量空间和组合规模;进而利用模型驱动范式的行为方法辨识构念影响路径, 或计量方法解析变量间的因果关系.这是一个“数据驱动+模型驱动”思路, 体现“关联+因果”的诉求, 这对于管理决策尤为重要.这里, 与传统模型相比一个重要区别是, 此时的变量空间中可能存在着一些新颖且潜在的变量及其关联, 在进一步融合运用模型驱动方法构建变量关系时存在困难, 因为已有的理论知识和领域经验不能直接支持相关的建模逻辑和关系形式.这就需要在更深 (包括间接、潜隐) 层面上探寻新的变量影响机理和理论, 并在方法论上另辟新径 (如通过步进/层次/迭代的试错和启发建模方式) .</p>
<p>特别地, 当数据具有4V等特征并且面对管理决策大数据问题时, 考虑数据驱动与模型驱动的结合、管理决策的关联因果特点、使能创新等元素的一类新型范式 (在此称作大数据驱动范式) 应运而生, 并在深入研究与应用过程中得到进一步发展完善.一般而言, 大数据驱动范式具有“数据驱动+模型驱动”的“关联+因果”性质.具体说来, 大数据驱动范式的框架可从三个角度来审视:外部嵌入、技术增强以及使能创新 (如图3所示) .前两个角度主要涉及方法论层面, 后一个角度主要涉及价值创造层面.</p>

<figure >
    
        <img src="img/3.jpg" width="800" />
    
    
</figure>

<br>
<h3 id="21-外部嵌入">2.1 外部嵌入</h3>
<p>外部嵌入指外部视角引入, 即将传统模型视角之外的一些重要变量 (包括构念、因素等) 引入到模型中.假设自变量集合为X'={x1, x2, …, xm, xm+1, …, xn}, 其中x1, x2, …, xm为传统建模变量, xm+1, …, xn为通过数据驱动方法新引入的变量 (多为富媒体形态) .如果没有变量引入 (n=m) , 传统模型的变量关系是Y=f (X) , X={x1, x2, …, xm}.在跨界关联情境下 (n&gt;m) , 将形成新变量关系Y'=f' (X') .换句话说, Y=f (X) 可以是Y'=f' (X') 的特例;一般意义上讲, X'≠X, f'≠f, Y'≠Y.显然, 新变量关系的构建面临着深刻的挑战, 既有新变量空间的发现, 又有新视角的洞察, 也有新变量关系的辨识和新理论的生成.当然, 对于研究和应用来讲, 这些挑战同时也是创新的机遇.例如, 在金融领域, 可以考虑引入搜索平台上的股票关注数据变量以及社交媒体平台上的相关公共事件数据变量等, 以构建新型股价预测模型;在商务领域, 可以考虑引入购物平台上的评论数据变量以及朋友圈中的体验和口碑数据变量等, 以构建新型商品营销模型;在医疗健康领域, 可以考虑引入院外病友智能检测终端数据变量以及区域环境诱因数据变量等, 以构建新型呼吸疾病预防诊疗模型;在公共管理领域, 可以考虑引入社交平台上的受众意见数据变量以及相关领域联动影响数据变量等, 以构建新型公共政策模型.</p>
<br>
<h3 id="22-技术增强">2.2 技术增强</h3>
<p>对于传统模型来讲, 通过外部嵌入而引入的变量多为富媒体、潜隐性、不可测或不可获, 通常需要利用数据驱动方法和技术.可以说, 数据和技术意识及其能力是大数据背景下研究和应用的核心竞争力, 也是大数据驱动范式的关键要素.技术增强旨在提升这样的能力与要素水平.</p>
<p>从大数据的“用”与“造”视角出发, 技术增强具有两方面含义.一方面, “用”的视角要求管理模型驱动的研究和应用能够增强对外部大数据的敏感性, 引入外部变量并构建其关系;同时, 能够增强对大数据分析技术的敏感性, 构建方法和工具的获取和使用能力.研究和应用创新通常体现在通过新型范式开发新的变量关系, 进而形成新的管理学模型和应用 (如面向管理问题的新型行为模型或计量模型) , 以获得更深入和更具解释力的管理决策洞见和策略.</p>
<p>另一方面, “造”的视角要求数据驱动的研究和应用能够增强对于管理决策问题的敏感性, 构建面向管理决策问题的方法和技术.研究和应用创新通常体现在根据管理决策问题特点及其数据属性开发相关性质、测度和策略, 以获得新颖有效的算法和解决方案.值得指出的是, 这里许多算法 (特别是启发式算法和近似解法) 需要经过实验数据的验证以评估其效率和效果.</p>
<p>多年来, 不管是“用”的视角还是“造”的视角在数据的使用标准上也经历了一个不断升级的过程, 从模拟数据到标杆数据, 再到相当规模的实际数据, 形成一个逐步丰富和叠加的验证实践.在大数据情境下, 实际数据的规模化得到了进一步强化.此外, 在算法比较中, 更关注算法带来的实用效果提升的显著性, 特别在涉及相关用户的场景中, 通常需要进行用户行为实验及其效果感知评测.</p>
<p>在数据类型方面, 富媒体形态 (如文本、图像、音频、视频等) 成为主流.其中, 音频数据、视频数据具有时间连续性特点.由于计算机中通常采用编码、采样等方式表示富媒体数据, 因而数据变换成为大数据分析的重要内容.常用的数据变换方法包括文本处理的向量空间模型 (VSM) [23]、主题模型 (topic model) [24], 图像处理的尺度不变特征转换 (SIFT) [25], 音频处理的短时傅里叶变换 (STFT) [26], 视频处理的时空兴趣点检测 (STIP) [27]等方法.近年来, 随着大数据平台化运算能力的显著提升, 基于深度神经网络的相关方法进一步发展, 并在富媒体数据变换上展现出良好的应用效果和发展前景.例如, 用于文本数据的单词嵌入 (word embedding) [28], 用于图像数据的卷积神经网络 (CNN) [29]和胶囊神经网络 (capsnet) [30], 用于音视频等具有时间序列特征数据的循环神经网络 (RNN) [31]、长短时记忆神经网络 (LSTM) [32]等.其他较新的数据变换方法还包括多层感知机 (MLP) 、自学习编码器 (AE) 、受限制玻尔兹曼机 (RBM) 、深度语义相似模型 (DSSM) 、神经自回归分布估计 (NADE) 、生成对抗网络 (GAN) 等[33,34].</p>
<br>
<h3 id="23-使能创新">2.3 使能创新</h3>
<p>大数据驱动的一个重要含义是大数据使能 (enabling) .大数据能力主要包括大数据战略、大数据基础设施、大数据分析 (6) 方法与技术等.大数据使能是指大数据能力带动的价值创造.例如, 从研究和应用范式角度看, 外部嵌入是一种使能情形, Y'=f' (X') 中, 大数据能力通过自变量X'体现, 创造的价值通过因变量Y'体现, 使能转换方式通过f'体现.从研究和应用情境角度看, 企业的价值创造可以体现在其价值链的环节上, 既包括价值链的主环节及其活动, 也包括价值链的支持环节及其活动[35].在企业内外部大数据环境下, 企业使能创新是通过构建大数据能力, 带动新洞察、新模式、新机会的发现, 进而推动产品/服务创新和商业模式创新, 以实现企业的价值创造 (如图4所示) .</p>

<figure >
    
        <img src="img/4.jpg" width="800" />
    
    
</figure>

<p>综上所述, 大数据驱动范式通过技术增强引入了新视角, 进而推动了新型变量关系、要素机理和理论模型构建, 并提升了大数据使能创新的价值创造.这对于应对新型商务形态的进一步发展机遇和挑战具有重要意义.简单说来, 新型商务可以通过两个阶段予以描述.第一个阶段称作数据商务 (digital business或data-centric business) , 即“数据化+商务分析 (BA) ”.此时通过细化数据粒度使得商务要素的“像素”显著提升, 并在此基础上进行商务分析, 针对不同管理场景和层次进行“成像”和决策.第二个阶段称作算法商务 (algorithmic business) , 即“商务分析+”.此时, 在已有的商务高像素基础上, 成像算法成为关注重点, 旨在获得面对新模式、新业态、新人群[3]的发展策略和竞争优势.这里, “商务分析+”包括BA算法创新和BA使能创新.</p>
<p>近年来, 人工智能 (artificial intelligence, AI) 的研究和应用得到了快速发展, 并受到各界的广泛重视.人工智能自二十世纪50年代以来的发展起起伏伏[36], 虽然在相关思想、模型和方法等方面取得了许多重要进展和成果, 但是由于常常受限于数据基础以及计算能力的不足, 其学习、进化以及推理等方面的能力难以得到发挥, 应用效果也受到影响.直至进入大数据时代, 人工智能的许多成果得到了工程化和产品化实现, 开始在深度和广度上渗透到社会经济活动中, 并引发人们对于未来产业和人类生存的遐想和担忧.机器人和智能产品早期用于替代人类简单重复体力性工作, 现在则可以开始尝试用于替代不少复杂并具有智力的工作, 诸如围棋[37]、翻译[38]、绘画[39,40]、作曲[40]、作诗[41]、无人驾驶[42]、人脸识别[42]、意念控制[43,44]等等.人工智能在管理领域的应用也初见端倪, 比如财务机器人[45]、自动金融交易[42,45]、竞争智能[46]、客户服务[45,47]、人力资源管理[48]、市场营销[42,45]等等.毫无疑问, 人工智能将在新型商务中发挥着越来越重要的角色.另一方面, 伴随着从弱人工智能到强人工智能乃至超人工智能的进阶, 人们对于人工智能应用在隐私和伦理方面的担忧也在不断加重[49].此外, 人工智能理论和技术发展也面临众多挑战 (如“黑盒子”特点、学习机理、语义理解等) , 这些对于强调“关联+因果”的管理决策领域尤为重要.</p>
<p>最后, 管理学是一门融合了“科学”与“艺术”的学科.在大数据背景下, “科学”层面的可测性、程式化和可重复性等要素正在越来越多地被数据和算法表达;而“艺术”层面的情感、心理以及认知等要素也开始被不断“量化”, 包括借助一些感知技术 (affective technologies) (如眼动、脑电技术等) .未来的管理学在探究组织内外“任务”与“人”有机结合的过程中, 数据驱动特征将愈加凸显, 相关范式转变也将进一步深化.</p>
<br>
<h2 id="全景式page框架">全景式PAGE框架</h2>
<p>全景式PAGE框架是融合大数据特征和重要研究方向的要素矩阵, 旨在刻画大数据驱动的“全景式”管理决策框架.全景式PAGE框架具有三个要件:大数据问题特征、PAGE内核、领域情境 (如图5所示) .大数据问题特征涵盖粒度缩放、跨界关联和全局视图, 并作为管理决策背景下的特征视角映射到研究内容方向上.PAGE内核是指四个研究方向, 即理论范式 (paradigm) 、分析技术 (analytics) 、资源治理 (governance) 以及使能创新 (enabling) .领域情境是指针对具体行业/领域 (如商务、金融、医疗健康和公共管理等) 进行集成升华.</p>

<figure >
    
        <img src="img/5.jpg" width="800" />
    
    
</figure>

<p>围绕PAGE内核, 在大数据问题特征映射下可以形成一个4×3的要素矩阵.在理论范式 (P) 研究方向上, 重点关注管理决策范式转变机理与理论.传统的管理决策正在从以管理流程为主的线性范式逐渐向以数据为中心的新型扁平化互动范式转变, 管理决策中各参与方的角色和相关信息流向更趋于多元和交互.概括说来, 新型管理决策范式呈现出大数据驱动的全景式特点.进而, 由于全景式的多维交互动态性以及全要素参与特点, 在研究上需要采用新型的研究范式 (即大数据驱动范式) .具体说来, 在粒度缩放方面, 需要决策要素在宏观和微观层面可测可获;在跨界关联方面, 需要引入外部要素并形成内外要素互动;在全局视图方面, 需要多维整合并能够针对不同决策环境进行情境映现和评估.</p>
<p>在分析技术 (A) 研究方向上, 重点关注管理决策问题导向的大数据分析方法和支撑技术.在粒度缩放方面, 需要数据的感知与采集, 并能够在不同维度和层次上进行分解与聚合;在跨界关联方面, 需要捕捉数据关系及其动态变化, 并能够进行针对多源异构的内外数据融合;在全局视图方面, 需要体系构建和平台计算能力, 并能够形成各类画像以及开展智能应用.</p>
<p>在资源治理 (G) 研究方向上, 重点关注大数据资源治理机制设计与协同管理.在粒度缩放方面, 需要进行资源要素的数据化, 并明确数据标准和权属;在跨界关联方面, 需要刻画资源流通的契约关系, 并形成有效协调共享模式;在全局视图方面, 需要建立资源管理机制, 并制定组织的资源战略.</p>
<p>在使能创新 (E) 研究方向上, 重点关注大数据使能的价值创造与模式创新.在粒度缩放方面, 需要提升业务价值环节的像素, 并把握业务状态;在跨界关联方面, 需要梳理业务逻辑和联系, 并辨识影响业务状态的因果关系;在全局视图方面, 需要提升大数据使能创新能力, 并促进组织发展与价值创造.</p>
<p>围绕领域情境, 可以对PAGE相关研究和应用进行凝练、整合和升华.以NSFC大数据重大研究计划集成平台构建为例, 一般来讲, 集成平台由三个部件组成, 分别是平台体系、内置部件、整合部件.作为简化示例, 对于商务领域集成平台, 平台体系由一个商务管理决策相关的数据池, 以及相应的数据管理和应用管理平台系统 (包括模型、方法、工具库) 等组成;内置部件由针对特定行业 (如汽车) 和特定领域 (如营销) 的研究成果及示范系统组成;整合部件由商务领域内 (不限于内置部件领域) 其它相关项目成果在平台体系框架下经过提炼升华汇集而成.对于金融领域集成平台, 平台体系由一个金融监测预警服务平台, 以及相应的数据管理和应用管理平台系统 (包括模型、方法、工具库) 等组成;内置部件由针对特定行业 (如互联网金融) 和特定领域 (如征信评估、风险预警等) 的研究成果及示范系统组成;整合部件由金融领域内 (不限于内置部件领域) 其它相关项目成果在平台体系框架下经过提炼升华汇集而成.</p>
<br>
<h2 id="结束语">结束语</h2>
<p>面向管理决策研究和应用的大数据驱动范式通过技术增强引入了新视角, 进而推动了新型变量关系、要素机理和理论模型构建, 并提升了大数据使能创新的价值创造.这对于应对新型商务形态的进一步机遇和挑战具有重要意义.此外, 全景式PAGE框架刻画了在粒度缩放、跨界关联和全局视图特征视角映射下的理论范式、分析技术、资源治理、使能创新等重要研究方向.</p>
<p>附注:国家自然科学基金委员会“大数据驱动的管理与决策研究”重大研究计划是一个具有统一目标的项目集群, 旨在充分发挥管理、信息、数理、医学等多学科交叉合作研究的优势, 以全景式PAGE框架作为总体思路框架, 坚持“有限目标、稳定支持、集成升华、跨越发展”的原则, 围绕学科领域趋势、理论应用特点, 注重基础性、前瞻性和交叉性研究创新.自2015年底至2017年底, 此重大研究计划部署了包括培育项目、重点项目和集成项目等一系列项目.其后续的项目部署将在全景式PAGE框架下, 进一步突出凝练、整合与升华, 强调与总体思路框架内容的契合性和贡献度.</p>
<p>本文素材部分来自国家自然科学基金委“大数据驱动的管理与决策研究”重大研究计划相关的系列研讨.由衷感谢不同学科领域专家学者 (包括NSFC大数据重大研究计划指导专家组、顾问专家组、管理工作组等专家学者) 的真知灼见和思想贡献!</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 周翔：作为法学研究方法的大数据技术</title>
      <link>https://textdata.cn/blog/big_data_method_in_law_research/</link>
      <pubDate>Tue, 07 Dec 2021 12:43:10 +0600</pubDate>
      
      <guid>/blog/big_data_method_in_law_research/</guid>
      <description>本文旨在回答大数据技术如何才能成为法学研究的方法，如何与法律实证研究、规范研究这两大传统的方法实现有效的互动。</description>
      <content:encoded><![CDATA[<blockquote>
<p>本文原载《法学家》2021年第6期。</p>
<p>作者 | 周翔（法学博士，浙江大学光华法学院特聘副研究员）</p>
<p>来源 |《法学家》2021年第6期“主题研讨二：跨学科法学研究的方法论检视”栏目。</p>
</blockquote>
<p>因篇幅较长，已略去原文注释。</p>
<h2 id="目录">目录</h2>
<ul>
<li>一、前大数据时代的法律实证研究</li>
<li>二、大数据技术运用的各个节点</li>
<li>三、大数据技术对于实证研究而言是一场接力</li>
<li>四、大数据技术对于规范研究而言是一种助力</li>
<li>结　语</li>
</ul>
<br>
<p>　　中国法学正在迎来“大数据”“人工智能”的研究热潮。“<strong>数字法学</strong>”“<strong>计算法学</strong>”等各类新词汇层出不穷，但研究者们却并不都是在同一内涵和外延下使用这些概念。因此，本文有必要在开篇之初先设置一套分类法，对既有的学术研究成果加以归类，从而明确本文在既有研究中的坐标位置。笔者将既有的相关研究分为如下四大类：</p>
<p>第一类称作“<strong>学科论</strong>”，此类研究看待技术的视角最广，其目标是希望厘清法学+数字技术的最大学科边界；<br>
第二类称作“<strong>对象论</strong>”，是把“大数据”“人工智能”等视为法律规制和法学研究的对象，此类文章占了当前本领域研究成果中的大多数；<br>
第三类称作“<strong>工程论</strong>”，这类研究关注到数字技术可以被引入执法、司法等各个场景当中，赋能法治的各环节；<br>
第四类才是“<strong>方法论</strong>”，是从学术研究方法的视角看待大数据技术，探讨其能为学术活动提供哪些新契机。本文的研究侧重于第四类，亦即集中讨论大数据分析技术作为学术活动的工具，能够给法学研究提供何种新方法。</p>
<p>　　本文旨在回答大数据技术如何才能成为法学研究的方法，如何与法律实证研究、规范研究这两大传统的方法实现有效的互动。关于此，左卫民在《迈向大数据法律研究》一文（以下简称为“左文”）中较早地提出了“<strong>大数据技术如何作为法学研究方法</strong>”这一命题，在“方法论”层面为将大数据技术引入法学研究当中起到了重要的推动作用。同时，笔者认为，以下方面仍值得进一步探讨：</p>
<p>第一，“左文”中提到“从研究范式看，大数据法律研究可能推动实证研究的跨越式发展，特别是机器学习方式的引入，会使法学研究从法教义学、社科法学和实证法律研究等范式转向数据科学式的法学研究”。“范式”一词在托马斯·库恩（Thomas Kuhn）那里，是指“一个成熟的科学共同体在某段时间内所认可的研究方法、问题领域和解题标准的源头活水”。形成一个范式，是任何一个学科在发展中达到成熟的标志。“数据科学式”的法学研究已经是一种成熟的范式了么？它与“左文”中提到的法教义学、社科法学等既有的法学研究范式之间又是什么样的关系？<br></p>
<p>第二，“左文”中提到“需要将小数据社科研究中已普遍运用和相对成熟的数据分析方法……运用到大数据分析中”。社会科学中开展定量研究，是以统计学原理为根基的，这和大数据技术联系密切的机器学习方法之间有何差异？法学研究又能够吸取大数据技术中的哪些优势？<br></p>
<p>第三，左文中还提到“一些大数据法律研究缺乏必要的问题意识，主要是描述式研究，沦为‘调查报告式’的数据展示”。这涉及的是大数据分析技术应用现状的问题。<br></p>
<p>上述三个方面，围绕大数据技术对法学研究的主要贡献展开，清晰地定位了大数据技术在既有的法学研究方法体系中的地位。</p>
<p>　　上述延伸和思考，一方面是对话，另一方面是形成本文研究的路径。本文的基本立足点在于：<strong>大数据分析技术如果对法学研究有影响的话，那么主要是使得法学更加社会科学化、更重视实证的方法、更习惯从大数据中探索法律世界的规律</strong>。这些贡献决定了大数据技术在法学研究方法体系中的位置，其既是对以统计学为基础的法律实证研究的接力，更是对以法教义学、立法论研究为核心的传统规范研究的助力。在这一基本立场之下，本文首先对前大数据时代的实证研究方法、研究对象作一番回顾；接着结合笔者担任大数据分析师的经历，针对技术细节作梳理；然后在这些基础之上，就如何实现大数据技术、定量的实证研究、规范的法教义学研究三者间的互动提出一套初步的方案；最后，鉴于当前学界在相关概念上存在较多的混用现象，本文就此作一些观点上的澄清。</p>
<br>
<h2 id="一前大数据时代的法律实证研究">一、前大数据时代的法律实证研究</h2>
<p>　　关于什么是实证研究，目前虽然尚无统一定论，但在“基于模型和数据的方法”这一点上则有比较明确的共识。包括法学在内的各个社会科学学科之所以都对定量方法感兴趣，是因为“定量的方法，乃一切科学进程的核心”。只要是跨越个案地探寻案件平均水平、共性特征、要素相关性的研究，都将被本文纳入前大数据时代法律实证研究的范畴。下文对此类法律实证研究的回顾和梳理，主要是从研究的方法、测量的工具、样本量的大小等三个维度展开。</p>
<h3 id="一以统计为主的研究方法">（一）以统计为主的研究方法</h3>
<p>　　从研究方法来看，既有的法律实证研究均奉统计学为同宗。通常认为，统计学的方法有描述性统计和相关性分析两大类。结合法学研究的特点，这里稍作更细致的划分。根据笔者的概括，以往的研究成果主要利用了如下三种方法。</p>
<p>　　方法一：<strong>多案例分析</strong>。对案例的运用，如果不是针对法条进行解释、对立法提出建议，那么在笔者看来即为一种实证研究的路径。例如陈杭平关于民事诉讼标的的研究，为案件类型化设定了一套分类标准，即诉讼标的的不同含义，纵轴根据不同学说见解区分为三个版本，横轴以诉讼标的的不同领域或场景为标准。从多个案例中挖掘某些规律性信息，是一种超越个案、试图通过案件类型化获得研究结论的方法尝试。</p>
<p>　　方法二：<strong>描述性统计</strong>。前述的多案例分析，还很难称得上是严格意义上的实证分析。左卫民的一系列文章有力地推动了法律实证研究向前发展，其主要采用的便是描述性统计方法。例如，他关于审判委员会的研究，统计了某地区的三级法院审判委员会委员的审判经验与学历背景，并将讨论的议题细化到宏观指导议题和个案议题，分别进行统计和分析；再比如，他另一份关于中国刑事法律援助的研究，通过调研和考察某省的三家法院，统计了各类型辩护的人数和占比，并由此回答“究竟应当在多大范围内推行并构建当代中国的法律援助制度”这一问题。</p>
<p>　　方法三：<strong>相关性分析</strong>。白建军等人的研究则在描述性统计之基础上，迈向了相关性分析这一相对复杂的层次。相关性研究也被称为推论统计，是将统计学手法与概率理论相融合，对“‘无法整体把握的大的对象’或‘还未发生而未来会发生的事情’进行推测”。白建军等人开始关注引起某一现象的原因，试图建立自变量和因变量之间统计学意义上的相关性，所采用的回归模型主要为多元线性回归、logistics回归等常见模型。例如，白建军迄今为止在中国知网上下载量最高的一篇论文，是通过相关系数、多元线性回归来研究犯罪率的社会成因；又如，李本森关于速裁程序的研究，则以诉讼效率、量刑均衡和诉讼权利作为其关心的因变量，采用的是多元线性回归模型。</p>
<p>　　在线性回归、logistics回归这两大常用的回归模型之基础上，法律实证研究方法也根据议题之需推陈出新。首先表现为统计方法趋于高级，例如白建军在其一贯的相关性分析之基础上，对无法观察的变量作了科学处理，将中国民众的刑法偏好这一因变量，拆解为犯罪圈大小、刑罚轻重、罪刑均衡程度等三个可通过问卷调查测量的因子，以打分取值的形式获得该变量的数值；其次是结果呈现方法上的创新，例如单勇关于盗窃罪的回归分析结果，用GIS作图的方法实现可视化，选取建筑物、停车场等10类空间因素为自变量，用于说明因变量和空间变量的地理联系；最后是体现在用于分析的软件工具之更新上，过去的法律实证研究以使用SPSS软件为多，而近来的研究很多提及使用了Stata、R等工具。当然，使用更高级的软件进行分析，其效果未必就一定更好，但上述变化至少标志着学者们在工具利用方面的水平提升，且有些回归模型是初阶工具所无法胜任的。</p>
<br>
<h3 id="二多元测量工具并存">（二）多元测量工具并存</h3>
<p>　　测量工具是指获得数据的方法。数据搜集在统计学中是重要的一环，“所有统计数据追踪其初始来源，都是来自调查或实验”。统计学上根据是否为直接获得第一手数据，区分直接来源和间接来源。法学实证研究多以一手的直接来源为主，主要的测量工具包括如下几种。</p>
<p>　　工具一：<strong>问卷调查</strong>。这种测量工具与传统的社会科学方法保持一致，通过设置问题、受调查者回答的方式收集受访者的信息。此种方法在获取受访者主观态度方面效果明显，为学界所常用。例如，程金华在研究过程中为了解检察人员针对检察人员分类改革的认识而发放问卷；胡铭关于司法公信力的研究，通过向社会公众和司法官分别发放问卷，比较和审视“对于影响司法公信力的要素的认知与评判”。</p>
<p>　　工具二：<strong>模拟实验</strong>。“实验大多是对自然现象而言的”，但在近年来的法学研究中也时常出现。司法裁判一般不具有可再现性，同一个案件在真实场景下只可能出现一次。模拟实验是一种对司法裁判过程的模拟再现，借此可发现一些影响裁判结果的变量。目前中国法学界的模拟实验主要是对一些经典案例裁判过程的复盘，以验证某些观点。例如李学尧等人关于案卷材料阅读流畅度与裁判尺度之关系的研究，通过问卷字体大小、是否斜体、是否加重、行间距以及案件数量的变化来操纵阅读流畅度的变化。</p>
<p>　　工具三：<strong>文本摘录</strong>。法律中的文本，其典型形态为裁判文书。在裁判文书大量公开上网后，有人认为数据法学的春天即将到来。的确，裁判文书是记录诉讼过程最终的、有法律效力的、体系最完整的文本。对裁判文书的利用，比如文姬关于信用卡诈骗罪的研究当中有很多维度的信息挖掘，包括审级、行为人出生年等16个变量。文本的种类近年来也出现不少创新，比如习超等人关于证券监管的研究采用的是对上市公司执法事件的披露信息。</p>
<p>　　工具四：<strong>实地/田野调查</strong>。倘若只是单纯采取个案式的访谈记录，则在方法论上一般将之归入定性研究的范畴。但如果是在田野调查中对多个样本进行观察或访谈，采取结构化的方式收集数据，最终对数据进行跨越个案的量化分析，那么也可以被视为实证研究的测量工具之一。此种方式在国内法学研究中不乏实例，比如一份关于当事人法律意识的研究，所主要利用的素材就是在某法院传达室对一百多位当事人进行访谈所收集的数据；再比如左卫民在研究基层法院的财政制度、法官的工作时间分配等问题时，课题组前往实地调研、观察记录收集数据资料。</p>
<p>　　除了采用上述工具之一，实证研究还可以多种测量工具结合、定量和定性方法混用。比如胡铭关于庭审实质化的研究就不仅利用了判决文书，且还通过观摩庭审直播并记录的方式收集数据。</p>
<br>
<h3 id="三万级以下的样本量">（三）万级以下的样本量</h3>
<p>　　实证研究的论文中约定俗成要报告样本量，而之所以特别指出研究所用的样本量大小，是由于样本量直接关系到根据小样本得出的结论能否推及至更大的范围，因此抽样是统计学中很重要的概念。建立一个好样本的关键，是尽量选择最符合总体的样本，如果样本具有代表性，那么表明样本与总体有十分相似的特性，进而可以通过样本预测出总体具有哪种规律。</p>
<p>　　法律实证研究中的样本量过去以百级、千级为主，比如文姬关于信用卡诈骗罪的研究所利用的裁判文书样本有2103份，习超等人对证券监管“旋转门”的研究则采用了7103个监管事件作为样本。样本量大小是个相对的概念，还要考虑“全体”的规模大小。当我们确定研究的问题后，从理论上讲“全体”的范围就固定了。若以裁判文书为测量工具，则有三个层次的案件范围，依次分别是客观真实发生的案件总数、裁判文书网上公开的案件数、用于实际研究的案件样本数。前大数据时代的法律实证研究，是在公开的裁判文书范围内选取一定的样本进行分析，距公开的案件“全体”和真实客观的案件“全体”相去甚远，正因如此，不少研究受到样本代表性不足的质疑。</p>
<p>　　除了抽样和样本的代表性问题外，前述提及的那些研究基本没有交待如何实现样本的数据结构化。根据笔者自身的数据分析经验，以传统方式处理样本耗时巨大。如果以阅读裁判文书并摘取的形式处理样本，那么一天工作8小时也只能阅读20—40份裁判文书，如此来算，处理千量级的裁判文书样本就得花费数月时间。如果再安排专人抽查数据录入的质量，那么工时还得另计。</p>
<br>
<h3 id="四留待提升的不足">（四）留待提升的不足</h3>
<p>　　在研究方法上，相关性分析方法之后难有新的突破；在分析工具上，研究者虽试图推陈出新，但总体变化不大。以构建回归模型做研究为例，中国法学界目前用过的回归算法种类屈指可数。某些研究虽然其方法有一定的创新，比如采用决策树的方法，但又和机器学习的决策树算法相去较远。另一个问题在于分析软件，小样本时代没有使用分析软件的明显障碍，但在大样本时代则要考虑借助的分析工具是否恰当，能否高效运行。上述两个方面，大数据技术都可能给其带来变革。</p>
<p>　　测量工具上，以往较强依赖于社会资源的协调能力。中国法学界当前使用的测量工具中，问卷调查、模拟实验的应用较为普遍，而上述二法的共同局限在于严重依赖研究者的社会资源协调能力。很多研究并不避讳利用了作者的挂职身份、承担横向课题的机会、本省司法资源的便利条件等。就数据获取而言，即使只是选择几个投放点，也要付出很大的成本，且若没有较大经费支持则难以做到。上述列举的那些研究成果，因此往往是知名学者的作品。这也造就了一个怪圈：是先成名后做实证研究，还是因实证研究而成名？而在大数据时代，研究者将在一定程度上不再依靠外部资源的协调能力。</p>
<p>　　以往样本量太小，使得据其发现的规律的代表性不足。前文已经提及，样本量关乎结论的普遍性，统计分析的根本目标在于“推论”。样本量越小，对抽样的随机性要求就越高，而抽样始终是一个难题。造成抽样偏差的成因很复杂，比如抽样空间条目不齐全、抽样单位不正确等，无法穷举。如何克服抽样的难题？换个角度试想一下，研究的样本如果就是公开的“全体”，那么抽样的重要性将被极大淡化，而基于公开的全体案例作分析，在大数据技术的赋能下是能够实现的。</p>
<br>
<h2 id="二大数据技术运用的各个节点">二、大数据技术运用的各个节点</h2>
<p>　　从实证研究的过程来看，在选择议题、提出假设和设计变量等步骤中，数据的收集和分析是大数据技术最相关的两个环节。其中，数据的收集又包括语料的获取、语料转为数据和数据清洗等步骤。本文认为，大数据分析技术正是借助数据的收集和分析这两点，助力法律实证研究向更高阶段发展。从已有大数据分析的实践经验来看，可大致分为以下几个步骤。</p>
<h3 id="一语料的获取">（一）语料的获取</h3>
<p>　　语料获取是应用大数据技术的第一个环节。凡是对立法活动、司法实践有所记录的载体，都可作为实证研究的原始语料。当然，文本仍是当前法律大数据分析主要的语料类型，大数据技术目前还比较难以有效处理图片、音视频等数据。所幸的是，法律文书本来就是记录法律活动最重要的、也是表达相对准确的语言形式。</p>
<p>　　当前的语料获取问题，应重点聚焦于如何便利地获取法律类文书。研究者作为个体要想获得供研究之用的文本，除逐一复制或下载外，还有两条路径值得重视：一是采取网络爬虫的方式，从数据源自动爬取，业内有句话叫作“可见即可得”，其意思是只要有该网站的访问权限，那么便可获得该数据，包括一般性的网页、API资源、文件资源和媒体资源；二是与拥有法律文书数据的公司进行合作，后者一般掌握较为完整的法律文书库。</p>
<p>　　网络上的其他数据资源也不可被忽视。在传统的法学研究中，我们便已看到许多研究者用到法律年鉴、地方志等信息，而此类信息如今已基本实现无纸化、网络化。我们可从以下几个渠道收集与自己研究有关的语料：一是国家及地方各公立机构的官方网站，比如图书馆、档案馆、财政局等行政事业单位；二是各行业的商业机构网站，比如上市公司财务报表的披露网站、各行业的商业情报网站等；三是一些人气活跃的社区论坛，比如在司法公信力、媒体和司法之关系等研究议题中，我们关心的案件舆情就在论坛社区中有丰富的表达。网络资源可有效弥补裁判文书这一测量工具的局限性，因为法治事件的真实场景变量复杂，法律文书只能反映其中的一小部分。</p>
<br>
<h3 id="二语料转为数据">（二）语料转为数据</h3>
<p>　　传统的实证研究是统计学思维，处理的是阿拉伯数字。这一点在大数据的语境下没有本质变化，即机器学习仍然难以根据文本直接构建模型，数据分析的对象仍是结构化数据。遗憾的是，法律领域的语料主要是自然语言，以数字形式呈现、直接可用的信息少之又少。因此，在获取与我们研究议题相关的文本语料后，还将面临如何将语料转为结构化数据的问题。前述提及的那些实证研究成果也用到文本，并主要采取人工摘录的方式进行处理，而大数据获取技术中的文本挖掘技术，通过计算机就可实现数据结构化。</p>
<p>　　将语料转为数据的过程，本质上是将自然语言转化为机器语言。处理自然语言的工具，大致可分为基于概率和基于规则两类。前者是通过人工标注一定的语料，再由机器模型识别剩余部分的语料，得到的是一个结果的分布概率；后者的典型代表是正则表达式，因其结果更为准确，故而成为当前适用广泛的提取方法。学术研究要求较高的准确性，因此基于规则的方法更为现实可取。正则表达式，在技术书中被定义为“一门袖珍编程语言的通用模式表示法，赋予使用者描述和分析文本的能力”，这里也可将其通俗地理解为高级版的关键词检索。正则表达式可将我们关心的、希望提取的某一要素，通过计算机能理解的方式表达出来。不过其具有的缺陷是，如果编写者未能预见同一意思下全部的汉语表达方式，那么该正则表达式也将无法识别出全部包含该意思的文书。</p>
<p>　　语言表达方式虽然具有多样性，但也并非无穷无尽，常见的文字表达类型是可以由正则表达式囊括的。实践中的通常做法为：先人工阅读一部分文书，枚举语言表述的类型→通过正则表达出每种类型→正则遍历文书，筛选出被命中的文书→再次阅读部分未经匹配的文书→优化正则表达式→再次遍历剩余未被命中的文书。多次循环后，正则表达式将会得到很大的改进，信息提取的准确性也会随之提高。数据的获取任务至此完成，这也是大数据技术相较于传统实证研究在技术上的巨大变革之处，即数据的获取不再高度依赖于外部资源的多寡，只要研究者掌握了一定的编程能力，那么就可以从最大的数据资源库即互联网中获取自己想要的各类数据。</p>
<br>
<h3 id="三数据清洗">（三）数据清洗</h3>
<p>　　数据清洗面临两种情形，一种是从文本到数据的过程中存在信息的遗漏，另一种是有些文书信息虽然被提取了出来，但存在错别字或其他不当之处。处理信息残缺的方法，至少有以下几种：第一种是根据信息有残缺的文书编号，追溯至该份文书，人工阅读发现原因并修正提取的方法。这和上文提及的不断优化正则、扩大匹配的范围的做法很接近。第二种是统计学中处理残缺值的传统方法，比如用平均值替代、剔除该样本等。第三种是借助Excel表格中的工具、pandas等第三方库对数据逐一进行修正，通过人工的个别修正使数据回归正常。例如裁判文书中存在诸多错别字、语病等错误需要清洗，笔者曾遇到过某一罪名在裁判文书中，有十余种错误的文字表述、八种不同的“零”写法，这些均属于若无人工预判则机器便无法自动识别的情形。</p>
<p>　　数据清洗，主要面对的是如何处理自然语言中不同词汇的相同意思（同义问题），以及一个词汇在不同背景中有不同的意思（多义问题）。在数据清洗的实践中，可以发现存在如下几种规律：（1）词汇类型有限的数据项，需要清洗的脏数据比较少。例如提取裁判文书中的法院审级，一般文书落款中有“中级”“高级”“最高”等关键词，正则很容易匹配成功；而当鉴别机构的名称时，由于全国各地的命名方式不一，便会出现很多数据空缺需要填补的问题。（2）数据的清洗和人工的投入量基本成正比。无论是用人工标签+机器学习的方法，还是用正则表达式提取，都需要人工阅读并发现数据错误，添加惩罚项或修改正则来减少脏数据。（3）数据清洗要适可而止，因为数据清洗需要耗费大量的人工成本。一些简单且明显的错误，比如审判员人数提取为“2人”，能够及时返查并纠正，但人工清洗全部的脏数据是不可能的。现阶段在计算机还没有能力自查和纠错的情况下，学术共同体对待数据获取环节的准确性只能给予更多的包容。</p>
<br>
<h3 id="四数据分析">（四）数据分析</h3>
<p>　　若要从数据中产生规律性的知识，则还要依赖于数据分析的工具。以统计学思维看待数据分析的方法，主要有描述性分析和相关性分析两大类。</p>
<p>　　大数据时代的分析环节，仍有运用描述性统计的必要。大数据时代虽然样本量呈几何式增长，但试图把握司法实践之一般规律的需求并未改变。而描述性分析是最有利于把握案件整体情况、聚焦重点案件类型的方法。</p>
<p>　　关于相关性分析，大数据技术主要借助于机器学习，“根据训练数据是否拥有标记信息，学习任务可大致划分为两大类：‘监督学习’和‘无监督学习’”。有监督的机器学习，其建模方法为研究者提供了一种新思路，即把样本一分为二，区分训练集和测试集，用训练集拟合参数，用测试集评估数据模型的准确性。机器学习与统计学中的回归建模方法之间最大的一点差异，在于检验模型参数的可靠性上，机器学习采用交叉检验的方法，而统计学上则主要采用假设检验的方法，其典型者如t检验。无监督的机器学习事前不作标记，通过对无标记训练样本的学习，来揭示数据的内在性质及规律。以统计学视角来看，即事前不设置因变量。通过该项技术，可以从中探索我们所关心的研究议题，从而确定因变量。</p>
<p>　　用大数据的机器学习方法建模，最好采取Python语言编写程序。该语言可调用各类第三方库，statsmodels、scikit-learn等第三方库中已内置有大部分的常用算法，调用接口即可满足现有的研究需要。在大数据时代，获取的数据样本量将轻松突破万级，例如研究某些社会高度关注的案件的网络舆情，从微博、论坛中获取的评论数甚至可以很轻松地突破百万条。自己编写程序做大数据分析，在面对大样本时，能更好地满足个性化的研究需求。</p>
<br>
<h3 id="五前述流程的局限">（五）前述流程的局限</h3>
<p>　　<strong>大数据技术并非没有局限性，它是一把双刃剑</strong>。“技术不是敌人，我们的敌人是寄居在技术里的浪漫又革命的‘解决问题兽’”。因此，人们要保持独立思考的能力，用批判性的眼光去接受、采用技术。在看待大数据技术在法学研究中的应用前景时，同样应重视可能存在的一些局限性。</p>
<p>　　<strong>第一个局限在于，大数据技术更难以关照到个案的细节之处</strong>。这也是左文中提到的大数据时代的一个特点，即样本量变大后，做不到人工查看每个样本。此为一个明显的缺陷。大数据分析所反映的只是数据间的相关性，但要解释此种相关性，还要依靠外部其他角度的素材。当回答为什么出现数据间存在显著相关性这一问题时，特别离不开对一些典型个案作具体的剖析。如前所述，大数据技术不再要求人工逐一阅读，便可将文本信息转为数据信息，但是小样本时代，逐一阅读案例，恰恰是发现有价值细节、启发研究灵感的历程。因此，大数据时代的法学研究，在用数据说话的同时，仍然少不了要深入到对典型个案的阅读中。</p>
<p>　　<strong>第二个局限在于，计算机技术的使用门槛较高，许多研究者面临着计算机技术有关知识匮乏的挑战</strong>。首先，在数据获取上，研究者最好能够掌握一些网络爬虫的技巧。从笔者的实战经验来看，爬取一般网站论坛上的数据相对容易，但爬取微博、微信公众号等数据就比较困难，这是因为后者设置了很多反爬虫的措施。再以法学研究常用的裁判文书为例，中国裁判文书网为确保正常访问，采取了一系列的加密措施，而这意味着研究者根本无法一劳永逸地解决数据获取的问题。其次，当前在研究成果发表时，法学期刊通常并不要求同步公开研究所依据的数据样本，也就是说，同行无法获知所采集的数据库详情、数据清洗的程度等。这是大数据法学研究早期阶段的特点。在大数据法学研究的成熟阶段，各研究者可能反复使用同一批大数据，并有一系列量化的模型衡量指标。</p>
<p>　　<strong>第三个局限在于，机器学习所用的部分算法，在变量参数和影响路径的可解释性上，不如那些简单的统计学算法</strong>。这部分是由于一些自身原理所造成的，比如机器学习中可能嵌套多层级函数，其目的是提高模型的拟合度。以神经网络的一般模型为例，有（d+l+1）*q+l个参数需确定，d、l、q分别代表输入、输出、隐层的神经元个数，神经网络的学习过程，就是根据训练数据来调整神经元之间的连接权，即参数值。这还只是一个隐层的情形，“容量”越大的深度学习，参数就越复杂，对法学研究而言的可解释性也越弱。如果认为法律实证研究主要是社会科学意义上的追求现象间相关性的分析，那么越是过程复杂的机器学习算法，越不能透过模型发现变量间的关系。</p>
<p>　　综上，笔者对待大数据技术的整体态度是，获取更大规模、更多类型的数据，对外部资源的依赖程度降低，是其最主要的贡献点，同时也要警惕研究过程中脱离个案细节、技术门槛提高、复杂模型的可解释性弱等风险。有效化解上述风险的策略包括：在跨越技术门槛上，可考虑借鉴其他学科团队式研究的模式，吸纳技术人员参与，改变过去一些法学期刊所认为的合署论文便有“搭便车”嫌疑的前见；在克服脱离个案细节这一问题上，则可以多采取混合研究的方法，即定性的方法和定量的方法相结合，实证研究和规范研究相结合；在数据分析时的算法选择上，则应尽可能选择一些原理简单、可解释性强的算法。</p>
<br>
<h2 id="三大数据技术对于实证研究而言是一场接力">三、大数据技术对于实证研究而言是一场接力</h2>
<p>　　大数据技术对于实证研究而言有一种接力的价值，两者的共性大于差异。大数据技术主要应定位于加强实证研究的某些环节，但并不改变实证研究基本的方法论框架。本文认为，大数据技术的接力作用，主要体现为：（1）降低了数据获取的难度，作为本文第一节中介绍过的那五种测量工具之外单独的一种数据获取途径，以网络爬虫、文本挖掘为代表的大数据技术，在获取数据上具有时间成本和经济成本更为低廉的优势。（2）加强了描述性统计的能力，适合探索性的量化研究。若对实证研究作描述性分析和相关性分析的二分，则大数据技术更擅长概览式地描述研究对象。（3）拓展了可量化研究的议题，使得某些议题的论证更加充分和有说服力。</p>
<h3 id="一拓新数据获取的重要渠道">（一）拓新数据获取的重要渠道</h3>
<p>　　任何定量研究均离不开信度可靠、效度可行的数据来源。<strong>对大样本的追求，在统计学上称为“一致性”，费希尔（Stanley Fischer）用数学公式说明了“你得到的数据越多，你计算出的统计量越有可能接近参数真值”</strong>。在本文前一节的大数据技术应用详解中，所提及的第一步便是大数据的获取技术，若能掌握大数据的获取技术，或者吸纳有相关技术能力的合作者参与，则将大大拓展数据获取的渠道，互联网将成为一种新的测量工具。本节结合一些国内外较新的关于应用大数据技术的文献，深入探讨互联网这一大数据的来源，以此说明大数据技术在获取数据时的优势。</p>
<p>　　首先，把互联网视为数据获取的来源时，主要是将互联网视为一个“知识库”。互联网沉淀了人类活动的大量数据，其中一些是与法律有关的行为数据，例如裁判文书是对司法过程和结果的一种记录，网友针对某一热点案件的留言是司法民意的表达。这些数据的特点是它们的产生最初并非为了供研究之用，故而只能提供有限的数据项，研究者需迁就网络数据可用的数据维度进行研究设计。利用裁判文书开展大数据分析就十分典型，裁判文书的数据维度并不是为研究所设计的，因此在选题时，就要充分考虑裁判文书中所体现的信息是否足够用于回答该问题、有无其他数据源可作补充。</p>
<p>　　其次，进一步拓宽数据获取的思路，还可以把互联网视为形成数据的“实验室”和“协作平台”。大数据技术可以把互联网作为提问数据和实验数据的来源，即前述实证研究测量工具中的实验方法、问卷和田野等方法也可以在互联网中大规模使用。通过网络发送调查问卷，在学术界目前已经有一些成功的研究案例。例如，在一项针对累犯成因机制的研究中，通过给刑满释放的研究对象发放智能手机，大数据采集平台每天向研究对象发送问卷收集数据，并与定位数据、短信数据等数据源相结合，分析再犯罪的成因机制。借助互联网还可以开展随机对照实验，例如一项在二手交易网络商城开展的实验，通过在商品详情描述中改变卖家手持商品的手背肤色、手臂是否有文身、出价和商品介绍的质量等变量，分析这些变量与商品成交价的相关性，从而实证分析商品交易中存在的种族歧视问题。甚至还可以设计一个研究任务分包的网站，将数据的收集、标注等任务进行拆解，让更多的人参与到结构化数据库的建设中来。例如在一份关于国外政党之竞选政策立场的研究文献中，研究者事先将政党宣言作出类型化的定义，然后在网站上分包，最终从1500名工人处收集到20万条的分类数据，而分类的结果经过事后的验证，与专家分类的结果高度吻合。此类对文本、图片的信息采集和分类工作，如果能够分拆为不需要大量专业训练且答案较明确的任务，那么通过互联网的方式，就可以实现低成本的大数据采集和处理。</p>
<br>
<h3 id="二提高实证研究的描述分析能力">（二）提高实证研究的描述分析能力</h3>
<p>　　社会科学所立足的成熟的研究范式，目前仍是提出假设、并用定量的统计方法加以验证的过程，这一套研究的基本方法在引入大数据技术后，并没有发生实质性改变。在数据分析的描述性和相关性之二分法中，大数据分析技术主要加强的是描述性部分，相关性分析仍主要沿用实证研究中倚赖的统计学算法。要想用好大数据分析技术，还应注意区分商业利用和学术研究的不同侧重点，商业领域的分析技术，不一定都能直接迁移至法学研究中来。</p>
<p>　　首先，大数据分析技术主要提高了研究对象的整体描述能力。实证研究中的描述性分析，针对研究对象设计变量，统计平均值、方差等。除这些外，大数据分析技术还有其他可供选择的方法，比如通过词频的计算提炼文本的关键词、通过情感分析的技术反映某些文本的正负情感及强度、通过文本摘要的技术浓缩海量文本的内容，分析的结果可以用词云、动图等多样的可视化方法来呈现。这些技术与实证分析中的描述性分析非常接近，只是起到丰富描述分析工具箱的作用。</p>
<p>　　其次，现有以统计学为基础的相关性分析，所用的算法仍将保持主流地位。统计学中最基本的线性回归、对数回归等模型，仍是当前最为成熟、较适合社会科学研究使用的方法。这并非法律实证研究特定阶段的现象，例如对美国在政治学、社会学领域最权威的6本期刊于2001—2010年间发表的实证研究论文所采用的方法进行统计后发现，最小二乘法（OLS）和logit回归的方法之和占比最高，达到六成。作为大数据分析的主要技术，机器学习在吸收统计学的基本算法后，通过模型的嵌套演变出神经网络、深度学习等高级算法，同时损失了算法的可解释性。而可解释性的本质是输入变量（即自变量）的参数、影响输出变量（即因变量）的路径透明可见，机器学习中的很多算法，在这方面其实不如过去实证研究中常用的统计学算法。</p>
<p>　　最后，应清醒地认识到，某些大数据分析技术之所以难以引入到实证研究中，是因为大数据技术的发展动力来自商业市场的需求，其初衷不是为学术研究而开发的。因此，要区别大数据技术在工程领域和在学术领域的使用差别。工程领域要求大数据模型有较强的结果预测能力，不太重视输入变量与输出结果间发生联系的路径。这使得技术开发的着力点在于如何能够准确预测未来，例如市场中多款量刑辅助的产品提供给办案人员的，是一个案件未来可能判处的刑期结果，而不是提供充分的说理。而学术研究更关心法律现象背后的社会成因机制，希望揭示出现象背后的原因。上述二者虽有共同点（进行精准预测的前提，也要有一个基于历史案件的模型），但考核模型表现优劣的标准是极为不同的。知晓此种差异后，研究者才能对当前眼花缭乱的大数据分析技术有所甄别，优先选择那些具有较好可解释性的机器学习算法。</p>
<br>
<h3 id="三加强某些议题的论证力度">（三）加强某些议题的论证力度</h3>
<p>　　数据源和样本量的扩大，分析能力的增强，使得某些研究议题有机会变换新的角度、充实更有力的论据、得出更有说服力的结论。大数据技术作为一种方法并不直接产生新议题，但是能够增强旧有议题的论证能力，为原先难以量化研究的重要议题开启新的篇章。本节选取“法治中国”这一研究议题，尝试构想一个引入大数据技术后的学术发展新空间。</p>
<p>　　“法治中国”在近年来备受关注，是我国法学研究中的一个重要议题。一般认为，“‘法治中国’的内涵比‘法治国家’更加丰富、更加深刻、更具中国特色”，关于“法治中国”的主体、客体、竞争力等，都是“法治中国”之科学含义研究中的重要子课题。“法治中国”同时是一个有待进一步发展的议题，自党的十八届四中全会提出该口号后，关于“法治中国”的核心价值和精神元素是什么、具体的模式如何这些问题，虽然已经有一些研究成果，但还要继续丰富其内涵。在一些知名法学家的带领下，亟待更多法学青年学者的跟进，特别是作为一个与我国法治实践紧密联系的议题，“法治中国”应首先从国家、社会的各个实践侧面做出事实的归纳。</p>
<p>　　大数据技术可以在归纳中国法治实践中发挥大作用。具体可勾连几个看似不相关但实则联系密切的议题。一是近年来强调“中国问题”的学术反思。2011年举办的“中国法学研究之转型”研讨会上，诸多学者曾呼吁法学研究范式应该转变，认为“当前对中国特有的问题关注不够，缺乏中国问题意识”。具体而言，中国问题是在中国的政治建构、区域发展的极不平衡、社会在转型期中的急剧变化、社会治理资源的多元化等背景下形成的。本文认为，地域间、时间跨度中的中国法治实践差异，可通过大数据的时间序列、地理坐标图等各种形式予以呈现，法学研究要逐渐习惯于用数字化的方法发现并解释中国法治实践中的问题。二是与大数据技术直接关联的“法治评估”，这是关于立法、执法、司法等各领域的评估，其最大特色在于将指标构建技术和统计方法作为工具。笔者认为法治评估的相关研究，主要不在于实现地区间法治状况的可比性（这的确是提出法治评估的原因之一），而是旨在强调各国治理结构的差别，总结不同国家间某问题的不同法治方案。在西方学界过往的法治评估中，“所有实行西方政治制度的国家的得分必定高”。法治中国的研究要想有说服力地破除上述迷局，既要讲道理，更要摆事实，特别是利用好大数据所呈现的事实。</p>
<br>
<h2 id="四大数据技术对于规范研究而言是一种助力">四、大数据技术对于规范研究而言是一种助力</h2>
<p>　　法学实证研究和传统的规范研究间如何衔接和对话，是一个困扰研究者和期刊编辑的共同问题。有期刊编辑抱怨说，很多看似眼花缭乱的定量研究，最终得出的结论却不那么新奇，其言外之意是不需如此费劲，读者也早已知道这样的结论。还有学者坦言，实证研究和传统的规范研究间缺少对话，存在“平行线”难题，法学实证研究存在“叫好不叫座”的现象，即便高质量的实证研究，其被引用率也不高。数字法学时代到来后，上述问题能否有所改善？笔者以为，与其勉为其难地与规范研究直接对话，不如以“提供给规范研究一定启示”的姿态，定位大数据技术的贡献。此种贡献主要是便于研究者更自主、低成本地了解法律实践的运行状况，它是一种助力的功能。</p>
<h3 id="一拓宽了解释论的问题边界">（一）拓宽了解释论的问题边界</h3>
<p>　　法律规范之所以需要解释，其原因在于“制定法的真实含义不只是隐藏在法条文字中，而且隐藏在具体的生活事实中”，生活事实的不断变化，使得法条一直有予以解释的必要性。换言之，这是一种来自司法实践中法条适用的困难所延伸出来的需求。但是，法教义学的规范研究，其传统重镇在高校，科研人员的作业模式与司法实践间隔较远，真正熟悉办案一线的学者并不多。这使得他们在发现哪个法条的哪个关键词存在司法适用困难、故而具有研究必要性上颇费周折。而哪怕是具有司法实践经验的研究者，在这个问题上的表现往往也好不到哪里去，因为他们的经验只是来自直接或间接经办过的案件，是一种主观的、个案式的感受。司法大数据的引入，有望改变上述局面。开展规范研究的学者可通过多个地区的法律案件文书，把文本向数据转换、提取文本背后的有用信息，进而全面获得实践中的裁判观点。波斯纳（Richard A. Posner）对此有过恰当的评论，他认为“法律决定和教义全都由事实驱动，而不是由理论驱动”。</p>
<p>　　首先，法律大数据所挖掘的信息，为规范研究提供了问题意识，为解释设定了起点。规范之所以需要解释，是因为存在疑义。此种疑义并非凭空而来，而是在法律的具体适用中凸显。在过去，此种凸显主要依靠典型案例的被发现而引起学术界的重视。如今，大数据技术的兴起，缩短了该种疑义被发现的进程，并克服了主观选择案例的片面性。这是因为，研究者可不再依赖于司法机关筛选出的指导案例，或者主观随意地挑选案件，而是通过公开的裁判文书进行全样本的大数据分析，挖掘出研究者所关心的司法实践的某一侧面情况。在评价中国的法教义学之缺陷时，有学者指责在中国看不到“法学与司法之间的深入对话”，进而强调中国学者应当虚心、耐心和诚心地向中国法官学习。面对面交流自然是学习的方式之一，但其成本太高。更有效的方式是跨越个案地、基于海量样本地分析法官所写的裁判文书（裁判文书是法官裁判观点的浓缩精华）。</p>
<p>　　其次，大数据方法赋能后的实证研究，为研究者提供了法律概念的社会语境。解释的最终目标是达致“裁定之案件获得公平的处理”，这种公平处理首先要具体化为探寻某一规则的立法目的。目的解释在某些学者眼里是指“探求法律在今日法秩序的标准意义”。那么，今日法秩序的理想图景又从何获知？学术研究者、法律适用者面临探寻这一出处的难题。以往的学理解释，一般是从部门法的基本价值出发解释法条，例如刑法的解释总是要考虑罪刑法定、罪刑均衡、法益保护、保障人权等，又如诉讼法中强调程序参与、诉讼效率、纠纷解决等。在学理解释者看来，这些基本就是衡量解释是否恰当的主要标准。而在具体法律适用者（例如面临个案裁判需要的法官）那里，还可能有其他社会、经济甚至政治的因素要加以考量。大数据的实证分析技术，为解释这些“关键词”提供了上述维度的信息参考，例如可利用大数据分析某个条款在不同案件背景下的不同解释结论，这些背景包括年代、当地的社会经济背景、原被告双方的身份等。以往的实证研究当中并非没有此类尝试，但毫无疑问，在大样本中分析裁判观点的社会语境，所得出的结论将更具有普适性。</p>
<p>　　当然，法教义学同样也给大数据分析以有价值的课题，规范研究者可以将其感兴趣的问题传递给法律大数据的分析者。如此一来，“这些学科的研究对象和知识兴趣就受到教义学的影响了，或者，也会引发交叉学科的研究课题的产生”。</p>
<br>
<h3 id="二为立法论提供效果评估工具">（二）为立法论提供效果评估工具</h3>
<p>　　2011年3月，时任全国人大常委会委员长吴邦国在十一届全国人大四次会议第二次全体会议上宣布“中国特色社会主义法律体系已经形成”。在此之后，学界有过一种观点，亦即认为我们的学术研究将从立法中心主义转向司法中心主义。另一种更谨慎的观点则认为，就中国特色社会主义法律体系这一宏大工程而言，上述时间节点是一个终点，但更是一个起点。在此后的七八年里，现实更加验证的似乎是后一种观点，即立法并没有消退，经济发展、社会转型向法律制度提出了新的要求。实证研究亦表明，立法中心主义的研究氛围始终存在。既然立法论的研究从未消失，我们更应当重视此类研究推动的立法质量和效果。法律体系形成及其规模的持续扩大，并不表明法律体系已经完备或能够自动产生实效，更不意味着立法必然合乎社会需要。就立法进行事前和事后的评估，这不只是立法机关的工作职责，同时也是借此反思立法论研究的良好契机。</p>
<p>　　这里主要探讨立法评估的方法，重点考察大数据技术是否有助于提升立法评估的广度和精度。当前的立法评估方式包括征集公众意见、问卷调查、实地走访等。例如在一份对地方法规的评估中，其研究者主要是在政府机构的主导下，通过第三方评估机构，推动各部门和区县自查、设计和布置调查问卷、文献梳理、重点走访和调研等方式，来完成评估。评估的方法当前“主要运用的是定性分析方法，很少运用定量分析方法及运用影响分析方法”，而这从评估的精确性来讲是不够的。毕竟，现代国家的管理是“数目字”管理，在现代政府的协调性行政控制中，对这些“官方数据”的例行监测是不可或缺的。大数据技术在立法评估中有如下两方面可能的贡献。</p>
<p>　　首先，大数据技术有助于更好地收集来自社会各界的反馈。笔者在研究中访问了全国人大和多个地方人大的网站，发现它们目前都还停留于前大数据时代的意见收集模式。大数据时代很注重对信息的标签化收集和处理，产业界将此称为“打标签”。若能在信息收集环节按照大数据分析的需要进行改造，增加备选的、对立法评估有价值的“标签”供用户勾选，则将有助于提高所收集的信息之质量。而互联网的发展，为利益相关者尤其是公众参与立法评估提供了手段。</p>
<p>　　其次，在立法有关材料的文本清洗和分类中，大数据技术也将提供更多的工具。在各地的立法评估实践中，会面对大量的文字材料。根据某省立法部门的反映，他们缺乏的是针对各方面立法意见的信息汇总和分类的能力。立法机关当前仍然停留于通过传统的“人工看、人工做统计”的方式来获悉各方面的反馈。大数据技术中的词频统计、主题分析、情感分析等相关技术，可以对庞杂的立法建议作清洗、聚类，而这些立法意见的文本处理能力是可积累和可复用的，根据过往的立法意见所构建的筛选模型，例如征集到的立法反馈有哪些意见类型、主要针对立法的哪部分提出意见、意见提出者的身份等，通过机器学习，可以应用于今后对立法意见的高效筛选之中。</p>
<p>　　最后，就立法评估的时间节点而言，大数据技术更能发挥作用的应该是立法后的评估。立法前评估与立法后评估的区别在于，立法前评估主要评估立法的必要性、合法性、协调性和可操作性，而立法后评估则重在考察法律法规对经济、社会和环境的实际影响。影响评估和成本—收益分析是两种不同的方法。成本—收益法是一种法经济学的路径，该方法之所以在立法前评估中经常被使用，是因为在立法之前一切影响都是估计的，并无立法产生的实际影响可以测量。较之事前的估计，关于事后的立法影响，其有关信息显然更多，数据分析也将更有应用的空间。因此，大数据技术和法经济学的方法，在立法前、后的评估中将体现出不同的分工。</p>
<p>　　上述主要讨论立法部门引入大数据技术展开立法评估，此外，大数据技术还应赋能学者的立法论研究，为研究提供检验成效、提示风险的能力。在一些西方学者看来，实证研究的前提为认同法律乃是一种工具，且由此对它可以用一种实证性的方法来加以检验。又由于法律规范对于维持社会秩序具有极大的重要性，社会变革一般不允许像其他科学领域中那样被“视为一种迭代过程”，因此，“在公共事务领域，失败是一个典型的只能在私下里低声讨论的事情”。但是，对某一制度的变革方案之效果进行大数据分析，绝对是有意义的，哪怕实证分析的结论不完全公开、仅供特定人参阅。例如陈卫东等人的课题组将某些改革举措限定在局部区域进行自然实验时，其中就用到大量的统计数据，该研究若能增加数据的维度和样本的数量，则其论证的效果也许会更好。此种对法律制度立法效果的大数据评估，已经在学术界得到一定的认可，例如在一项对精神损害赔偿发生机制的研究中，其研究者就意识到实证研究可以大幅度提高立法预测个体行动的精确性。</p>
<br>
<h2 id="结语">结　语</h2>
<p>　　揭开大数据技术的面纱，我们可以看到，作为法学研究的一种新方法，大数据技术增强了我们获取数据、分析数据的能力，使得在更大时空范围内研究法治实践的规律成为了可能。</p>
<p>　　笔者认为，“数字技术+法学”应区分不同的细分场景展开讨论，不同的法律场景具有不同的特点。例如，首先应区别工程和学术，在学术研究中引入大数据技术，模型设计有充裕的时间，过程的可解释性要求较高。其次应区别学科和学术，作为法学研究方法的大数据技术，只是学科论中的内容之一。有学者认为，“计算法学的研究方法中最主要、最具特点的方法还是本文所指的运用计算机科学智能化处理大量法律数据以解决法律问题的方法”。本文的见解与其相近，同时认为这套大数据的方法不只适用于计算法学，而是全面覆盖法学的各个二级学科。最后是研究中具体方法的细分，如果将法学研究的方法区分为规范研究和实证研究，那么大数据技术方法和法学研究的结合点主要是在实证研究上。有学者认为，“计算法学可归属为实证法学的基本范畴”，“计算法学通过兼收并蓄的统合吸纳了定性研究和定量研究各自的优长”。本文主张狭义地将大数据技术定位为是对定量研究产生的变革，这并不妨碍与定性研究的彼此互鉴。在我国法学界，实证研究将与规范研究长期并存、共同发展。若对此心存疑虑，则不妨回顾一下美国法学研究在20世纪60年代所谓的“跨学科”研究方法之转向，以及90年代对此的二次转向，还有我国法学界在2005年前后也出现了一次“中国法学向何处去”的热烈讨论，便可以发现规范研究和各种跨学科法学研究方法间存在着难舍难分、始终共存的关系。</p>
<br>
<h2 id="了解课程">了解课程</h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://textdata.cn/blog/management_python_course/">点击进入详情页</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>中文金融领域知识图谱的数据集ChainKnowledgeGraph</title>
      <link>https://textdata.cn/blog/chain_knowledge_graph/</link>
      <pubDate>Mon, 06 Dec 2021 16:42:10 +0600</pubDate>
      
      <guid>/blog/chain_knowledge_graph/</guid>
      <description>本文围绕金融领域，推出面向上市公司的产业链图谱。  </description>
      <content:encoded><![CDATA[<p>领域知识图谱的数据集，当前还比较缺失，而作为构建难度最大的产业链图谱领域更为空白。产业链作为产业经济学中的一个概念，是各个产业部门之间基于一定的技术经济关联，并依据特定的逻辑关系和时空布局关系客观形成的链条式关联关系形态。从本质上来说，产业链的本质是用于描述一个具有某种内在联系的企业群结构，产业链中大量存在着上下游关系和相互价值的交换，上游环节向下游环节输送产品或服务，下游环节向上游环节反馈信息。</p>
<p>作者已经先后发布两大领域的实体图谱数据： <br>
1、情报领域【武器装备知识图谱】，地址：https://github.com/liuhuanyong/QAonMilitaryKG<br>
2、医疗领域【医疗知识图谱】，地址： <a href="https://github.com/liuhuanyong/QASystemOnMedicalKG">https://github.com/liuhuanyong/QASystemOnMedicalKG</a></p>
<p>当前，为了进一步推动产业发展，本文围绕金融领域，推出面向上市公司的产业链图谱。</p>
<p>项目地址：</p>

<figure >
    
        <img src="img/1.png" width="800" />
    
    
</figure>

<br>
<h2 id="一项目构成">一、项目构成</h2>
<p>产业链知识图谱包括A股上市公司、行业和产品共3类实体，包括上市公司所属行业关系、行业上级关系、产品上游原材料关系、产品下游产品关系、公司主营产品、产品小类共6大类。</p>
<p>通过数据处理、抽取，最终建成图谱规模数十万，其中包括上市公司4,654家，行业511个，产品95,559条、上游材料56,824条，上级行业480条，下游产品390条，产品小类52,937条，所属行业3,946条。  <br>

<figure >
    
        <img src="img/2.png" width="800" />
    
    
</figure>
</p>
<br>
<h2 id="二项目构建">二、项目构建</h2>
<p>1、实体构建<br>
1）上市公司<br>
目前上市公司已经达到四千多家，是我国重要的公司代表与行业标杆，本图谱选取上市公司作为基础实体之一。通过交易所公开信息中，可以得到上市公司代码、全称、简称、注册地址、挂牌等多个信息。</p>

<figure >
    
        <img src="img/3.png" width="800" />
    
    
</figure>

<p>2）行业分类<br>
行业是产业链图谱中另一个核心内容，也是承载产业、公司及产品的一个媒介，通过这一领携作用，可以生产出大量的行业指数、热点行业等指标。<br>
目前关于行业，已经陆续出现多个行业规范，代表性的有申万三级行业分类、国民经济行业分类等。中国上市公司所属行业的分类准则是依据营业收入等财务数据为主要分类标准和依据，所采用财务数据为经过会计事务所审计并已公开披露的合并报表数据。<br>
2021年6月，申万发布了2021版的行业分类规范，将1级行业从28个调整至31个、2级行业从104个调整至134个、3级行业从227个调整至346个，新增1级行业美容护理等，新增2级行业，并将上市公司进行了归属。本图谱选用申万行业作为基础数据。<br>

<figure >
    
        <img src="img/4.png" width="800" />
    
    
</figure>
</p>
<p>3）业务产品 <br>
业务产品主要指公司主营范围、经营的产品，用于对一个公司的定位。可以从公司的经营范围、年报等文本中进行提取得到。<br>

<figure >
    
        <img src="img/5.png" width="800" />
    
    
</figure>
</p>
<p>2、关系构建 <br>
1）公司所属行业 <br>
通过公开的上市公司行业分类表，可以得到上市公司所对应的行业分类数据。 <br>

<figure >
    
        <img src="img/6.png" width="800" />
    
    
</figure>
</p>
<p>2）行业上级关系 <br>
通过公开的行业三级分类情况，可以通过组合的形式得到行业之间的上级关系数据。 <br>

<figure >
    
        <img src="img/7.png" width="800" />
    
    
</figure>
</p>
<p>3）公司主营产品关系<br>
上市公司的经营产品数据可以从两个方面来获得，一个是从公司简介中的经营范围中结合制定的规则进行提取，另一个是从公司每年发布的半年报、年报中进行提取。这些报告中会有按经营业务、经营产品、经营地域等几个角度对公司的营收占比进行统计，也可以通过制定规则的方式进行提取。第二种方法中，由于已经有统计数据，所以我们可以根据占比数据大小，对主营产品这一关系进行赋值。<br>

<figure >
    
        <img src="img/8.png" width="800" />
    
    
</figure>
</p>
<p>4）产品之间的上下游关系<br>
产品之间的上下游关系，是展示产品之间传导逻辑关系的一个重要方法，包括上游原材料以及下游产品两大类。我们可以多种来获取：<br>
一种是基于规则模式匹配的方式进行抽取，如抽取上游原材料这一关系可以由诸如&quot;a是b的原料/原材料/主要构件/重要原材料/  上游原料&quot;的模式进行抽取&quot;，而下游产品，则同理可以通过&quot;A是B的下游成品/产品&quot;等模式进行提取。<br>
另一种是基于序列标注的提取。还有一种是基于现有结构化知识图谱的提取，例如已经结构化好的百科知识三元组，可以通过设定谓词及其扩展进行过滤。<br>

<figure >
    
        <img src="img/9.png" width="800" />
    
    
</figure>
</p>
<p>5）产品之间的小类关系<br>
对于一个产品而言，其是有大小层级分类的，在缺少大类产品名称的时候，可以通过计算小类产品来得到相应指标。与产品之间的上下游数据类似，可以通过启发式规则的方式进行提取，如“A是一种B”，也可以通过字符之间的组成成分进行匹配生成，如“螺纹钢”是“精细螺纹钢”的一个大类。<br>

<figure >
    
        <img src="img/10.png" width="800" />
    
    
</figure>
</p>
<br>
<h2 id="三项目运行">三、项目运行</h2>
<p>1、data文件夹下包括了本项目的数据信息：<br>
1)company.json:公司实体数据<br>
2)industry.json:行业实体数据 <br>
3)product.json:产品实体数据 <br>
4)company_industry.json:公司-行业关系数据 <br>
5)industry_industry.json:行业-行业关系数据 <br>
6)product_product.json:产品-产品数据 <br>
7)company_product.json:公司-产品数据</p>
<p>2、项目运行:<br>
python build_graph.py</p>
<br>
<h2 id="四项目总结">四、项目总结</h2>
<p>产业链图谱是众多领域知识图谱中较为棘手的一种，本项目通过现有的数据，借助数据处理、结构化提取方式，设计、构建并形成了一个节点100,718，关系边169,153的十万级别产业链图谱。就产业链图谱的构建而言，我们需要至少从以上三个方面加以考虑：</p>
<ul>
<li>其一，产业链的主观性与标准性。产业链的主观性较强，不同的人对产业链的构建、产业链节点、关系的类型，产业链的颗粒度问题都有不同的理解。不同的设定会直接导致不同的应用结果。正如我们所看到的，目前存在不同的行业标准，不同的网站、机构也将公司归为不同的行业。</li>
<li>其二，产业链的动态性和全面性。产业链需要具备足够大的复用性和扩展性，几千家上市公司实际上是冰山一角。国内有几千万家公司，而且不断会有新增，如何将新增的公司融入到这个额产业链中，也是一个很大挑战。此外，产业本身是动态的， 随着行业的发展，不断会有新的行业出现。如何捕捉这种行业的变化，使得整个图谱变得与时俱进，也是需要考量的点。</li>
<li>其三，产业链的定量推理特性。单纯定性的构建产业链知识图谱，如果没有足够的参数，仅有知识表达是无法进行推理的，推理要求知识图谱Schema具备节点间推理传导的必备参数，以及影响推理传导的其他关键参数。对于必备参数来说，从公司到产品必须有主营占比、市场占比、产能占比等数据，从产品到产品必须有成本占比和消耗占比等数据。</li>
</ul>
<br>
<h2 id="参考数据来源">参考数据来源</h2>
<p>1、申万行业：http://www.swsindex.com<br>
2、深交所: <a href="http://www.szse.cn">http://www.szse.cn</a><br>
3、上交所: <a href="http://www.sse.com.cn">http://www.sse.com.cn</a></p>
<p>If any question about the project or me ,see <a href="https://liuhuanyong.github.io/">https://liuhuanyong.github.io/</a></p>
<p>如有自然语言处理、知识图谱、事理图谱、社会计算、语言资源建设等问题或合作，可联系我： <br>
1、我的github项目介绍：https://liuhuanyong.github.io<br>
2、我的csdn博客：https://blog.csdn.net/lhy2014<br>
3、about me:刘焕勇，lhy_<a href="mailto:in_blcu@126.com">in_blcu@126.com</a>.      <br>
4、我的技术公众号:老刘说NLP</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>中文语义常用词典ChineseSemanticKB</title>
      <link>https://textdata.cn/blog/chinese_semantic_kb/</link>
      <pubDate>Mon, 06 Dec 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/chinese_semantic_kb/</guid>
      <description>面向中文处理的12类、百万规模的语义常用词典，包括34万抽象语义库、34万反义语义库、43万同义语义库等，可支持句子扩展、转写、事件抽象与泛化等多种应用场景。</description>
      <content:encoded><![CDATA[<h2 id="chinesesemantickb">ChineseSemanticKB</h2>
<p>ChineseSemanticKB,chinese semantic knowledge base, 面向中文处理的12类、百万规模的语义常用词典，包括34万抽象语义库、34万反义语义库、43万同义语义库等，可支持句子扩展、转写、事件抽象与泛化等多种应用场景。</p>
<br>
<h2 id="项目介绍">项目介绍</h2>
<p>语义知识库是自然语言处理中十分重要的一个基础资源，与学术界追求算法模型不同，工业界的自然语言处理对于底层的词汇知识库、语义知识库等多种资源依赖度很高，具体体现在：<br>
1、具有落地场景的自然语言处理任务都是业务高度相关，一个业务需求刚进去，需要解决的是业务的词汇问题，无基础词库，无项目冷启动；<br>
2、规则和正则启动下的工业级应用，规则的扩展、泛化都需要底层的词汇网络做支撑；<br>
3、目前包括搜索、问答、舆情监控、事件分析等应用，与标签体系的运作关系密切，而这与先验的底层词汇库依赖性很强；<br>
4、自然语言场景越来越关注推理层面，即所谓的“认知”层面，认知背后的各种逻辑关系库，是驱动这一决策的根本途径；<br>
5、当前，面向中文开源词库的工作存在少量、分散的状态，无论从规模，还是质量，都需要进一步聚合；<br>
因此，我从过往的开源工作中进一步抽离和整理，形成了中文处理的12类、百万规模的语义常用词典，包括34万抽象语义库、34万反义语义库、43万同义语义库等，用于相关下游任务。</p>
<p>项目放于dict当中，可直接下载，不建议二次建库共享，尊重开源。</p>
<br>
<h2 id="词库的类别">词库的类别</h2>
<table>
<thead>
<tr>
<th style="text-align:left">词库类型</th>
<th style="text-align:center">词库规模</th>
<th style="text-align:center">词库举例</th>
<th style="text-align:center">词库应用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">抽象关系库</td>
<td style="text-align:center">346,048</td>
<td style="text-align:center">座椅,抽象,家具</td>
<td style="text-align:center">事件抽象与泛化，人民币贬值到货币贬值，再到美元贬值，可支持查询扩展、推荐等任务</td>
</tr>
<tr>
<td style="text-align:left">反义关系库</td>
<td style="text-align:center">34,380</td>
<td style="text-align:center">开心@苦恼</td>
<td style="text-align:center">可用于句子改写，开心改苦恼，支持数据增强，句子生成</td>
</tr>
<tr>
<td style="text-align:left">同义关系库</td>
<td style="text-align:center">424,826</td>
<td style="text-align:center">开心@高兴</td>
<td style="text-align:center">可用于查询扩展、数据增强，也可结合抽象关系库完成推荐等任务</td>
</tr>
<tr>
<td style="text-align:left">简称关系库</td>
<td style="text-align:center">136,081</td>
<td style="text-align:center">北京大学@北大</td>
<td style="text-align:center">可用于句子标准化、句子改写、实体消歧等任务</td>
</tr>
<tr>
<td style="text-align:left">程度副词</td>
<td style="text-align:center">222</td>
<td style="text-align:center">极其,2.0</td>
<td style="text-align:center">可用于情感强度计算，带情感色彩的句子生成</td>
</tr>
<tr>
<td style="text-align:left">否定词</td>
<td style="text-align:center">586</td>
<td style="text-align:center">不,无,没有</td>
<td style="text-align:center">可用于情感计算等任务</td>
</tr>
<tr>
<td style="text-align:left">节日时间词</td>
<td style="text-align:center">54</td>
<td style="text-align:center">春节、五四节</td>
<td style="text-align:center">可用于时间词识别等任务</td>
</tr>
<tr>
<td style="text-align:left">量比词</td>
<td style="text-align:center">7</td>
<td style="text-align:center">占比、环比、同比</td>
<td style="text-align:center">可用于金融领域指标类数据提取任务</td>
</tr>
<tr>
<td style="text-align:left">数量介词</td>
<td style="text-align:center">24</td>
<td style="text-align:center">大约、达到、超过</td>
<td style="text-align:center">可用于金融事件抽象或主干化的搭配词处理任务</td>
</tr>
<tr>
<td style="text-align:left">停用词</td>
<td style="text-align:center">3,861</td>
<td style="text-align:center">？、的、着</td>
<td style="text-align:center">常规的文本特征提取等任务</td>
</tr>
<tr>
<td style="text-align:left">修饰副词</td>
<td style="text-align:center">222</td>
<td style="text-align:center">所、有所</td>
<td style="text-align:center">可结合程度副词完成情感强度计算等任务</td>
</tr>
<tr>
<td style="text-align:left">情态词</td>
<td style="text-align:center">77</td>
<td style="text-align:center">肯定、应该、大概</td>
<td style="text-align:center">可用于句子主观性计算、舆情与可信度计算</td>
</tr>
</tbody>
</table>
<br>
<h2 id="总结">总结</h2>
<p>1、本项目开源了一个目前可用于事件处理以及工业舆情的12类语义词库，总规模数目一百余万；<br>
2、本项目开源的34万抽象语义库、34万反义语义库、43万同义语义库，在作者的实际工作中【事件处理、事理抽取、事件推理】等有重要用途;<br>
3、中文常用语义常用词典，均来源于公开文本+人工整理+机器抽取形成，其中若有质量不高之处，可积极批评指正;<br>
4、中文开源事业还是要坚持做下去，尽可能地缩短自然语言处理学术界和工业界之间的鸿沟。</p>
<blockquote>
<p>If any question about the project or me ,see <a href="https://liuhuanyong.github.io/">https://liuhuanyong.github.io/</a>.<br>
如有自然语言处理、知识图谱、事理图谱、社会计算、语言资源建设等问题或合作，可联系我：     <br>
1、我的github项目介绍：https://liuhuanyong.github.io  <br>
2、我的csdn技术博客：https://blog.csdn.net/lhy2014 <br>
3、我的联系方式: 刘焕勇，中国科学院软件研究所，lhy_<a href="mailto:in_blcu@126.com">in_blcu@126.com</a>. <br>
4、我的共享知识库项目：刘焕勇，数据地平线，http://www.openkg.cn/organization/datahorizon.<br>
5、我的工业项目：刘焕勇，数据地平线，大规模实时事理学习系统：https://xueji.datahorizon.cn.  <br>
6、我的工业项目：刘焕勇，数据地平线，面向事件和语义的自然语言处理工具箱：https://nlp.datahorizon.cn</p>
</blockquote>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>在会计研究中使用Python进行文本分析</title>
      <link>https://textdata.cn/blog/text_mining_in_accouting_research/</link>
      <pubDate>Fri, 26 Nov 2021 22:40:10 +0600</pubDate>
      
      <guid>/blog/text_mining_in_accouting_research/</guid>
      <description>会计文本分析知识大全</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="img/%e5%b0%81%e9%9d%a2.png" alt=""  />
</p>
<p><img loading="lazy" src="img/%e4%bb%a3%e7%a0%81.png" alt=""  />
</p>
<p>最近在google搜Python在经管中的内容，意外发现<strong>专著： 在会计研究中使用Python进行文本分析</strong>，内容特别新，专著中含有Python代码，也有会计领域文本分析的应用成果。跟 <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">视频专栏课| Python网络爬虫文本分析</a> 结合起来，特别适合会计领域python初学者，将文本分析应用于会计研究中。</p>
<blockquote>
<p>Vic Anand, Khrystyna Bochkay, Roman Chychyla and Andrew Leone (2020 isbn), “Using Python for Text Analysis in Accounting Research (forthcoming)”, Foundations and Trends ® in Accounting: Vol. xx, No. xx, pp 1–18. DOI: 10.1561/XXXXXXXXX.</p>
</blockquote>
<p><a href="http://dx.doi.org/10.1561/1400000062">http://dx.doi.org/10.1561/1400000062</a></p>
<h3 id="摘要">摘要</h3>
<p>会计研究中文本数据的重要性显着增加。为了帮助研究人员理解和使用文本数据，本专著定义和描述了文本数据的常用度量，然后演示了使用 Python 编程语言收集和处理文本数据。该专著充满了示例代码，这些<strong>代码复现了最近研究论文中的文本分析任务</strong>。</p>
<p>在专著的第一部分，我们提供了 Python 入门指南。我们首先描述 Anaconda，它是 Python 的一个发行版，它提供了文本分析所需的库及其安装。然后，我们介绍了 Jupyter notebook，这是一种改进研究工作流程并促进可复制研究的编程环境。接下来，我们将教授 Python 编程的基础知识，并演示使用 Pandas 包中的表格数据的基础知识。</p>
<p>专著的第二部分侧重于会计研究中常用的具体文本分析方法和技术。我们首先介绍<strong>正则表达式</strong>，这是一种用于在文本中查找模式的复杂语言。然后我们将展示<strong>如何使用正则表达式从文本中提取特定部分</strong>。接下来，我们介绍<strong>将文本数据（非结构化数据）转换为表示感兴趣变量（结构化数据）的数值度量的想法</strong>。具体来说，我们介绍了基于字典的方法</p>
<ol>
<li><strong>测量文档情绪</strong>，</li>
<li><strong>计算文本复杂度</strong>，</li>
<li><strong>识别前瞻性句子和风险披露</strong>，</li>
<li><strong>收集文本中的信息量</strong>，以及</li>
<li><strong>计算不同文本片段的相似度</strong>。</li>
</ol>
<p>对于这些任务中的每一个，我们引用相关论文并提供代码片段来实现这些论文中的相关指标。</p>
<p>最后，专著的第三部分侧重于<strong>自动化文本数据的收集</strong>。我们介绍了网页抓取并提供了从 EDGAR 下载文件的代码。</p>
<h3 id="关键词">关键词</h3>
<p><strong>文本分析，数据收集，Python，自然语言处理</strong></p>
<br>
<h2 id="using-python-for-text-analysis-in-accounting-research-forthcoming目录">Using Python for Text Analysis in Accounting Research (forthcoming)目录</h2>
<h3 id="1-引言">1. 引言</h3>
<h3 id="2-在电脑中配置python">2. 在电脑中配置Python</h3>
<ul>
<li>2.1 Python包的作用</li>
<li>2.2 Anaconda软件版本</li>
<li>2.3 安装Anaconda</li>
<li>2.4 Anaconda的使用</li>
</ul>
<h3 id="3--jupyter-notebook">3.  Jupyter Notebook</h3>
<ul>
<li>3.1 案例</li>
<li>JupyterLab: Jupyter Notebook的开发版(最新版)</li>
<li>如何启动JupyterLab</li>
<li>在JupyterLab中写代码</li>
<li>Markdown标记语言与格式化文本代码块</li>
</ul>
<h3 id="4-python编程语言简要介绍">4. Python编程语言简要介绍</h3>
<ul>
<li>4.1 基础知识</li>
<li>4.2 变量与数据类型</li>
<li>4.3 操作</li>
<li>4.4 print函数</li>
<li>4.5 控制流</li>
<li>4.6 函数</li>
<li>4.7 集合类型数据-list、tuple、dictionaries</li>
<li>4.8 处理字符串</li>
</ul>
<h3 id="5-处理表数据-pandas包">5. 处理表数据： Pandas包</h3>
<ul>
<li>5.1 Pandas使用场景</li>
<li>5.2 导入import 声明</li>
<li>5.3 加载数据、导出数据</li>
<li>5.4 在pandas中查看数据</li>
<li>5.5 筛选数据</li>
<li>5.6 创建新列（字段）</li>
<li>5.7 删除列（字段）、列（字段）名重命名</li>
<li>5.8 对数据排序</li>
<li>5.9 合并数据</li>
</ul>
<h3 id="6-正则表达式介绍">6 正则表达式介绍</h3>
<ul>
<li>6.1 查看文本中的模式</li>
<li>6.2 字符与字符集</li>
<li>6.3 Regex的定位与边界</li>
<li>6.4 模式匹配次数限定</li>
<li>6.5 分组</li>
<li>&hellip;</li>
</ul>
<h3 id="7-基于字典法-的文本分析">7. 基于字典法 的文本分析</h3>
<ul>
<li>7.1 字典法文本分析的优势</li>
<li>7.2 理解字典</li>
<li>7.3 识别文本中的词语与句子</li>
<li>7.4 词干化、词形还原</li>
<li>7.5 词语权重</li>
<li>7.6 基于词典法的词频统计函数</li>
</ul>
<h3 id="8-量化文本复杂度">8. 量化文本复杂度</h3>
<ul>
<li>8.1 理解文本复杂度</li>
<li>8.2 计算文本字符长度</li>
<li>8.3 使用Fog指数测量文本可读性</li>
<li>8.4 使用BOG指数测量文本可读性</li>
</ul>
<h3 id="9-句子结构与分类">9. 句子结构与分类</h3>
<ul>
<li>9.1 识别前瞻性陈述forward-looking sentences</li>
<li>9.2 使用字典法做文本分类</li>
<li>9.3 识别句子的主语与宾语</li>
<li>9.4 识别命名实体</li>
<li>9.5 词性标注与命名实体识别任务</li>
</ul>
<h3 id="10-测量文本相似度">10. 测量文本相似度</h3>
<ul>
<li>10.1 使用相似度比较文本</li>
<li>10.2 长文本使用cosine相似度计算相似度</li>
<li>10.3 短文本使用Levenshtein距离计算相似度</li>
<li>10.4 使用word2vec词嵌入计算语义相似度</li>
</ul>
<h3 id="11-识别文本中的具体信息">11. 识别文本中的具体信息</h3>
<ul>
<li>11.1 文本识别与抽取</li>
<li>11.2 案例: 从10-k filing中提取出MD&amp;A</li>
<li>11.3 案例: 从10-k html网页文件中提取处MD&amp;A</li>
<li>11.4 从XBRL金融报告中抽取文本</li>
</ul>
<h3 id="12-从网络中收集数据">12. 从网络中收集数据</h3>
<ul>
<li>12.1 在互联网中采集数据</li>
<li>12.2 证券交易委员会的EDGAR数据</li>
<li>12.3 网络爬虫</li>
<li>12.4 关于api接口</li>
</ul>
<h3 id="致谢">致谢</h3>
<br>
<h3 id="参考文献部分">参考文献(部分)</h3>
<blockquote>
<p>Bentley, J. W., T. E. Christensen, K. H. Gee, and B. C. Whipple. 2018. “Disentangling managers’ and analysts’ non-GAAP reporting”. Journal of Accounting Research. 56(4): 1039–1081.</p>
<p>Blankespoor, E. 2019. “The impact of information processing costs on ﬁrm disclosure choice: Evidence from the XBRL mandate”. Journal of Accounting Research. 57(4): 919–967.</p>
<p>Bochkay, K., R. Chychyla, and D. Nanda. 2019. “Dynamics of CEO disclosure style”. The Accounting Review. 94(4): 103–140.</p>
<p>Bochkay, K., J. Hales, and S. Chava. 2020. “Hyperbole or reality? Investor response to extreme language in earnings conference calls”. The Accounting Review. 95(2): 31–60.</p>
<p>Bochkay, K. and C. B. Levine. 2019. “Using MD&amp;A to improve earnings forecasts”. Journal of Accounting, Auditing &amp; Finance. 34(3): 458482.</p>
<p>Bonsall, S. B., A. J. Leone, B. P. Miller, and K. Rennekamp. 2017. “A plain English measure of ﬁnancial reporting readability”. Journal of Accounting and Economics. 63(2): 329–357.</p>
<p>Bozanic, Z., D. T. Roulstone, and A. Van Buskirk. 2018. “Management earnings forecasts and other forward-looking statements”. Journal of Accounting and Economics. 65(1): 1–20.</p>
<p>Chychyla, R., A. J. Leone, and M. Minutti-Meza. 2019. “Complexity of ﬁnancial reporting standards and accounting expertise”. Journal of Accounting and Economics. 67(1): 226–253.</p>
<p>Gow, I. D., D. F. Larcker, and A. A. Zakolyukina. 2019. “Non-answers during conference calls”. Chicago Booth Research Paper. (19-01). Guay, W., D. Samuels, and D. Taylor. 2016. “Guiding through the Fog:Financial statement complexity and voluntary disclosure”. Journal of Accounting and Economics. 62(2): 234–269.</p>
<p>Heitmann, M., C. Siebert, J. Hartmann, and C. Schamp. 2020. “More Than a Feeling: Benchmarks for Sentiment Analysis Accuracy”. Working Paper, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract">https://papers.ssrn.com/sol3/papers.cfm?abstract</a>_ id=3489963.</p>
</blockquote>
<br>
<h2 id="本书下载">本书下载</h2>
<p><a href="https://github.com/hiDaDeng/DaDengAndHisPython/blob/master/Using_Python_For_Text_Analysis_In_Accounting_Research.pdf">https://github.com/hiDaDeng/DaDengAndHisPython/blob/master/Using_Python_For_Text_Analysis_In_Accounting_Research.pdf</a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本可读性研究及应用清单</title>
      <link>https://textdata.cn/blog/text_readability/</link>
      <pubDate>Wed, 24 Nov 2021 23:40:10 +0600</pubDate>
      
      <guid>/blog/text_readability/</guid>
      <description>京语言大学智能计算机辅助语言学习（ICALL）研究组维护的文本可读性阅读清单。含综述、项目、代码等</description>
      <content:encoded><![CDATA[<p>这是北京语言大学智能计算机辅助语言学习（ICALL）研究组维护的文本可读性阅读清单。</p>
<table>
<thead>
<tr>
<th>目录</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. <a href="#1">综述</a></td>
</tr>
<tr>
<td>2. <a href="#2">相关研究</a></td>
</tr>
<tr>
<td>2.1 <a href="#2.2">中文可读性</a></td>
</tr>
<tr>
<td>2.2 <a href="#2.3">其他语言可读性</a></td>
</tr>
<tr>
<td>3. <a href="#3">可读性分析工具</a></td>
</tr>
<tr>
<td>4 <a href="#4">中文数据</a></td>
</tr>
</tbody>
</table>
<br>
<h2 id="1">1. 综述</h2>
<ul>
<li>
<p>Klare, G. R. (1974–1975). <a href="https://scholar.google.com/scholar_url?url=https://www.jstor.org/stable/747086&amp;hl=zh-TW&amp;sa=T&amp;oi=gsb&amp;ct=res&amp;cd=0&amp;d=6838320539766870596&amp;ei=-1t9Xoq9M8SBywSKyJqgDg&amp;scisig=AAGBfm1iWtmdPfAMXqFhp5eCXdApCr8JfQ">Assessing readability</a>. <em>Reading Research Quarterly</em>.</p>
</li>
<li>
<p>吴思远, 蔡建永, 于东, 江新. 2018. <a href="https://www.researchgate.net/profile/Xin_Jiang26/publication/332834238_A_Survey_on_the_Automatic_Text_Readability_Measureswenbenkeduxingdezidongfenxiyanjiuzongshu/links/5ccc04ca299bf11c2a3d46f3/A-Survey-on-the-Automatic-Text-Readability-Measureswenbenkeduxingdezidongfenxiyanjiuzongshu.pdf">文本可读性的自动分析研究综述</a>. <em>中文信息学报</em>.</p>
</li>
<li>
<p>郭凯、金檀、陆小飞. 2018. <a href="http://www.cnki.com.cn/Article/CJFDTotal-WYCJ201803005.htm">文本难度调控的研究与实践——从可读公式、多维特征到智能改编</a>. <em>外语测试与教学</em>.</p>
</li>
</ul>
<br>
<h2 id="2">2. Related Task</h2>
<h3 id="2.1">2.1 Research on Chinese Readability</h3>
<ul>
<li>
<p>Yao-Ting Sung, Tao Hsing Chang. 2016. <a href="https://link.springer.com/content/pdf/10.3758%2Fs13428-015-0649-1.pdf">CRIE: An automated analyzer for Chinese texts</a>. <em>Behavior Research Methods</em>.</p>
</li>
<li>
<p>Yao-Ting Sung, Weic Lin, SB Dyson, Kuoen Chang. 2015. <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1111/modl.12213">Leveling L2 Texts Through Readability: Combining Multilevel Linguistic Features with the CEFR</a>. *<em>The Modern Language Journal</em>.</p>
</li>
<li>
<p>LAU Tak Pang. 2006. <a href="https://core.ac.uk/download/pdf/48538871.pdf">Chinese Readability Analysis and its Applications on the Internet</a>. <em>Master&rsquo;s thesis, The Chinese University of Hong Kong</em>.</p>
</li>
<li>
<p>Yu Qiaona. 2016.<a href="https://scholarspace.manoa.hawaii.edu/bitstream/10125/51627/1/2016-12-phd-yu.pdf">Defining and Assessing Chinese Syntactic Complexity via TC-Units</a>. <em>Doctor&rsquo;s thesis, University of Hawaii at Manoa</em>.</p>
</li>
</ul>
<br>
<h3 id="2.2">2.2 Research on Readability in Other Languages</h3>
<ul>
<li>
<p>Arthur C. Graesser, Danielle S. McNamara. 2004. <a href="https://link.springer.com/content/pdf/10.3758%2FBF03195564.pdf">Coh-Metrix: Analysis of text on cohesion and language</a>. <em>Behavior Research Methods, Instruments, &amp; Computers</em>.</p>
</li>
<li>
<p>Arthur C. Graesser, Danielle S. McNamara. 2011. <a href="http://sage.cnpereading.com/paragraph/download/10.3102/0013189X11413260">Coh-Metrix: Providing multilevel analysis of text characteristic</a>. <em>Educational Researcher</em>.</p>
</li>
<li>
<p>Xiaofei Lu. 2010. <a href="https://www.jbe-platform.com/docserver/fulltext/ijcl.15.4.02lu.pdf?expires=1561207415&amp;id=id&amp;accname=jbid110151&amp;checksum=0E423CA22C4B7AAB06AEC4C0359EBEF9">Automatic analysis of syntactic complexity in second language writing</a>. <em>International Journal of Corpus Linguistics</em>.</p>
</li>
<li>
<p>Xiaofei Lu. 2013. <a href="https://s3.amazonaws.com/academia.edu.documents/32693735/Ai_Lu_2013_syntactic_complexity.pdf?response-content-disposition=inline%3B%20filename%3DA_corpus-based_comparison_of_syntactic_c.pdf&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWOWYYGZ2Y53UL3A%2F20190623%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20190623T072057Z&amp;X-Amz-Expires=3600&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=ec1c034b7a2f9191914b65ec60cc3d41a8ca932fcf137c45d23a87194977b080">A corpus-based comparison of syntactic complexity in NNS and NS university students’ writing</a>. <em>Automatic Treatment and Analysis of Learner Corpus Data</em></p>
</li>
<li>
<p>陆小飞, 许琪. 2016. <a href="http://www.cnki.com.cn/Article/CJFDTotal-WJYY201603008.htm">二语句法复杂度分析器及其在二语写作研究中的应用</a>. <em>外语教学与研究</em></p>
</li>
<li>
<p>Xiaofei Lu. 2017. <a href="http://sage.cnpereading.com/paragraph/download/10.1177/0265532217710675">Automated measurement of syntactic complexity in corpus-based L2 writing research and implications for writing assessment. Language Testing</a>. <em>Language Testing</em></p>
</li>
<li>
<p>Jin, T., Lu, X., &amp; Ni, J. (2020). <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/modl.12622">Syntactic complexity in adapted teaching materials: Differences among grade levels and implications for benchmarking</a>. <em>The Modern Language Journal</em></p>
</li>
<li>
<p>Menglin Xia ,Ekaterina Kochmar ,Ted Briscoe. 2016. <a href="https://www.aclweb.org/anthology/W16-0502.pdf">Text Readability Assessment for Second Language Learners</a>. <em>Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications</em></p>
</li>
<li>
<p>Xiaobin Chen, Detmar Meurers. 2016. <a href="https://www.aclweb.org/anthology/W16-4113.pdf">CTAP: A Web-Based Tool Supporting Automatic Complexity Analysis</a>. <em>Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity</em>.</p>
</li>
<li>
<p>Chen, X. 2018. <a href="https://publikationen.uni-tuebingen.de/xmlui/bitstream/handle/10900/85888/main.pdf?sequence=1">Automatic Analysis of Linguistic Complexity and Its Application in Language Learning Research</a>, <em>PhD thesis in computational linguistics,  Eberhard Karls Universität Tübingen</em>.</p>
</li>
<li>
<p>Nadezda Okinina, Jennifer-Carmen Frey. CTAP for Italian: Integrating Components for the Analysis of Italian into a Multilingual Linguistic Complexity Analysis Tool.</p>
</li>
<li>
<p>Zarah Weiss, Z. 2017. <a href="https://www.researchgate.net/profile/Zarah_Weiss/publication/334318057_Using_Measures_of_Linguistic_Complexity_to_Assess_German_L2_Proficiency_in_Learner_Corpora_under_Consideration_of_Task-Effects/links/5d24456c299bf1547ca4fe92/Using-Measures-of-Linguistic-Complexity-to-Assess-German-L2-Proficiency-in-Learner-Corpora-under-Consideration-of-Task-Effects.pdf">Using Measures of Linguistic Complexity to Assess German L2 Proficiency in Learner Corpora under Consideration of Task-Effects</a>. <em>M.A. Thesis in Computational Linguistics</em>.</p>
</li>
<li>
<p>Weiss Z, Meurers D. 2019. <a href="http://www.sfs.uni-tuebingen.de/~zweiss/rsrc/Weiss.Meurers-17-LCR-Presentation.pdf">Broad Linguistic Modeling is Beneficial for German L2 Proficiency Assessment</a>. <em>Widening the Scope of Learner Corpus Research, Selected Papers from the Fourth Learner Corpus Research Conference</em>.</p>
</li>
<li>
<p>S Tonelli, KT Manh, E Pianta. 2012. <a href="https://dl.acm.org/doi/pdf/10.5555/2390916.2390924?download=true">Making readability indices readable</a>. <em>Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations</em>.</p>
</li>
<li>
<p>Lijun Feng. 2010. <a href="https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=2964&amp;context=gc_etds">Automatic Readability Assessment</a>. *<em>Doctor&rsquo;s thesis, City University of New York</em>.</p>
</li>
</ul>
<br>
<h2 id="3">3. Readability Analysis Tools</h2>
<ul>
<li>
<p>Lu Xiaofei (2010). <a href="https://www.jbe-platform.com/docserver/fulltext/ijcl.15.4.02lu.pdf?">Automatic analysis of syntactic complexity in second language writing</a>. <em>International Journal of Corpus Linguistics</em>.
(<a href="https://aihaiyang.com/software/l2sca/">Web-based L2 Syntactical Complexity Analyzer (L2SCA)</a>)</p>
</li>
<li>
<p>Yao-Ting Sung, Tao Hsing Chang. 2016. <a href="https://link.springer.com/content/pdf/10.3758%2Fs13428-015-0649-1.pdf">CRIE: An automated analyzer for Chinese texts</a>. <em>Behavior Research Methods</em>.
(<a href="http://www.chinesereadability.net/CRIE/index.aspx?LANG=CHT">文本可读性指标自动化分析系统(Chinese Readability Index Explorer, CRIE)</a>)</p>
</li>
<li>
<p>Arthur C. Graesser, Danielle S. McNamara . 2011. <a href="http://sage.cnpereading.com/paragraph/download/10.3102/0013189X11413260">Coh-Metrix: Providing multilevel analysis of text characteristic</a>. <em>Educational Researcher</em>.
(<a href="http://210.240.188.161/Chinese_CohMetrix/index.html">中文文本自动化分析系统: Coh-Metrix</a>)</p>
</li>
<li>
<p>Xiaobin Chen, Detmar Meurers. 2016. <a href="https://www.aclweb.org/anthology/W16-4113.pdf">CTAP: A Web-Based Tool Supporting Automatic Complexity Analysis</a>. <em>Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC))</em>.
(<a href="http://samos.sfs.uni-tuebingen.de:8080/ctapweb/">CTAP</a>)</p>
</li>
<li>
<p>金檀、陆小飞、郭凯、李百川. 2018. Eng-Editor: An online English text evaluation and adaptation system. 广州：语言数据网(languagedata.net/tester).
( <a href="https://www.languagedata.net/tester/">英语阅读分级指难针</a> )</p>
</li>
</ul>
<br>
<h2 id="4">4. Chinese Data Resources</h2>
<ul>
<li><a href="https://mp.weixin.qq.com/s/VRiNJyILWMwNOAzXJUoKyA">汉语词法难度分级表</a></li>
<li><a href="https://mp.weixin.qq.com/s/IRSqMm75mjoI95VGArW9Jw">汉语句法难度分级表</a></li>
<li>国际汉语教师语法教学手册</li>
<li>国际汉语教师中级语法教学手册</li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理世界 | 使用文本分析&amp;机器学习测量短视主义</title>
      <link>https://textdata.cn/blog/text_mining_in_2021_management_world/</link>
      <pubDate>Tue, 23 Nov 2021 21:33:10 +0600</pubDate>
      
      <guid>/blog/text_mining_in_2021_management_world/</guid>
      <description>本文基于高层梯队理论和社会心理学中的时间导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现，年报 MD&amp;amp;A 中披露的“短期视域” 语言 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越 易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴。The Chief executive officer (CEO) plays a prominent role in ensuring sustainable business growth and enhancing long-term performance. However, not all CEOs have a long-term vision. In this paper, based on the upper echelons and time orientation theory, we investigate whether and how managerial myopia, defined as managers&amp;#39; innate traits to focus on short-term goals, affects their behaviors and decisions. While extant literature extensively centered on the impact of managers&amp;#39; demographic characteristics on long-term investment, little is known about the role of managerial myopia. One reason is the difficulty in coming up with an ex-ante measure for persons&amp;#39; inner and stable time orientation traits. In this paper, based on the textual analytical platform WinGo, we use textual analysis and machine learning technology to measure the managerial myopia. Since the controller of a firm in China is usually the chairman of the board (COB) rather than the CEO, we focus on the COB&amp;#39;s myopia in this study. Through a bat⁃ tery of validations and variance decomposition methods, this study verifies that our proposed textual measure effective⁃ ly captures COBs&amp;#39; innate myopia traits, rather than myopia induced by the external environment.</description>
      <content:encoded><![CDATA[<h2 id="案例文献">案例文献</h2>
<p>胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.</p>
<h2 id="摘要">摘要：</h2>
<p>在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于<strong>高层梯队理论</strong>和社会心理学中的<strong>时间导向理论</strong>，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并<strong>采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验</strong>。研究结果发现，<strong>年报 MD&amp;A 中披露的“短期视域” 语言</strong> 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越 易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。<strong>同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴</strong>。</p>
<h2 id="关键词">关键词：</h2>
<p>管理者短视 长期投资 文本分析 机器学习</p>
<h2 id="变量测量论证">变量测量论证</h2>
<p><strong>语言能够反映人的认知、偏好和个性（Webb et al.，1966），研究者可通过分析实验对象语言中使用的词语类型和词频来捕捉人的特质</strong>（Miller and Ross，1975；Pennebaker et al.，2003）。如一个人的语言中越强调“过去”、“ 曾经”等词汇，反映其越关注过去；一个人的语言中越强调“将来”、“ 可能”、“ 要去”等词汇，反映其越关注未来（Pennebaker et al.，2003）。<strong>基于此研究范式，本文结合已有的英文“短期视域”词集、MD&amp;A 中文语料特点以及 Word2Vec 机器学习制定出能够反映管理者“短期视域”的中文词集，随后通过词典法构建出管理者的短视主义指标。</strong></p>
<p>MD&amp;A 是管理者对报告期内企业经营状况的回顾以及对下一年度经营计划以及企业未来发展所面临的机遇、挑战和各种风险的阐述。已有利用 MD&amp;A 等文本刻画管理者特质的研究成果在一定程度上证实了其可靠性（Li，2012；蒋艳辉、冯楚建，2014）。如</p>
<ul>
<li>Li（2012）利用美国上市公司 MD&amp;A 文本来刻画管理者的 <strong>自我归因偏差</strong>。</li>
<li>蒋艳辉和冯楚建（2014）利用 MD&amp;A 中“我们”、“ 我公司”、“ 我们公司”等词语出现的频率刻画管理者的<strong>自我指涉度</strong>，从而衡量管理层对公司的认知和努力程度。</li>
<li>同时，国外文献表明 CEO 对企业的经营决策起着绝对的主导作用，能够直接影响企业未来的发展方向和命运（Chandler，1962；Finkelstein and Hambrick，1996）。CEO 的特质如自恋程度、学历和任期等都会极大影响公司的信息披露特点（Marquez Illescas et al.，2019；Lewis et al.，2019），因此年报披露的文本信息更多地反映了 CEO 的意思。而在我国，上市公司的董事长更像发达国家的 CEO（姜付秀等，2009；陈传明、孙俊华，2008；李健等，2012）。</li>
</ul>
<p><strong>因此，我们从 MD&amp;A 中捕获的管理者短视主义特质更多反映的是董事长的短视主义特质，本文的管理者指的是企业的董事长。</strong></p>
<h2 id="指标构建过程">指标构建过程</h2>
<p>具体来讲**，管理者短视主义指标**的构建过程如下。</p>
<ol>
<li>借鉴 Brochet 等（2015）的英文“<strong>短期视域</strong>”词集与 Li（2010）构建文本指标的思路，我们阅读了 500 份 MD&amp;A 语料以获取中文文本信息的特点，制定出中文 MD&amp;A 中有关“短期视域”的种子词集，包括直接和间接 两大类。<strong>直接短期视域</strong>大类包括：“ 天内”、“ 数月”、“ 年内”、“ 尽快”、“ 立刻”、“ 马上”；<strong>间接短期视域</strong>大类包括：“ 契机”、“ 之际”、 “压力”、“ 考验”。</li>
<li>针对同一概念或者事物，表达者往往使用多个语义相似的词汇进行描述，因此需要对种子词集进行相似词扩充。本文采用 Word2Vec 中的 CBOW 模型（Continuous Bag-of-words Model）对中文年度财务报告语料进行训练。</li>
<li>我们通过邀请 3 名业界和学术界专家以及对比 MD&amp;A 文本样例对指标词集进行核验，最终确定词集包含 43 个“短期视域”词汇（词集和语句示例详见《管理世界》网络发行版附录 2）。随后，本文基于词典法计算 “短期视域”词汇总词频占 MD&amp;A 总词频的比例，乘以 100 后得到<strong>管理者短视主义指标</strong>。该指标值越大，表明管理者越短视。</li>
</ol>
<h2 id="技术分析">技术分析</h2>
<blockquote>
<p>纯技术讨论，非论文内容</p>
</blockquote>
<p>这篇管理世界的论文，主要难点有两个：</p>
<ol>
<li>
<p>如何构建 <strong>短视主义词典(集)</strong> ？</p>
</li>
<li>
<ul>
<li>根据对研究和数据的了解，<strong>人工摘选</strong>一些 短视主义词典(集)种子词；人工，不需要python编程</li>
<li>使用Word2Vec技术扩充 短视主义词典(集)；需要python编程</li>
</ul>
</li>
<li>
<p>如何使用 <strong>短视主义词典(集)</strong> 计算  <strong>短视主义指标</strong>？</p>
</li>
<li>
<ul>
<li>需要使用Python编程语言，根据 <strong>词典法</strong> 实现短视主义指标的计算。</li>
</ul>
</li>
</ol>
<h2 id="python学习与实现">python学习与实现</h2>
<p>难点主要可在掌握 <strong><a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">视频专栏课| Python网络爬虫与文本分析</a></strong>  后，结合以下两个技能点实现</p>
<ul>
<li>扩充词集可以用到之前分享的<strong>wordexpansion库</strong>  <a href="https://github.com/DataPlusCommunity/wordexpansion">https://github.com/DataPlusCommunity/wordexpansion</a></li>
<li>计算短视主义指标，即词典法可以用到<strong>cnsenti库</strong>  <a href="https://github.com/DataPlusCommunity/cnsenti">https://github.com/DataPlusCommunity/cnsenti</a></li>
</ul>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本分析在市场营销研究中的应用</title>
      <link>https://textdata.cn/blog/text_mining_in_marketing_research/</link>
      <pubDate>Tue, 23 Nov 2021 21:32:10 +0600</pubDate>
      
      <guid>/blog/text_mining_in_marketing_research/</guid>
      <description>语言文字是营销场景中最常用的交互方式，比如在线评论、消费者服务热线、新闻发布、营销传播等活动都创造了有价值的文本数据。但营销研究者如何用好这些数据？本文回顾了文本分析相关研究，并详细介绍了如何用文本数据做市场研究。作者讨论了文本如何**反映**文本生产者， 文本信息如何**影响**信息接受者。接下来，本文讨论了文本如何**预测**并**理解**文本背后的信息，回顾了文本分析的方法和测量指标(metrics),提供了一整套的文本分析操作流程。最后，作者提到文本分析内部信度和外部效度问题，研究者如何解决。本文讨论营销各个领域可能存在的研究机会，虽然目前市场营销的研究问题大都是跨学科的，但是营销的各个子领域经常都是孤立，借助文本分析可能架构起连接营销各个子领域的桥梁。Language and text are the most commonly used interaction methods in marketing scenarios. Activities such as online reviews, consumer service hotlines, press releases, and marketing communications all create valuable text data. But how can marketing researchers make good use of this data? This article reviews the research on text analytics and details how to use text data for market research. The authors discuss how texts *reflect* text producers, and how textual information *influences* information recipients. Next, this article discusses how to **predict** and **understand** the information behind the text, reviews the methods and metrics of text analysis, and provides a complete set of text analysis operation procedures. Finally, the author mentions the problems of internal reliability and external validity of text analysis, and how researchers can solve them. This article discusses the research opportunities that may exist in various fields of marketing. Although the current marketing research issues are mostly interdisciplinary, each subfield of marketing is often isolated. With the help of text analysis, it is possible to build a bridge connecting various subfields of marketing.</description>
      <content:encoded><![CDATA[<br>
<blockquote>
<p>翻译自</p>
<p>Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &ldquo;Uniting the tribes: Using text for marketing insight.&rdquo; Journal of Marketing (2019): 0022242919873106.</p>
</blockquote>
<p>论文作者们的报告视频已上传到B站(下图)，感兴趣的童鞋可以先收藏再收看</p>
<p><a href="https://www.bilibili.com/video/BV1rJ411b7G6"><strong>Jourmal of Marketing Webinar｜2019市场营销</strong></a></p>
<br>
<h2 id="摘要">摘要</h2>
<p>语言文字是营销场景中最常用的交互方式，比如在线评论、消费者服务热线、新闻发布、营销传播等活动都创造了有价值的文本数据。但营销研究者如何用好这些数据？本文回顾了文本分析相关研究，并详细介绍了如何用文本数据做市场研究。作者讨论了文本如何<strong>反映</strong>文本生产者， 文本信息如何<strong>影响</strong>信息接受者。</p>
<p>接下来，本文讨论了文本如何<strong>预测</strong>并<strong>理解</strong>文本背后的信息，回顾了文本分析的方法和测量指标(metrics),提供了一整套的文本分析操作流程。最后，作者提到文本分析内部信度和外部效度问题，研究者如何解决。本文讨论营销各个领域可能存在的研究机会，虽然目前市场营销的研究问题大都是跨学科的，但是营销的各个子领域经常都是孤立，借助文本分析可能架构起连接营销各个子领域的桥梁。</p>
<br>
<h2 id="关键词">关键词</h2>
<ul>
<li>计算语义学coputational linguistics</li>
<li>机器学习machine learning</li>
<li>市场洞察marketing insight</li>
<li>跨学科interdisciplinary</li>
<li>自然语言处理natural language processing</li>
<li>文本分析text analysis</li>
<li>文本挖掘 text mining</li>
</ul>
<br>
<h2 id="无所不在文本">无所不在文本</h2>
<p>交流沟通是营销的重要组成部分，消费者、企业、消费者投资者、社会，不同水平或者统一水平都有信息交流与沟通。而信息交流的过程中往往会产生或者转化为文本数据。</p>
<p>最简单的的文本数据世界模型是<strong>生产者</strong>与<strong>消费者</strong>。模型内生产者和接受者都可能是消费者、企业、投资者和社会。消费者书写在线评论，公司制作会计年报，文化生产者代表社会意义制作出书籍、影片和艺术品（Table 1）</p>
<p>在此情形下，研究者可能选择文本如何反映或如何影响？</p>
<ul>
<li><strong>How text reflects its producer？</strong></li>
<li><strong>How text impacts its receiver？</strong></li>
</ul>
<p>尤其是文本可以反映一定的信息，这些信息是可以帮助营销人员洞察市场规律，进而利用规律影响文本信息的接受者。</p>
<p><img loading="lazy" src="img/textproducerReceiver.png" alt=""  />
</p>
<br>
<h2 id="文本反映生产者">文本反映生产者</h2>
<p>首先，文本可以反映了个人的一些信息。例如“在社交媒体某推特上写着某人谈论着上周他们做了什么。”这句话有很多待挖掘的信息，比如他们这些人什么情况，是内向还是外向、神经质还是严肃认真、他们感觉如何、某时刻他们想了什么(Moon and Kamakura 2017)。总之，文本可以看作指纹或签名(Pennebaker 2011)。</p>
<p>通过文本也可以用于理解领导人、机构或者文化精英。例如领导人用词表达会反映出其领导风格，对利益相关方的态度。透过广告、网站或者消费者服务商(consumer service agent)的言语，人们会了解公司的品牌个性(Opoku, Abratt, and Pitt 2006)，公司是如何看待消费者(Packard and Berger 2019a)，管理层对终端用户的定位(Molner, Prabhu, and Yadav 2019)。年报也会有未来业绩表现的有价值线索(Loughran and McDonald 2016)。</p>
<p>除了单独分析个人或组织的言语，也可以对多个内容生产者合并起来进行更大层面的研究。透过人群或组织产生的文本，我们可以更好理解他们的本质。例如，分析微博，可以得出老年人和年轻人之间如何看待幸福(as excitement vs. peacefulness; Mogilner, Kamvar, and Aaker 2011)。消费者们在品牌社区的言语能更深的投射出消费者对品牌的态度(Homburg, Ehm, and Artz 2015)。</p>
<p>而更宏大的层面，文本也能反映出文化差异。如美国人的表达相比东亚人具有更高的唤醒水平(Tsai 2007)，更喜欢用“我”而不是“我们”，也透露着崇尚个人主义，而不是集体主义。</p>
<p>透过时间，研究者也可以监测美国国民情绪是否在911恐怖袭击前后发生变化(Cohn, Mehl, and Pennebaker 2004)。透过新闻报告、歌词等内容也可以帮助研究者了解社会态度和社会规范，分析有关对女性、少数族裔(Boghrati and Berger 2019; Garg et al. 2018)和特定产业态度的时代变迁(Humphreys 2010)。</p>
<p>虽然文本分析并不容易，但企业和组织可以使用社交网络倾听民声。了解消费者是否喜欢新产品，消费者如何看待品牌，消费者最看重什么(Lee and Bradlow 2011; Netzer et al. 2012)。监管机构可以确定什么药物有不良部反映(Feldman et al. 2015; Netzer et al. 2012),公共卫生部门可以提前了解流感今年爆发最严重的地区(Alessa and Faezipour 2018),投资者可以预测股价涨跌 (Bollen, Mao, and Zeng 2011; Tirunillai and Tellis 2012).</p>
<br>
<h2 id="文本作用于消费者">文本作用于消费者</h2>
<p>文本不止可以反映生产者的信息，也可以知道文本如何影响消费者，消费者会有什么样的行为和选择。广告会塑造消费者的消费行为(Stewart and Furse 1986),报纸用语会改变消费者的态度(Humphreys and LaTour 2013), 消费者杂志会扭曲消费者产品分类感知(e.g., Rosa et al. 1999),电影剧本会影响观众的反应(Berger, Kim, and Meyer 2019; Eliashberg, Hui, and Zhang 2014; Reagan et al. 2016),等等。</p>
<p>需要注意的是文本的<strong>反映reflects</strong>和<strong>影响impacts</strong>并不是非此即彼，往往会同时起作用，尽管如此，研究人员倾向于使用<strong>文本差异</strong>来研究它俩。</p>
<p>当研究文本的<strong>reflects</strong>时，倾向于将reflects当作<strong>因变量</strong>，试图挖掘文本生产者的个性personality或属于什么社会团体。</p>
<p>当研究文本的<strong>impacts</strong>时，倾向于将impacts看作<strong>自变量</strong>，检验文本是否以及如何导致消费者诸如购买、分享和卷入行为。在本框架中，文本信息潜藏着某些潜在的影响力，是被当作诱因，对后续或者其他主体有作用力的。</p>
<h4 id="文本内容也会被客观条件影响"><strong>文本内容也会被客观条件影响</strong></h4>
<p>文本内容还可以被客观条件所塑造，如</p>
<ul>
<li>技术限制和社会文化基因(社会规范)</li>
<li>文本信息生产者与消费者之间的领域知识</li>
<li>先前客观历史</li>
</ul>
<p>首先，不同题材因社会规范，表达内容和方式有所不同。例如观点陈述时，新闻不如报告来的客观(Ljung 2000).酒店评论卡和其他反馈主要被极端观点占据。在Snapchat和其他SNS平台的推文达多较短，且昙花一现；而自在线评论经常较长且可以回溯到多年以前。</p>
<p>技术和物理也会改变文本表达。推特只能发少于280字符的推文。移动电话在键入方面受到限制，并且可能会影响人们在其上产生的文本（Melumad，Inman和Pham 2019; Ransbotham，Lurie和Liu 2019）。</p>
<p>其次，信息生产者和消费者之间的关系会影响说什么，怎么说。当生产者和消费者彼此很熟悉，文本表达会更非正式(Goffman 1959)，导致第三方很难通过直接明确的信息了解生产者与消费者之间的对话的态度。</p>
<p>这些因素对于解读文本信息至关重要，消费者给好朋友分享什么往往跟其他不同。企业可能会因为特定的冬季，其年报中可能会含有利好市场的信息。</p>
<p>最后，历史可能也会影响文本的内容。在留言板上，以前的帖子可能会影响以后的帖子；如果有人在先前的帖子中提出了要点，则被访者很可能会在以后的帖子中提及该要点。如果转发的帖子含有自己的分析，其内容会偏离大多数的帖子。更广泛地说，＃metoo或#blacklivesmatter之类的媒体框架可能使某些概念或事实更容易被演讲者使用，因此即使看起来似乎无关，它们也更可能出现在文本中（McCombs&amp;Shaw 1972; Xiong，Cho&amp;Boatwright 2019）。</p>
<br>
<h2 id="使用文本预测与理解">使用文本预测与理解</h2>
<p>文本除了<strong>reflects</strong> 和 <strong>impacts</strong>之外，还有<strong>predict</strong>和<strong>understanding</strong>。</p>
<h4 id="预测">预测</h4>
<p>某些文本研究出发点是做预测</p>
<ul>
<li>什么消费者最喜欢贷款(Netzer, Lemaire, and Herzenstein 2019)?</li>
<li>什么电影会大火(Eliashberg et al. 2014)?</li>
<li>未来股市走向(Bollen, Mao, and Zeng 2011; Tirunillai and Tellis 2012)?</li>
</ul>
<p>类似上面的研究，会使用很多文本特征来做机器学习和预测，研究人员不怎么关系任意的文本特质，他们更关心预测的表现。</p>
<p>用文本做预测的主要难点是，文本数据可以生成成千上万的特征(相当于变量x1，x2&hellip;xn)，而文本数据记录数甚至可能少于特征数。为了解决这个为题，使用新的特征分类方法，减少特征数量，又有可能存在拟合问题。</p>
<h4 id="理解">理解</h4>
<p>预测之外的研究主要是理解文本</p>
<ul>
<li>消费者怎样表达会如何影响口碑(Packard and Berger 2017)?</li>
<li>为何某些推文会被挑中分享？</li>
<li>歌曲为何变火？</li>
<li>品牌如何让消费者忠诚？</li>
</ul>
<p>理解的目标是理解为什么事情发生以及如何发生的。这类研究往往会用到心理学、社会学的方法，旨在理解文本的什么特征会导致什么后续结果，以及为什么产生这样的后果。</p>
<p>用文本做理解的难点是找出观测数据背后的因果关系。相应的，该领域的工作可能会强调实验数据，以允许对关键的独立变量进行操作。另一个挑战是解释文本特征之间的关系。使用第二人称的歌曲往往较火(Packard and Berger 2019b),但是为什么使用第二人称会火，单纯的文本数据很难挖掘出来作用机制。</p>
<p><strong>在prediction领域</strong>，研究人员利用 <strong>文本的reflects方面</strong> 来预测 生产者的状态、特性、满意度、性格等。研究人员利用 *文本impacts方面 * 来预测 消费者的阅读、分享和购买行为。</p>
<p><strong>在understand领域</strong>，研究人员利用 <strong>文本的reflects方面</strong> 来理解为什么当人们压抑的时候会使用特殊人称。利用 *文本impacts方面 * 来理解为何带有情绪的文本会更容易被阅读和分享。</p>
<br>
<h2 id="粘合营销各领域">粘合营销各领域</h2>
<p>尽管有reflects vs impacts， prediction vs understanding之分，做文本分析需要整合多种技能·技术和不同营销领域的相关知识。</p>
<p>就拿消费者行为学来说，在行为经济学大放异彩之前，假设情景操纵是存在争议的。实验可重复性问题，研究者开始寻找试图增强信度、效度的新工具。使用二手数据经常受限于只能做“是什么”的研究，不能做“为什么”的研究。但文本数据提供了做为什么的可能。例如在线评论可以用来理解为何某人购买了此商品的决策，尽管人们可能并不总是知道为什么要做某事，但他们的语言常常提供解释的痕迹（Pennebaker 2011），甚至超出了他们有意识地表达的范围。</p>
<p>定量建模人员一直在寻找新的数据源和工具来解释和预测行为。非结构化数据提供了一组丰富的预测变量，这些预测变量通常可以随时大规模获得，并且可以与结构化度量一起作为因变量或自变量组合。通过产品评论，用户驱动的社交媒体活动以及公司驱动的营销活动，文本可以实时提供可以阐明消费者需求/偏好的数据。这提供了对传统营销研究工具的替代或补充。在许多情况下，文本可以追溯到个人，从而可以区分个人差异和动态。</p>
<p>营销策略研究人员希望企业能实现其营销目标，并更好地理解影响组织成功的因素。文本分析提供了一种客观而系统的解决方案，以评估可能更有效的自然数据（例如，致股东的信，新闻稿，专利文本，营销信息，与分析师的电话会议）中可能的因素，如了解客户、合作伙伴和员工关系性质以及品牌情感强度(Kubbler，Colicev和Pauwels2017）使用词典和支持向量机方法来提取情绪并将其与消费者心态指标相关联。</p>
<p>也有学者借鉴人口和社会学领域，使用定性和内容分析研究文本数据。消费者文化领域，研究者对字里行间的意义、规范和价值观更感兴趣。文本分析提供了事物变化或比较不同事物的量化指标。文本分析为营销学者解锁了非结构化数据的开锁姿势，提供了文本的定性与定量研究的新疆界。</p>
<br>
<h2 id="文本分析工具方法和指标">文本分析工具、方法和指标</h2>
<p>给予前任做的文本数据驱动的洞察，有学者可能好奇如何开启文本研究之路。在本节会评述文本分析相关研究，包括</p>
<ul>
<li>构念如何用文本数据构建</li>
<li>将提取的文本信息整合到后续建模和分析中所需的过程</li>
</ul>
<p>本节目的是提供综合的入门指导，而是把可用的技术路线留给各位</p>
<ul>
<li>讨论各种方法如何恰当的使用</li>
<li>各种方法在使用时应该注意什么</li>
</ul>
<p>文本处理分析包括的步骤有</p>
<ol>
<li>数据预处理</li>
<li>文本信息提取</li>
<li>常用的文本分析指标</li>
</ol>
<br>
<h2 id="数据预处理">数据预处理</h2>
<p>文本数据是非结构化的脏数据。在任何常规数据分析之前，都要先将文本数据预先清洗处理，进而产生出类似excel表的干净的数据。常用的工具有R语言和Python语言，两种编程语言都有一套易用的数据预处理包。使用某些软件，如Linguistic Inquiry and Word Count (LIWC; Tausczik and Pennebaker 2010) 和WordStat (Peladeau 2016)之前，文本数据需要做少量的预处理。预处理可见Table 2和 Table 3 。</p>
<p><img loading="lazy" src="img/workflow.png" alt=""  />
</p>
<p><img loading="lazy" src="img/workflow2.png" alt=""  />
</p>
<h3 id="1-数据获取">1. 数据获取</h3>
<p>巧妇难为无米之炊，做文本研究的第一步就是采集数据，文本存在于邮件、公司年报、在线评论之中，无所不在，浩瀚无比。可以用人工手动复制粘贴到excel之中，但是效率太低，我们可以使用python设计网络爬虫采集数据。常见访问库requests、数据解析库pyquery和BeautifulSoup、数据存储库csv。</p>
<h3 id="2-分词">2. 分词</h3>
<p>将文本分词(切词)，数据尺度从章节段落拆解成颗粒度更小的词语层面，方便进行分析。但是要注意，英文是用空格间隔词语，而中文没有空格，还要注意粒度分的不能太细，如“the U. S.”按照空格分词会分出“the”、“U.” 和“S.”，导致美国这个实体被切分消失。</p>
<h3 id="3-清洗">3. 清洗</h3>
<p>网络爬虫在采集数据阶段，采集的并不是干净的文本数据，还有一些像HTML标签、图片、链接等字符，需要采集时清除掉。</p>
<h3 id="4-剔除停止词">4. 剔除停止词</h3>
<p>文本中有很多经常出现的无意义或者意义微乎其微的词，如&quot;a&quot;、the&quot;、&ldquo;is&rdquo; 等。一般情况下，这些词是需要剔除的。但是当研究的是书写者的语言风格，这些无意义词语往往含有千丝万缕的写作习惯信息，所以此时不能剔除。(e.g., Packard, Moore, and McFerran 2018；Pennebaker 2011).</p>
<h3 id="5-拼写">5. 拼写</h3>
<p>一般情况下，还需要将错误书写的词正确修改过来。但是当研究者对错误率感兴趣的时候，这时候就不要更正拼写问题。(e.g., Netzer, Lemaire, and Herzenstein 2019).</p>
<h3 id="6-词干化">6. 词干化</h3>
<p>词干化是为了将相同或者相近意思的词合并为一个词，如“car” ` “cars” 统一识别为 “car,”</p>
<h2 id="文本信息提取">文本信息提取</h2>
<p><img loading="lazy" src="img/%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e6%8f%90%e5%8f%96.png" alt=""  />
</p>
<h3 id="1-命名实体抽取">1. 命名实体抽取</h3>
<p>这是文本分析最基础、最简单、最常用的部分。例如姓名、地址、品牌、产品属性、情绪、词性等等都可以看作一种实体信息。实体抽取可以用来</p>
<ul>
<li>监测啥叫媒体讨论，商业竞争情报</li>
<li>也可用作机器学习中的特征（预测指标），预测是否是欺诈信息</li>
<li>构建更复杂的文本表达方式的度量指标，如情感、情绪、写作风格</li>
</ul>
<p>这部分一般需要强大的编程语言，如Python和R；当然有些情况下不用编程，使用WordStat也能做实体抽取。大多数情况下实体抽取经常伴随着专业词典或词表的使用，如(概念、品牌、分类、地址等)。通用的词典包括LIWC(Pennebaker et al. 2015)， EL 2.0 (Rocklage, Rucker, and Nordgren 2018), Diction 5.0 或General Inquirer for psychological states and traits (Berger and Milkman [2012]; Ludwig et al. [2013]; Netzer, Lemaire, and Herzenstein [2019]).</p>
<p>情感词典，如Hedonometer (Dodds et al. 2011), VADER (Hutto and Gilbert 2014), 和LIWC能计算出文本中含有的情感信息。情感分析经常使用词袋法（Bag of Words）计算文本中的情感。但是该方法不考虑词语在文本中的顺序，而顺序是能影响情感信息的。尽管词典法对构建构念和比较构念比较简单，但基于人工编码的机器学习方法(e.g.,Borah and Tellis 2016; Hartmann et al. 2018; Hennig-Thurau, Wiertz, and Feldhaus 2015)更适合做精准概念的度量(Hartmannetal.2019)，尤其是这个领域是不常见或者比较复杂。</p>
<p>如果研究者想挖掘出实体之间的关系就用到word2vec或者词嵌入word embedding (Mikolov et al. 2013)，这两种方法都把每一个词分配一个长度固定的向量，我们知道向量可以在空间中比较，如cos余弦计算词语之间的相似度。</p>
<h3 id="2-话题模型">2. 话题模型</h3>
<p>实体抽取有两个大问题:</p>
<ul>
<li>维度太高，经常能从文本数据中抽取出数千个实体</li>
<li>实体的解读与解释</li>
</ul>
<p>话题模型更多的是对文本的解释，而非预测(e.g., Berger and Packard 2018; Tirunillai and Tellis 2014)。话题模型最常见的是LDA，某个词以一定的概率属于话题，文本以多种话题按照一定的概率分布。</p>
<p>LDA是无监督学习，需要事先指定话题数，输出的结果是不同的类分布，需要研究者解读每一个话题到底是什么题材内容。话题区间范围一般建议结合统计分布和研究者经验确定话题数目。</p>
<h3 id="3-关系抽取">3. 关系抽取</h3>
<p>关系抽取可以用实体共现性来捕捉(e.g., Boghrati and Berger 2019; Netzer et al. 2012; Toubia and Netzer 2017).但营销学者对诸如产品、属性和情感之间的关系感兴趣。例如，研究者对评论中是否提及某个产品属性的问题。Feldman et al. (2015) and Netzer et al. (2012) 提供了药物与不良反应之间的关系来识别问题药物。</p>
<p>关系抽取用的实现大多思路不难，多是一些人工规则的设计，如产品“Ford”、属性“oid consumption”和问题“excessive”共现性来捕捉福特车耗油。然而这样的方法需要手写复杂的规则，现在变得慢慢不流行。</p>
<p>更通用的方法是机器学习法，人工标注相关的数据，训练机器学习模型。这类实现方法需要大量的人工标注，一种可用的工具是Stanford Sentence and Grammatical Dependency Parser (<a href="http://nlp.stanford.edu:8080/parser/">http://nlp.stanford.edu:8080/parser/</a>) 。该工具可以识别词语依存关系，如“the hotel was very nice,” ，“nice” 与 “hotel”相关联，说明这个hotel挺nice的。</p>
<p>当然，也可以扩文本之间做比较，这里不过多赘述。</p>
<br>
<h2 id="文本分析指标">文本分析指标</h2>
<p>早起市场营销，如在线评论领域的文本分析指标多为</p>
<ul>
<li>数量(e.g., Godes and Mayzlin 2004; Moe and Trusov2011)</li>
<li>效价，评论评分t (e.g., Godes and Silva 2012; Moe and Schweidel 2012; Ying, Feinberg and Wedel 2006)·</li>
<li>方差，如信息墒(e.g., Godes and Mayzlin 2004).</li>
</ul>
<p>然而如今这些指标经常忽略了文本的丰富度。以下几种是更好用的指标</p>
<h3 id="1-count-measure">1. count measure</h3>
<p>使用相应的词典，统计实体出现次数，这样可以对不同实体进行比较(Berger and Milkman 2012; Borah and Tellis 2016; Pennebaker et al. 2015; Schweidel and Moe 2014; Tirunillai and Tellis 2014)。缺点是更长的文本通常含有更多的实体(的数量)，还有一个局限就是某些实体会比其他实体更多的出现，如“电脑”商品的在线评论中“电脑”出现次数会远多于其他词。</p>
<h3 id="2-相似度">2. 相似度</h3>
<p>在某些情况下，研究者更对文档之间的相似度感兴趣(e.g., Ludwig et al. 2013).。两个广告之间的相似程度如何？两首歌的歌词相似程度多少？相似度的计算方法有cos余弦相似、jaccard相似 (e.g., Toubia and Netzer 2017)</p>
<h3 id="3-可读性">3. 可读性</h3>
<p>同样的意思可以用不同的难度的词汇去表达，造成阅读的难易程度。可读性反映了作者的内容复杂度和读者的阅读难度。(e.g., Ghose and Ipeirotis 2011)。</p>
<p>常见的可读性算法有Flesch–Kincaid和the simple measure of gobbledygook (SMOG)。可阅读性经常将得分设置到1-12分之间，在美国学校里阅读理解成绩水平得分就是1-12分。</p>
<br>
<h2 id="未来营销研究新机会">未来营销研究新机会</h2>
<h3 id="1-借鉴融合">1. 借鉴融合</h3>
<p>文本分析在营销界中可以起到促进各个子领域交叉授粉，避免同质化学术繁殖。品牌社群是最早被来自社会学背景的研究者发现和研究的(Mun˜iz and O’Guinn 2001)。随后，定性和定量范式研究者逐渐界定了概念、识别了社群中的地位和作用(e.g., Mathwick, Wiertz, and De Ruyter 2007)。文本分析可以让学者研究如何在更大尺度层面去量化社群中的消费者沟通行为。例如，社群中不同权利地位的人使用的语言是否存在差异，使用不同动态指标预测社群产出情况(e.g., Manchanda, Packard, and Pattabhitamaiah 2015)。研究人员也可以追踪到底哪类用户发明新用语，又是哪些人跟随使用这些新用语。研究可以检查人们是否随着时间的开始使用社群语言，并根据他们对群体语言的适应程度来预测哪些人可能会留下或离开(Danescu-Niculescu-Mizil et al. 2013; Srivastava and Goldberg 2017)。定量或机器学习的研究人员可能会发现社群中最常讨论的主题，以及这些主题如何随着社群的发展而动态变化。阐述性范式的研究人员可能会研究这些话语在概念上如何关联，以找到是哪些潜在社区准则促成成员留下。然后，营销战略领域的研究人员可能会使用或开发词典来将这些社区与公司绩效联系起来，并为公司提供有关如何保持不同品牌社区（或环境）成员参与度的指导。</p>
<p>不同子领域的营销学者会使用不同的技能集，研究不同的文本传播类型。消费者与消费者(consumer-to-consumer)之间的沟通主要研究的是两者间的行为，而营销战略学者倾向于研究企业与消费者、企业与企业之间的沟通。不同营销子领域的学者间的合作，能帮助他们结合不同的文本数据源。</p>
<p>它山之石可以攻玉，例如营销战略学者借鉴经济学领域的交易理论(代理理论)来研究企业间的关系，但现在营销战略相关发现可以用于研究消费者之间的沟通行为。</p>
<h3 id="2-扩展文本领域研究">2. 扩展文本领域研究</h3>
<p>我们希望看到更多的消费者-企业间的沟通的研究(e.g., Packard and Berger 2019a; Packard, Moore, and McFerran 2018)，这些沟通经常都是非约束非的，这其中蕴涵着有价值的关系数据，可以有很多应用价值。</p>
<p>而在企业间沟通方面，大多数侧重于沟通(Communication)的角色(e.g., Palmatier, Dant, and Grewal 2007)。然而在文本数据上，在词语层面上，有相关研究很少。例如很少有研究销售人员与消费者之间的信息交换类型。</p>
<p>类似的，在会计金融领域有很多人采用年报作为数据源(for a review, see Loughran and McDonald [2016])，但营销学者很少注意到公司与投资者之间的存在的研究机会。大多数学者只是用来研究如何预测公司股价或者开发新的公司市值估值模型。鉴于最近有兴趣将营销相关活动与公司估值联系起来（例如McCarthy和Fader 2018），这可能是一个需要进一步追求的领域。公司的所有沟通，包括年度报告等必需的文件，或广告和销售互动等任意形式的沟通，都可以用做观测变量，例如市场定位，营销能力，营销领导风格，甚至公司的品牌个性。</p>
<p>在消费者、企业、社会之间也存在着大量的研究机会。有关企业文化(规范)的数据，例如新闻媒体和政府报告，可能有助于阐明影响市场的力量。例如，要了解Uber这样的公司如何抵抗市场变化，可以研究市政厅会议的笔录和其他听取并回答市民意见的政府文件。诸如#metoo和#blacklivesmatter之类的社会运动形式的外来冲击影响了营销传播和品牌形象。未来研究的一种潜在途径是采用文化品牌化方法（Holt，2016年），研究不同公众如何定义，塑造和倡导市场中的特定含义。公司及其品牌并不是凭空存在的，它们独立于其经营所在的社会。但是，在市场营销方面的有限研究已经考虑了如何使用文本在社会层面上得出公司的意图和行为。例如，学者们展示了诸如locavores（这类人只食用当地产的食品；Thompson和Coskuner-Balli，2007年），时尚达人（Scaraboto和Fischer，2012年）以及博主（McQuarrie，Miller和Phillips，2012年），这几类人群塑造了市场。通过文本分析，可以衡量和更好地理解这些社会群体的意图对市场的影响。</p>
<p>未来研究的另一个机会是使用文本数据来研究文化和文化成功。跨学科研究了文化传播，艺术变革和创新传播等主题，目的是理解某些产品为何成功而其他产品却失败的原因(Bass 1969; Boyd and Richerson 1986; Cavalli-Sforza and Feldman 1981; Rogers 1995; Salganik, Dodds, and Watts 2006; Simonton 1980). While success may be random (Bielby and Bielby 1994; Hirsch 1972),可能的原因是没把握好消费者的口味偏好 (Berger and Heath 2005)。</p>
<p>通过在大范围更快速度地量化书籍、电影或其他文化物品，研究人员可以测量具体的叙事是否更具吸引力，更具情感波动性的电影是否更成功，使用某些语言特征的歌曲是否更有可能登上广告牌榜首 ，以及唤起特定情感的书籍是否售出更多。尽管没有像社交媒体数据那样广泛可用，但最近越来越多的文化项目数据可用。诸如Google Books语料库（Akpinar&amp;Berger 2015），歌曲歌词网站或电影脚本数据库等数据集可提供大量信息。此类数据可以使叙事结构分析，以识别&quot;基本情节&quot;'(Reagan et al 2016; Van Laer et al2019）。</p>
<h3 id="3-用文本测量关键构念">3. 用文本测量关键构念</h3>
<p>在个体层面上，情感和满意度可能是最常用的测量变量(e.g., Bu¨schken and Allenby, 2016; Homburg, Ehm, and Artz 2015; Herhausen et al. 2019; Ma, Baohung, and Kekre 2015; Schweidel and Moe 2014)其他从文本数据中提取的测量变量包括语言的真实性authenticity和情绪性emotion(e.g., Mogilner, Kamvar, and Aaker 2011; Van Laer et al. 2019)。也有心理学测量变量，如性格类型presonality type和建构水平construal level(Kern et al. 2016; Snefjella and Kuperman 2015),这都是潜在的可以借鉴应用到消费者话语研究的。</p>
<p>未来个体层面的研究会考虑社会认同和社会参与度， 研究人员目前对消费者已经可以测量情绪的积极或消极，但他们才刚刚开始探索重点（Rocklage&amp;Fazio 2015），信任，承诺和其他模式属性。为此，利用语用学的语言理论并研究语义学上的阶段性可能是有用的（Villarroel et al2017）。一旦开展了此类工作，我们建议研究人员仔细验证建议的方法，以按照上述方法测量此类构念。</p>
<p>在公司层面，已在公司生产的文本（例如年度报告和新闻稿）中确定了一些构念。诸如市场定位、广告目标、未来定位、欺骗意图、公司重点和创新定位均已使用此材料进行了测量和验证（详见Web Appendix Table 1)。未来企业层面的营销研究需要重新界定和丰富战略定位的测量(创新定位、市场驱动vs市场驱动定位)。组织文化、结构和能力由于难于测量，可以从企业、雇员和外部利益相关者的文本数据来测量(see Molner, Prabhu, and Yadav [2019])。类似的，企业领导层的思维和管理风格可以从他们怎么说来侦测(see Yadav, Prabhu, and Chandy [2007])。公司的绩效指标可以通过之前的公司相关文本数据进行预测(e.g., Herhausen et al. 2019)。从这个角度看，我们有很多使用数据的新机会。例如，从企业内部员工的相关信息(LinkedIn 和 Glassdoor)可以测量基于员工的品牌价值。最后，企业语言的更多微妙属性，如冲突、歧义、开放性都可以为管理学增加新发现。再比如，使用一些非正式文本数据，如员工邮件记录、销售通话记录或消费者服务中心通话记录。</p>
<p>营销工作较少在社会或文化层面上衡量结构，但这种工作趋向于集中于公司如何适应现有意义和规范的文化结构。例如，制度逻辑和合法性是通过分析媒体文本来衡量的，Berger等人的品牌公众崛起也增加了文化中对品牌的讨论（Arvidsson and Caliandro 2016）。在文化层面，营销研究可能会继续关注企业如何适应文化环境，但也可能会关注文化环境如何影响消费者。例如，对文化不确定性，风险，敌意和变化的测量可以理解文化对消费者和企业影响。通过文本衡量开放性和多样性也是适时探索的主题，并且可能会促进测量方面的创新，例如侧重于语言多样性。通过文本分析，也可以更好地理解重要的文化论述，例如围绕债务和信用的语言。与性别和种族有关的语言的测量可能有助于探索多样性和包容性，从而使公司和消费者对来自不同作家的文本做出反应。</p>
<br>
<h2 id="机遇与挑战">机遇与挑战</h2>
<p>本节是从技术角度出发探讨文本分析方法的新机遇与挑战。</p>
<h3 id="1-机遇">1. 机遇</h3>
<p>虽然我们的讨论集中于文本内容，但文本只是非结构化数据的一个示例，而音频，视频和图像则是其他示例。社交媒体帖子通常将文字与图片或视频结合在一起。平面广告通常会在精心构造的视觉效果上覆盖文字。尽管电视广告可能不会在屏幕上包含文本，但它可能具有音频轨道，其中包含与视频同步进行的文本。</p>
<p>直到最近，文本数据一直受到最多关注，这主要是由于存在提取有意义特征的工具。也就是说，诸如Praat（Boersma 2001）之类的工具允许研究人员从音频中提取信息（Van Zant和Berger 2019）。音频数据相对于文本数据的优势之一是，它以音调和语音标记的形式提供了丰富的内容，可以添加到所表达的实际单词中（Xiao，Kim和Ding 2013）。这使研究人员不仅可以研究说的内容，还可以研究说的方式，检查音调，语气以及其他声音或副语言特征如何塑造行为。</p>
<p>同样，最近的研究开发了分析图像的方法（Liu，Xuan等人2018），既可以表征图像的内容，也可以识别图像中的特征。文本和图像组合的影响的研究很少（例如Hartmann等人2019）。例如，可以根据图像的颜色来描述图像。在印刷广告的上下文中，当与特定调色板的图像结合使用时，文本内容的说服力可能会降低，而其他调色板可能会增强文本的说服力。与简单的图像结合使用，文本的重要性可能会非常明显。但是，当文本与复杂的图像配对时，观看者可能会主要关注图像，从而减少了文本的影响。在这种情况下，作为广告精美图片一部分的法律披露可能不会引起受众的注意。</p>
<p>当文本加到视频中时，其扮演的角色也引发了类似的问题。研究已经提出了表征视频内容的方法（例如Liu等人2018）。除了包含视频脚本之外，文本还可能在视觉上出现。除了在其中显示文本的音频上下文之外，其影响可能还取决于同时显示的视觉效果。也可能是其在视频中相对于视频开头的位置可能会降低其效果。例如，由于多种原因，在视频中稍后说出的情感性文字内容可能缺乏说服力（例如，观众在讲出文字时可能已经不再注意了）。或者，与音频配对的视觉效果可能对观众更具吸引力，或者视频的先前内容可能耗尽了观众的注意力资源。正如我们对图像和视频的讨论所暗示的那样，文本只是营销传播的一个组成部分。未来的研究必须调查其与其他特征的相互作用，不仅包括其出现的内容，还包括其出现的时间（Kanuri，Chen和Sridhar 2018），以及在哪种媒体上。</p>
<h3 id="2-挑战">2. 挑战</h3>
<p>尽管机会众多，但文本数据也带来了各种挑战。首先是面临可解释性的挑战。在某些方面，文本分析似乎提供了衡量行为过程的更客观的方法。例如，一个人可以计算第一人称“ I”和第二人称“ you”。第一人称在文本中越多，说明这个人更关心自己 （Berger 2014），这种量化词语数量的方法提供看起来更像很客观像真理的东西。但是，尽管该过程的一部分肯定是更客观的（例如，不同类型的代词的数量），但此类度量与基础过程（即，关于口碑传播者的说法）之间的联系仍然需要一定程度的解释。其他潜在的行为方式甚至更难以计数。例如，虽然某些词（例如“love”）通常是积极的，但它们的积极性可能在很大程度上取决于特质个体差异和上下文。</p>
<p>更普遍地，在理解文本信息出现的上下文中存在挑战和机遇。例如，餐厅评论可能包含很多否定词，但这是否意味着该人更讨厌食物，服务或餐厅？包含更多第二人称代词（“ you”）的歌曲可能会更成功（Packard and Berger 2019b），但要了解原因，了解歌词是否使用“ you”作为句子的主语或宾语是有帮助的。上下文提供了含义，而且越多的人不仅了解正在使用的单词，而且还了解如何使用它们，则越容易获得新知识新洞察。基于词典工具特别容易对使用场景变化特别敏感，建议尽可能使用针对特定研究环境创建的词典（例如，Loughran和McDonald [2016]开发的财务情感工具）。</p>
<p>数据隐私挑战是一个重大问题。研究通常使用从网站上抓取的在线产品评论和销售排名数据（Wang，Mai和Chiang 2013）或从社交媒体平台上抓取的消费者的活动数据（Godes和Mayzlin 2004；Tirunillai和Tellis 2012）。尽管这种方法很普遍，但是法律问题已经开始出现。LinkedIn未能成功阻止一家初创公司抓取用户公共资料中发布的数据（Rodriguez，2017）。虽然根据法律可能允许收集公共数据，但它可能与那些拥有研究人员感兴趣的数据的平台的服务条款相冲突。</p>
<p>随着从数字化文本和其他形式的数字化内容（例如图像，视频）中提取见解的兴趣日益浓厚，研究人员应确保他们已获得进行工作的适当权限。不这样做可能导致开展此类项目变得更加困难。一种潜在的解决方案是创建一个学术数据集，例如Yelp提供的数据集（https://www.yelp.com/dataset），该数据集可能包含过时或经过清理的数据，以确保不会产生 公司的运营或用户隐私风险。</p>
<p>对数字化文本以及其他用户创建的内容的收集和分析，也引发了有关用户对隐私的期望的问题。随着欧盟《通用数据保护条例》的发布以及有关Cambridge Analytica从Facebook收集用户数据的能力的启示，研究人员必须注意其工作的潜在滥用。我们还应考虑超出用户生成内容的预期用途的程度。例如，尽管用户可能会理解，Facebook采取的行动可能会导致他们针对与其互动的品牌进行专门的广告宣传，但他们可能无法预期其Facebook和Instagram活动的全部内容都将被用于构建其他品牌可能使用的心理特征。了解消费者关于其在线行为及其提供的文字的隐私偏好可以为从业者和研究人员提供重要的指导。未来研究的另一个亮点是可以提高营销的精确度，同时最大限度地减少对隐私的侵犯（Provost et al 2009）。</p>
<br>
<h2 id="总结">总结</h2>
<p>沟通是营销的重要方面，涵盖组织与合作伙伴之间，企业与消费者之间以及消费者之间的沟通。文本数据包含这些交流的详细信息，并且通过自动文本分析，研究人员已准备好将这种原始材料转换成有价值的见解。文本数据使用方面的许多最新进展是在营销之外的领域开发的。当我们展望未来和营销人员的角色时，这些最新进展应作为示例。营销人员在消费者，公司和组织之间的接口上处于有利位置，可以利用和改进工具来提取文本信息，以解决当今企业和社会所面临的一些关键问题，例如错误的信息滥用。营销提供了一种宝贵的观点，对这次对话至关重要，但这只有通过更广阔的视野，打破理论和方法论的孤岛，并与其他学科合作，我们的研究才能吸引尽可能多的受众来影响公众话语。我们希望这个框架能够鼓励人们对界定营销的界限进行反思，并为未来的突破性见解开辟道路。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本分析在经管领域中的应用概述</title>
      <link>https://textdata.cn/blog/review_about_the_application_of_text_mining_in_management_science/</link>
      <pubDate>Tue, 23 Nov 2021 21:30:10 +0600</pubDate>
      
      <guid>/blog/review_about_the_application_of_text_mining_in_management_science/</guid>
      <description>在大数据的今天，通过互联网超文本链接，无数的**个人、团体、公司、政府**等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。**社会、管理、经济、营销、金融**等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是三份文档翻译汇总而来，我觉得讲的挺明白的，其中加入了我的一点点理解和扩充。In today&amp;#39;s big data world, through Internet hypertext links, countless individuals, groups, companies, governments and other organizational entities are deeply embedded in the Internet world, leaving a large amount of text in the Internet world. **Society, management, economics, marketing, finance** and other disciplines can study a large amount of texts on the Internet, and broaden the research objects and research fields. Most of the content below is a summary of the translations of the three documents. I think it is quite clear, and I have added a little bit of my understanding and expansion.</description>
      <content:encoded><![CDATA[<p>在大数据的今天，通过互联网超文本链接，无数的<strong>个人、团体、公司、政府</strong>等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。<strong>社会、管理、经济、营销、金融</strong>等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是三份文档翻译汇总而来，我觉得讲的挺明白的，其中加入了我的一点点理解和扩充。</p>
<h2 id="一文本产生及其作用方式">一、文本产生及其作用方式</h2>
<ul>
<li>How text <strong>reflects</strong> its producer？</li>
<li>How text <strong>impacts</strong> its receiver？</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">graph LR
   Text_Producer --&gt; Text
   Text --&gt; Text_Receiver
   Text_Receiver --&gt;Text
   Text --&gt; Text_Producer
</code></pre></div><p>文本信息的==生产者producer== 与 ==消费者receiver==，涵盖 ==个人、公司(组织)、国家(社会)==三个层面。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">graph LR
    Consumers --&gt; Firms
    Consumers --&gt; Investors
    Consumers --&gt; Society
    Firms --&gt; Consumers
    Firms --&gt; Investors
    Investors --&gt; Firms
    Investors --&gt; Society
    Firms --&gt; Society
    Society --&gt; Investors
    Society --&gt; Consumers
    
</code></pre></div><p><img loading="lazy" src="img/%e7%94%9f%e4%ba%a7%e4%b8%8e%e6%b6%88%e8%b4%b9.png" alt=""  />
</p>
<blockquote>
<p>需要注意的是文本的==反映Reflects==和==影响Impacts==并不是非此即彼，往往会同时起作用。</p>
</blockquote>
<table>
<thead>
<tr>
<th>&mdash;</th>
<th>研究目的</th>
<th>自变量</th>
<th>因变量</th>
<th>因变量</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Reflects</strong></td>
<td>文本可以反映<strong>producer</strong>的一些信息，帮助研究者理解producer。<br>例如试图挖掘producer的个性personality或属于什么社会团体。</td>
<td>了解公司的品牌个性；<br>年报也会有未来业绩表现的有价值线索；<br>消费者们在品牌社区的言语能更深的投射出消费者对品牌的态度；<br>而更宏大的层面，文本也能反映出文化差异。<br>了解消费者是否喜欢新产品，消费者如何看待品牌，消费者最看重什么</td>
<td>文本</td>
<td>文本</td>
</tr>
<tr>
<td><strong>Affects</strong></td>
<td>知道文本如何影响<strong>receiver</strong>，receiver会有什么样的行为和选择。</td>
<td>检验文本是否以及如何导致消费者诸如购买、分享和卷入行为。<br>广告会塑造消费者的消费行为<br>消费者杂志会扭曲消费者产品分类感知<br>电影剧本会影响观众的反应</td>
<td>文本消费者</td>
<td>文本消费者</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="二如何使用文本数据">二、如何使用文本数据</h2>
<table>
<thead>
<tr>
<th>&mdash;</th>
<th>Reflects</th>
<th>Affects</th>
<th>目的</th>
<th>应用</th>
<th>难点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Predict</strong></td>
<td>预测 <strong>producer</strong>的状态、特性、性格等</td>
<td>预测 <strong>receiver</strong>阅读、分享和购买行为</td>
<td>研究人员不怎么关系任意的文本特质，他们更关心预测的表现。</td>
<td>什么消费者最喜欢贷款;<br>什么电影会大火;<br>未来股市走向;<br></td>
<td>文本数据可以生成成千上万的特征(相当于变量x1，x2&hellip;xn)，而文本数据记录数甚至可能少于特征数。<br>为了解决这个为题，使用新的特征分类方法，减少特征数量，又有可能存在拟合问题。</td>
</tr>
<tr>
<td><strong>Understanding</strong></td>
<td>为什么当人们压抑的时候会使用特殊人称。</td>
<td>来理解为何带有情绪的文本会更容易被阅读和分享</td>
<td>理解为什么事情发生以及如何发生的<br/>这类研究往往会用到心理学、社会学的方法，旨在理解文本的什么特征会导致什么后续结果，以及为什么产生这样的后果。</td>
<td>消费者怎样表达会如何影响口碑;<br>为何某些推文会被挑中分享？<br> 歌曲为何变火？<br> 品牌如何让消费者忠诚？</td>
<td>找出观测数据背后的因果关系。相应的，该领域的工作可能会强调实验数据，以允许对关键的独立变量进行操作。<br>另一个挑战是解释文本特征之间的关系。</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="三文本信息的指标">三、文本信息的指标</h2>
<p>粗略的分，文本信息可以分为定性与定量两种类型</p>
<table>
<thead>
<tr>
<th style="text-align:left">定性/量</th>
<th>分析方法</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>定性（text as text）</strong></td>
<td>质性（扎根）</td>
<td>依靠研究者领域知识，可以对少量的数据做出深刻洞见。</td>
<td>难以应对大规模数据；<br>编码过程并不能保证唯一；</td>
</tr>
<tr>
<td style="text-align:left"><strong>定量 textual data(text as data)</strong></td>
<td>明显的文本特征，如词频、可阅读性</td>
<td>标准如一;<br>适合大规模文本挖掘；<br>纷繁复杂中涌现出潜在规律</td>
<td>需要破坏文本的结构，丧失了部分信息量</td>
</tr>
</tbody>
</table>
<p>早先的营销领域，如在线评论文本分析指标多为</p>
<ul>
<li><strong>数量，如文本长度</strong>(e.g., Godes and Mayzlin 2004; Moe and Trusov2011)</li>
<li>**情感得分(效价，评论评分) **(e.g., Godes and Silva 2012; Moe and Schweidel 2012; Ying, Feinberg and Wedel 2006)·</li>
<li><strong>方差，如信息墒</strong>(e.g., Godes and Mayzlin 2004).</li>
</ul>
<p>然而如今这些指标经常忽略了文本的丰富度。以下几种是更好用的指标</p>
<table>
<thead>
<tr>
<th style="text-align:left">指标</th>
<th style="text-align:left">功能</th>
<th style="text-align:left">补充</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>实体词词频</strong></td>
<td style="text-align:left">使用相应的实体词典，统计实体出现次数，这样可以对不同实体进行比较</td>
<td style="text-align:left">更长的文本通常含有更多的实体(的数量)；<br>还有一个局限就是某些实体会比其他实体更多的出现，如“电脑”商品的在线评论中“电脑”出现次数会远多于其他词。</td>
</tr>
<tr>
<td style="text-align:left"><strong>相似度</strong></td>
<td style="text-align:left">文档之间的相似度感兴趣。<br>如两个广告之间的相似程度如何？<br>两首歌的歌词相似程度多少？</td>
<td style="text-align:left">相似度的计算方法有<br>cos余弦相似<br>jaccard相似</td>
</tr>
<tr>
<td style="text-align:left"><strong>可读性</strong></td>
<td style="text-align:left">同样的意思可以用不同的难度的词汇去表达，造成阅读的难易程度。可读性反映了作者的内容复杂度和读者的阅读难度。</td>
<td style="text-align:left">常见的可读性算法有Flesch–Kincaid和the simple measure of gobbledygook (SMOG)。<br>可阅读性经常将得分设置到1-12分之间，在美国学校里阅读理解成绩水平得分就是1-12分。</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="四文本分析步骤">四、文本分析步骤</h2>
<p><img loading="lazy" src="img/%e5%88%86%e6%9e%90%e6%ad%a5%e9%aa%a4.png" alt=""  />
</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>步骤</th>
<th>解释</th>
<th>中文</th>
<th>英文</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>读取数据</strong></td>
<td>数据一般存储于不同的文件夹不同文件内，需要将其导入到计算机</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td><strong>分词</strong></td>
<td>导入到计算的文本是字符串数据，需要整理为更好用的列表</td>
<td>例如“我爱你中国”分词后<br>得到[&ldquo;我&rdquo;, &ldquo;爱&rdquo;, &ldquo;你&rdquo;, &ldquo;中国&rdquo;]</td>
<td>&ldquo;I love China&quot;分为<br>[&ldquo;I&rdquo;, &ldquo;love&rdquo;, &ldquo;China&rdquo;]</td>
</tr>
<tr>
<td>3</td>
<td><strong>剔除符号和无意义的停止词</strong></td>
<td>为了降低计算机运行时间，对分析结果影响较小的字符，诸如符号和无意义的词语需要剔除掉</td>
<td>如“的”，“她”， ”呢”， “了”</td>
<td>&ldquo;is&rdquo; , &ldquo;a&rdquo;, &ldquo;the&rdquo;</td>
</tr>
<tr>
<td>4</td>
<td><strong>字母变小写，词干化</strong></td>
<td>同义词归并，同主体词归并</td>
<td>“中铁”，“中国铁建”，“中铁集团”都可以归并为“中铁”</td>
<td>先变为小写，这样“I”和“i”都归并为“i”；<br>“was”，“are”，“is”都归并为“be”</td>
</tr>
<tr>
<td>5</td>
<td><strong>构建文档词频矩阵</strong></td>
<td>使用一定的编码方式，即用某种方式表示文本。常见的有词袋法、tf-idf；<br>可以使用scikit-learn构建文档词频矩阵，但中英文略有区别，需要注意</td>
<td>“我爱你中国”需要先整理为“我 爱 你 中国”</td>
<td>“I love China”</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="五文本分析技术对比">五、文本分析技术对比</h2>
<p><img loading="lazy" src="img/%e5%88%86%e6%9e%90%e6%96%b9%e6%b3%95.png" alt=""  />
</p>
<p>从左向右，自动化程度越来越高，人工介入的越来越少</p>
<table>
<thead>
<tr>
<th>技术</th>
<th>描述</th>
<th>优点</th>
<th>缺点</th>
<th>常被应用(领域)</th>
<th>软件</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>主题分析</strong>Thematic analysis</td>
<td>需要有经验的人员基于自身经验和李俊杰，对研究的数据进行挖掘。编码过程为迭代进行</td>
<td>使用参与者自己的话语或者构念来挖掘数据，对少量文本理解的更深入</td>
<td>属于时间、劳动密集型任务，不适合大规模数据。<br>由于不同的编码人员有不同的经历和偏好，编码过程的标准不可靠</td>
<td>社会学、管理学</td>
<td>Nvivo；</td>
</tr>
<tr>
<td><strong>内容分析/基于字典方法</strong></td>
<td>统计文本中词语/词组的出现频率</td>
<td>允许对研究的数据进行定量分析</td>
<td>采用的词典应尽量与研究问题适应，词典适配性问题突出</td>
<td>管理学</td>
<td>LIWC、Nvivo、DICTION；</td>
</tr>
<tr>
<td><strong>词袋法</strong>（Bag of words）</td>
<td>将文本字符串转为计算机能理解的数字化向量</td>
<td>编码标准稳定简单，具有统计学特性，扩展性强</td>
<td>编码过程忽略词语的先后顺序</td>
<td>管理学</td>
<td>Python的scikit-learn、gensim、nltk等；R</td>
</tr>
<tr>
<td><strong>监督学习</strong>(Supervise models),如SVM、Bayes、Logistic Regression</td>
<td>研究者要知道输入数据X和标签y；需要核实的模型需要X和y之间的关系和规律</td>
<td>允许事先定义编码规则(如选择词袋法还是tfidf)；逻辑简单</td>
<td>需要高质量的标注数据(工作量大)；you与特征词太多，训练的模型很容易过拟合。</td>
<td>计算机学、政治学、管理学</td>
<td>Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）</td>
</tr>
<tr>
<td><strong>无监督学习</strong>(Kmeans、 LDA话题模型)</td>
<td>使用聚类、话题分析，让计算机自动对数据进行分组</td>
<td>在没有人工标注的情况下，加速了数据的“标注”或“分类”</td>
<td>“标注”是机器按照数字特征进行的分组，需要研究者解读才可以赋予“标准“意义；训练过程需要大量的调参</td>
<td>计算机学、政治学、管留学</td>
<td>Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）</td>
</tr>
<tr>
<td><strong>自然语言处理</strong></td>
<td>按照人类对语言的理解进行建模，考虑词语顺序</td>
<td>计算机自动化；可分析语义</td>
<td>大多数模型是人类无法解读的黑箱；<br>虽然代码编程量小，但训练代码耗时巨大</td>
<td>计算科学；市场营销；心理学</td>
<td>pytorch、tensorflow</td>
</tr>
</tbody>
</table>
<p>据被压缩成词组频数，定性的文本数据转化为定量的频数。本课程中会涉及到的内容</p>
<ul>
<li><input disabled="" type="checkbox"> Thematic Analysis 定性</li>
<li><input disabled="" type="checkbox"> Content Analysis</li>
<li><input checked="" disabled="" type="checkbox"> Dictionary</li>
<li><input checked="" disabled="" type="checkbox"> Bag of words 词袋法</li>
<li><input checked="" disabled="" type="checkbox"> Supervised ，监督学习 文本分类问题</li>
<li><input checked="" disabled="" type="checkbox"> Unsupervised，如非监督LDA话题模型</li>
<li><input disabled="" type="checkbox"> Natural language processing</li>
</ul>
<p><br><br></p>
<h2 id="应用案例">应用案例</h2>
<h3 id="众筹融资成功率与语言风格的说服性-基于kickstarter的实证研究">众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究</h3>
<p>摘要：众筹融资效果决定着众筹平台的兴衰。 众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。 而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。 不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。</p>
<p>首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。</p>
<p>然后，==借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。==</p>
<p>最后，建立语言说服风格对项目筹资影响的计量模型，并 对 ==Kickstarter 平台上的 128345 个项目进行实证分析==。 总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。</p>
<p>关键词：众筹 融资 语言风格 说服性 投资意愿</p>
<p><img loading="lazy" src="img/%e4%bc%97%e7%ad%b9%e8%af%ad%e8%a8%80%e9%a3%8e%e6%a0%bc.png" alt=""  />
</p>
<h3 id="copycats-vs-original-mobile-apps">Copycats vs. Original Mobile Apps</h3>
<p>摘要: 尽管移动应用程序市场的增长为移动应用程序开发人员创新提供了巨大的市场机会和经济诱因，但它也不可避免地刺激了模仿者开发盗版软件。原始应用的从业人员和开发人员声称，模仿者窃取了原始应用的想法和潜在需求，并呼吁应用平台对此类模仿者采取行动。令人惊讶的是，很少有严格的研究来分析模仿者是否以及如何影响原始应用的需求。</p>
<p>==进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原始或模仿者并检测两种模仿者的方法：欺骗性和非欺骗性。==</p>
<p>根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的==5,141个开发人员的10,100个动作游戏应用程序==样本中，模仿应用程序对原始应用程序需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原件的需求。</p>
<p>结果表明，从总体上讲，模仿者对原始移动应用程序需求的影响在统计上是微不足道的。==我们的研究通过提供一种识别模仿者的方法==，并提供模仿者对原始应用需求的影响的证据，为越来越多的移动应用消费文献做出了贡献。</p>
<p><img loading="lazy" src="img/copycat.png" alt=""  />
</p>
<h3 id="lazy-prices">LAZY PRICES</h3>
<p>摘要: 使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对报告进行积极更改时，这种行为蕴含着公司未来运营的重要信号。</p>
<p>财务报告的语言和结构的变化也对公司的未来收益产生重大影响：做空&quot;变化&quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。</p>
<p>研究发现，对10-K的变化可以预测未来的收益、获利能力、未来的新闻公告，甚至未来的公司破产。同时，不做任何变化的公司将获得显著的异常收益。与资产价格典型的反应不足研究不同，我们发现没有任何与这些变化相关的公告效应–仅在后来通过新闻，事件或收益披露信息时才产生回报–暗示投资者并未注意到整个公众领域的这些变化。</p>
<p><img loading="lazy" src="img/lazyprice1.png" alt=""  />
</p>
<blockquote>
<p>纽约时报在2010年4月23日发了一条FDA将有对输液泵(infusion pumps)更严格对审批管理规定的新闻，新闻中提到了Baxter公司。新闻公布当天，Baxter股价大跌。</p>
<p>10天后的（2010年5月4日），Baxter宣布召回问题的输液泵产品，股价当天再次大跌。</p>
</blockquote>
<p><img loading="lazy" src="img/lazyprice2.png" alt=""  />
</p>
<h2 id="相关文献">相关文献</h2>
<blockquote>
<p>[1]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &ldquo;Uniting the tribes: Using text for marketing insight.&rdquo; Journal of Marketing (2019): 0022242919873106.</p>
<p>[2]Kenneth Benoit. July 16, 2019. “Text as Data: An Overview” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. <em>Handbook of Research Methods in Political Science and International Relations</em>. Thousand Oaks: Sage.</p>
<p>[3]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. &ldquo;A review of best practice recommendations for text analysis in R (and a user-friendly app).&rdquo; <em>Journal of Business and Psychology</em> 33, no. 4 (2018): 445-459.</p>
<p>[4]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.管理世界.2016;5:81-98.</p>
<p>[5]Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; <em>Information Systems Research</em> 29, no. 2 (2018): 273-291.</p>
<p>[6]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. &ldquo;Lazy prices.&rdquo; <em>The Journal of Finance</em> 75, no. 3 (2020): 1371-1415.</p>
</blockquote>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>读完本文你就了解什么是文本分析</title>
      <link>https://textdata.cn/blog/read_this_you_will_know_what_is_text_mining/</link>
      <pubDate>Mon, 22 Nov 2021 23:40:10 +0600</pubDate>
      
      <guid>/blog/read_this_you_will_know_what_is_text_mining/</guid>
      <description>专注经济管理科研领域的Python数据分析，涵盖数据分析主要环节，如Python网络爬虫、Pandas数据探索性分析、中英文文本数据清洗、机器学习与自然语言处理。开发有专门的Python经济管理文本数据挖掘视频课程.Focus on Python data analysis in the field of economic management research, covering the main links of data analysis, such as Python web crawler, Pandas data exploratory analysis, Chinese and English text data cleaning, machine learning and natural language processing. Developed a special Python economic management text data mining video course</description>
      <content:encoded><![CDATA[<h2 id="一文本的作用">一、文本的作用</h2>
<p>文本涉及两个主体，即<strong>文本生产者</strong>和<strong>文本消费者</strong>：</p>
<ul>
<li>文本生产者: 生成文本的主体；传递生产者想表达的内容，可能也会潜在蕴含着生产者的一些特质属性</li>
<li>文本消费者: 阅读文本的主体；消费者阅读这段文本时，文本又对消费者认知活动产生影响。</li>
</ul>
<p>在大数据的今天，通过互联网超文本链接，无数的<strong>个人、团体、公司、政府</strong>等不同组织形态的主体均深深嵌入到互联网世界，在网络世界中留下了大量的文本。<strong>社会、管理、经济、营销、金融</strong>等不同学科，均可以研究网络上海量的文本，扩宽的研究对象和研究领域。下面大部分内容是从政治学和经管领域的两份文档翻译来，我觉得讲的挺明白的，其中加入了我的一些理解和扩充。</p>
<p><br><br></p>
<h2 id="二-理解文本">二、 理解文本</h2>
<ul>
<li>text as text 原始的文本，定性的文本</li>
<li>textual data(text as data)  量化后的文本数据，可定量</li>
</ul>
<h3 id="21-text-as-text">2.1 text as text</h3>
<blockquote>
<p>text as text 原始的文本，定性的文本</p>
</blockquote>
<p>文本的重点是传递着某种东西，从某种意义上说，所有形式的文本都包含可以被视为数据形式的信息。因此，文本总是以某种方式提供信息（即使我们不了解如何操作）。但是，言语活动的主要目标不是记录信息，而是进行交流：传达思想，指令，查询等。我们可以记录下来并将其视为数据，但是将我们的想法或思想表达为单词和句子的目的主要是交流，而不是将我们的想法或思想记录为数据形式。大多数数据是这样的：它表征的活动与数据本身完全不同。</p>
<p>例如，在经济学中，可能是我们想要刻画的经济交易（使用价值媒介交换商品或服务），而数据是以某种聚合形式对这些交易进行抽象，这有助于我们理解交易的意义。通过就抽象的相关特征达成共识，我们可以记录并分析人类活动，例如制造业，服务业或农业。从通信行为中提取文本数据特征的过程遵循相同的过程，但有一个主要区别：由于原始文本可以直接通过记录的语言与我们交谈，因此文本首先不需要进行处理或抽象化待分析。但是，<strong>我在这里的论点是，特征抽象的过程是将文本视为数据而不是直接将其视为文本的方法的独特之处</strong>。</p>
<p><strong>具有讽刺意味的是，只有当我们破坏了直接理解文本的能力时，才有可能利用文本的数据获取洞察力</strong>。为了使它作为数据有用，我们必须消除原始文本的结构，将文本转换为结构化的表格数据。定量分析是理解非语言数据的起点；另一方面，非结构的文本变成丑陋表格数据的过程，出于统计分析或机器学习目的，我们经常质疑这一过程丢失了什么信息。</p>
<p>机器是愚蠢的，但是将文本视为数据意味着让愚蠢的机器处理并可能分析我们的文本。关键是，为了<strong>将文本作为数据</strong> 而不是<strong>文本仅仅是文本</strong>，<strong>我们必须破坏原始文本的直接可解释性，但目的是从其样式化特征中进行更系统，更大规模的推断</strong>。我们应该坚定不移地认识到这一过程，但也不要因此而寝食不安，因为<strong>将文本作为数据进行分析的重点永远不是解释数据而是挖掘其深层次的模式</strong>。数据挖掘是一个破坏性的过程-正如采挖矿山资源-为了开采其宝贵资源，开发过程不可避免会破坏地表形态和环境。</p>
<br>
<h3 id="22-latent-versus-manifest-characteristics-from-textual-data">2.2 Latent versus manifest characteristics from textual data</h3>
<blockquote>
<p>textual data(text as data)  量化后的文本数据，可定量的数据。所以小标题我翻译为“量化后的文本数据隐藏的信息vs直观可见的信息”，</p>
</blockquote>
<p>在政治学领域，我们通常最感兴趣的不是文本本身，而是文本透漏给我们有关作者的一些隐藏特性。在政治（以及心理学）研究中，我们有关政治和社会行为者的一些重要理论，很多时候直接观察行为活动很难观察到其内在的品质。</p>
<p>例如，意识形态是研究政治竞争和政治偏好的基础，但是我们没有直接的衡量工具来记录个人或政党有关“社会和道德自由政策与保守政策”的相对偏好。其他偏好，包括支持或反对特定政策，如1846年废除了英国的《玉米法》（Schonhardt-Bailey，2003年）；在关于《莱肯公约》的辩论中支持或反对进一步的欧洲一体化（Benoit等，2005）；再比如支持或反对不信任运动（Laver和Benoit，2002年）。</p>
<p>这些偏好是作为政治行为者的内部状态而存在的，无论这些行为者是立法者，政党，代表还是候选人，都无法直接观察。<strong>非言语行为指标也可用于推断这些信息，但事实表明，政治行为者所说的话比其他行为形式更为真诚。</strong></p>
<p>因此，<strong>文本数据（Textual data）可能包含有关取向和信念的重要信息，对于这些取向和信念，非语言形式的行为可能会充当不良指标。长期以来，心理学领域也一直将言语行为作为可观察到的潜在兴趣状态的暗示，例如人格特质</strong>（例如Tausczik和Pennebaker，2010年）。缺少增强的询问技术或头脑阅读技术来识别政治和社会行为者的偏好，信念，意图，偏见或个性，下一个最佳选择是根据其说话或书写的内容来收集和分析数据。<strong>关注的对象不是文本包含的内容，而是其内容作为有关潜在特征的数据所揭示的内容，这些潜在特征为其提供了可观察的含义</strong>。最后一句话比较难理解，可以理解为万事万物有联系，通过联系思维来挖掘文本中的信息。</p>
<p>文本数据(Textual data)还可能具有较为明显的特征，例如，政治传播的许多领域都与文本所指出的潜在特征无关，而与文本本身所包含的传播形式和性质有关。举一个经典的例子，<strong>在一个著名的政治局委员对斯大林诞辰70周年之际的文章的研究中，莱特斯，伯努特和加索夫（1951）能够衡量各团体在共产主义意识形态方面的差异</strong>。在这一政治事件中，这些信息不仅预示了潜在的方向，而且还预示了在可预见的斯大林死后事件中有关领导权斗争的某种政治动作。这些信息本身是重要的，<strong>这些信息只能从每个政治局委员撰写的公开文章中搜集而来，它们必须充分了解将在党和苏联苏维埃新闻，并由其他政权参与者解释为信号</strong>。再举一个例子，如果我们对一个政治演说家是使用民粹主义还是种族主义语言感兴趣，那么该语言将直接以民粹主义或种族主义术语或参考形式出现在文本中，而要紧的是它们是否被使用。与其说这些术语代表什么，不如说是什么。<strong>例如Jagers和Walgrave（2007）在研究比利时政党的政党政治广播时，发现极右翼政党Vlaams Blok所使用的民粹词语远比其他比利时政党丰富的多。</strong></p>
<p>在实践中，从文本<strong>可观察到的明显特征</strong>与<strong>潜在特征之间的特征</strong>的有时候这两个概念区分的并不明显。举例来说，文体风格可以用一些明显的特征词对文本进行量化，体现出作者的一些写作偏好。例如，在使用适用于政治文本的<strong>可读性度量改编</strong>的研究中，我们可能会对<strong>政治成熟度</strong>的潜在水平感兴趣，这可以用来衡量说话者的意图或<strong>说话者的特征</strong>，这一点从观察到的文本样本中可以看出。或者，我们可能会对它们在可读性上的明显差异感兴趣，这是传播媒介更直接指标。例如，在对英国议会历史演讲的研究中，Spirling（2016）将19世纪末期向简单语言的转变归因于广播扩展特许经营的民主化效应。Benoit，Munger和Spirling（2019）使用类似的措施，比较了同一位总统当天在同一天发表的美国总统国情咨文演讲的样本，但其口头和书面形式均表明口头形式使用的语言较为简单。前一项研究可能对语言的<strong>易用性</strong>感兴趣，该语言的易用性是政治代表制更潜在的特征的指标，而后一项分析可能更侧重于交付媒介的明显后果。<strong>对于许多使用文本数据的研究设计而言，区别更多是研究目标的问题，而不是结构化和分析文本数据的某些内在方式。</strong></p>
<br>
<h3 id="23-文本分析的步骤">2.3 文本分析的步骤</h3>
<p><img loading="lazy" src="img/textprocesssteps.png" alt=""  />
</p>
<p>完整的文本分析步骤包括:</p>
<ol>
<li>读取数据</li>
<li>分词(中文必须有这一步，由于英文是空格间隔的语言，英文有时候不需要分词）</li>
<li>剔除符号和无意义的停止词</li>
<li>字母变小写，词干化</li>
<li>使用一定的编码方式构建文档词频矩阵</li>
</ol>
<table>
<thead>
<tr>
<th>序号</th>
<th>步骤</th>
<th>解释</th>
<th>中文</th>
<th>英文</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><strong>读取数据</strong></td>
<td>数据一般存储于不同的文件夹不同文件内，需要将其导入到计算机</td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td><strong>分词</strong></td>
<td>导入到计算的文本是字符串数据，需要整理为更好用的列表</td>
<td>例如“我爱你中国”分词后<br>得到[&ldquo;我&rdquo;, &ldquo;爱&rdquo;, &ldquo;你&rdquo;, &ldquo;中国&rdquo;]</td>
<td>&ldquo;I love China&quot;分为<br>[&ldquo;I&rdquo;, &ldquo;love&rdquo;, &ldquo;China&rdquo;]</td>
</tr>
<tr>
<td>3</td>
<td><strong>剔除符号和无意义的停止词</strong></td>
<td>为了降低计算机运行时间，对分析结果影响较小的字符，诸如符号和无意义的词语需要剔除掉</td>
<td>如“的”，“她”， ”呢”， “了”</td>
<td>&ldquo;is&rdquo; , &ldquo;a&rdquo;, &ldquo;the&rdquo;</td>
</tr>
<tr>
<td>4</td>
<td><strong>字母变小写，词干化</strong></td>
<td>同义词归并，同主体词归并</td>
<td>“中铁”，“中国铁建”，“中铁集团”都可以归并为“中铁”</td>
<td>先变为小写，这样“I”和“i”都归并为“i”；<br>“was”，“are”，“is”都归并为“be”</td>
</tr>
<tr>
<td>5</td>
<td><strong>构建文档词频矩阵</strong></td>
<td>使用一定的编码方式，即用某种方式表示文本。常见的有词袋法、tf-idf；<br>可以使用scikit-learn构建文档词频矩阵，但中英文略有区别，需要注意</td>
<td>“我爱你中国”需要先整理为“我 爱 你 中国”</td>
<td>“I love China”</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="三常见的文本分析技术有">三、常见的文本分析技术有</h2>
<ul>
<li>主题分析(Thematic analysis)</li>
<li>内容分析(content analysis)</li>
<li>基于词典的方法(dictionary analysis)</li>
<li>文本向量化(Bag-of-words)</li>
<li>监督学习如SVM、Bayes和Regression</li>
<li>无监督学习，如LDA话题模型</li>
<li>自然语言处理</li>
</ul>
<p>上述文本分析技术，按照人与机器参与程度，绘制在下图。一般来说，越向右，文本分析技术的自动化程度越高，需要注意的是自动化越高，并不代表人的工作量就越少。</p>
<p><img loading="lazy" src="img/textautomate.png" alt=""  />
</p>
<br>
<h3 id="31-主题分析thematic-analysis">3.1 主题分析Thematic Analysis</h3>
<p>主题分析(Thematic analysis)是一种专家方法，一般与扎根理论方法相结合(Baumer, Mimno, Guha, Quan, &amp; Gay, 2017)。扎根理论与主题分析的理念是基于专家自身经验和对世界的理解，做出对数据的见解，从而构建新理论。主题分析常见于组织科学和传播学(Gioia, Corley, &amp; Hamilton, 2013; Strauss &amp; Corbin, 1998)。</p>
<p>主题分析涉及一个反复迭代的过程，在此过程中，研究人员将开发出一系列源自文本的代码和类别。除非要精炼理论，否则一般在分析开始之前尚不知道类别。在这种情况下，数据分析需要对文献和数据进行不断的比较。</p>
<ol>
<li>研究人员从参与者自己的语言开始（称为“一阶编码”或“开放式编码”；Gioia等人，2013；Strauss＆Corbin，1998）</li>
<li>然后将相似的代码归为一类（称为“二阶代码”或“主轴编码”；Strauss＆Corbin，1998）。</li>
</ol>
<p>诸如NVivo和ATLAS.ti之类的计算机软件可以帮助简化上述过程，但文本的分类通常依赖于人类编码衍生的类别的操作定义，计算机自动化的程度依旧很低，分析的数据量通常不大。而且编码过程对编码者的要求严格，通常是对该领域有较深理解的人才适合做此类工作。</p>
<br>
<h3 id="32-内容分析基于词典的方法法">3.2 内容分析/基于词典的方法法</h3>
<p><strong>内容分析</strong> 和 <strong>其他基于字典的方法</strong> 通常是通过对特定文本中 <strong>单词/词组</strong> 的频率计数进行的（Reinard，2008；Short，Broberg，Cogliser＆Brigham，2010）。因为按照这种方法，文本数据被压缩成词组频数，定性的文本数据转化为定量的频数，索引可用于回答更多以定量为导向的研究问题（McKenny等，2016；Reinard，2008）。</p>
<p>比如进行文本情感分析，我们可以用很简单的思路。即统计文本中正面词出现的总数和负面词出现的总数，得出文本的情感值。而在此分析过程中，我们需要事先拥有一个正面词词典和负面词词典。</p>
<p>是否有成熟的领域词典、或者构建领域词典，这需要研究者对研究问题和研究的数据有一定的领域知识，工作量也会因是否有词典而不同。一般有现成的成熟的词典，计算机自动化程度高，人工工作量低。</p>
<p>与主题分析类似，计算机软件可以协助内容分析过程。像DICTION这样的程序会使用 <strong>分类字典</strong> 自动对文本评分（即，根据单词或n-gram而非操作定义确定主题）。可以与主题分析类似地使用其他程序，例如NVivo或ATLAS.ti，在主题分析中，通过软件的帮助手动进行编码和分类，以组织数据。</p>
<br>
<h3 id="33-词袋法bag-of-words">3.3 词袋法Bag-of-words</h3>
<p><img loading="lazy" src="img/bagofwords.png" alt=""  />
</p>
<p>文本数据是非结构化的定性数据，计算机并不能直接使用。我们需要按照计算机容易理解的方式去组织数据，类似于上图的第一步骤,四段英文文本被组织成一个文档特征矩阵（document-feature-matrix），矩阵中</p>
<ul>
<li>每一行代表一个英文文档</li>
<li>每一个列代表一个特征词</li>
</ul>
<h4 id="331-词袋法-vs-主题分析中的编码者">3.3.1 词袋法 vs 主题分析中的编码者</h4>
<p>为了理解词袋法，可以类比<strong>主题分析</strong> 中的编码者。我们可以将词袋法看做是一个死板的，不知变通的人，脑子很简单，只知道统计特征词在每个文档中出现的词频。那么据此我们就知道词袋法和人的优缺点。</p>
<p>对于词袋法，优点是规则标准统一，缺点是不知变通，牺牲了文本中很多的信息量。强调编码过程的高标准，牺牲了分析的深度。</p>
<p>对于研究者参与 <strong>主题分析</strong> 这样的编码过程，优点是研究者有很强的领域知识和强大的洞察力，可以灵活洞察规律，缺点是每个研究者都具有特殊的经历和偏好，编码标准不统一。用研究者编码的过程，强调编码的深度和质量，牺牲了编码分析过程的标准性。</p>
<h4 id="332-词袋法的用途">3.3.2 词袋法的用途</h4>
<p>词袋法编码是计算科学领域对文本数据的简化和压缩的方法，后续可以据此进行监督学习和无监督学习。</p>
<br>
<h3 id="34-监督学习">3.4 监督学习</h3>
<p>在有监督的方法中，研究人员事先知道ta正在寻找什么（罗伯茨等，2014）。比如要判断论文的作者身份这个问题，研究人员为程序提供输入（在这种情况下为文本）和输出（例如，文本作者的身份），然后系统创建一种算法来映射两者之间的联系（Janasik， Honkela和Bruun，2009年）。Mosteller and Wallace（1963）通过使用简单的贝叶斯单词概率来预测12篇有争议的联邦主义者论文（詹姆斯·麦迪逊或亚历山大·汉密尔顿）的作者身份。如今，朴素贝叶斯（Bayes）和支持向量机（SVM）等技术是用于文本分析的流行的监督算法（Manning，Prabhakar和Hinrich，2008年）。</p>
<br>
<h3 id="35-无监督学习">3.5 无监督学习</h3>
<p>无监督算法，如主题分析（Janasik等，2009）可识别数据中的<strong>单词簇</strong>和<strong>主题</strong>。但是，与<strong>主题分析</strong>不同，<strong>主题建模</strong>使用高度自动化的方法来确定重要主题，分析过程所需的时间和领域知识相对较少。尽管人类的洞察力仍然对帮助解释出现的主题很重要，主题建模适合分析大规模文本数据（Kobayashi1，Mol，Berkers，Kismihok和Den Hartog，2017）。<strong>主题建模利用了主题分析（即人类洞察力、解释力）和机器学习（即快速分析大量文本）的优势</strong>。</p>
<br>
<h3 id="36-自然语言处理">3.6 自然语言处理</h3>
<p>最后，自然语言处理(Natural Language Processing)通常是文本分析中自动化程度最高的形式（有关综述，请参阅Manning等人，2008）。这种方法模拟了人类如何理解和处理语言（Chowdhury，2003；Collobert等，2011；Joshi，1991）。例如，NLP技术可以标记句子中单词的词性（例如，名词，形容词等），将文档从一种语言翻译成另一种语言，甚至使用句子的上下文来阐明词语的词义（Buntine＆Jakulin，2004年）。</p>
<p>因此，与<strong>词袋法</strong>不同，NLP认为单词顺序很重要。当使用训练集时，使用深度学习和多模式（即结合文本和图像）等尖端技术进行情感分析是NLP的一种流行形式（Kouloumpis，Wilson和Moore，2011）。这种特殊的分析将文本的总体态度，情感或观点分类为肯定，否定或中立。</p>
<p>与<strong>主题分析</strong>形成鲜明对比的是，自然语言处理是一个完全计算机自动化的过程，因此几乎不需要人类的理解和或解释（Quinn等人，2010）。此外，相对于需要人工编码（例如，主题分析）的技术，NLP的执行速度非常快，并且比其他方法更具系统性。例如，计算机科学，信息科学，语言学和心理学的研究人员利用NLP作为文本分析工具（Chowdhury，2003年）。</p>
<p>大邓提醒一下，自然语言处理属于人工智能范畴，人工智能技术没有那么神，我们应该将其理解为“人工”+“智能”可能更妥当一些，即数据准备阶段用大量的人工时对数据进行标注，产生训练数据集合。之后借助于计算机的“智能”学习数据集中的规律，因此人工智能脱离了人工标注数据的喂养，只能做很简单的事情，更像是人工智障。</p>
<br>
<h3 id="37-不同文本分析技术汇总对比">3.7 不同文本分析技术汇总对比</h3>
<table>
<thead>
<tr>
<th>技术</th>
<th>描述</th>
<th>优点</th>
<th>缺点</th>
<th>常被应用(领域)</th>
<th>软件</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>主题分析</strong>Thematic analysis</td>
<td>需要有经验的人员基于自身经验和李俊杰，对研究的数据进行挖掘。编码过程为迭代进行</td>
<td>使用参与者自己的话语或者构念来挖掘数据，对少量文本理解的更深入</td>
<td>属于时间、劳动密集型任务，不适合大规模数据。<br>由于不同的编码人员有不同的经历和偏好，编码过程的标准不可靠</td>
<td>社会学、管理学</td>
<td>Nvivo；</td>
</tr>
<tr>
<td><strong>内容分析/基于字典方法</strong></td>
<td>统计文本中词语/词组的出现频率</td>
<td>允许对研究的数据进行定量分析</td>
<td>采用的词典应尽量与研究问题适应，词典适配性问题突出</td>
<td>管理学</td>
<td>LIWC、Nvivo、DICTION；</td>
</tr>
<tr>
<td><strong>词袋法</strong>（Bag of words）</td>
<td>将文本字符串转为计算机能理解的数字化向量</td>
<td>编码标准稳定简单，具有统计学特性，扩展性强</td>
<td>编码过程忽略词语的先后顺序</td>
<td>管理学</td>
<td>Python的scikit-learn、gensim、nltk等；R</td>
</tr>
<tr>
<td><strong>监督学习</strong>(Supervise models),如SVM、Bayes、Logistic Regression</td>
<td>研究者要知道输入数据X和标签y；需要核实的模型需要X和y之间的关系和规律</td>
<td>允许事先定义编码规则(如选择词袋法还是tfidf)；逻辑简单</td>
<td>需要高质量的标注数据(工作量大)；you与特征词太多，训练的模型很容易过拟合。</td>
<td>计算机学、政治学、管理学</td>
<td>Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）</td>
</tr>
<tr>
<td><strong>无监督学习</strong>(Kmeans、 LDA话题模型)</td>
<td>使用聚类、话题分析，让计算机自动对数据进行分组</td>
<td>在没有人工标注的情况下，加速了数据的“标注”或“分类”</td>
<td>“标注”是机器按照数字特征进行的分组，需要研究者解读才可以赋予“标准“意义；训练过程需要大量的调参</td>
<td>计算机学、政治学、管留学</td>
<td>Python的scikit-learn、gensim、nltk等；R（topicmodels， stm）</td>
</tr>
<tr>
<td><strong>自然语言处理</strong></td>
<td>按照人类对语言的理解进行建模，考虑词语顺序</td>
<td>计算机自动化；可分析语义</td>
<td>大多数模型是人类无法解读的黑箱；<br>虽然代码编程量小，但训练代码耗时巨大</td>
<td>计算科学；市场营销；心理学</td>
<td>pytorch、tensorflow</td>
</tr>
</tbody>
</table>
<br>
<h3 id="38-python能做哪些">3.8 Python能做哪些？</h3>
<p>计算机能做的文本分析，Python都能做到，包括</p>
<ul>
<li>基于词典的分析法；如基于词典法的情感计算</li>
<li>词袋法；可以进行文本相似度计算</li>
<li>有监督机器学习；如基于机器学习的情感分析；文本分类</li>
<li>无监督机器学习；lda话题模型对文本进行话题分析</li>
<li>自然语言处理；考虑词语顺序的LSTM</li>
</ul>
<p>除了自然语言处理部分，四种方法在我的<a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">《Python网络爬虫与文本数据分析》</a>视频课程中都有相关的讲解和实战代码</p>
<p><br><br></p>
<h2 id="相关文献">相关文献</h2>
<blockquote>
<p>[1]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &ldquo;Uniting the tribes: Using text for marketing insight.&rdquo; Journal of Marketing (2019): 0022242919873106.</p>
</blockquote>
<blockquote>
<p>[2]Kenneth Benoit. July 16, 2019. “[Text as Data: An Overview](<a href="https://kenbenoit.net/pdfs/28">https://kenbenoit.net/pdfs/28</a> Benoit Text as Data draft 2.pdf).” Forthcoming in Cuirini, Luigi and Robert Franzese, eds. <em>Handbook of Research Methods in Political Science and International Relations</em>. Thousand Oaks: Sage.</p>
</blockquote>
<blockquote>
<p>[3]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. &ldquo;A review of best practice recommendations for text analysis in R (and a user-friendly app).&rdquo; <em>Journal of Business and Psychology</em> 33, no. 4 (2018): 445-459.</p>
</blockquote>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>70G上交所年报数据集</title>
      <link>https://textdata.cn/blog/70g_china_market_anunal_report_datasets/</link>
      <pubDate>Mon, 22 Nov 2021 20:40:10 +0600</pubDate>
      
      <guid>/blog/70g_china_market_anunal_report_datasets/</guid>
      <description>Python网络爬虫与文本分析， 70g会计年报pdf数据集免费下载</description>
      <content:encoded><![CDATA[<h2 id="70g年报pdf数据集">70G年报pdf数据集</h2>
<p><img loading="lazy" src="img/1.gif" alt=""  />
</p>
<h2 id="数据下载说明">数据下载说明</h2>
<p>所有pdf均来自上海证券交易所官网，使用shreport库进行的下载。</p>
<p><img loading="lazy" src="img/2.png" alt=""  />
</p>
<h2 id="报告信息汇总文件">报告信息汇总文件</h2>
<h4 id="heading"></h4>
<p><img loading="lazy" src="img/3.gif" alt=""  />
</p>
<p>summary.xlsx内字段</p>
<ul>
<li>company 上市公司企业名</li>
<li>code 股票代码</li>
<li>type 报告类型</li>
<li>year 报告年份</li>
<li>date 报告发布日期</li>
<li>pdf 报告pdf文件下载链接</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import pandas as pd
from pathlib import Path


#报告汇总文件summary.xlsx
df = pd.read_excel(&#39;summary.xlsx&#39;)
df.head()
</code></pre></div><p><img loading="lazy" src="img/4.png" alt=""  />
</p>
<p>一共有报告71126份</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">len(df)
71149
</code></pre></div><p>一共有上市公司1486家</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">len(df[&#39;company&#39;].unique())
1486
</code></pre></div><h2 id="summary文件夹">summary文件夹</h2>
<p>summary文件夹内是每家公司的报告披露情况</p>
<p><img loading="lazy" src="img/5.gif" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">df1 = pd.read_excel(&#39;summary/600000.xlsx&#39;)
df1.head()
</code></pre></div><p><img loading="lazy" src="img/6.png" alt=""  />
</p>
<p>浦发银行一共有75份定期报告</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">len(df1)
75
</code></pre></div><h2 id="reports文件夹">reports文件夹</h2>
<p>reports文件夹存放着以各各公司股票代码命名的文件夹</p>
<p>文件夹内是该公司所有定期报告</p>
<p><img loading="lazy" src="img/7.gif" alt=""  />
</p>
<h2 id="读取pdf报告">读取pdf报告</h2>
<p>可使用pdfdocx库读取pdf,</p>
<p>pdfdocx文档链接 <a href="https://github.com/thunderhit/pdfdocx">https://github.com/thunderhit/pdfdocx</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">from pdfdocx import read_pdf

p_text = read_pdf(&#39;reports/600000/600000_2012_1.pdf&#39;)
p_text
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">上海浦东发展银行股份有限公司 \n\n2012 年第一季度报告 \n\n \n\n \n\n§1 重要提示 \n\n1.1 公司董事会、监事会及其董事、监事、高级管理人员保证本报告所载资料不存在任何虚假记载、\n\n误导性陈述或者重大遗漏，并对其内容的真实性、准确性和完整性承担个别及连带责任。\n\n1.2 公司于 2012 年 4 月 26 日以通讯表决的方式召开第四届董事会第二十六次会议审议通过本报告，\n\n1.4 公司董事长、行长吉晓辉、财务总监刘信义及财务机构负责人傅能声明：保证本季度报告中财务\n\n公司全体董事出席董事会会议并行使表决权。\n\n1.3 公司第一季度财务报告未经审计。\n\n报告的真实、完整。\n\n \n§2 公司基本情况 \n\n2.1 主要会计数据及财务指标 \n\n本报告期末 \n\n上年度期末 \n\n币种:人民币 \n\n本报告期末比上年\n度期末增减(%) \n\n总资产(千元) \n\n归属于上市公司股东的所有者权益(千元) \n\n2,804,646,567\n\n157,055,724\n\n2,684,693,689 \n148,891,235 \n\n归属于上市公司股东的每股净资产(元) \n\n8.420\n\n7.982 \n\n4.47 \n5.48 \n5.49 \n\n经营活动产生的现金流量净额(千元) \n\n每股经营活动产生的现金流\n\n \n\n \n \n母公司现金流量表 \n \n2012 年 1—3 月 \n \n编制单位: 上海浦东发展银行股份有限公司....
</code></pre></div><h2 id="70g数据下载">70G数据下载</h2>
<p>链接:https://pan.baidu.com/s/14PI6MbxunFQ3fZOfR33zkw 密码:osoi</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用Pandas处理文本数据</title>
      <link>https://textdata.cn/blog/text_analysis_in_pandas/</link>
      <pubDate>Tue, 16 Nov 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/text_analysis_in_pandas/</guid>
      <description>2000字详细说明</description>
      <content:encoded><![CDATA[<blockquote>
<p>src: <a href="https://mp.weixin.qq.com/s/MmfEtyKaMqNn_Ik1oJtitQ">https://mp.weixin.qq.com/s/MmfEtyKaMqNn_Ik1oJtitQ</a></p>
<p>author: 俊欣</p>
<p>公众号: 关于数据分析与可视化</p>
</blockquote>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="https://github.com/hidadeng/DaDengAndHisPython/blob/master/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86Pandas%E7%AF%87.ipynb">点击Pandas本文代码下载</a></p>
<p>今天我们来谈论一下pandas库当中文本数据的操作，希望大家再看完本篇文章之后会有不少的收获，我们大致会讲</p>
<ul>
<li>创建一个包含文本数据的DataFrame</li>
<li>常用处理文本数据的方法的总结</li>
<li>正则表达式与DataFrame内部方法的结合</li>
</ul>
<br>
<h2 id="创建文本内容的数据">创建文本内容的数据</h2>
<p>我们先来创建一个包含文本数据的DataFrame，来供我们后面使用</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
   <span class="s2">&#34;姓&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;李&#34;</span><span class="p">,</span><span class="s2">&#34;王&#34;</span><span class="p">,</span><span class="s2">&#34;戴&#34;</span><span class="p">,</span> <span class="s2">&#34;李&#34;</span><span class="p">,</span> <span class="s2">&#34;张&#34;</span><span class="p">],</span>
   <span class="s2">&#34;名&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;华&#34;</span><span class="p">,</span><span class="s2">&#34;硕&#34;</span><span class="p">,</span><span class="s2">&#34;建业&#34;</span><span class="p">,</span> <span class="s2">&#34;四&#34;</span><span class="p">,</span> <span class="s2">&#34;三&#34;</span><span class="p">],</span>
   <span class="s2">&#34;户籍地址&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34; 浙江省·宁波市 &#34;</span><span class="p">,</span> <span class="s2">&#34;   浙江省·杭州市    &#34;</span><span class="p">,</span> <span class="s2">&#34;  浙江省·丽水市  &#34;</span><span class="p">,</span> <span class="s2">&#34;  浙江省·衢州市 &#34;</span><span class="p">,</span> <span class="s2">&#34;  浙江省·湖州市           &#34;</span><span class="p">],</span>
   <span class="s2">&#34;微信ID&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;Tomoplplplut1248781&#34;</span><span class="p">,</span> <span class="s2">&#34;Smopopo857&#34;</span><span class="p">,</span> <span class="s2">&#34;Adahuhuifhhjfj&#34;</span><span class="p">,</span> <span class="s2">&#34;Tull1945121&#34;</span><span class="p">,</span> <span class="s2">&#34;ZPWERERTFD599557&#34;</span><span class="p">],</span>
   <span class="s2">&#34;邮箱地址&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;tom02159@163.com&#34;</span><span class="p">,</span> <span class="s2">&#34;smitt7821@163.com&#34;</span><span class="p">,</span> <span class="s2">&#34;adams623@163.com&#34;</span><span class="p">,</span> <span class="s2">&#34;tull0305@163.com&#34;</span><span class="p">,</span> <span class="s2">&#34;five7532@163.com&#34;</span><span class="p">]</span>
<span class="p">})</span>
<span class="n">df</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">|    | 姓   | 名   | 户籍地址      | 微信ID              | 邮箱地址          |
|---:|:-----|:-----|:--------------|:--------------------|:------------------|
|  0 | 李   | 华   | 浙江省·宁波市 | Tomoplplplut1248781 | tom02159@163.com  |
|  1 | 王   | 硕   | 浙江省·杭州市 | Smopopo857          | smitt7821@163.com |
|  2 | 戴   | 建业 | 浙江省·丽水市 | Adahuhuifhhjfj      | adams623@163.com  |
|  3 | 李   | 四   | 浙江省·衢州市 | Tull1945121         | tull0305@163.com  |
|  4 | 张   | 三   | 浙江省·湖州市 | ZPWERERTFD599557    | five7532@163.com  |
</code></pre></div><br>
<h2 id="常用处理文本数据的方法总结">常用处理文本数据的方法总结</h2>
<p>Python当中用来处理字符串数据的方法有很多，基本上都可以在DataFrame内部拿来使用，例如lower()方法和upper()方法，给字母大小写</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;微信ID&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0    tomoplplplut1248781
1             smopopo857
2         adahuhuifhhjfj
3            tull1945121
4       zpwerertfd599557
Name: 微信ID, dtype: object
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;微信ID&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len</span><span class="p">()</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0    19
1    10
2    14
3    11
4    16
Name: 微信ID, dtype: int64
</code></pre></div><p>当然我们看到户籍地址这一列中的数据有很多的空格</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0                浙江省·宁波市 
1             浙江省·杭州市    
2               浙江省·丽水市  
3                浙江省·衢州市 
4      浙江省·湖州市           
Name: 户籍地址, dtype: object
</code></pre></div><p>我们可以使用处理字符串时的strip()方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0    浙江省·宁波市
1    浙江省·杭州市
2    浙江省·丽水市
3    浙江省·衢州市
4    浙江省·湖州市
Name: 户籍地址, dtype: object
</code></pre></div><p>与之相类似的还有lstrip()方法以及rstrip()方法，这里就不做赘述。</p>
<p>在字符串的处理过程当中，startswith()方法和endswith()方法也是用的非常的频繁，例如我们想要挑选出户籍地址是“宁波市”的数据，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&#34;宁波市&#34;</span><span class="p">)]</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">   姓  名     户籍地址                 微信ID              邮箱地址
0  李  华  浙江省·宁波市  Tomoplplplut1248781  tom02159@163.com
</code></pre></div><p>另外我们还可以使用replace()方法来实现当中的字符串的替换</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&#34;·&#34;</span><span class="p">,</span> <span class="s2">&#34;--&#34;</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0    浙江省--宁波市
1    浙江省--杭州市
2    浙江省--丽水市
3    浙江省--衢州市
4    浙江省--湖州市
Name: 户籍地址, dtype: object
</code></pre></div><p>那既然用到了replace()方法，那么split()方法也可以尝试一下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&#34;·&#34;</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0    [浙江省, 宁波市]
1    [浙江省, 杭州市]
2    [浙江省, 丽水市]
3    [浙江省, 衢州市]
4    [浙江省, 湖州市]
Name: 户籍地址, dtype: object
</code></pre></div><p>在经过spilit()方法的切割过之后就变成了列表的形式，然后可以通过get()方法或者[]来获取里面的元素，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&#34;·&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

</code></pre></div><p>或者</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&#34;·&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">str</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0    浙江省
1    浙江省
2    浙江省
3    浙江省
4    浙江省
Name: 户籍地址, dtype: object
</code></pre></div><p>那么获取列表当中的第二个元素也是同样的道理，当然我们也可以在split()方法当中添加expand=True这个参数，来将上面列表形式的数据转化成DataFrame格式</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&#34;·&#34;</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">      0    1
0  浙江省  宁波市
1  浙江省  杭州市
2  浙江省  丽水市
3  浙江省  衢州市
4  浙江省  湖州市
</code></pre></div><p>同样地，我们可以在后面添加[]来获取我们想要的元素</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&#34;·&#34;</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0    宁波市
1    杭州市
2    丽水市
3    衢州市
4    湖州市
Name: 1, dtype: object
</code></pre></div><br>
<h2 id="正则表达式与dataframe内部方法的结合">正则表达式与DataFrame内部方法的结合</h2>
<p>假如我们想要提取文本数据内部的一部分数据，可以结合正则表达式来使用，例如我们想要提取“微信ID”这一列当中的字母和数字，并且将两者分开来</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">two_groups</span> <span class="o">=</span> <span class="s2">&#34;([a-zA-Z]+)([0-9]+)&#34;</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&#34;微信ID&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="n">two_groups</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">              0        1
0  Tomoplplplut  1248781
1       Smopopo      857
2           NaN      NaN
3          Tull  1945121
4    ZPWERERTFD   599557
</code></pre></div><p>当然了，如果想是要提取文本数据中的部分数据，可以直接在str方法后面添加索引，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;邮箱地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="p">[</span><span class="o">-</span><span class="mi">8</span><span class="p">:]</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0    @163.com
1    @163.com
2    @163.com
3    @163.com
4    @163.com
Name: 邮箱地址, dtype: object
</code></pre></div><p>当然，从另外一个角度讲，正则表达式也可以帮助我们确认文本数据是否符合某种规律，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">two_groups</span> <span class="o">=</span> <span class="s2">&#34;([a-zA-Z]+)([0-9]+)&#34;</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&#34;微信ID&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">two_groups</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0     True
1     True
2    False
3     True
4     True
Name: 微信ID, dtype: bool
</code></pre></div><p>当中有一个为False，不满足字母+数字的规律，我们再进一步，将满足条件的数据提取出来</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;微信ID&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">two_groups</span><span class="p">)]</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">   姓  名     户籍地址                 微信ID               邮箱地址
0  李  华  浙江省·宁波市  Tomoplplplut1248781   tom02159@163.com
1  王  硕  浙江省·杭州市           Smopopo857  smitt7821@163.com
3  李  四  浙江省·衢州市          Tull1945121   tull0305@163.com
4  张  三  浙江省·湖州市     ZPWERERTFD599557   five7532@163.com
</code></pre></div><p>针对文本数据而言，contains()方法也能够派上用场，例如下面的数据</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">   姓   名                  户籍地址                 微信ID               邮箱地址
0  李   华              浙江省·宁波市   Tomoplplplut1248781   tom02159@163.com
1  王   硕           浙江省·杭州市               Smopopo857  smitt7821@163.com
2  戴  建业             浙江省·丽水市         Adahuhuifhhjfj   adams623@163.com
3  李   四              浙江省·衢州市           Tull1945121   tull0305@163.com
4  张   三    浙江省·湖州市                ZPWERERTFD599557   five7532@163.com
5  黄   五               浙江省·宁波市        hunhunhu45652  1erdcvf127@16.com
</code></pre></div><p>我们用contains()来提取出户籍地址为“宁波市”的内容，可以这么做</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;户籍地址&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s2">&#34;宁波市&#34;</span><span class="p">)]</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">   姓  名     户籍地址                 微信ID               邮箱地址
0  李  华  浙江省·宁波市  Tomoplplplut1248781   tom02159@163.com
5  黄  五  浙江省·宁波市        hunhunhu45652  1erdcvf127@16.com
</code></pre></div><p>暂时就这些了，下一篇原创的文章安排在周天，非技术方面的，期待一下？</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>spacy产业级自然语言处理包</title>
      <link>https://textdata.cn/blog/spacy_industry_application/</link>
      <pubDate>Sun, 14 Nov 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/spacy_industry_application/</guid>
      <description>牛掰nlp库</description>
      <content:encoded><![CDATA[<h2 id="spacy">spacy</h2>
<p>产业级自然语言处理python包  <a href="https://spacy.io/">https://spacy.io/</a></p>
<p><a href="https://github.com/hidadeng/DaDengAndHisPython/blob/master/20211115spacy%E4%BA%A7%E4%B8%9A%E7%BA%A7%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%8C%85.ipynb">点击下载</a></p>
<br>
<h2 id="特性">特性</h2>
<ul>
<li>支持64+语言</li>
<li>针对19门语言的64流水线pipeline处理函数</li>
<li>多任务预训练transformers，如BERT</li>
<li>预训练词向量</li>
<li>支持命名实体识别</li>
<li>支持 POS词性标注</li>
<li>支持 句法依存</li>
<li>支持 文本分类</li>
<li>支持 词干化</li>
<li>内置可视化</li>
</ul>
<br>
<h2 id="spacy安装">spacy安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python">
<span class="n">pip</span> <span class="n">install</span> <span class="n">spacy</span><span class="o">==</span><span class="mf">3.2.0</span>

</code></pre></div><br>
<h2 id="模型下载安装">模型下载安装</h2>
<p>sm小型/ md中型/ lg大型</p>
<ul>
<li>
<p><strong>中文模型3.2.0版</strong></p>
<ul>
<li>zh_core_web_sm  <a href="https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.2.0/zh_core_web_sm-3.2.0-py3-none-any.whl">https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.2.0/zh_core_web_sm-3.2.0-py3-none-any.whl</a></li>
<li>zh_core_web_md <a href="https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.2.0/zh_core_web_md-3.2.0-py3-none-any.whl">https://github.com/explosion/spacy-models/releases/download/zh_core_web_md-3.2.0/zh_core_web_md-3.2.0-py3-none-any.whl</a></li>
<li>zh_core_web_lg   <a href="https://github.com/explosion/spacy-models/releases/download/zh_core_web_lg-3.2.0/zh_core_web_lg-3.2.0-py3-none-any.whl">https://github.com/explosion/spacy-models/releases/download/zh_core_web_lg-3.2.0/zh_core_web_lg-3.2.0-py3-none-any.whl</a></li>
</ul>
</li>
<li>
<p><strong>英文模型3.2.0版</strong></p>
<ul>
<li>en_core_web_sm <a href="https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl">https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl</a></li>
<li>en_core_web_md  <a href="https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl">https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl</a></li>
<li>en_core_web_lg  <a href="https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl">https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl</a></li>
</ul>
</li>
</ul>
<p><strong>注意</strong>： 模型大小的区别主要体现在词向量维度数的差距，模型越大， 词向量的维度越多。</p>
<p>以版本3.2.0的en_core_web_sm为例，点击对应链接，下载至桌面。</p>
<p>打开命令行， 依次执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd Desktop

pip3 install en_core_web_sm-3.2.0-py3-none-any.whl
</code></pre></div><p>即可安装完成。</p>
<br>
<br>
<h2 id="doc类型">Doc类型</h2>
<ul>
<li><code>doc.lang_</code>  doc的语言</li>
<li><code>doc.text</code>   doc的文本</li>
<li><code>doc.ents</code> 文本中的实体词</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1">#使用中文zh_core_web_sm模型</span>
<span class="n">zh_nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;zh_core_web_sm&#34;</span><span class="p">)</span>

<span class="n">test1</span> <span class="o">=</span> <span class="s1">&#39;国家管网微信公众号11月13日消息，11月12日，国家管网集团首期绿色超短期融资券在全国银行间市场成功发行。此次债券发行是国家管网集团成立后首次在公开市场亮相，由工商银行独家承销，发行金额60亿元、期限270天，为本年度单笔最大金额绿色超短期融资券，募集资金将全部用于储气库等绿色低碳天然气储运基础设施建设；发行票面利率2.41%，认购总规模达2.53倍，低于资本市场同期可比产品利率超10个BP，反映了市场对绿色基础设施建设项目的青睐。&#39;</span>

<span class="n">doc1</span> <span class="o">=</span> <span class="n">zh_nlp</span><span class="p">(</span><span class="n">test1</span><span class="p">)</span>

<span class="n">doc1</span>
</code></pre></div><pre><code>国家管网微信公众号11月13日消息，11月12日，国家管网集团首期绿色超短期融资券在全国银行间市场成功发行。此次债券发行是国家管网集团成立后首次在公开市场亮相，由工商银行独家承销，发行金额60亿元、期限270天，为本年度单笔最大金额绿色超短期融资券，募集资金将全部用于储气库等绿色低碳天然气储运基础设施建设；发行票面利率2.41%，认购总规模达2.53倍，低于资本市场同期可比产品利率超10个BP，反映了市场对绿色基础设施建设项目的青睐。
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">doc1</span><span class="o">.</span><span class="n">lang_</span>
</code></pre></div><pre><code>'zh'
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">doc1</span><span class="o">.</span><span class="n">text</span>
</code></pre></div><pre><code>'国家管网微信公众号11月13日消息，11月12日，国家管网集团首期绿色超短期融资券在全国银行间市场成功发行。此次债券发行是国家管网集团成立后首次在公开市场亮相，由工商银行独家承销，发行金额60亿元、期限270天，为本年度单笔最大金额绿色超短期融资券，募集资金将全部用于储气库等绿色低碳天然气储运基础设施建设；发行票面利率2.41%，认购总规模达2.53倍，低于资本市场同期可比产品利率超10个BP，反映了市场对绿色基础设施建设项目的青睐。'
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">doc1</span><span class="o">.</span><span class="n">vector</span>
</code></pre></div><pre><code>array([-1.81135774e-01,  2.31929451e-01,  1.45746097e-01,  6.82696044e-01,
       -8.44623148e-03, -2.21295916e-02,  4.06811416e-01, -4.60287899e-01,
       -5.73987663e-01, -1.33687481e-01, -5.34314513e-01, -6.64901555e-01,
       -3.94947737e-01,  6.35875063e-03, -2.03339502e-01,  5.78875951e-02,
       -3.34325433e-01, -3.77648622e-01,  2.43863747e-01, -5.56892566e-02,
       -7.30801523e-01, -2.41785884e-01, -4.50579911e-01, -3.13598923e-02,
        9.07084942e-02, -8.06667805e-01,  7.28501499e-01, -8.59559357e-01,
       -4.44110222e-02,  9.64611948e-01, -2.57230818e-01,  1.09481342e-01,
       -3.73580456e-01, -8.51007993e-04,  5.30374162e-02, -5.51876485e-01,
       -4.82654065e-01,  2.68822908e-01, -4.20012563e-01,  4.33068752e-01,
       -5.14427841e-01,  5.53584039e-01, -2.00293139e-02,  9.45062563e-02,
        1.04523234e-01,  1.34134221e+00, -5.23905218e-01,  1.31230903e+00,
        3.28943968e-01,  3.39987069e-01,  8.26785386e-01,  5.35273492e-01,
       -4.27510649e-01, -1.02807179e-01, -1.91500232e-01,  2.63696283e-01,
        6.33961499e-01, -5.65908328e-02, -1.94336250e-01, -5.89190602e-01,
        2.22078279e-01,  3.41992415e-02,  5.37312031e-01,  2.77926654e-01,
       -3.00608397e-01, -6.42910838e-01, -1.33188680e-01,  2.82793492e-01,
        6.25911206e-02,  2.08833948e-01,  2.69211121e-02,  1.65822819e-01,
       -4.32190485e-02, -6.67634964e-01,  6.50937319e-01, -2.43003711e-01,
        9.57057327e-02, -3.56370257e-03, -1.13566548e-01, -1.65319979e-01,
        7.40000159e-02,  3.65676880e-01, -2.21356809e-01,  2.03256473e-01,
        2.26293072e-01,  3.11525285e-01,  3.37869138e-01, -3.12896192e-01,
        5.31899095e-01, -1.86223835e-01, -6.03411011e-02,  4.97923464e-01,
        3.10418844e-01, -2.48594299e-01, -3.67455184e-01, -4.46804255e-01],
      dtype=float32)
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#doc1中的实体词</span>
<span class="n">doc1</span><span class="o">.</span><span class="n">ents</span>
</code></pre></div><pre><code>(11月13日, 11月12日, 国家管网集团, 全国银行, 国家管网集团, 工商银行, 60亿元, 270天, 2, 2, 53, 超10)
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#doc1中的实体词类别</span>
<span class="p">[</span><span class="n">ent</span><span class="o">.</span><span class="n">label_</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc1</span><span class="o">.</span><span class="n">ents</span><span class="p">]</span>
</code></pre></div><pre><code>['DATE',
 'DATE',
 'ORG',
 'ORG',
 'ORG',
 'ORG',
 'MONEY',
 'DATE',
 'CARDINAL',
 'CARDINAL',
 'CARDINAL',
 'CARDINAL']
</code></pre>
<h2 id="token类型">Token类型</h2>
<ul>
<li><code>token.text</code>   文本</li>
<li><code>token.pos_</code>  词性</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc1</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">)</span>
</code></pre></div><pre><code>国家   NOUN
管网   NOUN
微信   ADJ
公众号   NOUN
11月   NOUN
13日   NOUN
消息   NOUN
，   PUNCT
11月   NOUN
12日   NOUN
，   PUNCT
国家   NOUN
管网   NOUN
集团   NOUN
首期   ADV
绿色   VERB
超短   NOUN
期融   NOUN
资券   VERB
在   ADP
全国   ADJ
银行   NOUN
间   PART
市场   NOUN
成功   ADV
发行   VERB
。   PUNCT
此次   DET
债券   NOUN
发行   VERB
是   VERB
国家   NOUN
管网   NOUN
集团   NOUN
成立   VERB
后   PART
首次   ADV
在   ADP
公开   ADJ
市场   NOUN
亮相   VERB
，   PUNCT
由   ADP
工商   NOUN
银行   NOUN
独家   ADV
承销   VERB
，   PUNCT
发行   NOUN
金额   NOUN
60亿   NUM
元   NUM
、   PUNCT
期限   NOUN
270   NUM
天   NUM
，   PUNCT
为   ADP
本   DET
年度   NOUN
单笔   NOUN
最   ADV
大   ADJ
金额   NOUN
绿色   ADJ
超短   NOUN
期融   NOUN
资券   NOUN
，   PUNCT
募集   NOUN
资金   NOUN
将   ADV
全部   ADV
用于   VERB
储气库   NOUN
等   PART
绿色   ADJ
低碳   VERB
天然气   NOUN
储运   NOUN
基础   NOUN
设施   NOUN
建设   NOUN
；   PUNCT
发行   VERB
票面   ADJ
利率   NOUN
2   NUM
.   PUNCT
41%   NOUN
，   PUNCT
认购   NOUN
总   ADJ
规模   NOUN
达   VERB
2   NUM
.   PUNCT
53   NUM
倍   NUM
，   PUNCT
低于   VERB
资本   NOUN
市场   NOUN
同期   NOUN
可比   ADV
产品   NOUN
利率   NOUN
超10   VERB
个   NUM
BP   NOUN
，   PUNCT
反映   VERB
了   PART
市场   NOUN
对   ADP
绿色   ADJ
基础   NOUN
设施   NOUN
建设   NOUN
项目   NOUN
的   PART
青睐   NOUN
。   PUNCT
</code></pre>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext中文文本分析库 | 值得收藏</title>
      <link>https://textdata.cn/blog/cntext_v_1/</link>
      <pubDate>Mon, 08 Nov 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_v_1/</guid>
      <description>简单好用的中文Python文本分析包</description>
      <content:encoded><![CDATA[<h2 id="cntext">cntext</h2>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><strong>cntext</strong></li>
<li><strong>stats</strong>  文本统计,可读性等</li>
<li><strong>dictionary</strong> 构建词表(典)</li>
<li><strong>sentiment</strong>  情感分析</li>
<li><strong>similarity</strong>   文本相似度</li>
<li><strong>visualization</strong> 可视化，如词云图</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="https://github.com/hidadeng/cntext/tree/main/examples">https://github.com/hidadeng/cntext/tree/main/examples</a></p>
<br>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext==0.9
</code></pre></div><br>
<h2 id="一cntext">一、cntext</h2>
<p>查看cntext基本信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span>

<span class="n">help</span><span class="p">(</span><span class="n">cntext</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nf">description</span> <span class="p">(</span><span class="kn">package</span><span class="p">)</span>
    <span class="nf">dictionary</span> <span class="p">(</span><span class="kn">package</span><span class="p">)</span>
    <span class="nf">sentiment</span> <span class="p">(</span><span class="kn">package</span><span class="p">)</span>
    <span class="nf">similarity</span> <span class="p">(</span><span class="kn">package</span><span class="p">)</span>
    <span class="nf">visualization</span> <span class="p">(</span><span class="kn">package</span><span class="p">)</span>

<span class="nx">DATA</span>
    <span class="nx">ADV_words</span> <span class="p">=</span> <span class="p">[</span><span class="sc">&#39;都&#39;</span><span class="p">,</span> <span class="sc">&#39;全&#39;</span><span class="p">,</span> <span class="sc">&#39;单&#39;</span><span class="p">,</span> <span class="sc">&#39;共&#39;</span><span class="p">,</span> <span class="sc">&#39;光&#39;</span><span class="p">,</span> <span class="sc">&#39;尽&#39;</span><span class="p">,</span> <span class="sc">&#39;净&#39;</span><span class="p">,</span> <span class="sc">&#39;仅&#39;</span><span class="p">,</span> <span class="sc">&#39;就&#39;</span><span class="p">,</span> <span class="sc">&#39;只&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一共</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="o">...</span>
    <span class="nx">CONJ_words</span> <span class="p">=</span> <span class="p">[</span><span class="sc">&#39;乃&#39;</span><span class="p">,</span> <span class="sc">&#39;乍&#39;</span><span class="p">,</span> <span class="sc">&#39;与&#39;</span><span class="p">,</span> <span class="sc">&#39;无&#39;</span><span class="p">,</span> <span class="sc">&#39;且&#39;</span><span class="p">,</span> <span class="sc">&#39;丕&#39;</span><span class="p">,</span> <span class="sc">&#39;为&#39;</span><span class="p">,</span> <span class="sc">&#39;共&#39;</span><span class="p">,</span> <span class="sc">&#39;其&#39;</span><span class="p">,</span> <span class="sc">&#39;况&#39;</span><span class="p">,</span> <span class="sc">&#39;厥&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="o">...</span>
    <span class="nx">DUTIR_Ais</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">sigh</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一命呜呼</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一场春梦</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一场空</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一头跌在菜刀上</span><span class="err">－</span><span class="nx">切肤之痛</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一念之差</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">..</span>
    <span class="nx">DUTIR_Haos</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="mi">1</span><span class="nx">兒巴经</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="mi">3</span><span class="nx">x</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="mi">8</span><span class="nx">错</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">BUCUO</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">Cool毙</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">NB</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">DUTIR_Jings</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="mi">848</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">FT</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">_god</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">yun</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一个骰子掷七点</span><span class="err">－</span><span class="nx">出乎意料</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一举成名</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">......</span>
    <span class="nx">DUTIR_Jus</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">一则以喜</span><span class="err">，</span><span class="nx">一则以惧</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一发千钧</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一年被蛇咬</span><span class="err">，</span><span class="nx">三年怕草索</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一座皆惊</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一脸横肉</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一蛇两头</span><span class="o">...</span>
    <span class="nx">DUTIR_Les</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="p">:)</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">CC</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">Happy</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">LOL</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">_so</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">haha</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">DUTIR_Nus</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="mi">2</span><span class="nx">气斗狠</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">MD</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">TNND</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">gun</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">kao</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一刀两断</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">DUTIR_Wus</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">B4</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">BD</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">BS</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">HC</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">HJ</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">JJWW</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_deny</span> <span class="p">=</span> <span class="p">{</span><span class="sc">&#39;不&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不可</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不是</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不能</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不要</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;休&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_extreme</span> <span class="p">=</span> <span class="p">{</span><span class="sc">&#39;万&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">万万</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">万分</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">万般</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不亦乐乎</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不可开交</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_ish</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">一些</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一点</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一点儿</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不丁点儿</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不大</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不怎么</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_more</span> <span class="p">=</span> <span class="p">{</span><span class="sc">&#39;多&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">大不了</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">如斯</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">尤甚</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;强&#39;</span><span class="p">,</span> <span class="sc">&#39;愈&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_neg</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">一下子爆发</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一下子爆发的一连串</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一不小心</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一个屁</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一仍旧贯</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一偏</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_pos</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一专多能</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一丝不差</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一丝不苟</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一个心眼儿</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">一五一十</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">HOWNET_very</span> <span class="p">=</span> <span class="p">{</span><span class="err">&#39;</span><span class="nx">不为过</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不少</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不胜</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">不过</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">何啻</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">何止</span><span class="err">&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    <span class="nx">STOPWORDS_en</span> <span class="p">=</span> <span class="p">{</span><span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">about</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">above</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">across</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">after</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">afterwards</span><span class="err">&#39;</span><span class="o">...</span>
    <span class="nx">STOPWORDS_zh</span> <span class="p">=</span> <span class="p">{</span><span class="sc">&#39;、&#39;</span><span class="p">,</span> <span class="sc">&#39;。&#39;</span><span class="p">,</span> <span class="sc">&#39;〈&#39;</span><span class="p">,</span> <span class="sc">&#39;〉&#39;</span><span class="p">,</span> <span class="sc">&#39;《&#39;</span><span class="p">,</span> <span class="sc">&#39;》&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">}</span>
    
    <span class="nx">FORMAL_pos_words</span> <span class="p">=</span> <span class="p">[</span><span class="err">&#39;</span><span class="mi">100</span><span class="nx">强</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="mi">3</span><span class="nx">A级</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="mi">50</span><span class="nx">强</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">AAA级</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">AAA企业</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">爱戴</span><span class="err">&#39;</span><span class="p">,..]</span>
    <span class="nx">FORMAL_neg_words</span> <span class="p">=</span> <span class="p">[</span><span class="err">&#39;</span><span class="nx">安于现状</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">暗藏</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">暗淡</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">暗黑</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="nx">暗流</span><span class="err">&#39;</span><span class="p">,</span> <span class="p">..]</span>
    <span class="nx">UNFORMAL_pos_words</span> <span class="p">=</span> <span class="p">[</span><span class="err">&#39;</span><span class="nx">爱心</span><span class="sc">&#39;,&#39;</span><span class="nx">安定</span><span class="sc">&#39;,&#39;</span><span class="nx">安全</span><span class="sc">&#39;,&#39;</span><span class="nx">安然无恙</span><span class="sc">&#39;,&#39;</span><span class="nx">安泰</span><span class="sc">&#39;,&#39;</span><span class="nx">霸主</span><span class="err">&#39;</span><span class="p">,</span><span class="o">...</span><span class="p">]</span>
    <span class="nx">UNFORMAL_neg_words</span> <span class="p">=</span> <span class="p">[</span><span class="err">&#39;</span><span class="nx">哀鸿遍野</span><span class="sc">&#39;,&#39;</span><span class="nx">肮脏</span><span class="sc">&#39;,&#39;</span><span class="nx">罢免</span><span class="sc">&#39;,&#39;</span><span class="nx">白痴</span><span class="sc">&#39;,&#39;</span><span class="nx">败笔</span><span class="sc">&#39;,&#39;</span><span class="nx">败诉</span><span class="sc">&#39;,&#39;</span><span class="nx">半信半疑</span><span class="err">&#39;</span><span class="p">..]</span>



<span class="nx">FILE</span>
    <span class="o">/</span><span class="nx">Library</span><span class="o">/</span><span class="nx">Frameworks</span><span class="o">/</span><span class="nx">Python</span><span class="p">.</span><span class="nx">framework</span><span class="o">/</span><span class="nx">Versions</span><span class="o">/</span><span class="mf">3.7</span><span class="o">/</span><span class="nx">lib</span><span class="o">/</span><span class="nx">python3</span><span class="mf">.7</span><span class="o">/</span><span class="nx">site</span><span class="o">-</span><span class="nx">packages</span><span class="o">/</span><span class="nx">cntext</span><span class="o">/</span><span class="nx">__init__</span><span class="p">.</span><span class="nx">py</span>
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext</span> <span class="kn">import</span> <span class="n">dict_info</span>

<span class="n">dict_info</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> 【大连理工大学情感本体库】
     七大情绪分类，依次是哀、恶、好、惊、惧、乐、怒；对应的情绪词表依次：
    DUTIR_Ais = {&#34;泣血捶膺&#34;, &#34;望断白云&#34;, &#34;日暮途穷&#34;, &#34;身微力薄&#34;...}
    DUTIR_Wus = {&#34;饰非遂过&#34;, &#34;恶语&#34;, &#34;毁害&#34;, &#34;恶籍盈指&#34;, &#34;脾气爆躁&#34;, &#34;淫贱&#34;, &#34;凌乱&#34;...}
    DUTIR_Haos =  {&#34;打破砂锅璺到底&#34;, &#34;多彩&#34;, &#34;披沙拣金&#34;, &#34;见机行事&#34;, &#34;精神饱满&#34;...}
    DUTIR_Jings = {&#34;骇人视听&#34;, &#34;拍案惊奇&#34;, &#34;悬念&#34;, &#34;无翼而飞&#34;, &#34;原来&#34;, &#34;冷门&#34;...}
    DUTIR_Jus ={&#34;山摇地动&#34;, &#34;月黑风高&#34;, &#34;流血&#34;, &#34;老鼠偷猫饭－心惊肉跳&#34;, &#34;一发千钧&#34;...}
    DUTIR_Les ={&#34;含哺鼓腹&#34;, &#34;欢呼鼓舞&#34;, &#34;莺歌蝶舞&#34;, &#34;将伯之助&#34;, &#34;逸兴横飞&#34;, &#34;舒畅&#34;...}
    DUTIR_Nus = {&#34;怨气满腹&#34;, &#34;面有愠色&#34;, &#34;愤愤&#34;, &#34;直眉瞪眼&#34;, &#34;负气斗狠&#34;, &#34;挑眼&#34;...}
    
    【知网Hownet词典】
    含正负形容词、否定词、副词等词表，对应的词表依次:
    HOWNET_deny = {&#34;不&#34;, &#34;不是&#34;, &#34;不能&#34;, &#34;不可&#34;...}
    HOWNET_extreme = {&#34;百分之百&#34;, &#34;倍加&#34;, &#34;备至&#34;, &#34;不得了&#34;...}
    HOWNET_ish = {&#34;点点滴滴&#34;, &#34;多多少少&#34;, &#34;怪&#34;, &#34;好生&#34;, &#34;还&#34;, &#34;或多或少&#34;...}
    HOWNET_more = {&#34;大不了&#34;, &#34;多&#34;, &#34;更&#34;, &#34;比较&#34;, &#34;更加&#34;, &#34;更进一步&#34;, &#34;更为&#34;, &#34;还&#34;, &#34;还要&#34;...}
    HOWNET_neg = {&#34;压坏&#34;, &#34;鲁莽的&#34;, &#34;被控犯罪&#34;, &#34;银根紧&#34;, &#34;警惕的&#34;, &#34;残缺&#34;, &#34;致污物&#34;, &#34;柔弱&#34;...}
    HOWNET_pos = {&#34;无误&#34;, &#34;感激不尽&#34;, &#34;受大众欢迎&#34;, &#34;敬礼&#34;,  &#34;文雅&#34;, &#34;一尘不染&#34;, &#34;高精度&#34;, &#34;兴盛&#34;...}
    HOWNET_very = {&#34;不为过&#34;, &#34;超&#34;, &#34;超额&#34;, &#34;超外差&#34;, &#34;超微结构&#34;, &#34;超物质&#34;, &#34;出头&#34;...}
    
    【停用词表】
    中英文停用词表，依次
    STOPWORDS_zh = {&#34;经&#34;, &#34;得&#34;, &#34;则甚&#34;, &#34;跟&#34;, &#34;好&#34;, &#34;具体地说&#34;...}
    STOPWORDS_en = {&#39;a&#39;, &#39;about&#39;, &#39;above&#39;, &#39;across&#39;, &#39;after&#39;...}
    
    【中文副词/连词】
    副词ADV、连词CONJ
    ADV_words = [&#39;都&#39;, &#39;全&#39;, &#39;单&#39;, &#39;共&#39;, &#39;光&#39;...}
    CONJ_words = [&#39;乃&#39;, &#39;乍&#39;, &#39;与&#39;, &#39;无&#39;, &#39;且&#39;...}
    
    【金融情绪词典】
     姚加权,冯绪,王赞钧,纪荣嵘,张维.语调、情绪及市场影响:基于金融情绪词典[J].管理科学学报,2021,24(05):26-46.
     #正式-肯定情绪词典
     FORMAL_pos_words = [&#39;100强&#39;, &#39;3A级&#39;, &#39;50强&#39;, &#39;AAA级&#39;, &#39;AAA企业&#39;, &#39;爱戴&#39;,...]
     #正式-否定情绪词典
     FORMAL_neg_words = [&#39;安于现状&#39;, &#39;暗藏&#39;, &#39;暗淡&#39;, &#39;暗黑&#39;, &#39;暗流&#39;, ...]
     #非正式-肯定情绪词典
     UNFORMAL_pos_words = [&#39;爱心&#39;,&#39;安定&#39;,&#39;安全&#39;,&#39;安然无恙&#39;,&#39;安泰&#39;,&#39;霸主&#39;,...]
     #非正式-否定情绪词典
     UNFORMAL_neg_words = [&#39;哀鸿遍野&#39;,&#39;肮脏&#39;,&#39;罢免&#39;,&#39;白痴&#39;,&#39;败笔&#39;,&#39;败诉&#39;,&#39;半信半疑&#39;...]
</code></pre></div><br>
<p>查看词表</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext</span> <span class="kn">import</span> <span class="n">CONJ_words</span><span class="p">,</span> <span class="n">ADV_words</span>

<span class="c1">#获取连词词表</span>
<span class="n">CONJ_words</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;乃&#39;,
 &#39;乍&#39;,
 &#39;与&#39;,
 &#39;无&#39;,
 &#39;且&#39;,
 &#39;丕&#39;,
 &#39;为&#39;,
 &#39;共&#39;,
 &#39;其&#39;,
 &#39;况&#39;,
 &#39;厥&#39;,
 &#39;则&#39;,
 &#39;那&#39;,
 &#39;兼&#39;,
 ...
 ]
</code></pre></div><p><br><br></p>
<h2 id="二stats">二、stats</h2>
<p>目前含</p>
<ul>
<li>term_freq 词频统计函数，返回Counter类型</li>
<li>readability 中文可读性</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.stats</span> <span class="kn">import</span> <span class="n">term_freq</span><span class="p">,</span> <span class="n">readability</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更&#39;</span>
<span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<p>**中文可读性 ** 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
</blockquote>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">readability</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.17647058823529413,
 &#39;readability3&#39;: 13.588235294117647}
</code></pre></div><p><br><br></p>
<h2 id="三dictionary">三、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="31-sopmi-共现法">3.1 SoPmi 共现法</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.dictionary</span> <span class="kn">import</span> <span class="n">SoPmi</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">step 1/4:...seg corpus ...
Loading model cost 0.678 seconds.
Prefix dict has been built successfully.
step 1/4 finished:...cost 60.78995203971863...
step 2/4:...collect cowords ...
step 2/4 finished:...cost 0.6169600486755371...
step 3/4:...compute sopmi ...
step 1/4 finished:...cost 0.26422882080078125...
step 4/4:...save candiwords ...
finished! cost 61.8965539932251
</code></pre></div><br>
<h3 id="32-w2vmodels-词向量">3.2 W2VModels 词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.dictionary</span> <span class="kn">import</span> <span class="n">W2VModels</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据预处理开始.......
预处理结束...........
Word2Vec模型训练开始......
已将模型存入 /Users/Desktop/cntext/test/output/w2v_candi_words/w2v.model 

准备寻找每个seed在语料中所有的相似候选词
初步搜寻到 572 个相似的候选词
计算每个候选词 与 integrity 的相似度， 选出相似度最高的前 100 个候选词
已完成 【integrity 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/integrity.txt， 耗时 46 秒

准备寻找每个seed在语料中所有的相似候选词
初步搜寻到 516 个相似的候选词
计算每个候选词 与 innovation 的相似度， 选出相似度最高的前 100 个候选词
已完成 【innovation 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/innovation.txt， 耗时 46 秒

准备寻找每个seed在语料中所有的相似候选词
初步搜寻到 234 个相似的候选词
计算每个候选词 与 quality 的相似度， 选出相似度最高的前 100 个候选词
已完成 【quality 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/quality.txt， 耗时 46 秒

准备寻找每个seed在语料中所有的相似候选词
初步搜寻到 243 个相似的候选词
计算每个候选词 与 respect 的相似度， 选出相似度最高的前 100 个候选词
已完成 【respect 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/respect.txt， 耗时 46 秒

准备寻找每个seed在语料中所有的相似候选词
初步搜寻到 319 个相似的候选词
计算每个候选词 与 teamwork 的相似度， 选出相似度最高的前 100 个候选词
已完成 【teamwork 类】 的词语筛选，并保存于 /Users/Desktop/cntext/test/output/w2v_candi_words/teamwork.txt， 耗时 46 秒
</code></pre></div><p><br><br></p>
<h2 id="四-sentiment">四、 sentiment</h2>
<ul>
<li>senti_by_hownet 使用知网Hownet词典对文本进行<strong>情感</strong>分析</li>
<li>senti_by_dutir  使用大连理工大学情感本体库dutir对文本进行<strong>情绪</strong>分析</li>
<li>senti_by_diydict 使用<strong>自定义词典</strong> 对文本进行<strong>情感</strong>分析</li>
</ul>
<h3 id="41-senti_by_hownettext-adj_advfalse">4.1 senti_by_hownet(text, adj_adv=False)</h3>
<p>使用知网Hownet词典进行(中)文本数据的情感分析，统计正、负情感信息出现次数(得分)</p>
<ul>
<li>text:  待分析的中文文本数据</li>
<li>adj_adv:  是否考虑副词（否定词、程度词）对情绪形容词的反转和情感强度修饰作用，默认False。默认False只统计情感形容词出现个数；</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.sentiment</span> <span class="kn">import</span> <span class="n">senti_by_hownet</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;今天股票大涨，心情倍爽，非常开心啊。&#39;</span>

<span class="n">senti_by_dutir</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;word_num&#39;: 12,
 &#39;sentence_num&#39;: 2,
 &#39;stopword_num&#39;: 4,
 &#39;好_num&#39;: 0,
 &#39;乐_num&#39;: 1,
 &#39;哀_num&#39;: 0,
 &#39;怒_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;惊_num&#39;: 0}
</code></pre></div><br>
<p>考虑副词（否定词、程度词）对情绪形容词的反转和情感强度修饰作用</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">senti_by_hownet</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">adj_adv</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;sentence_num&#39;: 1,
 &#39;word_num&#39;: 12,
 &#39;stopword_num&#39;: 3,
 &#39;pos_score&#39;: 13.0,
 &#39;neg_score&#39;: 0.0}
</code></pre></div><p><br><br></p>
<h3 id="42-senti_by_dutirtext">4.2 senti_by_dutir(text)</h3>
<p>使用大连理工大学情感本体库对文本进行情绪分析，统计各情绪词语出现次数。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.sentiment</span> <span class="kn">import</span> <span class="n">senti_by_dutir</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;今天股票大涨，心情倍爽，非常开心啊。&#39;</span>

<span class="n">senti_by_dutir</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;word_num&#39;: 12,
 &#39;sentence_num&#39;: 2,
 &#39;stopword_num&#39;: 4,
 &#39;好_num&#39;: 0,
 &#39;乐_num&#39;: 1,
 &#39;哀_num&#39;: 0,
 &#39;怒_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;惊_num&#39;: 0}
</code></pre></div><blockquote>
<p>情绪分析使用的大连理工大学情感本体库，如发表论文，请注意用户许可协议</p>
<p>如果用户使用该资源发表论文或取得科研成果，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。</p>
<p>参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</blockquote>
<p><br><br></p>
<h3 id="43-senti_by_diytext">4.3 senti_by_diy(text)</h3>
<p>使用diy词典进行情感分析，计算各个情绪词出现次数，未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>sentiwords:  情感词字典；
{&lsquo;category1&rsquo;:  &lsquo;category1 词语列表&rsquo;,
&lsquo;category2&rsquo;: &lsquo;category2词语列表&rsquo;,
&lsquo;category3&rsquo;: &lsquo;category3词语列表&rsquo;,
&hellip;
}</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">sentiwords</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;开心&#39;</span><span class="p">,</span> <span class="s1">&#39;愉快&#39;</span><span class="p">,</span> <span class="s1">&#39;倍爽&#39;</span><span class="p">],</span>
              <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
              <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;倍&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;今天股票大涨，心情倍爽，非常开心啊。&#39;</span>
<span class="n">senti_by_diydict</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">sentiwords</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 1,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 4,
 &#39;sentence_num&#39;: 2,
 &#39;word_num&#39;: 12}
</code></pre></div><p><br><br></p>
<h3 id="44-注意">4.4 注意</h3>
<p><strong>返回结果</strong>:  <strong>num</strong>表示词语出现次数； score是考虑副词、否定词对情感的修饰，结果不是词频，是情感类别的得分。</p>
<p><br><br></p>
<h2 id="五similarity">五、similarity</h2>
<p>使用cosine、jaccard、miniedit等计算两文本的相似度，算法实现参考自</p>
<blockquote>
<p>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</p>
</blockquote>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">from cntext.similarity import similarity_score

text1 = &#39;编程真好玩编程真好玩&#39;
text2 = &#39;游戏真好玩编程真好玩&#39;

similarity_score(text1, text2)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;Sim_Cosine&#39;: 0.816496580927726,
 &#39;Sim_Jaccard&#39;: 0.6666666666666666,
 &#39;Sim_MinEdit&#39;: 1,
 &#39;Sim_Simple&#39;: 0.9183673469387755}
</code></pre></div><p><br><br></p>
<h2 id="六visualization">六、visualization</h2>
<p>文本信息可视化，含wordcloud、wordshiftor</p>
<ul>
<li>wordcloud 词云图</li>
<li>wordshiftor 两文本词移图</li>
</ul>
<h3 id="61-wordcloudtext-title-html_path">6.1 wordcloud(text, title, html_path)</h3>
<ul>
<li>text:  中文文本字符串数据</li>
<li>title:  词云图标题</li>
<li>html_path:  词云图html文件存储路径</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext.visualization</span> <span class="kn">import</span> <span class="n">wordcloud</span>

<span class="n">text1</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;在信息化时代，各种各样的数据被广泛采集和利用，有些数据看似无关紧要甚至好像是公开的，但同样关乎国家安全。11月1日是《反间谍法》颁布实施七周年。近年来，国家安全机关按照《反间谍法》《数据安全法》有关规定，依法履行数据安全监管职责，在全国范围内开展涉外数据专项执法行动，发现一些境外数据公司长期、大量、实时搜集我境内船舶数据，数据安全领域的“商业间谍”魅影重重。
</span><span class="s2">
</span><span class="s2">2020年6月，国家安全机关在反间谍专项行动中发现，有境外数据公司通过网络在境内私下招募“数据贡献员”。广东省湛江市国家安全局据此开展调查，在麻斜军港附近发现有可疑的无线电设备在持续搜集湛江港口舰船数据，并通过互联网实时传往境外。在临近海港的一个居民楼里，国家安全机关工作人员最终锁定了位置。
</span><span class="s2">
</span><span class="s2">一套简易的无线电设备是AIS陆基基站，用来接收AIS系统发射的船舶数据。AIS系统是船舶身份自动识别系统，国际海事组织要求300总吨以上船舶必须强制安装。船只在航行过程中，通过AIS系统向其他船只和主管部门发送船只航向、航速、目的港等信息，用于航行避让、交通导航、轨迹回溯等功能。国家安全机关查获的设备虽然看上去简陋，功能却十分强大。
</span><span class="s2">
</span><span class="s2">国家安全机关进一步调查发现，这个基站的来历并不简单。2016年，湛江市的无线电爱好者郑某偶然收到一封境外某海事数据公司发来的邀请邮件。
</span><span class="s2">
</span><span class="s2">作为资深的无线电爱好者，能免费领取价值几千元的设备还能获取更多的船舶信息，郑某当然心动。而且，这个基站的架设也非常容易，只要简单组装连上家里的网络，自己的任务就算完成。郑某马上浏览了这家公司申请无线电设备的页面，并按对方要求填写了信息。
</span><span class="s2">
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="n">wordcloud</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text1</span><span class="p">,</span> 
          <span class="n">title</span><span class="o">=</span><span class="s1">&#39;词云图测试&#39;</span><span class="p">,</span> 
          <span class="n">html_path</span><span class="o">=</span><span class="s1">&#39;output/词云图测试.html&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>

<figure >
    
        <img src="img/wordcloud.png" width="800" />
    
    
</figure>

<br>
<h3 id="62-wordshiftortext1-text2-title-top_n-matplotlib_family">6.2 wordshiftor(text1, text2, title, top_n, matplotlib_family)</h3>
<ul>
<li>text1:  文本数据1；字符串</li>
<li>text2:  文本数据2；字符串</li>
<li>title:  词移图标题</li>
<li>top_n:  显示最常用的前n词； 默认值15</li>
<li>matplotlib_family matplotlib中文字体，默认&quot;Arial Unicode MS&quot;；如绘图字体乱码请，请参考下面提示</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;在信息化时代，各种各样的数据被广泛采集和利用，有些数据看似无关紧要甚至好像是公开的，但同样关乎国家安全。11月1日是《反间谍法》颁布实施七周年。近年来，国家安全机关按照《反间谍法》《数据安全法》有关规定，依法履行数据安全监管职责，在全国范围内开展涉外数据专项执法行动，发现一些境外数据公司长期、大量、实时搜集我境内船舶数据，数据安全领域的“商业间谍”魅影重重。
</span><span class="s2">
</span><span class="s2">2020年6月，国家安全机关在反间谍专项行动中发现，有境外数据公司通过网络在境内私下招募“数据贡献员”。广东省湛江市国家安全局据此开展调查，在麻斜军港附近发现有可疑的无线电设备在持续搜集湛江港口舰船数据，并通过互联网实时传往境外。在临近海港的一个居民楼里，国家安全机关工作人员最终锁定了位置。
</span><span class="s2">
</span><span class="s2">一套简易的无线电设备是AIS陆基基站，用来接收AIS系统发射的船舶数据。AIS系统是船舶身份自动识别系统，国际海事组织要求300总吨以上船舶必须强制安装。船只在航行过程中，通过AIS系统向其他船只和主管部门发送船只航向、航速、目的港等信息，用于航行避让、交通导航、轨迹回溯等功能。国家安全机关查获的设备虽然看上去简陋，功能却十分强大。
</span><span class="s2">
</span><span class="s2">国家安全机关进一步调查发现，这个基站的来历并不简单。2016年，湛江市的无线电爱好者郑某偶然收到一封境外某海事数据公司发来的邀请邮件。
</span><span class="s2">
</span><span class="s2">作为资深的无线电爱好者，能免费领取价值几千元的设备还能获取更多的船舶信息，郑某当然心动。而且，这个基站的架设也非常容易，只要简单组装连上家里的网络，自己的任务就算完成。郑某马上浏览了这家公司申请无线电设备的页面，并按对方要求填写了信息。
</span><span class="s2">
</span><span class="s2">&#34;&#34;&#34;</span>


<span class="n">text2</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">通知强调，各地商务主管部门要紧紧围绕保供稳价工作目标，压实“菜篮子”市长负责制，细化工作措施；强化横向协作与纵向联动，加强与有关部门的工作协调，形成工作合力；建立完善省际间和本地区联保联供机制，健全有关工作方案，根据形势及时开展跨区域调运；加强市场运行监测，每日跟踪蔬菜、肉类等重点生活必需品供求和价格变化情况，及时预测，及早预警。
</span><span class="s2">
</span><span class="s2">通知要求，各地支持鼓励大型农产品流通企业与蔬菜、粮油、畜禽养殖等农产品生产基地建立紧密合作关系，签订长期供销协议；耐储蔬菜要提前采购，锁定货源，做好本地菜与客菜之间，北菜与南菜之间、设施菜与露天菜之间的梯次轮换和衔接供应；健全完备本地肉类储备规模及管理制度；北方省份要按时完成本年度冬春蔬菜储备计划，南方省份要根据自身情况建立完善蔬菜储备；及时投放肉类、蔬菜等生活必需品储备，补充市场供应。
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="kn">from</span> <span class="nn">cntext.visualization</span> <span class="kn">import</span> <span class="n">wordshiftor</span>

<span class="n">wordshiftor</span><span class="p">(</span><span class="n">text1</span><span class="o">=</span><span class="n">text1</span><span class="p">,</span> 
            <span class="n">text2</span><span class="o">=</span><span class="n">text2</span><span class="p">,</span> 
            <span class="n">title</span><span class="o">=</span><span class="s1">&#39;两文本对比&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>

<figure >
    
        <img src="img/wordshiftor.png" width="800" />
    
    
</figure>

<h3 id="63-textpictitlepython测试-subtitle使用python生成图片-fontalibaba-puhuiti-boldotf-titlesize18-subsize14">6.3 textpic(title=&lsquo;PYTHON测试&rsquo;, subtitle=&lsquo;使用Python生成图片&rsquo;, font=&lsquo;Alibaba-PuHuiTi-Bold.otf&rsquo;, titlesize=1.8, subsize=14)</h3>
<ul>
<li>title:  主标题</li>
<li>subtitle: 副标题</li>
<li>font:  本地中文字体路径</li>
<li>titlesize: 主标题字体大小</li>
<li>subsize: 副标题字体大小</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">textpic</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;PYTHON测试&#39;</span><span class="p">,</span> 
        <span class="n">subtitle</span><span class="o">=</span><span class="s1">&#39;使用Python生成图片&#39;</span><span class="p">,</span> 
        <span class="n">font</span><span class="o">=</span><span class="s1">&#39;data/Alibaba-PuHuiTi-Bold.otf&#39;</span><span class="p">,</span> 
        <span class="n">titlesize</span><span class="o">=</span><span class="mf">1.8</span><span class="p">,</span> 
        <span class="n">subsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/result.png" width="800" />
    
    
</figure>

<br>
<p><strong>注意</strong></p>
<blockquote>
<p>设置参数matplotlib_family，需要先运行下面代码获取本机字体列表
from matplotlib.font_manager import FontManager
mpl_fonts = set(f.name for f in FontManager().ttflist)
print(mpl_fonts)</p>
</blockquote>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Hugging Face | 自然语言处理平台</title>
      <link>https://textdata.cn/blog/huggingface_test/</link>
      <pubDate>Sun, 07 Nov 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/huggingface_test/</guid>
      <description>十行以内代码实现任意NLP功能</description>
      <content:encoded><![CDATA[<p>Huggingface（抱抱脸）总部位于纽约，是一家专注于自然语言处理、人工智能和分布式系统的创业公司。他们所提供的聊天机器人技术一直颇受欢迎，但更出名的是他们在NLP开源社区上的贡献。</p>
<p>Huggingface一直致力于自然语言处理NLP技术的平民化(democratize)，希望每个人都能用上最先进(SOTA, state-of-the-art)的NLP技术，而非困窘于训练资源的匮乏。</p>
<p><strong>Hugging Face所有模型的地址</strong></p>
<p><a href="https://huggingface.co/models">https://huggingface.co/models</a></p>
<p>你可以在这里下载所需要的模型，也可以上传你微调之后用于特定task的模型。</p>
<br>
<p><strong>Hugging Face使用文档的地址</strong></p>
<p><a href="https://huggingface.co/transformers/master/index.html">https://huggingface.co/transformers/master/index.html</a></p>
<p><br><br></p>
<h2 id="英汉互译">英汉互译</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">zh2en_model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;Helsinki-NLP/opus-mt-zh-en&#39;</span><span class="p">)</span>
<span class="n">zh2en_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;Helsinki-NLP/opus-mt-zh-en&#39;</span><span class="p">)</span>
<span class="n">zh2en_translation</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;translation_zh_to_en&#39;</span><span class="p">,</span> 
                       <span class="n">model</span><span class="o">=</span><span class="n">zh2en_model</span><span class="p">,</span> 
                       <span class="n">tokenizer</span><span class="o">=</span><span class="n">zh2en_tokenizer</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">zh2en_translation</span><span class="p">(</span><span class="s1">&#39;Python是一门非常强大的编程语言!&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>[{'translation_text': 'Python is a very powerful programming language!'}]
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">en2zh_model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;Helsinki-NLP/opus-mt-en-zh&#39;</span><span class="p">)</span>
<span class="n">en2zh_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;Helsinki-NLP/opus-mt-en-zh&#39;</span><span class="p">)</span>

<span class="n">en2zh_translation</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;translation_en_to_zh&#39;</span><span class="p">,</span> 
                       <span class="n">model</span><span class="o">=</span><span class="n">en2zh_model</span><span class="p">,</span> 
                       <span class="n">tokenizer</span><span class="o">=</span><span class="n">en2zh_tokenizer</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">en2zh_translation</span><span class="p">(</span><span class="s1">&#39;Python is a very powerful programming language!&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>[{'translation_text': 'Python是一个非常强大的编程语言!'}]
</code></pre>
<p><br><br></p>
<h2 id="文本分类">文本分类</h2>
<p>模型 <strong>uer/roberta-base-finetuned-chinanews-chinese</strong>是使用5个中文文本分类数据集训练得到</p>
<ul>
<li>京东full、京东binary和大众点评数据集包含不同情感极性的用户评论数据。</li>
<li>凤凰网 和 China Daily 包含不同主题类的新闻文本数据</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;uer/roberta-base-finetuned-chinanews-chinese&#39;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;uer/roberta-base-finetuned-chinanews-chinese&#39;</span><span class="p">)</span>
<span class="n">text_classification</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;sentiment-analysis&#39;</span><span class="p">,</span> 
                               <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> 
                               <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">test_text</span> <span class="o">=</span> <span class="s2">&#34;上证指数大涨2%&#34;</span>

<span class="n">text_classification</span><span class="p">(</span><span class="n">test_text</span><span class="p">,</span> <span class="n">return_all_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><pre><code>[[{'label': 'mainland China politics', 'score': 0.0002807585697155446},
  {'label': 'Hong Kong - Macau politics', 'score': 0.00015504546172451228},
  {'label': 'International news', 'score': 6.818029214628041e-05},
  {'label': 'financial news', 'score': 0.9991051554679871},
  {'label': 'culture', 'score': 0.00011297615128569305},
  {'label': 'entertainment', 'score': 0.00012184812658233568},
  {'label': 'sports', 'score': 0.0001558474759804085}]]
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">test_text</span> <span class="o">=</span> <span class="s2">&#34;Python是一门强大的编程语言&#34;</span>
<span class="n">text_classification</span><span class="p">(</span><span class="n">test_text</span><span class="p">,</span> <span class="n">return_all_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><pre><code>[[{'label': 'mainland China politics', 'score': 0.02050291746854782},
  {'label': 'Hong Kong - Macau politics', 'score': 0.0030984438490122557},
  {'label': 'International news', 'score': 0.005687597207725048},
  {'label': 'financial news', 'score': 0.03360358253121376},
  {'label': 'culture', 'score': 0.913349986076355},
  {'label': 'entertainment', 'score': 0.010810119099915028},
  {'label': 'sports', 'score': 0.012947351671755314}]]
</code></pre>
<p><br><br></p>
<h2 id="代码下载">代码下载</h2>
<p><a href="https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211108HuggingFace%E5%AD%A6%E4%B9%A0">https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211108HuggingFace学习</a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>automa插件|无编程基础也可自动化办公</title>
      <link>https://textdata.cn/blog/automa_rpa/</link>
      <pubDate>Wed, 27 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/automa_rpa/</guid>
      <description>automa从自动填写表单、执行重复性任务、截取屏幕截图到抓取网站数据，您想使用此扩展程序做什么取决于您。您甚至可以安排自动化执行的时间。</description>
      <content:encoded><![CDATA[<p>如果大家之前了解selenium库，那么antoma不用过多介绍，您就能知道ta是做浏览器自动化的。automa通过点击连接卡片实现浏览器的自动化运行。</p>
<p>没有做不到，只有想不到。从自动填写表单、执行重复性任务、截取屏幕截图到抓取网站数据，您想使用此扩展程序做什么取决于您。您甚至可以安排自动化执行的时间。下面我们看一下开发者制作的操作视频</p>
<video id="video" controls="" preload="none" poster="封面">
  <source id="mp4" src="Automa.mp4" type="video/mp4">
</videos>
<p>从视频中，大家可以看到，工作流可执行表单填写、屏幕截图、网站数据抓取等各种重复性工作。如果大家感兴趣，可以试着用一下<strong>automa</strong>。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>KeyBERT | 关键词发现</title>
      <link>https://textdata.cn/blog/keybert_tutorial/</link>
      <pubDate>Wed, 27 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/keybert_tutorial/</guid>
      <description>使用 BERT 嵌入 和 简单余弦相似度 来查找文档中与文档本身最相似的短语，自动挖掘文本中的关键词</description>
      <content:encoded><![CDATA[<p>尽管已经有很多方法可用于关键字生成（例如，Rake、YAKE!、TF-IDF 等），但我想创建一个非常基本但功能强大的方法来提取关键字和关键短语。这就是 KeyBERT 的用武之地！它使用 <strong>BERT 嵌入</strong> 和 <strong>简单余弦相似度</strong> 来查找文档中与文档本身最相似的短语。</p>
<p>KeyBERT步骤</p>
<ol>
<li>首先使用 BERT 提取文档嵌入以获得<strong>文档级向量表示</strong>。</li>
<li>随后，为 N-gram 词/短语提取<strong>词向量</strong>。</li>
<li>然后，我们使用余弦相似度来找到与文档最相似的单词/短语。</li>
<li>最后可以将最相似的词识别为最能描述整个文档的词。</li>
</ol>
<h2 id="代码下载">代码下载</h2>
<p><a href="KeyBERT%E5%AD%A6%E4%B9%A0.ipynb">click to download the code</a></p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">keybert</span><span class="o">==</span><span class="mf">0.5.0</span>
</code></pre></div><br>
<h2 id="初始化模型">初始化模型</h2>
<p>KeyBERT库需要安装配置spacy语言模型</p>
<p>具体参考<strong>公众号：大邓和他的Python</strong> 2021-10-29 的推文 查看spacy配置方法</p>
<p>初始化模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">import</span> <span class="nn">jieba</span>


<span class="n">zh_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;zh_core_web_sm&#34;</span><span class="p">)</span>
<span class="n">bertModel</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">zh_model</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="准备数据">准备数据</h2>
<p>中文测试数据需要先分词，而后构造成类英文的语言结构(用空格间隔的文本)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 测试数据</span>
<span class="n">doc</span> <span class="o">=</span>  <span class="s2">&#34;&#34;&#34;时值10月25日抗美援朝纪念日，《长津湖》片方发布了“纪念中国人民志愿军抗美援朝出国作战71周年特别短片”，再次向伟大的志愿军致敬！
</span><span class="s2">　　电影《长津湖》全情全景地还原了71年前抗美援朝战场上那场史诗战役，志愿军奋不顾身的英勇精神令观众感叹：“岁月峥嵘英雄不灭，丹心铁骨军魂永存！”影片上映以来票房屡创新高，目前突破53亿元，暂列中国影史票房总榜第三名。
</span><span class="s2">　　值得一提的是，这部影片的很多主创或有军人的血脉，或有当兵的经历，或者家人是军人。提起这些他们也充满自豪，影片总监制黄建新称：“当兵以后会有一种特别能坚持的劲儿。”饰演雷公的胡军透露：“我父亲曾经参加过抗美援朝，还得了一个三等功。”影片历史顾问王树增表示：“我当了五十多年的兵，我的老部队就是上甘岭上下来的，那些老兵都是我的偶像。”
</span><span class="s2">　　“身先士卒卫华夏家国，血战无畏护山河无恙。”片中饰演七连连长伍千里的吴京感叹：“要永远记住这些先烈们，他们给我们带来今天的和平。感谢他们的付出，才让我们有今天的幸福生活。”饰演新兵伍万里的易烊千玺表示：“战争的残酷、碾压式的伤害，其实我们现在的年轻人几乎很难能体会到，希望大家看完电影后能明白，是那些先辈们的牺牲奉献，换来了我们的现在。”
</span><span class="s2">　　影片对战争群像的恢弘呈现，对个体命运的深切关怀，令许多观众无法控制自己的眼泪，观众称：“当看到影片中的惊险战斗场面，看到英雄们壮怀激烈的拼杀，为国捐躯的英勇无畏和无悔付出，我明白了为什么说今天的幸福生活来之不易。”（记者 王金跃）
</span><span class="s2">        &#34;&#34;&#34;</span>


<span class="n">doc</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>


<span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">keywords</span>
</code></pre></div><pre><code>[('铁骨', 0.5028),
 ('纪念日', 0.495),
 ('丹心', 0.4894),
 ('战役', 0.4869),
 ('影史', 0.473),
 ('父亲', 0.4576),
 ('票房', 0.4571),
 ('偶像', 0.4497),
 ('精神', 0.4436),
 ('家国', 0.4373)]
</code></pre>
<br>
<h2 id="常用参数">常用参数</h2>
<p><strong>bertModel.extract_keywords(docs, keyphrase_ngram_range, stop_words, top_n)</strong></p>
<ul>
<li><strong>docs</strong> 文档字符串（空格间隔词语的字符串）</li>
<li><strong>keyphrase_ngram_range</strong> 设置ngram，默认(1, 1)</li>
<li><strong>stop_words</strong> 停用词列表</li>
<li><strong>top_n</strong> 显示前n个关键词，默认5</li>
<li><strong>highlight</strong> 可视化标亮关键词，默认False</li>
<li>use_maxsum: 默认False;是否使用Max Sum Similarity作为关键词提取标准，</li>
<li>use_mmr: 默认False;是否使用Maximal Marginal Relevance (MMR) 作为关键词提取标准</li>
<li>diversity 如果use_mmr=True，可以设置该参数。参数取值范围从0到1</li>
</ul>
<br>
<p>对于<strong>keyphrase_ngram_range</strong>参数，</p>
<ul>
<li>(1, 1) 只单个词， 如&quot;抗美援朝&quot;, &ldquo;纪念日&quot;是孤立的两个词</li>
<li>(2, 2) 考虑词组， 如出现有意义的词组 &ldquo;抗美援朝 纪念日&rdquo;</li>
<li>(1, 2) 同时考虑以上两者情况</li>
</ul>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">diversity</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> 
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">keywords</span>
</code></pre></div><pre><code>[('影片 总监制', 0.5412),
 ('丹心 铁骨', 0.5339),
 ('抗美援朝 纪念日', 0.5295),
 ('长津湖 片方', 0.5252),
 ('志愿军 致敬', 0.5207),
 ('老兵 偶像', 0.5192),
 ('票房 创新', 0.5108),
 ('军人 血脉', 0.5084),
 ('家国 血战', 0.4946),
 ('家人 军人', 0.4885)]
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#可视化</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">highlight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="highlight.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">use_mmr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">diversity</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> 
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">keywords</span>
</code></pre></div><pre><code>[('影片 总监制', 0.5412),
 ('长津湖 片方', 0.5252),
 ('抗美援朝 纪念日', 0.5295),
 ('丹心 铁骨', 0.5339),
 ('志愿军 致敬', 0.5207),
 ('老兵 偶像', 0.5192),
 ('票房 创新', 0.5108),
 ('军人 血脉', 0.5084),
 ('家国 血战', 0.4946),
 ('家人 军人', 0.4885)]
</code></pre>
<br>
<h2 id="英文keybert">英文KeyBERT</h2>
<p>同样需要配置spacy，参考<strong>公众号：大邓和他的Python</strong> 2021-10-29 的推文 查看spacy配置方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">en_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;en_core_web_sm&#34;</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">         Supervised learning is the machine learning task of learning a function that
</span><span class="s2">         maps an input to an output based on example input-output pairs. It infers a
</span><span class="s2">         function from labeled training data consisting of a set of training examples.
</span><span class="s2">         In supervised learning, each example is a pair consisting of an input object
</span><span class="s2">         (typically a vector) and a desired output value (also called the supervisory signal). 
</span><span class="s2">         A supervised learning algorithm analyzes the training data and produces an inferred function, 
</span><span class="s2">         which can be used for mapping new examples. An optimal scenario will allow for the 
</span><span class="s2">         algorithm to correctly determine the class labels for unseen instances. This requires 
</span><span class="s2">         the learning algorithm to generalize from the training data to unseen situations in a 
</span><span class="s2">         &#39;reasonable&#39; way (see inductive bias).
</span><span class="s2">      &#34;&#34;&#34;</span>
<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">kw_model</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">keywords</span>
</code></pre></div><p>Run</p>
<pre><code>[('supervised learning', 0.6779),
 ('supervised', 0.6676),
 ('signal supervised', 0.6152),
 ('examples supervised', 0.6112),
 ('labeled training', 0.6013)]
</code></pre>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>BERTopic库 | 使用预训练模型做话题建模</title>
      <link>https://textdata.cn/blog/bertopic_tutorial/</link>
      <pubDate>Tue, 26 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/bertopic_tutorial/</guid>
      <description>使用BERT主题建模技术,可以对经管等领域文本数据进行主题(话题)建模。效果堪比LDA，但比LDA智能</description>
      <content:encoded><![CDATA[<p>BERT是自然语言处理领域最新的词向量技术，而BERTopic 是基于BERT词向量进行主题建模技术，它利用 Transformer 和 c-TF-IDF 来创建密集的集群，允许轻松解释主题，同时在主题描述中保留重要词。</p>
<p>BERTopic亮点</p>
<ul>
<li>支持引导式Guided</li>
<li>支持（半）监督式</li>
<li>支持动态主题。</li>
<li>支持可视化</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">bertopic</span><span class="o">==</span><span class="mf">0.10.0</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">cntext</span><span class="o">==</span><span class="mf">1.6.5</span>
</code></pre></div><p><br><br></p>
<h2 id="准备数据">准备数据</h2>
<p>这里使用的新闻数据集， 共2000条。 新闻类别涵 <code>'娱乐', '教育', '游戏', '财经', '时政', '时尚', '科技', '体育', '家居', '房产'</code>
这里假设大家不知道有10类新闻题材， 构建模型的时候不会用到label字段的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;cnews.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 新闻题材</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>

<span class="c1">#记录数</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<pre><code>['娱乐' '教育' '游戏' '财经' '时政' '时尚' '科技' '体育' '家居' '房产']
2000
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 各类题材的新闻记录数</span>
<span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">时政    120
科技    106
时尚    106
财经    105
家居    103
教育     97
娱乐     96
体育     95
房产     87
游戏     85
</code></pre></div><br>
<p>这里定义了一个清洗数据函数clean_text，需要注意BERTopic需要先将中文分词改造成类似英文文本格式（用空格间隔词语）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;STOPWORDS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;STOPWORDS&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">test</span> <span class="o">=</span> <span class="s2">&#34;云南永善县级地震已致人伤间民房受损中新网月日电据云南昭通市防震减灾局官方网站消息截至日时云南昭通永善县级地震已造成人受伤其中重伤人轻伤人已全部送医院救治民房受损户间倒塌户间个乡镇所学校不同程度受损目前被损毁电力交通通讯设施已全部抢通修复当地已调拨帐篷顶紧急转移万人月日时分云南昭通永善县发生里氏级地震震源深度公里当地震感强烈此外成都等四川多地也有明显震感&#34;</span>

<span class="n">clean_text</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&#39;云南 永善县 级 地震 已致 伤间 民房 受损 中新网 日电 云南 昭通市 防震 减灾 局 官方网站 消息 日时 云南 昭通 永善县 级 地震 造成 受伤 重伤 轻伤 送 医院 救治 民房 受损 户间 倒塌 户间 乡镇 学校 不同 程度 受损 目前 损毁 电力 交通 通讯 设施 抢通 修复 调拨 帐篷 顶 紧急 转移 万人 时分 云南 昭通 永善县 发生 里氏 级 地震 震源 深度 公里 震感 强烈 成都 四川 多地 明显 震感&#39;
</code></pre></div><p>对2000条数据进行clean_text，得到的结果存储到content字段中。</p>
<p>我的macbook内存16G, 运行时间10s</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="训练topic模型">训练Topic模型</h2>
<p>文本分析步骤包括构建特征工程和训练，在本文中，直接使用开源的预训练中文词向量，省去了特征模型的学习时间。</p>
<p>选取的与训练模型均为word2vec格式，这样方便我们使用gensim将其导入。</p>
<table>
<thead>
<tr>
<th>模型名</th>
<th>数据</th>
<th>预训练模型资源地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>sgns.zhihu.words.bz2</td>
<td>知乎</td>
<td>链接: <a href="https://pan.baidu.com/s/1BDxP28KL_23Odj9NWZGe-Q">https://pan.baidu.com/s/1BDxP28KL_23Odj9NWZGe-Q</a> 提取码: n1qq</td>
</tr>
<tr>
<td>sgns.wiki.words.bz2</td>
<td>中文维基百科</td>
<td>链接: <a href="https://pan.baidu.com/s/1B1sxHmPeIPJYiCuP1zrmMw">https://pan.baidu.com/s/1B1sxHmPeIPJYiCuP1zrmMw</a> 提取码: hofj</td>
</tr>
<tr>
<td>sgns.financial.words.bz2</td>
<td>金融</td>
<td>链接: <a href="https://pan.baidu.com/s/1L_hmGjZMY2ExBn9Vfc_eRg">https://pan.baidu.com/s/1L_hmGjZMY2ExBn9Vfc_eRg</a> 提取码: hhn6</td>
</tr>
<tr>
<td>sgns.renmin.words.bz2</td>
<td>人民日报</td>
<td>链接: <a href="https://pan.baidu.com/s/1VQIDrwZH3Y3Lpy4-smPutw">https://pan.baidu.com/s/1VQIDrwZH3Y3Lpy4-smPutw</a> 提取码: 3b53</td>
</tr>
<tr>
<td>sgns.sougou.words.bz2</td>
<td>搜狗新闻</td>
<td>链接: <a href="https://pan.baidu.com/s/15nCaeB41mwK0ZVLrukXpFQ">https://pan.baidu.com/s/15nCaeB41mwK0ZVLrukXpFQ</a> 提取码: 04en</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Note</strong>:</p>
<p>除了表格外的资源，还可以使用spacy现有的预训练模型。</p>
</blockquote>
<p>本文案例cnews.csv是新闻类数据，这里最好选择使用同样为新闻题材的文本训练出的模型，这样BERTopic效果会更精准一些。sgns.sougou.words.bz2是使用搜狗新闻数据训练的语言模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">chinese_sougou_news_models</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;sgns.sogou.word.bz2&#39;</span><span class="p">,</span> <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">chinese_sougou_news_models</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x7f93e5b8cc10&gt;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>


<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="s2">&#34;chinese (simplified)&#34;</span><span class="p">,</span> 
                       <span class="n">embedding_model</span><span class="o">=</span><span class="n">chinese_sougou_news_models</span><span class="p">,</span>
                       <span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                       <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">docs</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1">#2000条进行fit_transform需要1min</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div><pre><code>100%|██████████| 2000/2000 [01:31&lt;00:00, 21.91it/s]
2021-10-28 12:11:25,583 - BERTopic - Transformed documents to Embeddings
2021-10-28 12:11:34,582 - BERTopic - Reduced dimensionality with UMAP
2021-10-28 12:11:34,718 - BERTopic - Clustered UMAP embeddings with HDBSCAN


CPU times: user 1min 50s, sys: 7.7 s, total: 1min 57s
Wall time: 1min 43s
</code></pre>
<p><br><br></p>
<h2 id="主题模型方法">主题模型方法</h2>
<ul>
<li>topic_model.get_topic_info 查看各主题信息</li>
<li>topic_model.find_topics(term, top_n=5)  查找term最有可能所属话题</li>
<li>topic_model.get_topic(0) 查看Topic 0的特征词</li>
<li>topic_model.visualize_topics() 话题间距离的可视化</li>
<li>topic_model.visualize_distribution(probs[0]) 查看某条文本的主题分布</li>
<li>topic_model.visualize_hierarchy(top_n_topics=20) 主题层次聚类可视化</li>
<li>topic_model.visualize_barchart(topics=[1]) 显示主题1的词条形图</li>
<li>topic_model.visualize_heatmap(n_clusters=10) 主题相似度热力图</li>
<li>topic_model.visualize_term_rank() 可视化词语</li>
<li>topic_model.save()  保存主题模型</li>
<li>topic_model.reduce_topics()  压缩主题个数(合并相近的主题)</li>
</ul>
<h3 id="get_topic_info">.get_topic_info()</h3>
<p>查看BERTopic基于cnews.csv数据， 跑出的各主题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_info</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/get_topic_info.png" alt=""  />
</p>
<br>
<h3 id="find_topicsterm">.find_topics(term)</h3>
<p>查看与词语【投资】最相关的主题，返回候选的最相思的5个主题id</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#</span>
<span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&#34;投资&#34;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">similar_topics</span>
</code></pre></div><p>Run</p>
<pre><code>[3, 9, 8, 10, 4]
</code></pre>
<br>
<h3 id="get_topic">.get_topic()</h3>
<p>查看id为3的主题信息（主题词及权重）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;基金&#39;, 0.15109221307919193),
 (&#39;投资&#39;, 0.042856192509064),
 (&#39;公司&#39;, 0.039785278320496976),
 (&#39;市场&#39;, 0.037072163603417835),
 (&#39;股票&#39;, 0.03230913401086524),
 (&#39;型基金&#39;, 0.02721898070238429),
 (&#39;收益&#39;, 0.025435672141638468),
 (&#39;投资者&#39;, 0.024633503649868493),
 (&#39;经理&#39;, 0.02458550023931051),
 (&#39;发行&#39;, 0.022672639068067168)]
</code></pre></div><br>
<h3 id="visualize_topics">.visualize_topics()</h3>
<p>可视化主题间距离</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">visualize_topics1</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">()</span>
<span class="c1">#可视化结果保存至html中，可以动态显示信息</span>
<span class="n">visualize_topics1</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;visualize_topics.html&#39;</span><span class="p">)</span>
<span class="n">visualize_topics1</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_topics1.png" alt=""  />
</p>
<p><a href="img/visualize_topics1.html">点击查看visualize_topics1.html</a></p>
<br>
<h3 id="visualize_distribution">.visualize_distribution()</h3>
<p>显示第一条新闻的主题概率分布</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">first_new_topic_probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_distribution</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">first_new_topic_probs</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;first_new_topic_probs.html&#39;</span><span class="p">)</span>
<span class="n">first_new_topic_probs</span>
</code></pre></div><p><img loading="lazy" src="img/first_new_topic_probs.png" alt=""  />

<a href="img/first_new_topic_probs.html">点击查看first_new_topic_probs.html</a></p>
<p>为了理解主题的潜在层次结构，我们可以使用 scipy.cluster.hierarchy 创建聚类并可视化它们之间的关系。 这有助于合并相似主题，达到降低主题模型主题数量nr_topics。</p>
<br>
<h3 id="visualize_hierarchytop_n_topics">.visualize_hierarchy(top_n_topics)</h3>
<p>话题层次聚类可视化，模型跑出12个主题，这里就按12进行分层聚类</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_hierarchy</span><span class="p">(</span><span class="n">top_n_topics</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_hierarchy.png" alt=""  />
</p>
<br>
<h3 id="visualize_barcharttopics">.visualize_barchart(topics)</h3>
<p>显示topics的词条形图</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_barchart</span><span class="p">(</span><span class="n">topics</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_barchart.png" alt=""  />
</p>
<br>
<h3 id="visualize_heatmapn_clusters">.visualize_heatmap(n_clusters)</h3>
<p>话题相似热力图。BERTopic可将主题以embeddings形式（向量）表示， 因此我们可以应用余弦相似度来创建相似度矩阵。 每两两主题可进行余弦计算，最终结果将是一个矩阵，显示主题间的相似程度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_similar_heatmap</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_heatmap</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">topic_similar_heatmap</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;topic_similar_heatmap.html&#39;</span><span class="p">)</span>
<span class="n">topic_similar_heatmap</span>
</code></pre></div><p><img loading="lazy" src="img/topic_similar_heatmap.png" alt=""  />

<a href="img/topic_similar_heatmap.html">点击查看topic_similar_heatmap.html</a></p>
<p>通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。 从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。 此外，可以轻松地将主题表示相互比较。</p>
<br>
<h3 id="visualize_term_rank">.visualize_term_rank()</h3>
<p>通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。</p>
<p>从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。</p>
<p>此外，可以轻松地将主题表示相互比较。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">term_score_decline</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_term_rank</span><span class="p">()</span>
<span class="n">term_score_decline</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;term_score_decline.html&#39;</span><span class="p">)</span>
<span class="n">term_score_decline</span>
</code></pre></div><p><img loading="lazy" src="img/term_score_decline.png" alt=""  />

<a href="img/term_score_decline.html">点击查看term_score_decline.html</a></p>
<h3 id="update_topics">.update_topics()</h3>
<p>更新主题模型。当您训练了一个模型并查看了代表它们的主题和单词时，您可能对表示不满意。 也许您忘记删除停用词，或者您想尝试不同的 n_gram_range。 我们可以使用函数 update_topics 使用 c-TF-IDF 的新参数更新主题表示。</p>
<p>使用.update_topics()更新，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">update_topics</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div><p>topic_model得到了更新，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&#34;手机&#34;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">similar_topics</span>
</code></pre></div><p>Run</p>
<pre><code>[2, 7, 4, 1, 5]
</code></pre>
<p>查看话题2的信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;功能&#39;, 0.022132351014298786),
 (&#39;采用&#39;, 0.02136925357979149),
 (&#39;像素&#39;, 0.020797285140907094),
 (&#39;拍摄&#39;, 0.017850841110848677),
 (&#39;机身&#39;, 0.015056931248982912),
 (&#39;英寸&#39;, 0.014624438184138326),
 (&#39;佳能&#39;, 0.012857768505732597),
 (&#39;支持&#39;, 0.012600856600766349),
 (&#39;光学&#39;, 0.012462085658291079),
 (&#39;相机&#39;, 0.011832978982454568)]
</code></pre></div><p>模型保存</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Save model</span>
<span class="c1">#model.save(&#34;my_model&#34;)</span>
<span class="c1"># Load model</span>
<span class="c1">#my_model = BERTopic.load(&#34;my_model&#34;)</span>
</code></pre></div><br>
<h3 id="reduce_topics">.reduce_topics()</h3>
<p>压缩主题数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">reduce_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>2021-10-28 12:28:01,976 - BERTopic - Reduced number of topics from 20 to 11
</code></pre>
<br>
<h2 id="代码数据">代码数据</h2>
<p><a href="bertopic_tutorial.zip">click to download</a></p>
<br>
<h2 id="总结">总结</h2>
<p>本文使用中文文本数据展示BERTopic部分功能，如果对英文数据感兴趣，可以前往  <a href="https://github.com/MaartenGr/BERTopic">https://github.com/MaartenGr/BERTopic</a> 深入学习。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Shifterator库 | 词移图分辨两文本用词风格差异</title>
      <link>https://textdata.cn/blog/shifterator_text_vis/</link>
      <pubDate>Tue, 26 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/shifterator_text_vis/</guid>
      <description>图文代码理解两文本用词风格差异</description>
      <content:encoded><![CDATA[<p>以往对比两个文本数据差异，比较简单的技术实现方法是生成两文个词云图，但是词云图无法直观显示词语层面的权重。</p>
<p><strong>Shifterator</strong>包提供了构建词移图的功能，垂直条形图可以量化哪些词会导致<strong>两个文本</strong>之间的成对差异以及它们如何起作用。 通过允许您查看单词使用方式的变化，单词转换可帮助您对情绪、熵和分歧进行分析，这些分析从根本上来说更具可解释性。</p>
<p>Shifterator亮点：</p>
<ul>
<li>提供可解释的工具，用于将文本作为数据处理并映射出两个文本相似性或差异性</li>
<li>实现常见的文本比较度量，包括相对频率、香农熵、Tsallis熵、Kullback-Leibler散度和 Jensen-Shannon 散度。</li>
<li>基于字典的情绪分析方法计算的加权平均值。</li>
<li>在研究初期可用于诊断数据、感知测量误差。</li>
</ul>
<p>计算社会科学家、数字人文主义者和其他文本分析从业者都可以使用 Shifterator 从文本数据构建可靠、稳健和可解释的故事。</p>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">shifterator</span><span class="o">==</span><span class="mf">0.2.2</span>
</code></pre></div><br>
<h2 id="导入数据">导入数据</h2>
<p>准备的外卖csv数据，含label和review两个字段。</p>
<p>其中label是好评差评的标注，</p>
<ul>
<li>0为差评，</li>
<li>1为好评</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">reviews_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;data/WaiMai8k.csv&#34;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
<span class="n">reviews_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/df.png" width="800" />
    
    
</figure>

<p>有个疑问，外卖好差评中的用词有什么差异(区别/特点)？</p>
<h2 id="准备两组文本数据">准备两组文本数据</h2>
<p>shifterator需要两组文本数据，格式为长度相同的词频统计字典。</p>
<p>按照label类别，将数据整理为两个文本数据。在准备的过程中，我们需要做一些清洗操作</p>
<ul>
<li>清除非中文字符，如网址、邮箱、标点符号</li>
<li>清除信息量比较低的停用词</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">re</span> 

<span class="n">texts_neg</span> <span class="o">=</span> <span class="n">reviews_df</span><span class="p">[</span><span class="n">reviews_df</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;review&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">texts_pos</span> <span class="o">=</span> <span class="n">reviews_df</span><span class="p">[</span><span class="n">reviews_df</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;review&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;清洗文本中的非中文字符、停用词，返回词频统计结果
</span><span class="s2">    docs : 待处理的文档列表
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">stop_words</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/stopwords.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&#34;&#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s2">&#34;&#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s2">&#34;[</span><span class="se">\u4e00</span><span class="s2">-</span><span class="se">\u9fa5</span><span class="s2">]+&#34;</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
    <span class="n">wordfreq_dict</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">wordfreq_dict</span>


<span class="n">clean_texts_neg</span> <span class="o">=</span> <span class="n">clean_text</span><span class="p">(</span><span class="n">texts_neg</span><span class="p">)</span>
<span class="n">clean_texts_pos</span> <span class="o">=</span> <span class="n">clean_text</span><span class="p">(</span><span class="n">texts_pos</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="无聊的传统绘图">无聊的传统绘图</h2>
<p>使用条形图、词云图绘制，为了缩小代码量，这里只绘制差评数据。需要注意的是matplotlib不显示中文，这里需要先使用下面三行代码获取电脑中自带的中文字体列表mpl_fonts，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">matplotlib.font_manager</span> <span class="kn">import</span> <span class="n">FontManager</span>
 
<span class="n">mpl_fonts</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">FontManager</span><span class="p">()</span><span class="o">.</span><span class="n">ttflist</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mpl_fonts</span><span class="p">)</span>
</code></pre></div><p>经过运行，我的电脑mpl_fonts里有<strong>Arial Unicode MS</strong> ，后面用matplotlib显示中文的地方，我都使用该字体。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#准备DataFrame数据</span>
<span class="n">common_neg</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">clean_texts_neg</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">15</span><span class="p">),</span>
                             <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;words&#39;</span><span class="p">,</span> <span class="s1">&#39;count&#39;</span><span class="p">])</span>

<span class="n">common_pos</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">clean_texts_pos</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">15</span><span class="p">),</span>
                             <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;words&#39;</span><span class="p">,</span> <span class="s1">&#39;count&#39;</span><span class="p">])</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&#34;whitegrid&#34;</span><span class="p">)</span>

<span class="c1">#为了显示中文</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s2">&#34;font&#34;</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">)</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1">#绘制水平条形图</span>
<span class="n">common_neg</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;count&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;words&#39;</span><span class="p">,</span>
                     <span class="n">y</span><span class="o">=</span><span class="s1">&#39;count&#39;</span><span class="p">,</span>
                      <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                      <span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&#34;外卖差评常见词&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/output_9_0.png" width="800" />
    
    
</figure>

<p>绘制词云图，这里使用的pyecharts包。由于该包作者更新强度比较大，为了保证日后本教程仍可正常运行，这里提供当前我使用的pyecharts相关的版本，大家可以运行下面代码保证运行出正确结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">pyecharts</span><span class="o">==</span><span class="mf">1.6.2</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">pyecharts</span><span class="o">-</span><span class="n">javascripthon</span><span class="o">==</span><span class="mf">0.0.6</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">pyecharts</span><span class="o">-</span><span class="n">jupyter</span><span class="o">-</span><span class="n">installer</span><span class="o">==</span><span class="mf">0.0.3</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">pyecharts</span><span class="o">-</span><span class="n">snapshot</span><span class="o">==</span><span class="mf">0.2.0</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>

<span class="n">wordfreqs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">f</span><span class="p">))</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">f</span> <span class="ow">in</span> <span class="nb">dict</span><span class="p">(</span><span class="n">clean_texts_neg</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>


<span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
<span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">wordfreqs</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
<span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span><span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;外卖差评词云图&#34;</span><span class="p">,</span> 
                                             <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">23</span><span class="p">)),</span>
                   <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">wc</span><span class="o">.</span><span class="n">load_javascript</span><span class="p">()</span>
<span class="n">wc</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/%e8%af%8d%e4%ba%91%e5%9b%be.png" width="800" />
    
    
</figure>

<h2 id="使用shifterator绘制词移图">使用Shifterator绘制词移图</h2>
<p>终于要用到 <strong>Shifterator</strong> 包了！ 我们可以使用这个包根据频率和情绪（或其他值）比较负面和正面的外卖评论，这里我只计算了频率作为权重</p>
<h3 id="熵移图entropy-shift">熵移图Entropy shift</h3>
<p>第一幅图是entropy shift graph</p>
<p>具体信息请查看文档  <a href="https://github.com/ryanjgallagher/shifterator">https://github.com/ryanjgallagher/shifterator</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">shifterator</span> <span class="kn">import</span> <span class="n">EntropyShift</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s2">&#34;font&#34;</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">)</span>

<span class="n">entropy_shift</span> <span class="o">=</span> <span class="n">EntropyShift</span><span class="p">(</span><span class="n">type2freq_1</span><span class="o">=</span><span class="n">clean_texts_neg</span><span class="p">,</span>
                             <span class="n">type2freq_2</span><span class="o">=</span><span class="n">clean_texts_pos</span><span class="p">,</span>
                             <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">entropy_shift</span><span class="o">.</span><span class="n">get_shift_graph</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;外卖差评 vs 外卖好评&#39;</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output141.png" width="800" />
    
    
</figure>

<p>看起来最能决定外卖差评的用语是配送时间，其次才是口味。</p>
<p>最能决定外卖好评的似乎是口味，其次才是配送时间。</p>
<p>通过Shifterator我们能够看出不同词在不同文本中的作用程度。需要注意的是，我们只使用了最高的前15词频，所以显示的词有些少</p>
<h2 id="总结">总结</h2>
<p>希望本文能对你的研究有帮助，代码下载地址</p>
<p><a href="https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211027shifterator%E5%AD%A6%E4%B9%A0">https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211027shifterator学习</a></p>
<p>代码撰写调试不易，希望帮忙转载</p>

<figure >
    
        <img src="img/Python%e4%b8%93%e6%a0%8f%e8%af%be.jpg" width="800" />
    
    
</figure>

<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Label-Studio|多媒体数据标注工具</title>
      <link>https://textdata.cn/blog/label_studio_test/</link>
      <pubDate>Sun, 18 Jul 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/label_studio_test/</guid>
      <description>自然语言处理前需要先标注数据,label-studio让数据标注过程变得轻松简单</description>
      <content:encoded><![CDATA[<h2 id="1-简介">1. 简介</h2>
<h3 id="label-studiohttpsgithubcomheartexlabslabel-studio"><a href="https://github.com/heartexlabs/label-studio">label-studio</a></h3>
<p>假设我们想使用机器学习做文本分析，一般都需要先对数据进行标注，才能训练出效果比较好的监督机器学习模型。</p>
<p>label-studio是多媒体数据标注工具，可以很方便的进行标注和导出。</p>
<p>Label Studio 是一款开源数据标注工具，用于标注和探索多种类型的数据。 您可以使用多种数据格式执行的标记任务。</p>
<p>您还可以将 Label Studio 与机器学习模型集成，以提供标签（预标签）的预测，或执行持续的主动学习。</p>
<p>官方文档 <a href="https://labelstud.io/">https://labelstud.io/</a></p>
<br>
<h3 id="操作步骤">操作步骤</h3>
<ol>
<li>安装Label Studio</li>
<li>启动Label Studio</li>
<li>创建Label Studio账号</li>
<li>项目默认配置</li>
<li>导入数据</li>
<li>标注数据</li>
<li>结束标记，导出标注数据</li>
</ol>
<br>
<h3 id="安装">安装</h3>
<p>命令行中执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install label-studio==1.1.0
</code></pre></div><h2 id="2-快速上手">2 快速上手</h2>
<p>在桌面创建自动生成一个名为Project的项目文件夹。</p>
<ul>
<li>Win命令行执行</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">label-studio --data-dir Desktop/Project
</code></pre></div><ul>
<li>Mac命令行执行</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">label-studio --data-dir desktop/Project
</code></pre></div><br>
<br>
<p>执行上方代码大概10s左右，会在浏览器弹出如下界面</p>

<figure >
    
        <img src="img/%e5%88%9b%e5%bb%ba%e8%b4%a6%e6%88%b7.png" width="800" />
    
    
</figure>

<p>注册好账号密码，点击<strong>Create Project</strong></p>

<figure >
    
        <img src="img/%e5%88%9b%e5%bb%ba%e9%a1%b9%e7%9b%ae.png" width="800" />
    
    
</figure>

<br>
<p>项目描述填写好，点击按钮**Data Import **，</p>

<figure >
    
        <img src="img/%e8%ae%be%e7%bd%ae%e9%a1%b9%e7%9b%ae%e6%8f%8f%e8%bf%b0.png" width="800" />
    
    
</figure>

<p>这里我们要做文本分析，导入csv</p>

<figure >
    
        <img src="img/%e5%af%bc%e5%85%a5%e6%95%b0%e6%8d%ae1.png" width="800" />
    
    
</figure>


<figure >
    
        <img src="img/%e5%af%bc%e5%85%a5%e6%95%b0%e6%8d%ae2.png" width="800" />
    
    
</figure>

<br>
<p>设置标注模式，点击按钮<strong>Labeling Setup</strong>,选择<strong>Natural Language Process</strong>、<strong>TEXT Classification</strong>。就考研进行pos、neg、neo三个类别的文本标注。</p>

<figure >
    
        <img src="img/%e8%ae%be%e7%bd%ae%e6%a0%87%e6%b3%a8%e6%a8%a1%e5%bc%8f1.png" width="800" />
    
    
</figure>

<p>注意label-studio提供了diy，考研根据自己需要点击<strong>Code</strong>设定标注类别名称、增减类别。大家感兴趣的可以深入研究。</p>

<figure >
    
        <img src="img/%e8%ae%be%e7%bd%ae%e6%a0%87%e6%b3%a8%e6%a8%a1%e5%bc%8f2.png" width="800" />
    
    
</figure>

<p>点击<strong>Save</strong> 按钮，开始准备标注数据啦</p>
<br>
<p>数据界面，勾选全部数据，点击蓝色按钮<strong>Label All Tasks</strong></p>

<figure >
    
        <img src="img/%e6%95%b0%e6%8d%ae%e7%95%8c%e9%9d%a2.png" width="800" />
    
    
</figure>

<p>开始标注，勾选你认为合适的标签，点击右侧<strong>Submit</strong></p>

<figure >
    
        <img src="img/%e5%bc%80%e5%a7%8b%e6%a0%87%e6%b3%a8.png" width="800" />
    
    
</figure>

<br>
<p>导出标注数据,先点击右侧<strong>Export</strong>按钮，选择导出格式，最后点击底部<strong>Export</strong>按钮执行导出。</p>

<figure >
    
        <img src="img/%e5%af%bc%e5%87%ba%e6%a0%87%e6%b3%a8%e6%95%b0%e6%8d%ae.png" width="800" />
    
    
</figure>

<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>tfidf有权重的情感分析</title>
      <link>https://textdata.cn/blog/weighted_tfidf_sentiment_analysis/</link>
      <pubDate>Sun, 18 Jul 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/weighted_tfidf_sentiment_analysis/</guid>
      <description>最简便的词典法(有权重的)情感分析</description>
      <content:encoded><![CDATA[<h1 id="情感分析">情感分析</h1>
<ul>
<li>无权重。直接计算文本中正、负情感词出现的次数</li>
<li>有权重。tf-idf， tf是词频，idf是权重。</li>
</ul>
<h2 id="tfidf法">Tfidf法</h2>
<p>scikit库除了CountVectorizer类，还有TfidfVectorizer类。TF-IDF这个定义相信大家应该已经耳熟能详了：</p>
<p>
<figure >
    
        <img src="img/tf.png" width="800" />
    
    
</figure>


<figure >
    
        <img src="img/idf.png" width="800" />
    
    
</figure>


<figure >
    
        <img src="img/tfidf.png" width="800" />
    
    
</figure>
</p>
<ul>
<li>TF 词语出现越多，这个词越有信息量</li>
<li>IDF 词语越少的出现在文本中，词语越有信息量。</li>
</ul>
<br>
<h2 id="原始数据">原始数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;hello, i am glad to meet you&#34;</span><span class="p">,</span>
           <span class="s2">&#34;it is wonderful&#34;</span><span class="p">,</span>
           <span class="s2">&#34;i hate you&#34;</span><span class="p">,</span>
           <span class="s2">&#34;i am sad&#34;</span><span class="p">]</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Text&#39;</span><span class="p">])</span>
<span class="n">df1</span>
</code></pre></div>
<figure >
    
        <img src="img/1.png" width="800" />
    
    
</figure>

<br>
<h2 id="构造tfidf">构造tfidf</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>


<span class="k">def</span> <span class="nf">createDTM</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;构建文档词语矩阵&#34;&#34;&#34;</span>
    <span class="n">vectorize</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
    <span class="c1">#注意fit_transform相当于fit之后又transform。</span>
    <span class="n">dtm</span> <span class="o">=</span> <span class="n">vectorize</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
    <span class="c1">#vectorize.fit(corpus)</span>
    <span class="c1">#dtm  = vectorize.transform(corpus) </span>
    <span class="c1">#打印dtm</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dtm</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> 
                        <span class="n">columns</span><span class="o">=</span><span class="n">vectorize</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span> 

<span class="n">df2</span> <span class="o">=</span> <span class="n">createDTM</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
<span class="n">df2</span>
</code></pre></div>
<figure >
    
        <img src="img/2.png" width="800" />
    
    
</figure>

<br>
<h2 id="合并df1和df2">合并df1和df2</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div>
<figure >
    
        <img src="img/3.png" width="800" />
    
    
</figure>

<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#积极词典</span>
<span class="n">pos_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;glad&#39;</span><span class="p">,</span> <span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;wonderful&#39;</span><span class="p">]</span>

<span class="c1">#消极词典</span>
<span class="n">neg_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sad&#39;</span><span class="p">,</span> <span class="s1">&#39;hate&#39;</span><span class="p">]</span>

</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#积极词典</span>
<span class="n">df</span><span class="p">[</span><span class="n">pos_words</span><span class="p">]</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="n">pos_words</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>0    0.873439
1    0.577350
2    0.000000
3    0.000000
dtype: float64
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Pos&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">pos_words</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div>
<figure >
    
        <img src="img/4.png" width="800" />
    
    
</figure>

<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Neg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">neg_words</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div>
<figure >
    
        <img src="img/5.png" width="800" />
    
    
</figure>

<br>
<h2 id="输出">输出</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;output/tfidf有权重的情感分析.csv&#39;</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>中文金融情感词典</title>
      <link>https://textdata.cn/blog/chinese_financial_dictionary/</link>
      <pubDate>Tue, 13 Jul 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/chinese_financial_dictionary/</guid>
      <description>基于 姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报 开发了中文的金融情感词典第一个权威的中文情感词典|配合cnsenti使用</description>
      <content:encoded><![CDATA[<p>可以使用cnsenti库中的自定义方法，计算年报或财经类社交媒体的文本情绪。</p>
<blockquote>
<p>姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报，2021. 24(5), 26-46.</p>
</blockquote>
<p>该论文开发了中文的金融情感词典，已有的中文金融情感词典有以下不足：</p>
<ul>
<li>大多采用形容情绪词，对于金融场景适用性差</li>
<li>将LM英文词典本土化，制作中文金融情绪词典</li>
<li>词典构建方法多为人工</li>
</ul>
<p>该论文开发中文情绪词典，从年报和社交媒体两个数据源出发，借助数据挖掘和深度学习算法，构建了正式用语 和 非正式用于两大类情感词典。</p>
<h2 id="标注思路">标注思路</h2>
<p>一般构建词典要么用多个词典融合，要么人工标准训练。该论文采用了一定的技巧，不需要人工标注即可实现近乎人工标注的效果。</p>
<h3 id="正式词典标注思路">正式词典标注思路</h3>
<p>正式用语情感词典，通过年报公布后3个交易日累积正负收益率为标准，将年报标记为正负面情绪两类。</p>
<h3 id="非正式词典标注思路">非正式词典标注思路</h3>
<p>使用所有中国上市公司在雪球论坛和东方财富股吧内相关帖子，共8130万条。</p>
<p>在网络股票论坛，用户发表自己的意见时，经常带有表情符号，从而使得帖子带有明显的情绪指标。 这种含有特殊指标的帖子，省去了人工标注文本情绪的工作。</p>
<br>
<p>具体构建词典的步骤，大家可以阅读论文原文。论文已经公开了中文情感词典，我已将其整理为4个txt文件</p>
<ul>
<li>formal_pos.txt  正式用语<strong>正面</strong>情绪词典</li>
<li>formal_neg.txt  正式用语<strong>负面</strong>情绪词典</li>
<li>unformal_pos.txt  非正式用语<strong>正面</strong>情绪词典</li>
<li>unformal_neg.txt  非正式用语<strong>负面</strong>情绪词典</li>
</ul>
<br>
<h2 id="中文金融词典使用方法">中文金融词典使用方法</h2>
<p>cnsenti实现了自定义词典功能，导入不同的txt词典文件，即可实现不同方面的情绪词统计。</p>
<h3 id="年报正式用语词典">年报正式用语词典</h3>
<ul>
<li>dict/formal_pos.txt   正式用语<strong>正面</strong>情绪词典</li>
<li>dict/formal_neg.txt    正式用语<strong>负面</strong>情绪词典</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cnsenti</span> <span class="kn">import</span> <span class="n">Sentiment</span>

<span class="n">senti</span> <span class="o">=</span> <span class="n">Sentiment</span><span class="p">(</span><span class="n">pos</span><span class="o">=</span><span class="s1">&#39;dict/formal_pos.txt&#39;</span><span class="p">,</span>  <span class="c1">#正面词典txt文件相对路径</span>
                  <span class="n">neg</span><span class="o">=</span><span class="s1">&#39;dict/formal_neg.txt&#39;</span><span class="p">,</span>  <span class="c1">#负面词典txt文件相对路径</span>
                  <span class="n">merge</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>             <span class="c1">#是否将cnsenti自带词典和用户导入的自定义词典融合</span>
                  <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>      <span class="c1">#两txt均为utf-8编码</span>

<span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;这家公司是行业的引领者，是中流砥柱。今年的业绩非常好。&#39;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">senti</span><span class="o">.</span><span class="n">sentiment_count</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sentiment_count&#39;</span><span class="p">,</span><span class="n">result</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sentiment_count {&#39;words&#39;: 16, &#39;sentences&#39;: 2, &#39;pos&#39;: 3, &#39;neg&#39;: 0}
</code></pre></div><br>
<h3 id="财经社交媒体非正式用语词典">财经社交媒体非正式用语词典</h3>
<ul>
<li>dict/unformal_pos.txt   非正式用语<strong>正面</strong>情绪词典</li>
<li>dict/unformal_neg.txt    非正式用语<strong>负面</strong>情绪词典</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cnsenti</span> <span class="kn">import</span> <span class="n">Sentiment</span>

<span class="n">senti</span> <span class="o">=</span> <span class="n">Sentiment</span><span class="p">(</span><span class="n">pos</span><span class="o">=</span><span class="s1">&#39;dict/unformal_pos.txt&#39;</span><span class="p">,</span>  <span class="c1">#正面词典txt文件相对路径</span>
                  <span class="n">neg</span><span class="o">=</span><span class="s1">&#39;dict/unformal_neg.txt&#39;</span><span class="p">,</span>  <span class="c1">#负面词典txt文件相对路径</span>
                  <span class="n">merge</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>             <span class="c1">#融合cnsenti自带词典和用户导入的自定义词典</span>
                  <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>      <span class="c1">#两txt均为utf-8编码</span>

<span class="n">test_text</span> <span class="o">=</span> <span class="s1">&#39;这个股票前期走势承压，现在阴跌，散户只能割肉离场，这股票真垃圾&#39;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">senti</span><span class="o">.</span><span class="n">sentiment_count</span><span class="p">(</span><span class="n">test_text</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sentiment_count&#39;</span><span class="p">,</span><span class="n">result</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">sentiment_count {&#39;words&#39;: 18, &#39;sentences&#39;: 1, &#39;pos&#39;: 0, &#39;neg&#39;: 2}
</code></pre></div><br>
<h2 id="说明">说明</h2>
<p>读者如需使用本项目词典，请引用如下参考文献：</p>
<blockquote>
<p>姚加权，冯绪，王赞钧，纪荣嵘，张维. 语调、情绪及市场影响：基于金融情绪词典. 管理科学学报，2021. 24(5), 26-46.</p>
</blockquote>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本相似 | Lazy Prices公司年报内容变动预示重大风险</title>
      <link>https://textdata.cn/blog/2019-12-08-lazy-prices/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-12-08-lazy-prices/</guid>
      <description>一个公司报告文件会有不同部分，我们需要将不同的部分分别识别出来。这里用到正则表达式，可以进行快速的数据清洗和数据抽取。文本转为向量后就可以进行相似度计算,</description>
      <content:encoded><![CDATA[<h2 id="文献">文献</h2>
<p>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</p>
<br>
<h2 id="摘要">摘要</h2>
<p>使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对<strong>报告进行积极更改</strong>时，这种行为<strong>蕴含着</strong>公司未来运营的<strong>重要信号</strong>。</p>
<p><strong>财务报告的语言和结构的变化也对公司的未来收益产生重大影响</strong>：做空&quot;变化&quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。</p>
<p>研究发现，对10-K的变化可以预测未来的收益、获利能力、未来的新闻公告，甚至未来的公司破产。同时，不做任何变化的公司将获得显著的异常收益。与资产价格典型的反应不足研究不同，我们发现没有任何与这些变化相关的公告效应–仅在后来通过新闻，事件或收益披露信息时才产生回报–暗示投资者并未注意到整个公众领域的这些变化。</p>
<br>
<h2 id="研究背景">研究背景</h2>
<p>之前的研究认为，尽管投资者一次对包含重大变化的财务报表的发布作出了迅时反应，但随着时间的流逝，这种公告作用是会减弱的(Brown and Tucker, 2011 and Feldman et al., 2010)。这表示10-K报告会随着时间推移，信息价值大打折扣。尽管我们复现了这个事实，即与常规文件的变更没有重大的公告效应，但我们认为，前人的研究忽略了更重要部分(如MD&amp;A)对对资产价格的影响。</p>
<p>确切的说，<strong>并不是报告的披露效应的信息价值变低了，而是投资者越来越难以发现报告中微妙的信息变化， 比如因为报告变得越来越冗杂。投资者只有看到某些新闻后，才会逐渐意识到之前公司报告内容变化的的真正价值</strong>。</p>
<p>例如Baxter公司</p>
<ul>
<li>纽约时报在 <strong>2010年4月23日</strong> 发了一条FDA将有对输液泵(infusion pumps)更严格对审批管理规定的新闻，<strong>新闻中提到了Baxter公司</strong>。新闻公布当天，<strong>Baxter股价大跌</strong>。</li>
<li><strong>10天</strong>后的（2010年5月4日），Baxter宣布<strong>召回问题的输液泵产品</strong>，股价当天再次大跌。</li>
</ul>
<p><img loading="lazy" src="img/lazy-prices-1.png" alt=""  />
</p>
<p>两次负面新闻导致Baxter股价大跌超过20%，最有意思的是Baxter公司一个多月前（<strong>2010年2月23日</strong>）10-k报告中 <strong>提到</strong> 了与这两条新闻类似的 <strong>线索</strong>。</p>
<p><img loading="lazy" src="img/clues_from_report.png" alt=""  />
</p>
<p>截图中写着 <strong>Baxter的产品COLLEGUE未来可能面脸额外的处罚，而且相关销售面临着FDA、OIG、DOI和FTC越来越严格的审批，面临的执法强度也越来越大</strong>。</p>
<p>因纽约时报发布的消息，股价大跌。但是大跌之前Baxter的10-k报告中似乎提示未来公司可能面临的风险，但是投资者怎么没有注意到这个重要线索呢？</p>
<br>
<h2 id="数据获取与分析方法">数据获取与分析方法</h2>
<p>这篇文章用到了很多 文本数据挖掘 方法，如</p>
<ul>
<li>数据采集(报告下载和信息监测)</li>
<li>正则表达式（数据分割与抽取）</li>
<li>文本相似度(计算报告变化程度)</li>
</ul>
<p>我大致说下这几部分技术在这篇论文中的应用。</p>
<h3 id="1-数据采集">1. 数据采集</h3>
<p>这篇论文研究者认为，只有投资者意识到本期报告和上一期报告做对比，才能发现报告变化，进而对股价有影响。所以当有新公告公布后，投资者是否下载本期报告的同时顺带着下载上一期报告，下载量又是多少。</p>
<p>下载量可以从Freedom of Information Act下载，</p>
<p><img loading="lazy" src="img/download_data.png" alt=""  />
</p>
<p>可以拿到的信息包括:</p>
<ul>
<li>报告文件</li>
<li>报告下载时间</li>
<li>报告下载的IP地址(可以通过这个ip来当作投资者的id)</li>
</ul>
<br>
<h3 id="2-正则表达式">2. 正则表达式</h3>
<p>一个公司报告文件会有不同部分，我们需要将不同的部分分别识别出来。这里用到正则表达式，可以进行快速的数据清洗和数据抽取。</p>
<p><img loading="lazy" src="img/regular_expression.png" alt=""  />
</p>
<br>
<h3 id="3-文本相似度">3. 文本相似度</h3>
<p>文本转为向量后就可以进行相似度计算,</p>
<p><img loading="lazy" src="img/similar-1.png" alt=""  />

<img loading="lazy" src="img/similar-2.png" alt=""  />

<img loading="lazy" src="img/similar-3.png" alt=""  />
</p>
<p>这里使用我开发的cntext包，可以实现cosine和jaccard相似度的计算。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">A</span> <span class="o">=</span> <span class="s1">&#39;We expect demand to increase.&#39;</span>
<span class="n">B</span> <span class="o">=</span> <span class="s1">&#39;We expect worldwide demand to increase.&#39;</span>
<span class="n">C</span> <span class="o">=</span> <span class="s1">&#39;We expect weakness in sales&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.40
0.83
</code></pre></div><p>如果对Baxter公司多个年度对报告进行相似度计算，绘制成图就会发现2010年与前后变化很大。相似度越低，说明公司报告前后变化很大，应该引起投资者注意，如果能注意到就会避免纽约时报导致到股价暴跌。如下图</p>
<p><img loading="lazy" src="img/lazy-prices-2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="案例实现">案例实现</h2>
<p>由于没有完全一样的数据，这里使用政府工作报告数据类比，使用cosine相似度画出趋势线条。</p>
<p>使用相似性识别变化的时间点</p>
<h3 id="准备数据">准备数据</h3>
<p>政府工作报告 <a href="http://www.gov.cn/guowuyuan/zfgzbg.htm">http://www.gov.cn/guowuyuan/zfgzbg.htm</a></p>
<p>prc_reports.xlsx 链接:https://pan.baidu.com/s/1sVU3mkEcP7Z3_hbG5AVNUA 密码:zjrq</p>
<p>将下载好后的 prc_reports.xlsx 文件放置于 .ipynb文件 所在的文件夹内。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;prc_reports.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<h3 id="计算相似度">计算相似度</h3>
<p>运行时间大概30s， 运算结果是列表数据 cosines</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">cosines</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">#row  Series</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">text1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;report&#39;</span><span class="p">]</span>
    <span class="n">text2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;report2&#39;</span><span class="p">]</span>
    <span class="n">simi</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">)</span>
    <span class="n">cosines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">simi</span><span class="p">)</span>
    
<span class="n">cosines</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[0.44&#39;, &#39;0.39&#39;, &#39;0.35&#39;, ... &#39;0.62&#39;, &#39;0.61&#39;, &#39;0.60&#39;]
</code></pre></div><h3 id="绘制柱状图">绘制柱状图</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">Bar</span>
<span class="kn">from</span> <span class="nn">pyecharts</span> <span class="kn">import</span> <span class="n">options</span> <span class="k">as</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>

<span class="n">bar</span> <span class="o">=</span> <span class="n">Bar</span><span class="p">()</span>

<span class="n">bar</span><span class="o">.</span><span class="n">add_xaxis</span><span class="p">(</span><span class="n">xaxis_data</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">])</span>
<span class="n">bar</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s2">&#34;相似度&#34;</span><span class="p">,</span> 
               <span class="n">cosines</span><span class="p">,</span> 
               <span class="n">label_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">LabelOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="n">bar</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span><span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;政府工作报告相似度可视化&#34;</span><span class="p">))</span>
<span class="n">bar</span><span class="o">.</span><span class="n">load_javascript</span><span class="p">()</span>

<span class="n">bar</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;政府工作报告相似度可视化1.html&#39;</span><span class="p">)</span>
<span class="n">bar</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/vis_res.png" alt=""  />
</p>
<h3 id="解读">解读</h3>
<p>从图中可以看到除1959年异常外，其他方面能挖掘出很多信息。从相似度整体趋势，</p>
<p>1959-1992 第一阶段，
1992-至今 第二阶段</p>
<p>1992年附近，第一次确立社会主义市场经济制度。之后的岁月里一直围绕着经济建设高速发展。</p>
<p>同时也可以看出在第一阶段前期相似度异常的低，可以理解为新中国初建，百废待兴，对于建设者而言，组着和管理这个国家的政府也在学习如何建设新中国。而90年代后，相似度越来越高，体现了政府越来越熟悉如何治理国家，如何搞经济建设。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>当cnsenti遇上streamlit</title>
      <link>https://textdata.cn/blog/cnsenti_streamlit/</link>
      <pubDate>Thu, 07 Jun 2018 10:40:10 +0600</pubDate>
      
      <guid>/blog/cnsenti_streamlit/</guid>
      <description>streamlit是web包，cnsenti是文本分析包，两者结合即可制造在线文本分析网站。</description>
      <content:encoded><![CDATA[<h1 id="cnsentidemo">cnsentiDemo</h1>
<p>这是使用streamlit库将中文情感分析[<strong>cnsenti</strong> 部署到网络世界，可<strong>在线提供简单的中文文本的情绪及情感计算</strong>。</p>
<p><strong>streamlit库</strong>(<a href="https://docs.streamlit.io/en/stable/">https://docs.streamlit.io/en/stable/</a>)， 是目前简单易用的数据可视化web框架，比flask和django少了很多的扩展性，但是容易学习上手，适合初学者把玩。</p>
<iframe
    src="//player.bilibili.com/player.html?bvid=bv17V411H7sZ&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<p><a href="https://cnsenti.herokuapp.com/"><strong>Demo</strong></a>
<img loading="lazy" src="img/%e6%95%88%e6%9e%9c%e5%9b%be.png" alt=""  />
</p>
<p><br><br></p>
<h1 id="网站">网站</h1>
<p>现在技术有限，该网站大致内容分为三部分</p>
<ul>
<li>准备数据</li>
<li>数据分析
<ul>
<li>情感分析</li>
<li>词云图</li>
</ul>
</li>
<li>谢谢支持</li>
</ul>
<p><br><br></p>
<h1 id="本地使用">本地使用</h1>
<p>本网站的<strong>cnsentiDemo项目文件夹</strong>的文件有</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- main.py
- cnsenti_example.csv
- 大邓和他的Python.png
- requirements.txt
- 其他文件
</code></pre></div><p>将cnsentiDemo项目下载，在<strong>电脑本地离线使用cnsenti的方法</strong></p>
<ol>
<li><a href="cnsentiDemo.zip">下载解压到桌面desktop</a></li>
<li>命令行, 执行 <code>cd desktop/cnsentiDemo</code></li>
<li>命令行，执行 <code>pip3 install -r requirements.txt</code></li>
<li>命令行, 执行 <code>streamlit run main.py</code></li>
<li>根据命令行的提示，复制粘贴网址到桌面。我这里是 <code>**http://localhost:8501**</code></li>
<li>浏览器打开效果就会与视频等同</li>
</ol>
<p>上述过程中，Mac和Win会有一些缺点导致无法使用，需要根据命令行提示解决各自系统的小问题，例如</p>
<ol>
<li>Win需要使用64位的Python</li>
<li>Mac可能需要安装Xcode-install</li>
<li>其他可能的问题</li>
</ol>
<p><br><br></p>
<h1 id="web部署方法">Web部署方法</h1>
<p>如果想将自己的streamlit项目部署成网站，可以使用Heroku和github帮助你完成人生第一个小网站。操作方法：</p>
<ol>
<li>将写好的streamlit项目上传至github自有仓库</li>
<li>Heroku注册账号</li>
<li>点击Heroku网页右上角New， 选择Create new app</li>
<li>绑定github，连接github里的streamlit项目</li>
<li>部署</li>
</ol>
<p>部署方法也可参考  <a href="https://www.youtube.com/watch?v=zK4Ch6e1zq8&amp;list=PLtqF5YXg7GLmCvTswG32NqQypOuYkPRUE&amp;index=5">Youtube视频</a></p>
<br>
<br>
<h1 id="广而告之">广而告之</h1>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用scipy实现层次聚类分析</title>
      <link>https://textdata.cn/blog/hierarchy_dendrogram_tutorial/</link>
      <pubDate>Fri, 18 May 2018 10:40:10 +0600</pubDate>
      
      <guid>/blog/hierarchy_dendrogram_tutorial/</guid>
      <description>使用scipy实现层次聚类分析</description>
      <content:encoded><![CDATA[<h2 id="代码下载">代码下载</h2>
<p><a href="hierarchy_dendrogram_code.zip"><strong>click to download</strong></a></p>
<h2 id="实验目的">实验目的</h2>
<p>如果您以前从未使用过树状图，那么使用树状图是查看多维数据如何聚集在一起的好方法。 在这本笔记本中，我将简单探索通过层次分析，借助树状图将其可视化。</p>
<br>
<h2 id="层次分析">层次分析</h2>
<p>层次分析是聚类分析的一种，scipy有这方面的封装包。</p>
<p>linkage函数从字面意思是链接，层次分析就是不断链接的过程，最终从n条数据，经过不断链接，最终聚合成一类，算法就此停止。</p>
<p>dendrogram是用来绘制树形图的函数。</p>
<br>
<h2 id="实验数据">实验数据</h2>
<p>grain_variety是标签，其他列为多种属性的值（特征）。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">dendrogram</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">seeds_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;seeds-less-rows.csv&#39;</span><span class="p">)</span>
<span class="n">seeds_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>area</th>
      <th>perimeter</th>
      <th>compactness</th>
      <th>length</th>
      <th>width</th>
      <th>asymmetry_coefficient</th>
      <th>groove_length</th>
      <th>grain_variety</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>14.88</td>
      <td>14.57</td>
      <td>0.8811</td>
      <td>5.554</td>
      <td>3.333</td>
      <td>1.018</td>
      <td>4.956</td>
      <td>Kama wheat</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14.69</td>
      <td>14.49</td>
      <td>0.8799</td>
      <td>5.563</td>
      <td>3.259</td>
      <td>3.586</td>
      <td>5.219</td>
      <td>Kama wheat</td>
    </tr>
    <tr>
      <th>2</th>
      <td>14.03</td>
      <td>14.16</td>
      <td>0.8796</td>
      <td>5.438</td>
      <td>3.201</td>
      <td>1.717</td>
      <td>5.001</td>
      <td>Kama wheat</td>
    </tr>
    <tr>
      <th>3</th>
      <td>19.31</td>
      <td>16.59</td>
      <td>0.8815</td>
      <td>6.341</td>
      <td>3.810</td>
      <td>3.477</td>
      <td>6.238</td>
      <td>Rosa wheat</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.99</td>
      <td>15.86</td>
      <td>0.8992</td>
      <td>5.890</td>
      <td>3.694</td>
      <td>2.068</td>
      <td>5.837</td>
      <td>Rosa wheat</td>
    </tr>
  </tbody>
</table>
</div>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#移除文本数据列</span>
<span class="n">varieties</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">seeds_df</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;grain_variety&#39;</span><span class="p">))</span>
<span class="n">varieties</span>
</code></pre></div><pre><code>['Kama wheat',
 'Kama wheat',
 'Kama wheat',
 'Rosa wheat',
 'Rosa wheat',
 'Rosa wheat',
 'Rosa wheat',
 'Rosa wheat',
 'Canadian wheat',
 'Canadian wheat',
 'Canadian wheat',
 'Canadian wheat',
 'Canadian wheat',
 'Canadian wheat']
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">samples</span> <span class="o">=</span> <span class="n">seeds_df</span><span class="o">.</span><span class="n">values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;samples的维度&#39;</span><span class="p">,</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div><pre><code>[[14.88   14.57    0.8811  5.554   3.333   1.018   4.956 ]
 [14.69   14.49    0.8799  5.563   3.259   3.586   5.219 ]
 [14.03   14.16    0.8796  5.438   3.201   1.717   5.001 ]
 [19.31   16.59    0.8815  6.341   3.81    3.477   6.238 ]
 [17.99   15.86    0.8992  5.89    3.694   2.068   5.837 ]
 [18.85   16.17    0.9056  6.152   3.806   2.843   6.2   ]
 [19.38   16.72    0.8716  6.303   3.791   3.678   5.965 ]
 [17.36   15.76    0.8785  6.145   3.574   3.526   5.971 ]
 [13.32   13.94    0.8613  5.541   3.073   7.035   5.44  ]
 [11.43   13.13    0.8335  5.176   2.719   2.221   5.132 ]
 [11.26   13.01    0.8355  5.186   2.71    5.335   5.092 ]
 [12.46   13.41    0.8706  5.236   3.017   4.987   5.147 ]
 [11.81   13.45    0.8198  5.413   2.716   4.898   5.352 ]
 [11.23   12.88    0.8511  5.14    2.795   4.325   5.003 ]]
samples的维度 (14, 7)
</code></pre>
<h3 id="使用linkage函数对samples进行层次聚类">使用linkage函数对samples进行层次聚类</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">X = linkage(y, method=&#39;single&#39;, metric=&#39;euclidean&#39;) 
</code></pre></div><p>sacipy中y是距离矩阵，我对此只是傻傻的理解成特征矩阵。 (m*n) m行代表m条记录,n代表n个特征</p>
<p>返回结果X是(m-1)*4的矩阵。 具体含义请看下面的案例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mergings</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1">#我们发现mergings比samples少一行</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sample维度&#39;</span><span class="p">,</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;mergings维度&#39;</span><span class="p">,</span><span class="n">mergings</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div><pre><code>sample维度 (14, 7)
mergings维度 (13, 4)
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#层次分析可视化，leaf的字体不旋转，大小为10。</span>
<span class="c1">#这里我们不显示每一条数据的具体名字标签（varieties），默认以数字标签显示</span>
<span class="n">dendrogram</span><span class="p">(</span><span class="n">mergings</span><span class="p">,</span><span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#在图中显示的数字是最细粒度的叶子，相当于每个样本数据点。</span>
</code></pre></div><p><img loading="lazy" src="output_7_0.png" alt="png"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">mergings</span>
</code></pre></div><pre><code>array([[ 3.        ,  6.        ,  0.37233454,  2.        ],
       [11.        , 12.        ,  0.77366442,  2.        ],
       [10.        , 15.        ,  0.89804259,  3.        ],
       [ 5.        , 14.        ,  0.90978998,  3.        ],
       [13.        , 16.        ,  1.02732924,  4.        ],
       [ 0.        ,  2.        ,  1.18832161,  2.        ],
       [ 4.        , 17.        ,  1.28425969,  4.        ],
       [ 7.        , 20.        ,  1.62187345,  5.        ],
       [ 1.        , 19.        ,  2.02587613,  3.        ],
       [ 9.        , 18.        ,  2.13385537,  5.        ],
       [ 8.        , 23.        ,  2.323123  ,  6.        ],
       [22.        , 24.        ,  2.87625877,  9.        ],
       [21.        , 25.        ,  3.12231564, 14.        ]])
</code></pre>
<p>层次分析图从上到下看，依次是枝和叶。</p>
<p>第一列和第二列代表类标签，包含叶子和枝子。</p>
<p>第三列代表叶叶（或叶枝，枝枝）之间的距离</p>
<p>第四列代表该层次类中含有的样本数（记录数）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">X = linkage(y, method=&#39;single&#39;, metric=&#39;euclidean&#39;) 
</code></pre></div><p>method是指计算类间距离的方法,比较常用的有3种:</p>
<p>(1)single:最近邻,把类与类间距离最近的作为类间距</p>
<p>(2)average:平均距离,类与类间所有pairs距离的平均</p>
<p>(3)complete:最远邻,把类与类间距离最远的作为类间距</p>
<p>我们写曾侧分析法函数，看看不同的method从图中有什么区别</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">hierarchy_analysis</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">):</span>
    <span class="n">mergings</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>

    <span class="n">dendrogram</span><span class="p">(</span><span class="n">mergings</span><span class="p">,</span>
              <span class="n">labels</span><span class="o">=</span><span class="n">varieties</span><span class="p">,</span>
              <span class="n">leaf_rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span>
              <span class="n">leaf_font_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#single</span>
<span class="n">hierarchy_analysis</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;single&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="output_12_0.png" alt="png"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#average</span>
<span class="n">hierarchy_analysis</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;average&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="output_13_0.png" alt="png"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#complete</span>
<span class="n">hierarchy_analysis</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;complete&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="output_14_0.png" alt="png"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">由于数据量比较少</span><span class="err">，</span><span class="n">complete和average方法做出来的图完全一样</span><span class="err">。</span>
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>视频课程 | Python实证指标构建与文本分析</title>
      <link>https://textdata.cn/blog/management_python_course/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/blog/management_python_course/</guid>
      <description>在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！在前大数据时代，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但大数据时代，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题： 网络爬虫技术 解决 如何从网络世界中高效地 采集数据?文本分析技术 解决 如何从杂乱的文本数据中实证指标(情感、态度、刻板印象等)？In scientific research, data acquisition and analysis are the most important and also the most difficult two links! In the pre-big data era, experimental methods, questionnaires, interviews, or second-hand data were generally used to organize data into structured tabular data, and then use various econometric analysis methods to analyze these tabular data. However, in the era of big data, network data has become a potential treasure that scholars from all walks of life urgently need to discover. A large amount of business information and social information are stored in massive web pages in unstructured and heterogeneous data formats such as text. So for the humanities and social sciences professional researchers represented by economics and management, Python can help scholars solve two problems faced by using Web data for scientific research: Web crawler technology solves how to efficiently collect data from the Internet world? Text analysis How can technical solutions extract empirical indicators (sentiment, attitudes, stereotypes, etc.) from messy text data?</description>
      <content:encoded><![CDATA[<h2 id="python实证指标构建与文本分析httpsmqlchatcomwechatpagechannel-introchannelid2000015158133596"><a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">Python实证指标构建与文本分析</a></h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<br>
<h2 id="概览">概览</h2>
<h3 id="为何要学python">为何要学Python？</h3>
<p>在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！</p>
<p>在<strong>前大数据时代</strong>，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。但<strong>大数据时代</strong>，网络数据成为各方学者亟待挖掘的潜在宝藏，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：</p>
<ol>
<li><strong>网络爬虫技术</strong> 解决 如何从网络世界中高效地 <strong>采集数据</strong>？</li>
<li><strong>文本分析技术</strong> 解决 如何从杂乱的文本数据中<strong>实证指标(情感、态度、刻板印象等)</strong>？</li>
</ol>
<br>
<h3 id="发票事项">发票事项</h3>
<p>如需发票，请先加微信372335839， 咨询发票细节，再作购买</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 企业名称：哈尔滨所以然信息技术有限公司 
- 企业税号：91230109MABT7KBC4M 
- 银行账户:  6228400176412884160
- 开户行:   中国农业银行哈尔滨香坊支行
</code></pre></div><p><img loading="lazy" src="img/dadeng_wechat.jpg" alt=""  />
</p>
<br>
<h3 id="课程纲要">课程纲要</h3>
<ul>
<li><strong>课程目标：</strong> 掌握Python语法、网络爬虫、数据分析Pandas、文本分析、机器学习、词嵌入与认知</li>
<li><strong>核心知识点：</strong> 爬虫原理及应用、 非结构化文本数据挖掘的思路及方法、机器学习应用等</li>
<li><strong>环境配置:</strong>  本文使用Anaconda作为Python的软件安装包，注意安装过程中勾选<strong>Add Anaconda 3.x to PATH</strong></li>
<li><strong>课件资料：</strong> 本课程全部使用jupyter notebook文件作为课程课件</li>
</ul>
<br>
<h3 id="课程特色">课程特色</h3>
<ul>
<li><strong>接地气：</strong> 以经管学术需求为导向， 将Python分为语法篇、采集数据篇、文本分析篇、机器学习篇四大部分</li>
<li><strong>好理解：</strong> 知识点力求通俗易懂，少了晦涩的计算机术语，多了通俗易懂的使用场景和实战讲解</li>
<li><strong>上手快：</strong> 所有知识点均有可重复使用的代码块，犹如一块块的积木，课后您可以根据分析需要，快速搭建出自己的Python代码</li>
<li><strong>技术新</strong>： 最新词嵌入，可挖掘文本中的态度、偏见、刻板印象等。</li>
</ul>
<br>
<h2 id="经管-经典文本分析方法">经管-经典文本分析方法</h2>
<p>在这里我把技术细分为词频、词袋、w2v建词典、w2v认知变迁四个维度，这四大技术方法在本课程中均有体现。为了直观了解课程价值，这里附上9篇文献，大家可以购课前以做参考。</p>
<table>
<thead>
<tr>
<th>文献</th>
<th>定性</th>
<th>词频</th>
<th>词袋</th>
<th>W2V建词典</th>
<th>W2V认知变迁</th>
</tr>
</thead>
<tbody>
<tr>
<td>王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</td>
<td>Y</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/jcr_concreteness_computation/">语言具体性如何影响顾客满意度</a><br>Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” <em>Journal of Consumer Research</em> 47, no. 5 (2021): 787-806.</td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2019-12-08-lazy-prices/">文本相似度</a><br>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>胡楠, 薛付婧 and 王昊楠, 2021. <a href="https://textdata.cn/blog/text_mining_in_2021_management_world/">管理者短视主义</a>影响企业长期投资吗———基于文本分析和机器学习. <em>管理世界</em>, <em>37</em>(5), pp.139-156.</td>
<td></td>
<td>Y</td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>Kai Li, Feng Mai, Rui Shen, Xinyan Yan, <a href="https://github.com/MS20190155/Measuring-Corporate-Culture-Using-Machine-Learning">Measuring Corporate Culture Using Machine Learning</a>, The Review of Financial Studies, 2020</td>
<td></td>
<td></td>
<td>Y</td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>女性就职高管改变组织内性别偏见<br>Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 119, no. 9 (2022): e2026443119.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
<tr>
<td>使用词嵌入技术，量化近百年以来性别和族群的刻板印象<br>Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. &ldquo;Word embeddings quantify 100 years of gender and ethnic stereotypes.&rdquo; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/">利用词嵌入技术，通过计算团队的话语多样性衡量团队的认知多样性</a><br>Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &ldquo;Aligning differences: Discursive diversity and team performance.&rdquo; <em>Management Science</em> 68, no. 11 (2022): 8430-8448.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
</tbody>
</table>
<br>
<h2 id="一课件下载">一、课件下载</h2>
<ol>
<li>
<p>课程介绍</p>
</li>
<li>
<p>Win中的Anaconda软件配置</p>
</li>
<li>
<p>Mac中的Anaconda软件配置</p>
</li>
</ol>
<br>
<h2 id="二python语法入门">二、Python语法入门</h2>
<ol>
<li>Python跟英语一样是一门语言</li>
<li>数据类型之字符串</li>
<li>数据类型之列表元组集合</li>
<li>数据类型之字典</li>
<li>数据类型之布尔值、None</li>
<li>逻辑语句(if&amp;for&amp;tryexcept)</li>
<li>列表推导式</li>
<li>理解函数</li>
<li>常用的内置函数</li>
<li>内置库文件路径pathlib库</li>
<li>内置库csv文件库</li>
<li>内置库正则表达式re库</li>
<li>初学python常出错误汇总</li>
</ol>
<br>
<h2 id="三数据采集">三、数据采集</h2>
<ol>
<li>网络爬虫原理</li>
<li>网络访问requests库</li>
<li>网页解析pyquery库</li>
<li><strong>「案例」</strong> 豆瓣读书</li>
<li><strong>「案例」</strong> Boss直聘</li>
<li>如何解析json数据</li>
<li><strong>「案例」</strong> 豆瓣电影</li>
<li><strong>「案例」</strong> 京东商城</li>
<li><strong>「案例」</strong> 用爬虫下载文档及多媒体文件</li>
<li><strong>「案例」</strong> 上市公司定期报告pdf批量下载</li>
<li><strong>「案例」</strong> 上交所招股说明pdf批量下载</li>
<li><strong>「案例」</strong> 深交所招股说明pdf批量下载</li>
<li>爬虫知识点总结</li>
</ol>
<br>
<h2 id="四数据分析">四、数据分析</h2>
<ol>
<li>Pandas基础知识</li>
<li>数据去重与缺失值处理</li>
<li>合并数据</li>
<li>重塑数据</li>
<li>选取表中指定记录(行)</li>
<li>选取表中指定字段(列)</li>
<li>描述性统计</li>
<li>在表中创建新字段(列)</li>
<li>批操作apply与agg</li>
<li>透视表pivot_table</li>
<li>数据分组groupby</li>
<li>时间序列时间点创建</li>
<li>日期数据的dt属性</li>
<li>日期行索引操作(选取指定日期的数据)</li>
<li>时间序列date_range</li>
<li>时间序列重采样resample</li>
<li>时间序列时间窗口rolling</li>
<li><strong>「案例」</strong> Kaggle titanic数据集探索性分析</li>
<li><strong>「案例」</strong> Boss直聘Python岗位分析</li>
</ol>
<br>
<h2 id="五初识文本分析">五、初识文本分析</h2>
<ol>
<li>
<p>从编码/解码视角重新理解文本</p>
</li>
<li>
<p>读取不同格式文件中的数据</p>
</li>
<li>
<p>如何将多个年报整理到一个excel中</p>
</li>
<li>
<p><strong>「案例」</strong> 中文分词及数据清洗</p>
</li>
<li>
<p><strong>「案例」</strong> 词频统计&amp;词云图</p>
</li>
<li>
<p><strong>「案例」</strong> 共现法扩展情感词典(领域词典)</p>
</li>
<li>
<p><strong>「案例」</strong> 词向量word2vec扩展领域词典</p>
</li>
<li>
<p><strong>「案例」</strong> 中文情感分析(词典法)</p>
</li>
<li>
<p>cntext库 情感分析代码操作</p>
</li>
<li>
<p><strong>「案例」</strong> 对excel中的文本进行情感分析  91</p>
</li>
<li>
<p><strong>「案例」</strong>:  语言具体性与心理距离 | 以JCR2021论文为例</p>
</li>
<li>
<p><strong>「案例」</strong>: 使用LM金融词典对年报进行「语调分析」 | 2018管理世界</p>
</li>
<li>
<p><strong>「案例」</strong>:  使用md&amp;a数据测量企业数字化 | 管理世界、财经研究</p>
</li>
<li>
<p><strong>「案例」</strong>:  使用md&amp;a数据构建标准信息、信息含量  |  中国工业经济</p>
</li>
</ol>
<br>    
<h2 id="六机器学习与文本分析">六、机器学习与文本分析</h2>
<ol>
<li>了解机器学习ML</li>
<li>使用机器学习做文本分析的流程</li>
<li>scikit-learn机器学习库简介</li>
<li>文本特征抽取(特征工程)</li>
<li><strong>「案例」</strong> 在线评论文本分类</li>
<li>使用标注工具对数据进行标注</li>
<li><strong>「案例」</strong> 计算文本情感分析(有权重)</li>
<li><strong>「案例」</strong>  文本相似性计算</li>
<li><strong>「案例」</strong> 使用文本相似性识别变化(政策连续性)</li>
<li><strong>「案例」</strong> 央行货币政策报告文本相似度计算与可视化 | 金融研究</li>
<li><strong>「案例」</strong> Kmeans聚类算法</li>
<li><strong>「案例」</strong> LDA话题模型</li>
<li>使用机器学习从图片中提取文本信息</li>
</ol>
<br>
<h2 id="七词嵌入与认知">七、词嵌入与认知</h2>
<ol>
<li>词嵌入原理及应用概述</li>
<li><strong>「案例」</strong>  豆瓣影评-训练词向量&amp;使用词向量</li>
<li><strong>「案例」</strong>  使用词向量做话题建模</li>
<li><strong>「案例」</strong>  认知指标(态度、偏见等)的测量</li>
<li>总结-文本分析在社科(经管)领域中的应用</li>
</ol>
<p><br><br></p>
<h2 id="相关应用">相关应用</h2>
<p>参照两篇论文的摘要，可以通过场景化等的方式帮助我们迅速理解上面两个问题。摘要部分的加粗内容是论文用到的分析技术，在我们的课程中均有与之对应的知识点和代码。</p>
<p><strong>曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">该文汉化了LM金融词典，并使用LM中文词典进行语调分析。 课程已整理了 LM中英文词典 及 对应代码。
</code></pre></div><blockquote>
<p><strong>摘要</strong>: 基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在**「口是心非」** 的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,<strong>年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与  「年报语调」 相关</strong>。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。</p>
<p><strong>关键词：</strong> 年报; 语调管理; 内部人交易; 信息不对称;</p>
</blockquote>
<br>
<p><strong>王伟,陈伟,祝效国,王洪伟.众筹融资成功率与语言风格的说服性——基于Kickstarter的实证研究[J].管理世界,2016(05):81-98.</strong></p>
<blockquote>
<p><strong>摘要</strong>：众筹融资效果决定着众筹平台的兴衰。众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。</p>
<p>然后，<strong>借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。</strong></p>
<p>最后，建立语言说服风格对项目筹资影响的计量模型，并对 <strong>Kickstarter 平台上的 128345 个项目进行实证分析</strong>。总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。</p>
</blockquote>
<br>
<p><strong>胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.</strong></p>
<blockquote>
<p><strong>摘要</strong> : 在可持续发展战略导向下，秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基 石。然而，作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间 导向理论，提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系，并<strong>采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验</strong>。研究结果发现，<strong>年报 MD&amp;A 中披露的「短期视域」 语言</strong> 能够反映管理者内在的短视主义特质，管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时，管理者短视主义对这些长期投资的负向影响越易受到抑制。最终，管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析，对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。<strong>同时，本文将文本分析和机器学习方法引入管理者短视主义的研究，为未来该领域的研究提供了参考和借鉴。</strong></p>
<p><strong>关键词</strong>: 管理者短视; 长期投资; 文本分析; 机器学习</p>
</blockquote>
<br>
<h2 id="相关文献">相关文献</h2>
<p>在这里我把技术细分为词频、词袋、w2v建词典、w2v认知变迁四个维度，整理了经管7篇论文。大家可以阅读这9篇论文，掌握文本分析的应用场景。</p>
<table>
<thead>
<tr>
<th>文献</th>
<th>定性</th>
<th>词频</th>
<th>词袋</th>
<th>W2V建词典</th>
<th>W2V认知变迁</th>
</tr>
</thead>
<tbody>
<tr>
<td>王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</td>
<td>Y</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/jcr_concreteness_computation/">语言具体性如何影响顾客满意度</a><br>Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” <em>Journal of Consumer Research</em> 47, no. 5 (2021): 787-806.</td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2019-12-08-lazy-prices/">文本相似度</a><br>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-01-10-similarity-of-cental-bank-monetary-policy/">文本相似度</a><br>姜富伟,胡逸驰,黄楠.央行货币政策报告文本信息、宏观经济与股票市场[J].金融研究,2021,(06):95-113.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>胡楠, 薛付婧 and 王昊楠, 2021. <a href="https://textdata.cn/blog/text_mining_in_2021_management_world/">管理者短视主义</a>影响企业长期投资吗———基于文本分析和机器学习. <em>管理世界</em>, <em>37</em>(5), pp.139-156.</td>
<td></td>
<td>Y</td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>Kai Li, Feng Mai, Rui Shen, Xinyan Yan, <a href="https://github.com/MS20190155/Measuring-Corporate-Culture-Using-Machine-Learning">Measuring Corporate Culture Using Machine Learning</a>, The Review of Financial Studies, 2020</td>
<td></td>
<td></td>
<td>Y</td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>女性就职高管改变组织内性别偏见<br>Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 119, no. 9 (2022): e2026443119.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
<tr>
<td>使用词嵌入技术，量化近百年以来性别和族群的刻板印象<br>Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. &ldquo;Word embeddings quantify 100 years of gender and ethnic stereotypes.&rdquo; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/">利用词嵌入技术，通过计算团队的话语多样性衡量团队的认知多样性</a><br>Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &ldquo;Aligning differences: Discursive diversity and team performance.&rdquo; <em>Management Science</em> 68, no. 11 (2022): 8430-8448.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216.
[1]洪永淼,刘俸奇,薛涧坡.政府与市场心理因素的经济影响及其测度[J].管理世界,2023,39(03):30-51.
[2]沈艳, 陈赟, &amp; 黄卓. (2019). 文本大数据分析在经济学和金融学中的应用: 一个文献综述. 经济学 (季刊), 18(4), 1153-1186.
[3]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.
[4]张楠,黄梅银,罗亚,马宝君.全国政府网站内容数据中的知识发现：从注意力分配到政策层级扩散[J].管理科学学报,2023,26(05):154-173.
[5]许帅,邵帅,何贤杰.业绩说明会前瞻性信息对分析师盈余预测准确性的影响——信口雌黄还是言而有征[J].中国管理科学:1-15.
[6]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.管理世界.2016;5:81-98.
[7]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[8]孟庆斌, 杨俊华, 鲁冰. 管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J]. 中国工业经济, 2017 (12): 132-150.
[9]曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.
[10]彭红枫, &amp; 林川. (2018). 言之有物: 网络借贷中语言有用吗?——来自人人贷借款描述的经验证据[J]. 金融研究, 461(11), 133-153.
[11]吴非, 胡慧芷, 林慧妍, and 任晓怡. “企业数字化转型与资本市场表现——来自股票流动性的经验证据[J].” 管理世界 (2021).
[12]姜富伟,胡逸驰,黄楠.央行货币政策报告文本信息、宏观经济与股票市场[J].金融研究,2021,(06):95-113.
[13]陈霄,叶德珠,邓洁.借款描述的可读性能够提高网络借款成功率吗[J].中国工业经济,2018,(03):174-192.
[14]罗勇根,饶品贵,陈灿.高管宏观认知具有管理者“烙印”吗?——基于管理者风格效应的实证检验[J].金融研究,2021(05):171-188.
[15]吴胜涛,茅云云,吴舒涵,冯健仁,张庆鹏,谢天,陈浩,朱廷劭.基于大数据的文化心理分析[J].心理科学进展:1-13.
[16]Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &#34;Aligning differences: Discursive diversity and team performance.&#34; *Management Science* 68, no. 11 (2022): 8430-8448.
[17]Rocklage, Matthew D., Sharlene He, Derek D. Rucker, and Loran F. Nordgren. &#34;Beyond Sentiment: The Value and Measurement of Consumer Certainty in Language.&#34; Journal of Marketing Research (2023): 00222437221134802.
[18]Wang, Quan, Beibei Li, and Param Vir Singh. &#34;Copycats vs. Original Mobile Apps: A Machine Learning Copycat-Detection Method and Empirical Analysis.&#34; *Information Systems Research* 29.2 (2018): 273-291.
[19]Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” _Journal of Consumer Research_ 47, no. 5 (2021): 787-806.
[20]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
[21]Loughran T, McDonald B. Textual analysis in accounting and finance: A survey[J]. *Journal of Accounting Research*, 2016, 54(4): 1187-1230. Author links open overlay panelComputational socioeconomics
[22]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &#34;Uniting the tribes: Using text for marketing insight.&#34; *Journal of Marketing* 84, no. 1 (2020): 1-25.
[23]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. &#34;Lazy prices.&#34; *The Journal of Finance* 75, no. 3 (2020): 1371-1415.
[24]Bellstam, Gustaf, Sanjai Bhagat, and J. Anthony Cookson. &#34;A text-based analysis of corporate innovation.&#34; _Management Science_ 67, no. 7 (2021): 4004-4031.
[25]Arts, Sam, Bruno Cassiman, and Jianan Hou. &#34;Position and Differentiation of Firms in Technology Space.&#34; Management Science (2023).
[26]Cookson, J. Anthony, and Marina Niessner. &#34;Why don&#39;t we agree? Evidence from a social network of investors.&#34; The Journal of Finance 75, no. 1 (2020): 173-228.
[27]Mansouri S, Momtaz P P. Financing sustainable entrepreneurship: ESG measurement, valuation, and performance[J]. Journal of Business Venturing, 2022, 37(6):106258.

</code></pre></div>]]></content:encoded>
    </item>
    
  </channel>
</rss>
