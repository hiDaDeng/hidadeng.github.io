<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Bert on 大邓和他的PYTHON</title>
    <link>/tags/bert/</link>
    <description>Recent content in Bert on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sun, 07 Nov 2021 16:40:10 +0600</lastBuildDate><atom:link href="/tags/bert/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hugging Face | 自然语言处理平台</title>
      <link>https://hidadeng.github.io/blog/huggingface_test/</link>
      <pubDate>Sun, 07 Nov 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/huggingface_test/</guid>
      <description>十行以内代码实现任意NLP功能</description>
      <content:encoded><![CDATA[<p>Huggingface（抱抱脸）总部位于纽约，是一家专注于自然语言处理、人工智能和分布式系统的创业公司。他们所提供的聊天机器人技术一直颇受欢迎，但更出名的是他们在NLP开源社区上的贡献。</p>
<p>Huggingface一直致力于自然语言处理NLP技术的平民化(democratize)，希望每个人都能用上最先进(SOTA, state-of-the-art)的NLP技术，而非困窘于训练资源的匮乏。</p>
<p><strong>Hugging Face所有模型的地址</strong></p>
<p><a href="https://huggingface.co/models">https://huggingface.co/models</a></p>
<p>你可以在这里下载所需要的模型，也可以上传你微调之后用于特定task的模型。</p>
<br>
<p><strong>Hugging Face使用文档的地址</strong></p>
<p><a href="https://huggingface.co/transformers/master/index.html">https://huggingface.co/transformers/master/index.html</a></p>
<p><br><br></p>
<h2 id="英汉互译">英汉互译</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">zh2en_model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;Helsinki-NLP/opus-mt-zh-en&#39;</span><span class="p">)</span>
<span class="n">zh2en_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;Helsinki-NLP/opus-mt-zh-en&#39;</span><span class="p">)</span>
<span class="n">zh2en_translation</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;translation_zh_to_en&#39;</span><span class="p">,</span> 
                       <span class="n">model</span><span class="o">=</span><span class="n">zh2en_model</span><span class="p">,</span> 
                       <span class="n">tokenizer</span><span class="o">=</span><span class="n">zh2en_tokenizer</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">zh2en_translation</span><span class="p">(</span><span class="s1">&#39;Python是一门非常强大的编程语言!&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>[{'translation_text': 'Python is a very powerful programming language!'}]
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">en2zh_model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;Helsinki-NLP/opus-mt-en-zh&#39;</span><span class="p">)</span>
<span class="n">en2zh_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;Helsinki-NLP/opus-mt-en-zh&#39;</span><span class="p">)</span>

<span class="n">en2zh_translation</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;translation_en_to_zh&#39;</span><span class="p">,</span> 
                       <span class="n">model</span><span class="o">=</span><span class="n">en2zh_model</span><span class="p">,</span> 
                       <span class="n">tokenizer</span><span class="o">=</span><span class="n">en2zh_tokenizer</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">en2zh_translation</span><span class="p">(</span><span class="s1">&#39;Python is a very powerful programming language!&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>[{'translation_text': 'Python是一个非常强大的编程语言!'}]
</code></pre>
<p><br><br></p>
<h2 id="文本分类">文本分类</h2>
<p>模型 <strong>uer/roberta-base-finetuned-chinanews-chinese</strong>是使用5个中文文本分类数据集训练得到</p>
<ul>
<li>京东full、京东binary和大众点评数据集包含不同情感极性的用户评论数据。</li>
<li>凤凰网 和 China Daily 包含不同主题类的新闻文本数据</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;uer/roberta-base-finetuned-chinanews-chinese&#39;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;uer/roberta-base-finetuned-chinanews-chinese&#39;</span><span class="p">)</span>
<span class="n">text_classification</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;sentiment-analysis&#39;</span><span class="p">,</span> 
                               <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> 
                               <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">test_text</span> <span class="o">=</span> <span class="s2">&#34;上证指数大涨2%&#34;</span>

<span class="n">text_classification</span><span class="p">(</span><span class="n">test_text</span><span class="p">,</span> <span class="n">return_all_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><pre><code>[[{'label': 'mainland China politics', 'score': 0.0002807585697155446},
  {'label': 'Hong Kong - Macau politics', 'score': 0.00015504546172451228},
  {'label': 'International news', 'score': 6.818029214628041e-05},
  {'label': 'financial news', 'score': 0.9991051554679871},
  {'label': 'culture', 'score': 0.00011297615128569305},
  {'label': 'entertainment', 'score': 0.00012184812658233568},
  {'label': 'sports', 'score': 0.0001558474759804085}]]
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">test_text</span> <span class="o">=</span> <span class="s2">&#34;Python是一门强大的编程语言&#34;</span>
<span class="n">text_classification</span><span class="p">(</span><span class="n">test_text</span><span class="p">,</span> <span class="n">return_all_scores</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><pre><code>[[{'label': 'mainland China politics', 'score': 0.02050291746854782},
  {'label': 'Hong Kong - Macau politics', 'score': 0.0030984438490122557},
  {'label': 'International news', 'score': 0.005687597207725048},
  {'label': 'financial news', 'score': 0.03360358253121376},
  {'label': 'culture', 'score': 0.913349986076355},
  {'label': 'entertainment', 'score': 0.010810119099915028},
  {'label': 'sports', 'score': 0.012947351671755314}]]
</code></pre>
<p><br><br></p>
<h2 id="代码下载">代码下载</h2>
<p><a href="https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211108HuggingFace%E5%AD%A6%E4%B9%A0">https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211108HuggingFace学习</a></p>
<br>
<h2 id="了解课程">了解课程</h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://hidadeng.github.io/blog/management_python_course/">点击进入详情页</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KeyBERT | 关键词发现</title>
      <link>https://hidadeng.github.io/blog/keybert/</link>
      <pubDate>Wed, 27 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/keybert/</guid>
      <description>使用 BERT 嵌入 和 简单余弦相似度 来查找文档中与文档本身最相似的短语，自动挖掘文本中的关键词</description>
      <content:encoded><![CDATA[<p>尽管已经有很多方法可用于关键字生成（例如，Rake、YAKE!、TF-IDF 等），但我想创建一个非常基本但功能强大的方法来提取关键字和关键短语。这就是 KeyBERT 的用武之地！它使用 <strong>BERT 嵌入</strong> 和 <strong>简单余弦相似度</strong> 来查找文档中与文档本身最相似的短语。</p>
<p>KeyBERT步骤</p>
<ol>
<li>首先使用 BERT 提取文档嵌入以获得<strong>文档级向量表示</strong>。</li>
<li>随后，为 N-gram 词/短语提取<strong>词向量</strong>。</li>
<li>然后，我们使用余弦相似度来找到与文档最相似的单词/短语。</li>
<li>最后可以将最相似的词识别为最能描述整个文档的词。</li>
</ol>
<h2 id="本文代码下载httpsgithubcomhidadengdadengandhispythontreemaster20211030keybert关键词提取"><a href="https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211030KeyBERT%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96">本文代码下载</a></h2>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">keybert</span><span class="o">==</span><span class="mf">0.5.0</span>
</code></pre></div><br>
<h2 id="初始化模型">初始化模型</h2>
<p>KeyBERT库需要安装配置spacy语言模型</p>
<p>具体参考<strong>公众号：大邓和他的Python</strong> 2021-10-29 的推文 查看spacy配置方法</p>
<p>初始化模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">import</span> <span class="nn">jieba</span>


<span class="n">zh_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;zh_core_web_sm&#34;</span><span class="p">)</span>
<span class="n">bertModel</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">zh_model</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="准备数据">准备数据</h2>
<p>中文测试数据需要先分词，而后构造成类英文的语言结构(用空格间隔的文本)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 测试数据</span>
<span class="n">doc</span> <span class="o">=</span>  <span class="s2">&#34;&#34;&#34;时值10月25日抗美援朝纪念日，《长津湖》片方发布了“纪念中国人民志愿军抗美援朝出国作战71周年特别短片”，再次向伟大的志愿军致敬！
</span><span class="s2">　　电影《长津湖》全情全景地还原了71年前抗美援朝战场上那场史诗战役，志愿军奋不顾身的英勇精神令观众感叹：“岁月峥嵘英雄不灭，丹心铁骨军魂永存！”影片上映以来票房屡创新高，目前突破53亿元，暂列中国影史票房总榜第三名。
</span><span class="s2">　　值得一提的是，这部影片的很多主创或有军人的血脉，或有当兵的经历，或者家人是军人。提起这些他们也充满自豪，影片总监制黄建新称：“当兵以后会有一种特别能坚持的劲儿。”饰演雷公的胡军透露：“我父亲曾经参加过抗美援朝，还得了一个三等功。”影片历史顾问王树增表示：“我当了五十多年的兵，我的老部队就是上甘岭上下来的，那些老兵都是我的偶像。”
</span><span class="s2">　　“身先士卒卫华夏家国，血战无畏护山河无恙。”片中饰演七连连长伍千里的吴京感叹：“要永远记住这些先烈们，他们给我们带来今天的和平。感谢他们的付出，才让我们有今天的幸福生活。”饰演新兵伍万里的易烊千玺表示：“战争的残酷、碾压式的伤害，其实我们现在的年轻人几乎很难能体会到，希望大家看完电影后能明白，是那些先辈们的牺牲奉献，换来了我们的现在。”
</span><span class="s2">　　影片对战争群像的恢弘呈现，对个体命运的深切关怀，令许多观众无法控制自己的眼泪，观众称：“当看到影片中的惊险战斗场面，看到英雄们壮怀激烈的拼杀，为国捐躯的英勇无畏和无悔付出，我明白了为什么说今天的幸福生活来之不易。”（记者 王金跃）
</span><span class="s2">        &#34;&#34;&#34;</span>


<span class="n">doc</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>


<span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">keywords</span>
</code></pre></div><pre><code>[('铁骨', 0.5028),
 ('纪念日', 0.495),
 ('丹心', 0.4894),
 ('战役', 0.4869),
 ('影史', 0.473),
 ('父亲', 0.4576),
 ('票房', 0.4571),
 ('偶像', 0.4497),
 ('精神', 0.4436),
 ('家国', 0.4373)]
</code></pre>
<br>
<h2 id="常用extract_keywords参数">常用extract_keywords参数</h2>
<p><strong>bertModel.extract_keywords(docs, keyphrase_ngram_range, stop_words, top_n)</strong></p>
<ul>
<li><strong>docs</strong> 文档字符串（空格间隔词语的字符串）</li>
<li><strong>keyphrase_ngram_range</strong> 设置ngram，默认(1, 1)</li>
<li><strong>stop_words</strong> 停用词列表</li>
<li><strong>top_n</strong> 显示前n个关键词，默认5</li>
<li><strong>highlight</strong> 可视化标亮关键词，默认False</li>
<li>use_maxsum: 默认False;是否使用Max Sum Similarity作为关键词提取标准，</li>
<li>use_mmr: 默认False;是否使用Maximal Marginal Relevance (MMR) 作为关键词提取标准</li>
<li>diversity 如果use_mmr=True，可以设置该参数。参数取值范围从0到1</li>
</ul>
<br>
<p>对于<strong>keyphrase_ngram_range</strong>参数，</p>
<ul>
<li>(1, 1) 只单个词， 如&quot;抗美援朝&quot;, &ldquo;纪念日&quot;是孤立的两个词</li>
<li>(2, 2) 考虑词组， 如出现有意义的词组 &ldquo;抗美援朝 纪念日&rdquo;</li>
<li>(1, 2) 同时考虑以上两者情况</li>
</ul>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">diversity</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> 
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">keywords</span>
</code></pre></div><pre><code>[('影片 总监制', 0.5412),
 ('丹心 铁骨', 0.5339),
 ('抗美援朝 纪念日', 0.5295),
 ('长津湖 片方', 0.5252),
 ('志愿军 致敬', 0.5207),
 ('老兵 偶像', 0.5192),
 ('票房 创新', 0.5108),
 ('军人 血脉', 0.5084),
 ('家国 血战', 0.4946),
 ('家人 军人', 0.4885)]
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#可视化</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">highlight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/highlight.png" width="800" />
    
    
</figure>

<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">use_mmr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">diversity</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> 
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">keywords</span>
</code></pre></div><pre><code>[('影片 总监制', 0.5412),
 ('长津湖 片方', 0.5252),
 ('抗美援朝 纪念日', 0.5295),
 ('丹心 铁骨', 0.5339),
 ('志愿军 致敬', 0.5207),
 ('老兵 偶像', 0.5192),
 ('票房 创新', 0.5108),
 ('军人 血脉', 0.5084),
 ('家国 血战', 0.4946),
 ('家人 军人', 0.4885)]
</code></pre>
<br>
<h2 id="英文keybert">英文KeyBERT</h2>
<p>同样需要配置spacy，参考<strong>公众号：大邓和他的Python</strong> 2021-10-29 的推文 查看spacy配置方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">en_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;en_core_web_sm&#34;</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">         Supervised learning is the machine learning task of learning a function that
</span><span class="s2">         maps an input to an output based on example input-output pairs. It infers a
</span><span class="s2">         function from labeled training data consisting of a set of training examples.
</span><span class="s2">         In supervised learning, each example is a pair consisting of an input object
</span><span class="s2">         (typically a vector) and a desired output value (also called the supervisory signal). 
</span><span class="s2">         A supervised learning algorithm analyzes the training data and produces an inferred function, 
</span><span class="s2">         which can be used for mapping new examples. An optimal scenario will allow for the 
</span><span class="s2">         algorithm to correctly determine the class labels for unseen instances. This requires 
</span><span class="s2">         the learning algorithm to generalize from the training data to unseen situations in a 
</span><span class="s2">         &#39;reasonable&#39; way (see inductive bias).
</span><span class="s2">      &#34;&#34;&#34;</span>
<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">kw_model</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">keywords</span>
</code></pre></div><p>Run</p>
<pre><code>[('supervised learning', 0.6779),
 ('supervised', 0.6676),
 ('signal supervised', 0.6152),
 ('examples supervised', 0.6112),
 ('labeled training', 0.6013)]
</code></pre>
<br>
<h2 id="了解课程">了解课程</h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://hidadeng.github.io/blog/management_python_course/">点击进入详情页</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>BERTopic 主题建模库 | 建议收藏</title>
      <link>https://hidadeng.github.io/blog/bertopic/</link>
      <pubDate>Tue, 26 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/bertopic/</guid>
      <description>使用BERT主题建模技术,可以对经管等领域文本数据进行主题(话题)建模。效果堪比LDA，但比LDA智能</description>
      <content:encoded><![CDATA[<p>BERTopic 是一种主题建模技术，它利用 Transformer 和 c-TF-IDF 来创建密集的集群，允许轻松解释主题，同时在主题描述中保留重要词。</p>
<p>BERTopic亮点</p>
<ul>
<li>支持引导式Guided</li>
<li>支持（半）监督式</li>
<li>支持动态主题。</li>
<li>支持可视化</li>
</ul>
<h3 id="python37ok-3839有问题暂时无解">python3.7Ok， 3.8、3.9有问题,暂时无解。</h3>
<p><a href="https://github.com/MaartenGr/BERTopic/issues/392">https://github.com/MaartenGr/BERTopic/issues/392</a></p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">bertopic</span><span class="o">==</span><span class="mf">0.9.3</span>
</code></pre></div><h2 id="准备数据">准备数据</h2>
<p>这里使用的新闻数据集， 共2000条。 新闻类别涵 <code>'娱乐', '教育', '游戏', '财经', '时政', '时尚', '科技', '体育', '家居', '房产'</code>
这里假设大家不知道有10类新闻题材， 构建模型的时候不会用到label字段的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/cnews.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/df.png" width="800" />
    
    
</figure>

<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 新闻题材</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>

<span class="c1">#记录数</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
</code></pre></div><pre><code>['娱乐' '教育' '游戏' '财经' '时政' '时尚' '科技' '体育' '家居' '房产']
2000
</code></pre>
<p>这里定义了一个清洗数据函数clean_text，需要注意BERTopic需要先将中文分词改造成类似英文文本格式（用空格间隔词语）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">jieba</span>


<span class="n">stoptext</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/stopwords.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">stopwords</span> <span class="o">=</span> <span class="n">stoptext</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">test</span> <span class="o">=</span> <span class="s2">&#34;云南永善县级地震已致人伤间民房受损中新网月日电据云南昭通市防震减灾局官方网站消息截至日时云南昭通永善县级地震已造成人受伤其中重伤人轻伤人已全部送医院救治民房受损户间倒塌户间个乡镇所学校不同程度受损目前被损毁电力交通通讯设施已全部抢通修复当地已调拨帐篷顶紧急转移万人月日时分云南昭通永善县发生里氏级地震震源深度公里当地震感强烈此外成都等四川多地也有明显震感&#34;</span>
<span class="n">clean_text</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
</code></pre></div><pre><code>'云南 永善县 级 地震 已致 伤间 民房 受损 中新网 月 日电 云南 昭通市 防震 减灾 局 官方网站 消息 截至 日时 云南 昭通 永善县 级 地震 受伤 重伤 轻伤 送 医院 救治 民房 受损 户间 倒塌 户间 乡镇 学校 程度 受损 损毁 电力 交通 通讯 设施 抢通 修复 当地 调拨 帐篷 紧急 转移 万人 月 日 时分 云南 昭通 永善县 发生 里氏 级 地震 震源 深度 公里 当地 震感 成都 四川 多地 震感'
</code></pre>
<p>对2000条数据进行clean_text，得到的结果存储到content字段中。</p>
<p>我的macbook内存16G, 运行时间10s</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/df2.png" width="800" />
    
    
</figure>

<h2 id="训练topic模型">训练Topic模型</h2>
<p>文本分析步骤包括构建特征工程和训练，在本文中，直接使用spacy的中文词向量，省去了特征模型的学习时间。</p>
<p>但这里需要</p>
<ol>
<li>安装spacy</li>
<li>下载&amp;安装zh_core_web_sm中文词向量模型</li>
</ol>
<p>具体配置方法请看【公众号: 大邓和他的Python】2021年10月29日 的推文 <strong>建议收藏 | nltk和spacy配置方法</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">zh_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;zh_core_web_sm&#34;</span><span class="p">)</span>

<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="s2">&#34;chinese (simplified)&#34;</span><span class="p">,</span> 
                       <span class="n">embedding_model</span><span class="o">=</span><span class="n">zh_model</span><span class="p">,</span>
                       <span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                       <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">docs</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1">#2000条进行fit_transform需要1min</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div><pre><code>100%|██████████| 2000/2000 [01:31&lt;00:00, 21.91it/s]
2021-10-28 12:11:25,583 - BERTopic - Transformed documents to Embeddings
2021-10-28 12:11:34,582 - BERTopic - Reduced dimensionality with UMAP
2021-10-28 12:11:34,718 - BERTopic - Clustered UMAP embeddings with HDBSCAN


CPU times: user 1min 50s, sys: 7.7 s, total: 1min 57s
Wall time: 1min 43s
</code></pre>
<p><br><br></p>
<h2 id="主题模型方法">主题模型方法</h2>
<ul>
<li>topic_model.get_topic_info 查看各主题信息</li>
<li>topic_model.find_topics(term, top_n=5)  查找term最有可能所属话题</li>
<li>topic_model.get_topic(0) 查看Topic 0的特征词</li>
<li>topic_model.visualize_topics() 话题间距离的可视化</li>
<li>topic_model.visualize_distribution(probs[0]) 查看某条文本的主题分布</li>
<li>topic_model.visualize_hierarchy(top_n_topics=20) 主题层次聚类可视化</li>
<li>topic_model.visualize_barchart(top_n_topics=6) 主题词条形图可视化</li>
<li>topic_model.visualize_heatmap(n_clusters=10) 主题相似度热力图</li>
<li>topic_model.visualize_term_rank() 可视化词语</li>
<li>topic_model.save()  保存主题模型</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_info</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/get_topic_info.png" width="800" />
    
    
</figure>

<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&#34;美国&#34;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">similar_topics</span>
</code></pre></div><pre><code>[0, 3, 1, 2, -1]
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div><pre><code>[('中国', 0.017740927481291097),
 ('美国', 0.009187523853389844),
 ('国际', 0.007387509919710244),
 ('北京', 0.006355315208051378),
 ('台湾', 0.004591519972746738),
 ('上海', 0.00398117373168178),
 ('电影', 0.003959013801339396),
 ('文化', 0.003635760343311582),
 ('主持人 韩悦', 0.003598325963444241),
 ('全球', 0.0034710750361063997)]
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/intertopicdistance.gif" width="800" />
    
    
</figure>

<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">visualize_topics</span>  
</code></pre></div><p>显示第一条新闻的主题概率分布</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_distribution</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<figure >
    
        <img src="img/visualize_distribution.png" width="800" />
    
    
</figure>

<p>为了理解主题的潜在层次结构，我们可以使用 scipy.cluster.hierarchy 创建聚类并可视化它们之间的关系。 这有助于合并相似主题，达到降低主题模型主题数量nr_topics。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_hierarchy</span><span class="p">(</span><span class="n">top_n_topics</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/visualize_hierarchy.png" width="800" />
    
    
</figure>

<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_barchart</span><span class="p">(</span><span class="n">top_n_topics</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span> <span class="mi">800</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/visualize_barchart.png" />
    
    
</figure>

<p>BERTopic可将主题以embeddings形式（向量）表示， 因此我们可以应用余弦相似度来创建相似度矩阵。 每两两主题可进行余弦计算，最终结果将是一个矩阵，显示主题间的相似程度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_heatmap</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/visualize_heatmap.png" width="800" />
    
    
</figure>

<p>通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。 从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。 此外，可以轻松地将主题表示相互比较。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_term_rank</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/visualize_term_rank.png" width="800" />
    
    
</figure>

<h2 id="更新主题模型">更新主题模型</h2>
<p>当您训练了一个模型并查看了代表它们的主题和单词时，您可能对表示不满意。 也许您忘记删除停用词，或者您想尝试不同的 n_gram_range。 我们可以使用函数 update_topics 使用 c-TF-IDF 的新参数更新主题表示。</p>
<p>经过更新，topic_model得到了更新，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">update_topics</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&#34;儿童&#34;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">similar_topics</span>
</code></pre></div><pre><code>[11, 4, 7, -1, 2]
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>
</code></pre></div><pre><code>[('学生', 0.015023352605066086),
 ('儿童', 0.010260062682561771),
 ('投资', 0.00907917809925075),
 ('投资 移民', 0.008539711279754461),
 ('海外', 0.007267950590362874),
 ('学校', 0.006227402241189809),
 ('奖学金', 0.005431476690391167),
 ('留学人员', 0.00520544712332708),
 ('教师', 0.004945988616826368),
 ('联邦', 0.00465078395869278)]
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Save model</span>
<span class="c1">#model.save(&#34;my_model&#34;)</span>
<span class="c1"># Load model</span>
<span class="c1">#my_model = BERTopic.load(&#34;my_model&#34;)</span>
</code></pre></div><br>
<h2 id="压缩主题数">压缩主题数</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">reduce_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><pre><code>2021-10-28 12:28:01,976 - BERTopic - Reduced number of topics from 20 to 11
</code></pre>
<br>
<h2 id="下载代码数据">下载代码数据</h2>
<p><a href="https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211029BERTopic%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B">https://github.com/hidadeng/DaDengAndHisPython/tree/master/20211029BERTopic主题模型</a></p>
<br>
<h2 id="总结">总结</h2>
<p>本文使用中文文本数据展示BERTopic部分功能，如果对英文数据感兴趣，可以前往  <a href="https://github.com/MaartenGr/BERTopic">https://github.com/MaartenGr/BERTopic</a> 深入学习。</p>
<br>
<h2 id="了解课程">了解课程</h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://hidadeng.github.io/blog/management_python_course/">点击进入详情页</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Label-Studio|多媒体数据标注工具</title>
      <link>https://hidadeng.github.io/blog/label_studio_test/</link>
      <pubDate>Sun, 18 Jul 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/label_studio_test/</guid>
      <description>自然语言处理前需要先标注数据,label-studio让数据标注过程变得轻松简单</description>
      <content:encoded><![CDATA[<h2 id="1-简介">1. 简介</h2>
<h3 id="label-studiohttpsgithubcomheartexlabslabel-studio"><a href="https://github.com/heartexlabs/label-studio">label-studio</a></h3>
<p>假设我们想使用机器学习做文本分析，一般都需要先对数据进行标注，才能训练出效果比较好的监督机器学习模型。</p>
<p>label-studio是多媒体数据标注工具，可以很方便的进行标注和导出。</p>
<p>Label Studio 是一款开源数据标注工具，用于标注和探索多种类型的数据。 您可以使用多种数据格式执行的标记任务。</p>
<p>您还可以将 Label Studio 与机器学习模型集成，以提供标签（预标签）的预测，或执行持续的主动学习。</p>
<p>官方文档 <a href="https://labelstud.io/">https://labelstud.io/</a></p>
<br>
<h3 id="操作步骤">操作步骤</h3>
<ol>
<li>安装Label Studio</li>
<li>启动Label Studio</li>
<li>创建Label Studio账号</li>
<li>项目默认配置</li>
<li>导入数据</li>
<li>标注数据</li>
<li>结束标记，导出标注数据</li>
</ol>
<br>
<h3 id="安装">安装</h3>
<p>命令行中执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install label-studio==1.1.0
</code></pre></div><h2 id="2-快速上手">2 快速上手</h2>
<p>在桌面创建自动生成一个名为Project的项目文件夹。</p>
<ul>
<li>Win命令行执行</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">label-studio --data-dir Desktop/Project
</code></pre></div><ul>
<li>Mac命令行执行</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">label-studio --data-dir desktop/Project
</code></pre></div><br>
<br>
<p>执行上方代码大概10s左右，会在浏览器弹出如下界面</p>

<figure >
    
        <img src="img/%e5%88%9b%e5%bb%ba%e8%b4%a6%e6%88%b7.png" width="800" />
    
    
</figure>

<p>注册好账号密码，点击<strong>Create Project</strong></p>

<figure >
    
        <img src="img/%e5%88%9b%e5%bb%ba%e9%a1%b9%e7%9b%ae.png" width="800" />
    
    
</figure>

<br>
<p>项目描述填写好，点击按钮**Data Import **，</p>

<figure >
    
        <img src="img/%e8%ae%be%e7%bd%ae%e9%a1%b9%e7%9b%ae%e6%8f%8f%e8%bf%b0.png" width="800" />
    
    
</figure>

<p>这里我们要做文本分析，导入csv</p>

<figure >
    
        <img src="img/%e5%af%bc%e5%85%a5%e6%95%b0%e6%8d%ae1.png" width="800" />
    
    
</figure>


<figure >
    
        <img src="img/%e5%af%bc%e5%85%a5%e6%95%b0%e6%8d%ae2.png" width="800" />
    
    
</figure>

<br>
<p>设置标注模式，点击按钮<strong>Labeling Setup</strong>,选择<strong>Natural Language Process</strong>、<strong>TEXT Classification</strong>。就考研进行pos、neg、neo三个类别的文本标注。</p>

<figure >
    
        <img src="img/%e8%ae%be%e7%bd%ae%e6%a0%87%e6%b3%a8%e6%a8%a1%e5%bc%8f1.png" width="800" />
    
    
</figure>

<p>注意label-studio提供了diy，考研根据自己需要点击<strong>Code</strong>设定标注类别名称、增减类别。大家感兴趣的可以深入研究。</p>

<figure >
    
        <img src="img/%e8%ae%be%e7%bd%ae%e6%a0%87%e6%b3%a8%e6%a8%a1%e5%bc%8f2.png" width="800" />
    
    
</figure>

<p>点击<strong>Save</strong> 按钮，开始准备标注数据啦</p>
<br>
<p>数据界面，勾选全部数据，点击蓝色按钮<strong>Label All Tasks</strong></p>

<figure >
    
        <img src="img/%e6%95%b0%e6%8d%ae%e7%95%8c%e9%9d%a2.png" width="800" />
    
    
</figure>

<p>开始标注，勾选你认为合适的标签，点击右侧<strong>Submit</strong></p>

<figure >
    
        <img src="img/%e5%bc%80%e5%a7%8b%e6%a0%87%e6%b3%a8.png" width="800" />
    
    
</figure>

<br>
<p>导出标注数据,先点击右侧<strong>Export</strong>按钮，选择导出格式，最后点击底部<strong>Export</strong>按钮执行导出。</p>

<figure >
    
        <img src="img/%e5%af%bc%e5%87%ba%e6%a0%87%e6%b3%a8%e6%95%b0%e6%8d%ae.png" width="800" />
    
    
</figure>

<br>
<h2 id="了解课程">了解课程</h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://hidadeng.github.io/blog/management_python_course/">点击进入详情页</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
