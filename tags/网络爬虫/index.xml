<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>网络爬虫 on 大邓和他的PYTHON</title>
    <link>/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in 网络爬虫 on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Wed, 15 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>使用 chatGPT 写 Python 网络爬虫</title>
      <link>https://hidadeng.github.io/blog/2023-02-15-write-web-scraper-with-chatgpt/</link>
      <pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-02-15-write-web-scraper-with-chatgpt/</guid>
      <description>在以上的操作中，Step1  **找网址规律** 只要你把已经有规律性的内容丢给ta，ta能给你生成对应的代码。Step2  **发起访问** 这一步是需要我们重点测试的，因为chatgpt不能联网，一定确保访问ok，且获取的源代码中含有我们关注的信息。Step3 **解析数据** 把输入的html源代码和输出的数据中蕴含着规律丢给ta，需要重点检查 Step4 **存至csv** 没啥问题 Step5 **代码汇总** **Bug最多，最容易睁眼瞎写代码**。我明明采集hiadadeng.github.io， 结果有几次给我汇总的代码采集的是别的网站。定位方式偷换为BeautifulSoup。所以如果能懂爬虫五步法，可以自己负责提问题，负责每个环节的检查，最终的汇总Step5暂时不要交给chatGPT，还是留给我们自己吧。</description>
      <content:encoded><![CDATA[<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/2023-02-01-v2net-science-network/">科学上网工具v2net</a></li>
<li><a href="https://hidadeng.github.io/blog/2023-02-15-how-to-sign-up-the-chatgpt-accout-and-upgrade-to-plus/">如何注册chatGPT账号</a></li>
<li><a href="https://hidadeng.github.io/blog/2023-02-11-credit_card_for_chatgpt-plus/">使用虚拟信用卡，国内用户升级为chatGPT plus会员</a></li>
<li><a href="chatgpt-wirte-scraper.png"><strong>本次chatgpt写爬虫完整截图</strong></a></li>
</ul>
<p><br><br></p>
<p>昨晚看到 <a href="https://www.bilibili.com/video/BV14T411Z7FE/">王树易老师</a> 的视频，挺有收获，做了下chatGPT写爬虫的尝试。</p>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV14T411Z7FE&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<br>
<h2 id="写爬虫流程">写爬虫流程</h2>
<h3 id="大邓">大邓</h3>
<p>chatgpt你好，我是大邓，下面是大邓使用Python写爬虫的步骤</p>
<table>
<thead>
<tr>
<th>Step</th>
<th>步骤名</th>
<th>解决的问题</th>
<th>实现的功能</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1</strong></td>
<td>请求</td>
<td>网页数据在哪里？</td>
<td>发现网址url规律</td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>请求</td>
<td>如何获取网页数据？</td>
<td>先尝试使用requests成功访问一个url，拿到一个页面数据</td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>解析</td>
<td>从html中定位需要的数据</td>
<td>使用pyquery对这一个页面的网页数据进行解析</td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>解析</td>
<td>从json中定位需要的数据</td>
<td>使用json 或 resp.json() 进行json网页数据解析</td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>存储</td>
<td>如何存储数据</td>
<td>使用csv库将数据存储到csv文件中</td>
</tr>
<tr>
<td><strong>5</strong></td>
<td>整合</td>
<td>重复2-4</td>
<td>for循环对所有的url进行访问解析存储</td>
</tr>
</tbody>
</table>
<p>我将按照步骤逐步提问你。可以开始了吗<br></p>
<h3 id="chatgpt">chatGPT</h3>
<p>可以的，我很乐意为您提供帮助。请继续提问。</p>
<p><br><br></p>
<h2 id="step1-网页数据在哪里">Step1 网页数据在哪里？</h2>
<h3 id="大邓-1">大邓</h3>
<p><img loading="lazy" src="img/blog_post_list.png" alt=""  />
</p>
<p>博客 <a href="https://hidadeng.github.io/">https://hidadeng.github.io/</a> 使用的github page构建的，生成的网站是静态网站， 页面是HTML类型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">page1</span>  <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">hidadeng</span><span class="o">.</span><span class="n">github</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">blog</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/</span>
<span class="n">page2</span>  <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">hidadeng</span><span class="o">.</span><span class="n">github</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">blog</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">2</span><span class="o">/</span>
<span class="n">page3</span>  <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">hidadeng</span><span class="o">.</span><span class="n">github</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">blog</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">3</span><span class="o">/</span>
<span class="o">......</span>
</code></pre></div><p>请使用字符串format方法，for循环，生成10页的网址。代码要简洁、易读。</p>
<br>
<h3 id="chatgpt-1">chatGPT</h3>
<p>您可以这样生成10页的网址：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">base_url</span> <span class="o">=</span> <span class="s2">&#34;https://hidadeng.github.io/blog/page/</span><span class="si">{}</span><span class="s2">/&#34;</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">base_url</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
</code></pre></div><p>这样的代码很简洁，很容易理解。您只需要在base_url字符串中指定占位符，然后在format方法中填充实际的数字即可。</p>
<p><br><br></p>
<h2 id="step2-如何获取网页数据">Step2 如何获取网页数据？</h2>
<h3 id="大邓-2">大邓</h3>
<p>写一个完整的代码很难，但是可以把问题抽象成一个局部与整体的关系。先把重复出现的局部搞明白，再使用for循环把整体解决。 现在我试图用requests的get方法访问</p>
<p>url = <a href="https://hidadeng.github.io/blog/page/2">https://hidadeng.github.io/blog/page/2</a></p>
<p>注意要加入headers， 代码要简洁，有阅读性！</p>
<br>
<h3 id="chatgpt-2">chatGPT</h3>
<p>您可以这样使用requests库的get方法访问网页数据：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&#34;https://hidadeng.github.io/blog/page/2&#34;</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;User-Agent&#34;</span><span class="p">:</span> <span class="s2">&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36&#34;</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>

<span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Failed to access the page with status code:&#34;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="p">)</span>
</code></pre></div><p>这样的代码非常简洁，容易理解。您只需要指定请求的URL和请求头，然后调用requests.get方法即可。如果请求成功（即响应状态码为200），则打印响应的网页数据；否则，打印响应的状态码。</p>
<p><br><br></p>
<h2 id="step3-从html中定位需要的数据">Step3 从html中定位需要的数据</h2>
<h3 id="大邓-3">大邓</h3>
<p>博客 <a href="https://hidadeng.github.io/">https://hidadeng.github.io/</a> 是静态网站， 页面是HTML类型， 可以使用pyquery对这一个页面的网页数据进行解析。</p>
<p>由于chatgpt 不能联网访问， 我将</p>
<p>url = &lsquo;<a href="https://hidadeng.github.io/blog/page/2/">https://hidadeng.github.io/blog/page/2/</a>’</p>
<p>对应的html源代码提供给你作为 response.text, 提供给你, 我需要你用pyquery帮我抽取出aritcle_infos，为了省事， 这里的 aritcle_infos 我只写了3个记录，请给我输出5个记录的。</p>
<p>代码要简洁、可阅读性高。返回的代码里不要显示 response.text 源代码内容， 只以变量 response.text 显示。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">response</span><span class="o">.</span><span class="n">text</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">main</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;main&#34;</span><span class="o">&gt;</span> 
<span class="o">&lt;</span><span class="n">header</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;page-header&#34;</span><span class="o">&gt;&lt;</span><span class="n">div</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;breadcrumbs&#34;</span><span class="o">&gt;&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s2">&#34;/&#34;</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;_blank&#34;</span><span class="o">&gt;</span><span class="n">Home</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;&lt;/</span><span class="n">div</span><span class="o">&gt;</span>
  <span class="o">&lt;</span><span class="n">h1</span><span class="o">&gt;</span><span class="n">Blogs</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">header</span><span class="o">&gt;</span>
<span class="o">...</span>
<span class="o">&lt;</span><span class="n">article</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;post-entry&#34;</span><span class="o">&gt;</span> 
<span class="o">&lt;</span><span class="n">figure</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;entry-cover&#34;</span><span class="o">&gt;&lt;</span><span class="n">img</span> <span class="n">loading</span><span class="o">=</span><span class="s2">&#34;lazy&#34;</span> <span class="n">src</span><span class="o">=</span><span class="s2">&#34;/images/blog/race-bar-pyanimate.png&#34;</span> <span class="n">alt</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">figure</span><span class="o">&gt;</span>
  <span class="o">&lt;</span><span class="n">header</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;entry-header&#34;</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">h2</span><span class="o">&gt;</span><span class="n">pynimate库</span> <span class="o">|</span> <span class="n">绘制动态可视化图</span>
    <span class="o">&lt;/</span><span class="n">h2</span><span class="o">&gt;</span>
  <span class="o">&lt;/</span><span class="n">header</span><span class="o">&gt;</span>
  <span class="o">&lt;</span><span class="n">section</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;entry-content&#34;</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">pynimate</span> <span class="n">创建条形数据动画</span><span class="err">。</span><span class="o">...&lt;/</span><span class="n">p</span><span class="o">&gt;</span>


<span class="o">&lt;</span><span class="n">article</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;post-entry&#34;</span><span class="o">&gt;</span> 
<span class="o">&lt;</span><span class="n">figure</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;entry-cover&#34;</span><span class="o">&gt;&lt;</span><span class="n">img</span> <span class="n">loading</span><span class="o">=</span><span class="s2">&#34;lazy&#34;</span> <span class="n">src</span><span class="o">=</span><span class="s2">&#34;/images/blog/chatgplt-plus.jpg&#34;</span> <span class="n">alt</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">figure</span><span class="o">&gt;</span>
  <span class="o">&lt;</span><span class="n">header</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;entry-header&#34;</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">h2</span><span class="o">&gt;</span><span class="n">使用虚拟信用卡</span><span class="err">，</span><span class="n">国内用户升级为chatGPT</span> <span class="n">plus会员</span>
    <span class="o">&lt;/</span><span class="n">h2</span><span class="o">&gt;</span>
  <span class="o">&lt;/</span><span class="n">header</span><span class="o">&gt;</span>
  <span class="o">&lt;</span><span class="n">section</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;entry-content&#34;</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">使用虚拟信用卡</span><span class="err">，</span><span class="n">国内用户升级为chatGPT</span> <span class="n">plus会员</span><span class="o">...&lt;/</span><span class="n">p</span><span class="o">&gt;</span>

    <span class="o">&lt;</span><span class="n">h2</span><span class="o">&gt;</span><span class="n">在巨型语言模型时代竞争</span><span class="err">：</span><span class="n">OpenAI</span> <span class="n">vs</span> <span class="n">谷歌</span>
    <span class="o">&lt;/</span><span class="n">h2</span><span class="o">&gt;</span>
  <span class="o">&lt;/</span><span class="n">header</span><span class="o">&gt;</span>
  <span class="o">&lt;</span><span class="n">section</span> <span class="n">class</span><span class="o">=</span><span class="s2">&#34;entry-content&#34;</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">在未来</span><span class="err">，</span><span class="n">巨型语言模型将继续发挥重要作用</span><span class="err">，</span><span class="n">并不断推动人工智能的发展</span><span class="err">。</span><span class="n">随着技术的不断提高</span><span class="err">，</span><span class="n">它们将越来越智能</span><span class="err">，</span><span class="n">更加逼近人类语言的表达能力</span><span class="err">，</span><span class="n">从而改变人类的生活和工作方式</span><span class="err">。</span><span class="o">...&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
    
<span class="o">...</span>
    
  <span class="o">&lt;/</span><span class="n">nav</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">footer</span><span class="o">&gt;</span>
    <span class="o">&lt;/</span><span class="n">main</span><span class="o">&gt;</span>
<span class="s2">&#34;&#34;&#34;
</span><span class="s2">
</span><span class="s2">
</span><span class="s2">aritcle_infos = [(&#39;pynimate库 | 绘制动态可视化图&#39;, 
</span><span class="s2">   &#39;https://hidadeng.github.io/blog/2023-02-11-pyanimate-create-vis-video/&#39;,
</span><span class="s2">   &#39;2023-02-09&#39;,
</span><span class="s2">   &#39;pynimate 创建条形数据动画。...&#39;),
</span><span class="s2">  
</span><span class="s2">  (&#39;使用 chatGPT 做词频统计&amp;词云图&#39;,
</span><span class="s2">   &#39;https://hidadeng.github.io/blog/2023-02-11-chatgpt-plus-for-text-mining/&#39;,
</span><span class="s2">   &#39;2023-02-09&#39;,
</span><span class="s2">   &#39;使用 chatGPT 做词频统计&amp;词云图，过程使用pandas、pyecharts、jieba等库。...&#39;),
</span><span class="s2">   
</span><span class="s2">   (&#39;使用虚拟信用卡，国内用户升级为chatGPT plus会员&#39;,
</span><span class="s2">    &#39;https://hidadeng.github.io/blog/2023-02-11-credit_card_for_chatgpt-plus/&#39;,
</span><span class="s2">    &#39;2023-02-09&#39;,
</span><span class="s2">    &#39;使用虚拟信用卡，国内用户升级为chatGPT plus会员...&#39;)
</span><span class="s2">  ]
</span></code></pre></div><br>
<h3 id="chatgpt-3">chatGPT</h3>
<p>使用pyquery解析HTML，可以通过对HTML中的标签和属性进行查询获取相关数据：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pyquery</span> <span class="kn">import</span> <span class="n">PyQuery</span> <span class="k">as</span> <span class="n">pq</span>

<span class="n">html</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">pq</span><span class="p">(</span><span class="n">html</span><span class="p">)</span>

<span class="n">aritcle_infos</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">article</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">(</span><span class="s1">&#39;article.post-entry&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">article</span><span class="p">(</span><span class="s1">&#39;header.entry-header h2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="p">()</span>
    <span class="n">link</span> <span class="o">=</span> <span class="n">article</span><span class="p">(</span><span class="s1">&#39;a.entry-link&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">attr</span><span class="p">(</span><span class="s1">&#39;href&#39;</span><span class="p">)</span>
    <span class="n">date</span> <span class="o">=</span> <span class="n">article</span><span class="p">(</span><span class="s1">&#39;footer.entry-footer span&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">attr</span><span class="p">(</span><span class="s1">&#39;title&#39;</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">article</span><span class="p">(</span><span class="s1">&#39;section.entry-content p&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="p">()</span>
    <span class="n">aritcle_infos</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">title</span><span class="p">,</span> <span class="n">link</span><span class="p">,</span> <span class="n">date</span><span class="p">,</span> <span class="n">description</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">aritcle_infos</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</code></pre></div><p><br><br></p>
<h2 id="step4-存储至csv">Step4 存储至csv</h2>
<h3 id="大邓-4">大邓</h3>
<p>使用csv库， 把Step3中得到的 aritcle_infos数据， 存储到csv文件中。csv文件编码为 utf-8编码， 字段名依次是title, link, date, intro。代码要简洁、可阅读性高。</p>
<br>
<h3 id="chatgpt-4">chatGPT</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">csv</span>

<span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;aritcle_infos.csv&#39;</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">writer</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">([</span><span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;link&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;intro&#39;</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">aritcle_infos</span><span class="p">:</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="step5-整合">Step5 整合</h2>
<h3 id="大邓-5">大邓</h3>
<p>采集hidadeng网 10 个页面的数据，存储到csv。使用for循环，重复Step2、Step3、Step4，  代码整理汇总到这次结果中。</p>
<p>注意是用requests库、pyquery、csv库代码变量前后衔接对应，代码要简洁、可阅读性高。注意，不要生成新的pyquery定位方式。</p>
<p>爬虫采集过程中，容易出现小错误，请让代码中带有try-except, 增强代码的健壮性。</p>
<br>
<h3 id="chatgpt-5">chatGPT</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">pyquery</span> <span class="kn">import</span> <span class="n">PyQuery</span> <span class="k">as</span> <span class="n">pq</span>
<span class="kn">import</span> <span class="nn">csv</span>

<span class="k">def</span> <span class="nf">get_article_info</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
            <span class="n">html</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span>
            <span class="n">doc</span> <span class="o">=</span> <span class="n">pq</span><span class="p">(</span><span class="n">html</span><span class="p">)</span>
            <span class="n">title</span> <span class="o">=</span> <span class="n">doc</span><span class="p">(</span><span class="s1">&#39;.entry-title a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="p">()</span>
            <span class="n">link</span> <span class="o">=</span> <span class="n">doc</span><span class="p">(</span><span class="s1">&#39;.entry-title a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">attr</span><span class="p">(</span><span class="s1">&#39;href&#39;</span><span class="p">)</span>
            <span class="n">date</span> <span class="o">=</span> <span class="n">doc</span><span class="p">(</span><span class="s1">&#39;.entry-date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="p">()</span>
            <span class="n">intro</span> <span class="o">=</span> <span class="n">doc</span><span class="p">(</span><span class="s1">&#39;.entry-summary p&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="p">()</span>
            <span class="n">article_info</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">title</span><span class="p">,</span> <span class="s1">&#39;link&#39;</span><span class="p">:</span> <span class="n">link</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="n">date</span><span class="p">,</span> <span class="s1">&#39;intro&#39;</span><span class="p">:</span> <span class="n">intro</span><span class="p">}</span>
            <span class="k">return</span> <span class="n">article_info</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">save_to_csv</span><span class="p">(</span><span class="n">article_infos</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;article_infos.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;link&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;intro&#39;</span><span class="p">])</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">article_info</span> <span class="ow">in</span> <span class="n">article_infos</span><span class="p">:</span>
            <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">article_info</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">base_url</span> <span class="o">=</span> <span class="s1">&#39;https://hidadeng.com/page/&#39;</span>
    <span class="n">article_infos</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
        <span class="n">url</span> <span class="o">=</span> <span class="n">base_url</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="n">article_info</span> <span class="o">=</span> <span class="n">get_article_info</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">article_info</span><span class="p">:</span>
            <span class="n">article_infos</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">article_info</span><span class="p">)</span>
    <span class="n">save_to_csv</span><span class="p">(</span><span class="n">article_infos</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div><p><br><br></p>
<h2 id="总结">总结</h2>
<p>在以上的操作中，</p>
<ul>
<li>Step1  <strong>找网址规律</strong> 只要你把已经有规律性的内容丢给ta，ta能给你生成对应的代码。</li>
<li>Step2  <strong>发起访问</strong> 这一步是需要我们重点测试的，因为chatgpt不能联网，一定确保访问ok，且获取的源代码中含有我们关注的信息。</li>
<li>Step3 <strong>解析数据</strong> 把输入的html源代码和输出的数据中蕴含着规律丢给ta，需要重点检查</li>
<li>Step4 <strong>存至csv</strong> 没啥问题</li>
<li>Step5 <strong>代码汇总</strong> <strong>Bug最多，最容易睁眼瞎写代码</strong>。我明明采集hiadadeng.github.io， 结果有几次给我汇总的代码采集的是别的网站。定位方式偷换为BeautifulSoup。</li>
</ul>
<p>所以如果能懂爬虫五步法，可以自己负责提问题，负责每个环节的检查，最终的汇总Step5暂时不要交给chatGPT，还是留给我们自己吧。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>JM2022综述 | 黄金领域: 为营销研究(新洞察)采集网络数据</title>
      <link>https://hidadeng.github.io/blog/2022-12-03-scraping-web-data-for-marketing-insights/</link>
      <pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-12-03-scraping-web-data-for-marketing-insights/</guid>
      <description>Journal of Marketing 2022年一篇关于营销领域网络爬虫的文献综述</description>
      <content:encoded><![CDATA[<p>Boegershausen, Johannes, Hannes Datta, Abhishek Borah, and Andrew Stephen. &ldquo;Fields of gold: Scraping web data for marketing insights.&rdquo; <em>Journal of Marketing</em> (2022).</p>
<p>本文是JM中少有的技术流综述文，阅读起来晦涩难懂，我们就大概知道怎么回事， 查看有没有自己感兴趣的研究(方法)即可。该文作者为该综述专门开发了一个 web-scraping.org 的网站,截图如下</p>
<p><img loading="lazy" src="img/01-web-scraping.png" alt=""  />

<img loading="lazy" src="img/02-web-scraping.png" alt=""  />
</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/KiyFyLEkqNk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<br>
<h2 id="摘要">摘要</h2>
<p>市场营销学者越来越多使用网络爬虫和API接口，从互联网收集数据。尽管网络数据得到广泛使用，但很少有学者关注收集过程中面临的各种挑战。<strong>研究人员如何确保采集的数据集是有效的？</strong> 虽然现有资源强调提取网络数据的技术细节，<strong>但作者提出了一种新的方法框架，重点是提高其有效性</strong>。特别是，该框架强调解决有效性问题， 需要在数据采集的三个阶段(<strong>选择数据源、设计数据收集和提取数据</strong>)联合考虑技术和法律/伦理问题。作者进一步审查了营销Top5期刊上300 篇使用网络数据的论文，并总结提出了如何使用网络数据促进营销研究。本文最后指出了未来研究的方向，高价值的网络数据源和新方法。</p>
<p><strong>Keywords：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- web scraping
- application programming interface, API
- crawling
- validity
- user-generated content
- social media
big data
</code></pre></div><br>
<h2 id="一网络数据的魅力">一、网络数据的魅力</h2>
<p>社会和商业生活的加速数字化创造了数量空前的消费者和企业行为数字痕迹。 每分钟，全球用户在 Google 上进行 570 万次搜索，进行 600 万次商业交易，并在 Instagram 上分享6.5万张照片（Statista 2021）。 由此产生的网络数据——规模庞大、形式多样，而且通常可以在互联网上公开访问——对于那些想要量化消费、深入了解企业行为并跟踪难以或昂贵地观察社会活动的营销学者来说，这是一个潜在的金矿 . 网络数据对营销研究的重要性反映在越来越多的有影响力的出版物中，涵盖消费者文化理论、消费者心理学、实证建模和营销策略等。</p>
<p><img loading="lazy" src="img/fig-1-increased-use-of-web-data-in-marketing.png" alt=""  />
</p>
<p>整理了 <strong>营销领域 top 5 期刊( JM、JMR、JCR、JCP、MS) 的 313 篇论文</strong> ，经过整理绘制图-1（Figure1）， 使用网络数据进行研究的量呈现快速上涨的趋势。使用网络数据的论文占比，从2010年的4%提升到2020年的15%。 者313篇论文，数据的获取方式统计</p>
<ul>
<li>**59% 的论文使用了 <strong>网络爬虫</strong> 采集数据</li>
<li>12% 的论文使用API收集数据</li>
<li>9% 的论文同时使用了网络爬虫和API</li>
<li>20% 使用人工从网站手动复制粘贴数据</li>
</ul>
<p><strong>使用 网络数据 的论文，平均被引用次数 7.55， 远高于 非网络数据 的 3.90</strong>。</p>
<br>
<p>使用网络数据做新研究，大致有4种实现路径</p>
<ol>
<li><strong>研究新现象，新场景</strong>
<ul>
<li>网络世界产生的不同于现实世界的情景，可以研究新现象</li>
</ul>
</li>
<li><strong>繁荣生态价值</strong>
<ul>
<li>比如，对亚马逊评论数据进行研究，研究发现可以帮助亚马逊平台进行改善。</li>
</ul>
</li>
<li><strong>促进方法论进步</strong>
<ul>
<li>文本、图片、音频、视频等</li>
</ul>
</li>
<li><strong>提高测量效果(快、准、好、全)</strong>
<ul>
<li>借助一些API，可以对已有的数据集增加新的信息量。</li>
<li>例如，日期数据，结合HolidayAPI，可以查看日期的节假日信息</li>
<li>给定日期和IP地址，使用Weather Underground可以查看天气信息</li>
</ul>
</li>
</ol>
<p><img loading="lazy" src="img/table-1-four-pathway-of-knowledge-creation-using-web-data.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二数据采集的方法框架">二、数据采集的方法框架</h2>
<p>在使用 **网络爬虫 和 API ** 自动收集网络数据时，研究人员通常会在 **研究有效性、技术可行性和法律/伦理风险 **1 三者间权衡利弊得失，研究人员如何解决这些权衡，通过增强或破坏 <strong>统计结论有效性、内部有效性、结构有效性和外部有效性</strong> 来塑造研究结果的可信度（Shadish、Cook 和 Campbell 2002）。</p>
<p><img loading="lazy" src="img/fig-2-methodological-framework-for-collecting-web-data.png" alt=""  />
</p>
<p>本文开发了一个方法框架，为使用 网络爬虫 和 API 自动收集网络数据提供指导。图 2（Figure 2） 涵盖三个关键阶段</p>
<ul>
<li><strong>数据源选择</strong></li>
<li><strong>设计方案</strong>
<ul>
<li>从网站中抽取哪些信息</li>
<li>采集频率，即 每天(周/月)重复运行一次爬虫，得到面板数据</li>
</ul>
</li>
<li><strong>执行数据采集</strong>
<ul>
<li>如何改善爬虫运行效率</li>
<li>如何处理原始信息，完整的保存为原始格式html、json，还是只抽取存储当前想要的字段</li>
</ul>
</li>
</ul>
<p>研究人员通常从一组广泛的潜在数据源开始，并根据三个关键考虑因素（有效性、技术可行性和法律/道德风险）剔除其中一些数据源。这三个考虑因素出现在倒金字塔的角落，底部的有效性强调其重要性。鉴于在收集最终数据集之前难以预测其确切特征，研究人员在设计、原型化和完善数据收集时经常重新考虑这些因素。未能解决技术或法律/伦理问题可能意味着网络数据无法有意义地告知研究问题。</p>
<h3 id="21-数据源面临的挑战解决办法">2.1 数据源面临的挑战(解决办法)</h3>
<ol>
<li>探索潜在网络数据源
<ul>
<li>由于网络资源在质量、稳定性和可检索性方面存在巨大差异，研究人员可能倾向于只考虑主要或熟悉的平台。 对数据世界的彻底探索允许令人信服的理论检验和识别可能难以以其他方式注意到的新颖的、新兴的营销现象。</li>
</ul>
</li>
<li>考虑网络爬虫的替代方案
<ul>
<li>由于网络抓取是最流行的网络数据提取方法，研究人员可能会忽视其他提取数据的方法。 API 提供了一种记录和授权的方式来获取许多来源的 Web 数据。 一些来源还提供现成的数据集。 使用此类替代方案可以节省时间并最大限度地减少法律风险。</li>
</ul>
</li>
<li>将数据与场景结合对应起来
<ul>
<li>Web 数据通常没有大量的文档。 尽早识别潜在相关的背景信息对于研究的相关性和有效性至关重要。
<img loading="lazy" src="img/table-2-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />
</li>
</ul>
</li>
</ol>
<br>
<h3 id="22-设计数据采集方案">2.2 设计数据采集方案</h3>
<ol>
<li>从页面抽取什么信息，从有效性、合法、技术可行性 三个方面论证。</li>
<li>如何进行数据抽样？</li>
<li>以什么频率(每天、周、月)进行数据采集</li>
</ol>
<p><img loading="lazy" src="img/table-3-1-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />

<img loading="lazy" src="img/table-3-2-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />
</p>
<br>
<h3 id="23-执行数据采集">2.3 执行数据采集</h3>
<ol>
<li>如何改善爬虫运行效率</li>
<li>如何监控数据质量</li>
<li>整理数据文档(记录)
<img loading="lazy" src="img/table-4-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />
</li>
</ol>
<br>
<h2 id="部分参考文献">部分参考文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]Allard, Thomas, Lea H. Dunn, and Katherine White. &#34;Negative reviews, positive impact: Consumer empathetic responding to unfair word of mouth.&#34; Journal of Marketing 84, no. 4 (2020): 86-108.
[2]Gao, Weihe, Li Ji, Yong Liu, and Qi Sun. &#34;Branding cultural products in international markets: a study of hollywood movies in China.&#34; Journal of Marketing 84, no. 3 (2020): 86-105.
[3]Reich, Taly, and Sam J. Maglio. &#34;Featuring mistakes: The persuasive impact of purchase mistakes in online reviews.&#34; Journal of Marketing 84, no. 1 (2020): 52-65.
[4]Lee, Jeffrey K., and Ann Kronrod. &#34;The strength of weak-tie consensus language.&#34; Journal of Marketing Research 57, no. 2 (2020): 353-374.
[5]Matz, Sandra C., Cristina Segalin, David Stillwell, Sandrine R. Müller, and Maarten W. Bos. &#34;Predicting the personal appeal of marketing images using computational methods.&#34; Journal of Consumer Psychology 29, no. 3 (2019): 370-390.
[6]Dai, Hengchen, and Dennis J. Zhang. &#34;Prosocial goal pursuit in crowdfunding: Evidence from kickstarter.&#34; Journal of Marketing Research 56, no. 3 (2019): 498-517.
[7]Luffarelli, Jonathan, Mudra Mukesh, and Ammara Mahmood. &#34;Let the logo do the talking: The influence of logo descriptiveness on brand equity.&#34; Journal of Marketing Research 56, no. 5 (2019): 862-878.
[8]Bond, Samuel D., Stephen X. He, and Wen Wen. &#34;Speaking for “free”: Word of mouth in free-and paid-product settings.&#34; Journal of Marketing Research 56, no. 2 (2019): 276-290.
[9]Han, Kyuhong, Jihye Jung, Vikas Mittal, Jinyong Daniel Zyung, and Hajo Adam. &#34;Political identity and financial risk taking: Insights from social dominance orientation.&#34; Journal of Marketing Research 56, no. 4 (2019): 581-601.
[10]Netzer, Oded, Alain Lemaire, and Michal Herzenstein. &#34;When words sweat: Identifying signals for loan default in the text of loan applications.&#34; Journal of Marketing Research 56, no. 6 (2019): 960-980.
[11]Toubia, Olivier, Garud Iyengar, Renée Bunnell, and Alain Lemaire. &#34;Extracting features of entertainment products: A guided latent dirichlet allocation approach informed by the psychology of media consumption.&#34; Journal of Marketing Research 56, no. 1 (2019): 18-36.
[12]Van Laer, Tom, Jennifer Edson Escalas, Stephan Ludwig, and Ellis A. Van Den Hende. &#34;What happens in Vegas stays on TripAdvisor? A theory and technique to understand narrativity in consumer reviews.&#34; Journal of Consumer Research 46, no. 2 (2019): 267-285.
[13]Zhong, Ning, and David A. Schweidel. &#34;Capturing changes in social media content: A multiple latent changepoint topic model.&#34; Marketing Science 39, no. 4 (2020): 827-846.
[14]Colicev, Anatoli, Ashwin Malshe, Koen Pauwels, and Peter O&#39;Connor. &#34;Improving consumer mindset metrics and shareholder value through social media: The different roles of owned and earned media.&#34; Journal of Marketing 82, no. 1 (2018): 37-56.
[15]Liu, Xuan, Savannah Wei Shi, Thales Teixeira, and Michel Wedel. &#34;Video content marketing: The making of clips.&#34; Journal of Marketing 82, no. 4 (2018): 86-101.
[16]Liu, Jia, and Olivier Toubia. &#34;A semantic approach for estimating consumer content preferences from online search queries.&#34; Marketing Science 37, no. 6 (2018): 930-952.
[17]Nam, Hyoryung, Yogesh V. Joshi, and P. K. Kannan. &#34;Harvesting brand information from social tags.&#34; Journal of Marketing 81, no. 4 (2017): 88-108.
[18]Packard, Grant, and Jonah Berger. &#34;How language shapes word of mouth&#39;s impact.&#34; Journal of Marketing Research 54, no. 4 (2017): 572-588.
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>百度指数 | 使用qdata采集百度指数</title>
      <link>https://hidadeng.github.io/blog/qdata_collect_baidu_index/</link>
      <pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/qdata_collect_baidu_index/</guid>
      <description>百度指数数据采集</description>
      <content:encoded><![CDATA[<p>之前一直没有好的办法采集，最近发现一个神奇的qdata包可以采集<a href="https://index.baidu.com/v2/index.html#/">百度指数</a>不太好采集。</p>
<h2 id="安装">安装</h2>
<p>打开命令行(cmd、terminal)，</p>
<pre><code># 避免与pycryptodome冲突
pip3 uninstall pycrypto  

#安装最新的qdata
pip3 install --upgrade qdata
</code></pre>
<p><br><br></p>
<h2 id="百度指数">百度指数</h2>
<p><a href="https://github.com/longxiaofei/spider-BaiduIndex">qdata包</a>内置百度指数的省份(城市)对应的地区代码，以省份代码为例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">qdata.baidu_index</span> <span class="kn">import</span> <span class="n">PROVINCE_CODE</span><span class="p">,</span> <span class="n">CITY_CODE</span>

<span class="n">PROVINCE_CODE</span>
</code></pre></div><p>Run</p>
<pre><code>{'山东': '901',
 '贵州': '902',
 '江西': '903',
 '重庆': '904',
 '内蒙古': '905',
 '湖北': '906',
 '辽宁': '907',
 '湖南': '908',
....
 '甘肃': '925',
 '新疆': '926',
 '河南': '927',
 '安徽': '928',
 '山西': '929',
 '海南': '930',
 '台湾': '931',
 '西藏': '932',
 '香港': '933',
 '澳门': '934'}
</code></pre>
<p><br><br></p>
<h2 id="案例">案例</h2>
<p><img loading="lazy" src="img/baidu_index.png" alt=""  />
</p>
<p>采集</p>
<ul>
<li>时间段 <code>2022-05-01 ~ 2022-08-01</code></li>
<li>地区 <code>山东</code></li>
<li>关键词 <code>['疫情', '锻炼', '居家']</code></li>
</ul>
<p>的百度指数数据。</p>
<p><br><br></p>
<h3 id="准备你的cookie">准备你的cookie</h3>
<p>在命令行 <strong>Python环境</strong> 下运行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">qdata.baidu_login</span> <span class="kn">import</span> <span class="n">get_cookie_by_qr_login</span>
<span class="n">get_cookie_by_qr_login</span><span class="p">()</span>
</code></pre></div><p>上方代码运行结束后，弹出一个二维码窗体。</p>
<p><img loading="lazy" src="img/get_cookie.png" alt=""  />
</p>
<p>使用百度相关app，笔者使用 <strong>百度网盘app</strong> 扫码, 命令行内出现了一串字符串就是cookie。</p>
<p><img loading="lazy" src="img/cmd_cookie.jpg" alt=""  />
</p>
<br>
<h3 id="代码">代码</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">qdata.baidu_index</span> <span class="kn">import</span> <span class="n">PROVINCE_CODE</span>
<span class="kn">from</span> <span class="nn">qdata.baidu_index</span> <span class="kn">import</span> <span class="n">get_search_index</span>
<span class="kn">import</span> <span class="nn">csv</span>

<span class="n">province</span> <span class="o">=</span> <span class="s1">&#39;山东&#39;</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;网购&#39;</span><span class="p">,</span> <span class="s1">&#39;居家&#39;</span><span class="p">]</span>
<span class="n">start_dt</span> <span class="o">=</span> <span class="s1">&#39;2022-07-01&#39;</span>
<span class="n">end_dt</span> <span class="o">=</span> <span class="s1">&#39;2022-08-01&#39;</span>

<span class="c1">#你的cookie</span>
<span class="n">cookie</span> <span class="o">=</span> <span class="s1">&#39;你的cookie&#39;</span>

<span class="c1">#数据存储于data文件夹内</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/</span><span class="si">{}</span><span class="s1">.csv&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">province</span><span class="p">),</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvf</span><span class="p">:</span>
    <span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;province&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;index&#39;</span><span class="p">]</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">get_search_index</span><span class="p">(</span><span class="n">keywords_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">keywords</span><span class="p">],</span> 
                                  <span class="n">start_date</span> <span class="o">=</span> <span class="n">start_dt</span><span class="p">,</span> 
                                  <span class="n">end_date</span> <span class="o">=</span> <span class="n">end_dt</span><span class="p">,</span> 
                                  <span class="n">area</span>  <span class="o">=</span> <span class="n">PROVINCE_CODE</span><span class="p">[</span><span class="n">province</span><span class="p">],</span>
                                  <span class="n">cookies</span> <span class="o">=</span> <span class="n">cookie</span><span class="p">):</span>

        
        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;province&#39;</span><span class="p">:</span> <span class="n">province</span><span class="p">,</span>
                <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">],</span>
                <span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">],</span> 
                <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">]}</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>{'province': '山东', 'type': 'all', 'date': '2022-07-01', 'index': '200'}
{'province': '山东', 'type': 'all', 'date': '2022-07-02', 'index': '148'}
{'province': '山东', 'type': 'all', 'date': '2022-07-03', 'index': '257'}

...
{'province': '山东', 'type': 'pc', 'date': '2022-07-01', 'index': '59'}
{'province': '山东', 'type': 'pc', 'date': '2022-07-02', 'index': '0'}
{'province': '山东', 'type': 'pc', 'date': '2022-07-03', 'index': '118'}

...
{'province': '山东', 'type': 'wise', 'date': '2022-07-01', 'index': '141'}
{'province': '山东', 'type': 'wise', 'date': '2022-07-02', 'index': '148'}
{'province': '山东', 'type': 'wise', 'date': '2022-07-03', 'index': '139'}
</code></pre>
<p>type字段的含义</p>
<ul>
<li>all 信息来自 <code>PC+移动</code></li>
<li>pc 信息来自 <code>PC</code></li>
<li>wise 信息来自 <code>移动</code></li>
</ul>
<p><code>山东 2022-07-01 PC+移动</code> 的指数是 <code>200</code>， 刚好等于 <code>pc59+移动141</code> ，也等于 <code>网购65+居家135</code> 。</p>
<p><img loading="lazy" src="img/baidu_index.png" alt=""  />
</p>
<p>最后数据存储于data文件夹内，如下图。</p>
<p><img loading="lazy" src="img/data.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>17G数据 | 企业社会责任报告数据集</title>
      <link>https://hidadeng.github.io/blog/coporate_social_responsibility_datasets/</link>
      <pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/coporate_social_responsibility_datasets/</guid>
      <description>近年来，企业社会责任（csr)已成为全球学术界研究的热点。国内外各大顶刊都先后刊登了多篇关于csr的文章，比如《企业绿色创新实践如何破解“和谐共生”难题？》（发表于管理世界）、《负责任的国际投资：ESG与中国OFDI》（发表于经济研究）、《Is my company really doing good? Factors influencing employees&amp;#39; evaluation of the authenticity of their company&amp;#39;s corporate social responsibility engagement》（发表于JBR）等。这些文章核心变量的构建大都基于对企业社会责任报告的内容分析和挖掘。比如《企业绿色创新实践如何破解“和谐共生”难题？》的被解释变量（绿色创新）以及部分解释变量（二元合法性和伦理型领导）。可见，社会责任报告对于我们研究esg至关重要。因此，接下来小编就带大家爬取深交所上市公司历年的社会责任报告，希望能够给大家带来一些帮助。In recent years, corporate social responsibility (csr) has become a research hotspot in the global academic circles. Major top journals at home and abroad have successively published many articles on CSR, such as How can corporate green innovation practice solve the problem of harmonious symbiosis? (published in Governance World), Responsible International Investment: ESG and China&amp;#39;s OFDI (published in Economic Research), Is my company really doing good? Factors influencing employees&amp;#39; evaluation of the authenticity of their company&amp;#39;s corporate social responsibility engagement (published in JBR) et al. The construction of core variables in these articles is mostly based on the content analysis and mining of corporate social responsibility reports. For example, How can corporate green innovation practice solve the problem of harmonious symbiosis? 》The explained variable (green innovation) and part of the explanatory variables (dual legitimacy and ethical leadership). It can be seen that the social responsibility report is very important for us to study esg. Therefore, the next editor will take you to crawl the social responsibility reports of companies listed on the Shenzhen Stock Exchange over the years, hoping to bring you some help.</description>
      <content:encoded><![CDATA[<blockquote>
<p>作者:张延丰 哈工程在读博士</p>
</blockquote>
<p>近年来，企业社会责任（csr)已成为全球学术界研究的热点。国内外各大顶刊都先后刊登了多篇关于csr的文章，比如《企业绿色创新实践如何破解“和谐共生”难题？》（发表于管理世界）、《负责任的国际投资：ESG与中国OFDI》（发表于经济研究）、《Is my company really doing good? Factors influencing employees' evaluation of the authenticity of their company&rsquo;s corporate social responsibility engagement》（发表于JBR）等。这些文章核心变量的构建大都基于对企业社会责任报告的内容分析和挖掘。比如《企业绿色创新实践如何破解“和谐共生”难题？》的被解释变量（绿色创新）以及部分解释变量（二元合法性和伦理型领导）。可见，社会责任报告对于我们研究esg至关重要。因此，接下来小编就带大家爬取深交所上市公司历年的社会责任报告，希望能够给大家带来一些帮助。</p>
<br>
<h2 id="获取数据集">获取数据集</h2>
<p>采集4000多个pdf文件。经过数据清洗，将20G的pdf数据，汇总整理到170M的csv文件内。</p>
<p><img loading="lazy" src="img/datasets.png" alt=""  />
</p>
<p>数据整理不易，如需获取本数据集，<strong>请转发本文至朋友圈集赞满30+</strong>， 加微信【372335839】，备注【深圳ESG数据集】</p>
<img src="img/wechat.jpg" style="zoom:50%;" />
<p><br><br></p>
<h2 id="一构建网络爬虫">一、构建网络爬虫</h2>
<p>数据采集分为多个步骤</p>
<ol>
<li>找网址规律(GET or POST), 构造url参数</li>
<li>伪装请求，防止被封</li>
<li>构造csv，存储信心</li>
<li>执行整个爬虫</li>
</ol>
<h3 id="11-url">1.1 url</h3>
<p>打开X交所的 <a href="http://www.szse.cn/disclosure/listed/notice/">http://www.szse.cn/disclosure/listed/notice/</a> ，同时打开浏览器开发者工具network面板，在截图左侧输入框输入关键词 『社会责任报告』，按下回车。</p>
<p>此时开发者工具network面板出现很多网络交换信息， 点击检查发现下图</p>
<p><img loading="lazy" src="img/01-%e7%bd%91%e5%9d%80%e8%a7%84%e5%be%8b.png" alt=""  />
</p>
<p>发现该页面数据是<strong>POST</strong>请求，网址为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">http://www.szse.cn/api/disc/announcement/annList?random=random参数
</code></pre></div><h3 id="12-headers">1.2 headers</h3>
<p>同时也能发现伪装头参数，现将两个重要信息整理为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://www.szse.cn/api/disc/announcement/annList?random=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">())</span>

<span class="c1">#伪装头</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Accept&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json, text/javascript, */*; q=0.01&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Encoding&#39;</span><span class="p">:</span> <span class="s1">&#39;gzip, deflate&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;zh-CN,zh;q=0.9,en;q=0.8&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Content-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Host&#39;</span><span class="p">:</span> <span class="s1">&#39;www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Origin&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Proxy-Connection&#39;</span><span class="p">:</span> <span class="s1">&#39;close&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Referer&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn/disclosure/listed/fixed/index.html&#39;</span><span class="p">,</span>
           <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Request-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;ajax&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Requested-With&#39;</span><span class="p">:</span> <span class="s1">&#39;XMLHttpRequest&#39;</span><span class="p">}</span>
</code></pre></div><h3 id="13-data参数">1.3 data参数</h3>
<p>POST请求需要构造data参数，在开发者对应于payload, 整理为Python格式</p>
<p><img loading="lazy" src="img/02-payload.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">
</code></pre></div><p><br><br></p>
<h3 id="14-preview">1.4 preview</h3>
<p>看到左侧渲染后的数据，同时也能在开发者工具network面板看到肉眼背后的源数据。我们使用preview预览截图再次确认网址规律没有问题。</p>
<p><img loading="lazy" src="img/03-data-preview.jpg" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">keyword</span> <span class="o">=</span> <span class="s1">&#39;社会责任报告&#39;</span>
<span class="n">page</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#post方法参数</span>
<span class="n">payload</span> <span class="o">=</span><span class="p">{</span><span class="s2">&#34;seDate&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">,</span><span class="s2">&#34;&#34;</span><span class="p">],</span>
           <span class="s2">&#34;searchKey&#34;</span><span class="p">:</span> <span class="p">[</span><span class="n">keyword</span><span class="p">],</span>
           <span class="s2">&#34;channelCode&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;listedNotice_disc&#34;</span><span class="p">],</span>
           <span class="s2">&#34;pageSize&#34;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
           <span class="s2">&#34;pageNum&#34;</span><span class="p">:</span> <span class="n">page</span><span class="p">}</span>
</code></pre></div><h3 id="15-csv">1.5 csv</h3>
<p>现在已经把爬虫最重要的工作做完了，剩下的就是想办法构造出csv，并将数据存入csv。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#定义csv字段，存储PDF链接信息至data/esg_links.csv</span>
<span class="n">csvf</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/esg_links.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf_link&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">]</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
<span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pdf_link&#39;</span><span class="p">:</span> <span class="s1">&#39;测试pdf文件链接&#39;</span><span class="p">,</span>
             <span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="s1">&#39;测试股票代码&#39;</span><span class="p">,</span>
             <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;股票名称&#39;</span><span class="p">,</span>
             <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="s1">&#39;报告名称&#39;</span><span class="p">,</span>
             <span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="s1">&#39;发布日期&#39;</span><span class="p">,</span>
             <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="s1">&#39;pdf文件字节大小&#39;</span><span class="p">,</span>
             <span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="s1">&#39;数据id&#39;</span><span class="p">}</span>
             
<span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

<span class="c1">#关闭csv</span>
<span class="n">csvf</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div><p><br><br></p>
<h2 id="二爬虫代码完整">二、爬虫代码(完整)</h2>
<p>当你看到本文时，该完整代码很有可能会随着网站变化而失效。不要悲伤难过， 按照爬虫思路自己diy即可。如果没有爬虫基础，学习 <a href="https://www.bilibili.com/video/BV1AE411r7ph">大邓的B站爬虫视频</a> ，</p>
<p><img loading="lazy" src="img/%e5%a4%a7%e9%82%93%e7%88%ac%e8%99%ab.jpg" alt=""  />
</p>
<p>自己懂爬虫原理diy代码，比改别人的代码来的更容易。将前面的准备工作组织起来, 就形成了下面的完整代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">csv</span>

<span class="n">keyword</span> <span class="o">=</span> <span class="s2">&#34;社会责任报告&#34;</span>

<span class="c1">#定义csv字段，存储PDF链接信息至data/esg_links.csv</span>
<span class="n">csvf</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/esg_links.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf_link&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">]</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
<span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

<span class="c1">#伪装头</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Accept&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json, text/javascript, */*; q=0.01&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Encoding&#39;</span><span class="p">:</span> <span class="s1">&#39;gzip, deflate&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;zh-CN,zh;q=0.9,en;q=0.8&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Content-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Host&#39;</span><span class="p">:</span> <span class="s1">&#39;www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Origin&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Proxy-Connection&#39;</span><span class="p">:</span> <span class="s1">&#39;close&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Referer&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn/disclosure/listed/fixed/index.html&#39;</span><span class="p">,</span>
           <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Request-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;ajax&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Requested-With&#39;</span><span class="p">:</span> <span class="s1">&#39;XMLHttpRequest&#39;</span><span class="p">}</span>

<span class="n">page</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#post方法参数</span>
<span class="n">payload</span> <span class="o">=</span><span class="p">{</span><span class="s2">&#34;seDate&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">,</span><span class="s2">&#34;&#34;</span><span class="p">],</span>
           <span class="s2">&#34;searchKey&#34;</span><span class="p">:</span> <span class="p">[</span><span class="n">keyword</span><span class="p">],</span>
           <span class="s2">&#34;channelCode&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;listedNotice_disc&#34;</span><span class="p">],</span>
           <span class="s2">&#34;pageSize&#34;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
           <span class="s2">&#34;pageNum&#34;</span><span class="p">:</span> <span class="n">page</span><span class="p">}</span>

<span class="c1">#发起请求</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://www.szse.cn/api/disc/announcement/annList?random=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">())</span>
<span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span>
                     <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                     <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">))</span>

<span class="c1">#当data关键词有对应的非空列表，循环一直进行。</span>
<span class="k">while</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s1">&#39;data&#39;</span><span class="p">]:</span>
    <span class="n">payload</span><span class="p">[</span><span class="s1">&#39;pageNum&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">page</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span>
                     <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                     <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">))</span>
    
    <span class="n">esgs</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">esg</span> <span class="ow">in</span> <span class="n">esgs</span><span class="p">:</span>
        <span class="c1">#以字典样式写入csv</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pdf_link&#39;</span><span class="p">:</span> <span class="s1">&#39;http://disc.static.szse.cn/download&#39;</span><span class="o">+</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;attachPath&#39;</span><span class="p">],</span>
                <span class="c1">#为防止股票代码被exel等软件识别为数字，特转为字符串，并加sz标识。</span>
                <span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="s1">&#39;sz&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">esg</span><span class="p">[</span><span class="s1">&#39;secCode&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span> 
                <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;secName&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">],</span>
                <span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;publishTime&#39;</span><span class="p">],</span>
                <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;attachSize&#39;</span><span class="p">],</span>
                <span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]}</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="n">page</span> <span class="o">=</span> <span class="n">page</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="c1">#关闭csv</span>
<span class="n">csvf</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div><p><br><br></p>
<h2 id="三查看csv">三、查看csv</h2>
<p>使用pandas读取 <code>data/esg_links.csv</code>,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;esg_links.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/04-df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">4392
</code></pre></div><p>一共有4392条 「企业社会责任」 的报告数据。</p>
<p><br><br></p>
<h2 id="四批量下载">四、批量下载</h2>
<p>下载就简单多了， 直接使用定义好的爬虫代码。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>

<span class="k">def</span> <span class="nf">download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    下载多媒体及文件
</span><span class="s2">    url： 多媒体文件链接（结尾有文件格式名）
</span><span class="s2">    file: 存储文件的路径（结尾有文件格式名）
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="c1">#获取到二进制数据</span>
    <span class="n">binarydata</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">content</span>
    <span class="c1">#以二进制形式将数据流存入fname中</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">binarydata</span><span class="p">)</span> 
        

<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">link</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pdf_link&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span>
    <span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">link</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/</span><span class="si">{}</span><span class="s1">.pdf&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0 山河药辅：山河药辅2021年度社会责任报告
1 新 希 望：2021年企业社会责任报告（英文版）
2 天原股份：宜宾天原集团股份有限公司社会责任报告
3 五 粮 液：2021年度社会责任报告（英文版）
4 中兵红箭：2021年度社会责任报告	
......
......
148 苏宁环球：2021年社会责任报告
149 蓝色光标：2021年度企业社会责任报告
150 开尔新材：2021年度社会责任报告
151 中顺洁柔：2021年社会责任报告
......
......

4391 闽东电力：2006年度社会责任报告
4392  阳光发展：2006年度社会责任报告书
</code></pre></div><p>采集过程中，被封锁在所难免，所以记得每次停止采集的位置，在csv中删除该位置之前的数据。然后重新运行代码即可。</p>
<h3 id="注意">注意</h3>
<p>即时解决以上问题，可能遇到奇怪的问题。比如</p>
<p><img loading="lazy" src="img/07-error.png" alt=""  />
</p>
<p>检查发现相比其他几百kb的pdf，问题文件大小只有几kb。问题可能是被网站封锁或网络不稳定导致，标记好问题pdf的链接，重新批量下载一遍。</p>
<p><img loading="lazy" src="img/06-error.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="汇总至csv">汇总至csv</h2>
<p>很多企业社会责任报告是图片合成的，所以这里的pdf体积很大。将data文件夹中的4000多个pdf汇总至esg_data.csv中，能节约出电脑内存空间，也方便后续数据分析。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pdfdocx</span> <span class="kn">import</span> <span class="n">read_pdf</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">csv</span>

<span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#新建esg_data.csv，用于存储企业社会责任报告数据</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;esg_data.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvf</span><span class="p">:</span>
    <span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf_link&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">,</span> <span class="s1">&#39;report_content&#39;</span><span class="p">]</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

    <span class="n">files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
        <span class="n">record_of_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">file</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.pdf&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">record_of_df</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
        <span class="n">data</span><span class="p">[</span><span class="s1">&#39;report_content&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;data/</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file</span><span class="p">))</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div><p>最后，数据从20G的data文件夹(4000多个PDF)压缩为一个170M的esg_data.csv文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">esg_reports_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;esg_data.csv&#39;</span><span class="p">)</span>
<span class="n">esg_reports_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/08-esg_reports_df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">esg_reports_df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">4346
</code></pre></div><p><br><br></p>
<h2 id="五相关文献">五、相关文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]解学梅, &amp; 朱琪玮. (2021). 企业绿色创新实践如何破解 “和谐共生” 难题?. 管理世界, 37(1), 128-149.
[2]谢红军 &amp; 吕雪.(2022).负责任的国际投资：ESG与中国OFDI. 经济研究(03),83-99.
[3]Schaefer, S. D., Terlutter, R., &amp; Diehl, S. (2019). Is my company really doing good? Factors influencing employees&#39; evaluation of the authenticity of their company&#39;s corporate social responsibility engagement. Journal of business research, 101, 128-143.
</code></pre></div><p><br><br></p>
<h2 id="六其他广告">六、其他(广告)</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>大邓整理的有用Python项目</title>
      <link>https://hidadeng.github.io/blog/python_tools/</link>
      <pubDate>Fri, 07 Jan 2022 12:43:10 +0600</pubDate>
      
      <guid>/blog/python_tools/</guid>
      <description>涵盖网络爬虫、数据库、数据分析、机器学习、可视化、文本分析</description>
      <content:encoded><![CDATA[<h2 id="tool_kits">Tool_Kits</h2>
<p>工具箱大全,主要是Python项目。</p>
<p>涵盖：</p>
<ol>
<li>网络爬虫</li>
<li>数据库</li>
<li>数据分析</li>
<li>机器学习</li>
<li>可视化</li>
<li>文本分析</li>
<li>GUI</li>
<li>自动化办公</li>
<li>其他</li>
</ol>
<br>
<h2 id="网络爬虫">网络爬虫</h2>
<ul>
<li><a href="https://github.com/psf/requests">requests</a>  最好用的网络爬虫访问库</li>
<li><a href="https://github.com/hidadeng/smartscraper">smartscraper</a>  最简单的网络爬虫访问&amp;解析库</li>
<li><a href="https://github.com/hidadeng/weibo_crawler">weibo_crawler</a> 最简单的微博爬虫</li>
<li><a href="https://mp.weixin.qq.com/s/qL1uEk4j_ks3uhjINe-xyA">崔庆才大神发布的测试站点</a></li>
<li><a href="https://github.com/Gerapy/GerapyAutoExtractor">gerapy-auto-extractor</a> 爬虫页面智能解析库</li>
<li><a href="https://github.com/davidteather/TikTok-Api">TikTok-Api</a> 抖音国际站爬虫库</li>
<li><a href="https://github.com/tebelorg/RPA-Python">rpa</a> Python自动化操纵包</li>
<li><a href="https://github.com/celery/celery/">celery</a> 可以用于制作爬虫访问队列</li>
<li><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a> 最简单的网页解析库</li>
<li><a href="https://github.com/gawel/pyquery">pyquery</a>  最简洁网页解析库</li>
<li><a href="https://github.com/scrapy/scrapy">scrapy</a> 最流行的爬虫框架</li>
<li><a href="https://github.com/binux/pyspider">pyspider</a> 国人开发的爬虫框架</li>
<li><a href="https://github.com/SeleniumHQ/selenium/">selenium</a> 浏览器自动化测试框架，可以用于爬虫反爬</li>
<li><a href="https://github.com/microsoft/playwright">playwright</a> 微软开源的浏览器自动化测试框架</li>
<li><a href="https://github.com/imWildCat/scylla">scylla</a> 智能IP代理池，用于反爬</li>
<li><a href="https://github.com/hidadeng/shreport">shreport</a> 上海证券交易所上市公司定期报告下载</li>
<li><a href="https://github.com/codelucas/newspaper">newspaper</a> 新闻爬虫库，根据提供的url可以抽取出新闻标题、作者、关键词、总结，部分功能支持中文</li>
<li><a href="https://github.com/sml2h3/ddddocr">ddddocr</a>  通用验证码识别OCR pypi版</li>
</ul>
<br>
<h2 id="web">Web</h2>
<ul>
<li><a href="https://github.com/getpelican/pelican">pelican</a> Python静态网站生成库</li>
<li><a href="https://github.com/pallets/flask">flask</a> 可以开发网站、分享rest-api接口;流行度top2的web框架</li>
<li><a href="https://github.com/streamlit/streamlit">streamlit</a>、<a href="https://github.com/wang0618/PyWebIO">PyWebIO</a>对Python小白最友好的的web库</li>
<li><a href="https://github.com/tiangolo/fastapi">fastapi</a>  web框架，高性能，易于学习，快速编写代码；</li>
<li><a href="https://github.com/wang0618/PyWebIO">PyWebIO</a>  不需要编写HTML和JS代码，就可以构建简单的基于浏览器的GUI应用。</li>
<li><a href="https://github.com/mkdocs/mkdocs">mkdocs</a> 制作文档网站
<br></li>
</ul>
<h2 id="数据库">数据库</h2>
<ul>
<li><a href="https://github.com/PyMySQL/PyMySQL">PyMySQL</a></li>
<li><a href="https://docs.python.org/3/library/sqlite3.html">Sqlite3</a>  轻量级sql数据库(python内置库)</li>
<li><a href="https://github.com/mongodb/mongo-python-driver">pymongo</a> 非关系型MongoDB库</li>
<li><a href="https://github.com/redis/redis">redis</a>  Redis数据库</li>
<li><a href="https://github.com/py2neo-org/py2neo">py2neo</a>  对接Neo4J数据库的python库</li>
<li><a href="https://github.com/simonw/datasette">datasette</a> 探索和发布数据的开源多功能工具，主要面向数据记者、博物馆馆长、档案管理员、地方政府、科学家、研究人员以及任何拥有希望与世界分享数据的人。</li>
</ul>
<br>
<h2 id="数据分析">数据分析</h2>
<ul>
<li><a href="https://github.com/pandas-dev/pandas">pandas</a> 必须Python数据分析库，读取文件、预处理数据、分析、存储</li>
<li><a href="https://github.com/garrettj403/SciencePlots">SciencePlots</a>  科学绘图的Python工具包</li>
<li><a href="https://github.com/orchest/orchest">Orchest</a> 创建数据科学工作量的工具。Orchest是一款Web数据科学工具，可在文件系统上运行</li>
<li><a href="https://github.com/statsmodels/statsmodels">statsmodels</a> Python的统计计量统计库</li>
<li><a href="https://github.com/bashtage/linearmodels">linearmodels</a> 添加线性模型，包括statsmodels中缺少的工具变量和面板数据模型。</li>
<li><a href="https://github.com/streamlit/streamlit">streamlit</a> 快速搭建本地数据分析类Web应用</li>
<li><a href="https://github.com/modin-project/modin">modin</a> pandas加速库，接口语法与pandas高度一致</li>
<li><a href="https://github.com/dask/dask">dask</a>  pandas加速库，接口语法与pandas高度一致</li>
<li><a href="https://github.com/has2k1/plydata%5D">plydata</a>  pandas管道语法库</li>
<li><a href="https://github.com/networkx/networkx">networkx</a> 社交网络分析库</li>
</ul>
<br>
<p>​</p>
<h2 id="机器学习">机器学习</h2>
<ul>
<li><a href="https://github.com/VowpalWabbit/vowpal_wabbit">vowpal wabbit</a>  机器学习的前沿库</li>
<li><a href="https://github.com/scikit-learn/scikit-learn">scikit-learn</a> 机器学习必学库，支持有监督、无监督多种算法，含文本分析功能</li>
<li><a href="https://github.com/biolab/orange3">Orange3</a> 点击操作的机器学习分析<strong>软件</strong>， 可文本分析</li>
<li><a href="https://github.com/doccano/doccano">doccano</a> 文本数据标注工具</li>
<li><a href="https://github.com/heartexlabs/label-studio">label-studio</a>  最牛掰的文本数据标注工具</li>
</ul>
<h2 id="可视化">可视化</h2>
<ul>
<li><a href="https://github.com/streamlit/streamlit">streamlit</a> 快速搭建本地数据分析类Web应用</li>
<li><a href="https://github.com/matplotlib/matplotlib">matplotlib</a>  Python中最万能绘图库，很少有ta画不出来的图；但语法较难、静态图</li>
<li><a href="https://github.com/nschloe/matplotx">matplotx</a>  Matplotlib扩展库，可以提供更多样式，简化样式设定</li>
<li><a href="https://github.com/mwaskom/seaborn">seaborn</a> 基于matplotlib开发的简化版可视化库， 一般的图可以用ta绘制； 高度定制仍需要结合matplotlib进行样式定制；静态图</li>
<li><a href="https://github.com/has2k1/plotnine">plotnine</a>  ggplot2语法的Python可视化库， 可与<a href="https://github.com/has2k1/plydata%5D">plydata</a> 库结合使用</li>
<li><a href="https://github.com/pyecharts/pyecharts">pyecharts</a> 国人开发并封装的动态可视化图绘制库; 中文文档</li>
<li><a href="https://github.com/plotly/plotly.py">plotly</a> 动态可视化图绘制库</li>
<li><a href="https://github.com/bokeh/bokeh">bokeh</a> 动态可视化图绘制库</li>
<li><a href="https://github.com/garrettj403/SciencePlots">SciencePlots</a>  科研论文绘图，基于matplotlib</li>
<li><a href="https://github.com/datapane/datapane">datapane</a>  数据分析报告生成</li>
<li><a href="https://github.com/apache/superset">superset</a> 开源商务智能分析可视化库</li>
<li><a href="https://github.com/alfonsosemeraro/pyplutchik">pyplutchik</a> 文本可视化，可将文本情感信息按照plutchik轮样式可视化</li>
</ul>
<br> 
<h2 id="文本分析">文本分析</h2>
<ul>
<li><a href="https://github.com/nltk/nltk">nltk</a> 自然语言分析套件，对中文不友好</li>
<li><a href="https://github.com/shaypal5/skift">skift</a>  使用scikit-learn语法封装了fastText功能的包。</li>
<li><a href="https://github.com/andrewtavis/kwx">kwx</a>  Python 中基于 BERT、LDA 和 TFIDF 的关键字提取</li>
<li><a href="https://github.com/explosion/spaCy">spacy</a> 工业级自然语言模型库，支持中文</li>
<li><a href="https://github.com/fxsjy/jieba">jieba</a>  中文文本分词库</li>
<li><a href="https://github.com/isnowfy/snownlp">snownlp</a> 中文情感分析库</li>
<li><a href="https://github.com/RaRe-Technologies/gensim">gensim</a>  最好用、最全的话题模型</li>
<li><a href="https://github.com/hidadeng/cntext">cntext</a> 中文文本分析库，含词频统计、情感分析、可视化</li>
<li><a href="https://github.com/heartexlabs/label-studio">label-studio</a>  最牛掰的文本数据标注工具</li>
<li><a href="https://github.com/doccano/doccano">doccano</a> 文本数据标注工具</li>
<li><a href="https://github.com/shivam5992/textstat">textstat</a>  文本可读性计算包(算法全，但仅支持英文)</li>
<li><a href="https://github.com/jbesomi/texthero">texthero</a> 文本预处理、展示、可视化库，仅支持英文</li>
<li><a href="https://github.com/textpipe/textpipe">textpipe</a> 文本分析流水线</li>
<li><a href="https://github.com/davidmcclure/textplot">textplot</a> 词语网络图</li>
<li><a href="https://github.com/ryanjgallagher/shifterator">shifterator</a> 通过让您查看单词使用方式的变化，单词移位可以帮助您进行从根本上更可解释的情感，熵和散度分析。量化不同单词对两个文本差异做出的贡献，以及它们如何发挥作用。</li>
<li><a href="https://github.com/vi3k6i5/GuidedLDA">GuidedLDA</a> 半监督LDA主题模型</li>
<li><a href="https://github.com/gregversteeg/corex_topic">corex_topic</a>  层次非监督、半监督话题模型</li>
<li><a href="https://github.com/MaartenGr/BERTopic">BERTopic</a> BERT话题模型</li>
<li><a href="https://github.com/RasaHQ/whatlies">whatlies</a>  词向量可视化</li>
<li><a href="https://github.com/HLasse/TextDescriptives">TextDescriptives</a> 文本描述性统计,不支持中文</li>
<li><a href="https://github.com/hidadeng/pdfdocx">pdfdocx</a> pdf、docx读取库</li>
<li><a href="https://github.com/ocrmypdf/OCRmyPDF">OCRmyPDF</a>    为<strong>扫描的 PDF</strong> 文件添加了 OCR 文本层，允许对其进行搜索</li>
<li><a href="https://github.com/ddangelov/Top2Vec">Top2Vec</a>  主题建模和语义搜索的算法, 自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。 适用于短文本;</li>
<li><a href="https://github.com/jboynyc/textnets">TextNet</a>  textnet将文档集表示为文档和单词的网络,为文本分析与可视化提供了新的可能性。</li>
<li><a href="https://github.com/remram44/taguette">taguette</a> 免费开源的定性研究工具</li>
</ul>
<br>
<h2 id="gui窗体软件开发">GUI窗体软件开发</h2>
<ul>
<li><a href="https://wiki.python.org/moin/TkInter">tkinter</a> Python内置的gui库</li>
<li><a href="https://github.com/PySimpleGUI/PySimpleGUI">PySimpleGUI</a> 最简单的gui开发库</li>
<li><a href="https://doc.qt.io/qt.html#qtforpython">pyqt5、pyside</a> 最牛掰的gui软件开发库</li>
<li><a href="https://github.com/hoffstadt/DearPyGui">DearPyGui</a>  易于使用且功能强大的Python GUI框架，它提供了DearImGui的包装。</li>
<li><a href="https://github.com/pywebio/PyWebIO">PyWebIO</a>  快速构建 Web 应用的 Python 工具</li>
<li><a href="https://github.com/kivy/kivy">kivy</a>  star数高达14k的gui库
<br></li>
</ul>
<h2 id="自动化办公">自动化办公</h2>
<ul>
<li><a href="https://github.com/zhangyunhao116/zmail">zmail</a> 自动化收发邮件管理库</li>
<li><a href="https://github.com/pywinauto/pywinauto">pywinauto</a> Windows电脑自动化Python库</li>
<li><a href="https://github.com/Kozea/WeasyPrint">WeasyPrint</a>  自动化生产pdf报告</li>
<li><a href="https://github.com/jorisschellekens/ptext-release"></a> 对PDF文件读取、更改、添加信息</li>
<li><a href="https://github.com/SeleniumHQ/selenium/">selenium</a> 浏览器自动化框架，可以自动化点击浏览器，完成某些工作</li>
<li><a href="https://github.com/mkdocs/mkdocs/">mkdocx</a></li>
<li><a href="https://github.com/python-openxml/python-docx">python-docx</a>  创建、修改docx文件库</li>
<li><a href="https://github.com/scanny/python-pptx">python-ppt</a>  创建、修改ppt文件库</li>
<li><a href="https://openpyxl.readthedocs.io/en/stable/">openpyxl</a> xlsx文件库</li>
<li><a href="https://github.com/wang0618/PyWebIO">PyWebIO</a>  不需要编写HTML和JS代码，就可以构建简单的基于浏览器的GUI应用。</li>
</ul>
<br>
<h2 id="其他">其他</h2>
<ul>
<li><a href="https://github.com/hiresearch/hiresearch.github.io">hiresearch</a> 丢弃繁杂收藏夹，定义简洁办公的浏览器首页</li>
<li><a href="https://github.com/hakimel/reveal.js">reveal.js</a> 最流行的幻灯片</li>
<li><a href="https://github.com/slidevjs/slidev">slidev</a> 编程人员使用的幻灯片</li>
<li><a href="https://github.com/mkdocs/mkdocs">mkdocs</a> 制作文档网站</li>
<li><a href="https://github.com/mockoon/cli">mockoon</a>  帮我们快速搭建 API 服务图形化界面工具</li>
<li><a href="https://www.codepng.app/">codepng</a>  把代码转为美观的截图的website</li>
<li><a href="https://github.com/amphibian-dev/toad">toad</a> 金融风险评分卡；覆盖了建模全流程，从 EDA、特征工程、特征筛选 到 模型验证和评分卡转化</li>
<li><a href="https://github.com/salomonelli/best-resume-ever">best-resume-ever</a> Latex项目， 基于 Web 的简历模板，可以生成网页简历，然后用浏览器打印成 PDF 文件。</li>
<li><a href="https://github.com/vivjay30/pychorus">pychorus</a> 将音频文件中的高潮部分剪辑出来的python包</li>
<li><a href="https://github.com/imageio/imageio">imageio</a>  用于读取和写入图像数据的 Python 库；</li>
<li><a href="https://github.com/Textualize/rich">rich</a> 让命令行输出更美观简洁的Python包</li>
<li><a href="https://github.com/Textualize/textual">textual</a> rich作者开发的<strong>文本用户界面</strong>用户</li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>案例实战 | 企业信息数据采集</title>
      <link>https://hidadeng.github.io/blog/%E7%88%B1%E4%BC%81%E6%9F%A5/</link>
      <pubDate>Wed, 08 Dec 2021 18:42:10 +0600</pubDate>
      
      <guid>/blog/%E7%88%B1%E4%BC%81%E6%9F%A5/</guid>
      <description>手把手教你写爬虫</description>
      <content:encoded><![CDATA[<h2 id="爱企查">爱企查</h2>
<p>想搜集企业信息，可以使用爱企查网站，例如通过该网站，搜”华为“，，可以获得与关键词<strong>华为</strong>相关的很多企业名信息

<figure >
    
        <img src="img/01-%e7%88%b1%e4%bc%81%e6%9f%a5%e9%a6%96%e9%a1%b5.png" width="100%" />
    
    
</figure>
</p>
<p>设计网络爬虫步骤</p>
<ol>
<li>使用开发者工具network面板审查网站的网址规律urls</li>
<li>对单个网址url尝试访问</li>
<li>确定网站是html或json类型</li>
<li>从网页中解析定位需要的数据。</li>
</ol>
<ul>
<li>使用pyquery解析html页面数据；</li>
<li>或使用json解析json页面数据</li>
</ul>
<ol start="5">
<li>存储到csv</li>
<li>重复2-5</li>
</ol>
<br>
<h2 id="尝试访问第一页">尝试访问第一页</h2>

<figure >
    
        <img src="img/02-%e7%bd%91%e5%9d%80%e8%a7%84%e5%be%8b.png" width="100%" />
    
    
</figure>

<p>经过开发者工具network，可以使用requests对其进行访问。</p>
<p>需要注意的是，headers中需要加入Referer参数，该参数作用是告诉服务器</p>
<blockquote>
<p>兄弟，我是经过Referer介绍的，不然我也不可能知道
<a href="https://aiqicha.baidu.com/s/advanceFilterAjax?q=%E5%8D%8E%E4%B8%BA&amp;t=&amp;p=1&amp;s=10&amp;o=0&amp;f=%7B%7D">https://aiqicha.baidu.com/s/advanceFilterAjax?q=%E5%8D%8E%E4%B8%BA&amp;t=&amp;p=1&amp;s=10&amp;o=0&amp;f=%7B%7D</a>
这个网址啊</p>
</blockquote>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="kn">import</span> <span class="n">quote</span>

<span class="n">query</span> <span class="o">=</span> <span class="s1">&#39;华为&#39;</span>

<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://aiqicha.baidu.com/s/advanceFilterAjax?q=</span><span class="si">{q}</span><span class="s1">&amp;t=&amp;p=1&amp;s=10&amp;o=0&amp;f=%7B%7D&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">quote</span><span class="p">(</span><span class="n">query</span><span class="p">))</span>

<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Referer&#39;</span><span class="p">:</span> <span class="s1">&#39;https://aiqicha.baidu.com/s?q=</span><span class="si">{q}</span><span class="s1">&amp;t=0&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">quote</span><span class="p">(</span><span class="n">query</span><span class="p">))}</span>

<span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>
<span class="n">resp</span>
</code></pre></div><pre><code>&lt;Response [200]&gt;
</code></pre>
<br>
<h2 id="解析网页数据">解析网页数据</h2>
<p>
<figure >
    
        <img src="img/03-%e8%a7%a3%e6%9e%90%e6%95%b0%e6%8d%ae.png" width="100%" />
    
    
</figure>

通过开发者工具可以看到，这个网站采用的json类型网页数据。</p>
<p>
<figure >
    
        <img src="img/04-%e6%95%b0%e6%8d%ae%e5%ad%97%e6%ae%b5.png" width="100%" />
    
    
</figure>

好在这类网页的解析定位比较简单。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">com</span> <span class="ow">in</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;resultList&#39;</span><span class="p">][:</span><span class="mi">3</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">com</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    {&#39;pid&#39;: &#39;31360200662522&#39;, &#39;entName&#39;: &#39;&lt;em&gt;华为&lt;/em&gt;技术有限公司&#39;, &#39;entType&#39;: &#39;有限责任公司(自然人投资或控股的法人独资)&#39;, &#39;validityFrom&#39;: &#39;1896-08-14&#39;, &#39;domicile&#39;: &#39;深圳市龙岗区坂田&lt;em&gt;华为&lt;/em&gt;总部办公楼&#39;, &#39;entLogo&#39;: &#39;https://zhengxin-pub.cdn.bcebos.com/logopic/67e739bf0e47768f4a1f542daf3f7f42_fullsize.jpg&#39;, &#39;openStatus&#39;: &#39;开业&#39;, &#39;legalPerson&#39;: &#39;赵明路&#39;, &#39;tags&#39;: {&#39;laTaxer&#39;: &#39;&lt;span class=&#34;zx-ent-tag laTaxer&#34;&gt;A级纳税人(2020)&lt;/span&gt;&#39;}, &#39;logoWord&#39;: &#39;华为技术&#39;, &#39;titleName&#39;: &#39;华为技术有限公司&#39;, &#39;titleLegal&#39;: &#39;赵明路&#39;, &#39;titleDomicile&#39;: &#39;深圳市龙岗区坂田华为总部办公楼&#39;, &#39;levelAtaxer&#39;: [2020, 2018, 2019, 2016, 2017, 2014, 2015], &#39;regCap&#39;: &#39;5,035,113.2万&#39;, &#39;scope&#39;: &#39;一般经营项目是:程控交换机、传输设备、数据通信设备、宽带多媒体设备、电源、无线通信设备、微电子产品、软件、系统集成工程、计算机及配套设备、终端设备及相关通信信息产品、数据中心机房基础设施及配套产品(含供配电、空调制冷设备、智能管理监控等)的开发、生产、销售、技术服务、工程安装、维修、咨询、代理、租赁;信息系统设计、集成、运行维护;集成电路设计、研发;统一通信及协作类产品,服务器及配套软硬件产品,存储设备及相关软件的研发、生产、销售;无线数据产品(不含限制项目)的研发、生产、销售;通信站点机房基础设施及通信配套设备(含通信站点、通信机房、通信电源、机柜、天线、通信线缆、配电、智能管理监控、锂电及储能系统等)的研发、生产、销售;能源科学技术研究及能源相关产品的研发、生产、销售;大数据产品、物联网及通信相关领域产品的研发、生产、销售;汽车零部件及智能系统的研发、生产、销售及服务;建筑工程;设计、制作、发布、代理各类广告;通信设备租赁(不含限制项目);培训服务;技术认证服务;信息咨询(不含限制项目);企业管理咨询(不含限制项目);进出口业务;国内商业、物资供销业业务(不含专营、专控、专卖商品);对外经济技术合作业务;房屋租赁业务(持许可经营证);以及其他法律法规不禁止的经营活动(依法须经批准的项目,经相关部门批准后方可开展经营活动)。,许可经营项目是:增值电信业务经营。&#39;, &#39;regNo&#39;: &#39;815503001822039217&#39;, &#39;hitReason&#39;: [{&#39;品牌项目&#39;: &#39;&lt;em&gt;华为&lt;/em&gt;&#39;}, {&#39;商标名称&#39;: &#39;&lt;em&gt;华为&lt;/em&gt;&#39;}, {&#39;企业名称&#39;: &#39;&lt;em&gt;华为&lt;/em&gt;技术有限公司&#39;}, {&#39;网站名称&#39;: &#39;&lt;em&gt;华为&lt;/em&gt;应用平台1&#39;}, {&#39;地址&#39;: &#39;深圳市龙岗区坂田&lt;em&gt;华为&lt;/em&gt;总部办公楼&#39;}], &#39;labels&#39;: {&#39;opening&#39;: {&#39;text&#39;: &#39;开业&#39;, &#39;style&#39;: &#39;blue&#39;, &#39;fontColor&#39;: &#39;#1EA930&#39;, &#39;bgColor&#39;: &#39;#EBF6EC&#39;}}, &#39;personTitle&#39;: &#39;法定代表人&#39;, &#39;personId&#39;: &#39;a9f275934f59110096757b656ba41382&#39;}
    
    {&#39;pid&#39;: &#39;28610144220343&#39;, &#39;entName&#39;: &#39;&lt;em&gt;华为&lt;/em&gt;终端(深圳)有限公司&#39;, &#39;entType&#39;: &#39;有限责任公司(外商投资、非独资)&#39;, &#39;validityFrom&#39;: &#39;2003-12-25&#39;, &#39;domicile&#39;: &#39;深圳市龙岗区坂田&lt;em&gt;华为&lt;/em&gt;基地B区2号楼&#39;, &#39;entLogo&#39;: &#39;https://zhengxin-pub.cdn.bcebos.com/logopic/a638462a7a48ab79f84b7db2c0e64230_fullsize.jpg&#39;, &#39;openStatus&#39;: &#39;开业&#39;, &#39;legalPerson&#39;: &#39;赵明路&#39;, &#39;tags&#39;: {&#39;laTaxer&#39;: &#39;&lt;span class=&#34;zx-ent-tag laTaxer&#34;&gt;A级纳税人(2020)&lt;/span&gt;&#39;}, &#39;logoWord&#39;: &#39;华为终端&#39;, &#39;titleName&#39;: &#39;华为终端(深圳)有限公司&#39;, &#39;titleLegal&#39;: &#39;赵明路&#39;, &#39;titleDomicile&#39;: &#39;深圳市龙岗区坂田华为基地B区2号楼&#39;, &#39;levelAtaxer&#39;: [2020, 2018, 2019, 2016, 2017, 2014, 2015], &#39;regCap&#39;: &#39;1,598,080.8万&#39;, &#39;scope&#39;: &#39;一般经营项目是：开发、生产、销售通信电子产品及配套产品，并提供技术咨询和售后服务。进出口业务（不含分销)。，许可经营项目是：&#39;, &#39;regNo&#39;: &#39;815503006447640305&#39;, &#39;hitReason&#39;: [{&#39;企业名称&#39;: &#39;&lt;em&gt;华为&lt;/em&gt;终端(深圳)有限公司&#39;}, {&#39;网站名称&#39;: &#39;&lt;em&gt;华为&lt;/em&gt;HARMONYOS网站&#39;}, {&#39;地址&#39;: &#39;深圳市龙岗区坂田&lt;em&gt;华为&lt;/em&gt;基地B区2号楼&#39;}], &#39;labels&#39;: {&#39;opening&#39;: {&#39;text&#39;: &#39;开业&#39;, &#39;style&#39;: &#39;blue&#39;, &#39;fontColor&#39;: &#39;#1EA930&#39;, &#39;bgColor&#39;: &#39;#EBF6EC&#39;}}, &#39;personTitle&#39;: &#39;法定代表人&#39;, &#39;personId&#39;: &#39;a9f275934f59110096757b656ba41382&#39;}
    
    {&#39;pid&#39;: &#39;30140456955334&#39;, &#39;entName&#39;: &#39;&lt;em&gt;华为&lt;/em&gt;终端有限公司&#39;, &#39;entType&#39;: &#39;有限责任公司(外商投资企业法人独资)&#39;, &#39;validityFrom&#39;: &#39;2012-11-23&#39;, &#39;domicile&#39;: &#39;广东省东莞市松山湖园区新城路2号&#39;, &#39;entLogo&#39;: &#39;https://zhengxin-pub.cdn.bcebos.com/logopic/cc662a5d573b793e9c5b84031350ced0_fullsize.jpg&#39;, &#39;openStatus&#39;: &#39;开业&#39;, &#39;legalPerson&#39;: &#39;赵明路&#39;, &#39;tags&#39;: {&#39;laTaxer&#39;: &#39;&lt;span class=&#34;zx-ent-tag laTaxer&#34;&gt;A级纳税人(2020)&lt;/span&gt;&#39;}, &#39;logoWord&#39;: &#39;华为终端&#39;, &#39;titleName&#39;: &#39;华为终端有限公司&#39;, &#39;titleLegal&#39;: &#39;赵明路&#39;, &#39;titleDomicile&#39;: &#39;广东省东莞市松山湖园区新城路2号&#39;, &#39;levelAtaxer&#39;: [2020, 2018, 2019, 2016, 2017, 2014, 2015], &#39;regCap&#39;: &#39;70,000.0万&#39;, &#39;scope&#39;: &#39;开发、生产、销售：通信及电子产品、计算机、卫星电视接收天线、高频头、数字卫星电视接收机及前述产品的配套产品，并提供技术咨询和售后服务；开发、生产、销售：医疗器械（第一类、第二类、第三类医疗器械），并提供技术咨询和售后服务；增值电信业务经营；佣金代理；货物或技术进出口（国家禁止或涉及行政审批的货物和技术进出口除外）。(依法须经批准的项目，经相关部门批准后方可开展经营活动)&#39;, &#39;regNo&#39;: &#39;815518000494355853&#39;, &#39;hitReason&#39;: [{&#39;企业名称&#39;: &#39;&lt;em&gt;华为&lt;/em&gt;终端有限公司&#39;}], &#39;labels&#39;: {&#39;opening&#39;: {&#39;text&#39;: &#39;开业&#39;, &#39;style&#39;: &#39;blue&#39;, &#39;fontColor&#39;: &#39;#1EA930&#39;, &#39;bgColor&#39;: &#39;#EBF6EC&#39;}}, &#39;personTitle&#39;: &#39;法定代表人&#39;, &#39;personId&#39;: &#39;a9f275934f59110096757b656ba41382&#39;}
</code></pre></div><br>
<h2 id="完整爬虫">完整爬虫</h2>
<p>经过刚刚的几个步骤，我们现在只需要</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">query</span> <span class="o">=</span> <span class="s1">&#39;华为&#39;</span>  
<span class="n">max_pages</span> <span class="o">=</span> <span class="mi">10</span>   <span class="c1">#获取前10页的企业信息数据</span>

<span class="c1">#存储数据</span>
<span class="n">csvf</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;企业信息.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;titleName&#39;</span><span class="p">,</span> <span class="s1">&#39;titleDomicile&#39;</span><span class="p">,</span> <span class="s1">&#39;titleLegal&#39;</span><span class="p">,</span> <span class="s1">&#39;validityFrom&#39;</span><span class="p">,</span> <span class="s1">&#39;regCap&#39;</span><span class="p">,</span> <span class="s1">&#39;regNo&#39;</span><span class="p">,</span> <span class="s1">&#39;scope&#39;</span><span class="p">]</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
<span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

<span class="c1">#访问</span>
<span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_pages</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://aiqicha.baidu.com/s/advanceFilterAjax?q=</span><span class="si">{q}</span><span class="s1">&amp;t=&amp;p=</span><span class="si">{p}</span><span class="s1">&amp;s=10&amp;o=0&amp;f=%7B%7D&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">quote</span><span class="p">(</span><span class="n">query</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">page</span><span class="p">)</span>

    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Referer&#39;</span><span class="p">:</span> <span class="s1">&#39;https://aiqicha.baidu.com/s?q=</span><span class="si">{q}</span><span class="s1">&amp;t=0&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="n">quote</span><span class="p">(</span><span class="n">query</span><span class="p">))}</span>

    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>
    
    <span class="c1">#解析数据</span>
    <span class="k">for</span> <span class="n">com</span> <span class="ow">in</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;resultList&#39;</span><span class="p">]:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">fieldname</span> <span class="ow">in</span> <span class="n">fieldnames</span><span class="p">:</span>
            <span class="n">data</span><span class="p">[</span><span class="n">fieldname</span><span class="p">]</span> <span class="o">=</span> <span class="n">com</span><span class="p">[</span><span class="n">fieldname</span><span class="p">]</span>
        <span class="c1">#写入csv</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        

<span class="n">csvf</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div><br>
<h2 id="运行结果">运行结果</h2>
<p>采集10页的爬虫运行结束后，尝试读取 <strong>企业信息.csv</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;企业信息.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/05-data.png" width="100%" />
    
    
</figure>

<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用Python采集多媒体文件数据</title>
      <link>https://hidadeng.github.io/blog/multi_media_examples/</link>
      <pubDate>Fri, 26 Nov 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/multi_media_examples/</guid>
      <description>使用Python对多媒体文件进行下载</description>
      <content:encoded><![CDATA[<p>多媒体文件的下载包括</p>
<ul>
<li>图片</li>
<li>音频</li>
<li>视频</li>
<li>文件</li>
</ul>
<p>代码非常简单，只要准备好多媒体文件链接url和存储路径file即可，代码如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>


<span class="k">def</span> <span class="nf">download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    下载多媒体及文件
</span><span class="s2">    url： 多媒体文件链接（结尾有文件格式名）
</span><span class="s2">    file: 存储文件的路径（结尾有文件格式名）
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="c1">#获取到二进制数据</span>
    <span class="n">binarydata</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">content</span>
    <span class="c1">#以二进制形式将数据流存入fname中</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">binarydata</span><span class="p">)</span>  
</code></pre></div><h2 id="案例数据">案例数据</h2>
<p>视频由于体积太大未能上传至我的博客服务器，我为大家准备了音频、pdf文件、图片文件三种数据类型。其实不论什么类型，只要是文件，均可使用上面的download函数下载。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pics</span>  <span class="o">=</span>  <span class="p">[</span><span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pic/about-page.jpg&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pic/about-us.jpg&#39;</span><span class="p">,</span>
               <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pic/AI_Knowledge_intro.png&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pic/banner-1.jpg&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pic/blog-post-1.jpg&#39;</span><span class="p">,</span>
               <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pic/blog-post-2.jpg&#39;</span><span class="p">,</span>
                <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pic/blog-post-3.jpg&#39;</span><span class="p">]</span>

<span class="n">pdfs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/网络爬虫.pdf&#39;</span><span class="p">,</span>
           <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/相似度.pdf&#39;</span><span class="p">,</span>
           <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/中文可读性.pdf&#39;</span><span class="p">]</span>


<span class="n">musics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/music/Elastic.mp3&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/music/HAPPYBEL.WAV&#39;</span><span class="p">,</span>
               <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/music/MU14.WAV&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/music/typing.wav&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/music/Water.mp3&#39;</span><span class="p">]</span>
</code></pre></div><br>
<h2 id="批量下载">批量下载</h2>
<p>以pdf为例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>

<span class="n">pdfs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/网络爬虫.pdf&#39;</span><span class="p">,</span>
           <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/相似度.pdf&#39;</span><span class="p">,</span>
           <span class="s1">&#39;https://hidadeng.github.io/blog/multimediaexamples/pdf/中文可读性.pdf&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">binarydata</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">content</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">binarydata</span><span class="p">)</span> 
        
<span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">pdfs</span><span class="p">:</span>
    <span class="c1">#注意，代码运行前所在文件夹内新建一个data文件夹</span>
    <span class="n">filepath</span><span class="o">=</span><span class="s1">&#39;data/&#39;</span><span class="o">+</span><span class="n">url</span><span class="o">.</span><span class="n">spit</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">link</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="n">filepath</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
