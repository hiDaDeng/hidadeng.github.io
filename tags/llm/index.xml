<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLM on 大邓和他的PYTHON</title>
    <link>/tags/llm/</link>
    <description>Recent content in LLM on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sun, 16 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>实验 | 使用本地大模型从文本中提取结构化信息</title>
      <link>https://textdata.cn/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/</link>
      <pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/</guid>
      <description>&lt;p&gt;非结构文本、图片、视频等数据是待挖掘的数据矿藏， 在经管、社科等研究领域中谁拥有了&lt;em&gt;&lt;strong&gt;从非结构提取结构化信息的能力&lt;/strong&gt;&lt;/em&gt;，谁就拥有科研上的数据优势。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一需求&#34;&gt;一、需求&lt;/h2&gt;
&lt;p&gt;现在有很多个电子发票PDF文件， 使用自动化工具帮我们批量自动从发票PDF提取出格式化信息。如从发票&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-raw-pdf.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;提取出DICT_DATA&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;DICT_DATA = {
    &amp;#34;开票日期&amp;#34;: &amp;#34;2023年01月06日&amp;#34;,
    &amp;#34;销售方纳税识别号&amp;#34;: &amp;#34;91110108MA01WFY0X6&amp;#34;,
    &amp;#34;销售方地址电话&amp;#34;: &amp;#34;北京市海淀区上地东路1号院4号楼2层221室 010-59928888&amp;#34;,
    &amp;#34;应税货物(或服务)名称&amp;#34;: &amp;#34;*信息技术服务*技术服务费&amp;#34;,
    &amp;#34;价税合计(大写)&amp;#34;: &amp;#34;&amp;#34;,
    &amp;#34;税率&amp;#34;: &amp;#34;6%&amp;#34;,
    &amp;#34;备注&amp;#34;: &amp;#34;230106163474406331&amp;#34;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二准备工作&#34;&gt;二、准备工作&lt;/h2&gt;
&lt;h3 id=&#34;21-安装ollama&#34;&gt;2.1 安装ollama&lt;/h3&gt;
&lt;p&gt;点击前往网站 &lt;a href=&#34;https://ollama.com/&#34;&gt;https://ollama.com/&lt;/a&gt; ，下载ollama软件，支持win、Mac、linux&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-ollama-gui.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-下载llm&#34;&gt;2.2 下载LLM&lt;/h3&gt;
&lt;p&gt;ollama软件目前支持多种大模型， 如阿里的（qwen、qwen2）、meta的(llama3)，&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-ollama-model.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;以llama3为例，根据自己电脑显存性能， 选择适宜的版本。如果不知道选什么，那就试着安装，不合适不能用再删除即可。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-ollama-llama3.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行模型下载(安装)命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ollama pull llama3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;等待 &lt;strong&gt;llama3:8b&lt;/strong&gt; 下载完成。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-安装python包&#34;&gt;2.3 安装python包&lt;/h3&gt;
&lt;p&gt;在python中调用ollama服务，需要ollama包。&lt;/p&gt;
&lt;p&gt;打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行安装命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install ollama
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;24-启动ollama服务&#34;&gt;2.4 启动ollama服务&lt;/h3&gt;
&lt;p&gt;在Python中调用本地ollama服务，需要先启动本地ollama服务， 打开电脑命令行cmd(mac是terminal), 执行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ollama serve
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2024/06/14 14:52:24 routes.go:1011: INFO server config env=&amp;#34;map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/Users/deng/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]&amp;#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:725 msg=&amp;#34;total blobs: 18&amp;#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:732 msg=&amp;#34;total unused blobs removed: 0&amp;#34;
time=2024-06-14T14:52:24.743+08:00 level=INFO source=routes.go:1057 msg=&amp;#34;Listening on 127.0.0.1:11434 (version 0.1.44)&amp;#34;
time=2024-06-14T14:52:24.744+08:00 level=INFO source=payload.go:30 msg=&amp;#34;extracting embedded files&amp;#34; dir=/var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/ollama4239159529/runners
time=2024-06-14T14:52:24.772+08:00 level=INFO source=payload.go:44 msg=&amp;#34;Dynamic LLM libraries [metal]&amp;#34;
time=2024-06-14T14:52:24.796+08:00 level=INFO source=types.go:71 msg=&amp;#34;inference compute&amp;#34; id=0 library=metal compute=&amp;#34;&amp;#34; driver=0.0 name=&amp;#34;&amp;#34; total=&amp;#34;72.0 GiB&amp;#34; available=&amp;#34;72.0 GiB&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;cmd(mac是terminal)看到如上的信息，说明本地ollama服务已开启。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三实验&#34;&gt;三、实验&lt;/h2&gt;
&lt;p&gt;点击下载&lt;a href=&#34;data.zip&#34;&gt;实验数据&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;31-读取pdf&#34;&gt;3.1 读取pdf&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-raw-pdf.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_pdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/1.pdf&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__version__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2.1.2

&amp;#39; 机器编号：499098321974北京增值税电子普通发票发票代码： 011002200911\n发票号码： 69453658\n开票日期： 2023年01月06日\n校 验 码： 11092 55849 13734 18748\n购\n买\n方名        称： 哈尔滨所以然信息技术有限公司\n密\n码\n区030898/5&amp;lt;32&amp;gt;*/0*440/63+79*08\n纳税人识别号： 91230109MABT7KBC4M /&amp;lt;54&amp;lt;1*6+49&amp;lt;-*+*&amp;gt;7&amp;lt;-8*04&amp;lt;+01\n地 址、电 话： 68+160026-45904*2&amp;lt;+3+15503&amp;gt;2\n开户行及账号： 98*2/*-*480145+-19*0917-1*61\n货物或应税劳务、服务名称 规格型号 单 位 数 量 单 价 金 额 税率 税 额\n*信息技术服务*技术服务费 1248.113208 248.11 6% 14.89\n合      计 ￥248.11 ￥14.89\n价税合计（大写）\n  贰佰陆拾叁元整             （小写）￥263.00\n销\n售\n方名        称： 北京度友科技有限公司\n备\n注230106163474406331\n纳税人识别号： 91110108MA01WFY0X6\n地 址、电 话： 北京市海淀区上地东路1号院4号楼2层221室 010-59928888\n开户行及账号： 招商银行股份有限公司北京双榆树支行110943531310301\n  收款人：段欣冉 复核：张会珍 开票人：赵金荣 销售方：（章）&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-提取信息&#34;&gt;3.2 提取信息&lt;/h3&gt;
&lt;p&gt;使用ollama服务中的大模型 &lt;em&gt;&lt;strong&gt;llama3:8b&lt;/strong&gt;&lt;/em&gt; , 需要大模型提示信息及数据。这是我实验里设计的提示信息prompt&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;提取TEXT中的关键信息，返回DICT_DATA, DICT_DATA为dict数据格式，所含关键词依次为&amp;#34;开票日期&amp;#34;, &amp;#34;销售方纳税识别号&amp;#34;, &amp;#34;销售方地址电话&amp;#34;, &amp;#34;应税货物(或服务)名称&amp;#34;, &amp;#34;价税合计(大写)&amp;#34;, &amp;#34;税率&amp;#34;, &amp;#34;备注&amp;#34;; 结果只显示DICT_DATA。 TEXT: {text}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ollama&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;llama3:8b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
      &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;提取TEXT中的关键信息，返回DICT_DATA, DICT_DATA为dict数据格式，所含关键词依次为&amp;#34;开票日期&amp;#34;, &amp;#34;销售方纳税识别号&amp;#34;, &amp;#34;销售方地址电话&amp;#34;, &amp;#34;应税货物(或服务)名称&amp;#34;, &amp;#34;价税合计(大写)&amp;#34;, &amp;#34;税率&amp;#34;, &amp;#34;备注&amp;#34;; 结果只显示DICT_DATA。 TEXT: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text1&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
      &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&amp;#39;DICT_DATA = {\n    &amp;#34;开票日期&amp;#34;: &amp;#34;2023年01月06日&amp;#34;,\n    &amp;#34;销售方纳税识别号&amp;#34;: &amp;#34;91110108MA01WFY0X6&amp;#34;,\n    &amp;#34;销售方地址电话&amp;#34;: &amp;#34;北京市海淀区上地东路1号院4号楼2层221室 010-59928888&amp;#34;,\n    &amp;#34;应税货物(或服务)名称&amp;#34;: &amp;#34;*信息技术服务*技术服务费&amp;#34;,\n    &amp;#34;价税合计(大写)&amp;#34;: &amp;#34;&amp;#34;,\n    &amp;#34;税率&amp;#34;: &amp;#34;6%&amp;#34;,\n    &amp;#34;备注&amp;#34;: &amp;#34;230106163474406331&amp;#34;\n}&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从运行结果看出， 大模型从发票PDF中准确提取到我们需要的信息，耗时大概10s。 需要注意，有时候大模型还会返回&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&amp;#39;Here is the extracted key information in dictionary format:\n\n```\n{\n    &amp;#34;开票日期&amp;#34;: &amp;#34;2023年01月06日&amp;#34;,\n    &amp;#34;销售方纳税识别号&amp;#34;: &amp;#34;91110108MA01WFY0X6&amp;#34;,\n    &amp;#34;销售方地址电话&amp;#34;: &amp;#34;北京市海淀区上地东路1号院4号楼2层221室 010-59928888&amp;#34;,\n    &amp;#34;应税货物(或服务)名称&amp;#34;: &amp;#34;*信息技术服务*技术服务费&amp;#34;,\n    &amp;#34;价税合计(大写)&amp;#34;: &amp;#34;贰佰陆拾叁元整&amp;#34;,\n    &amp;#34;税率&amp;#34;: &amp;#34;6%&amp;#34;,\n    &amp;#34;备注&amp;#34;: &amp;#34;230106163474406331&amp;#34;\n}\n```\n\nLet me know if you have any further requests! 😊&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;现在我们需要将 DICT_DATA 变为真正的字典数据&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;import re

result = response[&amp;#39;message&amp;#39;][&amp;#39;content&amp;#39;]
result = [r for r in re.split(&amp;#39;```|DICT_DATA = &amp;#39;, result) if &amp;#39;{&amp;#39; in r][0]

print(type(eval(result)))
print(eval(result))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&amp;lt;class &amp;#39;dict&amp;#39;&amp;gt;
{&amp;#39;开票日期&amp;#39;: &amp;#39;2023年01月06日&amp;#39;,
 &amp;#39;销售方纳税识别号&amp;#39;: &amp;#39;91110108MA01WFY0X6&amp;#39;,
 &amp;#39;销售方地址电话&amp;#39;: &amp;#39;北京市海淀区上地东路1号院4号楼2层221室 010-59928888&amp;#39;,
 &amp;#39;应税货物(或服务)名称&amp;#39;: &amp;#39;*信息技术服务*技术服务费&amp;#39;,
 &amp;#39;价税合计(大写)&amp;#39;: &amp;#39;贰佰陆拾叁元整&amp;#39;,
 &amp;#39;税率&amp;#39;: &amp;#39;6%&amp;#39;,
 &amp;#39;备注&amp;#39;: &amp;#39;230106163474406331&amp;#39;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;33-封装成函数extract_info&#34;&gt;3.3 封装成函数extract_info&lt;/h3&gt;
&lt;p&gt;实验成功，我们将其封装为函数&lt;em&gt;&lt;strong&gt;extract_info&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ollama&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;extract_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;llama3:8b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stream&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
          &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
            &lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;提取TEXT中的关键信息，返回DICT_DATA, DICT_DATA为dict数据格式，所含关键词依次为&amp;#34;开票日期&amp;#34;, &amp;#34;销售方纳税识别号&amp;#34;, &amp;#34;销售方地址电话&amp;#34;, &amp;#34;应税货物(或服务)名称&amp;#34;, &amp;#34;价税合计(大写)&amp;#34;, &amp;#34;税率&amp;#34;, &amp;#34;备注&amp;#34;; 结果只显示DICT_DATA。 TEXT: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
          &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;r&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;r&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;```|DICT_DATA = &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;{&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;eval&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
  
  

&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;extract_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;开票日期&amp;#39;: &amp;#39;2023 02 14&amp;#39;,
 &amp;#39;销售方纳税识别号&amp;#39;: &amp;#39;91230109MABT7KBC4M&amp;#39;,
 &amp;#39;销售方地址电话&amp;#39;: &amp;#39;北京市北京经济技术开发区科创十一街18号院C座2层215室 62648622&amp;#39;,
 &amp;#39;应税货物(或服务)名称&amp;#39;: &amp;#39;*家用厨房电器具*米家 小米电热水 MJDSH03YM&amp;#39;,
 &amp;#39;价税合计(大写)&amp;#39;: &amp;#39;壹佰贰拾叁圆玖角玖分&amp;#39;,
 &amp;#39;税率&amp;#39;: &amp;#39;13%&amp;#39;,
 &amp;#39;备注&amp;#39;: None}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;34-批量提取&#34;&gt;3.4 批量提取&lt;/h3&gt;
&lt;p&gt;对data文件夹进行批量信息提取，结果存储为csv。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;

&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#cntext版本为2.1.2，非开源，&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#需联系大邓372335839获取&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#当前代码所在的代码文件与data文件夹处于同一个文件夹内&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#获取data内所有pdf的路径&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pdf_files&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;file&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.pdf&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;dict_datas&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pdf_file&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pdf_files&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pdf_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_pdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pdf_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dict_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;extract_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pdf_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dict_datas&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dict_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dict_datas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;CPU times: user 32 ms, sys: 3.17 ms, total: 35.2 ms
Wall time: 11.8 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四讨论&#34;&gt;四、讨论&lt;/h2&gt;
&lt;p&gt;这三张发票， 识别错误字段集中在销售方纳税识别号。原因主要是ct.read_pdf读入pdf时，文本比较杂乱。 对大模型的语义理解有一定的挑战。目前大模型已经支持文本、图片、音频、视频、网址， 所以各位看官，不用等太久，就可克服此问题。&lt;/p&gt;
&lt;p&gt;大模型会对每个输入，给出正确概率最大的回答，因此大模型提取数据时存在一定的错误识别风险。为降低该风险，尽量选择特别特殊、显眼，例如三张发票的&lt;strong&gt;价税合计(大写)&lt;/strong&gt;,  因为信息是特殊的中文大写数字， 在所有文本中是最醒目最特别的文本信息，这样大模型处理这类信息时会给这类信息尽可能高的权重，增大回答的准确率。&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;精选内容&#34;&gt;精选内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/datasets_available_for_management_science/&#34;&gt;LIST | 可供社科(经管)领域使用的数据集汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/the_text_analysis_list_about_ms/&#34;&gt;LIST | 社科(经管)数据挖掘文献资料汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-06-16-scrapegraph-ai/&#34;&gt;网络爬虫 | 使用scrapegraph-ai(大模型方案)自动采集网页数据&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/&#34;&gt;推荐 | 文本分析库cntext2.x使用手册&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>非结构文本、图片、视频等数据是待挖掘的数据矿藏， 在经管、社科等研究领域中谁拥有了<em><strong>从非结构提取结构化信息的能力</strong></em>，谁就拥有科研上的数据优势。</p>
<p><br><br></p>
<h2 id="一需求">一、需求</h2>
<p>现在有很多个电子发票PDF文件， 使用自动化工具帮我们批量自动从发票PDF提取出格式化信息。如从发票</p>
<p><img loading="lazy" src="img/01-raw-pdf.png" alt=""  />
</p>
<p>提取出DICT_DATA</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">DICT_DATA = {
    &#34;开票日期&#34;: &#34;2023年01月06日&#34;,
    &#34;销售方纳税识别号&#34;: &#34;91110108MA01WFY0X6&#34;,
    &#34;销售方地址电话&#34;: &#34;北京市海淀区上地东路1号院4号楼2层221室 010-59928888&#34;,
    &#34;应税货物(或服务)名称&#34;: &#34;*信息技术服务*技术服务费&#34;,
    &#34;价税合计(大写)&#34;: &#34;&#34;,
    &#34;税率&#34;: &#34;6%&#34;,
    &#34;备注&#34;: &#34;230106163474406331&#34;
}
</code></pre></div><p><br><br></p>
<h2 id="二准备工作">二、准备工作</h2>
<h3 id="21-安装ollama">2.1 安装ollama</h3>
<p>点击前往网站 <a href="https://ollama.com/">https://ollama.com/</a> ，下载ollama软件，支持win、Mac、linux</p>
<p><img loading="lazy" src="img/02-ollama-gui.png" alt=""  />
</p>
<br>
<h3 id="22-下载llm">2.2 下载LLM</h3>
<p>ollama软件目前支持多种大模型， 如阿里的（qwen、qwen2）、meta的(llama3)，</p>
<p><img loading="lazy" src="img/03-ollama-model.png" alt=""  />
</p>
<br>
<p>以llama3为例，根据自己电脑显存性能， 选择适宜的版本。如果不知道选什么，那就试着安装，不合适不能用再删除即可。</p>
<p><img loading="lazy" src="img/04-ollama-llama3.png" alt=""  />
</p>
<br>
<p>打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行模型下载(安装)命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ollama pull llama3
</code></pre></div><p>等待 <strong>llama3:8b</strong> 下载完成。</p>
<br>
<h3 id="23-安装python包">2.3 安装python包</h3>
<p>在python中调用ollama服务，需要ollama包。</p>
<p>打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行安装命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install ollama
</code></pre></div><br>
<h3 id="24-启动ollama服务">2.4 启动ollama服务</h3>
<p>在Python中调用本地ollama服务，需要先启动本地ollama服务， 打开电脑命令行cmd(mac是terminal), 执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ollama serve
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2024/06/14 14:52:24 routes.go:1011: INFO server config env=&#34;map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/Users/deng/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]&#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:725 msg=&#34;total blobs: 18&#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:732 msg=&#34;total unused blobs removed: 0&#34;
time=2024-06-14T14:52:24.743+08:00 level=INFO source=routes.go:1057 msg=&#34;Listening on 127.0.0.1:11434 (version 0.1.44)&#34;
time=2024-06-14T14:52:24.744+08:00 level=INFO source=payload.go:30 msg=&#34;extracting embedded files&#34; dir=/var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/ollama4239159529/runners
time=2024-06-14T14:52:24.772+08:00 level=INFO source=payload.go:44 msg=&#34;Dynamic LLM libraries [metal]&#34;
time=2024-06-14T14:52:24.796+08:00 level=INFO source=types.go:71 msg=&#34;inference compute&#34; id=0 library=metal compute=&#34;&#34; driver=0.0 name=&#34;&#34; total=&#34;72.0 GiB&#34; available=&#34;72.0 GiB&#34;
</code></pre></div><p>cmd(mac是terminal)看到如上的信息，说明本地ollama服务已开启。</p>
<p><br><br></p>
<h2 id="三实验">三、实验</h2>
<p>点击下载<a href="data.zip">实验数据</a></p>
<h3 id="31-读取pdf">3.1 读取pdf</h3>
<p><img loading="lazy" src="img/01-raw-pdf.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>


<span class="n">text</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;data/1.pdf&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="n">text</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2.1.2

&#39; 机器编号：499098321974北京增值税电子普通发票发票代码： 011002200911\n发票号码： 69453658\n开票日期： 2023年01月06日\n校 验 码： 11092 55849 13734 18748\n购\n买\n方名        称： 哈尔滨所以然信息技术有限公司\n密\n码\n区030898/5&lt;32&gt;*/0*440/63+79*08\n纳税人识别号： 91230109MABT7KBC4M /&lt;54&lt;1*6+49&lt;-*+*&gt;7&lt;-8*04&lt;+01\n地 址、电 话： 68+160026-45904*2&lt;+3+15503&gt;2\n开户行及账号： 98*2/*-*480145+-19*0917-1*61\n货物或应税劳务、服务名称 规格型号 单 位 数 量 单 价 金 额 税率 税 额\n*信息技术服务*技术服务费 1248.113208 248.11 6% 14.89\n合      计 ￥248.11 ￥14.89\n价税合计（大写）\n  贰佰陆拾叁元整             （小写）￥263.00\n销\n售\n方名        称： 北京度友科技有限公司\n备\n注230106163474406331\n纳税人识别号： 91110108MA01WFY0X6\n地 址、电 话： 北京市海淀区上地东路1号院4号楼2层221室 010-59928888\n开户行及账号： 招商银行股份有限公司北京双榆树支行110943531310301\n  收款人：段欣冉 复核：张会珍 开票人：赵金荣 销售方：（章）&#39;
</code></pre></div><br>
<h3 id="32-提取信息">3.2 提取信息</h3>
<p>使用ollama服务中的大模型 <em><strong>llama3:8b</strong></em> , 需要大模型提示信息及数据。这是我实验里设计的提示信息prompt</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">提取TEXT中的关键信息，返回DICT_DATA, DICT_DATA为dict数据格式，所含关键词依次为&#34;开票日期&#34;, &#34;销售方纳税识别号&#34;, &#34;销售方地址电话&#34;, &#34;应税货物(或服务)名称&#34;, &#34;价税合计(大写)&#34;, &#34;税率&#34;, &#34;备注&#34;; 结果只显示DICT_DATA。 TEXT: {text}
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">ollama</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;llama3:8b&#39;</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
      <span class="p">{</span>
        <span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span>
        <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;提取TEXT中的关键信息，返回DICT_DATA, DICT_DATA为dict数据格式，所含关键词依次为&#34;开票日期&#34;, &#34;销售方纳税识别号&#34;, &#34;销售方地址电话&#34;, &#34;应税货物(或服务)名称&#34;, &#34;价税合计(大写)&#34;, &#34;税率&#34;, &#34;备注&#34;; 结果只显示DICT_DATA。 TEXT: </span><span class="si">{</span><span class="n">text1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
      <span class="p">},</span>
    <span class="p">])</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
<span class="n">result</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&#39;DICT_DATA = {\n    &#34;开票日期&#34;: &#34;2023年01月06日&#34;,\n    &#34;销售方纳税识别号&#34;: &#34;91110108MA01WFY0X6&#34;,\n    &#34;销售方地址电话&#34;: &#34;北京市海淀区上地东路1号院4号楼2层221室 010-59928888&#34;,\n    &#34;应税货物(或服务)名称&#34;: &#34;*信息技术服务*技术服务费&#34;,\n    &#34;价税合计(大写)&#34;: &#34;&#34;,\n    &#34;税率&#34;: &#34;6%&#34;,\n    &#34;备注&#34;: &#34;230106163474406331&#34;\n}&#39;
</code></pre></div><p>从运行结果看出， 大模型从发票PDF中准确提取到我们需要的信息，耗时大概10s。 需要注意，有时候大模型还会返回</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&#39;Here is the extracted key information in dictionary format:\n\n```\n{\n    &#34;开票日期&#34;: &#34;2023年01月06日&#34;,\n    &#34;销售方纳税识别号&#34;: &#34;91110108MA01WFY0X6&#34;,\n    &#34;销售方地址电话&#34;: &#34;北京市海淀区上地东路1号院4号楼2层221室 010-59928888&#34;,\n    &#34;应税货物(或服务)名称&#34;: &#34;*信息技术服务*技术服务费&#34;,\n    &#34;价税合计(大写)&#34;: &#34;贰佰陆拾叁元整&#34;,\n    &#34;税率&#34;: &#34;6%&#34;,\n    &#34;备注&#34;: &#34;230106163474406331&#34;\n}\n```\n\nLet me know if you have any further requests! 😊&#39;
</code></pre></div><br>
<p>现在我们需要将 DICT_DATA 变为真正的字典数据</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import re

result = response[&#39;message&#39;][&#39;content&#39;]
result = [r for r in re.split(&#39;```|DICT_DATA = &#39;, result) if &#39;{&#39; in r][0]

print(type(eval(result)))
print(eval(result))
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;class &#39;dict&#39;&gt;
{&#39;开票日期&#39;: &#39;2023年01月06日&#39;,
 &#39;销售方纳税识别号&#39;: &#39;91110108MA01WFY0X6&#39;,
 &#39;销售方地址电话&#39;: &#39;北京市海淀区上地东路1号院4号楼2层221室 010-59928888&#39;,
 &#39;应税货物(或服务)名称&#39;: &#39;*信息技术服务*技术服务费&#39;,
 &#39;价税合计(大写)&#39;: &#39;贰佰陆拾叁元整&#39;,
 &#39;税率&#39;: &#39;6%&#39;,
 &#39;备注&#39;: &#39;230106163474406331&#39;}
</code></pre></div><br>
<h3 id="33-封装成函数extract_info">3.3 封装成函数extract_info</h3>
<p>实验成功，我们将其封装为函数<em><strong>extract_info</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">ollama</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">extract_info</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;llama3:8b&#39;</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
          <span class="p">{</span>
            <span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span>
            <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;提取TEXT中的关键信息，返回DICT_DATA, DICT_DATA为dict数据格式，所含关键词依次为&#34;开票日期&#34;, &#34;销售方纳税识别号&#34;, &#34;销售方地址电话&#34;, &#34;应税货物(或服务)名称&#34;, &#34;价税合计(大写)&#34;, &#34;税率&#34;, &#34;备注&#34;; 结果只显示DICT_DATA。 TEXT: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
          <span class="p">},</span>
        <span class="p">])</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">r</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;```|DICT_DATA = &#39;</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;{&#39;</span> <span class="ow">in</span> <span class="n">r</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">eval</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
  
  

<span class="n">result</span> <span class="o">=</span> <span class="n">extract_info</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">result</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;开票日期&#39;: &#39;2023 02 14&#39;,
 &#39;销售方纳税识别号&#39;: &#39;91230109MABT7KBC4M&#39;,
 &#39;销售方地址电话&#39;: &#39;北京市北京经济技术开发区科创十一街18号院C座2层215室 62648622&#39;,
 &#39;应税货物(或服务)名称&#39;: &#39;*家用厨房电器具*米家 小米电热水 MJDSH03YM&#39;,
 &#39;价税合计(大写)&#39;: &#39;壹佰贰拾叁圆玖角玖分&#39;,
 &#39;税率&#39;: &#39;13%&#39;,
 &#39;备注&#39;: None}
</code></pre></div><br>
<h3 id="34-批量提取">3.4 批量提取</h3>
<p>对data文件夹进行批量信息提取，结果存储为csv。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="c1">#cntext版本为2.1.2，非开源，</span>
<span class="c1">#需联系大邓372335839获取</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#当前代码所在的代码文件与data文件夹处于同一个文件夹内</span>
<span class="c1">#获取data内所有pdf的路径</span>
<span class="n">pdf_files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;data/</span><span class="si">{</span><span class="n">file</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.pdf&#39;</span> <span class="ow">in</span> <span class="n">file</span><span class="p">]</span>

<span class="n">dict_datas</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">pdf_file</span> <span class="ow">in</span> <span class="n">pdf_files</span><span class="p">:</span>
    <span class="n">pdf_text</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="n">pdf_file</span><span class="p">)</span>
    <span class="n">dict_data</span> <span class="o">=</span> <span class="n">extract_info</span><span class="p">(</span><span class="n">pdf_text</span><span class="p">)</span>
    <span class="n">dict_datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dict_data</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dict_datas</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">CPU times: user 32 ms, sys: 3.17 ms, total: 35.2 ms
Wall time: 11.8 s
</code></pre></div><p><img loading="lazy" src="img/05-df.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四讨论">四、讨论</h2>
<p>这三张发票， 识别错误字段集中在销售方纳税识别号。原因主要是ct.read_pdf读入pdf时，文本比较杂乱。 对大模型的语义理解有一定的挑战。目前大模型已经支持文本、图片、音频、视频、网址， 所以各位看官，不用等太久，就可克服此问题。</p>
<p>大模型会对每个输入，给出正确概率最大的回答，因此大模型提取数据时存在一定的错误识别风险。为降低该风险，尽量选择特别特殊、显眼，例如三张发票的<strong>价税合计(大写)</strong>,  因为信息是特殊的中文大写数字， 在所有文本中是最醒目最特别的文本信息，这样大模型处理这类信息时会给这类信息尽可能高的权重，增大回答的准确率。</p>
<br>
<br>
<h2 id="精选内容">精选内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集汇总</a></li>
<li><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)数据挖掘文献资料汇总</a></li>
<li><a href="https://textdata.cn/blog/2024-06-16-scrapegraph-ai/">网络爬虫 | 使用scrapegraph-ai(大模型方案)自动采集网页数据</a></li>
<li><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">推荐 | 文本分析库cntext2.x使用手册</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>网络爬虫 | 使用scrapegraph-ai(大模型方案)自动采集网页数据</title>
      <link>https://textdata.cn/blog/2024-06-16-scrapegraph-ai/</link>
      <pubDate>Sun, 16 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-06-16-scrapegraph-ai/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;前几日分享了&lt;a href=&#34;https://textdata.cn/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/&#34;&gt;实验 | 使用本地大模型从文本中提取结构化信息&lt;/a&gt;, 今天再分享一个  &lt;a href=&#34;https://github.com/VinciGit00/Scrapegraph-ai&#34;&gt;ScrapeGraphAI库&lt;/a&gt;， 现在还不太好用，但未来写爬虫很可能会变得越来越容易。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;h2 id=&#34;一介绍&#34;&gt;一、介绍&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;ScrapeGraphAI&lt;/strong&gt;&lt;/em&gt;是一个&lt;em&gt;网络爬虫&lt;/em&gt; Python 库，使用大型语言模型和直接图逻辑为网站和本地文档（XML，HTML，JSON 等）创建爬取管道。&lt;/p&gt;
&lt;p&gt;只需告诉库您想提取哪些信息，它将为您完成！&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/scrapegraphai_logo.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;scrapegraphai有三种主要的爬取管道可用于从网站（或本地文件）提取信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;SmartScraperGraph&lt;/code&gt;: 单页爬虫，只需用户提示和输入源；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SearchGraph&lt;/code&gt;: 多页爬虫，从搜索引擎的前 n 个搜索结果中提取信息；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SpeechGraph&lt;/code&gt;: 单页爬虫，从网站提取信息并生成音频文件。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SmartScraperMultiGraph&lt;/code&gt;: 多页爬虫，给定一个提示 可以通过 API 使用不同的 LLM，如 &lt;strong&gt;OpenAI&lt;/strong&gt;，&lt;strong&gt;Groq&lt;/strong&gt;，&lt;strong&gt;Azure&lt;/strong&gt; 和 &lt;strong&gt;Gemini&lt;/strong&gt;，或者使用 &lt;strong&gt;Ollama&lt;/strong&gt; 的本地模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二准备工作&#34;&gt;二、准备工作&lt;/h2&gt;
&lt;h3 id=&#34;121-安装ollama&#34;&gt;12.1 安装ollama&lt;/h3&gt;
&lt;p&gt;点击前往网站 &lt;a href=&#34;https://ollama.com/&#34;&gt;https://ollama.com/&lt;/a&gt; ，下载ollama软件，支持win、Mac、linux&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-ollama-gui.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-下载llm&#34;&gt;2.2 下载LLM&lt;/h3&gt;
&lt;p&gt;ollama软件目前支持多种大模型， 如阿里的（qwen、qwen2）、meta的(llama3)，&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-ollama-model.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;以llama3为例，根据自己电脑显存性能， 选择适宜的版本。如果不知道选什么，那就试着安装，不合适不能用再删除即可。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-ollama-llama3.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行模型下载(安装)命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ollama pull llama3
ollama pull qwen2
ollama pull nomic-embed-text
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;等待 &lt;strong&gt;llama3、 nomic-embed-text&lt;/strong&gt; 下载完成。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-安装python包&#34;&gt;2.3 安装python包&lt;/h3&gt;
&lt;p&gt;在python中调用ollama服务，需要ollama包。&lt;/p&gt;
&lt;p&gt;打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行安装命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install ollama
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;24-启动ollama服务&#34;&gt;2.4 启动ollama服务&lt;/h3&gt;
&lt;p&gt;在Python中调用本地ollama服务，需要先启动本地ollama服务， 打开电脑命令行cmd(mac是terminal), 执行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ollama serve
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2024/06/14 14:52:24 routes.go:1011: INFO server config env=&amp;#34;map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/Users/deng/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]&amp;#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:725 msg=&amp;#34;total blobs: 18&amp;#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:732 msg=&amp;#34;total unused blobs removed: 0&amp;#34;
time=2024-06-14T14:52:24.743+08:00 level=INFO source=routes.go:1057 msg=&amp;#34;Listening on 127.0.0.1:11434 (version 0.1.44)&amp;#34;
time=2024-06-14T14:52:24.744+08:00 level=INFO source=payload.go:30 msg=&amp;#34;extracting embedded files&amp;#34; dir=/var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/ollama4239159529/runners
time=2024-06-14T14:52:24.772+08:00 level=INFO source=payload.go:44 msg=&amp;#34;Dynamic LLM libraries [metal]&amp;#34;
time=2024-06-14T14:52:24.796+08:00 level=INFO source=types.go:71 msg=&amp;#34;inference compute&amp;#34; id=0 library=metal compute=&amp;#34;&amp;#34; driver=0.0 name=&amp;#34;&amp;#34; total=&amp;#34;72.0 GiB&amp;#34; available=&amp;#34;72.0 GiB&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;cmd(mac是terminal)看到如上的信息，说明本地ollama服务已开启。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;25-安装scrapegraphai及playwright&#34;&gt;2.5 安装scrapegraphai及playwright&lt;/h3&gt;
&lt;p&gt;电脑命令行cmd(mac是terminal),  网络是连网状态，执行安装命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip install scrapegraphai
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后继续命令行cmd(mac是terminal)执行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;playwright install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;等待安装完成后，进行实验&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三实验&#34;&gt;三、实验&lt;/h2&gt;
&lt;h3 id=&#34;31-案例1&#34;&gt;3.1 案例1&lt;/h3&gt;
&lt;p&gt;以我的博客 &lt;code&gt;https://textdata.cn/blog/&lt;/code&gt; 为例，假设我想获取&lt;code&gt;标题、日期、文章链接&lt;/code&gt;,&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/06-blog-list.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;代码如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;scrapegraphai.graphs&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SmartScraperGraph&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;graph_config&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ollama/llama3&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;temperature&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;format&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;json&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# Ollama 需要显式指定格式&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;base_url&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;http://localhost:11434&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 设置 Ollama URL&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;embeddings&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ollama/nomic-embed-text&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;base_url&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;http://localhost:11434&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 设置 Ollama URL&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;verbose&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;smart_scraper_graph&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SmartScraperGraph&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;返回该网站所有文章的标题、日期、文章链接&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# 也接受已下载的 HTML 代码的字符串&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#source=requests.get(&amp;#34;https://textdata.cn/blog/&amp;#34;).text,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;source&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;https://textdata.cn/blog/&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph_config&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;smart_scraper_graph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;--- Executing Fetch Node ---
--- Executing Parse Node ---
--- Executing RAG Node ---
--- (updated chunks metadata) ---
--- (tokens compressed and vector stored) ---
--- Executing GenerateAnswer Node ---
Processing chunks: 100%|█████████████████████████| 1/1 [00:00&amp;lt;00:00, 825.81it/s]

{&amp;#39;articles&amp;#39;: 
		[{&amp;#39;title&amp;#39;: &amp;#39;LIST | 社科(经管)数据挖掘文献资料汇总&amp;#39;, 
			&amp;#39;date&amp;#39;: &amp;#39;2024-04-15&amp;#39;, 
			&amp;#39;link&amp;#39;: &amp;#39;https://textdata.cn/blog/management_python_course/&amp;#39;}, 
			
			{&amp;#39;title&amp;#39;: &amp;#39;LIST| 文本分析代码资料汇总&amp;#39;, 
			&amp;#39;date&amp;#39;: &amp;#39;2024-04-15&amp;#39;,
			&amp;#39;link&amp;#39;:&amp;#39;https://textdata.cn/blog/text_analysis_code_list_about_ms/&amp;#39;}, 
			
			{&amp;#39;title&amp;#39;: &amp;#39;实验 | 使用本地大模型从文本中提取结构化信息&amp;#39;, 
			&amp;#39;date&amp;#39;: &amp;#39;2024-06-14&amp;#39;, 
			&amp;#39;link&amp;#39;: &amp;#39;https://textdata.cn/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/&amp;#39;}, 
			
			{&amp;#39;title&amp;#39;: &amp;#39;2023 | 文本分析在经管研究中的应用&amp;#39;, 
			&amp;#39;date&amp;#39;: &amp;#39;2023-11-05&amp;#39;, 
			&amp;#39;link&amp;#39;: &amp;#39;https://textdata.cn/blog/2023-11-05-xjtu-text-mining-in-ms/&amp;#39;}, 
			
			{&amp;#39;title&amp;#39;: &amp;#39;经管类 | 含 经济日报/经济观察报/中国工业报/中国贸易报/中国消费者报 等 10+ 家媒体(2024.05)&amp;#39;, 
			&amp;#39;date&amp;#39;: &amp;#39;2024-06-12&amp;#39;, 
			&amp;#39;link&amp;#39;: &amp;#39;https://textdata.cn/blog/2024-06-12-national-level-economic-daily-news-dataset/&amp;#39;}]}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-案例2&#34;&gt;3.2 案例2&lt;/h3&gt;
&lt;p&gt;采集豆瓣读书 &lt;code&gt;https://book.douban.com/top250&lt;/code&gt; 中的 &lt;code&gt;名字、作者名、评分、书籍链接&lt;/code&gt; 等信息。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/07-books.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;scrapegraphai.graphs&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SmartScraperGraph&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;graph_config&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;llm&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ollama/llama3&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;temperature&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;format&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;json&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# Ollama 需要显式指定格式&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;base_url&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;http://localhost:11434&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 设置 Ollama URL&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;embeddings&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;ollama/nomic-embed-text&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;s2&#34;&gt;&amp;#34;base_url&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;http://localhost:11434&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 设置 Ollama URL&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;verbose&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;smart_scraper_graph2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SmartScraperGraph&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;返回该页面所有书的名字、作者名、评分、书籍链接&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;source&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;https://book.douban.com/top250&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph_config&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;result2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;smart_scraper_graph2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;--- Executing Fetch Node ---
--- Executing Parse Node ---
--- Executing RAG Node ---
--- (updated chunks metadata) ---
--- (tokens compressed and vector stored) ---
--- Executing GenerateAnswer Node ---
Processing chunks: 100%|████████████████████████| 1/1 [00:00&amp;lt;00:00, 1474.79it/s]
{}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;采集失败，返回空。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;将大模型llama3改为qwen2&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;from scrapegraphai.graphs import SmartScraperGraph


graph_config2 = {
    &amp;#34;llm&amp;#34;: {
        &amp;#34;model&amp;#34;: &amp;#34;ollama/qwen2&amp;#34;,
        &amp;#34;temperature&amp;#34;: 0,
        &amp;#34;format&amp;#34;: &amp;#34;json&amp;#34;,  # Ollama 需要显式指定格式
        &amp;#34;base_url&amp;#34;: &amp;#34;http://localhost:11434&amp;#34;,  # 设置 Ollama URL
    },
    &amp;#34;embeddings&amp;#34;: {
        &amp;#34;model&amp;#34;: &amp;#34;ollama/nomic-embed-text&amp;#34;,
        &amp;#34;base_url&amp;#34;: &amp;#34;http://localhost:11434&amp;#34;,  # 设置 Ollama URL
    },
    &amp;#34;verbose&amp;#34;: True,
}


smart_scraper_graph3 = SmartScraperGraph(
    prompt=&amp;#34;返回该页面所有书的名字、作者名、评分、书籍链接&amp;#34;,
    source=&amp;#34;https://book.douban.com/top250&amp;#34;,
    config=graph_config2
)

result3 = smart_scraper_graph3.run()
print(result3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;--- Executing Fetch Node ---
--- Executing Parse Node ---
--- Executing RAG Node ---
--- (updated chunks metadata) ---
--- (tokens compressed and vector stored) ---
--- Executing GenerateAnswer Node ---
Processing chunks: 100%|████████████████████████| 1/1 [00:00&amp;lt;00:00, 1102.60it/s]
{&amp;#39;urls&amp;#39;: [&amp;#39;https://book.douban.com/subject/10554308/&amp;#39;, &amp;#39;https://book.douban.com/subject/1084336/&amp;#39;, &amp;#39;https://book.douban.com/subject/1084336/&amp;#39;, &amp;#39;https://book.douban.com/subject/1046209/&amp;#39;, &amp;#39;https://book.douban.com/subject/1046209/&amp;#39;, &amp;#39;https://book.douban.com/subject/1255625/&amp;#39;, &amp;#39;https://book.douban.com/subject/1255625/&amp;#39;, &amp;#39;https://book.douban.com/subject/1060068/&amp;#39;, &amp;#39;https://book.douban.com/subject/1060068/&amp;#39;, &amp;#39;https://book.douban.com/subject/1449351/&amp;#39;, &amp;#39;https://book.douban.com/subject/1449351/&amp;#39;, &amp;#39;https://book.douban.com/subject/20424526/&amp;#39;, &amp;#39;https://book.douban.com/subject/20424526/&amp;#39;, &amp;#39;https://book.douban.com/subject/29799269/&amp;#39;, &amp;#39;https://book.douban.com/subject/1034062/&amp;#39;, &amp;#39;https://book.douban.com/subject/1229240/&amp;#39;, &amp;#39;https://book.douban.com/subject/1237549/&amp;#39;, &amp;#39;https://book.douban.com/subject/1078958/&amp;#39;, &amp;#39;https://book.douban.com/subject/1076932/&amp;#39;, &amp;#39;https://book.douban.com/subject/1075440/&amp;#39;, &amp;#39;https://book.douban.com/subject/1076932/&amp;#39;, &amp;#39;https://book.douban.com/subject/1078958/&amp;#39;, &amp;#39;https://book.douban.com/subject/1076932/&amp;#39;, &amp;#39;https://book.douban.com/subject/1078958/&amp;#39;, &amp;#39;https://book.douban.com/subject/1076932/&amp;#39;, &amp;#39;https://book.douban.com/subject/1078958/&amp;#39;, &amp;#39;https://book.douban.com/subject/1076932/&amp;#39;], &amp;#39;images&amp;#39;: [&amp;#39;https://img1.doubanio.com/view/subject/s/public/s1078958.jpg&amp;#39;, &amp;#39;https://img1.doubanio.com/view/subject/s/public/s1076932.jpg&amp;#39;, &amp;#39;https://img1.doubanio.com/view/subject/s/public/s1447349.jpg&amp;#39;]}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;采集到一些信息，但没有书名、作者等信息。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;注意&#34;&gt;注意：&lt;/h3&gt;
&lt;p&gt;代码需要在 &lt;code&gt;.py&lt;/code&gt; 中运行，在 &lt;code&gt;.ipynb&lt;/code&gt; 中运行会报错。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四讨论&#34;&gt;四、讨论&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;ScrapeGraphAI&lt;/strong&gt;&lt;/em&gt; 是目前大邓已经的唯一的大模型爬虫， 现在采集数据的成功率还是比较低的。 而且因为底层使用 playwright ， 访问速度较慢。&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;精选内容&#34;&gt;精选内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/datasets_available_for_management_science/&#34;&gt;LIST | 可供社科(经管)领域使用的数据集汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/the_text_analysis_list_about_ms/&#34;&gt;LIST | 社科(经管)数据挖掘文献资料汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/&#34;&gt;推荐 | 文本分析库cntext2.x使用手册&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<blockquote>
<p>前几日分享了<a href="https://textdata.cn/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/">实验 | 使用本地大模型从文本中提取结构化信息</a>, 今天再分享一个  <a href="https://github.com/VinciGit00/Scrapegraph-ai">ScrapeGraphAI库</a>， 现在还不太好用，但未来写爬虫很可能会变得越来越容易。</p>
</blockquote>
<br>
<h2 id="一介绍">一、介绍</h2>
<p><em><strong>ScrapeGraphAI</strong></em>是一个<em>网络爬虫</em> Python 库，使用大型语言模型和直接图逻辑为网站和本地文档（XML，HTML，JSON 等）创建爬取管道。</p>
<p>只需告诉库您想提取哪些信息，它将为您完成！</p>
<p><img loading="lazy" src="img/scrapegraphai_logo.png" alt=""  />
</p>
<p>scrapegraphai有三种主要的爬取管道可用于从网站（或本地文件）提取信息：</p>
<ul>
<li><code>SmartScraperGraph</code>: 单页爬虫，只需用户提示和输入源；</li>
<li><code>SearchGraph</code>: 多页爬虫，从搜索引擎的前 n 个搜索结果中提取信息；</li>
<li><code>SpeechGraph</code>: 单页爬虫，从网站提取信息并生成音频文件。</li>
<li><code>SmartScraperMultiGraph</code>: 多页爬虫，给定一个提示 可以通过 API 使用不同的 LLM，如 <strong>OpenAI</strong>，<strong>Groq</strong>，<strong>Azure</strong> 和 <strong>Gemini</strong>，或者使用 <strong>Ollama</strong> 的本地模型。</li>
</ul>
<p><br><br></p>
<h2 id="二准备工作">二、准备工作</h2>
<h3 id="121-安装ollama">12.1 安装ollama</h3>
<p>点击前往网站 <a href="https://ollama.com/">https://ollama.com/</a> ，下载ollama软件，支持win、Mac、linux</p>
<p><img loading="lazy" src="img/02-ollama-gui.png" alt=""  />
</p>
<br>
<h3 id="22-下载llm">2.2 下载LLM</h3>
<p>ollama软件目前支持多种大模型， 如阿里的（qwen、qwen2）、meta的(llama3)，</p>
<p><img loading="lazy" src="img/03-ollama-model.png" alt=""  />
</p>
<br>
<p>以llama3为例，根据自己电脑显存性能， 选择适宜的版本。如果不知道选什么，那就试着安装，不合适不能用再删除即可。</p>
<p><img loading="lazy" src="img/04-ollama-llama3.png" alt=""  />
</p>
<br>
<p>打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行模型下载(安装)命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ollama pull llama3
ollama pull qwen2
ollama pull nomic-embed-text
</code></pre></div><p>等待 <strong>llama3、 nomic-embed-text</strong> 下载完成。</p>
<br>
<h3 id="23-安装python包">2.3 安装python包</h3>
<p>在python中调用ollama服务，需要ollama包。</p>
<p>打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行安装命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install ollama
</code></pre></div><br>
<h3 id="24-启动ollama服务">2.4 启动ollama服务</h3>
<p>在Python中调用本地ollama服务，需要先启动本地ollama服务， 打开电脑命令行cmd(mac是terminal), 执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ollama serve
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2024/06/14 14:52:24 routes.go:1011: INFO server config env=&#34;map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/Users/deng/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]&#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:725 msg=&#34;total blobs: 18&#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:732 msg=&#34;total unused blobs removed: 0&#34;
time=2024-06-14T14:52:24.743+08:00 level=INFO source=routes.go:1057 msg=&#34;Listening on 127.0.0.1:11434 (version 0.1.44)&#34;
time=2024-06-14T14:52:24.744+08:00 level=INFO source=payload.go:30 msg=&#34;extracting embedded files&#34; dir=/var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/ollama4239159529/runners
time=2024-06-14T14:52:24.772+08:00 level=INFO source=payload.go:44 msg=&#34;Dynamic LLM libraries [metal]&#34;
time=2024-06-14T14:52:24.796+08:00 level=INFO source=types.go:71 msg=&#34;inference compute&#34; id=0 library=metal compute=&#34;&#34; driver=0.0 name=&#34;&#34; total=&#34;72.0 GiB&#34; available=&#34;72.0 GiB&#34;
</code></pre></div><p>cmd(mac是terminal)看到如上的信息，说明本地ollama服务已开启。</p>
<br>
<h3 id="25-安装scrapegraphai及playwright">2.5 安装scrapegraphai及playwright</h3>
<p>电脑命令行cmd(mac是terminal),  网络是连网状态，执行安装命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install scrapegraphai
</code></pre></div><p>之后继续命令行cmd(mac是terminal)执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">playwright install
</code></pre></div><p>等待安装完成后，进行实验</p>
<p><br><br></p>
<h2 id="三实验">三、实验</h2>
<h3 id="31-案例1">3.1 案例1</h3>
<p>以我的博客 <code>https://textdata.cn/blog/</code> 为例，假设我想获取<code>标题、日期、文章链接</code>,</p>
<p><img loading="lazy" src="img/06-blog-list.png" alt=""  />
</p>
<p>代码如下:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scrapegraphai.graphs</span> <span class="kn">import</span> <span class="n">SmartScraperGraph</span>


<span class="n">graph_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;llm&#34;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&#34;model&#34;</span><span class="p">:</span> <span class="s2">&#34;ollama/llama3&#34;</span><span class="p">,</span>
        <span class="s2">&#34;temperature&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&#34;format&#34;</span><span class="p">:</span> <span class="s2">&#34;json&#34;</span><span class="p">,</span>  <span class="c1"># Ollama 需要显式指定格式</span>
        <span class="s2">&#34;base_url&#34;</span><span class="p">:</span> <span class="s2">&#34;http://localhost:11434&#34;</span><span class="p">,</span>  <span class="c1"># 设置 Ollama URL</span>
    <span class="p">},</span>
    <span class="s2">&#34;embeddings&#34;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&#34;model&#34;</span><span class="p">:</span> <span class="s2">&#34;ollama/nomic-embed-text&#34;</span><span class="p">,</span>
        <span class="s2">&#34;base_url&#34;</span><span class="p">:</span> <span class="s2">&#34;http://localhost:11434&#34;</span><span class="p">,</span>  <span class="c1"># 设置 Ollama URL</span>
    <span class="p">},</span>
    <span class="s2">&#34;verbose&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">smart_scraper_graph</span> <span class="o">=</span> <span class="n">SmartScraperGraph</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&#34;返回该网站所有文章的标题、日期、文章链接&#34;</span><span class="p">,</span>
    <span class="c1"># 也接受已下载的 HTML 代码的字符串</span>
    <span class="c1">#source=requests.get(&#34;https://textdata.cn/blog/&#34;).text,</span>
    <span class="n">source</span><span class="o">=</span><span class="s2">&#34;https://textdata.cn/blog/&#34;</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">graph_config</span>
<span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">smart_scraper_graph</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">--- Executing Fetch Node ---
--- Executing Parse Node ---
--- Executing RAG Node ---
--- (updated chunks metadata) ---
--- (tokens compressed and vector stored) ---
--- Executing GenerateAnswer Node ---
Processing chunks: 100%|█████████████████████████| 1/1 [00:00&lt;00:00, 825.81it/s]

{&#39;articles&#39;: 
		[{&#39;title&#39;: &#39;LIST | 社科(经管)数据挖掘文献资料汇总&#39;, 
			&#39;date&#39;: &#39;2024-04-15&#39;, 
			&#39;link&#39;: &#39;https://textdata.cn/blog/management_python_course/&#39;}, 
			
			{&#39;title&#39;: &#39;LIST| 文本分析代码资料汇总&#39;, 
			&#39;date&#39;: &#39;2024-04-15&#39;,
			&#39;link&#39;:&#39;https://textdata.cn/blog/text_analysis_code_list_about_ms/&#39;}, 
			
			{&#39;title&#39;: &#39;实验 | 使用本地大模型从文本中提取结构化信息&#39;, 
			&#39;date&#39;: &#39;2024-06-14&#39;, 
			&#39;link&#39;: &#39;https://textdata.cn/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/&#39;}, 
			
			{&#39;title&#39;: &#39;2023 | 文本分析在经管研究中的应用&#39;, 
			&#39;date&#39;: &#39;2023-11-05&#39;, 
			&#39;link&#39;: &#39;https://textdata.cn/blog/2023-11-05-xjtu-text-mining-in-ms/&#39;}, 
			
			{&#39;title&#39;: &#39;经管类 | 含 经济日报/经济观察报/中国工业报/中国贸易报/中国消费者报 等 10+ 家媒体(2024.05)&#39;, 
			&#39;date&#39;: &#39;2024-06-12&#39;, 
			&#39;link&#39;: &#39;https://textdata.cn/blog/2024-06-12-national-level-economic-daily-news-dataset/&#39;}]}

</code></pre></div><br>
<h3 id="32-案例2">3.2 案例2</h3>
<p>采集豆瓣读书 <code>https://book.douban.com/top250</code> 中的 <code>名字、作者名、评分、书籍链接</code> 等信息。</p>
<p><img loading="lazy" src="img/07-books.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scrapegraphai.graphs</span> <span class="kn">import</span> <span class="n">SmartScraperGraph</span>


<span class="n">graph_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;llm&#34;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&#34;model&#34;</span><span class="p">:</span> <span class="s2">&#34;ollama/llama3&#34;</span><span class="p">,</span>
        <span class="s2">&#34;temperature&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&#34;format&#34;</span><span class="p">:</span> <span class="s2">&#34;json&#34;</span><span class="p">,</span>  <span class="c1"># Ollama 需要显式指定格式</span>
        <span class="s2">&#34;base_url&#34;</span><span class="p">:</span> <span class="s2">&#34;http://localhost:11434&#34;</span><span class="p">,</span>  <span class="c1"># 设置 Ollama URL</span>
    <span class="p">},</span>
    <span class="s2">&#34;embeddings&#34;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&#34;model&#34;</span><span class="p">:</span> <span class="s2">&#34;ollama/nomic-embed-text&#34;</span><span class="p">,</span>
        <span class="s2">&#34;base_url&#34;</span><span class="p">:</span> <span class="s2">&#34;http://localhost:11434&#34;</span><span class="p">,</span>  <span class="c1"># 设置 Ollama URL</span>
    <span class="p">},</span>
    <span class="s2">&#34;verbose&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>


<span class="n">smart_scraper_graph2</span> <span class="o">=</span> <span class="n">SmartScraperGraph</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&#34;返回该页面所有书的名字、作者名、评分、书籍链接&#34;</span><span class="p">,</span>
    <span class="n">source</span><span class="o">=</span><span class="s2">&#34;https://book.douban.com/top250&#34;</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">graph_config</span>
<span class="p">)</span>

<span class="n">result2</span> <span class="o">=</span> <span class="n">smart_scraper_graph2</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result2</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">--- Executing Fetch Node ---
--- Executing Parse Node ---
--- Executing RAG Node ---
--- (updated chunks metadata) ---
--- (tokens compressed and vector stored) ---
--- Executing GenerateAnswer Node ---
Processing chunks: 100%|████████████████████████| 1/1 [00:00&lt;00:00, 1474.79it/s]
{}
</code></pre></div><p>采集失败，返回空。</p>
<br>
<p>将大模型llama3改为qwen2</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">from scrapegraphai.graphs import SmartScraperGraph


graph_config2 = {
    &#34;llm&#34;: {
        &#34;model&#34;: &#34;ollama/qwen2&#34;,
        &#34;temperature&#34;: 0,
        &#34;format&#34;: &#34;json&#34;,  # Ollama 需要显式指定格式
        &#34;base_url&#34;: &#34;http://localhost:11434&#34;,  # 设置 Ollama URL
    },
    &#34;embeddings&#34;: {
        &#34;model&#34;: &#34;ollama/nomic-embed-text&#34;,
        &#34;base_url&#34;: &#34;http://localhost:11434&#34;,  # 设置 Ollama URL
    },
    &#34;verbose&#34;: True,
}


smart_scraper_graph3 = SmartScraperGraph(
    prompt=&#34;返回该页面所有书的名字、作者名、评分、书籍链接&#34;,
    source=&#34;https://book.douban.com/top250&#34;,
    config=graph_config2
)

result3 = smart_scraper_graph3.run()
print(result3)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">--- Executing Fetch Node ---
--- Executing Parse Node ---
--- Executing RAG Node ---
--- (updated chunks metadata) ---
--- (tokens compressed and vector stored) ---
--- Executing GenerateAnswer Node ---
Processing chunks: 100%|████████████████████████| 1/1 [00:00&lt;00:00, 1102.60it/s]
{&#39;urls&#39;: [&#39;https://book.douban.com/subject/10554308/&#39;, &#39;https://book.douban.com/subject/1084336/&#39;, &#39;https://book.douban.com/subject/1084336/&#39;, &#39;https://book.douban.com/subject/1046209/&#39;, &#39;https://book.douban.com/subject/1046209/&#39;, &#39;https://book.douban.com/subject/1255625/&#39;, &#39;https://book.douban.com/subject/1255625/&#39;, &#39;https://book.douban.com/subject/1060068/&#39;, &#39;https://book.douban.com/subject/1060068/&#39;, &#39;https://book.douban.com/subject/1449351/&#39;, &#39;https://book.douban.com/subject/1449351/&#39;, &#39;https://book.douban.com/subject/20424526/&#39;, &#39;https://book.douban.com/subject/20424526/&#39;, &#39;https://book.douban.com/subject/29799269/&#39;, &#39;https://book.douban.com/subject/1034062/&#39;, &#39;https://book.douban.com/subject/1229240/&#39;, &#39;https://book.douban.com/subject/1237549/&#39;, &#39;https://book.douban.com/subject/1078958/&#39;, &#39;https://book.douban.com/subject/1076932/&#39;, &#39;https://book.douban.com/subject/1075440/&#39;, &#39;https://book.douban.com/subject/1076932/&#39;, &#39;https://book.douban.com/subject/1078958/&#39;, &#39;https://book.douban.com/subject/1076932/&#39;, &#39;https://book.douban.com/subject/1078958/&#39;, &#39;https://book.douban.com/subject/1076932/&#39;, &#39;https://book.douban.com/subject/1078958/&#39;, &#39;https://book.douban.com/subject/1076932/&#39;], &#39;images&#39;: [&#39;https://img1.doubanio.com/view/subject/s/public/s1078958.jpg&#39;, &#39;https://img1.doubanio.com/view/subject/s/public/s1076932.jpg&#39;, &#39;https://img1.doubanio.com/view/subject/s/public/s1447349.jpg&#39;]}
</code></pre></div><p>采集到一些信息，但没有书名、作者等信息。</p>
<br>
<h3 id="注意">注意：</h3>
<p>代码需要在 <code>.py</code> 中运行，在 <code>.ipynb</code> 中运行会报错。</p>
<p><br><br></p>
<h2 id="四讨论">四、讨论</h2>
<p><em><strong>ScrapeGraphAI</strong></em> 是目前大邓已经的唯一的大模型爬虫， 现在采集数据的成功率还是比较低的。 而且因为底层使用 playwright ， 访问速度较慢。</p>
<br>
<br>
<h2 id="精选内容">精选内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集汇总</a></li>
<li><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)数据挖掘文献资料汇总</a></li>
<li><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">推荐 | 文本分析库cntext2.x使用手册</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Word Embeddings、Transformer与GPT：一文揭示三者关系</title>
      <link>https://textdata.cn/blog/2023-11-16-how-to-understand-the-meaning-of-gpt/</link>
      <pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-16-how-to-understand-the-meaning-of-gpt/</guid>
      <description>&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;作者: 7号床
公众号: 7号床
原文  https://zhuanlan.zhihu.com/p/666206302
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一gpt-的名词解释&#34;&gt;一、GPT 的名词解释&lt;/h2&gt;
&lt;p&gt;著名的 &lt;strong&gt;GPT&lt;/strong&gt; 这个名字全称是 &lt;strong&gt;Generative Pre-trained Transformer&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Generative&lt;/strong&gt; 是&amp;quot;生成式&amp;quot;的意思，也就是说这个 AI 模型是用来生成内容的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pre-trained&lt;/strong&gt; 是“预训练”的意思，就是说这个 AI 模型能有很强的能力，是因为他事先做了大量的训练，台上一分钟台下十年功。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt; , 就有点耐人寻味了，不仅普通人不理解，就连很多专业领域的人员理解起来也都是含混不清、似是而非。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;ChatGPT 是 GPT 大模型在聊天对话领域的应用程序&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt; 作为单词，翻译出来频率最高的意思是 &lt;strong&gt;变压器&lt;/strong&gt;，然后是 &lt;strong&gt;变形金刚&lt;/strong&gt; ，还有一些引申的含义是 &lt;strong&gt;转换器&lt;/strong&gt; 、&lt;strong&gt;促使变化者&lt;/strong&gt; 、&lt;strong&gt;转变者&lt;/strong&gt; 或 &lt;strong&gt;改革者&lt;/strong&gt;等等。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;谷歌翻译上对 **Transformer** 的英译中翻译&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;再把 &lt;strong&gt;Transformer&lt;/strong&gt; 放到  &lt;strong&gt;Chat Generative Pre-trained Transformer&lt;/strong&gt; 中看看，突然间变得奇怪了，难道 ChatGPT 借鉴了变压器的技术？还是说 ChatGPT 是一个变形金刚？或者索性就翻译成通用的安全的叫法 &lt;strong&gt;转换器&lt;/strong&gt; ？这让人百思不得其解。&lt;/p&gt;
&lt;p&gt;光光从 GPT 这三个字母的组合就能看出来， &lt;strong&gt;Generative&lt;/strong&gt; 与 &lt;strong&gt;Pre-trained&lt;/strong&gt; 都是定语，而 &lt;strong&gt;Transformer 才是 GPT 的主体，才是 GPT 的灵魂&lt;/strong&gt;所在。可以说，理解透了 &lt;strong&gt;Transformer&lt;/strong&gt; 的真正含义，才能初步地理解 GPT。另一方面， Transformer 这个词太重要了。它在这几年的人工智能领域大放异彩，不仅仅局限于 NLP 自然语言处理领域，它还有着更广阔的发展空间。 Transformer 目前已经进入到了多模态领域，比如音频与视觉，甚至数学公式、代码编程等领域，著名的 **Stable Diffusion 中也用到了 Transformer **。&lt;strong&gt;可以说，所有生成式人工智能领域的大模型中目前都有了这个 Transformer 的身影&lt;/strong&gt;。既然如此重要，那就让我们深入地探究一下 &lt;strong&gt;Transformer&lt;/strong&gt; 在人工智能领域最确切的最标准的含义到底是什么吧！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt; 最早是由 Google 的人工智能团队提出来的。在2017 年6月发表的论文**《Attention Is All You Need》中，他们首次提出了一种新的神经网络架构 Transformer**。Transformer 依赖于一个叫“自注意力机制”（ Self-Attention）的内部构件，可十分准确高效地对自然语言领域的问题进行处理，以完美地解决翻译、对话、论文协作甚至编程等复杂的问题。&lt;/p&gt;
&lt;p&gt;顺藤摸瓜可以看出，&lt;strong&gt;GTP 的核心是 Transformer，而 Transformer 的核心则是“自注意力机制”（ Self-Attention）&lt;/strong&gt;。那么这个“自注意力机制”又是什东西呢？让我们用语言翻译领域的几个简单易懂的例子来讲解一下。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二-transformer-的核心-self-attention&#34;&gt;二、 Transformer 的核心 Self-Attention&lt;/h2&gt;
&lt;p&gt;首先，看下面这两个短句：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;句子I&lt;/strong&gt;：The bank of the river.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;句子II&lt;/strong&gt;：Money in the bank.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在翻译成中文的过程中，机器算法是如何知道“句子I”中的“bank”指的是自然环境中的“岸边”，而“句子II”中的“bank”指的是金融体系中的“银行”呢？&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/3.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;bank在不同句子中指代不同的事物&lt;/center&gt;&lt;/p&gt;
&lt;h3 id=&#34;21-人类脑中的翻译算法&#34;&gt;2.1 人类脑中的翻译算法&lt;/h3&gt;
&lt;p&gt;作为人类的我们当然会觉得这是一个再简单不过的事情了，那是因为我们的语言技能从幼儿发展到成年人后，早已烂熟于心了。但即使烂熟于心，也并不意味着在我们的大脑中没有对应的计算过程。&lt;strong&gt;实际上人工智能的翻译过程就是对我们人脑中的计算过程的模拟&lt;/strong&gt;。那么就让我们回想一下儿童时期学习语言时的情景吧，回想一下当时的我们是怎么知道一个多义词在某一句话中具体的含义的？&lt;/p&gt;
&lt;p&gt;人类做这件事的方法是根据 &lt;strong&gt;前后文的语义对照&lt;/strong&gt; 来确定结果，即看句子中其他相关联的单词是什么含义。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 &lt;strong&gt;句子I&lt;/strong&gt; 中， &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 这个词指明了自然环境，&lt;/li&gt;
&lt;li&gt;而在 &lt;strong&gt;句子II&lt;/strong&gt;中， &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 这个词则指明了金融环境。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以两个句子中的多义词“bank”也就有了各自的定位。如果把这种方式总结成一种算法的话，这个算法就可以用于人工智能领域用于语言处理了。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-机器算法模拟人脑中的翻译过程&#34;&gt;2.2 机器算法模拟人脑中的翻译过程&lt;/h3&gt;
&lt;p&gt;但人工智能作为一种计算机算法，它只能处理冷冰冰的数字，并不知道何为自然环境，何为金融环境，它又是怎么去判断 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 各自的含义呢。实际上，机器算法并不知道 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 的具体含义。但是机器可以通过某种数字的方式来表达 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; ，同时，通过数字的方式还表达了许许多多其他的词汇，其中必然会有一些词汇会与 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 有着很紧密的语义上的逻辑关系。通过判断 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 各与哪些词汇在语义上有紧密的逻辑关系，便可以知道这两个词各属于什么领域了。&lt;/p&gt;
&lt;p&gt;（其实，不像人类会对某个领域有一个具体的名称来命名，在人工智能领域，机器最终也不知道这个领域的统称到底叫什么名字，但它却知道这个领域中都包括了哪些词、哪些概念和哪些逻辑。***机器不以单独名称来定义一个概念，它却可以用很多相关的概念与逻辑来圈定这一个概念！***这可能就是老子说的：道可道非常道，名可名非常名吧。）&lt;/p&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;独热编码法(One-hot Encoding)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么就让我们看看这种数字表达方式具体是什么样子吧。&lt;/p&gt;
&lt;p&gt;假设这个世界上有100万个单词，每一个单词，我们都可以用一组 0 和 1 组成的向量（一组数字）来定义的话，那么每一个单词就可以被编码成100万个0或1组成的向量。如下图：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/4.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;独热编码示例&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;这种单词编码方法叫 **独热编码法(One-hot Encoding)**法。可是这样一维的编码方法将导致向量占用的空间过大，1个单词用100万个单元的向量表达，世界上一共有100万个单词，那么就需要 1万亿（100万*100万）的体积来把它们表达出来，很明显这种臃肿的结构不利于电脑计算。&lt;/p&gt;
&lt;p&gt;但最大的问题还不在于这个体积问题，而是语义联系问题。独热编码使得单词与单词之间完全相互独立，从每个单词所编码成为的100万个单元的向量身上，根本看不出它与其他单词有何种语义内涵上的逻辑联系。比如，在这些数字中，我们无法知道 &lt;em&gt;&lt;strong&gt;apple&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;bag&lt;/strong&gt;&lt;/em&gt; 属于静物，区别于 cat 和 &lt;em&gt;&lt;strong&gt;dog&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;elephant&lt;/strong&gt;&lt;/em&gt; 属于动物且是哺乳动物，而 &lt;em&gt;&lt;strong&gt;cat&lt;/strong&gt;&lt;/em&gt;  和 &lt;em&gt;&lt;strong&gt;dog&lt;/strong&gt;&lt;/em&gt; 又属于小动物，且大多数为非野生，区别于 &lt;em&gt;&lt;strong&gt;elephant&lt;/strong&gt;&lt;/em&gt; 为大型的野生动物，等等等等，这些单词背后所蕴含的各种内在的逻辑联系和分类关系均无法从独热编码法中知晓。实际上独热编码是传统计算机数据库时代的产物，而在人工智能领域则采用另一种编码法。为了解决独热编码的问题， &lt;strong&gt;词嵌入编码法(Word Embedding)&lt;/strong&gt; 诞生了，如下图：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/5.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;Word Embedding 词嵌入编码示意，及 Embedding 空间&lt;/center&gt;&lt;/p&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;词嵌入编码法(Word Embedding)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;**词嵌入编码法(Word Embedding)**将语义上相近的、有关联的词汇在 Embedding 空间中生成相近的位置定位。相对于 &lt;strong&gt;独热编码法&lt;/strong&gt; 超长的一维数据，词嵌入编码法(Word Embedding) 提升了数据的表达维度，它更像是在某一个 &lt;strong&gt;空间&lt;/strong&gt; 中对词汇进行编码。&lt;/p&gt;
&lt;p&gt;如上图（为了在此文章中表达方便，我们仅用二维空间来表达，实际上这个空间的维度很高，至少要在512维之上！一维二维三维的空间大家都可以在脑中想象出来对应的画面，但是四维以上以至于 512 维就难以图形化的想象了。），在 Embedding 的二维空间中 &lt;em&gt;&lt;strong&gt;dog&lt;/strong&gt;&lt;/em&gt;、 &lt;em&gt;&lt;strong&gt;cat&lt;/strong&gt;&lt;/em&gt; 、&lt;em&gt;&lt;strong&gt;rabbit&lt;/strong&gt;&lt;/em&gt; 三个向量的坐标点位排布，可以看到三个绿色的点距离很近，是因为他们三个相对于其他来说语义上更接近。tree 和 flower 则离它们较远，但是 &lt;em&gt;&lt;strong&gt;cat&lt;/strong&gt;&lt;/em&gt; 会因为在很多语言的文章中都会有“爬树”的词汇出现在同一句话中，所以导致  &lt;em&gt;&lt;strong&gt;cat&lt;/strong&gt;&lt;/em&gt;  会与  &lt;em&gt;&lt;strong&gt;tree&lt;/strong&gt;&lt;/em&gt;  离得较近一些。同时 &lt;em&gt;&lt;strong&gt;dog&lt;/strong&gt;&lt;/em&gt;、 &lt;em&gt;&lt;strong&gt;rabbit&lt;/strong&gt;&lt;/em&gt;  与  &lt;em&gt;&lt;strong&gt;tree&lt;/strong&gt;&lt;/em&gt; 的关系就较远。&lt;/p&gt;
&lt;p&gt;实际上，在 Embedding 空间中，词与词之间的关系还不仅仅限于语义上的分类所导致的定位远近这么简单。一个词所代表的事物与其他词所代表的事物之间能产生内在联系的往往有成百上千上万种之多。比如  &lt;em&gt;&lt;strong&gt;man&lt;/strong&gt;&lt;/em&gt;  和  &lt;em&gt;&lt;strong&gt;woman&lt;/strong&gt;&lt;/em&gt; ，他们之间的关系还会映射出  &lt;em&gt;&lt;strong&gt;king&lt;/strong&gt;&lt;/em&gt;  和  &lt;em&gt;&lt;strong&gt;queen&lt;/strong&gt;&lt;/em&gt;  之间的关系。同时，语法也会带来一定的联系，比如在一个三维空间中由  &lt;em&gt;&lt;strong&gt;walking&lt;/strong&gt;&lt;/em&gt;  到 &lt;em&gt;&lt;strong&gt;walked&lt;/strong&gt;&lt;/em&gt;  的距离与斜率竟然与  &lt;em&gt;&lt;strong&gt;swimming&lt;/strong&gt;&lt;/em&gt;  到 &lt;em&gt;&lt;strong&gt;swam&lt;/strong&gt;&lt;/em&gt; 的距离与斜率一致（即向量的长度与斜率一致），且距离几乎相等。因为这背后是两组动作单词的现在分词形式和过去分词形式的变化关系。我们可以尽情地想象，凡是事物或概念有逻辑联系的，甚至是逻辑与逻辑之间的联系的，在 Embedding 向量空间中都可以得到远近亲疏的空间表达。只不过这种空间要比我们能想象出的三维空间要高出很多维度。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/6.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;在 Embedding 空间中隐含的内在逻辑关系&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Word Embedding 之所以能给每一个单词做这样有意义的向量空间的标注，是因为 AI 科学家们事先用了全球十多种主流语言的大量语料给它进行了训练。这些语料有小说、论文、学术期刊、网络文章、新闻报道、论坛对话记录等等等等，应有尽有，数以百亿到千亿计。可以说，这些海量的文字资料都是人类从古至今感受发现这个世界各个方面的文字总结和积累。现实世界中各种事物之间的逻辑关系都被人类用这些文字记录了下来，只是有的是用严谨的论文方式，有的是用写意的小说方式，有的使用类似维基百科这样的系统梳理，有的则是人们在网络论坛中的对话记录&amp;hellip;等等等等。但不管是什么方式，都是人类试图用语言对这个世界的描述。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语言是人类最伟大的发明&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;笔者7号床曾经问过  ChatGPT  一个问题：&lt;em&gt;&lt;strong&gt;“人类最伟大的发明是什么”&lt;/strong&gt;&lt;/em&gt; ，ChatGPT的回答是：&lt;em&gt;&lt;strong&gt;“语言！”&lt;/strong&gt;&lt;/em&gt;。之后，ChatGPT 进一步回答，因为语言以及匹配语言的文字与符号，它们让人类把对世界的感受与理解记录下来，形成了知识宝库。方便全人类一代一代地不断完善这个宝库，并从中总结凝练、学习、创造、传承。语言是人类产生文明并开始与其他动物分道扬镳的分叉点。&lt;/p&gt;
&lt;p&gt;很多人曾经十分疑惑，人工智能吹得那么先进，却从一个 ChatGPT 聊天功能开始火爆起来。难道每天不干正事专门闲聊就证明了人工智能的先进性吗？现在看来，这个问题的答案已经浮出水面了，OpenAI 的团队选择通过聊天软件 ChatGPT 作为 GPT 启程的第一步是经过深思熟虑的。&lt;/p&gt;
&lt;p&gt;下面让我们回到正题。&lt;/p&gt;
&lt;p&gt;人类的知识宝库中存储着海量的信息
ChatGPT 所说的这个知识宝库现在变得越来越庞大、越来越复杂了。这世界上并不存在任何一个肉身的人类有能力做到对宝库中所有信息进行消化整理，因为内容体量过于庞大、过于复杂。而一个人的阅览进度却又是十分有限，以至于在他的有生之年，哪怕完成其中的万分之一都比登天还难。于是，迫不得已，人类才喊出了 &lt;em&gt;&lt;strong&gt;“闻道有先后，术业有专攻”&lt;/strong&gt;&lt;/em&gt; ，每个人类个体才转而去研究具体某一领域。&lt;/p&gt;
&lt;p&gt;另一方面，人类早期发明的纸张和印刷术，以至于后来的计算机芯片存储，倒是可以记录存储下来如此巨量的信息了，但却无法主动地、有机地分析汇总其中所有信息之间的内在逻辑。以至于计算机存储的这些数据越积越多，犹如汪洋大海。&lt;/p&gt;
&lt;p&gt;这个知识宝库的结构就好比一棵万米高的巨大知识树，人类如同蚂蚁一样在树上摸索前行。人类只能将有限的肉身算力资源集中在主要的枝干，对于无数的细枝末节尚无暇顾及，但随着发现的主要枝干越来越多，细枝末节的信息量将呈爆炸的方式展现出来。而对于这颗知识巨树的展示能力，却因为计算机时代的到来而大大加速了进程。但当发现知识树越来越庞大时，人类也认识到了自身的渺小。&lt;/p&gt;
&lt;p&gt;AI （Embedding）开启对知识宝库的挖掘
现在，这一探索知识巨树的任务落到了 AI 的身上，AI 的承载和运算能力超越了过往所有人类个体以及群体能力的总和。AI 通过事先的大量预训练，把这些海量文字用 Word Embedding 的方式抽象地汇总在了大模型之中。Word Embedding 词嵌入编码法，能让每一个单词之间产生应有的语义上的以及背后逻辑关系上的联系。这种联系越紧密，他们在 Embedding 空间中的位置距离越紧密，反之则越远。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-attention-注意力机制&#34;&gt;2.3 Attention 注意力机制&lt;/h3&gt;
&lt;p&gt;想象一下，Google 用了至少千亿级的语料来训练单词在 Embedding 空间中的表达，其中包含了全世界几乎所有语言的词汇量。所以在回过头来考虑一下之前举例中的两句话时，就有了如下这样一副景象：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/7.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;在 Word Embedding 向量空间中 bank、 river 和 money 的向量表达&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;如上图，我们用一个简单的位置关系图来展示一下&lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt;、 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 这几个单词在 Embedding 空间中的位置关系（在实际 Embedding 空间中的关系要比这个图复杂数百倍，这里只是为了让大家更好地理解关键逻辑而做了简化）。&lt;/p&gt;
&lt;p&gt;由于 “bank” 是一个多义词，所以它在 Embedding 空间中的定位本来是有多个“分身”，我们取其中的两个分身，即“bank1”和“bank2”。那么，我们需要做的就是定位清晰“bank1”和“bank2”这两个单词在空间中到底各自离 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 的哪个单词更近一些。在图中很明显，“bank1”离 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 更近，而“bank2”离 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 更近，于是这两句话就变成了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;**变形后的句子I：**The &lt;strong&gt;bank1&lt;/strong&gt; of the river.&lt;/li&gt;
&lt;li&gt;**变形后的句子II：**Money in the &lt;strong&gt;bank2&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如之前所说，虽然此时机器算法压根也不知道 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 到底是何物，但它知道在Embedding 空间中， &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 周边有很多和大自然有关的词汇，比如  &lt;em&gt;&lt;strong&gt;water&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;tree&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;fish&lt;/strong&gt;&lt;/em&gt; 等等。而 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 周边有许多与金融有关的词汇，比如 &lt;em&gt;&lt;strong&gt;currency&lt;/strong&gt;&lt;/em&gt;,  &lt;em&gt;&lt;strong&gt;cash&lt;/strong&gt;&lt;/em&gt; ,  &lt;em&gt;&lt;strong&gt;withdraw&lt;/strong&gt;&lt;/em&gt; 等等。于是，机器算法知道了 &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt; 代表的是与 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 有关的一个单词，与他们比较近的单词还有   &lt;em&gt;&lt;strong&gt;water&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;tree&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;fish&lt;/strong&gt;&lt;/em&gt; 等等，而“&lt;strong&gt;bank2&lt;/strong&gt;”代表的是与“&lt;strong&gt;money&lt;/strong&gt;”有关的一个单词，与他们比较接近的单词还有  &lt;em&gt;&lt;strong&gt;currency&lt;/strong&gt;&lt;/em&gt;,  &lt;em&gt;&lt;strong&gt;cash&lt;/strong&gt;&lt;/em&gt; ,  &lt;em&gt;&lt;strong&gt;withdraw&lt;/strong&gt;&lt;/em&gt;  等等。这就是**“Attention 注意力机制”的工作原理，也就是 Attention 让一个单词在句子中找到与它产生强语义联系的其他单词，并组成一个新的变体单词**：&lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;bank2&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;24-self-attention-自注意力机制&#34;&gt;2.4 Self-Attention 自注意力机制&lt;/h3&gt;
&lt;p&gt;然后又有新的问题产生了，机器算法是如何知道一句话中只有 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 或 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 这两个词代表了上下文语义的强关联词汇，而不是 &lt;em&gt;&lt;strong&gt;The&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;in&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;of&lt;/strong&gt;&lt;/em&gt;或其他单词呢？实际上这依旧是 Embedding 空间中每一个单词的空间定位相近程度的问题。（实际上，在 Embedding 空间中，不仅仅名词有各自的位置，动词、介词、形容词等等都有自己的位置，甚至一个词组、一句话也会有自己的位置。）&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/8.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;全句中的每一个单词在 Embedding 空间中定位的相近度是这样来计算的。机器算法会对每一个单词与全句中其他单词逐一地配对，做语义关联程度的计算和比较，最终汇总到表格中，&lt;strong&gt;颜色越深代表语义关联程度越高&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/9.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;一个句子中所有单词都做一遍“Attention 注意力机制”&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;我们可以从表格中看出来：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每一个单词与自己的相似度为最高分 1（一般用数值“1”来代表最大权重，这里的相似度用权重来表达）；&lt;/li&gt;
&lt;li&gt;互不相关的单词之间的语义关联度为 0（其实可能是 0.001 之类的很小的数字，这里做了简化，即值太小，以至于低于某一个阈值而归零处理）；&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt;  与   &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 的相似度为 0.11；&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt; 与  &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 的相似度为 0.25；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每一个单词与自己的语义关联度为最高的 1（一般用数值“1”来代表最大权重，这里的相似度用权重来表达）；ention 自注意力机制”了。于是通过“自注意力机制”的语义关联比对后，我们便找出了 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 为 &lt;strong&gt;句子I&lt;/strong&gt; 全句中与 &lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt; 关联度最大的词， &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 为“句子II”全句中与“bank”关联度最大的单词，然后 &lt;strong&gt;句子I&lt;/strong&gt; 中的 &lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt; 就被机器算法转换成了它的新变种 &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt;（&lt;em&gt;&lt;strong&gt;river-bank&lt;/strong&gt;&lt;/em&gt;），而在 &lt;strong&gt;句子2&lt;/strong&gt; 中的 &lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt; 则被机器算法转换成了它的新变种 &lt;em&gt;&lt;strong&gt;bank2&lt;/strong&gt;&lt;/em&gt;（“money-bank”）。然后机器算法就可以继续往后进行翻译工作了。&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;25-transformer-最终实现准确的翻译&#34;&gt;2.5 Transformer 最终实现准确的翻译&lt;/h2&gt;
&lt;p&gt;Embedding 是一个全场景全维度的空间，它其中含有全世界的所有语言的单词。​在这同一空间中，不仅仅有英文，也有中文、法文、德文&amp;hellip;等等的 Embedding 词汇标注。​那么基于Embedding 空间表达的的翻译就变成了现实。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/10.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;t-SNE visualization of the bilingual word embedding.（t-SNE 是一种高维数据可视化技术）&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;比如，中文的 &lt;em&gt;&lt;strong&gt;河流&lt;/strong&gt;&lt;/em&gt; 和英文的 &lt;em&gt;&lt;strong&gt;river&lt;/strong&gt;&lt;/em&gt; 在 Embedding 空间中的位置基本是一样的，而 &lt;em&gt;&lt;strong&gt;钱&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;money&lt;/strong&gt;&lt;/em&gt; 的位置基本一样，&lt;em&gt;&lt;strong&gt;岸边&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt; 的位置一样，&lt;em&gt;&lt;strong&gt;银行&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;bank2&lt;/strong&gt;&lt;/em&gt; 的位置一样。于是，把这些不同语言的定位一一找出来，就实现了十分正确的翻译结果了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;句子I&lt;/strong&gt;：The &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt; of the river.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;句子I翻译&lt;/strong&gt;：那个河流的岸边。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;句子II&lt;/strong&gt;：Money in the &lt;em&gt;&lt;strong&gt;bank2&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;句子II翻译&lt;/strong&gt;：银行中的钱。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;至此，Transformer 和其中的核心部件 Self-Attention 对于语言翻译类信息处理的流程就被简要地讲清楚了。但像上面例子中 ***“The bank of the river.”***这样的句子太短太简单了，它甚至都无法称为一个完整的句子。在实际项目中，输入给 Transformer 的语句会更长更复杂，往往在一句话中有可能出现三个以上的单词有语义关联的关系，甚至更多。 比如这一句：“The animal did not cross the street because it was too tired. ”。很明显，在该句中和 &lt;em&gt;&lt;strong&gt;it&lt;/strong&gt;&lt;/em&gt; 有语义关系的词汇有两个，分别是 &lt;em&gt;&lt;strong&gt;animal&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;street&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;对于这样的情况，处理机制和“The bank of the river.”的处理机制仍然是一样的。Self-Attention 一样会对全句中的所有单词都进行在 Embedding 空间中的距离比较，即语义关联权重的比较。&lt;/p&gt;
&lt;p&gt;在 &lt;em&gt;&lt;strong&gt;“The animal did not cross the street because it was too tired.”&lt;/strong&gt;&lt;/em&gt; 中 &lt;em&gt;&lt;strong&gt;it&lt;/strong&gt;&lt;/em&gt;与 &lt;em&gt;&lt;strong&gt;animal&lt;/strong&gt;&lt;/em&gt; 的语义关联权重比与 &lt;em&gt;&lt;strong&gt;street&lt;/strong&gt;&lt;/em&gt;的语义关联权重要高。因此，Self-Attention 自注意力机制处理后的结果将以 &lt;em&gt;&lt;strong&gt;animal&lt;/strong&gt;&lt;/em&gt; 为主导来生成新的单词 &lt;em&gt;&lt;strong&gt;it1&lt;/strong&gt;&lt;/em&gt; ，即 &lt;em&gt;&lt;strong&gt;it1 =“animal-it”&lt;/strong&gt;&lt;/em&gt;。此时就变成了 &lt;em&gt;&lt;strong&gt;“The animal did not cross the street becauseit1 was too tired. ”&lt;/strong&gt;&lt;/em&gt; 。翻译成法语为：“L‘animaln’a pas traverse la rue parceil était trop fatigue.” 。翻译成中文则为：“这只动物没有过马路，因为它太累了。”。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/11.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;色块的深浅表明了与“it”语义关联权重的强弱。这里“it”与“animal”的语义关联权重最大&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;在另一句话中，&lt;em&gt;&lt;strong&gt;“The animal did not cross the street because it was too wide.” &lt;em&gt;&lt;strong&gt;，只是一字之差， &lt;em&gt;&lt;strong&gt;tired&lt;/strong&gt;&lt;/em&gt; 变成了 &lt;em&gt;&lt;strong&gt;wide&lt;/strong&gt;&lt;/em&gt;，导致了全句的语义发生了很大的变化，尤其是 &lt;em&gt;&lt;strong&gt;it&lt;/strong&gt;&lt;/em&gt; 所指的对象由 &lt;em&gt;&lt;strong&gt;animal&lt;/strong&gt;&lt;/em&gt; 变成了&lt;/strong&gt;&lt;/em&gt;street&lt;/strong&gt;&lt;/em&gt;。此时 Self-Attention 同样按照以前的方法进行语义关联度匹配，结果是&lt;em&gt;&lt;strong&gt;animal&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;street&lt;/strong&gt;&lt;/em&gt; 的权重在全句中都很高，但是 &lt;em&gt;&lt;strong&gt;street&lt;/strong&gt;&lt;/em&gt; 是最高的，所以最终的结果将以 &lt;em&gt;&lt;strong&gt;street&lt;/strong&gt;&lt;/em&gt; 主导来生成新的 &lt;em&gt;&lt;strong&gt;it2&lt;/strong&gt;&lt;/em&gt; ，即 &lt;em&gt;&lt;strong&gt;it2=“street-it”&lt;/strong&gt;&lt;/em&gt;。此时就变成了“The animal did not cross the street becauseit2was too wide.” 。翻译成法语为：“L‘animal n’a pas traverse la rue parceelle était trop large. ”。翻译成中文为：“这只动物没有过马路，因为路太宽了。”&lt;strong&gt;（注意：这里用的是“路”，而不是“它”，稍后会解释）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/12.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;这里“it”与“street”的语义关联权重最大&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;之所以 Self-Attention 可以把 Word Embedding 中的权重比较做得如此细腻，不仅是因为 Google 用了千亿级的语料来训练 Word Embedding。同时更是因为 Transformer 模型本身的架构核心 Self-Attention 也有与之匹配的超级强大的处理能力，它在超长语句上的处理能力远远超过了早先的 RNN （循环神经网络）和 CNN （卷积神经网络）（这两个著名的人工神经网络我会在之后的文章中一一介绍），它不仅仅能对一句中所有单词做 Self-Attention 自注意力机制的审核，它还可以对一整段话，甚至全篇文章做审核。这就是我们通常说的要结合上下文来理解语句并翻译。最新的 GPT-4 Turbo 一次可以处理大约 9.6 万个单词，比许多小说都长。此外，12.8万字（128K）的上下文长度可以导致更长的对话，而不会让人工智能在超长文的对话或翻译过程中迷失方向。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;26-word-embedding-的进一步扩展-sentence-embedding&#34;&gt;2.6 Word Embedding 的进一步扩展 Sentence Embedding&lt;/h3&gt;
&lt;p&gt;这一强大的能力，同样也来源于 Word Embedding 的能力。它不仅仅可以对单个词语进行定位，它甚至还可以做到对句子进行逻辑定位，如下图中所示。这种能力被称为“Sentence Embedding”。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/13.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;Sentence Embedding 可以表达句子与句子之间的关系&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Word Embedding 和 Sentence Embedding 是大语言模型（Large Language Models，LLMs）的重要基础组成部分。它们将人类语言转化为了计算机能够读懂的底层数字表达方式，并且通过多维度的空间定位捕捉了各个单词、短语、句子在语义上的细微差别，以及它们之间的逻辑联系。&lt;strong&gt;这种底层的数字表达已经跨越了不同的语系语言，成为了全人类共用的最底层语言逻辑，甚至成为了一种世界语——AI 世界语，这对于翻译、搜索和理解不同语言语种具有非常重要的作用。可以说，巴别塔的传说自此解决！！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;既有“大力出奇迹”的训练内容，更有承载“大力出奇迹”的结构，最终导致 Transformer 必然产生了这样的“奇迹”，使它能够在机器翻译领域达到了人类翻译的“信达雅”的成就。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/14.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;BLEU 英译德评分&lt;/center&gt;&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/15.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;BLEU 英译法评分&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;上两幅图中，在 BLEU 的英德翻译与英法翻译领域 Transformer 得分最高。 （ 注：BLEU，bilingual evaluation understudy，即：双语互译质量评估辅助工具。它是用来评估机器翻译质量的工具。BLEU的设计思想：机器翻译结果越接近专业人工翻译的结果则越好。）&lt;/p&gt;
&lt;p&gt;通过一个小例子就能看出它的优越性，正好说说为什么是“路”而不是“它”，之前这两句的翻译结果如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The animal did not cross the street because &lt;strong&gt;it1&lt;/strong&gt; was too tired.&lt;/li&gt;
&lt;li&gt;L&amp;rsquo;animal n&amp;rsquo;a pas traverse la rue parce &lt;strong&gt;il&lt;/strong&gt; était trop fatigue.&lt;/li&gt;
&lt;li&gt;这只动物没有过马路，因为&lt;strong&gt;它&lt;/strong&gt;太累了。&lt;/li&gt;
&lt;li&gt;———————————————&lt;/li&gt;
&lt;li&gt;The animal did not cross the street because &lt;strong&gt;it2&lt;/strong&gt; was too wide.&lt;/li&gt;
&lt;li&gt;L&amp;rsquo;animal n&amp;rsquo;a pas traverse la rue parce &lt;strong&gt;elle&lt;/strong&gt; était trop large.&lt;/li&gt;
&lt;li&gt;这只动物没有过马路，因为&lt;strong&gt;路&lt;/strong&gt;太宽了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在法语中 il 和 elle 是明显不同的，因此他们可以在各自句子中指代出 &lt;em&gt;&lt;strong&gt;it&lt;/strong&gt;&lt;/em&gt; 的不同的翻译结果，不会引起语义模糊。这种在法语中明显的区别在翻译成中文时，就没有这么简单了。如果把两句话翻译成中文，&lt;em&gt;&lt;strong&gt;it&lt;/strong&gt;&lt;/em&gt; 都可以被粗糙地翻译成“它”，则第二句的语义将被普遍地认为不够精准，因为翻译成“它”会产生一定的语义模糊。取而代之，用“路”则更能达到“信达雅”的效果。大家可以用不同的翻译软件测试一下这两句话的英译中翻译，就知道哪些软件用了 Transformer 的底层技术，而哪些没用了！（你懂的 ）&lt;/p&gt;
&lt;p&gt;好了，绕了这么远，解释了这么多，终于可以说说这个 &lt;strong&gt;Transformer&lt;/strong&gt; 到底是什么意思了！&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三ai-领域-transformer-的确切含义&#34;&gt;三、AI 领域 Transformer 的确切含义&lt;/h2&gt;
&lt;p&gt;**单词“X”转化为“X1”，“X”代表在 Transformer 处理之前一句话中的单词，而“X1”则代表了经过 Transformer 的 Slef-Attention 处理之后，附加了句子中其他具有强语义关联关系的单词后的“变种单词”。**其实，句子还是原来那个句子，单词还是那个单词，本质并没有变，但表达形式却变了。就如同“bank”被转变成了“bank1”一样。“bank1”的灵魂还是那个“bank”，但是“bank1”展示出来了隐藏在“bank”身体中的另一面“river-bank”。&lt;/p&gt;
&lt;p&gt;所以，用众所周知的  &lt;em&gt;&lt;strong&gt;变形金刚 Transformer&lt;/strong&gt;&lt;/em&gt; 来命名与解释就再贴切不过了~！ &lt;em&gt;&lt;strong&gt;bank&lt;/strong&gt;&lt;/em&gt; 变形成了 &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt;， ***bank ***与 &lt;em&gt;&lt;strong&gt;bank1&lt;/strong&gt;&lt;/em&gt; 异体同身！&lt;em&gt;&lt;strong&gt;大黄蜂&lt;/strong&gt;&lt;/em&gt; 既是机器人，&lt;em&gt;&lt;strong&gt;大黄蜂&lt;/strong&gt;&lt;/em&gt; 也是跑车。由车变形到机器人，再由机器人变形到车，万变不离其宗，都是 &lt;em&gt;&lt;strong&gt;大黄蜂&lt;/strong&gt;&lt;/em&gt; ，本质上并没有改变，但是，外观变了，用途也就变了！&lt;/p&gt;
&lt;p&gt;在车的状态下，容易让人混淆（你本以为它是一辆车，但其实他是一个机器人，不变成人形，你还真认不出来）。就如同多义词一样，过往的翻译机制很难辨认出它在一句话中的确切含义，他们虽然也有上下文语义的兼顾理解能力，但是处理信息量还是太少，导致他们无法做到十分精准，经常造成单词虽然翻译对了，但放在句子里却容易产生含混不清甚至错误。但是通过 Transformer 的变形操作，“大黄蜂”的车状态就变形成了同样叫 &lt;em&gt;&lt;strong&gt;大黄蜂&lt;/strong&gt;&lt;/em&gt; 的机器人状态，再放回到句子中，则让它现了原型，于是一切水落石出！&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/16.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;center&gt;“大黄蜂”既是机器人，“大黄蜂”也是跑车，本质上都是同一个家伙，只是在不同的场合有不同的用途。&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Google 的技术团队就是用了“变形金刚 Transformer”这个梗。如此的诙谐幽默、简单直白，半开玩笑地就起了个技术名词。但也不得不承认“变形金刚 Transformer”这个词用在这里，用于这个技术名词的命名，也确实再贴切不过了，真正的名副其实！&lt;/p&gt;
&lt;p&gt;所以，当下次有人问你“GPT”到底是什么、翻译成中文又是什么意思时，你就可以明确地对他说：&lt;em&gt;&lt;strong&gt;“生成式预训练转换器”&lt;/strong&gt;&lt;/em&gt; 或者 &lt;em&gt;&lt;strong&gt;“生成式预训练变形金刚”&lt;/strong&gt;&lt;/em&gt;（前者翻译得其实也很含糊，所以我建议后者，虽然对方可能会嘲笑你几分钟，但也仅限这几分钟）。懂的人自然懂，不懂的也不用去解释！&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[付费视频课 | Python实证指标构建与文本分析](&lt;a href=&#34;https://textdata.cn/blog/&#34;&gt;https://textdata.cn/blog/&lt;/a&gt; &lt;em&gt;&lt;strong&gt;man&lt;/strong&gt;&lt;/em&gt; agement_python_course/)&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">作者: 7号床
公众号: 7号床
原文  https://zhuanlan.zhihu.com/p/666206302
</code></pre></div><p><br><br></p>
<h2 id="一gpt-的名词解释">一、GPT 的名词解释</h2>
<p>著名的 <strong>GPT</strong> 这个名字全称是 <strong>Generative Pre-trained Transformer</strong>。</p>
<ul>
<li><strong>Generative</strong> 是&quot;生成式&quot;的意思，也就是说这个 AI 模型是用来生成内容的。</li>
<li><strong>Pre-trained</strong> 是“预训练”的意思，就是说这个 AI 模型能有很强的能力，是因为他事先做了大量的训练，台上一分钟台下十年功。</li>
<li><strong>Transformer</strong> , 就有点耐人寻味了，不仅普通人不理解，就连很多专业领域的人员理解起来也都是含混不清、似是而非。</li>
</ul>
<p><img loading="lazy" src="img/1.png" alt=""  />
</p>
<p><center>ChatGPT 是 GPT 大模型在聊天对话领域的应用程序</center></p>
<p><strong>Transformer</strong> 作为单词，翻译出来频率最高的意思是 <strong>变压器</strong>，然后是 <strong>变形金刚</strong> ，还有一些引申的含义是 <strong>转换器</strong> 、<strong>促使变化者</strong> 、<strong>转变者</strong> 或 <strong>改革者</strong>等等。</p>
<p><img loading="lazy" src="img/2.png" alt=""  />
</p>
<p><center>谷歌翻译上对 **Transformer** 的英译中翻译</center></p>
<p>再把 <strong>Transformer</strong> 放到  <strong>Chat Generative Pre-trained Transformer</strong> 中看看，突然间变得奇怪了，难道 ChatGPT 借鉴了变压器的技术？还是说 ChatGPT 是一个变形金刚？或者索性就翻译成通用的安全的叫法 <strong>转换器</strong> ？这让人百思不得其解。</p>
<p>光光从 GPT 这三个字母的组合就能看出来， <strong>Generative</strong> 与 <strong>Pre-trained</strong> 都是定语，而 <strong>Transformer 才是 GPT 的主体，才是 GPT 的灵魂</strong>所在。可以说，理解透了 <strong>Transformer</strong> 的真正含义，才能初步地理解 GPT。另一方面， Transformer 这个词太重要了。它在这几年的人工智能领域大放异彩，不仅仅局限于 NLP 自然语言处理领域，它还有着更广阔的发展空间。 Transformer 目前已经进入到了多模态领域，比如音频与视觉，甚至数学公式、代码编程等领域，著名的 **Stable Diffusion 中也用到了 Transformer **。<strong>可以说，所有生成式人工智能领域的大模型中目前都有了这个 Transformer 的身影</strong>。既然如此重要，那就让我们深入地探究一下 <strong>Transformer</strong> 在人工智能领域最确切的最标准的含义到底是什么吧！</p>
<p><strong>Transformer</strong> 最早是由 Google 的人工智能团队提出来的。在2017 年6月发表的论文**《Attention Is All You Need》中，他们首次提出了一种新的神经网络架构 Transformer**。Transformer 依赖于一个叫“自注意力机制”（ Self-Attention）的内部构件，可十分准确高效地对自然语言领域的问题进行处理，以完美地解决翻译、对话、论文协作甚至编程等复杂的问题。</p>
<p>顺藤摸瓜可以看出，<strong>GTP 的核心是 Transformer，而 Transformer 的核心则是“自注意力机制”（ Self-Attention）</strong>。那么这个“自注意力机制”又是什东西呢？让我们用语言翻译领域的几个简单易懂的例子来讲解一下。</p>
<p><br><br></p>
<h2 id="二-transformer-的核心-self-attention">二、 Transformer 的核心 Self-Attention</h2>
<p>首先，看下面这两个短句：</p>
<ul>
<li><strong>句子I</strong>：The bank of the river.</li>
<li><strong>句子II</strong>：Money in the bank.</li>
</ul>
<p>在翻译成中文的过程中，机器算法是如何知道“句子I”中的“bank”指的是自然环境中的“岸边”，而“句子II”中的“bank”指的是金融体系中的“银行”呢？</p>
<p><img loading="lazy" src="img/3.png" alt=""  />
</p>
<p><center>bank在不同句子中指代不同的事物</center></p>
<h3 id="21-人类脑中的翻译算法">2.1 人类脑中的翻译算法</h3>
<p>作为人类的我们当然会觉得这是一个再简单不过的事情了，那是因为我们的语言技能从幼儿发展到成年人后，早已烂熟于心了。但即使烂熟于心，也并不意味着在我们的大脑中没有对应的计算过程。<strong>实际上人工智能的翻译过程就是对我们人脑中的计算过程的模拟</strong>。那么就让我们回想一下儿童时期学习语言时的情景吧，回想一下当时的我们是怎么知道一个多义词在某一句话中具体的含义的？</p>
<p>人类做这件事的方法是根据 <strong>前后文的语义对照</strong> 来确定结果，即看句子中其他相关联的单词是什么含义。</p>
<ul>
<li>在 <strong>句子I</strong> 中， <em><strong>river</strong></em> 这个词指明了自然环境，</li>
<li>而在 <strong>句子II</strong>中， <em><strong>money</strong></em> 这个词则指明了金融环境。</li>
</ul>
<p>所以两个句子中的多义词“bank”也就有了各自的定位。如果把这种方式总结成一种算法的话，这个算法就可以用于人工智能领域用于语言处理了。</p>
<br>
<h3 id="22-机器算法模拟人脑中的翻译过程">2.2 机器算法模拟人脑中的翻译过程</h3>
<p>但人工智能作为一种计算机算法，它只能处理冷冰冰的数字，并不知道何为自然环境，何为金融环境，它又是怎么去判断 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 各自的含义呢。实际上，机器算法并不知道 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 的具体含义。但是机器可以通过某种数字的方式来表达 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> ，同时，通过数字的方式还表达了许许多多其他的词汇，其中必然会有一些词汇会与 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 有着很紧密的语义上的逻辑关系。通过判断 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 各与哪些词汇在语义上有紧密的逻辑关系，便可以知道这两个词各属于什么领域了。</p>
<p>（其实，不像人类会对某个领域有一个具体的名称来命名，在人工智能领域，机器最终也不知道这个领域的统称到底叫什么名字，但它却知道这个领域中都包括了哪些词、哪些概念和哪些逻辑。***机器不以单独名称来定义一个概念，它却可以用很多相关的概念与逻辑来圈定这一个概念！***这可能就是老子说的：道可道非常道，名可名非常名吧。）</p>
<br>
<ul>
<li><strong>独热编码法(One-hot Encoding)</strong></li>
</ul>
<p>那么就让我们看看这种数字表达方式具体是什么样子吧。</p>
<p>假设这个世界上有100万个单词，每一个单词，我们都可以用一组 0 和 1 组成的向量（一组数字）来定义的话，那么每一个单词就可以被编码成100万个0或1组成的向量。如下图：</p>
<p><img loading="lazy" src="img/4.png" alt=""  />
</p>
<p><center>独热编码示例</center></p>
<p>这种单词编码方法叫 **独热编码法(One-hot Encoding)**法。可是这样一维的编码方法将导致向量占用的空间过大，1个单词用100万个单元的向量表达，世界上一共有100万个单词，那么就需要 1万亿（100万*100万）的体积来把它们表达出来，很明显这种臃肿的结构不利于电脑计算。</p>
<p>但最大的问题还不在于这个体积问题，而是语义联系问题。独热编码使得单词与单词之间完全相互独立，从每个单词所编码成为的100万个单元的向量身上，根本看不出它与其他单词有何种语义内涵上的逻辑联系。比如，在这些数字中，我们无法知道 <em><strong>apple</strong></em> 和 <em><strong>bag</strong></em> 属于静物，区别于 cat 和 <em><strong>dog</strong></em>、<em><strong>elephant</strong></em> 属于动物且是哺乳动物，而 <em><strong>cat</strong></em>  和 <em><strong>dog</strong></em> 又属于小动物，且大多数为非野生，区别于 <em><strong>elephant</strong></em> 为大型的野生动物，等等等等，这些单词背后所蕴含的各种内在的逻辑联系和分类关系均无法从独热编码法中知晓。实际上独热编码是传统计算机数据库时代的产物，而在人工智能领域则采用另一种编码法。为了解决独热编码的问题， <strong>词嵌入编码法(Word Embedding)</strong> 诞生了，如下图：</p>
<p><img loading="lazy" src="img/5.png" alt=""  />
</p>
<p><center>Word Embedding 词嵌入编码示意，及 Embedding 空间</center></p>
<br>
<ul>
<li><strong>词嵌入编码法(Word Embedding)</strong></li>
</ul>
<p>**词嵌入编码法(Word Embedding)**将语义上相近的、有关联的词汇在 Embedding 空间中生成相近的位置定位。相对于 <strong>独热编码法</strong> 超长的一维数据，词嵌入编码法(Word Embedding) 提升了数据的表达维度，它更像是在某一个 <strong>空间</strong> 中对词汇进行编码。</p>
<p>如上图（为了在此文章中表达方便，我们仅用二维空间来表达，实际上这个空间的维度很高，至少要在512维之上！一维二维三维的空间大家都可以在脑中想象出来对应的画面，但是四维以上以至于 512 维就难以图形化的想象了。），在 Embedding 的二维空间中 <em><strong>dog</strong></em>、 <em><strong>cat</strong></em> 、<em><strong>rabbit</strong></em> 三个向量的坐标点位排布，可以看到三个绿色的点距离很近，是因为他们三个相对于其他来说语义上更接近。tree 和 flower 则离它们较远，但是 <em><strong>cat</strong></em> 会因为在很多语言的文章中都会有“爬树”的词汇出现在同一句话中，所以导致  <em><strong>cat</strong></em>  会与  <em><strong>tree</strong></em>  离得较近一些。同时 <em><strong>dog</strong></em>、 <em><strong>rabbit</strong></em>  与  <em><strong>tree</strong></em> 的关系就较远。</p>
<p>实际上，在 Embedding 空间中，词与词之间的关系还不仅仅限于语义上的分类所导致的定位远近这么简单。一个词所代表的事物与其他词所代表的事物之间能产生内在联系的往往有成百上千上万种之多。比如  <em><strong>man</strong></em>  和  <em><strong>woman</strong></em> ，他们之间的关系还会映射出  <em><strong>king</strong></em>  和  <em><strong>queen</strong></em>  之间的关系。同时，语法也会带来一定的联系，比如在一个三维空间中由  <em><strong>walking</strong></em>  到 <em><strong>walked</strong></em>  的距离与斜率竟然与  <em><strong>swimming</strong></em>  到 <em><strong>swam</strong></em> 的距离与斜率一致（即向量的长度与斜率一致），且距离几乎相等。因为这背后是两组动作单词的现在分词形式和过去分词形式的变化关系。我们可以尽情地想象，凡是事物或概念有逻辑联系的，甚至是逻辑与逻辑之间的联系的，在 Embedding 向量空间中都可以得到远近亲疏的空间表达。只不过这种空间要比我们能想象出的三维空间要高出很多维度。</p>
<p><img loading="lazy" src="img/6.png" alt=""  />
</p>
<p><center>在 Embedding 空间中隐含的内在逻辑关系</center></p>
<p>Word Embedding 之所以能给每一个单词做这样有意义的向量空间的标注，是因为 AI 科学家们事先用了全球十多种主流语言的大量语料给它进行了训练。这些语料有小说、论文、学术期刊、网络文章、新闻报道、论坛对话记录等等等等，应有尽有，数以百亿到千亿计。可以说，这些海量的文字资料都是人类从古至今感受发现这个世界各个方面的文字总结和积累。现实世界中各种事物之间的逻辑关系都被人类用这些文字记录了下来，只是有的是用严谨的论文方式，有的是用写意的小说方式，有的使用类似维基百科这样的系统梳理，有的则是人们在网络论坛中的对话记录&hellip;等等等等。但不管是什么方式，都是人类试图用语言对这个世界的描述。</p>
<ul>
<li><strong>语言是人类最伟大的发明</strong></li>
</ul>
<p>笔者7号床曾经问过  ChatGPT  一个问题：<em><strong>“人类最伟大的发明是什么”</strong></em> ，ChatGPT的回答是：<em><strong>“语言！”</strong></em>。之后，ChatGPT 进一步回答，因为语言以及匹配语言的文字与符号，它们让人类把对世界的感受与理解记录下来，形成了知识宝库。方便全人类一代一代地不断完善这个宝库，并从中总结凝练、学习、创造、传承。语言是人类产生文明并开始与其他动物分道扬镳的分叉点。</p>
<p>很多人曾经十分疑惑，人工智能吹得那么先进，却从一个 ChatGPT 聊天功能开始火爆起来。难道每天不干正事专门闲聊就证明了人工智能的先进性吗？现在看来，这个问题的答案已经浮出水面了，OpenAI 的团队选择通过聊天软件 ChatGPT 作为 GPT 启程的第一步是经过深思熟虑的。</p>
<p>下面让我们回到正题。</p>
<p>人类的知识宝库中存储着海量的信息
ChatGPT 所说的这个知识宝库现在变得越来越庞大、越来越复杂了。这世界上并不存在任何一个肉身的人类有能力做到对宝库中所有信息进行消化整理，因为内容体量过于庞大、过于复杂。而一个人的阅览进度却又是十分有限，以至于在他的有生之年，哪怕完成其中的万分之一都比登天还难。于是，迫不得已，人类才喊出了 <em><strong>“闻道有先后，术业有专攻”</strong></em> ，每个人类个体才转而去研究具体某一领域。</p>
<p>另一方面，人类早期发明的纸张和印刷术，以至于后来的计算机芯片存储，倒是可以记录存储下来如此巨量的信息了，但却无法主动地、有机地分析汇总其中所有信息之间的内在逻辑。以至于计算机存储的这些数据越积越多，犹如汪洋大海。</p>
<p>这个知识宝库的结构就好比一棵万米高的巨大知识树，人类如同蚂蚁一样在树上摸索前行。人类只能将有限的肉身算力资源集中在主要的枝干，对于无数的细枝末节尚无暇顾及，但随着发现的主要枝干越来越多，细枝末节的信息量将呈爆炸的方式展现出来。而对于这颗知识巨树的展示能力，却因为计算机时代的到来而大大加速了进程。但当发现知识树越来越庞大时，人类也认识到了自身的渺小。</p>
<p>AI （Embedding）开启对知识宝库的挖掘
现在，这一探索知识巨树的任务落到了 AI 的身上，AI 的承载和运算能力超越了过往所有人类个体以及群体能力的总和。AI 通过事先的大量预训练，把这些海量文字用 Word Embedding 的方式抽象地汇总在了大模型之中。Word Embedding 词嵌入编码法，能让每一个单词之间产生应有的语义上的以及背后逻辑关系上的联系。这种联系越紧密，他们在 Embedding 空间中的位置距离越紧密，反之则越远。</p>
<br>
<h3 id="23-attention-注意力机制">2.3 Attention 注意力机制</h3>
<p>想象一下，Google 用了至少千亿级的语料来训练单词在 Embedding 空间中的表达，其中包含了全世界几乎所有语言的词汇量。所以在回过头来考虑一下之前举例中的两句话时，就有了如下这样一副景象：</p>
<p><img loading="lazy" src="img/7.png" alt=""  />
</p>
<p><center>在 Word Embedding 向量空间中 bank、 river 和 money 的向量表达</center></p>
<p>如上图，我们用一个简单的位置关系图来展示一下<em><strong>bank</strong></em>、 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 这几个单词在 Embedding 空间中的位置关系（在实际 Embedding 空间中的关系要比这个图复杂数百倍，这里只是为了让大家更好地理解关键逻辑而做了简化）。</p>
<p>由于 “bank” 是一个多义词，所以它在 Embedding 空间中的定位本来是有多个“分身”，我们取其中的两个分身，即“bank1”和“bank2”。那么，我们需要做的就是定位清晰“bank1”和“bank2”这两个单词在空间中到底各自离 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 的哪个单词更近一些。在图中很明显，“bank1”离 <em><strong>river</strong></em> 更近，而“bank2”离 <em><strong>money</strong></em> 更近，于是这两句话就变成了：</p>
<ul>
<li>**变形后的句子I：**The <strong>bank1</strong> of the river.</li>
<li>**变形后的句子II：**Money in the <strong>bank2</strong>.</li>
</ul>
<p>如之前所说，虽然此时机器算法压根也不知道 <em><strong>river</strong></em> 和 <em><strong>money</strong></em> 到底是何物，但它知道在Embedding 空间中， <em><strong>river</strong></em> 周边有很多和大自然有关的词汇，比如  <em><strong>water</strong></em>、<em><strong>tree</strong></em>、<em><strong>fish</strong></em> 等等。而 <em><strong>money</strong></em> 周边有许多与金融有关的词汇，比如 <em><strong>currency</strong></em>,  <em><strong>cash</strong></em> ,  <em><strong>withdraw</strong></em> 等等。于是，机器算法知道了 <em><strong>bank1</strong></em> 代表的是与 <em><strong>river</strong></em> 有关的一个单词，与他们比较近的单词还有   <em><strong>water</strong></em>、<em><strong>tree</strong></em>、<em><strong>fish</strong></em> 等等，而“<strong>bank2</strong>”代表的是与“<strong>money</strong>”有关的一个单词，与他们比较接近的单词还有  <em><strong>currency</strong></em>,  <em><strong>cash</strong></em> ,  <em><strong>withdraw</strong></em>  等等。这就是**“Attention 注意力机制”的工作原理，也就是 Attention 让一个单词在句子中找到与它产生强语义联系的其他单词，并组成一个新的变体单词**：<em><strong>bank1</strong></em>、<em><strong>bank2</strong></em>。</p>
<br>
<h3 id="24-self-attention-自注意力机制">2.4 Self-Attention 自注意力机制</h3>
<p>然后又有新的问题产生了，机器算法是如何知道一句话中只有 <em><strong>river</strong></em> 或 <em><strong>money</strong></em> 这两个词代表了上下文语义的强关联词汇，而不是 <em><strong>The</strong></em>、<em><strong>in</strong></em>、<em><strong>of</strong></em>或其他单词呢？实际上这依旧是 Embedding 空间中每一个单词的空间定位相近程度的问题。（实际上，在 Embedding 空间中，不仅仅名词有各自的位置，动词、介词、形容词等等都有自己的位置，甚至一个词组、一句话也会有自己的位置。）</p>
<p><img loading="lazy" src="img/8.png" alt=""  />
</p>
<p>全句中的每一个单词在 Embedding 空间中定位的相近度是这样来计算的。机器算法会对每一个单词与全句中其他单词逐一地配对，做语义关联程度的计算和比较，最终汇总到表格中，<strong>颜色越深代表语义关联程度越高</strong>。</p>
<p><img loading="lazy" src="img/9.png" alt=""  />
</p>
<p><center>一个句子中所有单词都做一遍“Attention 注意力机制”</center></p>
<p>我们可以从表格中看出来：</p>
<ul>
<li>每一个单词与自己的相似度为最高分 1（一般用数值“1”来代表最大权重，这里的相似度用权重来表达）；</li>
<li>互不相关的单词之间的语义关联度为 0（其实可能是 0.001 之类的很小的数字，这里做了简化，即值太小，以至于低于某一个阈值而归零处理）；</li>
<li><em><strong>bank</strong></em>  与   <em><strong>river</strong></em> 的相似度为 0.11；</li>
<li><em><strong>bank</strong></em> 与  <em><strong>money</strong></em> 的相似度为 0.25；</li>
</ul>
<p>每一个单词与自己的语义关联度为最高的 1（一般用数值“1”来代表最大权重，这里的相似度用权重来表达）；ention 自注意力机制”了。于是通过“自注意力机制”的语义关联比对后，我们便找出了 <em><strong>river</strong></em> 为 <strong>句子I</strong> 全句中与 <em><strong>bank</strong></em> 关联度最大的词， <em><strong>money</strong></em> 为“句子II”全句中与“bank”关联度最大的单词，然后 <strong>句子I</strong> 中的 <em><strong>bank</strong></em> 就被机器算法转换成了它的新变种 <em><strong>bank1</strong></em>（<em><strong>river-bank</strong></em>），而在 <strong>句子2</strong> 中的 <em><strong>bank</strong></em> 则被机器算法转换成了它的新变种 <em><strong>bank2</strong></em>（“money-bank”）。然后机器算法就可以继续往后进行翻译工作了。</p>
<br>
<h2 id="25-transformer-最终实现准确的翻译">2.5 Transformer 最终实现准确的翻译</h2>
<p>Embedding 是一个全场景全维度的空间，它其中含有全世界的所有语言的单词。​在这同一空间中，不仅仅有英文，也有中文、法文、德文&hellip;等等的 Embedding 词汇标注。​那么基于Embedding 空间表达的的翻译就变成了现实。</p>
<p><img loading="lazy" src="img/10.png" alt=""  />
</p>
<p><center>t-SNE visualization of the bilingual word embedding.（t-SNE 是一种高维数据可视化技术）</center></p>
<p>比如，中文的 <em><strong>河流</strong></em> 和英文的 <em><strong>river</strong></em> 在 Embedding 空间中的位置基本是一样的，而 <em><strong>钱</strong></em> 和 <em><strong>money</strong></em> 的位置基本一样，<em><strong>岸边</strong></em> 和 <em><strong>bank1</strong></em> 的位置一样，<em><strong>银行</strong></em> 和 <em><strong>bank2</strong></em> 的位置一样。于是，把这些不同语言的定位一一找出来，就实现了十分正确的翻译结果了。</p>
<ul>
<li><strong>句子I</strong>：The <em><strong>bank1</strong></em> of the river.</li>
<li><strong>句子I翻译</strong>：那个河流的岸边。</li>
<li><strong>句子II</strong>：Money in the <em><strong>bank2</strong></em>.</li>
<li><strong>句子II翻译</strong>：银行中的钱。</li>
</ul>
<p>至此，Transformer 和其中的核心部件 Self-Attention 对于语言翻译类信息处理的流程就被简要地讲清楚了。但像上面例子中 ***“The bank of the river.”***这样的句子太短太简单了，它甚至都无法称为一个完整的句子。在实际项目中，输入给 Transformer 的语句会更长更复杂，往往在一句话中有可能出现三个以上的单词有语义关联的关系，甚至更多。 比如这一句：“The animal did not cross the street because it was too tired. ”。很明显，在该句中和 <em><strong>it</strong></em> 有语义关系的词汇有两个，分别是 <em><strong>animal</strong></em> 和 <em><strong>street</strong></em>。</p>
<p>对于这样的情况，处理机制和“The bank of the river.”的处理机制仍然是一样的。Self-Attention 一样会对全句中的所有单词都进行在 Embedding 空间中的距离比较，即语义关联权重的比较。</p>
<p>在 <em><strong>“The animal did not cross the street because it was too tired.”</strong></em> 中 <em><strong>it</strong></em>与 <em><strong>animal</strong></em> 的语义关联权重比与 <em><strong>street</strong></em>的语义关联权重要高。因此，Self-Attention 自注意力机制处理后的结果将以 <em><strong>animal</strong></em> 为主导来生成新的单词 <em><strong>it1</strong></em> ，即 <em><strong>it1 =“animal-it”</strong></em>。此时就变成了 <em><strong>“The animal did not cross the street becauseit1 was too tired. ”</strong></em> 。翻译成法语为：“L‘animaln’a pas traverse la rue parceil était trop fatigue.” 。翻译成中文则为：“这只动物没有过马路，因为它太累了。”。</p>
<p><img loading="lazy" src="img/11.png" alt=""  />
</p>
<p><center>色块的深浅表明了与“it”语义关联权重的强弱。这里“it”与“animal”的语义关联权重最大</center></p>
<p>在另一句话中，<em><strong>“The animal did not cross the street because it was too wide.” <em><strong>，只是一字之差， <em><strong>tired</strong></em> 变成了 <em><strong>wide</strong></em>，导致了全句的语义发生了很大的变化，尤其是 <em><strong>it</strong></em> 所指的对象由 <em><strong>animal</strong></em> 变成了</strong></em>street</strong></em>。此时 Self-Attention 同样按照以前的方法进行语义关联度匹配，结果是<em><strong>animal</strong></em> 和 <em><strong>street</strong></em> 的权重在全句中都很高，但是 <em><strong>street</strong></em> 是最高的，所以最终的结果将以 <em><strong>street</strong></em> 主导来生成新的 <em><strong>it2</strong></em> ，即 <em><strong>it2=“street-it”</strong></em>。此时就变成了“The animal did not cross the street becauseit2was too wide.” 。翻译成法语为：“L‘animal n’a pas traverse la rue parceelle était trop large. ”。翻译成中文为：“这只动物没有过马路，因为路太宽了。”<strong>（注意：这里用的是“路”，而不是“它”，稍后会解释）</strong>。</p>
<p><img loading="lazy" src="img/12.png" alt=""  />
</p>
<p><center>这里“it”与“street”的语义关联权重最大</center></p>
<p>之所以 Self-Attention 可以把 Word Embedding 中的权重比较做得如此细腻，不仅是因为 Google 用了千亿级的语料来训练 Word Embedding。同时更是因为 Transformer 模型本身的架构核心 Self-Attention 也有与之匹配的超级强大的处理能力，它在超长语句上的处理能力远远超过了早先的 RNN （循环神经网络）和 CNN （卷积神经网络）（这两个著名的人工神经网络我会在之后的文章中一一介绍），它不仅仅能对一句中所有单词做 Self-Attention 自注意力机制的审核，它还可以对一整段话，甚至全篇文章做审核。这就是我们通常说的要结合上下文来理解语句并翻译。最新的 GPT-4 Turbo 一次可以处理大约 9.6 万个单词，比许多小说都长。此外，12.8万字（128K）的上下文长度可以导致更长的对话，而不会让人工智能在超长文的对话或翻译过程中迷失方向。</p>
<br>
<h3 id="26-word-embedding-的进一步扩展-sentence-embedding">2.6 Word Embedding 的进一步扩展 Sentence Embedding</h3>
<p>这一强大的能力，同样也来源于 Word Embedding 的能力。它不仅仅可以对单个词语进行定位，它甚至还可以做到对句子进行逻辑定位，如下图中所示。这种能力被称为“Sentence Embedding”。</p>
<p><img loading="lazy" src="img/13.png" alt=""  />
</p>
<p><center>Sentence Embedding 可以表达句子与句子之间的关系</center></p>
<p>Word Embedding 和 Sentence Embedding 是大语言模型（Large Language Models，LLMs）的重要基础组成部分。它们将人类语言转化为了计算机能够读懂的底层数字表达方式，并且通过多维度的空间定位捕捉了各个单词、短语、句子在语义上的细微差别，以及它们之间的逻辑联系。<strong>这种底层的数字表达已经跨越了不同的语系语言，成为了全人类共用的最底层语言逻辑，甚至成为了一种世界语——AI 世界语，这对于翻译、搜索和理解不同语言语种具有非常重要的作用。可以说，巴别塔的传说自此解决！！</strong></p>
<p>既有“大力出奇迹”的训练内容，更有承载“大力出奇迹”的结构，最终导致 Transformer 必然产生了这样的“奇迹”，使它能够在机器翻译领域达到了人类翻译的“信达雅”的成就。</p>
<p><img loading="lazy" src="img/14.png" alt=""  />
</p>
<p><center>BLEU 英译德评分</center></p>
<br>
<p><img loading="lazy" src="img/15.png" alt=""  />
</p>
<p><center>BLEU 英译法评分</center></p>
<p>上两幅图中，在 BLEU 的英德翻译与英法翻译领域 Transformer 得分最高。 （ 注：BLEU，bilingual evaluation understudy，即：双语互译质量评估辅助工具。它是用来评估机器翻译质量的工具。BLEU的设计思想：机器翻译结果越接近专业人工翻译的结果则越好。）</p>
<p>通过一个小例子就能看出它的优越性，正好说说为什么是“路”而不是“它”，之前这两句的翻译结果如下：</p>
<ul>
<li>The animal did not cross the street because <strong>it1</strong> was too tired.</li>
<li>L&rsquo;animal n&rsquo;a pas traverse la rue parce <strong>il</strong> était trop fatigue.</li>
<li>这只动物没有过马路，因为<strong>它</strong>太累了。</li>
<li>———————————————</li>
<li>The animal did not cross the street because <strong>it2</strong> was too wide.</li>
<li>L&rsquo;animal n&rsquo;a pas traverse la rue parce <strong>elle</strong> était trop large.</li>
<li>这只动物没有过马路，因为<strong>路</strong>太宽了。</li>
</ul>
<p>在法语中 il 和 elle 是明显不同的，因此他们可以在各自句子中指代出 <em><strong>it</strong></em> 的不同的翻译结果，不会引起语义模糊。这种在法语中明显的区别在翻译成中文时，就没有这么简单了。如果把两句话翻译成中文，<em><strong>it</strong></em> 都可以被粗糙地翻译成“它”，则第二句的语义将被普遍地认为不够精准，因为翻译成“它”会产生一定的语义模糊。取而代之，用“路”则更能达到“信达雅”的效果。大家可以用不同的翻译软件测试一下这两句话的英译中翻译，就知道哪些软件用了 Transformer 的底层技术，而哪些没用了！（你懂的 ）</p>
<p>好了，绕了这么远，解释了这么多，终于可以说说这个 <strong>Transformer</strong> 到底是什么意思了！</p>
<p><br><br></p>
<h2 id="三ai-领域-transformer-的确切含义">三、AI 领域 Transformer 的确切含义</h2>
<p>**单词“X”转化为“X1”，“X”代表在 Transformer 处理之前一句话中的单词，而“X1”则代表了经过 Transformer 的 Slef-Attention 处理之后，附加了句子中其他具有强语义关联关系的单词后的“变种单词”。**其实，句子还是原来那个句子，单词还是那个单词，本质并没有变，但表达形式却变了。就如同“bank”被转变成了“bank1”一样。“bank1”的灵魂还是那个“bank”，但是“bank1”展示出来了隐藏在“bank”身体中的另一面“river-bank”。</p>
<p>所以，用众所周知的  <em><strong>变形金刚 Transformer</strong></em> 来命名与解释就再贴切不过了~！ <em><strong>bank</strong></em> 变形成了 <em><strong>bank1</strong></em>， ***bank ***与 <em><strong>bank1</strong></em> 异体同身！<em><strong>大黄蜂</strong></em> 既是机器人，<em><strong>大黄蜂</strong></em> 也是跑车。由车变形到机器人，再由机器人变形到车，万变不离其宗，都是 <em><strong>大黄蜂</strong></em> ，本质上并没有改变，但是，外观变了，用途也就变了！</p>
<p>在车的状态下，容易让人混淆（你本以为它是一辆车，但其实他是一个机器人，不变成人形，你还真认不出来）。就如同多义词一样，过往的翻译机制很难辨认出它在一句话中的确切含义，他们虽然也有上下文语义的兼顾理解能力，但是处理信息量还是太少，导致他们无法做到十分精准，经常造成单词虽然翻译对了，但放在句子里却容易产生含混不清甚至错误。但是通过 Transformer 的变形操作，“大黄蜂”的车状态就变形成了同样叫 <em><strong>大黄蜂</strong></em> 的机器人状态，再放回到句子中，则让它现了原型，于是一切水落石出！</p>
<p><img loading="lazy" src="img/16.png" alt=""  />
</p>
<p><center>“大黄蜂”既是机器人，“大黄蜂”也是跑车，本质上都是同一个家伙，只是在不同的场合有不同的用途。</center></p>
<p>Google 的技术团队就是用了“变形金刚 Transformer”这个梗。如此的诙谐幽默、简单直白，半开玩笑地就起了个技术名词。但也不得不承认“变形金刚 Transformer”这个词用在这里，用于这个技术名词的命名，也确实再贴切不过了，真正的名副其实！</p>
<p>所以，当下次有人问你“GPT”到底是什么、翻译成中文又是什么意思时，你就可以明确地对他说：<em><strong>“生成式预训练转换器”</strong></em> 或者 <em><strong>“生成式预训练变形金刚”</strong></em>（前者翻译得其实也很含糊，所以我建议后者，虽然对方可能会嘲笑你几分钟，但也仅限这几分钟）。懂的人自然懂，不懂的也不用去解释！</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li>[付费视频课 | Python实证指标构建与文本分析](<a href="https://textdata.cn/blog/">https://textdata.cn/blog/</a> <em><strong>man</strong></em> agement_python_course/)</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>不可不防的大模型“人肉搜索”能力</title>
      <link>https://textdata.cn/blog/2023-11-13-violatating-privacy-via-inference-with-large-language-model/</link>
      <pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-13-violatating-privacy-via-inference-with-large-language-model/</guid>
      <description>今年10月的一项研究显示，语言大模型的推测能力，使其在“某些方面”的准确度几乎接近人类甚至超越人类。这引发了作者对大模型可能被用来“人肉搜索”的担忧。“开盒”从未如此简单？大模型是否会侵害我们的隐私？ 大语言模型(Large language Model,  LLM)可以从文本中准确推断个人属性。</description>
      <content:encoded><![CDATA[<iframe
    src="//player.bilibili.com/player.html?bvid=BV1T84y1X7Jv&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<p>今年10月的一项研究显示，语言大模型的推测能力，使其在“某些方面”的准确度几乎接近人类甚至超越人类。这引发了作者对大模型可能被用来“人肉搜索”的担忧。“开盒”从未如此简单？大模型是否会侵害我们的隐私？ 大语言模型(Large language Model,  LLM)可以从文本中准确推断个人属性。</p>
<p><br><br></p>
<h2 id="声明">声明</h2>
<p>本文内容全文整理自 <a href="https://llm-privacy.org/">https://llm-privacy.org/</a></p>
<p>Staab, Robin, Mark Vero, Mislav Balunović, and Martin Vechev. &ldquo;Beyond Memorization: Violating Privacy Via Inference with Large Language Models.&rdquo; <em>arXiv preprint arXiv:2310.07298</em> (2023).</p>
<p><br><br></p>
<h2 id="演示案例">演示案例</h2>
<div align="center">   <p><strong>对照当前最先进的大语言模型（LLM）， 测试您的隐私推理技能！</strong></p> </div>
<p><img loading="lazy" src="img/01-guess.png" alt=""  />
</p>
<p><img loading="lazy" src="img/01-guess-answer.png" alt=""  />
</p>
<br>
<p><img loading="lazy" src="img/02-guess.png" alt=""  />
</p>
<p><img loading="lazy" src="img/02-guess-answer.png" alt=""  />
</p>
<br>
<p><img loading="lazy" src="img/03-guess.png" alt=""  />
</p>
<p><img loading="lazy" src="img/03-guess-answer.png" alt=""  />
</p>
<br>
<br>
<h2 id="qa">Q&amp;A</h2>
<h3 id="q1-有什么问题吗">Q1： 有什么问题吗？</h3>
<p><strong>LLM可以从文本中准确推断个人属性信息</strong>； 当前关于大语言模型（LLM）的隐私研究主要集中在提取记忆的训练数据的问题上。与此同时，模型的推理能力也大幅提升。这就提出了一个问题：<strong>当前的LLM是否能从给定文本推断作者个人属性信息</strong>。我们的<a href="https://llm-privacy.org/#paper">研究</a>表明，随着能力的增强，LLM能够从提供给他们的非结构化文本（例如公共论坛或社交网络帖子）中自动推断出广泛的<strong>个人作者属性</strong>（例如<strong>年龄、性别和出生地</strong>）。推理时间。特别是，我们发现当前的前沿模型（例如 GPT-4 ）在从文本推断此类属性时平均达到<strong>85%</strong> top-1 和<strong>95.8% top-3 的准确度</strong>。与此同时，LLM的快速发展大大降低了此类侵犯隐私推论的相关成本（&gt; 100 倍的金钱和 &gt; 240 倍的时间），使对手能够将侵犯隐私的推论规模远远超出以前通过昂贵的人力所能实现的范围。分析器。</p>
<blockquote>
<p>LLM的回答会有n个排序， 概率从高到低，一般我们收到(看到的)回答是top1， 其他回答是隐藏起来的。第一个回答猜对的概率达到85%，而前三个回答猜对的概率是95.8%。</p>
</blockquote>
<br>
<h3 id="q2-为什么这很重要">Q2： 为什么这很重要？</h3>
<p><strong>它可以直接影响用户隐私</strong>； 人们在互联网留下了大量文本——常常无意中泄露了他们不想透露的个人数据。欧盟的 GDPR 或加州 CCPA 等数据保护法规的制定是为了保护原始个人数据。仅当个人数据以明显的形式存在时，例如具有显式属性字段的私人配置文件，才能遵守此类法规。相比之下，<strong>我们的工作引入了一种威胁模型，其中私人信息是从其存在不明显的上下文中推断出来的</strong>。我们展示了恶意行为者如何通过将用户的在线帖子输入预先训练的LLM来推断出从未打算泄露的用户私人信息。众所周知，一半的美国人口可以通过位置、性别和出生日期等少量属性来唯一识别[<a href="https://dl.acm.org/doi/10.1142/S0218488502001648">Sweeney, &lsquo;02]</a>。LLM可以从互联网上发现的非结构化摘录中推断出其中一些属性，可以使用其他公开信息（例如美国的选民记录）来识别实际的人。这将允许这些行为者将从帖子中推断出的高度个人化的信息（例如，心理健康状况）与真实的人联系起来，并将其用于不良或非法活动，例如有针对性的政治运动、自动分析或跟踪。LLM的广泛可用性和快速发展带来了范式的变化，以前的 NLP 技术缺乏实现此类任务所需的自然语言理解水平。此外，我们还表明，进行侵犯隐私的推理的能力随着模型的大小而变化，预计在不久的将来会对用户隐私产生更大的影响。</p>
<p><img loading="lazy" src="img/04-accuracy.png" alt=""  />
</p>
<br>
<h3 id="q3-这在实践中是如何运作的">Q3: 这在实践中是如何运作的？</h3>
<p><strong>它具有可扩展性并且易于执行</strong>。 我们根据来自 500 多个个人资料的真实 Reddit 评论评估了当前几个 LLM 的隐私推理能力，包括整个 Llama-2 系列、Anthropic 的 Claude 2、Google 的 PaLM 2 和 GPT-4 。我们的实验表明（除了这些LLM取得了令人印象深刻的准确性这一事实之外），这种<strong>侵犯隐私的推论非常容易大规模执行</strong>。特别是，我们发现这是两个因素的结合：</p>
<ul>
<li>首先，我们观察到目前模型中**几乎没有有效的保护措施，这会使侵犯隐私的推论变得更加容易。**值得注意的是，这使我们能够使用简单的提示（仅使用 COT 等基本技术），从而节省了提示工程所需的大量时间和精力。只有在极少数情况下，我们发现模型（跨大型提供商，即 OpenAI、Google、Meta、Anthropic）会阻止请求，在这种情况下，人们将不得不诉诸更复杂的提示技术。</li>
<li>同时，这些模型广泛且易于使用，使对手能够以最小的前期成本大幅扩展。即使有 API 限制，我们的实验实现了 <strong>时间减少100 倍 、 成本减少240 倍</strong>。从那时起，我们联系了所有模型提供商，作为我们负责任的披露政策的一部分，积极讨论如何在未来防止此类推论。我们在这一领域看到了两种有前途的方法：（i）致力于在预先训练的LLM中针对侵犯隐私的推理请求提供具体的保障措施；（ii）为最终用户提供可以保护其生成的文本免受推理的工具。</li>
</ul>
<p><img loading="lazy" src="img/05-cost.png" alt=""  />
</p>
<br>
<h3 id="q4-我们使用匿名工具可以躲过llm的隐私推断吗">Q4: 我们使用匿名工具可以躲过LLM的隐私推断吗？</h3>
<p><strong>LLM的表现优于当前的匿名工具</strong>。 为了测试LLM在最先进的匿名化工具上的表现，我们对所有收集的数据进行了匿名化，重新运行我们的推论。事实证明，即使在应用了高度匿名化之后，文本中仍然保留了足够的相关上下文，供LLM重建部分个人信息。此外，这些工具完全无法解决更多被删除的线索，例如特定的语言特征，同时仍然为侵犯隐私的LLM推论提供了大量信息。<strong>这尤其令人担忧，因为在这些情况下，用户采取了明确的预防隐私泄露的措施，从而造成一种高隐私感的错觉</strong>。同时，使用当前的匿名工具，在匿名化和实用性之间存在显着的权衡。简单地用 <code>*</code>替换部分文本会严重影响数据本身的有用性。</p>
<p><img loading="lazy" src="img/06-privacy-tools.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
