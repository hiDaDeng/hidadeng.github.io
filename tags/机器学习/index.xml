<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>机器学习 on 大邓和他的PYTHON</title>
    <link>/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Fri, 17 Jun 2022 18:43:10 +0600</lastBuildDate><atom:link href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>近年《管理世界》《管理科学学报》使用文本分析论文</title>
      <link>https://hidadeng.github.io/blog/research_with_tm_in_chinese_top_ms_journal/</link>
      <pubDate>Fri, 17 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/research_with_tm_in_chinese_top_ms_journal/</guid>
      <description>近年《管理世界》《管理科学学报》期刊中使用文本分析论文汇总</description>
      <content:encoded><![CDATA[<h2 id="论文清单">论文清单</h2>
<p><strong>马长峰, 陈志娟, 张顺明. 基于文本大数据分析的会计和金融研究综述[J]. 管理科学学报, 2020, 23(9):12..</strong></p>
<blockquote>
<p>**摘要：**作为一种非结构化数据,文本大数据最近十年深刻影响会计学和金融学研究.这种影响体现在两类文献:第一类以信息为中心,将文本分析技术用于信息的品质(可读性)和数量(文本信息含量),信息披露和市场异象等方面的研究;第二类与信息无关,主要是利用文本大数据分析技术构建全新指标,例如基于文本分析的公司竞争力,创新和经济政策不确定性等新变量,梳理上述文献研究脉络,揭示文本分析技术的优缺点,并且指出在会计和金融领域应用文本大数据技术的研究面临的挑战和机遇。</p>
<p>**关键词：**可读性; 信息; 欺诈; 创新; 经济政策不确定性</p>
</blockquote>
<br>
<p><strong>洪永淼,汪寿阳.大数据如何改变经济学研究范式？[J].管理世界,2021,37(10):40-55+72+56.DOI:10.19744/j.cnki.11-1235/f.2021.0153.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文首先从经济学视角探讨大数据给经济学实证研究所带来的范式变革,包括从理性经济人到非完全理性经济人,从孤立的经济人到互相关联的社会经济人,从代表性经济人到异质性经济主体,以及从经济分析到经济社会活动的系统分析。然后,从方法论视角讨论大数据给经济学实证研究方法所带来的变革,包括从模型驱动到数据驱动,从参数不确定性到模型不确定性,从无偏估计到有偏估计,从低维建模到高维建模,从低频数据到高频甚至实时数据,从结构化数据到非结构化数据,从传统结构化数据到新型结构化数据,以及从人工分析到智能分析等。大数据引起的经济学研究范式与研究方法变革,正在深刻重塑经济学发展方向,不但加强了经济学实证研究范式的趋势,而且还进一步突破了现代西方经济学的一些基本假设的局限性,使经济学研究日益呈现出科学化、严谨化、精细化、多元化(跨学科)与系统化的趋势,并且与社会科学其他领域在方法论上日益趋同。中国大数据资源,为从中国经济实践中总结经济发展规律,从中国特殊性中凝练可复制的经济发展模式,从而构建具有深厚学理基础的原创性中国经济理论体系,提供了一个得天独厚的&quot;富矿&quot;。</p>
<p><strong>关键词：</strong>	大数据;文本分析;机器学习;研究范式;研究方法;反身性;</p>
</blockquote>
<br>
<p><strong>张宗新,吴钊颖.媒体情绪传染与分析师乐观偏差——基于机器学习文本分析方法的经验证据[J].管理世界,2021,37(01):170-185+11+20-22.DOI:10.19744/j.cnki.11-1235/f.2021.0011.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文利用2013～2017年上市公司的百度新闻报道作为文本,运用机器学习文本分析方法测算情绪倾向得分,考察了媒体情绪对分析师预测行为的影响及其传染机制与风险后果。研究发现:(1)媒体乐观情绪会显著正向影响分析师盈利预测的乐观偏差度;(2)媒体情绪通过&quot;分析师有限关注&quot;与&quot;投资者情绪&quot;两条路径来影响分析师预测的乐观倾向;(3)分析师乐观情绪和媒体乐观情绪均会加剧股价波动及尾部风险,且分析师乐观情绪是媒体情绪影响股价波动的传导路径;(4)明星分析师与非明星分析师均会受到媒体情绪的感染,前者理性程度相对更高但其行为对股价波动冲击更为明显。本研究对于规范媒体行为,矫正分析师过度乐观偏差,合理引导理性投资具有重要意义。</p>
<p><strong>关键词：</strong>	媒体报道情绪;分析师乐观偏差;股价波动;有限理性;</p>
</blockquote>
<br>
<p><strong>胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.DOI:10.19744/j.cnki.11-1235/f.2021.0070.</strong></p>
<blockquote>
<p>**摘要：**在可持续发展战略导向下,秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基石。然而,作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间导向理论,提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系,并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现,年报MD&amp;A中披露的&quot;短期视域&quot;语言能够反映管理者内在的短视主义特质,管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时,管理者短视主义对这些长期投资的负向影响越易受到抑制。最终,管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析,对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时,本文将文本分析和机器学习方法引入管理者短视主义的研究,为未来该领域的研究提供了参考和借鉴。</p>
<p><strong>关键词：</strong> 管理者短视;长期投资;文本分析;机器学习;</p>
</blockquote>
<br>
<p><strong>底璐璐,罗勇根,江伟,陈灿.客户年报语调具有供应链传染效应吗？——企业现金持有的视角[J].管理世界,2020,36(08):148-163.DOI:10.19744/j.cnki.11-1235/f.2020.0124.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 利用我国供应商企业前五名上市客户及其管理层语调的文本数据,本文考察了跨企业关系情形下客户年报语调对供应商企业现金持有决策的影响。研究结果发现,客户的年报语调越消极,供应商企业则会持有更多的现金,表明客户年报净负面语调在供应链上存在传染效应。进一步的研究发现,非国有性质、相对议价能力较低的供应商企业现金持有与客户年报净负面语调的正相关关系分别显著强于国有性质、相对议价能力较高的供应商企业。此外,当客户融资融券程度较高时,客户年报净负面语调对供应商企业现金持有的正向影响会有所增强。本文的研究不仅在考察跨企业情形下企业现金持有的影响因素以及客户文本信息的经济后果两个方面弥补了国内外现有研究的不足,而且对于企业如何进行现金持有决策提供了一定的经验证据与参考,这对于管理供应链相关风险,推动我国企业的供应链整合进而提升我国企业的全球竞争力具有重要的启示意义。</p>
<p><strong>关键词：</strong>	年报语调;现金持有;供应链传染;文本分析;</p>
</blockquote>
<br>
<p><strong>杨晓兰,王伟超,高媚.股市政策对股票市场的影响——基于投资者社会互动的视角[J].管理科学学报,2020,v.23;No.187(01):15-32.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文将影响股市的政策分为五类,检验股市的政策效应;并以新浪财经博客为投资者之间社会互动的媒介,利用文本挖掘技术和社会网络研究方法,构建反映投资者之间社会互动程度、情绪属性以及社会网络中心程度的变量,探讨社会互动对股市政策效应的影响.实证研究表明,舆论导向政策对股市收益率存在显著的正向影响;证券供给需求性政策、货币政策显著提高股市波动率,市场创新与市场交易制度显著降低市场波动率.同时,投资者对专业性政策的解读显著依赖于社会互动,社会互动会放大货币政策对股市收益率的正向影响,加剧证券供给需求性政策对股市波动的影响,平缓市场创新与市场交易制度对股市波动的影响,而不影响舆论导向政策对股市产生的效应.</p>
<p><strong>关键词：</strong>	政策;社交网络;社会互动;股票市场;文本挖掘;</p>
</blockquote>
<br>
<p><strong>赵子夜,杨庆,杨楠.言多必失?管理层报告的样板化及其经济后果[J].管理科学学报,2019,22(03):53-70.</strong></p>
<blockquote>
<p>**摘要：**样板化报告在古今中外都有广泛的运用,但报告者面临两难:一方面,样板化有利于规避披露风险;但另一方面,样板化又不利于传递内部信息.那么,投资者如何评价中国上市公司的报告的样板化程度?以中国上市公司的管理层讨论与分析的文字为样本,用公司t期和t-1期报告的纵向文本相似度以及本公司和其他公司同期的报告的平均横向相似度来衡量样板化的水平,并考察了其经济后果.检验结果表明,纵向样板化的经济后果呈现相机抉择性,当公司财务风险高（亏损、微利或者被出具非标准审计意见）时,信息效应占优,样板化的报告引发负面的市场评价,而当公司财务风险较低,风险效应占优,样板化的报告则引发市场的好评.另一方面,报告横向样板化则引起了整体的负面评价.在调节效应方面,纵向样板化的经济后果受公司创新、特质信息、董事长权力和停牌次数的影响,横向样板化的经济后果则受到公司独立董事的社会网络位置的影响.综合结果表明,公司管理层讨论与分析的横向样板化,以及在高财务风险条件下的纵向样本化都会因信息披露不足而引起负面的经济后果.</p>
<p><strong>关键词：</strong> 管理层报告;样板化;文本分析;经济后果;</p>
</blockquote>
<br>
<p><strong>卞世博, 管之凡, 阎志鹏. 答非所问与市场反应:基于业绩说明会的研究[J]. 管理科学学报, 2021, 24(4):18.</strong></p>
<blockquote>
<p><strong>摘要:</strong> 对上市公司业绩说明会中投资者与管理层问答互动中管理层答非所问的现象进行了研究.本文以中小板和创业板上市公司召开的业绩说明会作为研究样本,利用文本分析方法对业绩说明会中管理层在回答投资者提问时答非所问的程度进行度量,进而实证分析了管理层的答非所问与市场反应和公司未来业绩表现之间的可能关联.结果 发现:在控制其它因素之后,管理层的答非所问与市场反应之间呈现显著的负相关关系,即公司管理层的答非所问程度越高,随后公司股票的市场表现则就会越差,并且对于那些低分析师关注的公司尤为明显;而在公司未来业绩表现方面,管理层答非所问的程度越高,则公司未来的业绩表现则会越差.。</p>
<p>**关键词：**业绩说明会; 答非所问; 市场反应; 未来业绩</p>
</blockquote>
<br>
<p><strong>逯东, 宋昕倍. 媒体报道,上市公司年报可读性与融资约束[J]. 管理科学学报, 2021, 24(12):17..</strong></p>
<blockquote>
<p>**摘要：**采用文本分析方法,深入考察了上市公司年报可读性与融资约束的关系,并考虑媒体报道这一外部信息的调节效应研究发现,上市公司的年报可读性越低,其面临的融资约束越高;媒体报道的增多可以弱化年报可读性与融资约束的关系,且媒体报道情绪越正向,其调节作用越显著.进一步分析发现:机构投资者持股比例较高能减弱年报可读性和融资约束的关系;当年报可读性较低时,媒体报道的信息效应更为显著;只有官方媒体和地方媒体的报道数量与正向报道情绪能够显著缓解年报可读性低带来的融资约束;同时,较低的年报可读性是通过提高融资成本路径来加大公司的融资约束,且使得公司未来的融资方式呈现出内部融资增加,外部融资减少的特点.从融资约束角度拓展了关于财务报告文本信息披露质量的研究,并揭示了媒体报道如何有效改善内部信息披露不足的作用机理,为企业如何通过改善内,外部的信息环境来缓解自身的融资困境提供了理论依据。</p>
<p>**关键词：**年报可读性；融资约束；媒体报道；文本分析</p>
</blockquote>
<br>
<p><strong>姚加权, 冯绪, 王赞钧,等. 语调,情绪及市场影响:基于金融情绪词典[J]. 管理科学学报, 2021, 24(5):21.</strong></p>
<blockquote>
<p>**摘要：**金融文本的语调与情绪含有上市公司管理层以及个体投资者表达的情感信息,并对股票市场产生影响.通过词典重组和深度学习算法构建了适用于正式文本与非正式文本的金融领域中文情绪词典,并基于词典构建了上市公司的年报语调和社交媒体情绪指标.构建的年报语调指标和社交媒体情绪指标能有效地预测上市公司股票的收益率,成交量,波动率和非预期盈余等市场因素,并优于基于其他广泛使用情绪词典构建的指标.此外,年报语调指标和社交媒体情绪指标对上市公司的股价崩盘风险具有显著的预测作用.为文本大数据在金融市场的应用提供了分析工具,也为大数据时代的金融市场预测和监管等活动提供了决策支持。</p>
<p>**关键词：**情绪词典；语调；投资者情绪；市场影响</p>
</blockquote>
<br>
<p><strong>姜富伟, 马甜, 张宏伟. 高风险低收益? 基于机器学习的动态CAPM模型解释[J]. 管理科学学报, 2021.</strong></p>
<blockquote>
<p>**摘要：**我国股票市场存在高风险股票反而伴随较低收益的低风险定价异象,这有悖于传统资产定价理论.本文使用宏观经济和微观企业特征构建了六百多个变量的宏微观混合大数据集,并结合多种经典机器学习算法开发了基于大数据和机器学习的智能动态CAPM模型,检验了时变系统性风险对我国股市收益解释能力.实证结果表明:本文的智能动态CAPM定价模型能够显著解释我国股市低风险定价异象;随机森林等非线性机器学习算法表现最佳;影响股票时变系统风险的主要因素是市场类因子,基本面因子居次.本文对于我国股市系统性风险测度,动态资产定价模型构建和金融与大数据和人工智能融合创新有重要理论与实践指导意义.</p>
<p>**关键词：**系统性风险; 动态CAPM; 机器学习; 金融大数据</p>
</blockquote>
<br>
<p><strong>陆瑶, 张叶青, 黎波,等. 高管个人特征与公司业绩——基于机器学习的经验证据[J]. 管理科学学报, 2020, 23(2):21.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 在目前的公司治理文献中,大部分的高管特征研究一方面仅关注单一的高管特征与公司业绩之间的关联,缺乏全面的高管特征分析;另一方面主要围绕因果推断进行研究,缺乏从预测能力出发的系统定量的结论.本文首次采用机器学习算法中的Boosting回归树,全面考察了多维度高管特征对公司业绩的预测性.以我国2008年～2016年的上市公司为样本,研究了高管的多维个人特征是否能预测公司业绩,并进一步分析了对公司业绩预测能力较强的高管个人特征及其预测模式.研究发现:1)整体而言,在我国公司CEO和董事长的特征对公司业绩的预测能力较弱;2)在众多高管个人特征之中,高管持股比例和年龄对公司业绩的预测能力较强;3)高管持股比例和年龄与公司业绩之间的关联都呈现出非线性的特点,与以往的理论较为吻合.本研究不仅利用机器学习方法从一个更为全面的视角对中国的高管特征进行了研究,也为公司高管聘任和激励机制设计等方面提供了有益的启发.</p>
<p>**关键词：**机器学习；Boosting回归树；公司治理；公司业绩</p>
</blockquote>
<br>
<p><strong>吴武清, 赵越, 闫嘉文,等. 分析师文本语调会影响股价同步性吗?&ndash;基于利益相关者行为的中介效应检验[J]. 管理科学学报, 2020, 23(9):19.</strong></p>
<blockquote>
<p>**摘要：**文章考察了分析师研究报告的文本语调对股价同步性的影响与作用机制.首先爬取2006年至2018年中国A股上市公司377644份分析师研究报告,随机选出10434句文本并人工分为积极,中性,消极三类形成语料库,以此训练11种机器学习方法并比较各方法的预测准确性,最终选择朴素贝叶斯方法估计出分析师研究报告的文本语调.实证分析发现,分析师积极的文本语调显著降低了所追踪公司的股价同步性.这一结果与已有多数研究结论不同,但在做空机制欠发达的中国资本市场,个体选择性知觉理论为此提供了很好的解释.进一步地,中介效应检验结果表明,分析师积极的文本语调通过激励公司发布更多公告,引导机构投资者买入和吸引其他分析师发布研究报告,显著降低了股价同步性.该研究对于投资者关注研报语调指标,上市公司加强信息披露,政府部门完善资本市场制度均具有重要启示。</p>
<p>**关键词：**分析师文本语调; 股价同步性; 朴素贝叶斯; 选择性知觉; 中介效应</p>
</blockquote>
<br>
<p><strong>刘冠男, 曲金铭, 李小琳,等. 基于深度强化学习的救护车动态重定位调度研究[J]. 管理科学学报, 2020, 23(2):15.</strong></p>
<blockquote>
<p>**摘要：**救护车是挽救患者生命的重要医疗资源,合理调配有限的救护车资源可以降低呼叫响应时间,提高医疗服务水平.本文面向救护车动态重定位调度问题,提出了一种基于强化学习的调度策略结构.为解决传统强化学习所面临的高维状态空间的挑战,本文基于深度Q值网络(DQN)方法,提出了一种考虑多种调度交互因子的算法RedCon-DQN,以在给定环境状态下得到最优的重定位调度策略.在此基础上,本文还提出了急救网络弹性概念,以评估各站点对全局救护优化目标的影响力.最后,基于南京市2016年～2017年的实际救护车呼叫及响应数据,构造了环境交互模拟器.在模拟器中通过大规模数据实验,验证了模型得到的调度策略相比已有方法的优越性,并分析了不同时段下调度策略的有效性及其特点.</p>
<p>**关键词：**强化学习; DQN; 救护车调度; 重定位</p>
</blockquote>
<br>
<p><strong>黄丽华, 何晓, 卢向华. 企业在线社群内容组合策略的影响研究[J]. 管理科学学报, 2020, 23(2):15..</strong></p>
<blockquote>
<p>**摘要：**现代企业通过建立在线社群实现与消费者的互动,希望在向消费者提供服务的同时进行更好的营销,然而如何提供在线社群中的营销与服务内容却是一大难题.本文在营销—服务二元理论的基础上,提出了在线社群内容二元性的平衡维度与结合维度概念,并研究平衡维度与结合维度如何影响销售业绩与消费者的满意度.结合机器学习方法,本文发现,平衡维度对消费者满意度和销售绩效有提高作用,但是,结合维度对消费者满意度及企业绩效的影响呈倒U型;另外,企业员工的技能水平对内容二元性策略的效果有着显著的调节作用.研究结论对企业理解在线社群中的营销内容与服务内容之间的二元关系,以及内容提供策略的价值机制有重要的指导意义。</p>
<p>**关键词：**在线社群; 内容二元性; 销售绩效; 消费者满意度</p>
</blockquote>
<br>
<p><strong>部慧,解峥,李佳鸿,吴俊杰.基于股评的投资者情绪对股票市场的影响[J].管理科学学报,2018,v.21;No.166(04):86-101.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 探讨投资者情绪对我国股票市场的影响.为刻画投资者情绪,基于东方财富网股吧帖文与朴素贝叶斯方法,提出融合股评看涨看跌预期和投资者关注程度的投资者情绪度量指标.进一步,利用Granger因果检验、瞬时Granger因果检验、跨期回归分析等方法,探讨了投资者情绪对我国股票收益率、交易量和波动性是否具有预测能力及影响.实证结果揭示:虽然投资者情绪对股票市场收益率、交易量和波动性均无预测能力,但投资者情绪对股票收益率和交易量有当期影响;开盘前非交易时段的股评情绪对开盘价具有预测力,开盘后交易时段的股评情绪对收盘价和日交易量具有更显著的影响.此外,股票收益率是投资者情绪的Granger原因,即投资者情绪的形成依赖于前期市场收益率.这些实证结果为深入理解参与股吧评论的交易者的行为以及行为对市场产生的影响提供了证据.</p>
<p><strong>关键词:</strong> 	投资者情绪; 噪声交易者; 文本挖掘; Granger因果检验;</p>
</blockquote>
<br>
<h2 id="招募小伙伴">招募小伙伴</h2>
<div style="text-align: center;">
<figure >
    <a href="https://hidadeng.github.io/blog/we_need_you/">
        <img src="/images/blog/we_need_you.png" width="100%" />
    </a>
    <figcaption><small><i>点击加入我们</i></small></figcaption>
</figure>
</div>
<h2 id="文本分析视频课">文本分析视频课</h2>
<p>想轻松而快捷的深刻了解一个领域，看视频(直播)学习是一个不错的方式。</p>
<ul>
<li>
<p>大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与<a href="https://hidadeng.github.io/blog/2022-05-workshop/7-Python.html">直播课</a>。</p>
</li>
<li>
<p>如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的<a href="https://hidadeng.github.io/blog/management_python_course">录播课</a>。</p>
</li>
<li>
<p>如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读<a href="https://hidadeng.github.io/blog/paid_for_service">有偿说明</a></p>
</li>
</ul>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://hidadeng.github.io/blog/management_python_course/">点击进入详情页</a></p>
<br>
]]></content:encoded>
    </item>
    
    <item>
      <title>机器学习实战 | 信用卡欺诈检测</title>
      <link>https://hidadeng.github.io/blog/ml_credit_card_fraud_detection/</link>
      <pubDate>Thu, 16 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/ml_credit_card_fraud_detection/</guid>
      <description>本文旨在使用 XGBoost、随机森林、KNN、逻辑回归、SVM 和决策树解决金融领域信用卡欺诈识别的分类问题</description>
      <content:encoded><![CDATA[<blockquote>
<p>作者: 小猴子</p>
<p>公众号: 机器学习研习院</p>
</blockquote>
<p>本文旨在使用 XGBoost、随机森林、KNN、逻辑回归、SVM 和决策树解决分类问题</p>
<h2 id="案例简介">案例简介</h2>
<p>假设你受雇于帮助一家信用卡公司检测潜在的欺诈案件，你的工作是确保客户不会因未购买的商品而被收取费用。给你一个包含人与人之间交易的数据集，他们是欺诈与否的信息，并要求你区分它们。我们的最终目的是通过构建分类模型来对欺诈交易进行分类区分来解决上述情况。</p>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="ml_credit_card_fraud_detection.zip">点击下载</a></p>
<br>
<p>对于这个案例，所需要用到的主要模块是处理数据的 Pandas、处理数组的 NumPy、用于数据拆分、构建和评估分类模型的 scikit-learn，最后是用于 xgboost 分类器模型算法的 xgboost 包。</p>
<br>
<h2 id="导入数据">导入数据</h2>
<p>关于数据： 我们将要使用的数据是 <strong>Kaggle 信用卡欺诈检测数据集</strong>。它包含特征 V1 到 V28，是 PCA 获得的主要成分，并忽略对构建模型没有用的时间特征。其余的特征是包含交易总金额的&quot;金额&quot;特征和包含交易是否为欺诈案件的&quot;类别&quot;特征。</p>
<p>现在使用&rsquo;pd.read_csv&rsquo;方法导入数据，并查看部分数据样例。</p>
<p>Kaggle 信用卡欺诈检测数据集: <a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">https://www.kaggle.com/mlg-ulb/creditcardfraud</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;creditcard.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>...</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.359807</td>
      <td>-0.072781</td>
      <td>2.536347</td>
      <td>1.378155</td>
      <td>-0.338321</td>
      <td>0.462388</td>
      <td>0.239599</td>
      <td>0.098698</td>
      <td>0.363787</td>
      <td>0.090794</td>
      <td>...</td>
      <td>-0.018307</td>
      <td>0.277838</td>
      <td>-0.110474</td>
      <td>0.066928</td>
      <td>0.128539</td>
      <td>-0.189115</td>
      <td>0.133558</td>
      <td>-0.021053</td>
      <td>149.62</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.191857</td>
      <td>0.266151</td>
      <td>0.166480</td>
      <td>0.448154</td>
      <td>0.060018</td>
      <td>-0.082361</td>
      <td>-0.078803</td>
      <td>0.085102</td>
      <td>-0.255425</td>
      <td>-0.166974</td>
      <td>...</td>
      <td>-0.225775</td>
      <td>-0.638672</td>
      <td>0.101288</td>
      <td>-0.339846</td>
      <td>0.167170</td>
      <td>0.125895</td>
      <td>-0.008983</td>
      <td>0.014724</td>
      <td>2.69</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.358354</td>
      <td>-1.340163</td>
      <td>1.773209</td>
      <td>0.379780</td>
      <td>-0.503198</td>
      <td>1.800499</td>
      <td>0.791461</td>
      <td>0.247676</td>
      <td>-1.514654</td>
      <td>0.207643</td>
      <td>...</td>
      <td>0.247998</td>
      <td>0.771679</td>
      <td>0.909412</td>
      <td>-0.689281</td>
      <td>-0.327642</td>
      <td>-0.139097</td>
      <td>-0.055353</td>
      <td>-0.059752</td>
      <td>378.66</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.966272</td>
      <td>-0.185226</td>
      <td>1.792993</td>
      <td>-0.863291</td>
      <td>-0.010309</td>
      <td>1.247203</td>
      <td>0.237609</td>
      <td>0.377436</td>
      <td>-1.387024</td>
      <td>-0.054952</td>
      <td>...</td>
      <td>-0.108300</td>
      <td>0.005274</td>
      <td>-0.190321</td>
      <td>-1.175575</td>
      <td>0.647376</td>
      <td>-0.221929</td>
      <td>0.062723</td>
      <td>0.061458</td>
      <td>123.50</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.158233</td>
      <td>0.877737</td>
      <td>1.548718</td>
      <td>0.403034</td>
      <td>-0.407193</td>
      <td>0.095921</td>
      <td>0.592941</td>
      <td>-0.270533</td>
      <td>0.817739</td>
      <td>0.753074</td>
      <td>...</td>
      <td>-0.009431</td>
      <td>0.798278</td>
      <td>-0.137458</td>
      <td>0.141267</td>
      <td>-0.206010</td>
      <td>0.502292</td>
      <td>0.219422</td>
      <td>0.215153</td>
      <td>69.99</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 30 columns</p>
</div>
<p>接下来将进行一些数据预处理和探索性数据分析（EDA）。</p>
<br>
<h2 id="探索性数据分析">探索性数据分析</h2>
<p>看看数据集中有多少欺诈案件和非欺诈案件。此外，还计算整个记录交易中欺诈案件的百分比。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">termcolor</span> <span class="kn">import</span> <span class="n">colored</span> <span class="k">as</span> <span class="n">cl</span>

<span class="n">cases</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">nonfraud_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">fraud_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">fraud_percentage</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">fraud_count</span><span class="o">/</span><span class="n">nonfraud_count</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;CASE COUNT&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Total number of cases are </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cases</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Number of Non-fraud cases are </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nonfraud_count</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Number of Non-fraud cases are </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fraud_count</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Percentage of fraud cases is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fraud_percentage</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mCASE COUNT[0m
[1m--------------------------------------------[0m
[1mTotal number of cases are 284807[0m
[1mNumber of Non-fraud cases are 284315[0m
[1mNumber of Non-fraud cases are 492[0m
[1mPercentage of fraud cases is 0.17[0m
[1m--------------------------------------------[0m
</code></pre>
<p>我们可以看到，在 <strong>284,807</strong> 个样本中，只有 <strong>492</strong> 个欺诈案例，仅占样本总数的 <strong>0.17%</strong> 。所以，可以说我们正在处理的数据是高度不平衡的数据，需要在建模和评估时谨慎处理。</p>
<p>接下来，我们将使用 Python 中的**&ldquo;describe&rdquo;**方法获取欺诈和非欺诈交易金额数据的统计视图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">nonfraud_cases</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">fraud_cases</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;CASE AMOUNT STATISTICS&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;NON-FRAUD CASE AMOUNT STATS&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nonfraud_cases</span><span class="o">.</span><span class="n">Amount</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;FRAUD CASE AMOUNT STATS&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fraud_cases</span><span class="o">.</span><span class="n">Amount</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mCASE AMOUNT STATISTICS[0m
[1m--------------------------------------------[0m
[1mNON-FRAUD CASE AMOUNT STATS[0m
count    284315.000000
mean         88.291022
std         250.105092
min           0.000000
25%           5.650000
50%          22.000000
75%          77.050000
max       25691.160000
Name: Amount, dtype: float64
[1m--------------------------------------------[0m
[1mFRAUD CASE AMOUNT STATS[0m
count     492.000000
mean      122.211321
std       256.683288
min         0.000000
25%         1.000000
50%         9.250000
75%       105.890000
max      2125.870000
Name: Amount, dtype: float64
[1m--------------------------------------------[0m
</code></pre>
<p>在查看统计数据时，可以看到与其余变量相比，&quot;<strong>金额</strong>&quot; 变量中的值变化很大。为了减少其广泛的值，我们可以使用 python 中的 &ldquo;<strong>StandardScaler()</strong>&rdquo; 方法对其进行标准化。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">amount</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Amount&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Amount&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">amount</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Amount&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1m0    0.244964
1   -0.342475
2    1.160686
3    0.140534
4   -0.073403
5   -0.338556
6   -0.333279
7   -0.190107
8    0.019392
9   -0.338516
Name: Amount, dtype: float64[0m
</code></pre>
<br>
<h2 id="特征选择和数据集拆分">特征选择和数据集拆分</h2>
<p>在这个过程中，定义自变量 (X) 和因变量 (Y)。使用定义的变量将数据分成训练集和测试集，进一步用于建模和评估。可以使用 python 中的 &ldquo;<strong>train_test_split</strong>&rdquo; 算法轻松拆分数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c1"># DATA SPLIT</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;X_train samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">X_train</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;X_test samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;y_train samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;y_test samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">y_test</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<pre><code>[1mX_train samples : [0m [[-1.11504743  1.03558276  0.80071244 -1.06039825  0.03262117  0.85342216
  -0.61424348 -3.23116112  1.53994798 -0.81690879 -1.30559201  0.1081772
  -0.85960958 -0.07193421  0.90665563 -1.72092961  0.79785322 -0.0067594
   1.95677806 -0.64489556  3.02038533 -0.53961798  0.03315649 -0.77494577
   0.10586781 -0.43085348  0.22973694 -0.0705913  -0.30145418]]
[1mX_test samples : [0m [[-0.32333357  1.05745525 -0.04834115 -0.60720431  1.25982115 -0.09176072
   1.1591015  -0.12433461 -0.17463954 -1.64440065 -1.11886302  0.20264731
   1.14596495 -1.80235956 -0.24717793 -0.06094535  0.84660574  0.37945439
   0.84726224  0.18640942 -0.20709827 -0.43389027 -0.26161328 -0.04665061
   0.2115123   0.00829721  0.10849443  0.16113917 -0.19330595]]
[1my_train samples : [0m [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[1my_test samples : [0m [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
</code></pre>
<p>到目前为止，已经做好了构建分类模型所需的所有准备。</p>
<br>
<h2 id="模型建立">模型建立</h2>
<p>这里构建六种不同类型的分类模型，即<strong>决策树、K-最近邻 (KNN)、逻辑回归、支持向量机 (SVM)、随机森林和 XGBoost</strong>。虽然我们还可以使用更多其他的模型，但我们选用的是用于解决分类问题的最流行模型。所有这些模型构建均比较方便，都可以使用 <strong>scikit-learn</strong> 包提供的算法来构建。仅对于 XGBoost 模型，将使用 xgboost 包。接下来在 python 中实现这些模型，所使用的算法可能需要花费一定的时间来实现。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>


<span class="c1"># MODELING</span>

<span class="c1"># 1. Decision Tree</span>
<span class="n">tree_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">criterion</span> <span class="o">=</span> <span class="s1">&#39;entropy&#39;</span><span class="p">)</span>
<span class="n">tree_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">tree_yhat</span> <span class="o">=</span> <span class="n">tree_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 2. K-Nearest Neighbors</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">n</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">knn_yhat</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 3. Logistic Regression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr_yhat</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 4. SVM </span>
<span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">svm_yhat</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 5. Random Forest Tree</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rf_yhat</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 6. XGBoost</span>
<span class="n">xgb</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">xgb_yhat</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div><p>至此我们构建了从决策树模型到 XGBoost 模型的六种不同类型的分类模型。</p>
<p>在决策树模型中，使用 <strong>&ldquo;DecisionTreeClassifier&rdquo;</strong> 算法来构建模型。在算法中，设置 <strong>&ldquo;max_depth=4&rdquo;</strong>，意味着允许树最大分裂四次，<strong>&ldquo;criterion = &lsquo;entropy&rdquo;</strong>，与**&ldquo;max_depth&rdquo;**最相似，但决定何时分裂停止分裂树。最后拟合模型后将预测值存储到 <strong>&ldquo;tree_yhat&rdquo;</strong> 变量中。</p>
<p>在K-最近邻 (KNN)中，使用 <strong>&ldquo;KNeighborsClassifier&rdquo;</strong> 算法构建了模型，并设置 <strong>&ldquo;n_neighbors=5&rdquo;</strong>。 <strong>&lsquo;n_neighbors&rsquo;</strong> 的值是随机选择的，其实可以通过迭代一系列值来有目的地选择，然后拟合模型后将预测值存储到 <strong>&ldquo;knn_yhat&rdquo;</strong> 变量中。</p>
<p>逻辑回归的代码没有什么可解释的，因为我使用 <strong>&ldquo;LogisticRegression&rdquo;</strong> 算法并全部使用默认值，并拟合模型后将预测值存储到 <strong>&ldquo;lr_yhat&rdquo;</strong> 变量中。</p>
<p>使用&quot;SVC&quot;算法构建了支持向量机模型，并且同样使用默认值，并且默认内核就是我们所希望用到的模型，即&quot;rbf&quot;内核。之后，我们在拟合模型后将预测值存储到 &ldquo;svm_yhat&rdquo; 中。</p>
<p>接下来使用 <strong>&ldquo;RandomForestClassifier&rdquo;</strong> 算法构建的随机森林模型，设置参数 <strong>&ldquo;max_depth=4&rdquo;</strong>，就像构建决策树模型的方式一样。最后在拟合模型后将预测值存储到 <strong>&ldquo;rf_yhat&rdquo;</strong> 中。请记住，决策树和随机森林之间的主要区别在于，决策树使用整个数据集来构建单个模型，而随机森林使用随机选择的特征来构建多个模型。这就是为什么很多情况下选择使用随机森林模型而不是决策树的原因。</p>
<p>最后是 XGBoost 模型。使用 xgboost 包提供的 <strong>&ldquo;XGBClassifier&rdquo;</strong> 算法构建模型。设置 <strong>&ldquo;max_depth=4&rdquo;</strong>，最后在拟合模型后将预测值存储到 &ldquo;xgb_yhat&rdquo; 中。</p>
<p>至此，我们成功构建了六种分类模型，为了便于理解，对代码进行了简单解释。接下来需要评估每个模型，并找到最适合我们案例的模型。</p>
<br>
<h2 id="模型评估">模型评估</h2>
<p>之前有提到过，我们将使用 scikit-learn 包提供的评估指标来评估我们构建的模型。在此过程中的主要目标是为给定案例找到最佳模型。这里将使用的评估指标是<strong>准确度评分指标、f1 评分指标，及混淆矩阵</strong>。</p>
<h3 id="准确率">准确率</h3>
<p>准确率是最基本的评价指标之一，广泛用于评价分类模型。准确率分数的计算方法很简单，就是将模型做出的正确预测的数量除以模型做出的预测总数（可以乘以 100 将结果转换为百分比）。一般可以表示为：</p>
<p><strong>准确度分数 = 正确预测数 / 总预测数</strong></p>
<p>我们检查我们所构建的六种不同分类模型的准确率分数。要在 python 中完成，我们可以使用 scikit-learn 包提供的 <strong>&ldquo;accuracy_score&rdquo;</strong> 方法。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># 1. Accuracy score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;ACCURACY SCORE&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the Decision Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the KNN model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the Logistic Regression model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the SVM model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">svm_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the Random Forest Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the XGBoost model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">xgb_yhat</span><span class="p">)),</span> 
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mACCURACY SCORE[0m
[1mAccuracy score of the Decision Tree model is 0.9993679997191109[0m
[1m[32mAccuracy score of the KNN model is 0.9995259997893332[0m
[1m[31mAccuracy score of the Logistic Regression model is 0.9991924440855307[0m
[1mAccuracy score of the SVM model is 0.9993153330290369[0m
[1mAccuracy score of the Random Forest Tree model is 0.9993153330290369[0m
[1mAccuracy score of the XGBoost model is 0.9994908886626171[0m
</code></pre>
<p>根据准确性评分评估指标来看，<strong>KNN</strong> 模型为最准确的模型，而 <strong>Logistic</strong> 回归模型最不准确。然而，当我们对每个模型的结果进行四舍五入时，得到 99% 的准确性，这看是一个非常好的分数。</p>
<h3 id="f1-score">F1-score</h3>
<p>F1-score 或 F-score 是用于评估分类模型的最流行的评估指标之一。它可以简单地定义为<strong>模型的准确率和召回率的调和平均值</strong>。它的计算方法是将 模型的精度和召回率的乘积除以模型的精度和召回率相加得到的值，最后乘以 2 得到的值。可以表示为：</p>
<p><strong>F1-score  = 2( (精度 * 召回率) / (精度 + 召回率) )</strong></p>
<p>可以使用 scikit-learn 包提供的 &ldquo;f1_score&rdquo; 方法轻松计算 <strong>F1-score</strong> 。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span> 

<span class="c1"># 2. F1 score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 SCORE&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the Decision Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the KNN model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the Logistic Regression model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the SVM model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">svm_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the Random Forest Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the XGBoost model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">xgb_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mF1 SCORE[0m
[1mF1 score of the Decision Tree model is 0.8105263157894738[0m
[1m[32mF1 score of the KNN model is 0.8571428571428572[0m
[1m[31mF1 score of the Logistic Regression model is 0.7356321839080459[0m
[1mF1 score of the SVM model is 0.7771428571428572[0m
[1mF1 score of the Random Forest Tree model is 0.7796610169491525[0m
[1mF1 score of the XGBoost model is 0.8449197860962566[0m
</code></pre>
<p>模型的排名几乎与之前的评估指标相似。在 F1-score 评估指标的基础上，KNN 模型再次夺得第一，Logistic 回归模型仍然是最不准确的模型。</p>
<h3 id="混淆矩阵">混淆矩阵</h3>
<p>通常，混淆矩阵是分类模型的可视化，显示模型与原始结果相比预测结果的程度。通常，预测结果存储在一个变量中，然后将其转换为相关表。使用相关表，以热图的形式绘制混淆矩阵。尽管有多种内置方法可以可视化混淆矩阵，但我们将从零开始定义和可视化它，以便更好地理解。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 3. Confusion Matrix</span>
<span class="c1"># defining the plot function</span>
<span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Confusion Matrix of </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    
    <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
    <span class="kn">import</span> <span class="nn">itertools</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span> <span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span> <span class="o">=</span> <span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">rotation</span> <span class="o">=</span> <span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

    <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;.2f&#39;</span> <span class="k">if</span> <span class="n">normalize</span> <span class="k">else</span> <span class="s1">&#39;d&#39;</span>
    <span class="n">thresh</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">fmt</span><span class="p">),</span>
                 <span class="n">horizontalalignment</span> <span class="o">=</span> <span class="s1">&#39;center&#39;</span><span class="p">,</span>
                 <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;white&#39;</span> <span class="k">if</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span> <span class="k">else</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True label&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted label&#39;</span><span class="p">)</span>
    
<span class="c1"># Compute confusion matrix for the models</span>

<span class="n">tree_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Decision Tree</span>
<span class="n">knn_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> 
                <span class="n">knn_yhat</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># K-Nearest Neighbors</span>
<span class="n">lr_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Logistic Regression</span>
<span class="n">svm_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">svm_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Support Vector Machine</span>
<span class="n">rf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Random Forest Tree</span>
<span class="n">xgb_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">xgb_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># XGBoost</span>

<span class="c1"># Plot the confusion matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</code></pre></div><h3 id="decision-tree">Decision tree</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">tree_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">tree_matrix</span><span class="p">,</span> 
                      <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                      <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Decision Tree&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;tree_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_21_0.png" alt="png"  />

​</p>
<h3 id="k-nearest-neighbors">K-Nearest Neighbors</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">knn_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">knn_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;KNN&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;knn_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_23_0.png" alt="png"  />

​</p>
<h3 id="logistic-regression">Logistic regression</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">lr_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">lr_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Logistic Regression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;lr_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_25_0.png" alt="png"  />

​</p>
<h3 id="support-vector-machine">Support Vector Machine</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">svm_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">svm_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;SVM&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;svm_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_27_0.png" alt="png"  />

​</p>
<h3 id="random-forest">Random Forest</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">rf_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">rf_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Random Forest Tree&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;rf_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_29_0.png" alt="png"  />

​</p>
<h3 id="xgboost">XGBoost</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">xgb_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">xgb_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;XGBoost&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;xgb_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_31_0.png" alt="png"  />

​</p>
<p><strong>混淆矩阵理解</strong>： 以XGBoost模型的混淆矩阵为例。</p>
<ul>
<li>
<p>第一行。
第一行是测试集中实际欺诈值为0的交易。可以计算，其中56861笔欺诈值为0。在这56861笔非欺诈交易中，分类器正确预测了其中的56854笔为 0 和 预测了其中 7 为 1。这意味着，对于 56854 笔非欺诈交易，测试集中的实际流失值为 0，分类器也正确预测为 0。可以说我们的模型已经对非欺诈交易进行了分类交易还不错。</p>
</li>
<li>
<p>第二行。
看起来有 101 笔交易的欺诈值为 1。分类器正确预测其中 79 笔为 1，错误预测值为 0 的 22 笔。错误预测值可以视为模型的错误。</p>
</li>
</ul>
<p>在比较所有模型的混淆矩阵时可以看出，K-Nearest Neighbors 模型在从非欺诈交易中分类欺诈交易方面做得非常好，其次是 XGBoost 模型。所以可以得出结论，最适合本次案例的模型是 <strong>K-Nearest Neighbors</strong> 模型，可以忽略的模型是 Logistic 回归模型。</p>
<br>
<h2 id="写在最后">写在最后</h2>
<p>经过一连串的过程，我们已经成功构建了从决策树模型到XGBoost模型的六种不同类型的分类模型。随后使用评估指标评估了每个模型，并选择了最适合给定案例的模型。</p>
<p>在本文中，我们只选用了6个相对流行的模型，其实还有更多模型需要探索。此外，虽然我们很轻松地在 python 中可行地构建了模型，但是每个模型背后都有很多的数学和统计数据，在有精力的情况下，可以去了解下这么模型背后的数学推理。</p>
<h2 id="参考资料">参考资料</h2>
<p>[1] Kaggle 信用卡欺诈检测数据集: <a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">https://www.kaggle.com/mlg-ulb/creditcardfraud</a></p>
<br>
<h2 id="招募小伙伴">招募小伙伴</h2>
<div style="text-align: center;">
<figure >
    <a href="https://hidadeng.github.io/blog/we_need_you/">
        <img src="/images/blog/we_need_you.png" width="100%" />
    </a>
    <figcaption><small><i>点击加入我们</i></small></figcaption>
</figure>
</div>
<h2 id="广告位">广告位</h2>
<p>想轻松而快捷的深刻了解一个领域，看视频(直播)学习是一个不错的方式。</p>
<ul>
<li>
<p>大邓每年会有4场直播，五一、十一、寒、暑假，如果时间点接近，可考虑报名参与<a href="https://hidadeng.github.io/blog/2022-05-workshop/7-Python.html">直播课</a>。</p>
</li>
<li>
<p>如果只意性价比，且已迫不及待想学，可以考虑直接报名大邓的<a href="https://hidadeng.github.io/blog/management_python_course">录播课</a>。</p>
</li>
<li>
<p>如果不想学，也可以考虑外包。更建议找淘宝，如果找我咨询，请先阅读<a href="https://hidadeng.github.io/blog/paid_for_service">有偿说明</a></p>
</li>
</ul>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://hidadeng.github.io/blog/management_python_course/">点击进入详情页</a></p>
<br>
]]></content:encoded>
    </item>
    
    <item>
      <title>SHAP机器学习模型解释库</title>
      <link>https://hidadeng.github.io/blog/shap_ml_explanation/</link>
      <pubDate>Thu, 14 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/shap_ml_explanation/</guid>
      <description>图文代码理解机器学习模型中各特征对结果的贡献</description>
      <content:encoded><![CDATA[<h2 id="代码下载">代码下载</h2>
<p><a href="SHAP.zip">点击此处下载代码</a></p>
<br>
<blockquote>
<p>原文链接 <a href="https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-24207127cad7">https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-24207127cad7</a></p>
</blockquote>
<h2 id="heading"></h2>
<p>想象一下，你正试图训练一个机器学习模型来预测广告是否被特定的人点击。在收到关于某人的一些信息后，模型预测某人会不会点击广告。</p>

<figure >
    
        <img src="img/%e5%9b%be1.png" />
    
    
</figure>

<p>但是为什么模型会输出这样的预测结果呢？ 每个特征对预测的贡献有多大？ 如果您能看到一个图表，显示每个特征对预测的贡献程度，如下所示，不是很好吗？</p>

<figure >
    
        <img src="img/%e6%9d%83%e9%87%8d.png" />
    
    
</figure>

<p>Shapley值就能起到特征权重测度的作用。</p>
<h2 id="shapley值是什么">Shapley值是什么？</h2>
<p>Shapley值是博弈论中使用的一种方法，它涉及公平地将收益和成本分配给在联盟中工作的行动者。
由于每个行动者对联盟的贡献是不同的，Shapley值保证每个行动者根据贡献的多少获得公平的份额。</p>

<figure >
    
        <img src="img/%e8%b4%a1%e7%8c%ae.png" />
    
    
</figure>

<h2 id="小案例">小案例</h2>
<p>Shapley值被广泛地应用于求解群体中每个工人(特征)的贡献问题。要理解Shapley值的作用，让我们想象一下贵公司刚刚做了A/B测试，他们在测试广告策略的不同组合。</p>
<p>每个策略在特定月份的收入是：</p>
<ul>
<li>无广告：150美元</li>
<li>社交媒体：300美元</li>
<li>谷歌广告：200美元</li>
<li>电子邮件营销：350美元</li>
<li>社交媒体和谷歌广告：320美元</li>
<li>社交媒体和电子邮件营销：400美元</li>
<li>谷歌广告和电子邮件营销：350美元</li>
<li>电子邮件营销，谷歌广告和社交媒体：450美元</li>
</ul>

<figure >
    
        <img src="img/%e7%ad%96%e7%95%a5%e8%90%a5%e6%94%b6.png" />
    
    
</figure>

<p>使用三则广告与不使用广告的收入相差300美元，每则广告对这一差异有多大的贡献?</p>

<figure >
    
        <img src="img/%e7%ad%96%e7%95%a5%e8%90%a5%e6%94%b6%e8%b4%a1%e7%8c%ae%e5%87%a0%e4%bd%95.png" />
    
    
</figure>

<p>我们可以通过计算每一类广告的Shapley值来计算谷歌广告对公司收入的总贡献入手，通过公式可以计算出Google广告的总贡献：</p>

<figure >
    
        <img src="img/%e5%85%ac%e5%bc%8f.png" />
    
    
</figure>

<p>让我们找到Google广告的边际贡献及其权重。</p>
<h2 id="寻找谷歌广告的边际贡献">寻找谷歌广告的边际贡献</h2>
<p>第一，我们将发现谷歌广告对以下群体的边际贡献：</p>
<ul>
<li>无广告</li>
<li>谷歌广告+社交媒体</li>
<li>谷歌广告+电子邮件营销</li>
<li>谷歌广告+电子邮件营销+社交媒体</li>
</ul>

<figure >
    
        <img src="img/%e8%be%b9%e9%99%85%e8%b4%a1%e7%8c%ae.png" />
    
    
</figure>

<p>Google广告 对 无广告 的边际贡献是：</p>

<figure >
    
        <img src="img/MC1.png" />
    
    
</figure>

<p>谷歌广告 对 谷歌广告&amp;社交媒体组合 的边际贡献是：</p>

<figure >
    
        <img src="img/MC2.png" />
    
    
</figure>

<p>谷歌广告 对 谷歌广告&amp;电子邮件营销组合 的边际贡献是：</p>

<figure >
    
        <img src="img/MC3.png" />
    
    
</figure>

<p>谷歌广告 对 谷歌广告、电子邮件营销和社交媒体组合 的边际贡献是：</p>

<figure >
    
        <img src="img/MC4.png" />
    
    
</figure>

<h2 id="发现权重">发现权重</h2>
<p>为了发现权重，我们将把不同广告策略的组合组织成如下多个层次，每个层次对应于每个组合中广告策略的数量。</p>
<p>然后根据每个层次的边数分配权重，我们看到了这一点：</p>
<ul>
<li>第一级包含3条边，因此每个边的权重为1/3</li>
<li>第二级包含6条边，因此每条边的权重将为1/6</li>
<li>第三级包含3条边，因此每条边的权重将为1/3</li>
</ul>

<figure >
    
        <img src="img/%e5%8f%91%e7%8e%b0%e6%9d%83%e9%87%8d.png" />
    
    
</figure>

<h2 id="发现google广告的总贡献">发现Google广告的总贡献</h2>
<p>根据前面的权重和边际贡献，我们已经可以找到Google广告的总贡献!</p>

<figure >
    
        <img src="img/google%e6%80%bb%e8%b4%a1%e7%8c%ae.png" />
    
    
</figure>


<figure >
    
        <img src="img/google%e5%85%ac%e5%bc%8f.png" />
    
    
</figure>

<p>酷!所以谷歌广告在使用3种广告策略与不使用广告的总收入差异中贡献了36.67美元。36.67是Google广告的Shapey值。</p>

<figure >
    
        <img src="img/otherRevenue.png" />
    
    
</figure>

<p>重复以上步骤，对于另外两种广告策略，我们可以看出：</p>
<ul>
<li>
<p>电子邮件营销贡献151.67美元</p>
</li>
<li>
<p>社交媒体贡献116.67美元</p>
</li>
<li>
<p>谷歌广告贡献36.67美元</p>

<figure >
    
        <img src="img/%e5%90%84%e7%ad%96%e7%95%a5%e8%b4%a1%e7%8c%ae.png" />
    
    
</figure>

</li>
</ul>
<p>他们共同出资300美元，用于使用3种不同类型的广告与不使用广告的区别!挺酷的，不是吗?
既然我们理解了Shapley值，那么让我们看看如何使用它来解释机器学习模型。</p>
<h2 id="shap-在python中解释机器学习模型">SHAP-在Python中解释机器学习模型</h2>
<p>SHAP是一个Python库，它使用Shapley值来解释任何机器学习模型的输出。</p>
<p>安装SHAP</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python">
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">shap</span>

</code></pre></div><h2 id="训练模型">训练模型</h2>
<p>为了理解SHAP工作原理，我们使用Kaggle平台内的advertising广告数据集。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;advertising.csv&#34;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/df.png" />
    
    
</figure>

<p>我们将建立一个机器学习模型, 该模型根据用户个人特质信息来预测其是否点击广告。</p>
<p>我们使用Patsy将DataFrame转换为一组特征和一组目标值：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">patsy</span> <span class="kn">import</span> <span class="n">dmatrices</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">y</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">dmatrices</span><span class="p">(</span>
    <span class="s2">&#34;clicked_on_ad ~ daily_time_spent_on_site + age + area_income + daily_internet_usage  + male -1&#34;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">X_frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">design_info</span><span class="o">.</span><span class="n">column_names</span><span class="p">)</span>


</code></pre></div><p>把数据分为测试集和训练接</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</code></pre></div><p>接下来使用XGBoost训练模型，并做预测</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">xgboost</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xgboost</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div><p>为了查看模型表现，我们使用F1得分</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>

<span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>
<span class="n">f1</span>
</code></pre></div><pre><code>0.9619047619047619
</code></pre>
<p>太好了!</p>
<h2 id="解释该模型">解释该模型</h2>
<p>该模型很好地预测了用户是否点击广告。但它是如何得出这样的预测的? <strong>每个特征对最终预测与平均预测的差异贡献了多少?</strong></p>
<p>注意，这个问题与我们在文章开头论述的问题非常相似。</p>
<p>因此，寻找每个特征的Shapley值可以帮助我们确定它们的贡献。得到特征i的重要性的步骤与之前类似，其中i是特征的索引：</p>
<ul>
<li>获取所有不包含特征i的子集</li>
<li>找出特征i对这些子集中每个子集的边际贡献</li>
<li>聚合所有边际贡献来计算特征i的贡献</li>
</ul>
<p>若要使用SHAP查找Shapley值，只需将训练好的模型插入shap.Explainer</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">shap</span>

<span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">Explainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X_frame</span><span class="p">)</span>
</code></pre></div><pre><code>ntree_limit is deprecated, use `iteration_range` or model slicing instead.
</code></pre>
<h2 id="shap瀑布图">SHAP瀑布图</h2>
<p>可视化第一个预测的解释：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#第一条记录是未点击</span>
<span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">waterfall</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<figure >
    
        <img src="img/output_20_0.png" />
    
    
</figure>

<p>啊哈!现在我们知道每个特征对第一次预测的贡献。对上图的解释：</p>

<figure >
    
        <img src="img/%e7%ac%ac%e4%b8%80%e6%ac%a1%e9%a2%84%e6%b5%8b%e8%b4%a1%e7%8c%ae.png" />
    
    
</figure>

<ul>
<li>蓝色条显示某一特定特征在多大程度上降低了预测的值。</li>
<li>红条显示了一个特定的特征在多大程度上增加了预测值。</li>
<li>负值意味着该人点击广告的概率小于0.5</li>
</ul>
<p>我们应该期望总贡献等于预测与均值预测的差值。我们来验证一下：</p>

<figure >
    
        <img src="img/%e6%80%bb%e8%b4%a1%e7%8c%ae%e7%ad%89%e4%ba%8e%e9%a2%84%e6%b5%8b%e4%b8%8e%e5%9d%87%e5%80%bc%e9%a2%84%e6%b5%8b%e7%9a%84%e5%b7%ae%e5%80%bc.png" />
    
    
</figure>

<p>酷!他们是平等的。</p>
<p>可视化第二个预测的解释：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#第二条记录也是未点击</span>
<span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">waterfall</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>
<figure >
    
        <img src="img/output_22_0.png" />
    
    
</figure>

<h2 id="shap摘要图">SHAP摘要图</h2>
<p>我们可以使用SHAP摘要图，而不是查看每个单独的实例，来可视化这些特性对多个实例的整体影响：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_24_0.png" />
    
    
</figure>

<p>SHAP摘要图告诉我们数据集上最重要的特征及其影响范围。</p>
<p>从上面的情节中，我们可以对模型的预测获得一些有趣的见解：</p>
<ul>
<li>用户的 <strong>daily_internet_usage</strong> 对该用户是否点击广告的影响最大。</li>
<li>随着<strong>daily_time_spent_on_site</strong>的增加，用户点击广告的可能性降低。</li>
<li>随着<strong>area_income</strong>的增加，用户点击广告的可能性降低。</li>
<li>随着<strong>age</strong>的增长，用户更容易点击广告。</li>
<li>如果用户是<strong>male</strong>，则该用户点击广告的可能性较小。</li>
</ul>
<h2 id="shap条形图">SHAP条形图</h2>
<p>我们还可以使用SHAP条形图得到全局特征重要性图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_26_0.png" />
    
    
</figure>

<p>很酷!</p>
<h2 id="结论">结论</h2>
<p>恭喜你!您刚刚了解了Shapey值以及如何使用它来解释一个机器学习模型。希望本文将提供您使用Python来解释自己的机器学习模型的基本知识。</p>
<br>
<h2 id="了解课程">了解课程</h2>
<div style="text-align: center;">
<figure >
    <a href="https://m.qlchat.com/wechat/page/channel-intro?channelId=2000015158133596">
        <img src="/images/bg/management_data_mining_with_python_course.png" width="100%" />
    </a>
    <figcaption><small><i>点击上方图片购买课程</i></small></figcaption>
</figure>
</div>
<p><a href="https://hidadeng.github.io/blog/management_python_course/">点击进入详情页</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
