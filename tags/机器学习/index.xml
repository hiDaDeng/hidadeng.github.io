<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>机器学习 on 大邓和他的PYTHON</title>
    <link>/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Fri, 31 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PNAS | 14000&#43;篇心理学顶刊论文可复现性调研</title>
      <link>https://textdata.cn/blog/2023-03-31-pnas-measure-replicability-of-psychology-with-ml/</link>
      <pubDate>Fri, 31 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-31-pnas-measure-replicability-of-psychology-with-ml/</guid>
      <description>”可复现研究的数量远远低于科学界期望，我们创建了一个基于文本数据的机器学习模型，估计了自2000年以来心理学六个子领域中发布的超过14,000篇文章的可复现性分析。此外，我们还调查了可复现性与不同的研究方法、作者的生产力、引用影响力和机构声誉、论文的引用增长和社交媒体覆盖率有关的变化。我们的研究结果有助于建立大规模的经验模式，以便为推进复现研究提供依据。The number of manually replicated studies falls well below the abundance of important studies that the scientific community would like to see replicated. We created a text-based machine learning model to estimate the replication likelihood for more than 14,000 published articles in six subfields of Psychology since 2000. Additionally, we investigated how replicability varies with respect to different research methods, authors &amp;#39;productivity, citation impact, and institutional prestige, and a paper’s citation growth and social media coverage. Our findings help establish large-scale empirical patterns on which to prioritize manual replications and advance replication research.“</description>
      <content:encoded><![CDATA[<p>Youyou, W., Yang, Y., &amp; Uzzi, B. (2023). <strong>A discipline-wide investigation of the replicability of Psychology papers over the past two decades</strong>. <em>Proceedings of the National Academy of Sciences</em>, <em>120</em>(6), e2208863120.</p>
<h2 id="意义">意义</h2>
<p>可复现研究的数量远远低于科学界期望，我们创建了一个基于文本数据的机器学习模型，估计了自2000年以来心理学六个子领域中发布的超过14,000篇文章的可复现性分析。此外，我们还调查了可复现性与不同的研究方法、作者的生产力、引用影响力和机构声誉、论文的引用增长和社交媒体覆盖率有关的变化。我们的研究结果有助于建立大规模的经验模式，以便为推进复现研究提供依据。</p>
<h2 id="本文章节">本文章节</h2>
<p>大家可以选择感兴趣部分阅读。<strong>该论文提供了词向量模型文件，如果是做社科研究的同学，可以下载下来，探索下词向量。例如查看概念近义词</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">一、摘要
二、代码
2.1 训练集
2.2 测试集数据
2.3 词向量模型
三、正文
四、数据和方法 
4.1 机器学习模型
4.2 关于模型迁移学习是否会预测不准问题
4.3 评估可复现性前后出版的相关度量。
五、结果
六、讨论
</code></pre></div><br>
<h2 id="一摘要">一、摘要</h2>
<p>对社会科学中较弱的可复现性的猜测使学者们渴望量化这一学科的复现失败的规模和范围。然而，仅靠小规模的人工复现方法无法应对这个大数据问题。在这里，我们进行了一项跨学科的科学复现普查。<strong>我们的样本（N=14,126篇论文）涵盖了近20年来发表在六个顶级心理学期刊上的几乎所有论文</strong>。使用一个经过验证的机器学习模型，估计论文的复现可能性，我们发现证据既支持又反驳了从相对较小的人工复现样本中得出的推测。首先，我们发现单一的心理学复现率不能很好地捕捉不同子领域中可复现性的程度变化。其次，我们发现，在所有子领域中，复现率与研究方法强相关。<strong>实验法的复现率明显低于非实验研究的复现率</strong>。第三，我们发现作者的<strong>累积出版物数量和引用影响力与复现可能性呈正相关，而其他反映研究质量和严谨性的指标，如作者的大学声誉和论文的引用，与可复现性无关</strong>。最后，与媒体注意应该关注可复现性的研究的理想相反，我们发现<strong>媒体的注意力与复现失败的可能性呈正相关</strong>。我们对可复现性的规模和范围的评估是广泛解决可复现性问题的重要下一步。</p>
<p><br><br></p>
<h2 id="二代码">二、代码</h2>
<p>论文的数据及代码 <a href="https://osf.io/f5sxn/">下载地址</a></p>
<h3 id="21-训练集">2.1 训练集</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#标注的训练集数据388篇论文(314个实验论文+72个非实验研究)</span>
<span class="n">training_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;training_sample.csv&#34;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_df</span><span class="p">))</span>
<span class="n">training_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">388
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
<br></p>
<h3 id="22-测试集数据">2.2 测试集数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#带预测的14125篇论文数据  </span>
<span class="n">prediction_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;prediction_sample.csv&#34;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prediction_df</span><span class="p">))</span>
<span class="n">prediction_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">14126
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<h3 id="23-词向量模型">2.3 词向量模型</h3>
<p>词向量设置为200维， 训练得到的模型文件638M。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1"># 导入模型文件</span>
<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;mag_200d_psy_eco_word2vec&#39;</span><span class="p">)</span>

<span class="c1">#词汇量</span>
<span class="nb">len</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">275561
</code></pre></div><br>
<p>查看词向量模型的近义词，如果想用expand_dictionary函数，请阅读 <a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"><strong>预训练模型 | 金融会计类word2vec， 可扩展或构建领域内概念情感词典</strong></a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看幸福的同义词</span>
<span class="c1">#这个函数是我自己定义的，需要的点击上方说明的链接</span>
<span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;happiness&#39;</span><span class="p">],</span> 
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;happiness&#39;,
 &#39;happiness,&#39;,
 &#39;happiness.&#39;,
 &#39;well-being&#39;,
 &#39;well-being,&#39;,
 &#39;swb&#39;,
 &#39;contentment&#39;,
 &#39;wellbeing&#39;,
 &#39;peacefulness,&#39;,
 &#39;well-being:&#39;,
 &#39;happiness;&#39;,
 &#39;life-satisfaction,&#39;,
 &#39;gratitude&#39;,
 &#39;wellbeing,&#39;,
 &#39;unhappiness&#39;,
 &#39;eudaimonic&#39;,
 &#39;life-satisfaction&#39;,
 &#39;happier&#39;,
 &#39;loneliness&#39;,
 &#39;gratitude,&#39;,
 &#39;happiness:&#39;,
 &#39;peacefulness.&#39;,
 &#39;flourishing,&#39;,
 &#39;contentment,&#39;,
 &#39;materialism&#39;,
 &#39;swb,&#39;,
 &#39;flourishing&#39;,
 &#39;joy&#39;,
 &#39;vitality&#39;,
 &#39;gratefulness&#39;,
 &#39;eudemonic&#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mental&#39;</span><span class="p">],</span> 
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;mental&#39;,
 &#39;health&#39;,
 &#39;illness&#39;,
 &#39;backgroundmental&#39;,
 &#39;psychiatric&#39;,
 &#39;illness,&#39;,
 &#39;ofmental&#39;,
 &#39;abstractmental&#39;,
 &#39;illnesses&#39;,
 &#39;nonmental&#39;,
 &#39;mental-health&#39;,
 &#39;psychosocial&#39;,
 &#39;health/substance&#39;,
 &#39;illnesses,&#39;,
 &#39;psychological&#39;,
 &#39;health/social&#39;,
 &#39;diagnosable&#39;,
 &#39;disability&#39;,
 &#39;substance&#39;,
 &#39;ill-health&#39;,
 &#39;stigma&#39;,
 &#39;psycho-social&#39;,
 &#39;mdos.&#39;,
 &#39;health,&#39;,
 &#39;health/mental&#39;,
 &#39;non-mental&#39;,
 &#39;retardation/developmental&#39;,
 &#39;stigmatization&#39;,
 &#39;stigmatizing&#39;,
 &#39;illness.&#39;,
 &#39;disability,&#39;]
</code></pre></div><h2 id="三正文">三、正文</h2>
<p>可复现性对于加强科学预测和改善生活水平的战略至关重要，也是科学自我纠正的证明。<strong>卡尔·波普尔(1)得出结论，科学中的复现确保了“我们不仅仅在处理孤立的‘巧合’，而是处理具有规律性和可复现性的事件，这些事件从原则上来说是可以经过主体间的测试的</strong>。”2011年，一项有争议的关于“时间保留因果性”的研究(2)引发了一项罕见的复现研究。复现失败(3,4)，随之而来的是更多的复现研究，发现复现失败不仅仅是单一事件(5-10)。研究人员对系统性复现失败的影响表示担忧，包括知识库的削弱、公众对科学的不信任以及资金削减(11-13)。2016年，自然杂志对1500名科学家的调查报告称，51%的受访者认为科学正在经历复现危机(14)。这一回应促使美国国防高级研究计划局在2018年创建了一个计划，研究社会科学领域复现失败的规模和范围(15-17)。</p>
<p>尽管人们越来越担心复现失败的问题，但手动复现研究的样本数量很少，只占总文献的一小部分(18,19)。<strong>在心理学这个进行复现研究最多的科学学科中，直接、独立的复现研究数量不到400个</strong>。此外，样本的选择不平衡，主要来自于选定作者或特定子领域的经典论文(20,21)。大多数复现研究来自社会心理学和认知心理学等子领域，导致有关发展心理学、临床心理学和教育心理学等领域是否存在相似的复现失败率的猜测，尽管缺乏子领域特定的分析(22,23)。</p>
<p>**为了扩大和丰富复现数据，研究人员开发了替代方法来估计论文复现成功的可能性(**24)。预测市场已成为估计论文可复现性的主要方法。它涉及让专家们打赌，是否会在未来的手动复现测试中成功复现已经发表的论文(25)。该方法的高准确性使预测市场成为估计论文可复现性的有效解决方案(16然而，尽管预测市场比手动复现更具规模化，但仍需要在多年时间内招募数千名专家评审，以预测大量论文的可复现性(17)。</p>
<p>机器学习方法也被开发出来预测复现结果。机器学习模型可以从研究的叙述文本(26)或研究的数值特征，如P值或研究的样本大小(27,28)来预测可复现性。这两种类型的模型都能够准确预测，与预测市场的预测准确度相当(26)。<strong>基于文本的模型量化论文中的叙述，包括研究设计的描述和结果的解释(29)，这些内容在仅基于数值特征的模型中无法捕捉。此外，文本量化可以自动化，从而使该技术比从手稿中手动提取数值特征更具可扩展性和可复现性</strong>。</p>
<p>本文采用基于文本的机器学习方法来预测心理学文献的可复现性。我们的样本包括六个主要子领域的顶级心理学期刊近20年发表的几乎所有论文：<strong>临床心理学、认知心理学、发展心理学、组织心理学、人格心理学和社会心理学</strong>。总共有 <strong>14,126</strong> 篇论文，由 <strong>26,349</strong> 位不同的作者和 <strong>6,173</strong> 个不同的机构撰写，共有 <strong>1,222,292</strong> 次引用和 <strong>27,447</strong> 次媒体提及。</p>
<p>我们的分析如下：我们首先简要描述了我们的基于文本的机器学习模型，该模型已经得到验证，并显示能够准确预测手动复现的结果(26)。然后，我们应用该模型来预测心理学文献的可复现性，并着眼于研究可复现性如何在心理学子领域、研究方法、出版前后的特征以及作者团队的专业知识和经验方面存在差异。</p>
<p><br><br></p>
<h2 id="四数据和方法">四、数据和方法</h2>
<p>我们的分析使用了多种不同的文献、作者和媒体报道数据来源。生成数据的数据和代码已经存放在开放科学框架(OSF)上(30)。表格1列出了用于分析的大量期刊出版物，包括五个专门子领域的期刊和一份综合期刊《Psychological Science》。</p>
<p><img loading="lazy" src="img/table1.png" alt=""  />
</p>
<p>所有论文均在2000年至2019年期间发表，并根据期刊的子领域专业化进行分类：</p>
<ul>
<li><strong>Journal of Abnormal Psychology</strong> (临床心理学)</li>
<li><strong>Journal of Experimental Psychology, Learning, Memory &amp; Cognition</strong> (认知心理学)</li>
<li><strong>Child Development</strong> (发展心理学)</li>
<li><strong>Journal of Applied Psychology</strong>（组织心理学）</li>
<li><strong>Journal of Personality and Social Psychology</strong>（社会心理学）</li>
</ul>
<p>这里有两个例外。首先，因为人格研究出现在所有顶级期刊中，如果论文标题或摘要中含有 “<strong>人格personality</strong>” 一词，无论它们出现在哪个期刊中，我们都将其标记为人格研究。其次，由于 <strong>Psychological Science</strong> 刊登来自各个子领域的作品，我们将其论文分类为主要发表在哪个子领域期刊上的论文的所属子领域。总样本包括14,126篇论文。所有数据均按照出版商的使用条款和英国版权法进行收集。</p>
<br>
<h3 id="41-机器学习模型">4.1 机器学习模型</h3>
<p><strong>机器学习模型使用了随机森林和逻辑回归模型的集成，基于论文的文本预测了论文的可复现性概率</strong>。该模型在之前的一篇文章中经过了严格的样本外测试，并被证明其准确度与预测市场相当(26)。具体而言，创建模型的步骤如下(请参见SI附录，图S2说明程序)：</p>
<ul>
<li>第1步，<strong>将英语单词转换为向量。我们使用 word2vec(31) 在Microsoft学术图(MAG)(32)的 2000万社会科学出版物摘要语料库上训练模型。目标是将单词与社会科学文献的上下文联系起来，并以一个200维向量的形式量化表示这种联系</strong>。</li>
<li>第2步，<strong>将出版物转换为向量</strong>。为此，我们将训练样本(Table 2)中每篇论文中每个单词的标准化频率乘以其相应的200维单词向量，从而产生一个表示论文文本内容的论文级向量。</li>
<li>第3步，<strong>使用随机森林和逻辑回归的集成从论文级向量中预测每篇论文的复现结果(通过/失败)</strong>。为确定一项研究是否复现，我们使用所有复现研究报告的共同指标——复现团队对研究是否复现的总结判断(“是”或“否”)。步骤1到步骤3共同创建了一个使用论文的文本/叙述来预测其可复现性概率的机器学习模型，我们称之为“<strong>复现得分”(replication score)</strong>。SI附录，补充文本3提供了所有程序的详细信息。<strong>注意，论文中使用的是监督学习算法， 训练数据集仅有386篇（314个实验论文+72个非实验研究）</strong></li>
</ul>
<br>
<h3 id="42-关于模型迁移学习是否会预测不准问题">4.2 关于模型迁移学习是否会预测不准问题</h3>
<p>论文评估了与迁移学习相关的问题。<strong>当模型在一个领域中开发并应用于另一个领域时，就会发生迁移学习</strong>(43)。我们的研究中出现了这种情况，因为预测样本包含了训练样本中没有的两个子领域——临床心理学和发展心理学。<strong>这两个子领域的手动复现研究很少，如果要积累足够的样本可能需要另一个十年时间(44)。这引发了一个担忧，即模型能否为临床心理学和发展心理学中的论文提供有效的估计</strong>。为了解决这个问题，我们遵循协议，进行了三个独立的鲁棒性测试(43, 45, 46)。</p>
<p>(i)我们使用现有的社会和认知心理学数据来模拟迁移学习过程，估计使用一个基于一个心理学子领域的手动复现数据训练的模型来预测另一个心理学子领域的复现失败的表现，并将模型的预测与预测子领域中的实际手动复现数据进行比较。具体而言，<strong>我们研究了仅基于社会心理学论文( n=256)开发的模型在认知心理学论文(n=90)上的表现</strong>。我们发现，这种<strong>迁移学习到认知心理学的表现(AUC=0.72)与将模型应用于社会心理学的表现基准(AUC=0.73)相当。这为心理学子领域之间的迁移学习成功提供了支持</strong>。</p>
<p>(ii)有人可能会认为文本模型从社会心理学到认知心理学的成功转移并不保证其能够成功转移到临床心理学或发展心理学。为回答这个问题，我们比较了各个子领域的主题和文本相似性。<strong>先前的机器学习研究表明，基于文本的模型的迁移学习在训练和应用领域的文本特征更相似时更成功</strong>(47)。因此，<strong>如果社会-临床和社会-发展的相似性与社会-认知相似或更高，我们可以期望该模型在临床或发展心理学中的效度与在认知心理学中一样</strong>。</p>
<p>为了衡量两个子领域之间的研究主题重叠程度，我们从MAG数据库中收集了测试样本中每篇论文的研究主题。为了衡量两个子领域之间的文本相似性，我们计算了余弦相似度和词移距离（WMD）。附录SI，补充文本3.4.1详细描述了这些方法。</p>
<p><strong>结果显示，临床心理学（57％）和发展心理学论文（56％）与社会心理学论文的主题重叠比认知论文（42％）更高</strong>。此外，所有三个子领域都与社会心理学表现出相等的文本相似度（余弦相似度=0.90至0.91，WMD=0.24至0.26）。因为分析（i）显示基于社会心理学构建的模型可迁移到认知心理学，我们现在可以期望该模型可迁移到临床心理学和发展心理学，因为在这些领域观察到与社会心理学更高的特征相似性。</p>
<p><strong>我们评估了在临床心理学或发展心理学论文中，预测的复现得分与样本大小和P值等可复现性的替代指标的一致性</strong>。这两个指标都是可靠性的指标，因为样本大小越大，P值越低，虚阳性的风险就越小（5, 48, 49）。我们强调，预测模型不包含任何关于样本大小和P值的信息，因为训练样本中的论文已经去除了所有数字或统计信息。因此，如果一篇论文的样本大小和P值与我们模型的复现预测相关，那么它将为模型在临床心理学或发展心理学中的适用性提供独立支持。</p>
<p>在程序上，我们手动编码了来自预测结果的临床心理学和发展心理学的一组随机研究。为了获取样本大小，我们从论文中提取了参与者的数量。如果一篇论文有多个研究，我们取所有研究的平均样本大小。为了获得P值，我们从摘要中找到了论文的第一个主要声明，并提取了与该主要声明相关联的测试的P值。主要声明通常以“结果显示”或“我们的分析表明”等短语开头。补充文本3.4.2提供了更多的方法细节。</p>
<p>结果显示，预测的复现得分与原始样本大小r(97)=0.31，P=0.002和原始P值r(91)=-0.42，P &lt;0.001的等级顺序相关。由于预测模型不包含样本大小和P值信息，因此结果不是自证的，为成功的转移学习到临床心理学和发展心理学提供了支持。</p>
<br>
<h3 id="43-评估可复现性前后出版的相关度量">4.3 评估可复现性前后出版的相关度量。</h3>
<p>为了检查复现概率与论文其他可观察的出版特征之间的联系，我们构建了几个关键的可观察特征的度量。例如，已经有假设认为复现结果与研究人员的专业知识（34）或论文的媒体关注度（50）有关。我们收集了五个度量来捕捉论文的特征，其中三个度量预测了作者团队的特征，而另外两个度量预测了读者对研究的反应。预发布特征包括论文的第一作者和高级作者的经验和能力，衡量方法是他们在发表研究论文之前的：1）发表的论文数量累计总数，2）引用影响力，3）机构声誉，基于第一作者和高级作者的大学在2021 QS世界大学排名中的排名。高级作者是指在焦点论文发表时具有最高累积引用的作者。发表后的特征包括焦点论文的4）引用计数和5）媒体提及次数。媒体提及次数由Altmetric (52)计算。所有其他度量均来自Dimensions (53)，Dimensions已经批准我们在此项目中使用这些数据。为了控制度量中的出版年龄和子领域差异（见SI Appendix，图S1），我们通过将观察得分除以其子领域和出版年份的平均值来对所有度量进行了标准化。SI附录，补充文本2介绍了有关这些度量及其如何标准化的更多详细信息。</p>
<p><br><br></p>
<h2 id="五结果">五、结果</h2>
<p>**使用上述校准的机器学习模型，我们预测了每篇文章在可复现性预测样本（n = 14,126）中的可复现性得分。该得分可以解释为可复现性成功的相对可能性。**换句话说，一个可复现性得分为0.80的论文比得分更低的论文更有可能复现，并且比可复现性得分为0.40的论文有两倍的复现概率。使用可复现性得分，我们进行了三组分析：</p>
<ul>
<li>首先，我们确定了心理学子领域在估计可复现性率方面的差异，弥合了以前手动复现实验的小样本缺陷；</li>
<li>其次，我们比较了实验和非实验研究设计之间的可复现性率；</li>
<li>第三，我们研究了可复现性与论文其他出版前和出版后特征之间的相关性。</li>
</ul>
<p><img loading="lazy" src="img/table2.png" alt=""  />
</p>
<p>图1显示了所有14,126篇心理学论文的可复现性得分分布（范围为0.10到0.86，平均值为0.42，中位数为0.41，标准差为0.15，偏度为0.31）。有几点发现值得注意：</p>
<ul>
<li>首先，该分布与手动复现实验的猜测和预测市场的最新预测大致一致（15）。手动复现实验表明，心理学论文中略多于不及格的论文（43%的总体成功率）。最近20年的心理学出版物的可复现性得分估计分布也显示出类似的模式。</li>
<li>其次，有人认为，心理学中对可复现性失败的关注已经提高了可复现性的严格性（54）。当我们绘制我们20年期间的平均可复现性得分时，发现可复现性得分相对稳定。平均可复现性得分从2000年到2010年约下降了10%，然后从2010年到2019年回升到大约与2000年相同的水平（SI Appendix，Fig.S6）——这一模式与观察到的改变研究实践可能已经提高了心理学的可复现性率的观察相一致（9, 21, 55）。</li>
<li>第三，我们发现，汇总心理学子领域的可复现性得分掩盖了重要的子领域差异。下面，我们将详细说明心理学子领域之间的可复现性率差异。</li>
</ul>
<p><br><br></p>
<h2 id="六讨论">六、讨论</h2>
<p>本研究使用机器学习模型量化科学手稿中的文本，以预测其复现概率。该模型使我们能够首次对心理学六个主要子领域杂志上发表的几乎所有论文进行复现普查，分析重点是估计整个学科的复现率，以及复现率如何因子领域、实验和非实验方法以及其他研究论文特征而异。为了保持基于人类专业知识的结果，我们在可能的情况下验证了结果与可用的手动复现数据一致。结果进一步提供了可以推进复现理论和实践的见解。</p>
<p>我们方法的一个核心优势是其规模和范围。先前关于复现失败程度的推测基于相对较小的、有选择性的手动复现样本(21)。我们分析了多个子领域的14,000多篇论文，发现复现成功率在子领域间存在广泛差异。因此，不可能用一个单一的复现失败率来表征多样的学科分支。此外，我们的结果显示，复现成功率的子领域差异与研究方法有关。我们发现，对于所有子领域，实验研究的复现率明显低于非实验方法，并且在较少进行实验的子领域中，复现率相对较高。这一发现令人担忧，因为心理学在实验方面的熟练程度是其强有力的科学声誉的一部分。</p>
<p>分析可复现性与研究论文的其他度量标准的关系时，我们发现，虽然可复现性与研究人员的经验和能力呈正相关，但作者的大学声望和论文的引用量等研究质量的其他代理变量与心理学的可复现性无关。这些发现强调了需要在评估研究和学者时对前-后出版度量标准保持谨慎的态度。</p>
<p>我们还将媒体关注度与论文的可复现性进行了相关分析。媒体在创造科学公众形象和推广知识方面扮演着重要角色，但它通常有动机报道那些反直觉、引人注目的结果。理想情况下，媒体报道与心理学研究的可复现性率应有正向关系（或者没有关系）。然而，我们发现，媒体对论文的报道与其复现成功的可能性存在负相关。因此，基于媒体报道来判断一篇论文的价值是不明智的。对于媒体来说，提醒公众新的、创新的科学研究结果只是引发思考，需要未来的复现实验来证实其健壮性，是很有价值的。 我们设想了两种可能的应用方向：</p>
<p>第一，机器学习模型可用于预测难以或无法进行手动复现的研究（例如纵向研究和特殊或难以访问的人群）。</p>
<p>第二，预测的复现分数可以开始帮助优先选择需要手动复现的某些研究，面对资源有限的情况。每年，个人学者和组织（如心理学科学加速器（67）和协作复现和教育项目（68））都会遇到一个问题：从众多的心理学研究中选择哪些进行复现。Isager等人（69）提出，为了最大化复现的收益，社区应该优先复现那些价值高、结果不确定的研究。研究的价值可以通过引用量或媒体关注度等因素来近似计算，但不确定性部分尚未得到大量文献的充分衡量。我们建议，我们的机器学习模型可以提供复现不确定性的定量测量。 我们注意到，我们的发现在几个方面存在限制：</p>
<ul>
<li>
<p>首先，我们对所有论文的预测都来自于顶级期刊。未来的研究可以检查来自较低排名期刊的论文，以及它们的可复现性如何与发表前后的指标相关联（70）。</p>
</li>
<li>
<p>其次，可复现性的估计仅是近似值。在子领域级别上，我们分析的六个子领域中有五个子领域仅由一种顶级期刊代表。单个期刊不能涵盖整个子领域的范围。</p>
</li>
</ul>
<p>未来的研究可以在以下几个方向展开：</p>
<ul>
<li>
<p>我们的复现得分可以与其他方法结合使用，例如预测市场（16）或非文本机器学习模型（27、28），以进一步精确估计心理学研究的可复现性；</p>
</li>
<li>
<p>可以重复设计本研究，以在其他学科中进行复现普查；</p>
</li>
<li>
<p>可将可复现性得分进一步与其他感兴趣的指标进行相关性分析。</p>
</li>
</ul>
<p>社会科学中的可复现性受到变异性的限制，它最终是一种由各种方法组合而成的集体企业。波普尔在他的书《科学发现的逻辑》中提出：“即使是我们自己的观察结果，我们也不会完全认真对待，或将其视为科学观察结果，直到我们对其进行了重复和测试”（1）。然而，尽管波普尔对于重复和可重复性的洞察是正确的，但必须认识到测试带来了探索的成本。机器学习方法与人类智慧的结合，是发展更好的可复现性理解的有效方法。这种组合平衡了测试成本和科学探索的收益。</p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据分析 | 使用决策树分析小红书帖子数据(含代码)</title>
      <link>https://textdata.cn/blog/2023-03-11-xiaohongshu-data-analysis/</link>
      <pubDate>Sat, 11 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-11-xiaohongshu-data-analysis/</guid>
      <description>使用决策树分析小红书热门帖的特点，如何成为热帖。</description>
      <content:encoded><![CDATA[<h2 id="内容来源">内容来源</h2>
<p>整理自 <a href="https://www.kaggle.com/code/huzujun/xiaohongshu-data-mining/notebook">https://www.kaggle.com/code/huzujun/xiaohongshu-data-mining/notebook</a></p>
<h3 id="点击下载代码数据2023-03-11-xiaohongshu-data-analysiszip"><a href="2023-03-11-xiaohongshu-data-analysis.zip">点击下载代码数据</a></h3>
<p><br><br></p>
<h2 id="安装相关库">安装相关库</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">dtreeviz</span><span class="o">==</span><span class="mf">1.3.3</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">emoji</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">plotly</span>
</code></pre></div><p><br><br></p>
<h2 id="一导入数据">一、导入数据</h2>
<p>点击下载数据 <a href="xiaohongshu.csv">xiaohongshu.csv</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&#34;ignore&#34;</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;xiaohongshu.csv&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二数据预处理">二、数据预处理</h2>
<p>列是series类型数据， 使用apply方法(应用lambda函数)对列进行批处理。</p>
<p>这里可以做很多事情，例如</p>
<ul>
<li>desc的文本长度、表情数量</li>
<li>title标题的长度</li>
<li>几点发文(24小时制的)</li>
<li>&hellip;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">emoji</span>

<span class="c1">#desc中的表情数量</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;emoji_nums&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;desc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">desc</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">emoji</span><span class="o">.</span><span class="n">emoji_list</span><span class="p">(</span><span class="n">desc</span><span class="p">)))</span>

<span class="c1">#话题tag数量</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;hashTag_nums&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;desc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;#&#39;</span><span class="p">)</span>

<span class="c1">#标题title的文本长度</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;title_len&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;desc_len&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;desc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len</span><span class="p">()</span>
<span class="c1">#几点发文</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;coerce&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;hours&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">hour</span>

<span class="c1">#把文本数字转为数值型数字</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;image_nums&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;image_nums&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;liked_count&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;liked_count&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>

<span class="c1">#显示前5行</span>
<span class="n">df</span><span class="p">[[</span><span class="s1">&#39;emoji_nums&#39;</span><span class="p">,</span> <span class="s1">&#39;hashTag_nums&#39;</span><span class="p">,</span> <span class="s1">&#39;title_len&#39;</span><span class="p">,</span> <span class="s1">&#39;desc_len&#39;</span><span class="p">,</span> <span class="s1">&#39;hours&#39;</span><span class="p">,</span> <span class="s1">&#39;image_nums&#39;</span><span class="p">,</span> <span class="s1">&#39;liked_count&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三数据分析">三、数据分析</h2>
<h2 id="31-处理点赞数">3.1 处理点赞数</h2>
<p>看一下点赞数的数据分布</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">liked_count</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>count     5928.000000
mean       157.174764
std        713.708655
min          0.000000
25%          2.000000
50%         15.000000
75%         69.000000
max      22265.000000
Name: liked_count, dtype: float64
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>

<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>

<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># 如果是其他系统，可以使用系统默认字体</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>


<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">liked_count</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;点赞数&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;分布&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;liked_count核密度分布图&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_11_1.svg" alt="svg"  />

​</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">liked_count</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">liked_count</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">liked_count</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</code></pre></div><pre><code>0
15.0
22265
</code></pre>
<br>
<p>这里可以看出一个问题，点赞数具有很强的长尾效应，也就是说一半的文章点赞数小于15，而热门的文章点赞数可以上千上万，因此如果直接分析点赞数的数值，用回归之类的算法去分析，一定是灾难，所以我们不妨简单处理，在这里我先人为定义点赞数&gt;=50为热帖，其它的不是热帖，通过这个方式把笔记分成了两类</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;heat&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;liked_count&#34;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">22266</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;凉帖&#39;</span><span class="p">,</span> <span class="s1">&#39;热帖&#39;</span><span class="p">])</span>
</code></pre></div><p><br><br></p>
<h2 id="32-分析热帖比例和小时的关系">3.2 分析热帖比例和小时的关系</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">plotly</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objs</span> <span class="k">as</span> <span class="nn">go</span>
<span class="n">plotly</span><span class="o">.</span><span class="n">offline</span><span class="o">.</span><span class="n">init_notebook_mode</span><span class="p">()</span>

<span class="n">heats</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">24</span><span class="p">):</span>
    <span class="n">hour_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;hours&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">heats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hour_df</span><span class="p">[</span><span class="n">hour_df</span><span class="p">[</span><span class="s1">&#39;heat&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;热帖&#39;</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">hour_df</span><span class="p">))</span>

<span class="n">layout</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="s1">&#39;24小时随时间热帖图&#39;</span><span class="p">}</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">24</span><span class="p">)),</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">heats</span><span class="p">}],</span> <span class="n">layout</span> <span class="o">=</span> <span class="n">layout</span><span class="p">)</span>
<span class="n">plotly</span><span class="o">.</span><span class="n">offline</span><span class="o">.</span><span class="n">iplot</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/pic1.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="33-分析热帖比例和表情数的关系">3.3 分析热帖比例和表情数的关系</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;emoji_nums&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>count    5928.000000
mean        6.824055
std         9.996283
min         0.000000
25%         0.000000
50%         3.000000
75%        10.000000
max       103.000000
Name: emoji_nums, dtype: float64
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">emoji_nums</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;表情数&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;分布&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;emoji表情数核密度分布图&#39;</span><span class="p">)</span>    
</code></pre></div><p><img loading="lazy" src="img/output_19_1.svg" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">plotly.offline</span> <span class="k">as</span> <span class="nn">py</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objs</span> <span class="k">as</span> <span class="nn">go</span>

<span class="n">df</span><span class="p">[</span><span class="s2">&#34;emoji_level&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;emoji_nums&#34;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">90</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;表情少&#39;</span><span class="p">,</span> <span class="s1">&#39;表情多&#39;</span><span class="p">])</span>

<span class="n">emoji_levels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;表情少&#39;</span><span class="p">,</span> <span class="s1">&#39;表情多&#39;</span><span class="p">]:</span>
    <span class="n">emoji_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;emoji_level&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">emoji_levels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">emoji_df</span><span class="p">[</span><span class="n">emoji_df</span><span class="p">[</span><span class="s1">&#39;heat&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;热帖&#39;</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">emoji_df</span><span class="p">))</span>

<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">]</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="o">.</span><span class="n">Bar</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;表情数&lt;=4&#39;</span><span class="p">,</span> <span class="s1">&#39;表情数&gt;4&#39;</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">emoji_levels</span><span class="p">,</span>
    <span class="n">marker_color</span><span class="o">=</span><span class="n">colors</span> <span class="c1"># marker color can be a single color value or an iterable</span>
<span class="p">)])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">title_text</span><span class="o">=</span><span class="s1">&#39;表情数-热帖占比&#39;</span><span class="p">)</span>
<span class="n">plotly</span><span class="o">.</span><span class="n">offline</span><span class="o">.</span><span class="n">iplot</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/pic2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="34-分析热帖比例和标签的关系">3.4 分析热帖比例和标签的关系</h2>
<p>我原本以为的是“适当的标签最好，不是越多越好”，结果推翻了我的猜想</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;hashTag_nums&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">([</span><span class="n">i</span><span class="o">/</span><span class="mi">10</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)])</span>
</code></pre></div><p>Run</p>
<pre><code>count    5928.000000
mean        3.638327
std         4.077919
min         0.000000
10%         0.000000
20%         0.000000
30%         1.000000
40%         2.000000
50%         3.000000
60%         3.000000
70%         4.000000
80%         6.000000
90%         9.000000
100%       49.000000
max        49.000000
Name: hashTag_nums, dtype: float64
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;hashTags_level&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;hashTag_nums&#34;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">49</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="n">emoji_levels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">):</span>
    <span class="n">con</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;hashTags_level&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">emoji_levels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">con</span><span class="p">[</span><span class="n">con</span><span class="p">[</span><span class="s1">&#39;heat&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;热帖&#39;</span><span class="p">])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">con</span><span class="p">))</span>

<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="o">.</span><span class="n">Bar</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;&lt;= 1 hashTags&#39;</span><span class="p">,</span> <span class="s1">&#39;1 &lt; hashTags &lt;= 2&#39;</span><span class="p">,</span> <span class="s1">&#39;2 &lt; hashTags &lt;= 3&#39;</span><span class="p">,</span> <span class="s1">&#39;3 &lt; hashTags &lt;= 4&#39;</span><span class="p">,</span> <span class="s1">&#39;4 &lt; hashTags &lt;= 6&#39;</span><span class="p">,</span>
      <span class="s1">&#39;6 &lt; hashTags &lt;= 9&#39;</span><span class="p">,</span> <span class="s1">&#39;9 &lt; hashTags &lt;= 49&#39;</span><span class="p">],</span>
    <span class="n">y</span><span class="o">=</span><span class="n">emoji_levels</span><span class="p">,</span> <span class="c1"># marker color can be a single color value or an iterable</span>
<span class="p">)])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">title_text</span><span class="o">=</span><span class="s1">&#39;话题数-热帖占比&#39;</span><span class="p">)</span>
<span class="n">plotly</span><span class="o">.</span><span class="n">offline</span><span class="o">.</span><span class="n">iplot</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/pic3.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;hashTag_nums&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">9</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="35-决策树模型">3.5 决策树模型</h2>
<p>这里我们尝试用这些因子去预测帖子是否热门，我挑选的是决策树，虽然别的模型可能会搞出几个百分点的准确率，但是必要性不是很大，在这里我更想要看到可解释性强的结果，而不是黑盒模型，这是决策数的优点，因为预测是否热帖这个事情不是我们的目的。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;liked_count&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>count     5928.000000
mean       157.174764
std        713.708655
min          0.000000
25%          2.000000
50%         15.000000
75%         69.000000
max      22265.000000
Name: liked_count, dtype: float64
</code></pre>
<br>
<p>决策树模型要求种类要平衡，这里有两种解决方法：</p>
<ul>
<li>对不平衡的种类重新取样</li>
<li>这里因为数据本来就不算很多，我就人为重新定义了中位数作为热帖的分水岭</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s2">&#34;heat&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;liked_count&#34;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">22266</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;heat&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;heat&#34;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">]))</span>
</code></pre></div><pre><code>2982 2946
</code></pre>
<br>
<p>决策树模型根据尝试，在最大深度为3时近似就能取到很好的准确率了</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">df</span><span class="p">[</span><span class="s2">&#34;heat&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;liked_count&#34;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">22266</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&#34;emoji_level&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&#34;emoji_nums&#34;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">90</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;image_nums&#39;</span><span class="p">,</span>  <span class="s1">&#39;desc_len&#39;</span><span class="p">,</span> <span class="s1">&#39;title_len&#39;</span><span class="p">,</span> <span class="s1">&#39;hashTags_level&#39;</span> <span class="p">,</span><span class="s1">&#39;emoji_level&#39;</span><span class="p">,</span> <span class="s1">&#39;hours&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;heat&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div><pre><code>4446 1482
0.699055330634278
</code></pre>
<br>
<p>70%的准确率意味着比乱猜的50%要好不少，在我们完全没有去关注帖子内容，仅仅只根据图片数、文本长度来预测是否热帖已经有这个准确率，已经是非常好了。</p>
<p>我们可视化决策树</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">dtreeviz.trees</span> <span class="kn">import</span> <span class="n">dtreeviz</span> <span class="c1"># remember to load the package</span>

<span class="n">viz</span> <span class="o">=</span> <span class="n">dtreeviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span>
                <span class="n">target_name</span><span class="o">=</span><span class="s2">&#34;target&#34;</span><span class="p">,</span>
                <span class="n">feature_names</span><span class="o">=</span><span class="n">cols</span><span class="p">,</span>
                <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;unpopular&#39;</span><span class="p">,</span> <span class="s1">&#39;popular&#39;</span><span class="p">])</span>
<span class="n">viz</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&#34;viz.svg&#34;</span><span class="p">)</span>
<span class="n">viz</span>
</code></pre></div><p><img loading="lazy" src="img/output_33_0.svg" alt="svg"  />

​</p>
<br>
<p>可视化决策树的结论：一般来说，越多的图片、越长的标题、越长的正文能<strong>显著</strong>获得更高的热贴机会，这是根据决策树的结果得到的。不过如果图片和标题信息量足够了，正文不用特别长也可以了。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">888</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div><pre><code>0.7031039136302294
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">permutation_importance</span><span class="p">(</span><span class="n">forest</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> 
                                <span class="n">n_repeats</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">forest_importances</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">importances_mean</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">forest_importances</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">yerr</span><span class="o">=</span><span class="n">result</span><span class="o">.</span><span class="n">importances_std</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&#34;模型中特征重要性排序&#34;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&#34;平均精度下降&#34;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/output_36_0.svg" alt="svg"  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>近年《管理世界》《管理科学学报》《金融研究》使用文本分析论文</title>
      <link>https://textdata.cn/blog/research_with_tm_in_chinese_top_ms_journal/</link>
      <pubDate>Fri, 17 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/research_with_tm_in_chinese_top_ms_journal/</guid>
      <description>近年《管理世界》《管理科学学报》期刊中使用文本分析论文汇总</description>
      <content:encoded><![CDATA[<h2 id="管理世界">管理世界</h2>
<p><strong>曾庆生,周波,张程,陈信元.年报语调与内部人交易:“表里如一”还是“口是心非”?[J].管理世界,2018,34(09):143-160.DOI:10.19744/j.cnki.11-1235/f.2018.09.012.</strong></p>
<p><a href="https://textdata.cn/blog/manager_tone_analysis_with_lm/">本文代码实现</a></p>
<blockquote>
<p><strong>摘要</strong>: 基于中国A股非金融公司2007～2014年年报语调的文本分析,本文研究了年报语调与年报披露后的内部人交易行为之间的关系。研究发现,年报语调越积极,公司高管在年报公布后一段期间内的卖出股票规模越大,净买入股票规模越小,表明公司高管编制年报时存在&quot;口是心非&quot;的操纵嫌疑。进一步研究发现,年报披露后中期市场表现差、信息透明度低、非国有控股的公司高管交易与年报语调的反向关系分别显著强于年报披露后中期市场表现好、信息透明度高、国有控股的公司;而公司盈余管理程度、交易者职位（是否核心高管）对年报语调与高管交易关系的影响不显著。此外,年报语调越积极,高管亲属卖出股票的规模也越大,但未发现公司重要股东交易与年报语调相关。上述结果表明,中国上市公司年报存在语调管理行为,年报语调成为除会计报表以外另一种可以被内部人管理或操纵的信息。</p>
<p>**关键词：**年报; 语调管理; 内部人交易; 信息不对称;</p>
</blockquote>
<br>
<p><strong>洪永淼,汪寿阳.大数据如何改变经济学研究范式？[J].管理世界,2021,37(10):40-55+72+56.DOI:10.19744/j.cnki.11-1235/f.2021.0153.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文首先从经济学视角探讨大数据给经济学实证研究所带来的范式变革,包括从理性经济人到非完全理性经济人,从孤立的经济人到互相关联的社会经济人,从代表性经济人到异质性经济主体,以及从经济分析到经济社会活动的系统分析。然后,从方法论视角讨论大数据给经济学实证研究方法所带来的变革,包括从模型驱动到数据驱动,从参数不确定性到模型不确定性,从无偏估计到有偏估计,从低维建模到高维建模,从低频数据到高频甚至实时数据,从结构化数据到非结构化数据,从传统结构化数据到新型结构化数据,以及从人工分析到智能分析等。大数据引起的经济学研究范式与研究方法变革,正在深刻重塑经济学发展方向,不但加强了经济学实证研究范式的趋势,而且还进一步突破了现代西方经济学的一些基本假设的局限性,使经济学研究日益呈现出科学化、严谨化、精细化、多元化(跨学科)与系统化的趋势,并且与社会科学其他领域在方法论上日益趋同。中国大数据资源,为从中国经济实践中总结经济发展规律,从中国特殊性中凝练可复制的经济发展模式,从而构建具有深厚学理基础的原创性中国经济理论体系,提供了一个得天独厚的&quot;富矿&quot;。</p>
<p><strong>关键词：</strong>	大数据;文本分析;机器学习;研究范式;研究方法;反身性;</p>
</blockquote>
<br>
<p><strong>张宗新,吴钊颖.媒体情绪传染与分析师乐观偏差——基于机器学习文本分析方法的经验证据[J].管理世界,2021,37(01):170-185+11+20-22.DOI:10.19744/j.cnki.11-1235/f.2021.0011.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文利用2013～2017年上市公司的百度新闻报道作为文本,运用机器学习文本分析方法测算情绪倾向得分,考察了媒体情绪对分析师预测行为的影响及其传染机制与风险后果。研究发现:(1)媒体乐观情绪会显著正向影响分析师盈利预测的乐观偏差度;(2)媒体情绪通过&quot;分析师有限关注&quot;与&quot;投资者情绪&quot;两条路径来影响分析师预测的乐观倾向;(3)分析师乐观情绪和媒体乐观情绪均会加剧股价波动及尾部风险,且分析师乐观情绪是媒体情绪影响股价波动的传导路径;(4)明星分析师与非明星分析师均会受到媒体情绪的感染,前者理性程度相对更高但其行为对股价波动冲击更为明显。本研究对于规范媒体行为,矫正分析师过度乐观偏差,合理引导理性投资具有重要意义。</p>
<p><strong>关键词：</strong>	媒体报道情绪;分析师乐观偏差;股价波动;有限理性;</p>
</blockquote>
<br>
<p><strong>胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.DOI:10.19744/j.cnki.11-1235/f.2021.0070.</strong></p>
<blockquote>
<p>**摘要：**在可持续发展战略导向下,秉持长远理念是企业抵御外部环境威胁和拥有可持续经营能力的基石。然而,作为企业掌舵人的管理者并非都具有长远的目光。本文基于高层梯队理论和社会心理学中的时间导向理论,提出了管理者内在的短视主义特质与企业资本支出和研发支出的关系,并采用文本分析和机器学习技术构建出管理者短视主义指标从而对其进行实证检验。研究结果发现,年报MD&amp;A中披露的&quot;短期视域&quot;语言能够反映管理者内在的短视主义特质,管理者短视会导致企业减少资本支出和研发支出。当公司治理水平、监督型机构投资者的持股比例以及分析师关注度越高时,管理者短视主义对这些长期投资的负向影响越易受到抑制。最终,管理者短视主义导致的研发支出减少和资本投资效率降低会损害企业的未来绩效。本文拓宽了管理者短视主义的行为后果分析,对企业高层次管理人才的聘任以及企业和政府的监管具有重要的实践启示。同时,本文将文本分析和机器学习方法引入管理者短视主义的研究,为未来该领域的研究提供了参考和借鉴。</p>
<p><strong>关键词：</strong> 管理者短视;长期投资;文本分析;机器学习;</p>
</blockquote>
<br>
<p><strong>底璐璐,罗勇根,江伟,陈灿.客户年报语调具有供应链传染效应吗？——企业现金持有的视角[J].管理世界,2020,36(08):148-163.DOI:10.19744/j.cnki.11-1235/f.2020.0124.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 利用我国供应商企业前五名上市客户及其管理层语调的文本数据,本文考察了跨企业关系情形下客户年报语调对供应商企业现金持有决策的影响。研究结果发现,客户的年报语调越消极,供应商企业则会持有更多的现金,表明客户年报净负面语调在供应链上存在传染效应。进一步的研究发现,非国有性质、相对议价能力较低的供应商企业现金持有与客户年报净负面语调的正相关关系分别显著强于国有性质、相对议价能力较高的供应商企业。此外,当客户融资融券程度较高时,客户年报净负面语调对供应商企业现金持有的正向影响会有所增强。本文的研究不仅在考察跨企业情形下企业现金持有的影响因素以及客户文本信息的经济后果两个方面弥补了国内外现有研究的不足,而且对于企业如何进行现金持有决策提供了一定的经验证据与参考,这对于管理供应链相关风险,推动我国企业的供应链整合进而提升我国企业的全球竞争力具有重要的启示意义。</p>
<p><strong>关键词：</strong>	年报语调;现金持有;供应链传染;文本分析;</p>
</blockquote>
<br>
<p><strong>林晚发,赵仲匡,宋敏.管理层讨论与分析的语调操纵及其债券市场反应[J].管理世界,2022,38(01):164-180.DOI:10.19744/j.cnki.11-1235/f.2022.0012.</strong></p>
<blockquote>
<p>**摘要: **本文研究了管理层讨论与分析（MD&amp;A）语调的操纵行为及其债券市场反应。研究发现,MD&amp;A异常积极语调与预警Z值负相关,债务重组正相关,这表明MD&amp;A异常积极语调暗示了企业较高的未来风险,这与语调的信息增量解释相悖,因此MD&amp;A异常积极语调更可能是操纵的结果。进一步研究发现,MD&amp;A异常积极语调越大,债券信用评级越高,且该正向关系在与评级机构利益冲突大、信息透明度低的公司子样本中更显著;此外,债券投资者能够识别语调操纵行为,但随着债券市场公众投资者的参与,MD&amp;A异常积极语调与债券信用利差之间呈现出一定的负向关系,且这种负向关系在信息透明度低的企业组中更加显著。本文较早使用中国资本市场数据度量了MD&amp;A异常积极语调,且证实这种异常语调是管理层操纵的结果,并探讨了MD&amp;A语调操纵对于债券市场信息效率的影响,相关结论对于完善MD&amp;A文本信息披露监管法规、提高评级机构独立性以及提升债券市场信息效率具有重要启示。</p>
<p><strong>关键词：</strong></p>
<p>MD＆A语调操纵; 利益冲突; 债券信用评级; 债券信用利差;</p>
</blockquote>
<p><br><br></p>
<h2 id="管理科学学报">管理科学学报</h2>
<p><strong>马长峰, 陈志娟, 张顺明. 基于文本大数据分析的会计和金融研究综述[J]. 管理科学学报, 2020, 23(9):12..</strong></p>
<blockquote>
<p>**摘要：**作为一种非结构化数据,文本大数据最近十年深刻影响会计学和金融学研究.这种影响体现在两类文献:第一类以信息为中心,将文本分析技术用于信息的品质(可读性)和数量(文本信息含量),信息披露和市场异象等方面的研究;第二类与信息无关,主要是利用文本大数据分析技术构建全新指标,例如基于文本分析的公司竞争力,创新和经济政策不确定性等新变量,梳理上述文献研究脉络,揭示文本分析技术的优缺点,并且指出在会计和金融领域应用文本大数据技术的研究面临的挑战和机遇。</p>
<p>**关键词：**可读性; 信息; 欺诈; 创新; 经济政策不确定性</p>
</blockquote>
<br>
<p><strong>杨晓兰,王伟超,高媚.股市政策对股票市场的影响——基于投资者社会互动的视角[J].管理科学学报,2020,v.23;No.187(01):15-32.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 本文将影响股市的政策分为五类,检验股市的政策效应;并以新浪财经博客为投资者之间社会互动的媒介,利用文本挖掘技术和社会网络研究方法,构建反映投资者之间社会互动程度、情绪属性以及社会网络中心程度的变量,探讨社会互动对股市政策效应的影响.实证研究表明,舆论导向政策对股市收益率存在显著的正向影响;证券供给需求性政策、货币政策显著提高股市波动率,市场创新与市场交易制度显著降低市场波动率.同时,投资者对专业性政策的解读显著依赖于社会互动,社会互动会放大货币政策对股市收益率的正向影响,加剧证券供给需求性政策对股市波动的影响,平缓市场创新与市场交易制度对股市波动的影响,而不影响舆论导向政策对股市产生的效应.</p>
<p><strong>关键词：</strong>	政策;社交网络;社会互动;股票市场;文本挖掘;</p>
</blockquote>
<br>
<p><strong>赵子夜,杨庆,杨楠.言多必失?管理层报告的样板化及其经济后果[J].管理科学学报,2019,22(03):53-70.</strong></p>
<blockquote>
<p>**摘要：**样板化报告在古今中外都有广泛的运用,但报告者面临两难:一方面,样板化有利于规避披露风险;但另一方面,样板化又不利于传递内部信息.那么,投资者如何评价中国上市公司的报告的样板化程度?以中国上市公司的管理层讨论与分析的文字为样本,用公司t期和t-1期报告的纵向文本相似度以及本公司和其他公司同期的报告的平均横向相似度来衡量样板化的水平,并考察了其经济后果.检验结果表明,纵向样板化的经济后果呈现相机抉择性,当公司财务风险高（亏损、微利或者被出具非标准审计意见）时,信息效应占优,样板化的报告引发负面的市场评价,而当公司财务风险较低,风险效应占优,样板化的报告则引发市场的好评.另一方面,报告横向样板化则引起了整体的负面评价.在调节效应方面,纵向样板化的经济后果受公司创新、特质信息、董事长权力和停牌次数的影响,横向样板化的经济后果则受到公司独立董事的社会网络位置的影响.综合结果表明,公司管理层讨论与分析的横向样板化,以及在高财务风险条件下的纵向样本化都会因信息披露不足而引起负面的经济后果.</p>
<p><strong>关键词：</strong> 管理层报告;样板化;文本分析;经济后果;</p>
</blockquote>
<br>
<p><strong>卞世博, 管之凡, 阎志鹏. 答非所问与市场反应:基于业绩说明会的研究[J]. 管理科学学报, 2021, 24(4):18.</strong></p>
<blockquote>
<p><strong>摘要:</strong> 对上市公司业绩说明会中投资者与管理层问答互动中管理层答非所问的现象进行了研究.本文以中小板和创业板上市公司召开的业绩说明会作为研究样本,利用文本分析方法对业绩说明会中管理层在回答投资者提问时答非所问的程度进行度量,进而实证分析了管理层的答非所问与市场反应和公司未来业绩表现之间的可能关联.结果 发现:在控制其它因素之后,管理层的答非所问与市场反应之间呈现显著的负相关关系,即公司管理层的答非所问程度越高,随后公司股票的市场表现则就会越差,并且对于那些低分析师关注的公司尤为明显;而在公司未来业绩表现方面,管理层答非所问的程度越高,则公司未来的业绩表现则会越差.。</p>
<p>**关键词：**业绩说明会; 答非所问; 市场反应; 未来业绩</p>
</blockquote>
<br>
<p><strong>逯东, 宋昕倍. 媒体报道,上市公司年报可读性与融资约束[J]. 管理科学学报, 2021, 24(12):17..</strong></p>
<blockquote>
<p>**摘要：**采用文本分析方法,深入考察了上市公司年报可读性与融资约束的关系,并考虑媒体报道这一外部信息的调节效应研究发现,上市公司的年报可读性越低,其面临的融资约束越高;媒体报道的增多可以弱化年报可读性与融资约束的关系,且媒体报道情绪越正向,其调节作用越显著.进一步分析发现:机构投资者持股比例较高能减弱年报可读性和融资约束的关系;当年报可读性较低时,媒体报道的信息效应更为显著;只有官方媒体和地方媒体的报道数量与正向报道情绪能够显著缓解年报可读性低带来的融资约束;同时,较低的年报可读性是通过提高融资成本路径来加大公司的融资约束,且使得公司未来的融资方式呈现出内部融资增加,外部融资减少的特点.从融资约束角度拓展了关于财务报告文本信息披露质量的研究,并揭示了媒体报道如何有效改善内部信息披露不足的作用机理,为企业如何通过改善内,外部的信息环境来缓解自身的融资困境提供了理论依据。</p>
<p>**关键词：**年报可读性；融资约束；媒体报道；文本分析</p>
</blockquote>
<br>
<p><strong>姚加权, 冯绪, 王赞钧,等. 语调,情绪及市场影响:基于金融情绪词典[J]. 管理科学学报, 2021, 24(5):21.</strong></p>
<blockquote>
<p>**摘要：**金融文本的语调与情绪含有上市公司管理层以及个体投资者表达的情感信息,并对股票市场产生影响.通过词典重组和深度学习算法构建了适用于正式文本与非正式文本的金融领域中文情绪词典,并基于词典构建了上市公司的年报语调和社交媒体情绪指标.构建的年报语调指标和社交媒体情绪指标能有效地预测上市公司股票的收益率,成交量,波动率和非预期盈余等市场因素,并优于基于其他广泛使用情绪词典构建的指标.此外,年报语调指标和社交媒体情绪指标对上市公司的股价崩盘风险具有显著的预测作用.为文本大数据在金融市场的应用提供了分析工具,也为大数据时代的金融市场预测和监管等活动提供了决策支持。</p>
<p>**关键词：**情绪词典；语调；投资者情绪；市场影响</p>
</blockquote>
<br>
<p><strong>姜富伟, 马甜, 张宏伟. 高风险低收益? 基于机器学习的动态CAPM模型解释[J]. 管理科学学报, 2021.</strong></p>
<blockquote>
<p>**摘要：**我国股票市场存在高风险股票反而伴随较低收益的低风险定价异象,这有悖于传统资产定价理论.本文使用宏观经济和微观企业特征构建了六百多个变量的宏微观混合大数据集,并结合多种经典机器学习算法开发了基于大数据和机器学习的智能动态CAPM模型,检验了时变系统性风险对我国股市收益解释能力.实证结果表明:本文的智能动态CAPM定价模型能够显著解释我国股市低风险定价异象;随机森林等非线性机器学习算法表现最佳;影响股票时变系统风险的主要因素是市场类因子,基本面因子居次.本文对于我国股市系统性风险测度,动态资产定价模型构建和金融与大数据和人工智能融合创新有重要理论与实践指导意义.</p>
<p>**关键词：**系统性风险; 动态CAPM; 机器学习; 金融大数据</p>
</blockquote>
<br>
<p><strong>陆瑶, 张叶青, 黎波,等. 高管个人特征与公司业绩——基于机器学习的经验证据[J]. 管理科学学报, 2020, 23(2):21.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 在目前的公司治理文献中,大部分的高管特征研究一方面仅关注单一的高管特征与公司业绩之间的关联,缺乏全面的高管特征分析;另一方面主要围绕因果推断进行研究,缺乏从预测能力出发的系统定量的结论.本文首次采用机器学习算法中的Boosting回归树,全面考察了多维度高管特征对公司业绩的预测性.以我国2008年～2016年的上市公司为样本,研究了高管的多维个人特征是否能预测公司业绩,并进一步分析了对公司业绩预测能力较强的高管个人特征及其预测模式.研究发现:1)整体而言,在我国公司CEO和董事长的特征对公司业绩的预测能力较弱;2)在众多高管个人特征之中,高管持股比例和年龄对公司业绩的预测能力较强;3)高管持股比例和年龄与公司业绩之间的关联都呈现出非线性的特点,与以往的理论较为吻合.本研究不仅利用机器学习方法从一个更为全面的视角对中国的高管特征进行了研究,也为公司高管聘任和激励机制设计等方面提供了有益的启发.</p>
<p>**关键词：**机器学习；Boosting回归树；公司治理；公司业绩</p>
</blockquote>
<br>
<p><strong>吴武清, 赵越, 闫嘉文,等. 分析师文本语调会影响股价同步性吗?&ndash;基于利益相关者行为的中介效应检验[J]. 管理科学学报, 2020, 23(9):19.</strong></p>
<blockquote>
<p>**摘要：**文章考察了分析师研究报告的文本语调对股价同步性的影响与作用机制.首先爬取2006年至2018年中国A股上市公司377644份分析师研究报告,随机选出10434句文本并人工分为积极,中性,消极三类形成语料库,以此训练11种机器学习方法并比较各方法的预测准确性,最终选择朴素贝叶斯方法估计出分析师研究报告的文本语调.实证分析发现,分析师积极的文本语调显著降低了所追踪公司的股价同步性.这一结果与已有多数研究结论不同,但在做空机制欠发达的中国资本市场,个体选择性知觉理论为此提供了很好的解释.进一步地,中介效应检验结果表明,分析师积极的文本语调通过激励公司发布更多公告,引导机构投资者买入和吸引其他分析师发布研究报告,显著降低了股价同步性.该研究对于投资者关注研报语调指标,上市公司加强信息披露,政府部门完善资本市场制度均具有重要启示。</p>
<p>**关键词：**分析师文本语调; 股价同步性; 朴素贝叶斯; 选择性知觉; 中介效应</p>
</blockquote>
<br>
<p><strong>刘冠男, 曲金铭, 李小琳,等. 基于深度强化学习的救护车动态重定位调度研究[J]. 管理科学学报, 2020, 23(2):15.</strong></p>
<blockquote>
<p>**摘要：**救护车是挽救患者生命的重要医疗资源,合理调配有限的救护车资源可以降低呼叫响应时间,提高医疗服务水平.本文面向救护车动态重定位调度问题,提出了一种基于强化学习的调度策略结构.为解决传统强化学习所面临的高维状态空间的挑战,本文基于深度Q值网络(DQN)方法,提出了一种考虑多种调度交互因子的算法RedCon-DQN,以在给定环境状态下得到最优的重定位调度策略.在此基础上,本文还提出了急救网络弹性概念,以评估各站点对全局救护优化目标的影响力.最后,基于南京市2016年～2017年的实际救护车呼叫及响应数据,构造了环境交互模拟器.在模拟器中通过大规模数据实验,验证了模型得到的调度策略相比已有方法的优越性,并分析了不同时段下调度策略的有效性及其特点.</p>
<p>**关键词：**强化学习; DQN; 救护车调度; 重定位</p>
</blockquote>
<br>
<p><strong>黄丽华, 何晓, 卢向华. 企业在线社群内容组合策略的影响研究[J]. 管理科学学报, 2020, 23(2):15..</strong></p>
<blockquote>
<p>**摘要：**现代企业通过建立在线社群实现与消费者的互动,希望在向消费者提供服务的同时进行更好的营销,然而如何提供在线社群中的营销与服务内容却是一大难题.本文在营销—服务二元理论的基础上,提出了在线社群内容二元性的平衡维度与结合维度概念,并研究平衡维度与结合维度如何影响销售业绩与消费者的满意度.结合机器学习方法,本文发现,平衡维度对消费者满意度和销售绩效有提高作用,但是,结合维度对消费者满意度及企业绩效的影响呈倒U型;另外,企业员工的技能水平对内容二元性策略的效果有着显著的调节作用.研究结论对企业理解在线社群中的营销内容与服务内容之间的二元关系,以及内容提供策略的价值机制有重要的指导意义。</p>
<p>**关键词：**在线社群; 内容二元性; 销售绩效; 消费者满意度</p>
</blockquote>
<br>
<p><strong>部慧,解峥,李佳鸿,吴俊杰.基于股评的投资者情绪对股票市场的影响[J].管理科学学报,2018,v.21;No.166(04):86-101.</strong></p>
<blockquote>
<p><strong>摘要：</strong> 探讨投资者情绪对我国股票市场的影响.为刻画投资者情绪,基于东方财富网股吧帖文与朴素贝叶斯方法,提出融合股评看涨看跌预期和投资者关注程度的投资者情绪度量指标.进一步,利用Granger因果检验、瞬时Granger因果检验、跨期回归分析等方法,探讨了投资者情绪对我国股票收益率、交易量和波动性是否具有预测能力及影响.实证结果揭示:虽然投资者情绪对股票市场收益率、交易量和波动性均无预测能力,但投资者情绪对股票收益率和交易量有当期影响;开盘前非交易时段的股评情绪对开盘价具有预测力,开盘后交易时段的股评情绪对收盘价和日交易量具有更显著的影响.此外,股票收益率是投资者情绪的Granger原因,即投资者情绪的形成依赖于前期市场收益率.这些实证结果为深入理解参与股吧评论的交易者的行为以及行为对市场产生的影响提供了证据.</p>
<p><strong>关键词:</strong> 	投资者情绪; 噪声交易者; 文本挖掘; Granger因果检验;</p>
</blockquote>
<p><br><br></p>
<p>##金融研究</p>
<p>姜富伟, 胡逸驰, &amp; 黄楠. (2021). 央行货币政策报告文本信息, 宏观经济与股票市场. <em>金融研究</em>, <em>492</em>(6), 95-113.</p>
<blockquote>
<p>**摘要: ** 本文利用金融情感词典和文本分析技术,分析中国人民银行货币政策执行报告的文本情绪、文本相似度和文本可读性等多维文本信息,刻画央行货币政策执行报告的文本特征,探究货币政策报告的文本信息与宏观经济和股票市场的关系。实证研究发现,货币政策报告的文本情绪的改善会引起显著为正的股票市场价格反应,报告文本相似度的增加会引起股票市场波动性的显著降低,报告可读性对公布后股票市场的波动性影响不显著。货币政策报告文本情绪还与诸多宏观经济指标显著相关。进一步研究发现,引起股票市场显著反应的是报告文本情绪中反映货币政策指引的部分,而反映宏观经济历史状态的部分对股票市场的影响不显著。本文从文本大数据分析角度证明了我国央行沟通的有效性,对国内央行沟通相关研究形成了有益补充。</p>
<p><strong>关键词:</strong>  文本情绪分析  中央银行沟通  股票市场  宏观经济</p>
</blockquote>
<br>
<p>孙彤, 薛爽, &amp; 崔庆慧. (2021). 企业家前台化影响企业价值吗?——基于新浪微博的实证证据. 金融研究, 491(5), 189-206.</p>
<blockquote>
<p><strong>摘要:</strong> 互联网时代信息传递成本和沟通成本显著降低。微博作为自媒体的主要代表之一,为企业家从企业的幕后走向台前提供了一条便捷的途径。本文以信息传递理论为基础,利用新浪微博数据,检验了企业家前台化行为对企业价值的影响。实证结果表明:(1)企业家发布微博这一前台化行为有助于提升企业价值。从对价值影响的路径看,企业家微博发布后,企业经营活动现金流增加,系统性风险降低;(2)针对企业家微博进行文本分析,发现企业家微博中个性化微博比例越高、“艾特”人数越多或者微博内容中正向语调比例越高,对企业价值的正向影响越显著;(3)相对于信息不对称程度较低的企业,信息不对称程度较高的企业中企业家更倾向于发布微博。上述实证结果说明自媒体对缓解企业、企业家与投资者之间的信息不对称具有一定作用,为企业家前台化决策及路径选择提供了参考。</p>
<p><strong>关键词:</strong>  企业家  前台化  微博  企业价值  信息传递</p>
</blockquote>
<br>
<p>阮睿, 孙宇辰, 唐悦, &amp; 聂辉华. (2021). 资本市场开放能否提高企业信息披露质量?——基于 “沪港通” 和年报文本挖掘的分析. 金融研究, 488(2), 188-206.</p>
<blockquote>
<p><strong>摘要:</strong> 提高信息披露质量对于改善上市公司治理结构和保护股东权益具有重要意义。本文利用2014年开通的“沪港通”机制这一准自然实验,研究资本市场开放是否提高了企业的信息披露质量。从2010-2019年A股上市公司年报文本中提炼可读性指标衡量信息披露质量,使用匹配和双重差分方法进行实证研究,发现“沪港通”机制实施以后,标的公司(纳入“沪港通”的A股上市公司)的信息披露质量显著提高。这一结论对不同的估计方法、样本区间及控制变量组均保持稳健。异质性分析表明,对于盈余操纵水平较高、股价信息含量较低的企业,资本市场开放能够更好地改善其信息披露质量。本文丰富了资本市场开放对企业行为和绩效影响的实证研究,为继续推进资本市场开放政策提供了理论依据。</p>
<p><strong>关键词:</strong>  沪港通  资本市场开放  信息披露  文本分析</p>
</blockquote>
<br>
<p>李哲, &amp; 王文翰. (2021). “多言寡行” 的环境责任表现能否影响银行信贷获取——基于 “言” 和 “行” 双维度的文本分析. 金融研究, 498(12), 116-132.</p>
<blockquote>
<p><strong>摘要:</strong> 基于我国推行绿色信贷的政策背景,本文考察了企业“多言寡行”的环境责任表现能否影响银行的信贷决策。研究发现:(1)从总体来看,“多言寡行”的环境责任表现有助于企业获取更多的银行借款。(2)相比于长期银行借款,“多言寡行”对于短期银行借款的正向影响更为明显。(3)《关于构建绿色金融体系的指导意见》的出台抑制了“多言寡行”对银行借款的正向影响。(4)进一步分析发现,相比于环境责任表现“少言多行”以及“少言寡行”的企业,企业“多言寡行”的环境责任表现对于银行的信贷资源具有显著的正向影响;“多言寡行”对银行借款的正向影响在无背景关联、价值较低以及市场环境更差的企业中更为明显。本文有助于信贷机构认识到绿色信贷政策面临的执行风险,为确保绿色信贷的健康发展提供了新的决策参考。
<strong>关键词:</strong>  环境责任表现  绿色金融  绿色信贷  文本分析</p>
</blockquote>
<br>
<p>潘健平, 潘越, &amp; 马奕涵. (2019). 以 “合” 为贵? 合作文化与企业创新. 金融研究, 463(1), 148-167.</p>
<blockquote>
<p><strong>摘要:</strong> 本文以2006-2015年沪深A股非金融上市公司为样本,基于上市公司网站对于企业文化的叙述和年报董事会报告两份本文,采用文本分析方法,构建两个度量企业合作文化强弱的指标,并研究企业合作文化对企业创新产出和创新效率的影响。研究发现,企业文化越强调合作,企业的创新产出越多,创新效率越高。这一结论在采用增加控制变量、利用水稻播种面积作为工具变量以及以董事长的非正常离职事件为冲击进行PSM-DID等多种方法后仍然稳健。渠道检验的结果显示,合作文化是通过提高企业内部员工的凝聚力和促进企业的“产学研”合作这两种渠道来促进企业创新。进一步的研究表明,合作文化的促进作用在竞争性行业以及地区信任程度和产业集群程度较高的地区中尤为显著。本文不仅从微观层面揭示企业文化对公司财务行为的影响机理,丰富和补充了当前方兴未艾的“文化与金融”研究,而且为国家制定建设社会主义文化强国的方针战略提供理论基础和实证支持。</p>
<p><strong>关键词:</strong>  合作  企业文化  企业创新</p>
</blockquote>
<br>
<p>彭红枫, &amp; 林川. (2018). 言之有物: 网络借贷中语言有用吗?——来自人人贷借款描述的经验证据. 金融研究, 461(11), 133-153.</p>
<blockquote>
<p><strong>摘要:</strong> 本文以“人人贷”平台的388522条借款标的为样本,基于借款描述文本构造P2P网络借贷词典,并探究文本中六种类型词语比重对网络借贷行为的影响,实证结果表明:首先,各类词语比重发出的信号对贷款人的投资决策有显著影响,积极类词语和金融类词语比重与借款成功率呈正相关,消极类词语比重、强语气词语比重和弱语气词语比重均与借款成功率呈负相关关系;其次,不同年龄层次和不同收入水平的借款人提供的描述性文本中词语信号对贷款人行为的影响存在较大差异,而性别差异和学历高低基本不影响词语信号作用的发挥;最后,各类词语比重发出的质量信号是部分有效的,金融类词语比重发出的信号有效且被投资者正确识别,强语气词语比重发出的信号同样有效却未被投资者准确识别,其他类别词语比重不是有效质量信号。</p>
<p><strong>关键词:</strong>  网络借贷  文本分析  信号理论</p>
</blockquote>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>长期征稿</title>
      <link>https://textdata.cn/blog/call_for_paper/</link>
      <pubDate>Fri, 17 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/call_for_paper/</guid>
      <description>欢迎向我们提供Python/R技术、文本(数据)分析、经管科研(含Python或R)等内容的稿件</description>
      <content:encoded><![CDATA[<ul>
<li></li>
</ul>
<h2 id="引言">引言</h2>
<p>总有一些你不认识的人，知道你想知道的东西。『大邓和他的Python』 或许可以成为一座桥梁，在大数据时代，促使不同背景、不同方向的学者学术灵感相互碰撞，迸发出更多的可能性。</p>
<p>『大邓和他的Python』 鼓励分享 Python/R技术、文本(数据)分析、经管科研(含Python或R)等内容。目的只有一个，让数据科学在社会科学中更接地气。</p>
  <br>
<h2 id="内容选题">内容选题</h2>
<p>未来公众号的选题内容规划</p>
<ol>
<li>网络爬虫(数据采集)</li>
<li>文本、音频、视频、文件等数据处理</li>
<li>机器学习、自然语言处理</li>
<li>经管、社科领域，借助数据挖掘的研究和技术</li>
<li>Python相关技术分享</li>
<li>其他(待定)</li>
</ol>
  <br>
<h2 id="稿件要求">稿件要求</h2>
<ul>
<li>
<p>文章确系个人原创作品，未曾在公开渠道发表，
如为其他平台已发表或待发表的文章，请明确标注</p>
</li>
<li>
<p>稿件建议以 markdown 格式撰写，
示例链接: <a href="https://pan.baidu.com/s/1ZpvWhrGGbah71YbkW-7pjg">https://pan.baidu.com/s/1ZpvWhrGGbah71YbkW-7pjg</a> 提取码: upuc</p>
</li>
<li>
<p>投递邮件发送至 <a href="mailto:thunderhit@qq.com">thunderhit@qq.com</a></p>
</li>
</ul>
  <br>
<h2 id="作者福利">作者福利</h2>
<ul>
<li>
<p>尊重原作者署名权，并将为每篇被采纳的原创首发稿件，
提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算。</p>
</li>
<li>
<p>如作者内容分享成体系，文稿质量高，公众号可组织付费直播课。</p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>机器学习实战 | 信用卡欺诈检测</title>
      <link>https://textdata.cn/blog/ml_credit_card_fraud_detection/</link>
      <pubDate>Thu, 16 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/ml_credit_card_fraud_detection/</guid>
      <description>本文旨在使用 XGBoost、随机森林、KNN、逻辑回归、SVM 和决策树解决金融领域信用卡欺诈识别的分类问题</description>
      <content:encoded><![CDATA[<blockquote>
<p>作者: 小猴子</p>
<p>公众号: 机器学习研习院</p>
</blockquote>
<p>本文旨在使用 XGBoost、随机森林、KNN、逻辑回归、SVM 和决策树解决分类问题</p>
<h2 id="案例简介">案例简介</h2>
<p>假设你受雇于帮助一家信用卡公司检测潜在的欺诈案件，你的工作是确保客户不会因未购买的商品而被收取费用。给你一个包含人与人之间交易的数据集，他们是欺诈与否的信息，并要求你区分它们。我们的最终目的是通过构建分类模型来对欺诈交易进行分类区分来解决上述情况。</p>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="ml_credit_card_fraud_detection.zip">点击下载</a></p>
<br>
<p>对于这个案例，所需要用到的主要模块是处理数据的 Pandas、处理数组的 NumPy、用于数据拆分、构建和评估分类模型的 scikit-learn，最后是用于 xgboost 分类器模型算法的 xgboost 包。</p>
<br>
<h2 id="导入数据">导入数据</h2>
<p>关于数据： 我们将要使用的数据是 <strong>Kaggle 信用卡欺诈检测数据集</strong>。它包含特征 V1 到 V28，是 PCA 获得的主要成分，并忽略对构建模型没有用的时间特征。其余的特征是包含交易总金额的&quot;金额&quot;特征和包含交易是否为欺诈案件的&quot;类别&quot;特征。</p>
<p>现在使用&rsquo;pd.read_csv&rsquo;方法导入数据，并查看部分数据样例。</p>
<p>Kaggle 信用卡欺诈检测数据集: <a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">https://www.kaggle.com/mlg-ulb/creditcardfraud</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;creditcard.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>...</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.359807</td>
      <td>-0.072781</td>
      <td>2.536347</td>
      <td>1.378155</td>
      <td>-0.338321</td>
      <td>0.462388</td>
      <td>0.239599</td>
      <td>0.098698</td>
      <td>0.363787</td>
      <td>0.090794</td>
      <td>...</td>
      <td>-0.018307</td>
      <td>0.277838</td>
      <td>-0.110474</td>
      <td>0.066928</td>
      <td>0.128539</td>
      <td>-0.189115</td>
      <td>0.133558</td>
      <td>-0.021053</td>
      <td>149.62</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.191857</td>
      <td>0.266151</td>
      <td>0.166480</td>
      <td>0.448154</td>
      <td>0.060018</td>
      <td>-0.082361</td>
      <td>-0.078803</td>
      <td>0.085102</td>
      <td>-0.255425</td>
      <td>-0.166974</td>
      <td>...</td>
      <td>-0.225775</td>
      <td>-0.638672</td>
      <td>0.101288</td>
      <td>-0.339846</td>
      <td>0.167170</td>
      <td>0.125895</td>
      <td>-0.008983</td>
      <td>0.014724</td>
      <td>2.69</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.358354</td>
      <td>-1.340163</td>
      <td>1.773209</td>
      <td>0.379780</td>
      <td>-0.503198</td>
      <td>1.800499</td>
      <td>0.791461</td>
      <td>0.247676</td>
      <td>-1.514654</td>
      <td>0.207643</td>
      <td>...</td>
      <td>0.247998</td>
      <td>0.771679</td>
      <td>0.909412</td>
      <td>-0.689281</td>
      <td>-0.327642</td>
      <td>-0.139097</td>
      <td>-0.055353</td>
      <td>-0.059752</td>
      <td>378.66</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.966272</td>
      <td>-0.185226</td>
      <td>1.792993</td>
      <td>-0.863291</td>
      <td>-0.010309</td>
      <td>1.247203</td>
      <td>0.237609</td>
      <td>0.377436</td>
      <td>-1.387024</td>
      <td>-0.054952</td>
      <td>...</td>
      <td>-0.108300</td>
      <td>0.005274</td>
      <td>-0.190321</td>
      <td>-1.175575</td>
      <td>0.647376</td>
      <td>-0.221929</td>
      <td>0.062723</td>
      <td>0.061458</td>
      <td>123.50</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.158233</td>
      <td>0.877737</td>
      <td>1.548718</td>
      <td>0.403034</td>
      <td>-0.407193</td>
      <td>0.095921</td>
      <td>0.592941</td>
      <td>-0.270533</td>
      <td>0.817739</td>
      <td>0.753074</td>
      <td>...</td>
      <td>-0.009431</td>
      <td>0.798278</td>
      <td>-0.137458</td>
      <td>0.141267</td>
      <td>-0.206010</td>
      <td>0.502292</td>
      <td>0.219422</td>
      <td>0.215153</td>
      <td>69.99</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 30 columns</p>
</div>
<p>接下来将进行一些数据预处理和探索性数据分析（EDA）。</p>
<br>
<h2 id="探索性数据分析">探索性数据分析</h2>
<p>看看数据集中有多少欺诈案件和非欺诈案件。此外，还计算整个记录交易中欺诈案件的百分比。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">termcolor</span> <span class="kn">import</span> <span class="n">colored</span> <span class="k">as</span> <span class="n">cl</span>

<span class="n">cases</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">nonfraud_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">fraud_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">fraud_percentage</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">fraud_count</span><span class="o">/</span><span class="n">nonfraud_count</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;CASE COUNT&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Total number of cases are </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cases</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Number of Non-fraud cases are </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nonfraud_count</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Number of Non-fraud cases are </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fraud_count</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Percentage of fraud cases is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fraud_percentage</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mCASE COUNT[0m
[1m--------------------------------------------[0m
[1mTotal number of cases are 284807[0m
[1mNumber of Non-fraud cases are 284315[0m
[1mNumber of Non-fraud cases are 492[0m
[1mPercentage of fraud cases is 0.17[0m
[1m--------------------------------------------[0m
</code></pre>
<p>我们可以看到，在 <strong>284,807</strong> 个样本中，只有 <strong>492</strong> 个欺诈案例，仅占样本总数的 <strong>0.17%</strong> 。所以，可以说我们正在处理的数据是高度不平衡的数据，需要在建模和评估时谨慎处理。</p>
<p>接下来，我们将使用 Python 中的**&ldquo;describe&rdquo;**方法获取欺诈和非欺诈交易金额数据的统计视图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">nonfraud_cases</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">fraud_cases</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">Class</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;CASE AMOUNT STATISTICS&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;NON-FRAUD CASE AMOUNT STATS&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nonfraud_cases</span><span class="o">.</span><span class="n">Amount</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;FRAUD CASE AMOUNT STATS&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fraud_cases</span><span class="o">.</span><span class="n">Amount</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mCASE AMOUNT STATISTICS[0m
[1m--------------------------------------------[0m
[1mNON-FRAUD CASE AMOUNT STATS[0m
count    284315.000000
mean         88.291022
std         250.105092
min           0.000000
25%           5.650000
50%          22.000000
75%          77.050000
max       25691.160000
Name: Amount, dtype: float64
[1m--------------------------------------------[0m
[1mFRAUD CASE AMOUNT STATS[0m
count     492.000000
mean      122.211321
std       256.683288
min         0.000000
25%         1.000000
50%         9.250000
75%       105.890000
max      2125.870000
Name: Amount, dtype: float64
[1m--------------------------------------------[0m
</code></pre>
<p>在查看统计数据时，可以看到与其余变量相比，&quot;<strong>金额</strong>&quot; 变量中的值变化很大。为了减少其广泛的值，我们可以使用 python 中的 &ldquo;<strong>StandardScaler()</strong>&rdquo; 方法对其进行标准化。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">amount</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Amount&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Amount&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">amount</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Amount&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1m0    0.244964
1   -0.342475
2    1.160686
3    0.140534
4   -0.073403
5   -0.338556
6   -0.333279
7   -0.190107
8    0.019392
9   -0.338516
Name: Amount, dtype: float64[0m
</code></pre>
<br>
<h2 id="特征选择和数据集拆分">特征选择和数据集拆分</h2>
<p>在这个过程中，定义自变量 (X) 和因变量 (Y)。使用定义的变量将数据分成训练集和测试集，进一步用于建模和评估。可以使用 python 中的 &ldquo;<strong>train_test_split</strong>&rdquo; 算法轻松拆分数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c1"># DATA SPLIT</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;X_train samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">X_train</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;X_test samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;y_train samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;y_test samples : &#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]),</span>
      <span class="n">y_test</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<pre><code>[1mX_train samples : [0m [[-1.11504743  1.03558276  0.80071244 -1.06039825  0.03262117  0.85342216
  -0.61424348 -3.23116112  1.53994798 -0.81690879 -1.30559201  0.1081772
  -0.85960958 -0.07193421  0.90665563 -1.72092961  0.79785322 -0.0067594
   1.95677806 -0.64489556  3.02038533 -0.53961798  0.03315649 -0.77494577
   0.10586781 -0.43085348  0.22973694 -0.0705913  -0.30145418]]
[1mX_test samples : [0m [[-0.32333357  1.05745525 -0.04834115 -0.60720431  1.25982115 -0.09176072
   1.1591015  -0.12433461 -0.17463954 -1.64440065 -1.11886302  0.20264731
   1.14596495 -1.80235956 -0.24717793 -0.06094535  0.84660574  0.37945439
   0.84726224  0.18640942 -0.20709827 -0.43389027 -0.26161328 -0.04665061
   0.2115123   0.00829721  0.10849443  0.16113917 -0.19330595]]
[1my_train samples : [0m [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[1my_test samples : [0m [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
</code></pre>
<p>到目前为止，已经做好了构建分类模型所需的所有准备。</p>
<br>
<h2 id="模型建立">模型建立</h2>
<p>这里构建六种不同类型的分类模型，即<strong>决策树、K-最近邻 (KNN)、逻辑回归、支持向量机 (SVM)、随机森林和 XGBoost</strong>。虽然我们还可以使用更多其他的模型，但我们选用的是用于解决分类问题的最流行模型。所有这些模型构建均比较方便，都可以使用 <strong>scikit-learn</strong> 包提供的算法来构建。仅对于 XGBoost 模型，将使用 xgboost 包。接下来在 python 中实现这些模型，所使用的算法可能需要花费一定的时间来实现。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>


<span class="c1"># MODELING</span>

<span class="c1"># 1. Decision Tree</span>
<span class="n">tree_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">criterion</span> <span class="o">=</span> <span class="s1">&#39;entropy&#39;</span><span class="p">)</span>
<span class="n">tree_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">tree_yhat</span> <span class="o">=</span> <span class="n">tree_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 2. K-Nearest Neighbors</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">n</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">knn_yhat</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 3. Logistic Regression</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr_yhat</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 4. SVM </span>
<span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">svm_yhat</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 5. Random Forest Tree</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rf_yhat</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 6. XGBoost</span>
<span class="n">xgb</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">xgb_yhat</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div><p>至此我们构建了从决策树模型到 XGBoost 模型的六种不同类型的分类模型。</p>
<p>在决策树模型中，使用 <strong>&ldquo;DecisionTreeClassifier&rdquo;</strong> 算法来构建模型。在算法中，设置 <strong>&ldquo;max_depth=4&rdquo;</strong>，意味着允许树最大分裂四次，<strong>&ldquo;criterion = &lsquo;entropy&rdquo;</strong>，与**&ldquo;max_depth&rdquo;**最相似，但决定何时分裂停止分裂树。最后拟合模型后将预测值存储到 <strong>&ldquo;tree_yhat&rdquo;</strong> 变量中。</p>
<p>在K-最近邻 (KNN)中，使用 <strong>&ldquo;KNeighborsClassifier&rdquo;</strong> 算法构建了模型，并设置 <strong>&ldquo;n_neighbors=5&rdquo;</strong>。 <strong>&lsquo;n_neighbors&rsquo;</strong> 的值是随机选择的，其实可以通过迭代一系列值来有目的地选择，然后拟合模型后将预测值存储到 <strong>&ldquo;knn_yhat&rdquo;</strong> 变量中。</p>
<p>逻辑回归的代码没有什么可解释的，因为我使用 <strong>&ldquo;LogisticRegression&rdquo;</strong> 算法并全部使用默认值，并拟合模型后将预测值存储到 <strong>&ldquo;lr_yhat&rdquo;</strong> 变量中。</p>
<p>使用&quot;SVC&quot;算法构建了支持向量机模型，并且同样使用默认值，并且默认内核就是我们所希望用到的模型，即&quot;rbf&quot;内核。之后，我们在拟合模型后将预测值存储到 &ldquo;svm_yhat&rdquo; 中。</p>
<p>接下来使用 <strong>&ldquo;RandomForestClassifier&rdquo;</strong> 算法构建的随机森林模型，设置参数 <strong>&ldquo;max_depth=4&rdquo;</strong>，就像构建决策树模型的方式一样。最后在拟合模型后将预测值存储到 <strong>&ldquo;rf_yhat&rdquo;</strong> 中。请记住，决策树和随机森林之间的主要区别在于，决策树使用整个数据集来构建单个模型，而随机森林使用随机选择的特征来构建多个模型。这就是为什么很多情况下选择使用随机森林模型而不是决策树的原因。</p>
<p>最后是 XGBoost 模型。使用 xgboost 包提供的 <strong>&ldquo;XGBClassifier&rdquo;</strong> 算法构建模型。设置 <strong>&ldquo;max_depth=4&rdquo;</strong>，最后在拟合模型后将预测值存储到 &ldquo;xgb_yhat&rdquo; 中。</p>
<p>至此，我们成功构建了六种分类模型，为了便于理解，对代码进行了简单解释。接下来需要评估每个模型，并找到最适合我们案例的模型。</p>
<br>
<h2 id="模型评估">模型评估</h2>
<p>之前有提到过，我们将使用 scikit-learn 包提供的评估指标来评估我们构建的模型。在此过程中的主要目标是为给定案例找到最佳模型。这里将使用的评估指标是<strong>准确度评分指标、f1 评分指标，及混淆矩阵</strong>。</p>
<h3 id="准确率">准确率</h3>
<p>准确率是最基本的评价指标之一，广泛用于评价分类模型。准确率分数的计算方法很简单，就是将模型做出的正确预测的数量除以模型做出的预测总数（可以乘以 100 将结果转换为百分比）。一般可以表示为：</p>
<p><strong>准确度分数 = 正确预测数 / 总预测数</strong></p>
<p>我们检查我们所构建的六种不同分类模型的准确率分数。要在 python 中完成，我们可以使用 scikit-learn 包提供的 <strong>&ldquo;accuracy_score&rdquo;</strong> 方法。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># 1. Accuracy score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;ACCURACY SCORE&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the Decision Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the KNN model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the Logistic Regression model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the SVM model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">svm_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the Random Forest Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;Accuracy score of the XGBoost model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">xgb_yhat</span><span class="p">)),</span> 
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mACCURACY SCORE[0m
[1mAccuracy score of the Decision Tree model is 0.9993679997191109[0m
[1m[32mAccuracy score of the KNN model is 0.9995259997893332[0m
[1m[31mAccuracy score of the Logistic Regression model is 0.9991924440855307[0m
[1mAccuracy score of the SVM model is 0.9993153330290369[0m
[1mAccuracy score of the Random Forest Tree model is 0.9993153330290369[0m
[1mAccuracy score of the XGBoost model is 0.9994908886626171[0m
</code></pre>
<p>根据准确性评分评估指标来看，<strong>KNN</strong> 模型为最准确的模型，而 <strong>Logistic</strong> 回归模型最不准确。然而，当我们对每个模型的结果进行四舍五入时，得到 99% 的准确性，这看是一个非常好的分数。</p>
<h3 id="f1-score">F1-score</h3>
<p>F1-score 或 F-score 是用于评估分类模型的最流行的评估指标之一。它可以简单地定义为<strong>模型的准确率和召回率的调和平均值</strong>。它的计算方法是将 模型的精度和召回率的乘积除以模型的精度和召回率相加得到的值，最后乘以 2 得到的值。可以表示为：</p>
<p><strong>F1-score  = 2( (精度 * 召回率) / (精度 + 召回率) )</strong></p>
<p>可以使用 scikit-learn 包提供的 &ldquo;f1_score&rdquo; 方法轻松计算 <strong>F1-score</strong> 。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span> 

<span class="c1"># 2. F1 score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 SCORE&#39;</span><span class="p">,</span> <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the Decision Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the KNN model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">knn_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the Logistic Regression model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the SVM model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">svm_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the Random Forest Tree model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cl</span><span class="p">(</span><span class="s1">&#39;F1 score of the XGBoost model is </span><span class="si">{}</span><span class="s1">&#39;</span>
         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">xgb_yhat</span><span class="p">)),</span>
         <span class="n">attrs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bold&#39;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<pre><code>[1mF1 SCORE[0m
[1mF1 score of the Decision Tree model is 0.8105263157894738[0m
[1m[32mF1 score of the KNN model is 0.8571428571428572[0m
[1m[31mF1 score of the Logistic Regression model is 0.7356321839080459[0m
[1mF1 score of the SVM model is 0.7771428571428572[0m
[1mF1 score of the Random Forest Tree model is 0.7796610169491525[0m
[1mF1 score of the XGBoost model is 0.8449197860962566[0m
</code></pre>
<p>模型的排名几乎与之前的评估指标相似。在 F1-score 评估指标的基础上，KNN 模型再次夺得第一，Logistic 回归模型仍然是最不准确的模型。</p>
<h3 id="混淆矩阵">混淆矩阵</h3>
<p>通常，混淆矩阵是分类模型的可视化，显示模型与原始结果相比预测结果的程度。通常，预测结果存储在一个变量中，然后将其转换为相关表。使用相关表，以热图的形式绘制混淆矩阵。尽管有多种内置方法可以可视化混淆矩阵，但我们将从零开始定义和可视化它，以便更好地理解。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 3. Confusion Matrix</span>
<span class="c1"># defining the plot function</span>
<span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Confusion Matrix of </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    
    <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
    <span class="kn">import</span> <span class="nn">itertools</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span> <span class="n">cm</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">interpolation</span> <span class="o">=</span> <span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">tick_marks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">rotation</span> <span class="o">=</span> <span class="mi">45</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_marks</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

    <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;.2f&#39;</span> <span class="k">if</span> <span class="n">normalize</span> <span class="k">else</span> <span class="s1">&#39;d&#39;</span>
    <span class="n">thresh</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">fmt</span><span class="p">),</span>
                 <span class="n">horizontalalignment</span> <span class="o">=</span> <span class="s1">&#39;center&#39;</span><span class="p">,</span>
                 <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;white&#39;</span> <span class="k">if</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span> <span class="k">else</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True label&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted label&#39;</span><span class="p">)</span>
    
<span class="c1"># Compute confusion matrix for the models</span>

<span class="n">tree_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">tree_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Decision Tree</span>
<span class="n">knn_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> 
                <span class="n">knn_yhat</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># K-Nearest Neighbors</span>
<span class="n">lr_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lr_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Logistic Regression</span>
<span class="n">svm_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">svm_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Support Vector Machine</span>
<span class="n">rf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># Random Forest Tree</span>
<span class="n">xgb_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">xgb_yhat</span><span class="p">,</span> 
                <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># XGBoost</span>

<span class="c1"># Plot the confusion matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</code></pre></div><h3 id="decision-tree">Decision tree</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">tree_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">tree_matrix</span><span class="p">,</span> 
                      <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                      <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Decision Tree&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;tree_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_21_0.png" alt="png"  />

​</p>
<h3 id="k-nearest-neighbors">K-Nearest Neighbors</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">knn_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">knn_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;KNN&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;knn_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_23_0.png" alt="png"  />

​</p>
<h3 id="logistic-regression">Logistic regression</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">lr_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">lr_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Logistic Regression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;lr_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_25_0.png" alt="png"  />

​</p>
<h3 id="support-vector-machine">Support Vector Machine</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">svm_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">svm_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;SVM&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;svm_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_27_0.png" alt="png"  />

​</p>
<h3 id="random-forest">Random Forest</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">rf_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">rf_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Random Forest Tree&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;rf_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_29_0.png" alt="png"  />

​</p>
<h3 id="xgboost">XGBoost</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">xgb_cm_plot</span> <span class="o">=</span> <span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">xgb_matrix</span><span class="p">,</span> 
                                <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Non-Default(0)&#39;</span><span class="p">,</span><span class="s1">&#39;Default(1)&#39;</span><span class="p">],</span> 
                                <span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;XGBoost&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;xgb_cm_plot.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_31_0.png" alt="png"  />

​</p>
<p><strong>混淆矩阵理解</strong>： 以XGBoost模型的混淆矩阵为例。</p>
<ul>
<li>
<p>第一行。
第一行是测试集中实际欺诈值为0的交易。可以计算，其中56861笔欺诈值为0。在这56861笔非欺诈交易中，分类器正确预测了其中的56854笔为 0 和 预测了其中 7 为 1。这意味着，对于 56854 笔非欺诈交易，测试集中的实际流失值为 0，分类器也正确预测为 0。可以说我们的模型已经对非欺诈交易进行了分类交易还不错。</p>
</li>
<li>
<p>第二行。
看起来有 101 笔交易的欺诈值为 1。分类器正确预测其中 79 笔为 1，错误预测值为 0 的 22 笔。错误预测值可以视为模型的错误。</p>
</li>
</ul>
<p>在比较所有模型的混淆矩阵时可以看出，K-Nearest Neighbors 模型在从非欺诈交易中分类欺诈交易方面做得非常好，其次是 XGBoost 模型。所以可以得出结论，最适合本次案例的模型是 <strong>K-Nearest Neighbors</strong> 模型，可以忽略的模型是 Logistic 回归模型。</p>
<br>
<h2 id="写在最后">写在最后</h2>
<p>经过一连串的过程，我们已经成功构建了从决策树模型到XGBoost模型的六种不同类型的分类模型。随后使用评估指标评估了每个模型，并选择了最适合给定案例的模型。</p>
<p>在本文中，我们只选用了6个相对流行的模型，其实还有更多模型需要探索。此外，虽然我们很轻松地在 python 中可行地构建了模型，但是每个模型背后都有很多的数学和统计数据，在有精力的情况下，可以去了解下这么模型背后的数学推理。</p>
<h2 id="参考资料">参考资料</h2>
<p>[1] Kaggle 信用卡欺诈检测数据集: <a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">https://www.kaggle.com/mlg-ulb/creditcardfraud</a></p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>SHAP机器学习模型解释库</title>
      <link>https://textdata.cn/blog/shap_ml_explanation/</link>
      <pubDate>Thu, 14 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/shap_ml_explanation/</guid>
      <description>图文代码理解机器学习模型中各特征对结果的贡献</description>
      <content:encoded><![CDATA[<h2 id="代码下载">代码下载</h2>
<p><a href="SHAP.zip">点击此处下载代码</a></p>
<br>
<blockquote>
<p>原文链接 <a href="https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-24207127cad7">https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-24207127cad7</a></p>
</blockquote>
<h2 id="heading"></h2>
<p>想象一下，你正试图训练一个机器学习模型来预测广告是否被特定的人点击。在收到关于某人的一些信息后，模型预测某人会不会点击广告。</p>

<figure >
    
        <img src="img/%e5%9b%be1.png" />
    
    
</figure>

<p>但是为什么模型会输出这样的预测结果呢？ 每个特征对预测的贡献有多大？ 如果您能看到一个图表，显示每个特征对预测的贡献程度，如下所示，不是很好吗？</p>

<figure >
    
        <img src="img/%e6%9d%83%e9%87%8d.png" />
    
    
</figure>

<p>Shapley值就能起到特征权重测度的作用。</p>
<h2 id="shapley值是什么">Shapley值是什么？</h2>
<p>Shapley值是博弈论中使用的一种方法，它涉及公平地将收益和成本分配给在联盟中工作的行动者。
由于每个行动者对联盟的贡献是不同的，Shapley值保证每个行动者根据贡献的多少获得公平的份额。</p>

<figure >
    
        <img src="img/%e8%b4%a1%e7%8c%ae.png" />
    
    
</figure>

<h2 id="小案例">小案例</h2>
<p>Shapley值被广泛地应用于求解群体中每个工人(特征)的贡献问题。要理解Shapley值的作用，让我们想象一下贵公司刚刚做了A/B测试，他们在测试广告策略的不同组合。</p>
<p>每个策略在特定月份的收入是：</p>
<ul>
<li>无广告：150美元</li>
<li>社交媒体：300美元</li>
<li>谷歌广告：200美元</li>
<li>电子邮件营销：350美元</li>
<li>社交媒体和谷歌广告：320美元</li>
<li>社交媒体和电子邮件营销：400美元</li>
<li>谷歌广告和电子邮件营销：350美元</li>
<li>电子邮件营销，谷歌广告和社交媒体：450美元</li>
</ul>

<figure >
    
        <img src="img/%e7%ad%96%e7%95%a5%e8%90%a5%e6%94%b6.png" />
    
    
</figure>

<p>使用三则广告与不使用广告的收入相差300美元，每则广告对这一差异有多大的贡献?</p>

<figure >
    
        <img src="img/%e7%ad%96%e7%95%a5%e8%90%a5%e6%94%b6%e8%b4%a1%e7%8c%ae%e5%87%a0%e4%bd%95.png" />
    
    
</figure>

<p>我们可以通过计算每一类广告的Shapley值来计算谷歌广告对公司收入的总贡献入手，通过公式可以计算出Google广告的总贡献：</p>

<figure >
    
        <img src="img/%e5%85%ac%e5%bc%8f.png" />
    
    
</figure>

<p>让我们找到Google广告的边际贡献及其权重。</p>
<h2 id="寻找谷歌广告的边际贡献">寻找谷歌广告的边际贡献</h2>
<p>第一，我们将发现谷歌广告对以下群体的边际贡献：</p>
<ul>
<li>无广告</li>
<li>谷歌广告+社交媒体</li>
<li>谷歌广告+电子邮件营销</li>
<li>谷歌广告+电子邮件营销+社交媒体</li>
</ul>

<figure >
    
        <img src="img/%e8%be%b9%e9%99%85%e8%b4%a1%e7%8c%ae.png" />
    
    
</figure>

<p>Google广告 对 无广告 的边际贡献是：</p>

<figure >
    
        <img src="img/MC1.png" />
    
    
</figure>

<p>谷歌广告 对 谷歌广告&amp;社交媒体组合 的边际贡献是：</p>

<figure >
    
        <img src="img/MC2.png" />
    
    
</figure>

<p>谷歌广告 对 谷歌广告&amp;电子邮件营销组合 的边际贡献是：</p>

<figure >
    
        <img src="img/MC3.png" />
    
    
</figure>

<p>谷歌广告 对 谷歌广告、电子邮件营销和社交媒体组合 的边际贡献是：</p>

<figure >
    
        <img src="img/MC4.png" />
    
    
</figure>

<h2 id="发现权重">发现权重</h2>
<p>为了发现权重，我们将把不同广告策略的组合组织成如下多个层次，每个层次对应于每个组合中广告策略的数量。</p>
<p>然后根据每个层次的边数分配权重，我们看到了这一点：</p>
<ul>
<li>第一级包含3条边，因此每个边的权重为1/3</li>
<li>第二级包含6条边，因此每条边的权重将为1/6</li>
<li>第三级包含3条边，因此每条边的权重将为1/3</li>
</ul>

<figure >
    
        <img src="img/%e5%8f%91%e7%8e%b0%e6%9d%83%e9%87%8d.png" />
    
    
</figure>

<h2 id="发现google广告的总贡献">发现Google广告的总贡献</h2>
<p>根据前面的权重和边际贡献，我们已经可以找到Google广告的总贡献!</p>

<figure >
    
        <img src="img/google%e6%80%bb%e8%b4%a1%e7%8c%ae.png" />
    
    
</figure>


<figure >
    
        <img src="img/google%e5%85%ac%e5%bc%8f.png" />
    
    
</figure>

<p>酷!所以谷歌广告在使用3种广告策略与不使用广告的总收入差异中贡献了36.67美元。36.67是Google广告的Shapey值。</p>

<figure >
    
        <img src="img/otherRevenue.png" />
    
    
</figure>

<p>重复以上步骤，对于另外两种广告策略，我们可以看出：</p>
<ul>
<li>
<p>电子邮件营销贡献151.67美元</p>
</li>
<li>
<p>社交媒体贡献116.67美元</p>
</li>
<li>
<p>谷歌广告贡献36.67美元</p>

<figure >
    
        <img src="img/%e5%90%84%e7%ad%96%e7%95%a5%e8%b4%a1%e7%8c%ae.png" />
    
    
</figure>

</li>
</ul>
<p>他们共同出资300美元，用于使用3种不同类型的广告与不使用广告的区别!挺酷的，不是吗?
既然我们理解了Shapley值，那么让我们看看如何使用它来解释机器学习模型。</p>
<h2 id="shap-在python中解释机器学习模型">SHAP-在Python中解释机器学习模型</h2>
<p>SHAP是一个Python库，它使用Shapley值来解释任何机器学习模型的输出。</p>
<p>安装SHAP</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python">
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">shap</span>

</code></pre></div><h2 id="训练模型">训练模型</h2>
<p>为了理解SHAP工作原理，我们使用Kaggle平台内的advertising广告数据集。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;advertising.csv&#34;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>
<figure >
    
        <img src="img/df.png" />
    
    
</figure>

<p>我们将建立一个机器学习模型, 该模型根据用户个人特质信息来预测其是否点击广告。</p>
<p>我们使用Patsy将DataFrame转换为一组特征和一组目标值：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">patsy</span> <span class="kn">import</span> <span class="n">dmatrices</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">y</span><span class="p">,</span> <span class="n">X</span> <span class="o">=</span> <span class="n">dmatrices</span><span class="p">(</span>
    <span class="s2">&#34;clicked_on_ad ~ daily_time_spent_on_site + age + area_income + daily_internet_usage  + male -1&#34;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">X_frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">design_info</span><span class="o">.</span><span class="n">column_names</span><span class="p">)</span>


</code></pre></div><p>把数据分为测试集和训练接</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</code></pre></div><p>接下来使用XGBoost训练模型，并做预测</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">xgboost</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">xgboost</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div><p>为了查看模型表现，我们使用F1得分</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>

<span class="n">f1</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>
<span class="n">f1</span>
</code></pre></div><pre><code>0.9619047619047619
</code></pre>
<p>太好了!</p>
<h2 id="解释该模型">解释该模型</h2>
<p>该模型很好地预测了用户是否点击广告。但它是如何得出这样的预测的? <strong>每个特征对最终预测与平均预测的差异贡献了多少?</strong></p>
<p>注意，这个问题与我们在文章开头论述的问题非常相似。</p>
<p>因此，寻找每个特征的Shapley值可以帮助我们确定它们的贡献。得到特征i的重要性的步骤与之前类似，其中i是特征的索引：</p>
<ul>
<li>获取所有不包含特征i的子集</li>
<li>找出特征i对这些子集中每个子集的边际贡献</li>
<li>聚合所有边际贡献来计算特征i的贡献</li>
</ul>
<p>若要使用SHAP查找Shapley值，只需将训练好的模型插入shap.Explainer</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">shap</span>

<span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">Explainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X_frame</span><span class="p">)</span>
</code></pre></div><pre><code>ntree_limit is deprecated, use `iteration_range` or model slicing instead.
</code></pre>
<h2 id="shap瀑布图">SHAP瀑布图</h2>
<p>可视化第一个预测的解释：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#第一条记录是未点击</span>
<span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">waterfall</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<figure >
    
        <img src="img/output_20_0.png" />
    
    
</figure>

<p>啊哈!现在我们知道每个特征对第一次预测的贡献。对上图的解释：</p>

<figure >
    
        <img src="img/%e7%ac%ac%e4%b8%80%e6%ac%a1%e9%a2%84%e6%b5%8b%e8%b4%a1%e7%8c%ae.png" />
    
    
</figure>

<ul>
<li>蓝色条显示某一特定特征在多大程度上降低了预测的值。</li>
<li>红条显示了一个特定的特征在多大程度上增加了预测值。</li>
<li>负值意味着该人点击广告的概率小于0.5</li>
</ul>
<p>我们应该期望总贡献等于预测与均值预测的差值。我们来验证一下：</p>

<figure >
    
        <img src="img/%e6%80%bb%e8%b4%a1%e7%8c%ae%e7%ad%89%e4%ba%8e%e9%a2%84%e6%b5%8b%e4%b8%8e%e5%9d%87%e5%80%bc%e9%a2%84%e6%b5%8b%e7%9a%84%e5%b7%ae%e5%80%bc.png" />
    
    
</figure>

<p>酷!他们是平等的。</p>
<p>可视化第二个预测的解释：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#第二条记录也是未点击</span>
<span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">waterfall</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>
<figure >
    
        <img src="img/output_22_0.png" />
    
    
</figure>

<h2 id="shap摘要图">SHAP摘要图</h2>
<p>我们可以使用SHAP摘要图，而不是查看每个单独的实例，来可视化这些特性对多个实例的整体影响：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">shap</span><span class="o">.</span><span class="n">summary_plot</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_24_0.png" />
    
    
</figure>

<p>SHAP摘要图告诉我们数据集上最重要的特征及其影响范围。</p>
<p>从上面的情节中，我们可以对模型的预测获得一些有趣的见解：</p>
<ul>
<li>用户的 <strong>daily_internet_usage</strong> 对该用户是否点击广告的影响最大。</li>
<li>随着<strong>daily_time_spent_on_site</strong>的增加，用户点击广告的可能性降低。</li>
<li>随着<strong>area_income</strong>的增加，用户点击广告的可能性降低。</li>
<li>随着<strong>age</strong>的增长，用户更容易点击广告。</li>
<li>如果用户是<strong>male</strong>，则该用户点击广告的可能性较小。</li>
</ul>
<h2 id="shap条形图">SHAP条形图</h2>
<p>我们还可以使用SHAP条形图得到全局特征重要性图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">)</span>
</code></pre></div>
<figure >
    
        <img src="img/output_26_0.png" />
    
    
</figure>

<p>很酷!</p>
<h2 id="结论">结论</h2>
<p>恭喜你!您刚刚了解了Shapey值以及如何使用它来解释一个机器学习模型。希望本文将提供您使用Python来解释自己的机器学习模型的基本知识。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
