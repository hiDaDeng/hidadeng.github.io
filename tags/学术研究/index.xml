<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>学术研究 on 大邓和他的PYTHON</title>
    <link>/tags/%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6/</link>
    <description>Recent content in 学术研究 on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sat, 08 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>叙事经济学：揭示经济中的叙事</title>
      <link>https://textdata.cn/blog/2023-04-09-narrative-economic-method/</link>
      <pubDate>Sat, 08 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-04-09-narrative-economic-method/</guid>
      <description>在叙事经济学中，**叙事是指人们在经验、文化和信仰的基础上，通过描述和解释社会事件及其背后的意义，构建一个意义上的世界**。叙事作为一个社会现象，是人们对现实的理解、描述和解释，不仅仅包括人类生活中的日常对话，也包括文学、历史、政治、经济等领域的叙事。人们通过叙事来传递知识、价值观念、历史、文化等方面的信息，同时也对叙事本身进行评价和改变。</description>
      <content:encoded><![CDATA[<p>昨天分享了[管理世界 | 政府与市场心理因素的经济影响及其测度]((<a href="https://textdata.cn/blog/2023-04-08-measurement_of_psychological_factors_and_their_economic_impact">https://textdata.cn/blog/2023-04-08-measurement_of_psychological_factors_and_their_economic_impact</a>)， 感觉使用非结构数据，尤其是文本数据开展经济研究，可以阅读《叙事经济学》。叙事经济学(Narrative Economics)是2019年诺贝尔经济学奖得主罗伯特·希勒(Robert Shiller)的新作，这本书主要探讨了经济学中的叙事现象及其对经济发展的影响。</p>
<p><img loading="lazy" src="img/fields.png" alt=""  />
</p>
<p>读这本书的过程中，大邓脑子里蹦出了**「历史不会重演，但总是惊人的相似」**。 我们普通人透过历史的迷雾，看到的相似性其实是一种故事性的相似。学习历史， 可以学到智慧，所以使用叙事学研究经济问题，也是一种很智慧的合理的方法。</p>
<p>本文将从叙事的定义入手，介绍叙事经济学的研究内容及其价值，进而探讨如何开展叙事研究，最后结合Python文本分析，阐述叙事经济学与文本分析的关系。</p>
<br>
<h2 id="一什么是叙事">一、什么是叙事？</h2>
<p>在叙事经济学中，<strong>叙事是指人们在经验、文化和信仰的基础上，通过描述和解释社会事件及其背后的意义，构建一个意义上的世界</strong>。叙事作为一个社会现象，是人们对现实的理解、描述和解释，不仅仅包括人类生活中的日常对话，也包括文学、历史、政治、经济等领域的叙事。人们通过叙事来传递知识、价值观念、历史、文化等方面的信息，同时也对叙事本身进行评价和改变。</p>
<br>
<h2 id="二叙事经济学的研究内容及其价值">二、叙事经济学的研究内容及其价值</h2>
<p>叙事经济学主要关注经济领域中的叙事现象及其对经济发展的影响。书中列举了许多历史事件，如股市崩盘、经济危机、金融泡沫等，分析了这些事件发生时的社会叙事，以及叙事如何影响人们的行为和经济发展。叙事经济学的研究内容主要包括以下几个方面：</p>
<h3 id="1-叙事的构建与演变">1. 叙事的构建与演变</h3>
<p>叙事是社会历史的产物，它们不是独立存在的，而是构成一个相互联系、相互作用的叙事网络。叙事的演变往往具有惯性和路径依赖性，即过去的叙事会影响人们对现实的认知和解释，进而影响未来的叙事构建和演变。</p>
<h3 id="2-叙事的传播与影响">2. 叙事的传播与影响</h3>
<p>叙事的传播和影响具有复杂性和非线性性，受到多种因素的影响，包括媒体、政治、经济等方面。叙事可以影响人们的行为和决策，进而对经济产生重要的影响。</p>
<h3 id="3-叙事的评价与变革">3. 叙事的评价与变革</h3>
<p>叙事的评价和变革是人们对叙事的反思和改进，是推动叙事演变和发展的重要力量。叙事的评价和变革可以通过媒体、政治、经济等多种渠道进行，也可以通过个体的反思和探索实现。</p>
<p>叙事经济学的研究价值在于揭示了经济领域中的叙事现象，对经济发展的影响进行了深入的分析和探讨。这种研究方法超越了传统经济学中对冷静理性的假设，更加注重人的主观认知和情感因素对经济行为的影响。通过叙事经济学的研究，我们可以更好地理解人们的行为和决策，为经济政策的制定和实施提供更加全面和深入的思考。</p>
<br>
<h2 id="四与叙事相关的学科">四、与叙事相关的学科</h2>
<p>叙事作为一种人类基本的思维方式和交流形式，涉及到多个学科的研究和应用。以下是一些与叙事相关的学科：</p>
<ul>
<li>叙事学：研究叙事的语言、结构和功能，探讨叙事如何构建和传递意义。</li>
<li>心理学：研究叙事对个体认知、情感和行为的影响，探讨叙事与个体心理机制的关系。</li>
<li>文学学：研究文学作品中的叙事结构和技巧，探讨叙事在文学中的艺术表达和意义。</li>
<li>社会学：研究叙事在社会和文化中的作用和意义，探讨叙事与社会结构和变革的关系。</li>
<li>历史学：研究历史事件和过程中的叙事构建和演变，探讨叙事对历史认知和评价的影响。</li>
<li>经济学：研究经济领域中的叙事现象和影响，探讨叙事对经济决策和行为的影响。</li>
<li>认知科学：研究叙事构建和理解的认知机制，探讨叙事与个体认知和思维的关系。</li>
</ul>
<p>这些学科之间有着密切的联系和互动，可以结合起来进行研究。例如，结合心理学和叙事学的研究，可以探讨叙事对个体情感和行为的影响机制；结合经济学和叙事学的研究，可以探讨叙事对经济决策和行为的影响因素和机制。同时，也可以运用机器学习、自然语言处理、网络分析等技术，对大量文本数据进行处理和分析，揭示叙事中的关键词、主题和情感等信息，更深入地探讨叙事的影响和作用。</p>
<br>
<h2 id="三python文本分析与叙事的关系">三、Python文本分析与叙事的关系</h2>
<p>Python文本分析是一种常用的文本数据处理和分析工具，可以对大量文本数据进行快速的处理和分析。在叙事经济学的研究中，Python文本分析可以应用于以下方面：</p>
<ol>
<li>
<p>词频统计</p>
</li>
<li>
<p>叙事内容分析
通过Python文本分析工具，可以对大量文本数据进行词频统计（趋势分析）、主题分析、情感分析等处理，揭示叙事中的关键词、主题和情感等信息。这些信息可以帮助研究者深入了解叙事内容和叙事演变过程，进而探讨叙事对经济发展的影响。下图是文中的词频统计的趋势图，从中可以看到<strong>大萧条</strong>这个词的使用趋势。
<img loading="lazy" src="img/trends1.png" alt=""  />
</p>
</li>
<li>
<p>叙事传播分析
通过Python文本分析工具，可以对社交媒体平台、新闻报道等文本数据进行采集和处理，了解叙事在网络中的传播路径和影响程度，进而探讨叙事对人们行为和决策的影响。这些分析结果可以帮助研究者更好地理解叙事的传播机制和影响因素，为叙事经济学的研究提供数据支持。</p>
</li>
<li>
<p>叙事评价与变革分析
通过Python文本分析工具，可以对叙事内容的变化和评价进行跟踪和分析，了解叙事的评价和变革过程，进而探讨叙事对经济发展的影响。这些分析结果可以帮助研究者更好地理解叙事的演变和发展趋势，为叙事经济学的研究提供理论支持。</p>
</li>
</ol>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>管理世界 | 政府与市场心理因素的经济影响及其测度</title>
      <link>https://textdata.cn/blog/2023-04-08-measurement_of_psychological_factors_and_their_economic_impact/</link>
      <pubDate>Fri, 07 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-04-08-measurement_of_psychological_factors_and_their_economic_impact/</guid>
      <description>政府与市场关系是经济学的世界性难题，也是中国特色社会主义市场经济的核心问题。在政府政策制定与实施过程中，经济主体会基于自己掌握的信息和认知能力，学习、解读政策含义，形成对政策影响的预期，并基于自身利益最大化原则做出最优决策，从而影响宏观经济运行。因此，研究政府政策背景下各类经济主体的预期等心理因素的产生原因与形成过程，以及经济主体心理因素对经济运行与政策效应的影响机制，是深刻认识政府与市场关系的一个重要视角。本文提出利用人工智能特别是机器学习方法，从海量非结构化大数据提取政府政策变化与各类经济主体对政策变化的反应等信息，从理论和实证两个层面分析经济政策与经济主体的互动关系，以及经济主体心理因素如何影响经济运行与政策效应，并探讨发展非结构化大数据计量经济学，推动形成分析经济政策背景下经济主体心理因素及其影响的实证研究范式，以深入研究政府与市场关系。</description>
      <content:encoded><![CDATA[<h2 id="本文益处">本文益处</h2>
<p>阅读此文可以对以下几个方面有新的收获</p>
<ul>
<li>经济主体预期理论</li>
<li>测度经济主体心理因素方法论基础</li>
<li>测度经济政策不确定性、央行沟通与预期管理、通胀预期、预期冲击、经济主体信心与情绪等方面的应用</li>
<li>非结构化大数据测度心理因素及其影响所面临的主要困难与问题</li>
<li>非结构化大数据实证研究主要方法论的发展趋势和研究重点</li>
<li>&hellip;&hellip;</li>
</ul>
<p>洪永淼,刘俸奇,薛涧坡. <strong>政府与市场心理因素的经济影响及其测度</strong>[J].管理世界,2023,39(03):30-51.</p>
<p>摘要: 政府与市场关系是经济学的世界性难题，也是中国特色社会主义市场经济的核心问题。在政府政策制定与实施过程中，<strong>经济主体会基于自己掌握的信息和认知能力，学习、解读政策含义</strong>，形成对政策影响的预期，并基于自身利益最大化原则做出最优决策，从而影响宏观经济运行。<strong>因此，研究政府政策背景下各类经济主体的预期等心理因素的产生原因与形成过程，以及经济主体心理因素对经济运行与政策效应的影响机制，是深刻认识政府与市场关系的一个重要视角。本文提出利用人工智能特别是机器学习方法，从海量非结构化大数据提取政府政策变化与各类经济主体对政策变化的反应等信息，从理论和实证两个层面分析经济政策与经济主体的互动关系，以及经济主体心理因素如何影响经济运行与政策效应，并探讨发展非结构化大数据计量经济学，推动形成分析经济政策背景下经济主体心理因素及其影响的实证研究范式，以深入研究政府与市场关系</strong>。</p>
<p>关键词：政府与市场关系 心理因素 文本数据 非结构化大数据 政策传导机制</p>
<br>
<h2 id="一引言">一、引言</h2>
<p>本文提出一个基于政府政策背景下 济主体心理因素的产生、形成与变化来研究政府与市场关系的研究思路。 <strong>经济政策的落脚点是经济主体，通过影响经济主体决策发挥作用。 而经济主体具有主观意识与前瞻性思维，在政府制定、实施政策过程中，经济 主体会基于自己掌握的信息和认知能力，学习、解读政策含义，形成对政策影响的预期，并基于自身利益最大化原则，做出最优决策</strong> 。 在政府政策的传导机制中，经济主体的预期以及由预期衍生的情绪等心理因素发挥 着重要作用。 在社会经济活动中，心理因素比比皆是，包括福利经济学的幸福感、市场营销学的客户满意度、 金融学的投资者情绪、微观经济学的企业家精神与风险偏好、宏观经济学的政策不确定性、社会学的社会信任 与社会声誉、新闻学的社会舆情与媒体关注度、教育学的学生学习压力、政治学的群体政治倾向，以及人文学的文化因素（文化是长期实践中形成的比较稳定的社会或群体心理，如企业文化）等。 在金融市场中，流动性 陷阱、羊群效应、资产与房地产泡沫、金融传染病等现象，都与投资者情绪特别是与偏离经济基本面的乐观或 悲观情绪相关（席勒，2001）。 <strong>马克思主义认为，社会存在决定社会意识，同时社会意识对社会存在具有反作用。 经济主体心理因素是对客观经济现实的主观反映，由客观因素决定。 另一方面，经济主体心理因素会影 响经济主体决策，从而影响政府政策效应与宏观经济运行</strong>。</p>
<p>长期以来，由于测度心理因素所面临的困难与挑战，研究者大多只能采用定性分析方法研究 经济主体心理因素及其影响。 随着大数据革命和数字经济时代的来临，各级政府与各类经济主体在数字经济活动中产生海量经济大数据。 大数据特别是非结构化大数据，如文本、图像、音频、视频等数据，包含传统结构化数据没有的信息，包括政府政策变化（如语调变化）与各类经济主体对政策变化的反应等。 使用人工智能特别是机器学习技术，可从非结构化大数据中提取各类经济主体心理信息，构建心理变量，将原来的定性分析转变为符合现代经济学实证研究范式的定量分析。 由于非结构化大数据包含政府政策变化、重要事件冲击以及 各类经济主体对政策变化与重要事件冲击的反应等信息，经济学家可基于非结构化大数据从经济主体心理因 素视角研究政府与市场关系。 <strong>近年来，随着自然语言处理技术的迅速发展，利用文本数据测度经济主体心理因素及其影响，已成为经济学、金融学、会计学、社会学以及政治学等学科的一个热门领域</strong>（格里默、斯图尔特， 2013；埃文斯、阿希维斯，2016；洛克伦、麦克唐纳，2016；根茨科等，2019）。 <strong>本文的主要目的是提出基于非结构化大数据提取政府政策变化与各类经济主体对政策变化反应等信息的研究思路，分析政府政策变化如何影响各类经济主体的预期等心理因素，从而影响经济运行与政策效应，探讨如何发展非结构化大数据计量经济学， 推动形成分析政府政策背景下经济主体心理因素及其影响的实证研究范式，以深入研究政府与市场关系</strong>。</p>
<br>
<h2 id="二预期情绪与经济政策理论视角">二、预期、情绪与经济政策：理论视角</h2>
<p>预期在经济活动中发挥着重要作用，预期管理是宏观经济管理的一个重要手段。</p>
<h3 id="21-预期与预期形成机制">2.1 预期与预期形成机制</h3>
<p>经济学与自然科学的一个根本区别在于，经济主体决策具有<strong>前瞻性</strong>（埃文斯、洪卡波希亚，2001）。 宏观经济学中，预期发挥着关键性作用。 虽然经济学家对预期重要性有一定共识，但对预期形成机制却有不同理解。 目前，宏观经济学有 3 种主要假设解释预期的形成机制，分别是适应性预期、理性预期以及适应性学习。</p>
<p><strong>适应性预期</strong>可追溯至计量经济学先驱者之一费雪（费雪，1911），在 20 世纪 60~70 年代流行于宏观经济学， <strong>其核心思想是，经济主体利用一部分历史信息预测经济未来趋势，属于后顾行为。 在适应性预期假设下，未来的政策变化不一定能够影响经济主体当前的行为决策，因此宏观经济学模型的预测能力也极为有限</strong>。</p>
<p><strong>理性预期概念</strong>由穆斯（1961）正式提出，假设经济主体能够充分利用所获得的全部信息对未来进行预期， 其主观期望与客观的数学期望相一致，不存在系统性预期偏差，且具有前瞻性。 1995 年诺贝尔经济学奖获得者卢卡斯（1976）指出，<strong>不确定性市场条件下经济主体的理性预期对政策效应有重要影响</strong>。 例如，理性预期的 经济主体能够预见政府货币政策对未来经济的影响，形成对货币政策的正确预期，并且根据预期提前做出反应，最终会导致货币政策失效，即货币是中性的。 这是著名的“卢卡斯批判”。 此后，以理性预期为基础的动态随机一般均衡模型成为现代宏观经济学政策分析的一种主流方法。</p>
<p><strong>虽然理性预期理论广泛应用于政策分析，但其没有考虑经济主体形成预期的动态过程，也没有考虑经济主体在收集、处理信息以及认知能力等方面的局限性</strong>。 首先，<strong>理性预期假设经济主体不存在系统性认知偏差</strong>， 这种理想状态需要长期反复试错才能实现。 理性预期可被理解为经济运行过程中的一种长期均衡状态。 事实上，实证研究表明，货币政策并不是中性的（克里斯蒂诺等，1999）。 其次，理性预期理论存在某些缺陷（阿克 洛夫等，2000；席勒，2014），例如<strong>理性预期假设较为严苛，要求经济主体具备同质性，信念一致且具有充分信息，完全了解模型的结构、外生参数以及政策实施前后的经济特征</strong>。 但是，神经经济学研究揭示了经济主体认知水平的有限性（卡默勒等，2005），且经济主体异质性是一个普遍现象（李戎等，2022）。 例如，中国目前有超 过 1.6 亿个市场主体，其中，国有企业、民营企业、个体工商业者、港澳台企业、外资企业存在巨大异质性。</p>
<p>基于上述原因，宏观经济学在理性预期基础上提出一个新的预期理论，即<strong>适应性学习</strong>（萨金特，1993， 1999；埃文斯、洪卡波希亚，2001），假设经济主体能够感知长期均衡状态下的经济运行规则，利用其所有可得信息采用最小二乘法估计上述规则，并随数据集的更新不断改进预期效果。 由于经济主体在适应性学习过程 中利用了其所有信息，经济政策能够通过不断改变市场预期而影响经济运行（埃文斯等，2009）。</p>
<p><strong>适应性学习可视为有限理性的一种表现形式</strong>。 1978 年诺贝尔经济学奖获得者西蒙（1982）认为经济主体获取信息存在成本，且经济主体处理信息的能力有限，不能充分利用其所获得的信息。 在有限理性基础上， 2011 年诺贝尔经济学奖获得者西姆斯（2003）提出<strong>理性疏忽概念</strong>，认为经济主体在获得、吸收与处理信息时需要一定成本，只能从完全信息中选取部分形成预期。 由于获得的信息有限，经济主体不会对政策信号给予足够关注，因此政府通过市场预期影响经济运行的效果受到限制。 除了理性疏忽外，涉及有限理性的理论还有不完全信息理论与粘性预期理论等（安吉雷托斯、连，2018）。 随着计算技术的发展，有限理性由于更贴近现实而在财政与货币政策领域得到广泛应用，成为当前宏观经济学的一个研究热点。</p>
<br>
<h3 id="22-预期与经济波动">2.2 预期与经济波动</h3>
<p>政府释放的政策信号会影响经济主体预期，产生乐观或悲观情绪，继而影响其决策（凯恩斯，1936）。 经典文献认为经济主体预期可分为两部分，一部分是与宏观经济基本面相关的<strong>理性估算</strong>，另一部分是与宏观经济基本面无关的“动物精神”或<strong>情绪因素</strong>（巴斯基、西姆斯，2012）。 本节描述经济政策消息冲击、经济政策不确定性冲击与经济政策情绪冲击影响经济主体心理的形成机制以及经济波动的传导渠道。 其中，经济政策消息冲击和经济政策不确定性冲击与经济基本面相关，而经济政策情绪冲击与经济基本面无关，属于非理性因素。</p>
<br>
<ol>
<li><strong>经济政策消息冲击</strong></li>
</ol>
<p>“<strong>消息冲击</strong>”（也称“<strong>预期冲击</strong>”）的概念最早由庇古（1927）提出。 庇古指出，当公众获得未来经济景气的消息并形成乐观预期时，为应对总需求增加，公众会提前进行资本积累，从而造成当前经济繁荣，这被称为“<strong>庇古周期</strong>”。 与传统实际冲击的区别在于，消息冲击不会影响当前经济基本面，但有可能改变未来经济基本面。</p>
<p>经济政策消息冲击产生的客观原因是经济政策存在内部时滞与外部时滞（利珀等，2013）。 内部时滞是指经济政策变动从提出草案到确定实施之间存在时滞。</p>
<p>理性预期是经济政策消息冲击形成的理论基础。 经济主体信息集包含了其在当前可获得的全部信息。 在经济政策实施之前，经济主体就捕获到了未来经济政策变动的消息，对其当前行为做出相应调整，从而引起 经济波动。 实证研究表明，预期冲击是产生经济波动的重要原因，甚至比经济政策实际冲击更重要（施密特· 格罗赫、乌里韦，2012）。 利好的经济政策消息冲击会引起当前经济扩张，这为政府利用预期调控经济提供理论基础与经验支撑。</p>
<br>
<ol start="2">
<li><strong>经济政策不确定性冲击</strong></li>
</ol>
<p><strong>经济政策不确定性</strong>是指经济主体无法预见未来政策发生变化的可能性（奈特，1921），这是经济主体形成预期的重要因素。 经济政策不确定性包括经济政策能否出台、何时出台、执行力度以及执行效果等（居伦、伊昂，2016）。 贝克等（2016）构建的经济政策不确定性指数将经济政策不确定性细化为政策制定者、政策制定时 间与政策内容变化的不确定性。</p>
<p>经济政策的模糊性或噪音可用来刻画经济政策不确定性。 假设经济主体获得的经济政策变化的信息集包含政策变化消息与政策噪音两部分，其中政策噪音的方差测度经济政策不确定性的大小。 经济主体感知到的经济政策不确定性程度随噪音方差的增大而提高。 当噪音方差趋于 0，即不存在经济政策不确定性时，经济主体将完全依照获得的经济政策变化信息做出决策；当噪音方差趋于无穷大时，经济主体认为接收到的经济 政策变化信息都是噪音，将不会做出任何反应。 因此，经济政策不确定性直接作用于经济主体信息集，影响其预期形成，从而影响经济主体决策（费沃、皮特伦蒂，2016）。 为了抑制经济政策不确定性的消极影响，里科等 （2016）认为政府与经济主体之间应该加强政策沟通协调。</p>
<br>
<ol start="3">
<li><strong>经济政策情绪冲击</strong></li>
</ol>
<p>经济政策情绪冲击是指与未来政策基本面无关的乐观或悲观情绪冲击。 这一思想最早源于凯恩斯的“动物精神”，认为经济主体行为与经济基本面无关，乐观与悲观情绪是驱动经济波动的主要力量。 经济主体许多情绪是从其预期衍生出来的。 例如，由于宣布某个重大利好政策，经济主体改善了对未来经济增长的预期，由此产生一种乐观情绪。 反之，如果出现一个重大负面冲击，经济主体将下调对未来经济增长的预期，由此产生一种悲观情绪。 一个人的情绪也可能受其他因素影响，如受其他经济主体的情绪传染（席勒，2001，2019）。</p>
<p><strong>“动物精神”是非理性心理因素，但现代宏观经济学尝试从理性预期视角讨论经济政策情绪冲击，并使用“预期自我实现”概念刻画情绪冲击对政策效应的影响</strong>。 例如，施密特·格罗赫和乌里韦（1997）分析了平衡预算法则下征收个人所得税产生“预期自我实现”均衡的可能性。 当政府财政支出总量不变时，经济主体减税预 期会形成乐观情绪，提高劳动供给，产生扩张效应。 由于经济总量增加而政府支出规模不变，政府减税成为可 能，从而实现了经济主体最初的减税预期。 情绪冲击还应用于货币政策研究（黄等，2009）。 但鲜有理论刻画政府政策提升经济主体信心的作用机制，吉马良斯等（2016）通过理论模型说明政府财政支出增加会引发经济 主体对社会投资的乐观情绪而提振信心，实现经济扩张，为财政政策提振经济主体信心奠定微观理论基础。</p>
<br>
<h3 id="23-情感因素">2.3 情感因素</h3>
<p>经济主体情感因素也会影响其决策。 情感因素包括人天生拥有的攀比、妒忌与信任等本能心理因素。 <strong>新古典经济学认为经济主体是完全理性，忽视了由人性产生的复杂情感体验（卡默勒等，2005）。 神经经济学、认知经济学与基因经济学证实经济主体情感因素的存在</strong>。 例如，多曼等（2011）利用核磁共振神经影像技术，比 较经济主体血氧水平变化来研究经济主体是否存在攀比效应。 布鲁尼洛等（2020）发现，经济主体的基因会影 响其性格等心理因素，从而影响经济主体行为。</p>
<p><strong>经济政策如何通过经济主体情感因素影响宏观经济是当前一个研究热点</strong>。 在现代经济中，服务业特别是消费性服务业是经济主体情感产生、交流与传递的过程，情感因素对服务业的发展有重要影响。 <strong>传统政策分析建立在理性预期基础上，但随着行为经济学与实验经济学提供日益丰富的实证证据，越来越多的学者关注经济主体情感因素对经济政策传导机制的影响。 行为经济学通过洞察经济主体心理活动分析经济主体的行为决策，为经济学理论创新提供心理学证据</strong>。 经济政策与经济主体心理因素相互结合，催生了行为财政学与行为货币经济学，奠定了宏观经济政策分析的微观行为基础，为研究政府与市场关系提供了全新分析框架。</p>
<p><strong>行为财政学</strong>最早由麦卡弗里和斯莱姆罗德（2006）提出，将传统经济学基本原理的“理性”部分与行为经济学的“非理性”部分（或心理因素）结合起来，研究财政政策传导机制。 财政支出可通过经济主体同群效应这一 外部性因素影响宏观经济运行。 <strong>同群效应</strong>是指经济主体效用函数不仅取决于自身条件，还受其所处社会地位 （用消费、收入或工作时间等衡量）的影响。 在消费层面，政府支出可通过经济主体之间的攀比效应对产出产生扩张效应（黄等，2022b）。 具体机制是，政府支出增加能够提高社会平均财富，具有攀比心理的人会增加工作时间来追赶其他人的消费水平，从而推动产出扩张。 同群效应的另一表现形式是经济主体工作时间的“内 卷”，费沃等（2013）发现政府支出增加能够提高社会平均工作时间，由于存在“内卷”心理，个体观测到别人增 加工作量后也将努力工作，最终促进产出扩张。</p>
<p>在财政收入端，现有文献认为存在“<strong>税收遵从之谜</strong>”，即传统理性经济人假设认为经济主体依法纳税程度会随着税务机关逃税处罚力度加大而增强，但实证研究发现两者存在负相关。 这涉及经济主体心理因素的作用机制：当处罚力度过大时，经济主体会感受到不公平或出现逆反心理，从而产生税收不遵从行为（温策尔， 2003）。 同样，政府可利用经济主体心理因素增强其税收遵从能力，丹尼夫等（2021）发现，税务部门采用威慑手段可增强征税能力。 此外，财政幻觉理论认为，由于经济主体存在认知偏差，经济主体感受到的纳税负担会 小于实际纳税负担，从而降低其逃税动机，减少规避政府税收的影子经济规模（布恩等，2012）。</p>
<p><strong>行为货币经济学</strong>由罗瑟利（2015）提出，将心理要素纳入传统货币政策分析框架中，探讨货币政策通过心理因素发挥作用的传导机制。 例如，信任对货币政策效应具有重要影响。 布尔西安和法亚（2018）发现，经济主体对央行信任下降会恶化货币政策实施效果。 史蒂文森和沃尔弗斯（2011）和李新荣等（2014）发现，经济主 体对中央银行的信任程度会影响通胀预期。 当经济繁荣时，经济主体对央行有较强的信任度，从而产生较低的通胀预期。</p>
<p>经济主体心理因素也可解释现代货币政策理论中的悖论。 德尔内格罗等（2015）发现新凯恩理论存在 “<strong>前瞻性指引之谜</strong>”，即央行发布未来名义利率变化会引起产出与通胀波动，基于理论模型估算的波动率大于实际数据的波动率。 为解决上述难题，艾劳多（2020）将经济主体面临的诱惑和自控力等心理因素加入效用函 数，发现修正后的新凯恩斯理论结果与经验证据一致，较好解释了传统理论模型存在的“前瞻性指引之谜”。</p>
<p><br><br></p>
<h2 id="三经济学心理因素测度非结构化大数据视角">三、经济学心理因素测度：非结构化大数据视角</h2>
<p>经济学理论分析，特别是经济学建模，是理解经济主体心理因素及其影响的理论基础。 <strong>第二节的各种预期理论为人们理解政策变化如何影响经济主体预期，以及经济主体预期如何影响政策效应提供了学理基础。 更一般地，经济主体的心理活动，如预期、信心、情绪、情感等的产生、形成与变化，具有规律性，可通过经济学建模，包括使用数学模型，结合经济学与心理学理论进行研究</strong>。 例如，人工神经网络模型就是根据认知科学关 于人脑认知理论而提出的一个模仿人脑认知过程的数学模型或非参数模型，可用于近似任何未知平滑函数， 具有所谓泛逼近性质（怀特，1992），已成为机器学习的一个重要方法。 反过来，可借助数学模型研究经济主体的心理活动规律。 例如，人工智能特别是机器学习可用于研究经济主体预期的产生、形成与演变过程。 相对于宏观经济学中基于最小二乘法估计的适应性学习机制，基于机器学习的适应性学习不必假设经济主体能够感知均衡状态下的经济运行规则，机器学习能够充分利用经济主体所获得的信息，捕捉预期形成与变化过程中各种线性与非线性动态特征，最终趋近未知的经济运行规则。</p>
<p><strong>任何经济理论都建立在一定的假设基础之上，这些假设是否与经济现实相吻合、经济理论能否解释经济现象，都是需要回答的重要问题。 解决这些问题的关键是通过基于数据的实证研究，检验从经济理论推导出的可实证的重要预测或结论是否正确</strong>。 从实证研究视角分析政策变化如何影响经济主体预期等心理因素从而传导到经济运行的一个关键问题是，如何测度反映政府与市场互动关系的经济主体心理因素。 长期以来， 心理因素测度一直面临诸多挑战与困难，甚至有人质疑心理因素是否可以测度。 例如，经济统计学鼻祖威廉· 配第认为，经济主体心理因素是不可测度的，他在 1672 年出版的代表作《政治算术》中说：“ 至于那些从某些人 的容易变动的思想、意见、胃口和情绪为依据的原因，则留待别人去研究。 这里我敢明白地、老实说，以这些因 素（容易变动的思想等等）为依据（即使这些因素可以叫做依据）的原因是不能谈得透彻的。”（配第，2014）。</p>
<p>事实上，现代心理学与实验经济学对经济主体心理因素能否测度的问题，已提供了肯定的答案。 <strong>现代心理学有一门方法论学科叫心理计量学，该学科依据心理学理论，使用科学方法与工具测度人的能力、人格以及心理健康等心理特征与行为。 常用方法包括观察法、访谈法、问卷法、实验法、心理物理法等</strong>。 心理测度著名的例子包括对智商和情商的测度。 但是，心理学与实验经济学对心理因素及其影响的测度，大多通过实验室、 观察访谈或统计调查等方式，其研究的深度、广度与抽样频率不可避免受到各种限制与约束。 例如，经济主体幸福感是福利经济学一个非常重要的心理变量。 人们常采用问卷调查形式评估调查对象对于家庭、健康、安 全、价值观、自由、幸福以及生活满意度等多方面的“<strong>表述性偏好</strong>”，以此构建指数测度幸福感（班杰明等， 2014）。 美国家庭调查数据库通过问卷调查测度经济主体幸福感。 受访者以七分制回答问题：“你认为这些天的情况如何？”，其中数值“1”定义为“非常不开心”，数值“7”为“非常开心”，中间取值没有明确定义。 中国家庭追踪调查也包含有关个人幸福感的问卷问题。 <strong>但这种自我报告的幸福感指数无法有效进行跨群体比较（邦 德、兰格，2014），并且由于幸福感测度结果还依赖于回答者（包括其所处环境）以及问题的措辞，因此自我报告的幸福感指数的真实性存在一定质疑</strong>（迪顿、斯通，2013）。</p>
<p>随着互联网、移动互联网、物联网以及数字经济的蓬勃发展，各级政府与各类经济主体在数字经济活动中产生海量大数据，这些经济大数据包含政府政策变化、重要事件冲击以及各类经济主体对政策变化与重要事 件冲击的反应等信息。 人工智能技术的发展和大数据可获得性的提升，为实证研究政府与市场关系提供了一 个可行方法。 大数据，特别是文本、图像、音频与视频等非结构化大数据，包含很多传统结构化数据没有的信息，如政府政策的语调变化与经济主体的预期、信心、情绪、情感等，特别是，<strong>语言是人类进行信息传递、情感交流与沟通的最主要工具，因此文本数据包含大量各类经济主体的心理信息，可通过自然语言处理技术提取</strong>。 文本数据以及其他非结构化大数据的来源主要是社交数字平台、电商平台、上市公司财务报告、新闻报纸与其他新闻媒介、各级政府工作报告、会议纪要、公告、声明、谈话，以及政府领导人公开发表的文章等。 文本数据可分为正式文本数据与非正式文本数据。 正式文本数据主要包括政府工作报告、政策文件、官方新闻媒体报道与上市公司披露文件等官方文本；非正式文本数据包括社交网络文本（如微博、微信公众号等）与网络论坛 文本（如股吧）等个人文本。 在形式上，正式文本具有内容规范、表述相对客观以及信息密度高的特点；非正式文本信息密度较低，但包含大量口语与俚语等非正式语言，提供额外信息。 <strong>从文本数据中构建心理变量后，可代入计量经济学模型中，定量研究其经济影响。 这样，原来只能进行定性分析的问题，可转化为较严谨的定量实证研究。 这种研究范式通常称为文本回归</strong>。</p>
<p>与观察访谈、统计调查以及实验方法相比，基于海量非结构化大数据的心理因素测度具有更广泛的样本代表性，可在较大程度上减少样本选择偏差。 非结构化大数据具有高频性或实时性，可构建心理变量时间序列数据，甚至高频时间序列数据，用来刻画经济主体心理变量的动态特征。 海量微观行为大数据还包含经济主体之间互动关系信息，有助于研究情绪等心理因素如何在经济主体中相互传染。 但是，大数据不是统计调查数据，而是数字经济活动的数字化记录或观测数据。 大数据特别是非结构化数据种类繁多，结构复杂，来源不一。 因此，从这些非结构化大数据中提取经济主体心理信息并对其进行实证建模分析，在方法与技术上具有相当挑战性。 在第四节和第五节中，我们先介绍目前经济学、金融学、会计学等学科使用文本数据测度经济主体心理因素及其影响的实证研究成果，然后讨论如何构建基于非结构化大数据分析政府政策变化与经济主体心理因素互动关系的实证研究范式。</p>
<p><br><br></p>
<h2 id="四文本数据与经济主体心理因素测度实证视角">四、文本数据与经济主体心理因素测度：实证视角</h2>
<p><strong>文本数据分析是当前经济学、金融学、会计学等学科的一个主流实证研究范式，其中自然语言处理是最主要的文本数据信息提取技术，常用方法包括词典法、主题模型、词向量法等</strong>。 具体步骤为：</p>
<ol>
<li>使用分词技术对文本数据进行分词处理，生成文本矩阵，通常采用独热表示法将上述矩阵转化为高维稀疏数据矩 阵，然后再使用词嵌入技术进行降维处理，最终将非结构化数据转化为数据矩阵。</li>
<li>采用统计方法将 数据矩阵转化为目标信息，根据是否存在训练集，可分为词典法或主题分类模型等无监督学习方法，以及包含深度神经网络与卷积神经网络等新兴深度学习技术的有监督学习方法（沈艳等，2019）。</li>
</ol>
<p>在这一节，我们讨论如何基于文本数据测度 <strong>经济政策不确定性、央行沟通与预期管理、通胀预期、预期冲击、以及经济主体信心与情绪</strong>。</p>
<br>
<h3 id="41-经济政策不确定性的测度">4.1 经济政策不确定性的测度</h3>
<p><strong>经济政策不确定性</strong>是研究政府与市场关系的一个重要内容。 相关实证研究涉及财政政策不确定性、货币政策不确定性、贸易政策不确定性等。 在现实经济中，政策不确定性在不同程度上与不同范围内一直存在，影响经济主体行为决策与经济运行。 <strong>传统方法基于时间序列计量经济学模型，将经济政策波动性视为不确定 性。 由于经济政策与产出等变量相互影响，基于时间序列模型的经济政策不确定性包含宏观经济不确定性， 因此，这种方法存在内生性问题</strong>（凯利等，2016）。 为解决此问题，有学者将非经济虚拟变量作为代理变量测度经济政策不确定性，例如朱利欧和约克（2012）采用选举变量测度经济政策不确定性，但其缺点是所测度的政策不确定性时变性较差。 除计量经济学建模方法外，有文献基于调查数据构建经济政策不确定性指数，但这种指数仅能刻画某一政策不确定性，或政策不确定性的某一方面，不能全面刻画经济政策不确定性。 此外，接受调查人员可能存在样本选择偏差或系统性预测误差，导致政策不确定性测度的主观性较大（朱拉多等， 2015）。</p>
<p><strong>文本数据分析能够捕捉经济主体对经济政策不确定性的真实感知，已成为测度经济政策不确定性的主流方法</strong>。 贝克等（2016）在这方面做出开创性贡献，他们选取美国最具影响力的 10 家报纸，统计了每月包含经济、政策与不确定性等关键词频数，构建了美国经济政策不确定性指数。 随后，黄和陆（2020）采用中国大陆报纸测度中国经济政策不确定性月度指数。 陈和钟（2019）开发了新的机器学习算法（包括单词嵌入、多层感知 器和递归神经网络等），根据《人民日报》中文文本数据构建了中国政策变化指数。</p>
<p>贝克等（2016）的方法可用于构建各种分类政策不确定性指数。 例如，赫斯特德等（2020）基于《华盛顿邮报》《华尔街日报》以及《纽约时报》测度了美国货币政策不确定性。 汉德利和利茂（2017）以及卡尔达拉等 （2020）构建了美国贸易政策不确定性指数。 李和徐（2019）测度了美国地方税收政策不确定性指数。 朱军 （2017）基于汉语语境特征分析中国财政政策不确定性。 陈英楠等（2022）利用中国六家主流报纸构建了房地产政策不确定性指数。 部分学者还从经济主体主观感受角度构建政策不确定性指数。 邦坦皮等（2016）基于经济主体在其谷歌搜索时使用与经济政策不确定性相关的词汇量构建了美国政策不确定性指数。 聂辉华等 （2020）利用中国 A 股上市公司年报测度了中国企业对经济政策不确定性的主观感受。 本古里亚等（2022）基 于中国上市公司年报构建了中国企业视角下贸易政策不确定性指数。</p>
<br>
<h3 id="42-央行沟通与预期管理">4.2 央行沟通与预期管理</h3>
<p><strong>在现实经济中，中央政府与各级地方政府之间、不同政府部门之间以及政府与各类经济主体之间的信息传递经常遇到障碍，导致政策在从中央到地方政府传导过程中，被曲解或层层加码，或者在从政府向经济主体传导过程中，引起经济主体过度反应</strong>。 例如，2021 年，政府有关部门相继出台了一些反垄断措施，规范数字经 济平台、资本市场以及文化市场有序发展，以保障国家数据安全与经济安全，保护劳动者、消费者、中小企业等社会弱势群体的合法权益，同时还出台促进共同富裕的远景目标与政策措施。 这些政策在方向上是正确的， 但却引起了部分市场主体的不安情绪，导致国内外市场波动。 这在一定程度上反映了政策不确定性的客观存 在，同时也反映了政府与市场在政策沟通过程中存在堵点，有待化解、改进（洪永淼，2021）。 在这方面，一些国家央行在出台前瞻性货币政策时，通常会与市场主体密切沟通，尽量减少市场波动，这值得借鉴与学习。</p>
<p>央行传统预期管理注重实际政策措施，而现代预期管理则强调央行与经济主体的沟通交流（莫里斯、茜 恩，2018）。 <strong>2008 年金融危机后，受零利率下限约束，以调节名义利率为基础的通胀预期管理失效，央行无法通过利率渠道影响消费与投资，央行沟通便成为一个重要的新型货币政策工具</strong>（伯南克，2020）。 所谓央行沟通是指央行向经济主体公布货币政策目标与规则、经济展望以及货币政策走向等相关信息的过程（布林德等， 2008）。 央行沟通可释放货币政策变化消息，降低经济主体所感知的货币政策变化中的噪音成分，促使经济主体形成比较一致的预期，这已成为央行通过预期调节经济运行的一个重要方式，甚至比实际货币政策操作还重要（汉森、麦克马洪，2016）。 央行沟通通过释放经济展望与前瞻性指引等信息改变经济主体预期，因此央行沟通文本数据包含经济主体关于货币政策预期的丰富信息。 <strong>评估央行沟通效果已成为当前宏观经济学与金融学研究的一个热点</strong>，也为丰富货币政策预期管理理论提供实证基础。 虽然中国尚未受零利率下限的影响， 但央行沟通在引导中国经济主体预期方面也发挥着重要作用。 易纲在《人民日报》发文指出，建设现代中央银 行制度要使货币政策沟通制度化，有效管理和引导市场预期（易纲，2022）。</p>
<p>目前，中国央行沟通语料库包括口头沟通与书面沟通。 口头沟通主要包括央行领导人讲话、采访、新闻发 布会、窗口指导等；书面沟通主要包括每季度定期发布的《货币政策执行报告》以及《金融稳定报告》和央行货 币政策委员会的会议纪要等。 书面沟通相较于口头沟通更为正式，沟通也更加常态化。 <strong>如何从口头与书面沟通文本数据获得经济主体所感知的政策语调是现有研究的难点</strong>，林建浩等（2021）根据中国央行沟通特有的表达习惯，将自建词典与通用词典相结合，采用栅栏分布式多项回归模型解决文本数据的高维性和稀疏性问题， 测度中国央行沟通指数。 <strong>目前测度央行沟通的方法有叙事法、主观赋值法、措辞提取法以及机器学习方法</strong>。 与前 3 种方法相比，机器学习方法在提取央行沟通信息方面具有明显优势，这种方法可构建文本情绪、文本相似性以及文本可读性等指数。 在<strong>文本情绪</strong>方面，现有文献通常使用哈佛大学通用调查词典（第四版）等特定词典构建美联储沟通情绪指数（杰加迪西、吴，2015；施默林、瓦格纳，2019），以及测度欧洲央行货币政策立场（皮 考特、雷诺，2017），而姜富伟等（2021a）基于中文文本分词方法与情绪词典构建中国央行沟通情绪指数。 <strong>文本相似度</strong>是指不同的央行沟通文本数据在遣词造句或表达含义上的相近程度，可反映政策发布者信息公布的谨慎程度。 现有文献大多使用词频向量的余弦相似性测度文本相似度（埃尔曼、塔勒米，2020）。 <strong>文本可读性</strong>是指经济主体阅读与理解央行沟通文本的难易程度。 洛克伦和麦克唐纳（2014）使用弗莱什易读指数刻画了美 联储货币政策文本可读性。 费拉拉和安吉诺（2022）使用有监督与无监督的机器学习方法从欧洲央行演讲与推文等文本中提取主题信息，也采用弗莱什易读指数系统刻画欧洲<strong>央行沟通清晰度</strong>。</p>
<br>
<h3 id="43-通胀预期的测度">4.3 通胀预期的测度</h3>
<p>通胀预期在现代宏观经济学特别是货币政策研究中占有重要地位，影响经济主体投资决策和政府政策制定。 在新凯恩斯理论中，央行货币政策规则从盯住当前市场的通货膨胀率，逐渐发展为包含通胀预期的<strong>前瞻性规则</strong>（黄等，2009）。 因此，精准测度并管理通胀预期决定了货币政策的有效性。 由于传统计量经济学模型 可能包含过多噪音，通过构建计量经济学模型提取金融市场有效信息来预测通胀的效果并不理想（鲍尔、麦卡 锡，2015）。 大型机构关于通胀预期的调研数据似乎是更好选择。 现有研究表明基于调查数据的通胀预期预测结果优于计量经济学模型的预测（安等，2007；福斯特、赖特，2013），但调查数据本身存在不少缺点。 例如， 受成本限制，调查群体仅包含市场中的小部分个体且抽样频率过低。 此外，调查获得的专家与居民两组通胀预期数据在预测未来通胀能力方面差异较大，央行在制定货币政策时需要对两组通胀预期数据赋予不同权重 （利齐亚克、盛，2021）。</p>
<p><strong>电话、报纸与网络等媒体是经济主体理解政策与市场的重要中介，经济主体通常从媒体间接获得通胀预期信息，而不是直接在市场中花费大量时间追踪信息（卡罗尔，2003）。 因此可从文本数据构建通胀预期数据</strong>。 例如，加布里埃良等（2019）基于在线新闻数据构建英国日度通胀预期数据，比较准确反映通胀预期。 这种方法可推广到测度其他国家的通胀预期。 例如，安基利科等（2022）基于意大利推特数据，采用机器学习方法构建日度通胀预期数据；中岛等（2022）通过日本内阁办公室经济调查文本数据，采用机器学习方法构建了 日本价格情绪指数。 目前，鲜有研究基于文本数据构建中国通胀预期数据。</p>
<br>
<h3 id="44-经济政策预期冲击的识别">4.4 经济政策预期冲击的识别</h3>
<p>财政政策与货币政策预期冲击或消息冲击对宏观经济波动有重要影响。 <strong>由于计量经济学模型的信息集通常小于经济主体所获得的信息集，传统向量自回归模型无法识别预期冲击对经济波动的影响（杨，2005；利珀等， 2013），因此，扩展计量经济学模型信息集成为研究经济政策预期冲击的一个重要问题，其中一个办法是寻找预期的代理变量</strong>。 传统方法分别将金融市场信息（与政府支出相关行业股价）和调查数据作为经济政策预期的代理变量，但这两种测度方法均存在不足之处。 金融市场信息存在较大噪音，且仅反映金融市场参与者对经济政策的预期。 调查数据仅覆盖小样本群体，且抽样频率较低。 因此，采用叙事法获得经济主体对经济政策的预期 是一个较好选择。 在政府收入消息冲击方面，罗默和罗默（2010）利用美国总统演讲与国会记录中税收变化信息，构建税收消息变量，考察税收变化对宏观经济波动的影响。 在政府支出消息冲击方面，雷米（2011）利用商业报刊报道的国防支出数据，发现国防预期支出扩张可增加宏观经济产出与劳动力投入，降低消费与投资。</p>
<p>主流文献认为，罗默和罗默（2010）与雷米（2011）的叙事方法能够有效识别财政支出预期冲击。 但是，<strong>贾西姆等（2022）认为叙事法构建的指标存在明显不足，例如，没有充分考虑消息包含的噪音成分，且经济主体预期可能随时间发生变化等，因此，采用文本数据分析可能是更好选择</strong>。 拉森和托斯鲁德（2019）使用潜在狄利克雷分配主题模型，构建了挪威全要素生产率消息冲击数据，发现全要素生产率消息冲击带来产出扩张。 贾西姆等（2022）采用半监督文本分析方法准确区分了增税与减税新闻，通过美国总统演讲文本中关于税收变化消息的重复度确定税收消息是否实现，他们发现税收变化消息对产出具有显著的延迟性影响。 这是未来研经济政策预期冲击如何影响宏观经济波动的一个重要方向。</p>
<br>
<h3 id="45-经济主体信心情绪与情感的度量">4.5 经济主体信心、情绪与情感的度量</h3>
<p>现有文献大多采用调查数据测度经济主体信心，但调查数据仅反映部分群体信心指数，而且由于调查成本较高、抽样频率低，调查数据不能充分、及时地反映经济主体信心变化趋势。 相比之下，<strong>文本数据可更好测度经济主体信心与情绪</strong>。</p>
<p>经济主体情绪包含正面与负面、乐观与悲观、积极与消极、看涨与看跌等心理因素，文本数据分析也常用“语调”刻画情绪。 由于文本数据规模大、频率高，测度情绪已成为文本数据在金融学的一个重要应用（姜富伟等，2021b）。 目前，测度情绪的方法主要有以词典法为主的无监督学习和以机器学习为主的有监督学习。 词典法是依据事先给定的情绪词典，通过分析文本数据中积极与消极词汇出现的频数来测度经济主体情绪。 由于词典法依赖各个字典所选择的单词，且仅包含与情感相关的词汇，因此可能无法捕获全部情感信息。 近年来，机器学习越来越受到研究者的青睐（根茨科等，2019）。 有监督的机器学习方法把情感测度视为一个文本分类问题，将有标签的文本数据集划分为训练集与测试集，利用训练集来训练模型，最后将训练模型用于测试集进行预测。 主要机器学习方法包含基于概率论的朴素贝叶斯分类器（布尔迈尔、泽希纳，2021）和支持向量机（李等，2019）等。 对没有预定词典的文本数据而言，机器学习方法是较好的选择，但是与词典法相比，机器学习的训练集往往需要通过人工选择，因此耗时成本较大且分类结果因人而异，导致研究结论可复制性不高。</p>
<p>经济主体情绪显著影响宏观经济运行。 张成思等（2021）采用中国 A 股上市公司年报测度了中国企业宏观经济感知程度，分析表明，当央行实施积极货币政策时，对宏观经济感知更乐观的企业会积极响应政策刺激，增加投融资行为。 范小云等（2022）使用中国 32 家知名报刊 175 万条新闻数据，将所开发的词典与机器学习方法相结合，提出一种混合式情绪分析方法，发现新闻媒体情绪可通过提振经济主体信心来扩张消费与产出。 夏皮诺等 （2022）基于美国 16 家主要报纸近 24 万条经济金融新闻文本数据，将现有词典与专门构建的新词典相结合测度美 国市场情绪，发现其构建的美国市场情绪指数与经济周期和经济事件显著相关，基于文本数据的高频情绪指数能 预测美国市场情绪调查数据，且乐观情绪冲击能引起产出与消费的显著扩张。 现有文献也发现基于文本数据得到的经济主体情绪指数可改进宏观经济预测。 例如，阿吉拉尔等（2021）基于西班牙报纸构建了日度市场情绪指 数，该指数与经济信心调查数据高度相关，但在预测 GDP 变化时优于调查数据。 孔索莱等（2021）从意大利与西班牙经济新闻中构建新闻媒体情绪指数，发现这一指数有助于预测意大利与西班牙主权债券收益率利差。</p>
<p>除了与预期相关的信心与情绪等心理因素，<strong>行为财政学与行为货币经济学理论表明，情感因素（如攀比、 嫉妒、内卷等）对经济主体决策同样重要</strong>。 在行为财政学中，为提高经济主体税收遵从度，增强国家税收征收与公共服务供给能力，现有文献从经济政策威慑性视角研究政策宣传的有效传导渠道。 例如，为了测度税法 对经济主体的威慑性，毛捷等（2022）通过中国知网报刊获取 2.15 万条税法宣传报道，借助机器学习方法研究威胁性宣传效果，即通过曝光违法犯罪行为达到警示与宣传效果，他们发现税法宣传可降低经济主体侥幸心理，提升其税收遵从意愿。 在行为货币经济学中，理论模型通常认为经济主体对经济政策的信任是货币政策 发挥作用的重要因素，但如何测度经济主体信任度是一个实证研究难点。 余等（2021）基于推特评论文本数据，采用深度学习方法测度新闻可信性，发现金融报刊的新闻可信度对股票价格有正向影响。</p>
<p>鲜有文献测度经济主体的攀比、嫉妒和内卷等情感因素，主要是在测度方法上存在一定困难。 例如，在研究经济主体消费是否存在<strong>同群效应</strong>时，需要寻找周围群体消费水平的代理变量，目前国内研究基于调查数据构建区县层面同群组，代表周围群体消费水平（宋泽、邹红，2021），美国学者则构建了州层面同群组（伯特兰、 莫尔斯，2016），如此宏观层面的划分很难准确测度周围群体消费水平对经济主体消费的影响。 但随着网络技术的发展与普及，可直接从微博与贴吧等文本数据提取经济主体对周围群体消费变化的态度与看法等信息， 用于研究经济主体是否存在同群效应。</p>
<p><br><br></p>
<h2 id="五问题挑战与展望方法论视角">五、问题、挑战与展望：方法论视角</h2>
<p>目前，经济学、金融学与会计学等学科在利用文本数据测度经济主体心理因素及其影响方面，已获得一系列重要实证发现，凝炼了一些经验典型特征事实，为深刻理解政府政策背景下经济主体心理因素的重要作用提供了很多洞见。 同时，基于非结构化大数据的经济学实证研究尚处在初级阶段，还存在不少困难与问题，需要进一步改进、完善基于非结构化大数据的实证研究，特别是发展新的研究方法与分析工具，形成一个系统的研究范式。</p>
<h3 id="51-非结构化大数据实证研究面临的困难">5.1 非结构化大数据实证研究面临的困难</h3>
<h4 id="511-因果推断问题">5.1.1 因果推断问题</h4>
<p>经济学研究最主要目的是识别经济因果关系，揭示经济运行规律。 因果关系是指在其他因素保持不变的条件下，通过改变某个变量（如政策变量或心理变量），观测结果变量是否随之变化。 <strong>一个经济理论是否具有深刻解释力取决于它能否揭示可被验证的经济因果关系。 目前，文本数据实证研究发现，经济主体心理因素对宏观经济与金融市场有重要影响，但这种影响大多体现为相关性，而不是因果关系</strong>。 例如，贝克等（2016）发现，他们构建的美国经济不确定性指数与美国就业和产出变化存在负相关关系，与美国股票市场波动程度则呈现正相关关系，但这些相关关系不能被解释为因果关系。 推动政府政策背景下经济主体心理因素与经济变量之间的因果分析是未来政府与市场关系研究的一个重点。 在实验科学中，可通过实验手段控制其他因素不变，识别并测度所关注变量之间的因果关系。 但在经济学与社会科学中，很多情形难以实现可控实验，导致推断心理变量与经济变量的因果关系面临很大困难。 经济主体心理因素的因果分析的难点首先在于，海量经济大数据本质上是非实验性观察数据，很难甚至不可能控制其他因素保持不变。 更重要的是，经济主体决策时的前瞻性思维，具有“反身性”特征，容易形成双向、 复杂的互动关系。 <strong>因此，在研究经济主体心理因素的因果关系时，需要与有关经济政策传导机制的经济理论结合起来，借助经济学理论甚至心理学理论指导</strong>。 此外，还需要应用计量经济学、统计学与机器学习的新方法、新工具帮助推断因果关系。 例如，众所周知，机器学习样本外预测比很多传统计量经济模型要精准得多， 机器学习精准预测能力不是基于经济学因果关系，而是基于大数据中的变量特征与变量之间的统计关系（如 相关关系或预测关系）。 但是，机器学习可用来改进基于观测数据的因果关系推断。 例如，虚拟事实估计是使用观测数据识别与测度经济因果关系的一个重要方法（珀尔，2009；瓦里安，2016）。 通过挖掘大数据中变量之间的相关性或预测关系，机器学习可精准估计或预测虚拟事实结果，即假设经济政策没有实施时经济运行的潜在结果。 然后对比虚拟事实结果与实际结果的差别，便可识别经济政策或心理因素与经济结果之间的因果关系，并精准测度政策效应或心理因素影响的大小。</p>
<br>
<h4 id="512-经济主体的异质性与社会性问题">5.1.2 经济主体的异质性与社会性问题</h4>
<p><strong>现有文本数据实证研究大多假设经济主体是同质且独立的个体，通过简单加总得到宏观心理变量的测度，然后应用计量经济学方法推断宏观心理变量与宏观经济变量间的数量关系。 这种研究缺乏微观经济理论基础，且存在由于经济主体异质性而导致信息失真的可能性。 现实经济中的经济主体通常呈现显著的异质性特征</strong>，例如，在中国市场主体中，企业分为国有企业与非国有企业，非国有企业又分为民营企业与外资企业，此 外还有混合所有制企业与合资企业，以及大量个体工商业者。 这些企业具有明显不同的组织结构和行为方式，表现出巨大异质性。 地方政府之间在经济社会治理方面也存在显著差异。 在政策从中央政府向地方政府 传导过程中，地方政府由于认知、理解以及实际困难等各种原因，对中央政府政策的执行与响应程度存在差别。 此外，不同经济主体对同一政策的反应不一样，同一政策对不同经济主体的影响也是不一样的。 <strong>文本等非结构化大数据可提供各类异质性地方政府与经济主体的动态行为信息，从而更准确测度政策背景下经济主体心理因素及其影响，特别是经济政策对不同行业、不同地区、不同群体的动态分布效应</strong>。</p>
<p>另一方面，互联网等信息技术的快速进步与广泛应用，使经济主体之间结成复杂社会网络，这种网络会强化产生共情、情绪传染等现象的趋势，通过乘数效应放大或者收缩经济政策效应或重要事件冲击的影响。 2021 年初，美国股票市场大量散户在一家叫做“游戏驿站”的美国游戏零售公司股票交易中，抱团与对冲基金激烈博弈，他们受社交网络平台以及美国主流媒体报道传播的情绪感染，形成一股强大的乐观情绪，使该公司股价在不到一个月上涨 20 倍，迫使做空的机构投资者不断爆仓，充分显示了社交网络平台情绪传染的巨大威力。 海量经济行为大数据包含大量微观经济主体互动关系信息，可用于研究政府与经济主体以及各类经济主体之间的情绪传染机制，因此在研究经济主体心理因素时，不仅要注意经济主体异质性的影响，还要考虑经济主体社会性的影响，重视宏观经济心理因素的微观行为基础。</p>
<br>
<h4 id="513-经济主体心理因素的测度误差问题">5.1.3 经济主体心理因素的测度误差问题</h4>
<p>从非结构化大数据提取信息，主要依靠人工智能与机器学习技术，如分析文本数据的自然语言处理技术， 分析图像与视频数据的计算机视觉技术，以及分析音频数据的计算机语音识别技术等。 <strong>从统计学角度看，这些非结构化数据分析技术，本质上都是统计模型，这些模型大多是误设模型，因此所构建的心理变量存在不同程度的测度误差</strong>，虽然这些测度误差随着人工智能与机器学习技术的进步而不断减少。 目前实证研究在使用文本数据测度经济主体情绪时，<strong>大多使用词典法，通过关键词频数测度心理变量，这种方法显然没有将词与词的关系考虑在内，这将不可避免产生信息遗漏与测度误差</strong>。 从计量经济学视角看，心理变量测度误差的存在， 对计量经济学推断将造成一定影响。 例如，当解释变量包含测度误差时，普通最小二乘法估计将产生偏差，需要使用工具变量法（如二阶段最小二乘法）才能获得一致估计。 <strong>事实上，基于自然语言处理技术构建的心理变量的测度误差，并不满足统计意义上的独立同分布假设，可能还包含着重要的心理遗漏信息</strong>。 但是，目前基于文本数据的实证研究都没有考虑这些问题及其产生的影响。</p>
<p>利用文本数据特别是中文文本数据进行定量分析的技术难度较高。 英文文本数据的信息提取在现有研究文献中已有标准方法，其经济类管理类词典也已比较完善。 常用英文词典有哈佛大学通用调查词典、亨利词典（亨利，2008）、文辞乐观与悲观词汇词典和洛克伦-麦克唐纳词典（洛克伦、麦克唐纳，2011）等（唐国豪等， 2016）。 不同英文词典的应用范围和功能有所差异，例如哈佛大学通用调查词典（第四版）主要涉及心理学与社会学领域的正面和负面词汇，亨利词典是第一本金融文本词典，但其词汇量较为匮乏，而洛克伦-麦克唐纳 词典涵盖的金融情绪词汇较为全面且准确。 由于中英文语言结构差异，中文文本数据需要相对复杂的处理方 法。 中文文本数据分析刚起步不久，在经济学、管理学领域还缺乏比较全面、完善的词典。 中文文本数据的分词位置会影响对文本真实含义的理解，一些关键词的词性（如名词与动词）随着上下文语境的变化而变化，一些常用词的语义还会随着时代变化而产生截然不同的理解，特别是在互联网时代，中文语言进化速度加快，完 全相同的词汇，其含义可能在短短几年内便发生巨大变化，大量新的网络语言不断涌现，具有强烈的时代特征，无法按照常规中文语言含义进行分析。 此外，不同的中文文本来源在叙述同一事情或现象时经常采用不 同的词汇与叙事方式，使用同一词典可能会产生严重的测度偏差。 所有这些差异与困难，意味着不能照搬照抄英文文本数据分析方法，必须根据中文文本结构特点，开发适合中文文本数据的分析方法与技术。 <strong>如何从海量中文文本数据中提取经济主体心理因素等信息是中国经济学一个重要的学术前沿问题</strong>。 在这方面，已取得一定进展。 王靖一和黄益平（2018）、姚加权等（2021）、姜富伟等（2021b）、范小云等（2022）等开发了中文文 本情绪词典。 针对词典法与传统机器学习方法没有考虑语言结构、没有关注文档整体信息等缺陷，范等 （2022）提出因子增强正则化预测模型，这个方法考虑了词汇之间的逻辑关系，能够提取文本的隐藏主题，从而弥补现有研究的不足。 也有文献引入自然语言前沿处理方法提升数据信息分析质量，例如黄等（2022a）拓展目前自然语言处理领域流行的 BERT 模型，构建能充分利用财务文本上下文信息的 FinBERT 模型，并证明其在情绪分类方面显著优于洛克伦-麦克唐纳词典以及其他传统机器学习技术。</p>
<br>
<h3 id="52-构建非结构化大数据实证研">5.2 构建非结构化大数据实证研</h3>
<h4 id="521-充分利用各种类型的非结构化经济大数据">5.2.1 充分利用各种类型的非结构化经济大数据</h4>
<p><strong>目前有关经济主体心理因素的研究主要基于文本数据，相对较少使用其他非结构化数据，例如图像、音频、视频数据等</strong>。 事实上，其他非结构化大数据也包含丰富的关于各类经济主体的心理信息，例如人们常说 “一图胜千言”，包括社交网络平台上各种常见的“表情包”。 随着计算机视觉与人工视觉网络等机器学习技术 的发展，已有政治学学者从图像数据中挖掘经济主体信息。 例如，王等（2016）利用推特<strong>个人资料照片</strong>分析了美国总统选举中民主党与共和党总统候选人支持者的人口构成。 查克拉波蒂（2017）使用上述方法分析美国社会与政治活动发起人的特征。 在经济学领域，阿罗米和克莱门茨（2021）基于政策制定者、交易员、公司经理 等在媒体上发布的照片构建他们的面部表情指数与情绪指数。 奥贝德和普图通（2022）采用卷积神经网络方法对《华尔街日报》照片进行分类，依据每日新闻照片传递的悲观情绪图像所占比例，构建了投资者情绪指数。 <strong>图像信息挖掘技术在管理学中也有应用</strong>（布兰克斯普尔等，2017）。 目前，基于音频与视频数据的实证研 究大多集中在技术层面，政策应用还不多见。 例如，埃德曼斯等（2022）根据音乐平台的热门音乐所传递的正面情感测度投资者情绪。 库雷西和阿格沃尔（2022）基于视频数据提出一种利用卷积神经网络与深度学习技术测度经济主体情绪的方法。 常和彭（2022）基于抖音国际版产生的流行短视频，收集了每日 500 个最受欢迎短视频数据，并应用深度视觉音频注意力网络技术构建了各个国家的股票市场情绪指数。 <strong>上述研究仅使用单模态数据，即对非结构化大数据进行单独分析，没有考虑融合文本、图像、音频或视频等多模态数据蕴含的更 为丰富信息</strong>。 例如，经济主体在微博等社交媒体传达情绪时，往往会通过文字搭配图片方式发表评论；视频数据包含文字、音频和图像等各类非结构化数据。 分析多模态数据的难点在于如何融合不同类型非结构化大数据，巴尔特鲁沙炎等（2018）对多模态数据融合（特征融合、决策融合和混合融合）进行综述，为使用多模态数据 测度经济主体心理因素提供参考。 <strong>除图像、音频、视频等非结构化大数据外，一些学者利用卫星遥感技术测度夜间灯光、天气、地形、农业以 及城市用地、种植作物、建筑类型与自然资源等图像，并应用于经济学分析</strong>（唐纳森、斯托里加德，2016）。 作为 一种最有代表性的遥感数据，夜间灯光数据能够比较客观反映人类社会生产生活状况，一些学者因此将夜间 灯光数据作为地方经济活动的代理变量，使用夜间灯光数据估计、调整与修正国民生产总值（亨德森等， 2012），或研究各种经济问题（王贤彬、黄亮雄，2018；吉布森等，2020）。 李等（2022）使用海事卫星遥感数据测度公海上国际贸易货船二氧化碳排放量，填补了全球碳排放测度研究的一个空白。 陈等（2022）利用城际运输卡车物流遥感数据定量评估上海新冠肺炎疫情管控政策对上海乃至长三角地区经济的影响。 毫无疑问，各种非结构化大数据包含很多结构化数据所没有的信息，这些信息远没有充分挖掘出来，需要整合、集成并综合利用各种不同类型的非结构化大数据。</p>
<br>
<h4 id="522-系统构建政府政策数据库与各类经济主体心理数据库">5.2.2 系统构建政府政策数据库与各类经济主体心理数据库</h4>
<p>与结构化大数据相比，非结构化大数据包含更丰富的各级政府政策变化、各种重要事件冲击以及各类经济主体对政府政策变化与重要事件的反应等信息，因此<strong>在研究政府与市场关系时，必须重视非结构化经济大数据库建设。 非结构化大数据种类繁多、结构复杂、来源不同，收集、清洗、处理、整合、分析各种非结构化大数据，并将其转化为结构化数据，是一项非常艰巨但又非常重要的基础设施建设工程</strong>。 非结构化大数据是高维或超高维数据，将非结构化大数据转化为结构化数据，特别是构建经济政策变量与经济主体心理变量的结构化数据，本质上是一个降维问题。 应该加强与计算机科学和<strong>计量语言学</strong>的交叉融合，将这些学科用于非结构化数据分析的方法与工具，如自然语言处理技术、计算机视觉技术、计算机语言技术、计量语言学模型等，应用并普及到经济学实证研究中，以加快构建系统性、结构化政府政策数据库与经济主体心理数据库。</p>
<p>除各种非结构化大数据外，还应重视使用其他方式与渠道，例如采用统计调查方法搜集与整理经济主体心理数据。 例如，美国统计学会与美国国家经济研究局于 1968 年开创的，1990 年由美联储费城分行接手的 “<strong>美国专业预测者调查</strong>”，按季度发布专业预测者对美国主要宏观经济变量的预测结果。 里科等（2016）根据该 调查数据，通过考察不同个体对美国政府支出的预测差异来测度未来政府支出变化不确定性。 从 20 世纪 40 年代开始，美国密歇根大学调查中心基于对 500~600 名成年人的原始调查数据，编制并于每月第 10 天发布“<strong>密歇根消费者信心指数</strong>”，反映美国消费者对当前以及未来经济的信心强弱，成为预测美国经济走势与消费趋向的一个先行指标。 美国管理与组织实践调查是美国第一次大规模管理实践调查，涵盖超过 1 万家公司的 3 万 个工厂，调查企业管理层对当年与未来一年销售量的主观概率分布预测。 布卢姆等（2022）基于该调查数据， 测度企业管理层真实感受到的销售不确定性，即<strong>企业主观不确定性</strong>。 在中国，国家统计局编制的<strong>中国消费者信心指数与企业家信心指数</strong>，通过对城市消费者、法人企业以及依照法人单位进行统计的产业活动单位负责人进行问卷调查，以指数形式综合反映经济主体对宏观经济环境的主观感受与信心。 此外，中国国家统计局 中国经济景气监测中心的“<strong>中国百名经济学家信心调查</strong>”，中国人民银行的未来物价预期指数、未来就业预期 指数与企业家信心，中国银联的银行卡消费信心指数等，都是通过统计调查获得的中国经济主体预期数据。</p>
<br>
<h4 id="523-大力发展非结构化大数据计量经济学">5.2.3 大力发展非结构化大数据计量经济学</h4>
<p>除政府政策数据库与经济主体心理数据库建设外，非结构化大数据的分析、变量构建、模型设定、估计、推断 与预测等具有较高技术门槛，需要大力发展非结构化大数据计量经济学，系统性形成非结构化大数据信息提取、建模、估计、检验、预测的计量经济学理论、方法与工具。 大数据与人工智能特别是机器学习的发展给计量经济学带来了许多机遇与挑战（瓦里安，2014；洪永淼、汪寿阳，2021a，2021b）。 <strong>近年来，结构化大数据计量经济学理论与方法创新取得了巨大进展，包括高频与时变时间序列建模与预测、实时预测、高维计量经济学建模、新型结 构化数据（如函数数据、区间数据、矩阵数据、符号数据等）建模，以及基于机器学习的因果推断（阿西，2019）等</strong>。 相比之下，非结构化大数据计量经济学理论与方法创新则处于起步阶段。 一方面，计算机科学关于各种非结构化大数据的分析技术突飞猛进，但这些先进技术尚未广泛应用于分析非结构化经济大数据，在经济学领域存在明显的“应用时滞”。 另一方面，除构建非结构化大数据库外，还需要发展适合于分析非结构化大数据的计量经济学理论与方法，并与计算机科学和计量语言学关于非结构化大数据的分析方法与技术有机融合起来。</p>
<p>毫无疑问，在非结构化大数据被转化为结构化数据之后，很多现有计量经济学理论与方法可发挥重要作用。 但是，<strong>基于非结构化大数据构建而成的结构化数据，需要深入研究其经济含义与统计性质。 例如，基于非结构化大数据构建而成的经济主体情绪变量时间序列数据，是否为非平稳随机过程？有无存在结构性变化？ 是具有结构突变还是平稳结构变化？是短记忆过程还是长记忆过程？研究这些问题对理解情绪指数的心理学、经济学含义及其计量经济学建模与推断具有重要意义</strong>。 例如，如果所构建的时间序列情绪变量是计量经 济学意义上的非平稳一阶单整 I（1）过程，则可能会出现时间序列计量经济学中所谓的伪回归现象；而如果时 间序列情绪变量是一个局部平稳过程，则需要发展新的计量经济学建模、估计、推断与预测理论与方法。 王等 （2022）使用具有时变波动性的随机游走模型刻画中国股市投资者情绪指数，并发现投资者情绪对股票定价没有长期影响。 也有研究发现，美国投资者情绪指数具有显著的结构突变特征（姚，2022）。 此外，由于经济主体情绪是不可观测的潜在变量，因此计量经济学的潜在因子模型，包括高维时变因子模型，适合于对经济主体情绪进行建模。 新型数据需要新的分析方法与工具。 随着非结构化数据可获得性的提升与广泛使用，迫切需要创新非结构化大数据计量经济学理论与方法。</p>
<br>
<h4 id="524-促进跨学科系统性交叉研究">5.2.4 促进跨学科系统性交叉研究</h4>
<p>基于非结构化大数据的经济主体心理变量测度与建模涉及多学科理论与方法，包括经济学、心理学、社会学、社会心理学、语言学、认知科学、计量经济学、统计学、数据科学、计算机科学、计量语言学、复杂性科学等。 其中，</p>
<ul>
<li><strong>心理学、社会心理学、认知科学与语言学有助于理解经济主体心理因素及其变化规律</strong>。</li>
<li><strong>计量经济学、统计 学、数据科学、计算机科学、计量语言学以及复杂性科学能够为提取非结构化大数据中的心理信息提供方法论指导与技术支撑</strong>。</li>
<li><strong>经济学、心理学、社会学、社会心理学则为经济主体心理变量的经济学解释提供理论指导</strong>。</li>
</ul>
<p>应打破学科壁垒，促进不同学科的交叉融合，推动经济学与其他社会科学（包括心理学、社会学、社会心理 学、政治学、法学、历史学、文化学等）的跨学科系统性研究。</p>
<p><br><br></p>
<h2 id="六结论">六、结论</h2>
<p><strong>在研究政府与市场关系过程中，需要重视各类经济主体心理因素，如预期、信念、信心、情绪、情感等。 这些心理因素的产生、形成与变化有其客观原因。 由于其“反身性”特征，经济主体心理因素对政府政策传导机制有重要影响。 经济主体通过学习、领会、解读政策含义，形成对政策的预期，然后根据自身利益最大化原则做出最优决策，从而影响宏观经济运行。 因此，测度经济主体心理因素及其影响是研究政府政策传递机制的一个重要思路</strong>。</p>
<p><strong>如何科学测度经济主体心理因素在方法论上面临巨大困难与挑战</strong>。 虽然现代心理学与实验经济学为定量分析心理因素提供了一些分析工具，例如观察访谈、统计调查以及实验方法等，但其研究的深度、广度与抽样频率受到各种限制与约束。 人工智能技术的发展与非结构化大数据可获得性的提升，为测度经济主体心理因素及其影响，提供了一个可行方法。 <strong>非结构化大数据包含大量关于政府政策变化（如语调变化）、重要事件冲击以及经济主体对政策变化与重要事件冲击的反应等信息，特别是语言是人类表达思想与情感，进行沟通交流的最主要工具，可从文本等数据中提取、测度各类经济主体心理因素及其影响</strong>。 这样，原来的定性分析可转变为严谨的定量实证分析，从而打破经济学中定性分析与定量分析的界限，并使经济学与其他社会科学跨学科系统性研究成为可能。 由于中国巨大的人口规模与经济规模，新的信息技术及其应用层出不穷，数字经济发展具有巨大潜力与规模优势，中国即将成为全球最大的数据生产国。 大数据资源中，相当大一部分是非结构化大数据，这些数据包含许多传统结构化数据无法反映的重要信息，特别是各种政府政策变化与重要事件冲击、以及各类经济主体对政策变化与重要事件冲击的反应等信息。 中国庞大的数据资源与政府的重要作用，使中国拥有全球最大且独一无二的“政策数据库”这一富矿，在研究政府与市场关系方面具有天然优势。 <strong>我们应该积极推动学科交 叉，大胆借鉴心理学、社会心理学、认知科学、计算机科学、计量语言学以及复杂性科学等理论与方法，发展新的研究方法与工具，形成新的研究范式，构建具有深厚学理基础关于政府与市场关系的原创性自主经济理论， 以指导中国式现代化建设与全球化实践</strong> ① 。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 14000&#43;篇心理学顶刊论文可复现性调研</title>
      <link>https://textdata.cn/blog/2023-03-31-pnas-measure-replicability-of-psychology-with-ml/</link>
      <pubDate>Fri, 31 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-31-pnas-measure-replicability-of-psychology-with-ml/</guid>
      <description>”可复现研究的数量远远低于科学界期望，我们创建了一个基于文本数据的机器学习模型，估计了自2000年以来心理学六个子领域中发布的超过14,000篇文章的可复现性分析。此外，我们还调查了可复现性与不同的研究方法、作者的生产力、引用影响力和机构声誉、论文的引用增长和社交媒体覆盖率有关的变化。我们的研究结果有助于建立大规模的经验模式，以便为推进复现研究提供依据。The number of manually replicated studies falls well below the abundance of important studies that the scientific community would like to see replicated. We created a text-based machine learning model to estimate the replication likelihood for more than 14,000 published articles in six subfields of Psychology since 2000. Additionally, we investigated how replicability varies with respect to different research methods, authors &amp;#39;productivity, citation impact, and institutional prestige, and a paper’s citation growth and social media coverage. Our findings help establish large-scale empirical patterns on which to prioritize manual replications and advance replication research.“</description>
      <content:encoded><![CDATA[<p>Youyou, W., Yang, Y., &amp; Uzzi, B. (2023). <strong>A discipline-wide investigation of the replicability of Psychology papers over the past two decades</strong>. <em>Proceedings of the National Academy of Sciences</em>, <em>120</em>(6), e2208863120.</p>
<h2 id="意义">意义</h2>
<p>可复现研究的数量远远低于科学界期望，我们创建了一个基于文本数据的机器学习模型，估计了自2000年以来心理学六个子领域中发布的超过14,000篇文章的可复现性分析。此外，我们还调查了可复现性与不同的研究方法、作者的生产力、引用影响力和机构声誉、论文的引用增长和社交媒体覆盖率有关的变化。我们的研究结果有助于建立大规模的经验模式，以便为推进复现研究提供依据。</p>
<h2 id="本文章节">本文章节</h2>
<p>大家可以选择感兴趣部分阅读。<strong>该论文提供了词向量模型文件，如果是做社科研究的同学，可以下载下来，探索下词向量。例如查看概念近义词</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">一、摘要
二、代码
2.1 训练集
2.2 测试集数据
2.3 词向量模型
三、正文
四、数据和方法 
4.1 机器学习模型
4.2 关于模型迁移学习是否会预测不准问题
4.3 评估可复现性前后出版的相关度量。
五、结果
六、讨论
</code></pre></div><br>
<h2 id="一摘要">一、摘要</h2>
<p>对社会科学中较弱的可复现性的猜测使学者们渴望量化这一学科的复现失败的规模和范围。然而，仅靠小规模的人工复现方法无法应对这个大数据问题。在这里，我们进行了一项跨学科的科学复现普查。<strong>我们的样本（N=14,126篇论文）涵盖了近20年来发表在六个顶级心理学期刊上的几乎所有论文</strong>。使用一个经过验证的机器学习模型，估计论文的复现可能性，我们发现证据既支持又反驳了从相对较小的人工复现样本中得出的推测。首先，我们发现单一的心理学复现率不能很好地捕捉不同子领域中可复现性的程度变化。其次，我们发现，在所有子领域中，复现率与研究方法强相关。<strong>实验法的复现率明显低于非实验研究的复现率</strong>。第三，我们发现作者的<strong>累积出版物数量和引用影响力与复现可能性呈正相关，而其他反映研究质量和严谨性的指标，如作者的大学声誉和论文的引用，与可复现性无关</strong>。最后，与媒体注意应该关注可复现性的研究的理想相反，我们发现<strong>媒体的注意力与复现失败的可能性呈正相关</strong>。我们对可复现性的规模和范围的评估是广泛解决可复现性问题的重要下一步。</p>
<p><br><br></p>
<h2 id="二代码">二、代码</h2>
<p>论文的数据及代码 <a href="https://osf.io/f5sxn/">下载地址</a></p>
<h3 id="21-训练集">2.1 训练集</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#标注的训练集数据388篇论文(314个实验论文+72个非实验研究)</span>
<span class="n">training_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;training_sample.csv&#34;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_df</span><span class="p">))</span>
<span class="n">training_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">388
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
<br></p>
<h3 id="22-测试集数据">2.2 测试集数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#带预测的14125篇论文数据  </span>
<span class="n">prediction_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;prediction_sample.csv&#34;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prediction_df</span><span class="p">))</span>
<span class="n">prediction_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">14126
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<h3 id="23-词向量模型">2.3 词向量模型</h3>
<p>词向量设置为200维， 训练得到的模型文件638M。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1"># 导入模型文件</span>
<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;mag_200d_psy_eco_word2vec&#39;</span><span class="p">)</span>

<span class="c1">#词汇量</span>
<span class="nb">len</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">275561
</code></pre></div><br>
<p>查看词向量模型的近义词，如果想用expand_dictionary函数，请阅读</p>
<p><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"><strong>预训练模型 | 金融会计类word2vec， 可扩展或构建领域内概念情感词典</strong></a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看幸福的同义词</span>
<span class="c1">#这个函数是我自己定义的，需要的点击上方说明的链接</span>
<span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;happiness&#39;</span><span class="p">],</span> 
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;happiness&#39;,
 &#39;happiness,&#39;,
 &#39;happiness.&#39;,
 &#39;well-being&#39;,
 &#39;well-being,&#39;,
 &#39;swb&#39;,
 &#39;contentment&#39;,
 &#39;wellbeing&#39;,
 &#39;peacefulness,&#39;,
 &#39;well-being:&#39;,
 &#39;happiness;&#39;,
 &#39;life-satisfaction,&#39;,
 &#39;gratitude&#39;,
 &#39;wellbeing,&#39;,
 &#39;unhappiness&#39;,
 &#39;eudaimonic&#39;,
 &#39;life-satisfaction&#39;,
 &#39;happier&#39;,
 &#39;loneliness&#39;,
 &#39;gratitude,&#39;,
 &#39;happiness:&#39;,
 &#39;peacefulness.&#39;,
 &#39;flourishing,&#39;,
 &#39;contentment,&#39;,
 &#39;materialism&#39;,
 &#39;swb,&#39;,
 &#39;flourishing&#39;,
 &#39;joy&#39;,
 &#39;vitality&#39;,
 &#39;gratefulness&#39;,
 &#39;eudemonic&#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mental&#39;</span><span class="p">],</span> 
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;mental&#39;,
 &#39;health&#39;,
 &#39;illness&#39;,
 &#39;backgroundmental&#39;,
 &#39;psychiatric&#39;,
 &#39;illness,&#39;,
 &#39;ofmental&#39;,
 &#39;abstractmental&#39;,
 &#39;illnesses&#39;,
 &#39;nonmental&#39;,
 &#39;mental-health&#39;,
 &#39;psychosocial&#39;,
 &#39;health/substance&#39;,
 &#39;illnesses,&#39;,
 &#39;psychological&#39;,
 &#39;health/social&#39;,
 &#39;diagnosable&#39;,
 &#39;disability&#39;,
 &#39;substance&#39;,
 &#39;ill-health&#39;,
 &#39;stigma&#39;,
 &#39;psycho-social&#39;,
 &#39;mdos.&#39;,
 &#39;health,&#39;,
 &#39;health/mental&#39;,
 &#39;non-mental&#39;,
 &#39;retardation/developmental&#39;,
 &#39;stigmatization&#39;,
 &#39;stigmatizing&#39;,
 &#39;illness.&#39;,
 &#39;disability,&#39;]
</code></pre></div><p><br><br></p>
<h2 id="三正文">三、正文</h2>
<p>可复现性对于加强科学预测和改善生活水平的战略至关重要，也是科学自我纠正的证明。<strong>卡尔·波普尔(1)得出结论，科学中的复现确保了“我们不仅仅在处理孤立的‘巧合’，而是处理具有规律性和可复现性的事件，这些事件从原则上来说是可以经过主体间的测试的</strong>。”2011年，一项有争议的关于“时间保留因果性”的研究(2)引发了一项罕见的复现研究。复现失败(3,4)，随之而来的是更多的复现研究，发现复现失败不仅仅是单一事件(5-10)。研究人员对系统性复现失败的影响表示担忧，包括知识库的削弱、公众对科学的不信任以及资金削减(11-13)。2016年，自然杂志对1500名科学家的调查报告称，51%的受访者认为科学正在经历复现危机(14)。这一回应促使美国国防高级研究计划局在2018年创建了一个计划，研究社会科学领域复现失败的规模和范围(15-17)。</p>
<p>尽管人们越来越担心复现失败的问题，但手动复现研究的样本数量很少，只占总文献的一小部分(18,19)。<strong>在心理学这个进行复现研究最多的科学学科中，直接、独立的复现研究数量不到400个</strong>。此外，样本的选择不平衡，主要来自于选定作者或特定子领域的经典论文(20,21)。大多数复现研究来自社会心理学和认知心理学等子领域，导致有关发展心理学、临床心理学和教育心理学等领域是否存在相似的复现失败率的猜测，尽管缺乏子领域特定的分析(22,23)。</p>
<p>**为了扩大和丰富复现数据，研究人员开发了替代方法来估计论文复现成功的可能性(**24)。预测市场已成为估计论文可复现性的主要方法。它涉及让专家们打赌，是否会在未来的手动复现测试中成功复现已经发表的论文(25)。该方法的高准确性使预测市场成为估计论文可复现性的有效解决方案(16然而，尽管预测市场比手动复现更具规模化，但仍需要在多年时间内招募数千名专家评审，以预测大量论文的可复现性(17)。</p>
<p>机器学习方法也被开发出来预测复现结果。机器学习模型可以从研究的叙述文本(26)或研究的数值特征，如P值或研究的样本大小(27,28)来预测可复现性。这两种类型的模型都能够准确预测，与预测市场的预测准确度相当(26)。<strong>基于文本的模型量化论文中的叙述，包括研究设计的描述和结果的解释(29)，这些内容在仅基于数值特征的模型中无法捕捉。此外，文本量化可以自动化，从而使该技术比从手稿中手动提取数值特征更具可扩展性和可复现性</strong>。</p>
<p>本文采用基于文本的机器学习方法来预测心理学文献的可复现性。我们的样本包括六个主要子领域的顶级心理学期刊近20年发表的几乎所有论文：<strong>临床心理学、认知心理学、发展心理学、组织心理学、人格心理学和社会心理学</strong>。总共有 <strong>14,126</strong> 篇论文，由 <strong>26,349</strong> 位不同的作者和 <strong>6,173</strong> 个不同的机构撰写，共有 <strong>1,222,292</strong> 次引用和 <strong>27,447</strong> 次媒体提及。</p>
<p>我们的分析如下：我们首先简要描述了我们的基于文本的机器学习模型，该模型已经得到验证，并显示能够准确预测手动复现的结果(26)。然后，我们应用该模型来预测心理学文献的可复现性，并着眼于研究可复现性如何在心理学子领域、研究方法、出版前后的特征以及作者团队的专业知识和经验方面存在差异。</p>
<p><br><br></p>
<h2 id="四数据和方法">四、数据和方法</h2>
<p>我们的分析使用了多种不同的文献、作者和媒体报道数据来源。生成数据的数据和代码已经存放在开放科学框架(OSF)上(30)。表格1列出了用于分析的大量期刊出版物，包括五个专门子领域的期刊和一份综合期刊《Psychological Science》。</p>
<p><img loading="lazy" src="img/table1.png" alt=""  />
</p>
<p>所有论文均在2000年至2019年期间发表，并根据期刊的子领域专业化进行分类：</p>
<ul>
<li><strong>Journal of Abnormal Psychology</strong> (临床心理学)</li>
<li><strong>Journal of Experimental Psychology, Learning, Memory &amp; Cognition</strong> (认知心理学)</li>
<li><strong>Child Development</strong> (发展心理学)</li>
<li><strong>Journal of Applied Psychology</strong>（组织心理学）</li>
<li><strong>Journal of Personality and Social Psychology</strong>（社会心理学）</li>
</ul>
<p>这里有两个例外。首先，因为人格研究出现在所有顶级期刊中，如果论文标题或摘要中含有 “<strong>人格personality</strong>” 一词，无论它们出现在哪个期刊中，我们都将其标记为人格研究。其次，由于 <strong>Psychological Science</strong> 刊登来自各个子领域的作品，我们将其论文分类为主要发表在哪个子领域期刊上的论文的所属子领域。总样本包括14,126篇论文。所有数据均按照出版商的使用条款和英国版权法进行收集。</p>
<br>
<h3 id="41-机器学习模型">4.1 机器学习模型</h3>
<p><strong>机器学习模型使用了随机森林和逻辑回归模型的集成，基于论文的文本预测了论文的可复现性概率</strong>。该模型在之前的一篇文章中经过了严格的样本外测试，并被证明其准确度与预测市场相当(26)。具体而言，创建模型的步骤如下(请参见SI附录，图S2说明程序)：</p>
<ul>
<li>第1步，<strong>将英语单词转换为向量。我们使用 word2vec(31) 在Microsoft学术图(MAG)(32)的 2000万社会科学出版物摘要语料库上训练模型。目标是将单词与社会科学文献的上下文联系起来，并以一个200维向量的形式量化表示这种联系</strong>。</li>
<li>第2步，<strong>将出版物转换为向量</strong>。为此，我们将训练样本(Table 2)中每篇论文中每个单词的标准化频率乘以其相应的200维单词向量，从而产生一个表示论文文本内容的论文级向量。</li>
<li>第3步，<strong>使用随机森林和逻辑回归的集成从论文级向量中预测每篇论文的复现结果(通过/失败)</strong>。为确定一项研究是否复现，我们使用所有复现研究报告的共同指标——复现团队对研究是否复现的总结判断(“是”或“否”)。步骤1到步骤3共同创建了一个使用论文的文本/叙述来预测其可复现性概率的机器学习模型，我们称之为“<strong>复现得分”(replication score)</strong>。SI附录，补充文本3提供了所有程序的详细信息。<strong>注意，论文中使用的是监督学习算法， 训练数据集仅有386篇（314个实验论文+72个非实验研究）</strong></li>
</ul>
<br>
<h3 id="42-关于模型迁移学习是否会预测不准问题">4.2 关于模型迁移学习是否会预测不准问题</h3>
<p>论文评估了与迁移学习相关的问题。<strong>当模型在一个领域中开发并应用于另一个领域时，就会发生迁移学习</strong>(43)。我们的研究中出现了这种情况，因为预测样本包含了训练样本中没有的两个子领域——临床心理学和发展心理学。<strong>这两个子领域的手动复现研究很少，如果要积累足够的样本可能需要另一个十年时间(44)。这引发了一个担忧，即模型能否为临床心理学和发展心理学中的论文提供有效的估计</strong>。为了解决这个问题，我们遵循协议，进行了三个独立的鲁棒性测试(43, 45, 46)。</p>
<p>(i)我们使用现有的社会和认知心理学数据来模拟迁移学习过程，估计使用一个基于一个心理学子领域的手动复现数据训练的模型来预测另一个心理学子领域的复现失败的表现，并将模型的预测与预测子领域中的实际手动复现数据进行比较。具体而言，<strong>我们研究了仅基于社会心理学论文( n=256)开发的模型在认知心理学论文(n=90)上的表现</strong>。我们发现，这种<strong>迁移学习到认知心理学的表现(AUC=0.72)与将模型应用于社会心理学的表现基准(AUC=0.73)相当。这为心理学子领域之间的迁移学习成功提供了支持</strong>。</p>
<p>(ii)有人可能会认为文本模型从社会心理学到认知心理学的成功转移并不保证其能够成功转移到临床心理学或发展心理学。为回答这个问题，我们比较了各个子领域的主题和文本相似性。<strong>先前的机器学习研究表明，基于文本的模型的迁移学习在训练和应用领域的文本特征更相似时更成功</strong>(47)。因此，<strong>如果社会-临床和社会-发展的相似性与社会-认知相似或更高，我们可以期望该模型在临床或发展心理学中的效度与在认知心理学中一样</strong>。</p>
<p>为了衡量两个子领域之间的研究主题重叠程度，我们从MAG数据库中收集了测试样本中每篇论文的研究主题。为了衡量两个子领域之间的文本相似性，我们计算了余弦相似度和词移距离（WMD）。附录SI，补充文本3.4.1详细描述了这些方法。</p>
<p><strong>结果显示，临床心理学（57％）和发展心理学论文（56％）与社会心理学论文的主题重叠比认知论文（42％）更高</strong>。此外，所有三个子领域都与社会心理学表现出相等的文本相似度（余弦相似度=0.90至0.91，WMD=0.24至0.26）。因为分析（i）显示基于社会心理学构建的模型可迁移到认知心理学，我们现在可以期望该模型可迁移到临床心理学和发展心理学，因为在这些领域观察到与社会心理学更高的特征相似性。</p>
<p><strong>我们评估了在临床心理学或发展心理学论文中，预测的复现得分与样本大小和P值等可复现性的替代指标的一致性</strong>。这两个指标都是可靠性的指标，因为样本大小越大，P值越低，虚阳性的风险就越小（5, 48, 49）。我们强调，预测模型不包含任何关于样本大小和P值的信息，因为训练样本中的论文已经去除了所有数字或统计信息。因此，如果一篇论文的样本大小和P值与我们模型的复现预测相关，那么它将为模型在临床心理学或发展心理学中的适用性提供独立支持。</p>
<p>在程序上，我们手动编码了来自预测结果的临床心理学和发展心理学的一组随机研究。为了获取样本大小，我们从论文中提取了参与者的数量。如果一篇论文有多个研究，我们取所有研究的平均样本大小。为了获得P值，我们从摘要中找到了论文的第一个主要声明，并提取了与该主要声明相关联的测试的P值。主要声明通常以“结果显示”或“我们的分析表明”等短语开头。补充文本3.4.2提供了更多的方法细节。</p>
<p>结果显示，预测的复现得分与原始样本大小r(97)=0.31，P=0.002和原始P值r(91)=-0.42，P &lt;0.001的等级顺序相关。由于预测模型不包含样本大小和P值信息，因此结果不是自证的，为成功的转移学习到临床心理学和发展心理学提供了支持。</p>
<br>
<h3 id="43-评估可复现性前后出版的相关度量">4.3 评估可复现性前后出版的相关度量。</h3>
<p>为了检查复现概率与论文其他可观察的出版特征之间的联系，我们构建了几个关键的可观察特征的度量。例如，已经有假设认为复现结果与研究人员的专业知识（34）或论文的媒体关注度（50）有关。我们收集了五个度量来捕捉论文的特征，其中三个度量预测了作者团队的特征，而另外两个度量预测了读者对研究的反应。预发布特征包括论文的第一作者和高级作者的经验和能力，衡量方法是他们在发表研究论文之前的：1）发表的论文数量累计总数，2）引用影响力，3）机构声誉，基于第一作者和高级作者的大学在2021 QS世界大学排名中的排名。高级作者是指在焦点论文发表时具有最高累积引用的作者。发表后的特征包括焦点论文的4）引用计数和5）媒体提及次数。媒体提及次数由Altmetric (52)计算。所有其他度量均来自Dimensions (53)，Dimensions已经批准我们在此项目中使用这些数据。为了控制度量中的出版年龄和子领域差异（见SI Appendix，图S1），我们通过将观察得分除以其子领域和出版年份的平均值来对所有度量进行了标准化。SI附录，补充文本2介绍了有关这些度量及其如何标准化的更多详细信息。</p>
<p><br><br></p>
<h2 id="五结果">五、结果</h2>
<p>**使用上述校准的机器学习模型，我们预测了每篇文章在可复现性预测样本（n = 14,126）中的可复现性得分。该得分可以解释为可复现性成功的相对可能性。**换句话说，一个可复现性得分为0.80的论文比得分更低的论文更有可能复现，并且比可复现性得分为0.40的论文有两倍的复现概率。使用可复现性得分，我们进行了三组分析：</p>
<ul>
<li>首先，我们确定了心理学子领域在估计可复现性率方面的差异，弥合了以前手动复现实验的小样本缺陷；</li>
<li>其次，我们比较了实验和非实验研究设计之间的可复现性率；</li>
<li>第三，我们研究了可复现性与论文其他出版前和出版后特征之间的相关性。</li>
</ul>
<p><img loading="lazy" src="img/table2.png" alt=""  />
</p>
<p>图1显示了所有14,126篇心理学论文的可复现性得分分布（范围为0.10到0.86，平均值为0.42，中位数为0.41，标准差为0.15，偏度为0.31）。有几点发现值得注意：</p>
<ul>
<li>首先，该分布与手动复现实验的猜测和预测市场的最新预测大致一致（15）。手动复现实验表明，心理学论文中略多于不及格的论文（43%的总体成功率）。最近20年的心理学出版物的可复现性得分估计分布也显示出类似的模式。</li>
<li>其次，有人认为，心理学中对可复现性失败的关注已经提高了可复现性的严格性（54）。当我们绘制我们20年期间的平均可复现性得分时，发现可复现性得分相对稳定。平均可复现性得分从2000年到2010年约下降了10%，然后从2010年到2019年回升到大约与2000年相同的水平（SI Appendix，Fig.S6）——这一模式与观察到的改变研究实践可能已经提高了心理学的可复现性率的观察相一致（9, 21, 55）。</li>
<li>第三，我们发现，汇总心理学子领域的可复现性得分掩盖了重要的子领域差异。下面，我们将详细说明心理学子领域之间的可复现性率差异。</li>
</ul>
<p><br><br></p>
<h2 id="六讨论">六、讨论</h2>
<p>本研究使用机器学习模型量化科学手稿中的文本，以预测其复现概率。该模型使我们能够首次对心理学六个主要子领域杂志上发表的几乎所有论文进行复现普查，分析重点是估计整个学科的复现率，以及复现率如何因子领域、实验和非实验方法以及其他研究论文特征而异。为了保持基于人类专业知识的结果，我们在可能的情况下验证了结果与可用的手动复现数据一致。结果进一步提供了可以推进复现理论和实践的见解。</p>
<p>我们方法的一个核心优势是其规模和范围。先前关于复现失败程度的推测基于相对较小的、有选择性的手动复现样本(21)。我们分析了多个子领域的14,000多篇论文，发现复现成功率在子领域间存在广泛差异。因此，不可能用一个单一的复现失败率来表征多样的学科分支。此外，我们的结果显示，复现成功率的子领域差异与研究方法有关。我们发现，对于所有子领域，实验研究的复现率明显低于非实验方法，并且在较少进行实验的子领域中，复现率相对较高。这一发现令人担忧，因为心理学在实验方面的熟练程度是其强有力的科学声誉的一部分。</p>
<p>分析可复现性与研究论文的其他度量标准的关系时，我们发现，虽然可复现性与研究人员的经验和能力呈正相关，但作者的大学声望和论文的引用量等研究质量的其他代理变量与心理学的可复现性无关。这些发现强调了需要在评估研究和学者时对前-后出版度量标准保持谨慎的态度。</p>
<p>我们还将媒体关注度与论文的可复现性进行了相关分析。媒体在创造科学公众形象和推广知识方面扮演着重要角色，但它通常有动机报道那些反直觉、引人注目的结果。理想情况下，媒体报道与心理学研究的可复现性率应有正向关系（或者没有关系）。然而，我们发现，媒体对论文的报道与其复现成功的可能性存在负相关。因此，基于媒体报道来判断一篇论文的价值是不明智的。对于媒体来说，提醒公众新的、创新的科学研究结果只是引发思考，需要未来的复现实验来证实其健壮性，是很有价值的。 我们设想了两种可能的应用方向：</p>
<p>第一，机器学习模型可用于预测难以或无法进行手动复现的研究（例如纵向研究和特殊或难以访问的人群）。</p>
<p>第二，预测的复现分数可以开始帮助优先选择需要手动复现的某些研究，面对资源有限的情况。每年，个人学者和组织（如心理学科学加速器（67）和协作复现和教育项目（68））都会遇到一个问题：从众多的心理学研究中选择哪些进行复现。Isager等人（69）提出，为了最大化复现的收益，社区应该优先复现那些价值高、结果不确定的研究。研究的价值可以通过引用量或媒体关注度等因素来近似计算，但不确定性部分尚未得到大量文献的充分衡量。我们建议，我们的机器学习模型可以提供复现不确定性的定量测量。 我们注意到，我们的发现在几个方面存在限制：</p>
<ul>
<li>
<p>首先，我们对所有论文的预测都来自于顶级期刊。未来的研究可以检查来自较低排名期刊的论文，以及它们的可复现性如何与发表前后的指标相关联（70）。</p>
</li>
<li>
<p>其次，可复现性的估计仅是近似值。在子领域级别上，我们分析的六个子领域中有五个子领域仅由一种顶级期刊代表。单个期刊不能涵盖整个子领域的范围。</p>
</li>
</ul>
<p>未来的研究可以在以下几个方向展开：</p>
<ul>
<li>
<p>我们的复现得分可以与其他方法结合使用，例如预测市场（16）或非文本机器学习模型（27、28），以进一步精确估计心理学研究的可复现性；</p>
</li>
<li>
<p>可以重复设计本研究，以在其他学科中进行复现普查；</p>
</li>
<li>
<p>可将可复现性得分进一步与其他感兴趣的指标进行相关性分析。</p>
</li>
</ul>
<p>社会科学中的可复现性受到变异性的限制，它最终是一种由各种方法组合而成的集体企业。波普尔在他的书《科学发现的逻辑》中提出：“即使是我们自己的观察结果，我们也不会完全认真对待，或将其视为科学观察结果，直到我们对其进行了重复和测试”（1）。然而，尽管波普尔对于重复和可重复性的洞察是正确的，但必须认识到测试带来了探索的成本。机器学习方法与人类智慧的结合，是发展更好的可复现性理解的有效方法。这种组合平衡了测试成本和科学探索的收益。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Nature | 通用中英文六维语义情感词典</title>
      <link>https://textdata.cn/blog/2023-03-20-nature-six-semantic-dimension-database/</link>
      <pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-20-nature-six-semantic-dimension-database/</guid>
      <description>来自心理学和认知神经科学的证据表明，人类大脑的语义系统包含几个特定的子系统，每个子系统都代表语义信息的特定维度。对这些不同语义维度上的词语评分可以帮助研究语义维度对语言处理的行为和神经影响，并根据人类认知系统的语义空间建立语言含义的计算表示。现有的语义评分数据库提供了数百到数千个词语的评分，但这无法支持对自然文本或语音的全面语义分析。本文报告了一个大型数据库——六维语义数据库（SSDD， 后文「数据库」均用「词典」代替），其中包含对 17,940个常用汉语词语在六个主要语义维度上的主观评分：视觉、运动、社交、情感、时间和空间。此外，使用计算模型学习主观评分和词嵌入之间的映射关系，我们在SSDD中包括了1,427,992个汉语和1,515,633个英语词语的估计语义评分。SSDD将有助于自然语言处理、文本分析和大脑中的语义表示研究。</description>
      <content:encoded><![CDATA[<h2 id="应用价值">应用价值</h2>
<p>对于大量散落在网络中的文本数据， 可以度量用户在视觉、运动、社交、情感、时间和空间等维度上心理、认知、抽象层面的信息。</p>
<br>
<p>Wang, S., Zhang, Y., Shi, W. et al. A large dataset of semantic ratings and its computational extension. Sci Data 10, 106 (2023). <a href="https://doi.org/10.1038/s41597-023-01995-6">https://doi.org/10.1038/s41597-023-01995-6</a></p>
<p><img loading="lazy" src="img/cover.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="一摘要">一、摘要</h2>
<p>来自心理学和认知神经科学的证据表明，人类大脑的语义系统包含几个特定的子系统，每个子系统都代表语义信息的特定维度。对这些不同语义维度上的词语评分可以帮助研究语义维度对语言处理的行为和神经影响，并根据人类认知系统的语义空间建立语言含义的计算表示。现有的语义评分数据库提供了数百到数千个词语的评分，但这无法支持对自然文本或语音的全面语义分析。本文报告了一个大型数据库——<strong>六维语义数据库</strong>（SSDD， 后文「数据库」均用「词典」代替），其中包含对 <strong>17,940</strong> 个常用汉语词语在六个主要语义维度上的主观评分：<strong>视觉、运动、社交、情感、时间和空间</strong>。此外，使用计算模型学习主观评分和词嵌入之间的映射关系，我们在SSDD中包括了1,427,992个汉语和1,515,633个英语词语的估计语义评分。SSDD将有助于自然语言处理、文本分析和大脑中的语义表示研究。</p>
<p><br><br></p>
<h2 id="二背景">二、背景</h2>
<p>大量行为和神经证据表明，单词语义表示分布在不同的神经子系统中， 每个子系统代表着特定的语义信息维度。这些语义子系统和维度为人类语义系统的组织提供了重要线索。为了研究单词在人脑中的 <strong>意义表示</strong> 和 <strong>信息加工</strong>, 许多研究基于心理和神经生物学可行的语义维度，通过人员标准构建了单词的词典。与现有NLP领域的embeddings相比， 基于特定意义维度所构建的词典可以在经验语义纬度上提供量化的评分， 使得研究者可以调查语义维度对语言处理的行为和神经影响， 并建立语言含义的(表示)计算。</p>
<p>然而现有的词典只包含数百、数钱个词，不足以支持自然文本或语音的全面语义分析。该研究提供了一个大型的语义评分语义词典， 名为<strong>六维语义</strong>（Six Semantic Dimension Database，SSDD）词典， <strong>每个中英文词语含六个维度的得分(可以理解为效价valence),分别是视觉、运动、社交、情感、时间、空间</strong>。 其中视觉和运动维度反映了感觉运动体验对语义表示的影响。感觉和运动维度可能是最常研究的语义维度之一，它们对于对象和动作概念的重要性已经得到了很好的确认。在与语义表示相关的多个感官维度中，我们选择了视觉维度，因为视觉是主导的感觉模态。视觉和运动语义对认知处理的行为和神经影响已经被许多研究证实。社交和情感维度反映了社会情感体验对语义表示的影响。<strong>这些维度具有可分离的神经相关性，并且对于心理和抽象概念的表示尤其重要</strong>。Huth等人采用数据驱动方法研究了大脑中的语义表示组织，并发现社交情感和感官运动语义与最重要的数据驱动语义维度的两端相关联。因此，社交和情感维度可以作为视觉和运动维度的重要补充，以反映语义表示。时间和空间维度对于事件和情境的表示尤为重要。神经心理学和神经影像研究也表明这些维度具有可分离的神经相关性。Binder等人对经验语义属性进行了全面的综述，反映了六个维度的代表性。Binder等人总结了属于14个领域的65个语义维度，其中超过2/3的维度属于视觉、运动、社交、情感、时间和空间领域。SSDD将这六个领域作为粗粒度语义维度，并为每个维度提供了一般评分。</p>
<p><br><br></p>
<h2 id="三构建方法">三、构建方法</h2>
<h3 id="31-标准者被试人员">3.1 标准者(被试人员)</h3>
<p>该研究找了85位心理、神经都正常的本硕中国学生， 通过数据质量评估，最终保留了80位学生的数据标注结果。</p>
<br>
<h3 id="32-待标注的17940中文词">3.2 待标注的17940中文词</h3>
<p>待标注中文词，一共有17940个， 是由三种数据源筛选得来</p>
<ol>
<li>中文维基百科12814高频词</li>
<li>fMRI领域研究(发表&amp;未发表)的4915个中文词</li>
<li>最后一组项目是来自Binder等人和Tamir等人的语义评分实验中英文刺激词的211个汉语翻译。</li>
</ol>
<br>
<h3 id="33-标注过程">3.3 标注过程</h3>
<ul>
<li>该研究对17980个词进行了6个标注实验的， 每个实验聚焦于一个语义维度（视觉、运动、社交、情感、时间、空间）。</li>
<li>每个标注实验会分成18个session，每个session含1000个词(最后一个session有940个词)</li>
<li>标准过程使用问卷星</li>
<li>情绪维度标注的时候，使用13-point scale标准标注(-6表示非常负面， 0表示中性， 6表示非常积极)</li>
<li>剩下的5维（视觉、运动、社交、时间、空间）使用7-point scale标准标注(1表示非常低， 7表示非常高)</li>
<li>每次标注前， 标注者需要阅读标注指南，指南会含有一些语义例子。</li>
<li>为了控制标准数据质量， 保证每位标注者与所有标准者的相关性大于0.5，最终拒绝了28个session大概0.87%的数据量。</li>
</ul>
<br>
<h3 id="34-扩充词典">3.4 扩充词典</h3>
<p>标准的17940个中文词的六维度数据，可以认为是标准数据。用机器学习方法，想办法扩充词典。</p>
<p>该团队检验了语义上下文不敏感的词嵌入算法(word2vec/Glove)和 对上下文语义敏感的嵌入算法(GPT2, BERT ERNIE, and MacBERT) ，让这6类嵌入模型分别预测， 确定下表现效果较好的Word2vec和MacBERT算法。</p>
<p>使用Word2vec和MacBERT预测剩下所有的中文词，共扩展出1427992个中文词。</p>
<p>人类在视觉、运动、社交、情感、时间、空间六个维度上是共通的，结合语言嵌入模型可以在不同语言中进行语义空间对齐，该研究根据英文嵌入语言模型，也预测出了1,515,633个词。</p>
<br>
<br>
<h2 id="四ssdd">四、SSDD</h2>
<p>SSDD包含两个数据集：</p>
<ul>
<li>第一个是17,940个常用汉语词语在六个语义维度上的主观评分。</li>
<li>第二个是主观评分数据的计算扩展。我们将主观评分与计算模型相结合，然后估算出1,427,992个汉语和1,515,633个英语单词的语义评分。</li>
</ul>
<p>该研究标准、训练的源代码数据均已开源，https://osf.io/n5vke/</p>
<p><img loading="lazy" src="img/osf.png" alt=""  />
</p>
<p>由于数据量太大， 这里只给大家读取并显示17940个标注的6维语义数据, 其实对于经管社科研究， 标注的 <a href="Rated_semantic_dimensions.csv">17940个词</a> 已经是很大的情感词典了。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Rated_semantic_dimensions.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#词汇量</span>
<span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">17940
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 计算相关性矩阵</span>
<span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">corr_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">mark</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">cell</span><span class="p">:</span> <span class="s1">&#39;background-color: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="s1">&#39;red&#39;</span> <span class="k">if</span> <span class="n">cell</span> <span class="o">&gt;</span> <span class="mf">0.4</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
<span class="n">color_matrix</span> <span class="o">=</span> <span class="n">corr_df</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="n">mark</span><span class="p">)</span>
<span class="n">color_matrix</span>
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<p>相关性最高的单元格是Motor与Vision， 6个维度相关性均小于0.5 ， 六维的选择是很合理。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>正念爱好者的推特语言风格有何不同</title>
      <link>https://textdata.cn/blog/2023-03-19-mindfulness-influences-linguistic-markers/</link>
      <pubDate>Mon, 20 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-19-mindfulness-influences-linguistic-markers/</guid>
      <description>在《逻辑哲学论》中，维特根斯坦（1961）著名地写道：我的语言的限制意味着我的世界的限制（The limits of my language mean the limits of my world!）”（第23页）。作为表达思想和感情的符号系统，语言和词语揭示了我们的重要信息。我们如何表达自己反映了我们是谁，我们如何感受，我们如何处理信息以及我们关心什么。例如，经历积极情绪的人使用更多的积极情感词汇和感叹号（Hancock等人，2007），而那些处于痛苦中的人则倾向于关注自己并使用更多的第一人称单数代词（Rude等人，2004）。因此，语言和词语的研究可以帮助理解人类心理。</description>
      <content:encoded><![CDATA[<p>在《逻辑哲学论》中，维特根斯坦（1961）著名地写道：<strong>我的语言的限制意味着我的世界的限制（The limits of my language mean the limits of my world!）</strong>”（第23页）。作为表达思想和感情的符号系统，语言和词语揭示了我们的重要信息。我们如何表达自己反映了我们是谁，我们如何感受，我们如何处理信息以及我们关心什么。例如，经历积极情绪的人使用更多的积极情感词汇和感叹号（Hancock等人，2007），而那些处于痛苦中的人则倾向于关注自己并使用更多的第一人称单数代词（Rude等人，2004）。<strong>因此，语言和词语的研究可以帮助理解人类心理。</strong></p>
<br>
<h2 id="文献">文献</h2>
<p>Rivera, C.E., Kaunhoven, R.J. &amp; Griffith, G.M. How an Interest in Mindfulness Influences Linguistic Markers in Online Microblogging Discourse. <em>Mindfulness</em> (2023). <a href="https://doi.org/10.1007/s12671-023-02098-4">https://doi.org/10.1007/s12671-023-02098-4</a></p>
<p><a href="mindfulness-Influences-Linguistic-Markers.pdf">下载本文</a></p>
<p><img loading="lazy" src="img/mindfulness-Influences-Linguistic-Markers.png" alt=""  />
</p>
<br>
<h2 id="摘要">摘要</h2>
<p>这项研究旨在探究在Twitter上对正念（mindfulness）感兴趣的人们是否在他们的推文中使用了与其他人不同的语言。将用户分为正念组和非正念组，含19732位推特用户的1873万条推文。作者使用<strong>LIWC</strong>（Linguistic Inquiry and Word Count, LIWC）来查看推文中与<strong>正念、情感过程、社交取向和“存在模式”相关的语言标记</strong>。研究发现，对正念感兴趣的人在Twitter上更频繁地使用与正念、积极情感、幸福和社交取向相关的语言标记，并且使用与消极情感、过去、现在和未来焦点、家庭和友谊取向相关的标记更少。这表明，<strong>推特上对正念感兴趣的人使用了特定的语言标记</strong>，这可以为开发更自然的评估正念的方法提供可能的途径。</p>
<br>
<h2 id="数据">数据</h2>
<p>19732位推特用户的1873万条推文。构建研究数据的方法</p>
<ul>
<li>正念爱好组数据
<ul>
<li>找正念的大V，获取大V的粉丝列表fans_list</li>
<li>twitter api每15分有900次访问权限，最终获得 10,347 粉丝的个人数据</li>
</ul>
</li>
<li>控制组用户数据，从数<strong>Kaggle公开</strong>的 <a href="https://www.kaggle.com/datasets/kazanova/sentiment140">Sentiment140</a> 数据集随机抽取9385位用户的数据</li>
</ul>
<br>
<h2 id="分析">分析</h2>
<p>作者主要比较了正念爱好者、非正念爱好者，两组群体推特以下  5 个角度测量语言风格，测试是否正念与语言风格是否有显著性(0.001的显著性水平)</p>
<ul>
<li>
<p><strong>正念</strong> ，例如：curious、open、accepting。</p>
</li>
<li>
<p><strong>情感过程</strong>   happy, sadly, yay
- 快乐，例如：nice、nasty。<br>
- 积极情绪，例如：sweet。<br>
- 消极情绪，例如：hurt、ugly。</p>
</li>
<li>
<p><strong>社交定向</strong>   interact, talk, everyone
- 家庭定向，例如：daugher、dad、aunt</p>
<ul>
<li>朋友定向，例如：buddy、neighbour</li>
</ul>
</li>
<li>
<p><strong>思维模式</strong> does had will</p>
<ul>
<li>聚焦于现在，例如：I&rsquo;m, now, use</li>
<li>聚焦于过去，例如：were、walked, had。</li>
<li>聚焦于将来，例如：will, going to</li>
</ul>
</li>
<li>
<p><strong>时间</strong> 例如：since、wait、clock。</p>
</li>
</ul>
<p><img loading="lazy" src="img/table2.png" alt=""  />
</p>
<p><img loading="lazy" src="img/table3.png" alt=""  />
</p>
<p>研究发现，对正念感兴趣的人在Twitter上更频繁地使用与正念、积极情感、幸福和社交取向相关的语言标记，并且使用与消极情感、过去、现在和未来焦点、家庭和友谊取向相关的标记更少。</p>
<br>
<h2 id="关于liwc">关于LIWC</h2>
<p>LIWC(Linguistic Inquiry and Word Count, LIWC)，其实就是词典法文本分析，统计文本中体现构念信息的相关词语出现次数多寡或频繁的程度。之前分享过 <a href="https://textdata.cn/blog/liwc_python_text_mining/">LIWC vs Python  | 文本分析之词典词频法略讲(含代码)</a> ，感兴趣可以阅读一下， 更好的了解 LIWC 和 Python 。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 历史文本中的语言积极性反映了动态的环境和心理因素(含Python代码)</title>
      <link>https://textdata.cn/blog/2023-03-13-linguistic-positivity-in-historical-texts-reflects-dynamic-environmental-and-psychological-factors/</link>
      <pubDate>Mon, 13 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-13-linguistic-positivity-in-historical-texts-reflects-dynamic-environmental-and-psychological-factors/</guid>
      <description>Linguistic positivity in historical texts reflects dynamic environmental and psychological factors历史文本中的语言积极性反映了动态的环境和心理因素</description>
      <content:encoded><![CDATA[<p>分享一篇研究语言积极倾向的论文，论文中使用Python工具进行词频统计，大大降低了人工成本，提高了科研效率。</p>
<p>该论文发表在PNAS：</p>
<blockquote>
<p>Iliev, R., Hoover, J., Dehghani, M. and Axelrod, R., 2016. <strong>Linguistic positivity in historical texts reflects dynamic environmental and psychological factors</strong>. <em>Proceedings of the National Academy of Sciences</em>, <em>113</em>(49), pp.E7871-E7879.</p>
</blockquote>
<p><br><br></p>
<h2 id="一研究背景">一、研究背景</h2>
<p>作者指出：当前关于 <strong>LPB(language positivity bias)作用机制</strong> 的驱动因素有普遍的普遍的认知偏差、情感状态、客观环境和社会规范，但是究竟是哪种机制驱动LPB，以及这种影响是否可能是由于这些或其他因素子集之间的相互作用驱动的，尚无研究讨论。造成这种不确定性的一个原因是，以前对 LPB 的调查采用了语言的共时方法，并且无法提供任何洞察力来了解 LPB 在给定语言中是否或在何种程度上具有跨时间和上下文的稳定。在该研究中，作者采用了一种将 LPB 视为动态现象的方法。具体来说，使用两个带时间戳的美国英语语料库，研究 LPB 中的经度变化作为主观、客观和社会因素的函数。这种方法可以研究 LPB 的历时变化，这是一个未探索的影响维度，而且还可以在先前提出的 LPB 解释之间进行裁决。</p>
<p><br><br></p>
<h2 id="二研究过程">二、研究过程</h2>
<p>提出五个假设：</p>
<ol>
<li>
<p>LPB没有线性趋势</p>
</li>
<li>
<p>LPB 随着时间的推移而增加。</p>
</li>
<li>
<p>LPB 随着时间的推移而减少。</p>
</li>
<li>
<p>LPB 的变化来预测，环境恶化将与LPB的降低相关联。</p>
</li>
<li>
<p>LPB的变化将通过集体影响的变化来预测，因此国家层面幸福感的下降将与LPB的下降相关。</p>
</li>
</ol>
<p>开展四个研究：</p>
<ol>
<li>
<p>线性趋势</p>
</li>
<li>
<p>战争伤亡人数</p>
</li>
<li>
<p>经济苦难</p>
</li>
<li>
<p>主观幸福</p>
</li>
</ol>
<p><br><br></p>
<h2 id="三数据来源">三、数据来源</h2>
<ol>
<li><strong>情感词典</strong>：使用了语言查询和字数统计 (LIWC) 词典 (56) 中的正面和负面情绪词类别，其中包含 907 个词和词干。正面类别有 408 个条目，负面类别有 499 个条目。</li>
<li><strong>战争伤亡人数</strong>：使用了来自美国退伍军人事务部 (57) 的情况说明书中的数据，并计算了过去两个世纪美国参与的战争中美国军人的平均伤亡人数。</li>
<li><strong>痛苦指数</strong>：使用了来自 <a href="http://www.miseryindex.us">www.miseryindex.us</a> 的数据，其中包含 1948 年至 2015 年期间美国的苦难指数指标。</li>
<li><strong>幸福指数</strong>：使用了世界幸福数据库中有关美国幸福感的调查数据。</li>
</ol>
<p><br><br></p>
<h2 id="四研究发现">四、研究发现：</h2>
<ol>
<li>美式英语中情感词的使用随着时间的推移而减少。</li>
<li>发现了 LPB 纵向下降趋势的令人信服的证据。这种趋势在谷歌 Ngrams 语料库中非常强烈，在纽约时报语料库中略显重要。</li>
<li>发现 LPB 也随着战争的伤亡人数而减少。</li>
<li>LPB 可以通过客观环境的不太极端的测量来预测。在控制时间后，我们发现痛苦指数较高的年份在两个语料库中的 LPB 水平往往较低。</li>
<li>发现 LPB 的短期波动随全国幸福指数的变化而变化。</li>
<li>这些结果进一步证实了LPB 不能简单地解释为普遍认知机制的功能。</li>
</ol>
<p><br><br></p>
<h2 id="五代码">五、代码</h2>
<p>为简化学习难度，论文细节部分不展开。本文主要给大家展示用Python做情感词频统计、词频历时折线图这两部分内容。先安装需要的包</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">
!pip3 install pyecharts==1.6.2
!pip3 install pyecharts-javascripthon==0.0.6              
!pip3 install pyecharts-jupyter-installer==0.0.3              
!pip3 install pyecharts-snapshot==0.2.0 
</code></pre></div><br>
<h3 id="51-实验数据集">5.1 实验数据集</h3>
<p>美国政治在其年轻的生命周期中经历了许多意识形态的动荡。这些动荡被称为政治时代。“政治时代是指在历史和政治科学中使用的一种美国政治模式，用于分类存在于美国的政党制度。” 数据集作者创建这个数据集的原因是为了正确讨论过去的政治事件，了解各个时期的政治平台和氛围是很重要的。例如，民主党和共和党的政治理念在不同的时期发生了巨大的变化，党派之间的理念也在不断转变、涌现和消失。截至目前为止，美国政治中已经公认了6个政治时代，分别是：</p>
<ol>
<li>第一党派制度（1792年至1824年）</li>
<li>第二党派制度（1828年至1854年）</li>
<li>第三党派制度（1854年至1895年）</li>
<li>第四党派制度（1896年至1932年）</li>
<li>第五党派制度（1932年至1964年）</li>
<li>第六党派制度（1964年至今）</li>
</ol>
<p>我将总统同一个年度的演讲汇总到一起，最终得到  <a href="yearly_american_speech_dataset.csv">yearly_american_speech_dataset.csv</a> 。
<br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;yearly_american_speech_dataset.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<h3 id="52-小实验">5.2 小实验</h3>
<p>写代码讲究先把任务抽象化， 将大问题拆解成可组装的小问题。即先小后大，先局部后整体。这里推荐用Python中的cntext库，该库文档清晰，代码简洁。先安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">!pip3 install scikit-learn==1.0
!pip3 install cntext==1.8.4
</code></pre></div><br>
<p>最简单的情感分析，即分析一句话 text 的正负面情感词出现的词频。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;What a sunny day!&#39;</span>

<span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;sunny&#39;</span><span class="p">,</span> <span class="s1">&#39;good&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;bad&#39;</span><span class="p">,</span> <span class="s1">&#39;terrible&#39;</span><span class="p">]}</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>{'pos_num': 1,
 'neg_num': 0,
 'stopword_num': 1,
 'word_num': 5,
 'sentence_num': 1}
</code></pre>
<br>
<p>根据自定义的词典diciton， 可以看出 <strong>What a sunny day!</strong> 中有1个pos类词，0个neg类词。</p>
<br>
<h3 id="53-找个公开的情感词典">5.3 找个公开的情感词典</h3>
<p>计算文本中正负面情感词出现次数， 需要有情感词词表。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看cntext内置的词典</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><pre><code>['DUTIR.pkl',
 'HOWNET.pkl',
 'Chinese_Loughran_McDonald_Financial_Sentiment.pkl',
 'sentiws.pkl',
 'ChineseFinancialFormalUnformalSentiment.pkl',
 'Chinese_Digitalization.pkl',
 'ANEW.pkl',
 'LSD2015.pkl',
 'NRC.pkl',
 'geninqposneg.pkl',
 'HuLiu.pkl',
 'Loughran_McDonald_Financial_Sentiment.pkl',
 'AFINN.pkl',
 'ADV_CONJ.pkl',
 'STOPWORDS.pkl',
 'Concreteness.pkl',
 'ChineseEmoBank.pkl']
</code></pre>
<p>查看NRC词典的信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#NRC词典</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NRC词典描述: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;Desc&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NRC参考文献: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;Refer&#39;</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;NRC词典内词表有: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Pos词表前10个词: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;positive&#39;</span><span class="p">][:</span><span class="mi">10</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Neg词表前10个词: &#39;</span><span class="p">,</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;negative&#39;</span><span class="p">][:</span><span class="mi">10</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>NRC词典描述:  The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing.

NRC参考文献:  Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

NRC词典内词表有:  dict_keys(['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust'])

Pos词表前10个词:  ['abba', 'ability', 'abovementioned', 'absolute', 'absolution', 'absorbed', 'abundance', 'abundant', 'academic', 'academy']

Neg词表前10个词:  ['abandon', 'abandoned', 'abandonment', 'abduction', 'aberrant', 'aberration', 'abhor', 'abhorrent', 'abject', 'abnormal']
</code></pre>
<br>
<h3 id="54-情感词频统计">5.4 情感词频统计</h3>
<p>使用cntext设计一个函数，将计算得到的文本词数/正(负)面词出现次数, 得到情感词在文中的占比，方便后续的可视化绘图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">emotion_analysis</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;positive&#39;</span><span class="p">],</span>
               <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;negative&#39;</span><span class="p">],</span>
               <span class="s1">&#39;anger&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;anger&#39;</span><span class="p">],</span>
               <span class="s1">&#39;anticipation&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;anticipation&#39;</span><span class="p">],</span>
               <span class="s1">&#39;disgust&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;disgust&#39;</span><span class="p">],</span>
               <span class="s1">&#39;fear&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;fear&#39;</span><span class="p">],</span>
               <span class="s1">&#39;joy&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;joy&#39;</span><span class="p">],</span>
               <span class="s1">&#39;sadness&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;sadness&#39;</span><span class="p">],</span>
               <span class="s1">&#39;surprise&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;surprise&#39;</span><span class="p">],</span>
               <span class="s1">&#39;trust&#39;</span><span class="p">:</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;NRC.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;NRC&#39;</span><span class="p">][</span><span class="s1">&#39;trust&#39;</span><span class="p">]}</span>
    
    <span class="n">res</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
                       <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span>
                       <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>


<span class="c1">#实验ok</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;What a sunny day!&#39;</span>
<span class="n">emotion_analysis</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>pos_num             1
neg_num             0
anger_num           0
anticipation_num    1
disgust_num         0
fear_num            0
joy_num             1
sadness_num         0
surprise_num        1
trust_num           0
stopword_num        1
word_num            5
sentence_num        1
dtype: int64
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Transcript&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">emotion_analysis</span><span class="p">)</span>
<span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
<br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div><p>Run</p>
<pre><code>Index(['pos_num', 'neg_num', 'anger_num', 'anticipation_num', 'disgust_num',
       'fear_num', 'joy_num', 'sadness_num', 'surprise_num', 'trust_num',
       'stopword_num', 'word_num', 'sentence_num'],
      dtype='object')
</code></pre>
<br>
<p>计算情感、情绪的词频(占比)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;pos_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">grouped_df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;neg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;neg_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;anger&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;anger_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;anticipation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;anticipation_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;disgust&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;disgust_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;fear&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;fear_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;joy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;joy_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;sadness&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;sadness_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;surprise&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;surprise_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;trust&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;trust_num&#39;</span><span class="p">]</span><span class="o">/</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;word_num&#39;</span><span class="p">]</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;SentiScore&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;pos_num&#39;</span><span class="p">]</span><span class="o">-</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;neg_num&#39;</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;pos_num&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;neg_num&#39;</span><span class="p">])</span>
</code></pre></div><br>
<p>合并数据数据</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">res_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">df2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">res_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="六可视化">六、可视化</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">res_df</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div><p>Run</p>
<pre><code>Index(['pos_num', 'neg_num', 'anger_num', 'anticipation_num', 'disgust_num',
       'fear_num', 'joy_num', 'sadness_num', 'surprise_num', 'trust_num',
       'stopword_num', 'word_num', 'sentence_num', 'pos', 'neg', 'anger',
       'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise',
       'trust'],
      dtype='object')
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pyecharts</span> <span class="kn">import</span> <span class="n">options</span> <span class="k">as</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">Line</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>


<span class="c1"># 创建折线图对象</span>
<span class="n">line_chart1</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>


<span class="c1"># 添加 x 轴和 y 轴数据</span>
<span class="n">line_chart1</span><span class="o">.</span><span class="n">add_xaxis</span><span class="p">(</span><span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">line_chart1</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;Sentiment Score&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;SentiScore&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                     <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>


<span class="c1"># 配置图表选项</span>
<span class="n">line_chart1</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
    <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;美国总统演讲情感得分年度历时可视化&#34;</span><span class="p">),</span>
    <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">trigger</span><span class="o">=</span><span class="s2">&#34;axis&#34;</span><span class="p">,</span> <span class="n">axis_pointer_type</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">),</span>
    <span class="n">legend_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">LegendOpts</span><span class="p">(</span><span class="n">pos_right</span><span class="o">=</span><span class="s2">&#34;right&#34;</span><span class="p">,</span>
                                <span class="n">orient</span><span class="o">=</span><span class="s2">&#34;vertical&#34;</span><span class="p">,</span>
                                <span class="n">pos_top</span><span class="o">=</span><span class="s2">&#34;center&#34;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 显示图表</span>
<span class="n">line_chart1</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div><p><a href="line_chart1.html"><img loading="lazy" src="img/%e5%9b%be1.png" alt=""  />
</a></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pyecharts</span> <span class="kn">import</span> <span class="n">options</span> <span class="k">as</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">Line</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>


<span class="c1"># 创建折线图对象</span>
<span class="n">line_chart2</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>


<span class="c1"># 添加 x 轴和 y 轴数据</span>
<span class="n">line_chart2</span><span class="o">.</span><span class="n">add_xaxis</span><span class="p">(</span><span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="n">line_chart2</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;Positive&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;pos&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart2</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;Negative&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;neg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># 配置图表选项</span>
<span class="n">line_chart2</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
    <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;美国总统演讲正、负面情感年度历时可视化&#34;</span><span class="p">),</span>
    <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">trigger</span><span class="o">=</span><span class="s2">&#34;axis&#34;</span><span class="p">,</span> <span class="n">axis_pointer_type</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">),</span>
    <span class="n">legend_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">LegendOpts</span><span class="p">(</span><span class="n">pos_right</span><span class="o">=</span><span class="s2">&#34;right&#34;</span><span class="p">,</span>
                                <span class="n">orient</span><span class="o">=</span><span class="s2">&#34;vertical&#34;</span><span class="p">,</span>
                                <span class="n">pos_top</span><span class="o">=</span><span class="s2">&#34;center&#34;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 显示图表</span>
<span class="n">line_chart2</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div><p><a href="line_chart2.html"><img loading="lazy" src="img/%e5%9b%be2.png" alt=""  />
</a><br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pyecharts</span> <span class="kn">import</span> <span class="n">options</span> <span class="k">as</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">Line</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>


<span class="c1"># 创建折线图对象</span>
<span class="n">line_chart3</span> <span class="o">=</span> <span class="n">Line</span><span class="p">()</span>

<span class="c1"># 添加 x 轴和 y 轴数据</span>
<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_xaxis</span><span class="p">(</span><span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;Date&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;anger&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;anger&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;anticipation&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;anticipation&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;disgust&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;disgust&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;fear&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;fear&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;joy&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;joy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;sadness&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;sadness&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;surprise&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;surprise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="n">line_chart3</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s1">&#39;trust&#39;</span><span class="p">,</span> 
                      <span class="n">res_df</span><span class="p">[</span><span class="s1">&#39;trust&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                      <span class="n">itemstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">ItemStyleOpts</span><span class="p">(</span><span class="n">opacity</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>


<span class="c1"># 配置图表选项</span>
<span class="n">line_chart3</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
    <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;美国总统8类情绪词用量年度历时可视化&#34;</span><span class="p">),</span>
    <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">trigger</span><span class="o">=</span><span class="s2">&#34;axis&#34;</span><span class="p">,</span> <span class="n">axis_pointer_type</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">),</span>
    <span class="n">legend_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">LegendOpts</span><span class="p">(</span><span class="n">pos_right</span><span class="o">=</span><span class="s2">&#34;right&#34;</span><span class="p">,</span>
                                <span class="n">orient</span><span class="o">=</span><span class="s2">&#34;vertical&#34;</span><span class="p">,</span>
                                <span class="n">pos_top</span><span class="o">=</span><span class="s2">&#34;center&#34;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># 显示图表</span>
<span class="n">line_chart3</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div><p><a href="line_chart3.html"><img loading="lazy" src="img/%e5%9b%be3.png" alt=""  />
</a>
<a href="line_chart3.html"><img loading="lazy" src="img/%e5%9b%be4.png" alt=""  />
</a></p>
<p>最后一张图中trust指标在08年前后几年是下降趋势，可能的原因是， 那个阶段正是08年金融危机，美国政府为了救华尔街， 用公民腰包里的钱补贴华尔街金融巨鳄。信任下降。</p>
<p><br><br></p>
<h2 id="七保存">七、保存</h2>
<p>情感计算过程得到的 res_df 和 可视化结果line_chart 建议都保存起来， 方便下次可以快速进入可视化阶段。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">res_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;sentiment_anlysis_result.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">line_chart1</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;line_chart1.html&#39;</span><span class="p">)</span>
<span class="n">line_chart2</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;line_chart2.html&#39;</span><span class="p">)</span>
<span class="n">line_chart3</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;line_chart3.html&#39;</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="资料下载">资料下载</h2>
<ul>
<li><a href="code.ipynb">代码</a></li>
<li><a href="yearly_american_speech_dataset.csv">数据</a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>基于词嵌入技术的心理学研究: 方法及应用</title>
      <link>https://textdata.cn/blog/2023-03-10-psychological-research-with-word-embeddings/</link>
      <pubDate>Fri, 10 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-10-psychological-research-with-word-embeddings/</guid>
      <description>词嵌入是自然语言处理的一项基础技术。 其核心理念是根据大规模语料中词语和上下文的联系, 使用神经网络等机器学习算法自动提取有限维度的语义特征, 将每个词表示为一个低维稠密的数值向量(词向 量), 以用于后续分析。 心理学研究中, 词向量及其衍生的各种语义联系指标可用于探究人类的语义加工、认知判断、发散思维、社会偏见与刻板印象、社会与文化心理变迁等各类问题。 未来, 基于词嵌入技术的心理 学研究需要区分心理的内隐和外显成分, 深化拓展动态词向量和大型预训练语言模型(如 GPT、BERT)的应用, 并在时间和空间维度建立细粒度词向量数据库, 更多开展基于词嵌入的社会变迁和跨文化研究。 As a fundamental technique in natural language processing (NLP), word embedding quantifies a word as a low-dimensional, dense, and continuous numeric vector (i.e., word vector). Word embeddings can be obtained by using machine learning algorithms such as neural networks to predict the surrounding words given a word or vice versa (Word2Vec and FastText) or by predicting the probability of co-occurrence of multiple words (GloVe) in large-scale text corpora. Theoretically, the dimensions of a word vector reflect the pattern of how the word can be predicted in contexts; however, they also connote substantial semantic information of the word. Therefore, word embeddings can be used to analyze semantic meanings of text. In recent years, word embeddings have been increasingly applied to study human psychology, including human semantic processing, cognitive judgment, divergent thinking, social biases and stereotypes, and sociocultural changes at the societal or population level. Future research using word embeddings should (1) distinguish between implicit and explicit components of social cognition, (2) train fine-grained word vectors in terms of time and region to facilitate cross-temporal and cross-cultural research, and (3) apply contextualized word embeddings and large pre-trained language models such as GPT and BERT. To enhance the application of word embeddings in psychology。</description>
      <content:encoded><![CDATA[<p><a href="https://psychbruce.github.io/">包寒吴霜博客 https://psychbruce.github.io/</a></p>
<p><img loading="lazy" src="img/%e5%8c%85%e5%af%92%e5%90%b4%e9%9c%9c.png" alt=""  />
</p>
<br>
<p><img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-01.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-02.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-03.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-04.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-05.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-06.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-07.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-08.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-09.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-10.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-11.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-12.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-13.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>金融研究 | 使用Python构建「关键审计事项信息含量」</title>
      <link>https://textdata.cn/blog/2023-01-13-information-content-of-critical-audit/</link>
      <pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-01-13-information-content-of-critical-audit/</guid>
      <description>关键审计事项是来自审计师视角的信息， 其蕴含的特质性信息对实现沟通价值至关重要。本文采用文本 分析方法计算的文本相似度衡量关键审计事项特质性信息含量，考察其对公司债券发行定价的影响。 结果发现， 以较低文本相似度代表的较高关键审计事项信息含量能够降低公司债券发行定价。 较高的审计师专业胜任能力 和独立性能够增强关键审计事项信息含量对公司债券发行定价的降低作用。 信息不对称的缓解是关键审计事项 信息含量降低公司债券发行定价的具体影响渠道。考虑关键审计事项类型后发现， 关联交易类关键审计事项信 息含量对公司债券发行定价的降低作用更强。本文研究结论有助于未来改进关键审计事项的披露要求。</description>
      <content:encoded><![CDATA[<p>宋建波,冯晓晴.关键审计事项信息含量与公司债券发行定价——基于文本相似度视角[J].会计研究,2022,(03):174-191.
<img loading="lazy" src="img/%e5%85%b3%e9%94%ae%e5%ae%a1%e8%ae%a1%e4%ba%8b%e9%a1%b9%e4%bf%a1%e6%81%af%e5%90%ab%e9%87%8f%e4%b8%8e%e5%85%ac%e5%8f%b8%e5%80%ba%e5%88%b8%e5%8f%91%e8%a1%8c%e5%ae%9a%e4%bb%b7_cover.png" alt=""  />
</p>
<h2 id="摘要">摘要</h2>
<p>关键审计事项是来自审计师视角的信息， 其蕴含的特质性信息对实现沟通价值至关重要。<strong>本文采用文本分析方法计算的文本相似度衡量关键审计事项特质性信息含量，考察其对公司债券发行定价的影响。 结果发现， 以较低文本相似度代表的较高关键审计事项信息含量能够降低公司债券发行定价</strong>。 较高的审计师专业胜任能力 和独立性能够增强关键审计事项信息含量对公司债券发行定价的降低作用。 信息不对称的缓解是关键审计事项 信息含量降低公司债券发行定价的具体影响渠道。考虑关键审计事项类型后发现， 关联交易类关键审计事项信 息含量对公司债券发行定价的降低作用更强。本文研究结论有助于未来改进关键审计事项的披露要求。</p>
<p>关键词: 关键审计事项; 公司债券; 发行定价; 信息含量; 文本相似度</p>
<p><br><br></p>
<h2 id="一信息含量算法">一、信息含量算法</h2>
<p>文中关键审计事项信息含量算法</p>
<p><img loading="lazy" src="img/%e5%85%b3%e9%94%ae%e5%ae%a1%e8%ae%a1%e4%ba%8b%e9%a1%b9%e4%bf%a1%e6%81%af%e5%90%ab%e9%87%8f-%e5%ae%9e%e7%8e%b0%e7%ae%97%e6%b3%95.png" alt=""  />
</p>
<p>本文不仅限于关键审计事项，在别的应用场景中也可以使用相似度计算得到信息含量。这里将算法再简化为文本向量化，依次计算得到该企业的<strong>企业向量</strong>、该企业所在行业的<strong>行业向量</strong>、<strong>信息含量(特质性)</strong>。大致的算法思路如下</p>
<ol>
<li>使用sklearn，将该企业文本(审计报告文本)转为TF-IDF的<strong>企业向量</strong>。</li>
<li>当年同行业所有企业(排除该公司)向量求均值，得到<strong>行业向量</strong>。</li>
<li>计算企业向量与行业向量余弦值，乘以(-1)，得到该企业的特质性的<strong>信息含量</strong></li>
</ol>
<p><br><br></p>
<h2 id="二信息含量算法实现">二、信息含量算法实现</h2>
<p>计算关键审计事项信息含量，需要有审计报告、行业。这里参考论文，使用md&amp;a文本 和 md&amp;a数据,用于计算 <strong>企业信息含量(特质性)</strong> 。</p>
<p>为了减少计算工作量，这里只准备了 2020 年的数据。</p>
<h3 id="21-读入数据">2.1 读入数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># converters 强制声明该列为字符串，  防止股票代码 被程序识别为数字，</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;mda2020.xlsx&#39;</span><span class="p">,</span> <span class="n">converters</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;股票代码&#39;</span><span class="p">:</span> <span class="nb">str</span><span class="p">})</span>

<span class="c1">#显示前5行</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<h3 id="22-数据筛选">2.2 数据筛选</h3>
<p>行业内企业数量过少，会导致行业向量与某个或某几个企业向量相关性增大，极端情况下，一个企业就是一个行业。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><pre><code>C39    419
C26    272
C35    256
C38    254
I65    250
      ... 
M75      1
R88      1
O80      1
B10      1
E49      1
Name: 行业代码, Length: 81, dtype: int64
</code></pre>
<p>剔除掉企业数较少的行业，这里只保留大于20的行业。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ind_codes</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">ind_codes</span> <span class="o">=</span> <span class="n">ind_codes</span><span class="p">[</span><span class="n">ind_codes</span><span class="o">&gt;</span><span class="mi">20</span><span class="p">]</span><span class="o">.</span><span class="n">index</span>
<span class="n">ind_codes</span>
</code></pre></div><pre><code>Index(['C39', 'C26', 'C35', 'C38', 'I65', 'C27', 'C34', 'C36', 'K70', 'C30',
       'F52', 'C29', 'F51', 'C32', 'C33', 'D44', 'I64', 'E48', 'C40', 'C14',
       'C37', 'N77', 'J67', 'L72', 'C13', 'M74', 'C15', 'C17', 'C18', 'J66',
       'G54', 'C22', 'C31', 'G55', 'E50', 'C28', 'D45', 'R86', 'R85', 'C21',
       'B06', 'C41', 'B09', 'J69'],
      dtype='object')
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">filter_industry</span><span class="p">(</span><span class="n">ind_code</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ind_code</span> <span class="ow">in</span> <span class="n">ind_codes</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>


<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">filter_industry</span><span class="p">)]</span>
<span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<h3 id="23-文本向量化">2.3 文本向量化</h3>
<p>使用sklearn，将该企业文本(审计报告文本)转为TF-IDF的企业向量。步骤</p>
<ol>
<li>分词整理</li>
<li>tfidff文本向量化</li>
<li>合并多个字段为新的df</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;STOPWORDS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;STOPWORDS&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#只保留md&amp;a中的中文内容</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]+&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">))</span>
    <span class="c1">#剔除停用词</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">jieba</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="c1">#整理为用空格间隔的字符串(类西方语言文本格式)</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s1">&#39;经营讨论与分析内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
<span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df3.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_df</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> 
<span class="c1"># 生成稀疏bow矩阵</span>
<span class="c1">#dtm 文档-词频-矩阵</span>
<span class="n">dtm2020</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df2</span><span class="p">[</span><span class="s1">&#39;clean_text&#39;</span><span class="p">])</span> 
<span class="n">dtm2020</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dtm2020</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
<span class="n">dtm2020</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df4.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#合并多个字段为新的df</span>
<span class="n">dtm2020_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df2</span><span class="p">[[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">]],</span> <span class="n">dtm2020</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dtm2020_</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df5.png" alt=""  />
</p>
<br>
<h3 id="24-计算2020年信息含量">2.4 计算2020年信息含量</h3>
<ol>
<li>使用sklearn，将该企业文本(审计报告文本)转为TF-IDF的<strong>企业向量</strong>。</li>
<li>当年同行业所有企业(排除该公司)向量求均值，得到<strong>行业向量</strong>。</li>
<li>计算企业向量与行业向量余弦值，乘以(-1)，得到该企业的特质性的<strong>信息含量</strong></li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">csv</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;信息含量2020.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvf</span><span class="p">:</span>
    <span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">,</span> <span class="s1">&#39;信息含量&#39;</span><span class="p">]</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dtm2020_</span><span class="p">)):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">code</span> <span class="o">=</span> <span class="n">dtm2020_</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;股票代码&#39;</span><span class="p">]</span>
            <span class="n">ind</span> <span class="o">=</span> <span class="n">dtm2020_</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;行业代码&#39;</span><span class="p">]</span>
            <span class="n">year</span> <span class="o">=</span> <span class="n">dtm2020_</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;会计年度&#39;</span><span class="p">]</span>
            <span class="c1">#企业向量</span>
            <span class="n">corp_vec</span> <span class="o">=</span> <span class="p">[</span><span class="n">dtm2020_</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
            <span class="n">corp_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">corp_vec</span><span class="p">)</span>
            <span class="c1">#行业向量</span>
            <span class="n">ind_vec</span> <span class="o">=</span> <span class="p">[</span><span class="n">dtm2020_</span><span class="p">[</span><span class="n">dtm2020_</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">ind</span><span class="p">][</span><span class="n">dtm2020_</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span><span class="o">!=</span><span class="n">code</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
            <span class="n">ind_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ind_vec</span><span class="p">)</span>
            <span class="c1">#信息含量</span>
            <span class="n">special_info</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">corp_arr</span><span class="p">,</span> <span class="n">ind_arr</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="n">data</span><span class="p">[</span><span class="s1">&#39;股票代码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">code</span>
            <span class="n">data</span><span class="p">[</span><span class="s1">&#39;行业代码&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ind</span>
            <span class="n">data</span><span class="p">[</span><span class="s1">&#39;会计年度&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">year</span>
            <span class="n">data</span><span class="p">[</span><span class="s1">&#39;信息含量&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">special_info</span>

            <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
</code></pre></div><p>欣赏一下结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">idf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;信息含量2020.csv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">idf</span><span class="p">))</span>
<span class="n">idf</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df6.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三讨论">三、讨论</h2>
<p>最近陆续分享了几篇<strong>文本相似度</strong>、<strong>信息含量</strong>的论文</p>
<ul>
<li>[1]姜富伟,胡逸驰,黄楠.央行货币政策报告文本信息、宏观经济与股票市场[J].金融研究,2021,(06):95-113.</li>
<li>[2]宋建波,冯晓晴.关键审计事项信息含量与公司债券发行定价——基于文本相似度视角[J].会计研究,2022,(03):174-191.</li>
<li>[3]孟庆斌,杨俊华,鲁冰.管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J].中国工业经济,2017,(12):132-150.</li>
</ul>
<p>比较一下,三者均先使用了文本向量化，将本文数据转为向量。每篇论文的算法</p>
<br>
<table>
<thead>
<tr>
<th>论文</th>
<th>指标</th>
<th>算法</th>
</tr>
</thead>
<tbody>
<tr>
<td>[1]</td>
<td>文本相似度</td>
<td>将央行货币政策报告向量化， 临近的两个报告文本向量计算相似度，相似度越高，金融市场波动性越小。</td>
</tr>
<tr>
<td>[2]</td>
<td>信息含量</td>
<td>将同行业内所有企业向量Corp求均值得到行业向量Ind，求Corp与Ind的余弦相似度，并将结果乘以(-1),所得结果定义为信息向量。</td>
</tr>
<tr>
<td>[3]</td>
<td>信息含量</td>
<td>文本向量化+计量建模，认为md&amp;a中的信息向量Norm可以由市场Norm_Market、行业Norm_Industry、企业异质性μ三种信息向量组成，通过计算 <br><code>Norm = a0 + a1*Norm_Industry +  a2*Norm_Market + μ</code> <br>，将μ 向量的绝对值和作为信息含量，而a1+a2看标准信息。</td>
</tr>
</tbody>
</table>
<br>
<p>从中可以看到两个向量的余弦相似度，在不同场景，解读含义是不同的，亦正亦邪。在货币政策中，相似度越高，表示越政策稳定，金融市场波动星越小。而在关键审计场景中，特质性信息是缓解公司与投资者信息不对称的关键，公司向量Corp与行业向量Ind相似度越高，表示公司审计报告文本特质性信息越少。</p>
<br>
<br>
<h2 id="代码获取">代码获取</h2>
<ul>
<li>代码及视频讲解已经添加至 <a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a> 中，感兴趣的同学欢迎订阅该系列课，涵盖Python语法入门、数据采集、文本分析、机器学习等。</li>
<li>数据&amp;代码创作不易， 如果需要源代码和数据， <a href="https://mp.weixin.qq.com/s/wkETqR-e0pgN8QLar-GUmw">点击进入购买链接</a></li>
</ul>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 国外会计文本信息实证研究述评与展望</title>
      <link>https://textdata.cn/blog/2023-01-12-review_about_accounting_text_mining/</link>
      <pubDate>Thu, 12 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-01-12-review_about_accounting_text_mining/</guid>
      <description>近年来，文本信息逐渐成为国外会计实证研究的热点，许多学者开始致力于 运用文本分析方法来解决会计与财务问题，并取得了众多有价值的研究成果。与之相比，国 内的此类研究却相当缺乏。为了弥补国内研究的不足，本文对国外近十年来取得的研究成果 进行了系统的梳理和述评。首先，系统阐述了会计文本信息的定义、特征及其测量方法；其 次，从不同层面出发，总结并分析了会计文本信息的影响因素及其作用结果；再次，指出了 现今国外研究中存在的不足。在此基础上，本文提出了一个未来研究的框架，分别从基础、 引入、拓展三个方向来展望国内研究，具体包括如何构建适合中文会计语言的文本分析方 法、国外现有理论与问题在我国的本土化检验以及在中国情境下可以拓展的独创性研究。In recent years, text information has gradually become a hot spot in foreign accounting empirical research. Many scholars have begun to use text analysis methods to solve accounting and financial problems, and have achieved many valuable research results. In contrast, such research in China is quite lacking. In order to make up for the lack of domestic research, this paper systematically sorts out and reviews the research achievements abroad in the past ten years. Firstly, it systematically expounds the definition, characteristics and measurement methods of accounting textual information; secondly, it summarizes and analyzes the influencing factors and results of accounting textual information from different levels; thirdly, it points out the deficiencies in current foreign research . On this basis, this paper proposes a framework for future research, looking forward to domestic research from the three directions of foundation, introduction, and expansion, including how to construct a text analysis method suitable for Chinese accounting language, and the existing foreign theories and problems in my country. Indigenous testing and original research that can be extended in the Chinese context.</description>
      <content:encoded><![CDATA[<br>
<p>肖浩,詹雷,王征.国外会计文本信息实证研究述评与展望[J].外国经济与管理,2016,38(09):93-112.</p>
<br>
<p><img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-01.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-02.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-03.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-04.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-05.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-06.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-07.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-08.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-09.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-10.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-11.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-12.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-13.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-14.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-15.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-16.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-17.png" alt=""  />

<img loading="lazy" src="img/%e5%9b%bd%e5%a4%96%e4%bc%9a%e8%ae%a1%e6%96%87%e6%9c%ac%e4%bf%a1%e6%81%af%e5%ae%9e%e8%af%81%e7%a0%94%e7%a9%b6%e8%bf%b0%e8%af%84%e4%b8%8e%e5%b1%95%e6%9c%9b_%e8%82%96%e6%b5%a9-18.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 大数据驱动的「社会经济地位」分析研究综述</title>
      <link>https://textdata.cn/blog/2022-12-30-review-about-socioeconomic-status-analysis/</link>
      <pubDate>Fri, 30 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-12-30-review-about-socioeconomic-status-analysis/</guid>
      <description>大数据和机器学习技术的发展极大地促进了社 会经济地位的分析以及相关应用。 本文对应用于推断社会属性的大数据方法进行了全面的回顾,并系统地介绍了相应方法以及得到广泛使用的基准测试程序以及资源。 本文旨在提供 一份简洁、清晰的大数据方法应用于社会经济属性分析的概述,其不仅可以为对该方面感兴趣的读者提供帮助,而且可以为继续在该领域工作的研究人员和工程技术人员提供参考。</description>
      <content:encoded><![CDATA[<br>
<p>么晓明, 丁世昌, 赵涛, 黄宏, 罗家德, and 傅晓明. &ldquo;<strong>大数据驱动的社会经济地位分析研究综述</strong>.&rdquo; <em>计算机科学</em> 49, no. 4 (2022): 80-87.</p>
<br>
<p><img loading="lazy" src="img/%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%a9%b1%e5%8a%a8%e7%9a%84%e7%a4%be%e4%bc%9a%e7%bb%8f%e6%b5%8e%e5%9c%b0%e4%bd%8d%e5%88%86%e6%9e%90%e7%a0%94%e7%a9%b6%e7%bb%bc%e8%bf%b0-2.png" alt=""  />

<img loading="lazy" src="img/%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%a9%b1%e5%8a%a8%e7%9a%84%e7%a4%be%e4%bc%9a%e7%bb%8f%e6%b5%8e%e5%9c%b0%e4%bd%8d%e5%88%86%e6%9e%90%e7%a0%94%e7%a9%b6%e7%bb%bc%e8%bf%b0-3.png" alt=""  />

<img loading="lazy" src="img/%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%a9%b1%e5%8a%a8%e7%9a%84%e7%a4%be%e4%bc%9a%e7%bb%8f%e6%b5%8e%e5%9c%b0%e4%bd%8d%e5%88%86%e6%9e%90%e7%a0%94%e7%a9%b6%e7%bb%bc%e8%bf%b0-4.png" alt=""  />

<img loading="lazy" src="img/%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%a9%b1%e5%8a%a8%e7%9a%84%e7%a4%be%e4%bc%9a%e7%bb%8f%e6%b5%8e%e5%9c%b0%e4%bd%8d%e5%88%86%e6%9e%90%e7%a0%94%e7%a9%b6%e7%bb%bc%e8%bf%b0-5.png" alt=""  />

<img loading="lazy" src="img/%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%a9%b1%e5%8a%a8%e7%9a%84%e7%a4%be%e4%bc%9a%e7%bb%8f%e6%b5%8e%e5%9c%b0%e4%bd%8d%e5%88%86%e6%9e%90%e7%a0%94%e7%a9%b6%e7%bb%bc%e8%bf%b0-6.png" alt=""  />

<img loading="lazy" src="img/%e5%a4%a7%e6%95%b0%e6%8d%ae%e9%a9%b1%e5%8a%a8%e7%9a%84%e7%a4%be%e4%bc%9a%e7%bb%8f%e6%b5%8e%e5%9c%b0%e4%bd%8d%e5%88%86%e6%9e%90%e7%a0%94%e7%a9%b6%e7%bb%bc%e8%bf%b0-7.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>JM2022综述 | 黄金领域: 为营销研究(新洞察)采集网络数据</title>
      <link>https://textdata.cn/blog/2022-12-03-scraping-web-data-for-marketing-insights/</link>
      <pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-12-03-scraping-web-data-for-marketing-insights/</guid>
      <description>Journal of Marketing 2022年一篇关于营销领域网络爬虫的文献综述</description>
      <content:encoded><![CDATA[<p>Boegershausen, Johannes, Hannes Datta, Abhishek Borah, and Andrew Stephen. &ldquo;Fields of gold: Scraping web data for marketing insights.&rdquo; <em>Journal of Marketing</em> (2022).</p>
<p>本文是JM中少有的技术流综述文，阅读起来晦涩难懂，我们就大概知道怎么回事， 查看有没有自己感兴趣的研究(方法)即可。该文作者为该综述专门开发了一个 web-scraping.org 的网站,截图如下</p>
<p><img loading="lazy" src="img/01-web-scraping.png" alt=""  />

<img loading="lazy" src="img/02-web-scraping.png" alt=""  />
</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/KiyFyLEkqNk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<br>
<h2 id="摘要">摘要</h2>
<p>市场营销学者越来越多使用网络爬虫和API接口，从互联网收集数据。尽管网络数据得到广泛使用，但很少有学者关注收集过程中面临的各种挑战。<strong>研究人员如何确保采集的数据集是有效的？</strong> 虽然现有资源强调提取网络数据的技术细节，<strong>但作者提出了一种新的方法框架，重点是提高其有效性</strong>。特别是，该框架强调解决有效性问题， 需要在数据采集的三个阶段(<strong>选择数据源、设计数据收集和提取数据</strong>)联合考虑技术和法律/伦理问题。作者进一步审查了营销Top5期刊上300 篇使用网络数据的论文，并总结提出了如何使用网络数据促进营销研究。本文最后指出了未来研究的方向，高价值的网络数据源和新方法。</p>
<p><strong>Keywords：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- web scraping
- application programming interface, API
- crawling
- validity
- user-generated content
- social media
big data
</code></pre></div><br>
<h2 id="一网络数据的魅力">一、网络数据的魅力</h2>
<p>社会和商业生活的加速数字化创造了数量空前的消费者和企业行为数字痕迹。 每分钟，全球用户在 Google 上进行 570 万次搜索，进行 600 万次商业交易，并在 Instagram 上分享6.5万张照片（Statista 2021）。 由此产生的网络数据——规模庞大、形式多样，而且通常可以在互联网上公开访问——对于那些想要量化消费、深入了解企业行为并跟踪难以或昂贵地观察社会活动的营销学者来说，这是一个潜在的金矿 . 网络数据对营销研究的重要性反映在越来越多的有影响力的出版物中，涵盖消费者文化理论、消费者心理学、实证建模和营销策略等。</p>
<p><img loading="lazy" src="img/fig-1-increased-use-of-web-data-in-marketing.png" alt=""  />
</p>
<p>整理了 <strong>营销领域 top 5 期刊( JM、JMR、JCR、JCP、MS) 的 313 篇论文</strong> ，经过整理绘制图-1（Figure1）， 使用网络数据进行研究的量呈现快速上涨的趋势。使用网络数据的论文占比，从2010年的4%提升到2020年的15%。 者313篇论文，数据的获取方式统计</p>
<ul>
<li>**59% 的论文使用了 <strong>网络爬虫</strong> 采集数据</li>
<li>12% 的论文使用API收集数据</li>
<li>9% 的论文同时使用了网络爬虫和API</li>
<li>20% 使用人工从网站手动复制粘贴数据</li>
</ul>
<p><strong>使用 网络数据 的论文，平均被引用次数 7.55， 远高于 非网络数据 的 3.90</strong>。</p>
<br>
<p>使用网络数据做新研究，大致有4种实现路径</p>
<ol>
<li><strong>研究新现象，新场景</strong>
<ul>
<li>网络世界产生的不同于现实世界的情景，可以研究新现象</li>
</ul>
</li>
<li><strong>繁荣生态价值</strong>
<ul>
<li>比如，对亚马逊评论数据进行研究，研究发现可以帮助亚马逊平台进行改善。</li>
</ul>
</li>
<li><strong>促进方法论进步</strong>
<ul>
<li>文本、图片、音频、视频等</li>
</ul>
</li>
<li><strong>提高测量效果(快、准、好、全)</strong>
<ul>
<li>借助一些API，可以对已有的数据集增加新的信息量。</li>
<li>例如，日期数据，结合HolidayAPI，可以查看日期的节假日信息</li>
<li>给定日期和IP地址，使用Weather Underground可以查看天气信息</li>
</ul>
</li>
</ol>
<p><img loading="lazy" src="img/table-1-four-pathway-of-knowledge-creation-using-web-data.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二数据采集的方法框架">二、数据采集的方法框架</h2>
<p>在使用 **网络爬虫 和 API ** 自动收集网络数据时，研究人员通常会在 **研究有效性、技术可行性和法律/伦理风险 **1 三者间权衡利弊得失，研究人员如何解决这些权衡，通过增强或破坏 <strong>统计结论有效性、内部有效性、结构有效性和外部有效性</strong> 来塑造研究结果的可信度（Shadish、Cook 和 Campbell 2002）。</p>
<p><img loading="lazy" src="img/fig-2-methodological-framework-for-collecting-web-data.png" alt=""  />
</p>
<p>本文开发了一个方法框架，为使用 网络爬虫 和 API 自动收集网络数据提供指导。图 2（Figure 2） 涵盖三个关键阶段</p>
<ul>
<li><strong>数据源选择</strong></li>
<li><strong>设计方案</strong>
<ul>
<li>从网站中抽取哪些信息</li>
<li>采集频率，即 每天(周/月)重复运行一次爬虫，得到面板数据</li>
</ul>
</li>
<li><strong>执行数据采集</strong>
<ul>
<li>如何改善爬虫运行效率</li>
<li>如何处理原始信息，完整的保存为原始格式html、json，还是只抽取存储当前想要的字段</li>
</ul>
</li>
</ul>
<p>研究人员通常从一组广泛的潜在数据源开始，并根据三个关键考虑因素（有效性、技术可行性和法律/道德风险）剔除其中一些数据源。这三个考虑因素出现在倒金字塔的角落，底部的有效性强调其重要性。鉴于在收集最终数据集之前难以预测其确切特征，研究人员在设计、原型化和完善数据收集时经常重新考虑这些因素。未能解决技术或法律/伦理问题可能意味着网络数据无法有意义地告知研究问题。</p>
<h3 id="21-数据源面临的挑战解决办法">2.1 数据源面临的挑战(解决办法)</h3>
<ol>
<li>探索潜在网络数据源
<ul>
<li>由于网络资源在质量、稳定性和可检索性方面存在巨大差异，研究人员可能倾向于只考虑主要或熟悉的平台。 对数据世界的彻底探索允许令人信服的理论检验和识别可能难以以其他方式注意到的新颖的、新兴的营销现象。</li>
</ul>
</li>
<li>考虑网络爬虫的替代方案
<ul>
<li>由于网络抓取是最流行的网络数据提取方法，研究人员可能会忽视其他提取数据的方法。 API 提供了一种记录和授权的方式来获取许多来源的 Web 数据。 一些来源还提供现成的数据集。 使用此类替代方案可以节省时间并最大限度地减少法律风险。</li>
</ul>
</li>
<li>将数据与场景结合对应起来
<ul>
<li>Web 数据通常没有大量的文档。 尽早识别潜在相关的背景信息对于研究的相关性和有效性至关重要。
<img loading="lazy" src="img/table-2-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />
</li>
</ul>
</li>
</ol>
<br>
<h3 id="22-设计数据采集方案">2.2 设计数据采集方案</h3>
<ol>
<li>从页面抽取什么信息，从有效性、合法、技术可行性 三个方面论证。</li>
<li>如何进行数据抽样？</li>
<li>以什么频率(每天、周、月)进行数据采集</li>
</ol>
<p><img loading="lazy" src="img/table-3-1-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />

<img loading="lazy" src="img/table-3-2-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />
</p>
<br>
<h3 id="23-执行数据采集">2.3 执行数据采集</h3>
<ol>
<li>如何改善爬虫运行效率</li>
<li>如何监控数据质量</li>
<li>整理数据文档(记录)
<img loading="lazy" src="img/table-4-chanllenges-and-solutions-in-selecting-web-data-sources.png" alt=""  />
</li>
</ol>
<br>
<h2 id="部分参考文献">部分参考文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]Allard, Thomas, Lea H. Dunn, and Katherine White. &#34;Negative reviews, positive impact: Consumer empathetic responding to unfair word of mouth.&#34; Journal of Marketing 84, no. 4 (2020): 86-108.
[2]Gao, Weihe, Li Ji, Yong Liu, and Qi Sun. &#34;Branding cultural products in international markets: a study of hollywood movies in China.&#34; Journal of Marketing 84, no. 3 (2020): 86-105.
[3]Reich, Taly, and Sam J. Maglio. &#34;Featuring mistakes: The persuasive impact of purchase mistakes in online reviews.&#34; Journal of Marketing 84, no. 1 (2020): 52-65.
[4]Lee, Jeffrey K., and Ann Kronrod. &#34;The strength of weak-tie consensus language.&#34; Journal of Marketing Research 57, no. 2 (2020): 353-374.
[5]Matz, Sandra C., Cristina Segalin, David Stillwell, Sandrine R. Müller, and Maarten W. Bos. &#34;Predicting the personal appeal of marketing images using computational methods.&#34; Journal of Consumer Psychology 29, no. 3 (2019): 370-390.
[6]Dai, Hengchen, and Dennis J. Zhang. &#34;Prosocial goal pursuit in crowdfunding: Evidence from kickstarter.&#34; Journal of Marketing Research 56, no. 3 (2019): 498-517.
[7]Luffarelli, Jonathan, Mudra Mukesh, and Ammara Mahmood. &#34;Let the logo do the talking: The influence of logo descriptiveness on brand equity.&#34; Journal of Marketing Research 56, no. 5 (2019): 862-878.
[8]Bond, Samuel D., Stephen X. He, and Wen Wen. &#34;Speaking for “free”: Word of mouth in free-and paid-product settings.&#34; Journal of Marketing Research 56, no. 2 (2019): 276-290.
[9]Han, Kyuhong, Jihye Jung, Vikas Mittal, Jinyong Daniel Zyung, and Hajo Adam. &#34;Political identity and financial risk taking: Insights from social dominance orientation.&#34; Journal of Marketing Research 56, no. 4 (2019): 581-601.
[10]Netzer, Oded, Alain Lemaire, and Michal Herzenstein. &#34;When words sweat: Identifying signals for loan default in the text of loan applications.&#34; Journal of Marketing Research 56, no. 6 (2019): 960-980.
[11]Toubia, Olivier, Garud Iyengar, Renée Bunnell, and Alain Lemaire. &#34;Extracting features of entertainment products: A guided latent dirichlet allocation approach informed by the psychology of media consumption.&#34; Journal of Marketing Research 56, no. 1 (2019): 18-36.
[12]Van Laer, Tom, Jennifer Edson Escalas, Stephan Ludwig, and Ellis A. Van Den Hende. &#34;What happens in Vegas stays on TripAdvisor? A theory and technique to understand narrativity in consumer reviews.&#34; Journal of Consumer Research 46, no. 2 (2019): 267-285.
[13]Zhong, Ning, and David A. Schweidel. &#34;Capturing changes in social media content: A multiple latent changepoint topic model.&#34; Marketing Science 39, no. 4 (2020): 827-846.
[14]Colicev, Anatoli, Ashwin Malshe, Koen Pauwels, and Peter O&#39;Connor. &#34;Improving consumer mindset metrics and shareholder value through social media: The different roles of owned and earned media.&#34; Journal of Marketing 82, no. 1 (2018): 37-56.
[15]Liu, Xuan, Savannah Wei Shi, Thales Teixeira, and Michel Wedel. &#34;Video content marketing: The making of clips.&#34; Journal of Marketing 82, no. 4 (2018): 86-101.
[16]Liu, Jia, and Olivier Toubia. &#34;A semantic approach for estimating consumer content preferences from online search queries.&#34; Marketing Science 37, no. 6 (2018): 930-952.
[17]Nam, Hyoryung, Yogesh V. Joshi, and P. K. Kannan. &#34;Harvesting brand information from social tags.&#34; Journal of Marketing 81, no. 4 (2017): 88-108.
[18]Packard, Grant, and Jonah Berger. &#34;How language shapes word of mouth&#39;s impact.&#34; Journal of Marketing Research 54, no. 4 (2017): 572-588.
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>社会学研究 | 社会计算驱动的社会科学研究方法</title>
      <link>https://textdata.cn/blog/2022-12-03-social-computing-methodology-about-big-data-and-artificial-intelligence/</link>
      <pubDate>Sat, 03 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-12-03-social-computing-methodology-about-big-data-and-artificial-intelligence/</guid>
      <description>一篇关于计算社会学方法论的综述性论文</description>
      <content:encoded><![CDATA[<p><strong><a href="%E7%A4%BE%E4%BC%9A%E8%AE%A1%E7%AE%97%E9%A9%B1%E5%8A%A8%E7%9A%84%E7%A4%BE%E4%BC%9A%E7%A7%91%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95_%E5%91%A8%E6%B6%9B.pdf">周涛,高馨,罗家德.社会计算驱动的社会科学研究方法[J].社会学研究,2022,37(05):130-155+228-229.</a></strong></p>
<p><img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-01.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-02.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-03.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-04.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-05.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-06.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-07.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-08.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-09.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-10.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-11.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-12.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-13.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-14.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-15.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-16.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-17.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-18.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-19.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-20.png" alt=""  />

<img loading="lazy" src="img/%e5%91%a8%e6%b6%9b-21.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 历史GIS的研究现状和发展趋势</title>
      <link>https://textdata.cn/blog/2022-09-19-history-gis/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-09-19-history-gis/</guid>
      <description>d历史地理信息系统（历史GIS）属于地理信息系统（GIS）与历史学相结合的交叉研究领域，它有机地集成了GIS的技术方法、地理学家的空间视角和历史学家的时间视角，量化历史时期的地理过程并构建相应的时空模型，为面向未来的科学预测提供研究基础。</description>
      <content:encoded><![CDATA[<br>
<p>赵耀龙,巢子豪.历史GIS的研究现状和发展趋势[J].地球信息科学学报,2020,22(05):929-944.</p>
<ul>
<li>赵耀龙，华南师范大学地理科学学院教授、博士生导师、副院长。研究方向为地理信息科学与技术、空间综合人文学与社会科学。</li>
<li>巢子豪，华南师范大学地理科学学院GIS专业博士研究生，研究方向为空间藏学。</li>
</ul>
<h2 id="摘要">摘要</h2>
<p>历史地理信息系统（历史GIS）属于地理信息系统（GIS）与历史学相结合的交叉研究领域，它有机地集成了GIS的技术方法、地理学家的空间视角和历史学家的时间视角，量化历史时期的地理过程并构建相应的时空模型，为面向未来的科学预测提供研究基础。历史GIS兴起于20世纪90年代中期，给GIS学科、历史地理学和历史学都带来了新的研究机遇与活力，并表现出强劲的发展势头，但也存在一些尚待解决的问题。近年来，历史GIS向社会提供了日益丰富的历史地理信息服务，并逐渐跨越系统的技术层面，向着科学层面纵深发展。本文通过对国内外相关文献的梳理与分析，回顾了历史GIS产生的背景，从数字化、数据模型、数据库建设与系统开发、空间分析和可视化5个方面综述了国内外历史GIS的研究现状。最后，从历史资料的空间化与数字化、历史地理时空大数据、历史地理空间框架构建及历史地理信息服务、历史地理时空过程及模型构建、历史地理信息科学和技术学科体系的形成等角度展望了历史GIS的发展趋势，以期为历史GIS未来的发展提供新的研究思路。</p>
<p>关键词: 历史GIS 研究现状 发展趋势 空间化 历史地理时空大数据 历史地理空间框架 历史地理信息服务 历史地理信息科学与技术</p>
<br>
<h2 id="1-引言">1. 引言</h2>
<p>地理学与历史学分属于对空间差异、时间分化进行研究的学科，而对历史的研究不可能完全脱离地理空间，通过侧重于对人类生活中有形环境的研究，地理学日益进入到历史学的研究领域，并产生了相应的交叉研究学科。历史地理学即是这样一门研究人类历史时期地理环境变化及其规律的学科。传统历史地理学的研究方法主要集中于历史文献资料的归纳和演绎，缺乏现代科学技术手段的应用，较难对统计计量科学意义上的规律进行探讨。随着信息化、智慧化技术的迅猛发展和地球信息科学的兴起，历史地理学在迎来发展机遇的同时，也面临着诸多挑战。如何推动传统历史地理学中研究方法与科学思维在新时代的发展与创新，便是亟待解决的问题之一。地理信息系统（GIS）应用现代数字化技术手段解决时间和空间问题，在信息化和智慧化浪潮中得到了快速发展，并广泛应用于诸多学科领域，包括历史学和地理学，历史GIS应运而生。历史GIS集合了GIS的技术特点、地理学家的空间视角和历史学家的时间视角，通过GIS的技术方法将空间和时间视角相结合，研究时空尺度上的变化模式。</p>
<p>历史GIS常被应用于历史地理数据的管理、处理、空间分析、可视化表达等方面，为历史学研究提供了更加广泛的地理空间视角，并发挥出越来越重要的研究价值。①为历史学特别是历史地理文献中的描述性信息提供了空间化和定量化处理方法。②用于不同来源、不同类型、不同结构历史地理数据资源的有效整合，如书面文档、古地图、历史地图、现场调查结果、考古文物等。通过建立历史GIS专题数据库，实现不同历史地理数据在同一个时空框架内的综合管理，有助于历史研究的条理性与系统性。③将空间分析的概念引入历史研究，让历史学家们重新审视历史学中空间信息的价值和社会关系中空间要素的重要性，从历史事件的地理空间位置和时间角度出发，了解其时空演化过程和机制。④为传统历史研究提供了空间可视化的表达方法，通过动态地图、三维GIS(3D GIS)等手段生动直观地展现历史数据的空间变化过程。此外，历史GIS还促进了跨学科研究的发展，为来自历史学、地理学、测绘学、计算机科学与技术等领域的学者们提供了合作的空间；GIS与历史学的结合还可以发现传统方法难以发现的历史问题，促进历史学领域潜在信息的挖掘，并帮助检测传统数据的合理性与正确性。</p>
<p>历史GIS的出现无疑为传统的历史地理研究注入了新的活力，也为GIS研究本身提供了广阔的拓展空间。然而，目前历史GIS研究中也存在一些尚待解决的问题。例如，特定历史地理数据的缺失或难以量化，导致GIS功能难以充分发挥；部分GIS的研究方法并不完全适用于历史问题的研究，需要根据具体问题进行修正；历史GIS中大量空间和属性数据的高精度、规范化处理，以及数据库建设的高难度和高成本也是必须面对的实际问题；研究人员的知识结构难以满足历史GIS的多学科知识背景需求，历史学者往往将GIS理解为绘制地图的软件，难以在研究中挖掘出深层的时空信息等。这些问题在一定程度上制约了历史学研究中GIS作用的发挥。另外，随着GIS逐步从信息系统的技术层面走向信息服务的科学层面，历史GIS于历史地理学而言也不再仅仅是技术的实现，有必要构建历史地理信息科学与技术的学科体系，并为社会提供日益丰富的历史地理信息服务。</p>
<p>有学者从不同学科视角讨论了我国历史GIS在不同时期的发展态势。如潘威等于2012年回顾了GIS引入我国历史地理学界的10年发展历程；2018年，潘威提出了“数字人文”背景和大数据理念下我国历史地理信息化的应对策略；张萍回顾了GIS对中国历史研究的贡献和存在的问题。这些成果为历史GIS研究提供了重要参考。为全面借鉴国内外历史GIS的研究经验，本文在回顾历史GIS产生背景的基础上，从数字化、数据模型、数据库与系统开发、空间分析和可视化5个方面梳理了国内外历史GIS的发展历程和研究进展，提出历史GIS未来值得重点关注的发展趋势，以期推动历史GIS 的健康发展。</p>
<br>
<h2 id="2-历史gis产生的背景">2. 历史GIS产生的背景</h2>
<h3 id="21-国际上历史gis-产生的背景">2.1 国际上历史GIS 产生的背景</h3>
<p>历史地理学界提出将研究对象的时间和空间属性相结合始于20世纪初期。英国历史地理学家Darby于1936年提出的“横剖面法”，以时间序列中多个地理空间剖面展示了空间随着时间发生的变化。20世纪50年代，历史地理学界兴起了“定量革命”，不再局限于定性描述的研究方法，而是强调对研究对象的定量分析。20世纪80年代，数据库技术开始与历史学研究相结合。同一时期，GIS技术得到飞速发展，并被引入历史研究领域，开始用以研究历史事件发展进程、区域经济开发、聚落成长变迁、人群地域特征、灾害或疫病的空间演进等。但该类研究更多关注的是地表状况的变迁，忽略了历史地理学关心的历史现象更迭背后所隐藏的原因等问题，并且历史文献在此类研究中并未被使用，因此，该类研究不能算作GIS应用于历史地理学领域的真正开端。</p>
<p>学者们普遍认为历史GIS兴起于20世纪90年代中期，认为该领域的开山之作是1994年Goerke的《Coordinate for Historical Maps》一书，该书整理了1994 年于欧洲大学研究所举办的研讨会展示的文章。当时历史学正处于对GIS的初步了解阶段，历史GIS发展较为缓慢。这一观点在Ogborn的文章中得到了体现，该文综述了1997年历史地理领域的研究进展，但并未提及历史GIS。之后，由于GIS在历史研究领域的潜力，历史GIS经历了一个相对快速发展时期。1998年和1999年社会科学历史协会组织的历史GIS专题会议中提到了学者们热情地参与到GIS等新方法的使用中，参与者们认为新工具的使用同样意味着创新。Holdsworth在其2002年的文章中提及历史GIS，认为其是历史地理学领域一种新兴的发展趋势。Baker将历史GIS描述为历史地理学领域一项“有趣而又富有挑战的发展”。</p>
<p>对历史GIS更多、更全面的研究主要体现在2000年以后，这一时期出现了许多以历史GIS 为研究重点的出版物。Knowles在2000—2008年出版了历史GIS相关专著2部和期刊论文2篇，包括于2002年的《Past Place: GIS for History》和2008年与Hillier一同出版的《Placing history: how maps，spatial data， and GIS are changing historical scho larship》。Gregory在2003年发表了有关历史数据服务的《Guide to Practice in GIS》，并于2007 年和Ell一同出版了《Historical GIS: technolo gies， methodologies and scholarship》一书，进一步普及了历史GIS的理念、技术及相关应用。von Lünen和Travis于2013年出版了著作《GIS and History:Epistemologies，Reflections，and Conside rations》该书汇集了对多个学术领域专家的采访，分享了他们多年的经验和历史学者们为何应该接受GIS的思考。期刊方面，《Social Science History》在2000年的第24卷第3期中首先推出了历史GIS的专刊。《History and Computing》和《Historical Geography》也分别在2003年第13卷第1期和2005年第33卷中推出了历史GIS专刊。诸多刊物与文章的问世标志着历史GIS进入了快速发展时期。历史地理学者Knowles承认历史GIS的巨大进步，但也指出历史GIS距最初的期望值还相距甚远。并且GIS为历史研究带来了哪些东西，其局限又有哪些，这些都没有形成普遍可接受的观点。</p>
<br>
<h3 id="22-国内历史gis-产生的背景">2.2 国内历史GIS 产生的背景</h3>
<p>拥有悠久的历史文明使我国在古代地理学领域长期处于东亚领先地位，但在历史地理学领域日本起步更早，且领先于我国。“历史地理”这一名词于20世纪初由日本传入我国。1935 年“，历史地理”被顾颉刚、谭其骧创办的《禹贡》杂志用作英文刊名，由此扎根于国内学术界。2000年，冯仁国指出了国内历史地理学发展所遇到的问题，即历史地理学领域的技术手段滞后，与地理学主流发展相距甚远，仍停留在利用传统地图学法和事件枚举法来进行研究分析的阶段。同年，针对这一问题，中国历史地理国际研讨会在云南大学召开，会上着重讨论了领域内新兴的GIS技术。葛剑雄于会议中指出，需凭借现代化的高科技手段来提高历史地理学研究的精度。编绘数字历史地图，创建中国历史GIS的工作开始提上日程。相较于其他国家，我国发展历史GIS具有得天独厚的优势，所拥有的不同历史时期的气候、人口、经济、耕地、赋税等连续历史文字记录为发展历史GIS奠定了坚实的数据基础。</p>
<p>有学者认为，我国历史GIS的开拓者为复旦大学历史地理研究中心的满志敏，其在历史自然地理领域尝试将GIS与历史文献相结合。满志敏于2000 年发表的文章《光绪三年北方大旱的气候背景》被视为是国内GIS与历史地理研究相结合的开山之作。该文基于历史文献资料构建专题空间模型，用于模拟历史时期的地理现象，有着较高的研究起点。2001年，复旦大学历史地理研究所与美国哈佛大学等机构合作，在谭其骧主编的《中国历史地图集》基础上，开发了“中国历史地理信息系统”（China Historical Geographic Information System，CHGIS），建立了一套中国历史时期连续变化、开放的基础地理信息数据库。这一项目的启动标志着我国历史地理学信息化时代的到来。2007年，葛全胜等在总结我国现代历史地理学不同的发展阶段时提到了信息技术的引入时期，表达了当时历史地理学界对提高研究成果定量化程度和精确性的迫切需求，并介绍了历史GIS在国内取得的突破性进展。自此，历史GIS的研究在国内逐步展开。然而，仍有学者并没有真正理解历史GIS研究及技术背后的思考与判断，而是将其认为是由计算机操作的绘图工具，可见学界对历史GIS的认知仍不深刻。</p>
<p>与国外期刊相继推出历史GIS专刊相比，国内期刊对历史GIS的重视程度相对较低。截至目前，未有期刊出版过历史GIS专刊，相关文章也相对较少。</p>
<p>近年来，历史GIS的发展日益受到国内学术界的普遍重视。中国地理学会历史地理专业委员会和复旦大学历史地理研究中心于2015年举办了第一届“历史地理信息系统HGIS沙龙”，之后每年举办一次。中国地理信息科学理论与方法学术年会也曾多次组织了历史GIS分会场。</p>
<br>
<p>##3. 历史GIS研究的现状</p>
<h3 id="31-历史地理信息的数字化">3.1 历史地理信息的数字化</h3>
<p>地图是历史地理学的重要组成部分，其载体形式经历了从传统的石质、帛质、纸质等到数字化的转变。GIS具有空间表现力强、信息存储量大的特点，能够实现把历史上传统的地理空间信息描述方法和内容，转移到以地理空间坐标为基础的现代地图上，以数字化的形式再现。历史地理信息的数字化即是对历史地理信息由纸质或其它载体转化为数字形式的技术过程，有助于推动历史地理信息的深度挖掘和利用。</p>
<p>在国外，Spence于2000年在其专著中论述了历史地理信息数字化的潜力。他将伦敦17世纪90年代的行政地理学数据与税收数据相结合进行数字化，使用专题地图来探索这一时期伦敦地理的不同领域，包括营业租金、家庭密度与租金、按性别的家庭分布等，发现了数据中隐藏的空间模式。Baily等致力于将英国20世纪30至40年代开展的首次土地利用测绘图进行数字化，并评价了该半自动化程序的数字化精度。Scholzel 等开发了新的算法和数字化软件，给历史地理信息的数字化技术带来了革新，实现了对投影信息模糊的历史地图进行数字化的功能。被引入历史地理信息的数字化领域后，GIS也被广泛用于数字化历史地图集的制作。GIS技术与传统制图法的结合，既保证了制图质量又节省了时间和人工成本。并且，通过建立以行政边界为典型代表的不同类型数据层，将其与不同主题的属性数据相结合来制作数字化历史地图集已成为历史地理信息数字化的一种新模式。多个国家或地区已对既有的国家历史地图集进行了部分数字化工作。其中，美国的“世界电子图书馆”（WorldDigital Library Map Section）不仅存储有世界各地的历史地图千余件，还以电子地图插件的形式标识出历史地图所在的地理位置，以表格的形式记录了历史地图的属性信息。“欧洲历史地图资料库”(History and Maps of Europe，Euralas)以21幅历史地图表示欧洲和地中海盆地公元1世纪至20世纪的政治景观。“非洲历史动态地图集”(Animated Atlas of African History) 以动态地图的形式展现了非洲大陆1879—2002年的领土变更、政治体系变化、暴力冲突和经济人口等历史信息。“加拿大历史地图集”(Historical Atlas of Canada)在制作第二卷时引入GIS 技术，以地图、文字、图表的形式展现了加拿大19世纪和20世纪不同的历史主题与事件。“南亚电子图书馆”网站将“印度王国地名地图集”中的历史地理信息进行数字化。另外，展示专题信息的历史地图集数字化工作也相继展开。如展示19世纪40年代爱尔兰饥荒对当地劣质住宅影响等信息的“爱尔兰饥荒数据地图集”(The Atlas of Irish Famine Data)、含有德国及其部分周边地区地名信息的“德意志帝国地图集”、描绘有法国与印第安战争战场详情的“法国－印第安人战争地图”(Maps of the French and Indian War)等。</p>
<p>我国历史地理信息的数字化工作起步于复旦大学历史地理研究中心和哈佛大学等机构联合开展的“中国历史地理信息系统”项目，其数字化的阶段性成果已共享于CHGIS数据库网站。北京大历史地理研究中心与北京市测绘设计研究院合作的“北京历史数字地图”是我国第一项城市区域历史地图的数字化成果，应用GIS技术对《北京历史地图集》进行了数字化。自此，历史地图数字化工作在我国多个城市展开《天津城市历史地图集》《广西历史地图集》《沈阳市历史地图》《杭州市历史地图》《上海市历史地图》等的数字化工作相继提上日程，并已顺利完成。近年来，国内诸多学者在大比例尺历史地图数字化领域也进行了探索。上海交通大学对日本科学书院1998 年出版的《中国大陆五万分之一地图集成》一书中共计4088 幅民国历史地图进行了扫描、地理校准等数字化工作，存储于“中国历史地图地理信息系统”。韩昭庆对利用西方测绘方法绘制的康熙《皇舆全览图》进行了数字化，形成一套以1710年代为时间界面的国家基础历史地理数据库和地名数据库。史磊则尝试利用GIS技术，对民国初年河南省1:10万地形图进行数字化保护。在技术领域，谭瑛等探讨了基于数字化技术的历史地图空间解译方法，从历史地图中提取隐含的空间要素和内涵信息，进行了分类提取、类型分层、数字化转译。</p>
<p>我国历史悠久，历史地图或古地图遗存众多，在数字化、信息化的时代潮流中，越来越多的历地理信息数字化工作被提上日程。历史GIS的数字化功能为历史学及相关学科的研究提供了便利。历史地理信息数字化需求的增加也促进了历史GIS数字化技术的革新与进步。</p>
<br>
<h3 id="32-历史地理信息的数据模型">3.2 历史地理信息的数据模型</h3>
<p>GIS的数据模型是根据程式化、概念化的策略将现实世界发生的现象抽象化，可通过地理层面的点线面或离散连续场进行构建。数据模型中应定义数据类型、存取方式、关系、操作和规则，以维持数据库和信息系统的完整。如何设计一个坚固、连续的数据模型是历史GIS数据库及平台建设的关键问题之一，数据模型设计的好坏直接关系到历史GIS数据库及平台建设的成败。目前，国内外历史GIS在构建数据模型时多以时空数据模型为基础。时空数据模型也是实现不同尺度、不同时序空间数据互动与融合的基础，通过依赖于时间的表达方式来组织管理时态地理数据、属性、空间和时间语义更完整的地理数据模型，可重现历史状态，追踪历史变化以及预测未来趋势。基础、通用的历史GIS数据模型是历史GIS的基石，就像矢量数据模型之于GIS一样重要，决定了历史GIS 时空分析和表达能力的强弱。</p>
<p>国外，Langran较早做出了将时间与GIS相结合的研究尝试，他总结了时空立方体、时空复合、基态修正和时空快照模型这4种时空数据模型，并在应用实例中强调了时间分量的重要性。随后，Peuquet和Duan归纳并比较分析了基于事件的时空数据模型为代表的9种主要时空数据模型。Griffiths等针对当时的对象数据模型普遍无法整合时间与空间属性的弊端，提出了名为Tripod的空间—历史数据模型，该模型基于组件的设计使其内部的空间、时间和历史信息三者得到有机整合。在与复旦大学合作的CHGIS项目中，Berman在原有三维空间数据的基础上增加了时间维度，形成四维空间。CHGIS的数据模型满足了在特定时间点搜寻所有地理单元，通过对象特征类型筛选结果，展示某一地点随时间变化等的要求。Euratlas历史地图集网站的数据模型是不同时间点政治景观及其互相关系的集合，按等级划分为不同的政治领域，高等级领域由低等级领域构成，从低到高四级行政实体分别为：省级、一级行政单元、国家、超国家行政实体。</p>
<p>国内学者在历史地理信息数据模型领域的研究颇丰。徐志红等提出了基于事件语义的时态地理信息系统模型，并应用该模型进行了历史数据的查询及历史数据的回溯和再现。顾国民等针对历史GIS平台中时空数据的存储和显示问题进行研究，在已有时空模型的研究基础上，选用并改进了面向对象的数据模型，提出了更加适合历史GIS平台的时空数据模型。Chen等根据代表性数据的属性设计了针对历史地理名称的时空数据模型，建设了研究中国历史与文化的时空框架。于靖提出了基于地名生命周期的城市历史地名时空数据模型，解决了以可视化方式展示并查询时间截面地名等相关问题。史先瑞分析了基础历史地理信息的时空变化特征，改进基态修正数据模型，提出了一种基于对象的基础历史地理信息的数据组织方法。胡迪等认为现有的历史地理信息数据模型主要面向某一专题的历史地理数据库或信息系统，通用性差。他从地理与历史双重视角出发，以时间、地点、人物、事件历史4个要素为基础将历史信息抽象为历史人物、历史事件、历史地物和历史场景，以及关系和经历等要素，提出了一种通用的历史地理信息数据模型。历史地理信息数据模型的研究正随着各式历史地理信息数据库及系统需求的增加而日益深入。</p>
<br>
<h3 id="33-历史地理信息的数据库建设与系统开发">3.3 历史地理信息的数据库建设与系统开发</h3>
<p>历史地理空间数据的采集耗时较长，过程繁琐冗长，且往往是整个历史GIS建设过程中耗资最大的一个环节。历史地理信息数据库的建立不仅要考虑到建设成本，还要顾及到日后的管理与维护。历史地理信息数据库一旦建成，其实际需求常常超越其最初的构想，因此，设计者需考虑到诸如数据记录、元数据和数据的长期保存等问题，以便历史地理信息数据库价值得到充分展现。</p>
<p>世界上多个国家和地区已构建或正在建设本国的国家历史地理信息系统。美国地理家协会(The Association of American Geographers，AAG) 网站提供了世界各国主要的历史GIS目录。其中，由朴次茅斯大学主持的“大英历史地理信息系统”(Great Britain Historical Geographic Information System) 始建于1994年，主要提供了英国历史时期不同尺度的行政区划及区划内的人口统计信息。明尼苏达大学的明尼苏达人口研究中心的“国家历史地理信息系统”(National Historical Geographic Information System)在美国国家科学基金会资助下开发，其创建的最初意图为构建一个基础架构以支持国家范围内不同信息的专题制图。该系统汇集了1790年至今美国全国范围内的人口、住房、农业、经济统计数据和相应的历史边界矢量数据。“比利时历史地理信息系统”(Belgian Historical Geographic Information System，BHGIS)由根特大学于20世纪90年代开始建设，储存了1800—1963 年比利时国内的人口、工业、农业及贸易数据。悉尼大学开展的“时间地图计划”(TimeMap)开始于1996年，可通过网络索引的方式获取分布式资源、交互式地图、时间序列数据以及地图动画，是国际上首个基于时间序列的交互式与分布式相结合的制图系统。国际上著名的国家历史地理信息系统还包括了加拿大渥太华大学开展的“加拿大世纪研究项目”(Canadian Century Research Infrastructure)，韩国高丽大学开发的“南韩历史信息系统”(South Korea Historical GIS)，欧洲史研究所与德国梅茵兹应用技术大学联合开发的“德意志历史地理信息系统”(Historical Geographic Information System Germany)，荷兰拉德堡德大学开发的“荷兰地理信息系统”(Netherlands Geographic  Information System) 等。</p>
<p>相较于国家层面的历史地理信息平台，根据不同专题建设的历史地理信息平台数据来源更广，针对性更强。目前，世界范围内的专题历史地理信息平台较多，如哈佛大学地理分析中心开发的展现非洲历史地图与重要事件的“非洲地图(AfricaMap)”，越南佛教大学开发的可展示越南佛教场所等佛教文化的“越南佛教文化地图集网站”(Cultural Atlas of Vietnam-ese Buddhism)，悉尼大学研发的可视化“柬埔寨吴哥窟文化遗产历史地理信息平台”(Living with Heritage)，爱尔兰梅努斯大学开发的“爱尔兰人口变化地图集平台”(Irish Population Change Atlas)和“爱尔兰饥荒地图集平台”(The Atlas of Irish Famine Data) ，Migliaccio 等组织开发的意大利历史地震数据管理原型系统等。</p>
<p>我国的历史地理信息平台建设起步较晚，但发展迅速。国家层面的历史地理信息平台以“中国历史地理信息系统”和“中华文明之时空基础框架”最为著名。由复旦大学历史地理研究中心与美国哈佛大学等合作的“中国历史地理信息系统”(CHGIS)项目起始于2001年，致力于建设成为一个连续变化的中国历史时期基础地理信息数据库平台，为研究人员提供根据特定时间和地点的GIS数据查询、统计、下载、空间分析、专题制图与建模服务。CHGIS的历史空间数据来源于复旦大学历史地理研究中心的原始资料，目前已有空间数据的时间跨度为公元前221年至公元1911年。“中华文明之时空基础框架”(Chinese Civilization in Time and Space，CCTS)由中国台湾地区的中央研究院于2002 年研发完成。该系统存储了我国大量历史地图及主题化的属性信息。其中，历史地图以谭其骧先生主编的《中华历史地图集》为基础，提供有上古至清代逾两千年的中国历代疆域图等基本历史地图，并辅之持续整理的各类其它历史地图、遥感影像等。</p>
<p>除了国家历史地理信息平台，国内不同专题的历史地理信息平台也层出不穷。台湾中央研究院的“台湾历史文化地图系统”以历史文献、地名资料与古今地图为基础资料，搭建起了数字化的台湾时空基础架构平台。香港中文大学地球信息科学研究所研发的“民国时期北京都市文化历史地理信息数据库”，考察了由民国成立至抗战时期的北京市在多元文化激荡之下所呈现的文化变迁空间模式。首都师范大学历史地理研究中心与陕西师范大学出版社联合开发的“丝绸之路历史地理信息开放平台”，建立了一套自汉代张骞打通西域以来至1949年以前丝绸之路沿线逐年连续变化的基础历史地理数据库。华中科技大学建立了展现汉语方言分布与演化的“中国历史方言地理信息系统”。中国地图出版社与中国科学院遥感与数字地球研究所在中国31个主要城市古地图数字化成果的基础上，建成了可共享的“中国城市历史地理信息平台”。天津大学建筑学院建立了多角度还原明长城军事聚落历史地理原貌的“明长城军事聚落历史地理信息库”。同时，陕西理工大学的“蜀道历史地理信息系统”、华南师范大学与西藏大学联合开发的“藏语方言时空数据共享服务平台”等多个历史地理信息系统平台也处于研发之中。此外，国内诸多城市也有展现地方历史文化的地理信息系统。如，北京市测绘设计研究院开发的“北京历史文化地理信息系统”、上海师范大学主持的“国际化大都市（上海）历史人文地理信息系统”、南京大学开发的“六朝建康地名信息系统”、西安市城市规划设计研究院设计的“大西安区域历史地理信息共享服务平台”等。</p>
<p>目前，历史地理信息平台的建设正日益向着专题化、区域化的方向发展。同时，方兴未艾的历史地理信息平台建设也促进了数据库技术和基于历史地理信息平台的专题研究进步</p>
<br>
<h3 id="34-历史地理信息的空间分析">3.4 历史地理信息的空间分析</h3>
<p><strong>Gatrell将空间分析定义为内部相关的3大主题的研究：空间布局、时空过程和空间预测</strong>。空间位置信息及其随时间的变化是历史地理研究的核心，而空间分析可以定量的方式研究历史地理领域的时空现象和过程。空间分析的方法虽然不仅限于GIS，但GIS中属性数据与地理坐标相关联的数据模型使其非常适合于定量研究。历史研究有其特殊性，并不是所有的GIS空间分析方法都能直接应用于该领域，需要对其做相应调整。</p>
<p><strong>GIS在历史地理学领域的空间分析方法可以分为3大类，即：点模式分析、附有属性数据的点线面数据分析和栅格数据分析</strong>。其中，最基本的点模式分析从研究对象空间特征的角度出发，探寻点群的空间布局，不涉及其属性信息。如Longley等基于点模式使用平均中心与标准距离法研究了1850—1960年加拿大安大略省伦敦市住宅用地、公共用地、商业用地和工业用地4种土地利用类型的变化模式。其它适用于历史GIS点模式研究方法还包括了密度平滑法、距离衰退模型、地理分析机等。探索点线面数据中属性数据的空间模式，有单变量和多变量2类分析方法。局部空间自相关法是单变量分析的典型方法之一。地理加权回归是一种通过空间权重矩阵进行回归分析的多变量分析方法。Gregory 和Ell同时使用了局部空间自相关和地理加权回归分析了19世纪40年代末期爱尔兰大饥荒后的爱尔兰各地的人口减少情况。相较于矢量数据，栅格数据更适合表示连续的表面，处理没有清晰边界的不确定数据。Bartley 和Campbel收集了英格兰14 世纪上半叶的6000 多名土地私有人去世后的财产清单，根据位置信息生成点数据，并插值生成连续的栅格表面，制作了英格兰黑死病前的土地利用专题地图，进行了私有草地分布密度等分析。</p>
<p>近年来，国外学者使用GIS的空间分析功能在宗教、语言、军事、城市、疾病、遗迹、自然景观等历史地理相关研究中做了诸多尝试。Ayhan 等基于GIS 和空间统计学的方法研究了不同年份土耳其伊兹密尔清真寺的空间分布与城市发展的关系，结果表明该市清真寺的时空分布模式与城市发展模式相似。Blaxter基于核密度估计等方法追溯了挪威语言文化的时空扩散过程。Skaloš等基于数字化后的历史军事测量图和正射影像图，改良了已有的方法，研究捷克部分地区土地覆盖的长期变化情况，其方法更适用于时间尺度超过250年的长期景观变化研究。Keti从18世纪现代罗马城历史地图集中获取信息，根据不同的主题信息对不同的城市现象进行分析与解译。Bezymennyi等收集了1913—2012年乌克兰家畜炭疽病爆发位置数据，分析其时空分布模式和爆发的热点区域，并进一步研究一个世纪以来该疾病爆发地的变化趋势。Blanco等从1870—1960年的古地图中提取出5800多个马德里的水利遗产点数据，将其位置信息、时间信息和属性信息存入数据库，使用GIS 分析其多年的分布特征，得出了水利遗产日益受到城市化影响而破坏的结论。Madricardo和Donnici根据城市历史档案中的环境记录重建了意大利威尼斯舄湖从初建到现在的景观变化，并与描绘有舄湖详细信息的历史地图进行对比，揭示出舄湖景观变化对城市的影响。</p>
<p>从现有研究成果来看，历史GIS最早在国内萌芽时，空间分析功能即被应用于历史水文、历史气候、历史土地覆被、历史海岸线等多个自然地理研究领域。如潘威、满志敏等基于GIS技术构建格网体系，对近百年来上海市三角洲河网的复杂形态进行了研究。葛全胜等专注于历史气候变化及其带来的影响，先后研究了历史气候变化对农作物影响的时空分异，以及对红火蚁潜在分布区的影响。何凡能、方修琦等多次提到用GIS中空间格局网络化的方法重建历史耕地、森林等土地覆被，并分析其空间特征。张晓祥、康育龙等利用数字海岸线分析系统分别分析了历史时期江苏和杭州湾海岸线的演化方式及变化速率。GIS空间分析在历史人文地理领域也有着更加广泛的应用。其中，城市和村落空间形态与格局特征的历史演变是近年来最常被关注的领域。诸多学者基于历史文献、古地图等资料中提供的文字、图像信息，以国家级或地方历史文化名城名镇名村、历史时期城池、历史时期城市工农业用地，民国时期北京城内学校等为研究对象，利用GIS的缓冲区分析、叠加分析、统计分类分析、格网方法等GIS空间分析方法定量化研究其时空布局与形态变化过程，并结合史实分析其内在机理。历史文化与GIS的结合同样是研究的热门领域，包含有民族分布、宗教演变、方言发展、文化名人足迹等研究。杨宇亮等以GIS的技术方法对藏彝走廊与汉族走廊的民族格局开展研究，不仅描述了二者与地理空间的关联性，还反映出了民族的迁徙过程。韩冰利用地理编码技术和谷歌卫星影像获取浙江省1269座佛寺的矢量多边形、中心点数据、面积规模以及布局特征，并基于GIS对其进行了全局自相关分析、系统聚类分析等研究。张义将汉语置于一个广阔的时空背景中，直观地考察方言特征历时的演变与分布、相互影响的程度以及各方言间的亲疏度问题，并在此基础上综合考察中国历史时空坐标中的汉语方言演变轨迹。曾莹利用GIS空间化和数字化方法收集整理资料，设计空间数据库，并基于GIS分析方法探索了江西古代书院及文化名人的时空特征。与此同时，得益于GIS强大的空间分析能力和数据建模能力，诸多学者已经将GIS成功地运用于考古学领域。张海在其专著《GIS与考古学空间分析》一书中指出，考古学景观强调地表空间构成的连续性、区域性和综合性，这3大特征使得GIS技术十分适宜于考古学的景观分析。目前，作为考古学研究的一项新兴支撑技术，GIS中的数字地形分析、空间叠置分析、缓冲区分析和空间形态分析等空间分析方法常被用于研究考古遗址的空间分布规律，并用以探究遗址分布的倾向性及其与地理环境的关系。另外，国内学者还尝试将GIS应用于历史文化景观重现、文物保护、历史人口分布研究、古建筑价值评价与保护等历史人文地理领域，如首都师范大学的“丝绸之路历史地理信息系统建设”等。</p>
<p><strong>GIS中的诸多空间分析方法在历史地理领域已得到了较为广泛的应用，挖掘了传统历史地理研究中未曾发现的潜在信息。然而，历史GIS的空间分析不应该仅仅是GIS方法在历史地理领域的常规应用，随着历史GIS在历史学界的日益普及，越来越多的学者改进了GIS方法以适应不同历史研究的需求，不断丰富着历史GIS空间分析方法的理论</strong>。</p>
<br>
<h3 id="35-历史地理信息的可视化">3.5 历史地理信息的可视化</h3>
<p><strong>历史地理信息的可视化将空间、历史与属性等要素结合，在空间可视化的基础上表达时空变化与发展过程，直观反映时空对象在不同历史时间的各个状态与动态演化</strong>。GIS与历史地理空间数据结合可以反映变化的实体与现象，能够描述时空对象的产生、发展、演化与消亡的全过程。历史地理信息的可视化展现不仅仅只有纸质形式的地图，随着计算机技术的发展，越来越多的新技术与GIS相结合，传统的历史地理信息表现形式得到了颠覆性的革新。目前，GIS可视化方法中的3D GIS、动画、多媒体影像等多种技术已在历史地理研究中得到了应用。</p>
<p>3D情景的构造是目前历史地理信息可视化领域的一大热点。国外在3D GIS方面的发展已较为成熟，并与全球导航卫星系统GNSS、遥感、激光雷达等其他相关技术结合，运用于城市历史风貌、历史地形、历史古迹的还原等方面。日本的Sadahiro等使用DragonFly快速三维制图引擎，在其中叠加东京市的数字高程模型(DEM)、高分辨卫星影像和东京市的历史影像数据，实现了从不同角度和位置浏览东京城市历史风貌的功能。Harris基于数字地形模型(DTM)探索西弗吉尼亚芒兹维尔附近的格雷夫克里克土墩的史前地形。在历史考古学领域，越来越多的学者倾向于借助激光雷达和3D GIS技术进行研究，还原历史遗迹的原始面貌。尽管动画的方式生动且高效，但在历史研究领域中有效利用动画技术的实例较少。历史研究中较长的时间尺度决定了要想实现历史地理信息的动态呈现必须减少可视化对象本身的多样性和复杂性。动画、多媒体技术与GIS的结合在历史自然地理和历史人文地理两方面均有涉及。Cunfer与Rastner等分别运用GIS的动画功能探索美国大平原干旱尘暴区和Findelengletscher冰川随时间的变化。Siekierska 和Armenakis则借助多媒体方法展现加拿大历史上领土的演变。虚拟现实地理信息系统(VR-GIS)等其他可视化技术的综合应用同样对历史地理研究做出了重要贡献，其真实感和互动方式使历史地理信息的显示和观察更加生动和方便，但仍处于发展的早期阶段。Howey 和Brouwer认为考古GIS研究必将受到“数字人文”领域数据、方法与技术的影响，并提及了应用无人机影像进行“数字化故事叙述”、数字化重建和VR等技术在该领域的前景。Pérez-Martín等整合了手绘稿、照片、数字正射影像、计算机图形等多媒体图件影像，在数字摄影测量工作站、全球导航卫星系统、计算机辅助设计、和电脑动画等前沿技术的支持下，以一种新的形式展现了西班牙历史时期的风车情况。</p>
<p>在国内，基于历史文献的可视化研究也是学们较多涉及的研究领域。李海萍等以清代历史地图为例，参照国内现有的地图符号标准，研究并设计了基于GIS的清代历史地图符号库。朱锁玲和王明峰应用GIS技术实现了对《方志物产》中物产分布、物产传播等相关数据的管理和可视化制图。王哲梳理了史料中历史空间数据的可视化方法，探讨了未来经济史研究领域利用可视化手段的可能性。多名学者也在历史事件可视化角度展开过研究。王占刚等利用事件时态树结构实现历史事件时空过程可视化，并以Adobe Flex为工具开发实现了历史事件时空过程可视化算法。陈敏颉等探讨了战争历史事件可视化策略，开展了基于历史事件运动轨迹的可视化表达方法的分类研究。王加胜等利用统计图表、社会网络和GIS技术对南沙群岛历史事件演化的时空特征进行了可视化表达。GIS 与VR、多媒体等技术结合对历史古迹的三维重建也是国内历史地理信息可视化研究的热点之一。GIS三维重建的重点主要集中在古代城市、历史名迹、考古遗址等的空间格局与构造的复原上。另外，历史地理信息可视化的研究多与信息系统共生，并成为系统研究和展示的重要部分。如实现家族成员迁徙路线可视化的“族谱地理信息系统”和采用地图动画、序列地图、时间轴地图、三维场景模拟等表达人物、事件、时间、地点等多要素时空信息的“三国历史地理信息系统”。目前，关于历史变化的可视化表达技术仍需进一步深入研究。</p>
<br>
<h2 id="4-历史gis的发展趋势">4. 历史GIS的发展趋势</h2>
<p>GIS被引入历史学和历史地理学以来，解决了一些传统历史和历史地理研究中无法解决的技术和空间分析问题，得出的研究成果不仅可以为部分历史问题作出基于空间视角的解释，也在一定程度上推动着信息化时代历史学和历史地理学研究范式的创新。GIS与历史相关研究的结合日趋紧密，随着数字人文和信息化、智慧化时代的推进，其未来的发展又会呈现出新的趋势。</p>
<p>（1）构建历史资料的空间化和数字化体系，推动历史地理时空大数据的建设。</p>
<p>随着历史GIS的发展，诸多不同历史时期和不同形式的历史文献会越来越多地进入历史GIS学者的视野。历史文献资料与GIS结合程度的加深，必将推动GIS研究中地理空间的时间范围扩展并不断向前延伸。历史GIS研究需要对大量不同领域、不同尺度、不同类型的历史资料进行空间化和数字化，诸如古地图、历史地图、遥感影像、地名志、考古遗址、图版照片、历代史书资料、郡县府志、家谱、游记、诗赋等，构建历史地理时空大数据。但历史地理信息和现代地理信息有着不同的特征，比如在时态、比例尺、精度等方面，历史资料具有不同于现代地理信息的表现手法。历史资料的特殊性和差异性决定了历史资料空间化和数字化工作的困难，诸如投影、误差、接边等突出和常见问题，始终困扰着历史GIS研究者。需要梳理历史学研究对历史资料空间化和数字化的需求特点，总结历史资料的空间化和数字化技术要求，形成一套历史资料空间化和数字化的技术体系，以推动历史地理时空大数据的建设。</p>
<p>（2）建设历史地理空间框架，推动历史地理信息服务的普及。</p>
<p>历史GIS发展过程中产生了大量的历史地理信息数字化产品，不同学者和研究机构也建设了不同专题的历史GIS数据库及系统平台。然而，随着历史GIS的进一步发展，不同来源、不同尺度、不同类型的历史地理信息数字化产品往往缺乏统一基准、无法实现共享等问题逐渐突显出来。建设历史地理空间框架，是解决这一问题的重要途径。历史地理空间框架可为所有与地理位置相关的历史地理信息数据及产品提供一个统一的时空定位基准，为定位、嵌入或配准各类图形、图像、文本、视频、音频信息提供一个时空多维载体，可实现多元数据的无缝连接和整合，保证历史地理空间数据的一致性、兼容性和可转换性，使用户能够按照地理坐标或空间位置集成、检索、展示所关心的历史时期的自然、社会、经济、环境等信息。历史地理空间框架建设的宗旨是向社会公众尤其是科研人员提供易获得且通用的历史地理信息服务。历史地理信息服务的普及不仅便于科研人员研究工作的开展，也有利于社会公众了解历史地理和历史文化信息，有利于增强文化自信，从而进一步实现历史GIS 的社会价值。</p>
<p>（3）加强历史地理过程及模型研究，推动历史地理信息科学与技术学科体系的形成。</p>
<p>目前，国内外历史GIS的研究主要集中于历史地理信息的时空格局，揭示历史地理时空过程及模型构建的研究相对较少。历史地理学是研究历史时期地理环境发展、变化及其规律的科学，主要任务是探明历史时期的地理过程并构建相应的地理过程模型，从而为面向未来的科学预测提供研究基础。历史地理过程模型是抽象地表示不同历史时期地理环境状态或其组成要素特征的实物模型或数学统计模型，是理解和预测不同尺度历史地理系统格局和过程的重要研究方法。但目前的历史GIS研究成果和历史地理学的专业需求之间还存在着较大的差距。因此，充分利用历史地理时空大数据，通过数据挖掘和多源数据融合，进行历史地理过程的模型构建、模拟及重现，是未来历史GIS研究的重要发展方向之一。历史地理过程及其模型构建的研究，将推动历史GIS由信息系统向信息科学的转换，推动历史地理信息科学和技术学科体系的形成。</p>
<br>
<h2 id="5-结语">5. 结语</h2>
<p>和GIS学科本身相比，历史GIS的起步较晚，发展也较为缓慢，但已在历史地理信息的数字化、数据模型、数据库建设和系统开发、空间分析及可视化等方面取得了显著的研究进展。GIS作为一种研究手段和思维方式，越来越多地参与到历史学和历史地理研究中，在继承传统定性描述方法的基础上，以多样化的研究方法将历史学和历史地理学带入了定量化、信息化和智慧化时代。</p>
<p>得益于我国悠久历史长河中留存的大量历史文献古籍，国家层面的重视，以及历史学和历史地理学等学科领域发展的迫切需要，近年来我国的历史GIS研究取得了一定的成就，但也存在着较为明显的差距。怎样解决好历史地理数据不确定性问题？怎样充分发挥我国现存典籍多的优势？怎样促进历史地理学与GIS等新兴学科的融合发展？怎样通过构建历史地理空间框架做好历史地理信息服务的普及？如何通过历史地理过程模型的构建推动历史地理信息科学与技术学科体系的形成？这些问题的解决将成为促进我国历史GIS事业进一步发展的推动力。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>国庆直播 | Python实证指标与文本分析</title>
      <link>https://textdata.cn/blog/2022-09-19-text-mining-in-ms-workshop/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-09-19-text-mining-in-ms-workshop/</guid>
      <description>文本分析在经管研究中的应用</description>
      <content:encoded><![CDATA[<h2 id="报名信息">报名信息</h2>
<ul>
<li>
<p>时间：<strong>2022.10.03 ~ 2022.10.04</strong></p>
</li>
<li>
<p>地点: 小鹅通平台（线上直播）</p>
</li>
<li>
<p>报名咨询:  17816181460（同微信）（汪老师）</p>
</li>
<li>
<p>报名费：<strong>2500元</strong></p>
<ul>
<li>单位：杭州国商智库信息技术服务有限公司</li>
<li>开户银行： 中国银行杭州大学城支行</li>
<li>银行账户：6232636200100260588</li>
</ul>
</li>
</ul>
<br>
<h2 id="简介">简介</h2>
<p>在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！</p>
<p>在<strong>前大数据时代</strong>，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。<strong>大数据时代</strong>，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：</p>
<ol>
<li><strong>网络爬虫</strong> 解决 如何从网络世界中高效地 <strong>采集数据</strong>？</li>
<li><strong>文本分析</strong> 解决 如何从杂乱的文本数据中 <strong>构建指标</strong>？</li>
</ol>
<p>为方便大家感受到文本数据的魅力，按照是否采用某项技术(爬虫、词频、词袋、w2v建词典、w2v认知变迁)，从五个维度标记代表性的7篇论文。</p>
<table>
<thead>
<tr>
<th>文献</th>
<th>爬虫</th>
<th>定性</th>
<th>词频</th>
<th>词袋</th>
<th>W2V建词典</th>
<th>W2V认知变迁</th>
</tr>
</thead>
<tbody>
<tr>
<td>王伟 , 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>语言具体性如何影响顾客满意度</strong><br>Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” <em>Journal of Consumer Research</em> 47, no. 5 (2021): 787-806.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;<strong>Copycats vs. original mobile apps</strong>: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</td>
<td>Y</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>文本相似度</strong><br>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>胡楠, 薛付婧 and 王昊楠, 2021. <strong>管理者短视主义影响企业长期投资吗</strong>———基于文本分析和机器学习. <em>管理世界</em>, <em>37</em>(5), pp.139-156.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>Kai Li, Feng Mai, Rui Shen, Xinyan Yan, <strong>Measuring Corporate Culture Using Machine Learning</strong>, The Review of Financial Studies, 2020</td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td><strong>女性就职高管改变组织内性别偏见</strong><br>Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 119, no. 9 (2022): e2026443119.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
</tbody>
</table>
<br>
<h2 id="主讲老师">主讲老师</h2>
<p>大邓，哈尔滨工业大学(HIT)管理学院信息管理系统方向在读博士。在多所大学分享数据采集和文本分析。运营公众号：大邓和他的Python，主要分享Python、爬虫、文本分析、机器学习等内容。</p>
<br>
<h2 id="一入门语法">一、入门语法</h2>
<ul>
<li>Python跟英语一样是一门语言</li>
<li>数据类型之字符串</li>
<li>数据类型之列表元组集合</li>
<li>数据类型之字典</li>
<li>数据类型之布尔值、None</li>
<li>逻辑语句(if&amp;for&amp;tryexcept)</li>
<li>列表推导式</li>
<li>理解函数</li>
<li>常用的内置函数</li>
<li>os路径库</li>
<li>内置库csv文件库</li>
<li>常见错误汇总</li>
</ul>
<h2 id="二数据采集">二、数据采集</h2>
<ul>
<li>网络爬虫原理</li>
<li>寻找网址规律</li>
<li>获取网页-requests库</li>
<li>pyquery库解析html网页</li>
<li><strong>案例:</strong> 豆瓣小说</li>
<li>json库解析json网页</li>
<li><strong>案例:</strong> 豆瓣电影</li>
<li><strong>案例:</strong> 微博</li>
<li><strong>案例:</strong> 文件下载</li>
<li><strong>案例:</strong> 上市公司定期报告pdf批量下载</li>
<li>区分动态网站与静态网站</li>
</ul>
<h2 id="三文本初识">三、文本初识</h2>
<ul>
<li>从信息传播视角重新认识文本</li>
<li>读取各类文件中的数据</li>
<li><strong>案例:</strong>  识别图片中的文本</li>
<li>数据清洗re库</li>
<li><strong>案例:</strong> 将多个数据文件汇总至一个csv文件</li>
<li><strong>案例:</strong> 中文jieba分词、词频统计、制作词云图</li>
<li><strong>案例:</strong> 使用共现(word2vec)法扩展情感词典</li>
<li><strong>案例:</strong> 使用词典做情感分析(无权重)</li>
<li><strong>案例:</strong> 数据分析pandas库快速入门</li>
<li><strong>案例:</strong> 使用pandas对excel中的文本进行情感分析</li>
</ul>
<h2 id="四文本进阶">四、文本进阶</h2>
<ul>
<li>文本分析与机器学习</li>
<li>特征工程-认识词袋法、one-hot、Tf-Idf、word2vec</li>
<li>将文档转为机器可处理的向量</li>
<li><strong>案例:</strong> 使用情感词典和tf-idf做情感分析（有权重）</li>
<li><strong>案例:</strong> 在线评论文本分类</li>
<li><strong>案例:</strong> 使用文本相似性识别变化(政策连续性)</li>
<li><strong>案例:</strong> Kmeans聚类算法、LDA话题模型</li>
<li>文本中的人类记忆(认知)</li>
<li>如何测量人类认知偏见(刻板印象)</li>
<li><strong>案例:</strong> 词向量模型的使用方法-豆瓣影评</li>
<li>文本分析在经管社科领域中的应用概述</li>
</ul>
<br>
<h2 id="参考文献">参考文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]沈艳, 陈赟 and 黄卓, 2019. 文本大数据分析在经济学和金融学中的应用: 一个文献综述. *经济学 (季刊)*, *18*(4), pp.1153-1186.
[2]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html
[3]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.*管理世界*.2016;5:81-98.
[4]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[5]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
[6]Loughran T, McDonald B. Textual analysis in accounting and finance: A survey[J]. *Journal of Accounting Research*, 2016, 54(4): 1187-1230. Author links open overlay panelComputational socioeconomics
[7]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &#34;Uniting the tribes: Using text for marketing insight.&#34; *Journal of Marketing* 84, no. 1 (2020): 1-25.
[8]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. &#34;A review of best practice recommendations for text analysis in R (and a user-friendly app).&#34; *Journal of Business and Psychology* 33, no. 4 (2018): 445-459.
[9]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. &#34;Lazy prices.&#34; *The Journal of Finance* 75, no. 3 (2020): 1371-1415.
[10]孟庆斌, 杨俊华, 鲁冰. 管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J]. *中国工业经济*, 2017 (12): 132-150.
[11]Wang, Quan, Beibei Li, and Param Vir Singh. &#34;Copycats vs. Original Mobile Apps: A Machine Learning Copycat-Detection Method and Empirical Analysis.&#34; *Information Systems Research* 29.2 (2018): 273-291.
[12]Hoberg, Gerard, and Gordon Phillips. 2016, Text-based network industries and endogenous product differentiation,?*Journal of Political Economy* 124, 1423-1465
[13]Loughran, Tim, and Bill McDonald. &#34;When is a liability not a liability? Textual analysis, dictionaries, and 10‐Ks.&#34; *The Journal of Finance* 66, no. 1 (2011): 35-65.
[14]Fairclough, Norman. 2003. Analysing discourse: Textual analysis for social research (Psychology Press)
[15]Grimmer, Justin, and Brandon M Stewart. 2013, Text as data: The promise and pitfalls of automatic content analysis methods for political texts, *Political analysis*21, 267-297.
[16]Markowitz, D. M., &amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).
[17]Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” Journal of Consumer Research 47, no. 5 (2021): 787-806.
[18]Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.
[19]Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &#34;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&#34; *Proceedings of the National Academy of Sciences* 119, no. 9 (2022): e2026443119.
</code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>视频分享 | 文本分析在经管研究中的应用</title>
      <link>https://textdata.cn/blog/2022-09-08-dufe-text-mining-in-ms/</link>
      <pubDate>Thu, 08 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-09-08-dufe-text-mining-in-ms/</guid>
      <description>为进一步深入学术交流，增进学术氛围，应电子商务系邀请，哈尔滨工业大学邓旭东博士将于2022年09月09日通过腾讯会议进行学术交流并作学术报告。报告以文本分析方法为例，围绕着文本产生、作用、算法、编程四个方面展开。报告人结合自己的最新研究对大数据时代文本分析方法在管理领域的应用展开讨论，介绍文本编码常见算法，诸如词典法、文档向量化、词向量等，分享此类研究的过程和要点。东北财经大学管理科学与工程学院。Application of Text Analysis in Economics and Management Research</description>
      <content:encoded><![CDATA[<p><a href="https://smse.dufe.edu.cn/content_69944.html"><img loading="lazy" src="img/cover.png" alt=""  />
</a></p>
<p><br><br></p>
<h2 id="slideshttpstextdatacnblog2022-09-08-dufe-text-mining-in-msslideshtml"><a href="https://textdata.cn/blog/2022-09-08-dufe-text-mining-in-ms/slides.html">Slides</a></h2>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1se4y1C7MV&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<p><br><br></p>
<h2 id="背景">背景</h2>
<p><img loading="lazy" src="img/multitudes-of-content-illustration.jpeg" alt=""  />
</p>
<p>在经管研究中，往往会涉及很多文本数据的编码。但是做研究面临两个问题:</p>
<h3 id="难题1--数据量大">难题1- 数据量大</h3>
<p>量太大，以至于废人力所能及。</p>
<p>时代发展，体现在数据上的特点就是数据大爆炸，过去做经管研究，使用访谈等研究方法，收录的文本内容，规模大多停留在M级。但是现在大数据时代，研究对象相关的文本数据，G级的数据量也是很常见的。</p>
<h3 id="难题2--格式乱">难题2- 格式乱</h3>
<p>信息存储技术发展，有应用不同场景的不同数据存储格式。数据可能是pdf、txt、docx，也可能是音频、视频等转录的文件。如果快捷整理，这也是个难点。</p>
<h3 id="难题3-难编码">难题3-难编码</h3>
<p>数据量少，可以人工阅读对数据进行理解和编码。但是当数据量大到无法处理的级别后，选择何种算法、各种算法技术的优缺点如何把握，对经管学者也是一个需要攻克的的技术难题。</p>
<p><img loading="lazy" src="img/consumer_org_society.png" alt=""  />
</p>
<p>难度大，但因为文本涉及的主体错综复杂，千丝万缕，所以可以研究很多对象。如个人、组织、社会之间的交互。</p>
<p><br><br></p>
<h2 id="编码解码理论">编码解码理论</h2>
<p>斯图亚特·霍尔在《电视话语的编码和解码》提出 『编码-解码理论』。该理论形成于70年代冷战时期，冷战中不两大阵营为了维护各自的社会稳定，为了在意识形态宣传中取胜，都在宣传工作中投入了重金。</p>
<p>当时的宣传工具是单向的广播模式，媒体作为统治阶级的喉舌，要将统治阶级的偏好、价值观等进行加工，生产相应意识形态内容。</p>
<p>而普罗大众，作为内容的接受者， 一成长于该特定意识形态的社会，同时又有一定的自我意识，所以对于一个宣传内容可能会有三种反应，表里都认同、表认同里不认同、表里都不认同。</p>
<p><img loading="lazy" src="img/SenderReceiver.png" alt=""  />
</p>
<h3 id="使用文本想清楚两个问题">使用文本想清楚两个问题</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- How text reflects its Sender？
- How text impacts its Receiver？
</code></pre></div><h3 id="使用文本明晰三个角度">使用文本明晰三个角度</h3>
<p>我做的研究使用的文本数据，涉及哪些角色、作用力方向、感兴趣的内容。</p>
<ul>
<li>角色: Sender or Receiver</li>
<li>方向: Reflect or Impact</li>
<li>内容: Sender的意识(认知、偏好、&hellip;)   vs  Receiver的意识(认知、偏好、&hellip;)</li>
</ul>
<p>下面是经管领域研究部分汇总，每个学者根据自己学科研究对象，应该能在4*4的矩阵中找到自己对应的位置</p>
<p><img loading="lazy" src="img/%e7%94%9f%e4%ba%a7%e4%b8%8e%e6%b6%88%e8%b4%b9.png" alt=""  />
</p>
<blockquote>
<p>Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &ldquo;Uniting the tribes: Using text for marketing insight.&rdquo; Journal of Marketing 84, no. 1 (2020): 1-25.</p>
</blockquote>
<p><br><br></p>
<h2 id="人工编码与机器编码">人工编码与机器编码</h2>
<p><img loading="lazy" src="img/unstructrueddata.png" alt=""  />
</p>
<p>做研究需要有干净的数据做实证分析，最为理想的是表数据，例如excel文件，每一行代表一条记录，每一列代表一个字段。编码的作用就是将非机构化的、脏乱的数据整理为干净整洁的表数据。</p>
<p>要明确编码方法的优点和缺点，在合理的适用范围使用。对于文本数据的编码，需要理解人工和机器两种编码方式的优缺点</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th>分析方法</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">人工编码</td>
<td>质性（扎根）</td>
<td>少量数据，深刻洞见。</td>
<td>难以应对大数据；<br>编码标准不统一；</td>
</tr>
<tr>
<td style="text-align:left">机器编码</td>
<td>词频、向量相似度、向量距离</td>
<td>标准如一;<br>适合大规模文本挖掘；</td>
<td>需要破坏文本的结构，<br>丧失了部分信息量</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="机器编码-将文本转为数字或向量">机器编码-将文本转为数字或向量</h2>
<ul>
<li>
<p>符号法(每个词对应一个数字)</p>
<ul>
<li>词典(词频)法</li>
<li>词袋法、TF-IDF</li>
</ul>
</li>
<li>
<p>词嵌入(每个词对应一个向量)</p>
</li>
</ul>
<p>符号法算法假设词语彼此是语义不相关的，目的是把 <strong>文本</strong> 转为某个数字或<strong>向量</strong>。</p>
<p>而词嵌入算法假设不同的词语是由n维个语义组成的线性组合，目的是把 <strong>词语</strong> 转为<strong>向量</strong>。</p>
<br>
<h3 id="符号法">符号法</h3>
<p>符号法就是数某个词或某类词的出现次数(或占比)。符合法是计算机NLP领域的专业叫法，在经管社科领域，最常见的文本分析软件<a href="https://textdata.cn/blog/liwc_python_text_mining/">LIWC</a>其实也是符号法。而LIWC全(Linguistic Inquiry and Word Count，即语义查询与词频统计。</p>
<p><img loading="lazy" src="img/symbol-representation-1.png" alt=""  />
</p>
<h3 id="符号法的应用">符号法的应用</h3>
<table>
<thead>
<tr>
<th>概念</th>
<th>测量方法</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>认真(努力)</strong></td>
<td>测量文本中词语的个数</td>
</tr>
<tr>
<td><strong>情感</strong></td>
<td>使用情感词典，统计文本中正面词占比</td>
</tr>
<tr>
<td><strong>可读性</strong></td>
<td>文本中高难度(或专业性)词占比</td>
</tr>
<tr>
<td><strong>客观性</strong></td>
<td>文本中某个值的方差，如情感<br>- A<code>产品不错， 包装破损， 态度很好， 综合还是推荐大家购买!</code> [5, 1, 5, 4]<br>- B<code>产品垃圾，使用垃圾， 包装破损， 差评!!</code> [1,  1,  1,  1]<br>A的方差更大，更客观</td>
</tr>
<tr>
<td><strong>相似性(政策稳定性)</strong></td>
<td>cosine(text_vector1, text_vector2)</td>
</tr>
<tr>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>
<br>
<h3 id="词嵌入">词嵌入</h3>
<p>词嵌入技术有 Word2Vec、Glove，这类技术是挖掘出每个词的上下文语境，通俗的说法就是让计算机，对同样的文章数据，做千万次、上亿次完形填空。这样每个词语都有独特的上下文语义，并以n维向量形式表示，所以词嵌入也可以称之为词向量。</p>
<p><strong>向量模型有近义词相近、概念类似的平行两个特点</strong>。分别举几个例子，方便大家理解。</p>
<p>语义空间是n维，为了便于理解，将其压缩至二维空间。中学的向量大家都比较熟悉，在二维坐标中空间中，两个点的连线可以组成新的向量，相同的向量是平行的。</p>
<p>而在下图的2维语义空间中，good、best语义更接近，所以空间距离更近。同理bad、worst更近。</p>
<p>而vector(good, best)、vector(bad, worst)这两个向量均表示<code>原形-&gt;最高级</code>, 语义向量会近似平行。</p>
<p>同理， vector(good, bad)、 vector(best, worst)两个向量表示 <code>好-&gt;差</code>，语义向量也会近似平行。</p>
<p><img loading="lazy" src="img/embeddings-based.png" alt=""  />
</p>
<br>
<h3 id="词嵌入与认知">词嵌入与认知</h3>
<p>刚刚词嵌入的语义空间中的几个例子，其实就体现了语言的记忆。语义记录了使用该语言的人的记忆。不同的组织，对于同一种概念，会有不同的偏好。例如， Nature2022使用大规模语料数据训练出的词向量，发现语言中残存着人类的某些认知记忆。</p>
<p>通过构建概念词组对儿，在空间中投影，就可以挖掘出词语的在该概念中的分值。例如，使用</p>
<ul>
<li>SMALL = [small, tiny, little&hellip;]</li>
<li>BIG = [big, mega, large&hellip;]</li>
</ul>
<p>每个词都是一个n维的向量，SMALL或BIG都能计算出一个均值向量。大家记得中学的向量投影不，Nature2022就使用这个朴素的方法测量每个动物名称所蕴含的人类尺寸认知。</p>
<p><img loading="lazy" src="img/Concept_Words_Project.png" alt=""  />
</p>
<blockquote>
<p>Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Nature Human Behaviour, pp.1-13.</p>
</blockquote>
<br>
<h3 id="技术对比">技术对比</h3>
<p>这里做个表格对比，大家自己感受下三种技术的异同。</p>
<table>
<thead>
<tr>
<th>技术</th>
<th>技术</th>
<th>维度类比</th>
<th>任务</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>符号法-字典</strong>（词频）</td>
<td>数个数</td>
<td>原子</td>
<td>统计每句话里的名词个数</td>
<td>sent_num1 = 2<br>sent_num2 = 1</td>
</tr>
<tr>
<td><strong>符号法-词袋</strong></td>
<td>bag of words<br>one-hot<br>Tf-idf</td>
<td>分子</td>
<td>转化为词向量, 计算两个句子相似度。</td>
<td>vec1 = [1, 1, 1, 1, 1, 0]<br>vec2 = [0, 1, 0, 1, 0, 1]<br>similarity = cosine(vec1, vec2)</td>
</tr>
<tr>
<td><strong>词嵌入</strong></td>
<td>word2vec、<br>glove等</td>
<td>中子、质子、电子</td>
<td>词语相似度。(语义上大小相近，方向相反; 态度、偏见)</td>
<td>mom = [0.2, 0.7, 0.1]<br/>dad   = [0.3, 0.5, -0.2]</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="经管-文本分析-文献">经管-文本分析-文献</h2>
<p>在这里我把技术细分为词频、词袋、w2v建词典、w2v认知变迁四个维度，整理了经管7篇论文。大家可以阅读这7篇论文，掌握文本分析的应用场景。</p>
<table>
<thead>
<tr>
<th>文献</th>
<th>定性</th>
<th>词频</th>
<th>词袋</th>
<th>W2V建词典</th>
<th>W2V认知变迁</th>
</tr>
</thead>
<tbody>
<tr>
<td>王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</td>
<td>Y</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/jcr_concreteness_computation/">语言具体性如何影响顾客满意度</a><br>Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” <em>Journal of Consumer Research</em> 47, no. 5 (2021): 787-806.</td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2019-12-08-lazy-prices/">文本相似度</a><br>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>胡楠, 薛付婧 and 王昊楠, 2021. <a href="https://textdata.cn/blog/text_mining_in_2021_management_world/">管理者短视主义</a>影响企业长期投资吗———基于文本分析和机器学习. <em>管理世界</em>, <em>37</em>(5), pp.139-156.</td>
<td></td>
<td>Y</td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>Kai Li, Feng Mai, Rui Shen, Xinyan Yan, <a href="https://github.com/MS20190155/Measuring-Corporate-Culture-Using-Machine-Learning">Measuring Corporate Culture Using Machine Learning</a>, The Review of Financial Studies, 2020</td>
<td></td>
<td></td>
<td>Y</td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>女性就职高管改变组织内性别偏见<br>Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 119, no. 9 (2022): e2026443119.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
<tr>
<td>使用词嵌入技术，量化近百年以来性别和族群的刻板印象<br>Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. &ldquo;Word embeddings quantify 100 years of gender and ethnic stereotypes.&rdquo; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="案例">案例</h2>
<h3 id="案例1-众筹语言风格">案例1-众筹语言风格</h3>
<p>王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</p>
<blockquote>
<p>众筹融资效果决定着众筹平台的兴衰。 众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。 而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。 不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。 首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。</p>
<p>然后，<strong>借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。</strong></p>
<p>最后，建立语言说服风格对项目筹资影响的计量模型，并对 <strong>Kickstarter 平台上的 128345 个项目进行实证分析</strong>。 总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。</p>
</blockquote>
<p><img loading="lazy" src="img/%e4%bc%97%e7%ad%b9-%e7%a7%8d%e5%ad%90%e8%af%8d.png" alt=""  />

<img loading="lazy" src="img/%e4%bc%97%e7%ad%b9-%e6%b5%81%e7%a8%8b%e5%9b%be.png" alt=""  />
</p>
<br>
<h3 id="案例2-山寨-vs-原创">案例2 山寨 vs 原创</h3>
<p>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</p>
<blockquote>
<p><strong>进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原创app或模仿app，可检测两种模仿者的方法：欺骗性和非欺骗性。</strong></p>
<p>根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的<strong>5,141个开发人员的10,100个动作游戏应用程序</strong>样本中，模仿app对原创app需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原创app的需求。</p>
<p>结果表明，从总体上讲，模仿app对原创app需求的影响在统计上是微不足道的。<strong>我们的研究通过提供一种识别模仿app的方法</strong>，并提供模仿app对原创app需求影响的证据，为越来越多的移动应用消费文献做出了贡献。</p>
</blockquote>
<p><img loading="lazy" src="img/copycat.png" alt=""  />
</p>
<br>
<h3 id="案例3-lazy-prices文本相似性">案例3 Lazy prices文本相似性</h3>
<p>Cohen, L., Malloy, C. and Nguyen, Q., 2020. <a href="https://textdata.cn/blog/2019-12-08-lazy-prices/">Lazy prices</a>. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</p>
<blockquote>
<p>之前的研究认为，尽管投资者一次对包含重大变化的财务报表的发布作出了迅时反应，但随着时间的流逝，这种公告作用是会减弱的(Brown and Tucker, 2011 and Feldman et al., 2010)。这表示10-K报告会随着时间推移，信息价值大打折扣。尽管我们复现了这个事实，即与常规文件的变更没有重大的公告效应，但我们认为，前人的研究忽略了更重要部分(如MD&amp;A)对对资产价格的影响。</p>
</blockquote>
<blockquote>
<p>确切的说，<strong>并不是报告的披露效应的信息价值变低了，而是投资者越来越难以发现报告中微妙的信息变化， 比如因为报告变得越来越冗杂。投资者只有看到某些新闻后，才会逐渐意识到之前公司报告内容变化的的真正价值</strong>。</p>
<p>使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对<strong>报告进行积极更改</strong>时，这种行为<strong>蕴含着</strong>公司未来运营的<strong>重要信号</strong>。</p>
<p><strong>财务报告的语言和结构的变化也对公司的未来收益产生重大影响</strong>：做空&quot;变化&quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。</p>
</blockquote>
<p><img loading="lazy" src="img/lazy-prices-1.png" alt=""  />

<img loading="lazy" src="img/lazy-prices-2.png" alt=""  />
</p>
<br>
<h3 id="案例3-女性就职高管改变组织内性别刻板印象-pnas2022">案例3-女性就职高管改变组织内性别刻板印象 PNAS2022</h3>
<p>Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 119, no. 9 (2022): e2026443119.</p>
<blockquote>
<p>女性在领导职位上的代表性仍然不足。这种代表性不足至少部分是由将男性与成就导向的代理特征（例如，自信和果断）联系起来的性别刻板印象驱动的。这些刻板印象在语言中得到表达和延续，女性被描述的方式比男性少。目前的研究表明，任命女性担任高层管理人员可以减轻这些以语言表达的根深蒂固的刻板印象。我们使用自然语言处理技术分析了超过 <strong>43,000 份包含 12.3 亿字的文档，发现聘用女性首席执行官和董事会成员与组织使用语言的变化有关，因此女性的语义变得更类似于代理的语义</strong>。换句话说，雇用女性担任领导职务有助于将女性与对领导成功至关重要的特征联系起来。重要的是，我们的研究结果表明，通过增加女性代表来改变组织语言可能会为女性提供摆脱双重束缚的途径：当女性领导人被任命担任权力职位时，女性与代理的积极方面（例如，独立和自信）在语言上，不以减少与社区的联系（例如，善良和关怀）为代价。总而言之，我们的研究结果表明，女性代表不仅是目的，而且是系统地改变阴险的性别刻板印象并克服女性被认为是有能力或可爱的权衡的一种手段。</p>
<p>本文使用的词向量， 刻画研究对象的文化认知，是依对象依时间而变化的。</p>
</blockquote>
<p><img loading="lazy" src="img/hiring_women.png" alt=""  />
</p>
<br>
<h3 id="案例4--使用词嵌入技术量化近百年以来性别和族群的刻板印象-pnas2018">案例4- 使用词嵌入技术，量化近百年以来性别和族群的刻板印象 PNAS2018</h3>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1b4411X7i1&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<h2 id="关于合作与交流">关于合作与交流</h2>
<p>如果正在推进的项目，需要用到文本分析，欢迎交流与合作 ~</p>
<p>可加微信372335839， 备注【姓名-学校-专业】, 说明来意。如果想系统获取技术细节，课程 <a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a> 内都有的技术代码和讲解，欢迎了解。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>ManagementScience | 使用网络算法识别创新的颠覆性与否</title>
      <link>https://textdata.cn/blog/2022-09-07-management-science-disrupt-science-and-technology/</link>
      <pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-09-07-management-science-disrupt-science-and-technology/</guid>
      <description>The CD index is a new approach to finding important points in evolving networks. When applied to large-scale data sets like U.S. patent citations, the index is useful for identifying influential innovations and other features of technological change.</description>
      <content:encoded><![CDATA[


<p>颠覆式创新是一个很火的概念，在创新创业、科学学等研究中，每个专利、论文的正文中都会引用关系，而引用关系会构成一个引用网络。</p>
<p>那么创新如何从网络形态进行区分，如何计算网络节点的创新程度，本文列举两篇与此相关的论文，分别是 Management science 和 Science 。</p>
<p><br><br></p>
<div id="文献摘要" class="section level2">
<h2>文献摘要</h2>
<p><strong>Funk, Russell J., and Jason Owen-Smith. “A dynamic network measure of technological change.” <em>Management science</em> 63, no. 3 (2017): 791-817.</strong></p>
<p>该文使用网络分析方法研究技术变革，论文认为 <strong>颠覆性的新发明，通过将发明者的注意力转移到或远离这些发明所依赖的知识，来重塑相互关联的技术网络。即更广的视野或更久远的视角，往往有利于颠覆性创新的产生</strong>。<strong>基于该思路，本文开发了新发明的颠覆性与否的计算指标cdindex</strong>。我们将这些指标应用于大学研究商业化的分析，并发现 <strong>联邦研究资金推动校园产生颠覆性创新，而商业联系会有利于巩固现状的创新</strong>。通过量化新技术，我们提出的指数允许基于专利的创新研究捕捉概念上重要的现象， 这些现象无法通过既定措施检测到。该测量方法提供了支持创新、创业、技术战略、科学政策和社会网络理论研究的理论发展的经验见解。</p>
<blockquote>
<p>Abstract: This article outlines a network approach to the study of technological change. We propose that new inventions reshape networks of interlinked technologies by shifting inventors’ attention to or away from the knowledge on which those inventions build. Using this approach, we develop novel indexes of the extent to which a new invention consolidates or destabilizes existing technology streams. We apply these indexes in analyses of university research commercialization and ﬁnd that, although federal research funding pushes campuses to create inventions that are more destabilizing, deeper commercial ties lead them to produce technologies that consolidate the status quo. By quantifying the eﬀects that new technologies have on their predecessors, the indexes we propose allow patent-based studies of innovation to capture conceptually important phenomena that are not detectable with established measures. The measurement approach presented here oﬀers empirical insights that support theoretical development in studies of innovation, entrepreneurship, technology strategy, science policy, and social network theory.</p>
</blockquote>
<p><br></p>
<p><strong>Wu, Lingfei, Dashun Wang, and James A. Evans. “Large teams develop and small teams disrupt science and technology.” Nature 566, no. 7744 (2019): 378-382.</strong></p>
<p>当今科学和技术最普遍的趋势之一是各个领域的大型团队的增长，因为孤独的研究人员和小型团队的流行程度正在减少 。团队规模的增加归因于科学活动的专业化、通信技术的改进 或需要跨学科解决方案的现代问题的复杂性。团队规模的这种转变引发了一个问题，即大团队所产生的科技特征是否以及如何不同于小团队。分析了 1954-2014 年期间超过 6500 万篇论文、专利和软件产品，证明在此期间，<strong>较小的团队倾向于将拉长到更大的时间尺度，借鉴过去，用新的想法和机会来颠覆科学和技术；而较大的团队倾向于聚焦于当前流行的，完善当前现有的</strong>。不论团队大小，均对于蓬勃发展的科学技术生态至关重要，并表明，为实现这一目标，科学政策应旨在支持团队规模的多样性。</p>
<blockquote>
<p>Abstract: One of the most universal trends in science and technology today is the growth of large teams in all areas, as solitary researchers and small teams diminish in prevalence. Increases in team size have been attributed to the specialization of scientific activities,
improvements in communication technology, or the complexity
of modern problems that require interdisciplinary solutions.This shift in team size raises the question of whether and how the character of the science and technology produced by large teams differs from that of small teams. Here we analyse more than 65 million papers, patents and software products that span the period 1954–2014, and demonstrate that across this period smaller teams have tended to disrupt science and technology with new ideas and opportunities, whereas larger teams have tended to develop existing ones. Work from larger teams builds on morerecent and popular developments, and attention to their work comes
immediately. By contrast, contributions by smaller teams search more deeply into the past, are viewed as disruptive to science and technology and succeed further into the future—if at all. Observed differences between small and large teams are magnified for higherimpact work, with small teams known for disruptive work and large teams for developing work. Differences in topic and research design
account for a small part of the relationship between team size and disruption; most of the effect occurs at the level of the individual, as people move between smaller and larger teams. These results demonstrate that both small and large teams are essential to a flourishing ecology of science and technology, and suggest that, to achieve this, science policies should aim to support a diversity of team sizes.</p>
</blockquote>
<p><br><br></p>
</div>
<div id="算法对比" class="section level2">
<h2>算法对比</h2>
<p>我没阅读两篇论文，仅就颠覆性与否的计算方法和图例，感觉算法实现差不多。</p>
<div class="figure">
<img src="img/cdindex-managent_science_2017.png" alt="" />
<p class="caption">上图为2017年Management Science的插图</p>
</div>
<p><br></p>
<div class="figure">
<img src="img/disruption_nature_2019.png" alt="" />
<p class="caption">上图为2019年Nature的插图</p>
</div>
<p><br><br></p>
</div>
<div id="代码数据" class="section level2">
<h2>代码数据</h2>
<p>下面分别为Management2017和Nature2019的主页，均含数据和代码。</p>
<p><a href="http://russellfunk.org/cdindex/"><img src="img/cdindex-homepage.png" /></a></p>
<p><br></p>
<p><a href="https://lingfeiwu.github.io/smallTeams/"><img src="img/nature2019-disrupt-homepage.png" /></a></p>
<p><br><br></p>
</div>
<div id="算法实现" class="section level2">
<h2>算法实现</h2>
<p>按照时间优先原则，本文就只分享Management2017论文作者Funk, Russell开源了cdindex库 (开发语言C和Python) ，安装</p>
<p><br></p>
<pre><code>pip3 install cdindex</code></pre>
<p>将Management2017 cdindex算法图 标注为如下图， 下图中左右两个网络节点是相同的，只需构造一套节点，两套边数据即可完成实验。</p>
<p><img src="img/cdindex-managent_science_2017_demo.png" /></p>
<p><br></p>
<p>我们就直接上代码</p>
<pre class="python"><code>import cdindex
import datetime

#节点，理解为专利号或者论文doi号；同时节点有先后时间属性
vertices = [{&quot;name&quot;: &quot;x1&quot;, &quot;time&quot;: datetime.datetime(1990, 1, 1)},
           {&quot;name&quot;: &quot;x2&quot;, &quot;time&quot;: datetime.datetime(1990, 1, 1)},
           {&quot;name&quot;: &quot;x3&quot;, &quot;time&quot;: datetime.datetime(1990, 1, 1)},
           {&quot;name&quot;: &quot;x4&quot;, &quot;time&quot;: datetime.datetime(1990, 1, 1)},
        
           {&quot;name&quot;: &quot;y&quot;, &quot;time&quot;: datetime.datetime(1991, 1, 1)},
          
           {&quot;name&quot;: &quot;z1&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)},
           {&quot;name&quot;: &quot;z2&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)},
           {&quot;name&quot;: &quot;z3&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)},
           {&quot;name&quot;: &quot;z4&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)},
           {&quot;name&quot;: &quot;z5&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)},
           {&quot;name&quot;: &quot;z6&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)}]
           
    
#edges_1边关系
#edges_1中的y为颠覆型
edges_1 = [{&quot;source&quot;: &quot;z1&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z2&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z3&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z4&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z5&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z6&quot;, &quot;target&quot;: &quot;y&quot;},
           
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x1&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x2&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x3&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x4&quot;}]


#edges_2边关系 
#edges_2中的y为巩固型
edges_2 = [{&quot;source&quot;: &quot;z1&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z2&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z3&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z4&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z5&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z6&quot;, &quot;target&quot;: &quot;y&quot;},
           
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x1&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x2&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x3&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x4&quot;},
          
          {&quot;source&quot;: &quot;z1&quot;, &quot;target&quot;: &quot;x1&quot;},
          {&quot;source&quot;: &quot;z2&quot;, &quot;target&quot;: &quot;x1&quot;},
          {&quot;source&quot;: &quot;z3&quot;, &quot;target&quot;: &quot;x2&quot;},
           
          {&quot;source&quot;: &quot;z4&quot;, &quot;target&quot;: &quot;x3&quot;},
          {&quot;source&quot;: &quot;z5&quot;, &quot;target&quot;: &quot;x3&quot;},
          {&quot;source&quot;: &quot;z5&quot;, &quot;target&quot;: &quot;x4&quot;},
          {&quot;source&quot;: &quot;z6&quot;, &quot;target&quot;: &quot;x4&quot;}]



# 构建两个网络
graph1 = cdindex.Graph() #颠覆型
graph2 = cdindex.Graph() #发展型

# 添加节点
for vertex in vertices:
    graph1.add_vertex(vertex[&quot;name&quot;], cdindex.timestamp_from_datetime(vertex[&quot;time&quot;]))
    graph2.add_vertex(vertex[&quot;name&quot;], cdindex.timestamp_from_datetime(vertex[&quot;time&quot;]))

# 添加引用关系
for edge in edges_1:
    graph1.add_edge(edge[&quot;source&quot;], edge[&quot;target&quot;])
for edge in edges_2:
    graph2.add_edge(edge[&quot;source&quot;], edge[&quot;target&quot;])
    
    
#y研究发布后1825天内，引用y的论文(专利)列入网络。
t_delta = int(datetime.timedelta(days=1825).total_seconds())

#计算cdindex得分
score1 = graph1.cdindex(&quot;y&quot;, t_delta)
score2 = graph2.cdindex(&quot;y&quot;, t_delta)

print(&#39;左侧-网络中的y节点的cdinex得分: {}, 节点y 为颠覆性创新&#39;.format(score1))</code></pre>
<pre><code>## 左侧-网络中的y节点的cdinex得分: 1.0, 节点y 为颠覆性创新</code></pre>
<p><br></p>
<pre class="python"><code>print(&#39;右侧-网络中的y节点的cdinex得分: {}, 节点y 为发展性创新&#39;.format(score2))</code></pre>
<pre><code>## 右侧-网络中的y节点的cdinex得分: -1.0, 节点y 为发展性创新</code></pre>
<p><br><br></p>
</div>
<div id="cdindex" class="section level2">
<h2>cdindex</h2>
<p>对比Python的结果，与论文计算过程，完全一致。cdindex内部实现我不太熟悉，如果想了解cdindex内部实现，可前往 <a href="https://github.com/russellfunk/cdindex" class="uri">https://github.com/russellfunk/cdindex</a> 阅读cdindex库的源码。
<img src="img/cdindex-managent_science_2017.png" /></p>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>17G数据 | 企业社会责任报告数据集</title>
      <link>https://textdata.cn/blog/coporate_social_responsibility_datasets/</link>
      <pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/coporate_social_responsibility_datasets/</guid>
      <description>近年来，企业社会责任（csr)已成为全球学术界研究的热点。国内外各大顶刊都先后刊登了多篇关于csr的文章，比如《企业绿色创新实践如何破解“和谐共生”难题？》（发表于管理世界）、《负责任的国际投资：ESG与中国OFDI》（发表于经济研究）、《Is my company really doing good? Factors influencing employees&amp;#39; evaluation of the authenticity of their company&amp;#39;s corporate social responsibility engagement》（发表于JBR）等。这些文章核心变量的构建大都基于对企业社会责任报告的内容分析和挖掘。比如《企业绿色创新实践如何破解“和谐共生”难题？》的被解释变量（绿色创新）以及部分解释变量（二元合法性和伦理型领导）。可见，社会责任报告对于我们研究esg至关重要。因此，接下来小编就带大家爬取深交所上市公司历年的社会责任报告，希望能够给大家带来一些帮助。In recent years, corporate social responsibility (csr) has become a research hotspot in the global academic circles. Major top journals at home and abroad have successively published many articles on CSR, such as How can corporate green innovation practice solve the problem of harmonious symbiosis? (published in Governance World), Responsible International Investment: ESG and China&amp;#39;s OFDI (published in Economic Research), Is my company really doing good? Factors influencing employees&amp;#39; evaluation of the authenticity of their company&amp;#39;s corporate social responsibility engagement (published in JBR) et al. The construction of core variables in these articles is mostly based on the content analysis and mining of corporate social responsibility reports. For example, How can corporate green innovation practice solve the problem of harmonious symbiosis? 》The explained variable (green innovation) and part of the explanatory variables (dual legitimacy and ethical leadership). It can be seen that the social responsibility report is very important for us to study esg. Therefore, the next editor will take you to crawl the social responsibility reports of companies listed on the Shenzhen Stock Exchange over the years, hoping to bring you some help.</description>
      <content:encoded><![CDATA[<blockquote>
<p>作者:张延丰 哈工程在读博士</p>
</blockquote>
<p>近年来，企业社会责任（csr)已成为全球学术界研究的热点。国内外各大顶刊都先后刊登了多篇关于csr的文章，比如《企业绿色创新实践如何破解“和谐共生”难题？》（发表于管理世界）、《负责任的国际投资：ESG与中国OFDI》（发表于经济研究）、《Is my company really doing good? Factors influencing employees' evaluation of the authenticity of their company&rsquo;s corporate social responsibility engagement》（发表于JBR）等。这些文章核心变量的构建大都基于对企业社会责任报告的内容分析和挖掘。比如《企业绿色创新实践如何破解“和谐共生”难题？》的被解释变量（绿色创新）以及部分解释变量（二元合法性和伦理型领导）。可见，社会责任报告对于我们研究esg至关重要。因此，接下来小编就带大家爬取深交所上市公司历年的社会责任报告，希望能够给大家带来一些帮助。</p>
<br>
<h2 id="获取数据集">获取数据集</h2>
<p>采集4000多个pdf文件。经过数据清洗，将20G的pdf数据，汇总整理到170M的csv文件内。</p>
<p><img loading="lazy" src="img/datasets.png" alt=""  />
</p>
<p>数据整理不易，如需获取本数据集，<strong>请转发本文至朋友圈集赞满30+</strong>， 加微信【372335839】，备注【深圳ESG数据集】</p>
<img src="img/wechat.jpg" style="zoom:50%;" />
<p><br><br></p>
<h2 id="一构建网络爬虫">一、构建网络爬虫</h2>
<p>数据采集分为多个步骤</p>
<ol>
<li>找网址规律(GET or POST), 构造url参数</li>
<li>伪装请求，防止被封</li>
<li>构造csv，存储信心</li>
<li>执行整个爬虫</li>
</ol>
<h3 id="11-url">1.1 url</h3>
<p>打开X交所的 <a href="http://www.szse.cn/disclosure/listed/notice/">http://www.szse.cn/disclosure/listed/notice/</a> ，同时打开浏览器开发者工具network面板，在截图左侧输入框输入关键词 『社会责任报告』，按下回车。</p>
<p>此时开发者工具network面板出现很多网络交换信息， 点击检查发现下图</p>
<p><img loading="lazy" src="img/01-%e7%bd%91%e5%9d%80%e8%a7%84%e5%be%8b.png" alt=""  />
</p>
<p>发现该页面数据是<strong>POST</strong>请求，网址为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">http://www.szse.cn/api/disc/announcement/annList?random=random参数
</code></pre></div><h3 id="12-headers">1.2 headers</h3>
<p>同时也能发现伪装头参数，现将两个重要信息整理为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://www.szse.cn/api/disc/announcement/annList?random=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">())</span>

<span class="c1">#伪装头</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Accept&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json, text/javascript, */*; q=0.01&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Encoding&#39;</span><span class="p">:</span> <span class="s1">&#39;gzip, deflate&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;zh-CN,zh;q=0.9,en;q=0.8&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Content-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Host&#39;</span><span class="p">:</span> <span class="s1">&#39;www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Origin&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Proxy-Connection&#39;</span><span class="p">:</span> <span class="s1">&#39;close&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Referer&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn/disclosure/listed/fixed/index.html&#39;</span><span class="p">,</span>
           <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Request-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;ajax&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Requested-With&#39;</span><span class="p">:</span> <span class="s1">&#39;XMLHttpRequest&#39;</span><span class="p">}</span>
</code></pre></div><h3 id="13-data参数">1.3 data参数</h3>
<p>POST请求需要构造data参数，在开发者对应于payload, 整理为Python格式</p>
<p><img loading="lazy" src="img/02-payload.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">
</code></pre></div><p><br><br></p>
<h3 id="14-preview">1.4 preview</h3>
<p>看到左侧渲染后的数据，同时也能在开发者工具network面板看到肉眼背后的源数据。我们使用preview预览截图再次确认网址规律没有问题。</p>
<p><img loading="lazy" src="img/03-data-preview.jpg" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">keyword</span> <span class="o">=</span> <span class="s1">&#39;社会责任报告&#39;</span>
<span class="n">page</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#post方法参数</span>
<span class="n">payload</span> <span class="o">=</span><span class="p">{</span><span class="s2">&#34;seDate&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">,</span><span class="s2">&#34;&#34;</span><span class="p">],</span>
           <span class="s2">&#34;searchKey&#34;</span><span class="p">:</span> <span class="p">[</span><span class="n">keyword</span><span class="p">],</span>
           <span class="s2">&#34;channelCode&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;listedNotice_disc&#34;</span><span class="p">],</span>
           <span class="s2">&#34;pageSize&#34;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
           <span class="s2">&#34;pageNum&#34;</span><span class="p">:</span> <span class="n">page</span><span class="p">}</span>
</code></pre></div><h3 id="15-csv">1.5 csv</h3>
<p>现在已经把爬虫最重要的工作做完了，剩下的就是想办法构造出csv，并将数据存入csv。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#定义csv字段，存储PDF链接信息至data/esg_links.csv</span>
<span class="n">csvf</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/esg_links.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf_link&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">]</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
<span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pdf_link&#39;</span><span class="p">:</span> <span class="s1">&#39;测试pdf文件链接&#39;</span><span class="p">,</span>
             <span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="s1">&#39;测试股票代码&#39;</span><span class="p">,</span>
             <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;股票名称&#39;</span><span class="p">,</span>
             <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="s1">&#39;报告名称&#39;</span><span class="p">,</span>
             <span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="s1">&#39;发布日期&#39;</span><span class="p">,</span>
             <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="s1">&#39;pdf文件字节大小&#39;</span><span class="p">,</span>
             <span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="s1">&#39;数据id&#39;</span><span class="p">}</span>
             
<span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

<span class="c1">#关闭csv</span>
<span class="n">csvf</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div><p><br><br></p>
<h2 id="二爬虫代码完整">二、爬虫代码(完整)</h2>
<p>当你看到本文时，该完整代码很有可能会随着网站变化而失效。不要悲伤难过， 按照爬虫思路自己diy即可。如果没有爬虫基础，学习 <a href="https://www.bilibili.com/video/BV1AE411r7ph">大邓的B站爬虫视频</a> ，</p>
<p><img loading="lazy" src="img/%e5%a4%a7%e9%82%93%e7%88%ac%e8%99%ab.jpg" alt=""  />
</p>
<p>自己懂爬虫原理diy代码，比改别人的代码来的更容易。将前面的准备工作组织起来, 就形成了下面的完整代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">csv</span>

<span class="n">keyword</span> <span class="o">=</span> <span class="s2">&#34;社会责任报告&#34;</span>

<span class="c1">#定义csv字段，存储PDF链接信息至data/esg_links.csv</span>
<span class="n">csvf</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/esg_links.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf_link&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">]</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
<span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

<span class="c1">#伪装头</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Accept&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json, text/javascript, */*; q=0.01&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Encoding&#39;</span><span class="p">:</span> <span class="s1">&#39;gzip, deflate&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;zh-CN,zh;q=0.9,en;q=0.8&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Content-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Host&#39;</span><span class="p">:</span> <span class="s1">&#39;www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Origin&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Proxy-Connection&#39;</span><span class="p">:</span> <span class="s1">&#39;close&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Referer&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.szse.cn/disclosure/listed/fixed/index.html&#39;</span><span class="p">,</span>
           <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Request-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;ajax&#39;</span><span class="p">,</span>
           <span class="s1">&#39;X-Requested-With&#39;</span><span class="p">:</span> <span class="s1">&#39;XMLHttpRequest&#39;</span><span class="p">}</span>

<span class="n">page</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#post方法参数</span>
<span class="n">payload</span> <span class="o">=</span><span class="p">{</span><span class="s2">&#34;seDate&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;&#34;</span><span class="p">,</span><span class="s2">&#34;&#34;</span><span class="p">],</span>
           <span class="s2">&#34;searchKey&#34;</span><span class="p">:</span> <span class="p">[</span><span class="n">keyword</span><span class="p">],</span>
           <span class="s2">&#34;channelCode&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;listedNotice_disc&#34;</span><span class="p">],</span>
           <span class="s2">&#34;pageSize&#34;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
           <span class="s2">&#34;pageNum&#34;</span><span class="p">:</span> <span class="n">page</span><span class="p">}</span>

<span class="c1">#发起请求</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://www.szse.cn/api/disc/announcement/annList?random=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">())</span>
<span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span>
                     <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                     <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">))</span>

<span class="c1">#当data关键词有对应的非空列表，循环一直进行。</span>
<span class="k">while</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s1">&#39;data&#39;</span><span class="p">]:</span>
    <span class="n">payload</span><span class="p">[</span><span class="s1">&#39;pageNum&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">page</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span>
                     <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                     <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">))</span>
    
    <span class="n">esgs</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">esg</span> <span class="ow">in</span> <span class="n">esgs</span><span class="p">:</span>
        <span class="c1">#以字典样式写入csv</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pdf_link&#39;</span><span class="p">:</span> <span class="s1">&#39;http://disc.static.szse.cn/download&#39;</span><span class="o">+</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;attachPath&#39;</span><span class="p">],</span>
                <span class="c1">#为防止股票代码被exel等软件识别为数字，特转为字符串，并加sz标识。</span>
                <span class="s1">&#39;code&#39;</span><span class="p">:</span> <span class="s1">&#39;sz&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">esg</span><span class="p">[</span><span class="s1">&#39;secCode&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span> 
                <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;secName&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">],</span>
                <span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;publishTime&#39;</span><span class="p">],</span>
                <span class="s1">&#39;size&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;attachSize&#39;</span><span class="p">],</span>
                <span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="n">esg</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]}</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="n">page</span> <span class="o">=</span> <span class="n">page</span> <span class="o">+</span> <span class="mi">1</span>
    
<span class="c1">#关闭csv</span>
<span class="n">csvf</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div><p><br><br></p>
<h2 id="三查看csv">三、查看csv</h2>
<p>使用pandas读取 <code>data/esg_links.csv</code>,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;esg_links.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/04-df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">4392
</code></pre></div><p>一共有4392条 「企业社会责任」 的报告数据。</p>
<p><br><br></p>
<h2 id="四批量下载">四、批量下载</h2>
<p>下载就简单多了， 直接使用定义好的爬虫代码。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">requests</span>

<span class="k">def</span> <span class="nf">download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    下载多媒体及文件
</span><span class="s2">    url： 多媒体文件链接（结尾有文件格式名）
</span><span class="s2">    file: 存储文件的路径（结尾有文件格式名）
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="c1">#获取到二进制数据</span>
    <span class="n">binarydata</span> <span class="o">=</span> <span class="n">resp</span><span class="o">.</span><span class="n">content</span>
    <span class="c1">#以二进制形式将数据流存入fname中</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">binarydata</span><span class="p">)</span> 
        

<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">link</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pdf_link&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">title</span><span class="p">)</span>
    <span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">link</span><span class="p">,</span> <span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/</span><span class="si">{}</span><span class="s1">.pdf&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">title</span><span class="p">))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0 山河药辅：山河药辅2021年度社会责任报告
1 新 希 望：2021年企业社会责任报告（英文版）
2 天原股份：宜宾天原集团股份有限公司社会责任报告
3 五 粮 液：2021年度社会责任报告（英文版）
4 中兵红箭：2021年度社会责任报告	
......
......
148 苏宁环球：2021年社会责任报告
149 蓝色光标：2021年度企业社会责任报告
150 开尔新材：2021年度社会责任报告
151 中顺洁柔：2021年社会责任报告
......
......

4391 闽东电力：2006年度社会责任报告
4392  阳光发展：2006年度社会责任报告书
</code></pre></div><p>采集过程中，被封锁在所难免，所以记得每次停止采集的位置，在csv中删除该位置之前的数据。然后重新运行代码即可。</p>
<h3 id="注意">注意</h3>
<p>即时解决以上问题，可能遇到奇怪的问题。比如</p>
<p><img loading="lazy" src="img/07-error.png" alt=""  />
</p>
<p>检查发现相比其他几百kb的pdf，问题文件大小只有几kb。问题可能是被网站封锁或网络不稳定导致，标记好问题pdf的链接，重新批量下载一遍。</p>
<p><img loading="lazy" src="img/06-error.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="汇总至csv">汇总至csv</h2>
<p>很多企业社会责任报告是图片合成的，所以这里的pdf体积很大。将data文件夹中的4000多个pdf汇总至esg_data.csv中，能节约出电脑内存空间，也方便后续数据分析。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pdfdocx</span> <span class="kn">import</span> <span class="n">read_pdf</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">csv</span>

<span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#新建esg_data.csv，用于存储企业社会责任报告数据</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;esg_data.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">csvf</span><span class="p">:</span>
    <span class="n">fieldnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;code&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;pdf_link&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">,</span> <span class="s1">&#39;report_content&#39;</span><span class="p">]</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">DictWriter</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">fieldnames</span><span class="o">=</span><span class="n">fieldnames</span><span class="p">)</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">writeheader</span><span class="p">()</span>

    <span class="n">files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
        <span class="n">record_of_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">file</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.pdf&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">record_of_df</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>
        <span class="n">data</span><span class="p">[</span><span class="s1">&#39;report_content&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;data/</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file</span><span class="p">))</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div><p>最后，数据从20G的data文件夹(4000多个PDF)压缩为一个170M的esg_data.csv文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">esg_reports_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;esg_data.csv&#39;</span><span class="p">)</span>
<span class="n">esg_reports_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/08-esg_reports_df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">len</span><span class="p">(</span><span class="n">esg_reports_df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">4346
</code></pre></div><p><br><br></p>
<h2 id="五相关文献">五、相关文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]解学梅, &amp; 朱琪玮. (2021). 企业绿色创新实践如何破解 “和谐共生” 难题?. 管理世界, 37(1), 128-149.
[2]谢红军 &amp; 吕雪.(2022).负责任的国际投资：ESG与中国OFDI. 经济研究(03),83-99.
[3]Schaefer, S. D., Terlutter, R., &amp; Diehl, S. (2019). Is my company really doing good? Factors influencing employees&#39; evaluation of the authenticity of their company&#39;s corporate social responsibility engagement. Journal of business research, 101, 128-143.
</code></pre></div><p><br><br></p>
<h2 id="六其他广告">六、其他(广告)</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本相似 | Lazy Prices公司年报内容变动预示重大风险</title>
      <link>https://textdata.cn/blog/2019-12-08-lazy-prices/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-12-08-lazy-prices/</guid>
      <description>一个公司报告文件会有不同部分，我们需要将不同的部分分别识别出来。这里用到正则表达式，可以进行快速的数据清洗和数据抽取。文本转为向量后就可以进行相似度计算,</description>
      <content:encoded><![CDATA[<h2 id="文献">文献</h2>
<p>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</p>
<br>
<h2 id="摘要">摘要</h2>
<p>使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对<strong>报告进行积极更改</strong>时，这种行为<strong>蕴含着</strong>公司未来运营的<strong>重要信号</strong>。</p>
<p><strong>财务报告的语言和结构的变化也对公司的未来收益产生重大影响</strong>：做空&quot;变化&quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。</p>
<p>研究发现，对10-K的变化可以预测未来的收益、获利能力、未来的新闻公告，甚至未来的公司破产。同时，不做任何变化的公司将获得显著的异常收益。与资产价格典型的反应不足研究不同，我们发现没有任何与这些变化相关的公告效应–仅在后来通过新闻，事件或收益披露信息时才产生回报–暗示投资者并未注意到整个公众领域的这些变化。</p>
<br>
<h2 id="研究背景">研究背景</h2>
<p>之前的研究认为，尽管投资者一次对包含重大变化的财务报表的发布作出了迅时反应，但随着时间的流逝，这种公告作用是会减弱的(Brown and Tucker, 2011 and Feldman et al., 2010)。这表示10-K报告会随着时间推移，信息价值大打折扣。尽管我们复现了这个事实，即与常规文件的变更没有重大的公告效应，但我们认为，前人的研究忽略了更重要部分(如MD&amp;A)对对资产价格的影响。</p>
<p>确切的说，<strong>并不是报告的披露效应的信息价值变低了，而是投资者越来越难以发现报告中微妙的信息变化， 比如因为报告变得越来越冗杂。投资者只有看到某些新闻后，才会逐渐意识到之前公司报告内容变化的的真正价值</strong>。</p>
<p>例如Baxter公司</p>
<ul>
<li>纽约时报在 <strong>2010年4月23日</strong> 发了一条FDA将有对输液泵(infusion pumps)更严格对审批管理规定的新闻，<strong>新闻中提到了Baxter公司</strong>。新闻公布当天，<strong>Baxter股价大跌</strong>。</li>
<li><strong>10天</strong>后的（2010年5月4日），Baxter宣布<strong>召回问题的输液泵产品</strong>，股价当天再次大跌。</li>
</ul>
<p><img loading="lazy" src="img/lazy-prices-1.png" alt=""  />
</p>
<p>两次负面新闻导致Baxter股价大跌超过20%，最有意思的是Baxter公司一个多月前（<strong>2010年2月23日</strong>）10-k报告中 <strong>提到</strong> 了与这两条新闻类似的 <strong>线索</strong>。</p>
<p><img loading="lazy" src="img/clues_from_report.png" alt=""  />
</p>
<p>截图中写着 <strong>Baxter的产品COLLEGUE未来可能面脸额外的处罚，而且相关销售面临着FDA、OIG、DOI和FTC越来越严格的审批，面临的执法强度也越来越大</strong>。</p>
<p>因纽约时报发布的消息，股价大跌。但是大跌之前Baxter的10-k报告中似乎提示未来公司可能面临的风险，但是投资者怎么没有注意到这个重要线索呢？</p>
<br>
<h2 id="数据获取与分析方法">数据获取与分析方法</h2>
<p>这篇文章用到了很多 文本数据挖掘 方法，如</p>
<ul>
<li>数据采集(报告下载和信息监测)</li>
<li>正则表达式（数据分割与抽取）</li>
<li>文本相似度(计算报告变化程度)</li>
</ul>
<p>我大致说下这几部分技术在这篇论文中的应用。</p>
<h3 id="1-数据采集">1. 数据采集</h3>
<p>这篇论文研究者认为，只有投资者意识到本期报告和上一期报告做对比，才能发现报告变化，进而对股价有影响。所以当有新公告公布后，投资者是否下载本期报告的同时顺带着下载上一期报告，下载量又是多少。</p>
<p>下载量可以从Freedom of Information Act下载，</p>
<p><img loading="lazy" src="img/download_data.png" alt=""  />
</p>
<p>可以拿到的信息包括:</p>
<ul>
<li>报告文件</li>
<li>报告下载时间</li>
<li>报告下载的IP地址(可以通过这个ip来当作投资者的id)</li>
</ul>
<br>
<h3 id="2-正则表达式">2. 正则表达式</h3>
<p>一个公司报告文件会有不同部分，我们需要将不同的部分分别识别出来。这里用到正则表达式，可以进行快速的数据清洗和数据抽取。</p>
<p><img loading="lazy" src="img/regular_expression.png" alt=""  />
</p>
<br>
<h3 id="3-文本相似度">3. 文本相似度</h3>
<p>文本转为向量后就可以进行相似度计算,</p>
<p><img loading="lazy" src="img/similar-1.png" alt=""  />

<img loading="lazy" src="img/similar-2.png" alt=""  />

<img loading="lazy" src="img/similar-3.png" alt=""  />
</p>
<p>这里使用我开发的cntext包，可以实现cosine和jaccard相似度的计算。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">A</span> <span class="o">=</span> <span class="s1">&#39;We expect demand to increase.&#39;</span>
<span class="n">B</span> <span class="o">=</span> <span class="s1">&#39;We expect worldwide demand to increase.&#39;</span>
<span class="n">C</span> <span class="o">=</span> <span class="s1">&#39;We expect weakness in sales&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.40
0.83
</code></pre></div><p>如果对Baxter公司多个年度对报告进行相似度计算，绘制成图就会发现2010年与前后变化很大。相似度越低，说明公司报告前后变化很大，应该引起投资者注意，如果能注意到就会避免纽约时报导致到股价暴跌。如下图</p>
<p><img loading="lazy" src="img/lazy-prices-2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="案例实现">案例实现</h2>
<p>由于没有完全一样的数据，这里使用政府工作报告数据类比，使用cosine相似度画出趋势线条。</p>
<p>使用相似性识别变化的时间点</p>
<h3 id="准备数据">准备数据</h3>
<p>政府工作报告 <a href="http://www.gov.cn/guowuyuan/zfgzbg.htm">http://www.gov.cn/guowuyuan/zfgzbg.htm</a></p>
<p>prc_reports.xlsx 链接:https://pan.baidu.com/s/1sVU3mkEcP7Z3_hbG5AVNUA 密码:zjrq</p>
<p>将下载好后的 prc_reports.xlsx 文件放置于 .ipynb文件 所在的文件夹内。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;prc_reports.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<h3 id="计算相似度">计算相似度</h3>
<p>运行时间大概30s， 运算结果是列表数据 cosines</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">cosines</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">#row  Series</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">text1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;report&#39;</span><span class="p">]</span>
    <span class="n">text2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;report2&#39;</span><span class="p">]</span>
    <span class="n">simi</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">)</span>
    <span class="n">cosines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">simi</span><span class="p">)</span>
    
<span class="n">cosines</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[0.44&#39;, &#39;0.39&#39;, &#39;0.35&#39;, ... &#39;0.62&#39;, &#39;0.61&#39;, &#39;0.60&#39;]
</code></pre></div><h3 id="绘制柱状图">绘制柱状图</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">Bar</span>
<span class="kn">from</span> <span class="nn">pyecharts</span> <span class="kn">import</span> <span class="n">options</span> <span class="k">as</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>

<span class="n">bar</span> <span class="o">=</span> <span class="n">Bar</span><span class="p">()</span>

<span class="n">bar</span><span class="o">.</span><span class="n">add_xaxis</span><span class="p">(</span><span class="n">xaxis_data</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">])</span>
<span class="n">bar</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s2">&#34;相似度&#34;</span><span class="p">,</span> 
               <span class="n">cosines</span><span class="p">,</span> 
               <span class="n">label_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">LabelOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="n">bar</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span><span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;政府工作报告相似度可视化&#34;</span><span class="p">))</span>
<span class="n">bar</span><span class="o">.</span><span class="n">load_javascript</span><span class="p">()</span>

<span class="n">bar</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;政府工作报告相似度可视化1.html&#39;</span><span class="p">)</span>
<span class="n">bar</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/vis_res.png" alt=""  />
</p>
<h3 id="解读">解读</h3>
<p>从图中可以看到除1959年异常外，其他方面能挖掘出很多信息。从相似度整体趋势，</p>
<p>1959-1992 第一阶段，
1992-至今 第二阶段</p>
<p>1992年附近，第一次确立社会主义市场经济制度。之后的岁月里一直围绕着经济建设高速发展。</p>
<p>同时也可以看出在第一阶段前期相似度异常的低，可以理解为新中国初建，百废待兴，对于建设者而言，组着和管理这个国家的政府也在学习如何建设新中国。而90年代后，相似度越来越高，体现了政府越来越熟悉如何治理国家，如何搞经济建设。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
