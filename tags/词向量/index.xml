<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>词向量 on 大邓和他的PYTHON</title>
    <link>/tags/%E8%AF%8D%E5%90%91%E9%87%8F/</link>
    <description>Recent content in 词向量 on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sun, 16 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>数据集 | 多语言对齐词向量预训练模型</title>
      <link>https://hidadeng.github.io/blog/2022-10-16-aligned-word-vectors/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-16-aligned-word-vectors/</guid>
      <description>借助该预训练模型，应该能做可做跨文化对比分析</description>
      <content:encoded><![CDATA[<h2 id="介绍">介绍</h2>
<p>Facebook研究者使用 fastText 算法，对维基百科(44种语言)语料数据进行了训练，最终生成了 44 种语言的对齐词向量。</p>
<br>
<h2 id="用途">用途</h2>
<p>wiki数据集有个优点，即由于众人分享、翻译，将不同语言的百科词条进行了翻译整理。所以facebook使用wiki训练对齐词向量有助于提升翻译准确性。与此同时，因为翻译者处于不同的语言和文化背景下，词条及词条内容必然蕴含着语言所特有的文化信息线索，有可能有助于我们挖掘跨语言的文化差异。例如中文词条<code>护士</code>和 英文词条<code>nurse</code> ，可以借助对齐词向量，比较护士这个群体在性别、种族等语义上的差异。</p>
<p>之前分享过的内容</p>
<ul>
<li><a href="https://hidadeng.github.io/blog/embeddingsandattitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
<li><a href="https://hidadeng.github.io/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展&mdash;&mdash;基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://hidadeng.github.io/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
<li><a href="https://hidadeng.github.io/blog/literatureembeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
</ul>
<p>不过fastText算法认为词语有不同的大小划分层次，从大到小分别是词语、词缀、字符等，使用 Joulin 等人 (2018) 中描述的 RCSLS 方法进行比对。</p>
<table>
<thead>
<tr>
<th><strong>Code</strong></th>
<th><strong>en-es</strong></th>
<th><strong>es-en</strong></th>
<th><strong>en-fr</strong></th>
<th><strong>fr-en</strong></th>
<th><strong>en-de</strong></th>
<th><strong>de-en</strong></th>
<th><strong>en-ru</strong></th>
<th><strong>ru-en</strong></th>
<th><strong>en-zh</strong></th>
<th><strong>zh-en</strong></th>
<th><strong>avg</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Joulin et al. [<a href="https://arxiv.org/abs/1804.07745">1</a>]</td>
<td>84.1</td>
<td>86.3</td>
<td>83.3</td>
<td>84.1</td>
<td><strong>79.1</strong></td>
<td>76.3</td>
<td><strong>57.9</strong></td>
<td><strong>67.2</strong></td>
<td>45.9</td>
<td>46.4</td>
<td>71.1</td>
</tr>
<tr>
<td>This implementation (10 epochs)</td>
<td>84.2</td>
<td><strong>86.6</strong></td>
<td><strong>83.9</strong></td>
<td>84.7</td>
<td>78.3</td>
<td>76.6</td>
<td>57.6</td>
<td>66.7</td>
<td><strong>47.6</strong></td>
<td><strong>47.4</strong></td>
<td>71.4</td>
</tr>
<tr>
<td>This implementation (unsup. model selection)</td>
<td><strong>84.3</strong></td>
<td><strong>86.6</strong></td>
<td><strong>83.9</strong></td>
<td><strong>85.0</strong></td>
<td>78.7</td>
<td><strong>76.7</strong></td>
<td>57.6</td>
<td>67.1</td>
<td><strong>47.6</strong></td>
<td><strong>47.4</strong></td>
<td><strong>71.5</strong></td>
</tr>
</tbody>
</table>
<p>算法得出的词向量在西方，尤其是西欧语言之间进行语义对齐，效果可能更好。而中文、日语等汉字语言，是由偏旁部首组成，与西方字母语言还是存在一定差异。上表也可以看出中英语义对齐准确率47%， 而其他语言之间对齐准确率平均为71%。</p>
<br>
<h2 id="模型资源">模型资源</h2>
<p><a href="https://fasttext.cc/docs/en/aligned-vectors.html">https://fasttext.cc/docs/en/aligned-vectors.html</a></p>
<p>对齐预训练向量模型下载链接</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Afrikaans: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.af.align.vec"><em>text</em></a></td>
<td>Arabic: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ar.align.vec"><em>text</em></a></td>
<td>Bulgarian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bg.align.vec"><em>text</em></a></td>
<td>Bengali: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bn.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Bosnian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bs.align.vec"><em>text</em></a></td>
<td>Catalan: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ca.align.vec"><em>text</em></a></td>
<td>Czech: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.cs.align.vec"><em>text</em></a></td>
<td>Danish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.da.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>German: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec"><em>text</em></a></td>
<td>Greek: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.el.align.vec"><em>text</em></a></td>
<td>English: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.en.align.vec"><em>text</em></a></td>
<td>Spanish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.es.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Estonian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.et.align.vec"><em>text</em></a></td>
<td>Persian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fa.align.vec"><em>text</em></a></td>
<td>Finnish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fi.align.vec"><em>text</em></a></td>
<td>French: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fr.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Hebrew: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.he.align.vec"><em>text</em></a></td>
<td>Hindi: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hi.align.vec"><em>text</em></a></td>
<td>Croatian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hr.align.vec"><em>text</em></a></td>
<td>Hungarian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hu.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Indonesian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.id.align.vec"><em>text</em></a></td>
<td>Italian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.it.align.vec"><em>text</em></a></td>
<td>Korean: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ko.align.vec"><em>text</em></a></td>
<td>Lithuanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.lt.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Latvian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.lv.align.vec"><em>text</em></a></td>
<td>Macedonian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.mk.align.vec"><em>text</em></a></td>
<td>Malay: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ms.align.vec"><em>text</em></a></td>
<td>Dutch: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.nl.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Norwegian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.no.align.vec"><em>text</em></a></td>
<td>Polish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.pl.align.vec"><em>text</em></a></td>
<td>Portuguese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.pt.align.vec"><em>text</em></a></td>
<td>Romanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ro.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Russian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ru.align.vec"><em>text</em></a></td>
<td>Slovak: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sk.align.vec"><em>text</em></a></td>
<td>Slovenian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sl.align.vec"><em>text</em></a></td>
<td>Albanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sq.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Swedish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sv.align.vec"><em>text</em></a></td>
<td>Tamil: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ta.align.vec"><em>text</em></a></td>
<td>Thai: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.th.align.vec"><em>text</em></a></td>
<td>Tagalog: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.tl.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Turkish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.tr.align.vec"><em>text</em></a></td>
<td>Ukrainian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.uk.align.vec"><em>text</em></a></td>
<td>Vietnamese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.vi.align.vec"><em>text</em></a></td>
<td>Chinese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.zh.align.vec"><em>text</em></a></td>
</tr>
</tbody>
</table>
<br>
<h2 id="格式">格式</h2>
<p>词向量默认使用的fastText格式</p>
<ul>
<li>第一行给了词向量的维数</li>
<li>从第二行开始，每一行由词语及对应的词向量组成。</li>
<li>数值之间使用空格间隔</li>
</ul>
<br>
<h2 id="代码">代码</h2>
<h3 id="导入模型">导入模型</h3>
<p>使用gensim导入fastText方法训练出的 预训练语言模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1">#导入刚刚下载的预训练模型</span>
<span class="c1">#该词向量模型300维</span>
<span class="n">zh_w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;wiki.zh.align.vec&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1">#英文词向量模型5G，太大了。如果内存小于16G不要使用下面命令</span>
<span class="c1">#en_w2v_model = KeyedVectors.load_word2vec_format(&#39;wiki.en.align.vec&#39;, binary=False)</span>
</code></pre></div><p>一旦导入成功，就可以进行向量计算。这里仅进行简单演示</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取某词的词向量</span>
<span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>array([ 0.0733,  0.0782,  0.0188, -0.0027, -0.0052,...,  0.0586,  0.0166,
       -0.1401, -0.0545, -0.0125,  0.0373, -0.0681,  0.063 ],
      dtype=float32)
</code></pre>
<br>
<p>在中文中， 护士职业的主要从业者为女性，反应在词向量相似度上，如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">,</span> <span class="s1">&#39;女性&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">,</span> <span class="s1">&#39;男性&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<pre><code>0.4417011
0.378651
</code></pre>
<br>
<p>更多w2v_model用法可参考 <a href="https://hidadeng.github.io/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="文献">文献</h2>
<p>如果使用了facebook的预训练词向量，请引用以下两篇文献。</p>
<ul>
<li>Joulin, Armand, Piotr Bojanowski, Tomas Mikolov, Hervé Jégou, and Edouard Grave. &ldquo;Loss in translation: Learning bilingual word mapping with a retrieval criterion.&rdquo; arXiv preprint arXiv:1804.07745 (2018).</li>
<li>Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. &ldquo;Enriching word vectors with subword information.&rdquo; Transactions of the association for computational linguistics 5 (2017): 135-146.</li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>R语言 | 使用word2vec词向量模型</title>
      <link>https://hidadeng.github.io/blog/2022-10-12-r-word2vec/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-12-r-word2vec/</guid>
      <description>R语言训练和使用词向量word2vec模型</description>
      <content:encoded><![CDATA[


<p>Python的gensim库可以训练和使用word2vec模型，R语言中也有与之对应的<code>word2vec包</code>。word2vec是词嵌入技术中最常用的一种技术，如果对词嵌入不太了解，可以阅读前文</p>
<ul>
<li><a href="https://hidadeng.github.io/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://hidadeng.github.io/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
</ul>
<p>本文需要的R包</p>
<pre><code>install.packages(c(&quot;word2vec&quot;, &quot;jiebaR&quot;, &quot;tidyverse&quot;, &quot;readtext&quot;))</code></pre>
<p><br></p>
<div id="word2vec包常用函数" class="section level2">
<h2>word2vec包常用函数</h2>
<ul>
<li>word2vec 使用文本数据训练word2vec模型</li>
<li>as.matrix 获取词向量</li>
<li>doc2vec 获取文档向量</li>
<li>predict 获取</li>
<li>write.word2vec 保存word2vec模型至文件</li>
<li>read.word2vec 读取word2vec模型文件</li>
</ul>
<p><br></p>
</div>
<div id="准备数据" class="section level2">
<h2>准备数据</h2>
<p>原始数据是从网站下载的 <code>三体.txt</code>, 未分词处理，现在需要</p>
<ol style="list-style-type: decimal">
<li>读中文取txt数据</li>
<li>保留标点符号，进行分词处理</li>
<li>分词结果重新整理为类似英文(空格间隔词语的形式)字符串</li>
<li>结果存入新的txt</li>
</ol>
<pre class="r"><code>library(jiebaR)
library(tidyverse)
library(word2vec)


#导入数据
tri_body &lt;- readtext::readtext(&#39;data/三体.txt&#39;)$text 

#分词（保留标点符号）
tokenizer &lt;- worker(symbol=T)
tri_words &lt;- segment(tri_body, tokenizer)

# 整理为英文格式（词语之间加空格）
segmented_text &lt;- stringr::str_c(tri_words, collapse = &quot; &quot;) %&gt;% c()

#写入txt
readr::write_file(segmented_text, file=&#39;data/santi.txt&#39;)</code></pre>
<p><br></p>
</div>
<div id="训练word2vec模型" class="section level2">
<h2>训练word2vec模型</h2>
<pre><code>word2vec(
  x,
  type = c(&quot;cbow&quot;, &quot;skip-gram&quot;),
  dim = 50,
  window = ifelse(type == &quot;cbow&quot;, 5L, 10L),
  iter = 5L,
  lr = 0.05,
  min_count = 5L,
  split = c(&quot; \n,.-!?:;/\&quot;#$%&amp;&#39;()*+&lt;=&gt;@[]\\^_`{|}~\t\v\f\r&quot;, &quot;.\n?!&quot;),
  stopwords = character(),
  threads = 1L,
  ...
)</code></pre>
<ul>
<li>x 英文文本数据txt文件(中文数据txt文件是分词后的txt文件，空格间隔词语)</li>
<li>type 训练方式，默认CBOW</li>
<li>dim 词向量维度，默认50维</li>
<li>window 词向量窗口，默认5</li>
<li>iter 训练迭代次数，默认5</li>
<li>split 分词、分句对应的分隔符。</li>
<li>lr 学习率，默认0.05</li>
<li>min_count 词语在语料中至少要出现5次(低于5次的词语，训练好的结果中没有该词语）</li>
<li>stopwords 停用词表，默认空字符集</li>
<li>threads 并行加速，cpu核数，默认1。为了加速训练过程，可以使用 <code>parallel::detectCores()</code> 获得本电脑的核数</li>
</ul>
<pre class="r"><code>#训练10维的词向量模型
model &lt;- word2vec(x = &#39;data/santi.txt&#39;, 
                  dim = 10,  
                  iter = 20, 
                  split = c(&quot; &quot;,  &quot;。？！；&quot;),
                  threads = parallel::detectCores()) #并行，使用cpu多核加速

emb &lt;- as.matrix(model)

#显示6个词
head(emb)</code></pre>
<pre><code>##             [,1]       [,2]        [,3]        [,4]      [,5]        [,6]
## 煮   -1.02566934 -0.9271542 -0.42417252 -0.54280633 1.8847700  0.41640753
## 报   -0.83992052  1.9440031  0.09093992  0.83522910 1.7909089  0.72149992
## 悬空 -0.06369513 -1.3519955 -2.13137460 -0.06198586 0.6096401  1.32933748
## 略    1.74687469 -0.4278547 -0.33822438  1.08505321 2.0168977 -0.07693915
## 伏   -0.68947995 -1.4147453 -1.95522511 -0.39963767 0.5269030  0.30352208
## 石柱 -0.40561640 -1.3643234  0.30329546 -0.94012892 2.1579018  0.79654717
##            [,7]       [,8]       [,9]      [,10]
## 煮   -1.1708908 -0.7624418 -0.6275516  1.2417521
## 报    0.5235919  0.8448864 -0.2960095 -0.0773837
## 悬空  0.1527163 -0.1337370 -0.1646384  1.1892601
## 略   -0.3246748 -0.9813624  0.5045205  0.2771466
## 伏    0.3166684 -1.4238008 -1.0167172 -0.0976937
## 石柱  0.2237919  0.6933151  0.7412233 -0.7918702</code></pre>
<p><br></p>
</div>
<div id="查看某词的vector" class="section level2">
<h2>查看某词的vector</h2>
<p>查看词语 <code>汪淼</code> 的vector</p>
<pre class="r"><code>emb[&quot;汪淼&quot;,]</code></pre>
<pre><code>##  [1] -0.77559733 -0.90021265  0.66555792 -0.10277803  1.89924443 -0.88817298
##  [7] -1.32665634 -0.75938725 -0.09628224  1.18008399</code></pre>
<p>查看词语 <code>地球</code> 的vector</p>
<pre class="r"><code>emb[&quot;地球&quot;,]</code></pre>
<pre><code>##  [1]  0.29645494 -0.61688840  0.91209215 -0.64530188  0.62816381 -0.72807491
##  [7]  0.50655973  2.38137436  1.19238114 -0.09610342</code></pre>
<p><br></p>
</div>
<div id="predict" class="section level2">
<h2>predict()</h2>
<p>找到语料中，词语 <code>罗辑</code> 最相似的 20个词</p>
<pre class="r"><code>predict(model, &#39;罗辑&#39;, type=&#39;nearest&#39;, top_n = 20)</code></pre>
<pre><code>## $罗辑
##    term1    term2 similarity rank
## 1   罗辑     胡文  0.9744400    1
## 2   罗辑   申玉菲  0.9678891    2
## 3   罗辑   瓦季姆  0.9550550    3
## 4   罗辑 狄奥伦娜  0.9518393    4
## 5   罗辑     蓝西  0.9472395    5
## 6   罗辑     护士  0.9471439    6
## 7   罗辑   法扎兰  0.9458703    7
## 8   罗辑   白艾思  0.9451101    8
## 9   罗辑     坎特  0.9396626    9
## 10  罗辑     白蓉  0.9387447   10
## 11  罗辑   参谋长  0.9377206   11
## 12  罗辑   弗雷斯  0.9369408   12
## 13  罗辑   第一眼  0.9357565   13
## 14  罗辑     父亲  0.9350463   14
## 15  罗辑   多少次  0.9314436   15
## 16  罗辑     门去  0.9291503   16
## 17  罗辑     维德  0.9267251   17
## 18  罗辑     褐蚁  0.9203902   18
## 19  罗辑       刚  0.9200501   19
## 20  罗辑     吴岳  0.9191605   20</code></pre>
<p>查看均值向量（多个词向量中心的）的10个近义词</p>
<pre class="r"><code>vectors &lt;- emb[c(&quot;汪淼&quot;, &quot;罗辑&quot;, &quot;叶文洁&quot;), ]
centroid_vector &lt;- colMeans(vectors)

predict(model, centroid_vector, type = &quot;nearest&quot;, top_n = 10)</code></pre>
<pre><code>##        term similarity rank
## 1      罗辑  0.9185568    1
## 2  狄奥伦娜  0.9104245    2
## 3      文洁  0.9088279    3
## 4      汪淼  0.9054156    4
## 5    白艾思  0.9046930    5
## 6      张翔  0.9026827    6
## 7      尴尬  0.8952187    7
## 8      庄颜  0.8952166    8
## 9      皇帝  0.8949283    9
## 10     父亲  0.8915347   10</code></pre>
<p><br></p>
</div>
<div id="doc2vec" class="section level2">
<h2>doc2vec()</h2>
<ul>
<li>doc2vec(object, newdata, split = ” “)
<ul>
<li>object word2vec模型对象</li>
<li>newdata 文档列表(用空格间隔的字符串列表)</li>
<li>split 默认分隔符是空格</li>
</ul></li>
</ul>
<p>将文档转为向量</p>
<pre class="r"><code>docs &lt;- c(&quot;哦 ， 对不起 ， 汪 教授 。 这是 我们 史强 队长 。&quot;, 
          &quot; 丁仪 博士 ， 您 能否 把 杨冬 的 遗书 给 汪 教授 看 一下 ？ &quot;)

doc2vec(object=model, newdata = docs, split=&#39; &#39;)</code></pre>
<pre><code>##            [,1]       [,2]       [,3]     [,4]      [,5]       [,6]       [,7]
## [1,] -1.1769752 -0.1065619  0.1983950 1.734068 0.5478012 -0.8320528 -0.2387014
## [2,] -0.4827189  0.0664595 -0.2119484 1.895074 0.6729840 -0.3008853 -0.6857539
##            [,8]      [,9]      [,10]
## [1,] -0.5519856 -2.007002  0.4182127
## [2,] -0.5976922 -2.130454 -0.4653725</code></pre>
<p><br></p>
</div>
<div id="保存word2vec模型" class="section level2">
<h2>保存word2vec模型</h2>
<p>保存模型，一般有两个目的</p>
<ul>
<li>为了分享word2vec模型</li>
<li>避免反复训练模型，节约数据分析时间</li>
</ul>
<pre class="r"><code>word2vec::write.word2vec(x = model, 
                         #新建output文件夹，将模型存入output文件夹内
                         file = &quot;output/santi_word2vec.bin&quot;)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p><br></p>
</div>
<div id="导入预训练模型" class="section level2">
<h2>导入预训练模型</h2>
<p>导入 <code>output/santi_word2vec.bin</code> 的预训练word2vec模型</p>
<pre class="r"><code>pre_trained_model &lt;- word2vec::read.word2vec(file = &quot;output/santi_word2vec.bin&quot;)
pre_trained_emb &lt;- as.matrix(pre_trained_model)
head(pre_trained_emb)</code></pre>
<pre><code>##              [,1]       [,2]       [,3]       [,4]       [,5]        [,6]
## 回荡   -1.9563367 -0.3099073 -1.2969902 -0.5719763  1.1507142 -0.05515177
## 听证会  0.2756990  1.3702289 -1.3303705 -0.1827691  0.6622804 -1.92008448
## 纲领    0.4495552  1.9311246 -0.5812275 -0.1470096 -0.2678985 -0.01694358
## 很亮    0.3621844 -1.0048453  0.7036168 -2.0917876  0.6459805  1.18436253
## 秒      1.9033701  1.6510324 -0.2616904  0.3671210  1.0618066  0.06588747
## 杰森   -1.2904713 -1.2501229  0.3380587  0.8590797  1.6798494 -0.58775252
##              [,7]       [,8]       [,9]      [,10]
## 回荡    1.1082711 -0.2064489 -0.9264346 -0.7816723
## 听证会 -1.0952694  0.6120903 -0.1326561  0.7252344
## 纲领   -0.6097277  2.1051276 -0.2405726 -0.8808851
## 很亮    0.1964065 -1.3926132 -0.4042619 -0.1645472
## 秒     -0.8347995  0.2591044  0.3594093  1.1929117
## 杰森    0.4941484 -1.1393189 -0.4687541  0.9951217</code></pre>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>NLP资源 | 汽车、金融等9大领域预训练词向量模型下载资源</title>
      <link>https://hidadeng.github.io/blog/pretained_nlp_models/</link>
      <pubDate>Wed, 25 May 2022 10:43:10 +0600</pubDate>
      
      <guid>/blog/pretained_nlp_models/</guid>
      <description>本文主要开放汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等9大领域预训练词向量，以及字符、依存、拼音与词性4类预训练向量地址，供大家一起使用</description>
      <content:encoded><![CDATA[<p>在前面的文章中，我们介绍了关于词向量的一些基础理论和训练方法，<strong>本文主要开放汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等9大领域预训练词向量，以及字符、依存、拼音与词性4类预训练向量地址，供大家一起使用</strong>。</p>
<h2 id="一汽车房产等9大领域预训练词向量">一、汽车、房产等9大领域预训练词向量</h2>
<p>通过收集多文本分类语料库，对汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等多个领域文本进行词向量训练，得到了如下预训练词向量的结果：</p>
<table>
<thead>
<tr>
<th>领域类型</th>
<th>模型类型</th>
<th>关键词集合</th>
<th>词的规模</th>
</tr>
</thead>
<tbody>
<tr>
<td>汽车</td>
<td>word_vector_auto.model.bin</td>
<td>117,510</td>
<td>200</td>
</tr>
<tr>
<td>房产</td>
<td>word_vector_house.model.bin</td>
<td>145,287</td>
<td>200</td>
</tr>
<tr>
<td>教育</td>
<td>word_vector_edu.model.bin</td>
<td>242,874</td>
<td>200</td>
</tr>
<tr>
<td>社会</td>
<td>word_vector_society.model.bin</td>
<td>221,395</td>
<td>200</td>
</tr>
<tr>
<td>娱乐</td>
<td>word_vector_ent.model.bin</td>
<td>230,665</td>
<td>200</td>
</tr>
<tr>
<td>体育</td>
<td>word_vector_sports.model.bin</td>
<td>95724</td>
<td>200</td>
</tr>
<tr>
<td>金融</td>
<td>word_vector_finance.model.bin</td>
<td>284035</td>
<td>200</td>
</tr>
<tr>
<td>科技</td>
<td>word_vector_tech.model.bin</td>
<td>108188</td>
<td>200</td>
</tr>
<tr>
<td>游戏</td>
<td>word_vector_games.model.bin</td>
<td>100821</td>
<td>200</td>
</tr>
</tbody>
</table>
<p><strong>开放地址：</strong></p>
<p><a href="https://pan.baidu.com/s/1jEHFoAmVXlB67Q28-CeTvw">https://pan.baidu.com/s/1jEHFoAmVXlB67Q28-CeTvw</a> 密码: 1pa6</p>
<h2 id="二预训练字符依存拼音与词性向量">二、预训练字符、依存、拼音与词性向量</h2>
<p>通过对字符、依存、拼音与词性进行切分，使用同样的方式，可以得到相应的预训练词向量。</p>
<table>
<thead>
<tr>
<th>向量名称</th>
<th style="text-align:center">向量含义</th>
<th style="text-align:center">词数</th>
<th style="text-align:center">维度</th>
<th style="text-align:center">例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>de_vec_10</td>
<td style="text-align:center">依存关系向量</td>
<td style="text-align:center">13</td>
<td style="text-align:center">10</td>
<td style="text-align:center">SBV, ATT</td>
</tr>
<tr>
<td>pinyin_vec_300</td>
<td style="text-align:center">汉语拼音向量</td>
<td style="text-align:center">146242</td>
<td style="text-align:center">300</td>
<td style="text-align:center">ni, hao</td>
</tr>
<tr>
<td>postag_vec_30</td>
<td style="text-align:center">汉语词性向量</td>
<td style="text-align:center">59</td>
<td style="text-align:center">300</td>
<td style="text-align:center">n,v,a,d</td>
</tr>
<tr>
<td>token_vec_300</td>
<td style="text-align:center">汉语字向量</td>
<td style="text-align:center">20029</td>
<td style="text-align:center">300</td>
<td style="text-align:center">刘,焕,勇</td>
</tr>
<tr>
<td>word_vec_300</td>
<td style="text-align:center">汉语词向量</td>
<td style="text-align:center">673266</td>
<td style="text-align:center">300</td>
<td style="text-align:center">刘焕勇</td>
</tr>
</tbody>
</table>
<p><strong>开放地址：</strong></p>
<p><a href="https://github.com/liuhuanyong/ChineseEmbedding">https://github.com/liuhuanyong/ChineseEmbedding</a></p>
<p><strong>向量效果：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> 
 ***********************字符向量************************
    token:刘
    (&#39;李&#39;, 0.7306396961212158),(&#39;陈&#39;, 0.7201231122016907)
    (&#39;赵&#39;, 0.6974461674690247),(&#39;杨&#39;, 0.6972213983535767)
    (&#39;吴&#39;, 0.6851627230644226),(&#39;徐&#39;, 0.6516467332839966)
    (&#39;郭&#39;, 0.6499480605125427),(&#39;蔡&#39;, 0.6175302267074585)
    (&#39;郑&#39;, 0.6092196106910706),(&#39;孙&#39;, 0.5950524210929871)
    token:丑
    (&#39;卯&#39;, 0.6074919700622559),(&#39;酉&#39;, 0.5910211801528931)
    (&#39;巳&#39;, 0.5581363439559937),(&#39;戌&#39;, 0.43932047486305237)
    (&#39;戊&#39;, 0.41449615359306335),(&#39;壬&#39;, 0.40456631779670715)
    (&#39;謤&#39;, 0.367109090089798),(&#39;绯&#39;, 0.3643313944339752),
    (&#39;寅&#39;, 0.36351141333580017),(&#39;旽&#39;, 0.3549465537071228)


***********************依存向量************************
    dependency rel:ATT
    (&#39;COO&#39;, 0.14239487051963806),(&#39;ADV&#39;, -0.16987691819667816)
    (&#39;RAD&#39;, -0.2357601821422577),(&#39;HED&#39;, -0.2401314228773117)
    (&#39;SBV&#39;, -0.25625932216644287),(&#39;WP&#39;, -0.27165737748146057)
    (&#39;LAD&#39;, -0.2902592420578003),(&#39;POB&#39;, -0.2990782558917999)
    (&#39;VOB&#39;, -0.37553706765174866),(&#39;IOB&#39;, -0.6669262647628784)
    dependency rel:POB
    (&#39;IOB&#39;, 0.16698899865150452),(&#39;DBL&#39;, 0.16678886115550995)
    (&#39;FOB&#39;, 0.1657436639070511),(&#39;CMP&#39;, 0.14784857630729675)
    (&#39;VOB&#39;, 0.1461176574230194),(&#39;SBV&#39;, 0.08011472970247269)
    (&#39;LAD&#39;, -0.022307466715574265),(&#39;WP&#39;, -0.022942926734685898)
    (&#39;HED&#39;, -0.037264980375766754),(&#39;RAD&#39;, -0.042251598089933395)

  
  ***********************拼音向量************************
    pinyin:wo
    (&#39;shei&#39;, 0.6129732131958008)(&#39;ta&#39;, 0.6081706285476685)
    (&#39;nin&#39;, 0.5819231867790222),(&#39;！&#39;, 0.5435523986816406)
    (&#39;……&#39;, 0.48428624868392944),(&#39;ai&#39;, 0.47832390666007996)
    (&#39;o&#39;, 0.4761071801185608),(&#39;。』&#39;, 0.4598163366317749)
    (&#39;...&#39;, 0.45207729935646057),(&#39;ni&#39;, 0.44975683093070984)
    pinyin:guo
    (&#39;dang&#39;, 0.3908974528312683),(&#39;yuan&#39;, 0.378823846578598)
    (&#39;zu&#39;, 0.35387369990348816),(&#39;hua&#39;, 0.3405681848526001)
    (&#39;zheng&#39;, 0.3355437219142914),(&#39;yi&#39;, 0.3333034813404083)
    (&#39;ren&#39;, 0.3194104731082916),(&#39;jun&#39;, 0.3187354505062103)
    (&#39;hui&#39;, 0.31342023611068726),(&#39;xin&#39;, 0.3096797466278076)

   
   ***********************词性向量************************
    word postag:a
    (&#39;d&#39;, 0.7203904986381531),(&#39;c&#39;, 0.6124969720840454)
    (&#39;v&#39;, 0.4963228106498718),(&#39;an&#39;, 0.4531499147415161)
    (&#39;uz&#39;, 0.4459834396839142),(&#39;ud&#39;, 0.42059916257858276)
    (&#39;r&#39;, 0.4090540111064911),(&#39;uj&#39;, 0.4061364233493805)
    (&#39;i&#39;, 0.38707998394966125),(&#39;l&#39;, 0.3551557660102844)
    word postag:n
    (&#39;b&#39;, 0.7030695676803589),(&#39;vn&#39;, 0.490166038274765)
    (&#39;p&#39;, 0.4858315885066986),(&#39;v&#39;, 0.4499088227748871)
    (&#39;nt&#39;, 0.44155171513557434),(&#39;f&#39;, 0.26609259843826294)
    (&#39;s&#39;, 0.2639649212360382),(&#39;l&#39;, 0.24365971982479095)
    (&#39;ns&#39;, 0.2278469204902649),(&#39;m&#39;, 0.202927365899086)
    
    ***********************词向量************************
    word:爱情
    (&#39;爱恋&#39;, 0.6931096315383911),(&#39;真爱&#39;, 0.6897798776626587)
    (&#39;婚姻&#39;, 0.6540514826774597),(&#39;浪漫爱情&#39;, 0.6535360813140869)
    (&#39;情感&#39;, 0.6501022577285767),(&#39;感情&#39;, 0.6403399705886841)
    (&#39;纯爱&#39;, 0.6394841074943542),(&#39;爱情故事&#39;, 0.6282097101211548)
    (&#39;校园爱情&#39;, 0.6078493595123291),(&#39;情爱&#39;, 0.5976818799972534)
    word:创新
    (&#39;技术创新&#39;, 0.7648976445198059),(&#39;不断创新&#39;, 0.7172579765319824)
    (&#39;创新型&#39;, 0.6573833227157593),(&#39;创新能力&#39;, 0.6533682942390442)
    (&#39;创新性&#39;, 0.6160774827003479),(&#39;革新&#39;, 0.6159394383430481)
    (&#39;人才培养&#39;, 0.6093565821647644),(&#39;开拓创新&#39;, 0.6015594601631165)
    (&#39;探索&#39;, 0.5987343788146973),(&#39;技术革新&#39;, 0.5949685573577881)
</code></pre></div><h2 id="关于作者">关于作者</h2>
<p>老刘，刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。</p>
<p>就职于360人工智能研究院、曾就职于中国科学院软件研究所。</p>
<p><strong>老刘说NLP</strong>，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 |  Python文本分析包更新</title>
      <link>https://hidadeng.github.io/blog/cntext_tutorial/</link>
      <pubDate>Mon, 09 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_tutorial/</guid>
      <description>扩展词典、情感分析、可阅读性，内置9种情感词典，涵盖中英文</description>
      <content:encoded><![CDATA[<p><a href="https://github.com/hidadeng/cntext"><img loading="lazy" src="https://img.shields.io/badge/cntext-%e4%b8%ad%e6%96%87%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%ba%93-orange?style=for-the-badge&amp;logo=appveyor" alt=""  />
</a></p>
<p><a href="version1.2.md">旧版cntext入口</a></p>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> <strong>stats</strong>  文本统计指标
<ul>
<li><input checked="" disabled="" type="checkbox"> 词频统计</li>
<li><input checked="" disabled="" type="checkbox"> 可读性</li>
<li><input checked="" disabled="" type="checkbox"> 内置pkl词典</li>
<li><input checked="" disabled="" type="checkbox"> <strong>情感分析</strong></li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>dictionary</strong> 构建词表(典)
<ul>
<li><input checked="" disabled="" type="checkbox"> Sopmi 互信息扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> W2Vmodels 词向量扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> Glove Glove词嵌入模型</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>similarity</strong>   文本相似度
<ul>
<li><input checked="" disabled="" type="checkbox"> cos相似度</li>
<li><input checked="" disabled="" type="checkbox"> jaccard相似度</li>
<li><input checked="" disabled="" type="checkbox"> 编辑距离相似度</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>mind.py</strong> 计算文本中的认知方向（态度、偏见）</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="cntext_examples.zip">click to download</a></p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext
</code></pre></div><br>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">mind</span>
    <span class="nx">dictionary</span>
    <span class="nx">similarity</span>
    <span class="nx">stats</span>
</code></pre></div><br>
<h2 id="一stats">一、stats</h2>
<p>目前stats内置的函数有</p>
<ul>
<li><strong>readability</strong>  文本可读性</li>
<li><strong>term_freq</strong> 词频统计函数</li>
<li><strong>dict_pkl_list</strong>  获取cntext内置词典列表(pkl格式)</li>
<li><strong>load_pkl_dict</strong> 导入pkl词典文件</li>
<li><strong>sentiment</strong> 情感分析</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="11--readability">1.1  readability</h3>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p>readability(text, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text: 文本字符串数据</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p><strong>中文可读性</strong> 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>


<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 28.0,
 &#39;readability2&#39;: 0.15789473684210525,
 &#39;readability3&#39;: 14.078947368421053}
</code></pre></div><br>
<p>句子中的符号变更会影响结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.16666666666666666,
 &#39;readability3&#39;: 13.583333333333334}
</code></pre></div><p><br><br></p>
<h3 id="12--term_freq">1.2  term_freq</h3>
<p>词频统计函数，返回Counter类型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="13-dict_pkl_list">1.3 dict_pkl_list</h3>
<p>获取cntext内置词典列表(pkl格式)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取cntext内置词典列表(pkl格式)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;, 
 &#39;concreteness.pkl&#39;]
</code></pre></div><p>词典对应关系, 部分情感词典资料整理自 <a href="https://github.com/quanteda/quanteda.sentiment">quanteda.sentiment</a></p>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>DUTIR.pkl</td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td>HOWNET.pkl</td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>sentiws.pkl</td>
<td>SentimentWortschatz (SentiWS)</td>
<td>英文</td>
<td>正面词、负面词；<br>效价</td>
</tr>
<tr>
<td>ChineseFinancialFormalUnformalSentiment.pkl</td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td>ANEW.pkl</td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>词语效价信息</td>
</tr>
<tr>
<td>LSD2015.pkl</td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>NRC.pkl</td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td>geninqposneg.pkl</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HuLiu.pkl</td>
<td>Hu&amp;Liu (2004)正、负情感词典</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>AFINN.pkl</td>
<td>尼尔森 (2011) 的“新 ANEW”效价词表</td>
<td>英文</td>
<td>情感效价信息valence</td>
</tr>
<tr>
<td>LoughranMcDonald.pkl</td>
<td>会计金融LM词典</td>
<td>英文</td>
<td>金融领域正、负面情感词</td>
</tr>
<tr>
<td>ADV_CONJ.pkl</td>
<td>副词连词</td>
<td>中文</td>
<td></td>
</tr>
<tr>
<td>STOPWORDS.pkl</td>
<td></td>
<td>中、英</td>
<td>停用词</td>
</tr>
<tr>
<td>concreteness.pkl</td>
<td>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</td>
<td>English</td>
<td>word &amp; concreateness score</td>
</tr>
</tbody>
</table>
<h3 id="注意">注意:</h3>
<ul>
<li>
<p>如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</li>
<li>
<p>如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue</p>
</li>
</ul>
<br>
<h3 id="14-load_pkl_dict">1.4 load_pkl_dict</h3>
<p>导入pkl词典文件，返回字典样式数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入pkl词典文件,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, ...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...], 
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;, &#39;刿目怵心&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;, &#39;喜糖&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,]
           }
</code></pre></div><br>
<h3 id="15-sentiment">1.5 sentiment</h3>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)
使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>diction:  情感词字典；</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">],</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p>如果不适用pkl词典，可以自定义自己的词典，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<h3 id="16-sentiment_by_valence">1.6 sentiment_by_valence</h3>
<p>sentiment函数默认所有情感词权重均为1，只需要统计文本中情感词的个数，即可得到文本情感得分。</p>
<p>sentiment_by_valence(text, diction, lang=&lsquo;english&rsquo;)函数考虑了词语的效价(valence)</p>
<ul>
<li>text 待输入文本</li>
<li>diction 带效价的词典，DataFrame格式。</li>
<li>lang 语言类型&rsquo;chinese' 或 &lsquo;english&rsquo;，默认&rsquo;english'</li>
</ul>
<p>这里我们以文本具体性度量为例， <strong>concreteness.pkl</strong> 整理自 Brysbaert2014的文章。</p>
<blockquote>
<p>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># load the concreteness.pkl dictionary file</span>
<span class="n">concreteness_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;concreteness.pkl&#39;</span><span class="p">)</span>
<span class="n">concreteness_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">word</th>
<th style="text-align:right">valence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">roadsweeper</td>
<td style="text-align:right">4.85</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">traindriver</td>
<td style="text-align:right">4.54</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">tush</td>
<td style="text-align:right">4.45</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">hairdress</td>
<td style="text-align:right">3.93</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">pharmaceutics</td>
<td style="text-align:right">3.77</td>
</tr>
</tbody>
</table>
<br>
<p>先看一条文本的具体性度量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">reply</span> <span class="o">=</span> <span class="s2">&#34;I&#39;ll go look for that&#34;</span>

<span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                              <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                              <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">score</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1.85
</code></pre></div><br>
<p>很多条文本的具体性度量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">employee_replys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I&#39;ll go look for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that top&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go look for that t-shirt in grey&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt in grey&#34;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">reply</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">employee_replys</span><span class="p">):</span>
    <span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                                  <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    
    <span class="n">template</span> <span class="o">=</span> <span class="s2">&#34;Concreteness Score: </span><span class="si">{score:.2f}</span><span class="s2"> | Example-</span><span class="si">{idx}</span><span class="s2">: </span><span class="si">{exmaple}</span><span class="s2">&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="n">score</span><span class="p">,</span> 
                          <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> 
                          <span class="n">exmaple</span><span class="o">=</span><span class="n">reply</span><span class="p">))</span>
    
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Concreteness Score: 1.55 | Example-0: I&#39;ll go look for that
Concreteness Score: 1.55 | Example-1: I&#39;ll go search for that
Concreteness Score: 1.89 | Example-2: I&#39;ll go search for that top
Concreteness Score: 2.04 | Example-3: I&#39;ll go search for that t-shirt
Concreteness Score: 2.37 | Example-4: I&#39;ll go look for that t-shirt in grey
Concreteness Score: 2.37 | Example-5: I&#39;ll go search for that t-shirt in grey
</code></pre></div><br>
<p><br><br></p>
<h2 id="二dictionary">二、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="21-sopmi">2.1 SoPmi</h3>
<p>SoPmi 共现法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                   <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                   <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                   <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span>
                   <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 44.49 s
</code></pre></div><br>
<h3 id="22-w2vmodels">2.2 W2VModels</h3>
<p>W2VModels 词向量</p>
<p><strong>特别要注意代码需要设定lang语言参数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   174 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s

</code></pre></div><br>
<h3 id="需要注意">需要注意</h3>
<p>训练出的w2v模型可以后续中使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">model路径</span><span class="p">)</span>
<span class="c1">#找出word的词向量</span>
<span class="c1">#w2v_model.get_vector(word)</span>
<span class="c1">#更多w2_model方法查看</span>
<span class="c1">#help(w2_model)</span>
</code></pre></div><p>例如本代码，运行生成的结果路径<code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;technology&#39;, 0.689210832118988),
 (&#39;infrastructure&#39;, 0.669672966003418),
 (&#39;resources&#39;, 0.6695448160171509),
 (&#39;talent&#39;, 0.6627111434936523),
 (&#39;execution&#39;, 0.6549549102783203),
 (&#39;marketing&#39;, 0.6533523797988892),
 (&#39;merchandising&#39;, 0.6504817008972168),
 (&#39;diversification&#39;, 0.6479553580284119),
 (&#39;expertise&#39;, 0.6446896195411682),
 (&#39;digital&#39;, 0.6326863765716553)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.45616838, -0.7799563 ,  0.56367606, -0.8570078 ,  0.600359  ,
       -0.6588043 ,  0.31116748, -0.11956959, -0.47599426,  0.21840936,
       -0.02268819,  0.1832016 ,  0.24452794,  0.01084935, -1.4213187 ,
        0.22840202,  0.46387577,  1.198386  , -0.621511  , -0.51598716,
        0.13352732,  0.04140598, -0.23470387,  0.6402956 ,  0.20394802,
        0.10799981,  0.24908689, -1.0117126 , -2.3168423 , -0.0402851 ,
        1.6886286 ,  0.5357047 ,  0.22932841, -0.6094084 ,  0.4515793 ,
       -0.5900931 ,  1.8684244 , -0.21056202,  0.29313338, -0.221067  ,
       -0.9535679 ,  0.07325   , -0.15823542,  1.1477109 ,  0.6716076 ,
       -1.0096023 ,  0.10605699,  1.4148282 ,  0.24576302,  0.5740349 ,
        0.19984631,  0.53964925,  0.41962907,  0.41497853, -1.0322098 ,
        0.01090925,  0.54345983,  0.806317  ,  0.31737605, -0.7965337 ,
        0.9282971 , -0.8775608 , -0.26852605, -0.06743863,  0.42815775,
       -0.11774074, -0.17956367,  0.88813037, -0.46279573, -1.0841943 ,
       -0.06798118,  0.4493006 ,  0.71962464, -0.02876493,  1.0282255 ,
       -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922,
       -0.16753489,  0.14065655, -1.8609751 ,  0.03587054,  1.2792674 ,
        1.2732009 , -0.74120265, -0.98000383,  0.4521185 , -0.26387128,
        0.37045383,  0.3680011 ,  0.7197629 , -0.3570571 ,  0.8016917 ,
        0.39243212, -0.5027844 , -1.2106236 ,  0.6412354 , -0.878307  ],
      dtype=float32)
</code></pre></div><p><br><br></p>
<h3 id="23-co_occurrence_matrix">2.3 co_occurrence_matrix</h3>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence2.png" alt=""  />
</p>
<p><br><br></p>
<h3 id="24--glove">2.4  Glove</h3>
<p>构建Glove词嵌入模型，使用英文数据<code>data/brown_corpus.txt</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Glove</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">create_vocab</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/brown_corpus.txt&#39;</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">cooccurrence_matrix</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_embeddings</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4: ...Create vocabulary for Glove.
Step 2/4: ...Create cooccurrence matrix.
Step 3/4: ...Train glove embeddings. 
             Note, this part takes a long time to run
Step 3/4: ... Finish! Use 175.98 s
</code></pre></div><p>生成的Glove词嵌入文件位于<code>output/Glove</code> 。</p>
<p><br><br></p>
<h2 id="三similarity">三、similarity</h2>
<p>四种相似度计算函数</p>
<ul>
<li>cosine_sim(text1, text2)  cos余弦相似</li>
<li>jaccard_sim(text1, text2)     jaccard相似</li>
<li>minedit_sim(text1, text2)  最小编辑距离相似度；</li>
<li>simple_sim(text1, text2) 更改变动算法</li>
</ul>
<p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩啊&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.82
0.67
2.00
0.87
</code></pre></div><p><br><br></p>
<h2 id="四text2mind">四、Text2Mind</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<p>- <strong>多个对象在某概念的远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
<p>- 多个对象在某<strong>概念的分量(fen，一声)的多少</strong>， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
<p>这两种认知分别可以用向量距离、向量语义投影计算得来。</p>
<ul>
<li>tm.sematic_distance(words, c_words1, c_words2)  向量距离</li>
<li>tm.sematic_projection(words, c_words1, c_words2)  向量语义投影</li>
</ul>
<h3 id="41-tmsematic_distancewords-c_words1-c_words2">4.1 tm.sematic_distance(words, c_words1, c_words2)</h3>
<p>分别计算words与c_words1、c_words2语义距离，返回距离差值。</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果d1-d2&lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<p><strong>下载glove_w2v.6B.100d.txt</strong>链接: <a href="https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw">https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw</a> 提取码: 72l0</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#Note: this is a word2vec format model</span>
<span class="n">tm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Text2Mind</span><span class="p">(</span><span class="n">w2v_model_path</span><span class="o">=</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="n">engineer</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">mans</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">womans</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">mans</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">womans</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><p>-0.38 意味着工程师更接近于男人，而不是女人。</p>
<br>
<h3 id="42-tmsematic_projectionwords-c_words1-c_words2">4.2 tm.sematic_projection(words, c_words1, c_words2)</h3>
<p><strong>语义投影</strong>，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，word越位于c_words2一侧。</p>
<p>下图是语义投影示例图，本文算法和图片均来自 &ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&rdquo;</p>
<p><img loading="lazy" src="img/Nature_Semantic_projection_recovering_human_knowledge_of.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">smalls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">bigs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">smalls</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">bigs</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p>在这几个动物尺寸的感知上，人类觉得老鼠体型是最小，马的体型是最大。</p>
<p><br><br></p>
<h2 id="引用说明">引用说明</h2>
<p>如果研究中使用cntext，请使用以下格式进行引用</p>
<h3 id="apalike">apalike</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Deng X., Nan P. (2022). cntext: a Python tool for text mining (version 1.7.9). DOI: 10.5281/zenodo.7063523 URL: https://github.com/hiDaDeng/cntext
</code></pre></div><h3 id="bibtex">bibtex</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@misc{YourReferenceHere,
author = {Deng, Xudong and Nan, Peng},
doi = {10.5281/zenodo.7063523},
month = {9},
title = {cntext: a Python tool for text mining},
url = {https://github.com/hiDaDeng/cntext},
year = {2022}
}
</code></pre></div><h3 id="endnote">endnote</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">%0 Generic
%A Deng, Xudong
%A Nan, Peng
%D 2022
%K text mining
%K text analysi
%K social science
%K management science
%K semantic analysis
%R 10.5281/zenodo.7063523
%T cntext: a Python tool for text mining
%U https://github.com/hiDaDeng/cntext
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>sentence-transformer库 | 句子语义向量化</title>
      <link>https://hidadeng.github.io/blog/sentence-transformer-tutorial/</link>
      <pubDate>Mon, 09 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/sentence-transformer-tutorial/</guid>
      <description>使用sentence-transformer库BERT技术，将句子语义向量化</description>
      <content:encoded><![CDATA[<blockquote>
<p>内容摘自</p>
<p>刘焕勇博客: <a href="https://liuhuanyong.github.io/">https://liuhuanyong.github.io/</a></p>
<p>原文地址: <a href="https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q">https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q</a></p>
<p>公众号: 老刘说NLP</p>
</blockquote>
<p>word2vec、glove是两种静态的词向量模型，即每个词语只有一个固定的向量表示。但在不同语境中，词语的语义会发生变化，按道理词向量也应该动态调整。相比word2vec、glove生成的静态词向量， BERT是一种动态的技术，可以根据上下文情景，得到语义变化的词向量。</p>
<p>HuggingFace网站提供了简易可用的数据集、丰富的预训练语言模型， 通过sentence-transformer库，我们可以使用HuggingFace内的预训练模型，得到不同情景的文本的语义向量。</p>
<p>HuggingFace网站  <a href="https://huggingface.co/">https://huggingface.co/</a></p>
<p><img loading="lazy" src="img/HuggingFace.png" alt=""  />
</p>
<br>
<h2 id="动态句向量">动态句向量</h2>
<p>sentence-transformer框架提供了一种简便的方法来计算句子和段落的向量表示（也称为句子嵌入）</p>
<p><img loading="lazy" src="img/sentence-transformer.png" alt=""  />
</p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">sentence</span><span class="o">-</span><span class="n">transformers</span>
</code></pre></div><br>
<h2 id="代码">代码</h2>
<p><a href="sentence-transformer-tutorial.zip">click to download the code</a></p>
<p>使用huggingface中的distiluse-base-multilingual-cased与训练模型，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">util</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;distiluse-base-multilingual-cased&#39;</span><span class="p">)</span>
</code></pre></div><p>第一次运行上方的代码，需要运行一定的时间用于下载。下载完成后，我们使用同种语义的中英文句子，分别计算得到emb1和emb2两个句向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">emb1 = model.encode(&#39;Natural language processing is a hard task for human&#39;)

emb2 = model.encode(&#39;自然语言处理对于人类来说是个困难的任务&#39;)
emb1
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 2.58186590e-02,  4.65703346e-02,  4.25276496e-02, -1.67875513e-02,
        5.56012690e-02, -3.44308838e-02, -6.53978735e-02,  1.77450478e-02,
       -3.47155109e-02,  2.86140274e-02,  2.48657260e-02,  7.94188876e-04,
        5.09755425e-02, -1.76107027e-02, -1.04308855e-02,  7.61642214e-03,
        ...
        4.28482369e-02,  1.76657233e-02, -5.83355911e-02,  1.92921527e-03,
        2.81221420e-02,  5.24400780e-03,  2.10703332e-02,  7.96715263e-03,
       -6.80630878e-02, -2.05304120e-02, -2.43293475e-02, -1.87458862e-02],
      dtype=float32)
</code></pre></div><p>在distiluse-base-multilingual-cased这种模型中， 不同语言的同义句应该具有类似的语义，那么cos相似度应该是很大的。越接近于1越相似；越接近于0，越不相似。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">cos_sim</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">pytorch_cos_sim</span><span class="p">(</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb2</span><span class="p">)</span>
<span class="n">cos_sim</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[0.8960]])
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 从符号到嵌入：计算社会科学的两种文本表示</title>
      <link>https://hidadeng.github.io/blog/from_sysbol_to_embeddings_in_computational_social_science/</link>
      <pubDate>Mon, 25 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/from_sysbol_to_embeddings_in_computational_social_science/</guid>
      <description>如何有效地表示数据以挖掘我们想要的计算社会科学的含义？为了探索答案，我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示</description>
      <content:encoded><![CDATA[<p>B站看到大牛刘知远关于文本分析在计算社会科学领域应用的分享，解答了我对文本表示的疑惑，看完了能对文本的特征工程加深理解，同时也能更清晰未来如何借助计算机科学技术开展社会科学研究。</p>
<blockquote>
<p><strong>全文摘抄自</strong></p>
<p>Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.</p>
</blockquote>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1qi4y1Q7qj&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<h2 id="摘要">摘要</h2>
<p><strong>计算社会科学</strong>（CSS），旨在利用计算方法来解决社会科学问题，是一个新兴和快速发展的领域。 CSS 的研究是数据驱动的，并且显着受益于在线用户生成内容和社交网络的可用性，其中包含用于调查的富文本和网络数据。然而，这些大规模、多模态的数据也给研究人员带来了很大的挑战：<strong>如何有效地表示数据以挖掘我们想要的 CSS 含义</strong>？为了探索答案，<strong>我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示</strong>，并为每个方案介绍了一系列典型的方法。随后，我们基于对来自 6 个涉及 CSS 的顶级场所的 400 多篇研究文章的调查，展示了上述表示的应用。从这些应用程序的统计数据中，<strong>我们挖掘出每种表示的强度，并发现基于嵌入的表示在过去十年中出现并获得越来越多的关注的趋势</strong>。最后，我们讨论了几个关键挑战和未来方向的开放性问题。本调查旨在为 CSS 研究人员提供对数据表示的更深入理解和更明智的应用。</p>
<p><strong>关键词</strong>：计算社会科学；基于符号的表示；基于嵌入的表示；社交网络</p>
<br>
<h2 id="一计算社会学数据分析流程">一、计算社会学数据分析流程</h2>
<p>其中第二步，数据表示目前有两大类表示(特征工程)方法</p>
<ul>
<li><strong>基于符号的文本表示</strong>；符号可以是单词(或词组)，也可以是概念(如正面情感、负面情感)</li>
<li><strong>基于嵌入(分布式)的文本表示</strong>；相比于符号法，将词(词组)看做一个点。嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(king)-v(queen)约等于v(man)-v(woman)</li>
</ul>
<p><img loading="lazy" src="img/fig1.png" alt=""  />
</p>
<br>
<h2 id="二基于符号的文本表示">二、基于符号的文本表示</h2>
<p>基于符号的文本表示一般来说默认词语是不可分的符号，每个词能根据词频统计出现次数的多与少，或是否存在。</p>
<h3 id="21-词语层面">2.1 词语层面</h3>
<ul>
<li>
<p>基于词频表示</p>
<ul>
<li>是否出现，出现标位1，反之标位0。</li>
<li>出现多少，词语出现几次，标为几个。</li>
</ul>
</li>
<li>
<p>基于特征表示，如每个词带有权重(得分)</p>
</li>
<li>
<p>基于网络表示，如词语共现网络(矩阵)</p>
</li>
</ul>
<h3 id="22-句子层面">2.2 句子层面</h3>
<ul>
<li>
<p>基于词频的表示</p>
<ul>
<li>one-hot 将文本转为向量，向量中每个数，词语出现标位1，反之标位0</li>
<li>bag-of-words，将文本转为向量，向量中每个数，词语出现n次标记为n</li>
<li>n-grams，对词组的处理，将词组看做一个单词(整体)。</li>
<li>Tf-Idf ,该算法分为tf和idf两部分。其中tf与bag-of-words类似，考虑词语出现次数。而idf还考虑词语在语料中出现场景的稀缺性程度。</li>
</ul>
</li>
<li>
<p>基于语法特征，如句法依存关系，类似于英语语法，将句子分为主谓宾、动词、名词等。</p>
</li>
<li>
<p>词典法，如使用正、负情感词典，对文本数据进行情感分析，可以得到pos和neg的各自得分</p>
</li>
</ul>
<p><img loading="lazy" src="img/fig2.png" alt=""  />
</p>
<br>
<h2 id="三基于嵌入的文本表示">三、基于嵌入的文本表示</h2>
<h3 id="31词语层面">3.1词语层面</h3>
<p>嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(best)-v(good)约等于v(worst)-v(bad)</p>
<h3 id="32-句子层面">3.2 句子层面</h3>
<p>词语是向量，那么由词语组成的句子也会加权得到一个向量。含有相似话题或含义相近的句子在多维向量空间中会比较接近。</p>
<p><img loading="lazy" src="img/fig7.png" alt=""  />
</p>
<br>
<h2 id="四任务分类文本的用法">四、任务分类：文本的用法</h2>
<p><img loading="lazy" src="img/fig16.png" alt=""  />
</p>
<p>有了文本数据，刚刚解决了如何表示文本。接下来，需要明确，我们使用文本目的是为了做哪类分析，得到哪些信息。有8种常见的文本分析图式</p>
<ul>
<li>描述性。如随时间推移，词频的发展趋势是变大的</li>
<li>相关性。</li>
<li>聚类。如lda话题分析、k-means聚类</li>
<li>相似度。两个文档转为向量后，可以通过cosine计算相似度</li>
<li>分类。机器学习分类，判断某文本隶属于哪个类别</li>
<li>回归。例如根据文本，判断某件事发生的概率</li>
<li>语言模型。</li>
<li>排序。</li>
</ul>
<br>
<h2 id="五发文趋势-符号vs嵌入">五、发文趋势-符号vs嵌入</h2>
<p>基于上一节中对应用程序的介绍，可以观察到基于符号和基于嵌入的表示在 <strong>计算社会科学</strong>中都得到了相当大的采用。为了明确研究它们的覆盖范围，我们计算了每年使用两种表示中的一种或两种的作品数量，如图 17 所示。通过比较nature、science、pnas三大顶级期刊，我们可以发现使用<strong>基于嵌入表示</strong>的文章比例在过去几年中逐渐。这表明越来越多的 计算社会科学文章 已经考虑并受益于基于嵌入表示。</p>
<p>图 18 显示了在 计算机领域ACL、WWW 和 KDD 的会议上中，发现使用基于嵌入的表示的文章数量已大大超过使用基于符号的表示的文章数量。然而，与图 17 相比，计算机科学会议中基于嵌入的表示的数量与三个多学科期刊之间存在很大差距。</p>
<p><img loading="lazy" src="img/3_top_journals.png" alt=""  />
</p>
<p><img loading="lazy" src="img/nlp.png" alt=""  />
</p>
<p>总而言之，在过去十年中，基于嵌入的表示已经出现并在 计算社会科学 中发挥着越来越重要的作用。</p>
<br>
<h2 id="六趋势解读">六、趋势解读</h2>
<p>基于它们的内部机制和现有应用，对趋势解读，我们总结出以下三个关键点。</p>
<p>基于符号的表示因其明确性和可解释性而擅长描述和关系的任务。</p>
<p>基于符号的表示中的每个值都表示一定的人类可读的含义，因此我们可以直接使用它来观察数据的分布，以及提取对象之间的关系。例如，基于频率的词表示用于观察文化变化并捕捉新闻中提及次数与公司股票交易量之间的关系。虽然基于主题模型的表示和一些基于神经的表示在一定程度上具有实际意义，但它们对于社会科学研究人员来说仍然是模糊的并且不那么引人注目。</p>
<p>由于神经网络具有强大的拟合数据和提取深度语义的能力，基于嵌入的表示在预测（例如分类和回归）和相似性任务中表现更好。一方面，神经网络通过大规模神经元的连接实现高效的输入输出映射功能。另一方面，通过多层网络的构建，实现深层语义和抽象概念的提取。现有研究表明，深层捕获相对于浅层更抽象的特征。诸如社会偏见和道德化之类的抽象概念都可以通过基于嵌入的表示来很好地衡量。虽然我们提到基于符号的表示可以通过一些定义的符号来代表抽象概念，但这种表示仍然是部分和肤浅的，很难捕捉到它们的全貌。</p>
<p>基于嵌入的表示需要更少的人力。基于符号的表示通常需要大量的专家知识来定义研究对象的特征，这是劳动密集型的。此外，对于一些没有充分特征的抽象概念或对象，它们的表现将受到限制。与它们不同的是，基于嵌入的表示是从数据中自动提取的，几乎不需要人工干预，甚至可以补充人类知识。例如，可以使用神经网络来自动恢复丢失的巴比伦文本，这即使对专家来说也是具有挑战性的。此外，基于嵌入的表示可以在没有手动定义的情况下描述语言的复杂性和歧义性。</p>
<br> 
<h2 id="七未来展望">七、未来展望</h2>
<p>尽管在过去十年中出现了从符号到嵌入的趋势，但仍有许多挑战和悬而未决的问题有待探索。展望未来，我们列出了一些与计算社会科学 中的数据表示相关的基本和潜在的未来方向。</p>
<p>预训练的语言模型。近年来，预训练的语言模型受到了相当大的关注，并在处理文本数据方面取得了巨大的成功 [100, 240]。这些模型从百科全书和书籍等海量文本数据中学习丰富的语义信息，仅在下游任务中进行微调以实现有效的基于嵌入的表示。因此，对于 计算社会科学，我们可以借助预训练的语言模型获得更通用、更健壮的文本表示。与从传统神经网络模型中学习的表示相比，这些表示不仅可以更广泛、更准确地从文本中分析社会现象，而且还可以减少那些需要大量标记数据的任务的人工注释。</p>
<p>图神经网络。通过消息传递机制，图神经网络 [461] 可以同时有效地对网络拓扑和节点/边缘特征（例如文本信息）进行建模，从而提供一个统一的框架来利用来自异构来源的信息。 计算社会科学 中的许多场景需要处理社交网络以及个人特征。因此，图神经网络技术在 计算社会科学 研究中具有很大的应用潜力，可以学习融合文本和网络信息的表示。事实上，计算机科学中的各种应用，例如自然语言处理 [418] 和推荐系统 [439]，已经采用图神经网络进行建模。</p>
<p>设计为预测和相似性。基于嵌入的表示以丰富和深层次的语义而闻名，而基于符号的表示通常保留在部分和浅层语义中。同时，基于嵌入的表示擅长预测和相似性的任务。因此，为了充分利用嵌入中的强语义，鼓励 计算社会科学 研究人员尽可能将研究问题设计为预测或相似性任务。例如，我们可以将社会偏见问题设计为性别词和中性词嵌入之间的相似性度量 [59, 133]。此外，人类语言的复杂性可以设计为一项预测任务，它以语言模型为指标查看单词或句子的预测概率[155]。</p>
<p>可解释性。诚然，基于嵌入的方法的一个缺点是缺乏可解释性。这个问题会损害与道德、安全或隐私相关的决策关键系统的应用。尽管嵌入模型，尤其是神经网络模型的可解释性尚未完全解决，但计算机科学领域的研究人员已经做出了一些努力，以提高基于神经模型的可解释性 [16]。因此，利用基于嵌入的模型和可解释性分析方法进行有效和（部分）可解释的预测将是一个有趣的方向。</p>
<br>
<h2 id="结论">结论</h2>
<p>计算社会科学作为一个新兴且有前途的跨学科领域，近年来吸引了相当多的研究兴趣。 计算社会科学 研究中广泛使用两种主要类型的数据，即文本数据和网络数据。在本次调查中，我们首先将数据表示总结为基于符号和基于嵌入的表示，并在构建这些表示时进一步介绍典型的方法。之后，我们基于来自 6 个经典期刊和会议的 400 多篇高被引文献，对这两类表示的应用进行了全面回顾。根据对这些应用的统计，发现了 计算社会科学 中基于嵌入的文本和网络表示正在出现和增长的趋势，我们进一步讨论了其中的原因。最后，我们提出了 计算社会科学 中的四个挑战和未解决的问题，它们是需要探索的基本和潜在方向。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>中文词向量资源汇总 &amp; 使用方法</title>
      <link>https://hidadeng.github.io/blog/embeddings_resource_usage_method/</link>
      <pubDate>Thu, 21 Apr 2022 15:40:10 +0600</pubDate>
      
      <guid>/blog/embeddings_resource_usage_method/</guid>
      <description>数十种中文词向量模型资源下载&amp;amp;使用方法</description>
      <content:encoded><![CDATA[<br>
<h2 id="项目地址">项目地址</h2>
<p><a href="https://github.com/Embedding/Chinese-Word-Vectors">https://github.com/Embedding/Chinese-Word-Vectors</a></p>
<p>Chinese-Word-Vectors项目提供超过100种中文词向量，其中包括不同的表示方式（稠密SGNS和稀疏PPMI）、不同的上下文特征（词、N元组、字等等）、以及不同的训练语料。获取预训练词向量非常方便，下载后即可用于下游任务。</p>
<br>
<h2 id="参考文献">参考文献</h2>
<p>如果使用了本项目的词向量和CA8数据集请进行如下引用：</p>
<p>Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, <a href="http://aclweb.org/anthology/P18-2023"><em>Analogical Reasoning on Chinese Morphological and Semantic Relations</em></a>, ACL 2018.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@InProceedings{P18-2023,
  author =  &#34;Li, Shen
    and Zhao, Zhe
    and Hu, Renfen
    and Li, Wensi
    and Liu, Tao
    and Du, Xiaoyong&#34;,
  title =   &#34;Analogical Reasoning on Chinese Morphological and Semantic Relations&#34;,
  booktitle =   &#34;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&#34;,
  year =  &#34;2018&#34;,
  publisher =   &#34;Association for Computational Linguistics&#34;,
  pages =   &#34;138--143&#34;,
  location =  &#34;Melbourne, Australia&#34;,
  url =   &#34;http://aclweb.org/anthology/P18-2023&#34;
}
</code></pre></div><br>
<h3 id="不同领域">不同领域</h3>
<p>下列词向量基于不同的表示方式、不同的上下文特征以及不同领域的语料训练而成。</p>
<table align="center">
    <tr align="center">
        <td colspan="5"><b>Word2vec / Skip-Gram with Negative Sampling (SGNS)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1IiIbQGJ_AooTj5s8aZYcvA">300d</a> / PWD: 5555</td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1AmXYWVgkxrG4GokevPtNgA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZKePwxwsDdzNrfkc6WKdGQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZBVVD4mUSUuXOxlZ3V71ZA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19wQrclyynOnco3JBvnI5pA">300d</td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/19sqMz-JAhhxh3o6ecvQxQw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1upPkA8KJnxTZBfjuNDtaeQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1BvKk2QjbtQMch7EISppW2A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19Vso_k79FZb5OZCWQPAnFQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1tUghuTno5yOvOx4LXA9-wg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13yVrXeGYkxdGW3P6juiQmA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1pUqyn7mnPcUmzxT64gGpSw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1svFOwFBKnnlsqrF1t99Lnw">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/1EhtsbDa3ekzZPODWNLHcXA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FcPHv7S4vUgnL7WeWf4_PA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13CAxY5ffRFuOcHZu8VmArw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sqvrUtGBAZ7YWEsGz41DRQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VGOs0RH7DXE5vRrtw6boQA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1OQ6fQLCgqT43WTwh5fh_lg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1_xogqF9kJT6tmQHSAYrYeg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Fo27Lv_0nz8FXg-xbOz14Q">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/1zbuUJEEEpZRNHxZ7Gezzmw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/11PWBcvruXEDvKf2TiIXntg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/10bhJpaXMCUK02nHvRAttqA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FHl_bQkYucvVk-j2KG4dxA">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1ciq8iXtcrHpu3ir_VhK0zg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Oa4CkPd8o2xd6LEAaa4gmg">300d</a> / PWD: z5b4</td>
      <td><a href="https://pan.baidu.com/s/1IG8IxNp2s7vVklz-vyZR9A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1SEOKrJYS14HpqIaQT462kA">300d</a> / PWD: yenb</td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1vPSeUsSiWYXEWAuokLR0qQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sS9E7sclvS_UZcBgHN7xLQ">300d</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合<br>Baidu Netdisk / Google Drive</td>
      <td>
        <a href="https://pan.baidu.com/s/1luy-GlTdqqvJ3j-A4FcIOw">300d</a><br>
        <a href="https://drive.google.com/open?id=1Zh9ZCEu8_eSQ-qkYVQufQDNKPC4mtEKR">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1oJol-GaRMk4-8Ejpzxo6Gw">300d</a><br>
        <a href="https://drive.google.com/open?id=1WUU9LnoAjs--1E_WqcghLJ-Pp8bb38oS">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1DjIGENlhRbsVyHW-caRePg">300d</a><br>
        <a href="https://drive.google.com/open?id=1aVAK0Z2E5DkdIH6-JHbiWSL5dbAcz6c3">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/14JP1gD7hcmsWdSpTvA3vKA">300d</a><br>
        <a href="https://drive.google.com/open?id=1kSAl4_AOg3_6ayU7KRM0Nk66uGdSZdnk">300d</a>
      </td>
    </tr>
</table>
<table align="center">
    <tr align="center">
        <td colspan="5"><b>Positive Pointwise Mutual Information (PPMI)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1_itcjrQawCwcURa7WZLPOA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1cEZzN1S2senwWSyHOnL7YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1KcfFdyO0-kE9S9CwzIisfw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FXYM3CY161_4QMgiH8vasQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1MGXRrc54nITPzQ7sfEUjMA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1mtxZna8UJ7xBIxhBFntumQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dDImpAx41V73Byl2julOGA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1bsBQHXFpxMHGBexYof1_rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/1NLr1K7aapU2sYBvzbVny5g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1LJl3Br0ccGDHP0XX2k3pVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1GQQXGMn1AHh-BlifT0JD2g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1Xm9Ec3O3rJ6ayrwVwonC7g">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1ECA51CZLp9_JB_me7YZ9-Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FO39ZYy1mStERf_b53Y_yQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1lLBFBk8nn3spFAvKY9IJ6A">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1f-dLQZlZo_-B5ZKcPIc6rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/10wtgdmrTsTrjpSDvI0KzOw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b6zjvhOIqTdACSSbriisVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1w24vCfgqcoJvPxsB5VrRvw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b9BPiDRhiEZ-6ybTcovrqQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VaUP3YJC0IZKTbJ-1_8HZg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1g39PKwT0kSmpneKOgXR5YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1d8Bsuak0fyXxQOVUiNr-2w">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1D5fteBX0Vy4czEqpxXjlrQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/15O2EbToOzjNSkzJwAOk_Ug">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/11Dqywn0hfMhysto7bZS1Dw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1wY-7mfV6nwDj_tru6W9h4Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1DMW-MgLApbQnWwDd-pT_qw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1HTHhlr8zvzhTwed7dO0sDg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1jAuGJBxKqgapt__urGsBOQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/173AJfCoAV0ZA8Z31tKBdTA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dFCxke_Su3lLsuwZr7co3A">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1NJ1Gc99oE0-GV0QxBqy-qw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1YGEgyXIbw0O4NtoM1ohjdA">Sparse</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
    </tr>
</table>
<p><sup>*</sup>由于古汉语中绝大部份词均为单字词，因此只需字向量。</p>
<br>
<h2 id="语料">语料</h2>
<p>项目花费了大量精力来收集了来自多个领域的语料。所有的文本数据均移除了html和xml标记，仅保留了纯文本。之后采用了<a href="https://github.com/hankcs/HanLP">HanLP(v_1.5.3)</a>对文本进行了分词。此外，我们将繁体中文用<a href="https://github.com/BYVoid/OpenCC">Open Chinese Convert (OpenCC)</a>转换为了简体中文。更详细的语料信息如下所示：</p>
<table align="center">
	<tr align="center">
		<td><b>语料</b></td>
		<td><b>大小</b></td>
		<td><b>词数量</b></td>
		<td><b>词汇量</b></td>
		<td><b>详情</b></td>
	</tr>
	<tr align="center">
		<td>Baidu Encyclopedia<br />百度百科</td>
		<td>4.1G</td>
		<td>745M</td>
		<td>5422K</td>
		<td>中文百科<br />https://baike.baidu.com/</td>
	</tr>
	<tr align="center">
		<td>Wikipedia_zh<br />中文维基百科</td>
		<td>1.3G</td>
		<td>223M</td>
		<td>2129K</td>
		<td>中文维基百科<br />https://dumps.wikimedia.org/</td>
	</tr>
	<tr align="center">
		<td>People's Daily News<br />人民日报</td>
		<td>3.9G</td>
		<td>668M</td>
		<td>1664K</td>
		<td>人民日报新闻数据(1946-2017)<br />http://data.people.com.cn/</td>
	</tr>
	<tr align="center">
		<td>Sogou News<br />搜狗新闻</td>
		<td>3.7G</td>
		<td>649M</td>
		<td>1226K</td>
		<td>Sogou labs的新闻数据<br />http://www.sogou.com/labs/</td>
	</tr>
  <tr align="center">
    <td>Financial News<br />金融新闻</td>
    <td>6.2G</td>
    <td>1055M</td>
    <td>2785K</td>
    <td>从多个网站收集到的金融新闻</td>
  </tr>
	<tr align="center">
		<td>Zhihu_QA<br />知乎问答</td>
		<td>2.1G</td>
		<td>384M</td>
		<td>1117K</td>
		<td>中文问答数据<br />https://www.zhihu.com/</td>
	</tr>
	<tr align="center">
		<td>Weibo<br />微博</td>
		<td>0.73G</td>
		<td>136M</td>
		<td>850K</td>
		<td>NLPIR Lab提供的微博数据<br />http://www.nlpir.org/wordpress/download/weibo.7z</td>
	</tr>
	<tr align="center">
		<td>Literature<br />文学作品</td>
		<td>0.93G</td>
		<td>177M</td>
		<td>702K</td>
		<td>8599篇现代文学作品</td>
	</tr>
	<tr align="center">
		<td>Mixed-large<br />综合</td>
		<td>22.6G</td>
    <td>4037M</td>
    <td>10653K</td>
		<td>上述所有数据的汇总</td>
	</tr>
  <tr align="center">
    <td>Complete Library in Four Sections<br />四库全书</td>
    <td>1.5G</td>
    <td>714M</td>
    <td>21.8K</td>
    <td>目前最大的古代文献汇总</td>
  </tr>
</table>
上述统计结果中，所有词都被计算在内，包括低频词。
<br>
<h2 id="导入模型代码">导入模型(代码)</h2>
<p>例如我下载了多个词模型，下载得到bz2结尾的文件名，例如<code>sgns.financial.bigram.bz2</code>。</p>
<p><img loading="lazy" src="models.png" alt=""  />
</p>
<p>使用方式</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1">#以金融sgns.financial.bigram.bz2为例</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.financial.bigram.bz2&#39;</span><span class="p">,</span> 
                                          <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                          <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>


<span class="n">model</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x7fe7fad79d60&gt;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;投资&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.084635,  0.890228, -0.23223 , -0.308985,  0.058241,  0.458777,
       -0.152547, -0.413471,  0.269701, -0.078043, -0.4155  ,  0.074735,
        0.35714 ,  0.103431,  0.601784, -0.390854,  0.814801, -0.122664,
       -1.076744,  0.516941, -0.293319, -0.310251, -0.407794,  0.003898,
       -0.210962,  0.378095, -0.345955, -0.223848,  0.700162,  0.207644,
        0.426249, -0.272832, -0.110305, -0.701062, -0.173407, -0.172121,
       -0.682592,  0.593414,  0.279591, -0.408284, -0.166693,  0.753402,
        0.037375,  0.141865, -0.246024, -0.108663, -0.225255, -0.856601,
        0.381026,  0.401248,  0.012108, -0.126305, -0.374255,  0.728795,
        0.219549, -0.354029, -0.353131,  0.064867,  0.49565 , -0.503267,
       -0.304075,  0.145036,  0.688948,  0.063382, -0.223243,  0.474251,
        0.80543 ,  0.683178,  0.118159,  0.408411, -0.020066,  0.009045,
       -0.135446, -0.069633,  0.206357,  0.482845, -0.075307,  0.06433 ,
       -0.112367,  0.011816,  0.87427 , -0.120287, -0.31036 ,  0.369985,
        0.560386, -0.215248,  0.389631,  0.042943, -0.319149,  0.951551,
       -0.335188,  0.642246, -0.55546 ,  0.322397,  0.659618, -0.213124,
        0.346696, -0.342239,  0.31479 ,  0.078533, -0.345148,  0.815577,
       -0.530134,  0.303419, -0.158916, -0.190564,  0.436046, -0.112251,
       -0.339966,  0.253645,  0.181076,  0.122875, -0.310951, -0.126253,
        1.641405,  0.357906,  0.165796,  0.398656, -0.330591,  0.20328 ,
       -0.077191, -0.421248, -0.078504, -0.734519,  0.146212,  0.535727,
        0.014134,  0.040322, -0.44809 , -0.758205, -0.151237,  0.248258,
       -0.319704,  0.656033, -0.518857,  0.932356, -1.01786 , -0.46354 ,
        0.160921, -0.243597,  0.106666, -0.03404 ,  0.010672,  0.260243,
        0.899813,  0.171735, -0.108209, -0.009843, -0.18113 ,  0.302494,
        0.187285,  0.064669, -0.502041, -0.724377, -0.294312, -0.522256,
        0.334543,  0.740455, -0.357653,  0.540747,  0.256146,  0.513839,
        1.116628, -0.626111,  0.505574,  0.089774, -0.381137, -0.282352,
       -0.457542,  0.198909,  0.313638,  0.560809,  0.25295 ,  0.878158,
       -0.289311, -0.629047,  0.011103,  0.041058, -0.291302, -0.014001,
       -0.027697, -0.445817, -0.070086,  0.159816, -0.120071,  1.280489,
       -0.108866,  0.01586 , -0.505574, -0.679772, -0.343165,  0.595633,
        0.438108, -0.364066, -0.393667,  0.442285,  0.24979 , -0.191607,
        0.425692,  0.535577, -0.480332, -0.737461,  0.588498, -0.380264,
        0.151292,  0.077519, -0.221384,  0.699436,  0.401642,  0.509026,
       -0.411141,  0.206719, -0.097051, -0.451834, -0.825617,  0.602984,
        0.2853  ,  0.46055 ,  0.96472 ,  0.322712, -0.373446,  0.207944,
        0.236688,  0.566523,  0.037644,  1.241091,  0.025682,  0.373211,
        0.097712, -0.195355,  0.264579, -0.072992, -0.121629,  0.041688,
        0.213666,  0.329652, -0.015182,  0.396307,  0.117955,  0.119577,
       -0.334761, -0.135917,  0.409983,  0.512367, -0.292204,  0.302897,
       -0.325733,  0.383173, -0.92419 , -0.377535, -0.059801, -0.606275,
       -0.240482,  0.054021, -0.581386, -0.555691,  0.158354,  0.103765,
        0.107681,  0.248877, -0.597925,  0.193332,  0.844085,  0.00584 ,
        0.041622, -0.111235,  0.617778,  0.234883, -0.09562 ,  0.408324,
       -0.107121,  0.717875,  0.674794,  0.127214, -0.178357,  0.331436,
        0.417898, -0.650833, -0.428309, -0.576132,  0.210533, -0.057879,
       -0.578397,  0.468586,  0.103365, -0.403216, -0.398776,  0.094514,
       -0.130387,  0.628187, -0.463082, -0.951649,  0.561544,  0.118903,
        0.448327, -0.171685, -0.672348,  0.069471,  0.556452, -0.335425],
      dtype=float32)
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">model.similar_by_key(&#39;投资&#39;)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;长期投资&#39;, 0.5135656595230103),
 (&#39;投资规模&#39;, 0.5089880228042603),
 (&#39;智百扬&#39;, 0.49565914273262024),
 (&#39;投资总额&#39;, 0.4955061078071594),
 (&#39;洛辉&#39;, 0.489188551902771),
 (&#39;337409&#39;, 0.48917514085769653),
 (&#39;洛盛&#39;, 0.4819018244743347),
 (&#39;洛腾&#39;, 0.4728960692882538),
 (&#39;394150&#39;, 0.4704836308956146),
 (&#39;投资额&#39;, 0.4685181975364685)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">similar_by_key</span><span class="p">(</span><span class="s1">&#39;风险&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;提示&#39;, 0.6549968123435974),
 (&#39;经营风险&#39;, 0.6316577792167664),
 (&#39;景气衰退&#39;, 0.544153094291687),
 (&#39;风险分析&#39;, 0.5439289212226868),
 (&#39;遇宏观&#39;, 0.5435716509819031),
 (&#39;信用风险&#39;, 0.5345730185508728),
 (&#39;承受能力&#39;, 0.5291797518730164),
 (&#39;防范&#39;, 0.5271924138069153),
 (&#39;系统性&#39;, 0.5178108811378479),
 (&#39;不确定性&#39;, 0.5173759460449219)]
</code></pre></div><p>向量运行效果还行，感兴趣的同学也可以根据自己的数据训练word2vec模型，训练及使用的办法参照文章</p>
<p><a href="https://hidadeng.github.io/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>豆瓣影评 | 探索词向量妙处</title>
      <link>https://hidadeng.github.io/blog/douban_w2v/</link>
      <pubDate>Thu, 21 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/douban_w2v/</guid>
      <description>使用cntext训练、使用词向量。</description>
      <content:encoded><![CDATA[<p>本文要点</p>
<ul>
<li>读取csv</li>
<li>cntext训练词向量模型</li>
<li>cntext扩展pos、neg词典</li>
<li>导入词向量模型</li>
<li>运用词向量模型</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p>链接: <a href="https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg">https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg</a> 提取码: og9t</p>
<p><br><br></p>
<h2 id="一读取数据">一、读取数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;douban.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;电影  : </span><span class="si">{}</span><span class="s2"> 部&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Movie_Name_CN</span><span class="o">.</span><span class="n">nunique</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;评论  : </span><span class="si">{}</span><span class="s2"> 条&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)))</span>
</code></pre></div><pre><code>电影  : 28 部
评论  : 2125056 条
</code></pre>
<br>
<h2 id="二训练模型">二、训练模型</h2>
<p>使用<a href="https://hidadeng.github.io/blog/cntext_simplification/">cntext库</a>训练词向量word2vec模型,这里我把csv数据整理为txt</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext</span> <span class="kn">import</span> <span class="n">W2VModels</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#训练word2vec模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>  <span class="c1">#语料数据</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;douban.txt&#39;</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...预处理    语料 ...
Step 2/4:...训练   word2vec模型
            耗时   2001 s
        
</code></pre></div><p>cntext可以用于扩展词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;pos.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;neg.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 3/4:...准备 每个seed在word2vec模型中的相似候选词...
Step 4/4 完成! 耗时 2302 s
Step 3/4:...准备 每个seed在word2vec模型中的相似候选词...
Step 4/4 完成! 耗时 2303 s
</code></pre></div><p>在代码所在文件夹内可以找到</p>
<ul>
<li>output/w2v_candi_words/w2v.model</li>
<li>新的  pos.txt</li>
<li>新的  neg.txt</li>
</ul>
<p>新的pos.txt是对pos.txt词典的扩展。</p>
<br>
<br>
<h2 id="三导入w2v模型">三、导入w2v模型</h2>
<p>有的时候数据量特别大，模型训练十分不易。</p>
<p>这时，保存已训练好的模型，不止下次不用再同样的数据再次训练，也可分享给其他人使用。</p>
<p>训练结束后，在代码所在文件夹内可以找到 <code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span>
</code></pre></div><pre><code>&lt;gensim.models.keyedvectors.KeyedVectors at 0x7face0574880&gt;
</code></pre>
<p>w2v_models数据类型为KeyedVectors， 在本文中使用w2v_models代指KeyedVectors</p>
<br>
<h2 id="四玩转词向量">四、玩转词向量</h2>
<p>用户级的数据(如在线评论)感觉生成的向量会准一些，<strong>词向量的方向，近义反义在向量中都有体现</strong>。</p>
<p><img loading="lazy" src="man-woman.png" alt=""  />
</p>
<p>例如本文使用的是28部电影的2125056条影评， 一般评论内容包含电影相关信息，如电影题材、是否值的观影等。</p>
<p>而在我们训练出模型w2v_models存在一些常用的方法</p>
<ul>
<li><strong>w2v_model.get_vector(key)</strong> 获取key的词向量</li>
<li><strong>w2v_model.most_similar_to_given(key1, keys_list)</strong>  从 keys_list 中获取与 key1 最相似的词</li>
<li><strong>w2v_model.n_similarity(ws1, ws2)</strong> 两组词ws1, ws2 的相似度</li>
<li><strong>w2v_model.closer_than(key1, key2)</strong> 更接近于key1的词向量(相比于key2)</li>
<li><strong>w2v_model.most_similar(positive, negative)</strong> 找出与positive同方向，与negative反向相反的词。</li>
</ul>
<h3 id="41-get_vectorkey">4.1 get_vector(key)</h3>
<p>w2v_model.get_vector(key) 获取key的词向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取某词语的向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>array([ 0.06488553,  0.74188954,  0.25468495,  0.89755714,  1.8139195 ,
       -0.6950082 ,  0.24339403, -1.2188634 ,  0.543618  , -0.9988698 ,
        0.27471313,  0.9325699 , -0.5860608 , -0.5081917 ,  1.6423215 ,
       -0.0490295 , -0.3927043 ,  0.659067  ,  0.03185922, -1.021391  ,
       -1.3214804 , -0.28208104, -0.7819419 , -0.30637202, -1.5944146 ,
       -0.12383854, -0.70463836,  0.45689437,  1.223081  , -1.9453759 ,
       -0.5538997 , -0.9750523 , -0.10031194, -0.9568689 ,  0.30341247,
        1.1102395 ,  0.667315  , -1.1600997 , -0.26674765, -0.55144155,
       -0.3246094 ,  0.82902473, -0.47339582, -0.9009957 ,  1.7722464 ,
        0.28959563, -0.03453476,  0.4786787 , -0.48074463, -0.23090109,
       -0.49390873,  0.71246386,  2.1557336 ,  2.4899387 , -0.51481706,
        0.5579966 , -0.6973235 , -1.1408254 ,  0.72495663, -1.0326954 ,
       -0.5455598 ,  0.98941576, -1.2155218 , -0.9088408 ,  1.9184568 ,
       -0.21800426, -1.2009395 ,  0.29684314,  1.3672423 , -2.269391  ,
        0.6188098 , -0.02714545, -0.44811317,  1.4397241 , -1.0594722 ,
       -0.08088647, -0.13015983, -0.99255013,  0.62044877,  2.5046496 ,
        0.4054545 , -0.38767585, -0.6956541 ,  0.22991426,  0.5928579 ,
       -0.12684819, -0.17408212,  0.25033692, -1.4419957 , -0.27390227,
        1.166638  , -0.00624323, -1.6046506 ,  2.1633575 , -0.395548  ,
       -1.1297956 , -3.1474566 ,  0.38729438, -2.0434535 , -1.5511289 ],
      dtype=float32)
</code></pre>
<br>
<h3 id="42-most_similar_to_givenkey1-keys_list">4.2 most_similar_to_given(key1, keys_list)</h3>
<p>从 keys_list 中获取与 key1 最相似的词。例如在212w影评中，从<code>'爱情', '悬疑', '飞船', '历史', '战争'</code>找出最接近<code>'太空'</code>，最后返回<code>'飞船'</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#从 `keys_list` 中获取与 `key1` 最相似的 `key`。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar_to_given</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;太空&#39;</span><span class="p">,</span> 
                                <span class="n">keys_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;爱情&#39;</span><span class="p">,</span> <span class="s1">&#39;悬疑&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;历史&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>'飞船'
</code></pre>
<br> 
<h3 id="43-w2v_modeln_similarityws1-ws2">4.3 w2v_model.n_similarity(ws1, ws2)</h3>
<p>两组词ws1, ws2 的相似度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="n">cosine_similarity</span><span class="p">([</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;理想&#39;</span><span class="p">)],</span>  
                  <span class="p">[</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;现实&#39;</span><span class="p">)])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div><pre><code>0.5371934
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cosine算法</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.5371934
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#计算两组键之间的余弦相似度。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;赞&#39;</span><span class="p">,</span> <span class="s1">&#39;推荐&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;无聊&#39;</span><span class="p">,</span> <span class="s1">&#39;尴尬&#39;</span><span class="p">,</span> <span class="s1">&#39;垃圾&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.35008422
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> <span class="s1">&#39;梦想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">,</span> <span class="s1">&#39;生活&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.48020104
</code></pre>
<br>
<h3 id="44-w2v_modelcloser_thankey1-key2">4.4 w2v_model.closer_than(key1, key2)</h3>
<p>更接近于key1的词向量(相比于key2)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取所有更接近 `key1` 的键，而不是 `key2` 。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">closer_than</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> 
                      <span class="n">key2</span><span class="o">=</span><span class="s1">&#39;现实&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>['梦想', '妥协', '追梦', '愿望', '骨感']
</code></pre>
<br>
<h3 id="45-w2v_modelmost_similarpositive-negative">4.5 w2v_model.most_similar(positive, negative)</h3>
<p>找出与positive同方向，与negative反向相反的词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;过瘾&#39;</span><span class="p">],</span>
                       <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;垃圾&#39;</span><span class="p">],</span>
                       <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><pre><code>[('激动人心', 0.6859163045883179),
 ('惊心动魄', 0.6767394542694092),
 ('带感', 0.6723690032958984),
 ('惊险刺激', 0.667783796787262),
 ('刺激', 0.6445038318634033),
 ('燃', 0.6429688930511475),
 ('爽快', 0.6287934184074402),
 ('带劲', 0.6254130005836487),
 ('爽', 0.624543309211731),
 ('酣畅淋漓', 0.6140543818473816)]
</code></pre>
<br>
<h3 id="46-类比king-manwomanqueen">4.6 类比king-man+woman~queen</h3>
<p><img loading="lazy" src="kingqueenformular.png" alt=""  />
</p>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。</p>
<p>这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p>gender_direction_1 = vector(man)-vector(woman)</p>
<p>gender_direction_2 = vector(king)-vector(queen)</p>
<p>那两个性别方向应该近似，假设这里将其gender_direction_1=gender_direction_2，则对于公式中任意一个词，都可以由等式中的其他三个词经过运算得到。例如</p>
<p>vector(queen) =  vector(king)-vector(man)+vector(woman)</p>
<p>这里构造了一个情绪的公式，计算如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 开心 - 难过 ~=  享受 - d</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;开心&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;难过&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;享受&#39;</span><span class="p">)</span>

<span class="c1">#d = a-b+c</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
</code></pre></div><pre><code>[('享受', 0.7833479046821594),
 ('开心', 0.6825607419013977),
 ('愉快', 0.6298696994781494),
 ('娱乐', 0.6215130090713501),
 ('感官', 0.6085000038146973),
 ('图个', 0.6052624583244324),
 ('图一乐', 0.6039161682128906),
 ('休闲', 0.60273677110672),
 ('视觉享受', 0.6006160378456116),
 ('轻松愉快', 0.5961319804191589)]
</code></pre>
<p>很遗憾，d没有运算出煎熬之类的词语，但好在都是形容词，而且是快乐居多的形容词，类别是对的，就是方向是反的。</p>
<br>
<h3 id="词向量总结">词向量总结</h3>
<p>需要注意的是经典的运算king-man+woman~queen来自glove模型，而不是本文使用的word2vec模型。两者相同点，glove与word2vec均为词嵌入embeddings技术。区别在于glove获取的词的全局语义空间，而word2vec一般是某个词前后n个词(例如前后5个词)范围内的语义。做概念四则运算，以后如可能，建议用glove。</p>
<p>此外，即时使用glove，尽量使用概念的词组均值向量。首先要训练数据要存在这些人类认知的线索。其次，认知概念往往不是由一个词决定的，可能需要相关的很多词。例如人类社会中的<code>雄雌(没有贬义，包含了男女在内的概念)</code>，</p>
<ul>
<li>雄性概念词有<code>他、男人、男孩、父亲、爷爷、爸爸、姥爷...</code></li>
<li>雌性概念词有<code>她、女人、女孩、母亲、奶奶、妈妈、姥姥...</code></li>
<li>国王概念词有<code>查理n世、乔治、路易...</code></li>
<li>女王概念词有<code>伊丽莎白n世、维多利亚女王、叶卡捷琳娜二世...</code></li>
</ul>
<p>或许改成概念向量四则运算，公式可能更容易成立。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 |  Python文本分析包更新</title>
      <link>https://hidadeng.github.io/blog/cntext_simplification/</link>
      <pubDate>Fri, 01 Apr 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_simplification/</guid>
      <description>扩展词典、情感分析、可阅读性，内置9种情感词典，涵盖中英文</description>
      <content:encoded><![CDATA[<p><a href="https://github.com/hidadeng/cntext"><img loading="lazy" src="https://img.shields.io/badge/cntext-%e4%b8%ad%e6%96%87%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%ba%93-orange?style=for-the-badge&amp;logo=appveyor" alt=""  />
</a></p>
<p><a href="version1.2.md">旧版cntext入口</a></p>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> <strong>stats</strong>  文本统计指标
<ul>
<li><input checked="" disabled="" type="checkbox"> 词频统计</li>
<li><input checked="" disabled="" type="checkbox"> 可读性</li>
<li><input checked="" disabled="" type="checkbox"> 内置pkl词典</li>
<li><input checked="" disabled="" type="checkbox"> <strong>情感分析</strong></li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>dictionary</strong> 构建词表(典)
<ul>
<li><input checked="" disabled="" type="checkbox"> Sopmi 互信息扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> W2Vmodels 词向量扩充词典法</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>similarity</strong>   文本相似度
<ul>
<li><input checked="" disabled="" type="checkbox"> cos相似度</li>
<li><input checked="" disabled="" type="checkbox"> jaccard相似度</li>
<li><input checked="" disabled="" type="checkbox"> 编辑距离相似度</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>mind.py</strong> 计算文本中的认知方向（态度、偏见）</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext
</code></pre></div><br>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">mind</span>
    <span class="nx">dictionary</span>
    <span class="nx">similarity</span>
    <span class="nx">stats</span>
</code></pre></div><br>
<h2 id="一stats">一、stats</h2>
<p>目前stats内置的函数有</p>
<ul>
<li><strong>readability</strong>  文本可读性</li>
<li><strong>term_freq</strong> 词频统计函数</li>
<li><strong>dict_pkl_list</strong>  获取cntext内置词典列表(pkl格式)</li>
<li><strong>load_pkl_dict</strong> 导入pkl词典文件</li>
<li><strong>sentiment</strong> 情感分析</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="11--readability">1.1  readability</h3>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p>readability(text, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text: 文本字符串数据</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p>**中文可读性 ** 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>


<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 28.0,
 &#39;readability2&#39;: 0.15789473684210525,
 &#39;readability3&#39;: 14.078947368421053}
</code></pre></div><br>
<p>句子中的符号变更会影响结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.16666666666666666,
 &#39;readability3&#39;: 13.583333333333334}
</code></pre></div><p><br><br></p>
<h3 id="12--term_freq">1.2  term_freq</h3>
<p>词频统计函数，返回Counter类型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="13-dict_pkl_list">1.3 dict_pkl_list</h3>
<p>获取cntext内置词典列表(pkl格式)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取cntext内置词典列表(pkl格式)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;]
</code></pre></div><p>词典对应关系, 部分情感词典资料整理自 <a href="https://github.com/quanteda/quanteda.sentiment">quanteda.sentiment</a></p>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>DUTIR.pkl</td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td>HOWNET.pkl</td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>sentiws.pkl</td>
<td>SentimentWortschatz (SentiWS)</td>
<td>英文</td>
<td>正面词、负面词；<br>效价</td>
</tr>
<tr>
<td>ChineseFinancialFormalUnformalSentiment.pkl</td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td>ANEW.pkl</td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>词语效价信息</td>
</tr>
<tr>
<td>LSD2015.pkl</td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>NRC.pkl</td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td>geninqposneg.pkl</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HuLiu.pkl</td>
<td>Hu&amp;Liu (2004)正、负情感词典</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>AFINN.pkl</td>
<td>尼尔森 (2011) 的“新 ANEW”效价词表</td>
<td>英文</td>
<td>情感效价信息valence</td>
</tr>
<tr>
<td>LoughranMcDonald.pkl</td>
<td>会计金融LM词典</td>
<td>英文</td>
<td>金融领域正、负面情感词</td>
</tr>
<tr>
<td>ADV_CONJ.pkl</td>
<td>副词连词</td>
<td>中文</td>
<td></td>
</tr>
<tr>
<td>STOPWORDS.pkl</td>
<td></td>
<td>中、英</td>
<td>停用词</td>
</tr>
</tbody>
</table>
<h3 id="注意">注意:</h3>
<ul>
<li>
<p>如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</li>
<li>
<p>如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue</p>
</li>
</ul>
<br>
<h3 id="14-load_pkl_dict">1.4 load_pkl_dict</h3>
<p>导入pkl词典文件，返回字典样式数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入pkl词典文件,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, ...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...], 
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;, &#39;刿目怵心&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;, &#39;喜糖&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,]
           }
</code></pre></div><br>
<h3 id="15-sentiment">1.5 sentiment</h3>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)
使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>diction:  情感词字典；</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">],</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p>如果不适用pkl词典，可以自定义自己的词典，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p><br><br></p>
<h2 id="二dictionary">二、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="21-sopmi-共现法">2.1 SoPmi 共现法</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                   <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                   <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                   <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 44.49 s
</code></pre></div><br>
<h3 id="22-w2vmodels-词向量">2.2 W2VModels 词向量</h3>
<p><strong>特别要注意代码需要设定lang语言参数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   174 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s

</code></pre></div><br>
<h3 id="需要注意">需要注意</h3>
<p>训练出的w2v模型可以后续中使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">model路径</span><span class="p">)</span>
<span class="c1">#找出word的词向量</span>
<span class="c1">#w2v_model.get_vector(word)</span>
<span class="c1">#更多w2_model方法查看</span>
<span class="c1">#help(w2_model)</span>
</code></pre></div><p>例如本代码，运行生成的结果路径<code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;technology&#39;, 0.689210832118988),
 (&#39;infrastructure&#39;, 0.669672966003418),
 (&#39;resources&#39;, 0.6695448160171509),
 (&#39;talent&#39;, 0.6627111434936523),
 (&#39;execution&#39;, 0.6549549102783203),
 (&#39;marketing&#39;, 0.6533523797988892),
 (&#39;merchandising&#39;, 0.6504817008972168),
 (&#39;diversification&#39;, 0.6479553580284119),
 (&#39;expertise&#39;, 0.6446896195411682),
 (&#39;digital&#39;, 0.6326863765716553)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.45616838, -0.7799563 ,  0.56367606, -0.8570078 ,  0.600359  ,
       -0.6588043 ,  0.31116748, -0.11956959, -0.47599426,  0.21840936,
       -0.02268819,  0.1832016 ,  0.24452794,  0.01084935, -1.4213187 ,
        0.22840202,  0.46387577,  1.198386  , -0.621511  , -0.51598716,
        0.13352732,  0.04140598, -0.23470387,  0.6402956 ,  0.20394802,
        0.10799981,  0.24908689, -1.0117126 , -2.3168423 , -0.0402851 ,
        1.6886286 ,  0.5357047 ,  0.22932841, -0.6094084 ,  0.4515793 ,
       -0.5900931 ,  1.8684244 , -0.21056202,  0.29313338, -0.221067  ,
       -0.9535679 ,  0.07325   , -0.15823542,  1.1477109 ,  0.6716076 ,
       -1.0096023 ,  0.10605699,  1.4148282 ,  0.24576302,  0.5740349 ,
        0.19984631,  0.53964925,  0.41962907,  0.41497853, -1.0322098 ,
        0.01090925,  0.54345983,  0.806317  ,  0.31737605, -0.7965337 ,
        0.9282971 , -0.8775608 , -0.26852605, -0.06743863,  0.42815775,
       -0.11774074, -0.17956367,  0.88813037, -0.46279573, -1.0841943 ,
       -0.06798118,  0.4493006 ,  0.71962464, -0.02876493,  1.0282255 ,
       -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922,
       -0.16753489,  0.14065655, -1.8609751 ,  0.03587054,  1.2792674 ,
        1.2732009 , -0.74120265, -0.98000383,  0.4521185 , -0.26387128,
        0.37045383,  0.3680011 ,  0.7197629 , -0.3570571 ,  0.8016917 ,
        0.39243212, -0.5027844 , -1.2106236 ,  0.6412354 , -0.878307  ],
      dtype=float32)
</code></pre></div><p><br><br></p>
<h2 id="23-co_occurrence_matrix">2.3 co_occurrence_matrix</h2>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三similarity">三、similarity</h2>
<p>四种相似度计算函数</p>
<ul>
<li>cosine_sim(text1, text2)  cos余弦相似</li>
<li>jaccard_sim(text1, text2)     jaccard相似</li>
<li>minedit_sim(text1, text2)  最小编辑距离相似度；</li>
<li>simple_sim(text1, text2) 更改变动算法</li>
</ul>
<p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩啊&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.82
0.67
2.00
0.87
</code></pre></div><p><br><br></p>
<h2 id="四text2mind">四、Text2Mind</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<p>- <strong>多个对象在某概念的远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
<p>- 多个对象在某<strong>概念的分量(fen，一声)的多少</strong>， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
<p>这两种认知分别可以用向量距离、向量语义投影计算得来。</p>
<ul>
<li>tm.sematic_distance(words, c_words1, c_words2)  向量距离</li>
<li>tm.sematic_projection(words, c_words1, c_words2)  向量语义投影</li>
</ul>
<h3 id="41-tmsematic_distancewords-c_words1-c_words2">4.1 tm.sematic_distance(words, c_words1, c_words2)</h3>
<p>分别计算words与c_words1、c_words2语义距离，返回距离差值。</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果d1-d2&lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<p><strong>下载glove_w2v.6B.100d.txt</strong>链接: <a href="https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw">https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw</a> 提取码: 72l0</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#Note: this is a word2vec format model</span>
<span class="n">tm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Text2Mind</span><span class="p">(</span><span class="n">w2v_model_path</span><span class="o">=</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="n">engineer</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">mans</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">womans</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">mans</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">womans</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><br>
<h3 id="42-tmsematic_projectionwords-c_words1-c_words2">4.2 tm.sematic_projection(words, c_words1, c_words2)</h3>
<p><strong>语义投影</strong>，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，word越位于c_words2一侧。</p>
<p>下图是语义投影示例图，本文算法和图片均来自 &ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&rdquo;</p>
<p><img loading="lazy" src="img/Nature_Semantic_projection_recovering_human_knowledge_of.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">smalls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">bigs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">smalls</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">bigs</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Top2Vec|主题建模和语义搜索库</title>
      <link>https://hidadeng.github.io/blog/top2vec_tutorial/</link>
      <pubDate>Mon, 13 Dec 2021 10:43:10 +0600</pubDate>
      
      <guid>/blog/top2vec_tutorial/</guid>
      <description>Python主题建模和语义搜索库</description>
      <content:encoded><![CDATA[<p>Top2Vec 是一种用于主题建模和语义搜索的算法。**我个人从理解代码和使用代码难度来看， 对于Python小白，BERTopic更适合直接用预训练词向量，而Top2Vec更适合对小规模数据训练词向量后做主题建模。**它自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。训练 Top2Vec 模型后，您可以：</p>
<ul>
<li>获取检测到的主题数。</li>
<li>获取话题。</li>
<li>获取主题大小。</li>
<li>获取分层主题。</li>
<li>按关键字搜索主题。</li>
<li>按主题搜索文档。</li>
<li>按关键字搜索文档。</li>
<li>找出相似的词。</li>
<li>查找类似的文档。</li>
<li>使用 RESTful-Top2Vec 公开模型</li>
<li>有关其工作原理的更多详细信息，请参阅论文。</li>
</ul>
<p><strong>亮点</strong></p>
<ul>
<li>自动查找主题数。</li>
<li>不需要停用词列表。</li>
<li>不需要词干/词形还原。</li>
<li>适用于短文本。</li>
<li>创建联合嵌入的主题、文档和词向量。</li>
<li>内置搜索功能。</li>
</ul>
<p><strong>它是如何工作的？</strong></p>
<p>该算法做出的假设是，许多语义相似的文档都表明了一个潜在的主题。</p>
<p>第一步是创建文档和词向量的联合嵌入。一旦文档和单词被嵌入到一个向量空间中，算法的目标就是找到密集的文档集群，然后确定哪些单词将这些文档吸引到一起。每个密集区域是一个主题，将文档吸引到密集区域的词就是主题词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">top2vec</span><span class="o">==</span><span class="mf">1.0.27</span>
</code></pre></div><h2 id="0-代码下载">0. 代码下载</h2>
<p><a href="top2vec_tutorial.zip">click to download code</a></p>
<p><br><br></p>
<h2 id="1-导入数据">1. 导入数据</h2>
<p>使用某灾难数据集，这里是存在标注的标签，但是我们假设不用label的，仅作为评判Top2vec运行效果的标准。<a href="cnews.csv">点击cnews.csv下载</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">top2vec</span> <span class="kn">import</span> <span class="n">Top2Vec</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;STOPWORDS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;STOPWORDS&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;cnews.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>时政    120
科技    106
时尚    106
财经    105
家居    103
教育     97
娱乐     96
体育     95
房产     87
游戏     85
Name: label, dtype: int64
</code></pre>
<p><br><br></p>
<h2 id="2-清洗数据">2. 清洗数据</h2>
<p>一般而言，作中文文本分析，需要把中文分词构造成类西方语言(空格间隔词语的文本)风格。在此期间，顺便将停用词剔除。其实在用top2vec时，不剔除停用词影响也不大。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleantext&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="3-训练模型">3. 训练模型</h2>
<p>Top2vec有一下四个常用参数</p>
<p><strong>Top2vec(documents, min_count, speed, workers)</strong></p>
<ul>
<li>documents: 文档列表</li>
<li>min_count: 词语最少出现次数。低于min_count的词不加入模型中</li>
<li>speed: 训练速度，参数默认&quot;learn&quot;
<ul>
<li>&ldquo;fast-learn&rdquo;  速度最快，训练效果最差</li>
<li>&ldquo;learn&rdquo;       速度，训练效果中等</li>
<li>&ldquo;deep-learn&rdquo;  速度最慢，训练效果最佳</li>
</ul>
</li>
<li>workers: 并行运行数，该值最大取值为电脑CPU的核数。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="n">Top2Vec</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleantext&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">(),</span> 
                <span class="n">min_count</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">speed</span><span class="o">=</span><span class="s2">&#34;deep-learn&#34;</span><span class="p">,</span>  
                <span class="n">workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>2021-12-14 20:21:10,318 - top2vec - INFO - Pre-processing documents for training
2021-12-14 20:21:10,871 - top2vec - INFO - Creating joint document/word embedding
2021-12-14 20:25:06,082 - top2vec - INFO - Creating lower dimension embedding of documents
2021-12-14 20:25:14,645 - top2vec - INFO - Finding dense areas of documents
2021-12-14 20:25:14,683 - top2vec - INFO - Finding topics
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 话题个数</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_num_topics</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>9
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 各话题数量</span>
<span class="n">topic_sizes</span><span class="p">,</span> <span class="n">topic_nums</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_sizes</span><span class="p">()</span>

<span class="p">{</span><span class="s2">&#34;topic_sizes&#34;</span><span class="p">:</span><span class="n">topic_sizes</span><span class="p">,</span> 
 <span class="s2">&#34;topic_ids&#34;</span><span class="p">:</span><span class="n">topic_nums</span><span class="p">}</span>
</code></pre></div><p>Run</p>
<pre><code>{'topic_sizes': array([361, 116, 107,  99,  97,  93,  82,  25,  20]),
 'topic_ids': array([0, 1, 2, 3, 4, 5, 6, 7, 8])}
</code></pre>
<p><br><br></p>
<h2 id="4-get_topics">4. get_topics</h2>
<p>用pyecharts词云图显示<strong>话题信息</strong>， 为了简化代码，将该功能封装为函数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gen_wordcloud</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    topic_words: 主题词列表
</span><span class="s2">    word_scores: 主题特征词的权重得分(词语表征主题的能力)
</span><span class="s2">    topic_id: 主题id
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
    <span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
    <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
    
    <span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">)]</span>

    <span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">88</span><span class="p">])</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
        <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;Topic_</span><span class="si">{topic_id}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">),</span> 
                                  <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">23</span><span class="p">)),</span>
        <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="n">display</span><span class="p">(</span><span class="n">wc</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">())</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topics</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>

<span class="k">for</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">):</span>
    <span class="n">gen_wordcloud</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/vis1.png" alt=""  />

<img loading="lazy" src="img/vis2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="5-get_documents_topics">5. get_documents_topics</h2>
<p>get_documents_topics(doc_ids, num_topics=1)</p>
<ul>
<li>doc_ids: 待查询文档id列表</li>
<li>num_topics: 返回某文档可能归属话题的个数</li>
</ul>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查第一条文档的</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_documents_topics</span><span class="p">(</span><span class="n">doc_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(array([0]),
 array([0.1384481], dtype=float32),
 array([[&#39;政府&#39;, &#39;经济&#39;, &#39;政策&#39;, &#39;建设&#39;, &#39;中方&#39;, &#39;发展&#39;, &#39;促进&#39;, &#39;部门&#39;, &#39;留学&#39;, &#39;学生&#39;, &#39;会议&#39;,
         &#39;我要&#39;, &#39;事务&#39;, &#39;日电&#39;, &#39;房价&#39;, &#39;教育&#39;, &#39;国务院&#39;, &#39;温家宝&#39;, &#39;留学生&#39;, &#39;人数&#39;, &#39;移民&#39;,
         &#39;会见&#39;, &#39;推动&#39;, &#39;申请者&#39;, &#39;申请&#39;, &#39;官员&#39;, &#39;住房&#39;, &#39;房屋&#39;, &#39;加强&#39;, &#39;中国政府&#39;, &#39;购房&#39;,
         &#39;国家&#39;, &#39;支付&#39;, &#39;楼市&#39;, &#39;外交部&#39;, &#39;接收&#39;, &#39;两国&#39;, &#39;原则&#39;, &#39;各地&#39;, &#39;总理&#39;, &#39;战略&#39;,
         &#39;和平&#39;, &#39;框架&#39;, &#39;评论&#39;, &#39;有序&#39;, &#39;装修&#39;, &#39;中国&#39;, &#39;就业&#39;, &#39;友好&#39;, &#39;人力资源&#39;]],
       dtype=&#39;&lt;U9&#39;),
 array([[0.3623712 , 0.36037514, 0.35219163, 0.35109183, 0.3499857 ,
         0.34666985, 0.3426961 , 0.34161803, 0.34010434, 0.3382269 ,
         0.33710504, 0.336056  , 0.33598724, 0.33488944, 0.3303768 ,
         0.32483265, 0.324798  , 0.32201332, 0.3174801 , 0.3153757 ,
         0.3152491 , 0.31338856, 0.31334093, 0.31244045, 0.31202242,
         0.30908576, 0.3086405 , 0.30838227, 0.30605763, 0.3053521 ,
         0.30474398, 0.30268514, 0.30253592, 0.30242488, 0.30227807,
         0.3017046 , 0.30116442, 0.30062813, 0.2996228 , 0.29806197,
         0.2972776 , 0.29709277, 0.29706252, 0.29584888, 0.29578486,
         0.29524648, 0.2944737 , 0.2939484 , 0.29286712, 0.29246706]],
       dtype=float32))
</code></pre></div><p><br><br></p>
<h2 id="6-search_topics">6. search_topics</h2>
<p>根据关键词搜索话题，查某词是否属于某话题，属于该主题的概率
search_topics(keywords, num_topics, keywords_neg=None)</p>
<ul>
<li>keywords: 关键词列表</li>
<li>num_topics: 返回话题个数，按照语义相似度从高到低排序</li>
<li>keywords_neg: 反义词列表</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gen_wordcloud2</span><span class="p">(</span><span class="n">query_word</span><span class="p">,</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">,</span> <span class="n">topic_probability</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    query_word: 待查询词
</span><span class="s2">    topic_words: 主题词列表
</span><span class="s2">    word_scores: 主题特征词的权重得分(词语表征主题的能力)
</span><span class="s2">    topic_id: 主题id
</span><span class="s2">    topic_probability: 主题概率
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
    <span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
    <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
    
    <span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">)]</span>

    <span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">88</span><span class="p">])</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;Word</span><span class="si">{query_word}</span><span class="se">\n</span><span class="s2">Topic_</span><span class="si">{topic_id}</span><span class="se">\n</span><span class="s2">Probability:</span><span class="si">{probability:.2f}</span><span class="s2">&#34;&#34;&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">query_word</span><span class="o">=</span><span class="n">query_word</span><span class="p">,</span>
                                                              <span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> 
                                                              <span class="n">probability</span><span class="o">=</span><span class="n">topic_probability</span><span class="p">)</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
        <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> 
                                  <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">18</span><span class="p">)),</span>
        <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="n">display</span><span class="p">(</span><span class="n">wc</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">())</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">query_word</span> <span class="o">=</span> <span class="s2">&#34;电影&#34;</span>
<span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_topics</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="n">query_word</span><span class="p">],</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_score</span><span class="p">,</span> <span class="n">topic_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">topic_score</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">:</span>
        <span class="n">gen_wordcloud2</span><span class="p">(</span><span class="n">query_word</span><span class="o">=</span><span class="n">query_word</span><span class="p">,</span> 
                       <span class="n">topic_words</span><span class="o">=</span><span class="n">topic_words</span><span class="p">,</span> 
                       <span class="n">word_scores</span><span class="o">=</span><span class="n">word_scores</span><span class="p">,</span> 
                       <span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> <span class="n">topic_probability</span><span class="o">=</span><span class="n">topic_score</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/vis5.png" alt=""  />
</p>
<br>
<h2 id="7-query_topics">7. query_topics</h2>
<p>根据一段文本寻找最符合该文本的话题
query_topics(query, num_topics)</p>
<ul>
<li>query: 查询文本，注意是用空格间隔词语的文本</li>
<li>num_topics: 返回的话题数</li>
</ul>
<p>返回话题特征词列表， 话题特征词权重， 话题概率， 话题id</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">querytext</span> <span class="o">=</span> <span class="s1">&#39;刘晓庆 55 岁 近日 颁奖礼 刘晓庆 一袭 宝蓝色 超低 胸 V 领 长裙 亮相 轻薄 蕾丝 奢华 皮草 艳丽 色彩 翠绿&#39;</span>
<span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">query_topics</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">querytext</span><span class="p">,</span> 
                                                                       <span class="n">num_topics</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;可能归属的话题有: &#39;</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;归属于该话题的概率&#39;</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">可能归属的话题有:  [1 4]
归属于该话题的概率 [0.32036728 0.1276904 ]
</code></pre></div><br>
<h2 id="8-search_documents_by_keywords">8. search_documents_by_keywords</h2>
<p>根据关键词，筛选文档</p>
<p>search_documents_by_keywords(keywords,
num_docs,
keywords_neg=None,
return_documents=True)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#文档， 语义相关性， 文档id</span>
<span class="n">docs</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">doc_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_keywords</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;搭配&#39;</span><span class="p">],</span> 
                                                         <span class="n">num_docs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                                                         <span class="n">keywords_neg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                                                         <span class="n">return_documents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">doc_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;----------&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Document: 106, Semantic similarity: 0.4943176805973053
白色 短裙 百变 休闲 感 要点 一定 敞开 衬衫 配合 牛仔裤 休闲 感 短裤 衬衫 短 敞开 显得 好好 穿 裤子 搭配 七分裤 遮住 臀部 长度 关键 尽量 选择 艳丽 颜色 带 出 青春 感 NO.3     白色 短裙 tips :   白色 短裙 + 粉色 上衣 这是 一套 减龄 百分百 搭配 白色 短裙 本来 清纯 粉色 上衣 搭配 更加 具有活力 tips :   白色 短裙 + 抹胸 + 外套 想要 性感 一点 就加 一件 抹胸 抹胸 胸前 构造 曲线 完美 再加 外套 保暖 得体 看似 简单 一款 搭配 其实 暗地里 偷偷地 修饰 身材
----------

Document: 870, Semantic similarity: 0.4483542740345001
组图 看达人 演绎 豹纹 军装 风 导语 懂得 潮流 总是 知道 适合 今冬 流行 亮点 太 军装 豹纹 类似 民族风情 想要 知道 搭配 快 看看 时尚 达 穿 军绿色 宽松 款 大衣 不失 俏皮 味道 高腰 设计 短裙 有效 提升 腰线 衬托出 修长 美腿 豹纹 今年 冬季 抢眼 搭配 元素 加上 驼色 针织衫 灰色 围巾 暖 棕色 手 挎包 整体 色调 统一 迷人 棕色 蓝色 结合能 眼前一亮 简洁 款式 依然 突显 独特 品味 宽松 针织 外套 衬托出 优美 身形 搭配 同样 沉闷 黑色 包包 性感 丝袜 装扮 依然 透露 出 迷人 气息 立领 衬衫 加上 深黄 高腰 裤 摩登 感 十足 随意 披上 外套 更显 慵懒 个性 法式 风情
----------

Document: 450, Semantic similarity: 0.4471719563007355
街 拍 爱 招摇过市 毛茸茸 ( 组图 ) 导语 皮草 每个 冬天 可能 丢弃 每个 需要 温暖 早些 相比 人造皮 草比 真皮 草 风头 更劲 时尚 环保 大牌 秀 场上 超模 一个个 穿着 人造皮 草 “ 招摇过市 ” 之后 街头 潮人 没有 理由 拒绝 外形 酷酷 这件 气场 皮草 单品 配合默契 摇滚 风 配饰 搭配 黑色 皮草 长 背心 更显 利落 酷酷 黑色 皮草 搭配 蓝色 衬衣 不同 感觉 加上 下半身 底裤 时髦 包包 颜色 提亮 整身 装扮 抹胸 式 皮草 特点 高贵典雅 适合 搭配 连衣裙 装饰 增添 时尚 美感 复古 圆点 连衣裙 搭配 宽松 棕色 皮草 衣 名媛 感觉 典雅 淑女 短款 黑色 皮草 搭配 贴身 仔裤 搭配 长靴 潇洒 帅气 茸茸 帽子 增添 不少 甜美 感
----------
</code></pre></div><p><br><br></p>
<h2 id="9-search_documents_by_topic">9. search_documents_by_topic</h2>
<p>根据指定的topic_id， 显示该主题前num_docs个文档，显示的文档是根据概率从高到低降序显示</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看topic4的前5条文档</span>
<span class="n">topic_id</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_topic</span><span class="p">(</span><span class="n">topic_num</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> <span class="n">num_docs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Document: 905, Semantic similarity: 0.4941929578781128
-----------
现场 阿联 第三节 未 亮相   奇才 连续 3 记 重扣 逆转 比分 新浪 体育讯 北京 时间 4 2 奇才 主场 迎战 联盟 垫底 骑士 奇才 新秀 后卫 约翰 - 沃尔因 一场 对阵 热火 比赛 斗殴 禁赛 一场 伤愈 复出 安德雷 - 布 莱切 回到 首发 阵容 奇才 本赛季 首场 客场 胜利 面对 骑士 取得 当时 奇才 115 - 110 击败 对手 上半场 骑士 命中率 达到 53.8% 奇才 仅 44% 骑士 希克 森 ( 16 ) 塞 申斯 ( 12 ) 得分 双 奇才 布 莱切 ( 22 ) 麦基 ( 12 ) 埃文斯 4 投 0 仅 抢下 1 篮板 易建联 上场 7 08 2 投 0 抢下 3 篮板 异地 再战 埃文斯 终于 得分 抢断 吉后 犯规 两罚 命中 吉 随即 突破 上篮 命中 回敬 球 杰 弗斯 三分 不进 吉 抢下 篮板 上篮 再进 一球 布 莱切 中投 命中 霍林斯 篮下 出手 不进 布 莱切 抢下 篮板 此后 克劳福德 连续 突破 先是 助攻 麦基 扣篮 盖掉 戴维斯 投篮 助攻 布 莱切 扣篮 戴维斯 运球 被断 布 莱切 传给 杰 弗斯 一记 暴扣 奇才 连续 3 次 颇具 气势 扣篮 连得 6 反超 骑士 1 骑士 请求 暂停 回到 比赛 吉 上篮 不进 麦基 低位 单打 不进 布 莱切 抢下 篮板 3 得手 骑士 进攻 24 违例 奇才 越战越勇 克劳福德 身体 失去 重心 情况 仍然 将球 投进 一个打 3 骑士 连续 吉 挺身而出 三分 命中 个人 已经 得到 10 此人 本赛季 短暂 效力 奇才 麦基 中投 不进 布 莱切 抢下 前场 篮板 将球 放进 麦基 防守 领到 犯规 希克 森两罚 命中 麦基 强攻 造成 霍林斯 犯规 两罚 一中 戴维斯 三分 不进 克劳福德 跑 投 命中 戴维斯 突分 霍林斯 暴扣 命中 回过头来 克劳福德 助攻 麦基扣 劲 爆 哈兰 高迪 中投 不进 克劳福德 投篮 偏出 布 莱切 3 报价 连续 抢 篮板 进攻 最后 犯规 两罚 一中 现在 已经 得到 32 18 篮板 布 莱切 底线 遭 报价 分球 埃文斯 三分 命中 霍林斯 篮下 重扣 奇才 请求 暂停 布 莱切 继续 得分 吉布森 上篮 命中 克劳福德 中投 不进 抢下 篮板 杰 弗斯 运球 突破 犯规 两罚 命中 易建联 节 没有 登场 第三节 比赛 结束 骑士 82 - 83 奇才 ( 草头 王 )
-----------

Document: 689, Semantic similarity: 0.4917592704296112
-----------
直击 康大 内线 一柱擎天   13 优势 到手 胜利在望 新浪 体育讯 北京 时间 4 5 ( 休斯敦 时间 4 4 ) 消息 NCAA   Final   4 总决赛 休斯敦 Reliant 球馆 举行 比赛 进入 最后 6 分钟 本场 表现 十分 亮眼 康涅狄格 内线 阿莱克斯 - 奥里 瓦基接 队友 直传 空切 篮下 扣篮 得分 打成 2 + 1 目前 已经 拿下 10 9 篮板 3 封盖 巴特勒 仍然 没 解决 进攻 端的 问题 下半场 23 投 仅仅 3 屡次 外线 空挡 出手 均 打铁 告终 仅仅 入账 8 目前 康大 已经 取得 13 优势 胜利在望 ( silencer )
-----------

Document: 425, Semantic similarity: 0.47035443782806396
-----------
今日 数据 趣谈 魔兽 悲情 似 张大帅   基德 焕发 第二 春 新浪 体育讯 北京 时间 4 17 NBA 季后赛 正式 开打 进行 4 场 比赛 以下 今日 比赛 诞生 有趣 数据 今日 首场 季后赛 芝加哥 公牛 第四节 剩 4 分钟 仍以 88 - 98 落后 接下来 打出 16 - 1 攻击 波 主场 一举 逆转 印第安纳 步行者 取胜 继 2004 之后 NBA 季后赛 舞台 再次出现 终场 前 4 分钟 落后 两位数 最终 翻盘 成功 案例 2004 5 9 西部 决赛 明尼苏达 森林狼 萨克拉门托 国王 比赛 森林狼 同样 终场 前 4 分钟 仍以 78 - 88 落后 接下来 打出 16 - 1 ( 惊人 相似 ) 最终 94 - 89 逆转 取胜 今天 公牛 逆转 步行者 比赛 德里克 - 罗斯 砍 39 罚球 21 投 19 2008 洛杉矶 湖人 对阵 犹他 爵士 一场 季后赛 科比 - 布莱恩特 创下 单场 罚球 23 投 21 季后赛 纪录 罗斯 位居 全场 三分球 9 次 出手 竟无一 命中 季后赛 历史 此前 两次 类似 案例 2008 奥兰多 魔术 对阵 多伦多 猛龙 一场 比赛 拉沙德 - 刘易斯 三分球 9 投 0 一次 熟知 1994 总决赛 第七场 约翰 - 斯塔克 斯 三分 线外 11 投 0 纽约 尼克斯 负于 休斯敦 火箭 冠军 擦肩而过 今天 亚特兰大 老鹰 客场 战胜 奥兰多 魔术 比赛 老鹰 五名 球员 得分 低于 13 — — 乔 - 约翰逊 ( 25 16 投 9 ) 贾马尔 - 克劳福德 ( 23 14 投 7 ) 艾尔 - 霍福德 ( 16 14 投 7 ) 约什 - 史密斯 ( 15 12 投 6 ) 科克 - 辛里奇 ( 13 10 投 6 ) 该队 过去 199 场 季后赛 尚属 首次 老鹰队 史上 一次 出现 这种 盛况 1966 4 14 131 - 127 战胜 洛杉矶 湖人 比赛 当时 书写 纪录 五人 里奇 - 古尔林 克里夫 - 哈根 泽尔莫 - 比蒂 比尔 - 布里奇斯 乔 - 考 德维尔 今天 负于 老鹰 比赛 德怀特 - 霍华德 ( 46 ) 贾 米尔 - 尼尔森 ( 27 ) 砍 73 队友 总共 仅 拿下 20 魔术 最终 93 - 103 负于 更为 均衡 对手 NBA 历史 8 支 球队 一场 季后赛 比赛 有过 两名 球员 联手 砍 全队 至少 75% 得分 1 队 取胜 追溯到 1950 4 9 当年 总决赛 第一场 比赛 乔治 - 麦肯 得到 37 吉姆 - 波 拉德 得到 14 率领 明尼阿波利斯 湖人 68 - 66 战胜 锡 拉丘兹 民族 ( 费城 76 前身 ) 7 队则 败北 得到 46 霍华德 抢下 19 篮板 常规 时间 取得 1975 4 19 布法罗 勇敢者 ( 洛杉矶 快船 前身 ) 战胜 华盛顿 子弹 ( 华盛顿 奇才 前身 ) 一场 季后赛 效力 勇敢者 鲍勃 - 麦卡 杜 同样 没有 加时赛 情况 砍 50 21 篮板 威尔特 - 张伯伦 一场 季后赛 常规 时间 砍 46 19 篮板 球队 却输 ( 事实上 张大帅 生涯 3 场 比赛 取得 数据 竟 败北 ) 刚 谢幕 本赛季 常规赛 杰森 - 基德 仅 两场 比赛 得分 达到 20 + 1 20 对阵 湖人 比赛 砍 赛季 最高 21 今天 达拉斯 小牛 主场 战胜 波特兰 开拓者 比赛 砍 24 命中 6 记 三分球 一场 季后赛 比赛 砍 20 + 得分 刷新 常规赛 创下 赛季 新高 NBA 历史 壮举 球员 如今 38 岁 基德 年龄 最大 成为 NBA 历史 一场 季后赛 比赛 单场 命中 6 记 三分球 年龄 最大 球员 此前 纪录 雷吉 - 米勒 2002 创下 当时 36 岁 今天 小牛 战胜 开拓者 比赛 德克 - 诺维茨基 第四节 13 次 罚球 出手 命中 追平 迈克尔 - 乔丹 纪录 1990 - 91 赛季 季后赛 一场 公牛 底特律 活塞 比赛 乔丹 单节 命中 13 次 罚球 率队 105 - 97 取胜 最终 公牛 获得 赛季 总冠军 今天 迈阿密 热火 战胜 费城 76 比赛 克里斯 - 波什 得到 25 12 篮板 勒布朗 - 詹姆斯 得到 21 14 篮板 他俩 队友 参加 首场 季后赛 前 一个 赛季 各为其主 接下来 赛季 并肩作战 季后赛 首场 比赛 砍 得分 20 + 篮板 10 + 组合 波什 詹姆斯 之前 无先例 ( 魑魅 )
-----------

Document: 155, Semantic similarity: 0.45704954862594604
-----------
现场 麦蒂 返场 销魂 跳投 两 连击   小拜 纳姆 单节 11 新浪 体育讯 北京 时间 4 6 华盛顿 奇才 主场 迎战 底特律 活塞 此前 球队 已经 客场 两连胜 若能 战胜 活塞 奇才 本赛季 首次 迎来 三连胜 异地 再战 埃文斯 中投 命中率 先 得分 拜纳姆 中投 不进 克劳福德 一人 带球 运 前场 对手 尚未 落位 情况 直接 出手 投篮 命中 这种 投篮 欠缺 考虑 根本 没有 战术 配合 全 个人 手感 遇到 防守 稍 一点 球队 沃尔 抢断 埃文斯 直接 暴扣 奇才 反超 4 活塞 请求 暂停 沃尔 报价 对手 拜纳姆 得到 机会 三分 出手 命中 布 莱切 上篮 得手 门罗 助攻 威尔 考克斯 扣篮 命中 埃文斯 三分 不进 拜纳姆 突破 上篮 命中 威尔 考克斯 拿布 莱切 没有 办法 运球 进攻 威尔 考克斯 只能 伸直 手臂 不断 滑步 被布 莱切 强投 命中 活塞 拜纳姆 发力 突破 上篮 命中 布 莱切 中投 不进 拜纳姆 卷土重来 造成 沃尔 犯规 两罚 命中 个人 已经 得到 11 门罗 抢断 布 莱切 普林斯 上篮 命中 活塞 反超 3 麦基 传球 失误 奇才 请求 暂停 威尔 考克斯 篮下 强打 奇才 反击 埃文斯 上篮 命中 普林斯 糟糕 状态 继续 中投 偏出 布 莱切 运球 单打 活塞 两名 内线 屡试不爽 造成 门罗 犯规 两罚 命中 汉密尔顿 中投 不进 威尔 考克斯 抢下 前场 篮板 直接 扣篮 命中 布 莱切 继续 发威 转身 摆脱 上篮 命中 拜纳姆 三分 偏出 球 砸 远 活塞 球员 退守 不及 克劳福德 轻松 上篮 命中 沃尔 中投 不进 拜纳姆 反击 遭 侵犯 两罚 命中 个人 单节 已经 得到 11 布 莱切 对手 包夹 中投 偏出 普林斯 跑 投 命中 活塞 反超 一分 克劳福德 中投 打铁 拜纳姆 没能 命中 三分 麦蒂 回到 赛场 塞拉芬 进攻 犯规 普林斯 中投 不进 门罗 补篮 命中 麦蒂断 球 直接 中投 命中 布 莱切 走步 麦蒂 假动作 点飞 克劳福德 投篮 再进 第三节 比赛 结束 活塞 81 - 78 奇才 ( 草头 王 )
-----------

Document: 254, Semantic similarity: 0.45255911350250244
-----------
奇才 vs 步行者 前瞻 走出 客场 阴影   斗狠 东部 老八 新浪 体育讯 北京 时间 4 7 奇才队 客场 挑战 东区 第八 步行者 目前 奇才 客场 战绩 3 胜 35 负 最近 客场 两连胜 奇才队 背靠背 作战 今天 主场 107 - 105 险胜 活塞 球队 一举 拿到 赛季 最长 三连胜 实际上 这是 奇才队 2007 - 08 赛季 以后 球队 第一个 赛季 三连胜 这场 比赛 奇才 惊人 获得 35 次 罚球 沃尔一人 包办 16 次 全场 得到 26 12 次 助攻 6 篮板 4 次 抢断 布 莱切 无疑 三连胜 第一 功臣 连胜 期间 场均 得到 29 15.3 篮板 克劳福德 同样 火爆 异常 一位 前锋 首发 埃文斯 表现 低估 活塞 比赛 埃文斯 13 投 9 中射下 20 沃尔 拿到 职业生涯 首个 三连胜 “ 联盟 留下 标签 一名 菜鸟 证明 一部分 很多 想 站 球场上 一分钟 全力以赴 ” 奇才 三连胜 对手 名副其实 鱼腩 球队 无论如何 三连胜 这支 弱旅 一个 不小 激励 尤其 伤病 满营 情况 目前 球队 6 可能 赛季 结束 前 无法 归队 包括 得分王 尼克 - 杨 约什 - 霍华德 拉沙德 - 刘易斯 布克 恩戴 耶 卡 蒂尔 - 马丁 步行者 35 胜 43 负 暂居 东部 第八 目前 东部 前七 已经 锁定 季后赛 剩下 第八名 悬念 步行者 领先 第九位 山猫 2.5 个胜场 领先 10 位 雄鹿 3.5 个胜场 剩下 4 场 比赛 情况 悬念 并不大 明天 山猫 雄鹿 迎战 强敌 ( 魔术 热火 ) 步行者 机会 扩大 领先 场次 优势 球队 头号 得分手 格兰杰 状态 过去 5 场 比赛 得分 20 以下 最近 三场 三分球 12 投 3 汉斯 布鲁在 过去 6 场 比赛 陷入 挣扎 场均 9.3 5.7 篮板 ( 之前 11 场 比赛 贡献 20.6 7.8 篮板 ) 一场 比赛 步行者 12 输给 黄蜂队 主教练 沃格尔 称之为 “ 惨痛 失败 ” 本赛季 两队 战成 2 - 1 步行者 赢下 最近 两次 交锋 两场 比赛 奇才 命中率 均 低于 40% 总 失误 高达 41 次 预计 两队 首发 奇才 沃尔 克劳福德 埃文斯 布 莱切 麦基 步行者 科 里森 格兰杰 乔治 汉斯 布鲁 希伯特 ( 木瓜 丁 )
-----------
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_keywords</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;搭配&#34;</span><span class="p">,</span> <span class="s2">&#34;高跟鞋&#34;</span><span class="p">],</span> <span class="n">num_docs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><pre><code>Document: 727, Semantic similarity: 0.5883481502532959
-----------
组图 冷气 办公室   连衣裙 配小 坎肩 美国 设计师 Diane   Von   Furstenberg 曾经 感觉 女人 穿 连衣裙 女人 找到 一件 适合 dream   dress 重要 无需 费神 搭配 单穿 连身 优雅 飞扬 裙摆 似乎 告诉 女 连衣裙 玩起 High   Fashion 变脸 游戏 DKNY 绿色 连衣裙   新品 未 定价 H &amp; M 黑色 外套   新品 未 定价 Agatha 配件 新品 未 定价 C . Banner 高跟鞋   新品 未 定价 低 V 领 连衣裙 秀出 属于 性感 更好 展现出 颈部 线条 搭配 修身 剪裁 西装 短款 皮手套 极具 欧美 明星 范儿 细 高跟鞋 更好 突出 双腿 长度 整体 显得 轻盈 不少 On &amp; on 米色 连衣裙   新品 未 定价 Asobio 针织 外套   RMB   449 Kookai 金色 腰带 Jc  
-----------

Document: 435, Semantic similarity: 0.5440454483032227
-----------
组图 秋冬 优雅 妖娆   女星 爱 裸 色系 导语 裸色 优雅 代名词 女星 近来 誓 裸色 进行 到底 无论是 徐若 ? 性感 乐基儿 气质 搭配 各色 礼服 赏心悦目 娇俏 款式 更是 大饱眼福 徐若 ? 飘逸 丝带    立刻 彰显 天王 嫂 贵妇 气质 袁咏仪 翻领 西装   气质 非凡 裸色 短款 紧身 西装 皮质 面料 彰显 个性 夹带 一点 蕾丝 装饰 女性 柔美 油然而生 搭配 碎花 蛋糕 裙 气质 非凡
-----------

Document: 870, Semantic similarity: 0.523485541343689
-----------
组图 看达人 演绎 豹纹 军装 风 导语 懂得 潮流 总是 知道 适合 今冬 流行 亮点 太 军装 豹纹 类似 民族风情 想要 知道 搭配 快 看看 时尚 达 穿 军绿色 宽松 款 大衣 不失 俏皮 味道 高腰 设计 短裙 有效 提升 腰线 衬托出 修长 美腿 豹纹 今年 冬季 抢眼 搭配 元素 加上 驼色 针织衫 灰色 围巾 暖 棕色 手 挎包 整体 色调 统一 迷人 棕色 蓝色 结合能 眼前一亮 简洁 款式 依然 突显 
-----------

Document: 522, Semantic similarity: 0.4756317138671875
-----------
女星 争当 蓝色妖姬 &amp; nbsp ; 英国 气质 女演员 瑞切尔 ・ 薇 兹 时尚 点评 英国 气质 女演员 瑞切尔 · 薇 兹 ( Rachel   Weisz )   美貌 非常 头脑 修身 印花 连衣裙 搭配 抢眼 棕红色 短 夹克 非常 好看 搭配 黑色 罗马 feel 高跟鞋 特别 有潮味 时尚 点评 身材 不算 瘦 女星 Lea   Michele 搭配 起来 非常 特色 一味 地瘦 风格 满是 褶皱 裙子 非常 修身 亮眼 颜色 非常
-----------

Document: 707, Semantic similarity: 0.47334203124046326
-----------
组图 黑丝 短裙 上阵   5 旬 女星 胜过 90 红星 导语 气温 越来越低 女星 不畏 严寒 纷纷 穿着 短裙 透视装 出席 活动 一番 比拼 不难 发现 气质 年轻 难得 厉害 一起 看看 刘晓庆 55 岁 近日 颁奖礼 刘晓庆 一袭 宝蓝色 超低 胸 V 领 长裙 亮相 轻薄 蕾丝 奢华 皮草 艳丽 色彩 翠绿 首饰 配上 短小 精炼 波波 头 瞬间 减龄 15 岁 张曼玉 46 岁 一向 气质 型 美女 著称 反倒 少 繁琐 修饰 刻意 打扮 超级 简单 Lanvin   for   H &amp; M 斜肩 礼裙 搭配 一双 皮质 手套 
-----------
</code></pre>
<p><br><br></p>
<h2 id="10-get_topic_hierarchy">10. get_topic_hierarchy</h2>
<p>对话题进行分类，需要</p>
<ol>
<li>先执行model.hierarchical_topic_reduction</li>
<li>再执行model.get_topic_hierarchy。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 将话题分为2类</span>
<span class="n">model</span><span class="o">.</span><span class="n">hierarchical_topic_reduction</span><span class="p">(</span><span class="n">num_topics</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_topic_hierarchy</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>[[7, 6, 1, 8, 5, 4, 3], [2, 0]]
</code></pre>
<p><br><br></p>
<h2 id="11-similar_words">11. similar_words</h2>
<p>查找相似词， 该方法其实也可以用于扩充词典。</p>
<p>similar_words(keywords, num_words, keywords_neg=None)</p>
<ul>
<li>keywords: 待查询关键词列表</li>
<li>num_words: 返回相似词个数</li>
<li>keywords_neg: 指定反义词列表</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查找【增进】的最相似的10个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">similar_words</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;增进&#34;</span><span class="p">],</span> 
                    <span class="n">num_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                    <span class="n">keywords_neg</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>(array(['两国关系', '两国', '温家宝', '王刚', '战略', '友好', '中欧', '政治', '会见', '人民'],
       dtype='&lt;U4'),
 array([0.50498132, 0.49835259, 0.4636392 , 0.45802986, 0.45299921,
        0.44836198, 0.43550295, 0.43471974, 0.43099192, 0.42711113]))
</code></pre>
<br>
<h2 id="12-save">12. save</h2>
<p>训练不易， 记得保存模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;随便起个名字.pkl&#39;</span><span class="p">)</span>
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>KeyBERT | 关键词发现</title>
      <link>https://hidadeng.github.io/blog/keybert_tutorial/</link>
      <pubDate>Wed, 27 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/keybert_tutorial/</guid>
      <description>使用 BERT 嵌入 和 简单余弦相似度 来查找文档中与文档本身最相似的短语，自动挖掘文本中的关键词</description>
      <content:encoded><![CDATA[<p>尽管已经有很多方法可用于关键字生成（例如，Rake、YAKE!、TF-IDF 等），但我想创建一个非常基本但功能强大的方法来提取关键字和关键短语。这就是 KeyBERT 的用武之地！它使用 <strong>BERT 嵌入</strong> 和 <strong>简单余弦相似度</strong> 来查找文档中与文档本身最相似的短语。</p>
<p>KeyBERT步骤</p>
<ol>
<li>首先使用 BERT 提取文档嵌入以获得<strong>文档级向量表示</strong>。</li>
<li>随后，为 N-gram 词/短语提取<strong>词向量</strong>。</li>
<li>然后，我们使用余弦相似度来找到与文档最相似的单词/短语。</li>
<li>最后可以将最相似的词识别为最能描述整个文档的词。</li>
</ol>
<h2 id="代码下载">代码下载</h2>
<p><a href="KeyBERT%E5%AD%A6%E4%B9%A0.ipynb">click to download the code</a></p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">keybert</span><span class="o">==</span><span class="mf">0.5.0</span>
</code></pre></div><br>
<h2 id="初始化模型">初始化模型</h2>
<p>KeyBERT库需要安装配置spacy语言模型</p>
<p>具体参考<strong>公众号：大邓和他的Python</strong> 2021-10-29 的推文 查看spacy配置方法</p>
<p>初始化模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">import</span> <span class="nn">jieba</span>


<span class="n">zh_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;zh_core_web_sm&#34;</span><span class="p">)</span>
<span class="n">bertModel</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">zh_model</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="准备数据">准备数据</h2>
<p>中文测试数据需要先分词，而后构造成类英文的语言结构(用空格间隔的文本)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 测试数据</span>
<span class="n">doc</span> <span class="o">=</span>  <span class="s2">&#34;&#34;&#34;时值10月25日抗美援朝纪念日，《长津湖》片方发布了“纪念中国人民志愿军抗美援朝出国作战71周年特别短片”，再次向伟大的志愿军致敬！
</span><span class="s2">　　电影《长津湖》全情全景地还原了71年前抗美援朝战场上那场史诗战役，志愿军奋不顾身的英勇精神令观众感叹：“岁月峥嵘英雄不灭，丹心铁骨军魂永存！”影片上映以来票房屡创新高，目前突破53亿元，暂列中国影史票房总榜第三名。
</span><span class="s2">　　值得一提的是，这部影片的很多主创或有军人的血脉，或有当兵的经历，或者家人是军人。提起这些他们也充满自豪，影片总监制黄建新称：“当兵以后会有一种特别能坚持的劲儿。”饰演雷公的胡军透露：“我父亲曾经参加过抗美援朝，还得了一个三等功。”影片历史顾问王树增表示：“我当了五十多年的兵，我的老部队就是上甘岭上下来的，那些老兵都是我的偶像。”
</span><span class="s2">　　“身先士卒卫华夏家国，血战无畏护山河无恙。”片中饰演七连连长伍千里的吴京感叹：“要永远记住这些先烈们，他们给我们带来今天的和平。感谢他们的付出，才让我们有今天的幸福生活。”饰演新兵伍万里的易烊千玺表示：“战争的残酷、碾压式的伤害，其实我们现在的年轻人几乎很难能体会到，希望大家看完电影后能明白，是那些先辈们的牺牲奉献，换来了我们的现在。”
</span><span class="s2">　　影片对战争群像的恢弘呈现，对个体命运的深切关怀，令许多观众无法控制自己的眼泪，观众称：“当看到影片中的惊险战斗场面，看到英雄们壮怀激烈的拼杀，为国捐躯的英勇无畏和无悔付出，我明白了为什么说今天的幸福生活来之不易。”（记者 王金跃）
</span><span class="s2">        &#34;&#34;&#34;</span>


<span class="n">doc</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>


<span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">keywords</span>
</code></pre></div><pre><code>[('铁骨', 0.5028),
 ('纪念日', 0.495),
 ('丹心', 0.4894),
 ('战役', 0.4869),
 ('影史', 0.473),
 ('父亲', 0.4576),
 ('票房', 0.4571),
 ('偶像', 0.4497),
 ('精神', 0.4436),
 ('家国', 0.4373)]
</code></pre>
<br>
<h2 id="常用参数">常用参数</h2>
<p><strong>bertModel.extract_keywords(docs, keyphrase_ngram_range, stop_words, top_n)</strong></p>
<ul>
<li><strong>docs</strong> 文档字符串（空格间隔词语的字符串）</li>
<li><strong>keyphrase_ngram_range</strong> 设置ngram，默认(1, 1)</li>
<li><strong>stop_words</strong> 停用词列表</li>
<li><strong>top_n</strong> 显示前n个关键词，默认5</li>
<li><strong>highlight</strong> 可视化标亮关键词，默认False</li>
<li>use_maxsum: 默认False;是否使用Max Sum Similarity作为关键词提取标准，</li>
<li>use_mmr: 默认False;是否使用Maximal Marginal Relevance (MMR) 作为关键词提取标准</li>
<li>diversity 如果use_mmr=True，可以设置该参数。参数取值范围从0到1</li>
</ul>
<br>
<p>对于<strong>keyphrase_ngram_range</strong>参数，</p>
<ul>
<li>(1, 1) 只单个词， 如&quot;抗美援朝&quot;, &ldquo;纪念日&quot;是孤立的两个词</li>
<li>(2, 2) 考虑词组， 如出现有意义的词组 &ldquo;抗美援朝 纪念日&rdquo;</li>
<li>(1, 2) 同时考虑以上两者情况</li>
</ul>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">diversity</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> 
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">keywords</span>
</code></pre></div><pre><code>[('影片 总监制', 0.5412),
 ('丹心 铁骨', 0.5339),
 ('抗美援朝 纪念日', 0.5295),
 ('长津湖 片方', 0.5252),
 ('志愿军 致敬', 0.5207),
 ('老兵 偶像', 0.5192),
 ('票房 创新', 0.5108),
 ('军人 血脉', 0.5084),
 ('家国 血战', 0.4946),
 ('家人 军人', 0.4885)]
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#可视化</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">highlight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="highlight.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">use_mmr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">diversity</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> 
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">keywords</span>
</code></pre></div><pre><code>[('影片 总监制', 0.5412),
 ('长津湖 片方', 0.5252),
 ('抗美援朝 纪念日', 0.5295),
 ('丹心 铁骨', 0.5339),
 ('志愿军 致敬', 0.5207),
 ('老兵 偶像', 0.5192),
 ('票房 创新', 0.5108),
 ('军人 血脉', 0.5084),
 ('家国 血战', 0.4946),
 ('家人 军人', 0.4885)]
</code></pre>
<br>
<h2 id="英文keybert">英文KeyBERT</h2>
<p>同样需要配置spacy，参考<strong>公众号：大邓和他的Python</strong> 2021-10-29 的推文 查看spacy配置方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">en_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;en_core_web_sm&#34;</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">         Supervised learning is the machine learning task of learning a function that
</span><span class="s2">         maps an input to an output based on example input-output pairs. It infers a
</span><span class="s2">         function from labeled training data consisting of a set of training examples.
</span><span class="s2">         In supervised learning, each example is a pair consisting of an input object
</span><span class="s2">         (typically a vector) and a desired output value (also called the supervisory signal). 
</span><span class="s2">         A supervised learning algorithm analyzes the training data and produces an inferred function, 
</span><span class="s2">         which can be used for mapping new examples. An optimal scenario will allow for the 
</span><span class="s2">         algorithm to correctly determine the class labels for unseen instances. This requires 
</span><span class="s2">         the learning algorithm to generalize from the training data to unseen situations in a 
</span><span class="s2">         &#39;reasonable&#39; way (see inductive bias).
</span><span class="s2">      &#34;&#34;&#34;</span>
<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">kw_model</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">keywords</span>
</code></pre></div><p>Run</p>
<pre><code>[('supervised learning', 0.6779),
 ('supervised', 0.6676),
 ('signal supervised', 0.6152),
 ('examples supervised', 0.6112),
 ('labeled training', 0.6013)]
</code></pre>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>BERTopic库 | 使用预训练模型做话题建模</title>
      <link>https://hidadeng.github.io/blog/bertopic_tutorial/</link>
      <pubDate>Tue, 26 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/bertopic_tutorial/</guid>
      <description>使用BERT主题建模技术,可以对经管等领域文本数据进行主题(话题)建模。效果堪比LDA，但比LDA智能</description>
      <content:encoded><![CDATA[<p>BERT是自然语言处理领域最新的词向量技术，而BERTopic 是基于BERT词向量进行主题建模技术，它利用 Transformer 和 c-TF-IDF 来创建密集的集群，允许轻松解释主题，同时在主题描述中保留重要词。</p>
<p>BERTopic亮点</p>
<ul>
<li>支持引导式Guided</li>
<li>支持（半）监督式</li>
<li>支持动态主题。</li>
<li>支持可视化</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">bertopic</span><span class="o">==</span><span class="mf">0.10.0</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">cntext</span><span class="o">==</span><span class="mf">1.6.5</span>
</code></pre></div><p><br><br></p>
<h2 id="准备数据">准备数据</h2>
<p>这里使用的新闻数据集， 共2000条。 新闻类别涵 <code>'娱乐', '教育', '游戏', '财经', '时政', '时尚', '科技', '体育', '家居', '房产'</code>
这里假设大家不知道有10类新闻题材， 构建模型的时候不会用到label字段的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;cnews.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 新闻题材</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>

<span class="c1">#记录数</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<pre><code>['娱乐' '教育' '游戏' '财经' '时政' '时尚' '科技' '体育' '家居' '房产']
2000
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 各类题材的新闻记录数</span>
<span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">时政    120
科技    106
时尚    106
财经    105
家居    103
教育     97
娱乐     96
体育     95
房产     87
游戏     85
</code></pre></div><br>
<p>这里定义了一个清洗数据函数clean_text，需要注意BERTopic需要先将中文分词改造成类似英文文本格式（用空格间隔词语）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;STOPWORDS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;STOPWORDS&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">test</span> <span class="o">=</span> <span class="s2">&#34;云南永善县级地震已致人伤间民房受损中新网月日电据云南昭通市防震减灾局官方网站消息截至日时云南昭通永善县级地震已造成人受伤其中重伤人轻伤人已全部送医院救治民房受损户间倒塌户间个乡镇所学校不同程度受损目前被损毁电力交通通讯设施已全部抢通修复当地已调拨帐篷顶紧急转移万人月日时分云南昭通永善县发生里氏级地震震源深度公里当地震感强烈此外成都等四川多地也有明显震感&#34;</span>

<span class="n">clean_text</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&#39;云南 永善县 级 地震 已致 伤间 民房 受损 中新网 日电 云南 昭通市 防震 减灾 局 官方网站 消息 日时 云南 昭通 永善县 级 地震 造成 受伤 重伤 轻伤 送 医院 救治 民房 受损 户间 倒塌 户间 乡镇 学校 不同 程度 受损 目前 损毁 电力 交通 通讯 设施 抢通 修复 调拨 帐篷 顶 紧急 转移 万人 时分 云南 昭通 永善县 发生 里氏 级 地震 震源 深度 公里 震感 强烈 成都 四川 多地 明显 震感&#39;
</code></pre></div><p>对2000条数据进行clean_text，得到的结果存储到content字段中。</p>
<p>我的macbook内存16G, 运行时间10s</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="训练topic模型">训练Topic模型</h2>
<p>文本分析步骤包括构建特征工程和训练，在本文中，直接使用开源的预训练中文词向量，省去了特征模型的学习时间。</p>
<p>选取的与训练模型均为word2vec格式，这样方便我们使用gensim将其导入。</p>
<table>
<thead>
<tr>
<th>模型名</th>
<th>数据</th>
<th>预训练模型资源地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>sgns.zhihu.words.bz2</td>
<td>知乎</td>
<td>链接: <a href="https://pan.baidu.com/s/1BDxP28KL_23Odj9NWZGe-Q">https://pan.baidu.com/s/1BDxP28KL_23Odj9NWZGe-Q</a> 提取码: n1qq</td>
</tr>
<tr>
<td>sgns.wiki.words.bz2</td>
<td>中文维基百科</td>
<td>链接: <a href="https://pan.baidu.com/s/1B1sxHmPeIPJYiCuP1zrmMw">https://pan.baidu.com/s/1B1sxHmPeIPJYiCuP1zrmMw</a> 提取码: hofj</td>
</tr>
<tr>
<td>sgns.financial.words.bz2</td>
<td>金融</td>
<td>链接: <a href="https://pan.baidu.com/s/1L_hmGjZMY2ExBn9Vfc_eRg">https://pan.baidu.com/s/1L_hmGjZMY2ExBn9Vfc_eRg</a> 提取码: hhn6</td>
</tr>
<tr>
<td>sgns.renmin.words.bz2</td>
<td>人民日报</td>
<td>链接: <a href="https://pan.baidu.com/s/1VQIDrwZH3Y3Lpy4-smPutw">https://pan.baidu.com/s/1VQIDrwZH3Y3Lpy4-smPutw</a> 提取码: 3b53</td>
</tr>
<tr>
<td>sgns.sougou.words.bz2</td>
<td>搜狗新闻</td>
<td>链接: <a href="https://pan.baidu.com/s/15nCaeB41mwK0ZVLrukXpFQ">https://pan.baidu.com/s/15nCaeB41mwK0ZVLrukXpFQ</a> 提取码: 04en</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Note</strong>:</p>
<p>除了表格外的资源，还可以使用spacy现有的预训练模型。</p>
</blockquote>
<p>本文案例cnews.csv是新闻类数据，这里最好选择使用同样为新闻题材的文本训练出的模型，这样BERTopic效果会更精准一些。sgns.sougou.words.bz2是使用搜狗新闻数据训练的语言模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">chinese_sougou_news_models</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;sgns.sogou.word.bz2&#39;</span><span class="p">,</span> <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">chinese_sougou_news_models</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x7f93e5b8cc10&gt;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>


<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="s2">&#34;chinese (simplified)&#34;</span><span class="p">,</span> 
                       <span class="n">embedding_model</span><span class="o">=</span><span class="n">chinese_sougou_news_models</span><span class="p">,</span>
                       <span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                       <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">docs</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1">#2000条进行fit_transform需要1min</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div><pre><code>100%|██████████| 2000/2000 [01:31&lt;00:00, 21.91it/s]
2021-10-28 12:11:25,583 - BERTopic - Transformed documents to Embeddings
2021-10-28 12:11:34,582 - BERTopic - Reduced dimensionality with UMAP
2021-10-28 12:11:34,718 - BERTopic - Clustered UMAP embeddings with HDBSCAN


CPU times: user 1min 50s, sys: 7.7 s, total: 1min 57s
Wall time: 1min 43s
</code></pre>
<p><br><br></p>
<h2 id="主题模型方法">主题模型方法</h2>
<ul>
<li>topic_model.get_topic_info 查看各主题信息</li>
<li>topic_model.find_topics(term, top_n=5)  查找term最有可能所属话题</li>
<li>topic_model.get_topic(0) 查看Topic 0的特征词</li>
<li>topic_model.visualize_topics() 话题间距离的可视化</li>
<li>topic_model.visualize_distribution(probs[0]) 查看某条文本的主题分布</li>
<li>topic_model.visualize_hierarchy(top_n_topics=20) 主题层次聚类可视化</li>
<li>topic_model.visualize_barchart(topics=[1]) 显示主题1的词条形图</li>
<li>topic_model.visualize_heatmap(n_clusters=10) 主题相似度热力图</li>
<li>topic_model.visualize_term_rank() 可视化词语</li>
<li>topic_model.save()  保存主题模型</li>
<li>topic_model.reduce_topics()  压缩主题个数(合并相近的主题)</li>
</ul>
<h3 id="get_topic_info">.get_topic_info()</h3>
<p>查看BERTopic基于cnews.csv数据， 跑出的各主题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_info</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/get_topic_info.png" alt=""  />
</p>
<br>
<h3 id="find_topicsterm">.find_topics(term)</h3>
<p>查看与词语【投资】最相关的主题，返回候选的最相思的5个主题id</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#</span>
<span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&#34;投资&#34;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">similar_topics</span>
</code></pre></div><p>Run</p>
<pre><code>[3, 9, 8, 10, 4]
</code></pre>
<br>
<h3 id="get_topic">.get_topic()</h3>
<p>查看id为3的主题信息（主题词及权重）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;基金&#39;, 0.15109221307919193),
 (&#39;投资&#39;, 0.042856192509064),
 (&#39;公司&#39;, 0.039785278320496976),
 (&#39;市场&#39;, 0.037072163603417835),
 (&#39;股票&#39;, 0.03230913401086524),
 (&#39;型基金&#39;, 0.02721898070238429),
 (&#39;收益&#39;, 0.025435672141638468),
 (&#39;投资者&#39;, 0.024633503649868493),
 (&#39;经理&#39;, 0.02458550023931051),
 (&#39;发行&#39;, 0.022672639068067168)]
</code></pre></div><br>
<h3 id="visualize_topics">.visualize_topics()</h3>
<p>可视化主题间距离</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">visualize_topics1</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">()</span>
<span class="c1">#可视化结果保存至html中，可以动态显示信息</span>
<span class="n">visualize_topics1</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;visualize_topics.html&#39;</span><span class="p">)</span>
<span class="n">visualize_topics1</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_topics1.png" alt=""  />
</p>
<p><a href="img/visualize_topics1.html">点击查看visualize_topics1.html</a></p>
<br>
<h3 id="visualize_distribution">.visualize_distribution()</h3>
<p>显示第一条新闻的主题概率分布</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">first_new_topic_probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_distribution</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">first_new_topic_probs</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;first_new_topic_probs.html&#39;</span><span class="p">)</span>
<span class="n">first_new_topic_probs</span>
</code></pre></div><p><img loading="lazy" src="img/first_new_topic_probs.png" alt=""  />

<a href="img/first_new_topic_probs.html">点击查看first_new_topic_probs.html</a></p>
<p>为了理解主题的潜在层次结构，我们可以使用 scipy.cluster.hierarchy 创建聚类并可视化它们之间的关系。 这有助于合并相似主题，达到降低主题模型主题数量nr_topics。</p>
<br>
<h3 id="visualize_hierarchytop_n_topics">.visualize_hierarchy(top_n_topics)</h3>
<p>话题层次聚类可视化，模型跑出12个主题，这里就按12进行分层聚类</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_hierarchy</span><span class="p">(</span><span class="n">top_n_topics</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_hierarchy.png" alt=""  />
</p>
<br>
<h3 id="visualize_barcharttopics">.visualize_barchart(topics)</h3>
<p>显示topics的词条形图</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_barchart</span><span class="p">(</span><span class="n">topics</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_barchart.png" alt=""  />
</p>
<br>
<h3 id="visualize_heatmapn_clusters">.visualize_heatmap(n_clusters)</h3>
<p>话题相似热力图。BERTopic可将主题以embeddings形式（向量）表示， 因此我们可以应用余弦相似度来创建相似度矩阵。 每两两主题可进行余弦计算，最终结果将是一个矩阵，显示主题间的相似程度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_similar_heatmap</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_heatmap</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">topic_similar_heatmap</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;topic_similar_heatmap.html&#39;</span><span class="p">)</span>
<span class="n">topic_similar_heatmap</span>
</code></pre></div><p><img loading="lazy" src="img/topic_similar_heatmap.png" alt=""  />

<a href="img/topic_similar_heatmap.html">点击查看topic_similar_heatmap.html</a></p>
<p>通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。 从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。 此外，可以轻松地将主题表示相互比较。</p>
<br>
<h3 id="visualize_term_rank">.visualize_term_rank()</h3>
<p>通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。</p>
<p>从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。</p>
<p>此外，可以轻松地将主题表示相互比较。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">term_score_decline</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_term_rank</span><span class="p">()</span>
<span class="n">term_score_decline</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;term_score_decline.html&#39;</span><span class="p">)</span>
<span class="n">term_score_decline</span>
</code></pre></div><p><img loading="lazy" src="img/term_score_decline.png" alt=""  />

<a href="img/term_score_decline.html">点击查看term_score_decline.html</a></p>
<h3 id="update_topics">.update_topics()</h3>
<p>更新主题模型。当您训练了一个模型并查看了代表它们的主题和单词时，您可能对表示不满意。 也许您忘记删除停用词，或者您想尝试不同的 n_gram_range。 我们可以使用函数 update_topics 使用 c-TF-IDF 的新参数更新主题表示。</p>
<p>使用.update_topics()更新，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">update_topics</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div><p>topic_model得到了更新，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&#34;手机&#34;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">similar_topics</span>
</code></pre></div><p>Run</p>
<pre><code>[2, 7, 4, 1, 5]
</code></pre>
<p>查看话题2的信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;功能&#39;, 0.022132351014298786),
 (&#39;采用&#39;, 0.02136925357979149),
 (&#39;像素&#39;, 0.020797285140907094),
 (&#39;拍摄&#39;, 0.017850841110848677),
 (&#39;机身&#39;, 0.015056931248982912),
 (&#39;英寸&#39;, 0.014624438184138326),
 (&#39;佳能&#39;, 0.012857768505732597),
 (&#39;支持&#39;, 0.012600856600766349),
 (&#39;光学&#39;, 0.012462085658291079),
 (&#39;相机&#39;, 0.011832978982454568)]
</code></pre></div><p>模型保存</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Save model</span>
<span class="c1">#model.save(&#34;my_model&#34;)</span>
<span class="c1"># Load model</span>
<span class="c1">#my_model = BERTopic.load(&#34;my_model&#34;)</span>
</code></pre></div><br>
<h3 id="reduce_topics">.reduce_topics()</h3>
<p>压缩主题数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">reduce_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>2021-10-28 12:28:01,976 - BERTopic - Reduced number of topics from 20 to 11
</code></pre>
<br>
<h2 id="代码数据">代码数据</h2>
<p><a href="bertopic_tutorial.zip">click to download</a></p>
<br>
<h2 id="总结">总结</h2>
<p>本文使用中文文本数据展示BERTopic部分功能，如果对英文数据感兴趣，可以前往  <a href="https://github.com/MaartenGr/BERTopic">https://github.com/MaartenGr/BERTopic</a> 深入学习。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
