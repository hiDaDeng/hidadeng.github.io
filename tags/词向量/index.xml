<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>词向量 on 大邓和他的PYTHON</title>
    <link>/tags/%E8%AF%8D%E5%90%91%E9%87%8F/</link>
    <description>Recent content in 词向量 on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Tue, 31 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>使用 Word2Vec 和 TF-IDF 计算五类企业文化</title>
      <link>https://textdata.cn/blog/2024-12-31-measure-corporate-culture-using-word2vec/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-12-31-measure-corporate-culture-using-word2vec/</guid>
      <description>我们使用最新的机器学习技术——**词嵌入模型**——和209,480份盈利电话会议记录创建了一本文化词典。我们对2001年至2018年期间的62,664个公司年度观察数据的**五个公司文化价值——创新、诚信、质量、尊重和团队合作**进行评分。结果表明，创新文化比公司创新的通常衡量标准——研发支出和专利数量——更广泛。此外，我们还表明，企业文化与业务结果相关，包括运营效率、风险承担、盈利管理、高管薪酬设计、企业价值和交易等，并且文化-绩效联系在困难时期更加显著。最后，我们提供了初步证据，表明企业文化受到重大公司事件（如合并和收购）的影响。</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="img/cover.png" alt=""  />
</p>
<p>Kai Li, Feng Mai, Rui Shen, Xinyan Yan, <strong>Measuring Corporate Culture Using Machine Learning</strong>, The Review of Financial Studies, 2020</p>
<p>摘要: 我们使用最新的机器学习技术——<strong>词嵌入模型</strong>——和209,480份盈利电话会议记录创建了一本文化词典。我们对2001年至2018年期间的62,664个公司年度观察数据的<strong>五个公司文化价值——创新、诚信、质量、尊重和团队合作</strong>进行评分。结果表明，创新文化比公司创新的通常衡量标准——研发支出和专利数量——更广泛。此外，我们还表明，企业文化与业务结果相关，包括运营效率、风险承担、盈利管理、高管薪酬设计、企业价值和交易等，并且文化-绩效联系在困难时期更加显著。最后，我们提供了初步证据，表明企业文化受到重大公司事件（如合并和收购）的影响。</p>
<h2 id="内容概况">内容概况</h2>
<p>今天分享的内容主要包括两部分， 即</p>
<ul>
<li>使用word2vec扩展得到五大类企业文化词典</li>
<li>使用TFIDF算法，结合五类文化词典对公司进行评分</li>
</ul>
<br>
<h2 id="算法步骤">算法步骤</h2>
<ol>
<li>构建种子词； 人工构建五类企业文化的种子词典， 每类词典人工准备5-10个词</li>
<li>word2vec扩充； 使用word2vec扩充五类企业文化词典的词汇量</li>
<li>tf-idf；将文本数据转为tf-idf格式</li>
<li>计算五类企业文化得分； 筛选含有文化词的列，按不同企业文化类别，分别求和得到得分。</li>
</ol>
<p><br><br></p>
<h2 id="一人工构建种子词典">一、人工构建种子词典</h2>
<p>词向量法程序会挖掘出原始数据中的所有词的词向量，这时候如果给词向量模型传入种子词，会根据语义空间的距离远近识别出多个近义词。</p>
<p>手工构建了五大类企业文化词典，存放在txt中，即</p>
<ul>
<li>data/w2v_seeds/innovation.txt</li>
<li>data/w2v_seeds/integrity.txt</li>
<li>data/w2v_seeds/quality.txt</li>
<li>data/w2v_seeds/respect.txt</li>
<li>data/w2v_seeds/teamwork.txt</li>
</ul>
<p>注意，在txt中，每行一个词语。原始语料</p>
<ul>
<li>data/w2v_corpus.txt</li>
</ul>
<p><br><br></p>
<h2 id="二-词向量法扩展词典">二、 词向量法扩展词典</h2>
<p>论文使用gensim的word2vec算法扩充企业文化词典。但代码太复杂，对初学Python的小白而言，代码的调试难度较大。大邓在这里将代码进行压缩和封装，只需要几行就能原作者几十行才能实现的词向量扩充的功能。需要安装gensim库和cntext库</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#安装需要的包</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">gensim</span><span class="o">==</span><span class="mf">4.2.0</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">cntext</span> <span class="o">--</span><span class="n">upgrade</span>
</code></pre></div><p><img loading="lazy" src="img/nltk.png" alt=""  />
</p>
<p>注意，下方代码运行可能会出现nltk_data问题，解决办法参考视频  <a href="https://www.bilibili.com/video/BV14A411i7DB/">https://www.bilibili.com/video/BV14A411i7DB/</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>

<span class="c1">#训练词向量模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词和训练好的词向量模型，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><pre><code>Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   42 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 46 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 46 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 46 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 46 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 46 s
</code></pre>
<p><br><br></p>
<h2 id="三-使用tf-idf有权重计算情感词典">三、 使用TF-IDF有权重计算情感词典</h2>
<p>一般的情感分析是无权重算法，即每个词语的权重都是1，只需要统计文本中<strong>某类概念词</strong>出现的多寡，就能确定<strong>该概念的得分</strong></p>
<p><img loading="lazy" src="img/tf.png" alt=""  />

<img loading="lazy" src="img/idf.png" alt=""  />

<img loading="lazy" src="img/tfidf.png" alt=""  />
</p>
<p>这篇论文使用的tf-idf， 我们可以将tf简单的理解为某词在文本出现的次数， idf是该词的稀奇程度(少见多怪程度)。一般我们认为一个词语出现的次数越多，信息量越大。但有时候，稀缺性也是一种很重要的信息量。例如以下两类词</p>
<ul>
<li>A 的、它、呢、了&hellip;&hellip;</li>
<li>B 核能、战争、死&hellip;&hellip;</li>
</ul>
<p>A类词，几乎出现在中文所有的句子中，我们可以忽略掉这类词，不影响对句子语义的理解。而B类词很少出现在我们日常文本句子中，但一旦出现，直接影响句子的语义。所以只考虑TF词频的大小还不全面，我们还需要纳入稀缺性信息IDF。</p>
<h3 id="31-读入数据">3.1 读入数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 读企业文本数据</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">corporate_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;data/corporate_culture.xlsx&#39;</span><span class="p">)</span>
<span class="n">corporate_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>idx</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Thank you sir. Ladies and gentlemen, at this t...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>[OPERATOR INSTRUCTIONS]. Our first question is...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>Thank you, Mr. Gallagher. [Caller instructions...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>(OPERATOR INSTRUCTIONS.)  Ann Gillen, Lehman B...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>(OPERATOR INSTRUCTIONS) John Harmon.    I have...</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看记录数(企业文本数）</span>
<span class="nb">len</span><span class="p">(</span><span class="n">corporate_df</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>105
</code></pre>
<br>
<h3 id="32-读词典">3.2 读词典</h3>
<p>word2vec扩展后的企业文化五大类，需要人工检查，剔除不符合词典含义的词，留下可用的词语。<strong>这里我们假装已经人工检查过了</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">read_dict</span><span class="p">(</span><span class="n">file</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">words</span>

<span class="n">innovation_words</span> <span class="o">=</span> <span class="n">read_dict</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/innovation.txt&#39;</span><span class="p">)</span>
<span class="n">integrity_words</span> <span class="o">=</span> <span class="n">read_dict</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/integrity.txt&#39;</span><span class="p">)</span>
<span class="n">quality_words</span> <span class="o">=</span> <span class="n">read_dict</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/quality.txt&#39;</span><span class="p">)</span>
<span class="n">respect_words</span> <span class="o">=</span> <span class="n">read_dict</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/respect.txt&#39;</span><span class="p">)</span>
<span class="n">teamwork_words</span> <span class="o">=</span> <span class="n">read_dict</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/teamwork.txt&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">respect_words</span><span class="p">)</span>
</code></pre></div><pre><code>['respectful', 'talent', 'talented', 'employee', 'dignity', 'empowerment', 'empower', 'skills', 'ibos', 'hr', 'salespeople', 'designers', 'creative', 'organizations', 'dedicated', 'backbone', 'abilities', 'missions', 'engine', 'tools', 'training', 'tackling', 'resource', 'adapting', 'interface', 'selecting', 'functions', 'expertise', 'cryocooler', 'sdk', 'affiliated', 'computers', 'departments', 'awareness', 'logistical', 'in-house', 'associate', 'optimization', 'functioning', 'outsource', 'organized', 'dedicate', 'outbound', 'pride', 'organization', 'referral', 'contacts', 'culture', 'motor', 'coordination', 'financially', 'onsite', 'web-based', 'functionality', 'wholesalers', 'provider', 'telesales', 'professionally', 'dealers', 'managers', 'involves', 'backhaul', 'crm', 'beefing', 'rf', 'computer', 'outreach', 'branding', 'appealing', 'networks', 'knowledge', 'electrical', 'industry-leading', 'providers', 'desires', 'guests', 'managerial', 'enhanced', 'assigned', 'railroad', 'durability', 'individuals', 'co2', 'believes', 'long-standing', 'high-quality', 'third-party', 'systems', 'groups', 'party', 'connecting', 'community', 'complementary', 'practices', 'reputation', 'processes', 'merchandising', 'next-generation', 'bundles', 'refocus', 'infrastructure', 'physician', 'transportation', 'aircraft', 'responsiveness', 'trained', 'full-time']
</code></pre>
<br>
<h3 id="33-生成每条记录的tfidf值">3.3 生成每条记录的tfidf值</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>


<span class="k">def</span> <span class="nf">createDTM</span><span class="p">(</span><span class="n">corpus</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;构建文档词语矩阵&#34;&#34;&#34;</span>
    <span class="n">vectorize</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
    <span class="c1">#注意fit_transform相当于fit之后又transform。</span>
    <span class="n">dtm</span> <span class="o">=</span> <span class="n">vectorize</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
    <span class="c1">#vectorize.fit(corpus)</span>
    <span class="c1">#dtm  = vectorize.transform(corpus) </span>
    <span class="c1">#打印dtm</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dtm</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> 
                        <span class="n">columns</span><span class="o">=</span><span class="n">vectorize</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span> 

<span class="n">corporate_tfidf_df</span> <span class="o">=</span> <span class="n">createDTM</span><span class="p">(</span><span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
<span class="n">corporate_tfidf_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/tfidf-df5.png" alt=""  />
</p>
<br>
<h3 id="34-更新五大类词典">3.4 更新五大类词典</h3>
<p>企业文化词典中的词，并不是都出现在corporate_tfidf_df中的列里， 为避免列操作出错。 需要重新更新文化词典。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">Innovation_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">innovation_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">corporate_tfidf_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">Integrity_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">integrity_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">corporate_tfidf_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">Quality_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">quality_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">corporate_tfidf_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">Respect_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">respect_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">corporate_tfidf_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">Teamwork_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">teamwork_words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">corporate_tfidf_df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
</code></pre></div><br>
<h3 id="35-计算不同类别得分">3.5 计算不同类别得分</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">corporate_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">],</span>
        <span class="s1">&#39;Innovation&#39;</span><span class="p">:</span> <span class="n">corporate_tfidf_df</span><span class="p">[</span><span class="n">Innovation_words</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;Integrity&#39;</span><span class="p">:</span><span class="n">corporate_tfidf_df</span><span class="p">[</span><span class="n">Integrity_words</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;Quality&#39;</span><span class="p">:</span><span class="n">corporate_tfidf_df</span><span class="p">[</span><span class="n">Quality_words</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;Respect&#39;</span><span class="p">:</span><span class="n">corporate_tfidf_df</span><span class="p">[</span><span class="n">Respect_words</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;Teamwork&#39;</span><span class="p">:</span><span class="n">corporate_tfidf_df</span><span class="p">[</span><span class="n">Teamwork_words</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">}</span>

<span class="n">CultureResultDf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">CultureResultDf</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/output.png" alt=""  />
</p>
<br>
<h3 id="36-保存结果">3.6 保存结果</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">CultureResultDf</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;output/企业五大类文化tfidf有权重计算.csv&#39;</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>如何用图嵌入(网络思维和嵌入思维)表征企业，表征高管的职业经历</title>
      <link>https://textdata.cn/blog/2024-12-31-the-experience-of-ceo-to-vector-with-graphe-embeddings/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-12-31-the-experience-of-ceo-to-vector-with-graphe-embeddings/</guid>
      <description>管理的本质是一种实践，在某些情形下，阅历比简历更重要，丰富的职业经历有助于企业高管形成多元化的思维结构、广阔的管理视野、丰富的社会资源和过人的胆识。因此，对于企业而言，了解高管的职业经历非常重要，这可以帮助企业更好地了解高管的背景和潜力，从而更好地为企业的发展提供支持。而研究高管的个人特质，已有的研究，主要从年龄、性别、学历等类别型变量开展研究，即使从从职业经历研究，也是作为离散变量，没有充分挖掘职业经历的信息。</description>
      <content:encoded><![CDATA[<p>今天分享的内容主要是 <strong>如何用图嵌入(网络思维和嵌入思维)表征企业，表征高管的职业经历</strong>。</p>
<p><br><br></p>
<h2 id="一高管职业经历">一、高管职业经历</h2>
<p>管理的本质是一种实践，在某些情形下，阅历比简历更重要，丰富的职业经历有助于企业高管形成多元化的思维结构、广阔的管理视野、丰富的社会资源和过人的胆识。因此，对于企业而言，了解高管的职业经历非常重要，这可以帮助企业更好地了解高管的背景和潜力，从而更好地为企业的发展提供支持。</p>
<p>而研究高管的个人特质，已有的研究，主要从年龄、性别、学历等类别型变量开展研究，即使从从职业经历研究，也是作为离散变量，没有充分挖掘职业经历的信息。</p>
<p>今天分享的内容主要是 <strong>如何用网络思维和嵌入思维表征企业，表征高管的职业经历</strong>。</p>
<p><img loading="lazy" src="img/graph-embeddings-numerical.jpg" alt=""  />
</p>
<p>本技术文的创新价值</p>
<blockquote>
<ol>
<li><strong>网络思维</strong>;<br>
节点、边。 企业是节点，高管一般有多个企业就职经历，可以在任意2个企业构件一个边。</li>
</ol>
</blockquote>
<ol start="2">
<li><strong>嵌入思维</strong>;</li>
</ol>
<blockquote>
<p>把企业网络中的节点转化为向量表示，企业是n维向量，高管也是同样的n维向量。类似于阴阳五行思维表征世间万物完事。</p>
<ol start="3">
<li>
<p><strong>向量计算</strong>；</p>
<p>事物都用N维向量表征，那么不同类别的事物，我们就可以对高管和企业之间就可以进行向量计算。</p>
<p><strong>Kmeans聚类</strong>;<br>
将很多企业向量或高管职业经历向量进行聚类，可能得到理想的&quot;分类&quot;标签(人工解读后的cluster数字就是很好的分类标签)。</p>
<p><strong>相似度计算</strong>
招聘来的高管对于该企业是否带来更多的异质性、或者带来更多的相似性。某企业的高管团队的异质性可以度量出一个准确的数字。招聘新的高管，也可以测量给改企业带来多少的相似性或者异质性。</p>
</li>
</ol>
</blockquote>
<p><img loading="lazy" src="img/vector_similarity.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二职业经历聚类算法步骤">二、职业经历聚类算法步骤</h2>
<p>首先，我们需要构建一个企业的网络图，其中每个节点代表一个企业，每条边代表两个企业之间的关系，比如同城市、同行业， 而<strong>高管职业经历中的任意两家企业天然也是一个边</strong>。然后，我们使用node2vec算法对这个网络图进行嵌入，得到每个企业的向量表示。</p>
<p>接着我们需要对每个高管的职业经历进行处理。一般来说，一个高管的职业经历可能涉及多个企业，因此我们需要将这些企业的向量求平均，得到一个高管的职业经历向量。这个向量可以代表高管的职业经历背景，可以用于后续的各类分析，比如聚类。</p>
<p>最后我们可以使用聚类算法（比如k-means算法）对高管职业经历向量进行聚类分析， 具有相似职业经历的高管分为一组。</p>
<p><br><br></p>
<h2 id="三数据导入">三、数据导入</h2>
<p>高管中的职业经历信息一般存在于没有规律的个人简历中，提取起来是比较复杂，这里可能需要用到正则表达式、命名实体识别等将处于简介中的企业名识别出来。</p>
<p>高管职业经历信息一般存在于个人简历中，而个人简历中的职业经历信息往往不太规律，多个企业名也没有明显的分隔符，因此需要用到一些技术手段来将企业名识别出来，以便后续的处理和分析。</p>
<p><strong>首先，我们可以使用正则表达式来匹配企业名</strong>。正则表达式是一种用于匹配文本的工具，可以根据一定的规则来匹配出所需要的信息。在这里，我们可以使用正则表达式来匹配一些常见的企业名词，比如“有限公司”、“股份有限公司”等。这样可以一定程度上识别出企业名，但对于一些特殊的企业名还需要进行进一步处理。</p>
<p><strong>其次，我们可以使用命名实体识别（NER）技术来识别企业名</strong>。命名实体识别是自然语言处理的一项任务，旨在识别文本中的命名实体，比如人名、地名、组织名等。在这里，我们可以使用NER技术来识别文本中的企业名，从而更准确地提取出高管的职业经历信息。</p>
<p><strong>最后，我们需要对提取出的企业名进行去重和整理</strong>。在提取出的企业名中，有一些是重复的，有一些是不必要的，需要将它们去掉。同时，对于每个高管的职业经历信息，我们需要将提取出的企业名整理成一个有序的序列，方便后续的处理和分析。</p>
<p>总之，对于高管职业经历信息的提取，需要用到一些技术手段，包括正则表达式、命名实体识别等。这些技术手段可以帮助我们更准确地提取出高管的职业经历信息，为后续的分析和处理提供基础。<strong>但限于篇幅和主题，今天使用虚构的50条高管记录数据做实验。</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;high_executives.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>姓名</th>
      <th>性别</th>
      <th>出生年份</th>
      <th>学历</th>
      <th>职业经历</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>高管1</td>
      <td>女</td>
      <td>2000</td>
      <td>本科</td>
      <td>华为-CFO,Facebook-CFO,Facebook-CEO</td>
    </tr>
    <tr>
      <th>1</th>
      <td>高管2</td>
      <td>女</td>
      <td>1989</td>
      <td>本科</td>
      <td>百度-CEO,阿里巴巴-CFO,亚马逊-COO</td>
    </tr>
    <tr>
      <th>2</th>
      <td>高管3</td>
      <td>女</td>
      <td>1992</td>
      <td>博士</td>
      <td>谷歌-CTO,腾讯-COO,百度-COO</td>
    </tr>
    <tr>
      <th>3</th>
      <td>高管4</td>
      <td>女</td>
      <td>1989</td>
      <td>本科</td>
      <td>IBM-COO,苹果-CFO,微软-COO</td>
    </tr>
    <tr>
      <th>4</th>
      <td>高管5</td>
      <td>男</td>
      <td>1960</td>
      <td>本科</td>
      <td>谷歌-COO,苹果-CFO,百度-COO</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#记录数</span>
<span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div><pre><code>50
</code></pre>
<br>
<br>
<h2 id="四训练node2vec模型">四、训练Node2Vec模型</h2>
<h3 id="41-node2vec算法">4.1 node2vec算法</h3>
<p>Node2Vec是一种基于深度学习的图嵌入算法，旨在将图中的节点映射到低维向量空间中，从而方便后续的分析和处理。具体来说，Node2Vec算法可以将节点的局部邻域结构转化为向量表示，同时保留节点之间的全局结构信息。如果熟悉Wordc2Vec的同学，理解起来会比较容易，Node2Vec是基于word2vec算法开发出来的，将职业经历中每个企业看做词语，训练得到企业向量表示。</p>
<p><img loading="lazy" src="img/node2vec.png" alt=""  />
</p>
<p>在Node2Vec算法中，每个节点的向量表示由两个部分组成：一个是节点自身的特征向量，另一个是节点在不同邻域结构下的向量表示。算法的核心思想是通过两个步骤来生成节点的向量表示：</p>
<ul>
<li>随机游走：对于每个节点，从它的邻居节点中随机选择一个节点进行访问，然后在这个节点的邻居中进行同样的随机选择。这个过程可以生成一系列的节点序列，其中每个序列都代表了一个从起始节点出发的随机游走路径。</li>
<li>Skip-gram模型：基于这些随机游走路径，使用Skip-gram模型进行向量表示的学习。Skip-gram模型是一种常见的自然语言处理模型，用于学习词向量。在Node2Vec算法中，可以将节点序列看作“句子”，将每个节点看作“词”，然后使用Skip-gram模型来学习节点向量的表示。</li>
</ul>
<p>Node2Vec算法通过随机游走和Skip-gram模型的结合，可以生成具有丰富语义信息的节点向量，同时保留了节点之间的全局结构信息。这种算法可以应用于各种不同的图结构，包括社交网络、知识图谱、生物信息学等领域，具有广泛的应用前景。</p>
<p><strong>在高管职业经历数据的应用中，我们可以将每个企业看作图中的一个节点，然后使用Node2Vec算法来训练企业向量模型</strong>。这样可以将企业的职业经历信息转化为向量表示，方便后续的分析和处理。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">from</span> <span class="nn">node2vec</span> <span class="kn">import</span> <span class="n">Node2Vec</span>


<span class="c1"># 读取CSV文件并提取职业经历中的公司名列表</span>
<span class="n">companies</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;IBM&#39;</span><span class="p">,</span> <span class="s1">&#39;谷歌&#39;</span><span class="p">,</span> <span class="s1">&#39;Facebook&#39;</span><span class="p">,</span> <span class="s1">&#39;苹果&#39;</span><span class="p">,</span> <span class="s1">&#39;微软&#39;</span><span class="p">,</span> <span class="s1">&#39;亚马逊&#39;</span><span class="p">,</span> <span class="s1">&#39;阿里巴巴&#39;</span><span class="p">,</span> <span class="s1">&#39;腾讯&#39;</span><span class="p">,</span> <span class="s1">&#39;百度&#39;</span><span class="p">,</span> <span class="s1">&#39;华为&#39;</span><span class="p">]</span>
<span class="n">companies_regex</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;|&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">companies</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;companies&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;职业经历&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">companies_regex</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>

<span class="c1"># 构建公司名之间的边</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">for</span> <span class="n">companies</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;companies&#39;</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">company1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">companies</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">company2</span> <span class="ow">in</span> <span class="n">companies</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">company1</span><span class="p">,</span> <span class="n">company2</span><span class="p">)</span>

<span class="c1"># 使用node2vec库生成公司名向量</span>
<span class="c1"># 企业向量维度 16</span>
<span class="n">node2vec</span> <span class="o">=</span> <span class="n">Node2Vec</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">walk_length</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_walks</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">node2vec</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># model.wv[company] 查询某个企业的向量</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">company</span><span class="p">]</span> <span class="k">for</span> <span class="n">company</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">()]</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    Computing transition probabilities:   0%|          | 0/10 [00:00&lt;?, ?it/s]
    Generating walks (CPU: 1): 100%|████████████| 100/100 [00:00&lt;00:00, 3920.94it/s]
</code></pre></div><br>
<h3 id="42-理解嵌入">4.2 理解嵌入</h3>
<p>本文嵌入的维度是16维， 其实这里用任意 N 维都可以嵌入表示任意一个事物。这里理解起来比较难，咱们用熟悉的事情来假装理解嵌入维度的设定。
在中国传统文化中，经常使用n维来刻画、描述、表征任意事物。例如</p>
<ul>
<li>2维， 阴阳思维去描述事物的阴阳</li>
<li>5维， 五行，金木水火土描述事物</li>
</ul>
<p>而在本技术中， <code>Node2Vec(G, dimensions=16, walk_length=10, num_walks=100)</code> dimensions=16 即用16维表征每个企业，得到16维的企业向量。 需要注意的是， 我们可能都无法理解 16维的任意一个维度的含义。如果设置成2维、5维或者其他维，我们也是无法理解对应的维度含义。因为中国传统文化，已经定义了每个维度的含义，然后再去表征事物。但是我们是先定义了维度数，所以维度的含义是未知的。</p>
<p><br><br></p>
<h2 id="五计算高管职业经历向量">五、计算高管职业经历向量</h2>
<p>定义一个 <strong>companys2vec(companys)</strong> 函数 ，该函数可以把多家企业就职的经历转为 「职业经历向量」。</p>
<p>将高管的职业经历转化为向量，每家企业是一个 16 维向量，最简单粗暴的办法是求平多个就职企业向量的均值，均值向量也是 16 维。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">companys2vec</span><span class="p">(</span><span class="n">companys</span><span class="p">):</span>
    <span class="n">cvs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">company</span> <span class="ow">in</span> <span class="n">companys</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">cvs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">company</span><span class="p">])</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cvs</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="n">companys2vec</span><span class="p">(</span><span class="n">companys</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;华为&#39;</span><span class="p">,</span> <span class="s1">&#39;Facebook&#39;</span><span class="p">,</span> <span class="s1">&#39;Facebook&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>array([-0.03054439,  0.28417936,  0.23475488,  0.36075735, -0.16633254,
       -0.06266979,  0.74403137,  0.16226356,  0.01991086, -0.15565623,
        0.34757233,  0.3079434 ,  0.19876878,  0.01175458, -0.55069256,
        0.01623819], dtype=float32)
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#批量计算，得到高管的职业经历向量 career_vec</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;career_vec&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;companies&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">companys2vec</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>姓名</th>
      <th>性别</th>
      <th>出生年份</th>
      <th>学历</th>
      <th>职业经历</th>
      <th>companies</th>
      <th>career_vec</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>高管1</td>
      <td>女</td>
      <td>2000</td>
      <td>本科</td>
      <td>华为-CFO,Facebook-CFO,Facebook-CEO</td>
      <td>[华为, Facebook, Facebook]</td>
      <td>[-0.03054439, 0.28417936, 0.23475488, 0.360757...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>高管2</td>
      <td>女</td>
      <td>1989</td>
      <td>本科</td>
      <td>百度-CEO,阿里巴巴-CFO,亚马逊-COO</td>
      <td>[百度, 阿里巴巴, 亚马逊]</td>
      <td>[-0.007476224, 0.2823672, 0.20479268, 0.281800...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>高管3</td>
      <td>女</td>
      <td>1992</td>
      <td>博士</td>
      <td>谷歌-CTO,腾讯-COO,百度-COO</td>
      <td>[谷歌, 腾讯, 百度]</td>
      <td>[-0.040801946, 0.27067217, 0.20773596, 0.34468...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>高管4</td>
      <td>女</td>
      <td>1989</td>
      <td>本科</td>
      <td>IBM-COO,苹果-CFO,微软-COO</td>
      <td>[IBM, 苹果, 微软]</td>
      <td>[-0.022640707, 0.31426176, 0.1841877, 0.323161...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>高管5</td>
      <td>男</td>
      <td>1960</td>
      <td>本科</td>
      <td>谷歌-COO,苹果-CFO,百度-COO</td>
      <td>[谷歌, 苹果, 百度]</td>
      <td>[-0.030689942, 0.25639233, 0.19499178, 0.33937...</td>
    </tr>
  </tbody>
</table>
</div>
<p><br><br></p>
<h2 id="六高管职业经历聚类kmeans">六、高管职业经历聚类Kmeans</h2>
<p>Kmeans是一种常见的聚类算法，旨在将相似的数据点分组为同一类别，从而发现数据的内在结构。Kmeans算法的优点是简单易实现，对于大型数据集也具有较高的效率。它可以适用于各种类型的数据，包括数值型、文本型和图像型等数据。同时，Kmeans算法可以通过调整簇的个数来控制聚类结果的细粒度程度，比如选择较大的簇个数可以得到更细致的聚类结果。</p>
<p><img loading="lazy" src="img/kmeans.png" alt=""  />
</p>
<p><strong>Kmeans算法的缺点是需要指定簇的个数k，这个参数选择较大或者较小都可能导致聚类结果不理想</strong>。同时，Kmeans算法对于离群点比较敏感，可能会导致簇中心偏离聚类的本质结构。如果对数据不了解，需要使用手肘法则等方式确定K。 <strong>这里假设我们对数据很了解，那么可以指定K=5</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># 对高管的职业经历向量进行聚类分析</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;career_vec&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()))</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>

<span class="c1"># 保存结果为CSV文件</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;high_executives_with_clusters.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8-sig&#39;</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="最后-结果呈现">最后， 结果呈现</h2>
<p><strong>注意cluster是没有意义的数字，不同的数字代表着丰富的信息，例如职能、行业、地域、晋升路径等， 需要需要我们「人工解读」理解每个数字对应的含义</strong>。 这里摘抄一下 <strong>何瑛,于文蕾,戴逸驰,王砚羽.高管职业经历与企业创新[J].管理世界,2019,35(11):174-192.</strong> 内的内容，该文章没有使用向量表示，但是可以提供理解cluster的角度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">在 CEO 职业经历的分类上，现有文献的研究视角各有不同。 

- 在「职能方面」，比较受认可的是 Hambrick 和 Mason（1984）所提出的 3 部门分类法，即产出型职能（营销与研发）、生产型职能（过程管理、设备管理与会计） 和外围型职能（法律与融资），之后 Abebe 等（2010）较多学者沿用这种分类；

- 在「行业方面」，Crossland 等（2014）将 其区分为能源、材料、工业、非必需消费品、日用消费品、保健、金融、信息技术、电信服务和公用设施等 10 种类 型；在组织机构方面，Hu 和 Liu（2015）分为生产性组织（如企业）、非生产性组织（如大学）以及行政或政府组织 3 类；

- 在「地域类型」方面，Schmid 和 Wurster（2017）根据是否具有国际工作经历分为两类；

- 在「工作背景」方面，Fan 等 （2007）和 Benmelech 和 Frydman（2015）分别根据是否具有从政经历和从军经历进行区分；

- 另外在「晋升路径」上， Brockman 等（2019）则区分了内部提拔与外部聘请两种类型。 

总的来说，上述有关管理者职业经历的文献大多 集中于研究管理者职业经历的某一方面，对复合型职业经历进行整合研究的文献十分罕见。 事实上，不同方面的职业经历之间往往存在某种联系，相互作用最终塑造了独特的管理风格（Kaplan et al.，2008）。
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;high_executives_with_clusters.csv&#39;</span><span class="p">)</span>
<span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>姓名</th>
      <th>性别</th>
      <th>出生年份</th>
      <th>学历</th>
      <th>职业经历</th>
      <th>companies</th>
      <th>career_vec</th>
      <th>cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>高管1</td>
      <td>女</td>
      <td>2000</td>
      <td>本科</td>
      <td>华为-CFO,Facebook-CFO,Facebook-CEO</td>
      <td>['华为', 'Facebook', 'Facebook']</td>
      <td>[-0.03054439  0.28417936  0.23475488  0.360757...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>高管2</td>
      <td>女</td>
      <td>1989</td>
      <td>本科</td>
      <td>百度-CEO,阿里巴巴-CFO,亚马逊-COO</td>
      <td>['百度', '阿里巴巴', '亚马逊']</td>
      <td>[-0.00747622  0.2823672   0.20479268  0.281800...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>高管3</td>
      <td>女</td>
      <td>1992</td>
      <td>博士</td>
      <td>谷歌-CTO,腾讯-COO,百度-COO</td>
      <td>['谷歌', '腾讯', '百度']</td>
      <td>[-0.04080195  0.27067217  0.20773596  0.344685...</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>高管4</td>
      <td>女</td>
      <td>1989</td>
      <td>本科</td>
      <td>IBM-COO,苹果-CFO,微软-COO</td>
      <td>['IBM', '苹果', '微软']</td>
      <td>[-2.2640707e-02  3.1426176e-01  1.8418770e-01 ...</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>高管5</td>
      <td>男</td>
      <td>1960</td>
      <td>本科</td>
      <td>谷歌-COO,苹果-CFO,百度-COO</td>
      <td>['谷歌', '苹果', '百度']</td>
      <td>[-0.03068994  0.25639233  0.19499178  0.339379...</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><pre><code>2    14
3    13
4     9
0     7
1     7
Name: cluster, dtype: int64
</code></pre>
<p><br><br></p>
<h2 id="代码获取">代码获取</h2>
<p>链接: <a href="https://pan.baidu.com/s/1pZQj5_s2sv5LYZ-EerJD1Q">https://pan.baidu.com/s/1pZQj5_s2sv5LYZ-EerJD1Q</a> 提取码: 6m7v</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 |  使用1000w条豆瓣影评训练Word2Vec</title>
      <link>https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/</link>
      <pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/</guid>
      <description>&lt;p&gt;本文内容&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;介绍豆瓣影评数据集&lt;/li&gt;
&lt;li&gt;构造语料训练Word2Vec模型&lt;/li&gt;
&lt;li&gt;获取数据&amp;amp;cntext&amp;amp;Word2Vec模型文件&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一豆瓣影评数据集&#34;&gt;一、豆瓣影评数据集&lt;/h2&gt;
&lt;h3 id=&#34;11-数据集介绍&#34;&gt;1.1 数据集介绍&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据集: douba-movie-1000w

数据源: 豆瓣电影
   
记录数:
   - 电影 10269 部
   - 影评 10310989 条
   
体积: 1.35G 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;该数据集正好弥补下国内公开电影数据集的空缺， 数据已经过初步清洗，可用于推荐系统、情感分析、知识图谱、新闻传播学、社会学文化变迁等多个领域(或主题)。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;12-读取数据&#34;&gt;1.2 读取数据&lt;/h3&gt;
&lt;p&gt;下载 &lt;em&gt;&lt;strong&gt;douba-movie-1000w.zip&lt;/strong&gt;&lt;/em&gt; 解压后，可以看到数据集中有一个 &lt;em&gt;&lt;strong&gt;all_movies_with_id.csv&lt;/strong&gt;&lt;/em&gt; 文件。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;all_movies_with_id.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;13-所含字段&#34;&gt;1.3 所含字段&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;col&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; - &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt; - ID
 - Movie_Name  电影名
 - Score  豆瓣电影评分(1-10)
 - Review_People  评论者人数
 - Star_Distribution  评论评分分布(1-5, 含多个数值，数值以%间隔)
 - Craw_Date 爬虫运行日期
 - Username 豆瓣评论者用户名
 - Date 影评日期
 - Star  影评评分(1-5)
 - Comment 影评内容
 - Comment_Distribution 影评评分分布
 - Like 影评获得的喜欢数
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二-构造语料训练word2vec&#34;&gt;二、 构造语料&amp;amp;训练Word2Vec&lt;/h2&gt;
&lt;h3 id=&#34;21-构造语料&#34;&gt;2.1 构造语料&lt;/h3&gt;
&lt;p&gt;将字段 &lt;em&gt;&lt;strong&gt;Comment&lt;/strong&gt;&lt;/em&gt; 中所有文本汇总到 &lt;em&gt;&lt;strong&gt;douban-movie-1000w.txt&lt;/strong&gt;&lt;/em&gt;,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;douban-movie-1000w.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Comment&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-配置cntext211&#34;&gt;2.2 配置cntext2.1.1&lt;/h3&gt;
&lt;p&gt;将 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 放置于桌面，打开 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal) 中使用 &lt;em&gt;&lt;strong&gt;pip3&lt;/strong&gt;&lt;/em&gt; 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文末有 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 获取方式&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-训练word2vec&#34;&gt;2.3 训练Word2Vec&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#cntext为2.1.1&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2VModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;douban-movie-1000w.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Starting Preprocessing Corpus ...
Starting Training! This may take a while.Please be patient...
Traning word2vec model took 3965 seconds
Note: The Word2Vec model hase saved to output/Word2Vec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/word2vec.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;经过大概一个小时的训练， 得到模型文件 &lt;em&gt;&lt;strong&gt;douban-movie-1000w.200.6.bin&lt;/strong&gt;&lt;/em&gt; 及相关文件， 注意不要删掉哦。 已训练好的模型，可以自己用， 也可分享给其他人使用。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四使用word2vec&#34;&gt;四、使用Word2Vec&lt;/h2&gt;
&lt;h3 id=&#34;41-导入word2vec模型文件&#34;&gt;4.1 导入Word2Vec模型文件&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
 
&lt;span class=&#34;c1&#34;&gt;#导入模型，请注意路径。&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 【当前代码】 与 【Word2Vec文件夹】 同处于一个文件夹内&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Word2Vec/douban-movie-1000w.200.6.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Loading word2vec model...
&amp;lt;gensim.models.word2vec.Word2Vec at 0x10cb02090&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-常用函数&#34;&gt;4.2 常用函数&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;dm_w2v.wv.get_vector(key)&lt;/strong&gt;&lt;/em&gt; 获取key的词向量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;dm_w2v.most_similar_to_given(key1, keys_list)&lt;/strong&gt;&lt;/em&gt; 从 keys_list 中获取与 key1 最相似的词&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;dm_w2v.n_similarity(ws1, ws2)&lt;/strong&gt;&lt;/em&gt;  两组词ws1, ws2 的相似度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;dm_w2v.closer_than(key1, key2)&lt;/strong&gt;&lt;/em&gt;  更接近于key1的词向量(相比于key2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;dm_w2v.most_similar(positive, negative)&lt;/strong&gt;&lt;/em&gt;  找出与positive同方向，与negative反向相反的词。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&#34;421-get_vectorkey&#34;&gt;4.2.1 get_vector(key)&lt;/h4&gt;
&lt;p&gt;使用词向量查看某&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;3.55084002e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.22685611e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;8.48365605e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.23056602e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;1.35057056e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.65976137e-02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.26512849e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.47152972e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;9.99028236e-03&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.00873756e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.05153358e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.39181948e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;6.02373898e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.00308895e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;2.33978868e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.83010173e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;9.67333555e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;3.04877937e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;6.59058094e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;3.19660306e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.21165246e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;3.68000716e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;2.36653373e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;6.83727741e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
      &lt;span class=&#34;o&#34;&gt;......&lt;/span&gt;
      &lt;span class=&#34;o&#34;&gt;......&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.23901594e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;5.07202707e-02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;8.75848413e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;4.31963325e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;1.31377324e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.19606090e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.68391216e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;6.27069890e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;7.37121344e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;2.49946609e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.47220814e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.33507824e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;2.97913142e-02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;4.91593599e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;5.83192170e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;8.48378658e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;3.30877733e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;2.17747837e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;2.22701088e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.00758147e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;3.41430195e-02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;7.27023900e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;7.94953525e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.03226733e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
       &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;4.55965906e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.66779244e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;1.16857982e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.02211344e+00&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;4.11061406e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;8.95921767e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;9.48565483e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.48802996e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;mf&#34;&gt;9.36261594e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;3.98367733e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;mf&#34;&gt;3.12385857e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;8.67059827e-01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
      &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;float32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h4 id=&#34;422-most_similar_to_givenkey1-keys_list&#34;&gt;4.2.2 most_similar_to_given(key1, keys_list)&lt;/h4&gt;
&lt;p&gt;从 keys_list 中获取与 key1 最相似的词。例如在 1000w 影评中，从&lt;code&gt;&#39;爱情&#39;, &#39;悬疑&#39;, &#39;飞船&#39;, &#39;历史&#39;, &#39;战争&#39;&lt;/code&gt;找出最接近&lt;code&gt;&#39;太空&#39;&lt;/code&gt;，最后返回&lt;code&gt;&#39;飞船&#39;&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#从 `keys_list` 中获取与 `key1` 最相似的 `key`。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar_to_given&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;太空&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                                &lt;span class=&#34;n&#34;&gt;keys_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;爱情&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;悬疑&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;飞船&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;历史&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;战争&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&amp;#39;飞船&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h4 id=&#34;423-w2v_modeln_similarityws1-ws2&#34;&gt;4.2.3 w2v_model.n_similarity(ws1, ws2)&lt;/h4&gt;
&lt;p&gt;两组词ws1, ws2 的相似度。注意相似值更多的是体现了语义的相关性， 并不能准确反映语义的远近。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.metrics.pairwise&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cosine_similarity&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;理想&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)],&lt;/span&gt;  
                  &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;现实&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)])[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0.4698379
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#cosine算法&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;理想&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
                       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;现实&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0.4698379
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#计算两组键之间的余弦相似度。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;精彩&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;赞&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;推荐&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
                       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;无聊&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;尴尬&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;垃圾&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0.109311774
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;理想&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;梦想&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
                       &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;现实&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;生活&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;0.48020104
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h4 id=&#34;424-closer_thankey1-key2&#34;&gt;4.2.4 closer_than(key1, key2)&lt;/h4&gt;
&lt;p&gt;更接近于key1的词向量(相比于key2)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#获取所有更接近 `key1` 的键，而不是 `key2` 。&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;closer_than&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;理想&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                      &lt;span class=&#34;n&#34;&gt;key2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;现实&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;梦想&amp;#39;,
 &amp;#39;追求&amp;#39;,
 &amp;#39;实现&amp;#39;,
 &amp;#39;向往&amp;#39;,
 &amp;#39;信念&amp;#39;,
 &amp;#39;妥协&amp;#39;,
 &amp;#39;奋斗&amp;#39;,
 &amp;#39;乌托邦&amp;#39;,
 &amp;#39;愿望&amp;#39;,
 &amp;#39;理想主义&amp;#39;,
 &amp;#39;理想化&amp;#39;,
 &amp;#39;虚幻&amp;#39;,
 &amp;#39;憧憬&amp;#39;,
 &amp;#39;现实残酷&amp;#39;,
 &amp;#39;不切实际&amp;#39;,
 &amp;#39;实现梦想&amp;#39;,
 &amp;#39;崇高&amp;#39;,
 &amp;#39;理想主义者&amp;#39;,
 &amp;#39;追求自由&amp;#39;,
 &amp;#39;破灭&amp;#39;,
 &amp;#39;名利&amp;#39;,
 &amp;#39;追梦&amp;#39;,
 &amp;#39;奢望&amp;#39;,
 &amp;#39;追求梦想&amp;#39;,
 &amp;#39;现实现实&amp;#39;,
 &amp;#39;执著&amp;#39;,
 &amp;#39;理想现实&amp;#39;,
 &amp;#39;拼搏&amp;#39;,
 &amp;#39;面对现实&amp;#39;,
 &amp;#39;美好事物&amp;#39;,
 &amp;#39;追逐梦想&amp;#39;,
 &amp;#39;勇往直前&amp;#39;,
 &amp;#39;遥不可及&amp;#39;,
 &amp;#39;怀揣&amp;#39;,
 &amp;#39;梦想现实&amp;#39;,
 &amp;#39;美好生活&amp;#39;,
 &amp;#39;脚踏实地&amp;#39;,
 &amp;#39;本心&amp;#39;,
 &amp;#39;坚持梦想&amp;#39;,
 &amp;#39;梦想实现&amp;#39;,
 &amp;#39;青春梦想&amp;#39;,
 &amp;#39;热忱&amp;#39;,
 &amp;#39;空想&amp;#39;,
 &amp;#39;抱负&amp;#39;,
 &amp;#39;努力奋斗&amp;#39;,
 &amp;#39;美好幻想&amp;#39;,
 &amp;#39;务实&amp;#39;,
 &amp;#39;坚定信念&amp;#39;,
 &amp;#39;梦想努力&amp;#39;,
 &amp;#39;理想国&amp;#39;,
 &amp;#39;无法实现&amp;#39;,
 &amp;#39;美好愿望&amp;#39;,
 &amp;#39;理想生活&amp;#39;,
 &amp;#39;坚持自我&amp;#39;,
 &amp;#39;事业爱情&amp;#39;,
 &amp;#39;放弃梦想&amp;#39;,
 &amp;#39;愿景&amp;#39;,
 &amp;#39;自我价值&amp;#39;,
 &amp;#39;自我实现&amp;#39;,
 &amp;#39;现实面前&amp;#39;,
 &amp;#39;梦想坚持&amp;#39;,
 &amp;#39;梦想梦想&amp;#39;,
 &amp;#39;志向&amp;#39;,
 &amp;#39;乌托邦式&amp;#39;,
 &amp;#39;可能实现&amp;#39;,
 &amp;#39;追寻梦想&amp;#39;,
 &amp;#39;追求自我&amp;#39;,
 &amp;#39;追求理想&amp;#39;,
 &amp;#39;人生理想&amp;#39;,
 &amp;#39;追求完美&amp;#39;,
 &amp;#39;诗远方&amp;#39;,
 &amp;#39;梦想追求&amp;#39;,
 &amp;#39;追求艺术&amp;#39;,
 &amp;#39;执着追求&amp;#39;,
 &amp;#39;不断努力&amp;#39;,
 &amp;#39;怀揣梦想&amp;#39;,
 &amp;#39;儿时梦想&amp;#39;,
 &amp;#39;最初梦想&amp;#39;,
 &amp;#39;梦想奋斗&amp;#39;,
 &amp;#39;曾经梦想&amp;#39;,
 &amp;#39;美好向往&amp;#39;,
 &amp;#39;理想状态&amp;#39;,
 &amp;#39;现实妥协&amp;#39;,
 &amp;#39;实现理想&amp;#39;,
 &amp;#39;梦想执着&amp;#39;,
 &amp;#39;坚持理想&amp;#39;,
 &amp;#39;一个理想主义者&amp;#39;,
 &amp;#39;不切实际幻想&amp;#39;,
 &amp;#39;实现不了&amp;#39;,
 &amp;#39;努力追求&amp;#39;,
 &amp;#39;精神追求&amp;#39;,
 &amp;#39;现实打败&amp;#39;,
 &amp;#39;过于理想&amp;#39;,
 &amp;#39;美好憧憬&amp;#39;,
 &amp;#39;追寻自由&amp;#39;,
 &amp;#39;美好愿景&amp;#39;,
 &amp;#39;远大&amp;#39;,
 &amp;#39;梦想破灭&amp;#39;,
 &amp;#39;美好未来&amp;#39;,
 &amp;#39;最终实现&amp;#39;,
 &amp;#39;现实主义者&amp;#39;,
 &amp;#39;心中理想&amp;#39;,
 &amp;#39;努力实现&amp;#39;,
 &amp;#39;理想追求&amp;#39;,
 &amp;#39;理想丰满&amp;#39;,
 &amp;#39;难以实现&amp;#39;,
 &amp;#39;自由梦想&amp;#39;,
 &amp;#39;未竟&amp;#39;,
 &amp;#39;理想信念&amp;#39;,
 &amp;#39;追名逐利&amp;#39;,
 &amp;#39;崇尚自由&amp;#39;,
 &amp;#39;理想奋斗&amp;#39;,
 &amp;#39;摇滚梦&amp;#39;,
 &amp;#39;心中梦想&amp;#39;,
 &amp;#39;梦想追逐&amp;#39;,
 &amp;#39;崇高理想&amp;#39;,
 &amp;#39;爱与梦想&amp;#39;,
 &amp;#39;梦想放弃&amp;#39;,
 &amp;#39;自由理想&amp;#39;,
 &amp;#39;远大理想&amp;#39;,
 &amp;#39;革命理想&amp;#39;,
 &amp;#39;勇于追求&amp;#39;,
 &amp;#39;世俗成功&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h4 id=&#34;425-most_similarpositive-negative&#34;&gt;4.2.5 most_similar(positive, negative)&lt;/h4&gt;
&lt;p&gt;找出与positive同方向，与negative反向相反的词。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;positive&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;精彩&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;过瘾&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                       &lt;span class=&#34;n&#34;&gt;negative&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;垃圾&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                       &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;看得过瘾&amp;#39;, 0.7470669746398926),
 (&amp;#39;相当精彩&amp;#39;, 0.7082503437995911),
 (&amp;#39;带劲&amp;#39;, 0.6865044236183167),
 (&amp;#39;非常过瘾&amp;#39;, 0.6556571125984192),
 (&amp;#39;非常精彩&amp;#39;, 0.6555824875831604),
 (&amp;#39;够劲&amp;#39;, 0.6424692869186401),
 (&amp;#39;太精彩&amp;#39;, 0.6424689292907715),
 (&amp;#39;十分精彩&amp;#39;, 0.6388185024261475),
 (&amp;#39;足够精彩&amp;#39;, 0.6384131908416748),
 (&amp;#39;十分过瘾&amp;#39;, 0.6383010745048523)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;43-类比king-manwomanqueen&#34;&gt;4.3 类比king-man+woman~queen&lt;/h3&gt;
&lt;p&gt;每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。&lt;/p&gt;
&lt;p&gt;这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/king-queen-formular.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;这两个词相减，按感觉应该得到的是性别方向，雄性-&amp;gt;雌性。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;gender_direction_1 = vector(man)-vector(woman)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;gender_direction_2 = vector(king)-vector(queen)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;那两个性别方向应该近似，假设这里将其 &lt;em&gt;&lt;strong&gt;gender_direction_1=gender_direction_2&lt;/strong&gt;&lt;/em&gt; ，则对于公式中任意一个词，都可以由等式中的其他三个词经过运算得到。例如&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;vector(queen) = vector(king)-vector(man)+vector(woman)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;这里构造了一个 &lt;code&gt;北京a - 中国b~=  巴黎c - 某国d&lt;/code&gt; 的公式，计算如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 北京a - 中国b~=  巴黎c - 某国d&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;北京&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;中国&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;巴黎&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#d = b-a+c&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;中国&amp;#39;, 0.6384854912757874),
 (&amp;#39;法国&amp;#39;, 0.599371612071991),
 (&amp;#39;欧洲&amp;#39;, 0.5970593094825745),
 (&amp;#39;法国人&amp;#39;, 0.5338885188102722),
 (&amp;#39;欧洲人&amp;#39;, 0.5236572027206421),
 (&amp;#39;意大利&amp;#39;, 0.5203548669815063),
 (&amp;#39;西方&amp;#39;, 0.4940629303455353),
 (&amp;#39;亚洲&amp;#39;, 0.4907427728176117),
 (&amp;#39;美国&amp;#39;, 0.490087628364563),
 (&amp;#39;欧美&amp;#39;, 0.48989546298980713)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;大概是跑出了我们预期的 &lt;strong&gt;法国&lt;/strong&gt;， 但不够Perfect， 有些遗憾。 毕竟语料是影评，且讨论环境不够正式， 豆瓣用户没那么多心思研究地理和政治，所以网络记忆不全不准。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五获取数据&#34;&gt;五、获取数据&lt;/h2&gt;
&lt;h3 id=&#34;51-获取影评数据&#34;&gt;5.1 获取影评数据&lt;/h3&gt;
&lt;p&gt;除了本文介绍的这个 1000w 条影评数据集， 大邓还有2个类似的豆瓣影评数据集，影评记录量 212w和442 w 条。 两个数据集下载链接我都公开，感兴趣的可以都下载下来。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;douba-movie-1000w&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/1NHttdosb0VZUQV7Tg7MHXw?pwd=rndk&#34;&gt;https://pan.baidu.com/s/1NHttdosb0VZUQV7Tg7MHXw?pwd=rndk&lt;/a&gt; 提取码: rndk&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;douban-movie-442w&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/10KK5FrGL0ZHx4wiuhlvuXw?pwd=db7m&#34;&gt;https://pan.baidu.com/s/10KK5FrGL0ZHx4wiuhlvuXw?pwd=db7m&lt;/a&gt; 提取码: db7m&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;【douban-movie-442w介绍】

采集时间: 
   - 电影&amp;amp;明星 2019年8月上旬
   - 影评(用户、评分、评论) 2019年9月初

记录数:
   - 电影 140502 部
   - 演员 72959 人
   - 影评 4428475 条
   - 评分 4169420 条
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;douban-movie-212w&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/1iCKGu_6zTe6ZhlB_9Bf1HA?pwd=cv2p&#34;&gt;https://pan.baidu.com/s/1iCKGu_6zTe6ZhlB_9Bf1HA?pwd=cv2p&lt;/a&gt; 提取码: cv2p&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;52-cntext211&#34;&gt;5.2 cntext2.1.1&lt;/h3&gt;
&lt;p&gt;cntext2.1.1 是非公开内容， &lt;strong&gt;100元&lt;/strong&gt;  可得 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt;  ， 加微信 &lt;em&gt;&lt;strong&gt;372335839&lt;/strong&gt;&lt;/em&gt;， 备注「姓名-学校-专业」&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;53-word2vec模型文件&#34;&gt;5.3 Word2Vec模型文件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;douba-movie-1000w.200.6.bin&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/1ahbYq2IOqUA_AE0T3XIb9g?pwd=su1y&#34;&gt;https://pan.baidu.com/s/1ahbYq2IOqUA_AE0T3XIb9g?pwd=su1y&lt;/a&gt; 提取码: su1y&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;douban-movie-442w.200.6.bin&lt;/strong&gt;&lt;/em&gt;  链接: &lt;a href=&#34;https://pan.baidu.com/s/181eVuM0qldUJ53i7u1a5vA?pwd=uarj&#34;&gt;https://pan.baidu.com/s/181eVuM0qldUJ53i7u1a5vA?pwd=uarj&lt;/a&gt; 提取码: uarj&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;douban-movie-212w200.6.bin&lt;/strong&gt;&lt;/em&gt; 链接: &lt;a href=&#34;https://pan.baidu.com/s/1bvIZAM4zqX_35WHrBJSFUg?pwd=mf9u&#34;&gt;https://pan.baidu.com/s/1bvIZAM4zqX_35WHrBJSFUg?pwd=mf9u&lt;/a&gt; 提取码: mf9u&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/&#34;&gt;数据集 | 3394w条豆瓣书评数据集&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>本文内容</p>
<ol>
<li>介绍豆瓣影评数据集</li>
<li>构造语料训练Word2Vec模型</li>
<li>获取数据&amp;cntext&amp;Word2Vec模型文件</li>
</ol>
<p><br><br></p>
<h2 id="一豆瓣影评数据集">一、豆瓣影评数据集</h2>
<h3 id="11-数据集介绍">1.1 数据集介绍</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据集: douba-movie-1000w

数据源: 豆瓣电影
   
记录数:
   - 电影 10269 部
   - 影评 10310989 条
   
体积: 1.35G 
</code></pre></div><p>该数据集正好弥补下国内公开电影数据集的空缺， 数据已经过初步清洗，可用于推荐系统、情感分析、知识图谱、新闻传播学、社会学文化变迁等多个领域(或主题)。</p>
<br>
<h3 id="12-读取数据">1.2 读取数据</h3>
<p>下载 <em><strong>douba-movie-1000w.zip</strong></em> 解压后，可以看到数据集中有一个 <em><strong>all_movies_with_id.csv</strong></em> 文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;all_movies_with_id.csv&#39;</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<br>
<h3 id="13-所含字段">1.3 所含字段</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39; - </span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> - ID
 - Movie_Name  电影名
 - Score  豆瓣电影评分(1-10)
 - Review_People  评论者人数
 - Star_Distribution  评论评分分布(1-5, 含多个数值，数值以%间隔)
 - Craw_Date 爬虫运行日期
 - Username 豆瓣评论者用户名
 - Date 影评日期
 - Star  影评评分(1-5)
 - Comment 影评内容
 - Comment_Distribution 影评评分分布
 - Like 影评获得的喜欢数
</code></pre></div><p><br><br></p>
<h2 id="二-构造语料训练word2vec">二、 构造语料&amp;训练Word2Vec</h2>
<h3 id="21-构造语料">2.1 构造语料</h3>
<p>将字段 <em><strong>Comment</strong></em> 中所有文本汇总到 <em><strong>douban-movie-1000w.txt</strong></em>,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;douban-movie-1000w.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Comment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="22-配置cntext211">2.2 配置cntext2.1.1</h3>
<p>将 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
</code></pre></div><p>文末有 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 获取方式</p>
<br>
<h3 id="23-训练word2vec">2.3 训练Word2Vec</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cntext为2.1.1</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;douban-movie-1000w.txt&#39;</span><span class="p">,</span>
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>

<span class="n">w2v_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Starting Preprocessing Corpus ...
Starting Training! This may take a while.Please be patient...
Traning word2vec model took 3965 seconds
Note: The Word2Vec model hase saved to output/Word2Vec
</code></pre></div><p><img loading="lazy" src="img/word2vec.png" alt=""  />
</p>
<p>经过大概一个小时的训练， 得到模型文件 <em><strong>douban-movie-1000w.200.6.bin</strong></em> 及相关文件， 注意不要删掉哦。 已训练好的模型，可以自己用， 也可分享给其他人使用。</p>
<p><br><br></p>
<h2 id="四使用word2vec">四、使用Word2Vec</h2>
<h3 id="41-导入word2vec模型文件">4.1 导入Word2Vec模型文件</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
 
<span class="c1">#导入模型，请注意路径。</span>
<span class="c1"># 【当前代码】 与 【Word2Vec文件夹】 同处于一个文件夹内</span>
<span class="n">dm_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;Word2Vec/douban-movie-1000w.200.6.bin&#39;</span><span class="p">)</span>
<span class="n">dm_w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading word2vec model...
&lt;gensim.models.word2vec.Word2Vec at 0x10cb02090&gt;
</code></pre></div><br>
<h3 id="42-常用函数">4.2 常用函数</h3>
<ul>
<li>
<p><em><strong>dm_w2v.wv.get_vector(key)</strong></em> 获取key的词向量</p>
</li>
<li>
<p><em><strong>dm_w2v.most_similar_to_given(key1, keys_list)</strong></em> 从 keys_list 中获取与 key1 最相似的词</p>
</li>
<li>
<p><em><strong>dm_w2v.n_similarity(ws1, ws2)</strong></em>  两组词ws1, ws2 的相似度</p>
</li>
<li>
<p><em><strong>dm_w2v.closer_than(key1, key2)</strong></em>  更接近于key1的词向量(相比于key2)</p>
</li>
<li>
<p><em><strong>dm_w2v.most_similar(positive, negative)</strong></em>  找出与positive同方向，与negative反向相反的词。</p>
</li>
</ul>
<br>
<h4 id="421-get_vectorkey">4.2.1 get_vector(key)</h4>
<p>使用词向量查看某</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">3.55084002e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.22685611e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.48365605e-01</span><span class="p">,</span>  <span class="mf">1.23056602e+00</span><span class="p">,</span>
        <span class="mf">1.35057056e+00</span><span class="p">,</span>  <span class="mf">1.65976137e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.26512849e+00</span><span class="p">,</span>  <span class="mf">1.47152972e+00</span><span class="p">,</span>
        <span class="mf">9.99028236e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.00873756e+00</span><span class="p">,</span>  <span class="mf">1.05153358e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.39181948e+00</span><span class="p">,</span>
        <span class="mf">6.02373898e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.00308895e+00</span><span class="p">,</span>  <span class="mf">2.33978868e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.83010173e+00</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">9.67333555e-01</span><span class="p">,</span>  <span class="mf">3.04877937e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.59058094e-01</span><span class="p">,</span>  <span class="mf">3.19660306e+00</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">1.21165246e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.68000716e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.36653373e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.83727741e-01</span><span class="p">,</span>
      <span class="o">......</span>
      <span class="o">......</span>
       <span class="o">-</span><span class="mf">1.23901594e+00</span><span class="p">,</span>  <span class="mf">5.07202707e-02</span><span class="p">,</span>  <span class="mf">8.75848413e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.31963325e-01</span><span class="p">,</span>
        <span class="mf">1.31377324e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.19606090e+00</span><span class="p">,</span>  <span class="mf">1.68391216e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.27069890e-01</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">7.37121344e-01</span><span class="p">,</span>  <span class="mf">2.49946609e-01</span><span class="p">,</span>  <span class="mf">1.47220814e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.33507824e+00</span><span class="p">,</span>
        <span class="mf">2.97913142e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.91593599e-01</span><span class="p">,</span>  <span class="mf">5.83192170e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.48378658e-01</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">3.30877733e+00</span><span class="p">,</span>  <span class="mf">2.17747837e-01</span><span class="p">,</span>  <span class="mf">2.22701088e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.00758147e+00</span><span class="p">,</span>
        <span class="mf">3.41430195e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.27023900e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.94953525e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.03226733e+00</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">4.55965906e-01</span><span class="p">,</span>  <span class="mf">1.66779244e+00</span><span class="p">,</span>  <span class="mf">1.16857982e+00</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.02211344e+00</span><span class="p">,</span>
        <span class="mf">4.11061406e-01</span><span class="p">,</span>  <span class="mf">8.95921767e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">9.48565483e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.48802996e-01</span><span class="p">,</span>
        <span class="mf">9.36261594e-01</span><span class="p">,</span>  <span class="mf">3.98367733e-01</span><span class="p">,</span>  <span class="mf">3.12385857e-01</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.67059827e-01</span><span class="p">],</span>
      <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</code></pre></div><br>
<h4 id="422-most_similar_to_givenkey1-keys_list">4.2.2 most_similar_to_given(key1, keys_list)</h4>
<p>从 keys_list 中获取与 key1 最相似的词。例如在 1000w 影评中，从<code>'爱情', '悬疑', '飞船', '历史', '战争'</code>找出最接近<code>'太空'</code>，最后返回<code>'飞船'</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#从 `keys_list` 中获取与 `key1` 最相似的 `key`。</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar_to_given</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;太空&#39;</span><span class="p">,</span> 
                                <span class="n">keys_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;爱情&#39;</span><span class="p">,</span> <span class="s1">&#39;悬疑&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;历史&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&#39;飞船&#39;
</code></pre></div><br>
<h4 id="423-w2v_modeln_similarityws1-ws2">4.2.3 w2v_model.n_similarity(ws1, ws2)</h4>
<p>两组词ws1, ws2 的相似度。注意相似值更多的是体现了语义的相关性， 并不能准确反映语义的远近。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="n">cosine_similarity</span><span class="p">([</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;理想&#39;</span><span class="p">)],</span>  
                  <span class="p">[</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;现实&#39;</span><span class="p">)])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.4698379
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cosine算法</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.4698379
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#计算两组键之间的余弦相似度。</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;赞&#39;</span><span class="p">,</span> <span class="s1">&#39;推荐&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;无聊&#39;</span><span class="p">,</span> <span class="s1">&#39;尴尬&#39;</span><span class="p">,</span> <span class="s1">&#39;垃圾&#39;</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.109311774
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> <span class="s1">&#39;梦想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">,</span> <span class="s1">&#39;生活&#39;</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.48020104
</code></pre></div><br>
<h4 id="424-closer_thankey1-key2">4.2.4 closer_than(key1, key2)</h4>
<p>更接近于key1的词向量(相比于key2)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取所有更接近 `key1` 的键，而不是 `key2` 。</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">closer_than</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> 
                      <span class="n">key2</span><span class="o">=</span><span class="s1">&#39;现实&#39;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;梦想&#39;,
 &#39;追求&#39;,
 &#39;实现&#39;,
 &#39;向往&#39;,
 &#39;信念&#39;,
 &#39;妥协&#39;,
 &#39;奋斗&#39;,
 &#39;乌托邦&#39;,
 &#39;愿望&#39;,
 &#39;理想主义&#39;,
 &#39;理想化&#39;,
 &#39;虚幻&#39;,
 &#39;憧憬&#39;,
 &#39;现实残酷&#39;,
 &#39;不切实际&#39;,
 &#39;实现梦想&#39;,
 &#39;崇高&#39;,
 &#39;理想主义者&#39;,
 &#39;追求自由&#39;,
 &#39;破灭&#39;,
 &#39;名利&#39;,
 &#39;追梦&#39;,
 &#39;奢望&#39;,
 &#39;追求梦想&#39;,
 &#39;现实现实&#39;,
 &#39;执著&#39;,
 &#39;理想现实&#39;,
 &#39;拼搏&#39;,
 &#39;面对现实&#39;,
 &#39;美好事物&#39;,
 &#39;追逐梦想&#39;,
 &#39;勇往直前&#39;,
 &#39;遥不可及&#39;,
 &#39;怀揣&#39;,
 &#39;梦想现实&#39;,
 &#39;美好生活&#39;,
 &#39;脚踏实地&#39;,
 &#39;本心&#39;,
 &#39;坚持梦想&#39;,
 &#39;梦想实现&#39;,
 &#39;青春梦想&#39;,
 &#39;热忱&#39;,
 &#39;空想&#39;,
 &#39;抱负&#39;,
 &#39;努力奋斗&#39;,
 &#39;美好幻想&#39;,
 &#39;务实&#39;,
 &#39;坚定信念&#39;,
 &#39;梦想努力&#39;,
 &#39;理想国&#39;,
 &#39;无法实现&#39;,
 &#39;美好愿望&#39;,
 &#39;理想生活&#39;,
 &#39;坚持自我&#39;,
 &#39;事业爱情&#39;,
 &#39;放弃梦想&#39;,
 &#39;愿景&#39;,
 &#39;自我价值&#39;,
 &#39;自我实现&#39;,
 &#39;现实面前&#39;,
 &#39;梦想坚持&#39;,
 &#39;梦想梦想&#39;,
 &#39;志向&#39;,
 &#39;乌托邦式&#39;,
 &#39;可能实现&#39;,
 &#39;追寻梦想&#39;,
 &#39;追求自我&#39;,
 &#39;追求理想&#39;,
 &#39;人生理想&#39;,
 &#39;追求完美&#39;,
 &#39;诗远方&#39;,
 &#39;梦想追求&#39;,
 &#39;追求艺术&#39;,
 &#39;执着追求&#39;,
 &#39;不断努力&#39;,
 &#39;怀揣梦想&#39;,
 &#39;儿时梦想&#39;,
 &#39;最初梦想&#39;,
 &#39;梦想奋斗&#39;,
 &#39;曾经梦想&#39;,
 &#39;美好向往&#39;,
 &#39;理想状态&#39;,
 &#39;现实妥协&#39;,
 &#39;实现理想&#39;,
 &#39;梦想执着&#39;,
 &#39;坚持理想&#39;,
 &#39;一个理想主义者&#39;,
 &#39;不切实际幻想&#39;,
 &#39;实现不了&#39;,
 &#39;努力追求&#39;,
 &#39;精神追求&#39;,
 &#39;现实打败&#39;,
 &#39;过于理想&#39;,
 &#39;美好憧憬&#39;,
 &#39;追寻自由&#39;,
 &#39;美好愿景&#39;,
 &#39;远大&#39;,
 &#39;梦想破灭&#39;,
 &#39;美好未来&#39;,
 &#39;最终实现&#39;,
 &#39;现实主义者&#39;,
 &#39;心中理想&#39;,
 &#39;努力实现&#39;,
 &#39;理想追求&#39;,
 &#39;理想丰满&#39;,
 &#39;难以实现&#39;,
 &#39;自由梦想&#39;,
 &#39;未竟&#39;,
 &#39;理想信念&#39;,
 &#39;追名逐利&#39;,
 &#39;崇尚自由&#39;,
 &#39;理想奋斗&#39;,
 &#39;摇滚梦&#39;,
 &#39;心中梦想&#39;,
 &#39;梦想追逐&#39;,
 &#39;崇高理想&#39;,
 &#39;爱与梦想&#39;,
 &#39;梦想放弃&#39;,
 &#39;自由理想&#39;,
 &#39;远大理想&#39;,
 &#39;革命理想&#39;,
 &#39;勇于追求&#39;,
 &#39;世俗成功&#39;]
</code></pre></div><br>
<h4 id="425-most_similarpositive-negative">4.2.5 most_similar(positive, negative)</h4>
<p>找出与positive同方向，与negative反向相反的词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;过瘾&#39;</span><span class="p">],</span>
                       <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;垃圾&#39;</span><span class="p">],</span>
                       <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;看得过瘾&#39;, 0.7470669746398926),
 (&#39;相当精彩&#39;, 0.7082503437995911),
 (&#39;带劲&#39;, 0.6865044236183167),
 (&#39;非常过瘾&#39;, 0.6556571125984192),
 (&#39;非常精彩&#39;, 0.6555824875831604),
 (&#39;够劲&#39;, 0.6424692869186401),
 (&#39;太精彩&#39;, 0.6424689292907715),
 (&#39;十分精彩&#39;, 0.6388185024261475),
 (&#39;足够精彩&#39;, 0.6384131908416748),
 (&#39;十分过瘾&#39;, 0.6383010745048523)]
</code></pre></div><br>
<h3 id="43-类比king-manwomanqueen">4.3 类比king-man+woman~queen</h3>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。</p>
<p>这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p><img loading="lazy" src="img/king-queen-formular.png" alt=""  />
</p>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p><em><strong>gender_direction_1 = vector(man)-vector(woman)</strong></em></p>
<p><em><strong>gender_direction_2 = vector(king)-vector(queen)</strong></em></p>
<p>那两个性别方向应该近似，假设这里将其 <em><strong>gender_direction_1=gender_direction_2</strong></em> ，则对于公式中任意一个词，都可以由等式中的其他三个词经过运算得到。例如</p>
<p><em><strong>vector(queen) = vector(king)-vector(man)+vector(woman)</strong></em></p>
<p>这里构造了一个 <code>北京a - 中国b~=  巴黎c - 某国d</code> 的公式，计算如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 北京a - 中国b~=  巴黎c - 某国d</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;北京&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;中国&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;巴黎&#39;</span><span class="p">)</span>

<span class="c1">#d = b-a+c</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="n">a</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;中国&#39;, 0.6384854912757874),
 (&#39;法国&#39;, 0.599371612071991),
 (&#39;欧洲&#39;, 0.5970593094825745),
 (&#39;法国人&#39;, 0.5338885188102722),
 (&#39;欧洲人&#39;, 0.5236572027206421),
 (&#39;意大利&#39;, 0.5203548669815063),
 (&#39;西方&#39;, 0.4940629303455353),
 (&#39;亚洲&#39;, 0.4907427728176117),
 (&#39;美国&#39;, 0.490087628364563),
 (&#39;欧美&#39;, 0.48989546298980713)]
</code></pre></div><p>大概是跑出了我们预期的 <strong>法国</strong>， 但不够Perfect， 有些遗憾。 毕竟语料是影评，且讨论环境不够正式， 豆瓣用户没那么多心思研究地理和政治，所以网络记忆不全不准。</p>
<p><br><br></p>
<h2 id="五获取数据">五、获取数据</h2>
<h3 id="51-获取影评数据">5.1 获取影评数据</h3>
<p>除了本文介绍的这个 1000w 条影评数据集， 大邓还有2个类似的豆瓣影评数据集，影评记录量 212w和442 w 条。 两个数据集下载链接我都公开，感兴趣的可以都下载下来。</p>
<ul>
<li>
<p><em><strong>douba-movie-1000w</strong></em> 链接: <a href="https://pan.baidu.com/s/1NHttdosb0VZUQV7Tg7MHXw?pwd=rndk">https://pan.baidu.com/s/1NHttdosb0VZUQV7Tg7MHXw?pwd=rndk</a> 提取码: rndk</p>
</li>
<li>
<p><em><strong>douban-movie-442w</strong></em> 链接: <a href="https://pan.baidu.com/s/10KK5FrGL0ZHx4wiuhlvuXw?pwd=db7m">https://pan.baidu.com/s/10KK5FrGL0ZHx4wiuhlvuXw?pwd=db7m</a> 提取码: db7m</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">【douban-movie-442w介绍】

采集时间: 
   - 电影&amp;明星 2019年8月上旬
   - 影评(用户、评分、评论) 2019年9月初

记录数:
   - 电影 140502 部
   - 演员 72959 人
   - 影评 4428475 条
   - 评分 4169420 条
</code></pre></div><ul>
<li><em><strong>douban-movie-212w</strong></em> 链接: <a href="https://pan.baidu.com/s/1iCKGu_6zTe6ZhlB_9Bf1HA?pwd=cv2p">https://pan.baidu.com/s/1iCKGu_6zTe6ZhlB_9Bf1HA?pwd=cv2p</a> 提取码: cv2p</li>
</ul>
<p><br><br></p>
<h3 id="52-cntext211">5.2 cntext2.1.1</h3>
<p>cntext2.1.1 是非公开内容， <strong>100元</strong>  可得 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em>  ， 加微信 <em><strong>372335839</strong></em>， 备注「姓名-学校-专业」</p>
<br>
<h3 id="53-word2vec模型文件">5.3 Word2Vec模型文件</h3>
<ul>
<li><em><strong>douba-movie-1000w.200.6.bin</strong></em> 链接: <a href="https://pan.baidu.com/s/1ahbYq2IOqUA_AE0T3XIb9g?pwd=su1y">https://pan.baidu.com/s/1ahbYq2IOqUA_AE0T3XIb9g?pwd=su1y</a> 提取码: su1y</li>
<li><em><strong>douban-movie-442w.200.6.bin</strong></em>  链接: <a href="https://pan.baidu.com/s/181eVuM0qldUJ53i7u1a5vA?pwd=uarj">https://pan.baidu.com/s/181eVuM0qldUJ53i7u1a5vA?pwd=uarj</a> 提取码: uarj</li>
<li><em><strong>douban-movie-212w200.6.bin</strong></em> 链接: <a href="https://pan.baidu.com/s/1bvIZAM4zqX_35WHrBJSFUg?pwd=mf9u">https://pan.baidu.com/s/1bvIZAM4zqX_35WHrBJSFUg?pwd=mf9u</a> 提取码: mf9u</li>
</ul>
<br>
<br>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/">数据集 | 3394w条豆瓣书评数据集</a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>以聚类为例 | 使用大语言模型LLM做文本分析</title>
      <link>https://textdata.cn/blog/2023-11-20-how-to-use-llms-tobuild-better-clustering-models/</link>
      <pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-20-how-to-use-llms-tobuild-better-clustering-models/</guid>
      <description>&lt;p&gt;本文主要分享&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;传统聚类算法&lt;/li&gt;
&lt;li&gt;LLM与嵌入算法&lt;/li&gt;
&lt;li&gt;嵌入算法聚类&lt;/li&gt;
&lt;li&gt;启发； LLM的其他用法&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;p&gt;聚类是一种无监督机器学习技术，旨在根据相似的数据点的特征将其分组在一起。使用聚类成簇，有助于解决各种问题，例如客户细分、异常检测和文本分类等。尽管传统的聚类技术被广泛使用，但它仍然面临着挑战。 今天代码很少，也没有实验数据， 主要是偏思路分享。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一编码挑战&#34;&gt;一、编码挑战&lt;/h2&gt;
&lt;h3 id=&#34;11--字段单位不统一&#34;&gt;1.1  字段单位不统一&lt;/h3&gt;
&lt;p&gt;我想在本文中解决的主要挑战是选择如何编码或转换输入特征。一般来说，您需要将每个特征转换为相同的比例，否则，聚类模型将在特征之间分配不成比例的权重。例如， 假设数据中有重量 &lt;strong&gt;weight1&lt;/strong&gt; 、 &lt;strong&gt;weight2&lt;/strong&gt;  两个字段，weight1单位是市斤，而weight2单位是公斤。如果不首先对这些测量进行标准化，即使实际重量相同，我们的模型也会推断出以市斤为单位（对于类似重量的物体）测量的重量差异大于以公斤为单位的差异。&lt;/p&gt;
&lt;p&gt;现实中，数据集中不会出现对一个信息使用两种单位进行度量。使用这个例子， 只为说明数据中不同字段分布不同，训练模型时不同字段承载的权重也不一样。为了减轻这个问题，一般是训练之前先将字段标准化。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;12-字段之间存在相关性&#34;&gt;1.2 字段之间存在相关性&lt;/h3&gt;
&lt;p&gt;让我们使用颜色组成的特征作为另一个示例。通常，许多人会选择将此特征 one-hot 编码到 n-1 个附加列中，其中 n 是唯一颜色的数量。虽然这有效，但它忽略了颜色之间的任何潜在关系。&lt;/p&gt;
&lt;p&gt;为什么是这样？让我们考虑数据集中的一个特征具有以下颜色：红色、栗色、深红色、猩红色和绿色。如果我们要对该列进行 one-hot 编码，我们将得到一个如下所示的数据帧：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-color.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;在 &lt;strong&gt;欧几里德距离空间&lt;/strong&gt; 中，任意两个记录(行)之间的距离是相同的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;import numpy as np

def euclidean_distance(vec1, vec2):
    if len(vec1) != len(vec2):
        raise ValueError(&amp;#34;vecs must have the same length.&amp;#34;)
        
    squared_differences = [(a - b) ** 2 for a, b in zip(vec1, vec2)]
    distance = np.sqrt(sum(squared_differences))
    return distance
    
red = np.array([0, 0, 0, 1, 0])
maroon = np.array([0, 0, 1, 0, 0])
green = np.array([0, 1, 0, 0, 0])

print(euclidean_distance(red, maroon))
print(euclidean_distance(red, green))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;1.4142135623730951
1.4142135623730951
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h2 id=&#34;二有更好的办法吗&#34;&gt;二、有更好的办法吗？&lt;/h2&gt;
&lt;p&gt;当然， &lt;strong&gt;红色&lt;/strong&gt; 和 &lt;strong&gt;栗色&lt;/strong&gt; 是两种不同的颜色，但为了我们的聚类算法，我们其实不希望euclidean_distance(red, maroon) 与 euclidean_distance(red, green) 是相等的。&lt;/p&gt;
&lt;p&gt;那么该如何解决这个缺点呢？&lt;/p&gt;
&lt;p&gt;如果您阅读这篇文章的标题，我相信您可能已经get到本文的ieda……我们将结合 &lt;strong&gt;大语言模型&lt;/strong&gt; (Large language model, LLM)， 将每条记录字段和数值整理成一个字符串， 并通过LLM获得每条记录对应的嵌入表示。&lt;/p&gt;
&lt;p&gt;对于此示例，我将使用 Huggingface 中的句子转换器库以及我围绕工作申请综合创建的数据集。&lt;/p&gt;
&lt;p&gt;让我们从句子转换器开始。该 LLM 的工作原理与 BERT 类似，只不过它经过专门训练以在句子级别而不是单词或标记级别输出嵌入。这些句子级嵌入可以更好地捕获含义，并且计算速度更快。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sentence_transformers&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SentenceTransformer&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sentence_transformers.util&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cos_sim&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#使用hugginface，需要科学上网&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SentenceTransformer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;r&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sentence-transformers/paraphrase-MiniLM-L6-v2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;prompt_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#每条记录整合为一个字符串&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;p_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
        &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Age: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; Gender: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Gender&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lower&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; Role: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; &amp;#34;&lt;/span&gt;
        &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Hiring Department: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;HiringDepartment&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; &amp;#34;&lt;/span&gt;
        &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Travel Preference: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;TravelPreference&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; Extracurriculars: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ExtraCurriculars&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; &amp;#34;&lt;/span&gt;
        &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Distance From Home: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;DistanceFromHome&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; &amp;#34;&lt;/span&gt;
        &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Internships: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Internships&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; Education Level: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;EducationLevel&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; Education Field: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;EducationField&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; &amp;#34;&lt;/span&gt;
        &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Summary: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Summary&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; 
    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p_text&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;output_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#返回的嵌入表示的尺寸(记录数, 384)&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#sentence-transformers/paraphrase-MiniLM-L6-v2 模型的词向量维度是384&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;embd&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reshape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;384&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;preprocess_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;embd&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;combined_text&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;preprocess_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们的数据集包括有关求职者的信息，例如招聘部门、职位、年龄和教育水平等特征。这是一个数据截图：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;我们的目标是将所有求职者分为不同的簇(可以理解为群体)。&lt;/p&gt;
&lt;p&gt;让我们看看如何将句子嵌入应用于每个求职者。第一步是通过将所有功能连接到一个字符串中来创建单个文本prompt。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Age: 28.
Gender: male.
Role: Research Scientist.
Hiring Department: Research &amp;amp; Development.
Travel Preference: Travel_Frequently.
Extracurriculars: nan.
Distance From Home: 4.
Internships: 9.
Education Level: 3.
Education Field: Engineering.
Summary: As you can see, I am very dedicated and I am ready to start at your firm immediately.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;将原记录(行)转为如上图所示的文本，之后调用 SBERT LLM 检索文本对应的嵌入向量。为方便展示，这里使用 dataframe.style 功能来突出显示低值和大值，以使表格更容易扫描：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-df-style.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;三用嵌入编码有什么益处&#34;&gt;三、用嵌入编码有什么益处？&lt;/h2&gt;
&lt;p&gt;之前讲了传统聚类算法使用one-hot编码方式的不足，但没有解释用嵌入表示的益处。 先不讲理论， 就像探索颜色编码，我们看一个例子。 我想测量 &lt;strong&gt;Role&lt;/strong&gt; (岗位角色) 的相似程度， 我更倾向于用余弦相似度，而不是欧几里德距离， 请问这其中的差异是？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;欧几里得距离&lt;/strong&gt; 是两点之间几何距离的度量，而 &lt;strong&gt;余弦相似度&lt;/strong&gt; 度量向量的方向。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;欧几里得距离对向量的大小敏感&lt;/strong&gt;，而余弦相似度则不然。&lt;/li&gt;
&lt;li&gt;欧氏距离的值范围从 0（相同向量）到无穷大，而 &lt;strong&gt;余弦相似度的范围从 -1（完全不相似）到 1（完全相似）&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;让我们选择两个岗位角色：&lt;strong&gt;销售代表&lt;/strong&gt;（sales representative）和&lt;strong&gt;销售主管&lt;/strong&gt;(sales executive)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;使用 one-hot 编码的 销售代表 和 销售主管 的余弦相似度为 0.5，这意味着他们&lt;strong&gt;有些相关&lt;/strong&gt;。这是有道理的，因为他们都是销售角色。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用嵌入编码的余弦相似度为 0.82。&lt;strong&gt;它们的相关性要高得多&lt;/strong&gt;。这更有意义，因为销售代表和销售主管在实践中是极其相似的角色。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;31--传统的聚类&#34;&gt;3.1  传统的聚类&lt;/h3&gt;
&lt;p&gt;传统聚类算法大致流程如下图所示，&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/06-traditional-process.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;原文作者实验使用K=3的聚类算法，但k如何设置不是最关键的点。 我们的聚类模型中最重要的字段是求职者的&lt;strong&gt;个人总结&lt;/strong&gt;（Summary），其次是 &lt;strong&gt;招聘部门&lt;/strong&gt;（HiringDepartment）、&lt;strong&gt;是否喜欢旅行&lt;/strong&gt;(TravelPreference)。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-weight.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;为了更好的理解3个簇， 我们输出了数据汇总，每个数值字段平均值 及 非数值字段的高频项。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-stats.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;按道理聚类算法的结果应该不同簇之间的差异尽可能的大。糟糕的是不同簇之间的， 年龄(Age)、实习次数(Internships) 差异很小，而更糟糕的是招聘部门(HiringDepartment) 和 岗位角色(Role) 完全相同。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-嵌入的聚类&#34;&gt;3.2 嵌入的聚类&lt;/h3&gt;
&lt;p&gt;使用嵌入编码的聚类算法流程如下图所示。与传统 聚类方法相比，使用嵌入的流程只需处理数字特征， 因为由求职者提示信息(代码里的prompt_text)转化来的嵌入是严格数字化的。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/07-emb-process.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;在这里，我们不能像上次那样直接计算字段重要性。我们有数百个难以理解的特征，它们的重要性各不相同，我们无法理解。那么我们该怎么办？让我们训练另一个模型（这次是有监督的三类分类模型），使用原始特征集来预测嵌入模型生成的类标签。这样就可以以同类的方式重现字段重要性。结果如下&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/08-weight.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;我们找到一种新的嵌入表示来编码求职者信息， 并运算出了聚类结果。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/09-statas.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;从统计信息(上图)中可以看出，不同簇之间的差异变的更加清晰。 使用嵌入编码， 让更多申请销售岗位的的销售主管划分到cluster2， 让更多申请研发岗位的的科学家划分到cluster1 和 cluster3.&lt;/p&gt;
&lt;br&gt;
&lt;blockquote&gt;
&lt;p&gt;前文内容翻译整理自&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/@swansburg.justin/how-to-use-llms-to-build-better-clustering-models-9b17a5491bb4&#34;&gt;https://medium.com/@swansburg.justin/how-to-use-llms-to-build-better-clustering-models-9b17a5491bb4&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四启发&#34;&gt;四、启发&lt;/h2&gt;
&lt;p&gt;读完以上内容，大邓想到一个问题， 假设 没有简历系统，没有大数据，求职者与面试官坐在现场，  数据就是面试过程中的交流， 而交流必然通过话语这一媒介。 例如求职者的个人信息&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;“大家好，我叫张三， 今年24岁，哈尔滨人。本科毕业于哈尔滨工业大学，市场营销专业。 我是一个很外向的人，对销售很感兴趣，在大学期间摆了很多地摊。很希望获得贵公司的机会，让我在营销岗位上大发异彩。”
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;面试期间，记录人员将该哈尔滨张三的个人信息被整理为&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;name: 张三
age: 24
city: 哈尔滨
edu: 哈尔滨工业大学
major: 市场营销
experience: 摆摊
summary: 我是外向的人，对销售很感兴趣。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;求职者的信息汇总成xlsx， 每个人的信息都或多或少的被压缩了。 这种表示方式， 在小规模时， 求职者的总结summary还是有很大信息量的，能够让面试者回忆起当时的场景和情景。但是当求职者的规模上升到几千上万， 备注note信息这种很重要的信息反而无法利用。&lt;/p&gt;
&lt;p&gt;使用大语言模型LLM，将文本提示转化为嵌入表示。我们可以将LLM看成是一个察言观色，见微知著，明察秋毫的智者。  这个智者可以&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分类&lt;/li&gt;
&lt;li&gt;提取信息&lt;/li&gt;
&lt;li&gt;补全&lt;/li&gt;
&lt;li&gt;相似性&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以往缺失数据， 用插值或者其他技巧， 现在我们可以借助LLM， 只有有其他字段残存的微弱线索， LLM就能帮我们补全缺失值。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;41-分类&#34;&gt;4.1 分类&lt;/h3&gt;
&lt;p&gt;如图所示， 对于很多短文本， 我们可以推断话题，也可以推断情绪。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;https://huggingface.co/morit/chinese_xlm_xnli
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/10-classification.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/11-classification.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h3 id=&#34;42-提取信息&#34;&gt;4.2 提取信息&lt;/h3&gt;
&lt;p&gt;假设有一些信息存储在文本中， 可以用正则表达式提取， 下面的例子用正则会很难设计， 但用LLM很简单。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;https://huggingface.co/luhua/chinese_pretrain_mrc_roberta_wwm_ext_large
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/12-extract.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;43-补全&#34;&gt;4.3 补全&lt;/h3&gt;
&lt;p&gt;填充缺失值信息&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/13-mask.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;44-相似性&#34;&gt;4.4 相似性&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/14-sim.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;当然LLM功能还有很多，大家可以自己探索探索&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/15-func.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>本文主要分享</p>
<ol>
<li>传统聚类算法</li>
<li>LLM与嵌入算法</li>
<li>嵌入算法聚类</li>
<li>启发； LLM的其他用法</li>
</ol>
<br>
<p>聚类是一种无监督机器学习技术，旨在根据相似的数据点的特征将其分组在一起。使用聚类成簇，有助于解决各种问题，例如客户细分、异常检测和文本分类等。尽管传统的聚类技术被广泛使用，但它仍然面临着挑战。 今天代码很少，也没有实验数据， 主要是偏思路分享。</p>
<p><br><br></p>
<h2 id="一编码挑战">一、编码挑战</h2>
<h3 id="11--字段单位不统一">1.1  字段单位不统一</h3>
<p>我想在本文中解决的主要挑战是选择如何编码或转换输入特征。一般来说，您需要将每个特征转换为相同的比例，否则，聚类模型将在特征之间分配不成比例的权重。例如， 假设数据中有重量 <strong>weight1</strong> 、 <strong>weight2</strong>  两个字段，weight1单位是市斤，而weight2单位是公斤。如果不首先对这些测量进行标准化，即使实际重量相同，我们的模型也会推断出以市斤为单位（对于类似重量的物体）测量的重量差异大于以公斤为单位的差异。</p>
<p>现实中，数据集中不会出现对一个信息使用两种单位进行度量。使用这个例子， 只为说明数据中不同字段分布不同，训练模型时不同字段承载的权重也不一样。为了减轻这个问题，一般是训练之前先将字段标准化。</p>
<br>
<h3 id="12-字段之间存在相关性">1.2 字段之间存在相关性</h3>
<p>让我们使用颜色组成的特征作为另一个示例。通常，许多人会选择将此特征 one-hot 编码到 n-1 个附加列中，其中 n 是唯一颜色的数量。虽然这有效，但它忽略了颜色之间的任何潜在关系。</p>
<p>为什么是这样？让我们考虑数据集中的一个特征具有以下颜色：红色、栗色、深红色、猩红色和绿色。如果我们要对该列进行 one-hot 编码，我们将得到一个如下所示的数据帧：</p>
<p><img loading="lazy" src="img/01-color.png" alt=""  />
</p>
<p>在 <strong>欧几里德距离空间</strong> 中，任意两个记录(行)之间的距离是相同的。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import numpy as np

def euclidean_distance(vec1, vec2):
    if len(vec1) != len(vec2):
        raise ValueError(&#34;vecs must have the same length.&#34;)
        
    squared_differences = [(a - b) ** 2 for a, b in zip(vec1, vec2)]
    distance = np.sqrt(sum(squared_differences))
    return distance
    
red = np.array([0, 0, 0, 1, 0])
maroon = np.array([0, 0, 1, 0, 0])
green = np.array([0, 1, 0, 0, 0])

print(euclidean_distance(red, maroon))
print(euclidean_distance(red, green))
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1.4142135623730951
1.4142135623730951
</code></pre></div><br>
<h2 id="二有更好的办法吗">二、有更好的办法吗？</h2>
<p>当然， <strong>红色</strong> 和 <strong>栗色</strong> 是两种不同的颜色，但为了我们的聚类算法，我们其实不希望euclidean_distance(red, maroon) 与 euclidean_distance(red, green) 是相等的。</p>
<p>那么该如何解决这个缺点呢？</p>
<p>如果您阅读这篇文章的标题，我相信您可能已经get到本文的ieda……我们将结合 <strong>大语言模型</strong> (Large language model, LLM)， 将每条记录字段和数值整理成一个字符串， 并通过LLM获得每条记录对应的嵌入表示。</p>
<p>对于此示例，我将使用 Huggingface 中的句子转换器库以及我围绕工作申请综合创建的数据集。</p>
<p>让我们从句子转换器开始。该 LLM 的工作原理与 BERT 类似，只不过它经过专门训练以在句子级别而不是单词或标记级别输出嵌入。这些句子级嵌入可以更好地捕获含义，并且计算速度更快。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">from</span> <span class="nn">sentence_transformers.util</span> <span class="kn">import</span> <span class="n">cos_sim</span>

<span class="c1">#使用hugginface，需要科学上网</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="sa">r</span><span class="s2">&#34;sentence-transformers/paraphrase-MiniLM-L6-v2&#34;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">prompt_text</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1">#每条记录整合为一个字符串</span>
    <span class="n">p_text</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&#34;Age: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> Gender: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;Gender&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2"> Role: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;Role&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> &#34;</span>
        <span class="sa">f</span><span class="s2">&#34;Hiring Department: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;HiringDepartment&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> &#34;</span>
        <span class="sa">f</span><span class="s2">&#34;Travel Preference: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;TravelPreference&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> Extracurriculars: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;ExtraCurriculars&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> &#34;</span>
        <span class="sa">f</span><span class="s2">&#34;Distance From Home: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;DistanceFromHome&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> &#34;</span>
        <span class="sa">f</span><span class="s2">&#34;Internships: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;Internships&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> Education Level: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;EducationLevel&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> Education Field: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;EducationField&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> &#34;</span>
        <span class="sa">f</span><span class="s2">&#34;Summary: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;Summary&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&#34;</span> 
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">p_text</span>

<span class="k">def</span> <span class="nf">output_embedding</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">#返回的嵌入表示的尺寸(记录数, 384)</span>
    <span class="c1">#sentence-transformers/paraphrase-MiniLM-L6-v2 模型的词向量维度是384</span>
    <span class="n">embd</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">embd</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">384</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">prompt_text</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">embd</span> <span class="o">=</span> <span class="n">output_embedding</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embd</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;combined_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">preprocess_text</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>我们的数据集包括有关求职者的信息，例如招聘部门、职位、年龄和教育水平等特征。这是一个数据截图：</p>
<p><img loading="lazy" src="img/02-df.png" alt=""  />
</p>
<br>
<p>我们的目标是将所有求职者分为不同的簇(可以理解为群体)。</p>
<p>让我们看看如何将句子嵌入应用于每个求职者。第一步是通过将所有功能连接到一个字符串中来创建单个文本prompt。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Age: 28.
Gender: male.
Role: Research Scientist.
Hiring Department: Research &amp; Development.
Travel Preference: Travel_Frequently.
Extracurriculars: nan.
Distance From Home: 4.
Internships: 9.
Education Level: 3.
Education Field: Engineering.
Summary: As you can see, I am very dedicated and I am ready to start at your firm immediately.
</code></pre></div><p>将原记录(行)转为如上图所示的文本，之后调用 SBERT LLM 检索文本对应的嵌入向量。为方便展示，这里使用 dataframe.style 功能来突出显示低值和大值，以使表格更容易扫描：</p>
<p><img loading="lazy" src="img/03-df-style.png" alt=""  />
</p>
<br>
<h2 id="三用嵌入编码有什么益处">三、用嵌入编码有什么益处？</h2>
<p>之前讲了传统聚类算法使用one-hot编码方式的不足，但没有解释用嵌入表示的益处。 先不讲理论， 就像探索颜色编码，我们看一个例子。 我想测量 <strong>Role</strong> (岗位角色) 的相似程度， 我更倾向于用余弦相似度，而不是欧几里德距离， 请问这其中的差异是？</p>
<ul>
<li><strong>欧几里得距离</strong> 是两点之间几何距离的度量，而 <strong>余弦相似度</strong> 度量向量的方向。</li>
<li><strong>欧几里得距离对向量的大小敏感</strong>，而余弦相似度则不然。</li>
<li>欧氏距离的值范围从 0（相同向量）到无穷大，而 <strong>余弦相似度的范围从 -1（完全不相似）到 1（完全相似）</strong></li>
</ul>
<p>让我们选择两个岗位角色：<strong>销售代表</strong>（sales representative）和<strong>销售主管</strong>(sales executive)。</p>
<ul>
<li>
<p>使用 one-hot 编码的 销售代表 和 销售主管 的余弦相似度为 0.5，这意味着他们<strong>有些相关</strong>。这是有道理的，因为他们都是销售角色。</p>
</li>
<li>
<p>使用嵌入编码的余弦相似度为 0.82。<strong>它们的相关性要高得多</strong>。这更有意义，因为销售代表和销售主管在实践中是极其相似的角色。</p>
</li>
</ul>
<h3 id="31--传统的聚类">3.1  传统的聚类</h3>
<p>传统聚类算法大致流程如下图所示，</p>
<p><img loading="lazy" src="img/06-traditional-process.png" alt=""  />
</p>
<p>原文作者实验使用K=3的聚类算法，但k如何设置不是最关键的点。 我们的聚类模型中最重要的字段是求职者的<strong>个人总结</strong>（Summary），其次是 <strong>招聘部门</strong>（HiringDepartment）、<strong>是否喜欢旅行</strong>(TravelPreference)。</p>
<p><img loading="lazy" src="img/04-weight.png" alt=""  />
</p>
<br>
<p>为了更好的理解3个簇， 我们输出了数据汇总，每个数值字段平均值 及 非数值字段的高频项。</p>
<p><img loading="lazy" src="img/05-stats.png" alt=""  />
</p>
<p>按道理聚类算法的结果应该不同簇之间的差异尽可能的大。糟糕的是不同簇之间的， 年龄(Age)、实习次数(Internships) 差异很小，而更糟糕的是招聘部门(HiringDepartment) 和 岗位角色(Role) 完全相同。</p>
<br>
<h3 id="32-嵌入的聚类">3.2 嵌入的聚类</h3>
<p>使用嵌入编码的聚类算法流程如下图所示。与传统 聚类方法相比，使用嵌入的流程只需处理数字特征， 因为由求职者提示信息(代码里的prompt_text)转化来的嵌入是严格数字化的。</p>
<p><img loading="lazy" src="img/07-emb-process.png" alt=""  />
</p>
<p>在这里，我们不能像上次那样直接计算字段重要性。我们有数百个难以理解的特征，它们的重要性各不相同，我们无法理解。那么我们该怎么办？让我们训练另一个模型（这次是有监督的三类分类模型），使用原始特征集来预测嵌入模型生成的类标签。这样就可以以同类的方式重现字段重要性。结果如下</p>
<p><img loading="lazy" src="img/08-weight.png" alt=""  />
</p>
<p>我们找到一种新的嵌入表示来编码求职者信息， 并运算出了聚类结果。</p>
<p><img loading="lazy" src="img/09-statas.png" alt=""  />
</p>
<p>从统计信息(上图)中可以看出，不同簇之间的差异变的更加清晰。 使用嵌入编码， 让更多申请销售岗位的的销售主管划分到cluster2， 让更多申请研发岗位的的科学家划分到cluster1 和 cluster3.</p>
<br>
<blockquote>
<p>前文内容翻译整理自</p>
<p><a href="https://medium.com/@swansburg.justin/how-to-use-llms-to-build-better-clustering-models-9b17a5491bb4">https://medium.com/@swansburg.justin/how-to-use-llms-to-build-better-clustering-models-9b17a5491bb4</a></p>
</blockquote>
<p><br><br></p>
<h2 id="四启发">四、启发</h2>
<p>读完以上内容，大邓想到一个问题， 假设 没有简历系统，没有大数据，求职者与面试官坐在现场，  数据就是面试过程中的交流， 而交流必然通过话语这一媒介。 例如求职者的个人信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">“大家好，我叫张三， 今年24岁，哈尔滨人。本科毕业于哈尔滨工业大学，市场营销专业。 我是一个很外向的人，对销售很感兴趣，在大学期间摆了很多地摊。很希望获得贵公司的机会，让我在营销岗位上大发异彩。”
</code></pre></div><p>面试期间，记录人员将该哈尔滨张三的个人信息被整理为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">name: 张三
age: 24
city: 哈尔滨
edu: 哈尔滨工业大学
major: 市场营销
experience: 摆摊
summary: 我是外向的人，对销售很感兴趣。
</code></pre></div><p>求职者的信息汇总成xlsx， 每个人的信息都或多或少的被压缩了。 这种表示方式， 在小规模时， 求职者的总结summary还是有很大信息量的，能够让面试者回忆起当时的场景和情景。但是当求职者的规模上升到几千上万， 备注note信息这种很重要的信息反而无法利用。</p>
<p>使用大语言模型LLM，将文本提示转化为嵌入表示。我们可以将LLM看成是一个察言观色，见微知著，明察秋毫的智者。  这个智者可以</p>
<ul>
<li>分类</li>
<li>提取信息</li>
<li>补全</li>
<li>相似性</li>
<li>&hellip;</li>
</ul>
<p>以往缺失数据， 用插值或者其他技巧， 现在我们可以借助LLM， 只有有其他字段残存的微弱线索， LLM就能帮我们补全缺失值。</p>
<br>
<h3 id="41-分类">4.1 分类</h3>
<p>如图所示， 对于很多短文本， 我们可以推断话题，也可以推断情绪。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">https://huggingface.co/morit/chinese_xlm_xnli
</code></pre></div><p><img loading="lazy" src="img/10-classification.png" alt=""  />
</p>
<p><img loading="lazy" src="img/11-classification.png" alt=""  />
</p>
<h3 id="42-提取信息">4.2 提取信息</h3>
<p>假设有一些信息存储在文本中， 可以用正则表达式提取， 下面的例子用正则会很难设计， 但用LLM很简单。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">https://huggingface.co/luhua/chinese_pretrain_mrc_roberta_wwm_ext_large
</code></pre></div><p><img loading="lazy" src="img/12-extract.png" alt=""  />
</p>
<br>
<h3 id="43-补全">4.3 补全</h3>
<p>填充缺失值信息</p>
<p><img loading="lazy" src="img/13-mask.png" alt=""  />
</p>
<br>
<h3 id="44-相似性">4.4 相似性</h3>
<p><img loading="lazy" src="img/14-sim.png" alt=""  />
</p>
<p>当然LLM功能还有很多，大家可以自己探索探索</p>
<p><img loading="lazy" src="img/15-func.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用3751w专利申请数据集按年份(按省份)训练词向量</title>
      <link>https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/</link>
      <pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-20-word2vec-by-year-by-province/</guid>
      <description>&lt;p&gt;想用 &lt;a href=&#34;https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/&#34;&gt;3571w中国专利申请数据集&lt;/a&gt;，按年份(或按省份)训练词向量的同学，可以好好看本文，能节省你几十个小时时间。
&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一检查数据&#34;&gt;一、检查数据&lt;/h2&gt;
&lt;p&gt;这个数据集很大， 如图所示，文件动辄几G
&lt;img loading=&#34;lazy&#34; src=&#34;img/01-data-screen.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;之前分享过 &lt;a href=&#34;&#34;&gt;&lt;/a&gt; , 面对巨大csv文件，我们要了解内部有哪些字段、字段的含义， 只读取需要的字段，减轻电脑内存压力， 让你能轻松应对几倍于内存的巨大csv文件。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#以山东省.csv 为例， 只读第一行(前1行)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;山东省.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nrows&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-shandong_df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;字段展示的不全，完整的字段应该有&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Index([&amp;#39;专利公开号&amp;#39;, &amp;#39;专利名称&amp;#39;, &amp;#39;专利类型&amp;#39;, &amp;#39;专利摘要&amp;#39;, &amp;#39;申请人&amp;#39;, &amp;#39;专利申请号&amp;#39;, &amp;#39;申请日&amp;#39;, &amp;#39;申请公布日&amp;#39;,
       &amp;#39;授权公布号&amp;#39;, &amp;#39;授权公布日&amp;#39;, &amp;#39;申请地址&amp;#39;, &amp;#39;主权项&amp;#39;, &amp;#39;发明人&amp;#39;, &amp;#39;分类号&amp;#39;, &amp;#39;主分类号&amp;#39;, &amp;#39;代理机构&amp;#39;, &amp;#39;分案原申请号&amp;#39;,
       &amp;#39;优先权&amp;#39;, &amp;#39;国际申请&amp;#39;, &amp;#39;国际公布&amp;#39;, &amp;#39;代理人&amp;#39;, &amp;#39;省份或国家代码&amp;#39;, &amp;#39;法律状态&amp;#39;, &amp;#39;专利领域&amp;#39;, &amp;#39;专利学科&amp;#39;,
       &amp;#39;多次公布&amp;#39;],
      dtype=&amp;#39;object&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;训练词向量主要用文本数据， 在本案例中， 需要的字段 [&lt;strong&gt;专利摘要&lt;/strong&gt;] 。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二构造语料&#34;&gt;二、构造语料&lt;/h2&gt;
&lt;p&gt;在 [3751万专利申请全量数据1985-2022] 文件夹中，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;新建 [province_corpus] 和 [year_corpus] 两个文件夹&lt;/li&gt;
&lt;li&gt;新建 [code.ipynb]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;构造语料对电脑的性能要求不高， 不论你的电脑是什么配置，基本都能运行， 而且耗时在能接受的范围。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;21-文件树结构&#34;&gt;2.1 文件树结构&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;3751万专利申请全量数据1985-2022
  |---安徽省.csv
  |---浙江省.csv
  |---江苏省.csv
  |---...
  |---code.ipynb
  |---province_corpus
     |---安徽省.txt
     |---浙江省.txt
     |---...
  |---year_corpus
     |---2022.txt
     |---2021.txt
     |---...
  |---output
     |---provin_w2vs
        |---安徽省.100.6.bin
        |---安徽省.100.6.bin.syn1neg.npy
        |---安徽省.100.6.bin.wv.vectors.npy
        |---...
     |---year_w2vs
        |---2022.100.6.bin
        |---2022.100.6.bin.syn1neg.npy
        |---2022.100.6.bin.wv.vectors.npy
        |---...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-按省份构造语料&#34;&gt;2.2 按省份构造语料&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 在jupyter内可以得到Cell的运行时间&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt; 
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 获取 code.ipynb 所在文件夹内的所有csv文件路径列表&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.csv&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 构造省份 txt 语料数据&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;province&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;replace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;province_corpus/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;province&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;usecols&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;prov_f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;上海市.csv
云南省.csv
...
安徽省.csv

CPU times: total: 1500 s
Wall time: 1520 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;构造省份语料用了半个小时。
&lt;img loading=&#34;lazy&#34; src=&#34;img/03-province-corpus.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-按年份构造语料&#34;&gt;2.3 按年份构造语料&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt; 
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;


&lt;span class=&#34;c1&#34;&gt;# 获取 code.ipynb 所在文件夹内的所有csv文件路径列表&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.csv&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 构造年份 txt 语料数据&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;usecols&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请日&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请日&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请日&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请日&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;errors&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ignore&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请日&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value_counts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year_corpus/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;a+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请日&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;==&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;year_f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;上海市.csv 2022
上海市.csv 2021
上海市.csv 2020
...
...
安徽省.csv 2022
安徽省.csv 2021
...
...

CPU times: total: 1600 s
Wall time: 1650 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;构造年份语料大概用了半个小时。
&lt;img loading=&#34;lazy&#34; src=&#34;img/03-year-corpus.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三训练word2vec&#34;&gt;三、训练word2vec&lt;/h2&gt;
&lt;p&gt;需要注意， 训练word2vec需要耗费很大的计算能力， 训练时间需要一两三。 本文使用的cntext2.1.1版本，需要付费获取cntext-2.1.1-py3-none-any.whl。&lt;/p&gt;
&lt;h3 id=&#34;31-安装cntext&#34;&gt;3.1 安装cntext&lt;/h3&gt;
&lt;p&gt;将 cntext-2.1.1-py3-none-any.whl 放置于电脑桌面， 打开 &lt;strong&gt;命令行cmd&lt;/strong&gt; (Mac打开&lt;strong&gt;terminal&lt;/strong&gt;)， 输入&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
pip install distinctiveness
pip install cntext-2.1.1-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;有部分使用win的同学，如果按照操作没有安装成功，再试试&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd Desktop
pip install distinctiveness
pip install cntext-2.1.1-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-按省份训练&#34;&gt;3.2 按省份训练&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 获取 province_corpus 内的语料 txt 文件列表&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;provin_fs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;province_corpus/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.txt&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2VModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;save_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;provin_w2vs&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;province_corpus/上海市.txt
Start Preprocessing Corpus...
Loading model cost 0.632 seconds.
Prefix dict has been built successfully.
Start Training! This may take a while. Please be patient...
Training word2vec model took 3284 seconds
Note: The Word2Vec model has been saved to output\provin_w2vs

province_corpus/云南省.txt
Start Preprocessing Corpus...
Loading model cost 0.632 seconds.
Prefix dict has been built successfully.
Start Training! This may take a while. Please be patient...
Training word2vec model took 564 seconds
Note: The Word2Vec model has been saved to output\provin_w2vs
...
...

CPU times: total: 21354 s
Wall time: 21758 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;训练省份词向量大概用了 6 小时，模型文件保存在 output/provin_w2vs
&lt;img loading=&#34;lazy&#34; src=&#34;img/05-province-w2vs.png&#34; alt=&#34;&#34;  /&gt;

&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;33-按年份训练&#34;&gt;3.3 按年份训练&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 获取 province_corpus 内的语料 txt 文件列表&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_fs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year_corpus/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.txt&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2VModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;save_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year_w2vs&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;year_corpus/1980.txt
Start Preprocessing Corpus...
Start Training! This may take a while. Please be patient...
Training word2vec model took 0 seconds
Note: The Word2Vec model has been saved to output\year_w2vs

year_corpus/1984.txt
Start Preprocessing Corpus...
Start Training! This may take a while. Please be patient...
Training word2vec model took 0 seconds
Note: The Word2Vec model has been saved to output\year_w2vs

...
...

CPU times: total: 19354 s
Wall time: 20000 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;训练省份词向量大概用了 5.5 小时，模型文件保存在 output/year_w2vs
&lt;img loading=&#34;lazy&#34; src=&#34;img/06-years-w2vs.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三使用word2vec&#34;&gt;三、使用word2vec&lt;/h2&gt;
&lt;h3 id=&#34;31-导入模型&#34;&gt;3.1 导入模型&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;output/provin_w2vs&lt;/code&gt; 和 &lt;code&gt;output/year_w2vs&lt;/code&gt; 内有多个模型， 单个的模型文件大约几十M ~ 几百M， &lt;strong&gt;但不建议一次性导入进来&lt;/strong&gt;。大邓的电脑内存96G，为了省事，就一次性全导入了。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;gensim.models&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;provin_w2vs_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;provin_w2v_fs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/provin_w2vs/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/provin_w2vs&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.npy&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#如果没有cntext就用注释掉的代码，使用gensim导入&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#provin_w2v = KeyedVectors.load(provin_w2v_f)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;provin_w2vs_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;gensim.models&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;year_w2vs_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_w2v_fs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/year_w2vs/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt; 
               &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;sorted&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/year_w2vs&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reverse&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
               &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.npy&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#如果没有cntext就用注释掉的代码，使用gensim导入&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#year_w2v = KeyedVectors.load(year_w2v_f)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;year_w2vs_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-查看词汇量&#34;&gt;3.2 查看词汇量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;省份Word2vec词汇量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2vs_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;province&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;[&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\u4e00&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\u9fa5&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;]+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;province&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 词汇量: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;省份Word2vec词汇量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;北京市&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;679126&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;安徽省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;471459&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;江西省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;216389&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;重庆市&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;269875&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;澳门特别行政区&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4235&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;湖北省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;416464&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;吉林省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;169665&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;香港特别行政区&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;37948&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;天津市&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;323214&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;湖南省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;341033&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;辽宁省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;331955&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;广东省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;935412&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;黑龙江省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;223448&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;其他国家&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;460&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;河北省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;281543&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;内蒙古自治区&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;103331&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;河南省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;381151&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;福建省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;367768&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;贵州省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;163641&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;陕西省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;332231&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;云南省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;174191&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;浙江省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;751976&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;甘肃省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;125789&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;台湾省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;175827&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;宁夏回族自治区&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;65428&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;山西省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;163094&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;山东省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;633187&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;江苏省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;928838&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;海南省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;59742&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;西藏自治区&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;14884&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;青海省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;40403&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;广西壮族自治区&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;237805&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;四川省&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;437751&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;新疆维吾尔自治区&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100079&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;上海市&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;537777&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;年份word2vec词汇量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2vs_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;\d&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{4}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;年份word2vec词汇量
2022: 191145
2021: 605364
2020: 903230
2019: 901583
2018: 911762
2017: 881858
2016: 810445
2015: 621050
2014: 388647
2013: 517991
2012: 484417
2011: 281045
2010: 264882
2009: 245851
2008: 218349
2007: 191569
2006: 177269
2005: 153533
2004: 130384
2003: 120306
2002: 102266
2001: 78116
2000: 63233
1999: 53341
1998: 47007
1997: 44221
1996: 42709
1995: 40084
1994: 40987
1993: 42781
1992: 40149
1991: 33159
1990: 28480
1989: 23548
1988: 23502
1987: 19851
1986: 14330
1985: 11535
1984: 2
1980: 2
1900: 1
1899: 4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;33-语义检查-省份&#34;&gt;3.3 语义检查-省份&lt;/h3&gt;
&lt;p&gt;先检查省份， 查看与[&amp;lsquo;创新&amp;rsquo;, &amp;lsquo;新颖&amp;rsquo;]最相似的5个词，通过语义捕捉准确与否，粗略判断Word2vec训练效果的好坏。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2vs_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;province&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;[&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\u4e00&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\u9fa5&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;]+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;wordweigths&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;新颖&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wordweigths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;province&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;province&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;北京市: 独特 全新 创造性 独创 先进
安徽省: 简洁 独特 精巧 精简 原理简单
江西省: 巧妙 简单实用 简单可靠 简单成本低 精巧
重庆市: 先进 独特 现代 科学 易用
澳门特别行政区: 
湖北省: 巧妙 精巧 科学 精良 简洁
吉林省: 先进 科学 推广 独特 可行
香港特别行政区: 工业化生产 公知 相对现有 克服现有 石头纸
天津市: 独特 先进 精巧 科学 构思
湖南省: 科学 简洁 先进 简明 独特
辽宁省: 科学 理念 构思 先进 独特
广东省: 独特 创新性 巧妙 科学 简单巧妙
黑龙江省: 科学 独特 精巧 先进 小巧
其他国家: 
河北省: 科学合理 新颖独特 简单实用 科学 精巧
内蒙古自治区: 经济实用 广泛使用 资源丰富 应用广泛 应用范围广
河南省: 简单实用 独特 较为新颖 精巧 简明
福建省: 原理简单 简单实用 巧妙 灵巧 独特
贵州省: 取材方便 价格便宜 安全经济 生产成本低廉 原料易得
陕西省: 独特 简单实用 结构新颖 小巧 简约
云南省: 科学 构思新颖 独特 价廉 简便易行,
浙江省: 独特 科学 全新 巧妙 较为合理
甘肃省: 先进 切实可行 性能优良 独特 一种较为理想
台湾省: 独特 多元 特有 经济 特色
宁夏回族自治区: 重量轻 体积小 性能可靠 具有使用方便 功能丰富
山西省: 独特 科学 先进 简洁 广泛
山东省: 新颖独特 巧妙 精巧 独特 先进
江苏省: 独特 全新 构思 精巧 简单巧妙
海南省: 多样 传统工艺 改变传统 制作过程 精华素
西藏自治区: 
青海省: 投饵 具有重量轻 具有成本低 极为 隔热保温
广西壮族自治区: 先进 独特 现代 广泛 明
四川省: 独特 先进 科学合理 人性化 科学
新疆维吾尔自治区: 现代 方
 携带方便 广泛 原料来源
上海市: 独特 设计理念 巧妙 科学 构思
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从上面的运行结果看， 绝大多数的省份Word2vec都很准确的捕捉到了专利摘要的语义信息。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;34-语义检查-年份&#34;&gt;3.4 语义检查-年份&lt;/h3&gt;
&lt;p&gt;查看与[&amp;lsquo;创新&amp;rsquo;, &amp;lsquo;新颖&amp;rsquo;]最相似的5个词，通过语义捕捉准确与否，粗略判断Word2vec训练效果的好坏。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2vs_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;\d&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{4}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;wordweigths&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;新颖&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wordweigths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2022: 出错率 同井 Git 铝合金膜 传动件传动
2021: 铁前驱 甲基氢 单向阀进液 储存罐 Seq2Seq
2020: 减温层 洗浴间 球形活性炭 OASQ 目标语言句子
2019: 面膜布 自承式 网状加强 铲衣 温头
2018: 简单易行 工艺流程 成本高 克服现有 操作简便
2017: 问题 容易 良好 性能 实用
2016: 相互绝缘 不需 未知 窄边框 耐腐层
2015: 人体温度 压盖 后座 漏电检测 截骨刀
2014: 循环热泵 特细 多处 尾轴 名称内裤
2013: 精制 重复性 强 优势 少
2012: 提升 解决 多种 能够实现 避免
2011: 光亮 具有高 分层 木 品
2010: 转换 具有 强度 整个 时间

2009: 科学 先进 独特 创造性 全新
2008: 独特 先进 全新 思路 创造性
2007: 独特 全新 先进 现代 科学
2006: 先进 独特 全新 新颖性 创造性
2005: 科学 独特 简洁 简捷 独特,
2004: 独特 简单易行 科学 优越性 经济
2003: 独特 科学 多样, 实用 经济实用
2002: 研制 ,采取 多方面 效果差 直接应用

2001: 人造 管材 废弃 有机结合 缺点
2000: 防滑 光亮 着色 系列产品 清晰
1999: ,适于 病人 价廉 ,目前 害虫
1998: 优化 城市 设施 磁化 节水
1997: 病症 手工 透气 不足, 不足
1996: 制造 ,属于 应用 实现 传递文字
1995: 显著疗效 味美 极佳 活血 明目
1994: 样式 坚固耐用, 便利, 巧妙 使用方便安全
1993: 先进 完善 知识性 成本高 功能单一
1992: 经济实用 现行 ,实为 住房 结构设计
1991: 易损坏 保留 庞大 普遍 物两用
1990: 笔算 不足之处, 功能单一 ,需用 结构复杂
1989: 缺点 力小 设施 临时 不便
1988: 设施, 改造 美发 油田 机械装置
1987: 各种类型 用途单一 之用 水上 室内外
1986: 飞行 化油器 具 显微镜 雨水
1985: 很大 浪费 器件 地区 具有结构
1984: 
1980: 
1900: 
1899: 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;也试了其他的词语，好像只有 2002-2009 之间的语义是准确的。 &lt;strong&gt;原因未知，也训练了&lt;a href=&#34;https://textdata.cn/blog/2023-11-18-failure-example-show-word-meaning-shift-using-word2vec/&#34;&gt;裁判文书的Word2vec&lt;/a&gt;,  年份的是一点准头都没有。专利数据训练的好在还有点准头&lt;/strong&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;四研究潜力-语义变迁研究方法介绍&#34;&gt;四、研究潜力: 语义变迁研究方法介绍&lt;/h2&gt;
&lt;p&gt;假设语义都很准的话， 是可以研究 &lt;strong&gt;语义变迁&lt;/strong&gt; 或者 &lt;strong&gt;语义差异&lt;/strong&gt; 的。 但需要注意， 不能直接使用两个年份或者两个省份的中word1和word2的距离来体现语义的变迁或者语义的差异。 如果想做省份间差异或者某省份随时间的变化， 需要用到 &lt;strong&gt;对齐算法&lt;/strong&gt;， 常用的算法是 &lt;strong&gt;正交Procrustes矩阵对齐&lt;/strong&gt;， 使得同省份不同年份或者通年份不同省份的word2vec都有相同的语义空间。&lt;/p&gt;
&lt;h3 id=&#34;41-正交procrustes算法&#34;&gt;4.1 正交Procrustes算法&lt;/h3&gt;
&lt;p&gt;正交Procrustes矩阵对齐是一种将两个预训练语言模型的词向量矩阵对齐的方法，使得它们在相同的语义空间中表示。具体来说，它通过计算一个正交矩阵，将两个词向量矩阵进行线性变换，使得它们的Frobenius范数之和最小，从而实现对齐。 gensim库有该算法，大邓后续有时间会分享如何用Procrustes对齐语言模型。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;42-语义变迁流程图&#34;&gt;4.2 语义变迁流程图&lt;/h3&gt;
&lt;p&gt;语义变迁类研究的流程图可参考 &lt;a href=&#34;https://github.com/Living-with-machines/DiachronicEmb-BigHistData&#34;&gt;DiachronicEmb-BigHistData&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/w2v-time-shifting-flowchart.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;43-识别语义变化时间点&#34;&gt;4.3 识别语义变化时间点&lt;/h3&gt;
&lt;p&gt;该项目研究了1800-1910期间， 每10年为一个单位训练词向量， 探究词语变化。以 &lt;em&gt;&lt;strong&gt;railway&lt;/strong&gt;&lt;/em&gt; 和  &lt;em&gt;&lt;strong&gt;traffic&lt;/strong&gt;&lt;/em&gt; 为例, 先用 余弦相似度(cosine-similarity)算法识别词语语义变化的时间点，如下图&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/consine-sim-cpdetection.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;44-绘制语义变化轨迹&#34;&gt;4.4 绘制语义变化轨迹&lt;/h3&gt;
&lt;p&gt;语义变化轨迹&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/railway-time-shifting.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五获取资源&#34;&gt;五、获取资源&lt;/h2&gt;
&lt;h3 id=&#34;51-免费&#34;&gt;5.1 免费&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;年份词向量 链接: &lt;a href=&#34;https://pan.baidu.com/s/1-EngiWU9IAkqfk2Qf2W5lA&#34;&gt;https://pan.baidu.com/s/1-EngiWU9IAkqfk2Qf2W5lA&lt;/a&gt; 提取码: d925&lt;/li&gt;
&lt;li&gt;省份词向量  链接: &lt;a href=&#34;https://pan.baidu.com/s/1TnZf5YkRZMWf4liN04XO_g&#34;&gt;https://pan.baidu.com/s/1TnZf5YkRZMWf4liN04XO_g&lt;/a&gt; 提取码: cjs8&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&#34;52-付费&#34;&gt;5.2 付费&lt;/h3&gt;
&lt;p&gt;内容整理不易， 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;100元  cntext-2.1.1-py3-none-any.whl&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>想用 <a href="https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/">3571w中国专利申请数据集</a>，按年份(或按省份)训练词向量的同学，可以好好看本文，能节省你几十个小时时间。
<br><br></p>
<h2 id="一检查数据">一、检查数据</h2>
<p>这个数据集很大， 如图所示，文件动辄几G
<img loading="lazy" src="img/01-data-screen.png" alt=""  />
</p>
<p>之前分享过 <a href=""></a> , 面对巨大csv文件，我们要了解内部有哪些字段、字段的含义， 只读取需要的字段，减轻电脑内存压力， 让你能轻松应对几倍于内存的巨大csv文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#以山东省.csv 为例， 只读第一行(前1行)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;山东省.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/02-shandong_df.png" alt=""  />
</p>
<br>
<p>字段展示的不全，完整的字段应该有</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Index([&#39;专利公开号&#39;, &#39;专利名称&#39;, &#39;专利类型&#39;, &#39;专利摘要&#39;, &#39;申请人&#39;, &#39;专利申请号&#39;, &#39;申请日&#39;, &#39;申请公布日&#39;,
       &#39;授权公布号&#39;, &#39;授权公布日&#39;, &#39;申请地址&#39;, &#39;主权项&#39;, &#39;发明人&#39;, &#39;分类号&#39;, &#39;主分类号&#39;, &#39;代理机构&#39;, &#39;分案原申请号&#39;,
       &#39;优先权&#39;, &#39;国际申请&#39;, &#39;国际公布&#39;, &#39;代理人&#39;, &#39;省份或国家代码&#39;, &#39;法律状态&#39;, &#39;专利领域&#39;, &#39;专利学科&#39;,
       &#39;多次公布&#39;],
      dtype=&#39;object&#39;)
</code></pre></div><br>
<p>训练词向量主要用文本数据， 在本案例中， 需要的字段 [<strong>专利摘要</strong>] 。</p>
<p><br><br></p>
<h2 id="二构造语料">二、构造语料</h2>
<p>在 [3751万专利申请全量数据1985-2022] 文件夹中，</p>
<ol>
<li>新建 [province_corpus] 和 [year_corpus] 两个文件夹</li>
<li>新建 [code.ipynb]</li>
</ol>
<p>构造语料对电脑的性能要求不高， 不论你的电脑是什么配置，基本都能运行， 而且耗时在能接受的范围。</p>
<br>
<h3 id="21-文件树结构">2.1 文件树结构</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">3751万专利申请全量数据1985-2022
  |---安徽省.csv
  |---浙江省.csv
  |---江苏省.csv
  |---...
  |---code.ipynb
  |---province_corpus
     |---安徽省.txt
     |---浙江省.txt
     |---...
  |---year_corpus
     |---2022.txt
     |---2021.txt
     |---...
  |---output
     |---provin_w2vs
        |---安徽省.100.6.bin
        |---安徽省.100.6.bin.syn1neg.npy
        |---安徽省.100.6.bin.wv.vectors.npy
        |---...
     |---year_w2vs
        |---2022.100.6.bin
        |---2022.100.6.bin.syn1neg.npy
        |---2022.100.6.bin.wv.vectors.npy
        |---...
</code></pre></div><br>
<h3 id="22-按省份构造语料">2.2 按省份构造语料</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 在jupyter内可以得到Cell的运行时间</span>
<span class="o">%%</span><span class="n">time</span> 
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># 获取 code.ipynb 所在文件夹内的所有csv文件路径列表</span>
<span class="n">csvfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.csv&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>

<span class="c1"># 构造省份 txt 语料数据</span>
<span class="k">for</span> <span class="n">csvf</span> <span class="ow">in</span> <span class="n">csvfs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">csvf</span><span class="p">)</span>
    <span class="n">province</span> <span class="o">=</span> <span class="n">csvf</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;province_corpus/</span><span class="si">{</span><span class="n">province</span><span class="si">}</span><span class="s1">.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">prov_f</span><span class="p">:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;专利摘要&#39;</span><span class="p">])</span>
        <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;专利摘要&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;专利摘要&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="n">prov_f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">上海市.csv
云南省.csv
...
安徽省.csv

CPU times: total: 1500 s
Wall time: 1520 s
</code></pre></div><p>构造省份语料用了半个小时。
<img loading="lazy" src="img/03-province-corpus.png" alt=""  />
</p>
<br>
<h3 id="23-按年份构造语料">2.3 按年份构造语料</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span> 
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>


<span class="c1"># 获取 code.ipynb 所在文件夹内的所有csv文件路径列表</span>
<span class="n">csvfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.csv&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>

<span class="c1"># 构造年份 txt 语料数据</span>
<span class="k">for</span> <span class="n">csvf</span> <span class="ow">in</span> <span class="n">csvfs</span><span class="p">:</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;申请日&#39;</span><span class="p">,</span> <span class="s1">&#39;专利摘要&#39;</span><span class="p">])</span>
    <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;申请日&#39;</span><span class="p">,</span> <span class="s1">&#39;专利摘要&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;申请日&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;申请日&#39;</span><span class="p">],</span> <span class="n">errors</span> <span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">year</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;申请日&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">year</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;year_corpus/</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">year_f</span><span class="p">:</span>
            <span class="n">year_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;申请日&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">year</span><span class="o">==</span><span class="n">year</span><span class="p">]</span>
            <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;专利摘要&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
            <span class="n">year_f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">上海市.csv 2022
上海市.csv 2021
上海市.csv 2020
...
...
安徽省.csv 2022
安徽省.csv 2021
...
...

CPU times: total: 1600 s
Wall time: 1650 s
</code></pre></div><p>构造年份语料大概用了半个小时。
<img loading="lazy" src="img/03-year-corpus.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三训练word2vec">三、训练word2vec</h2>
<p>需要注意， 训练word2vec需要耗费很大的计算能力， 训练时间需要一两三。 本文使用的cntext2.1.1版本，需要付费获取cntext-2.1.1-py3-none-any.whl。</p>
<h3 id="31-安装cntext">3.1 安装cntext</h3>
<p>将 cntext-2.1.1-py3-none-any.whl 放置于电脑桌面， 打开 <strong>命令行cmd</strong> (Mac打开<strong>terminal</strong>)， 输入</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
pip install distinctiveness
pip install cntext-2.1.1-py3-none-any.whl
</code></pre></div><p>有部分使用win的同学，如果按照操作没有安装成功，再试试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd Desktop
pip install distinctiveness
pip install cntext-2.1.1-py3-none-any.whl
</code></pre></div><br>
<h3 id="32-按省份训练">3.2 按省份训练</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取 province_corpus 内的语料 txt 文件列表</span>
<span class="n">provin_fs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;province_corpus/</span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.txt&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
<span class="k">for</span> <span class="n">provin_f</span> <span class="ow">in</span> <span class="n">provin_fs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">provin_f</span><span class="p">)</span>
    <span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="n">provin_f</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
    <span class="n">w2v_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="s1">&#39;provin_w2vs&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">province_corpus/上海市.txt
Start Preprocessing Corpus...
Loading model cost 0.632 seconds.
Prefix dict has been built successfully.
Start Training! This may take a while. Please be patient...
Training word2vec model took 3284 seconds
Note: The Word2Vec model has been saved to output\provin_w2vs

province_corpus/云南省.txt
Start Preprocessing Corpus...
Loading model cost 0.632 seconds.
Prefix dict has been built successfully.
Start Training! This may take a while. Please be patient...
Training word2vec model took 564 seconds
Note: The Word2Vec model has been saved to output\provin_w2vs
...
...

CPU times: total: 21354 s
Wall time: 21758 s
</code></pre></div><p>训练省份词向量大概用了 6 小时，模型文件保存在 output/provin_w2vs
<img loading="lazy" src="img/05-province-w2vs.png" alt=""  />

<br></p>
<h3 id="33-按年份训练">3.3 按年份训练</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取 province_corpus 内的语料 txt 文件列表</span>
<span class="n">year_fs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;year_corpus/</span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.txt&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
<span class="k">for</span> <span class="n">year_f</span> <span class="ow">in</span> <span class="n">year_fs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">year_f</span><span class="p">)</span>
    <span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="n">year_f</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
    <span class="n">w2v_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="s1">&#39;year_w2vs&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">year_corpus/1980.txt
Start Preprocessing Corpus...
Start Training! This may take a while. Please be patient...
Training word2vec model took 0 seconds
Note: The Word2Vec model has been saved to output\year_w2vs

year_corpus/1984.txt
Start Preprocessing Corpus...
Start Training! This may take a while. Please be patient...
Training word2vec model took 0 seconds
Note: The Word2Vec model has been saved to output\year_w2vs

...
...

CPU times: total: 19354 s
Wall time: 20000 s
</code></pre></div><p>训练省份词向量大概用了 5.5 小时，模型文件保存在 output/year_w2vs
<img loading="lazy" src="img/06-years-w2vs.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三使用word2vec">三、使用word2vec</h2>
<h3 id="31-导入模型">3.1 导入模型</h3>
<p><code>output/provin_w2vs</code> 和 <code>output/year_w2vs</code> 内有多个模型， 单个的模型文件大约几十M ~ 几百M， <strong>但不建议一次性导入进来</strong>。大邓的电脑内存96G，为了省事，就一次性全导入了。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>


<span class="n">provin_w2vs_</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">provin_w2v_fs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;output/provin_w2vs/</span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;output/provin_w2vs&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.npy&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
<span class="k">for</span> <span class="n">provin_w2v_f</span> <span class="ow">in</span> <span class="n">provin_w2v_fs</span><span class="p">:</span>
    <span class="n">provin_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">provin_w2v_f</span><span class="p">)</span>
    <span class="c1">#如果没有cntext就用注释掉的代码，使用gensim导入</span>
    <span class="c1">#provin_w2v = KeyedVectors.load(provin_w2v_f)</span>
    <span class="n">provin_w2vs_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">provin_w2v</span><span class="p">)</span>
    
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">year_w2vs_</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">year_w2v_fs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;output/year_w2vs/</span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s1">&#39;</span> 
               <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;output/year_w2vs&#39;</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
               <span class="k">if</span> <span class="s1">&#39;.npy&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>

<span class="k">for</span> <span class="n">year_w2v_f</span> <span class="ow">in</span> <span class="n">year_w2v_fs</span><span class="p">:</span>
    <span class="n">year_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">year_w2v_f</span><span class="p">)</span>
    <span class="c1">#如果没有cntext就用注释掉的代码，使用gensim导入</span>
    <span class="c1">#year_w2v = KeyedVectors.load(year_w2v_f)</span>
    <span class="n">year_w2vs_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">year_w2v</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="32-查看词汇量">3.2 查看词汇量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;省份Word2vec词汇量&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">provin_w2v_f</span><span class="p">,</span> <span class="n">provin_w2v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">provin_w2v_fs</span><span class="p">,</span> <span class="n">provin_w2vs_</span><span class="p">):</span>
    <span class="n">province</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]+&#39;</span><span class="p">,</span> <span class="n">provin_w2v_f</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">province</span><span class="si">}</span><span class="s1"> 词汇量: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">provin_w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">省份Word2vec词汇量</span>
<span class="n">北京市</span><span class="p">:</span> <span class="mi">679126</span>
<span class="n">安徽省</span><span class="p">:</span> <span class="mi">471459</span>
<span class="n">江西省</span><span class="p">:</span> <span class="mi">216389</span>
<span class="n">重庆市</span><span class="p">:</span> <span class="mi">269875</span>
<span class="n">澳门特别行政区</span><span class="p">:</span> <span class="mi">4235</span>
<span class="n">湖北省</span><span class="p">:</span> <span class="mi">416464</span>
<span class="n">吉林省</span><span class="p">:</span> <span class="mi">169665</span>
<span class="n">香港特别行政区</span><span class="p">:</span> <span class="mi">37948</span>
<span class="n">天津市</span><span class="p">:</span> <span class="mi">323214</span>
<span class="n">湖南省</span><span class="p">:</span> <span class="mi">341033</span>
<span class="n">辽宁省</span><span class="p">:</span> <span class="mi">331955</span>
<span class="n">广东省</span><span class="p">:</span> <span class="mi">935412</span>
<span class="n">黑龙江省</span><span class="p">:</span> <span class="mi">223448</span>
<span class="n">其他国家</span><span class="p">:</span> <span class="mi">460</span>
<span class="n">河北省</span><span class="p">:</span> <span class="mi">281543</span>
<span class="n">内蒙古自治区</span><span class="p">:</span> <span class="mi">103331</span>
<span class="n">河南省</span><span class="p">:</span> <span class="mi">381151</span>
<span class="n">福建省</span><span class="p">:</span> <span class="mi">367768</span>
<span class="n">贵州省</span><span class="p">:</span> <span class="mi">163641</span>
<span class="n">陕西省</span><span class="p">:</span> <span class="mi">332231</span>
<span class="n">云南省</span><span class="p">:</span> <span class="mi">174191</span>
<span class="n">浙江省</span><span class="p">:</span> <span class="mi">751976</span>
<span class="n">甘肃省</span><span class="p">:</span> <span class="mi">125789</span>
<span class="n">台湾省</span><span class="p">:</span> <span class="mi">175827</span>
<span class="n">宁夏回族自治区</span><span class="p">:</span> <span class="mi">65428</span>
<span class="n">山西省</span><span class="p">:</span> <span class="mi">163094</span>
<span class="n">山东省</span><span class="p">:</span> <span class="mi">633187</span>
<span class="n">江苏省</span><span class="p">:</span> <span class="mi">928838</span>
<span class="n">海南省</span><span class="p">:</span> <span class="mi">59742</span>
<span class="n">西藏自治区</span><span class="p">:</span> <span class="mi">14884</span>
<span class="n">青海省</span><span class="p">:</span> <span class="mi">40403</span>
<span class="n">广西壮族自治区</span><span class="p">:</span> <span class="mi">237805</span>
<span class="n">四川省</span><span class="p">:</span> <span class="mi">437751</span>
<span class="n">新疆维吾尔自治区</span><span class="p">:</span> <span class="mi">100079</span>
<span class="n">上海市</span><span class="p">:</span> <span class="mi">537777</span>
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;年份word2vec词汇量&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">year_w2v_f</span><span class="p">,</span> <span class="n">year_w2v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">year_w2v_fs</span><span class="p">,</span> <span class="n">year_w2vs_</span><span class="p">):</span>
    <span class="n">year</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;\d</span><span class="si">{4}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">year_w2v_f</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">year_w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">年份word2vec词汇量
2022: 191145
2021: 605364
2020: 903230
2019: 901583
2018: 911762
2017: 881858
2016: 810445
2015: 621050
2014: 388647
2013: 517991
2012: 484417
2011: 281045
2010: 264882
2009: 245851
2008: 218349
2007: 191569
2006: 177269
2005: 153533
2004: 130384
2003: 120306
2002: 102266
2001: 78116
2000: 63233
1999: 53341
1998: 47007
1997: 44221
1996: 42709
1995: 40084
1994: 40987
1993: 42781
1992: 40149
1991: 33159
1990: 28480
1989: 23548
1988: 23502
1987: 19851
1986: 14330
1985: 11535
1984: 2
1980: 2
1900: 1
1899: 4
</code></pre></div><br>
<h3 id="33-语义检查-省份">3.3 语义检查-省份</h3>
<p>先检查省份， 查看与[&lsquo;创新&rsquo;, &lsquo;新颖&rsquo;]最相似的5个词，通过语义捕捉准确与否，粗略判断Word2vec训练效果的好坏。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>

<span class="k">for</span> <span class="n">provin_w2v_f</span><span class="p">,</span> <span class="n">provin_w2v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">provin_w2v_fs</span><span class="p">,</span> <span class="n">provin_w2vs_</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">province</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;[</span><span class="se">\u4e00</span><span class="s1">-</span><span class="se">\u9fa5</span><span class="s1">]+&#39;</span><span class="p">,</span> <span class="n">provin_w2v_f</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">wordweigths</span> <span class="o">=</span> <span class="n">provin_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;新颖&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">p</span> <span class="ow">in</span> <span class="n">wordweigths</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">province</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">province</span><span class="si">}</span><span class="s1">: &#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">北京市: 独特 全新 创造性 独创 先进
安徽省: 简洁 独特 精巧 精简 原理简单
江西省: 巧妙 简单实用 简单可靠 简单成本低 精巧
重庆市: 先进 独特 现代 科学 易用
澳门特别行政区: 
湖北省: 巧妙 精巧 科学 精良 简洁
吉林省: 先进 科学 推广 独特 可行
香港特别行政区: 工业化生产 公知 相对现有 克服现有 石头纸
天津市: 独特 先进 精巧 科学 构思
湖南省: 科学 简洁 先进 简明 独特
辽宁省: 科学 理念 构思 先进 独特
广东省: 独特 创新性 巧妙 科学 简单巧妙
黑龙江省: 科学 独特 精巧 先进 小巧
其他国家: 
河北省: 科学合理 新颖独特 简单实用 科学 精巧
内蒙古自治区: 经济实用 广泛使用 资源丰富 应用广泛 应用范围广
河南省: 简单实用 独特 较为新颖 精巧 简明
福建省: 原理简单 简单实用 巧妙 灵巧 独特
贵州省: 取材方便 价格便宜 安全经济 生产成本低廉 原料易得
陕西省: 独特 简单实用 结构新颖 小巧 简约
云南省: 科学 构思新颖 独特 价廉 简便易行,
浙江省: 独特 科学 全新 巧妙 较为合理
甘肃省: 先进 切实可行 性能优良 独特 一种较为理想
台湾省: 独特 多元 特有 经济 特色
宁夏回族自治区: 重量轻 体积小 性能可靠 具有使用方便 功能丰富
山西省: 独特 科学 先进 简洁 广泛
山东省: 新颖独特 巧妙 精巧 独特 先进
江苏省: 独特 全新 构思 精巧 简单巧妙
海南省: 多样 传统工艺 改变传统 制作过程 精华素
西藏自治区: 
青海省: 投饵 具有重量轻 具有成本低 极为 隔热保温
广西壮族自治区: 先进 独特 现代 广泛 明
四川省: 独特 先进 科学合理 人性化 科学
新疆维吾尔自治区: 现代 方
 携带方便 广泛 原料来源
上海市: 独特 设计理念 巧妙 科学 构思
</code></pre></div><p>从上面的运行结果看， 绝大多数的省份Word2vec都很准确的捕捉到了专利摘要的语义信息。</p>
<br>
<h3 id="34-语义检查-年份">3.4 语义检查-年份</h3>
<p>查看与[&lsquo;创新&rsquo;, &lsquo;新颖&rsquo;]最相似的5个词，通过语义捕捉准确与否，粗略判断Word2vec训练效果的好坏。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>

<span class="k">for</span> <span class="n">year_w2v_f</span><span class="p">,</span> <span class="n">year_w2v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">year_w2v_fs</span><span class="p">,</span> <span class="n">year_w2vs_</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">year</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;\d</span><span class="si">{4}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">year_w2v_f</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">wordweigths</span> <span class="o">=</span> <span class="n">year_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;新颖&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">p</span> <span class="ow">in</span> <span class="n">wordweigths</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">: &#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2022: 出错率 同井 Git 铝合金膜 传动件传动
2021: 铁前驱 甲基氢 单向阀进液 储存罐 Seq2Seq
2020: 减温层 洗浴间 球形活性炭 OASQ 目标语言句子
2019: 面膜布 自承式 网状加强 铲衣 温头
2018: 简单易行 工艺流程 成本高 克服现有 操作简便
2017: 问题 容易 良好 性能 实用
2016: 相互绝缘 不需 未知 窄边框 耐腐层
2015: 人体温度 压盖 后座 漏电检测 截骨刀
2014: 循环热泵 特细 多处 尾轴 名称内裤
2013: 精制 重复性 强 优势 少
2012: 提升 解决 多种 能够实现 避免
2011: 光亮 具有高 分层 木 品
2010: 转换 具有 强度 整个 时间

2009: 科学 先进 独特 创造性 全新
2008: 独特 先进 全新 思路 创造性
2007: 独特 全新 先进 现代 科学
2006: 先进 独特 全新 新颖性 创造性
2005: 科学 独特 简洁 简捷 独特,
2004: 独特 简单易行 科学 优越性 经济
2003: 独特 科学 多样, 实用 经济实用
2002: 研制 ,采取 多方面 效果差 直接应用

2001: 人造 管材 废弃 有机结合 缺点
2000: 防滑 光亮 着色 系列产品 清晰
1999: ,适于 病人 价廉 ,目前 害虫
1998: 优化 城市 设施 磁化 节水
1997: 病症 手工 透气 不足, 不足
1996: 制造 ,属于 应用 实现 传递文字
1995: 显著疗效 味美 极佳 活血 明目
1994: 样式 坚固耐用, 便利, 巧妙 使用方便安全
1993: 先进 完善 知识性 成本高 功能单一
1992: 经济实用 现行 ,实为 住房 结构设计
1991: 易损坏 保留 庞大 普遍 物两用
1990: 笔算 不足之处, 功能单一 ,需用 结构复杂
1989: 缺点 力小 设施 临时 不便
1988: 设施, 改造 美发 油田 机械装置
1987: 各种类型 用途单一 之用 水上 室内外
1986: 飞行 化油器 具 显微镜 雨水
1985: 很大 浪费 器件 地区 具有结构
1984: 
1980: 
1900: 
1899: 
</code></pre></div><p>也试了其他的词语，好像只有 2002-2009 之间的语义是准确的。 <strong>原因未知，也训练了<a href="https://textdata.cn/blog/2023-11-18-failure-example-show-word-meaning-shift-using-word2vec/">裁判文书的Word2vec</a>,  年份的是一点准头都没有。专利数据训练的好在还有点准头</strong>。</p>
<br>
<h2 id="四研究潜力-语义变迁研究方法介绍">四、研究潜力: 语义变迁研究方法介绍</h2>
<p>假设语义都很准的话， 是可以研究 <strong>语义变迁</strong> 或者 <strong>语义差异</strong> 的。 但需要注意， 不能直接使用两个年份或者两个省份的中word1和word2的距离来体现语义的变迁或者语义的差异。 如果想做省份间差异或者某省份随时间的变化， 需要用到 <strong>对齐算法</strong>， 常用的算法是 <strong>正交Procrustes矩阵对齐</strong>， 使得同省份不同年份或者通年份不同省份的word2vec都有相同的语义空间。</p>
<h3 id="41-正交procrustes算法">4.1 正交Procrustes算法</h3>
<p>正交Procrustes矩阵对齐是一种将两个预训练语言模型的词向量矩阵对齐的方法，使得它们在相同的语义空间中表示。具体来说，它通过计算一个正交矩阵，将两个词向量矩阵进行线性变换，使得它们的Frobenius范数之和最小，从而实现对齐。 gensim库有该算法，大邓后续有时间会分享如何用Procrustes对齐语言模型。</p>
<br>
<h3 id="42-语义变迁流程图">4.2 语义变迁流程图</h3>
<p>语义变迁类研究的流程图可参考 <a href="https://github.com/Living-with-machines/DiachronicEmb-BigHistData">DiachronicEmb-BigHistData</a></p>
<p><img loading="lazy" src="img/w2v-time-shifting-flowchart.png" alt=""  />
</p>
<br>
<h3 id="43-识别语义变化时间点">4.3 识别语义变化时间点</h3>
<p>该项目研究了1800-1910期间， 每10年为一个单位训练词向量， 探究词语变化。以 <em><strong>railway</strong></em> 和  <em><strong>traffic</strong></em> 为例, 先用 余弦相似度(cosine-similarity)算法识别词语语义变化的时间点，如下图</p>
<p><img loading="lazy" src="img/consine-sim-cpdetection.png" alt=""  />
</p>
<br>
<h3 id="44-绘制语义变化轨迹">4.4 绘制语义变化轨迹</h3>
<p>语义变化轨迹</p>
<p><img loading="lazy" src="img/railway-time-shifting.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="五获取资源">五、获取资源</h2>
<h3 id="51-免费">5.1 免费</h3>
<ul>
<li>年份词向量 链接: <a href="https://pan.baidu.com/s/1-EngiWU9IAkqfk2Qf2W5lA">https://pan.baidu.com/s/1-EngiWU9IAkqfk2Qf2W5lA</a> 提取码: d925</li>
<li>省份词向量  链接: <a href="https://pan.baidu.com/s/1TnZf5YkRZMWf4liN04XO_g">https://pan.baidu.com/s/1TnZf5YkRZMWf4liN04XO_g</a> 提取码: cjs8</li>
</ul>
<br>
<h3 id="52-付费">5.2 付费</h3>
<p>内容整理不易， 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」</p>
<ul>
<li>100元  cntext-2.1.1-py3-none-any.whl</li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>失败案例 |  使用裁判文书数据集逐年训练年份词向量</title>
      <link>https://textdata.cn/blog/2023-11-18-failure-example-show-word-meaning-shift-using-word2vec/</link>
      <pubDate>Sat, 18 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-18-failure-example-show-word-meaning-shift-using-word2vec/</guid>
      <description>&lt;p&gt;想用裁判文书数据集，逐年训练词向量的同学，可以好好看本文，能节省你几十个小时时间。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一检查数据&#34;&gt;一、检查数据&lt;/h2&gt;
&lt;p&gt;裁判文书数据集，每个月份存储到一个csv， 每个年份有一个对应的文件夹。下图是 2021 年的文件夹截图
&lt;img loading=&#34;lazy&#34; src=&#34;img/2021.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;csv字段格式是一致的，我们只需要找一个文件，尝试着读取前5行，查看数据中有哪些字段。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2013/2013-01.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nrows&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;文书内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二构造语料&#34;&gt;二、构造语料&lt;/h2&gt;
&lt;p&gt;我们只从csv中选取 &amp;ldquo;&lt;strong&gt;文书内容&lt;/strong&gt;&amp;rdquo; ，并将其存储到语料txt文件中。&lt;/p&gt;
&lt;p&gt;考虑到电脑性能， 预料不要太大， 1G左右是比较适中，在电脑内存为8G的情况下，应该能跑通。&lt;/p&gt;
&lt;p&gt;2010/2011/2013这三个年度的数据只有几百M， 数据全部保留。 剩下的年份，设置不同的抽样比例，尽可能将生成的语料txt文件控制在1G左右。下面是经过粗略计算设定的比例，实际最终数据控制在800M左右。&lt;/p&gt;
&lt;p&gt;裁判文书数据量高达300G， 读取、抽样、存储，全部过程耗时大概6小时。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;年份&lt;/th&gt;
&lt;th&gt;解压后文件大小&lt;/th&gt;
&lt;th&gt;抽样比例&lt;/th&gt;
&lt;th&gt;语料txt大小&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2010&lt;/td&gt;
&lt;td&gt;761M&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;684M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2011&lt;/td&gt;
&lt;td&gt;452M&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;396M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2012&lt;/td&gt;
&lt;td&gt;757M&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;665M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2013&lt;/td&gt;
&lt;td&gt;5.13G&lt;/td&gt;
&lt;td&gt;20%&lt;/td&gt;
&lt;td&gt;984M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;td&gt;23.7G&lt;/td&gt;
&lt;td&gt;4%&lt;/td&gt;
&lt;td&gt;905M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td&gt;33.6G&lt;/td&gt;
&lt;td&gt;3%&lt;/td&gt;
&lt;td&gt;968M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;td&gt;39.9G&lt;/td&gt;
&lt;td&gt;2.4%&lt;/td&gt;
&lt;td&gt;914M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td&gt;44.6G&lt;/td&gt;
&lt;td&gt;2.2%&lt;/td&gt;
&lt;td&gt;882M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;24.8G&lt;/td&gt;
&lt;td&gt;4%&lt;/td&gt;
&lt;td&gt;875M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;48.3G&lt;/td&gt;
&lt;td&gt;2%&lt;/td&gt;
&lt;td&gt;833M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;91.2G&lt;/td&gt;
&lt;td&gt;1%&lt;/td&gt;
&lt;td&gt;779M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;32.3G&lt;/td&gt;
&lt;td&gt;3%&lt;/td&gt;
&lt;td&gt;816M&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 年份、抽样比例&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_fracs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2010&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2011&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2012&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; 
    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2013&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2014&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.04&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2015&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.03&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2016&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2017&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.022&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2018&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.04&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2019&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2020&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2021&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.03&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;


 &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frac&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_fracs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;正在构造 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 年的语料txt文件&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;裁判文书&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;yf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;# &lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.csv&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;# 为节省内存开销， &lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;# 只读 csv 中的 “文书内容” 一个字段，&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;# 且设置 chunksize 分批次读取&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;usecols&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;文书内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunksize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;文书内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;mdf&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;frac&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;frac&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;文书内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;yf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三训练word2vec&#34;&gt;三、训练word2vec&lt;/h2&gt;
&lt;p&gt;使用data内的语料txt，每个txt训练出一个对应的word2vec，结果自动存储到output/Word2Vec&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-corpus.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;使用cntext2.0.0， 代码如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;txtfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.txt&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;txtf&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;txtfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;txtf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2VModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;txtf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;裁判文书年份、语料txt大小及训练时间长度汇总如下表&lt;/p&gt;
&lt;br&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;年份&lt;/th&gt;
&lt;th&gt;语料txt大小&lt;/th&gt;
&lt;th&gt;训练word2vec耗时&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2010&lt;/td&gt;
&lt;td&gt;684M&lt;/td&gt;
&lt;td&gt;2127s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2011&lt;/td&gt;
&lt;td&gt;396M&lt;/td&gt;
&lt;td&gt;1225s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2012&lt;/td&gt;
&lt;td&gt;665M&lt;/td&gt;
&lt;td&gt;2105s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2013&lt;/td&gt;
&lt;td&gt;984M&lt;/td&gt;
&lt;td&gt;2967s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;td&gt;905M&lt;/td&gt;
&lt;td&gt;2810s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td&gt;968M&lt;/td&gt;
&lt;td&gt;3032s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;td&gt;914M&lt;/td&gt;
&lt;td&gt;2880s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td&gt;882M&lt;/td&gt;
&lt;td&gt;2882s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;875M&lt;/td&gt;
&lt;td&gt;2852s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;833M&lt;/td&gt;
&lt;td&gt;2765s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;779M&lt;/td&gt;
&lt;td&gt;2539s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;816M&lt;/td&gt;
&lt;td&gt;2609s&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三使用word2vec&#34;&gt;三、使用word2vec&lt;/h2&gt;
&lt;p&gt;训练结果如下图&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-word2vec-models.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;31-导入模型&#34;&gt;3.1 导入模型&lt;/h3&gt;
&lt;p&gt;output/Word2Vec中有多个年份的模型， 模型文件不大， 如果内存允许，可以同时导入。首先要获取模型文件路径&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_fs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/Word2Vec/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/Word2Vec&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.npy&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_fs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;output/Word2Vec/裁判文书2010.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2011.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2012.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2013.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2014.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2015.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2016.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2017.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2018.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2019.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2020.100.6.bin&amp;#39;,
 &amp;#39;output/Word2Vec/裁判文书2021.100.6.bin&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_models&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;findall&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;\d&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{4}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{year}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;w2v_models&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2010
Loading word2vec model...

2011
Loading word2vec model...

2012
Loading word2vec model...

......


2021
Loading word2vec model...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-模型词汇量&#34;&gt;3.2 模型词汇量&lt;/h3&gt;
&lt;p&gt;查看不同年份模型的词汇量&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_models&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;wordnum&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;词汇量: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wordnum&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2010词汇量: 374105
2011词汇量: 312039
2012词汇量: 490673
2013词汇量: 675057
2014词汇量: 634497
2015词汇量: 667753
2016词汇量: 638568
2017词汇量: 656776
2018词汇量: 667265
2019词汇量: 629285
2020词汇量: 582988
2021词汇量: 571346
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;33-语义检查&#34;&gt;3.3 语义检查&lt;/h3&gt;
&lt;p&gt;先查看2020年的， 以 “&lt;strong&gt;犯罪&lt;/strong&gt;” 为例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 最相似的10个词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_models&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;犯罪&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;认真接受&amp;#39;, 0.9499362707138062),
 (&amp;#39;民事诉讼法&amp;#39;, 0.9497376084327698),
 (&amp;#39;建设工程有限公司&amp;#39;, 0.9491338729858398),
 (&amp;#39;望春监狱&amp;#39;, 0.9488678574562073),
 (&amp;#39;人身损害赔偿&amp;#39;, 0.9487593173980713),
 (&amp;#39;判决发生&amp;#39;, 0.9485785365104675),
 (&amp;#39;本案受理费&amp;#39;, 0.9484607577323914),
 (&amp;#39;裁定准许&amp;#39;, 0.9480699896812439),
 (&amp;#39;现在宁波市&amp;#39;, 0.9479051828384399),
 (&amp;#39;温州银行&amp;#39;, 0.9478054046630859)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;同时检查2010-2021， 分别返回前5个最相似的词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;与“犯罪”最相似5个词&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_models&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;wordtuples&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;犯罪&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wordtuples&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;模型: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2010模型: 认真接受 民事诉讼法 建设工程有限公司 望春监狱 人身损害赔偿
2011模型: 欲证明 辩护意见 被告中铁 应承担 蔡某甲
2012模型: �� 判处有期徒刑 盒 执行 黄某乙
2013模型: 雇员受害 AFT8 箐 牛鲁敬 被告金析航
2014模型: 齐立权 永川支公司 徐正青 万给 鲁子双
2015模型: 蒋明良 19KM 二百一十八条 一定独创性 类型主要
2016模型: 钧益公司 王乌旦 未予缴纳 之后离开 元系金
2017模型: 巫山县振兴 工程承揽 立奥 会展 皖1004
2018模型: 浙湖 公路东向西 雷德佑 福建省长汀县 辽宁成工
2019模型: 6.2475% 郑善 陈能颂 招某 14726.91
2020模型: 促进法 流传 四川省米易县 有奴 共有产权
2021模型: 柳凯 张鲁 几张照片 挪走 耿正会
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;额， 每个模型不能说跟“犯罪”毫无关系，只能说是一毛钱关系都没有！难道是我选的词有问题，错怪模型，那再试试“婚姻”&lt;/p&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;与“婚姻”最相似5个词&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;end&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_models&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;wordtuples&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;婚姻&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wordtuples&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;模型: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;与“婚姻”最相似5个词

2010模型: 签收 相关 姜明 20107 依法
2011模型: 下列条件 连带责任保证 债务 举证质证 申请撤诉
2012模型: 竹乐 程志街 7808292.65 乐至刑初 衢江
2013模型: 分歧双方 孙明霞〇 裁明 孙林 多派
2014模型: 山林土地 道成 被告肖小健 董锐 梓民初
2015模型: 秀洲 永乐街道 此笔费用 上特阀业 唐厚洪
2016模型: 其于2007 故予认定 千荣公司 工程验收报告 邢子
2017模型: 拆排栅 53612 初字252 被告彭国强 鸿恒昌公司
2018模型: 淄博分中心 三厅 临街门市 20200 科健
2019模型: 新岗位 2006) 风险稳控 主要树种 重量价格
2020模型: 模板款 生活用房 从轻处理建议 张春林 刘俊平
2021模型: 时二乐 对童 黄青书 一经通知 电话问
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;哎！ 依然是彼此毫无语义关系， 种种迹象表明，这些训练出的模型就真不咋地！&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四原因分析&#34;&gt;四、原因分析&lt;/h2&gt;
&lt;p&gt;如果说数据没有清洗，去停用词，可能会干扰训练效果。但是我经过去停词等数据清洗，训练得到的模型表现与之前没啥变化，依然是捕捉不到语义关系信息。&lt;br&gt;&lt;/p&gt;
&lt;p&gt;至于数据量大小的问题， 大邓之前分享的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/&#34;&gt;预训练模型 | 金融会计类word2vec， 可扩展或构建领域内概念情感词典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/&#34;&gt;预训练模型 | 使用1000w专利摘要训练word2vec模型，可用于开发词典&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两个推文中使用的语料都是好几个G的语料txt， 基本上语义捕捉的都很完美。但前几天 &lt;a href=&#34;https://textdata.cn/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/&#34;&gt;词向量 | 使用1亿B站用户签名训练word2vec词向量&lt;/a&gt; 中语料只有302M， 但语义信息捕捉的很好。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>想用裁判文书数据集，逐年训练词向量的同学，可以好好看本文，能节省你几十个小时时间。</p>
<p><br><br></p>
<h2 id="一检查数据">一、检查数据</h2>
<p>裁判文书数据集，每个月份存储到一个csv， 每个年份有一个对应的文件夹。下图是 2021 年的文件夹截图
<img loading="lazy" src="img/2021.png" alt=""  />
</p>
<br>
<p>csv字段格式是一致的，我们只需要找一个文件，尝试着读取前5行，查看数据中有哪些字段。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;2013/2013-01.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;文书内容&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二构造语料">二、构造语料</h2>
<p>我们只从csv中选取 &ldquo;<strong>文书内容</strong>&rdquo; ，并将其存储到语料txt文件中。</p>
<p>考虑到电脑性能， 预料不要太大， 1G左右是比较适中，在电脑内存为8G的情况下，应该能跑通。</p>
<p>2010/2011/2013这三个年度的数据只有几百M， 数据全部保留。 剩下的年份，设置不同的抽样比例，尽可能将生成的语料txt文件控制在1G左右。下面是经过粗略计算设定的比例，实际最终数据控制在800M左右。</p>
<p>裁判文书数据量高达300G， 读取、抽样、存储，全部过程耗时大概6小时。</p>
<table>
<thead>
<tr>
<th>年份</th>
<th>解压后文件大小</th>
<th>抽样比例</th>
<th>语料txt大小</th>
</tr>
</thead>
<tbody>
<tr>
<td>2010</td>
<td>761M</td>
<td>100%</td>
<td>684M</td>
</tr>
<tr>
<td>2011</td>
<td>452M</td>
<td>100%</td>
<td>396M</td>
</tr>
<tr>
<td>2012</td>
<td>757M</td>
<td>100%</td>
<td>665M</td>
</tr>
<tr>
<td>2013</td>
<td>5.13G</td>
<td>20%</td>
<td>984M</td>
</tr>
<tr>
<td>2014</td>
<td>23.7G</td>
<td>4%</td>
<td>905M</td>
</tr>
<tr>
<td>2015</td>
<td>33.6G</td>
<td>3%</td>
<td>968M</td>
</tr>
<tr>
<td>2016</td>
<td>39.9G</td>
<td>2.4%</td>
<td>914M</td>
</tr>
<tr>
<td>2017</td>
<td>44.6G</td>
<td>2.2%</td>
<td>882M</td>
</tr>
<tr>
<td>2018</td>
<td>24.8G</td>
<td>4%</td>
<td>875M</td>
</tr>
<tr>
<td>2019</td>
<td>48.3G</td>
<td>2%</td>
<td>833M</td>
</tr>
<tr>
<td>2020</td>
<td>91.2G</td>
<td>1%</td>
<td>779M</td>
</tr>
<tr>
<td>2021</td>
<td>32.3G</td>
<td>3%</td>
<td>816M</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># 年份、抽样比例</span>
<span class="n">year_fracs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;2010&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2011&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2012&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
    <span class="p">(</span><span class="s1">&#39;2013&#39;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2014&#39;</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2015&#39;</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;2016&#39;</span><span class="p">,</span> <span class="mf">0.024</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2017&#39;</span><span class="p">,</span> <span class="mf">0.022</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2018&#39;</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;2019&#39;</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2020&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2021&#39;</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">)</span>
    <span class="p">]</span>


 <span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">frac</span> <span class="ow">in</span> <span class="n">year_fracs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;正在构造 </span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1"> 年的语料txt文件&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;裁判文书</span><span class="si">{}</span><span class="s1">.txt&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="p">),</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">yf</span><span class="p">:</span>
        <span class="c1"># </span>
        <span class="n">csvfs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">csvf</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">csvf</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">year</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.csv&#39;</span> <span class="ow">in</span> <span class="n">csvf</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">csvf</span> <span class="ow">in</span> <span class="n">csvfs</span><span class="p">:</span>
            <span class="c1"># 为节省内存开销， </span>
            <span class="c1"># 只读 csv 中的 “文书内容” 一个字段，</span>
            <span class="c1"># 且设置 chunksize 分批次读取</span>
            <span class="n">chunk_dfs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;文书内容&#39;</span><span class="p">],</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">chunk_df</span> <span class="ow">in</span> <span class="n">chunk_dfs</span><span class="p">:</span>
                <span class="n">chunk_df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;文书内容&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">mdf</span> <span class="o">=</span> <span class="n">chunk_df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="n">frac</span><span class="p">)</span>
                <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">mdf</span><span class="p">[</span><span class="s1">&#39;文书内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
                <span class="n">yf</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="三训练word2vec">三、训练word2vec</h2>
<p>使用data内的语料txt，每个txt训练出一个对应的word2vec，结果自动存储到output/Word2Vec</p>
<p><img loading="lazy" src="img/01-corpus.png" alt=""  />
</p>
<p>使用cntext2.0.0， 代码如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="n">txtfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.txt&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
<span class="k">for</span> <span class="n">txtf</span> <span class="ow">in</span> <span class="n">txtfs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">txtf</span><span class="p">)</span>
    <span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="n">txtf</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
    <span class="n">w2v_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div><p>裁判文书年份、语料txt大小及训练时间长度汇总如下表</p>
<br>
<table>
<thead>
<tr>
<th>年份</th>
<th>语料txt大小</th>
<th>训练word2vec耗时</th>
</tr>
</thead>
<tbody>
<tr>
<td>2010</td>
<td>684M</td>
<td>2127s</td>
</tr>
<tr>
<td>2011</td>
<td>396M</td>
<td>1225s</td>
</tr>
<tr>
<td>2012</td>
<td>665M</td>
<td>2105s</td>
</tr>
<tr>
<td>2013</td>
<td>984M</td>
<td>2967s</td>
</tr>
<tr>
<td>2014</td>
<td>905M</td>
<td>2810s</td>
</tr>
<tr>
<td>2015</td>
<td>968M</td>
<td>3032s</td>
</tr>
<tr>
<td>2016</td>
<td>914M</td>
<td>2880s</td>
</tr>
<tr>
<td>2017</td>
<td>882M</td>
<td>2882s</td>
</tr>
<tr>
<td>2018</td>
<td>875M</td>
<td>2852s</td>
</tr>
<tr>
<td>2019</td>
<td>833M</td>
<td>2765s</td>
</tr>
<tr>
<td>2020</td>
<td>779M</td>
<td>2539s</td>
</tr>
<tr>
<td>2021</td>
<td>816M</td>
<td>2609s</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="三使用word2vec">三、使用word2vec</h2>
<p>训练结果如下图</p>
<p><img loading="lazy" src="img/02-word2vec-models.png" alt=""  />
</p>
<br>
<h3 id="31-导入模型">3.1 导入模型</h3>
<p>output/Word2Vec中有多个年份的模型， 模型文件不大， 如果内存允许，可以同时导入。首先要获取模型文件路径</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>

<span class="n">w2v_fs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;output/Word2Vec/</span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;output/Word2Vec&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.npy&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
<span class="n">w2v_fs</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;output/Word2Vec/裁判文书2010.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2011.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2012.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2013.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2014.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2015.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2016.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2017.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2018.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2019.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2020.100.6.bin&#39;,
 &#39;output/Word2Vec/裁判文书2021.100.6.bin&#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="n">w2v_models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">years</span> <span class="o">=</span> <span class="p">[</span><span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s1">&#39;\d</span><span class="si">{4}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">f</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">w2v_fs</span><span class="p">]</span>
<span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">w2v_f</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">w2v_fs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{year}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">w2v_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">w2v_f</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2010
Loading word2vec model...

2011
Loading word2vec model...

2012
Loading word2vec model...

......


2021
Loading word2vec model...
</code></pre></div><br>
<h3 id="32-模型词汇量">3.2 模型词汇量</h3>
<p>查看不同年份模型的词汇量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>

<span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">w2v_model</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">w2v_models</span><span class="p">):</span>
    <span class="n">wordnum</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">词汇量: </span><span class="si">{</span><span class="n">wordnum</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2010词汇量: 374105
2011词汇量: 312039
2012词汇量: 490673
2013词汇量: 675057
2014词汇量: 634497
2015词汇量: 667753
2016词汇量: 638568
2017词汇量: 656776
2018词汇量: 667265
2019词汇量: 629285
2020词汇量: 582988
2021词汇量: 571346
</code></pre></div><br>
<h3 id="33-语义检查">3.3 语义检查</h3>
<p>先查看2020年的， 以 “<strong>犯罪</strong>” 为例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 最相似的10个词</span>
<span class="n">w2v_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;犯罪&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;认真接受&#39;, 0.9499362707138062),
 (&#39;民事诉讼法&#39;, 0.9497376084327698),
 (&#39;建设工程有限公司&#39;, 0.9491338729858398),
 (&#39;望春监狱&#39;, 0.9488678574562073),
 (&#39;人身损害赔偿&#39;, 0.9487593173980713),
 (&#39;判决发生&#39;, 0.9485785365104675),
 (&#39;本案受理费&#39;, 0.9484607577323914),
 (&#39;裁定准许&#39;, 0.9480699896812439),
 (&#39;现在宁波市&#39;, 0.9479051828384399),
 (&#39;温州银行&#39;, 0.9478054046630859)]
</code></pre></div><p>同时检查2010-2021， 分别返回前5个最相似的词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;与“犯罪”最相似5个词&#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">w2v_model</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">w2v_models</span><span class="p">):</span>
    <span class="n">wordtuples</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;犯罪&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">wordtuples</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">模型: </span><span class="si">{</span><span class="n">words</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2010模型: 认真接受 民事诉讼法 建设工程有限公司 望春监狱 人身损害赔偿
2011模型: 欲证明 辩护意见 被告中铁 应承担 蔡某甲
2012模型: �� 判处有期徒刑 盒 执行 黄某乙
2013模型: 雇员受害 AFT8 箐 牛鲁敬 被告金析航
2014模型: 齐立权 永川支公司 徐正青 万给 鲁子双
2015模型: 蒋明良 19KM 二百一十八条 一定独创性 类型主要
2016模型: 钧益公司 王乌旦 未予缴纳 之后离开 元系金
2017模型: 巫山县振兴 工程承揽 立奥 会展 皖1004
2018模型: 浙湖 公路东向西 雷德佑 福建省长汀县 辽宁成工
2019模型: 6.2475% 郑善 陈能颂 招某 14726.91
2020模型: 促进法 流传 四川省米易县 有奴 共有产权
2021模型: 柳凯 张鲁 几张照片 挪走 耿正会
</code></pre></div><p>额， 每个模型不能说跟“犯罪”毫无关系，只能说是一毛钱关系都没有！难道是我选的词有问题，错怪模型，那再试试“婚姻”</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;与“婚姻”最相似5个词&#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">w2v_model</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">years</span><span class="p">,</span> <span class="n">w2v_models</span><span class="p">):</span>
    <span class="n">wordtuples</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;婚姻&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">wordtuples</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">模型: </span><span class="si">{</span><span class="n">words</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">与“婚姻”最相似5个词

2010模型: 签收 相关 姜明 20107 依法
2011模型: 下列条件 连带责任保证 债务 举证质证 申请撤诉
2012模型: 竹乐 程志街 7808292.65 乐至刑初 衢江
2013模型: 分歧双方 孙明霞〇 裁明 孙林 多派
2014模型: 山林土地 道成 被告肖小健 董锐 梓民初
2015模型: 秀洲 永乐街道 此笔费用 上特阀业 唐厚洪
2016模型: 其于2007 故予认定 千荣公司 工程验收报告 邢子
2017模型: 拆排栅 53612 初字252 被告彭国强 鸿恒昌公司
2018模型: 淄博分中心 三厅 临街门市 20200 科健
2019模型: 新岗位 2006) 风险稳控 主要树种 重量价格
2020模型: 模板款 生活用房 从轻处理建议 张春林 刘俊平
2021模型: 时二乐 对童 黄青书 一经通知 电话问
</code></pre></div><p>哎！ 依然是彼此毫无语义关系， 种种迹象表明，这些训练出的模型就真不咋地！</p>
<p><br><br></p>
<h2 id="四原因分析">四、原因分析</h2>
<p>如果说数据没有清洗，去停用词，可能会干扰训练效果。但是我经过去停词等数据清洗，训练得到的模型表现与之前没啥变化，依然是捕捉不到语义关系信息。<br></p>
<p>至于数据量大小的问题， 大邓之前分享的</p>
<ul>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">预训练模型 | 金融会计类word2vec， 可扩展或构建领域内概念情感词典</a></li>
<li><a href="https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/">预训练模型 | 使用1000w专利摘要训练word2vec模型，可用于开发词典</a></li>
</ul>
<p>两个推文中使用的语料都是好几个G的语料txt， 基本上语义捕捉的都很完美。但前几天 <a href="https://textdata.cn/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/">词向量 | 使用1亿B站用户签名训练word2vec词向量</a> 中语料只有302M， 但语义信息捕捉的很好。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>词向量 | 使用1亿B站用户签名训练word2vec词向量</title>
      <link>https://textdata.cn/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/</guid>
      <description>&lt;h2 id=&#34;一用户签名&#34;&gt;一、用户签名&lt;/h2&gt;
&lt;p&gt;1亿B站用户群体十分庞大，文本中蕴含着这个群体的认知信息(如兴趣、身份、座右铭等)，如果能用签名训练word2vec词向量模型，说不定就有利用这个模型，对每个用户签名进行量化,  对用户进行分类。 本文要解决&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;构建语料训练出模型&lt;/li&gt;
&lt;li&gt;简单看看模型训练效果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二准备语料&#34;&gt;二、准备语料&lt;/h2&gt;
&lt;p&gt;Kaggle网有1亿B站用户数据集，下载地址&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/datasets/beats0/bilibili-user&#34;&gt;https://www.kaggle.com/datasets/beats0/bilibili-user&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;之前分享过 &lt;a href=&#34;&#34;&gt;数据集 | 哔哩哔哩 1 亿用户数据&lt;/a&gt; ， 阅读此文可以熟悉pandas的一些基本操作，如数据读取、文本操作等。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#从kaggle下载B站1亿用户数据&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;User.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#签名非空的记录&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;sign&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;数据集用户数量: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据集用户数量: 100000000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;将9093092个非空签名汇总到 &lt;em&gt;&lt;strong&gt;B站用户签名语料.txt&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;B站用户签名语料.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sign&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tolist&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;代码运行后，得到 302M &lt;em&gt;&lt;strong&gt;B站用户签名语料.txt&lt;/strong&gt;&lt;/em&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三训练word2vec&#34;&gt;三、训练Word2Vec&lt;/h2&gt;
&lt;p&gt;我使用的自己 &lt;strong&gt;未公开&lt;/strong&gt; 的cntext 2.0.0版本， Bug频出，等调整好了再公开。&lt;/p&gt;
&lt;h3 id=&#34;31-安装cntext&#34;&gt;3.1 安装cntext&lt;/h3&gt;
&lt;p&gt;将 cntext-2.0.0-py3-none-any.whl 放置于桌面，打开 &lt;strong&gt;cmd&lt;/strong&gt;  (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 &lt;strong&gt;cmd&lt;/strong&gt;  (苹果电脑打开terminal) 中使用pip3 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install cntext-2.0.0-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文末有cntext-2.0.0-py3-none-any.whl获取方式&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-训练word2vec&#34;&gt;3.2 训练word2vec&lt;/h3&gt;
&lt;p&gt;cntext训练时候Word2Vec模型参数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;window = 6&lt;/li&gt;
&lt;li&gt;vector_size = 100&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#cntext2.0.0未公开，获取2.0.0请阅读文末获取方式&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2VModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;B站用户签名语料.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;window&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Starting Processing Corpus ...
Start Training! This may take a while. Please be patient...
Traning word2vec model took 1329 seconds
Note: The Word2Vec model has been saved to output/Word2Vec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;耗时1329s， 模型训练完成！得到的模型文件，如下截图&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/word2vec.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四使用word2vec&#34;&gt;四、使用word2vec&lt;/h2&gt;
&lt;h3 id=&#34;41-读取模型&#34;&gt;4.1 读取模型&lt;/h3&gt;
&lt;p&gt;使用gensim录入模型 “&lt;strong&gt;B站用户签名语料&lt;/strong&gt;.100.6.bin” ,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;gensim.models&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/Word2Vec/B站用户签名语料.100.6.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;模型词汇量: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;模型词汇量:  343650
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-查询某词的词向量&#34;&gt;4.2 查询某词的词向量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;w2v.wv[&amp;#39;高冷&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([-1.1872591e+00, -1.2438694e+00, -9.4200081e-01, -4.0355644e+00,
        3.3588424e-01, -4.2525402e-01, -4.1175735e-01,  1.8802526e+00,
       -3.5992053e-01,  2.8361969e+00, -1.1437206e+00,  4.4662678e-01,
        1.2098696e+00,  7.2761238e-02,  3.0633178e-01,  6.7496544e-01,
       -3.0175522e-01, -1.1573459e+00, -7.4999934e-01,  1.6484009e+00,
        1.3102854e+00,  1.0134944e+00,  1.0711597e+00, -1.2194906e+00,
        2.1523576e+00, -4.4606316e-01,  1.0664939e+00,  5.3640699e-01,
        1.1061722e+00, -2.7679250e-01,  8.6652651e-02,  1.6876321e-02,
        6.3120401e-01, -1.6914845e-02,  9.6446878e-01,  1.7046971e+00,
       -1.7395537e+00,  1.7914917e+00, -1.3069035e+00,  6.6770411e-01,
       -3.4902021e-01, -1.3345592e-02, -4.3595994e-01, -5.1443088e-01,
        1.0884547e+00, -3.3695351e-02, -5.7088321e-01,  1.4533000e+00,
        8.0498764e-03,  9.2341286e-01, -1.6219637e-01, -2.5400406e-01,
       -1.4215972e-01,  7.1577376e-01, -1.2812414e+00, -1.7630520e-01,
       -1.7600318e+00, -1.4759690e+00, -2.3313001e-01, -8.8995326e-01,
        4.5749718e-01,  2.1950055e-02,  2.5749078e-01, -7.6623499e-01,
        3.1834408e-03,  7.2732526e-01, -2.5459883e+00, -1.5240467e+00,
        2.4574394e+00,  7.3715413e-01,  2.2769032e+00,  1.7492032e+00,
       -4.1084039e-01,  6.4300962e-02,  6.2454116e-01, -4.0486854e-02,
        7.7100635e-01, -1.9729427e+00, -8.4130460e-01, -3.0110097e-01,
       -1.0786959e+00, -1.9136167e+00, -1.7037696e-01, -7.3208618e-03,
        4.8502678e-01, -1.0348318e+00,  3.1141058e-01,  2.9913974e-01,
       -2.1714316e-01, -3.1645024e+00,  7.0972210e-03,  7.8701675e-01,
       -2.2510442e-01, -9.8428482e-01,  1.0685140e+00,  2.1938827e+00,
       -9.1963351e-01,  6.3011467e-01, -1.1531134e+00, -9.2123538e-02],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;43-查看近义词&#34;&gt;4.3 查看近义词&lt;/h3&gt;
&lt;p&gt;通过给定词语，查看其近义词，可以了解模型训练的好坏。语义捕捉的合理，说明语料合理，模型训练的好。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#列表中可以传入任意多个词，这里大邓偷懒，都只传入了一两个词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;高冷&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;腹黑&amp;#39;, 0.8282514810562134),
 (&amp;#39;呆萌&amp;#39;, 0.8148132562637329),
 (&amp;#39;傲娇&amp;#39;, 0.7791209816932678),
 (&amp;#39;逗比&amp;#39;, 0.7720615863800049),
 (&amp;#39;闷骚&amp;#39;, 0.7617782354354858),
 (&amp;#39;精分&amp;#39;, 0.7545589208602905),
 (&amp;#39;文静&amp;#39;, 0.7545390725135803),
 (&amp;#39;慢热&amp;#39;, 0.7387350797653198),
 (&amp;#39;自恋&amp;#39;, 0.7299264669418335),
 (&amp;#39;淑女&amp;#39;, 0.7261008620262146),
 (&amp;#39;耿直&amp;#39;, 0.7238353490829468),
 (&amp;#39;帅气&amp;#39;, 0.7233086824417114),
 (&amp;#39;暖男&amp;#39;, 0.720333456993103),
 (&amp;#39;内向&amp;#39;, 0.7159033417701721),
 (&amp;#39;蠢&amp;#39;, 0.7157402038574219),
 (&amp;#39;逗逼&amp;#39;, 0.7091616988182068),
 (&amp;#39;神经质&amp;#39;, 0.7085140347480774),
 (&amp;#39;女汉子&amp;#39;, 0.707956850528717),
 (&amp;#39;毒舌&amp;#39;, 0.7058071494102478),
 (&amp;#39;逗&amp;#39;, 0.7048983573913574)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;女汉子&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;女汉纸&amp;#39;, 0.8832258582115173),
 (&amp;#39;汉子&amp;#39;, 0.8506060838699341),
 (&amp;#39;萌妹子&amp;#39;, 0.8475067615509033),
 (&amp;#39;暖男&amp;#39;, 0.8445340394973755),
 (&amp;#39;女神经&amp;#39;, 0.838117241859436),
 (&amp;#39;萌妹纸&amp;#39;, 0.8303463459014893),
 (&amp;#39;闷骚&amp;#39;, 0.8296418786048889),
 (&amp;#39;妹纸&amp;#39;, 0.8289912343025208),
 (&amp;#39;软妹子&amp;#39;, 0.8211091756820679),
 (&amp;#39;腹黑&amp;#39;, 0.8019399046897888),
 (&amp;#39;汉纸&amp;#39;, 0.7941007614135742),
 (&amp;#39;糙汉子&amp;#39;, 0.7915611267089844),
 (&amp;#39;孩纸&amp;#39;, 0.783301055431366),
 (&amp;#39;处女座&amp;#39;, 0.7807960510253906),
 (&amp;#39;腐女&amp;#39;, 0.779699444770813),
 (&amp;#39;宅女&amp;#39;, 0.7794589400291443),
 (&amp;#39;软妹&amp;#39;, 0.7725212574005127),
 (&amp;#39;小萝莉&amp;#39;, 0.7603519558906555),
 (&amp;#39;摩羯座&amp;#39;, 0.7602179646492004),
 (&amp;#39;呆萌&amp;#39;, 0.7555979490280151)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;流氓&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;风骚&amp;#39;, 0.7411526441574097),
 (&amp;#39;气质&amp;#39;, 0.7314842343330383),
 (&amp;#39;霸道&amp;#39;, 0.7147162556648254),
 (&amp;#39;伪装成&amp;#39;, 0.7128302454948425),
 (&amp;#39;天生&amp;#39;, 0.7044478058815002),
 (&amp;#39;十足&amp;#39;, 0.6987764835357666),
 (&amp;#39;斯文&amp;#39;, 0.6978859901428223),
 (&amp;#39;禽兽&amp;#39;, 0.6960264444351196),
 (&amp;#39;病态&amp;#39;, 0.6890178322792053),
 (&amp;#39;才华&amp;#39;, 0.6817525029182434),
 (&amp;#39;正派&amp;#39;, 0.6785053610801697),
 (&amp;#39;文静&amp;#39;, 0.6763062477111816),
 (&amp;#39;聪慧&amp;#39;, 0.6758238077163696),
 (&amp;#39;自恋&amp;#39;, 0.6680983901023865),
 (&amp;#39;凡夫俗子&amp;#39;, 0.6680223345756531),
 (&amp;#39;冷血&amp;#39;, 0.6673165559768677),
 (&amp;#39;白痴&amp;#39;, 0.666796863079071),
 (&amp;#39;清纯&amp;#39;, 0.6666175127029419),
 (&amp;#39;愤青&amp;#39;, 0.6663431525230408),
 (&amp;#39;颇具&amp;#39;, 0.6648291945457458)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;内向&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;外向&amp;#39;, 0.8674373030662537),
 (&amp;#39;腼腆&amp;#39;, 0.8610992431640625),
 (&amp;#39;开朗&amp;#39;, 0.8451307415962219),
 (&amp;#39;神经质&amp;#39;, 0.8253246545791626),
 (&amp;#39;孤僻&amp;#39;, 0.8227512836456299),
 (&amp;#39;胆小&amp;#39;, 0.7949897050857544),
 (&amp;#39;慢热&amp;#39;, 0.7939849495887756),
 (&amp;#39;大大咧咧&amp;#39;, 0.7869692444801331),
 (&amp;#39;古怪&amp;#39;, 0.7838969230651855),
 (&amp;#39;情绪化&amp;#39;, 0.7805034518241882),
 (&amp;#39;幽默&amp;#39;, 0.7713088989257812),
 (&amp;#39;不爱说话&amp;#39;, 0.76982182264328),
 (&amp;#39;活泼&amp;#39;, 0.7689502239227295),
 (&amp;#39;闷骚&amp;#39;, 0.766051173210144),
 (&amp;#39;记仇&amp;#39;, 0.7653043270111084),
 (&amp;#39;极度&amp;#39;, 0.7642502784729004),
 (&amp;#39;敏感&amp;#39;, 0.7624457478523254),
 (&amp;#39;自卑&amp;#39;, 0.7609980702400208),
 (&amp;#39;很宅&amp;#39;, 0.7600659132003784),
 (&amp;#39;矫情&amp;#39;, 0.7573622465133667)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;牛&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;牛B&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;牛皮&amp;#39;, 0.7266886830329895),
 (&amp;#39;叼&amp;#39;, 0.7169520258903503),
 (&amp;#39;吊&amp;#39;, 0.7075901627540588),
 (&amp;#39;流弊&amp;#39;, 0.6949068307876587),
 (&amp;#39;张嘴&amp;#39;, 0.6911835074424744),
 (&amp;#39;逼人&amp;#39;, 0.6845391988754272),
 (&amp;#39;欠揍&amp;#39;, 0.6771396994590759),
 (&amp;#39;这块&amp;#39;, 0.6755802035331726),
 (&amp;#39;他妈&amp;#39;, 0.672274112701416),
 (&amp;#39;像不像&amp;#39;, 0.6720238924026489),
 (&amp;#39;长帅&amp;#39;, 0.669898509979248),
 (&amp;#39;跟个&amp;#39;, 0.6674190163612366),
 (&amp;#39;仁波切&amp;#39;, 0.6618945002555847),
 (&amp;#39;隔壁老王&amp;#39;, 0.6596662998199463),
 (&amp;#39;捞&amp;#39;, 0.6594889760017395),
 (&amp;#39;能装&amp;#39;, 0.658306896686554),
 (&amp;#39;盗号狗&amp;#39;, 0.6573488116264343),
 (&amp;#39;竟敢&amp;#39;, 0.654305636882782),
 (&amp;#39;牛掰&amp;#39;, 0.6534903049468994),
 (&amp;#39;老实&amp;#39;, 0.6533665657043457)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;色&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;不遇倾城&amp;#39;, 0.7234371304512024),
 (&amp;#39;柔&amp;#39;, 0.6971151232719421),
 (&amp;#39;温&amp;#39;, 0.696250855922699),
 (&amp;#39;浮&amp;#39;, 0.6961503028869629),
 (&amp;#39;牡丹&amp;#39;, 0.6916242241859436),
 (&amp;#39;薄&amp;#39;, 0.6892343759536743),
 (&amp;#39;飘逸&amp;#39;, 0.687305212020874),
 (&amp;#39;丝&amp;#39;, 0.6816737651824951),
 (&amp;#39;彩&amp;#39;, 0.6801170110702515),
 (&amp;#39;骨&amp;#39;, 0.6786245703697205),
 (&amp;#39;细&amp;#39;, 0.6709766387939453),
 (&amp;#39;春&amp;#39;, 0.6705066561698914),
 (&amp;#39;羽&amp;#39;, 0.6662278771400452),
 (&amp;#39;沁&amp;#39;, 0.6659229397773743),
 (&amp;#39;华&amp;#39;, 0.6649417281150818),
 (&amp;#39;唇&amp;#39;, 0.6640968322753906),
 (&amp;#39;露&amp;#39;, 0.6638047695159912),
 (&amp;#39;墨&amp;#39;, 0.663625180721283),
 (&amp;#39;阳&amp;#39;, 0.6616363525390625),
 (&amp;#39;碧&amp;#39;, 0.6599227786064148)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;五获取资源&#34;&gt;五、获取资源&lt;/h2&gt;
&lt;p&gt;内容整理不易， 本文内容分免费和付费部分。 免费部分可以直接下载数据、构建语料、使用word2vec模型。 付费部分主要是cntext，用于训练word2vec模型。 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;免费获取&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;1亿用户数据集 &lt;a href=&#34;https://www.kaggle.com/datasets/beats0/bilibili-user&#34;&gt;https://www.kaggle.com/datasets/beats0/bilibili-user&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;B站用户签名语料.100.6.bin&lt;/strong&gt;     链接: &lt;a href=&#34;https://pan.baidu.com/s/1SM6fWZ3Jt7VFaZ2dedt5CA&#34;&gt;https://pan.baidu.com/s/1SM6fWZ3Jt7VFaZ2dedt5CA&lt;/a&gt; 提取码: bzmp&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;50元   获得cntext-2.0.0-py3-none-any.whl&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一用户签名">一、用户签名</h2>
<p>1亿B站用户群体十分庞大，文本中蕴含着这个群体的认知信息(如兴趣、身份、座右铭等)，如果能用签名训练word2vec词向量模型，说不定就有利用这个模型，对每个用户签名进行量化,  对用户进行分类。 本文要解决</p>
<ul>
<li>构建语料训练出模型</li>
<li>简单看看模型训练效果</li>
</ul>
<p><br><br></p>
<h2 id="二准备语料">二、准备语料</h2>
<p>Kaggle网有1亿B站用户数据集，下载地址</p>
<blockquote>
<p><a href="https://www.kaggle.com/datasets/beats0/bilibili-user">https://www.kaggle.com/datasets/beats0/bilibili-user</a></p>
</blockquote>
<p>之前分享过 <a href="">数据集 | 哔哩哔哩 1 亿用户数据</a> ， 阅读此文可以熟悉pandas的一些基本操作，如数据读取、文本操作等。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#从kaggle下载B站1亿用户数据</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;User.csv&#39;</span><span class="p">)</span>

<span class="c1">#签名非空的记录</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sign&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;数据集用户数量: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">df2</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据集用户数量: 100000000
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<p>将9093092个非空签名汇总到 <em><strong>B站用户签名语料.txt</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;B站用户签名语料.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">sign</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>
</code></pre></div><p>代码运行后，得到 302M <em><strong>B站用户签名语料.txt</strong></em> 。</p>
<p><br><br></p>
<h2 id="三训练word2vec">三、训练Word2Vec</h2>
<p>我使用的自己 <strong>未公开</strong> 的cntext 2.0.0版本， Bug频出，等调整好了再公开。</p>
<h3 id="31-安装cntext">3.1 安装cntext</h3>
<p>将 cntext-2.0.0-py3-none-any.whl 放置于桌面，打开 <strong>cmd</strong>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <strong>cmd</strong>  (苹果电脑打开terminal) 中使用pip3 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext-2.0.0-py3-none-any.whl
</code></pre></div><p>文末有cntext-2.0.0-py3-none-any.whl获取方式</p>
<br>
<h3 id="32-训练word2vec">3.2 训练word2vec</h3>
<p>cntext训练时候Word2Vec模型参数</p>
<ul>
<li>window = 6</li>
<li>vector_size = 100</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cntext2.0.0未公开，获取2.0.0请阅读文末获取方式</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;B站用户签名语料.txt&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">window</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">vector_size</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Starting Processing Corpus ...
Start Training! This may take a while. Please be patient...
Traning word2vec model took 1329 seconds
Note: The Word2Vec model has been saved to output/Word2Vec
</code></pre></div><p>耗时1329s， 模型训练完成！得到的模型文件，如下截图</p>
<p><img loading="lazy" src="img/word2vec.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四使用word2vec">四、使用word2vec</h2>
<h3 id="41-读取模型">4.1 读取模型</h3>
<p>使用gensim录入模型 “<strong>B站用户签名语料</strong>.100.6.bin” ,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/Word2Vec/B站用户签名语料.100.6.bin&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;模型词汇量: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">模型词汇量:  343650
</code></pre></div><br>
<h3 id="42-查询某词的词向量">4.2 查询某词的词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">w2v.wv[&#39;高冷&#39;]
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-1.1872591e+00, -1.2438694e+00, -9.4200081e-01, -4.0355644e+00,
        3.3588424e-01, -4.2525402e-01, -4.1175735e-01,  1.8802526e+00,
       -3.5992053e-01,  2.8361969e+00, -1.1437206e+00,  4.4662678e-01,
        1.2098696e+00,  7.2761238e-02,  3.0633178e-01,  6.7496544e-01,
       -3.0175522e-01, -1.1573459e+00, -7.4999934e-01,  1.6484009e+00,
        1.3102854e+00,  1.0134944e+00,  1.0711597e+00, -1.2194906e+00,
        2.1523576e+00, -4.4606316e-01,  1.0664939e+00,  5.3640699e-01,
        1.1061722e+00, -2.7679250e-01,  8.6652651e-02,  1.6876321e-02,
        6.3120401e-01, -1.6914845e-02,  9.6446878e-01,  1.7046971e+00,
       -1.7395537e+00,  1.7914917e+00, -1.3069035e+00,  6.6770411e-01,
       -3.4902021e-01, -1.3345592e-02, -4.3595994e-01, -5.1443088e-01,
        1.0884547e+00, -3.3695351e-02, -5.7088321e-01,  1.4533000e+00,
        8.0498764e-03,  9.2341286e-01, -1.6219637e-01, -2.5400406e-01,
       -1.4215972e-01,  7.1577376e-01, -1.2812414e+00, -1.7630520e-01,
       -1.7600318e+00, -1.4759690e+00, -2.3313001e-01, -8.8995326e-01,
        4.5749718e-01,  2.1950055e-02,  2.5749078e-01, -7.6623499e-01,
        3.1834408e-03,  7.2732526e-01, -2.5459883e+00, -1.5240467e+00,
        2.4574394e+00,  7.3715413e-01,  2.2769032e+00,  1.7492032e+00,
       -4.1084039e-01,  6.4300962e-02,  6.2454116e-01, -4.0486854e-02,
        7.7100635e-01, -1.9729427e+00, -8.4130460e-01, -3.0110097e-01,
       -1.0786959e+00, -1.9136167e+00, -1.7037696e-01, -7.3208618e-03,
        4.8502678e-01, -1.0348318e+00,  3.1141058e-01,  2.9913974e-01,
       -2.1714316e-01, -3.1645024e+00,  7.0972210e-03,  7.8701675e-01,
       -2.2510442e-01, -9.8428482e-01,  1.0685140e+00,  2.1938827e+00,
       -9.1963351e-01,  6.3011467e-01, -1.1531134e+00, -9.2123538e-02],
      dtype=float32)
</code></pre></div><br>
<h3 id="43-查看近义词">4.3 查看近义词</h3>
<p>通过给定词语，查看其近义词，可以了解模型训练的好坏。语义捕捉的合理，说明语料合理，模型训练的好。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#列表中可以传入任意多个词，这里大邓偷懒，都只传入了一两个词</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;高冷&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;腹黑&#39;, 0.8282514810562134),
 (&#39;呆萌&#39;, 0.8148132562637329),
 (&#39;傲娇&#39;, 0.7791209816932678),
 (&#39;逗比&#39;, 0.7720615863800049),
 (&#39;闷骚&#39;, 0.7617782354354858),
 (&#39;精分&#39;, 0.7545589208602905),
 (&#39;文静&#39;, 0.7545390725135803),
 (&#39;慢热&#39;, 0.7387350797653198),
 (&#39;自恋&#39;, 0.7299264669418335),
 (&#39;淑女&#39;, 0.7261008620262146),
 (&#39;耿直&#39;, 0.7238353490829468),
 (&#39;帅气&#39;, 0.7233086824417114),
 (&#39;暖男&#39;, 0.720333456993103),
 (&#39;内向&#39;, 0.7159033417701721),
 (&#39;蠢&#39;, 0.7157402038574219),
 (&#39;逗逼&#39;, 0.7091616988182068),
 (&#39;神经质&#39;, 0.7085140347480774),
 (&#39;女汉子&#39;, 0.707956850528717),
 (&#39;毒舌&#39;, 0.7058071494102478),
 (&#39;逗&#39;, 0.7048983573913574)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;女汉子&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;女汉纸&#39;, 0.8832258582115173),
 (&#39;汉子&#39;, 0.8506060838699341),
 (&#39;萌妹子&#39;, 0.8475067615509033),
 (&#39;暖男&#39;, 0.8445340394973755),
 (&#39;女神经&#39;, 0.838117241859436),
 (&#39;萌妹纸&#39;, 0.8303463459014893),
 (&#39;闷骚&#39;, 0.8296418786048889),
 (&#39;妹纸&#39;, 0.8289912343025208),
 (&#39;软妹子&#39;, 0.8211091756820679),
 (&#39;腹黑&#39;, 0.8019399046897888),
 (&#39;汉纸&#39;, 0.7941007614135742),
 (&#39;糙汉子&#39;, 0.7915611267089844),
 (&#39;孩纸&#39;, 0.783301055431366),
 (&#39;处女座&#39;, 0.7807960510253906),
 (&#39;腐女&#39;, 0.779699444770813),
 (&#39;宅女&#39;, 0.7794589400291443),
 (&#39;软妹&#39;, 0.7725212574005127),
 (&#39;小萝莉&#39;, 0.7603519558906555),
 (&#39;摩羯座&#39;, 0.7602179646492004),
 (&#39;呆萌&#39;, 0.7555979490280151)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;流氓&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;风骚&#39;, 0.7411526441574097),
 (&#39;气质&#39;, 0.7314842343330383),
 (&#39;霸道&#39;, 0.7147162556648254),
 (&#39;伪装成&#39;, 0.7128302454948425),
 (&#39;天生&#39;, 0.7044478058815002),
 (&#39;十足&#39;, 0.6987764835357666),
 (&#39;斯文&#39;, 0.6978859901428223),
 (&#39;禽兽&#39;, 0.6960264444351196),
 (&#39;病态&#39;, 0.6890178322792053),
 (&#39;才华&#39;, 0.6817525029182434),
 (&#39;正派&#39;, 0.6785053610801697),
 (&#39;文静&#39;, 0.6763062477111816),
 (&#39;聪慧&#39;, 0.6758238077163696),
 (&#39;自恋&#39;, 0.6680983901023865),
 (&#39;凡夫俗子&#39;, 0.6680223345756531),
 (&#39;冷血&#39;, 0.6673165559768677),
 (&#39;白痴&#39;, 0.666796863079071),
 (&#39;清纯&#39;, 0.6666175127029419),
 (&#39;愤青&#39;, 0.6663431525230408),
 (&#39;颇具&#39;, 0.6648291945457458)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;内向&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;外向&#39;, 0.8674373030662537),
 (&#39;腼腆&#39;, 0.8610992431640625),
 (&#39;开朗&#39;, 0.8451307415962219),
 (&#39;神经质&#39;, 0.8253246545791626),
 (&#39;孤僻&#39;, 0.8227512836456299),
 (&#39;胆小&#39;, 0.7949897050857544),
 (&#39;慢热&#39;, 0.7939849495887756),
 (&#39;大大咧咧&#39;, 0.7869692444801331),
 (&#39;古怪&#39;, 0.7838969230651855),
 (&#39;情绪化&#39;, 0.7805034518241882),
 (&#39;幽默&#39;, 0.7713088989257812),
 (&#39;不爱说话&#39;, 0.76982182264328),
 (&#39;活泼&#39;, 0.7689502239227295),
 (&#39;闷骚&#39;, 0.766051173210144),
 (&#39;记仇&#39;, 0.7653043270111084),
 (&#39;极度&#39;, 0.7642502784729004),
 (&#39;敏感&#39;, 0.7624457478523254),
 (&#39;自卑&#39;, 0.7609980702400208),
 (&#39;很宅&#39;, 0.7600659132003784),
 (&#39;矫情&#39;, 0.7573622465133667)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;牛&#39;</span><span class="p">,</span> <span class="s1">&#39;牛B&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;牛皮&#39;, 0.7266886830329895),
 (&#39;叼&#39;, 0.7169520258903503),
 (&#39;吊&#39;, 0.7075901627540588),
 (&#39;流弊&#39;, 0.6949068307876587),
 (&#39;张嘴&#39;, 0.6911835074424744),
 (&#39;逼人&#39;, 0.6845391988754272),
 (&#39;欠揍&#39;, 0.6771396994590759),
 (&#39;这块&#39;, 0.6755802035331726),
 (&#39;他妈&#39;, 0.672274112701416),
 (&#39;像不像&#39;, 0.6720238924026489),
 (&#39;长帅&#39;, 0.669898509979248),
 (&#39;跟个&#39;, 0.6674190163612366),
 (&#39;仁波切&#39;, 0.6618945002555847),
 (&#39;隔壁老王&#39;, 0.6596662998199463),
 (&#39;捞&#39;, 0.6594889760017395),
 (&#39;能装&#39;, 0.658306896686554),
 (&#39;盗号狗&#39;, 0.6573488116264343),
 (&#39;竟敢&#39;, 0.654305636882782),
 (&#39;牛掰&#39;, 0.6534903049468994),
 (&#39;老实&#39;, 0.6533665657043457)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;色&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;不遇倾城&#39;, 0.7234371304512024),
 (&#39;柔&#39;, 0.6971151232719421),
 (&#39;温&#39;, 0.696250855922699),
 (&#39;浮&#39;, 0.6961503028869629),
 (&#39;牡丹&#39;, 0.6916242241859436),
 (&#39;薄&#39;, 0.6892343759536743),
 (&#39;飘逸&#39;, 0.687305212020874),
 (&#39;丝&#39;, 0.6816737651824951),
 (&#39;彩&#39;, 0.6801170110702515),
 (&#39;骨&#39;, 0.6786245703697205),
 (&#39;细&#39;, 0.6709766387939453),
 (&#39;春&#39;, 0.6705066561698914),
 (&#39;羽&#39;, 0.6662278771400452),
 (&#39;沁&#39;, 0.6659229397773743),
 (&#39;华&#39;, 0.6649417281150818),
 (&#39;唇&#39;, 0.6640968322753906),
 (&#39;露&#39;, 0.6638047695159912),
 (&#39;墨&#39;, 0.663625180721283),
 (&#39;阳&#39;, 0.6616363525390625),
 (&#39;碧&#39;, 0.6599227786064148)]
</code></pre></div><br>
<br>
<h2 id="五获取资源">五、获取资源</h2>
<p>内容整理不易， 本文内容分免费和付费部分。 免费部分可以直接下载数据、构建语料、使用word2vec模型。 付费部分主要是cntext，用于训练word2vec模型。 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」</p>
<ul>
<li>
<p>免费获取</p>
<ul>
<li>
<p>1亿用户数据集 <a href="https://www.kaggle.com/datasets/beats0/bilibili-user">https://www.kaggle.com/datasets/beats0/bilibili-user</a></p>
</li>
<li>
<p><strong>B站用户签名语料.100.6.bin</strong>     链接: <a href="https://pan.baidu.com/s/1SM6fWZ3Jt7VFaZ2dedt5CA">https://pan.baidu.com/s/1SM6fWZ3Jt7VFaZ2dedt5CA</a> 提取码: bzmp</p>
</li>
</ul>
</li>
<li>
<p>50元   获得cntext-2.0.0-py3-none-any.whl</p>
</li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>词向量(付费) | 使用1985年-2022年专利申请摘要训练word2vec模型</title>
      <link>https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/</link>
      <pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/</guid>
      <description>&lt;h2 id=&#34;一说明&#34;&gt;一、说明&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/&#34;&gt;&lt;strong&gt;3571万条专利申请数据集(1985-2022年)&lt;/strong&gt;&lt;/a&gt; 中随机抽取了28%的 「&lt;strong&gt;专利摘要&lt;/strong&gt;」，构成6.14G的训练语料(千万级别)， 耗时6小时，训练得到word2vec模型。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;需要注意， 100%全部语料有24G， 使用服务区内存128G，跑了20小时预处理还没完成，内存就炸了。&lt;/p&gt;
&lt;p&gt;没办法，我不会优化代码性能，所以只能抽取28%的文本数据来训练word2vec&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;p&gt;本文需要用到新cntext，因为bug较多， 直接上传到PyPi，将导致之前制作的课程和公众号推文相关内容全部重新一遍。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一语料构建&#34;&gt;一、语料构建&lt;/h2&gt;
&lt;p&gt;随机抽取28%的记录，构成千万专利文本摘要训练语料。&lt;/p&gt;
&lt;p&gt;为了防止电脑内存爆炸， 对任意单个大csv文件，分批次读取，每次读10w行。最终将专利摘要文本保存到txt文件中，编码方式为utf-8。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果想开发一些词典，可以跳过此部分内容，并不影响代码运行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/screen-datasets.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#将代码放在csv数据文件夹内&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;1000w专利摘要.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;txtf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;#获得当前文件夹内所有的csv文件路径&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.csv&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;#分批次读取csv，每次读10w行&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunksize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;#剔除专利摘要为空的记录&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;#随机抽取28%的记录&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;sample_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;frac&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.28&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;txtf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tolist&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;最终得到的 &lt;strong&gt;1000w专利摘要.txt&lt;/strong&gt;  文件有 6.14G&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二训练word2vec&#34;&gt;二、训练word2vec&lt;/h2&gt;
&lt;p&gt;我使用的自己 &lt;strong&gt;未公开&lt;/strong&gt; 的cntext 2.1.1 版本， Bug频出，等调整好了再公开。&lt;/p&gt;
&lt;h3 id=&#34;21-安装&#34;&gt;2.1 安装&lt;/h3&gt;
&lt;p&gt;将 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 放置于桌面，打开 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal) 中使用 &lt;em&gt;&lt;strong&gt;pip3&lt;/strong&gt;&lt;/em&gt; 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文末有 &lt;em&gt;&lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 获取方式&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;Word2Vec模型参数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;window = 6&lt;/li&gt;
&lt;li&gt;vector_size = 100&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#cntext为2.0.0&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2VModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;1000w专利摘要.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Starting Preprocessing Corpus ...
Starting Training! This may take a while.Please be patient...
Traning word2vec model took 22806 seconds
Note: The Word2Vec model hase saved to output/Word2Vec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/model-dir.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;cntext.W2VModel训练中考虑到了词组情况，为了将&amp;quot;科学技术&amp;quot;这类短语词(词组)也纳入到word2vec训练中， 使用到gensim.models.phrases.Phrase。&lt;/p&gt;
&lt;p&gt;大邓不会优化性能，训练word2vec时，预处理部分占用内存很大，  我用的服务器内存128G， 训练时间6.335小时。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三使用词向量&#34;&gt;三、使用词向量&lt;/h2&gt;
&lt;h3 id=&#34;31-录入模型&#34;&gt;3.1 录入模型&lt;/h3&gt;
&lt;p&gt;需要注意， 专利模型文件是三个哦， 三个是一个整体，不要随意删除&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/model-dir.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#2.0.0版本cntext，未公开&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;gensim.models&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Word2Vec/1000w专利摘要文本.100.6.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#如果没有cntext就用注释掉的代码，使用gensim导入&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#w2v = KeyedVectors.load(&amp;#39;Word2Vec/1000w专利摘要文本.100.6.bin&amp;#39;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Loading word2vec model...
&amp;lt;gensim.models.word2vec.Word2Vec at 0x2afb3f650&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-词汇量&#34;&gt;3.2 词汇量&lt;/h3&gt;
&lt;p&gt;查看模型中的词汇量&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;#模型中词汇量
len(w2v.wv)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;1120752
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;33-查看词向量&#34;&gt;3.3 查看词向量&lt;/h3&gt;
&lt;p&gt;查看任意词的词向量，例如“&lt;strong&gt;创新&lt;/strong&gt;”&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#查看 ”创新” 的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([-2.3267136 ,  2.3038454 ,  2.8232517 , -3.23959   , -2.9036384 ,
       -2.0450666 , -1.5516403 ,  0.00575857, -0.64638597,  1.3585284 ,
       -1.7491045 , -1.3659543 ,  1.9901325 , -1.5066692 ,  0.5094756 ,
       -1.7032526 , -0.35252815, -4.00833   ,  3.5424068 , -0.0426405 ,
       -0.24548595, -0.7675196 ,  2.366155  , -0.18583044,  0.83989865,
        1.5965563 ,  0.30173486, -0.80054444,  2.0068777 ,  1.770656  ,
        0.06608703, -2.5833828 ,  1.7995895 , -0.281671  ,  0.06354411,
        1.2502885 ,  1.1960976 , -0.19735877, -2.3988242 ,  1.0004953 ,
        0.950612  , -2.9186552 ,  2.5141885 ,  0.5993077 ,  1.2969743 ,
       -3.7506597 ,  2.6031113 , -0.30022916, -1.0615158 , -0.2402753 ,
       -0.29447314, -1.7101966 , -2.6812305 ,  1.1898949 , -0.34348696,
       -1.7013234 ,  0.27328706, -0.67401695, -2.8010712 , -1.5993378 ,
        0.55218667, -0.15136468,  0.67049694,  0.6745255 , -0.80350083,
        2.254024  , -0.8005472 , -2.0170422 ,  2.882873  , -0.46188217,
        0.8481421 , -1.3741239 ,  0.7432127 ,  1.1100464 , -0.64173746,
       -1.3264686 , -1.991515  , -0.27887765, -0.62801987, -3.0960062 ,
       -3.2658167 , -0.065689  ,  2.5853407 , -1.6554247 , -0.49887556,
       -2.146973  , -0.45912525,  0.28037554,  1.0885888 ,  1.6503012 ,
        1.0013059 ,  0.3194557 ,  3.0309706 , -4.5257196 ,  0.4644844 ,
        3.0723457 ,  0.49002075,  2.4370434 , -0.7763012 ,  3.2541463 ],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;&lt;strong&gt;注意: 如果查询的词未在模型中，会出现KeyError报错&lt;/strong&gt;。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;牛逼&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/error.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;34-最相似词&#34;&gt;3.4 最相似词&lt;/h3&gt;
&lt;p&gt;与&amp;rsquo;创新&#39;, &amp;lsquo;颠覆&amp;rsquo;最相似的20个词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;#词语列表中可传入任意多个词，
#大邓词穷，只想到这两个相似的种子词
w2v.wv.most_similar([&amp;#39;创新&amp;#39;, &amp;#39;颠覆&amp;#39;], topn=20)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;革新&amp;#39;, 0.8313461542129517),
 (&amp;#39;变革&amp;#39;, 0.8260877728462219),
 (&amp;#39;革命性&amp;#39;, 0.79015052318573),
 (&amp;#39;从根本上改变&amp;#39;, 0.7867545485496521),
 (&amp;#39;改革&amp;#39;, 0.7788680791854858),
 (&amp;#39;技术创新&amp;#39;, 0.7715167999267578),
 (&amp;#39;核心技术&amp;#39;, 0.7679213881492615),
 (&amp;#39;独创&amp;#39;, 0.7668667435646057),
 (&amp;#39;创新型&amp;#39;, 0.7655373811721802),
 (&amp;#39;颠覆性&amp;#39;, 0.7575560212135315),
 (&amp;#39;借鉴&amp;#39;, 0.7570509910583496),
 (&amp;#39;全新&amp;#39;, 0.7496902942657471),
 (&amp;#39;有别于&amp;#39;, 0.7489079236984253),
 (&amp;#39;打破常规&amp;#39;, 0.7397119402885437),
 (&amp;#39;改变目前&amp;#39;, 0.735921323299408),
 (&amp;#39;打破传统&amp;#39;, 0.7265862226486206),
 (&amp;#39;大胆&amp;#39;, 0.7247217893600464),
 (&amp;#39;加以改进&amp;#39;, 0.7223487496376038),
 (&amp;#39;划时代&amp;#39;, 0.7221404910087585),
 (&amp;#39;改变过去&amp;#39;, 0.7220492959022522)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;刚刚的运行，体现模型很好的学习到了专利摘要中的语义关系。&lt;/p&gt;
&lt;p&gt;如果我想开发三个词典，分别是 &lt;strong&gt;创新&lt;/strong&gt;、&lt;strong&gt;成本&lt;/strong&gt;、&lt;strong&gt;质量&lt;/strong&gt; ，想直接将结果保存到txt中，可以运行如下代码&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;seeds&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新概念&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;颠覆&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
         &lt;span class=&#34;s1&#34;&gt;&amp;#39;成本概念&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;成本&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
         &lt;span class=&#34;s1&#34;&gt;&amp;#39;质量概念&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;质量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;       &lt;span class=&#34;c1&#34;&gt;#word2vec词向量&lt;/span&gt;
                     &lt;span class=&#34;n&#34;&gt;seeddict&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seeds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#种子词字典&lt;/span&gt;
                     &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;         &lt;span class=&#34;c1&#34;&gt;#保留20个最相似的词&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Finish! 创新概念 candidates saved to output/Word2Vec
Finish! 成本概念 candidates saved to output/Word2Vec
Finish! 质量概念 candidates saved to output/Word2Vec
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/similar-words.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四获取资源&#34;&gt;四、获取资源&lt;/h2&gt;
&lt;p&gt;内容整理不易， 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 100元   cntext-2.1.1-py3-none-any.whl
- 100元   word2vec模型文件
- 200元 获得
  - cntext-2.1.1-py3-none-any.whl
  - word2vec模型文件
  
  
声明: 仅用于科研用途
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一说明">一、说明</h2>
<p><a href="https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/"><strong>3571万条专利申请数据集(1985-2022年)</strong></a> 中随机抽取了28%的 「<strong>专利摘要</strong>」，构成6.14G的训练语料(千万级别)， 耗时6小时，训练得到word2vec模型。</p>
<blockquote>
<p>需要注意， 100%全部语料有24G， 使用服务区内存128G，跑了20小时预处理还没完成，内存就炸了。</p>
<p>没办法，我不会优化代码性能，所以只能抽取28%的文本数据来训练word2vec</p>
</blockquote>
<br>
<p>本文需要用到新cntext，因为bug较多， 直接上传到PyPi，将导致之前制作的课程和公众号推文相关内容全部重新一遍。</p>
<p><br><br></p>
<h2 id="一语料构建">一、语料构建</h2>
<p>随机抽取28%的记录，构成千万专利文本摘要训练语料。</p>
<p>为了防止电脑内存爆炸， 对任意单个大csv文件，分批次读取，每次读10w行。最终将专利摘要文本保存到txt文件中，编码方式为utf-8。</p>
<blockquote>
<p>如果想开发一些词典，可以跳过此部分内容，并不影响代码运行。</p>
</blockquote>
<p><img loading="lazy" src="img/screen-datasets.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#将代码放在csv数据文件夹内</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;1000w专利摘要.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">txtf</span><span class="p">:</span>
    <span class="c1">#获得当前文件夹内所有的csv文件路径</span>
    <span class="n">csvfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.csv&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">csvf</span> <span class="ow">in</span> <span class="n">csvfs</span><span class="p">:</span>
        <span class="c1">#分批次读取csv，每次读10w行</span>
        <span class="n">chunk_dfs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">chunk_df</span> <span class="ow">in</span> <span class="n">chunk_dfs</span><span class="p">:</span>
            <span class="c1">#剔除专利摘要为空的记录</span>
            <span class="n">chunk_df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;专利摘要&#39;</span><span class="p">])</span>
            <span class="c1">#随机抽取28%的记录</span>
            <span class="n">sample_df</span> <span class="o">=</span> <span class="n">chunk_df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">0.28</span><span class="p">)</span>
            <span class="n">txtf</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;专利摘要&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()))</span>
</code></pre></div><p>最终得到的 <strong>1000w专利摘要.txt</strong>  文件有 6.14G<br><br><br></p>
<h2 id="二训练word2vec">二、训练word2vec</h2>
<p>我使用的自己 <strong>未公开</strong> 的cntext 2.1.1 版本， Bug频出，等调整好了再公开。</p>
<h3 id="21-安装">2.1 安装</h3>
<p>将 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl
</code></pre></div><p>文末有 <em><strong>cntext-2.1.1-py3-none-any.whl</strong></em> 获取方式</p>
<br>
<p>Word2Vec模型参数</p>
<ul>
<li>window = 6</li>
<li>vector_size = 100</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cntext为2.0.0</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;1000w专利摘要.txt&#39;</span><span class="p">,</span>
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>

<span class="n">w2v_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Starting Preprocessing Corpus ...
Starting Training! This may take a while.Please be patient...
Traning word2vec model took 22806 seconds
Note: The Word2Vec model hase saved to output/Word2Vec
</code></pre></div><p><img loading="lazy" src="img/model-dir.png" alt=""  />
</p>
<p>cntext.W2VModel训练中考虑到了词组情况，为了将&quot;科学技术&quot;这类短语词(词组)也纳入到word2vec训练中， 使用到gensim.models.phrases.Phrase。</p>
<p>大邓不会优化性能，训练word2vec时，预处理部分占用内存很大，  我用的服务器内存128G， 训练时间6.335小时。</p>
<p><br><br></p>
<h2 id="三使用词向量">三、使用词向量</h2>
<h3 id="31-录入模型">3.1 录入模型</h3>
<p>需要注意， 专利模型文件是三个哦， 三个是一个整体，不要随意删除</p>
<p><img loading="lazy" src="img/model-dir.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#2.0.0版本cntext，未公开</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;Word2Vec/1000w专利摘要文本.100.6.bin&#39;</span><span class="p">)</span>
<span class="c1">#如果没有cntext就用注释掉的代码，使用gensim导入</span>
<span class="c1">#w2v = KeyedVectors.load(&#39;Word2Vec/1000w专利摘要文本.100.6.bin&#39;)</span>
<span class="n">w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading word2vec model...
&lt;gensim.models.word2vec.Word2Vec at 0x2afb3f650&gt;
</code></pre></div><br>
<h3 id="32-词汇量">3.2 词汇量</h3>
<p>查看模型中的词汇量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">#模型中词汇量
len(w2v.wv)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1120752
</code></pre></div><br>
<h3 id="33-查看词向量">3.3 查看词向量</h3>
<p>查看任意词的词向量，例如“<strong>创新</strong>”</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看 ”创新” 的词向量</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-2.3267136 ,  2.3038454 ,  2.8232517 , -3.23959   , -2.9036384 ,
       -2.0450666 , -1.5516403 ,  0.00575857, -0.64638597,  1.3585284 ,
       -1.7491045 , -1.3659543 ,  1.9901325 , -1.5066692 ,  0.5094756 ,
       -1.7032526 , -0.35252815, -4.00833   ,  3.5424068 , -0.0426405 ,
       -0.24548595, -0.7675196 ,  2.366155  , -0.18583044,  0.83989865,
        1.5965563 ,  0.30173486, -0.80054444,  2.0068777 ,  1.770656  ,
        0.06608703, -2.5833828 ,  1.7995895 , -0.281671  ,  0.06354411,
        1.2502885 ,  1.1960976 , -0.19735877, -2.3988242 ,  1.0004953 ,
        0.950612  , -2.9186552 ,  2.5141885 ,  0.5993077 ,  1.2969743 ,
       -3.7506597 ,  2.6031113 , -0.30022916, -1.0615158 , -0.2402753 ,
       -0.29447314, -1.7101966 , -2.6812305 ,  1.1898949 , -0.34348696,
       -1.7013234 ,  0.27328706, -0.67401695, -2.8010712 , -1.5993378 ,
        0.55218667, -0.15136468,  0.67049694,  0.6745255 , -0.80350083,
        2.254024  , -0.8005472 , -2.0170422 ,  2.882873  , -0.46188217,
        0.8481421 , -1.3741239 ,  0.7432127 ,  1.1100464 , -0.64173746,
       -1.3264686 , -1.991515  , -0.27887765, -0.62801987, -3.0960062 ,
       -3.2658167 , -0.065689  ,  2.5853407 , -1.6554247 , -0.49887556,
       -2.146973  , -0.45912525,  0.28037554,  1.0885888 ,  1.6503012 ,
        1.0013059 ,  0.3194557 ,  3.0309706 , -4.5257196 ,  0.4644844 ,
        3.0723457 ,  0.49002075,  2.4370434 , -0.7763012 ,  3.2541463 ],
      dtype=float32)
</code></pre></div><br>
<p><strong>注意: 如果查询的词未在模型中，会出现KeyError报错</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;牛逼&#39;</span><span class="p">]</span>
</code></pre></div><p><img loading="lazy" src="img/error.png" alt=""  />
</p>
<br>
<h3 id="34-最相似词">3.4 最相似词</h3>
<p>与&rsquo;创新', &lsquo;颠覆&rsquo;最相似的20个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">#词语列表中可传入任意多个词，
#大邓词穷，只想到这两个相似的种子词
w2v.wv.most_similar([&#39;创新&#39;, &#39;颠覆&#39;], topn=20)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;革新&#39;, 0.8313461542129517),
 (&#39;变革&#39;, 0.8260877728462219),
 (&#39;革命性&#39;, 0.79015052318573),
 (&#39;从根本上改变&#39;, 0.7867545485496521),
 (&#39;改革&#39;, 0.7788680791854858),
 (&#39;技术创新&#39;, 0.7715167999267578),
 (&#39;核心技术&#39;, 0.7679213881492615),
 (&#39;独创&#39;, 0.7668667435646057),
 (&#39;创新型&#39;, 0.7655373811721802),
 (&#39;颠覆性&#39;, 0.7575560212135315),
 (&#39;借鉴&#39;, 0.7570509910583496),
 (&#39;全新&#39;, 0.7496902942657471),
 (&#39;有别于&#39;, 0.7489079236984253),
 (&#39;打破常规&#39;, 0.7397119402885437),
 (&#39;改变目前&#39;, 0.735921323299408),
 (&#39;打破传统&#39;, 0.7265862226486206),
 (&#39;大胆&#39;, 0.7247217893600464),
 (&#39;加以改进&#39;, 0.7223487496376038),
 (&#39;划时代&#39;, 0.7221404910087585),
 (&#39;改变过去&#39;, 0.7220492959022522)]
</code></pre></div><br>
<p>刚刚的运行，体现模型很好的学习到了专利摘要中的语义关系。</p>
<p>如果我想开发三个词典，分别是 <strong>创新</strong>、<strong>成本</strong>、<strong>质量</strong> ，想直接将结果保存到txt中，可以运行如下代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">seeds</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;创新概念&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;颠覆&#39;</span><span class="p">],</span>
         <span class="s1">&#39;成本概念&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;成本&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">],</span>
         <span class="s1">&#39;质量概念&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;质量&#39;</span><span class="p">]}</span>

<span class="n">ct</span><span class="o">.</span><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span>       <span class="c1">#word2vec词向量</span>
                     <span class="n">seeddict</span><span class="o">=</span><span class="n">seeds</span><span class="p">,</span>  <span class="c1">#种子词字典</span>
                     <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>         <span class="c1">#保留20个最相似的词</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Finish! 创新概念 candidates saved to output/Word2Vec
Finish! 成本概念 candidates saved to output/Word2Vec
Finish! 质量概念 candidates saved to output/Word2Vec
</code></pre></div><p><img loading="lazy" src="img/similar-words.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四获取资源">四、获取资源</h2>
<p>内容整理不易， 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 100元   cntext-2.1.1-py3-none-any.whl
- 100元   word2vec模型文件
- 200元 获得
  - cntext-2.1.1-py3-none-any.whl
  - word2vec模型文件
  
  
声明: 仅用于科研用途
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</title>
      <link>https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/</link>
      <pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-03-organization-science-with-word-embeddings/</guid>
      <description>&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-09-literature-about-embeddings/&#34;&gt;文献汇总 | 词嵌入 与 社会科学中的偏见(态度)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/&#34;&gt;转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/&#34;&gt;可视化 | 人民日报语料反映七十年文化演变&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/&#34;&gt;词向量  | 使用&lt;strong&gt;人民网领导留言板&lt;/strong&gt;语料训练Word2Vec模型&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Aceves, Pedro, and James A. Evans. &amp;ldquo;&lt;strong&gt;Mobilizing conceptual spaces: How word embedding models can inform measurement and theory within organization science.&lt;/strong&gt;&amp;rdquo; &lt;em&gt;Organization Science&lt;/em&gt; (2023).&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;
&lt;p&gt;词嵌入模型是一种表示多维概念空间的强大方法，在多维概念空间中，所传达的概念可以相互关联、组合和竞争。此类模型代表了机器学习的最新进展，使学者能够用大规模文本数据局部和全局的单词共现，以最小的语义失真程度， 有效地编码复杂的意义系统。尽管词嵌入的使用有可能扩大组织科学中的理论可能性，但嵌入对于组织学者来说很大程度上是未知的，未发挥出词嵌入应有的潜力。我们的目标是通过为用户提供实用的路线图来展示嵌入模型在组织科学中的前景，以在他们的研究中调动该方法，并为开展该类研究的学者提供理论指导。 我们首先明确定义 &lt;strong&gt;概念&lt;/strong&gt; 和 &lt;strong&gt;概念空间&lt;/strong&gt; 的概念，然后继续展示如何使用词嵌入模型来表示和测量这些概念，并指出该方法的优点和缺点。然后，我们提供一组嵌入测量及其理论解释和灵活的扩展。我们的目标是从词嵌入的技术处理中提取概念，并将其置于实践的理论框架中，以加速此类研究。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一介绍&#34;&gt;一、介绍&lt;/h2&gt;
&lt;p&gt;过去十年，文本作为数据的计算使用在组织科学中显着增长（Hasan 等人，2015 年；Goldberg 等人，2016 年；Srivastava 等人，2018 年；Hannigan 等人，2019 年）。这种增长的主要原因是文本编码的概念信息赋予个人、组织、经济和社会行为以意义（Evans 和 Aceves 2016，Gentzkow 等人 2019），并且在过去十年中，来自组织环境的文本数据急剧增长，大大提高了文本的可用性。然而，文本中编码的 &lt;strong&gt;概念意义&lt;/strong&gt; 本质上是高维的，这使得降低概念复杂性成为研究文本的学者的中心任务。&lt;strong&gt;词嵌入模型是由计算机科学家和语言学家开发的一个新兴工具系列，用于文本信息降维，以此提取概念及其数字表示&lt;/strong&gt;。词嵌入技术的发展使组织科学家依赖于文本数据进行理论构造， 相比之前，数据中信息的保真度更高，由此文本数据与组织研究交叉场景形成了新的理论研究路线。尽管词嵌入模型在组织科学之外得到广泛使用，但由于组织科学领域的学者缺乏对词嵌入技术的理解， 不知如何将它们纳入理论发展过程的原则框架，词嵌入模型对于理论发展的价值仍然被掩盖。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;词嵌入模型建立在高效的神经网络架构之上，并通过将复杂的语义系统有效编码到具有最小失真的稠密几何空间中，彻底改变了语义分析&lt;/strong&gt;。这些模型代表了数十到数百个维度的空间中的语义，相对于语言中的单词和概念的数量来说，这个维度较低； 但相对于正式社会和文化理论家之前试图呈现概念信息的两到三个维度来说，这个维度却很高（奥斯古德 1964 年，史密斯-洛文和海斯 1988 年）。出于组织科学的目的，这些嵌入模型创建了社会系统中个体所持有的集体知识的 &lt;strong&gt;数字替身&lt;/strong&gt; ， 嵌入可以解决文化上隐含类比（Mikolov et al. 2013b），回答文化偶然问题（Devlin et al. 2019，Radford et al. 2022），并预测未来的知识发现（Tshitoyan等人 2019；Sourati 和 Evans 2021）。组织科学长期以来一直借鉴人工智能（AI）的表征概念， 在这里，我们使用人工智能的表示机制来增强组织理论研究（Csaszar 和 Steinberger 2022）。&lt;/p&gt;
&lt;p&gt;然而，由于神经网络复杂，且难以理解的黑盒性质特性，围绕神经嵌入和人工智能方法对理论发展的价值存在争议。尽管预测能力很强，但此类方法往往缺乏可解释性（Knight 2017，Leavitt et al. 2021）。&lt;strong&gt;在组织科学领域中，学者缺乏此技术的理解，即&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;对于嵌入何时成为组织科学有用的方法论选择&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;如何在既定认识论标准内证明使用“复杂”神经嵌入方法的合理性&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;如何在各种嵌入中进行选择 等方法&lt;/strong&gt;（例如，静态词嵌入与上下文嵌入、预训练嵌入与自定义嵌入）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;使用嵌入进行研究的适当步骤以及评估嵌入研究的相关标准&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;最值得注意的是，研究界，特别是那些研究组织认知、文化、知识和意义的人，似乎对嵌入方法 &lt;strong&gt;如何适应将方法论选择与理论发展联系起来&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;我们的目的是通过两项贡献来解决这些问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;首先，我们的目标是提供一个理论指南，为嵌入模型提供一个原则性的概念框架，学者可以使用该框架为他们的模型注入意义，并使学者们能够在理论发展过程中运用这些模型。我们这里的主要论点是，词嵌入模型中的每个向量代表一个概念，整个嵌入模型代表生成文本数据的社会系统的概念空间&lt;/strong&gt;。嵌入模型所代表的概念空间是多维空间，其中从规范和知识到想法和发明的概念相互关联。这个框架使组织学者能够利用嵌入模型的概念空间，与组织科学的许多领域之间建立联系。例如，不同公司基于知识视角对该空间的差异化覆盖（Grant 1996），组织理论家在描述规范和制度（Scott 2003），类别学者援引在决定将一个物体归类到哪个概念时（Pontikes 和 Barnett 2015 ），创新学者直接理论化寻求测量发现和发明的新颖性（Fleming 和 Sorenson 2001，2004），并且团队研究人员寻求了解成员在空间中的不同立场如何影响创造力、协调性和绩效（Srikanth 等人，2016）。因为我们以 &lt;strong&gt;概念&lt;/strong&gt; 和 &lt;strong&gt;概念空间&lt;/strong&gt; 为中心的理论框架可以推广到组织理论的许多背景，所以我们希望嵌入模型所支持的研究将促进这些子领域之间更深入、更持久的对话。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其次，我们的目标是为利用嵌入模型进行理论发展提供实用的路线图&lt;/strong&gt;。在此过程中，我们引导读者完成使用专利摘要语料库来实现词嵌入模型的过程，以表示现代技术创新的概念空间。我们解释了研究人员需要设置的模型参数，并逐步完成了他们应该采取的验证步骤，以评估模型是否有效地代表了他们感兴趣的概念空间，并提供了方法附录，其中包含实现所讨论的所有内容所需的代码。在注意到嵌入模型的可供性的同时，我们还讨论了它们不断发展的局限性，并提出了它们何时不适合组织分析的建议。然后，我们展示嵌入模型如何实现依赖于概念和概念空间的构造的理论化和测量。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;我们概述了两大类词嵌入使用方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;度量之内/之间进行标记&lt;/strong&gt;，我们提出了跟踪相关分析集内部和之间的概念关系的度量，以帮助我们跟踪与概念广度、概念距离和概念相似性&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;意义及其维度&lt;/strong&gt;，我们提出了四种衡量标准，为了解意义及其与组织的关系提供了不同的窗口。为找出这些测量机会的理论可能性，我们重点介绍了一些研究进展。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;本论文的一个核心主张是，在组织研究不同广度和深度，词嵌入工具现在使我们能够表示其概念空间，并且比以前更精细地表示细节&lt;/strong&gt;。有鉴于此，我们的目标是展示嵌入模型如何在与组织科学家相关的领域中操作概念空间，使研究人员能够扩展和完善现有理论。我们希望这一理论指南和实践路线图将促进组织科学内部的理论扩展，该扩展首先是扩大对文本数据的访问以及用于分析的随附计算工具（Kovács 等人，2013 年; Goldberg 等人; 2016年，Hannigan 等人, 2016年, 2019； Guo 等人，2020）。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二概念和概念空间&#34;&gt;二、概念和概念空间&lt;/h2&gt;
&lt;p&gt;概念是人类生活的一个基本特征，我们的日常思维很大程度上依赖于它们所代表的信息，使我们能够对周围的人、物体和事件进行分类，并将这些信息传达给其他人（Murphy 2002；Bergen 和 Feldman 2008 年； Cassanto 和 Lupyan，2015 年）。概念是将我们的精神世界粘合在一起的粘合剂（Murphy 2002），赋予精神和物质体验以意义（Hannan et al. 2019）。&lt;strong&gt;在认知科学和心理学的语言中，概念是“事物类别的「心理表征」”（Murphy 2002）。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;概念有两大功能：分类和交流（Medin and Rips 2005），这些功能都需要语言的帮助。实际上，我们通过在语言中分配一个单词或短语来表示一个稳定概念的信息内容。这就是为什么我们通过说出或写出 “&lt;em&gt;&lt;strong&gt;manager&lt;/strong&gt;&lt;/em&gt;” 一词来提及经理的概念，从而引出它所包含的概念信息，例如对他人的责任、做出决策以及相对于组织同行获得更高的薪水。然后，语言的单词分割并链接了社区的共享概念空间（Lupyan 和 Bergen 2015）。这样，“一个概念就是一个单词或短语的含义……[包括]像 ‘&lt;em&gt;&lt;strong&gt;red&lt;/strong&gt;&lt;/em&gt;’ 和 ‘&lt;em&gt;&lt;strong&gt;grasp&lt;/strong&gt;&lt;/em&gt;’这样的基本的、具体化的单词，以及像 ‘&lt;em&gt;&lt;strong&gt;goal&lt;/strong&gt;&lt;/em&gt;’ 和 ‘&lt;em&gt;&lt;strong&gt;continuity&lt;/strong&gt;&lt;/em&gt;’ 这样的抽象和技术单词”（卑尔根）和 Feldman 2008]）。&lt;/p&gt;
&lt;p&gt;概念并不作为唯一的信息单位存在于真空中。相反，概念之所以有意义，是因为它们彼此相关（Hannan et al. 2019），“通过相似性和上下文的关系紧密地缝合在一起”（Hofstadter and Sander 2013）。在这种多重概念关系中存在着“我们对世界的大部分知识，告诉我们存在什么以及它们具有什么属性”（Murphy 2002，p.1）。例如，概念 &lt;em&gt;&lt;strong&gt;resource&lt;/strong&gt;&lt;/em&gt;  与  &lt;em&gt;&lt;strong&gt;firm&lt;/strong&gt;&lt;/em&gt;、&lt;em&gt;&lt;strong&gt;constraint&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;natural&lt;/strong&gt;&lt;/em&gt; 等概念相关。在文化系统的层面上，概念之间的相互关系引发了表征概念之间宏观层面有意义的维度。 &lt;em&gt;&lt;strong&gt;manager&lt;/strong&gt;&lt;/em&gt; 概念在某些方面与 &lt;em&gt;&lt;strong&gt;coach&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;president&lt;/strong&gt;&lt;/em&gt; 的概念很接近，而在其他方面则与&lt;em&gt;&lt;strong&gt;employee&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;bureaucracy&lt;/strong&gt;&lt;/em&gt; 的概念很接近。将概念理解为存在于复杂几何空间中的点，使我们能够思考和测量概念之间的距离远近（Hannan 等人，2019）。例如，与  &lt;em&gt;&lt;strong&gt;playground&lt;/strong&gt;&lt;/em&gt; 或 &lt;em&gt;&lt;strong&gt;ice cream&lt;/strong&gt;&lt;/em&gt; 相比， &lt;em&gt;&lt;strong&gt;manager&lt;/strong&gt;&lt;/em&gt; 与&lt;em&gt;&lt;strong&gt;organization&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;leader&lt;/strong&gt;&lt;/em&gt; 概念的联系更加紧密。&lt;strong&gt;我们将这种概念相关的多维空间称为概念空间&lt;/strong&gt;（Hannan et al. 2019)&lt;/p&gt;
&lt;p&gt;重要的是我们用复数来指代概念空间。对于许多单词来说，它们会根据使用的上下文表现出不同的概念信息模式。首先，概念可能会根据使用它们的社会背景而有所不同。例如，如果在执行董事会议室、商品交易大厅或附近的储蓄和贷款机构的背景下说出 “&lt;em&gt;&lt;strong&gt;Bank&lt;/strong&gt;&lt;/em&gt;”，指的是银行而不是河流。概念也可能根据使用时间的不同而有所不同。例如，“&lt;em&gt;&lt;strong&gt;高科技&lt;/strong&gt;&lt;/em&gt;” 一词所引发的概念关系会根据我们研究的是 1960 年代、1990 年代还是今天而有所不同。最后，概念关系因使用它们的社区而异，因此 “&lt;em&gt;&lt;strong&gt;债务&lt;/strong&gt;&lt;/em&gt;” 所捕获的概念将根据其是由首席财务官还是低收入个人使用而有所不同。概念所含信息存在多样性， 正如 Hannan等人（2019）指出，“虽然有些概念可能是天生的或生物驱动的，但大多数都是社会构建的。”&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三先前研究中的概念和概念空间&#34;&gt;三、先前研究中的概念和概念空间&lt;/h2&gt;
&lt;p&gt;概念以及扩展的概念空间是人类思维和交流的基础（Sperber 和 Wilson 1986；，Murphy 2002；Hofstadter 和 Sander 2013）。正因为如此，概念和概念空间对于许多组织理论框架来说或多或少是明确和关键的。在某些研究（例如类别研究）中，概念具有核心重要性并且已经被明确地理论化。然而，在其他情况下，（例如，公司基于知识视角）概念被隐含地假定，即使它们是决定许多理论期望的基本成分。鉴于概念无处不在，对组织科学所有领域使用概念信息进行全面回顾超出了本文的范围。我们将简短、非详尽的回顾集中在概念和概念空间概念的三个领域——&lt;strong&gt;类别、知识和文化&lt;/strong&gt;。通过嵌入技术处理并追踪存在于个人和社区头脑中的概念信息，研究其对组织行为和结果的影响。&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;31-类别&#34;&gt;3.1 类别&lt;/h3&gt;
&lt;p&gt;类别是具有共同特征和属性的实体组。如前所述，概念是类别的心理表征。对类别的研究主要集中在跨类别或模糊类别是否会增加或减少分类实体的估值。自Zuckerman（1999）以来的工作一直集中在消除歧义条件上，在这些条件下，类别跨越和模糊性会导致积极或消极的估值。许多研究表明，由于感知偏差（Durand et al. 2007）、不符合受众期望（Hsu 2006)、Hsu et al. 2009；Leung and Sharkey 2014） ，跨越模糊的类别会损害实体估值，或降低分类对比度（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B119&#34;&gt;Negro et al. 2010&lt;/a&gt;）。其他研究表明，跨越类别可以创造积极的估值结果，因为它表明非典型性可以放大良好的表现并缓冲不良表现（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B159&#34;&gt;Smith 2011&lt;/a&gt;），一个类别可以锚定认知，而另一个类别可以有益地修改认知（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B188&#34;&gt;Wry et al. 2014&lt;/a&gt;）。还有其他研究表明，效果取决于受众，有些人喜欢跨类别，而另一些人则不喜欢（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B135&#34;&gt;Pontikes 2012&lt;/a&gt;）。通过这些方式，类别可以通过影响有关类别成员资格的概念信息的解释方式，对行为和绩效产生积极或消极的影响。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn5&#34;&gt;4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;尽管类别范式的贡献历来是通过类别成员的集合和模糊集合理论（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B64&#34;&gt;Hannan et al. 2007&lt;/a&gt;）概念来实现的，但最近的工作开始纳入其多维性（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65&#34;&gt;Hannan et al. 2019&lt;/a&gt;）和类别的分级归属感。组织学者感兴趣的许多现象都是由概念及其代表的类别之间的精确距离支撑的。例如，鉴于专利所贡献的技术领域，专利通常分为类别和子类。然而，专利中编码的想法可以传播到创新空间的广泛领域，即使只分类在一个类别中。正如我们稍后讨论的，转向概念的几何概念，使分析师能够考虑隶属度、重叠和连续距离影响底层实体评估判断的方式&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65&#34;&gt;（Hannan 等人，2019 &lt;/a&gt;。&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;32-知识&#34;&gt;3.2 知识&lt;/h3&gt;
&lt;p&gt;众所周知，知识很难具体说明，并且在哲学、认知科学和社会科学领域，围绕其概念性质进行了长期而活跃的争论（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B166&#34;&gt;Steup 和 Neta 2020&lt;/a&gt;）。然而，过去几十年来，组织科学在微观、中观和宏观层面上进行了大量研究，解决有关知识及其在团队、组织和经济活动中的作用的问题。从对团队成员专业知识的研究（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B164&#34;&gt;Srikanth et al. 2016&lt;/a&gt;）到公司基于知识和注意力的观点（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B87&#34;&gt;Kogut and Zander 1992&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B56&#34;&gt;Grant 1996&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B123&#34;&gt;Ocasio 1997&lt;/a&gt;）；从交互记忆系统（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B143&#34;&gt;Ren 和 Argote，2011&lt;/a&gt;）到创新流程（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B50&#34;&gt;Garud 等，2013&lt;/a&gt;）；从组织设计（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B45&#34;&gt;Foss et al. 2013&lt;/a&gt;）到搜索和探索（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B93&#34;&gt;Lavie et al. 2010&lt;/a&gt;），知识在最近的组织理论化中发挥着核心作用。&lt;/p&gt;
&lt;p&gt;无论人们对知识的定义如何选择，命题性知识从根本上都与概念信息相关。&lt;em&gt;&lt;strong&gt;命题知识采取“ S [主体]知道p [命题]”&lt;/strong&gt;&lt;/em&gt; 的形式（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B80&#34;&gt;Ichikawa and Steup 2018&lt;/a&gt;）。在某种程度上，命题是由语言中的单词编码的，并且单词代表概念信息，命题知识依赖于概念以及它们如何在概念空间中交织（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B110&#34;&gt;McGrath and Frank 2020&lt;/a&gt;）。以命题“泰勒知道氢的主要工业应用是氨的制造”和“特里知道量子算法可以具有较低的时间复杂度”为例。这些知识命题中的每一个都代表了不同的概念意义，前面提到的领域将以不同的方式操作它们。例如，团队学者可能会强调，由泰勒和特里组成的专利团队将拥有多样化的基础知识。采取基于注意力观点的学者会注意到，泰勒和特里可能会以不同的方式关注知识空间，以应对组织变革。研究创新的人可能会注意到如果泰勒和特里共享办公空间，知识重组的潜力。研究搜索的人可能会假设，为了解决问题，泰勒和特里会以不同的方式搜索概念性解决方案。在所有这些情况下，就这些领域通过诉诸语言编码的命题知识来理论化知识动态而言，它们以基本和可测量的方式参与概念和概念空间。&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;33-文化&#34;&gt;3.3 文化&lt;/h3&gt;
&lt;p&gt;文化被不同地概念化为集体的共同价值观、故事、框架、工具包和类别（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B52&#34;&gt;Geertz 1973&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B131&#34;&gt;Pettigrew 1979&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B92&#34;&gt;Lamont 和 Small 2008&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B158&#34;&gt;Small 等人 2010&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B54&#34;&gt;Giorgi 等人 2015&lt;/a&gt;）。文化建构已成为组织研究的核心，在从个人和团队到组织和国家的各个层面的分析中都得到了运用（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B54&#34;&gt;Giorgi et al. 2015&lt;/a&gt;）。从理解文化如何塑造职业结构（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B195&#34;&gt;Glynn 2000&lt;/a&gt;）、组织领域（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B194&#34;&gt;Anteby 2010&lt;/a&gt;）和创业环境（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B106&#34;&gt;Lounsbury and Glynn 2001&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B141&#34;&gt;Rao and Giorgi 2006&lt;/a&gt;）到它在讲故事（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B106&#34;&gt;Lounsbury and Glynn 2001&lt;/a&gt;）和身份建设中的作用（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B196&#34;&gt;Ravasi 和 Schultz 2006&lt;/a&gt;），从其对人际沟通的塑造（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165&#34;&gt;Srivastava 等人，2018&lt;/a&gt;）到对组织绩效的影响（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26&#34;&gt;Corritore 等人，2020&lt;/a&gt;），文化深深地受到概念及其互动方式的调节。文化以集体认知过程为基础（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B36&#34;&gt;DiMaggio 1997&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B128&#34;&gt;Patterson 2014&lt;/a&gt;），很大程度上可以通过语言痕迹来获取。语言进入文化的窗口（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B55&#34;&gt;Goldberg et al. 2016&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165&#34;&gt;Srivastava et al. 2018&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26&#34;&gt;Corritore et al. 2020&lt;/a&gt;）很大程度上是通过它所表达的概念来呈现的，使得概念和概念空间成为组织文化研究的重要支柱。&lt;/p&gt;
&lt;p&gt;基于它们在形成范畴、知识和文化方面的关键作用，概念和概念空间已成为许多组织理论赖以建立的知识支架的重要组成部分。然而，概念和概念空间通常仅被用作缺乏精确和可扩展的经验表征的不明确的隐喻。这限制了研究使用粗粒度的代理测量或允许手动编码和解释的小数据集。接下来，我们提出词嵌入模型是一种最先进的工具，用于表示概念和概念空间，可以添加到组织学者工具包中。就组织学者寻求将概念和概念信息所支撑的结构操作化而言，他们将得到这类新模型的帮助。考虑到这一点，我们接下来介绍嵌入模型如何工作以及为什么它们可以作为概念和概念空间的有效表示。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四使用词嵌入来表示概念和概念空间&#34;&gt;四、使用词嵌入来表示概念和概念空间&lt;/h2&gt;
&lt;h3 id=&#34;41-越来越多地使用文本作为数据&#34;&gt;4.1 越来越多地使用文本作为数据&lt;/h3&gt;
&lt;p&gt;过去 10 年，通过计算工具和方法进行文本数据分析出现了爆炸性增长。从社会学（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B40&#34;&gt;Evans and Aceves 2016&lt;/a&gt;）到经济学（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B53&#34;&gt;Gentzkow et al. 2019&lt;/a&gt;）和政治学（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B58&#34;&gt;Grimmer and Stewart 2013&lt;/a&gt;），文本正迅速成为组织、经济和社会生活的中心观察站。文本数据提供了在线知识社区、财报电话会议和公司报告、产品评估、组织电子邮件和讨论板、历史档案、视频转录和电影字幕、医疗记录、电子商务、社交媒体等多种领域的丰富思想和行为痕迹。媒体平台、新闻文章、科学学科等等。总而言之，这些文本数据源比以往任何时候都更深入、更广泛地进入组织生活。正如&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B40&#34;&gt;Evans 和 Aceves（2016 年&lt;/a&gt;）指出的那样，文本数据现在使我们能够访问“有关正在玩的社交游戏的隐藏元素及其背后的社交世界”的深层信息。然而，这些语料库的庞大规模及其广泛的范围意味着，提取理论上有意义的信息信号越来越多地受到计算方法的帮助，利用信息技术方法获取大量非结构化文本数据，并将它们转换为有意义且相关的度量。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn6&#34;&gt;5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文本数据与组织学者习惯使用的定量数据之间的一个主要区别是文本是高维的。正如&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B53&#34;&gt;Gentzkow 等人（2019 年&lt;/a&gt;）指出，“仅使用英语中一千个最常用单词的 30 个单词的 Twitter 消息样本 [&amp;hellip;] 的维度大致与宇宙中的原子一样多。” 因此，使用文本作为数据的学者的中心任务是通过对数据施加限制来降低维度。&lt;strong&gt;过去二十年里，组织科学中用于降低这一维度的一些最常用的计算工具是词典法、语义网络和主题模型。尽管这些方法有其优点，但一个主要缺点是它们无法对文本中存在的细粒度概念关系和关联进行编码&lt;/strong&gt; 。接下来，我们将展示嵌入模型如何利用文本中的局部和更广泛的信息来训练概念含义和概念空间的高保真表示。在此过程中，我们展示了词嵌入模型如何克服先前方法来表示文本中编码的含义的一些局限性，从而允许对理论结构进行更细粒度的测量，并实现新的理论可能性。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;42-词嵌入&#34;&gt;4.2 词嵌入&lt;/h3&gt;
&lt;p&gt;我们之前解释过，概念是事物类别的心理表征，人类通过在词典中分配一个单词或短语来表示稳定的概念，并指出，概念只有在与跨多个维度的其他概念相关并为其提供信息时才有意义。密集的概念空间。在这里，我们认为词嵌入模型是最近开发的一类从机器学习应用于自然语言处理的模型，它使我们能够有效且高效地表示概念空间，并将这些空间用于追求组织科学。词嵌入模型是文本语料库中单词的连续表示，可以进行几何解释。&lt;strong&gt;词嵌入的方法论假设，一个词的含义很大程度上是由出现在其直接和更广泛上下文中的词所决定的，这一想法受到结构语言学家的启发，他们已经证明，含义的差异与局部分布相关（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B68&#34;&gt;Harris 1954&lt;/a&gt;）， 这个想法现在被称为 「分布式语义学」，Firth 的著名描述是：“观其伴而知其意”（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B42&#34;&gt;Firth 1957&lt;/a&gt;，you shall know a word by the company it keeps）， 一个单词所代表的概念或含义可以通过它周围的单词的分布来推断&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;以这种分布式方式思考概念和概念空间的底层计算架构可以追溯到 20 世纪 80 年代初期计算机科学家 Geoffrey Hinton 的工作（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B71&#34;&gt;Hinton 1986&lt;/a&gt; , &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B72&#34;&gt;Hinton et al. 1986&lt;/a&gt;）以及认知科学家在这一时期研究的并行分布式处理模型（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B149&#34;&gt;Rumelhart 等人，1986a&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B150&#34;&gt;b&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B109&#34;&gt;McClelland 和 Rumelhart，1989&lt;/a&gt;）。分布式架构是当前嵌入语言模型的基础（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115&#34;&gt;Mikolov et al. 2013b&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130&#34;&gt;Pennington et al. 2014&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35&#34;&gt;Devlin et al. 2019&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104&#34;&gt;Liu et al. 2019&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B17&#34;&gt;Brown et al. 2020&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B41&#34;&gt;Fedus et al. 2020）。 2021&lt;/a&gt;）， 嵌入模型 Word2Vec 算法(&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115&#34;&gt;Mikolov 等 2013b&lt;/a&gt;) 相对简单易用，能够处理中等规模的语料库来。 &lt;strong&gt;Word2Vec 与  GloVe（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130&#34;&gt;Pennington 等人，2014 年&lt;/a&gt;）和 FastText（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B13&#34;&gt;Bojanowski 等人，2017 年&lt;/a&gt;）等嵌入算法，是 ChatGPT 和相关模型的基础&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;找个例子来帮助理解算法， 现在我们要创建过去 50 年创新的概念空间表示。首先需要概念活动领域的文本数据， 美国专利局数据提供了创新活动的踪迹，其中包括所有专利的文本、摘要、描述和权利要求。在整篇论文中，我们使用这个专利摘要语料库来指导读者完成训练这个概念空间和构建相关概念测量的过程。数据是从&lt;a href=&#34;https://patentsview.org/&#34;&gt;Patentsview.org&lt;/a&gt;免费下载的，使用 1976 年至 2019 年间发布的所有专利来构建本文中发现的词嵌入模型和测量相关指标。&lt;/p&gt;
&lt;p&gt;想象一下，专利语料库中的每个独特单词都是从放置在巨大冰箱上的随机放置的 &lt;strong&gt;“word magnet”&lt;/strong&gt; 开始的（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B76&#34;&gt;Hovy 2020&lt;/a&gt;）。当连续词袋 (CBOW) 算法滚动浏览语料库时，使用每个目标词周围的单词词(滑动窗口的上下文)来预测目标词（更多内容见下文）。该算法的最终目标是产生一种语义模型，其中出现在相似上下文中的单词彼此接近，而来自不同上下文的单词则相距很远。由于用2维概念空间不足以捕获每个单词的全部含义，因此该算法改为在更高的（100-1,000）维空间内捕捉语义。通过这种方式，目标单词的概念信息是从它周围的单词中归纳出来的，将语料库中的每个单词绘制为&lt;em&gt;n&lt;/em&gt;维空间中的坐标或向量。正是单词在这个&lt;em&gt;n&lt;/em&gt;维向量空间中的相对位置，使我们能够将词嵌入模型可以描述代表人类概念活动区域的概念空间。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn7&#34;&gt;6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;概念意义的识别假定了嵌入空间的可解释性。接下来，我们提出了对这些概念空间的一系列提示和测量，作为从中产生结构化解释的方法。这很像心理学家使用 &lt;strong&gt;心理测量调查&lt;/strong&gt; 将概念印象转化为可解释的观点（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B112&#34;&gt;Michael Furr 2021&lt;/a&gt;）。或者&lt;strong&gt;认知人类学家如何使用结构化任务，例如排序和排名（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B163&#34;&gt;Spradley 2016&lt;/a&gt;），将概念性的世界观转变为可解释的世界观&lt;/strong&gt;。我们认为嵌入模型必须接受结构化测量（就像向人类受试者提供的心理测量问卷）使他们的 **概念景观(conceptual landscape)**变得可解释。接下来，我们将引导读者如何用专利语料库训练创新概念空间表示的过程。之后， 我们概述了该方法的优点和局限性，并指出这些方法与先前的文本分析方法和组织研究实践的关系。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;43-选择语料库&#34;&gt;4.3 选择语料库&lt;/h3&gt;
&lt;p&gt;学者可以根据应用使用两种词嵌入模型。一方面，研究人员可以使用自有文本语料库来训练表示， 据此了解文本所涉主体(个人、团体、社会)行为的概念空间是什么样子， 以及概念关系揭示人类活动背景。在我们的示例中，专利创新在专利语料库中得到了很好的体现，因此我们在下面展示了如何从头开始训练概念空间表示, 以及它揭示了哪些概念联系。研究人员可以从头开始训练语料库的其他例子包括在线社区（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18&#34;&gt;Burtch et al. 2021&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2&#34;&gt;Aceves et al. 2022&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B23&#34;&gt;Chambers et al. 2022&lt;/a&gt;）、学术学科（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74&#34;&gt;Hofstra et al. 2020&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B102&#34;&gt;Lin et al. 2022&lt;/a&gt;） 、劳动力市场（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B9&#34;&gt;Bana 2022&lt;/a&gt;）、公共记录（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B6&#34;&gt;Arseniev-Koehler et al. 2022&lt;/a&gt;）、产品和公司描述（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61&#34;&gt;Guzman and Li 2023&lt;/a&gt;）以及财报电话会议和公开演讲（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B85&#34;&gt;Kirgil and Voyer 2022&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;或者，如果研究人员想要在较小的语料库中追踪概念动态，而该语料库的大小不足以训练独特的、特定于上下文的嵌入，那么研究者可以使用预训练嵌入模型，需要注意，训练预训练嵌入模型的文本与研究者小语料库在内容、场景要有相似性。广泛使用的预训练嵌入已经在来自海量语料库的文本上进行了训练，例如新闻（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B4&#34;&gt;Google 2013&lt;/a&gt;）、维基百科（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35&#34;&gt;Devlin et al. 2019&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B57&#34;&gt;Grave et al. 2018&lt;/a&gt;）。训练这些预训练嵌入模型的文本语料体量很大， 内容题材往往包含我们较小文本样本中存在的概念。因此使用预训练嵌入对这些概念的信息进行编码，并可用于近似相关距离。政治和历史语义背景下的研究发现，预训练嵌入提供的结果与特定于上下文的嵌入相当（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;Kozlowski et al. 2019&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145&#34;&gt;Rodriguez and Spirling 2022&lt;/a&gt;）。如果有理由相信研究项目中包含的概念和想法没有在这些大量预训练嵌入中得到很好的体现，研究人员可以使用较小语料库中的文本对其进行 &lt;strong&gt;微调（Fine-Tune）&lt;/strong&gt;（ &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104&#34;&gt;Liu et al. 2019，Burtch et al.2019&lt;/a&gt;）&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18&#34;&gt;， 2021&lt;/a&gt;）。微调将预训练的概念空间扭曲为与样本一致（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104&#34;&gt;Liu et al. 2019&lt;/a&gt;），更好地反映概念之间的关系。&lt;/p&gt;
&lt;p&gt;最后，使用哪一种嵌入(自己训练的嵌入、 预训练的嵌入、微调的嵌入)将取决于研究人员的目的以及他们寻求追踪的概念动态的类型。接下来，我们将重点描述从头开始训练和验证嵌入模型的过程。在接下来的部分中，我们讨论不同参数设置和策略之间的权衡，并鼓励读者遵循文章文本和在线附录。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;44-清理语料库&#34;&gt;4.4 清理语料库&lt;/h3&gt;
&lt;p&gt;训练嵌入模型的第一步是使用 Python 等编程语言录入文本语料库， 首先获取每个专利摘要中的文本， 并将连续的文本进行切词，转化为单词列表 。然后，我们将文本小写，删除标点符号和数字字符串，并将每个摘要转换为称为token的单词列表。但是这可能破坏一些词组语义，这里使用 &lt;em&gt;&lt;strong&gt;bi-gram&lt;/strong&gt;&lt;/em&gt;， 识别高频共现的词组成词组，例如当 &lt;em&gt;&lt;strong&gt;“electric”&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;“vehicle”&lt;/strong&gt;&lt;/em&gt; 这两个词在某些上下文中一起出现时，它们将被统一形成短语和概念 &lt;em&gt;&lt;strong&gt;“electric_vehicle”&lt;/strong&gt;&lt;/em&gt; 。建立单词或短语列表后，执行单词嵌入算法来学习单词或二元组及其语言上下文之间的最佳距离，以保留语言中单词和短语的概念空间。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;45-训练嵌入模型&#34;&gt;4.5 训练嵌入模型&lt;/h3&gt;
&lt;p&gt;第一步是选择词嵌入算法， 浅层神经网络构建的单词表示（例如，Word2Vec、FastText；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115&#34;&gt;Mikolov 等人，2013b&lt;/a&gt;）、共现矩阵的低秩近似（GloVe；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130&#34;&gt;Pennington 等人，2014&lt;/a&gt;） ，或来自 Transformer 的深度上下文嵌入（例如 BERT、&lt;em&gt;GPT&lt;/em&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35&#34;&gt;Devlin 等人 2019&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B139&#34;&gt;Radford 等人 2022&lt;/a&gt;）。这些不同算法输出，都可以被解释为&lt;em&gt;n&lt;/em&gt;维概念空间，其中单词或短语由空间内的向量位置表示。本文我们只介绍 Word2Vec 算法， word2vec 是一种广泛使用的训练概念空间的算法（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B113&#34;&gt;Mikolov 等人，2013a&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115&#34;&gt;b&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;Word2Vec 算法的一种流行实现算法是连续词袋 (CBOW) 算法，可以在 Gensim python 库中轻松访问，该算法使用目标单词的语言上下文来预测被扣掉的目标词 (可以简单的理解为让机器做完形填空题) ， 比较适合小规模数据集。 Word2Vec 还实现了另一种 Skip-Gram 算法，该算法通过从目标单词预测上下文单词来反转 CBOW 的预测任务，比较适合大规模数据集。相比之下，skip-gram 将每个上下文目标对（例如，T：“房子”，C：“宽敞”）视为单独的观察，因此可以更好地捕获精确的语义，但需要更大的语料库才能获得卓越的性能。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;46-维数&#34;&gt;4.6 维数&lt;/h3&gt;
&lt;p&gt;考虑维数很有必要。朴素的模型可以将不重复总词数作为维度， 例如包含 100,000 个不重复单词的语料库， 任何单词都需要  100,000 维才能准确表示。然而，当单词从上下文中被识别为相似时，可以一定范围内减少维度数。&lt;strong&gt;维度过多会导致内存需求和冗余增加，并降低可解释性；维度太少会扭曲距离并且无法解释语言的不及物性&lt;/strong&gt;。通过这种方式，通过具有至少足够的维度来捕获所讨论的复杂语义关系，可以获得准确的预测。&lt;/p&gt;
&lt;p&gt;在实践中，300 维已经成为一个标准，很大程度上源于最初的 Word2Vec 论文之后的惯例，该论文通过交叉验证确定了最佳维数，以减少预测屏蔽词任务中的错误。大多数后续分析都是建立在较小、多样性较低的文本集合上，需要较少的维度，因此 300 通常被用作上限。最近的工作表明，应根据语料库统计数据选择维度 - 语料库词汇表中成对等距单词的数量提供了维度数量的下限，低于此界限通常会导致单词嵌入质量下降（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B127&#34;&gt;帕特尔和巴塔查亚 2017&lt;/a&gt;）。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74&#34;&gt;霍夫斯特拉等人。(2020)&lt;/a&gt;使用 100、200 和 300 维的模型找到了稳健的结果。&lt;/p&gt;
&lt;p&gt;如果分析师寻求实现维度可解释性，他们必须以最小失真来确定表示数据所需的维度数。 但这最后一步一半很少执行，因为维度的优化需要大量的时间和计算资源。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;47-窗口尺寸&#34;&gt;4.7 窗口尺寸&lt;/h3&gt;
&lt;p&gt;回想一下，窗口大小是指算法将用来焦点目标词（或其邻居）之前和之后的单词数量。该窗口最小可以是 1。对于较小的窗口，算法将倾向于对句法关系进行编码（例如，名词后跟动词）。&lt;strong&gt;随着窗口大小的增加，更多的含义和语义被编码到模型输出中&lt;/strong&gt;。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145&#34;&gt;考虑Rodriguez 和 Spirling (2022)&lt;/a&gt;的示例，其中包含两个句子的语料库：(1)“狮子吃肉”和 (2)“牛吃草”。当窗口大小为一时，我们会知道牛和狮子都吃东西，从这个意义上说，牛和狮子在语法上是等价的，因为我们没有足够的信息来区分两者。然而，随着窗口的增加，算法开始对牛与狮子的含义进行更多编码。&lt;strong&gt;与维度数量一样，这里的回报也递减，窗口大于五个字的模型性能略有改善&lt;/strong&gt;（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145&#34;&gt;Rodriguez 和 Spirling 2022&lt;/a&gt;）。 &lt;strong&gt;BERT 和 GPT 系列等上下文模型具有更大的窗口，这些窗口通过注意力过程进行驯服，算法通过该过程识别哪些上下文单词对于解释焦点单词的含义很重要&lt;/strong&gt;（Vaswani 等人，2017 年&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B176&#34;&gt;）&lt;/a&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;48-验证模型&#34;&gt;4.8 验证模型&lt;/h3&gt;
&lt;p&gt;最后一步是验证词嵌入模型，这样做是为了确认算法学习的表示与文本数据所承载的真实人类活动的概念空间表示尽可能相近。论文附录第 2 节描述了关于专利嵌入的七个详细验证程序，表明该模型有效地学习了创新空间的表示。这些包括（1）邻近嵌入词的语义相似性；(2)具有嵌入距离的语义梯度；(3)嵌入簇与语义域之间的对应关系；（4）物理世界距离与嵌入之间的相关性；(5) 社会距离与嵌入之间的相关性；(6) 嵌入空间类比推理的准确性；(7)嵌入文档的语义一致性。我们还讨论了第八个“额外”测试，即图灵测试（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B174&#34;&gt;Turing 1950&lt;/a&gt;）。由 Transformer 支持的现代上下文嵌入的评估标准是它们是否能够与人类毫无区别地参与任何分类、关联、意义生成或集成任务，包括普通对话和专家教程。OpenAI 的 ChatGPT 和许多竞争的聊天机器人已经展示了如此强大的性能，以至于图灵测试正在迅速从上限转变为基线基准。这些验证步骤与论文最后部分的测量相结合，作为嵌入模型的有用提示prompt和测量，使研究人员能够对其编码的概念空间提供结构化解释。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;49-词嵌入方法的优点和缺点&#34;&gt;4.9 词嵌入方法的优点和缺点&lt;/h3&gt;
&lt;h4 id=&#34;491--无需正式指定相关尺寸&#34;&gt;4.9.1  无需正式指定相关尺寸&lt;/h4&gt;
&lt;p&gt;对概念建模的正式尝试试图通过逻辑演绎方法清楚地枚举概念的相关维度（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B47&#34;&gt;Gärdenfors 2004&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B48&#34;&gt;Gardenfors 2014&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65&#34;&gt;Hannan 等人 2019&lt;/a&gt;）。尽管这种方法对于理解限定领域内的概念很有用，但即使如此，它也可能不切实际且难以衡量，因为很难先验地陈述分析师应预期的相关维度&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B73&#34;&gt;（Hofstadter 和 Sander 2013 ）&lt;/a&gt;。 &lt;strong&gt;词嵌入的优点在于，概念之间的关系以及对任何给定概念重要的相关维度可以从语言的使用方式中推断出来，因此不需要事前指定&lt;/strong&gt;。鉴于在分析之前没有必要陈述相关维度，即使是最复杂的组织行为剧场也变得易于分析处理。正如其他人所指出的，“词嵌入为语言中包含的多个维度的含义提供了全面且有意义的见解，这是以前的方法无法捕获的”（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105&#34;&gt;Lix 等人，2022 年&lt;/a&gt;，第 8434 页）。在某种程度上，这种优势源于这样一个事实：神经网络架构能高效地记录意义的维度。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;492-更大的有效维度&#34;&gt;4.9.2 更大的有效维度。&lt;/h4&gt;
&lt;p&gt;嵌入通常由 100 到 1,000 个密集编码维度表示。&lt;strong&gt;编码的密度意味着每个词向量在所有建模维度上都有一个非零坐标&lt;/strong&gt;。正如附录中所指出的，主题模型可能具有相同数量的主题（例如，100-1,000），但这些主题被稀疏编码以方便人类解释，使得主题仅具有一些基本上非零的单词加载，并且文档仅具有少量非零的主题负载。&lt;strong&gt;因此，主题模型是为了描述而构建的，但代价是迫使其表示的有效维度从数百个减少到几个，从而扭曲了本来可以在主题空间内计算的距离。相比主体模型， 词嵌入使用密集编码，每维度的嵌入很难理解和描述，但距离具有更大的自由度，可以更精确地编码含义&lt;/strong&gt;。通过这种方式，相对于低维理论和测量，嵌入为分析师提供了“大量潜在轴，个人和社会群体可以沿着这些轴竞争、合作、分裂或合并”（Kozlowski et al. 2019，p.27 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;）&lt;/a&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;493-无监督训练&#34;&gt;4.9.3 无监督训练。&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;词嵌入还有一个特殊优点，即训练模型时， 以看似无监督或自监督的方式进行，从而避免了手动编码文本语义内容的繁琐，完全由机器学习&lt;/strong&gt;。在我们的创新示例中，向量空间由我们专利语料库中的每个发明人按照他们所写句子的数量和长度的比例进行监督。每个单词的滑动窗口都是为了向专利审查员和未来的发明者传达一种含义而构建的，该算法用于构建向量空间并以概念上适当的方式定位单词。因此，学者们可以利用专利语料库来训练 &lt;strong&gt;技术创新&lt;/strong&gt; 的概念空间，利用财报电话会议记录和新闻稿来训练 &lt;strong&gt;上市公司沟通&lt;/strong&gt; 的概念空间，利用分析师报告来训练 &lt;strong&gt;投资分析&lt;/strong&gt; 的概念空间，或者特定领域的概念空间。使用内部通信（例如 Slack 和电子邮件）来了解公司的知识。这些概念空间可以在最少的监督下进行训练，因此很快成为有价值的观察站，用于追踪组织科学家关注的组织生活的静态和动态（Hofstra et al. 2020，Whalen et al. 2020，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74&#34;&gt;Burtch&lt;/a&gt; et &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184&#34;&gt;al&lt;/a&gt; . &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18&#34;&gt;2021&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B177&#34;&gt;Waller 和 Anderson 2021&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2&#34;&gt;Aceves 等人 2022&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20&#34;&gt;Carlson 2022&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B23&#34;&gt;Chambers 等人 2022&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61&#34;&gt;Guzman 和 Li 2023&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94&#34;&gt;Lawson 等人 2022&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105&#34;&gt;Lix 等人 2022&lt;/a&gt;）。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;494-共现是不必要的&#34;&gt;4.9.4 共现是不必要的。&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;这些模型的另一个优点是，两个概念不必在任何文档中同时出现，就可以将它们编码为相似的向量&lt;/strong&gt;。所需要的只是它们与相似的概念同时出现。例如，我们可以先验地指出 &lt;em&gt;&lt;strong&gt;医生&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;律师&lt;/strong&gt;&lt;/em&gt; 在某些方面非常相似（例如，他们需要高级学位，具有高收入水平等），但他们可能永远不会同时出现在语料库的同一文档中。尽管彼此之间缺乏共现性，但它们很可能都独立地与高收入*、&lt;em&gt;高学历&lt;/em&gt;、*白领等概念同时出现，从而最终拥有编码这些相似性的接近向量。&lt;strong&gt;因此，嵌入模型的底层计算架构可以更好地近似社会和文化含义，而无需求助于严格的共现&lt;/strong&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;495-上下文相关的含义结构&#34;&gt;4.9.5 上下文相关的含义结构。&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;使用定制训练的嵌入模型的一个优点是它将捕获上下文相关的含义结构&lt;/strong&gt;。例如，&lt;em&gt;&lt;strong&gt;“甜”&lt;/strong&gt;&lt;/em&gt; 的含义在软件团队的背景下与 &lt;em&gt;&lt;strong&gt;烹饪&lt;/strong&gt;&lt;/em&gt; 的背景下会有所不同。正如&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105&#34;&gt;Lix 等人。（2022）&lt;/a&gt;指出，在软件团队的背景下，与 &lt;em&gt;&lt;strong&gt;“甜蜜”&lt;/strong&gt;&lt;/em&gt; 最接近的术语是 &lt;em&gt;&lt;strong&gt;“强烈”&lt;/strong&gt;&lt;/em&gt;、 &lt;em&gt;&lt;strong&gt;“兴奋”&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;“耶”&lt;/strong&gt;&lt;/em&gt;。此外，就同一个单词编码不同概念（一词多义）而言，单词每种含义的概念信息都位于单词嵌入内的线性叠加（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B5&#34;&gt;Arora et al. 2018&lt;/a&gt;）。这意味着编码诸如 &lt;em&gt;&lt;strong&gt;“Bank”&lt;/strong&gt;&lt;/em&gt; 之类的单词的&lt;em&gt;n&lt;/em&gt;维向量包含其代表的所有概念的概念信息，例如 &lt;em&gt;&lt;strong&gt;河边&lt;/strong&gt;&lt;/em&gt; 或 &lt;em&gt;&lt;strong&gt;金融机构&lt;/strong&gt;&lt;/em&gt;。通过这种方式，即使在多义词的情况下，单词的上下文相关含义也会被编码到模型中。当这些上下文相关的含义不仅不同，而且是排他的或相反的时，来自转换器的上下文相关嵌入可以为上下文中的每个单词呈现不同的单词向量。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;496-几何有助于概念人群体和组织的细粒度表示&#34;&gt;4.9.6 几何有助于概念、人、群体和组织的细粒度表示。&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;我们认为，词嵌入模型可以在训练的语料库范围内产生人类活动概念空间的细粒度表示&lt;/strong&gt;。&lt;strong&gt;这意味着，从概念空间内编码的信息中，我们可以恢复个人、群体和组织本身的细粒度表示&lt;/strong&gt;。以我们的创新案例为例，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F1&#34;&gt;图 1&lt;/a&gt;描述了在说明性二维空间中这是如何实现的。学习到的概念空间将由单词或短语w表示的概念作为其最原子的分析级别。我们的限制性示例显示了在2维空间中排列的九个单词。单词 1-3 由发明人 1 使用，单词 4-6 由发明人 2 使用，单词 7-9 由发明人 3 使用。&lt;strong&gt;通过获取每个人的单词向量的质心向量，我们可以得出每个发明人在创新的概念空间&lt;/strong&gt;。&lt;strong&gt;将这个过程提升到团队和组织级别，我们可以在发明人团队和组织的概念空间内得出独特的向量&lt;/strong&gt;。因此，词嵌入架构不仅在概念的最原子级别上是细粒度的，而且还可以在更聚合级别上提供细粒度的表示。相对于团队多样性、组织差异化和注意力等结构的粗粒度代理，这形成了显着的测量改进，这些结构在嵌入特定概念空间时是有意义的。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/figure-1.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;strong&gt;图 1.嵌入作为概念、人员、群体和组织的细粒度表示&lt;/strong&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;497-细粒度几何减少了上下文信息的丢失&#34;&gt;4.9.7 细粒度几何减少了上下文信息的丢失。&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;由于粗糙、粗粒度的代理指标无法承载相关信息，在实证分析和相关理论构建中就无法利用这些信息&lt;/strong&gt;。嵌入模型的优势在于其独特的信息表征，可以携带更多的信息，信息的粒度更小，保存的信息量更多。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F2&#34;&gt;图 2&lt;/a&gt;使用团队多样性的示例来说明如何实现这一点。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F2&#34;&gt;图 2(a)&lt;/a&gt;显示了两个团队，1 和 2，每个团队要么通过熵（一种标准的、集合论多样性的理论度量（顶行））来表示，要么通过概念广度（基于底层概念的细粒度度量）来表示。团队调动的信息（底行）。团队 1 和团队 2 都有四名成员，团队 1 由两名生物化学家、一名化学家和一名分析化学家组成，团队 2 由两名生物化学家、一名海洋学家和一名计算机科学家组成。&lt;strong&gt;由于两个团队的团队成员类型比例相同，因此它们都被编码为具有相同的团队多样性熵度量 1.5&lt;/strong&gt;。**然而，当考虑团队成员的概念信息时，我们发现它们是本质上不同类型的团队，团队 1 的多样性或概念范围远不如团队 2 **。这表明粗粒度的测量可能会留下未开发的有价值的上下文信息（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B187&#34;&gt;Wolpert et al. 2014&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B33&#34;&gt;DeDeo 2017&lt;/a&gt;）。因此，我们应该看到更细粒度的衡量标准与相关的、理论上的绩效结果之间的联系更加紧密和一致。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/figure-2.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;strong&gt;图 2.（在线彩色)细粒度表示可防止有价值的信息丢失&lt;/strong&gt;&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;专利数据集使我们能够通过三种构建的措施来说明这一主张。首先，集合论团队多样性度量，使用团队先前专利在专利主要类别中的分布（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B79&#34;&gt;Huo 等人，2019&lt;/a&gt;）。第二种替代措施使用专利子类，以便它们提供相对于第一种更细粒度的措施。第三个衡量标准依赖于团队成员先前专利在创新概念空间内的&lt;strong&gt;概念广度&lt;/strong&gt;。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;498-词嵌入的局限&#34;&gt;4.9.8 词嵌入的局限。&lt;/h4&gt;
&lt;p&gt;到目前为止，我们的注意力仅限于讨论嵌入模型的结构，描述它们与概念空间的关系，并注意到它们的优点。在这里我们将说明其局限性，讨论它们的严重性、改善方式，以及何时不要用词嵌入的意外情况。我们讨论三类限制。第一个源于神经网络模型一般复杂的“黑匣子”性质，以及这带来的具体挑战，涉及输入数据的偏差，以及模型正确推理的范围，特别是那些对超出分析师背景的数据进行预训练的模型。第二个与这些模型的大小以及训练它们所需的数据量有关。第三个问题涉及词嵌入模型的具体局限性以及从脱离韵律和表达上下文的文本数据中分析含义的挑战。&lt;/p&gt;
&lt;p&gt;许多学者首先担心的是，多级神经网络模型显得复杂且在统计上难以理解，&lt;strong&gt;经常被批评为“黑匣子”方法&lt;/strong&gt;，无法“打开”以询问其性能背后的机制（Knight 2017，Leavitt et &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B86&#34;&gt;al&lt;/a&gt; . &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B95&#34;&gt;2021&lt;/a&gt;） 。现代神经网络词嵌入模型通常作为自监督模型实现，该模型启发式搜索单词之间的依赖关系空间以预测屏蔽词的身份。&lt;strong&gt;自从第一个高性能嵌入发布（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115&#34;&gt;Mikolov 等人，2013b&lt;/a&gt;）以来，对其黑盒性质的一些担忧已经减弱，因为数学家发现最流行的“浅”词嵌入模型（如 Word2Vec 和 FastText）获得了很大的优势&lt;/strong&gt;。其强大功能来自于近似易于理解的矩阵分解方法的运算，例如因子分析、主成分分析和对应分析（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B100&#34;&gt;Levy 和 Goldberg 2014&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;“黑盒”输入输出方法带来的一个相关潜在限制是，&lt;strong&gt;输入的偏差将转化为输出中的偏差&lt;/strong&gt;——用于训练嵌入的语料库的偏差将被编码在生成的单词嵌入模型中（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B14&#34;&gt;Bolukbasi等人，2016&lt;/a&gt;）。当模型用于现实世界的下游应用程序（例如推荐服务）时，这可能是有害的。例如，硬编码到嵌入中的种族和性别刻板印象可能会导致有偏见的建议（例如，评估是否适合招聘职位或预测财务违约的可能性），并导致不公平和不道德的决定（例如，拒绝工作或信贷） 。学者们应该根据他们的研究问题和设计，主动考虑这种负外部性是否可能，并在对人类造成伤害的可能性足够高时，偶然放弃嵌入。&lt;strong&gt;然而，在某种程度上，理解社区和研究背景中概念关联的本质是核心，研究人员将需要这些偏见进行分析。如果不包括它们，模型以及研究设计就会错过表征其研究背景的关键社会和文化规律。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果分析人员对生成语料库的上下文没有清晰的了解，就会出现另一个相关的限制，这样他们最终可能会做出不适用和不相关的推论&lt;/strong&gt;。例如，强调意义随时间变化的研究（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19&#34;&gt;Caliskan et al. 2017&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49&#34;&gt;Garg et al. 2018&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;Kozlowski et al. 2019&lt;/a&gt;）的特点是词义表现出来自外源冲击的间断变化，从而重新配置了概念关联的结构。穿过空间。想一想 2005 年卡特里娜飓风之后“卡特里娜”的含义发生了怎样的变化。2009 年金融危机之后，金融术语的含义发生了重新配置，部分原因是添加了“问题资产救助计划”等许多新术语。忽略外源冲击可能会导致对后面和验证部分中描述的措施的错误解释，将其视为仅由进化产生的结果，从而导致错误的推论。这是一个特别成问题的问题，因为许多最准确的词嵌入模型都是在从网络上提取的大量文本语料库上进行预训练的。此类模型可用于引导非常小的文本数据之间的有意义距离，这是一项常见任务，但&lt;strong&gt;如果预训练数据是异构的，则距离可能无法反映焦点文本的概念世界&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;接下来的两个限制必然是其嵌入优势的另一面。词嵌入模型产生的细粒度信息会带来特定研究可能或可能无法维持的成本。首先是模型尺寸。&lt;strong&gt;每个单词的数百个维度的细粒度信息或上下文嵌入需要比简单的字典计数或潜在狄利克雷分配主题模型更大的存储空间&lt;/strong&gt;。这与通常用于将数据维度减少到两个或三个的因子和主成分分析形成鲜明对比。词嵌入模型使用更多维度（通常为 200-500）来更准确地预测数据的屏蔽部分。尽管如此，当前个人计算机的计算能力和存储能力现在允许训练合理大小的嵌入。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;与此相关的是，词嵌入模型需要比先前模型更多的文本才能稳健地估计概念空间&lt;/strong&gt;。当大型语料库与研究主题相似并且可以用作理论相关文档或微调过程的初始化的代理时，可以通过迁移学习来弥补这一挑战。&lt;strong&gt;然而，有时相关语言在内容、目的或形式上与模型预训练的数据有很大不同，它需要独立建模，但又足够小，无法维持对嵌入模型的稳健估计。在这种情况下，使用字典计数或主题模型可能会更好，因为数据只能维持粗粒度的关联，而这些方法旨在捕获粗粒度的关联。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;最后一类通常涉及词嵌入和文本方法的特殊限制。首先，静态词嵌入本身并不处理一词多义，即一个词（例如 &lt;em&gt;&lt;strong&gt;“bank”&lt;/strong&gt;&lt;/em&gt; ）编码多个概念（例如金融机构、河边、侧向倾斜）的情况。尽管多义词的存在可能会影响后续一些指标的测量，但也存在抵消的力量。一方面，研究发现多义词的含义以相互线性叠加的方式编码在单词向量内（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B5&#34;&gt;Arora et al. 2018&lt;/a&gt;）。这意味着该算法通过同时考虑单词的所有含义来对单词在概念空间中的位置进行编码，从而克服了原本可能存在的严重缺陷。另一方面，上下文嵌入架构（在线附录中有更详细的描述）通过根据焦点词周围的上下文输出不同的向量来明确解决多义词的问题。每个单词不是单个向量，而是根据用途而变化的向量云。如果分析师怀疑一词多义可能是特定分析的严重问题，他们可以偶然使用上下文嵌入并规避这种担忧。&lt;/p&gt;
&lt;p&gt;最后一个潜在的限制是文本方法的一般特征。只要文本数据是转录语音话语的产物（例如，欧洲央行或美联储主席演讲、政治演讲、财报电话会议、电视或电影文字记录、对话互动），语音的语调、语气和音色将没有纳入到嵌入表示中。考虑到。&lt;strong&gt;鉴于某些语言（例如中文）更严重地依赖语调来传达含义，这可能或多或少存在问题，具体取决于话语发生的社会背景及其表达语言&lt;/strong&gt;。因此，在语调和语气在语料库中发挥重要作用的情况下，学者们应该讨论他们的嵌入模型选择和解释决策的后果。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;410-在研究中使用词嵌入模型的路线图&#34;&gt;4.10 在研究中使用词嵌入模型的路线图&lt;/h3&gt;
&lt;p&gt;现在我们大脑对词嵌入模型是什么、如何表示概念空间、如何训练、优点和局限性有了框架性的认知，接下来可以将它们整合到研究和理论构建的标准方法中。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#T1&#34;&gt;表 1&lt;/a&gt;列出了如何将嵌入模型集成到科学流程中的路线图。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;步骤 1-3 是研究过程中的标准步骤，包括确定一个可行且有趣的研究问题，通过在适当的实证背景下进行评估，为重要的理论问题提供信息（Weick 1989 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B179&#34;&gt;）&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;步骤 4-9 总结了本文到目前为止对嵌入模型的讨论。&lt;/li&gt;
&lt;li&gt;步骤 10 和 11 ，与下一节指标度量有关，通过标准定量和定性方法调动该度量。&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;表 1.在研究中使用词嵌入模型的路线图&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;步骤&lt;/th&gt;
&lt;th&gt;活动&lt;/th&gt;
&lt;th&gt;基本原理&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1. &lt;strong&gt;确定研究问题&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;如果研究问题至关重要，请确定文本数据是否有助于在理论研究上有帮助。&lt;/td&gt;
&lt;td&gt;吸引研究人员把注意力聚焦在理论问题、词嵌入构建研究构念回答问题的交叉点。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2. &lt;strong&gt;理论建立及相关理论构建&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;确定使用哪种理论框架来解决研究问题以及通过嵌入模型来操作哪种理论构念。&lt;/td&gt;
&lt;td&gt;理论构念与其词嵌入指标(构念的衡量）之间的紧密联系能够实现累积的理论发展。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3. &lt;strong&gt;定义经验背景&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;选择适当的实证背景，在其中回答研究问题并动员理论框架和构念。&lt;/td&gt;
&lt;td&gt;确保研究问题、理论框架和用构念以逻辑方式相互加强。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4.&lt;strong&gt;指定将用于表示经验背景的概念空间的文本数据&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;描述将用于训练词嵌入模型和测量感兴趣的理论构念的文本数据的范围。 数据是否有效地涵盖了您想要得出理论结论的经验背景下的行为活动范围？&lt;/td&gt;
&lt;td&gt;确保用于计算理论构造度量的词嵌入模型在逻辑上映射到并有效地代表所提出的理论框架内的实证研究背景。 文本数据的范围应该在逻辑上映射到所讲述的理论故事的范围。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5.&lt;strong&gt;确定文本数据的大小和范围&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;数据是否足够大以学习相关概念空间的准确表示？&lt;/td&gt;
&lt;td&gt;文本数据的大小将决定是否应该训练自定义嵌入，或者是否应该使用可用数据对现成的嵌入进行微调。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6. &lt;strong&gt;给定数据大小，要么训练独特的词嵌入模型，要么微调现有模型&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;如果文本数据足够大，则训练自定义嵌入来表示感兴趣的经验上下文的概念空间。 如果文本数据不够大，请使用这些数据来微调现有的现成嵌入模型。&lt;/td&gt;
&lt;td&gt;确保用于测量理论结构的嵌入模型能够有效地表示经验背景的相关概念空间。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7. &lt;strong&gt;如果训练独特的模型，请选择一种算法&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;在连续词袋 (CBOW) 或 Skip-Gram 模型之间进行选择。&lt;/td&gt;
&lt;td&gt;CBOW：在较小的数据集上可以有更好的性能。 &lt;br&gt;Skip-gram：可以更好地捕获语义。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8. &lt;strong&gt;如果训练独特的模型，确定相关参数&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;选择窗口大小和维数。&lt;/td&gt;
&lt;td&gt;窗口大小：标准做法是 5。较小的窗口可以更大程度地捕获语法，较大的窗口可以更大程度地捕获语义，但收益递减并增加计算成本。 维度数：标准做法是 300，超过此点后性能回报递减。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9. &lt;strong&gt;验证词嵌入模型&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;请遵循在线附录中的验证程序。&lt;/td&gt;
&lt;td&gt;确认嵌入模型准确有效地表示了经验背景的概念空间。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10. &lt;strong&gt;计算相关度量&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;通过确定将用于实施感兴趣的理论构念的相关概念集，创建“实际措施和应用”部分中的措施之一。&lt;/td&gt;
&lt;td&gt;使学者能够将该测量用于定量或定性分析。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11. &lt;strong&gt;在标准定性或定量方法中使用计算的度量&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;对于定量分析，该度量要么成为自变量，要么成为因变量。 对于定性分析，学者可以提供解释性分析，因为它们可能适用于其他类型的档案、民族志或视听数据。&lt;/td&gt;
&lt;td&gt;嵌入模型表示对生成数据的社会背景的概念空间的描述。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五实际措施与应用&#34;&gt;五、实际措施与应用&lt;/h2&gt;
&lt;p&gt;现在已经正式定义了 &lt;strong&gt;概念&lt;/strong&gt; 和 &lt;strong&gt;概念空间&lt;/strong&gt; 的含义，并说明了先前的文献如何处理概念信息,  介绍了嵌入模型表示能力的底层逻辑，并在在线附录中完成了支持这种直觉的几个验证步骤。也评论了嵌入模型给概念信息分析带来的几个优点和相关缺点。&lt;/p&gt;
&lt;p&gt;在本章中，我们将介绍一些新研究， 学习他们如何用嵌入生成独特指标。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#T2&#34;&gt;表 2&lt;/a&gt;总结了这些指标及示例应用。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;表 2.词嵌入测量和示例应用&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;措施&lt;/th&gt;
&lt;th&gt;研究性学习&lt;/th&gt;
&lt;th&gt;关键构念&lt;/th&gt;
&lt;th&gt;研究问题&lt;/th&gt;
&lt;th&gt;代表性调查结果&lt;/th&gt;
&lt;th&gt;嵌入在这种情况下的优点&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1. &lt;strong&gt;概念广度&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105&#34;&gt;利克斯等人。(2022)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;话语多样性——在一组给定的互动中，群体成员所传达的含义彼此分歧的程度。&lt;/td&gt;
&lt;td&gt;一个群体的话语多样性如何影响其绩效？&lt;/td&gt;
&lt;td&gt;高绩效团队会调整他们的共享认知以匹配任务的要求（例如，构思与协调）。&lt;/td&gt;
&lt;td&gt;能够随着时间的推移以细粒度的细节和动态地追踪小组对话的概念广度，使学者们能够追踪话语多样性的新理论构造。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.&lt;strong&gt;概念距离和相似度&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74&#34;&gt;霍夫斯特拉等人。(2020)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;语义遥远的科学新颖性：博士论文中新链接概念的语义距离。&lt;/td&gt;
&lt;td&gt;代表性不足的群体是否更有可能产生科学创新？&lt;/td&gt;
&lt;td&gt;相对于男性，女性引入了更遥远的新奇事物。 然而，这种语义上遥远的新颖性在该学科中很少受到关注。&lt;/td&gt;
&lt;td&gt;能够追踪新概念组合的概念距离，使学者不仅可以研究是否做出了新组合，还可以研究这些组合的语义距离最终如何影响其影响。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3.&lt;strong&gt;概念X性&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94&#34;&gt;劳森等人。(2022)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;性别刻板印象：男性（而非女性）与以成就为导向的代理特征（例如自信和果断）相关的程度。&lt;/td&gt;
&lt;td&gt;雇用女性首席执行官和董事会成员是否与组织对代理语言的性别使用发生变化有关？&lt;/td&gt;
&lt;td&gt;当组织雇用女性首席执行官和董事会成员时，女性的语义与代理的语义变得更加一致。&lt;/td&gt;
&lt;td&gt;对 22 家标准普尔 500 强公司的 43,000 多份文件（包含超过 12 亿字）进行分析，深入细致地研究女性的含义如何因聘用女性领导者而发生变化。否则这样的分析是不可能的。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;4.概念意义&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63&#34;&gt;汉密尔顿等人。(2016)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;词语的文化意义：词语的含义随时间变化的程度。&lt;/td&gt;
&lt;td&gt;语义演化的可能驱动因素是什么？&lt;/td&gt;
&lt;td&gt;跨历史时期的语义变化率与词频的逆幂律成正比。 与频率无关，具有更多含义的单词具有更高的语义变化率。&lt;/td&gt;
&lt;td&gt;能够探索跨多个知识和文化领域的大型历史时期和大量文本中的语义变化。例如，他们可以详细追踪同性恋这个词的含义如何从&lt;em&gt;快乐&lt;/em&gt;和&lt;em&gt;艳丽&lt;/em&gt;等概念转向&lt;em&gt;同性恋&lt;/em&gt;和&lt;em&gt;女同性恋&lt;/em&gt;等概念。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5. &lt;strong&gt;文化和知识连续体中的概念立场&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;科兹洛夫斯基等人。(2019)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;社会阶层标记：区分社会阶层维度的概念。&lt;/td&gt;
&lt;td&gt;20世纪社会阶级的标志是如何变化的？&lt;/td&gt;
&lt;td&gt;尽管社会阶级维度在历史上保持稳定，但阶级文化标记在每个维度中的定位方式却不断发生变化（例如，员工从士兵和肌肉等概念转变&lt;em&gt;为&lt;/em&gt;白领&lt;em&gt;和&lt;/em&gt;中产阶级&lt;em&gt;等&lt;/em&gt;概念*）*。&lt;/td&gt;
&lt;td&gt;能够将文化相关的概念投射到文化相关的兴趣连续体上，从而使研究人员不仅可以在单个历史时期内而且可以在其历史演变过程中了解广泛共享的社会关联。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6. &lt;strong&gt;概念维度&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;科兹洛夫斯基等人。(2019)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;阶级的文化维度：理解社会阶级的维度（富裕、教育、修养、地位、就业、道德、性别）&lt;/td&gt;
&lt;td&gt;20 世纪文化阶层的规模有多稳定？&lt;/td&gt;
&lt;td&gt;20世纪，尽管发生了巨大的经济转型，阶级规模仍然非常稳定。&lt;/td&gt;
&lt;td&gt;能够对阶级的多个概念维度进行实证分析，从而理解 20 世纪美国它们之间的相互关系。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h3 id=&#34;51-概念广度&#34;&gt;5.1 概念广度&lt;/h3&gt;
&lt;h4 id=&#34;511-指标&#34;&gt;5.1.1 指标&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;可以测量文档中单词之间的距离来计算它们在概念空间中的分布范围&lt;/strong&gt;。文档可以是从专利到个人电子邮件通信的任何内容。我们可以测量每个单词与其他单词的平均距离有多远。&lt;strong&gt;获取文档内元素的平均距离（或每个单词与文档质心之间的距离）可以衡量该文档内的「概念宽度」&lt;/strong&gt;。例如，我们衡量每项专利的概念广度， 可以从两个简单的文档开始，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;doc1 =  [&amp;#34;biochemistry&amp;#34;, &amp;#34;chemistry&amp;#34;, &amp;#34;analytical_chemistry&amp;#34;]
doc2 =  [&amp;#34;chemistry&amp;#34;, &amp;#34;oceanography&amp;#34;, &amp;#34;computer&amp;#34;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用我们的专利嵌入模型，我们得到第一组(doc1)的平均宽度为 29，第二组（doc2）平均宽度为 47。这表明第二组在概念上比第一组更广泛。&lt;/p&gt;
&lt;p&gt;当我们衡量文档集合而不是单词的概念广度时，同样的逻辑也适用。例如，我们想了解发明者团队的广度。在这种情况下，我们可以将团队中的每个发明人视为嵌入概念空间中的“文档”，参考如图&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F1&#34;&gt;1&lt;/a&gt; , 从下往上，依次是词概念空间、发明人概念空间、团队概念空间、组织概念空间。一个发明人团队的成员已经在涉及纳米技术、生物技术和软件的概念空间领域发表了先前的专利，那么在概念上将被认为比所有成员只发表了纳米技术专利的团队更广泛。即使所有发明人都将其公开的专利限制在一个类别内，该指标仍然会提供显着的变化。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/figure-1.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;512--应用&#34;&gt;5.1.2  应用&lt;/h4&gt;
&lt;p&gt;这种概念广度的度量已在最近的工作中用于追踪各种理论构念。&lt;strong&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105&#34;&gt;利克斯等人。(2022)&lt;/a&gt;衡量团队成员在参与软件项目的不同阶段时的 话语广度&lt;/strong&gt;。&lt;strong&gt;他们能够追踪每个独特项目阶段概念参与的多样性，发现表现最好的团队有能力改变他们的认知以适应手头不断变化的任务，在提出新想法时表现出更大的话语广度，而在转换时表现出较低的广度依赖于协调的任务。这种细粒度的知识参与概念很难用以前的文本分析方法来追踪&lt;/strong&gt;。 详细内容可阅读大邓近期推文 &lt;a href=&#34;https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/&#34;&gt;MS2022 | 使用语言差异性测量团队认知差异性&lt;/a&gt; 。&lt;/p&gt;
&lt;p&gt;另外，研究人员使用概念广度来追踪在线社区成员根据状态变化分配注意力的范围，发现状态和注意力广度之间存在 U 形关系（Aceves et al. 2022 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2&#34;&gt;）&lt;/a&gt;。这些研究人员训练了 150 个知识领域的概念空间，从而能够追踪不同知识领域的相似注意力动态，从计算机编程和数学到育儿和园艺。由于他们有能力在数百个社区的文本中大规模部署算法，因此他们能够计算出超过 2000 万成员如何在这些问答社区上发布的 2300 万个问题中分配注意力。&lt;/p&gt;
&lt;p&gt;其他工作在整个语言中实施了这种方法，追踪语言在所有知识领域具有更宽或更窄的概念空间的程度（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B1&#34;&gt;Aceves 和 Evans 2021&lt;/a&gt;）。使用圣经、电影字幕和以多种语言编写的政治文件等文本的并行翻译（包含相同的信息但以不同的语言编码），他们能够追踪概念在不同语言中相互关联的程度存在显着差异。他们发现，尽管一些语言将不同的概念子空间紧密地联系在一起，并将不同的概念领域编织在一起，但其他语言却稀疏且更加支离破碎，更强烈地分隔了不同的意义域。然后，他们观察概念空间的语言密度如何塑造数百种语言的真实对话和维基百科文章的概念广度。&lt;/p&gt;
&lt;p&gt;所有三篇论文都为不同文献的研究开辟了新的理论途径，例证了该方法的潜力。如果没有概念空间的概念及其通过嵌入模型的表示，这些新的研究途径将很难实施。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;52-概念距离和相似度&#34;&gt;5.2 概念距离和相似度&lt;/h3&gt;
&lt;h4 id=&#34;521-指标&#34;&gt;5.2.1 指标&lt;/h4&gt;
&lt;p&gt;当我们的分析重点在于集合内的元素时，前面描述的概念广度构念是相关的。当我们的分析重点是不同集合之间的关系时，可以使用相同的基础度量。在这种情况下，我们将指的是概念距离或相似性，而不是概念广度。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn14&#34;&gt;13&lt;/a&gt;形式上，如果我们有至少两个集合，每个集合中至少有一个元素，我们可以计算这些集合之间的&lt;strong&gt;概念距离，作为每个集合的质心或多维平均值之间的距离&lt;/strong&gt;。最基本的是，我们可以计算两个集合之间的概念距离，每个集合包含一个单词。这无非是衡量这些词之间的概念距离。随着元素数量和集合数量的增加，底层计算保持不变，但理论可能性的范围扩大。还可以通过训练文档嵌入模型来计算这种距离/相似性度量，该模型在嵌入空间中为每个文档分配一个向量，其权重按照单词共现的相同逻辑进行训练（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B96&#34;&gt;Le 和 Mikolov 2014&lt;/a&gt;），将文档本身视为文档中的另一个单词，将这些单词用作与其共现的单词。&lt;/p&gt;
&lt;p&gt;通过将概念相似性与衡量专利相似性的现有技术进行比较，我们可以一睹该衡量标准的潜力。首先，研究人员可以通过查看专利授予机构使用的官方分类来追踪专利的相似性，同一类别的专利被认为比不同类别的专利更相似（Singh 和 Marx 2013，Aharonson 和&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B157&#34;&gt;Schilling&lt;/a&gt; 2016 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B3&#34;&gt;）&lt;/a&gt;）。这种方法的局限性在于分类度量是粗粒度的，并且不太可能考虑所有相关的技术特征，特别是当类别边界必然滞后于技术进化时（Thompson 和 Melanie Fox-Kean 2005，Singh&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B172&#34;&gt;和&lt;/a&gt;Agrawal &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B155&#34;&gt;2011&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B7&#34;&gt;Arts 等人，2018&lt;/a&gt;）。其次，研究人员可以获取两项专利并测量它们之间的单词重叠（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B7&#34;&gt;Arts et al. 2018&lt;/a&gt;）。然而，这种方法是有限的，因为它仅适用于成对的文档，无法确定专利相对于整个知识体系的位置。&lt;/p&gt;
&lt;p&gt;概念相似性解决了这些限制（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184&#34;&gt;Whalen 等人，2020&lt;/a&gt;）。首先，它允许我们追踪专利在相关知识空间中的精确位置，从而访问知识系统中的所有相关的细粒度信息。其次，我们能够精确量化任何专利或专利组相对于任何其他专利或专利组的位置。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn15&#34;&gt;14&lt;/a&gt;第三，随着新知识进入系统，知识的性质和结构不断演变，随着时间的推移重塑 &lt;strong&gt;概念边界&lt;/strong&gt; 和关联。&lt;strong&gt;嵌入使我们能够衡量专利发布时存在的概念空间内的专利相似性，使我们能够摆脱使用滞后的、周期性偏离的类别，并可能对连续的发明概念空间强加类别差异&lt;/strong&gt;。概念距离的所有这些优点都适用于其他知识和文化领域，在这些领域中，我们寻求测量思想、个人、群体或组织之间的距离或相似性，从而扩展现有的跨研究领域并开辟新的理论领域。&lt;/p&gt;
&lt;h4 id=&#34;522-应用&#34;&gt;5.2.2 应用&lt;/h4&gt;
&lt;p&gt;正如我们上面所做的那样，这种&lt;strong&gt;概念相似性的衡量方法最近被用来描述专利数据中的创新空间&lt;/strong&gt;（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184&#34;&gt;Whalen 等人，2020&lt;/a&gt;）。研究人员使用  &lt;strong&gt;doc2vec&lt;/strong&gt;  框架计算了超过 6 亿个专利对的相似度。在生成这些知识相似性度量时，作者还使用这些分数提出了有趣的辅助度量，包括可操作的度量（a）现有技术接近度——专利引用与其自身相似或不相似的现有技术的程度，（b）现有技术同质性——一项专利引用知识空间领域彼此远离的程度，(c) 影响邻近性——一项专利被与其自身相似或不相似的未来专利引用的程度，以及(d) 影响同质性——一项专利通过其前向引用与一组不同的未来专利相关的程度。&lt;/p&gt;
&lt;p&gt;学者们也使用了这一衡量标准，重点关注概念距离。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18&#34;&gt;伯奇等人。(2021)&lt;/a&gt;使用概念距离的 &lt;strong&gt;doc2vec&lt;/strong&gt; 实现来调查同行奖励是否会影响在线社区内贡献的新颖性。这里的&lt;strong&gt;新颖性是根据社区成员获奖前后贡献的距离来衡量的&lt;/strong&gt;。作者发现，获奖后，奖项会导致知识空间内的新颖性减少，剥削行为增多。同样，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74&#34;&gt;霍夫斯特拉等人。（2020）&lt;/a&gt;使用 Word2Vec 距离度量来捕获科学论文将新颖性引入科学文献的程度，发现来自代表性不足群体的学生负责将最具新颖性引入系统。&lt;/p&gt;
&lt;p&gt;其他人则利用这一措施来实施公司差异化。在发展中国家微型企业的背景下，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20&#34;&gt;Carlson（2022）&lt;/a&gt;使用 BERT 架构（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35&#34;&gt;Devlin et al. 2019&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B142&#34;&gt;Reimers and Gurevych 2020&lt;/a&gt;）来计算其数据集中所有微型企业的成对余弦距离。通过这些距离，他们能够估计八个发展中国家的 10,000 家微型企业的差异化与收入和利润的增加相关。同样，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61&#34;&gt;Guzman 和 Li（2023）&lt;/a&gt;使用距离的 doc2vec 实现来使用 Crunchbase 数据来衡量初创公司的创始战略差异化。作者发现与&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20&#34;&gt;Carlson (2022)&lt;/a&gt;类似的结果，即差异化经验的新公司在早期融资和股权结果方面有所增加。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;53-概念x&#34;&gt;5.3 概念X&lt;/h3&gt;
&lt;h4 id=&#34;531-指标&#34;&gt;5.3.1 指标&lt;/h4&gt;
&lt;p&gt;文档距离的另一个用途是追踪语料库中的任何文档与捕获感兴趣的构念X的焦点(原型）的相似性， 这样的测量将捕获任何观察的 &lt;strong&gt;概念X性&lt;/strong&gt;( Conceptual X-ness)。这种测量的第一步是描述与我们寻求尽可能精确测量的结构相关的概念信息。例如，如果我们想要捕获专利与 &lt;strong&gt;时间&lt;/strong&gt; 或 &lt;strong&gt;几何&lt;/strong&gt; 等概念相关的程度，我们可以构建一个我们认为映射到、定义或与这些概念相关的单词列表 。对于每个列表，我们计算其质心向量 (c#27)，然后测量任何给定专利距离 &lt;strong&gt;时间&lt;/strong&gt; 和 &lt;strong&gt;几何&lt;/strong&gt; 概念有多远。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn16&#34;&gt;15&lt;/a&gt; 对于附录表 A2 中使用的专利，我们可以看到，与头颈约束装置专利相关的前两项专利更接近时间概念，正如所预期的那样光和时间在概念上交织的程度。概念性的&lt;em&gt;X&lt;/em&gt;度度量可用于追踪思想、个人、团体、组织或任何其他相关聚集的组成。&lt;/p&gt;
&lt;h4 id=&#34;532-应用&#34;&gt;5.3.2 应用&lt;/h4&gt;
&lt;p&gt;最近在一篇论文中使用了这种方法，该论文追踪了雇用女性担任高级领导角色对女性在这些组织中意味着什么的影响（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94&#34;&gt;Lawson 等人，2022&lt;/a&gt;）。作者首先使用 SEC 文件和财报电话会议记录训练了 Word2Vec 嵌入。然后，他们创建并验证了一组 100 个单词来捕捉 &lt;strong&gt;代理概念&lt;/strong&gt; 的含义（例如，有能力、独立、主导），并观察了内部任命高级女性领导前后 &lt;strong&gt;代理概念&lt;/strong&gt; 与 &lt;strong&gt;女性&lt;/strong&gt; 概念之间的距离。该组织发现，在 &lt;strong&gt;女性&lt;/strong&gt; 被任命为高层管理人员之后的一段时间内，女性的含义在概念空间中更加接近于机构职位。作者使用不同的嵌入超参数和维度大小复制了他们的结果，说明了嵌入模型的鲁棒性，条件是具有捕获概念空间内语义变化的最小必要维度。&lt;/p&gt;
&lt;p&gt;这里有趣的理论机会包括更深入地参与理论传统的可能性，这些理论传统在组织科学以外的领域具有影响力，但由于缺乏可行的方法来以原则性的方式量化其理论构造，因此这些理论传统仍然处于我们的领域之外。依赖文学解释。正如我们所提出的，测量 &lt;strong&gt;概念X性&lt;/strong&gt; 使得扩大与理想形式（* &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B134&#34;&gt;Plato Bloom 1968&lt;/a&gt;）、理想类型（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B178&#34;&gt;Weber 2011&lt;/a&gt;）、家族相似性（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B186&#34;&gt;Wittgenstein 2010&lt;/a&gt;）和原型（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B147&#34;&gt;Rosch 1973&lt;/a&gt;）相关的理论构造的测量成为可能。以一致、有原则和可复制的方式。在这方面，概念性的&lt;em&gt;X&lt;/em&gt;性代表着开放大量的认知和社会理论，以便在组织的背景下进行实证检验和扩展。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;54-语义转变和漂移&#34;&gt;5.4 语义转变和漂移&lt;/h3&gt;
&lt;h4 id=&#34;541-指标&#34;&gt;5.4.1 指标&lt;/h4&gt;
&lt;p&gt;概念空间使我们能够识别术语的含义如何随着时间和空间的变化而变化。探索概念意义的一种方法是为不同的个人、公司、行业、地理位置或时间段创建独特的嵌入模型，以了解它们之间的含义有何不同（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B148&#34;&gt;Roy 等人，2019 年&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B181&#34;&gt;Welch 等人，2020a&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B182&#34;&gt;b&lt;/a&gt;）。一旦识别出相关的兴趣分歧，我们就可以采用相关的语料库（例如，专利、财报电话会议、报纸）并为数据中的每个语料库训练概念空间。&lt;strong&gt;在我们的专利示例中，我们可能会训练两种嵌入模型，一种是 1990 年功能性磁共振成像技术发明之前的时期，另一种是 1990 年之后的时期&lt;/strong&gt;。然后我们可以探索与大脑和神经科学相关的概念的含义如何随着这一创新而改变。例如，在功能性磁共振成像发明之前和之后与不同大脑区域最相关的术语是什么。接下来，我们可以比较不同公司或国家的含义变化有何不同，以及这种变化的格局如何影响所涉及的公司和行业的组织和市场结果。显式动态词嵌入允许嵌入之间具有更大的可比性，但必然会忽略特殊的词和用途（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63&#34;&gt;Hamilton et al. 2016&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B192&#34;&gt;Zhang et al. 2016&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B190&#34;&gt;Yao et al. 2018&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B103&#34;&gt;Liu et al. 2020&lt;/a&gt;）。这些算法的输出带有时间戳词向量包含特定时期的语义信息，但在历史上保持可比性。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;54-2-应用&#34;&gt;5.4. 2 应用&lt;/h4&gt;
&lt;p&gt;第一篇在社会科学背景下使用词嵌入方法的主要论文就是使用这种方法来研究意义（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63&#34;&gt;Hamilton et al. 2016&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19&#34;&gt;Caliskan et al. 2017&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49&#34;&gt;Garg et al. 2018&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;Kozlowski et al. 2019&lt;/a&gt;）。在第一篇论文中，研究人员使用四种语言的六个历史语料库，通过观察概念空间中最近的单词在过去几十年中如何变化来追踪单词含义随时间的变化（Hamilton et al. 2016 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63&#34;&gt;）&lt;/a&gt;。使用 Word2Vec 嵌入，他们追踪了 &lt;strong&gt;同性恋&lt;/strong&gt;  概念的含义如何从 1900 年代围绕 &lt;strong&gt;“愚蠢”&lt;/strong&gt;、**“甜蜜” **和 **“开朗”  **等术语的含义转变为围绕 1950 年代 &lt;strong&gt;“嬉闹”&lt;/strong&gt;、 &lt;strong&gt;“机智”&lt;/strong&gt; 和 &lt;strong&gt;“聪明”&lt;/strong&gt; 等术语的含义，并且最终以 20 世纪 90 年代女同性恋、双性恋和同性恋等术语的含义结束。在另一篇论文中，研究人员研究了词嵌入中的刻板关联之间的关系及其与当代社会经验数据的关系（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19&#34;&gt;Caliskan et al. 2017&lt;/a&gt;）。例如，他们追踪了职业的性别刻板印象，发现职业具有女性意义，因为它们与女性参与劳动力市场相关。在另一项研究中，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49&#34;&gt;Garg 等人。(2018)使用预先训练的 Google News Word2Vec 模型（ &lt;/a&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B4&#34;&gt;Google 2013&lt;/a&gt; ）量化了美国 100 多年历史中的性别和种族刻板印象，阐明了不同的形容词和职业如何或多或少地与不同人群（例如，男性与女性）密切相关，白人与亚洲人与西班牙裔）随着时间的推移。&lt;/p&gt;
&lt;p&gt;最近通过词嵌入追踪含义的工作已经使用这种方法更深入地研究了特定的上下文。一项研究使用 19 世纪第一人称叙述的语料库来追踪黑人和白人男性和女性的交叉身份如何映射到五个社会机构，包括政治、经济、文化、家庭领域和权威关系（Nelson 2021 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B120&#34;&gt;）&lt;/a&gt;。&lt;strong&gt;举论文中的一个例子，作者测量了与“精致”概念的距离，发现它与白人女性的联系最密切，而与黑人男性的联系最少&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在其他工作中，研究人员利用这种方法来衡量政治领导人的 &lt;strong&gt;集体意向性&lt;/strong&gt; （人们参与集体推理和行动的能力），并比较共和党和民主党领导人如何以不同的方式动员集体意向性（Kirgil and Voyer 2022 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B85&#34;&gt;）&lt;/a&gt;。他们通过创建复数代词（我们，我们的）、复数常量（国家名称）和复数名词（人）的复合列表来测量集体意向性。然后，使用词嵌入模型，他们找到了各州集体意向向量最接近的术语，使他们能够比较不同领导人如何不同地动员集体意向。总的来说，这些意义研究表明，就语言为我们提供了解文化的窗口而言（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B55&#34;&gt;Goldberg et al. 2016&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165&#34;&gt;Srivastava et al. 2018&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26&#34;&gt;Corritore et al. 2020&lt;/a&gt;），嵌入模型为我们提供了一种独特的表达方式透过那扇窗户看到的照片。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;55-文化和知识连续性中的概念地位&#34;&gt;5.5 文化和知识连续性中的概念地位&lt;/h3&gt;
&lt;h4 id=&#34;551-指标&#34;&gt;5.5.1 指标&lt;/h4&gt;
&lt;p&gt;另一种新颖的测量方法可以通过追踪概念相对于感兴趣的概念维度的位置来创建。如前所述，嵌入模型可用于解决类比推理任务，例如**“国王”-“男人”+“女人”=“女王”&lt;strong&gt;。 该架构可用于定义概念空间内任何感兴趣的维度。&lt;strong&gt;在国王-王后的例子中，性别维度通过“男人”-“女人”和“国王”-“女王”向量进行操作。&lt;/strong&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;科兹洛夫斯基等人。（2019）&lt;/a&gt;详细介绍了如何在概念空间内构建此类维度。首先，研究人员需要确定感兴趣的维度。对于我们这里的例子，我们将把不同的概念投射到男性-女性性别维度上。为此&lt;/strong&gt;，我们首先确定定义性别维度的相关术语**。这里我们使用集合 [&amp;lsquo;man&amp;rsquo;, &amp;lsquo;him&amp;rsquo;, &amp;lsquo;he&amp;rsquo;, &amp;lsquo;male&amp;rsquo;, &amp;lsquo;men&amp;rsquo;] 和 [&amp;lsquo;woman&amp;rsquo;, &amp;lsquo;her&amp;rsquo;, &amp;lsquo;she&amp;rsquo;, &amp;lsquo;female&amp;rsquo;, &amp;lsquo;women&amp;rsquo;]。 &lt;strong&gt;然后我们计算不同概念在这个男性-女性概念轴(维度)上的正交投影。&lt;/strong&gt; 在线附录中的图 A4 将每个概念投射到 &lt;strong&gt;男性-女性概念轴&lt;/strong&gt;。 更消极的预测表明与女性气质的关联更强，而更积极的预测表明与男性气质的相关性相当。如图 A4 所示，这些预测与关于这些概念的性别状态的一般直觉一致，使我们能够明确说明每个概念相对于其他概念在这个维度中的位置。正如预期的那样，&lt;strong&gt;军事&lt;/strong&gt; 和 &lt;strong&gt;农业&lt;/strong&gt; 与 &lt;strong&gt;男性气质&lt;/strong&gt; 的联系最为密切，而 &lt;strong&gt;卫生棉条&lt;/strong&gt; 和 &lt;strong&gt;口红则&lt;/strong&gt; 与 女性气质的联系最为密切。按照这个程序，学者们现在可以测量任何概念在任何感兴趣的维度和任何文本丰富的时空背景中的位置。此外，不同语言的语料库可以独立训练和对齐，或者同时训练和对齐，以方便国际分析（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B81&#34;&gt;Johnson et al. 2017&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116&#34;&gt;Milbauer et al. 2021&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;双极概念维度的投影方法可以进一步扩展到锚定具有多种含义的低维子空间，其中单词和概念可以被绘制并理解为这些含义的混合&lt;/strong&gt;。这可以通过理论上选择“原型”的集合来执行，即具有已知且广泛共享含义的极值点，并在这些极值锚定义的子空间中绘制所有相关单词或概念。[例如，在对一个新的基于信息技术的创业企业进行分类时，人们可能会问它在 Uber、亚马逊、谷歌或比特币所刻画的空间中适合什么位置(Breiman 1994，Eugster 2012，Damle 和 Sun 2017）。&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;552-应用&#34;&gt;5.5.2 应用&lt;/h4&gt;
&lt;p&gt;这项措施的制定和运用是为了研究 20 世纪和 21 世纪社会阶层的演变（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;Kozlowski 等人，2019&lt;/a&gt;）。他们研究了根据 20 世纪出版的数百万本书的文本训练的嵌入，按照上述程序操作了阶级的维度，试图了解社会阶级的底层维度在 20 世纪是如何变化的。为此，他们提出了以下理论上的&lt;strong&gt;概念轴(维度)&lt;/strong&gt;：富裕程度（富人与穷人）、教育程度（受过教育与未受教育）、修养（有教养与未受教育）、地位（有声望与无声望）、道德（善与恶）、就业（雇主-雇员）和性别（男人-女人），分别嵌入 20 世纪的每个十年。然后，他们可以在这些维度上投射不同类别的概念，例如音乐风格、体育和职业，以了解这些概念在本世纪的过程中如何演变和发展。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B6&#34;&gt;研究人员应用这种方法来研究健康、道德（ Arseniev-Koehler et al. 2022&lt;/a&gt;）、政治意识形态（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B171&#34;&gt;Taylor and Stoltz 2021&lt;/a&gt;）和地位（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B129&#34;&gt;Peng et al. 2021&lt;/a&gt; ）等背景下的其他类型的文化关联。&lt;/p&gt;
&lt;p&gt;研究人员不仅将概念投射到这些概念轴(维度)上，而且将整个文档投射到这些维度上，从而推动了测量的可能性（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B170&#34;&gt;Taylor 和 Stoltz 2020&lt;/a&gt;）。此外，尽管以前的措施依赖于研究人员指定感兴趣的连续体的相关维度，但最近的工作已经转向自动识别这些连续体（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116&#34;&gt;Milbauer et al. 2021&lt;/a&gt;）。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116&#34;&gt;Milbauer 等人&lt;/a&gt;利用 Reddit 社区的内容。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116&#34;&gt;（2021）&lt;/a&gt;创建了一个无监督的程序来识别社区中的多个意识形态极点，使他们能够超越静态的左右意识形态维度，发现现代话语中发挥作用的许多两极分化和意识形态差异的轴。人们可以想象在许多组织环境中使用这种方法来识别团队、小组、单位或部门之间存在的许多潜在冲突来源。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;56-概念维度&#34;&gt;5.6 概念维度&lt;/h3&gt;
&lt;p&gt;之前，我们讨论了研究人员如何调查关键术语在相关文化维度上的位置，描述概念的位置在性别维度上的差异。然而，这并不是概念轴(维度)的唯一用途，因为概念空间还允许我们测量和理解相关维度本身如何相互关联。该措施的扩展是使用空间内的编码维度并将它们相互比较。例如，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89&#34;&gt;科兹洛夫斯基等人。（2019）&lt;/a&gt;利用他们既定的阶级维度来追踪整个 20 世纪每个维度如何与其他维度相关，例如，表明随着世纪的发展，富裕与教育的关系变得更加密切，而与教育的关系无关。栽培。通过这种方式，组织学者可以理解相关维度之间的关系在相关概念空间中可能有何不同。例如，学者可以研究不同文化维度在组织或行业内部和之间紧密或松散联系的程度。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;六讨论&#34;&gt;六、讨论&lt;/h2&gt;
&lt;p&gt;最后，我们简要讨论了一些利用嵌入模型进行思考的新兴方法，然后讨论了我们认为理论、方法论和组织的有价值的机会，这些机会源于将这些模型理解为概念空间的细粒度表示。这个讨论必然是说明性的，但暗示了现在这些精致的意义模型的可操作性的广泛可能性。&lt;/p&gt;
&lt;h3 id=&#34;61-词嵌入方法的富有成果的扩展&#34;&gt;6.1 词嵌入方法的富有成果的扩展&lt;/h3&gt;
&lt;p&gt;词嵌入的底层计算架构最近经历了扩展，可以在与之前讨论的不同方向上动员组织研究。我们简要提到三个，并在在线附录中提供更详细的描述。首先，概念和语言的层次结构在“直线”、欧几里得几何中很难得到体现，需要许多难以理解的维度来用标准嵌入来捕获。然而，层次结构可以用负弯曲双曲嵌入来原生表示（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B90&#34;&gt;Krioukov et al. 2010&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B126&#34;&gt;Papadopoulos et al. 2012&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B22&#34;&gt;Chamberlain et al. 2017&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B121&#34;&gt;Nickel and Kiela 2017&lt;/a&gt;），为探索复杂现代的交叉层次结构提供了新的测量可能性。组织。例如，将公司名称嵌入双曲空间中将能够直接发现典型的“中心公司”，并在商业新闻语料库中与所有其他公司进行比较。额外的双曲维度将揭示子层次结构，反映商业评论员所持有的概念和比较价值的不同维度。&lt;/p&gt;
&lt;p&gt;其次，模型语言的深度学习方法为词嵌入增加了关键的上下文敏感性。考虑像 BERT ( &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35&#34;&gt;Devlin et al. 2019&lt;/a&gt; ) 和 GPT 系列模型 ( &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B140&#34;&gt;Radford et al. 2019&lt;/a&gt; ) 这样的大规模模型，它们使用“注意力”的神经网络机制来识别影响焦点词含义的上下文词 ( &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B176&#34;&gt;Vaswani ) et al. 2017&lt;/a&gt;），组装成一个称为转换器的架构，可以将问题转换为答案，将文本转换为翻译，将请求转换为响应。这种模型产生的内容可以被描述为上下文嵌入，这样每个单词不是由单个向量表示，而是由向量云表示，每个向量代表不同上下文中的该单词。“google”上下文中的“Apple”与“orange”上下文中的“apple”具有不同的值。这些模型极大地提高了预测能力，并进一步扩展了我们对概念空间进行精确建模的能力，但代价是复杂性和计算量更大。&lt;/p&gt;
&lt;p&gt;最后，嵌入架构可以扩展到在序列或更高维上下文中排列的任意符号集。例如，图像已被用来衡量抽象艺术图像的新颖性和创造力（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B10&#34;&gt;Banerjee and Ingram 2022&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B11&#34;&gt;Banerjee and Kaplan 2022&lt;/a&gt;），分析警察预约照片（大头照），并识别与司法拒绝保释相关的先前未概念化的紧急特征听证会（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B107&#34;&gt;Ludwig 和 Mullainathan 2022&lt;/a&gt;）。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B101&#34;&gt;音乐（ Liang et al. 2020&lt;/a&gt;）、音频剪辑（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B70&#34;&gt;Hershey et al. 2017&lt;/a&gt;、&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B189&#34;&gt;Xie and Virtanen 2019&lt;/a&gt;）和视频（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B191&#34;&gt;Zellers et al. 2021&lt;/a&gt; ）的多维空间是使用audio2vec（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B168&#34;&gt;Taglisacchi et al. 2020&lt;/a&gt;）等工具构建的。 、signal2vec（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B118&#34;&gt;Nalmpantis 和 Vrakas 2019&lt;/a&gt;）和 video2vec（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B62&#34;&gt;Habibian 等人 2017&lt;/a&gt;），为组织学者接触代表组织生活视听体验的新型媒体打开了大门。&lt;/p&gt;
&lt;p&gt;最近对双曲线、上下文、图像和音频嵌入的扩展表明，嵌入模型的底层计算框架的持续改进和扩展将继续下去，为组织科学中持续的实证、测量和理论创新奠定了基础。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;62-词嵌入和组织理论&#34;&gt;6.2 词嵌入和组织理论&lt;/h3&gt;
&lt;p&gt;在理论层面上，将嵌入模型理解为概念空间的有原则的、细粒度的表示有可能刺激新的理论发展并完善现有理论。例如，意义研究中的经典陈述影响了文学理论和文化社会学等其他领域，但未能在组织科学中站稳脚跟。自20 世纪初德索绪尔 ( &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B32&#34;&gt;de Saussure 1986&lt;/a&gt; )的著作带来语言学的结构转向以来，许多人都试图将意义在组织和社会生活中的作用理论化。列维-斯特劳斯汇集了来自全球各地的多样而广泛的民族志，以向世界文化所特有的表面混乱提出深层的文化秩序，并认为复杂的意义是从有意义的元素的结合中产生的（列维-斯特劳斯 2016 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B99&#34;&gt;）&lt;/a&gt;。福柯理论化了话语和权力如何紧密相连，权力和知识如何以自我强化的联盟结合在一起（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B46&#34;&gt;Foucault 2012&lt;/a&gt;）。&lt;em&gt;布迪厄将惯习&lt;/em&gt;的概念阐述为“持久的、可互换的处置系统，倾向于充当结构结构的结构化结构，即作为实践的生成和结构的原则”（Bourdieu 1977，第72页&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B16&#34;&gt;）&lt;/a&gt;。尽管这些理论很有吸引力，但迄今为止它们只能进行松散且间接的测试。如果没有可靠的实证立足点，他们就永远无法在管理和组织理论中取得突出地位。然而，概念空间的实证操作化现在使得这些文化理论基础著作的参与和扩展变得容易处理，其中的许多结构现在可以辩护地测量。嵌入模型将使这些理论与管理和组织理论相关。&lt;/p&gt;
&lt;p&gt;我们还希望嵌入模型能够对现有理论框架进行更深入的研究和锐化。一组能够受益的文献是那些与知识相关的文献。鉴于组织学者可以获得的大部分知识都被编码在语言的符号概念系统中，现在可以通过更多可用的文本数据源来获取知识，并且可以通过嵌入模型的概念空间来表示。材料科学领域的最新工作已经使用此类模型来有效预测未来的知识发现，比科学家提出的知识发现早几十年（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B173&#34;&gt;Tshitoyan 等人，2019 年&lt;/a&gt;；&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B161&#34;&gt;Sourati 和 Evans，2021 年&lt;/a&gt;）。其他工作表明，这些发现可以推广到生物和物理科学（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B154&#34;&gt;Shi 和 Evans 2023&lt;/a&gt;）。概念空间的明确表示可以对整个社会系统中知识的特征和结构进行详细的调查。一方面，这些模型就像望远镜一样，打开了知识的天空，使其大规模结构变得可见，以供研究、理论发展和完善。另一方面，这些模型充当显微镜，使我们能够更深入地观察构成更大知识系统的意义原子结构。测量方面的这一进步将丰富对定义人类和组织经验的大型多维知识系统中的机制的测试。它还将使我们能够递归地评估管理和组织奖学金的知识，从而刺激创新。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;63-词嵌入和实证研究&#34;&gt;6.3 词嵌入和实证研究&lt;/h3&gt;
&lt;p&gt;在&lt;em&gt;实证层面&lt;/em&gt;，词嵌入模型可以提高组织科学不同领域的测量保真度，从而在实证结果与理论主张和框架之间实现更好的映射。我们用团队和群体内部多样性研究的例子来说明这一点。据说，不同群体所获得的许多好处是由于群体中的个人代表问题和解决方案的方式不同而产生的（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B75&#34;&gt;Hong 和 Page 2004&lt;/a&gt;）。由具有不同方法的个人组成的小组将更好地执行各种任务，因为他们将拥有更广泛的知识、观点和可供借鉴的信息资源（Cox et al. 1991，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B27&#34;&gt;Williams&lt;/a&gt; and &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B185&#34;&gt;O&amp;rsquo;Reilly 1998&lt;/a&gt;）。然而，由于测量困难，对团队多样性的研究很少测量问题和解决方案空间的不同概念。相反，它假设解决问题的团队成员的身份多样性（人口、文化、种族或经验）与其功能多样性（团队成员如何代表和解决问题）之间存在联系（Nisbett 和 Ross 1980，Hong&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B122&#34;&gt;和&lt;/a&gt;Page &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B75&#34;&gt;2004&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175&#34;&gt;van Dijk 等人，2017&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;由于缺乏高保真方法来访问团队成员在问题和解决方案的概念空间中的位置，因此通常假定身份和功能多样性之间存在联系。用于操作研究的身份多样性和用于理论化的功能多样性之间脱节的一个重要后果是，虽然理论积极使用功能多样性的思想和术语（从根本上讲是几何和高维的），但测试依赖于集合-与身份成员资格相关的理论概念。&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175&#34;&gt;我们预测，这将解释团队多样性文献（ van Dijk et al. 2017&lt;/a&gt; ）结果中的大部分歧义，因为研究设计忽视了功能多样性和身份多样性之间的同源性。然而，诸如概念广度之类的衡量标准可以阐明这一理论交叉点上的悬而未决的问题。我们现在可以指定（1）团队的基本概念广度，以及（2）这种基本广度可能驱动结果的程度。解决这些问题可以为许多分析层面的研究提供信息，从个人和团队的成功（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B164&#34;&gt;Srikanth 等人，2016 年&lt;/a&gt;，&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175&#34;&gt;van Dijk 等人，2017 年&lt;/a&gt;）到公司和行业绩效（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B144&#34;&gt;Roberson 等人，2017 年&lt;/a&gt;）。我们希望我们的插图能够激发在组织研究领域生成细粒度意义测量的新可能性。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;64-组织内部的词嵌入&#34;&gt;6.4 组织内部的词嵌入&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;最后，我们认为词嵌入方法将对我们研究的组织产生影响&lt;/strong&gt;。我们说明了在劳动力市场背景下潜在的嵌入必须塑造组织行为。从招聘到工作设计，从培训到晋升，人力资源管理的一个核心挑战是有效地将个人与组织内的角色、工作、情况和任务相匹配（Weller et al. 2019 &lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B183&#34;&gt;）&lt;/a&gt;。随着比赛质量的提高，各种绩效指标也会提高，包括工作满意度（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B8&#34;&gt;Ashforth 和 Saks 1996&lt;/a&gt;）、个人生产力（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B125&#34;&gt;Paauwe 2009&lt;/a&gt;）和组织绩效（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B38&#34;&gt;Dyer 和 Reeves 1995&lt;/a&gt;）。有效匹配的一个问题是不同维度的匹配的重要性程度。在一家公司中，技能可能最为重要，而在其他公司中，技能可能是文化契合度、态度、技能和经验的相互作用。由于嵌入模型捕获了所有这些维度，管理者可以为每个相关维度嵌入不同的原型描述，同时还嵌入个人资料和其他相关通信（例如电子邮件、松弛消息等），以衡量每个人与每个相关维度之间的匹配接近度。这样做可以让管理者更好地识别高维匹配及其对员工、社区和公司绩效的影响。&lt;/p&gt;
&lt;p&gt;嵌入模型旨在为人力资源的宏观管理提供新的视角（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B183&#34;&gt;Weller et al. 2019&lt;/a&gt;）。来自大型组织的相关信息存储在人力资源经理、一线经理、员工、同事和外部招聘人员中。然而，无法集中访问这些信息。通过嵌入，组织可以从所有数字面包屑的文本（电子邮件、聊天、工作描述、正式报告、绩效管理记录等）构建概念空间。这样做并使用相似性分析将使公司能够绘制和了解相关人力资本的位置位于公司对面。管理人员可以利用这些系统来准确了解任何员工的概念职位与任何给定的公司要求的差距有多大。这不仅可以为招聘、雇用、员工流动和流动等流程提供信息，还可以为培训、社交、工作设计和公司重组提供信息。因此，在劳动力市场和组织适应的背景下，嵌入模型可以产生有用的创新。人们可以想象许多其他组织实践和结构可以从这些模型及其测量可能性中受益，包括产品设计、市场分析和战略生成。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;我们同意&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65&#34;&gt;Hanan等人的观点。（2019&lt;/a&gt;，第 2 页）当他们观察到，考虑到概念和分类对几乎所有人类行为和社会互动的中心地位，人们对概念如何运作的关注如此之少，这是多么令人惊讶。现代组织内部及其周围进行的许多活动都需要概念信息的激活和传播。当一个人解决新问题、提出新想法或与他人合作时，就会发生这种情况。从围绕饮水机的良性闲聊到重新配置全球资本主义秩序或将人类登陆火星，概念及其所嵌入的概念空间发挥着核心、关键的作用。&lt;/p&gt;
&lt;p&gt;正如本文所示，我们现在拥有一系列重要的工具，可以为广泛而深入的理论想象和实证研究打开&lt;strong&gt;概念世界&lt;/strong&gt;和&lt;strong&gt;概念空间&lt;/strong&gt;。我们希望本文能够激发对嵌入可以提供信息的大量问题和理论的学术探索。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量  | 使用<strong>人民网领导留言板</strong>语料训练Word2Vec模型</a></li>
</ul>
<p><br><br></p>
<p>Aceves, Pedro, and James A. Evans. &ldquo;<strong>Mobilizing conceptual spaces: How word embedding models can inform measurement and theory within organization science.</strong>&rdquo; <em>Organization Science</em> (2023).</p>
<br>
<h2 id="摘要">摘要</h2>
<p>词嵌入模型是一种表示多维概念空间的强大方法，在多维概念空间中，所传达的概念可以相互关联、组合和竞争。此类模型代表了机器学习的最新进展，使学者能够用大规模文本数据局部和全局的单词共现，以最小的语义失真程度， 有效地编码复杂的意义系统。尽管词嵌入的使用有可能扩大组织科学中的理论可能性，但嵌入对于组织学者来说很大程度上是未知的，未发挥出词嵌入应有的潜力。我们的目标是通过为用户提供实用的路线图来展示嵌入模型在组织科学中的前景，以在他们的研究中调动该方法，并为开展该类研究的学者提供理论指导。 我们首先明确定义 <strong>概念</strong> 和 <strong>概念空间</strong> 的概念，然后继续展示如何使用词嵌入模型来表示和测量这些概念，并指出该方法的优点和缺点。然后，我们提供一组嵌入测量及其理论解释和灵活的扩展。我们的目标是从词嵌入的技术处理中提取概念，并将其置于实践的理论框架中，以加速此类研究。</p>
<p><br><br></p>
<h2 id="一介绍">一、介绍</h2>
<p>过去十年，文本作为数据的计算使用在组织科学中显着增长（Hasan 等人，2015 年；Goldberg 等人，2016 年；Srivastava 等人，2018 年；Hannigan 等人，2019 年）。这种增长的主要原因是文本编码的概念信息赋予个人、组织、经济和社会行为以意义（Evans 和 Aceves 2016，Gentzkow 等人 2019），并且在过去十年中，来自组织环境的文本数据急剧增长，大大提高了文本的可用性。然而，文本中编码的 <strong>概念意义</strong> 本质上是高维的，这使得降低概念复杂性成为研究文本的学者的中心任务。<strong>词嵌入模型是由计算机科学家和语言学家开发的一个新兴工具系列，用于文本信息降维，以此提取概念及其数字表示</strong>。词嵌入技术的发展使组织科学家依赖于文本数据进行理论构造， 相比之前，数据中信息的保真度更高，由此文本数据与组织研究交叉场景形成了新的理论研究路线。尽管词嵌入模型在组织科学之外得到广泛使用，但由于组织科学领域的学者缺乏对词嵌入技术的理解， 不知如何将它们纳入理论发展过程的原则框架，词嵌入模型对于理论发展的价值仍然被掩盖。</p>
<p><strong>词嵌入模型建立在高效的神经网络架构之上，并通过将复杂的语义系统有效编码到具有最小失真的稠密几何空间中，彻底改变了语义分析</strong>。这些模型代表了数十到数百个维度的空间中的语义，相对于语言中的单词和概念的数量来说，这个维度较低； 但相对于正式社会和文化理论家之前试图呈现概念信息的两到三个维度来说，这个维度却很高（奥斯古德 1964 年，史密斯-洛文和海斯 1988 年）。出于组织科学的目的，这些嵌入模型创建了社会系统中个体所持有的集体知识的 <strong>数字替身</strong> ， 嵌入可以解决文化上隐含类比（Mikolov et al. 2013b），回答文化偶然问题（Devlin et al. 2019，Radford et al. 2022），并预测未来的知识发现（Tshitoyan等人 2019；Sourati 和 Evans 2021）。组织科学长期以来一直借鉴人工智能（AI）的表征概念， 在这里，我们使用人工智能的表示机制来增强组织理论研究（Csaszar 和 Steinberger 2022）。</p>
<p>然而，由于神经网络复杂，且难以理解的黑盒性质特性，围绕神经嵌入和人工智能方法对理论发展的价值存在争议。尽管预测能力很强，但此类方法往往缺乏可解释性（Knight 2017，Leavitt et al. 2021）。<strong>在组织科学领域中，学者缺乏此技术的理解，即</strong></p>
<ul>
<li><strong>对于嵌入何时成为组织科学有用的方法论选择</strong></li>
<li><strong>如何在既定认识论标准内证明使用“复杂”神经嵌入方法的合理性</strong></li>
<li><strong>如何在各种嵌入中进行选择 等方法</strong>（例如，静态词嵌入与上下文嵌入、预训练嵌入与自定义嵌入）</li>
<li><strong>使用嵌入进行研究的适当步骤以及评估嵌入研究的相关标准</strong></li>
<li>最值得注意的是，研究界，特别是那些研究组织认知、文化、知识和意义的人，似乎对嵌入方法 <strong>如何适应将方法论选择与理论发展联系起来</strong></li>
</ul>
<br>
<p>我们的目的是通过两项贡献来解决这些问题。</p>
<p><strong>首先，我们的目标是提供一个理论指南，为嵌入模型提供一个原则性的概念框架，学者可以使用该框架为他们的模型注入意义，并使学者们能够在理论发展过程中运用这些模型。我们这里的主要论点是，词嵌入模型中的每个向量代表一个概念，整个嵌入模型代表生成文本数据的社会系统的概念空间</strong>。嵌入模型所代表的概念空间是多维空间，其中从规范和知识到想法和发明的概念相互关联。这个框架使组织学者能够利用嵌入模型的概念空间，与组织科学的许多领域之间建立联系。例如，不同公司基于知识视角对该空间的差异化覆盖（Grant 1996），组织理论家在描述规范和制度（Scott 2003），类别学者援引在决定将一个物体归类到哪个概念时（Pontikes 和 Barnett 2015 ），创新学者直接理论化寻求测量发现和发明的新颖性（Fleming 和 Sorenson 2001，2004），并且团队研究人员寻求了解成员在空间中的不同立场如何影响创造力、协调性和绩效（Srikanth 等人，2016）。因为我们以 <strong>概念</strong> 和 <strong>概念空间</strong> 为中心的理论框架可以推广到组织理论的许多背景，所以我们希望嵌入模型所支持的研究将促进这些子领域之间更深入、更持久的对话。</p>
<p><strong>其次，我们的目标是为利用嵌入模型进行理论发展提供实用的路线图</strong>。在此过程中，我们引导读者完成使用专利摘要语料库来实现词嵌入模型的过程，以表示现代技术创新的概念空间。我们解释了研究人员需要设置的模型参数，并逐步完成了他们应该采取的验证步骤，以评估模型是否有效地代表了他们感兴趣的概念空间，并提供了方法附录，其中包含实现所讨论的所有内容所需的代码。在注意到嵌入模型的可供性的同时，我们还讨论了它们不断发展的局限性，并提出了它们何时不适合组织分析的建议。然后，我们展示嵌入模型如何实现依赖于概念和概念空间的构造的理论化和测量。</p>
<br>
<p>我们概述了两大类词嵌入使用方法</p>
<ul>
<li><strong>度量之内/之间进行标记</strong>，我们提出了跟踪相关分析集内部和之间的概念关系的度量，以帮助我们跟踪与概念广度、概念距离和概念相似性</li>
<li><strong>意义及其维度</strong>，我们提出了四种衡量标准，为了解意义及其与组织的关系提供了不同的窗口。为找出这些测量机会的理论可能性，我们重点介绍了一些研究进展。</li>
</ul>
<p><strong>本论文的一个核心主张是，在组织研究不同广度和深度，词嵌入工具现在使我们能够表示其概念空间，并且比以前更精细地表示细节</strong>。有鉴于此，我们的目标是展示嵌入模型如何在与组织科学家相关的领域中操作概念空间，使研究人员能够扩展和完善现有理论。我们希望这一理论指南和实践路线图将促进组织科学内部的理论扩展，该扩展首先是扩大对文本数据的访问以及用于分析的随附计算工具（Kovács 等人，2013 年; Goldberg 等人; 2016年，Hannigan 等人, 2016年, 2019； Guo 等人，2020）。</p>
<p><br><br></p>
<h2 id="二概念和概念空间">二、概念和概念空间</h2>
<p>概念是人类生活的一个基本特征，我们的日常思维很大程度上依赖于它们所代表的信息，使我们能够对周围的人、物体和事件进行分类，并将这些信息传达给其他人（Murphy 2002；Bergen 和 Feldman 2008 年； Cassanto 和 Lupyan，2015 年）。概念是将我们的精神世界粘合在一起的粘合剂（Murphy 2002），赋予精神和物质体验以意义（Hannan et al. 2019）。<strong>在认知科学和心理学的语言中，概念是“事物类别的「心理表征」”（Murphy 2002）。</strong></p>
<p>概念有两大功能：分类和交流（Medin and Rips 2005），这些功能都需要语言的帮助。实际上，我们通过在语言中分配一个单词或短语来表示一个稳定概念的信息内容。这就是为什么我们通过说出或写出 “<em><strong>manager</strong></em>” 一词来提及经理的概念，从而引出它所包含的概念信息，例如对他人的责任、做出决策以及相对于组织同行获得更高的薪水。然后，语言的单词分割并链接了社区的共享概念空间（Lupyan 和 Bergen 2015）。这样，“一个概念就是一个单词或短语的含义……[包括]像 ‘<em><strong>red</strong></em>’ 和 ‘<em><strong>grasp</strong></em>’这样的基本的、具体化的单词，以及像 ‘<em><strong>goal</strong></em>’ 和 ‘<em><strong>continuity</strong></em>’ 这样的抽象和技术单词”（卑尔根）和 Feldman 2008]）。</p>
<p>概念并不作为唯一的信息单位存在于真空中。相反，概念之所以有意义，是因为它们彼此相关（Hannan et al. 2019），“通过相似性和上下文的关系紧密地缝合在一起”（Hofstadter and Sander 2013）。在这种多重概念关系中存在着“我们对世界的大部分知识，告诉我们存在什么以及它们具有什么属性”（Murphy 2002，p.1）。例如，概念 <em><strong>resource</strong></em>  与  <em><strong>firm</strong></em>、<em><strong>constraint</strong></em> 和 <em><strong>natural</strong></em> 等概念相关。在文化系统的层面上，概念之间的相互关系引发了表征概念之间宏观层面有意义的维度。 <em><strong>manager</strong></em> 概念在某些方面与 <em><strong>coach</strong></em> 和 <em><strong>president</strong></em> 的概念很接近，而在其他方面则与<em><strong>employee</strong></em> 和 <em><strong>bureaucracy</strong></em> 的概念很接近。将概念理解为存在于复杂几何空间中的点，使我们能够思考和测量概念之间的距离远近（Hannan 等人，2019）。例如，与  <em><strong>playground</strong></em> 或 <em><strong>ice cream</strong></em> 相比， <em><strong>manager</strong></em> 与<em><strong>organization</strong></em> 和 <em><strong>leader</strong></em> 概念的联系更加紧密。<strong>我们将这种概念相关的多维空间称为概念空间</strong>（Hannan et al. 2019)</p>
<p>重要的是我们用复数来指代概念空间。对于许多单词来说，它们会根据使用的上下文表现出不同的概念信息模式。首先，概念可能会根据使用它们的社会背景而有所不同。例如，如果在执行董事会议室、商品交易大厅或附近的储蓄和贷款机构的背景下说出 “<em><strong>Bank</strong></em>”，指的是银行而不是河流。概念也可能根据使用时间的不同而有所不同。例如，“<em><strong>高科技</strong></em>” 一词所引发的概念关系会根据我们研究的是 1960 年代、1990 年代还是今天而有所不同。最后，概念关系因使用它们的社区而异，因此 “<em><strong>债务</strong></em>” 所捕获的概念将根据其是由首席财务官还是低收入个人使用而有所不同。概念所含信息存在多样性， 正如 Hannan等人（2019）指出，“虽然有些概念可能是天生的或生物驱动的，但大多数都是社会构建的。”</p>
<p><br><br></p>
<h2 id="三先前研究中的概念和概念空间">三、先前研究中的概念和概念空间</h2>
<p>概念以及扩展的概念空间是人类思维和交流的基础（Sperber 和 Wilson 1986；，Murphy 2002；Hofstadter 和 Sander 2013）。正因为如此，概念和概念空间对于许多组织理论框架来说或多或少是明确和关键的。在某些研究（例如类别研究）中，概念具有核心重要性并且已经被明确地理论化。然而，在其他情况下，（例如，公司基于知识视角）概念被隐含地假定，即使它们是决定许多理论期望的基本成分。鉴于概念无处不在，对组织科学所有领域使用概念信息进行全面回顾超出了本文的范围。我们将简短、非详尽的回顾集中在概念和概念空间概念的三个领域——<strong>类别、知识和文化</strong>。通过嵌入技术处理并追踪存在于个人和社区头脑中的概念信息，研究其对组织行为和结果的影响。<br></p>
<h3 id="31-类别">3.1 类别</h3>
<p>类别是具有共同特征和属性的实体组。如前所述，概念是类别的心理表征。对类别的研究主要集中在跨类别或模糊类别是否会增加或减少分类实体的估值。自Zuckerman（1999）以来的工作一直集中在消除歧义条件上，在这些条件下，类别跨越和模糊性会导致积极或消极的估值。许多研究表明，由于感知偏差（Durand et al. 2007）、不符合受众期望（Hsu 2006)、Hsu et al. 2009；Leung and Sharkey 2014） ，跨越模糊的类别会损害实体估值，或降低分类对比度（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B119">Negro et al. 2010</a>）。其他研究表明，跨越类别可以创造积极的估值结果，因为它表明非典型性可以放大良好的表现并缓冲不良表现（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B159">Smith 2011</a>），一个类别可以锚定认知，而另一个类别可以有益地修改认知（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B188">Wry et al. 2014</a>）。还有其他研究表明，效果取决于受众，有些人喜欢跨类别，而另一些人则不喜欢（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B135">Pontikes 2012</a>）。通过这些方式，类别可以通过影响有关类别成员资格的概念信息的解释方式，对行为和绩效产生积极或消极的影响。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn5">4</a></p>
<p>尽管类别范式的贡献历来是通过类别成员的集合和模糊集合理论（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B64">Hannan et al. 2007</a>）概念来实现的，但最近的工作开始纳入其多维性（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65">Hannan et al. 2019</a>）和类别的分级归属感。组织学者感兴趣的许多现象都是由概念及其代表的类别之间的精确距离支撑的。例如，鉴于专利所贡献的技术领域，专利通常分为类别和子类。然而，专利中编码的想法可以传播到创新空间的广泛领域，即使只分类在一个类别中。正如我们稍后讨论的，转向概念的几何概念，使分析师能够考虑隶属度、重叠和连续距离影响底层实体评估判断的方式<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65">（Hannan 等人，2019 </a>。<br></p>
<h3 id="32-知识">3.2 知识</h3>
<p>众所周知，知识很难具体说明，并且在哲学、认知科学和社会科学领域，围绕其概念性质进行了长期而活跃的争论（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B166">Steup 和 Neta 2020</a>）。然而，过去几十年来，组织科学在微观、中观和宏观层面上进行了大量研究，解决有关知识及其在团队、组织和经济活动中的作用的问题。从对团队成员专业知识的研究（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B164">Srikanth et al. 2016</a>）到公司基于知识和注意力的观点（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B87">Kogut and Zander 1992</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B56">Grant 1996</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B123">Ocasio 1997</a>）；从交互记忆系统（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B143">Ren 和 Argote，2011</a>）到创新流程（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B50">Garud 等，2013</a>）；从组织设计（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B45">Foss et al. 2013</a>）到搜索和探索（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B93">Lavie et al. 2010</a>），知识在最近的组织理论化中发挥着核心作用。</p>
<p>无论人们对知识的定义如何选择，命题性知识从根本上都与概念信息相关。<em><strong>命题知识采取“ S [主体]知道p [命题]”</strong></em> 的形式（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B80">Ichikawa and Steup 2018</a>）。在某种程度上，命题是由语言中的单词编码的，并且单词代表概念信息，命题知识依赖于概念以及它们如何在概念空间中交织（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B110">McGrath and Frank 2020</a>）。以命题“泰勒知道氢的主要工业应用是氨的制造”和“特里知道量子算法可以具有较低的时间复杂度”为例。这些知识命题中的每一个都代表了不同的概念意义，前面提到的领域将以不同的方式操作它们。例如，团队学者可能会强调，由泰勒和特里组成的专利团队将拥有多样化的基础知识。采取基于注意力观点的学者会注意到，泰勒和特里可能会以不同的方式关注知识空间，以应对组织变革。研究创新的人可能会注意到如果泰勒和特里共享办公空间，知识重组的潜力。研究搜索的人可能会假设，为了解决问题，泰勒和特里会以不同的方式搜索概念性解决方案。在所有这些情况下，就这些领域通过诉诸语言编码的命题知识来理论化知识动态而言，它们以基本和可测量的方式参与概念和概念空间。<br></p>
<h3 id="33-文化">3.3 文化</h3>
<p>文化被不同地概念化为集体的共同价值观、故事、框架、工具包和类别（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B52">Geertz 1973</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B131">Pettigrew 1979</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B92">Lamont 和 Small 2008</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B158">Small 等人 2010</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B54">Giorgi 等人 2015</a>）。文化建构已成为组织研究的核心，在从个人和团队到组织和国家的各个层面的分析中都得到了运用（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B54">Giorgi et al. 2015</a>）。从理解文化如何塑造职业结构（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B195">Glynn 2000</a>）、组织领域（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B194">Anteby 2010</a>）和创业环境（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B106">Lounsbury and Glynn 2001</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B141">Rao and Giorgi 2006</a>）到它在讲故事（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B106">Lounsbury and Glynn 2001</a>）和身份建设中的作用（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B196">Ravasi 和 Schultz 2006</a>），从其对人际沟通的塑造（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165">Srivastava 等人，2018</a>）到对组织绩效的影响（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26">Corritore 等人，2020</a>），文化深深地受到概念及其互动方式的调节。文化以集体认知过程为基础（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B36">DiMaggio 1997</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B128">Patterson 2014</a>），很大程度上可以通过语言痕迹来获取。语言进入文化的窗口（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B55">Goldberg et al. 2016</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165">Srivastava et al. 2018</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26">Corritore et al. 2020</a>）很大程度上是通过它所表达的概念来呈现的，使得概念和概念空间成为组织文化研究的重要支柱。</p>
<p>基于它们在形成范畴、知识和文化方面的关键作用，概念和概念空间已成为许多组织理论赖以建立的知识支架的重要组成部分。然而，概念和概念空间通常仅被用作缺乏精确和可扩展的经验表征的不明确的隐喻。这限制了研究使用粗粒度的代理测量或允许手动编码和解释的小数据集。接下来，我们提出词嵌入模型是一种最先进的工具，用于表示概念和概念空间，可以添加到组织学者工具包中。就组织学者寻求将概念和概念信息所支撑的结构操作化而言，他们将得到这类新模型的帮助。考虑到这一点，我们接下来介绍嵌入模型如何工作以及为什么它们可以作为概念和概念空间的有效表示。</p>
<p><br><br></p>
<h2 id="四使用词嵌入来表示概念和概念空间">四、使用词嵌入来表示概念和概念空间</h2>
<h3 id="41-越来越多地使用文本作为数据">4.1 越来越多地使用文本作为数据</h3>
<p>过去 10 年，通过计算工具和方法进行文本数据分析出现了爆炸性增长。从社会学（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B40">Evans and Aceves 2016</a>）到经济学（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B53">Gentzkow et al. 2019</a>）和政治学（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B58">Grimmer and Stewart 2013</a>），文本正迅速成为组织、经济和社会生活的中心观察站。文本数据提供了在线知识社区、财报电话会议和公司报告、产品评估、组织电子邮件和讨论板、历史档案、视频转录和电影字幕、医疗记录、电子商务、社交媒体等多种领域的丰富思想和行为痕迹。媒体平台、新闻文章、科学学科等等。总而言之，这些文本数据源比以往任何时候都更深入、更广泛地进入组织生活。正如<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B40">Evans 和 Aceves（2016 年</a>）指出的那样，文本数据现在使我们能够访问“有关正在玩的社交游戏的隐藏元素及其背后的社交世界”的深层信息。然而，这些语料库的庞大规模及其广泛的范围意味着，提取理论上有意义的信息信号越来越多地受到计算方法的帮助，利用信息技术方法获取大量非结构化文本数据，并将它们转换为有意义且相关的度量。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn6">5</a></p>
<p>文本数据与组织学者习惯使用的定量数据之间的一个主要区别是文本是高维的。正如<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B53">Gentzkow 等人（2019 年</a>）指出，“仅使用英语中一千个最常用单词的 30 个单词的 Twitter 消息样本 [&hellip;] 的维度大致与宇宙中的原子一样多。” 因此，使用文本作为数据的学者的中心任务是通过对数据施加限制来降低维度。<strong>过去二十年里，组织科学中用于降低这一维度的一些最常用的计算工具是词典法、语义网络和主题模型。尽管这些方法有其优点，但一个主要缺点是它们无法对文本中存在的细粒度概念关系和关联进行编码</strong> 。接下来，我们将展示嵌入模型如何利用文本中的局部和更广泛的信息来训练概念含义和概念空间的高保真表示。在此过程中，我们展示了词嵌入模型如何克服先前方法来表示文本中编码的含义的一些局限性，从而允许对理论结构进行更细粒度的测量，并实现新的理论可能性。</p>
<br>
<h3 id="42-词嵌入">4.2 词嵌入</h3>
<p>我们之前解释过，概念是事物类别的心理表征，人类通过在词典中分配一个单词或短语来表示稳定的概念，并指出，概念只有在与跨多个维度的其他概念相关并为其提供信息时才有意义。密集的概念空间。在这里，我们认为词嵌入模型是最近开发的一类从机器学习应用于自然语言处理的模型，它使我们能够有效且高效地表示概念空间，并将这些空间用于追求组织科学。词嵌入模型是文本语料库中单词的连续表示，可以进行几何解释。<strong>词嵌入的方法论假设，一个词的含义很大程度上是由出现在其直接和更广泛上下文中的词所决定的，这一想法受到结构语言学家的启发，他们已经证明，含义的差异与局部分布相关（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B68">Harris 1954</a>）， 这个想法现在被称为 「分布式语义学」，Firth 的著名描述是：“观其伴而知其意”（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B42">Firth 1957</a>，you shall know a word by the company it keeps）， 一个单词所代表的概念或含义可以通过它周围的单词的分布来推断</strong>。</p>
<p>以这种分布式方式思考概念和概念空间的底层计算架构可以追溯到 20 世纪 80 年代初期计算机科学家 Geoffrey Hinton 的工作（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B71">Hinton 1986</a> , <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B72">Hinton et al. 1986</a>）以及认知科学家在这一时期研究的并行分布式处理模型（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B149">Rumelhart 等人，1986a</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B150">b</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B109">McClelland 和 Rumelhart，1989</a>）。分布式架构是当前嵌入语言模型的基础（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115">Mikolov et al. 2013b</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130">Pennington et al. 2014</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35">Devlin et al. 2019</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104">Liu et al. 2019</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B17">Brown et al. 2020</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B41">Fedus et al. 2020）。 2021</a>）， 嵌入模型 Word2Vec 算法(<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115">Mikolov 等 2013b</a>) 相对简单易用，能够处理中等规模的语料库来。 <strong>Word2Vec 与  GloVe（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130">Pennington 等人，2014 年</a>）和 FastText（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B13">Bojanowski 等人，2017 年</a>）等嵌入算法，是 ChatGPT 和相关模型的基础</strong>。</p>
<p>找个例子来帮助理解算法， 现在我们要创建过去 50 年创新的概念空间表示。首先需要概念活动领域的文本数据， 美国专利局数据提供了创新活动的踪迹，其中包括所有专利的文本、摘要、描述和权利要求。在整篇论文中，我们使用这个专利摘要语料库来指导读者完成训练这个概念空间和构建相关概念测量的过程。数据是从<a href="https://patentsview.org/">Patentsview.org</a>免费下载的，使用 1976 年至 2019 年间发布的所有专利来构建本文中发现的词嵌入模型和测量相关指标。</p>
<p>想象一下，专利语料库中的每个独特单词都是从放置在巨大冰箱上的随机放置的 <strong>“word magnet”</strong> 开始的（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B76">Hovy 2020</a>）。当连续词袋 (CBOW) 算法滚动浏览语料库时，使用每个目标词周围的单词词(滑动窗口的上下文)来预测目标词（更多内容见下文）。该算法的最终目标是产生一种语义模型，其中出现在相似上下文中的单词彼此接近，而来自不同上下文的单词则相距很远。由于用2维概念空间不足以捕获每个单词的全部含义，因此该算法改为在更高的（100-1,000）维空间内捕捉语义。通过这种方式，目标单词的概念信息是从它周围的单词中归纳出来的，将语料库中的每个单词绘制为<em>n</em>维空间中的坐标或向量。正是单词在这个<em>n</em>维向量空间中的相对位置，使我们能够将词嵌入模型可以描述代表人类概念活动区域的概念空间。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn7">6</a></p>
<p>概念意义的识别假定了嵌入空间的可解释性。接下来，我们提出了对这些概念空间的一系列提示和测量，作为从中产生结构化解释的方法。这很像心理学家使用 <strong>心理测量调查</strong> 将概念印象转化为可解释的观点（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B112">Michael Furr 2021</a>）。或者<strong>认知人类学家如何使用结构化任务，例如排序和排名（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B163">Spradley 2016</a>），将概念性的世界观转变为可解释的世界观</strong>。我们认为嵌入模型必须接受结构化测量（就像向人类受试者提供的心理测量问卷）使他们的 **概念景观(conceptual landscape)**变得可解释。接下来，我们将引导读者如何用专利语料库训练创新概念空间表示的过程。之后， 我们概述了该方法的优点和局限性，并指出这些方法与先前的文本分析方法和组织研究实践的关系。</p>
<br>
<h3 id="43-选择语料库">4.3 选择语料库</h3>
<p>学者可以根据应用使用两种词嵌入模型。一方面，研究人员可以使用自有文本语料库来训练表示， 据此了解文本所涉主体(个人、团体、社会)行为的概念空间是什么样子， 以及概念关系揭示人类活动背景。在我们的示例中，专利创新在专利语料库中得到了很好的体现，因此我们在下面展示了如何从头开始训练概念空间表示, 以及它揭示了哪些概念联系。研究人员可以从头开始训练语料库的其他例子包括在线社区（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18">Burtch et al. 2021</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2">Aceves et al. 2022</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B23">Chambers et al. 2022</a>）、学术学科（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74">Hofstra et al. 2020</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B102">Lin et al. 2022</a>） 、劳动力市场（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B9">Bana 2022</a>）、公共记录（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B6">Arseniev-Koehler et al. 2022</a>）、产品和公司描述（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61">Guzman and Li 2023</a>）以及财报电话会议和公开演讲（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B85">Kirgil and Voyer 2022</a>）。</p>
<p>或者，如果研究人员想要在较小的语料库中追踪概念动态，而该语料库的大小不足以训练独特的、特定于上下文的嵌入，那么研究者可以使用预训练嵌入模型，需要注意，训练预训练嵌入模型的文本与研究者小语料库在内容、场景要有相似性。广泛使用的预训练嵌入已经在来自海量语料库的文本上进行了训练，例如新闻（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B4">Google 2013</a>）、维基百科（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35">Devlin et al. 2019</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B57">Grave et al. 2018</a>）。训练这些预训练嵌入模型的文本语料体量很大， 内容题材往往包含我们较小文本样本中存在的概念。因此使用预训练嵌入对这些概念的信息进行编码，并可用于近似相关距离。政治和历史语义背景下的研究发现，预训练嵌入提供的结果与特定于上下文的嵌入相当（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">Kozlowski et al. 2019</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145">Rodriguez and Spirling 2022</a>）。如果有理由相信研究项目中包含的概念和想法没有在这些大量预训练嵌入中得到很好的体现，研究人员可以使用较小语料库中的文本对其进行 <strong>微调（Fine-Tune）</strong>（ <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104">Liu et al. 2019，Burtch et al.2019</a>）<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18">， 2021</a>）。微调将预训练的概念空间扭曲为与样本一致（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B104">Liu et al. 2019</a>），更好地反映概念之间的关系。</p>
<p>最后，使用哪一种嵌入(自己训练的嵌入、 预训练的嵌入、微调的嵌入)将取决于研究人员的目的以及他们寻求追踪的概念动态的类型。接下来，我们将重点描述从头开始训练和验证嵌入模型的过程。在接下来的部分中，我们讨论不同参数设置和策略之间的权衡，并鼓励读者遵循文章文本和在线附录。</p>
<br>
<h3 id="44-清理语料库">4.4 清理语料库</h3>
<p>训练嵌入模型的第一步是使用 Python 等编程语言录入文本语料库， 首先获取每个专利摘要中的文本， 并将连续的文本进行切词，转化为单词列表 。然后，我们将文本小写，删除标点符号和数字字符串，并将每个摘要转换为称为token的单词列表。但是这可能破坏一些词组语义，这里使用 <em><strong>bi-gram</strong></em>， 识别高频共现的词组成词组，例如当 <em><strong>“electric”</strong></em> 和 <em><strong>“vehicle”</strong></em> 这两个词在某些上下文中一起出现时，它们将被统一形成短语和概念 <em><strong>“electric_vehicle”</strong></em> 。建立单词或短语列表后，执行单词嵌入算法来学习单词或二元组及其语言上下文之间的最佳距离，以保留语言中单词和短语的概念空间。</p>
<br>
<h3 id="45-训练嵌入模型">4.5 训练嵌入模型</h3>
<p>第一步是选择词嵌入算法， 浅层神经网络构建的单词表示（例如，Word2Vec、FastText；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115">Mikolov 等人，2013b</a>）、共现矩阵的低秩近似（GloVe；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B130">Pennington 等人，2014</a>） ，或来自 Transformer 的深度上下文嵌入（例如 BERT、<em>GPT</em>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35">Devlin 等人 2019</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B139">Radford 等人 2022</a>）。这些不同算法输出，都可以被解释为<em>n</em>维概念空间，其中单词或短语由空间内的向量位置表示。本文我们只介绍 Word2Vec 算法， word2vec 是一种广泛使用的训练概念空间的算法（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B113">Mikolov 等人，2013a</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115">b</a>）。</p>
<p>Word2Vec 算法的一种流行实现算法是连续词袋 (CBOW) 算法，可以在 Gensim python 库中轻松访问，该算法使用目标单词的语言上下文来预测被扣掉的目标词 (可以简单的理解为让机器做完形填空题) ， 比较适合小规模数据集。 Word2Vec 还实现了另一种 Skip-Gram 算法，该算法通过从目标单词预测上下文单词来反转 CBOW 的预测任务，比较适合大规模数据集。相比之下，skip-gram 将每个上下文目标对（例如，T：“房子”，C：“宽敞”）视为单独的观察，因此可以更好地捕获精确的语义，但需要更大的语料库才能获得卓越的性能。</p>
<br>
<h3 id="46-维数">4.6 维数</h3>
<p>考虑维数很有必要。朴素的模型可以将不重复总词数作为维度， 例如包含 100,000 个不重复单词的语料库， 任何单词都需要  100,000 维才能准确表示。然而，当单词从上下文中被识别为相似时，可以一定范围内减少维度数。<strong>维度过多会导致内存需求和冗余增加，并降低可解释性；维度太少会扭曲距离并且无法解释语言的不及物性</strong>。通过这种方式，通过具有至少足够的维度来捕获所讨论的复杂语义关系，可以获得准确的预测。</p>
<p>在实践中，300 维已经成为一个标准，很大程度上源于最初的 Word2Vec 论文之后的惯例，该论文通过交叉验证确定了最佳维数，以减少预测屏蔽词任务中的错误。大多数后续分析都是建立在较小、多样性较低的文本集合上，需要较少的维度，因此 300 通常被用作上限。最近的工作表明，应根据语料库统计数据选择维度 - 语料库词汇表中成对等距单词的数量提供了维度数量的下限，低于此界限通常会导致单词嵌入质量下降（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B127">帕特尔和巴塔查亚 2017</a>）。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74">霍夫斯特拉等人。(2020)</a>使用 100、200 和 300 维的模型找到了稳健的结果。</p>
<p>如果分析师寻求实现维度可解释性，他们必须以最小失真来确定表示数据所需的维度数。 但这最后一步一半很少执行，因为维度的优化需要大量的时间和计算资源。</p>
<br>
<h3 id="47-窗口尺寸">4.7 窗口尺寸</h3>
<p>回想一下，窗口大小是指算法将用来焦点目标词（或其邻居）之前和之后的单词数量。该窗口最小可以是 1。对于较小的窗口，算法将倾向于对句法关系进行编码（例如，名词后跟动词）。<strong>随着窗口大小的增加，更多的含义和语义被编码到模型输出中</strong>。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145">考虑Rodriguez 和 Spirling (2022)</a>的示例，其中包含两个句子的语料库：(1)“狮子吃肉”和 (2)“牛吃草”。当窗口大小为一时，我们会知道牛和狮子都吃东西，从这个意义上说，牛和狮子在语法上是等价的，因为我们没有足够的信息来区分两者。然而，随着窗口的增加，算法开始对牛与狮子的含义进行更多编码。<strong>与维度数量一样，这里的回报也递减，窗口大于五个字的模型性能略有改善</strong>（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B145">Rodriguez 和 Spirling 2022</a>）。 <strong>BERT 和 GPT 系列等上下文模型具有更大的窗口，这些窗口通过注意力过程进行驯服，算法通过该过程识别哪些上下文单词对于解释焦点单词的含义很重要</strong>（Vaswani 等人，2017 年<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B176">）</a>。</p>
<br>
<h3 id="48-验证模型">4.8 验证模型</h3>
<p>最后一步是验证词嵌入模型，这样做是为了确认算法学习的表示与文本数据所承载的真实人类活动的概念空间表示尽可能相近。论文附录第 2 节描述了关于专利嵌入的七个详细验证程序，表明该模型有效地学习了创新空间的表示。这些包括（1）邻近嵌入词的语义相似性；(2)具有嵌入距离的语义梯度；(3)嵌入簇与语义域之间的对应关系；（4）物理世界距离与嵌入之间的相关性；(5) 社会距离与嵌入之间的相关性；(6) 嵌入空间类比推理的准确性；(7)嵌入文档的语义一致性。我们还讨论了第八个“额外”测试，即图灵测试（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B174">Turing 1950</a>）。由 Transformer 支持的现代上下文嵌入的评估标准是它们是否能够与人类毫无区别地参与任何分类、关联、意义生成或集成任务，包括普通对话和专家教程。OpenAI 的 ChatGPT 和许多竞争的聊天机器人已经展示了如此强大的性能，以至于图灵测试正在迅速从上限转变为基线基准。这些验证步骤与论文最后部分的测量相结合，作为嵌入模型的有用提示prompt和测量，使研究人员能够对其编码的概念空间提供结构化解释。</p>
<br>
<h3 id="49-词嵌入方法的优点和缺点">4.9 词嵌入方法的优点和缺点</h3>
<h4 id="491--无需正式指定相关尺寸">4.9.1  无需正式指定相关尺寸</h4>
<p>对概念建模的正式尝试试图通过逻辑演绎方法清楚地枚举概念的相关维度（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B47">Gärdenfors 2004</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B48">Gardenfors 2014</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65">Hannan 等人 2019</a>）。尽管这种方法对于理解限定领域内的概念很有用，但即使如此，它也可能不切实际且难以衡量，因为很难先验地陈述分析师应预期的相关维度<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B73">（Hofstadter 和 Sander 2013 ）</a>。 <strong>词嵌入的优点在于，概念之间的关系以及对任何给定概念重要的相关维度可以从语言的使用方式中推断出来，因此不需要事前指定</strong>。鉴于在分析之前没有必要陈述相关维度，即使是最复杂的组织行为剧场也变得易于分析处理。正如其他人所指出的，“词嵌入为语言中包含的多个维度的含义提供了全面且有意义的见解，这是以前的方法无法捕获的”（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105">Lix 等人，2022 年</a>，第 8434 页）。在某种程度上，这种优势源于这样一个事实：神经网络架构能高效地记录意义的维度。</p>
<br>
<h4 id="492-更大的有效维度">4.9.2 更大的有效维度。</h4>
<p>嵌入通常由 100 到 1,000 个密集编码维度表示。<strong>编码的密度意味着每个词向量在所有建模维度上都有一个非零坐标</strong>。正如附录中所指出的，主题模型可能具有相同数量的主题（例如，100-1,000），但这些主题被稀疏编码以方便人类解释，使得主题仅具有一些基本上非零的单词加载，并且文档仅具有少量非零的主题负载。<strong>因此，主题模型是为了描述而构建的，但代价是迫使其表示的有效维度从数百个减少到几个，从而扭曲了本来可以在主题空间内计算的距离。相比主体模型， 词嵌入使用密集编码，每维度的嵌入很难理解和描述，但距离具有更大的自由度，可以更精确地编码含义</strong>。通过这种方式，相对于低维理论和测量，嵌入为分析师提供了“大量潜在轴，个人和社会群体可以沿着这些轴竞争、合作、分裂或合并”（Kozlowski et al. 2019，p.27 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">）</a>。</p>
<br>
<h4 id="493-无监督训练">4.9.3 无监督训练。</h4>
<p><strong>词嵌入还有一个特殊优点，即训练模型时， 以看似无监督或自监督的方式进行，从而避免了手动编码文本语义内容的繁琐，完全由机器学习</strong>。在我们的创新示例中，向量空间由我们专利语料库中的每个发明人按照他们所写句子的数量和长度的比例进行监督。每个单词的滑动窗口都是为了向专利审查员和未来的发明者传达一种含义而构建的，该算法用于构建向量空间并以概念上适当的方式定位单词。因此，学者们可以利用专利语料库来训练 <strong>技术创新</strong> 的概念空间，利用财报电话会议记录和新闻稿来训练 <strong>上市公司沟通</strong> 的概念空间，利用分析师报告来训练 <strong>投资分析</strong> 的概念空间，或者特定领域的概念空间。使用内部通信（例如 Slack 和电子邮件）来了解公司的知识。这些概念空间可以在最少的监督下进行训练，因此很快成为有价值的观察站，用于追踪组织科学家关注的组织生活的静态和动态（Hofstra et al. 2020，Whalen et al. 2020，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74">Burtch</a> et <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184">al</a> . <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18">2021</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B177">Waller 和 Anderson 2021</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2">Aceves 等人 2022</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20">Carlson 2022</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B23">Chambers 等人 2022</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61">Guzman 和 Li 2023</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94">Lawson 等人 2022</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105">Lix 等人 2022</a>）。</p>
<br>
<h4 id="494-共现是不必要的">4.9.4 共现是不必要的。</h4>
<p><strong>这些模型的另一个优点是，两个概念不必在任何文档中同时出现，就可以将它们编码为相似的向量</strong>。所需要的只是它们与相似的概念同时出现。例如，我们可以先验地指出 <em><strong>医生</strong></em> 和 <em><strong>律师</strong></em> 在某些方面非常相似（例如，他们需要高级学位，具有高收入水平等），但他们可能永远不会同时出现在语料库的同一文档中。尽管彼此之间缺乏共现性，但它们很可能都独立地与高收入*、<em>高学历</em>、*白领等概念同时出现，从而最终拥有编码这些相似性的接近向量。<strong>因此，嵌入模型的底层计算架构可以更好地近似社会和文化含义，而无需求助于严格的共现</strong>。</p>
<br>
<h4 id="495-上下文相关的含义结构">4.9.5 上下文相关的含义结构。</h4>
<p><strong>使用定制训练的嵌入模型的一个优点是它将捕获上下文相关的含义结构</strong>。例如，<em><strong>“甜”</strong></em> 的含义在软件团队的背景下与 <em><strong>烹饪</strong></em> 的背景下会有所不同。正如<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105">Lix 等人。（2022）</a>指出，在软件团队的背景下，与 <em><strong>“甜蜜”</strong></em> 最接近的术语是 <em><strong>“强烈”</strong></em>、 <em><strong>“兴奋”</strong></em> 和 <em><strong>“耶”</strong></em>。此外，就同一个单词编码不同概念（一词多义）而言，单词每种含义的概念信息都位于单词嵌入内的线性叠加（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B5">Arora et al. 2018</a>）。这意味着编码诸如 <em><strong>“Bank”</strong></em> 之类的单词的<em>n</em>维向量包含其代表的所有概念的概念信息，例如 <em><strong>河边</strong></em> 或 <em><strong>金融机构</strong></em>。通过这种方式，即使在多义词的情况下，单词的上下文相关含义也会被编码到模型中。当这些上下文相关的含义不仅不同，而且是排他的或相反的时，来自转换器的上下文相关嵌入可以为上下文中的每个单词呈现不同的单词向量。</p>
<br>
<h4 id="496-几何有助于概念人群体和组织的细粒度表示">4.9.6 几何有助于概念、人、群体和组织的细粒度表示。</h4>
<p><strong>我们认为，词嵌入模型可以在训练的语料库范围内产生人类活动概念空间的细粒度表示</strong>。<strong>这意味着，从概念空间内编码的信息中，我们可以恢复个人、群体和组织本身的细粒度表示</strong>。以我们的创新案例为例，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F1">图 1</a>描述了在说明性二维空间中这是如何实现的。学习到的概念空间将由单词或短语w表示的概念作为其最原子的分析级别。我们的限制性示例显示了在2维空间中排列的九个单词。单词 1-3 由发明人 1 使用，单词 4-6 由发明人 2 使用，单词 7-9 由发明人 3 使用。<strong>通过获取每个人的单词向量的质心向量，我们可以得出每个发明人在创新的概念空间</strong>。<strong>将这个过程提升到团队和组织级别，我们可以在发明人团队和组织的概念空间内得出独特的向量</strong>。因此，词嵌入架构不仅在概念的最原子级别上是细粒度的，而且还可以在更聚合级别上提供细粒度的表示。相对于团队多样性、组织差异化和注意力等结构的粗粒度代理，这形成了显着的测量改进，这些结构在嵌入特定概念空间时是有意义的。</p>
<br>
<p><img loading="lazy" src="img/figure-1.jpeg" alt=""  />
<strong>图 1.嵌入作为概念、人员、群体和组织的细粒度表示</strong></p>
<br>
<h4 id="497-细粒度几何减少了上下文信息的丢失">4.9.7 细粒度几何减少了上下文信息的丢失。</h4>
<p><strong>由于粗糙、粗粒度的代理指标无法承载相关信息，在实证分析和相关理论构建中就无法利用这些信息</strong>。嵌入模型的优势在于其独特的信息表征，可以携带更多的信息，信息的粒度更小，保存的信息量更多。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F2">图 2</a>使用团队多样性的示例来说明如何实现这一点。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F2">图 2(a)</a>显示了两个团队，1 和 2，每个团队要么通过熵（一种标准的、集合论多样性的理论度量（顶行））来表示，要么通过概念广度（基于底层概念的细粒度度量）来表示。团队调动的信息（底行）。团队 1 和团队 2 都有四名成员，团队 1 由两名生物化学家、一名化学家和一名分析化学家组成，团队 2 由两名生物化学家、一名海洋学家和一名计算机科学家组成。<strong>由于两个团队的团队成员类型比例相同，因此它们都被编码为具有相同的团队多样性熵度量 1.5</strong>。**然而，当考虑团队成员的概念信息时，我们发现它们是本质上不同类型的团队，团队 1 的多样性或概念范围远不如团队 2 **。这表明粗粒度的测量可能会留下未开发的有价值的上下文信息（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B187">Wolpert et al. 2014</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B33">DeDeo 2017</a>）。因此，我们应该看到更细粒度的衡量标准与相关的、理论上的绩效结果之间的联系更加紧密和一致。</p>
<br>
<p><img loading="lazy" src="img/figure-2.jpeg" alt=""  />
<strong>图 2.（在线彩色)细粒度表示可防止有价值的信息丢失</strong></p>
<br>
<p>专利数据集使我们能够通过三种构建的措施来说明这一主张。首先，集合论团队多样性度量，使用团队先前专利在专利主要类别中的分布（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B79">Huo 等人，2019</a>）。第二种替代措施使用专利子类，以便它们提供相对于第一种更细粒度的措施。第三个衡量标准依赖于团队成员先前专利在创新概念空间内的<strong>概念广度</strong>。</p>
<br>
<h4 id="498-词嵌入的局限">4.9.8 词嵌入的局限。</h4>
<p>到目前为止，我们的注意力仅限于讨论嵌入模型的结构，描述它们与概念空间的关系，并注意到它们的优点。在这里我们将说明其局限性，讨论它们的严重性、改善方式，以及何时不要用词嵌入的意外情况。我们讨论三类限制。第一个源于神经网络模型一般复杂的“黑匣子”性质，以及这带来的具体挑战，涉及输入数据的偏差，以及模型正确推理的范围，特别是那些对超出分析师背景的数据进行预训练的模型。第二个与这些模型的大小以及训练它们所需的数据量有关。第三个问题涉及词嵌入模型的具体局限性以及从脱离韵律和表达上下文的文本数据中分析含义的挑战。</p>
<p>许多学者首先担心的是，多级神经网络模型显得复杂且在统计上难以理解，<strong>经常被批评为“黑匣子”方法</strong>，无法“打开”以询问其性能背后的机制（Knight 2017，Leavitt et <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B86">al</a> . <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B95">2021</a>） 。现代神经网络词嵌入模型通常作为自监督模型实现，该模型启发式搜索单词之间的依赖关系空间以预测屏蔽词的身份。<strong>自从第一个高性能嵌入发布（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B115">Mikolov 等人，2013b</a>）以来，对其黑盒性质的一些担忧已经减弱，因为数学家发现最流行的“浅”词嵌入模型（如 Word2Vec 和 FastText）获得了很大的优势</strong>。其强大功能来自于近似易于理解的矩阵分解方法的运算，例如因子分析、主成分分析和对应分析（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B100">Levy 和 Goldberg 2014</a>）。</p>
<p>“黑盒”输入输出方法带来的一个相关潜在限制是，<strong>输入的偏差将转化为输出中的偏差</strong>——用于训练嵌入的语料库的偏差将被编码在生成的单词嵌入模型中（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B14">Bolukbasi等人，2016</a>）。当模型用于现实世界的下游应用程序（例如推荐服务）时，这可能是有害的。例如，硬编码到嵌入中的种族和性别刻板印象可能会导致有偏见的建议（例如，评估是否适合招聘职位或预测财务违约的可能性），并导致不公平和不道德的决定（例如，拒绝工作或信贷） 。学者们应该根据他们的研究问题和设计，主动考虑这种负外部性是否可能，并在对人类造成伤害的可能性足够高时，偶然放弃嵌入。<strong>然而，在某种程度上，理解社区和研究背景中概念关联的本质是核心，研究人员将需要这些偏见进行分析。如果不包括它们，模型以及研究设计就会错过表征其研究背景的关键社会和文化规律。</strong></p>
<p><strong>如果分析人员对生成语料库的上下文没有清晰的了解，就会出现另一个相关的限制，这样他们最终可能会做出不适用和不相关的推论</strong>。例如，强调意义随时间变化的研究（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19">Caliskan et al. 2017</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49">Garg et al. 2018</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">Kozlowski et al. 2019</a>）的特点是词义表现出来自外源冲击的间断变化，从而重新配置了概念关联的结构。穿过空间。想一想 2005 年卡特里娜飓风之后“卡特里娜”的含义发生了怎样的变化。2009 年金融危机之后，金融术语的含义发生了重新配置，部分原因是添加了“问题资产救助计划”等许多新术语。忽略外源冲击可能会导致对后面和验证部分中描述的措施的错误解释，将其视为仅由进化产生的结果，从而导致错误的推论。这是一个特别成问题的问题，因为许多最准确的词嵌入模型都是在从网络上提取的大量文本语料库上进行预训练的。此类模型可用于引导非常小的文本数据之间的有意义距离，这是一项常见任务，但<strong>如果预训练数据是异构的，则距离可能无法反映焦点文本的概念世界</strong>。</p>
<p>接下来的两个限制必然是其嵌入优势的另一面。词嵌入模型产生的细粒度信息会带来特定研究可能或可能无法维持的成本。首先是模型尺寸。<strong>每个单词的数百个维度的细粒度信息或上下文嵌入需要比简单的字典计数或潜在狄利克雷分配主题模型更大的存储空间</strong>。这与通常用于将数据维度减少到两个或三个的因子和主成分分析形成鲜明对比。词嵌入模型使用更多维度（通常为 200-500）来更准确地预测数据的屏蔽部分。尽管如此，当前个人计算机的计算能力和存储能力现在允许训练合理大小的嵌入。</p>
<p><strong>与此相关的是，词嵌入模型需要比先前模型更多的文本才能稳健地估计概念空间</strong>。当大型语料库与研究主题相似并且可以用作理论相关文档或微调过程的初始化的代理时，可以通过迁移学习来弥补这一挑战。<strong>然而，有时相关语言在内容、目的或形式上与模型预训练的数据有很大不同，它需要独立建模，但又足够小，无法维持对嵌入模型的稳健估计。在这种情况下，使用字典计数或主题模型可能会更好，因为数据只能维持粗粒度的关联，而这些方法旨在捕获粗粒度的关联。</strong></p>
<p>最后一类通常涉及词嵌入和文本方法的特殊限制。首先，静态词嵌入本身并不处理一词多义，即一个词（例如 <em><strong>“bank”</strong></em> ）编码多个概念（例如金融机构、河边、侧向倾斜）的情况。尽管多义词的存在可能会影响后续一些指标的测量，但也存在抵消的力量。一方面，研究发现多义词的含义以相互线性叠加的方式编码在单词向量内（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B5">Arora et al. 2018</a>）。这意味着该算法通过同时考虑单词的所有含义来对单词在概念空间中的位置进行编码，从而克服了原本可能存在的严重缺陷。另一方面，上下文嵌入架构（在线附录中有更详细的描述）通过根据焦点词周围的上下文输出不同的向量来明确解决多义词的问题。每个单词不是单个向量，而是根据用途而变化的向量云。如果分析师怀疑一词多义可能是特定分析的严重问题，他们可以偶然使用上下文嵌入并规避这种担忧。</p>
<p>最后一个潜在的限制是文本方法的一般特征。只要文本数据是转录语音话语的产物（例如，欧洲央行或美联储主席演讲、政治演讲、财报电话会议、电视或电影文字记录、对话互动），语音的语调、语气和音色将没有纳入到嵌入表示中。考虑到。<strong>鉴于某些语言（例如中文）更严重地依赖语调来传达含义，这可能或多或少存在问题，具体取决于话语发生的社会背景及其表达语言</strong>。因此，在语调和语气在语料库中发挥重要作用的情况下，学者们应该讨论他们的嵌入模型选择和解释决策的后果。</p>
<br>
<h3 id="410-在研究中使用词嵌入模型的路线图">4.10 在研究中使用词嵌入模型的路线图</h3>
<p>现在我们大脑对词嵌入模型是什么、如何表示概念空间、如何训练、优点和局限性有了框架性的认知，接下来可以将它们整合到研究和理论构建的标准方法中。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#T1">表 1</a>列出了如何将嵌入模型集成到科学流程中的路线图。</p>
<ul>
<li>步骤 1-3 是研究过程中的标准步骤，包括确定一个可行且有趣的研究问题，通过在适当的实证背景下进行评估，为重要的理论问题提供信息（Weick 1989 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B179">）</a>。</li>
<li>步骤 4-9 总结了本文到目前为止对嵌入模型的讨论。</li>
<li>步骤 10 和 11 ，与下一节指标度量有关，通过标准定量和定性方法调动该度量。</li>
</ul>
<br>
<p><strong>表 1.在研究中使用词嵌入模型的路线图</strong></p>
<table>
<thead>
<tr>
<th>步骤</th>
<th>活动</th>
<th>基本原理</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. <strong>确定研究问题</strong></td>
<td>如果研究问题至关重要，请确定文本数据是否有助于在理论研究上有帮助。</td>
<td>吸引研究人员把注意力聚焦在理论问题、词嵌入构建研究构念回答问题的交叉点。</td>
</tr>
<tr>
<td>2. <strong>理论建立及相关理论构建</strong></td>
<td>确定使用哪种理论框架来解决研究问题以及通过嵌入模型来操作哪种理论构念。</td>
<td>理论构念与其词嵌入指标(构念的衡量）之间的紧密联系能够实现累积的理论发展。</td>
</tr>
<tr>
<td>3. <strong>定义经验背景</strong></td>
<td>选择适当的实证背景，在其中回答研究问题并动员理论框架和构念。</td>
<td>确保研究问题、理论框架和用构念以逻辑方式相互加强。</td>
</tr>
<tr>
<td>4.<strong>指定将用于表示经验背景的概念空间的文本数据</strong></td>
<td>描述将用于训练词嵌入模型和测量感兴趣的理论构念的文本数据的范围。 数据是否有效地涵盖了您想要得出理论结论的经验背景下的行为活动范围？</td>
<td>确保用于计算理论构造度量的词嵌入模型在逻辑上映射到并有效地代表所提出的理论框架内的实证研究背景。 文本数据的范围应该在逻辑上映射到所讲述的理论故事的范围。</td>
</tr>
<tr>
<td>5.<strong>确定文本数据的大小和范围</strong></td>
<td>数据是否足够大以学习相关概念空间的准确表示？</td>
<td>文本数据的大小将决定是否应该训练自定义嵌入，或者是否应该使用可用数据对现成的嵌入进行微调。</td>
</tr>
<tr>
<td>6. <strong>给定数据大小，要么训练独特的词嵌入模型，要么微调现有模型</strong></td>
<td>如果文本数据足够大，则训练自定义嵌入来表示感兴趣的经验上下文的概念空间。 如果文本数据不够大，请使用这些数据来微调现有的现成嵌入模型。</td>
<td>确保用于测量理论结构的嵌入模型能够有效地表示经验背景的相关概念空间。</td>
</tr>
<tr>
<td>7. <strong>如果训练独特的模型，请选择一种算法</strong></td>
<td>在连续词袋 (CBOW) 或 Skip-Gram 模型之间进行选择。</td>
<td>CBOW：在较小的数据集上可以有更好的性能。 <br>Skip-gram：可以更好地捕获语义。</td>
</tr>
<tr>
<td>8. <strong>如果训练独特的模型，确定相关参数</strong></td>
<td>选择窗口大小和维数。</td>
<td>窗口大小：标准做法是 5。较小的窗口可以更大程度地捕获语法，较大的窗口可以更大程度地捕获语义，但收益递减并增加计算成本。 维度数：标准做法是 300，超过此点后性能回报递减。</td>
</tr>
<tr>
<td>9. <strong>验证词嵌入模型</strong></td>
<td>请遵循在线附录中的验证程序。</td>
<td>确认嵌入模型准确有效地表示了经验背景的概念空间。</td>
</tr>
<tr>
<td>10. <strong>计算相关度量</strong></td>
<td>通过确定将用于实施感兴趣的理论构念的相关概念集，创建“实际措施和应用”部分中的措施之一。</td>
<td>使学者能够将该测量用于定量或定性分析。</td>
</tr>
<tr>
<td>11. <strong>在标准定性或定量方法中使用计算的度量</strong></td>
<td>对于定量分析，该度量要么成为自变量，要么成为因变量。 对于定性分析，学者可以提供解释性分析，因为它们可能适用于其他类型的档案、民族志或视听数据。</td>
<td>嵌入模型表示对生成数据的社会背景的概念空间的描述。</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="五实际措施与应用">五、实际措施与应用</h2>
<p>现在已经正式定义了 <strong>概念</strong> 和 <strong>概念空间</strong> 的含义，并说明了先前的文献如何处理概念信息,  介绍了嵌入模型表示能力的底层逻辑，并在在线附录中完成了支持这种直觉的几个验证步骤。也评论了嵌入模型给概念信息分析带来的几个优点和相关缺点。</p>
<p>在本章中，我们将介绍一些新研究， 学习他们如何用嵌入生成独特指标。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#T2">表 2</a>总结了这些指标及示例应用。</p>
<br>
<p><strong>表 2.词嵌入测量和示例应用</strong></p>
<table>
<thead>
<tr>
<th>措施</th>
<th>研究性学习</th>
<th>关键构念</th>
<th>研究问题</th>
<th>代表性调查结果</th>
<th>嵌入在这种情况下的优点</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. <strong>概念广度</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105">利克斯等人。(2022)</a></td>
<td>话语多样性——在一组给定的互动中，群体成员所传达的含义彼此分歧的程度。</td>
<td>一个群体的话语多样性如何影响其绩效？</td>
<td>高绩效团队会调整他们的共享认知以匹配任务的要求（例如，构思与协调）。</td>
<td>能够随着时间的推移以细粒度的细节和动态地追踪小组对话的概念广度，使学者们能够追踪话语多样性的新理论构造。</td>
</tr>
<tr>
<td>2.<strong>概念距离和相似度</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74">霍夫斯特拉等人。(2020)</a></td>
<td>语义遥远的科学新颖性：博士论文中新链接概念的语义距离。</td>
<td>代表性不足的群体是否更有可能产生科学创新？</td>
<td>相对于男性，女性引入了更遥远的新奇事物。 然而，这种语义上遥远的新颖性在该学科中很少受到关注。</td>
<td>能够追踪新概念组合的概念距离，使学者不仅可以研究是否做出了新组合，还可以研究这些组合的语义距离最终如何影响其影响。</td>
</tr>
<tr>
<td>3.<strong>概念X性</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94">劳森等人。(2022)</a></td>
<td>性别刻板印象：男性（而非女性）与以成就为导向的代理特征（例如自信和果断）相关的程度。</td>
<td>雇用女性首席执行官和董事会成员是否与组织对代理语言的性别使用发生变化有关？</td>
<td>当组织雇用女性首席执行官和董事会成员时，女性的语义与代理的语义变得更加一致。</td>
<td>对 22 家标准普尔 500 强公司的 43,000 多份文件（包含超过 12 亿字）进行分析，深入细致地研究女性的含义如何因聘用女性领导者而发生变化。否则这样的分析是不可能的。</td>
</tr>
<tr>
<td><strong>4.概念意义</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63">汉密尔顿等人。(2016)</a></td>
<td>词语的文化意义：词语的含义随时间变化的程度。</td>
<td>语义演化的可能驱动因素是什么？</td>
<td>跨历史时期的语义变化率与词频的逆幂律成正比。 与频率无关，具有更多含义的单词具有更高的语义变化率。</td>
<td>能够探索跨多个知识和文化领域的大型历史时期和大量文本中的语义变化。例如，他们可以详细追踪同性恋这个词的含义如何从<em>快乐</em>和<em>艳丽</em>等概念转向<em>同性恋</em>和<em>女同性恋</em>等概念。</td>
</tr>
<tr>
<td>5. <strong>文化和知识连续体中的概念立场</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">科兹洛夫斯基等人。(2019)</a></td>
<td>社会阶层标记：区分社会阶层维度的概念。</td>
<td>20世纪社会阶级的标志是如何变化的？</td>
<td>尽管社会阶级维度在历史上保持稳定，但阶级文化标记在每个维度中的定位方式却不断发生变化（例如，员工从士兵和肌肉等概念转变<em>为</em>白领<em>和</em>中产阶级<em>等</em>概念*）*。</td>
<td>能够将文化相关的概念投射到文化相关的兴趣连续体上，从而使研究人员不仅可以在单个历史时期内而且可以在其历史演变过程中了解广泛共享的社会关联。</td>
</tr>
<tr>
<td>6. <strong>概念维度</strong></td>
<td><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">科兹洛夫斯基等人。(2019)</a></td>
<td>阶级的文化维度：理解社会阶级的维度（富裕、教育、修养、地位、就业、道德、性别）</td>
<td>20 世纪文化阶层的规模有多稳定？</td>
<td>20世纪，尽管发生了巨大的经济转型，阶级规模仍然非常稳定。</td>
<td>能够对阶级的多个概念维度进行实证分析，从而理解 20 世纪美国它们之间的相互关系。</td>
</tr>
</tbody>
</table>
<br>
<h3 id="51-概念广度">5.1 概念广度</h3>
<h4 id="511-指标">5.1.1 指标</h4>
<p><strong>可以测量文档中单词之间的距离来计算它们在概念空间中的分布范围</strong>。文档可以是从专利到个人电子邮件通信的任何内容。我们可以测量每个单词与其他单词的平均距离有多远。<strong>获取文档内元素的平均距离（或每个单词与文档质心之间的距离）可以衡量该文档内的「概念宽度」</strong>。例如，我们衡量每项专利的概念广度， 可以从两个简单的文档开始，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">doc1 =  [&#34;biochemistry&#34;, &#34;chemistry&#34;, &#34;analytical_chemistry&#34;]
doc2 =  [&#34;chemistry&#34;, &#34;oceanography&#34;, &#34;computer&#34;]
</code></pre></div><p>使用我们的专利嵌入模型，我们得到第一组(doc1)的平均宽度为 29，第二组（doc2）平均宽度为 47。这表明第二组在概念上比第一组更广泛。</p>
<p>当我们衡量文档集合而不是单词的概念广度时，同样的逻辑也适用。例如，我们想了解发明者团队的广度。在这种情况下，我们可以将团队中的每个发明人视为嵌入概念空间中的“文档”，参考如图<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#F1">1</a> , 从下往上，依次是词概念空间、发明人概念空间、团队概念空间、组织概念空间。一个发明人团队的成员已经在涉及纳米技术、生物技术和软件的概念空间领域发表了先前的专利，那么在概念上将被认为比所有成员只发表了纳米技术专利的团队更广泛。即使所有发明人都将其公开的专利限制在一个类别内，该指标仍然会提供显着的变化。</p>
<p><img loading="lazy" src="img/figure-1.jpeg" alt=""  />
</p>
<br>
<h4 id="512--应用">5.1.2  应用</h4>
<p>这种概念广度的度量已在最近的工作中用于追踪各种理论构念。<strong><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B105">利克斯等人。(2022)</a>衡量团队成员在参与软件项目的不同阶段时的 话语广度</strong>。<strong>他们能够追踪每个独特项目阶段概念参与的多样性，发现表现最好的团队有能力改变他们的认知以适应手头不断变化的任务，在提出新想法时表现出更大的话语广度，而在转换时表现出较低的广度依赖于协调的任务。这种细粒度的知识参与概念很难用以前的文本分析方法来追踪</strong>。 详细内容可阅读大邓近期推文 <a href="https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/">MS2022 | 使用语言差异性测量团队认知差异性</a> 。</p>
<p>另外，研究人员使用概念广度来追踪在线社区成员根据状态变化分配注意力的范围，发现状态和注意力广度之间存在 U 形关系（Aceves et al. 2022 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B2">）</a>。这些研究人员训练了 150 个知识领域的概念空间，从而能够追踪不同知识领域的相似注意力动态，从计算机编程和数学到育儿和园艺。由于他们有能力在数百个社区的文本中大规模部署算法，因此他们能够计算出超过 2000 万成员如何在这些问答社区上发布的 2300 万个问题中分配注意力。</p>
<p>其他工作在整个语言中实施了这种方法，追踪语言在所有知识领域具有更宽或更窄的概念空间的程度（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B1">Aceves 和 Evans 2021</a>）。使用圣经、电影字幕和以多种语言编写的政治文件等文本的并行翻译（包含相同的信息但以不同的语言编码），他们能够追踪概念在不同语言中相互关联的程度存在显着差异。他们发现，尽管一些语言将不同的概念子空间紧密地联系在一起，并将不同的概念领域编织在一起，但其他语言却稀疏且更加支离破碎，更强烈地分隔了不同的意义域。然后，他们观察概念空间的语言密度如何塑造数百种语言的真实对话和维基百科文章的概念广度。</p>
<p>所有三篇论文都为不同文献的研究开辟了新的理论途径，例证了该方法的潜力。如果没有概念空间的概念及其通过嵌入模型的表示，这些新的研究途径将很难实施。</p>
<br>
<h3 id="52-概念距离和相似度">5.2 概念距离和相似度</h3>
<h4 id="521-指标">5.2.1 指标</h4>
<p>当我们的分析重点在于集合内的元素时，前面描述的概念广度构念是相关的。当我们的分析重点是不同集合之间的关系时，可以使用相同的基础度量。在这种情况下，我们将指的是概念距离或相似性，而不是概念广度。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn14">13</a>形式上，如果我们有至少两个集合，每个集合中至少有一个元素，我们可以计算这些集合之间的<strong>概念距离，作为每个集合的质心或多维平均值之间的距离</strong>。最基本的是，我们可以计算两个集合之间的概念距离，每个集合包含一个单词。这无非是衡量这些词之间的概念距离。随着元素数量和集合数量的增加，底层计算保持不变，但理论可能性的范围扩大。还可以通过训练文档嵌入模型来计算这种距离/相似性度量，该模型在嵌入空间中为每个文档分配一个向量，其权重按照单词共现的相同逻辑进行训练（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B96">Le 和 Mikolov 2014</a>），将文档本身视为文档中的另一个单词，将这些单词用作与其共现的单词。</p>
<p>通过将概念相似性与衡量专利相似性的现有技术进行比较，我们可以一睹该衡量标准的潜力。首先，研究人员可以通过查看专利授予机构使用的官方分类来追踪专利的相似性，同一类别的专利被认为比不同类别的专利更相似（Singh 和 Marx 2013，Aharonson 和<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B157">Schilling</a> 2016 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B3">）</a>）。这种方法的局限性在于分类度量是粗粒度的，并且不太可能考虑所有相关的技术特征，特别是当类别边界必然滞后于技术进化时（Thompson 和 Melanie Fox-Kean 2005，Singh<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B172">和</a>Agrawal <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B155">2011</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B7">Arts 等人，2018</a>）。其次，研究人员可以获取两项专利并测量它们之间的单词重叠（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B7">Arts et al. 2018</a>）。然而，这种方法是有限的，因为它仅适用于成对的文档，无法确定专利相对于整个知识体系的位置。</p>
<p>概念相似性解决了这些限制（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184">Whalen 等人，2020</a>）。首先，它允许我们追踪专利在相关知识空间中的精确位置，从而访问知识系统中的所有相关的细粒度信息。其次，我们能够精确量化任何专利或专利组相对于任何其他专利或专利组的位置。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn15">14</a>第三，随着新知识进入系统，知识的性质和结构不断演变，随着时间的推移重塑 <strong>概念边界</strong> 和关联。<strong>嵌入使我们能够衡量专利发布时存在的概念空间内的专利相似性，使我们能够摆脱使用滞后的、周期性偏离的类别，并可能对连续的发明概念空间强加类别差异</strong>。概念距离的所有这些优点都适用于其他知识和文化领域，在这些领域中，我们寻求测量思想、个人、群体或组织之间的距离或相似性，从而扩展现有的跨研究领域并开辟新的理论领域。</p>
<h4 id="522-应用">5.2.2 应用</h4>
<p>正如我们上面所做的那样，这种<strong>概念相似性的衡量方法最近被用来描述专利数据中的创新空间</strong>（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B184">Whalen 等人，2020</a>）。研究人员使用  <strong>doc2vec</strong>  框架计算了超过 6 亿个专利对的相似度。在生成这些知识相似性度量时，作者还使用这些分数提出了有趣的辅助度量，包括可操作的度量（a）现有技术接近度——专利引用与其自身相似或不相似的现有技术的程度，（b）现有技术同质性——一项专利引用知识空间领域彼此远离的程度，(c) 影响邻近性——一项专利被与其自身相似或不相似的未来专利引用的程度，以及(d) 影响同质性——一项专利通过其前向引用与一组不同的未来专利相关的程度。</p>
<p>学者们也使用了这一衡量标准，重点关注概念距离。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B18">伯奇等人。(2021)</a>使用概念距离的 <strong>doc2vec</strong> 实现来调查同行奖励是否会影响在线社区内贡献的新颖性。这里的<strong>新颖性是根据社区成员获奖前后贡献的距离来衡量的</strong>。作者发现，获奖后，奖项会导致知识空间内的新颖性减少，剥削行为增多。同样，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B74">霍夫斯特拉等人。（2020）</a>使用 Word2Vec 距离度量来捕获科学论文将新颖性引入科学文献的程度，发现来自代表性不足群体的学生负责将最具新颖性引入系统。</p>
<p>其他人则利用这一措施来实施公司差异化。在发展中国家微型企业的背景下，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20">Carlson（2022）</a>使用 BERT 架构（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35">Devlin et al. 2019</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B142">Reimers and Gurevych 2020</a>）来计算其数据集中所有微型企业的成对余弦距离。通过这些距离，他们能够估计八个发展中国家的 10,000 家微型企业的差异化与收入和利润的增加相关。同样，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B61">Guzman 和 Li（2023）</a>使用距离的 doc2vec 实现来使用 Crunchbase 数据来衡量初创公司的创始战略差异化。作者发现与<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B20">Carlson (2022)</a>类似的结果，即差异化经验的新公司在早期融资和股权结果方面有所增加。</p>
<br>
<h3 id="53-概念x">5.3 概念X</h3>
<h4 id="531-指标">5.3.1 指标</h4>
<p>文档距离的另一个用途是追踪语料库中的任何文档与捕获感兴趣的构念X的焦点(原型）的相似性， 这样的测量将捕获任何观察的 <strong>概念X性</strong>( Conceptual X-ness)。这种测量的第一步是描述与我们寻求尽可能精确测量的结构相关的概念信息。例如，如果我们想要捕获专利与 <strong>时间</strong> 或 <strong>几何</strong> 等概念相关的程度，我们可以构建一个我们认为映射到、定义或与这些概念相关的单词列表 。对于每个列表，我们计算其质心向量 (c#27)，然后测量任何给定专利距离 <strong>时间</strong> 和 <strong>几何</strong> 概念有多远。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#fn16">15</a> 对于附录表 A2 中使用的专利，我们可以看到，与头颈约束装置专利相关的前两项专利更接近时间概念，正如所预期的那样光和时间在概念上交织的程度。概念性的<em>X</em>度度量可用于追踪思想、个人、团体、组织或任何其他相关聚集的组成。</p>
<h4 id="532-应用">5.3.2 应用</h4>
<p>最近在一篇论文中使用了这种方法，该论文追踪了雇用女性担任高级领导角色对女性在这些组织中意味着什么的影响（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B94">Lawson 等人，2022</a>）。作者首先使用 SEC 文件和财报电话会议记录训练了 Word2Vec 嵌入。然后，他们创建并验证了一组 100 个单词来捕捉 <strong>代理概念</strong> 的含义（例如，有能力、独立、主导），并观察了内部任命高级女性领导前后 <strong>代理概念</strong> 与 <strong>女性</strong> 概念之间的距离。该组织发现，在 <strong>女性</strong> 被任命为高层管理人员之后的一段时间内，女性的含义在概念空间中更加接近于机构职位。作者使用不同的嵌入超参数和维度大小复制了他们的结果，说明了嵌入模型的鲁棒性，条件是具有捕获概念空间内语义变化的最小必要维度。</p>
<p>这里有趣的理论机会包括更深入地参与理论传统的可能性，这些理论传统在组织科学以外的领域具有影响力，但由于缺乏可行的方法来以原则性的方式量化其理论构造，因此这些理论传统仍然处于我们的领域之外。依赖文学解释。正如我们所提出的，测量 <strong>概念X性</strong> 使得扩大与理想形式（* <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B134">Plato Bloom 1968</a>）、理想类型（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B178">Weber 2011</a>）、家族相似性（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B186">Wittgenstein 2010</a>）和原型（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B147">Rosch 1973</a>）相关的理论构造的测量成为可能。以一致、有原则和可复制的方式。在这方面，概念性的<em>X</em>性代表着开放大量的认知和社会理论，以便在组织的背景下进行实证检验和扩展。</p>
<br>
<h3 id="54-语义转变和漂移">5.4 语义转变和漂移</h3>
<h4 id="541-指标">5.4.1 指标</h4>
<p>概念空间使我们能够识别术语的含义如何随着时间和空间的变化而变化。探索概念意义的一种方法是为不同的个人、公司、行业、地理位置或时间段创建独特的嵌入模型，以了解它们之间的含义有何不同（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B148">Roy 等人，2019 年</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B181">Welch 等人，2020a</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B182">b</a>）。一旦识别出相关的兴趣分歧，我们就可以采用相关的语料库（例如，专利、财报电话会议、报纸）并为数据中的每个语料库训练概念空间。<strong>在我们的专利示例中，我们可能会训练两种嵌入模型，一种是 1990 年功能性磁共振成像技术发明之前的时期，另一种是 1990 年之后的时期</strong>。然后我们可以探索与大脑和神经科学相关的概念的含义如何随着这一创新而改变。例如，在功能性磁共振成像发明之前和之后与不同大脑区域最相关的术语是什么。接下来，我们可以比较不同公司或国家的含义变化有何不同，以及这种变化的格局如何影响所涉及的公司和行业的组织和市场结果。显式动态词嵌入允许嵌入之间具有更大的可比性，但必然会忽略特殊的词和用途（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63">Hamilton et al. 2016</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B192">Zhang et al. 2016</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B190">Yao et al. 2018</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B103">Liu et al. 2020</a>）。这些算法的输出带有时间戳词向量包含特定时期的语义信息，但在历史上保持可比性。</p>
<br>
<h4 id="54-2-应用">5.4. 2 应用</h4>
<p>第一篇在社会科学背景下使用词嵌入方法的主要论文就是使用这种方法来研究意义（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63">Hamilton et al. 2016</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19">Caliskan et al. 2017</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49">Garg et al. 2018</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">Kozlowski et al. 2019</a>）。在第一篇论文中，研究人员使用四种语言的六个历史语料库，通过观察概念空间中最近的单词在过去几十年中如何变化来追踪单词含义随时间的变化（Hamilton et al. 2016 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B63">）</a>。使用 Word2Vec 嵌入，他们追踪了 <strong>同性恋</strong>  概念的含义如何从 1900 年代围绕 <strong>“愚蠢”</strong>、**“甜蜜” **和 **“开朗”  **等术语的含义转变为围绕 1950 年代 <strong>“嬉闹”</strong>、 <strong>“机智”</strong> 和 <strong>“聪明”</strong> 等术语的含义，并且最终以 20 世纪 90 年代女同性恋、双性恋和同性恋等术语的含义结束。在另一篇论文中，研究人员研究了词嵌入中的刻板关联之间的关系及其与当代社会经验数据的关系（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B19">Caliskan et al. 2017</a>）。例如，他们追踪了职业的性别刻板印象，发现职业具有女性意义，因为它们与女性参与劳动力市场相关。在另一项研究中，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B49">Garg 等人。(2018)使用预先训练的 Google News Word2Vec 模型（ </a><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B4">Google 2013</a> ）量化了美国 100 多年历史中的性别和种族刻板印象，阐明了不同的形容词和职业如何或多或少地与不同人群（例如，男性与女性）密切相关，白人与亚洲人与西班牙裔）随着时间的推移。</p>
<p>最近通过词嵌入追踪含义的工作已经使用这种方法更深入地研究了特定的上下文。一项研究使用 19 世纪第一人称叙述的语料库来追踪黑人和白人男性和女性的交叉身份如何映射到五个社会机构，包括政治、经济、文化、家庭领域和权威关系（Nelson 2021 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B120">）</a>。<strong>举论文中的一个例子，作者测量了与“精致”概念的距离，发现它与白人女性的联系最密切，而与黑人男性的联系最少</strong>。</p>
<p>在其他工作中，研究人员利用这种方法来衡量政治领导人的 <strong>集体意向性</strong> （人们参与集体推理和行动的能力），并比较共和党和民主党领导人如何以不同的方式动员集体意向性（Kirgil and Voyer 2022 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B85">）</a>。他们通过创建复数代词（我们，我们的）、复数常量（国家名称）和复数名词（人）的复合列表来测量集体意向性。然后，使用词嵌入模型，他们找到了各州集体意向向量最接近的术语，使他们能够比较不同领导人如何不同地动员集体意向。总的来说，这些意义研究表明，就语言为我们提供了解文化的窗口而言（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B55">Goldberg et al. 2016</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B165">Srivastava et al. 2018</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B26">Corritore et al. 2020</a>），嵌入模型为我们提供了一种独特的表达方式透过那扇窗户看到的照片。</p>
<br>
<h3 id="55-文化和知识连续性中的概念地位">5.5 文化和知识连续性中的概念地位</h3>
<h4 id="551-指标">5.5.1 指标</h4>
<p>另一种新颖的测量方法可以通过追踪概念相对于感兴趣的概念维度的位置来创建。如前所述，嵌入模型可用于解决类比推理任务，例如**“国王”-“男人”+“女人”=“女王”<strong>。 该架构可用于定义概念空间内任何感兴趣的维度。<strong>在国王-王后的例子中，性别维度通过“男人”-“女人”和“国王”-“女王”向量进行操作。</strong><a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">科兹洛夫斯基等人。（2019）</a>详细介绍了如何在概念空间内构建此类维度。首先，研究人员需要确定感兴趣的维度。对于我们这里的例子，我们将把不同的概念投射到男性-女性性别维度上。为此</strong>，我们首先确定定义性别维度的相关术语**。这里我们使用集合 [&lsquo;man&rsquo;, &lsquo;him&rsquo;, &lsquo;he&rsquo;, &lsquo;male&rsquo;, &lsquo;men&rsquo;] 和 [&lsquo;woman&rsquo;, &lsquo;her&rsquo;, &lsquo;she&rsquo;, &lsquo;female&rsquo;, &lsquo;women&rsquo;]。 <strong>然后我们计算不同概念在这个男性-女性概念轴(维度)上的正交投影。</strong> 在线附录中的图 A4 将每个概念投射到 <strong>男性-女性概念轴</strong>。 更消极的预测表明与女性气质的关联更强，而更积极的预测表明与男性气质的相关性相当。如图 A4 所示，这些预测与关于这些概念的性别状态的一般直觉一致，使我们能够明确说明每个概念相对于其他概念在这个维度中的位置。正如预期的那样，<strong>军事</strong> 和 <strong>农业</strong> 与 <strong>男性气质</strong> 的联系最为密切，而 <strong>卫生棉条</strong> 和 <strong>口红则</strong> 与 女性气质的联系最为密切。按照这个程序，学者们现在可以测量任何概念在任何感兴趣的维度和任何文本丰富的时空背景中的位置。此外，不同语言的语料库可以独立训练和对齐，或者同时训练和对齐，以方便国际分析（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B81">Johnson et al. 2017</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116">Milbauer et al. 2021</a>）。</p>
<p><strong>双极概念维度的投影方法可以进一步扩展到锚定具有多种含义的低维子空间，其中单词和概念可以被绘制并理解为这些含义的混合</strong>。这可以通过理论上选择“原型”的集合来执行，即具有已知且广泛共享含义的极值点，并在这些极值锚定义的子空间中绘制所有相关单词或概念。[例如，在对一个新的基于信息技术的创业企业进行分类时，人们可能会问它在 Uber、亚马逊、谷歌或比特币所刻画的空间中适合什么位置(Breiman 1994，Eugster 2012，Damle 和 Sun 2017）。</p>
<br>
<h4 id="552-应用">5.5.2 应用</h4>
<p>这项措施的制定和运用是为了研究 20 世纪和 21 世纪社会阶层的演变（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">Kozlowski 等人，2019</a>）。他们研究了根据 20 世纪出版的数百万本书的文本训练的嵌入，按照上述程序操作了阶级的维度，试图了解社会阶级的底层维度在 20 世纪是如何变化的。为此，他们提出了以下理论上的<strong>概念轴(维度)</strong>：富裕程度（富人与穷人）、教育程度（受过教育与未受教育）、修养（有教养与未受教育）、地位（有声望与无声望）、道德（善与恶）、就业（雇主-雇员）和性别（男人-女人），分别嵌入 20 世纪的每个十年。然后，他们可以在这些维度上投射不同类别的概念，例如音乐风格、体育和职业，以了解这些概念在本世纪的过程中如何演变和发展。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B6">研究人员应用这种方法来研究健康、道德（ Arseniev-Koehler et al. 2022</a>）、政治意识形态（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B171">Taylor and Stoltz 2021</a>）和地位（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B129">Peng et al. 2021</a> ）等背景下的其他类型的文化关联。</p>
<p>研究人员不仅将概念投射到这些概念轴(维度)上，而且将整个文档投射到这些维度上，从而推动了测量的可能性（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B170">Taylor 和 Stoltz 2020</a>）。此外，尽管以前的措施依赖于研究人员指定感兴趣的连续体的相关维度，但最近的工作已经转向自动识别这些连续体（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116">Milbauer et al. 2021</a>）。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116">Milbauer 等人</a>利用 Reddit 社区的内容。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B116">（2021）</a>创建了一个无监督的程序来识别社区中的多个意识形态极点，使他们能够超越静态的左右意识形态维度，发现现代话语中发挥作用的许多两极分化和意识形态差异的轴。人们可以想象在许多组织环境中使用这种方法来识别团队、小组、单位或部门之间存在的许多潜在冲突来源。</p>
<br>
<h3 id="56-概念维度">5.6 概念维度</h3>
<p>之前，我们讨论了研究人员如何调查关键术语在相关文化维度上的位置，描述概念的位置在性别维度上的差异。然而，这并不是概念轴(维度)的唯一用途，因为概念空间还允许我们测量和理解相关维度本身如何相互关联。该措施的扩展是使用空间内的编码维度并将它们相互比较。例如，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B89">科兹洛夫斯基等人。（2019）</a>利用他们既定的阶级维度来追踪整个 20 世纪每个维度如何与其他维度相关，例如，表明随着世纪的发展，富裕与教育的关系变得更加密切，而与教育的关系无关。栽培。通过这种方式，组织学者可以理解相关维度之间的关系在相关概念空间中可能有何不同。例如，学者可以研究不同文化维度在组织或行业内部和之间紧密或松散联系的程度。</p>
<p><br><br></p>
<h2 id="六讨论">六、讨论</h2>
<p>最后，我们简要讨论了一些利用嵌入模型进行思考的新兴方法，然后讨论了我们认为理论、方法论和组织的有价值的机会，这些机会源于将这些模型理解为概念空间的细粒度表示。这个讨论必然是说明性的，但暗示了现在这些精致的意义模型的可操作性的广泛可能性。</p>
<h3 id="61-词嵌入方法的富有成果的扩展">6.1 词嵌入方法的富有成果的扩展</h3>
<p>词嵌入的底层计算架构最近经历了扩展，可以在与之前讨论的不同方向上动员组织研究。我们简要提到三个，并在在线附录中提供更详细的描述。首先，概念和语言的层次结构在“直线”、欧几里得几何中很难得到体现，需要许多难以理解的维度来用标准嵌入来捕获。然而，层次结构可以用负弯曲双曲嵌入来原生表示（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B90">Krioukov et al. 2010</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B126">Papadopoulos et al. 2012</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B22">Chamberlain et al. 2017</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B121">Nickel and Kiela 2017</a>），为探索复杂现代的交叉层次结构提供了新的测量可能性。组织。例如，将公司名称嵌入双曲空间中将能够直接发现典型的“中心公司”，并在商业新闻语料库中与所有其他公司进行比较。额外的双曲维度将揭示子层次结构，反映商业评论员所持有的概念和比较价值的不同维度。</p>
<p>其次，模型语言的深度学习方法为词嵌入增加了关键的上下文敏感性。考虑像 BERT ( <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B35">Devlin et al. 2019</a> ) 和 GPT 系列模型 ( <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B140">Radford et al. 2019</a> ) 这样的大规模模型，它们使用“注意力”的神经网络机制来识别影响焦点词含义的上下文词 ( <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B176">Vaswani ) et al. 2017</a>），组装成一个称为转换器的架构，可以将问题转换为答案，将文本转换为翻译，将请求转换为响应。这种模型产生的内容可以被描述为上下文嵌入，这样每个单词不是由单个向量表示，而是由向量云表示，每个向量代表不同上下文中的该单词。“google”上下文中的“Apple”与“orange”上下文中的“apple”具有不同的值。这些模型极大地提高了预测能力，并进一步扩展了我们对概念空间进行精确建模的能力，但代价是复杂性和计算量更大。</p>
<p>最后，嵌入架构可以扩展到在序列或更高维上下文中排列的任意符号集。例如，图像已被用来衡量抽象艺术图像的新颖性和创造力（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B10">Banerjee and Ingram 2022</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B11">Banerjee and Kaplan 2022</a>），分析警察预约照片（大头照），并识别与司法拒绝保释相关的先前未概念化的紧急特征听证会（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B107">Ludwig 和 Mullainathan 2022</a>）。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B101">音乐（ Liang et al. 2020</a>）、音频剪辑（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B70">Hershey et al. 2017</a>、<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B189">Xie and Virtanen 2019</a>）和视频（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B191">Zellers et al. 2021</a> ）的多维空间是使用audio2vec（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B168">Taglisacchi et al. 2020</a>）等工具构建的。 、signal2vec（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B118">Nalmpantis 和 Vrakas 2019</a>）和 video2vec（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B62">Habibian 等人 2017</a>），为组织学者接触代表组织生活视听体验的新型媒体打开了大门。</p>
<p>最近对双曲线、上下文、图像和音频嵌入的扩展表明，嵌入模型的底层计算框架的持续改进和扩展将继续下去，为组织科学中持续的实证、测量和理论创新奠定了基础。</p>
<br>
<h3 id="62-词嵌入和组织理论">6.2 词嵌入和组织理论</h3>
<p>在理论层面上，将嵌入模型理解为概念空间的有原则的、细粒度的表示有可能刺激新的理论发展并完善现有理论。例如，意义研究中的经典陈述影响了文学理论和文化社会学等其他领域，但未能在组织科学中站稳脚跟。自20 世纪初德索绪尔 ( <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B32">de Saussure 1986</a> )的著作带来语言学的结构转向以来，许多人都试图将意义在组织和社会生活中的作用理论化。列维-斯特劳斯汇集了来自全球各地的多样而广泛的民族志，以向世界文化所特有的表面混乱提出深层的文化秩序，并认为复杂的意义是从有意义的元素的结合中产生的（列维-斯特劳斯 2016 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B99">）</a>。福柯理论化了话语和权力如何紧密相连，权力和知识如何以自我强化的联盟结合在一起（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B46">Foucault 2012</a>）。<em>布迪厄将惯习</em>的概念阐述为“持久的、可互换的处置系统，倾向于充当结构结构的结构化结构，即作为实践的生成和结构的原则”（Bourdieu 1977，第72页<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B16">）</a>。尽管这些理论很有吸引力，但迄今为止它们只能进行松散且间接的测试。如果没有可靠的实证立足点，他们就永远无法在管理和组织理论中取得突出地位。然而，概念空间的实证操作化现在使得这些文化理论基础著作的参与和扩展变得容易处理，其中的许多结构现在可以辩护地测量。嵌入模型将使这些理论与管理和组织理论相关。</p>
<p>我们还希望嵌入模型能够对现有理论框架进行更深入的研究和锐化。一组能够受益的文献是那些与知识相关的文献。鉴于组织学者可以获得的大部分知识都被编码在语言的符号概念系统中，现在可以通过更多可用的文本数据源来获取知识，并且可以通过嵌入模型的概念空间来表示。材料科学领域的最新工作已经使用此类模型来有效预测未来的知识发现，比科学家提出的知识发现早几十年（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B173">Tshitoyan 等人，2019 年</a>；<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B161">Sourati 和 Evans，2021 年</a>）。其他工作表明，这些发现可以推广到生物和物理科学（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B154">Shi 和 Evans 2023</a>）。概念空间的明确表示可以对整个社会系统中知识的特征和结构进行详细的调查。一方面，这些模型就像望远镜一样，打开了知识的天空，使其大规模结构变得可见，以供研究、理论发展和完善。另一方面，这些模型充当显微镜，使我们能够更深入地观察构成更大知识系统的意义原子结构。测量方面的这一进步将丰富对定义人类和组织经验的大型多维知识系统中的机制的测试。它还将使我们能够递归地评估管理和组织奖学金的知识，从而刺激创新。</p>
<br>
<h3 id="63-词嵌入和实证研究">6.3 词嵌入和实证研究</h3>
<p>在<em>实证层面</em>，词嵌入模型可以提高组织科学不同领域的测量保真度，从而在实证结果与理论主张和框架之间实现更好的映射。我们用团队和群体内部多样性研究的例子来说明这一点。据说，不同群体所获得的许多好处是由于群体中的个人代表问题和解决方案的方式不同而产生的（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B75">Hong 和 Page 2004</a>）。由具有不同方法的个人组成的小组将更好地执行各种任务，因为他们将拥有更广泛的知识、观点和可供借鉴的信息资源（Cox et al. 1991，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B27">Williams</a> and <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B185">O&rsquo;Reilly 1998</a>）。然而，由于测量困难，对团队多样性的研究很少测量问题和解决方案空间的不同概念。相反，它假设解决问题的团队成员的身份多样性（人口、文化、种族或经验）与其功能多样性（团队成员如何代表和解决问题）之间存在联系（Nisbett 和 Ross 1980，Hong<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B122">和</a>Page <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B75">2004</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175">van Dijk 等人，2017</a>）。</p>
<p>由于缺乏高保真方法来访问团队成员在问题和解决方案的概念空间中的位置，因此通常假定身份和功能多样性之间存在联系。用于操作研究的身份多样性和用于理论化的功能多样性之间脱节的一个重要后果是，虽然理论积极使用功能多样性的思想和术语（从根本上讲是几何和高维的），但测试依赖于集合-与身份成员资格相关的理论概念。<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175">我们预测，这将解释团队多样性文献（ van Dijk et al. 2017</a> ）结果中的大部分歧义，因为研究设计忽视了功能多样性和身份多样性之间的同源性。然而，诸如概念广度之类的衡量标准可以阐明这一理论交叉点上的悬而未决的问题。我们现在可以指定（1）团队的基本概念广度，以及（2）这种基本广度可能驱动结果的程度。解决这些问题可以为许多分析层面的研究提供信息，从个人和团队的成功（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B164">Srikanth 等人，2016 年</a>，<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B175">van Dijk 等人，2017 年</a>）到公司和行业绩效（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B144">Roberson 等人，2017 年</a>）。我们希望我们的插图能够激发在组织研究领域生成细粒度意义测量的新可能性。</p>
<br>
<h3 id="64-组织内部的词嵌入">6.4 组织内部的词嵌入</h3>
<p><strong>最后，我们认为词嵌入方法将对我们研究的组织产生影响</strong>。我们说明了在劳动力市场背景下潜在的嵌入必须塑造组织行为。从招聘到工作设计，从培训到晋升，人力资源管理的一个核心挑战是有效地将个人与组织内的角色、工作、情况和任务相匹配（Weller et al. 2019 <a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B183">）</a>。随着比赛质量的提高，各种绩效指标也会提高，包括工作满意度（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B8">Ashforth 和 Saks 1996</a>）、个人生产力（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B125">Paauwe 2009</a>）和组织绩效（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B38">Dyer 和 Reeves 1995</a>）。有效匹配的一个问题是不同维度的匹配的重要性程度。在一家公司中，技能可能最为重要，而在其他公司中，技能可能是文化契合度、态度、技能和经验的相互作用。由于嵌入模型捕获了所有这些维度，管理者可以为每个相关维度嵌入不同的原型描述，同时还嵌入个人资料和其他相关通信（例如电子邮件、松弛消息等），以衡量每个人与每个相关维度之间的匹配接近度。这样做可以让管理者更好地识别高维匹配及其对员工、社区和公司绩效的影响。</p>
<p>嵌入模型旨在为人力资源的宏观管理提供新的视角（<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B183">Weller et al. 2019</a>）。来自大型组织的相关信息存储在人力资源经理、一线经理、员工、同事和外部招聘人员中。然而，无法集中访问这些信息。通过嵌入，组织可以从所有数字面包屑的文本（电子邮件、聊天、工作描述、正式报告、绩效管理记录等）构建概念空间。这样做并使用相似性分析将使公司能够绘制和了解相关人力资本的位置位于公司对面。管理人员可以利用这些系统来准确了解任何员工的概念职位与任何给定的公司要求的差距有多大。这不仅可以为招聘、雇用、员工流动和流动等流程提供信息，还可以为培训、社交、工作设计和公司重组提供信息。因此，在劳动力市场和组织适应的背景下，嵌入模型可以产生有用的创新。人们可以想象许多其他组织实践和结构可以从这些模型及其测量可能性中受益，包括产品设计、市场分析和战略生成。</p>
<p><br><br></p>
<h2 id="结论">结论</h2>
<p>我们同意<a href="https://pubsonline.informs.org/doi/full/10.1287/orsc.2023.1686#B65">Hanan等人的观点。（2019</a>，第 2 页）当他们观察到，考虑到概念和分类对几乎所有人类行为和社会互动的中心地位，人们对概念如何运作的关注如此之少，这是多么令人惊讶。现代组织内部及其周围进行的许多活动都需要概念信息的激活和传播。当一个人解决新问题、提出新想法或与他人合作时，就会发生这种情况。从围绕饮水机的良性闲聊到重新配置全球资本主义秩序或将人类登陆火星，概念及其所嵌入的概念空间发挥着核心、关键的作用。</p>
<p>正如本文所示，我们现在拥有一系列重要的工具，可以为广泛而深入的理论想象和实证研究打开<strong>概念世界</strong>和<strong>概念空间</strong>。我们希望本文能够激发对嵌入可以提供信息的大量问题和理论的学术探索。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>MS2022 | 使用语言差异性测量团队认知差异性</title>
      <link>https://textdata.cn/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/</link>
      <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-02-measure-cognitive-diversity-through-language-discursive-diversity/</guid>
      <description>&lt;p&gt;词嵌入在经管中的应用很多，但大多数是训练词嵌入模型，依据词嵌入构建或扩展词典。 今天我们将分享一篇用词嵌入测量团队认知多样性。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/paper-cover-discursive-diversity.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h2 id=&#34;一研究&#34;&gt;一、研究&lt;/h2&gt;
&lt;p&gt;Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &amp;ldquo;&lt;strong&gt;Aligning differences: Discursive diversity and team performance.&lt;/strong&gt;&amp;rdquo; &lt;em&gt;Management Science&lt;/em&gt; 68, no. 11 (2022): 8430-8448.&lt;/p&gt;
&lt;h3 id=&#34;11-摘要&#34;&gt;1.1 摘要&lt;/h3&gt;
&lt;p&gt;团队中的认知多样性如何影响其绩效？先前的研究表明，团队的认知多样性存在绩效权衡：多样性团队在创造力和创新方面表现出色，但在协调行动方面则有困难。基于团队认知不是静态的，而是动态互动产生的观点，我们引入了 &lt;strong&gt;话语多样性&lt;/strong&gt; 的概念，这是团队认知多样性的一种表现，反映了在一组互动中团队成员传达的含义在多大程度上相互不同。&lt;strong&gt;我们提出，高绩效团队是那些具有调节共享认知以适应不断变化的任务要求的集体能力的团队：在进行构思任务时，它们表现出更高的话语多样性，在执行协调任务时，表现出较低的话语多样性&lt;/strong&gt;。我们进一步认为，表现出一致调节的团队——即，在成员对不断变化的任务要求的个人语义变化中团队层面方差较低的团队——更有可能取得成功，而不是由成员之间存在不一致的调节。我们利用 &lt;strong&gt;计算语言学&lt;/strong&gt; 工具来衡量话语多样性，并借助一组新型纵向数据，包括117个在线平台 &lt;a href=&#34;http://www.gigster.com&#34;&gt;www.gigster.com&lt;/a&gt; 上的远程软件开发团队的团内电子通信和绩效结果，得出了对我们理论的支持。我们的研究结果表明，团队认知多样性的绩效权衡并非不可避免：团队可以通过将话语多样性水平与任务要求相匹配以及在进行这些调整时使成员保持一致来应对这一权衡。&lt;/p&gt;
&lt;h3 id=&#34;12-创新点&#34;&gt;1.2 创新点&lt;/h3&gt;
&lt;p&gt;这篇论文的创新点主要包括以下几个方面：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;研究了团队内部的差异对团队绩效的影响&lt;/strong&gt;：该论文通过分析团队成员之间的差异，探讨了这些差异对团队绩效的影响。这一研究角度对于理解团队内部动态和绩效提升具有重要意义。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;引入了阶段性的话语差异概念&lt;/strong&gt;：论文提出了阶段性的话语差异概念，即团队成员在不同阶段的沟通中所表现出的差异。这一概念有助于更好地理解团队内部沟通的动态过程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;探讨了团队内部沟通差异的调节作用&lt;/strong&gt;：论文研究了团队内部沟通差异与团队绩效之间的关系，并发现团队内部沟通差异在不同阶段对团队绩效的影响存在差异。这一发现为团队管理和绩效提升提供了重要的启示。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;结合了多个学科领域的理论和方法&lt;/strong&gt;：该论文综合运用了心理学、经济学和组织学等多个学科领域的理论和方法，从多个角度深入研究了团队内部差异和绩效之间的关系，为相关领域的研究提供了新的视角和方法。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二文献梳理&#34;&gt;二、文献梳理&lt;/h2&gt;
&lt;h3 id=&#34;21-认知多样性&#34;&gt;2.1 认知多样性&lt;/h3&gt;
&lt;p&gt;认知多样性(cognitive diversity)对团队绩效的影响是一个长期存在的问题。以往的研究表明，团队的认知多样性存在绩效权衡：多样性团队在创造力和创新方面表现出色，但在协调行动方面存在困难。然而，&lt;strong&gt;最近的研究提出了一种新的观点，即团队的「认知多样性」可以通过调节团队的「共享认知」来实现绩效的平衡。这意味着团队可以根据任务要求调整其认知多样性的水平，以在创造性任务和协调任务之间找到平衡点&lt;/strong&gt;。高绩效团队具备调节团队认知的能力，使其能够在创造性任务中展现较高的认知多样性，在协调任务中展现较低的认知多样性。这种能力使团队能够在创新和执行之间找到平衡，从而提高绩效。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-话语多样性&#34;&gt;2.2 话语多样性&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;话语多样性(discursive diversity) 是指团队成员在交流和讨论中表达的观点、意见和想法的多样性程度。它反映了团队成员在思考和表达上的差异程度。话语多样性可以包括词汇选择、句子结构、表达方式等方面的差异&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;话语多样性对团队的协调行动有影响。在协调任务中，团队成员需要相互理解、协调行动，达成共识并共同努力实现共同目标。如果团队成员的话语多样性过高，意味着他们在表达观点和意见时存在较大的差异，这可能导致沟通困难、理解不一致和冲突的产生，从而影响团队的协调行动。&lt;/p&gt;
&lt;p&gt;因此，在协调任务中，团队成员的话语多样性应该相对较低，以便更好地理解和协调彼此的行动。相反，在创意和思考任务中，话语多样性可以促进团队成员的创新和思考，帮助他们从不同的角度和观点来解决问题，从而提高团队的创造力和创新能力。总之，话语多样性在团队中起着重要的作用，它需要根据任务的性质和要求进行调节，以实现团队的协调行动和创新能力。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-两者关系&#34;&gt;2.3 两者关系&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;在这篇论文中，话语多样性被用来衡量认知多样性&lt;/strong&gt;。研究人员使用计算语言学的工具来推导出话语多样性的度量，并将其应用于团队的电子沟通数据中。他们认为，团队的话语多样性可以反映成员之间的认知多样性，即在思维方式、知识和技能等方面的差异程度。通过分析团队的话语多样性，研究人员试图探索团队在不同任务要求下的表现，并研究团队如何调节共享认知以适应任务需求的变化。因此，话语多样性被视为一种衡量团队认知多样性的指标。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三数据及方法&#34;&gt;三、数据及方法&lt;/h2&gt;
&lt;h3 id=&#34;31-数据&#34;&gt;3.1 数据&lt;/h3&gt;
&lt;p&gt;Gigster(&lt;a href=&#34;http://www.gigster.com&#34;&gt;www.gigster.com&lt;/a&gt;),是一个在线平台， 自由软件开发人员可以在该平台上为个人和企业客户制作按需软件。该平台将个人自由职业开发人员组装成由团队领导领导的临时团队，并将他们分配给需要复杂、相互依赖的长期项目。该平台上的自由职业者分布在全球各地，从事从移动到网络应用程序开发的各种项目。这些项目通常是知识密集型的，需要高水平的创造力、技术问题解决能力和人际协调能力。软件项目规模巨大，成本从数万美元到数十万美元不等（极端情况下可达一百万美元以上）。&lt;/p&gt;
&lt;p&gt;我们的数据集由 117 个团队组成，代表 421 个不同的个体（36% 为女性），时间跨度从 2015 年初到 2017 年底。一个典型的团队有 5 名成员，其中包括一名项目经理；至少一名后端、前端或“全栈”工程师；设计师；和用户界面专家。根据项目类型，团队有时还包括作家、自然语言处理工程师和其他类型的专业人士。在我们数据中的团队中，项目平均持续 159 天（中位数：150 天），并分为平均持续两周的里程碑阶段（平均：14 天；中位数：14 天）。要加入该平台，专业人士必须通过旨在验证其专业知识的各种技术面试。平均而言，单个团队的成员代表 3.6 个国家/地区（中位数：3 个）。在我们的样本中，42% 的人将其原籍国列为北美。另外 13% 来自亚洲，其次是 12% 来自欧洲。其余 23% 居住在拉丁美洲、非洲和世界其他地区。&lt;/p&gt;
&lt;p&gt;由于地理位置分散且缺乏实体办公空间，团队成员几乎完全通过名为 Slack 的在线即时通讯工具进行沟通。我们可以访问整个团队的 Slack 档案——超过 800,000 条消息。每条消息都带有时间戳并可归因（通过匿名标识符）其作者。团队在整个生命周期中平均在公共渠道中交换 1,873 条 Slack 消息（中位数：1,220 条）。我们对 Gigster 的高级领导和团队领导进行了非正式采访，他们一致表示团队沟通几乎完全通过 Slack 进行。一位高级领导描述了其中的原因：“几乎所有团队对话都发生在 Slack 上。这是一个有用的工具，因为我们运营全球团队，而且 Slack 允许在一个平台内进行实时和异步通信。它还允许轻松地共享项目文件。” 多位知情人士强调，团队成员始终依赖 Slack，而不是其他工具，因为“一切都在一个地方”对于促进团队协作非常重要。知情人士还表示，团队成员有动力使用 Slack，因为它提供了团队流程和事件的透明档案，可用于对一些罕见的争议案例进行分类。&lt;/p&gt;
&lt;p&gt;除了 Slack 消息之外，我们还可以获得有关团队成员特征（职能角色、性别和原籍国）的数据，以及团队在实现各个项目里程碑方面的整体绩效。这些数据共同构成了团队内部动态和结果的丰富且连续的历史记录。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-计算话语多样性&#34;&gt;3.2 计算话语多样性&lt;/h3&gt;
&lt;p&gt;之前的工作表明，词嵌入模型对于捕获单词之间的语义关系特别有用， 例如，(2018) 证明，根据应用于 20 世纪出版的英语书籍的词嵌入模型推断出的不同职业的语义性别关联与这些职业的历史性别构成相对应。同样，科兹洛夫斯基等人(2019)说明了不同的生活方式活动如何与阶级、种族和性别认同相关。因此，词嵌入为语言中包含的众多意义维度提供了全面且有意义的见解，而这是以前的方法无法捕获的。因此，本论文使用词嵌入模型开发了话语多样性度量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我们首先对 Slack 数据进行预处理，并使用 Word2Vec（连续词袋词嵌入模型的流行实现）来训练词嵌入模型。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;按照标准实践，窗口大小设置为10， 维度设置成200来训练word2vec模型（&lt;a href=&#34;https://pubsonline.informs.org/doi/full/10.1287/mnsc.2021.4274#B54&#34;&gt;Mikolov 等人，2013&lt;/a&gt;）。从这个训练过程中，我们获得了语料库中每个单词的一个 200 维坐标向量，表示该单词在语义空间中的位置。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;窗口大小&lt;/strong&gt;: 每个词的上下文范围。 人阅读书籍，一般视野只有十来个词，逐行阅读。 跟人类似， 在计算机中训练词嵌入模型时候，数据不是一次性灌入习得词语的向量，而是像人一样是有上下文范围的，这个范围叫做窗口。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如前所述，嵌入空间的维度表示训练语料库中语言使用的潜在特征。尽管这些维度本身不具有定性可解释的含义，但这些维度是提供信息的，因为具有更相似含义的单词彼此更接近。&lt;strong&gt;下图(a)是从聊天消息构建词嵌入向量的过程&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-compute-discursive-diverisity-with-embeddings.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;**使用词嵌入模型，我们就可以把词语、每句话、某人某时期的话、某团队某时期的话、所有团队所有的话，通过一定的计算，都表征为200维的向量。**上图 (b) 从聊天消息构建团队话语多样性得分(Discursive Diversity)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;通过公式1，计算出两个人差异性。通过公式2， 计算出团队话语多样性。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-formular-1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-formular-2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;假设团队只有三个人， 低话语多样性 与 高话语多样性， 分别对应下图的左侧和右侧。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-low-and-high-examples-of-discursive-diversity.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四python伪码&#34;&gt;四、Python伪码&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;discursive_diversity_score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# wv: 词嵌入模型; gensim.models.keyedvectors.KeyedVectors&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# words: 一个时间窗口内的词语列表&lt;/span&gt;
    
    &lt;span class=&#34;c1&#34;&gt;# 计算词嵌入向量的平均值&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;embedding_vectors&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;centroid&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embedding_vectors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;c1&#34;&gt;# 计算词嵌入向量之间的余弦相似度&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;pairwise_distances&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;centroid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linalg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;centroid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linalg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;embedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embedding&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;embedding_vectors&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    
    &lt;span class=&#34;c1&#34;&gt;# 计算语言多样性得分&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;diversity_score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mean&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pairwise_distances&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;diversity_score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;函数 &lt;em&gt;&lt;strong&gt;discursive_diversity_score&lt;/strong&gt;&lt;/em&gt; 已内置到 cntext2.x 版本中。 对cntext2.x 感兴趣，可阅读 &lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/&#34;&gt;文本分析库cntext2.x使用手册&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;五词嵌入应用文献&#34;&gt;五、词嵌入应用文献&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;# 使用词嵌入技术构建词典
[1]胡楠, 薛付婧 and 王昊楠, 2021. **管理者短视主义影响企业长期投资吗———基于文本分析和机器学习**. *管理世界*, *37*(5), pp.139-156.    
[2]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, **Measuring Corporate Culture Using Machine Learning**, *The Review of Financial Studies*,2020


# 使用词嵌入测量偏见(刻板印象)、认知
[3]Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &amp;#34;**Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.**&amp;#34; _Proceedings of the National Academy of Sciences_ 119, no. 9 (2022): e2026443119.
[4]Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &amp;#34;**Aligning differences: Discursive diversity and team performance.**&amp;#34; *Management Science* 68, no. 11 (2022): 8430-8448.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;六广告&#34;&gt;六、广告&lt;/h2&gt;
&lt;p&gt;对词嵌入技术在经管中的应用，大邓也一直持续追踪。 课程 &lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;&lt;strong&gt;Python实证指标与文本分析&lt;/strong&gt;&lt;/a&gt;中有词嵌入相关知识点和代码，对该类文本分析技术感兴趣同学，欢迎报名课程。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>词嵌入在经管中的应用很多，但大多数是训练词嵌入模型，依据词嵌入构建或扩展词典。 今天我们将分享一篇用词嵌入测量团队认知多样性。</p>
<p><br><br></p>
<p><img loading="lazy" src="img/paper-cover-discursive-diversity.png" alt=""  />
</p>
<h2 id="一研究">一、研究</h2>
<p>Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &ldquo;<strong>Aligning differences: Discursive diversity and team performance.</strong>&rdquo; <em>Management Science</em> 68, no. 11 (2022): 8430-8448.</p>
<h3 id="11-摘要">1.1 摘要</h3>
<p>团队中的认知多样性如何影响其绩效？先前的研究表明，团队的认知多样性存在绩效权衡：多样性团队在创造力和创新方面表现出色，但在协调行动方面则有困难。基于团队认知不是静态的，而是动态互动产生的观点，我们引入了 <strong>话语多样性</strong> 的概念，这是团队认知多样性的一种表现，反映了在一组互动中团队成员传达的含义在多大程度上相互不同。<strong>我们提出，高绩效团队是那些具有调节共享认知以适应不断变化的任务要求的集体能力的团队：在进行构思任务时，它们表现出更高的话语多样性，在执行协调任务时，表现出较低的话语多样性</strong>。我们进一步认为，表现出一致调节的团队——即，在成员对不断变化的任务要求的个人语义变化中团队层面方差较低的团队——更有可能取得成功，而不是由成员之间存在不一致的调节。我们利用 <strong>计算语言学</strong> 工具来衡量话语多样性，并借助一组新型纵向数据，包括117个在线平台 <a href="http://www.gigster.com">www.gigster.com</a> 上的远程软件开发团队的团内电子通信和绩效结果，得出了对我们理论的支持。我们的研究结果表明，团队认知多样性的绩效权衡并非不可避免：团队可以通过将话语多样性水平与任务要求相匹配以及在进行这些调整时使成员保持一致来应对这一权衡。</p>
<h3 id="12-创新点">1.2 创新点</h3>
<p>这篇论文的创新点主要包括以下几个方面：</p>
<ol>
<li>
<p><strong>研究了团队内部的差异对团队绩效的影响</strong>：该论文通过分析团队成员之间的差异，探讨了这些差异对团队绩效的影响。这一研究角度对于理解团队内部动态和绩效提升具有重要意义。</p>
</li>
<li>
<p><strong>引入了阶段性的话语差异概念</strong>：论文提出了阶段性的话语差异概念，即团队成员在不同阶段的沟通中所表现出的差异。这一概念有助于更好地理解团队内部沟通的动态过程。</p>
</li>
<li>
<p><strong>探讨了团队内部沟通差异的调节作用</strong>：论文研究了团队内部沟通差异与团队绩效之间的关系，并发现团队内部沟通差异在不同阶段对团队绩效的影响存在差异。这一发现为团队管理和绩效提升提供了重要的启示。</p>
</li>
<li>
<p><strong>结合了多个学科领域的理论和方法</strong>：该论文综合运用了心理学、经济学和组织学等多个学科领域的理论和方法，从多个角度深入研究了团队内部差异和绩效之间的关系，为相关领域的研究提供了新的视角和方法。</p>
</li>
</ol>
<p><br><br></p>
<h2 id="二文献梳理">二、文献梳理</h2>
<h3 id="21-认知多样性">2.1 认知多样性</h3>
<p>认知多样性(cognitive diversity)对团队绩效的影响是一个长期存在的问题。以往的研究表明，团队的认知多样性存在绩效权衡：多样性团队在创造力和创新方面表现出色，但在协调行动方面存在困难。然而，<strong>最近的研究提出了一种新的观点，即团队的「认知多样性」可以通过调节团队的「共享认知」来实现绩效的平衡。这意味着团队可以根据任务要求调整其认知多样性的水平，以在创造性任务和协调任务之间找到平衡点</strong>。高绩效团队具备调节团队认知的能力，使其能够在创造性任务中展现较高的认知多样性，在协调任务中展现较低的认知多样性。这种能力使团队能够在创新和执行之间找到平衡，从而提高绩效。</p>
<br>
<h3 id="22-话语多样性">2.2 话语多样性</h3>
<p><strong>话语多样性(discursive diversity) 是指团队成员在交流和讨论中表达的观点、意见和想法的多样性程度。它反映了团队成员在思考和表达上的差异程度。话语多样性可以包括词汇选择、句子结构、表达方式等方面的差异</strong>。</p>
<p>话语多样性对团队的协调行动有影响。在协调任务中，团队成员需要相互理解、协调行动，达成共识并共同努力实现共同目标。如果团队成员的话语多样性过高，意味着他们在表达观点和意见时存在较大的差异，这可能导致沟通困难、理解不一致和冲突的产生，从而影响团队的协调行动。</p>
<p>因此，在协调任务中，团队成员的话语多样性应该相对较低，以便更好地理解和协调彼此的行动。相反，在创意和思考任务中，话语多样性可以促进团队成员的创新和思考，帮助他们从不同的角度和观点来解决问题，从而提高团队的创造力和创新能力。总之，话语多样性在团队中起着重要的作用，它需要根据任务的性质和要求进行调节，以实现团队的协调行动和创新能力。</p>
<br>
<h3 id="23-两者关系">2.3 两者关系</h3>
<p><strong>在这篇论文中，话语多样性被用来衡量认知多样性</strong>。研究人员使用计算语言学的工具来推导出话语多样性的度量，并将其应用于团队的电子沟通数据中。他们认为，团队的话语多样性可以反映成员之间的认知多样性，即在思维方式、知识和技能等方面的差异程度。通过分析团队的话语多样性，研究人员试图探索团队在不同任务要求下的表现，并研究团队如何调节共享认知以适应任务需求的变化。因此，话语多样性被视为一种衡量团队认知多样性的指标。</p>
<p><br><br></p>
<h2 id="三数据及方法">三、数据及方法</h2>
<h3 id="31-数据">3.1 数据</h3>
<p>Gigster(<a href="http://www.gigster.com">www.gigster.com</a>),是一个在线平台， 自由软件开发人员可以在该平台上为个人和企业客户制作按需软件。该平台将个人自由职业开发人员组装成由团队领导领导的临时团队，并将他们分配给需要复杂、相互依赖的长期项目。该平台上的自由职业者分布在全球各地，从事从移动到网络应用程序开发的各种项目。这些项目通常是知识密集型的，需要高水平的创造力、技术问题解决能力和人际协调能力。软件项目规模巨大，成本从数万美元到数十万美元不等（极端情况下可达一百万美元以上）。</p>
<p>我们的数据集由 117 个团队组成，代表 421 个不同的个体（36% 为女性），时间跨度从 2015 年初到 2017 年底。一个典型的团队有 5 名成员，其中包括一名项目经理；至少一名后端、前端或“全栈”工程师；设计师；和用户界面专家。根据项目类型，团队有时还包括作家、自然语言处理工程师和其他类型的专业人士。在我们数据中的团队中，项目平均持续 159 天（中位数：150 天），并分为平均持续两周的里程碑阶段（平均：14 天；中位数：14 天）。要加入该平台，专业人士必须通过旨在验证其专业知识的各种技术面试。平均而言，单个团队的成员代表 3.6 个国家/地区（中位数：3 个）。在我们的样本中，42% 的人将其原籍国列为北美。另外 13% 来自亚洲，其次是 12% 来自欧洲。其余 23% 居住在拉丁美洲、非洲和世界其他地区。</p>
<p>由于地理位置分散且缺乏实体办公空间，团队成员几乎完全通过名为 Slack 的在线即时通讯工具进行沟通。我们可以访问整个团队的 Slack 档案——超过 800,000 条消息。每条消息都带有时间戳并可归因（通过匿名标识符）其作者。团队在整个生命周期中平均在公共渠道中交换 1,873 条 Slack 消息（中位数：1,220 条）。我们对 Gigster 的高级领导和团队领导进行了非正式采访，他们一致表示团队沟通几乎完全通过 Slack 进行。一位高级领导描述了其中的原因：“几乎所有团队对话都发生在 Slack 上。这是一个有用的工具，因为我们运营全球团队，而且 Slack 允许在一个平台内进行实时和异步通信。它还允许轻松地共享项目文件。” 多位知情人士强调，团队成员始终依赖 Slack，而不是其他工具，因为“一切都在一个地方”对于促进团队协作非常重要。知情人士还表示，团队成员有动力使用 Slack，因为它提供了团队流程和事件的透明档案，可用于对一些罕见的争议案例进行分类。</p>
<p>除了 Slack 消息之外，我们还可以获得有关团队成员特征（职能角色、性别和原籍国）的数据，以及团队在实现各个项目里程碑方面的整体绩效。这些数据共同构成了团队内部动态和结果的丰富且连续的历史记录。</p>
<br>
<h3 id="32-计算话语多样性">3.2 计算话语多样性</h3>
<p>之前的工作表明，词嵌入模型对于捕获单词之间的语义关系特别有用， 例如，(2018) 证明，根据应用于 20 世纪出版的英语书籍的词嵌入模型推断出的不同职业的语义性别关联与这些职业的历史性别构成相对应。同样，科兹洛夫斯基等人(2019)说明了不同的生活方式活动如何与阶级、种族和性别认同相关。因此，词嵌入为语言中包含的众多意义维度提供了全面且有意义的见解，而这是以前的方法无法捕获的。因此，本论文使用词嵌入模型开发了话语多样性度量。</p>
<p><strong>我们首先对 Slack 数据进行预处理，并使用 Word2Vec（连续词袋词嵌入模型的流行实现）来训练词嵌入模型。</strong></p>
<p>按照标准实践，窗口大小设置为10， 维度设置成200来训练word2vec模型（<a href="https://pubsonline.informs.org/doi/full/10.1287/mnsc.2021.4274#B54">Mikolov 等人，2013</a>）。从这个训练过程中，我们获得了语料库中每个单词的一个 200 维坐标向量，表示该单词在语义空间中的位置。</p>
<blockquote>
<p><strong>窗口大小</strong>: 每个词的上下文范围。 人阅读书籍，一般视野只有十来个词，逐行阅读。 跟人类似， 在计算机中训练词嵌入模型时候，数据不是一次性灌入习得词语的向量，而是像人一样是有上下文范围的，这个范围叫做窗口。</p>
</blockquote>
<p>如前所述，嵌入空间的维度表示训练语料库中语言使用的潜在特征。尽管这些维度本身不具有定性可解释的含义，但这些维度是提供信息的，因为具有更相似含义的单词彼此更接近。<strong>下图(a)是从聊天消息构建词嵌入向量的过程</strong>:</p>
<p><img loading="lazy" src="img/01-compute-discursive-diverisity-with-embeddings.jpeg" alt=""  />
</p>
<p>**使用词嵌入模型，我们就可以把词语、每句话、某人某时期的话、某团队某时期的话、所有团队所有的话，通过一定的计算，都表征为200维的向量。**上图 (b) 从聊天消息构建团队话语多样性得分(Discursive Diversity)。</p>
<p><strong>通过公式1，计算出两个人差异性。通过公式2， 计算出团队话语多样性。</strong></p>
<p><img loading="lazy" src="img/03-formular-1.png" alt=""  />
</p>
<p><img loading="lazy" src="img/04-formular-2.png" alt=""  />
</p>
<br>
<p>假设团队只有三个人， 低话语多样性 与 高话语多样性， 分别对应下图的左侧和右侧。</p>
<p><img loading="lazy" src="img/02-low-and-high-examples-of-discursive-diversity.jpeg" alt=""  />
</p>
<p><br><br></p>
<h2 id="四python伪码">四、Python伪码</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">discursive_diversity_score</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
    <span class="c1"># wv: 词嵌入模型; gensim.models.keyedvectors.KeyedVectors</span>
    <span class="c1"># words: 一个时间窗口内的词语列表</span>
    
    <span class="c1"># 计算词嵌入向量的平均值</span>
    <span class="n">embedding_vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">wv</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
    <span class="n">centroid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">embedding_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># 计算词嵌入向量之间的余弦相似度</span>
    <span class="n">pairwise_distances</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">centroid</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">centroid</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">embedding</span><span class="p">))</span> <span class="k">for</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="n">embedding_vectors</span><span class="p">]</span>
    
    <span class="c1"># 计算语言多样性得分</span>
    <span class="n">diversity_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pairwise_distances</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">diversity_score</span>
</code></pre></div><p>函数 <em><strong>discursive_diversity_score</strong></em> 已内置到 cntext2.x 版本中。 对cntext2.x 感兴趣，可阅读 <a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">文本分析库cntext2.x使用手册</a></p>
<br>
<br>
<h2 id="五词嵌入应用文献">五、词嵌入应用文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># 使用词嵌入技术构建词典
[1]胡楠, 薛付婧 and 王昊楠, 2021. **管理者短视主义影响企业长期投资吗———基于文本分析和机器学习**. *管理世界*, *37*(5), pp.139-156.    
[2]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, **Measuring Corporate Culture Using Machine Learning**, *The Review of Financial Studies*,2020


# 使用词嵌入测量偏见(刻板印象)、认知
[3]Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &#34;**Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.**&#34; _Proceedings of the National Academy of Sciences_ 119, no. 9 (2022): e2026443119.
[4]Lix, Katharina, Amir Goldberg, Sameer B. Srivastava, and Melissa A. Valentine. &#34;**Aligning differences: Discursive diversity and team performance.**&#34; *Management Science* 68, no. 11 (2022): 8430-8448.
</code></pre></div><p><br><br></p>
<h2 id="六广告">六、广告</h2>
<p>对词嵌入技术在经管中的应用，大邓也一直持续追踪。 课程 <a href="https://textdata.cn/blog/management_python_course/"><strong>Python实证指标与文本分析</strong></a>中有词嵌入相关知识点和代码，对该类文本分析技术感兴趣同学，欢迎报名课程。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>心理科学进展 | 语义距离与创造性思维关系的元分析</title>
      <link>https://textdata.cn/blog/2023-10-18-the-relationship-between-semantic-distance-with-creativity/</link>
      <pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-10-18-the-relationship-between-semantic-distance-with-creativity/</guid>
      <description>自然语言处理的发展为探究语义距离与创造性思维的关系提供了可靠且有效的研究方法。 近些年关于两者之间关系的研究逐渐增多, 但研究结论并不一致。本研究基于创造力联想理论及扩散激活模型, 通过元分析的方法探讨了语义距离与创造性思维的整体关系, 并且分析了以往研究结论不一致的原因。 结果显示：语义距离与创造性思维存在中等程度的正相关，二者的相关强度受到被试年龄和创造性思维不同测量指标的调节。 研究结果表明语义距离与创造性思维关系 密切, 同时解释了以往研究结论不一致的原因。 上述结果不仅能为更深入地探讨创造性思维的认知神经机制 提供新的研究视角和理论解释, 而且有助于更全面地理解语义距离与创造性思维二者的关系及其边界条件, 为更好地解释、预测和提升创造力提供科学依据和重要启示。</description>
      <content:encoded><![CDATA[<p>博客之前分享过 <a href="https://textdata.cn/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/"><strong>PNAS(含代码) | 使用语义距离测量一个人的创新力(发散思维)得分</strong></a>  , 通过语义距离测量创新力， 该教程含Python代码。今天摘抄一篇&lt;心理科学进展&gt;的论文， 帮助大家更深入了解语义距离与创造性思维之间的关系。</p>
<p><br><br></p>
<h2 id="一文献">一、文献</h2>
<p>李亚丹,杜颖,谢聪,刘春宇,杨毅隆,李阳萍,邱江.<strong>语义距离与创造性思维关系的元分析</strong>[J].心理科学进展,2023,31(04):519-534.</p>
<p>摘要:  自然语言处理的发展为探究 <strong>语义距离</strong> 与 <strong>创造性思维</strong> 的关系提供了可靠且有效的研究方法。近些年关于两者之间关系的研究逐渐增多,但研究结论并不一致。本研究基于 <strong>创造力联想理论</strong> 及扩散激活模型, 通过元分析的方法探讨了语义距离与创造性思维的整体关系,并且分析了以往研究结论不一致的原因。本文经过文献检索和筛选后获得14项研究,提取r值作为效应值(共53个效应值,4729个独立样本),并使用随机效应模型进行了元分析。**结果显示：语义距离与创造性思维存在中等程度的正相关(r=0.379, 95%CI [0.300, 0.452]); 二者的相关强度受到被试年龄和创造性思维不同测量指标的调节。**研究结果表明语义距离与创造性思维关系密切, 同时解释了以往研究结论不一致的原因。上述结果不仅能为更深入地探讨创造性思维的认知神经机制提供新的研究视角和理论解释,而且有助于更全面地理解语义距离与创造性思维二者的关系及其边界条件,为更好地解释、预测和提升创造力提供科学依据和重要启示。</p>
<p><br><br></p>
<p><strong>创造性思维</strong> 是一种高层次的思维活动, 对科学进步和社会发展具有深远的影响, 其核心的认知成分之一就是基于语义记忆的 <strong>联想能力</strong>(Acar &amp; Runco, 2014; Marron et al., 2018)。 个体的联想能力及其在进行创造性活动时的联想过程均可以通过语义距离(semantic distance)表现出来(Beaty et al., 2014; Benedek &amp; Neubauer, 2013)。因此, 语义距离是帮助我们理解创造性思维和创造性认知过程的重要手段。</p>
<p>在认知科学领域, 通常利用 <strong>心理词典</strong> (mental lexicon) 所构成的语义网络 (semantic network) 来表征语义记忆结构 (Christensen &amp; Kenett, 2021)。在语义网络中, 概念被表示为通过 “边(edge)”相互连结的“节点(node)”, 语义距离则用来表示概念与概念之间的距离, 即语义相似性 (Paulsen et al., 1996)。</p>
<p>在实证研究中研究者们常用发散思维测验(Divergent Thinking Test)来衡量创造性思维, 但发散思维测验的评分存在着一些不足, 如流畅性和独特性有较高程度的相关致使得分极易混淆、独特性评分依赖于样本等问题(Silvia et al.,2008)。因此, 除了对原有测量技术的优化和改进, 还需要提升创造力测量的客观性和准确性。 目前 已有学者提出使用语义距离来测量创造性思维, <strong>但是使用语义距离测量创造性思维这一方法的有效性还存在着争议</strong>(Marron et al., 2018; Wang et al., 2018)。</p>
<br>
<br>
<h2 id="二创造性思维及其度量">二、创造性思维及其度量</h2>
<p>创造力(creativity)是指产生新颖(original)且 适 宜 (appropriate) 产品的能力 (Kaufman &amp; Sternberg, 2010; Runco, 2002)。发散思维(Divergent Thinking)是个体针对给 定问题或提示产生多个原创想法的心理能力 (Acar &amp; Runco, 2019; Forthmann, Wilken et al., 2019), 长期以来一直是创造性思维研究中的一个 重要内容(Hocevar, 1980)。发散思维测验是迄今为止创造力研究中使用最多、应用最为广泛的主流测验形式(Plucker &amp; Makel, 2010; Reiter-Palmon et al., 2019)。</p>
<p>在以往研究中, Guilford (1950)的 <strong>多用途任务</strong> (Alternate Use Task, AUT)和 Torrance (1972) 的 <strong>创造性思维测试</strong> (Torrance Tests of Creative Thinking, TTCT)使用频率较高。<strong>发散思维通常包括 4 个维度, 即流畅性、灵活性、独特性(或 独创性)和精致性</strong>。其中, 流畅性(fluency)指给出 的想法或解决方案的数量; 灵活性(flexibility)指 想法的多样性; 独特性(originality)指想法的不寻 常或唯一性; 精致性(elaboration)指给出想法或答案的详细程度(Torrance, 1965, 1988)。 在评分时, 发散思维测验也常从这四个维度来计分, 并由此衡量被试答案的创造性水平。但发散思维测试存在一些潜在问题，如无法进一步探讨创造性思维过程的手段(Hass, 2017; Marron et al., 2018)。其次， 四个主要的评价指标， 除了流畅性能够被客观测量，其余三个指标的传统评分方法存在一定弊端。第三， 在发散思维测验计分时， 流畅性和独创性的得分容易混淆。以上三点导致发散思维测验的客观性、信度饱受争议(Benedek &amp; Neubauer, 2013)。</p>
<p><br><br></p>
<h2 id="三语义距离与创造性思维的关系">三、语义距离与创造性思维的关系</h2>
<p>语义距离这个概念来源于 Collins 和 Loftus (1975)提出的<strong>扩散激活模型</strong>(Spreading-Activation Model)。 在概念与概念之间, 共同的定义性特征 越多, 它们之间的关系就越近, 这个关系就称为 语义距离(Volle, 2018)。例如, “雪”和“白”经常共同出现在文本中, 所以语义距离较小; 相反, “雪”和 “石油”很少同时出现, 因此二者之间具有较大的 语义距离。</p>
<p>Mednick 在 1962 年提出了 <strong>创造力联想理论</strong> (Associative Theory of Creativity), 该理论解释了创造性思维与语义记忆结构之间的关系(Mednick, 1962)。 该理论认为,  创造性思维涉及将弱相关或远距离概念联接成新颖且有用的概念的认知过程。 如果某些概念在语义层面相距越远, 由它们所产生的新的组合就越有创意, 新颖度越高。</p>
<p>Benedek 等人(2012)在 Mednick (1962)的理论 基础上提出, 解离能力(dissociative ability)和联想整合能力(associative combination ability)是与创造性思维密切相关的基本认知能力。</p>
<ul>
<li><strong>解离能力</strong> 是指生成不相关的概念的能力, 也可以被理解为一 种语义抑制能力, 它有助于人们获得新的语义距离遥远的概念。</li>
<li><strong>联想整合能力</strong> 指的是对看似不相关的概念形成合理联想的能力。</li>
</ul>
<p>据此我们可以推断, 语义距离作为概念与概念之间关系的量化指 标(Volle, 2018), 也即衡量个体联想能力的指标, 可以有效反映个体以联想过程为基础的创造性思维。 扩散激活模型也提到, 有创造力的人拥有更加复杂(Collins &amp; Loftus, 1975; Gruszka &amp; Necka, 2002; Kenett, 2019) 、 更加灵活的语义网络 (Schilling, 2005)。</p>
<p>近年来语义距离也开始作为测量创造性表现 的指标。 研究者们(Green et al., 2012; Prabhakaran et al., 2014; Weinberger et al., 2016)在探究状态创 造力(state creativity)时, 通常将语义距离作为创 造力水平高低的测量指标。 状态创造力即被试在 不同指导语或线索提示下所表现出的不同创造力 水平。也有研究利用语义距离来测量创造性思维, 结果显示相比较传统测量方法, 基于语义距离的 测量方法在创造性思维各指标间有着更好的区分 效度和结构信度(Dumas &amp; Dunbar, 2014)。</p>
<p>此外, 通过对语义距离的应用,  研究者能够对创造性思维的质量有更为客观的认识, 从而更 好地探讨创造性思维的认知神经机制。 人们普遍认为, 创造性思维认知过程需要 <strong>联想过程</strong> (associative processes) 与 <strong>执行过程</strong> (executive processes)的耦合(Silvia et al., 2013)。 目前, 大多 数创造性思维任务并未区分这两种认知过程 (Mednick, 1968; Runco et al., 2016), 而对这两种 认知过程的细分有助于我们更深入地理解创造性思维和创造性认知过程(Fox et al., 2015)。 而语义距离作为联想能力的衡量指标, 可以更好地反映 出个体在进行创造性思维任务时的联想过程 (Beaty, Nusbaum et al., 2014; Beaty, Silvia et al., 2014; Marron et al., 2018)。因此, 语义距离也被用来作为认知神经科学研究中创造性思维的测量指标, 它不仅可以用于比较个体在产生不同创造性 水平的答案时其大脑激活模式的差异(Beaty et al., 2017; Green et al., 2015; Tempest &amp; Radel, 2019)及个体的创造性表现随时间动态变化 (Green, 2016), 还可以用来研究不同个体之间的创造力水平差异(Green, 2016)。</p>
<p><br><br></p>
<h2 id="四年龄可能调节语义距离与创造性思维关系">四、年龄可能调节语义距离与创造性思维关系</h2>
<p>近几年, 国内外开展了一些语义距离与创造性思维关系的研究, 但是研究结果却不尽相同。 这可能与研究对象的人口学因素(年龄)和评估创 造性思维时所使用的测量指标有关。</p>
<p>根据已有研究, 年龄可能会影响语义距离与创造性思维之间的关系。</p>
<p><strong>首先</strong>, 年龄与语言能力和词汇量有关。 老年人的词汇量及语义知识存储与年轻人相比更加丰富(Kavé &amp; Halamish, 2015; Verhaeghen, 2003), 而语言能力较强、词汇量较多 的个体在表达想法时更不容易受到表达能力的限制, 因此往往在言语创造性任务中表现得更好。 语言能力较强的个体也可能会有更多的认知资源用来产生创造性想法(Wu et al., 2005)。  <strong>其次</strong>, 不同年龄被试的语义结构和语义记忆也是不同的, 例如, 老年人语义记忆中的概念更加模块化, 也更分散(Dubossarsky et al., 2017; Wulff et al., 2019; Zortea et al., 2014)。因此, 样本群体的年龄 可能会影响语义距离与创造性思维之间的关系。</p>
<p><br><br></p>
<h2 id="五元分析结果">五、元分析结果</h2>
<h3 id="51-语义距离测量创造性思维的有效性">5.1 语义距离测量创造性思维的有效性</h3>
<p><strong>本研究结果显示 , 语义距离与创造性思维呈显著正相关 (r = 0.379, p &lt; 0.001), 与以往研究结 果一致 (Hass, 2017; Heinen &amp; Johnson, 2018) 。该结果进一步验证了 Mednick (1962)提出的创造力联想理论 , 即如果某些概念在语义层面相距越远 , 由它们所产生的新的组合就越有创意新颖性越高</strong>。</p>
<p>语义距离作为一种连续变量 , 可以更精准地反映出创造性思维的定量变化 , 而不仅仅是二元对比 ( 例如 , 创造性与非创造性条件 ) (Kenett et al., 2017; Kenett, 2018; Kenett, 2019)。 因此 , 语义距离具有测量创造性思维的独特优势 (Green, 2016)。</p>
<p>然而 , 本研究发现 , 语义距离与创造性思维 关系的效应值为 0.379, 仍处于中等程度的正相关 (Cohen, 1988) 。 这说明尽管使用语义距离测量创 造性思维有一定的有效性 , 但是语义距离对创造 性思维的代表程度有限。</p>
<br>
<h3 id="52-语义距离与创造性思维关系中存在的调节效应">5.2 语义距离与创造性思维关系中存在的调节效应</h3>
<p><strong>被试年龄对语义距离与创造性思维的关系具有显著的调节作用，二者的相关性随着年龄的增加而逐渐降低</strong>。 原因可能在于 , 随着年龄的增长 , 个体的语义记忆结构和知识储备也在逐渐发生改变 , 从而影响了语义距离与创造性思维的关系。首先是语义记忆结 构的变化。 个体的语义记忆结构会随着年龄的增 长而逐渐变得稀疏 (Dubossarsky et al., 2017; Wulff et al., 2019; Zortea et al., 2014)。其次 是个体的知识储备和生活经验的变化。 常见的言语类创造性思维任务介于现实问题任务和图形任 务之间 , 完成这类任务需要一定的知识储备 (Wu et al., 2005)。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>实验 | 互联网黑话与MD&amp;A</title>
      <link>https://textdata.cn/blog/2023-04-26-chinese-it-industry-slangs-words/</link>
      <pubDate>Wed, 26 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-04-26-chinese-it-industry-slangs-words/</guid>
      <description>&lt;p&gt;最近大邓意外发现，使用mda预训练语言模型扩展互联网黑近义词，模型返回的有鼻子有眼的，这意味着上市公司高管在md&amp;amp;a中可能频繁使用了互联网黑话。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一互联网黑话&#34;&gt;一、互联网黑话&lt;/h2&gt;
&lt;h3 id=&#34;二字动词&#34;&gt;二字动词&lt;/h3&gt;
&lt;p&gt;复盘，赋能，沉淀，倒逼，落地，串联，协同，反晡，兼容，包装，重组，履约，晌应，量化，发力，布局，联动，细分，梳理，输出，加速，共建，支撑，融合，聚合，集成，对齐，对标，对焦，拆解，拉通，抽象，摸索，提炼，打通，打透，吃透，迁移，分发，分层，分装，穿梭，辐射，围绕，复用，渗透，扩展，开拓。&lt;/p&gt;
&lt;h3 id=&#34;二字名词&#34;&gt;二字名词&lt;/h3&gt;
&lt;p&gt;漏斗，中台，闭环，打法，拉通，纽带，矩阵，刺激，规模，场景，聚焦，维度，格局，形态，生态，话术，体系，抓手，赛道，认知，玩法，体感，感知，调性，心智，战役，合力，心力。&lt;/p&gt;
&lt;h3 id=&#34;三字名词&#34;&gt;三字名词&lt;/h3&gt;
&lt;p&gt;颗粒度，感知度，方法论，组合拳，引爆点，点线面，精细化，差异化，平台化，结构化，影响力，耦合性，易用性，一致性，端到端，短平快。&lt;/p&gt;
&lt;h3 id=&#34;四字名词&#34;&gt;四字名词&lt;/h3&gt;
&lt;p&gt;生命周期，价值转化，强化认知，资源倾斜，完善逻辑，抽离透传，复用打法，商业模式，快速响应，定性定量，关键路径，去中心化，结果导向，垂直领域，如何收口，归因分析，体验度量，信息屏障。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二模型近义词&#34;&gt;二、模型近义词&lt;/h2&gt;
&lt;p&gt;之前分享过一个中文金融领域的word2vec预训练语言模型，这里就不详细介绍模型参数。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/&#34;&gt;使用中文MD&amp;amp;A数据集训练word2vec预训练模型， 可扩展或新建会计金融等领域的情感词典&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;文本分析最常用的方法是词典法(例如，LIWC)，而词向量模型可以帮助我们扩展或者构建概念情感词典。&lt;/p&gt;
&lt;p&gt;现在给大家演示只给一个词，返回topn个语义最相关的词。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 与 seedwords 最相关的前topn个词&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# wv是预训练语言模型&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;复盘&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;复盘&amp;#39;,
 &amp;#39;检视&amp;#39;,
 &amp;#39;检讨&amp;#39;,
 &amp;#39;KPI&amp;#39;,
 &amp;#39;考核评估&amp;#39;,
 &amp;#39;量化考核&amp;#39;,
 &amp;#39;跟踪考核&amp;#39;,
 &amp;#39;纠偏&amp;#39;,
 &amp;#39;过程跟踪&amp;#39;,
 &amp;#39;分析总结&amp;#39;,
 &amp;#39;KPI指标&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;赋能&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;赋能&amp;#39;,
 &amp;#39;技术赋能&amp;#39;,
 &amp;#39;全面赋能&amp;#39;,
 &amp;#39;平台赋能&amp;#39;,
 &amp;#39;科技赋能&amp;#39;,
 &amp;#39;助力&amp;#39;,
 &amp;#39;数字化赋能&amp;#39;,
 &amp;#39;数据赋能&amp;#39;,
 &amp;#39;数智化&amp;#39;,
 &amp;#39;数据驱动&amp;#39;,
 &amp;#39;生态构建&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt; &lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;感知度&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;感知度&amp;#39;,
 &amp;#39;体验度&amp;#39;,
 &amp;#39;产品认知度&amp;#39;,
 &amp;#39;知晓度&amp;#39;,
 &amp;#39;购买率&amp;#39;,
 &amp;#39;品牌黏性&amp;#39;,
 &amp;#39;满意度忠诚度&amp;#39;,
 &amp;#39;忠诚度美誉度&amp;#39;,
 &amp;#39;消费者满意度&amp;#39;,
 &amp;#39;体验满意度&amp;#39;,
 &amp;#39;好感度&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;倒逼&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;倒逼&amp;#39;, 
&amp;#39;倒逼企业&amp;#39;, 
&amp;#39;势在必行&amp;#39;, 
&amp;#39;迫使&amp;#39;, 
&amp;#39;大势所趋&amp;#39;, 
&amp;#39;促使&amp;#39;, 
&amp;#39;优胜劣汰&amp;#39;, 
&amp;#39;加速淘汰&amp;#39;, 
&amp;#39;势必&amp;#39;, 
&amp;#39;趋严&amp;#39;, 
&amp;#39;成为常态&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;闭环&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;闭环&amp;#39;,
&amp;#39;完整闭环&amp;#39;, 
&amp;#39;全链路&amp;#39;, 
&amp;#39;全链条&amp;#39;, 
&amp;#39;全流程&amp;#39;, 
&amp;#39;闭环式&amp;#39;, 
&amp;#39;端端&amp;#39;, 
&amp;#39;端到端&amp;#39;, 
&amp;#39;服务闭环&amp;#39;, 
&amp;#39;全周期&amp;#39;, 
&amp;#39;闭环管理&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;端到端&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;端到端&amp;#39;,
 &amp;#39;端端&amp;#39;,
 &amp;#39;端到端的&amp;#39;,
 &amp;#39;全链路&amp;#39;,
 &amp;#39;端端的&amp;#39;,
 &amp;#39;数字化运营&amp;#39;,
 &amp;#39;全业务流程&amp;#39;,
 &amp;#39;场景全&amp;#39;,
 &amp;#39;全链条&amp;#39;,
 &amp;#39;敏捷&amp;#39;,
 &amp;#39;全价值链&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看到， 返回的近义词都是挺互联网范儿的。 只有较为频繁使用， 语言模型才有可能捕捉到这种语义关系。这从侧面反映了近年来互联网高级黑话影响力之大。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三获取模型&#34;&gt;三、获取模型&lt;/h2&gt;
&lt;p&gt;模型训练不易， 为付费资源，如需使用请 &lt;a href=&#34;https://mp.weixin.qq.com/s/zle9-BR-ei1V8Nmul19_7w&#34;&gt;&lt;strong&gt;点击进入跳转购买链接&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;期待合作&#34;&gt;期待合作&lt;/h2&gt;
&lt;p&gt;cntext目前仍在技术迭代，版本2.0.0综合了训练语言模型&amp;amp;多语言模型对齐， 有较大的应用价值，期待有独特文本数据集交流合作。&lt;/p&gt;
&lt;p&gt;通过cntext2.0.0，理论上可以对文本所涉社会主体进行计算，适合企业文化、品牌印象、旅游目的地形象、国家形象等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同主体不同时间段， 文本中蕴含的文化态度认知变迁，&lt;/li&gt;
&lt;li&gt;或同时间段，不同主体的大样本文本蕴含的差异性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>最近大邓意外发现，使用mda预训练语言模型扩展互联网黑近义词，模型返回的有鼻子有眼的，这意味着上市公司高管在md&amp;a中可能频繁使用了互联网黑话。</p>
<p><br><br></p>
<h2 id="一互联网黑话">一、互联网黑话</h2>
<h3 id="二字动词">二字动词</h3>
<p>复盘，赋能，沉淀，倒逼，落地，串联，协同，反晡，兼容，包装，重组，履约，晌应，量化，发力，布局，联动，细分，梳理，输出，加速，共建，支撑，融合，聚合，集成，对齐，对标，对焦，拆解，拉通，抽象，摸索，提炼，打通，打透，吃透，迁移，分发，分层，分装，穿梭，辐射，围绕，复用，渗透，扩展，开拓。</p>
<h3 id="二字名词">二字名词</h3>
<p>漏斗，中台，闭环，打法，拉通，纽带，矩阵，刺激，规模，场景，聚焦，维度，格局，形态，生态，话术，体系，抓手，赛道，认知，玩法，体感，感知，调性，心智，战役，合力，心力。</p>
<h3 id="三字名词">三字名词</h3>
<p>颗粒度，感知度，方法论，组合拳，引爆点，点线面，精细化，差异化，平台化，结构化，影响力，耦合性，易用性，一致性，端到端，短平快。</p>
<h3 id="四字名词">四字名词</h3>
<p>生命周期，价值转化，强化认知，资源倾斜，完善逻辑，抽离透传，复用打法，商业模式，快速响应，定性定量，关键路径，去中心化，结果导向，垂直领域，如何收口，归因分析，体验度量，信息屏障。</p>
<p><br><br></p>
<h2 id="二模型近义词">二、模型近义词</h2>
<p>之前分享过一个中文金融领域的word2vec预训练语言模型，这里就不详细介绍模型参数。</p>
<p><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">使用中文MD&amp;A数据集训练word2vec预训练模型， 可扩展或新建会计金融等领域的情感词典</a></p>
<br>
<p>文本分析最常用的方法是词典法(例如，LIWC)，而词向量模型可以帮助我们扩展或者构建概念情感词典。</p>
<p>现在给大家演示只给一个词，返回topn个语义最相关的词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 与 seedwords 最相关的前topn个词</span>
<span class="c1"># wv是预训练语言模型</span>
<span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;复盘&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;复盘&#39;,
 &#39;检视&#39;,
 &#39;检讨&#39;,
 &#39;KPI&#39;,
 &#39;考核评估&#39;,
 &#39;量化考核&#39;,
 &#39;跟踪考核&#39;,
 &#39;纠偏&#39;,
 &#39;过程跟踪&#39;,
 &#39;分析总结&#39;,
 &#39;KPI指标&#39;]
</code></pre></div> <br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;赋能&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;赋能&#39;,
 &#39;技术赋能&#39;,
 &#39;全面赋能&#39;,
 &#39;平台赋能&#39;,
 &#39;科技赋能&#39;,
 &#39;助力&#39;,
 &#39;数字化赋能&#39;,
 &#39;数据赋能&#39;,
 &#39;数智化&#39;,
 &#39;数据驱动&#39;,
 &#39;生态构建&#39;]
</code></pre></div> <br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;感知度&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;感知度&#39;,
 &#39;体验度&#39;,
 &#39;产品认知度&#39;,
 &#39;知晓度&#39;,
 &#39;购买率&#39;,
 &#39;品牌黏性&#39;,
 &#39;满意度忠诚度&#39;,
 &#39;忠诚度美誉度&#39;,
 &#39;消费者满意度&#39;,
 &#39;体验满意度&#39;,
 &#39;好感度&#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;倒逼&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;倒逼&#39;, 
&#39;倒逼企业&#39;, 
&#39;势在必行&#39;, 
&#39;迫使&#39;, 
&#39;大势所趋&#39;, 
&#39;促使&#39;, 
&#39;优胜劣汰&#39;, 
&#39;加速淘汰&#39;, 
&#39;势必&#39;, 
&#39;趋严&#39;, 
&#39;成为常态&#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;闭环&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;闭环&#39;,
&#39;完整闭环&#39;, 
&#39;全链路&#39;, 
&#39;全链条&#39;, 
&#39;全流程&#39;, 
&#39;闭环式&#39;, 
&#39;端端&#39;, 
&#39;端到端&#39;, 
&#39;服务闭环&#39;, 
&#39;全周期&#39;, 
&#39;闭环管理&#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;端到端&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;端到端&#39;,
 &#39;端端&#39;,
 &#39;端到端的&#39;,
 &#39;全链路&#39;,
 &#39;端端的&#39;,
 &#39;数字化运营&#39;,
 &#39;全业务流程&#39;,
 &#39;场景全&#39;,
 &#39;全链条&#39;,
 &#39;敏捷&#39;,
 &#39;全价值链&#39;]
</code></pre></div><p>可以看到， 返回的近义词都是挺互联网范儿的。 只有较为频繁使用， 语言模型才有可能捕捉到这种语义关系。这从侧面反映了近年来互联网高级黑话影响力之大。</p>
<p><br><br></p>
<h2 id="三获取模型">三、获取模型</h2>
<p>模型训练不易， 为付费资源，如需使用请 <a href="https://mp.weixin.qq.com/s/zle9-BR-ei1V8Nmul19_7w"><strong>点击进入跳转购买链接</strong></a></p>
<p><br><br></p>
<h2 id="期待合作">期待合作</h2>
<p>cntext目前仍在技术迭代，版本2.0.0综合了训练语言模型&amp;多语言模型对齐， 有较大的应用价值，期待有独特文本数据集交流合作。</p>
<p>通过cntext2.0.0，理论上可以对文本所涉社会主体进行计算，适合企业文化、品牌印象、旅游目的地形象、国家形象等</p>
<ul>
<li>同主体不同时间段， 文本中蕴含的文化态度认知变迁，</li>
<li>或同时间段，不同主体的大样本文本蕴含的差异性</li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 14000&#43;篇心理学顶刊论文可复现性调研</title>
      <link>https://textdata.cn/blog/2023-03-31-pnas-measure-replicability-of-psychology-with-ml/</link>
      <pubDate>Fri, 31 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-31-pnas-measure-replicability-of-psychology-with-ml/</guid>
      <description>”可复现研究的数量远远低于科学界期望，我们创建了一个基于文本数据的机器学习模型，估计了自2000年以来心理学六个子领域中发布的超过14,000篇文章的可复现性分析。此外，我们还调查了可复现性与不同的研究方法、作者的生产力、引用影响力和机构声誉、论文的引用增长和社交媒体覆盖率有关的变化。我们的研究结果有助于建立大规模的经验模式，以便为推进复现研究提供依据。The number of manually replicated studies falls well below the abundance of important studies that the scientific community would like to see replicated. We created a text-based machine learning model to estimate the replication likelihood for more than 14,000 published articles in six subfields of Psychology since 2000. Additionally, we investigated how replicability varies with respect to different research methods, authors &amp;#39;productivity, citation impact, and institutional prestige, and a paper’s citation growth and social media coverage. Our findings help establish large-scale empirical patterns on which to prioritize manual replications and advance replication research.“</description>
      <content:encoded><![CDATA[<p>Youyou, W., Yang, Y., &amp; Uzzi, B. (2023). <strong>A discipline-wide investigation of the replicability of Psychology papers over the past two decades</strong>. <em>Proceedings of the National Academy of Sciences</em>, <em>120</em>(6), e2208863120.</p>
<h2 id="意义">意义</h2>
<p>可复现研究的数量远远低于科学界期望，我们创建了一个基于文本数据的机器学习模型，估计了自2000年以来心理学六个子领域中发布的超过14,000篇文章的可复现性分析。此外，我们还调查了可复现性与不同的研究方法、作者的生产力、引用影响力和机构声誉、论文的引用增长和社交媒体覆盖率有关的变化。我们的研究结果有助于建立大规模的经验模式，以便为推进复现研究提供依据。</p>
<h2 id="本文章节">本文章节</h2>
<p>大家可以选择感兴趣部分阅读。<strong>该论文提供了词向量模型文件，如果是做社科研究的同学，可以下载下来，探索下词向量。例如查看概念近义词</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">一、摘要
二、代码
2.1 训练集
2.2 测试集数据
2.3 词向量模型
三、正文
四、数据和方法 
4.1 机器学习模型
4.2 关于模型迁移学习是否会预测不准问题
4.3 评估可复现性前后出版的相关度量。
五、结果
六、讨论
</code></pre></div><br>
<h2 id="一摘要">一、摘要</h2>
<p>对社会科学中较弱的可复现性的猜测使学者们渴望量化这一学科的复现失败的规模和范围。然而，仅靠小规模的人工复现方法无法应对这个大数据问题。在这里，我们进行了一项跨学科的科学复现普查。<strong>我们的样本（N=14,126篇论文）涵盖了近20年来发表在六个顶级心理学期刊上的几乎所有论文</strong>。使用一个经过验证的机器学习模型，估计论文的复现可能性，我们发现证据既支持又反驳了从相对较小的人工复现样本中得出的推测。首先，我们发现单一的心理学复现率不能很好地捕捉不同子领域中可复现性的程度变化。其次，我们发现，在所有子领域中，复现率与研究方法强相关。<strong>实验法的复现率明显低于非实验研究的复现率</strong>。第三，我们发现作者的<strong>累积出版物数量和引用影响力与复现可能性呈正相关，而其他反映研究质量和严谨性的指标，如作者的大学声誉和论文的引用，与可复现性无关</strong>。最后，与媒体注意应该关注可复现性的研究的理想相反，我们发现<strong>媒体的注意力与复现失败的可能性呈正相关</strong>。我们对可复现性的规模和范围的评估是广泛解决可复现性问题的重要下一步。</p>
<p><br><br></p>
<h2 id="二代码">二、代码</h2>
<p>论文的数据及代码 <a href="https://osf.io/f5sxn/">下载地址</a></p>
<h3 id="21-训练集">2.1 训练集</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#标注的训练集数据388篇论文(314个实验论文+72个非实验研究)</span>
<span class="n">training_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;training_sample.csv&#34;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_df</span><span class="p">))</span>
<span class="n">training_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">388
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
<br></p>
<h3 id="22-测试集数据">2.2 测试集数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#带预测的14125篇论文数据  </span>
<span class="n">prediction_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;prediction_sample.csv&#34;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prediction_df</span><span class="p">))</span>
<span class="n">prediction_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">14126
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<h3 id="23-词向量模型">2.3 词向量模型</h3>
<p>词向量设置为200维， 训练得到的模型文件638M。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1"># 导入模型文件</span>
<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;mag_200d_psy_eco_word2vec&#39;</span><span class="p">)</span>

<span class="c1">#词汇量</span>
<span class="nb">len</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">275561
</code></pre></div><br>
<p>查看词向量模型的近义词，如果想用expand_dictionary函数，请阅读</p>
<p><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/"><strong>预训练模型 | 金融会计类word2vec， 可扩展或构建领域内概念情感词典</strong></a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看幸福的同义词</span>
<span class="c1">#这个函数是我自己定义的，需要的点击上方说明的链接</span>
<span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;happiness&#39;</span><span class="p">],</span> 
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;happiness&#39;,
 &#39;happiness,&#39;,
 &#39;happiness.&#39;,
 &#39;well-being&#39;,
 &#39;well-being,&#39;,
 &#39;swb&#39;,
 &#39;contentment&#39;,
 &#39;wellbeing&#39;,
 &#39;peacefulness,&#39;,
 &#39;well-being:&#39;,
 &#39;happiness;&#39;,
 &#39;life-satisfaction,&#39;,
 &#39;gratitude&#39;,
 &#39;wellbeing,&#39;,
 &#39;unhappiness&#39;,
 &#39;eudaimonic&#39;,
 &#39;life-satisfaction&#39;,
 &#39;happier&#39;,
 &#39;loneliness&#39;,
 &#39;gratitude,&#39;,
 &#39;happiness:&#39;,
 &#39;peacefulness.&#39;,
 &#39;flourishing,&#39;,
 &#39;contentment,&#39;,
 &#39;materialism&#39;,
 &#39;swb,&#39;,
 &#39;flourishing&#39;,
 &#39;joy&#39;,
 &#39;vitality&#39;,
 &#39;gratefulness&#39;,
 &#39;eudemonic&#39;]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mental&#39;</span><span class="p">],</span> 
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;mental&#39;,
 &#39;health&#39;,
 &#39;illness&#39;,
 &#39;backgroundmental&#39;,
 &#39;psychiatric&#39;,
 &#39;illness,&#39;,
 &#39;ofmental&#39;,
 &#39;abstractmental&#39;,
 &#39;illnesses&#39;,
 &#39;nonmental&#39;,
 &#39;mental-health&#39;,
 &#39;psychosocial&#39;,
 &#39;health/substance&#39;,
 &#39;illnesses,&#39;,
 &#39;psychological&#39;,
 &#39;health/social&#39;,
 &#39;diagnosable&#39;,
 &#39;disability&#39;,
 &#39;substance&#39;,
 &#39;ill-health&#39;,
 &#39;stigma&#39;,
 &#39;psycho-social&#39;,
 &#39;mdos.&#39;,
 &#39;health,&#39;,
 &#39;health/mental&#39;,
 &#39;non-mental&#39;,
 &#39;retardation/developmental&#39;,
 &#39;stigmatization&#39;,
 &#39;stigmatizing&#39;,
 &#39;illness.&#39;,
 &#39;disability,&#39;]
</code></pre></div><p><br><br></p>
<h2 id="三正文">三、正文</h2>
<p>可复现性对于加强科学预测和改善生活水平的战略至关重要，也是科学自我纠正的证明。<strong>卡尔·波普尔(1)得出结论，科学中的复现确保了“我们不仅仅在处理孤立的‘巧合’，而是处理具有规律性和可复现性的事件，这些事件从原则上来说是可以经过主体间的测试的</strong>。”2011年，一项有争议的关于“时间保留因果性”的研究(2)引发了一项罕见的复现研究。复现失败(3,4)，随之而来的是更多的复现研究，发现复现失败不仅仅是单一事件(5-10)。研究人员对系统性复现失败的影响表示担忧，包括知识库的削弱、公众对科学的不信任以及资金削减(11-13)。2016年，自然杂志对1500名科学家的调查报告称，51%的受访者认为科学正在经历复现危机(14)。这一回应促使美国国防高级研究计划局在2018年创建了一个计划，研究社会科学领域复现失败的规模和范围(15-17)。</p>
<p>尽管人们越来越担心复现失败的问题，但手动复现研究的样本数量很少，只占总文献的一小部分(18,19)。<strong>在心理学这个进行复现研究最多的科学学科中，直接、独立的复现研究数量不到400个</strong>。此外，样本的选择不平衡，主要来自于选定作者或特定子领域的经典论文(20,21)。大多数复现研究来自社会心理学和认知心理学等子领域，导致有关发展心理学、临床心理学和教育心理学等领域是否存在相似的复现失败率的猜测，尽管缺乏子领域特定的分析(22,23)。</p>
<p>**为了扩大和丰富复现数据，研究人员开发了替代方法来估计论文复现成功的可能性(**24)。预测市场已成为估计论文可复现性的主要方法。它涉及让专家们打赌，是否会在未来的手动复现测试中成功复现已经发表的论文(25)。该方法的高准确性使预测市场成为估计论文可复现性的有效解决方案(16然而，尽管预测市场比手动复现更具规模化，但仍需要在多年时间内招募数千名专家评审，以预测大量论文的可复现性(17)。</p>
<p>机器学习方法也被开发出来预测复现结果。机器学习模型可以从研究的叙述文本(26)或研究的数值特征，如P值或研究的样本大小(27,28)来预测可复现性。这两种类型的模型都能够准确预测，与预测市场的预测准确度相当(26)。<strong>基于文本的模型量化论文中的叙述，包括研究设计的描述和结果的解释(29)，这些内容在仅基于数值特征的模型中无法捕捉。此外，文本量化可以自动化，从而使该技术比从手稿中手动提取数值特征更具可扩展性和可复现性</strong>。</p>
<p>本文采用基于文本的机器学习方法来预测心理学文献的可复现性。我们的样本包括六个主要子领域的顶级心理学期刊近20年发表的几乎所有论文：<strong>临床心理学、认知心理学、发展心理学、组织心理学、人格心理学和社会心理学</strong>。总共有 <strong>14,126</strong> 篇论文，由 <strong>26,349</strong> 位不同的作者和 <strong>6,173</strong> 个不同的机构撰写，共有 <strong>1,222,292</strong> 次引用和 <strong>27,447</strong> 次媒体提及。</p>
<p>我们的分析如下：我们首先简要描述了我们的基于文本的机器学习模型，该模型已经得到验证，并显示能够准确预测手动复现的结果(26)。然后，我们应用该模型来预测心理学文献的可复现性，并着眼于研究可复现性如何在心理学子领域、研究方法、出版前后的特征以及作者团队的专业知识和经验方面存在差异。</p>
<p><br><br></p>
<h2 id="四数据和方法">四、数据和方法</h2>
<p>我们的分析使用了多种不同的文献、作者和媒体报道数据来源。生成数据的数据和代码已经存放在开放科学框架(OSF)上(30)。表格1列出了用于分析的大量期刊出版物，包括五个专门子领域的期刊和一份综合期刊《Psychological Science》。</p>
<p><img loading="lazy" src="img/table1.png" alt=""  />
</p>
<p>所有论文均在2000年至2019年期间发表，并根据期刊的子领域专业化进行分类：</p>
<ul>
<li><strong>Journal of Abnormal Psychology</strong> (临床心理学)</li>
<li><strong>Journal of Experimental Psychology, Learning, Memory &amp; Cognition</strong> (认知心理学)</li>
<li><strong>Child Development</strong> (发展心理学)</li>
<li><strong>Journal of Applied Psychology</strong>（组织心理学）</li>
<li><strong>Journal of Personality and Social Psychology</strong>（社会心理学）</li>
</ul>
<p>这里有两个例外。首先，因为人格研究出现在所有顶级期刊中，如果论文标题或摘要中含有 “<strong>人格personality</strong>” 一词，无论它们出现在哪个期刊中，我们都将其标记为人格研究。其次，由于 <strong>Psychological Science</strong> 刊登来自各个子领域的作品，我们将其论文分类为主要发表在哪个子领域期刊上的论文的所属子领域。总样本包括14,126篇论文。所有数据均按照出版商的使用条款和英国版权法进行收集。</p>
<br>
<h3 id="41-机器学习模型">4.1 机器学习模型</h3>
<p><strong>机器学习模型使用了随机森林和逻辑回归模型的集成，基于论文的文本预测了论文的可复现性概率</strong>。该模型在之前的一篇文章中经过了严格的样本外测试，并被证明其准确度与预测市场相当(26)。具体而言，创建模型的步骤如下(请参见SI附录，图S2说明程序)：</p>
<ul>
<li>第1步，<strong>将英语单词转换为向量。我们使用 word2vec(31) 在Microsoft学术图(MAG)(32)的 2000万社会科学出版物摘要语料库上训练模型。目标是将单词与社会科学文献的上下文联系起来，并以一个200维向量的形式量化表示这种联系</strong>。</li>
<li>第2步，<strong>将出版物转换为向量</strong>。为此，我们将训练样本(Table 2)中每篇论文中每个单词的标准化频率乘以其相应的200维单词向量，从而产生一个表示论文文本内容的论文级向量。</li>
<li>第3步，<strong>使用随机森林和逻辑回归的集成从论文级向量中预测每篇论文的复现结果(通过/失败)</strong>。为确定一项研究是否复现，我们使用所有复现研究报告的共同指标——复现团队对研究是否复现的总结判断(“是”或“否”)。步骤1到步骤3共同创建了一个使用论文的文本/叙述来预测其可复现性概率的机器学习模型，我们称之为“<strong>复现得分”(replication score)</strong>。SI附录，补充文本3提供了所有程序的详细信息。<strong>注意，论文中使用的是监督学习算法， 训练数据集仅有386篇（314个实验论文+72个非实验研究）</strong></li>
</ul>
<br>
<h3 id="42-关于模型迁移学习是否会预测不准问题">4.2 关于模型迁移学习是否会预测不准问题</h3>
<p>论文评估了与迁移学习相关的问题。<strong>当模型在一个领域中开发并应用于另一个领域时，就会发生迁移学习</strong>(43)。我们的研究中出现了这种情况，因为预测样本包含了训练样本中没有的两个子领域——临床心理学和发展心理学。<strong>这两个子领域的手动复现研究很少，如果要积累足够的样本可能需要另一个十年时间(44)。这引发了一个担忧，即模型能否为临床心理学和发展心理学中的论文提供有效的估计</strong>。为了解决这个问题，我们遵循协议，进行了三个独立的鲁棒性测试(43, 45, 46)。</p>
<p>(i)我们使用现有的社会和认知心理学数据来模拟迁移学习过程，估计使用一个基于一个心理学子领域的手动复现数据训练的模型来预测另一个心理学子领域的复现失败的表现，并将模型的预测与预测子领域中的实际手动复现数据进行比较。具体而言，<strong>我们研究了仅基于社会心理学论文( n=256)开发的模型在认知心理学论文(n=90)上的表现</strong>。我们发现，这种<strong>迁移学习到认知心理学的表现(AUC=0.72)与将模型应用于社会心理学的表现基准(AUC=0.73)相当。这为心理学子领域之间的迁移学习成功提供了支持</strong>。</p>
<p>(ii)有人可能会认为文本模型从社会心理学到认知心理学的成功转移并不保证其能够成功转移到临床心理学或发展心理学。为回答这个问题，我们比较了各个子领域的主题和文本相似性。<strong>先前的机器学习研究表明，基于文本的模型的迁移学习在训练和应用领域的文本特征更相似时更成功</strong>(47)。因此，<strong>如果社会-临床和社会-发展的相似性与社会-认知相似或更高，我们可以期望该模型在临床或发展心理学中的效度与在认知心理学中一样</strong>。</p>
<p>为了衡量两个子领域之间的研究主题重叠程度，我们从MAG数据库中收集了测试样本中每篇论文的研究主题。为了衡量两个子领域之间的文本相似性，我们计算了余弦相似度和词移距离（WMD）。附录SI，补充文本3.4.1详细描述了这些方法。</p>
<p><strong>结果显示，临床心理学（57％）和发展心理学论文（56％）与社会心理学论文的主题重叠比认知论文（42％）更高</strong>。此外，所有三个子领域都与社会心理学表现出相等的文本相似度（余弦相似度=0.90至0.91，WMD=0.24至0.26）。因为分析（i）显示基于社会心理学构建的模型可迁移到认知心理学，我们现在可以期望该模型可迁移到临床心理学和发展心理学，因为在这些领域观察到与社会心理学更高的特征相似性。</p>
<p><strong>我们评估了在临床心理学或发展心理学论文中，预测的复现得分与样本大小和P值等可复现性的替代指标的一致性</strong>。这两个指标都是可靠性的指标，因为样本大小越大，P值越低，虚阳性的风险就越小（5, 48, 49）。我们强调，预测模型不包含任何关于样本大小和P值的信息，因为训练样本中的论文已经去除了所有数字或统计信息。因此，如果一篇论文的样本大小和P值与我们模型的复现预测相关，那么它将为模型在临床心理学或发展心理学中的适用性提供独立支持。</p>
<p>在程序上，我们手动编码了来自预测结果的临床心理学和发展心理学的一组随机研究。为了获取样本大小，我们从论文中提取了参与者的数量。如果一篇论文有多个研究，我们取所有研究的平均样本大小。为了获得P值，我们从摘要中找到了论文的第一个主要声明，并提取了与该主要声明相关联的测试的P值。主要声明通常以“结果显示”或“我们的分析表明”等短语开头。补充文本3.4.2提供了更多的方法细节。</p>
<p>结果显示，预测的复现得分与原始样本大小r(97)=0.31，P=0.002和原始P值r(91)=-0.42，P &lt;0.001的等级顺序相关。由于预测模型不包含样本大小和P值信息，因此结果不是自证的，为成功的转移学习到临床心理学和发展心理学提供了支持。</p>
<br>
<h3 id="43-评估可复现性前后出版的相关度量">4.3 评估可复现性前后出版的相关度量。</h3>
<p>为了检查复现概率与论文其他可观察的出版特征之间的联系，我们构建了几个关键的可观察特征的度量。例如，已经有假设认为复现结果与研究人员的专业知识（34）或论文的媒体关注度（50）有关。我们收集了五个度量来捕捉论文的特征，其中三个度量预测了作者团队的特征，而另外两个度量预测了读者对研究的反应。预发布特征包括论文的第一作者和高级作者的经验和能力，衡量方法是他们在发表研究论文之前的：1）发表的论文数量累计总数，2）引用影响力，3）机构声誉，基于第一作者和高级作者的大学在2021 QS世界大学排名中的排名。高级作者是指在焦点论文发表时具有最高累积引用的作者。发表后的特征包括焦点论文的4）引用计数和5）媒体提及次数。媒体提及次数由Altmetric (52)计算。所有其他度量均来自Dimensions (53)，Dimensions已经批准我们在此项目中使用这些数据。为了控制度量中的出版年龄和子领域差异（见SI Appendix，图S1），我们通过将观察得分除以其子领域和出版年份的平均值来对所有度量进行了标准化。SI附录，补充文本2介绍了有关这些度量及其如何标准化的更多详细信息。</p>
<p><br><br></p>
<h2 id="五结果">五、结果</h2>
<p>**使用上述校准的机器学习模型，我们预测了每篇文章在可复现性预测样本（n = 14,126）中的可复现性得分。该得分可以解释为可复现性成功的相对可能性。**换句话说，一个可复现性得分为0.80的论文比得分更低的论文更有可能复现，并且比可复现性得分为0.40的论文有两倍的复现概率。使用可复现性得分，我们进行了三组分析：</p>
<ul>
<li>首先，我们确定了心理学子领域在估计可复现性率方面的差异，弥合了以前手动复现实验的小样本缺陷；</li>
<li>其次，我们比较了实验和非实验研究设计之间的可复现性率；</li>
<li>第三，我们研究了可复现性与论文其他出版前和出版后特征之间的相关性。</li>
</ul>
<p><img loading="lazy" src="img/table2.png" alt=""  />
</p>
<p>图1显示了所有14,126篇心理学论文的可复现性得分分布（范围为0.10到0.86，平均值为0.42，中位数为0.41，标准差为0.15，偏度为0.31）。有几点发现值得注意：</p>
<ul>
<li>首先，该分布与手动复现实验的猜测和预测市场的最新预测大致一致（15）。手动复现实验表明，心理学论文中略多于不及格的论文（43%的总体成功率）。最近20年的心理学出版物的可复现性得分估计分布也显示出类似的模式。</li>
<li>其次，有人认为，心理学中对可复现性失败的关注已经提高了可复现性的严格性（54）。当我们绘制我们20年期间的平均可复现性得分时，发现可复现性得分相对稳定。平均可复现性得分从2000年到2010年约下降了10%，然后从2010年到2019年回升到大约与2000年相同的水平（SI Appendix，Fig.S6）——这一模式与观察到的改变研究实践可能已经提高了心理学的可复现性率的观察相一致（9, 21, 55）。</li>
<li>第三，我们发现，汇总心理学子领域的可复现性得分掩盖了重要的子领域差异。下面，我们将详细说明心理学子领域之间的可复现性率差异。</li>
</ul>
<p><br><br></p>
<h2 id="六讨论">六、讨论</h2>
<p>本研究使用机器学习模型量化科学手稿中的文本，以预测其复现概率。该模型使我们能够首次对心理学六个主要子领域杂志上发表的几乎所有论文进行复现普查，分析重点是估计整个学科的复现率，以及复现率如何因子领域、实验和非实验方法以及其他研究论文特征而异。为了保持基于人类专业知识的结果，我们在可能的情况下验证了结果与可用的手动复现数据一致。结果进一步提供了可以推进复现理论和实践的见解。</p>
<p>我们方法的一个核心优势是其规模和范围。先前关于复现失败程度的推测基于相对较小的、有选择性的手动复现样本(21)。我们分析了多个子领域的14,000多篇论文，发现复现成功率在子领域间存在广泛差异。因此，不可能用一个单一的复现失败率来表征多样的学科分支。此外，我们的结果显示，复现成功率的子领域差异与研究方法有关。我们发现，对于所有子领域，实验研究的复现率明显低于非实验方法，并且在较少进行实验的子领域中，复现率相对较高。这一发现令人担忧，因为心理学在实验方面的熟练程度是其强有力的科学声誉的一部分。</p>
<p>分析可复现性与研究论文的其他度量标准的关系时，我们发现，虽然可复现性与研究人员的经验和能力呈正相关，但作者的大学声望和论文的引用量等研究质量的其他代理变量与心理学的可复现性无关。这些发现强调了需要在评估研究和学者时对前-后出版度量标准保持谨慎的态度。</p>
<p>我们还将媒体关注度与论文的可复现性进行了相关分析。媒体在创造科学公众形象和推广知识方面扮演着重要角色，但它通常有动机报道那些反直觉、引人注目的结果。理想情况下，媒体报道与心理学研究的可复现性率应有正向关系（或者没有关系）。然而，我们发现，媒体对论文的报道与其复现成功的可能性存在负相关。因此，基于媒体报道来判断一篇论文的价值是不明智的。对于媒体来说，提醒公众新的、创新的科学研究结果只是引发思考，需要未来的复现实验来证实其健壮性，是很有价值的。 我们设想了两种可能的应用方向：</p>
<p>第一，机器学习模型可用于预测难以或无法进行手动复现的研究（例如纵向研究和特殊或难以访问的人群）。</p>
<p>第二，预测的复现分数可以开始帮助优先选择需要手动复现的某些研究，面对资源有限的情况。每年，个人学者和组织（如心理学科学加速器（67）和协作复现和教育项目（68））都会遇到一个问题：从众多的心理学研究中选择哪些进行复现。Isager等人（69）提出，为了最大化复现的收益，社区应该优先复现那些价值高、结果不确定的研究。研究的价值可以通过引用量或媒体关注度等因素来近似计算，但不确定性部分尚未得到大量文献的充分衡量。我们建议，我们的机器学习模型可以提供复现不确定性的定量测量。 我们注意到，我们的发现在几个方面存在限制：</p>
<ul>
<li>
<p>首先，我们对所有论文的预测都来自于顶级期刊。未来的研究可以检查来自较低排名期刊的论文，以及它们的可复现性如何与发表前后的指标相关联（70）。</p>
</li>
<li>
<p>其次，可复现性的估计仅是近似值。在子领域级别上，我们分析的六个子领域中有五个子领域仅由一种顶级期刊代表。单个期刊不能涵盖整个子领域的范围。</p>
</li>
</ul>
<p>未来的研究可以在以下几个方向展开：</p>
<ul>
<li>
<p>我们的复现得分可以与其他方法结合使用，例如预测市场（16）或非文本机器学习模型（27、28），以进一步精确估计心理学研究的可复现性；</p>
</li>
<li>
<p>可以重复设计本研究，以在其他学科中进行复现普查；</p>
</li>
<li>
<p>可将可复现性得分进一步与其他感兴趣的指标进行相关性分析。</p>
</li>
</ul>
<p>社会科学中的可复现性受到变异性的限制，它最终是一种由各种方法组合而成的集体企业。波普尔在他的书《科学发现的逻辑》中提出：“即使是我们自己的观察结果，我们也不会完全认真对待，或将其视为科学观察结果，直到我们对其进行了重复和测试”（1）。然而，尽管波普尔对于重复和可重复性的洞察是正确的，但必须认识到测试带来了探索的成本。机器学习方法与人类智慧的结合，是发展更好的可复现性理解的有效方法。这种组合平衡了测试成本和科学探索的收益。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li>
<p><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集汇总</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)数据挖掘文献资料汇总</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">推荐 | 文本分析库cntext2.x使用手册</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>词向量 | 使用MD&amp;A2001-2022语料训练Word2Vec模型</title>
      <link>https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/</link>
      <pubDate>Fri, 24 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/</guid>
      <description>&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/the_text_analysis_list_about_ms/&#34;&gt;LIST | 社科(经管)文本挖掘文献汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/text_analysis_code_list_about_ms/&#34;&gt;LIST | 文本分析代码汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/datasets_available_for_management_science/&#34;&gt;LIST | 可供社科(经管)领域使用的数据集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/&#34;&gt;使用3751w专利申请数据集按年份(按省份)训练词向量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/&#34;&gt;预训练模型 | 使用1000w专利摘要训练word2vec模型，可用于开发词典&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相关文献&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216.
[1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.
[3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一训练&#34;&gt;一、训练&lt;/h2&gt;
&lt;h3 id=&#34;11-导入mda数据&#34;&gt;1.1 导入mda数据&lt;/h3&gt;
&lt;p&gt;读取 &lt;a href=&#34;https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/&#34;&gt;&lt;strong&gt;数据集 | 2001-2022年A股上市公司年报&amp;amp;管理层讨论与分析&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_excel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mda01-22.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#gz解压后读取csv&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#df = pd.read_excel(&amp;#39;mda01-22.csv&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;55439
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;12-构造语料&#34;&gt;1.2 构造语料&lt;/h3&gt;
&lt;p&gt;从 &lt;strong&gt;mda01-22.xlsx&lt;/strong&gt; 数据中抽取出所有文本，写入到 &lt;strong&gt;mda01-22.txt&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mda01-22.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;a+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;13-配置cntext环境&#34;&gt;1.3 配置cntext环境&lt;/h3&gt;
&lt;p&gt;使用2.1.1版本 cntext 库(该版本暂不开源，需付费购买)。 将得到的 &lt;strong&gt;cntext-2.1.1-py3-none-any.whl&lt;/strong&gt; 文件放置于电脑桌面，  win系统打开&lt;strong&gt;cmd&lt;/strong&gt;(Mac打开terminal)， 输入如下命令(将工作环境切换至桌面)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;个别Win用户如无效，试试&lt;code&gt;cd Desktop&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;继续在cmd (terminal) 中执行如下命令安装cntext2.1.1&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;14-训练word2vec&#34;&gt;1.4 训练word2vec&lt;/h3&gt;
&lt;p&gt;设置模型参数配置&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mda01-22 使用2001-2022年度mda数据训练&lt;/li&gt;
&lt;li&gt;200 嵌入的维度数，即每个词的向量长度是200&lt;/li&gt;
&lt;li&gt;6 词语上下文的窗口是6&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#程序结束后，可查看总的运行时间&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;W2VModel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mda01-22.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;min_count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;save_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Word2Vec&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Building prefix dict from the default dictionary ...
Start Preprocessing Corpus...
Dumping model to file cache /var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/jieba.cache
Loading model cost 0.278 seconds.
Prefix dict has been built successfully.
Start Training! This may take a while. Please be patient...

Training word2vec model took 3532 seconds

Note: The Word2Vec model has been saved to output/Word2Vec

CPU times: user 1h 30min 45s, sys: 30.1 s, total: 1h 31min 15s
Wall time: 58min 57s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;经过不到两个小时时间， 训练出的中国A股市场词向量模型(如下截图)，词汇量 914058， 模型文件 1.49G。模型可广泛用于经济管理等领域概念(情感)词典的构建或扩展。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;mda01-22.200.6.bin&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mda01-22.200.6.bin.syn1neg.npy&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mda01-22.200.6.bin.wv.vectors.npy&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/pretained-screen.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;为什么这样确定200和6，可以看这篇 &lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;二导入模型&#34;&gt;二、导入模型&lt;/h2&gt;
&lt;p&gt;需要用到两个自定义函数load_w2v、expand_dictionary，源代码太长，为了提高阅读体验， 放在文末。大家记得用这两个函数前一定要先导入。&lt;a href=&#34;mda_pretained_model_code.ipynb&#34;&gt;&lt;strong&gt;点击代码&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#先导入load_w2v、expand_dictionary函数源代码&lt;/span&gt;


&lt;span class=&#34;c1&#34;&gt;#读取模型文件&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Word2Vec/mda01-22.200.6.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Loading word2vec model...
&amp;lt;gensim.models.word2vec.Word2Vec at 0x310dd9990&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;h2 id=&#34;注意&#34;&gt;注意&lt;/h2&gt;
&lt;p&gt;之前购买过mda01-22.100.6.bin的可以留意下， &amp;lt;gensim.models.word2vec.Word2Vec&amp;gt;和&amp;lt;gensim.models.keyedvectors.KeyedVectors&amp;gt;
是有区别的。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;三w2v_model的使用&#34;&gt;三、w2v_model的使用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;查看词汇量&lt;/li&gt;
&lt;li&gt;查询某词向量&lt;/li&gt;
&lt;li&gt;查看多个词的均值向量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更多内容，建议查看下gensim库的文档&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#词汇量&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;914058  
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#查询某词的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([-1.36441350e-01, -2.02002168e+00, -1.49168205e+00,  2.65202689e+00,
        1.49721682e+00,  2.14851022e+00, -1.54925853e-01, -2.25241160e+00,
       -3.58773202e-01,  1.54530525e+00, -7.62950361e-01, -9.77181852e-01,
        6.70365512e-01, -3.20203233e+00,  3.18079638e+00,  1.66510820e+00,
        9.80131567e-01,  1.62199986e+00,  1.80585206e+00,  4.08179426e+00,
       -1.26518166e+00,  3.75929743e-01,  5.72038591e-01,  1.16134119e+00,
        2.55617023e+00, -2.25110960e+00, -2.61538339e+00, -5.71992218e-01,
        8.70356798e-01, -1.85045290e+00, -2.85597444e-01, -9.15628672e-01,
       -2.03667688e+00,  2.11716801e-01,  2.94088912e+00, -2.32688546e+00,
        2.20858502e+00,  8.81347775e-01, -7.99135566e-01, -8.61206651e-01,
       -4.45446587e+00, -1.73757005e+00, -3.36678886e+00, -2.82611530e-02,
       -1.62726247e+00, -8.49750221e-01,  4.13731128e-01, -1.62519825e+00,
        3.03865957e+00, -1.39746085e-01,  8.22233260e-01, -7.97697455e-02,
        1.72468078e+00,  2.94929433e+00,  9.72453177e-01, -1.12741642e-01,
        8.18425417e-01, -9.05264139e-01,  2.61516261e+00,  8.02830994e-01,
        2.40420485e+00,  8.85799348e-01, -1.08665645e+00,  8.21912348e-01,
       -4.39456075e-01, -2.57663131e+00,  2.38062453e+00, -4.58515882e-01,
        2.12767506e+00, -2.01356173e-01,  2.71096081e-01,  9.51708496e-01,
       -3.05705309e+00, -6.06385887e-01, -1.38406023e-01,  2.36809158e+00,
       -2.49158549e+00,  2.71105647e+00, -3.07211792e-03,  1.04273570e+00,
        1.44201803e+00, -5.65704823e-01,  2.85488725e-01,  1.43495277e-01,
       -1.39421299e-01,  9.24086392e-01,  4.25374925e-01, -1.56690669e+00,
        1.67641795e+00, -1.03729677e+00, -1.45472065e-01, -2.11022258e+00,
       -1.81541741e+00, -8.66766050e-02,  8.72350857e-02,  1.17173791e+00,
       -3.07721123e-02,  5.84330797e-01,  1.47265148e+00, -1.76913440e+00,
       -8.48391712e-01, -3.25056529e+00,  7.14846313e-01, -2.98076987e-01,
        1.13966620e+00, -1.42698896e+00,  6.93505168e-01, -2.04717040e+00,
       -1.53559577e+00,  1.01942134e+00, -1.58283603e+00,  9.08654630e-01,
       -1.90529859e+00, -9.43309963e-01,  4.12964225e-01, -2.50713086e+00,
       -4.24056143e-01, -4.10613680e+00,  3.60615468e+00, -4.19765860e-01,
       -2.41174579e+00,  6.80675328e-01,  2.99834704e+00,  1.05610855e-01,
       -7.84325838e-01,  3.24065971e+00, -1.85072863e+00, -2.12448812e+00,
       -2.83468294e+00, -5.77759802e-01, -3.13433480e+00, -6.91670418e-01,
        2.99401569e+00, -5.16145706e-01,  9.09552336e-01, -5.52680910e-01,
       -2.88360894e-01,  1.11991334e+00, -1.11737549e+00,  1.15479147e+00,
       -4.63319182e-01,  1.38351321e+00, -3.02179503e+00,  1.24334955e+00,
        1.93393975e-01, -8.27962995e-01, -2.37227559e+00, -9.26931739e-01,
        6.72517180e-01,  1.27736795e+00,  1.98695862e+00,  1.41960573e+00,
       -3.73892736e+00, -3.14201683e-01, -7.19093859e-01,  1.86080355e-02,
       -2.68105698e+00,  1.04344964e+00,  9.46133554e-01, -2.06151366e+00,
       -2.84214950e+00,  1.17004764e+00,  1.24577022e+00, -1.10806060e+00,
        9.93207514e-01,  8.46789181e-01, -3.09691691e+00,  2.12616014e+00,
       -1.49274826e+00, -1.53214395e+00, -9.95470941e-01,  1.23463202e+00,
       -2.18907285e+00, -4.94913310e-01,  2.80939412e+00,  1.68149090e+00,
        1.48991072e+00,  3.83729649e+00,  4.72325265e-01,  1.37606680e+00,
        2.14257884e+00,  3.18186909e-01,  5.98093605e+00,  1.46744043e-01,
       -2.37729326e-01,  1.20463884e+00, -1.55812174e-01, -5.03088772e-01,
        4.53981996e-01,  1.95544350e+00, -2.32564354e+00, -4.09389853e-01,
        1.89125270e-01,  2.62835431e+00,  9.81123984e-01, -9.51041043e-01,
       -1.14294410e-01,  1.10983588e-01,  9.30419266e-02, -9.84693542e-02],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#查询多个词的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_mean_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;研发&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Ruj&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([ 0.03019853, -0.01928307, -0.05371316,  0.00053774,  0.02516318,
        0.10103251, -0.03914721, -0.08307559,  0.00444389,  0.09456791,
       -0.05761364, -0.03459097,  0.04394419, -0.10181106,  0.1418381 ,
        0.05334964,  0.01820264,  0.01493831,  0.01626587,  0.17402864,
       -0.02859601,  0.04538149,  0.03768233,  0.05431981,  0.15405464,
       -0.03632693, -0.08566202, -0.00595666,  0.08378439, -0.11071078,
       -0.05904576, -0.06451955, -0.1076955 ,  0.05141645,  0.11710279,
       -0.09403889,  0.08633652, -0.06743232,  0.00328483,  0.01589498,
       -0.11226317, -0.05367877, -0.057222  , -0.00685401, -0.04531868,
       -0.02090884,  0.01426806, -0.04787309,  0.1325518 , -0.00498158,
        0.01912023, -0.02292867,  0.08855374,  0.07697155,  0.01407153,
       -0.02378988,  0.03745927,  0.00889686,  0.12555045,  0.04007044,
        0.06247196,  0.04912657, -0.06158784,  0.06346396,  0.00197599,
       -0.04995281,  0.05125345, -0.01584197,  0.07572784,  0.02580263,
       -0.02904062, -0.0008835 , -0.08365948, -0.05539802, -0.07523517,
        0.04622741, -0.12007375,  0.05453204, -0.02054051,  0.02937108,
        0.10272598, -0.0089594 ,  0.05172383,  0.00588922, -0.0010917 ,
        0.02603476, -0.01580217, -0.07810815,  0.06964722, -0.04709972,
       -0.0316673 , -0.05055645, -0.05096703,  0.02772727, -0.03495743,
        0.09567484, -0.0071935 , -0.01266821,  0.00074132, -0.07593331,
       -0.02928162, -0.12574387,  0.02437552, -0.0228716 , -0.03047204,
       -0.03948782,  0.07722469, -0.07440004, -0.00951135,  0.05531401,
       -0.03240326,  0.00389662, -0.05632257, -0.05030375,  0.02883579,
       -0.06157173,  0.00584065, -0.16594191,  0.1108149 , -0.00243916,
       -0.09964953,  0.02029083,  0.03522225, -0.01167114, -0.04048527,
        0.08301719, -0.04682562, -0.0714631 , -0.07355815, -0.0496731 ,
       -0.05303175, -0.03625978,  0.06879813, -0.09117774,  0.0323513 ,
       -0.01808765, -0.01746182,  0.02472609, -0.00873791, -0.00951474,
       -0.02176155,  0.02394484, -0.07035318,  0.10963078,  0.01004294,
       -0.02269555, -0.09929934, -0.02897175,  0.02157164,  0.05608977,
        0.09083252, -0.00525982, -0.09866816, -0.02736895, -0.02923711,
        0.05582205, -0.04462272,  0.01932517,  0.04468061,  0.00317996,
       -0.04182415,  0.03061792,  0.04278665,  0.02939183,  0.03475334,
       -0.00898206, -0.08902986,  0.08294971, -0.00942507, -0.02125597,
       -0.01008157,  0.04477865, -0.08366893, -0.00074587,  0.08328778,
        0.02653155,  0.04581301,  0.10532658, -0.04637942,  0.04722971,
        0.06853952, -0.00235328,  0.18312256, -0.0457427 ,  0.00874868,
        0.08945092, -0.01135547, -0.04203002,  0.02408407,  0.0594779 ,
       -0.05467811,  0.01946783,  0.07095537,  0.04226222, -0.0018304 ,
       -0.00086302,  0.04624099,  0.01009499,  0.04783599,  0.02535392],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;有了每个词或者概念的向量，可以结合cntext旧版本单语言模型内的态度偏见的度量。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四扩展词典&#34;&gt;四、扩展词典&lt;/h2&gt;
&lt;p&gt;做词典法的文本分析，最重要的是有自己的领域词典。之前受限于技术难度，文科生的我也一直在用形容词的通用情感词典。现在依托word2vec技术， 可以加速人工构建的准确率和效率。&lt;/p&gt;
&lt;p&gt;下面是在 mda01-22.200.6.bin 上做的词典扩展测试，函数expand_dictionary会根据种子词选取最准确的topn个词。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#短视主义词  实验&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;抓紧&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;立刻&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;月底&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;年底&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;年终&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;争取&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;力争&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;抓紧&#39;,
 &#39;立刻&#39;,
 &#39;月底&#39;,
 &#39;年底&#39;,
 &#39;年终&#39;,
 &#39;争取&#39;,
 &#39;力争&#39;,
 &#39;争取&#39;,
 &#39;力争&#39;,
 &#39;年底&#39;,
 &#39;月底&#39;,
 &#39;3月底&#39;,
 &#39;尽快&#39;,
 &#39;上半年&#39;,
 &#39;努力争取&#39;,
 &#39;年内实现&#39;,
 &#39;抓紧&#39;,
 &#39;工作争取&#39;,
 &#39;尽早&#39;,
 &#39;6月底&#39;,
 &#39;工作力争&#39;,
 &#39;7月份&#39;,
 &#39;年底完成&#39;,
 &#39;确保&#39;,
 &#39;早日&#39;,
 &#39;有望&#39;,
 &#39;全力&#39;,
 &#39;创造条件&#39;,
 &#39;3月份&#39;,
 &#39;加紧&#39;,
 &#39;力争实现&#39;,
 &#39;力争今年&#39;,
 &#39;月底前&#39;,
 &#39;10月底&#39;,
 &#39;4月份&#39;,
 &#39;继续&#39;,
 &#39;月初&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;团结&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;拼搏&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;克服&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;勇攀高峰&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;友善&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;进取&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;团结&#39;,
 &#39;拼搏&#39;,
 &#39;克服&#39;,
 &#39;勇攀高峰&#39;,
 &#39;友善&#39;,
 &#39;进取&#39;,
 &#39;拼搏&#39;,
 &#39;艰苦奋斗&#39;,
 &#39;团结拼搏&#39;,
 &#39;勇于担当&#39;,
 &#39;锐意进取&#39;,
 &#39;勇气&#39;,
 &#39;团结&#39;,
 &#39;团结奋进&#39;,
 &#39;团结一致&#39;,
 &#39;顽强拼搏&#39;,
 &#39;上下一心&#39;,
 &#39;实干&#39;,
 &#39;拼搏进取&#39;,
 &#39;积极进取&#39;,
 &#39;奋力拼搏&#39;,
 &#39;奋进&#39;,
 &#39;坚定信念&#39;,
 &#39;团结一心&#39;,
 &#39;精诚团结&#39;,
 &#39;顽强&#39;,
 &#39;踏实&#39;,
 &#39;团结协作&#39;,
 &#39;求真务实&#39;,
 &#39;团结奋斗&#39;,
 &#39;奋发有为&#39;,
 &#39;同心协力&#39;,
 &#39;脚踏实地&#39;,
 &#39;开拓进取&#39;,
 &#39;进取&#39;,
 &#39;勇于&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;科技&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;研发&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;技术&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;标准&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;创新&#39;,
 &#39;科技&#39;,
 &#39;研发&#39;,
 &#39;技术&#39;,
 &#39;标准&#39;,
 &#39;技术创新&#39;,
 &#39;技术研发&#39;,
 &#39;先进技术&#39;,
 &#39;关键技术&#39;,
 &#39;创新性&#39;,
 &#39;前沿技术&#39;,
 &#39;科技创新&#39;,
 &#39;技术应用&#39;,
 &#39;产品开发&#39;,
 &#39;自主创新&#39;,
 &#39;新技术&#39;,
 &#39;科研&#39;,
 &#39;产品研发&#39;,
 &#39;自主研发&#39;,
 &#39;技术开发&#39;,
 &#39;工艺技术&#39;,
 &#39;技术标准&#39;,
 &#39;基础研究&#39;,
 &#39;集成创新&#39;,
 &#39;核心技术&#39;,
 &#39;成熟技术&#39;,
 &#39;研发创新&#39;,
 &#39;理论技术&#39;,
 &#39;前沿技术研发&#39;,
 &#39;工艺&#39;,
 &#39;科技成果&#39;,
 &#39;技术研究&#39;,
 &#39;标准制定&#39;,
 &#39;技术装备&#39;,
 &#39;技术相结合&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;竞争&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;竞争力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;竞争&#39;,
 &#39;竞争力&#39;,
 &#39;竞争能力&#39;,
 &#39;市场竞争&#39;,
 &#39;竞争优势&#39;,
 &#39;市场竞争力&#39;,
 &#39;竞&#39;,
 &#39;竞争实力&#39;,
 &#39;激烈竞争&#39;,
 &#39;参与市场竞争&#39;,
 &#39;国际竞争&#39;,
 &#39;市场竞争能力&#39;,
 &#39;竞争态势&#39;,
 &#39;市场竞争优势&#39;,
 &#39;行业竞争&#39;,
 &#39;综合竞争力&#39;,
 &#39;竞争对手&#39;,
 &#39;未来市场竞争&#39;,
 &#39;产品竞争力&#39;,
 &#39;之间竞争&#39;,
 &#39;核心竞争力&#39;,
 &#39;参与竞争&#39;,
 &#39;核心竞争能力&#39;,
 &#39;竞争日趋激烈&#39;,
 &#39;国际化竞争&#39;,
 &#39;国际竞争力&#39;,
 &#39;竟争力&#39;,
 &#39;市场化竞争&#39;,
 &#39;同质化竞争&#39;,
 &#39;竞争力关键&#39;,
 &#39;价格竞争&#39;,
 &#39;整体竞争力&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;疫情&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;扩散&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;防控&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;反复&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;冲击&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;疫情&#39;,
 &#39;扩散&#39;,
 &#39;防控&#39;,
 &#39;反复&#39;,
 &#39;冲击&#39;,
 &#39;蔓延&#39;,
 &#39;疫情&#39;,
 &#39;疫情爆发&#39;,
 &#39;疫情冲击&#39;,
 &#39;新冠疫情&#39;,
 &#39;肆虐&#39;,
 &#39;新冠肺炎&#39;,
 &#39;疫情蔓延&#39;,
 &#39;本次疫情&#39;,
 &#39;散发&#39;,
 &#39;疫情扩散&#39;,
 &#39;疫情影响&#39;,
 &#39;疫情反复&#39;,
 &#39;疫情传播&#39;,
 &#39;肺炎疫情&#39;,
 &#39;国内疫情&#39;,
 &#39;击&#39;,
 &#39;各地疫情&#39;,
 &#39;疫情全球&#39;,
 &#39;疫情多点&#39;,
 &#39;全球疫情&#39;,
 &#39;持续蔓延&#39;,
 &#39;多点散发&#39;,
 &#39;疫情导致&#39;,
 &#39;疫情暴发&#39;,
 &#39;病毒疫情&#39;,
 &#39;疫情持续&#39;,
 &#39;疫情初期&#39;,
 &#39;疫情出现&#39;,
 &#39;防控措施&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;旧&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;老&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;后&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;落后&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;旧&#39;,
 &#39;老&#39;,
 &#39;后&#39;,
 &#39;落后&#39;,
 &#39;老&#39;,
 &#39;旧&#39;,
 &#39;陈旧&#39;,
 &#39;老旧&#39;,
 &#39;淘汰&#39;,
 &#39;低效率&#39;,
 &#39;低效&#39;,
 &#39;部分老旧&#39;,
 &#39;进行改造&#39;,
 &#39;老旧设备&#39;,
 &#39;工艺落后&#39;,
 &#39;设备陈旧&#39;,
 &#39;能耗高&#39;,
 &#39;更新改造&#39;,
 &#39;落后工艺&#39;,
 &#39;技术落后&#39;,
 &#39;改造&#39;,
 &#39;翻新&#39;,
 &#39;简陋&#39;,
 &#39;旧设备&#39;,
 &#39;拆除&#39;,
 &#39;现象严重&#39;,
 &#39;原有&#39;,
 &#39;相对落后&#39;,
 &#39;产能淘汰&#39;,
 &#39;加快淘汰&#39;,
 &#39;搬&#39;,
 &#39;替换&#39;,
 &#39;大批&#39;,
 &#39;迁&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五源代码&#34;&gt;五、源代码&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;gensim.models&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pathlib&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    Load word2vec model
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    Args:
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        w2v_path (str): path of word2vec model
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    Returns:
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        model: word2vec model
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Loading word2vec model...&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    According to the seed word file, select the top n words with the most similar semantics and save them in the directory save_dir.
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    Args:
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        wv (Word2VecKeyedVectors): the word embedding model
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        seedwords (list): 种子词
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        topn (int, optional): Set the number of most similar words to retrieve to topn. Defaults to 100.
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        save_dir (str, optional): the directory to save the candidate words. Defaults to &amp;#39;Word2Vec&amp;#39;.
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    Returns:
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;simidx_scores&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;similars_candidate_idxs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#the candidate words of seedwords&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;seedidxs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#transform word to index&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;seedidx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;seedidxs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seedidx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seedidx&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seedidxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;# sims_words such as [(&amp;#39;by&amp;#39;, 0.99984), (&amp;#39;or&amp;#39;, 0.99982), (&amp;#39;an&amp;#39;, 0.99981), (&amp;#39;up&amp;#39;, 0.99980)]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;sims_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seedidx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;#Convert words to index and store them&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;similars_candidate_idxs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sim&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sims_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;similars_candidate_idxs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similars_candidate_idxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;similars_candidate_idxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seedidxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;simidx_scores&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;simidxs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;sorted&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;simidx_scores&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reverse&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;simwords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;simidxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;resultwords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;resultwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;resultwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;simwords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;resultwords&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;六获取模型&#34;&gt;六、获取模型&lt;/h2&gt;
&lt;p&gt;内容创作不易， 本文为付费内容，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 100元    cntext-2.1.1-py3-none-any.whl

- 100元   Word2Vec相关模型文件(mda01-22.200.6.bin)

- 200元   
    - cntext-2.1.1-py3-none-any.whl  
    - Word2Vec相关模型文件(mda01-22.200.6.bin)
    
    
声明： 仅供科研使用
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;加微信 372335839， 备注「姓名-学校-专业-word2vec」&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)文本挖掘文献汇总</a></li>
<li><a href="https://textdata.cn/blog/text_analysis_code_list_about_ms/">LIST | 文本分析代码汇总</a></li>
<li><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">Python实证指标构建与文本分析</a></li>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">使用3751w专利申请数据集按年份(按省份)训练词向量</a></li>
<li><a href="https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/">预训练模型 | 使用1000w专利摘要训练word2vec模型，可用于开发词典</a></li>
</ul>
<p>相关文献</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216.
[1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.
[3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
</code></pre></div><p><br><br></p>
<h2 id="一训练">一、训练</h2>
<h3 id="11-导入mda数据">1.1 导入mda数据</h3>
<p>读取 <a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/"><strong>数据集 | 2001-2022年A股上市公司年报&amp;管理层讨论与分析</strong></a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;mda01-22.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">)</span>
<span class="c1">#gz解压后读取csv</span>
<span class="c1">#df = pd.read_excel(&#39;mda01-22.csv&#39;)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">55439
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<br>
<h3 id="12-构造语料">1.2 构造语料</h3>
<p>从 <strong>mda01-22.xlsx</strong> 数据中抽取出所有文本，写入到 <strong>mda01-22.txt</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;mda01-22.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="13-配置cntext环境">1.3 配置cntext环境</h3>
<p>使用2.1.1版本 cntext 库(该版本暂不开源，需付费购买)。 将得到的 <strong>cntext-2.1.1-py3-none-any.whl</strong> 文件放置于电脑桌面，  win系统打开<strong>cmd</strong>(Mac打开terminal)， 输入如下命令(将工作环境切换至桌面)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>个别Win用户如无效，试试<code>cd Desktop</code> 。</p>
<p>继续在cmd (terminal) 中执行如下命令安装cntext2.1.1</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install distinctiveness
pip3 install cntext-2.1.1-py3-none-any.whl 
</code></pre></div><br>
<h3 id="14-训练word2vec">1.4 训练word2vec</h3>
<p>设置模型参数配置</p>
<ul>
<li>mda01-22 使用2001-2022年度mda数据训练</li>
<li>200 嵌入的维度数，即每个词的向量长度是200</li>
<li>6 词语上下文的窗口是6</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>  <span class="c1">#程序结束后，可查看总的运行时间</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModel</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;mda01-22.txt&#39;</span><span class="p">)</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="s1">&#39;Word2Vec&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Building prefix dict from the default dictionary ...
Start Preprocessing Corpus...
Dumping model to file cache /var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/jieba.cache
Loading model cost 0.278 seconds.
Prefix dict has been built successfully.
Start Training! This may take a while. Please be patient...

Training word2vec model took 3532 seconds

Note: The Word2Vec model has been saved to output/Word2Vec

CPU times: user 1h 30min 45s, sys: 30.1 s, total: 1h 31min 15s
Wall time: 58min 57s
</code></pre></div><p>经过不到两个小时时间， 训练出的中国A股市场词向量模型(如下截图)，词汇量 914058， 模型文件 1.49G。模型可广泛用于经济管理等领域概念(情感)词典的构建或扩展。</p>
<ul>
<li><strong>mda01-22.200.6.bin</strong></li>
<li><strong>mda01-22.200.6.bin.syn1neg.npy</strong></li>
<li><strong>mda01-22.200.6.bin.wv.vectors.npy</strong></li>
</ul>
<p><img loading="lazy" src="img/pretained-screen.png" alt=""  />
</p>
<p>为什么这样确定200和6，可以看这篇 <a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></p>
<br>
<br>
<h2 id="二导入模型">二、导入模型</h2>
<p>需要用到两个自定义函数load_w2v、expand_dictionary，源代码太长，为了提高阅读体验， 放在文末。大家记得用这两个函数前一定要先导入。<a href="mda_pretained_model_code.ipynb"><strong>点击代码</strong></a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#先导入load_w2v、expand_dictionary函数源代码</span>


<span class="c1">#读取模型文件</span>
<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">load_w2v</span><span class="p">(</span><span class="n">w2v_path</span><span class="o">=</span><span class="s1">&#39;Word2Vec/mda01-22.200.6.bin&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span>
</code></pre></div><pre><code>Loading word2vec model...
&lt;gensim.models.word2vec.Word2Vec at 0x310dd9990&gt;
</code></pre>
<br>
<h2 id="注意">注意</h2>
<p>之前购买过mda01-22.100.6.bin的可以留意下， &lt;gensim.models.word2vec.Word2Vec&gt;和&lt;gensim.models.keyedvectors.KeyedVectors&gt;
是有区别的。</p>
<p><br><br></p>
<h3 id="三w2v_model的使用">三、w2v_model的使用</h3>
<ul>
<li>查看词汇量</li>
<li>查询某词向量</li>
<li>查看多个词的均值向量</li>
</ul>
<p>更多内容，建议查看下gensim库的文档</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#词汇量</span>
<span class="nb">len</span><span class="p">(</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>914058  
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查询某词的词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;创新&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>array([-1.36441350e-01, -2.02002168e+00, -1.49168205e+00,  2.65202689e+00,
        1.49721682e+00,  2.14851022e+00, -1.54925853e-01, -2.25241160e+00,
       -3.58773202e-01,  1.54530525e+00, -7.62950361e-01, -9.77181852e-01,
        6.70365512e-01, -3.20203233e+00,  3.18079638e+00,  1.66510820e+00,
        9.80131567e-01,  1.62199986e+00,  1.80585206e+00,  4.08179426e+00,
       -1.26518166e+00,  3.75929743e-01,  5.72038591e-01,  1.16134119e+00,
        2.55617023e+00, -2.25110960e+00, -2.61538339e+00, -5.71992218e-01,
        8.70356798e-01, -1.85045290e+00, -2.85597444e-01, -9.15628672e-01,
       -2.03667688e+00,  2.11716801e-01,  2.94088912e+00, -2.32688546e+00,
        2.20858502e+00,  8.81347775e-01, -7.99135566e-01, -8.61206651e-01,
       -4.45446587e+00, -1.73757005e+00, -3.36678886e+00, -2.82611530e-02,
       -1.62726247e+00, -8.49750221e-01,  4.13731128e-01, -1.62519825e+00,
        3.03865957e+00, -1.39746085e-01,  8.22233260e-01, -7.97697455e-02,
        1.72468078e+00,  2.94929433e+00,  9.72453177e-01, -1.12741642e-01,
        8.18425417e-01, -9.05264139e-01,  2.61516261e+00,  8.02830994e-01,
        2.40420485e+00,  8.85799348e-01, -1.08665645e+00,  8.21912348e-01,
       -4.39456075e-01, -2.57663131e+00,  2.38062453e+00, -4.58515882e-01,
        2.12767506e+00, -2.01356173e-01,  2.71096081e-01,  9.51708496e-01,
       -3.05705309e+00, -6.06385887e-01, -1.38406023e-01,  2.36809158e+00,
       -2.49158549e+00,  2.71105647e+00, -3.07211792e-03,  1.04273570e+00,
        1.44201803e+00, -5.65704823e-01,  2.85488725e-01,  1.43495277e-01,
       -1.39421299e-01,  9.24086392e-01,  4.25374925e-01, -1.56690669e+00,
        1.67641795e+00, -1.03729677e+00, -1.45472065e-01, -2.11022258e+00,
       -1.81541741e+00, -8.66766050e-02,  8.72350857e-02,  1.17173791e+00,
       -3.07721123e-02,  5.84330797e-01,  1.47265148e+00, -1.76913440e+00,
       -8.48391712e-01, -3.25056529e+00,  7.14846313e-01, -2.98076987e-01,
        1.13966620e+00, -1.42698896e+00,  6.93505168e-01, -2.04717040e+00,
       -1.53559577e+00,  1.01942134e+00, -1.58283603e+00,  9.08654630e-01,
       -1.90529859e+00, -9.43309963e-01,  4.12964225e-01, -2.50713086e+00,
       -4.24056143e-01, -4.10613680e+00,  3.60615468e+00, -4.19765860e-01,
       -2.41174579e+00,  6.80675328e-01,  2.99834704e+00,  1.05610855e-01,
       -7.84325838e-01,  3.24065971e+00, -1.85072863e+00, -2.12448812e+00,
       -2.83468294e+00, -5.77759802e-01, -3.13433480e+00, -6.91670418e-01,
        2.99401569e+00, -5.16145706e-01,  9.09552336e-01, -5.52680910e-01,
       -2.88360894e-01,  1.11991334e+00, -1.11737549e+00,  1.15479147e+00,
       -4.63319182e-01,  1.38351321e+00, -3.02179503e+00,  1.24334955e+00,
        1.93393975e-01, -8.27962995e-01, -2.37227559e+00, -9.26931739e-01,
        6.72517180e-01,  1.27736795e+00,  1.98695862e+00,  1.41960573e+00,
       -3.73892736e+00, -3.14201683e-01, -7.19093859e-01,  1.86080355e-02,
       -2.68105698e+00,  1.04344964e+00,  9.46133554e-01, -2.06151366e+00,
       -2.84214950e+00,  1.17004764e+00,  1.24577022e+00, -1.10806060e+00,
        9.93207514e-01,  8.46789181e-01, -3.09691691e+00,  2.12616014e+00,
       -1.49274826e+00, -1.53214395e+00, -9.95470941e-01,  1.23463202e+00,
       -2.18907285e+00, -4.94913310e-01,  2.80939412e+00,  1.68149090e+00,
        1.48991072e+00,  3.83729649e+00,  4.72325265e-01,  1.37606680e+00,
        2.14257884e+00,  3.18186909e-01,  5.98093605e+00,  1.46744043e-01,
       -2.37729326e-01,  1.20463884e+00, -1.55812174e-01, -5.03088772e-01,
        4.53981996e-01,  1.95544350e+00, -2.32564354e+00, -4.09389853e-01,
        1.89125270e-01,  2.62835431e+00,  9.81123984e-01, -9.51041043e-01,
       -1.14294410e-01,  1.10983588e-01,  9.30419266e-02, -9.84693542e-02],
      dtype=float32)
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查询多个词的词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">get_mean_vector</span><span class="p">([</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;研发&#39;</span><span class="p">])</span>
</code></pre></div><p>Ruj</p>
<pre><code>array([ 0.03019853, -0.01928307, -0.05371316,  0.00053774,  0.02516318,
        0.10103251, -0.03914721, -0.08307559,  0.00444389,  0.09456791,
       -0.05761364, -0.03459097,  0.04394419, -0.10181106,  0.1418381 ,
        0.05334964,  0.01820264,  0.01493831,  0.01626587,  0.17402864,
       -0.02859601,  0.04538149,  0.03768233,  0.05431981,  0.15405464,
       -0.03632693, -0.08566202, -0.00595666,  0.08378439, -0.11071078,
       -0.05904576, -0.06451955, -0.1076955 ,  0.05141645,  0.11710279,
       -0.09403889,  0.08633652, -0.06743232,  0.00328483,  0.01589498,
       -0.11226317, -0.05367877, -0.057222  , -0.00685401, -0.04531868,
       -0.02090884,  0.01426806, -0.04787309,  0.1325518 , -0.00498158,
        0.01912023, -0.02292867,  0.08855374,  0.07697155,  0.01407153,
       -0.02378988,  0.03745927,  0.00889686,  0.12555045,  0.04007044,
        0.06247196,  0.04912657, -0.06158784,  0.06346396,  0.00197599,
       -0.04995281,  0.05125345, -0.01584197,  0.07572784,  0.02580263,
       -0.02904062, -0.0008835 , -0.08365948, -0.05539802, -0.07523517,
        0.04622741, -0.12007375,  0.05453204, -0.02054051,  0.02937108,
        0.10272598, -0.0089594 ,  0.05172383,  0.00588922, -0.0010917 ,
        0.02603476, -0.01580217, -0.07810815,  0.06964722, -0.04709972,
       -0.0316673 , -0.05055645, -0.05096703,  0.02772727, -0.03495743,
        0.09567484, -0.0071935 , -0.01266821,  0.00074132, -0.07593331,
       -0.02928162, -0.12574387,  0.02437552, -0.0228716 , -0.03047204,
       -0.03948782,  0.07722469, -0.07440004, -0.00951135,  0.05531401,
       -0.03240326,  0.00389662, -0.05632257, -0.05030375,  0.02883579,
       -0.06157173,  0.00584065, -0.16594191,  0.1108149 , -0.00243916,
       -0.09964953,  0.02029083,  0.03522225, -0.01167114, -0.04048527,
        0.08301719, -0.04682562, -0.0714631 , -0.07355815, -0.0496731 ,
       -0.05303175, -0.03625978,  0.06879813, -0.09117774,  0.0323513 ,
       -0.01808765, -0.01746182,  0.02472609, -0.00873791, -0.00951474,
       -0.02176155,  0.02394484, -0.07035318,  0.10963078,  0.01004294,
       -0.02269555, -0.09929934, -0.02897175,  0.02157164,  0.05608977,
        0.09083252, -0.00525982, -0.09866816, -0.02736895, -0.02923711,
        0.05582205, -0.04462272,  0.01932517,  0.04468061,  0.00317996,
       -0.04182415,  0.03061792,  0.04278665,  0.02939183,  0.03475334,
       -0.00898206, -0.08902986,  0.08294971, -0.00942507, -0.02125597,
       -0.01008157,  0.04477865, -0.08366893, -0.00074587,  0.08328778,
        0.02653155,  0.04581301,  0.10532658, -0.04637942,  0.04722971,
        0.06853952, -0.00235328,  0.18312256, -0.0457427 ,  0.00874868,
        0.08945092, -0.01135547, -0.04203002,  0.02408407,  0.0594779 ,
       -0.05467811,  0.01946783,  0.07095537,  0.04226222, -0.0018304 ,
       -0.00086302,  0.04624099,  0.01009499,  0.04783599,  0.02535392],
      dtype=float32)
</code></pre>
<p>有了每个词或者概念的向量，可以结合cntext旧版本单语言模型内的态度偏见的度量。</p>
<p><br><br></p>
<h2 id="四扩展词典">四、扩展词典</h2>
<p>做词典法的文本分析，最重要的是有自己的领域词典。之前受限于技术难度，文科生的我也一直在用形容词的通用情感词典。现在依托word2vec技术， 可以加速人工构建的准确率和效率。</p>
<p>下面是在 mda01-22.200.6.bin 上做的词典扩展测试，函数expand_dictionary会根据种子词选取最准确的topn个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#短视主义词  实验</span>
<span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;抓紧&#39;</span><span class="p">,</span> <span class="s1">&#39;立刻&#39;</span><span class="p">,</span> <span class="s1">&#39;月底&#39;</span><span class="p">,</span> <span class="s1">&#39;年底&#39;</span><span class="p">,</span> <span class="s1">&#39;年终&#39;</span><span class="p">,</span> <span class="s1">&#39;争取&#39;</span><span class="p">,</span> <span class="s1">&#39;力争&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['抓紧',
 '立刻',
 '月底',
 '年底',
 '年终',
 '争取',
 '力争',
 '争取',
 '力争',
 '年底',
 '月底',
 '3月底',
 '尽快',
 '上半年',
 '努力争取',
 '年内实现',
 '抓紧',
 '工作争取',
 '尽早',
 '6月底',
 '工作力争',
 '7月份',
 '年底完成',
 '确保',
 '早日',
 '有望',
 '全力',
 '创造条件',
 '3月份',
 '加紧',
 '力争实现',
 '力争今年',
 '月底前',
 '10月底',
 '4月份',
 '继续',
 '月初']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;团结&#39;</span><span class="p">,</span> <span class="s1">&#39;拼搏&#39;</span><span class="p">,</span>  <span class="s1">&#39;克服&#39;</span><span class="p">,</span>  <span class="s1">&#39;勇攀高峰&#39;</span><span class="p">,</span>  <span class="s1">&#39;友善&#39;</span><span class="p">,</span>  <span class="s1">&#39;进取&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['团结',
 '拼搏',
 '克服',
 '勇攀高峰',
 '友善',
 '进取',
 '拼搏',
 '艰苦奋斗',
 '团结拼搏',
 '勇于担当',
 '锐意进取',
 '勇气',
 '团结',
 '团结奋进',
 '团结一致',
 '顽强拼搏',
 '上下一心',
 '实干',
 '拼搏进取',
 '积极进取',
 '奋力拼搏',
 '奋进',
 '坚定信念',
 '团结一心',
 '精诚团结',
 '顽强',
 '踏实',
 '团结协作',
 '求真务实',
 '团结奋斗',
 '奋发有为',
 '同心协力',
 '脚踏实地',
 '开拓进取',
 '进取',
 '勇于']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;科技&#39;</span><span class="p">,</span>  <span class="s1">&#39;研发&#39;</span><span class="p">,</span>  <span class="s1">&#39;技术&#39;</span><span class="p">,</span>  <span class="s1">&#39;标准&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['创新',
 '科技',
 '研发',
 '技术',
 '标准',
 '技术创新',
 '技术研发',
 '先进技术',
 '关键技术',
 '创新性',
 '前沿技术',
 '科技创新',
 '技术应用',
 '产品开发',
 '自主创新',
 '新技术',
 '科研',
 '产品研发',
 '自主研发',
 '技术开发',
 '工艺技术',
 '技术标准',
 '基础研究',
 '集成创新',
 '核心技术',
 '成熟技术',
 '研发创新',
 '理论技术',
 '前沿技术研发',
 '工艺',
 '科技成果',
 '技术研究',
 '标准制定',
 '技术装备',
 '技术相结合']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;竞争&#39;</span><span class="p">,</span> <span class="s1">&#39;竞争力&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['竞争',
 '竞争力',
 '竞争能力',
 '市场竞争',
 '竞争优势',
 '市场竞争力',
 '竞',
 '竞争实力',
 '激烈竞争',
 '参与市场竞争',
 '国际竞争',
 '市场竞争能力',
 '竞争态势',
 '市场竞争优势',
 '行业竞争',
 '综合竞争力',
 '竞争对手',
 '未来市场竞争',
 '产品竞争力',
 '之间竞争',
 '核心竞争力',
 '参与竞争',
 '核心竞争能力',
 '竞争日趋激烈',
 '国际化竞争',
 '国际竞争力',
 '竟争力',
 '市场化竞争',
 '同质化竞争',
 '竞争力关键',
 '价格竞争',
 '整体竞争力']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;疫情&#39;</span><span class="p">,</span> <span class="s1">&#39;扩散&#39;</span><span class="p">,</span> <span class="s1">&#39;防控&#39;</span><span class="p">,</span> <span class="s1">&#39;反复&#39;</span><span class="p">,</span> <span class="s1">&#39;冲击&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['疫情',
 '扩散',
 '防控',
 '反复',
 '冲击',
 '蔓延',
 '疫情',
 '疫情爆发',
 '疫情冲击',
 '新冠疫情',
 '肆虐',
 '新冠肺炎',
 '疫情蔓延',
 '本次疫情',
 '散发',
 '疫情扩散',
 '疫情影响',
 '疫情反复',
 '疫情传播',
 '肺炎疫情',
 '国内疫情',
 '击',
 '各地疫情',
 '疫情全球',
 '疫情多点',
 '全球疫情',
 '持续蔓延',
 '多点散发',
 '疫情导致',
 '疫情暴发',
 '病毒疫情',
 '疫情持续',
 '疫情初期',
 '疫情出现',
 '防控措施']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;旧&#39;</span><span class="p">,</span> <span class="s1">&#39;老&#39;</span><span class="p">,</span> <span class="s1">&#39;后&#39;</span><span class="p">,</span> <span class="s1">&#39;落后&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['旧',
 '老',
 '后',
 '落后',
 '老',
 '旧',
 '陈旧',
 '老旧',
 '淘汰',
 '低效率',
 '低效',
 '部分老旧',
 '进行改造',
 '老旧设备',
 '工艺落后',
 '设备陈旧',
 '能耗高',
 '更新改造',
 '落后工艺',
 '技术落后',
 '改造',
 '翻新',
 '简陋',
 '旧设备',
 '拆除',
 '现象严重',
 '原有',
 '相对落后',
 '产能淘汰',
 '加快淘汰',
 '搬',
 '替换',
 '大批',
 '迁']
</code></pre>
<p><br><br></p>
<h2 id="五源代码">五、源代码</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>


<span class="k">def</span> <span class="nf">load_w2v</span><span class="p">(</span><span class="n">w2v_path</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    Load word2vec model
</span><span class="s2">
</span><span class="s2">    Args:
</span><span class="s2">        w2v_path (str): path of word2vec model
</span><span class="s2">
</span><span class="s2">    Returns:
</span><span class="s2">        model: word2vec model
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loading word2vec model...&#39;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">seedwords</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    According to the seed word file, select the top n words with the most similar semantics and save them in the directory save_dir.
</span><span class="s2">    
</span><span class="s2">    Args:
</span><span class="s2">        wv (Word2VecKeyedVectors): the word embedding model
</span><span class="s2">        seedwords (list): 种子词
</span><span class="s2">        topn (int, optional): Set the number of most similar words to retrieve to topn. Defaults to 100.
</span><span class="s2">        save_dir (str, optional): the directory to save the candidate words. Defaults to &#39;Word2Vec&#39;.
</span><span class="s2">    
</span><span class="s2">    Returns:
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">simidx_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">similars_candidate_idxs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#the candidate words of seedwords</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span>
    <span class="n">seedidxs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#transform word to index</span>
    <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seedwords</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
            <span class="n">seedidx</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">seed</span><span class="p">]</span>
            <span class="n">seedidxs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seedidx</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">seedidx</span> <span class="ow">in</span> <span class="n">seedidxs</span><span class="p">:</span>
        <span class="c1"># sims_words such as [(&#39;by&#39;, 0.99984), (&#39;or&#39;, 0.99982), (&#39;an&#39;, 0.99981), (&#39;up&#39;, 0.99980)]</span>
        <span class="n">sims_words</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">seedidx</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="n">topn</span><span class="p">)</span>
        <span class="c1">#Convert words to index and store them</span>
        <span class="n">similars_candidate_idxs</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">dictionary</span><span class="p">[</span><span class="n">sim</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="k">for</span> <span class="n">sim</span> <span class="ow">in</span> <span class="n">sims_words</span><span class="p">])</span>
    <span class="n">similars_candidate_idxs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">similars_candidate_idxs</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">similars_candidate_idxs</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="n">idx</span><span class="p">],</span> <span class="n">seedidxs</span><span class="p">)</span>
        <span class="n">simidx_scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">score</span><span class="p">))</span>
    <span class="n">simidxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">simidx_scores</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>

    <span class="n">simwords</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">simidxs</span><span class="p">][:</span><span class="n">topn</span><span class="p">]</span>

    <span class="n">resultwords</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">resultwords</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">seedwords</span><span class="p">)</span>
    <span class="n">resultwords</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">simwords</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">resultwords</span>
</code></pre></div><p><br><br></p>
<h2 id="六获取模型">六、获取模型</h2>
<p>内容创作不易， 本文为付费内容，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 100元    cntext-2.1.1-py3-none-any.whl

- 100元   Word2Vec相关模型文件(mda01-22.200.6.bin)

- 200元   
    - cntext-2.1.1-py3-none-any.whl  
    - Word2Vec相关模型文件(mda01-22.200.6.bin)
    
    
声明： 仅供科研使用
</code></pre></div><p>加微信 372335839， 备注「姓名-学校-专业-word2vec」</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</title>
      <link>https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/</link>
      <pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/</guid>
      <description>Literally, **word embedding (Embeddings)** is the use of dense vectors to represent the semantics of a word. **Scholars have shown that by comparing the distance between these word vectors, we can understand how &amp;#34;humans&amp;#34; understand the meaning of words**. So, if we have a corpus comparing the distance between &amp;#34;taxes&amp;#34; and social groups (&amp;#34;conservatives&amp;#34;, &amp;#34;socialists&amp;#34;), semantically, &amp;#34;taxes&amp;#34; should be farther away from &amp;#34;socialists&amp;#34;, after all The money collected is for the service of the general public and has elements of socialism. In the word embedding space, word vectors contain rich information, such as analogies. Spain is to Madrid what Germany is to Berlin and France to Paris.字面上，**词嵌入(Embeddings)**是使用稠密向量表示一个词语的语义。**学者们已经表明，通过比较这些词向量之间的距离，我们可以了解“人类”如何理解单词的含义**。因此，如果我们有一个语料库，比较“税收” 与 社会团体(“保守派”、“社会主义者”)  之间的距离， 按照语义，“税收”应该距离 “社会主义者” 跟多一些，毕竟收上来的钱是为了社会大众服务，有社会主义的成分。在词嵌入空间中，词向量含有丰富的信息，例如可以做类比。西班牙之于马德里， 正如德国至于柏林、法国之于巴黎。&amp;#34;</description>
      <content:encoded><![CDATA[<p>Rodriguez, Pedro L., and Arthur Spirling. &ldquo;<strong>Word embeddings: What works, what doesn’t, and how to tell the difference for applied research</strong>.&rdquo; <em>The Journal of Politics</em> 84, no. 1 (2022): 101-115.</p>
<p>论文作者是政治学领域的， 但是词嵌入方法在社科研究是通用的，大邓觉得挺有用的，使用chatGPT进行翻译+大邓整理， 耗时约2个小时。</p>
<br>
<h2 id="1-什么是词嵌入">1. 什么是词嵌入？</h2>
<p>字面上，**词嵌入(Embeddings)**是使用稠密向量表示一个词语的语义。<strong>学者们已经表明，通过比较这些词向量之间的距离，我们可以了解“人类”如何理解单词的含义</strong>。因此，如果我们有一个语料库，比较“税收” 与 社会团体(“保守派”、“社会主义者”)  之间的距离， 按照语义，“税收”应该距离 “社会主义者” 跟多一些，毕竟收上来的钱是为了社会大众服务，有社会主义的成分。在词嵌入空间中，词向量含有丰富的信息，例如可以做类比。西班牙之于马德里， 正如德国至于柏林、法国之于巴黎。</p>
<p><img loading="lazy" src="img/word-embeddings.png" alt=""  />
</p>
<br>
<h2 id="2-我没有听说过它们这是一个新的想法吗">2. 我没有听说过它们。这是一个新的想法吗？</h2>
<p>不是。这个想法相当古老，至少可以追溯到20世纪50年代。如果非要说「新」，变化的只是相比过去，现在可以更快、更容易获取嵌入。</p>
<br>
<h2 id="3-好的那么这个旧的想法是什么">3. 好的。那么这个“旧”的想法是什么？</h2>
<p><img loading="lazy" src="img/firth_words.png" alt=""  />
</p>
<p>Firth（1957）有一个人人引用的谚语：</p>
<blockquote>
<p>You shall know a word by the company it keeps</p>
</blockquote>
<p>通过一个单词所处的语境，我们可以了解该单词的含义。 <strong>该谚语源于英国语言学家 J.R. Firth 的理论，他认为单词的含义是由其周围的语境和与之相伴的其他单词所决定的，因此我们需要通过单词出现的上下文来理解其含义。这一理论在语言学、自然语言处理等领域有着广泛的应用</strong>。</p>
<p>简而言之，这就是所谓的 “<strong>分布假设</strong>” 。字面上，这个想法是，出现在类似的 <strong>上下文Context</strong> 中的单词可能意味着相似的事物。如果“<strong>咖啡</strong>”和“茶”总是接近于“<strong>杯子</strong>”，那么我们可能会推断出“咖啡”和“<strong>茶</strong>”是相似的。</p>
<p>利用这一洞见的模型有时被称为 <strong>分布语义模型（distributional semantic models， DSMs）</strong>。</p>
<br>
<h2 id="4-在这种情况下上下文字面上是什么意思">4. 在这种情况下，“上下文”字面上是什么意思？</h2>
<p>在嵌入文献中，上下文通常是一个局部的对称窗口，围绕一个词展开。因此，假设我们的句子是：</p>
<blockquote>
<p>&ldquo;We then heard some nice, relaxing <strong>music</strong> that gently worked to a crescendo.&rdquo;</p>
<p>“然后我们听到一些美妙、轻松的音乐，它慢慢地推向高潮。”</p>
</blockquote>
<p>这里以 <strong>music</strong>  为中心的一个对称窗口可以是 <strong>(nice, relaxing)</strong>  和  <strong>(that, gently)</strong>。</p>
<p>三个词的窗口可以是 <strong>(some, nice, relaxing)</strong>  和  <strong>(&ldquo;that, gently, worked)</strong> 。窗口内music前后的词就是ta的上下文。</p>
<br>
<h2 id="5-所以所有的dsms都使用本地窗口吗">5. 所以所有的DSMs都使用本地窗口吗？</h2>
<p>不是的。 <strong>分布语义模型</strong>（DSMs）包括像隐含狄利克雷分配（LDA）这样的东西，政治学通常用于“主题模型”。但是它们通常不使用本地窗口。</p>
<br>
<h2 id="6-我明白了所以嵌入模型没有做词袋假设">6. 我明白了！所以嵌入模型没有做“词袋”假设？</h2>
<p>好吧，这要看你的意思是什么。显然，本地窗口在某种意义上有助于考虑词序。但是在窗口内，模型通常将其视为词袋（即无序的）。</p>
<p><img loading="lazy" src="img/%e8%af%8d%e8%a2%8b%e6%b3%95%e4%b8%8e%e8%af%8d%e5%b5%8c%e5%85%a5.png" alt=""  />
</p>
<br>
<h2 id="7-嵌入向量与我在文本数据课程中学习的向量空间模型有关系吗">7. 嵌入向量与我在文本数据课程中学习的“向量空间”模型有关系吗？</h2>
<p><strong>有也没有</strong>。在典型的向量空间模型中，每个文档都是一个实值向量（通常是计数）。因此，“dog eat dog world”可能表示为<code>[2, 1, 1]</code>，其中第一个元素表示“dog”，第二个表示“eat”，第三个表示“world”等等。在词嵌入中，每个单词都有自己的向量，而这些向量是由模型学习的。它与向量空间模型有关，因为单词在多维空间中以向量形式表示。</p>
<br>
<h2 id="8-听起来很有趣但是使用单词嵌入有什么好处">8. 听起来很有趣。但是使用单词嵌入有什么好处？</h2>
<p>事实证明，以这种方式表示单词对于许多“下游”的自然语言处理和机器学习任务是有帮助的。例如，词性标注：嵌入可以帮助我们区分单词在给定上下文中使用的“意义”。更普遍地，<strong>了解概念之间的关系可能是有用的</strong>：如果我们知道在我们的语料库中，“雨伞”比“晒霜”更接近于“雨衣”，我们可能想要向那些寻找雨衣而不是晒霜的人推广雨伞。 在政治学中的一个自然应用是建立词典：如果“共和党”在嵌入空间中靠近“保守派”和“新保守主义者”，那么这可能告诉我们，我们应该将所有这些都视为美国政治中右翼意识形态的例子。</p>
<p>有一个经典的关于嵌入的运算公式， 假设我们有以下单词的嵌入向量：“国王”，“女王”，“男人”，“女人”。对于某些规范，事实证明，大致上有： “国王”-“男人”+“女人”=“女王”。也就是说，“国王”类似于“女王”，就像“男人”类似于“女人”一样。</p>
<p><img loading="lazy" src="img/word2vec-king-queen.png" alt=""  />
</p>
<p><img loading="lazy" src="img/word2vec-semantic-queen-king.jpeg" alt=""  />
</p>
<br>
<h2 id="9-你已经说服我了那么如何获得这些嵌入">9. 你已经说服我了。那么如何获得这些嵌入？</h2>
<p>你需要一个模型。有很多很多选择，从2000年初期的  **神经neural ** 网络模型开始。</p>
<br>
<h2 id="10-哇神经听起来非常复杂啊">10. 哇，“神经”听起来非常复杂啊？</h2>
<p>不，实际上不是。这些模型也已经存在很长时间了（至少自1990年代末/2000年代初以来）。重申一下，它们只是因为现在算力最近变得快速，具有可扩展性。</p>
<br>
<h2 id="11我应该使用哪种模型">11.我应该使用哪种模型？</h2>
<p>由你自己决定，但最受欢迎的是：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>年份</th>
<th>资料</th>
</tr>
</thead>
<tbody>
<tr>
<td>Word2Vec</td>
<td>2013</td>
<td>论文 <a href="https://arxiv.org/pdf/1310.4546.pdf">https://arxiv.org/pdf/1310.4546.pdf</a><br>代码 <a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a></td>
</tr>
<tr>
<td>Glove</td>
<td>2014</td>
<td>网站 <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></td>
</tr>
</tbody>
</table>
<br>
<h2 id="12-什么是-word2vec">12. 什么是 Word2Vec？</h2>
<p>它是一种实现 <strong>词嵌入</strong> 的方法，有两种不同的类型（称为“体系结构”）：</p>
<ol>
<li><strong>连续词袋（CBOW）</strong>。这假设您想要在给定上下文词（来自本地窗口）的情况下预测目标词（上面的“音乐”），<strong>这有点像做英文完形填空</strong>。</li>
<li><strong>Skip-gram</strong>。这假设您想要预测给定特定单词（在我们的例子中为 <strong>music</strong> ）的上下文词（在 <strong>music</strong> 周围的本地窗口中的内容）。</li>
</ol>
<br>
<h2 id="13-word2vec如何拟合数据">13. Word2Vec如何拟合数据？</h2>
<p>它通过单词与单词之间进行，尝试预测目标单词或上下文，具体取决于所需的体系结构。最终，它使用神经网络模型来完成这个任务。</p>
<br>
<h2 id="14-word2vec-是深度学习吗">14. Word2Vec 是深度学习吗？</h2>
<p>不是的。word2vec 的神经网络只有一层，所以它不是真正的 <strong>深度学习Deep Learning</strong> ， 而是“浅层学习”。</p>
<br>
<h2 id="15-什么是-glove-">15. 什么是 GloVe ？</h2>
<p><strong>GloVe</strong> 的全称是 Global Vectors for Word Representation， 它也是一种生成 <strong>词嵌入</strong> 的方法，字面上表示用于单词表示的全局向量。<br></p>
<h2 id="16-glove-如何拟合数据">16. GloVe 如何拟合数据？</h2>
<p>它使用 “全局”（聚合）共现计数。请注意，<code>Word2Vec</code>不会这样做：它按单词进行处理，从不将整个语料库视为一个整体。</p>
<br>
<h2 id="17-那么-glove-不是深度学习">17. 那么 GloVe 不是深度学习？</h2>
<p>不是，但<code>Word2Vec</code>也不是（参见上面）。</p>
<br>
<h2 id="17-哪个更好-glove-还是-word2vec">17. 哪个更好？ GloVe 还是 Word2Vec？</h2>
<p>没有。首先，从根本上讲，它们在所做的事情上是基本相似的。<strong>有一些证据表明，在某些任务，GloVe 更稳定，表现更好</strong>。</p>
<p>在我们的研究中，我们发现，“开箱即用”，相对于<code>Word2Vec</code>（skipgram），<code>GloVe</code>最初在建议提示词的良好最近邻方面表现更好（请参见下面的#27）。但是，一旦我们将<code>Word2Vec</code>词汇子集排除了非常罕见的单词，就在人类编码器偏好方面表现几乎相同。</p>
<br>
<h2 id="18-我有一个语料库如何使用这些模型">18. 我有一个语料库，如何使用这些模型？</h2>
<p>你使用 word2vec 或 glove 代码， 并将文本数据(语料)导入到代码中， 运行得到词嵌入模型文件。大邓的 cntext库支持两种算法的实现，需要注意的是， glove训练速度较慢， 而斯坦福大学训练Glove使用的是C语言代码。</p>
<ul>
<li>如果担心Glove速度，就用斯坦福的代码。</li>
<li>如果想简单点，不考虑速度，可以考虑大邓整理出的cntext库</li>
</ul>
<p><a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></p>
<br>
<h2 id="19-嗯听起来很复杂还有其他选择吗">19. 嗯。听起来很复杂。还有其他选择吗？</h2>
<p>很多时候自己训练的词嵌入模型耗时耗力， 其实如果你研究对象所涉及的数据与已有研究机构使用的语料是类似的， 也可以使用研究机构公开出的  <strong>预先训练好的词嵌入模型</strong> 。</p>
<br>
<h2 id="20-如果我的语料库与维基百科完全不同怎么办">20. 如果我的语料库与维基百科完全不同怎么办？</h2>
<p>你可以将 word2vec 或 glove 拟合到你的文本数据中。但请注意，拟合的数据越多，词嵌入模型任务表现的越好）。因此，你可能需要相当多的数据：几百M 几个G 甚至更多，而不是区区 50 份宣言文档。</p>
<br>
<h2 id="21--好的本地训练还是使用预训练模型还有哪些决策需要做">21.  好的，本地训练还是使用预训练模型。还有哪些决策需要做？</h2>
<p>回到 <strong>FAQ 18</strong>，你需要做出决策的关键参数包括：</p>
<ul>
<li><strong>窗口大小</strong>：也就是你的单词周围的（对称的）窗口大小。例如2、4、6、10等。</li>
<li><strong>嵌入向量长度</strong>：代表单词的向量大小。例如50、100、300、450等。</li>
</ul>
<p>需要注意的是，即使你使用 <strong>预训练的词嵌入模型</strong>，你也需要做出这些决策，因为你将根据需求下载不同的词嵌入预训练模型。</p>
<p><img loading="lazy" src="img/%e6%a8%a1%e5%9e%8b%e5%86%b3%e7%ad%96.png" alt=""  />
</p>
<br>
<h2 id="22-最佳的窗口大小如何确定">22. 最佳的窗口大小如何确定？</h2>
<p>对社会科学而言，窗口大小并没有固定的说法。 总体上呢，我们知道更大的窗口（&gt;2）捕捉更多主题关系（例如“Obama-President”），而更小的窗口（&lt;2）则捕捉词汇前后的语法关系（例如“jumps-jumping”）。</p>
<br>
<h2 id="23--最佳的词嵌入向量长度维度数如何确定">23.  最佳的词嵌入向量长度(维度数)如何确定？</h2>
<p>对社会科学而言，<strong>维度数</strong> 也没有固定的说法。 总体上， 我们知道更大的维度数可以捕捉更多词汇语义信息。 但是维度数过大也可能不是最优的，因为这会导致冗余（即有很多维度没有作用，可能只是捕捉噪声）。但是太小也不好：极限情况下，可能无法区分单词。常见的维度数如50， 100， 200维</p>
<br>
<h2 id="24-好的还有什么需要不需要了解的">24. 好的，还有什么需要（不需要）了解的？</h2>
<p>词嵌入的向量并不稳定。如果你在本地拟合你的语料库，每次你会得到不同的结果。这是由于模型的性质（<strong>机器学习类算法大多是要进行模型随机初始化，这导致每次得到的模型会有差异</strong>），这基本上是一个事实。  <strong>缓解这种情况的一种方法是多次估计相同的模型并在所需的距离度量上进行聚合</strong>。</p>
<br>
<h2 id="25-还有其他需要注意的事项吗">25. 还有其他需要注意的事项吗？</h2>
<p>是的，还有一些与技术模型拟合相关的超参数可以选择（例如收敛阈值、迭代次数）。但是我们不会深入讨论这些问题，因为许多这些问题都有适当的默认值。</p>
<br>
<h2 id="26-好的那么对于我的政治科学案例我该怎么做呢">26. 好的。那么对于我的「政治科学案例」，我该怎么做呢？</h2>
<p>很高兴你问到这个问题。我们的论文涵盖了政治科学中几个语料库的这些问题，我们认为这些语料库在实际研究中是有一定代表性的。我们评估了在超参数选择之间变化时“性能”如何变化以及它如何影响稳定性。</p>
<br>
<h2 id="27-我们如何评估政治科学中的嵌入性能">27. 我们如何评估政治科学中的嵌入“性能”？</h2>
<p>我们使用了四个工具：</p>
<ol>
<li>技术标准：本质上是“适合度”（作为窗口大小的函数），加上计算时间。</li>
<li>稳定性： 同一模型的不同运行在关键词的最近邻居方面的相关性如何。</li>
<li>查询搜索排名相似性： 两个不同模型的查询单词的最近邻居有多相似。</li>
<li>图灵测试：请人类比较嵌入模型生成的最近邻居和（不同、独立的）人类编码器生成的最近邻居。</li>
</ol>
<br>
<h2 id="28-在上下文中什么是最近邻查询单词是什么">28. 在上下文中，什么是最近邻？查询单词是什么？</h2>
<p>无论规范如何，<strong>词嵌入模型</strong> 都会为词汇表中的任何单词提供向量表示。 对于给定的单词的向量——我们可以使用一定的算法(余弦相似性最大)找到距离ta最近的n个词，这些是它的最近邻，彼此语义会比较紧密相似。</p>
<p>显然，我们不会检查词汇表中的每个单词，而是某些与政治科学有关的单词。对我们而言，这些单词与以下内容相关：</p>
<ul>
<li>大的议题，如： <code>民主</code>，<code>自由</code>，<code>平等</code>，<code>正义</code>。</li>
<li>美国的政策辩论：<code>移民</code>，<code>堕胎</code>，<code>福利</code>，<code>税收</code>。</li>
<li>党派： <code>共和党</code>，<code>民主党</code></li>
</ul>
<p>我们也可以选择其他单词，但我们认为这些是一个不错的开始。</p>
<br>
<h2 id="29-我们的语料库是什么">29. 我们的语料库是什么？</h2>
<p>我们的重点语料库是第102-111届国会的 <strong>Congressional Record</strong> 记录。它是美式英语，大约有140万个文档。 但我们还有其他语言的语料库（见下文）。</p>
<br>
<h2 id="30-我们尝试了哪些参数组合">30. 我们尝试了哪些参数组合？</h2>
<ul>
<li>窗口大小：1、6、12、24、48</li>
<li>嵌入维度：50、100、200、300、450</li>
</ul>
<br>
<h2 id="31-我们使用哪个模型">31. 我们使用哪个模型？</h2>
<p>我们同时使用 <strong>GloVe</strong> 和 <strong>Word2Vec</strong>。但是在文章的前两个部分，我们将 <strong>GloVe</strong> 视为一种基准线。 然后在论文的后面，我们将 <strong>GloVe</strong> 的性能与 <strong>Word2Vec</strong> 进行比较，以人类编码者的喜好为衡量标准。在实践中，由于模型使用的词汇不同，这种比较稍微有些复杂。</p>
<br>
<h2 id="32-图灵测试是如何实际工作的">32. 图灵测试是如何实际工作的？</h2>
<p>我们给人类编码者一组查询或提示词（<strong>来自FAQ 28</strong>）。然后告诉他们编写（十个）与这些单词相似，意义相近，且可能会出现在语料库中等等的单词。从“机器”方面，我们提取模型建议的十个最近邻。最后，我们要求（不同的，独立的）人类编码者比较“人类”的最近邻和“机器”的最近邻（我们不告诉他们哪个是哪个）。他们只需要告诉我们哪一个更适合所述提示单词。</p>
<p>这是一种近似的“图灵测试”，因为我们试图找出机器（模型）是否能够很好地模仿人类的能力。</p>
<br>
<h2 id="33-基于技术标准的结果如何">33. 基于技术标准的结果如何？</h2>
<p>首先，请注意，要评估这一点，我们参考的是 <strong>GloVe</strong> 本地拟合我们的***Congressional Record ***语料库。</p>
<p><strong>更大的窗口和更多的嵌入维度总是改善模型拟合——但是二者都存在递减的回报</strong>。毫无疑问，使用非常小的<strong>嵌入维度数</strong>（&lt;100）对拟合是不好的。并不奇怪，更大的模型——更大的窗口，更长的向量——更需要时间来拟合。但即使是最大的模型也不那么糟糕：拟合最大的模型需要大约3个小时左右。</p>
<p>在交换拟合与时间方面，使用<strong>标准选项6-300</strong>（即窗口大小为6，嵌入维度为300）是一个合理的选择。</p>
<br>
<h2 id="34-稳定性的结果是什么">34. 稳定性的结果是什么？</h2>
<p>在其他条件相同的情况下，较大窗口size的模型更加稳定，但是在窗口size过大，尤其是在维度数很大的模型中，会出现一些性能下降。较少的维度数（其他条件相同）似乎也不太稳定。<strong>少量的维度数和小的窗口大小会产生特别高的方差结果</strong>。</p>
<br>
<h2 id="35-查询排名的相关性如何">35. 查询排名的相关性如何？</h2>
<p>首先，无论查看关键政治单词还是随机单词，所有预训练模型都显示出高相关性（通常&gt; 0.6）。也就是说，它们之间实际上没有太多“实质性”差异。 预训练模型和本地模型在这方面，相关性也很高（&gt; 0.5），尤其是对于我们的政治单词。</p>
<p><strong>这表明使用现成的预训练嵌入模型 与使用本地拟合的模型能产生近似的结论。</strong></p>
<br>
<h2 id="36-人类更喜欢哪一个人类还是机器">36. 人类更喜欢哪一个：人类还是机器？</h2>
<p>令人惊讶的是，在我们的政治查询集合中，并没有明显的优胜者。 在自己电脑本地训练出的词嵌入模型通常可以达到人类表现约70%。预训练嵌入模型的表现更好：它们可以达到高达87%的人类表现。</p>
<br>
<h2 id="37-这些发现是否具有普适性还是只适用于这个语料库">37. 这些发现是否具有普适性？还是只适用于这个语料库？</h2>
<p>我们在几个语料库上进行了相同的实验（不包括 Turing 测试），结果与上述结果大致相似。也就是说，我们认为这些结果并不是源于 <em>Congressional Record</em> 这个语料库本身的原因。</p>
<p>我们看过的其他语料库包括：</p>
<table>
<thead>
<tr>
<th>语料库</th>
<th>语言</th>
<th>年份</th>
<th>数据量</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hansard</td>
<td>英国英语</td>
<td>1935-2013年</td>
<td>约4.5M个文档</td>
</tr>
<tr>
<td>State of the Union国情咨文</td>
<td>美国英语</td>
<td>1790-2018年</td>
<td>小语料库</td>
</tr>
<tr>
<td>Cortes Generales</td>
<td>西班牙语</td>
<td>1993-2018年</td>
<td>约1.3M个文档</td>
</tr>
<tr>
<td>Deutscher Bundestag</td>
<td>德语</td>
<td>1998-2018年</td>
<td>约1.2M个文档</td>
</tr>
</tbody>
</table>
<br>
<h2 id="38-最终结论在应用设置中研究人员应该怎么做">38. 最终结论：在应用设置中，研究人员应该怎么做？</h2>
<p>对于政治学而言，<strong>预训练词嵌入模型</strong> 的标准设定是 <strong>6-300 GloVe模型</strong>（窗口大小=6，维数=300）的表现与其他任何方法相比都差不多。这些模型与您进行的其他事情相当高度相关。更重要的是，人类喜欢这些结果——尤其是与本地训练模型相比，它甚至接近人类生成的结果。显然，使用预训练模型更便宜。但我们的经验是，这可能本身并不足以说服人，因为对于大约100万个文档的语料库而言，本地训练处这些模型是相当快速。</p>
<p>您可能会通过进行本地拟合来在实质上获得一些收益，但从我们的人类验证来看，这些收益并不明显。但是，您可能正在优化一些标准，而我们并不知道这些标准是什么。<strong>如果您必须进行本地拟合，则要避免小窗口和小维数，特别是在结合使用时</strong>。</p>
<br>
<h2 id="39-最后的想法是">39. 最后的想法是</h2>
<p>我们没有研究词嵌入是否本身相对于其他算法（如词袋法）代表着更好的东西。粗略地说，仅仅因为词嵌入模型现在非常流行，就不意味着它们应该取代您正在进行的主题模型或其他模型。</p>
<p>这是一个发展迅速的领域。更复杂，表现更好的词嵌入模型版本将会（已经）出现。因此，我们认为我们的关键贡献是图灵测试的安排：我们推测，在政治科学中，受监督的问题将继续相对较少，因此学者们应该确保他们的文档和单词的模型表示确实与有用的东西相符。</p>
<p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>基于词嵌入技术的心理学研究: 方法及应用</title>
      <link>https://textdata.cn/blog/2023-03-10-psychological-research-with-word-embeddings/</link>
      <pubDate>Fri, 10 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-10-psychological-research-with-word-embeddings/</guid>
      <description>词嵌入是自然语言处理的一项基础技术。 其核心理念是根据大规模语料中词语和上下文的联系, 使用神经网络等机器学习算法自动提取有限维度的语义特征, 将每个词表示为一个低维稠密的数值向量(词向 量), 以用于后续分析。 心理学研究中, 词向量及其衍生的各种语义联系指标可用于探究人类的语义加工、认知判断、发散思维、社会偏见与刻板印象、社会与文化心理变迁等各类问题。 未来, 基于词嵌入技术的心理 学研究需要区分心理的内隐和外显成分, 深化拓展动态词向量和大型预训练语言模型(如 GPT、BERT)的应用, 并在时间和空间维度建立细粒度词向量数据库, 更多开展基于词嵌入的社会变迁和跨文化研究。 As a fundamental technique in natural language processing (NLP), word embedding quantifies a word as a low-dimensional, dense, and continuous numeric vector (i.e., word vector). Word embeddings can be obtained by using machine learning algorithms such as neural networks to predict the surrounding words given a word or vice versa (Word2Vec and FastText) or by predicting the probability of co-occurrence of multiple words (GloVe) in large-scale text corpora. Theoretically, the dimensions of a word vector reflect the pattern of how the word can be predicted in contexts; however, they also connote substantial semantic information of the word. Therefore, word embeddings can be used to analyze semantic meanings of text. In recent years, word embeddings have been increasingly applied to study human psychology, including human semantic processing, cognitive judgment, divergent thinking, social biases and stereotypes, and sociocultural changes at the societal or population level. Future research using word embeddings should (1) distinguish between implicit and explicit components of social cognition, (2) train fine-grained word vectors in terms of time and region to facilitate cross-temporal and cross-cultural research, and (3) apply contextualized word embeddings and large pre-trained language models such as GPT and BERT. To enhance the application of word embeddings in psychology。</description>
      <content:encoded><![CDATA[<p><a href="https://psychbruce.github.io/">包寒吴霜博客 https://psychbruce.github.io/</a></p>
<p><img loading="lazy" src="img/%e5%8c%85%e5%af%92%e5%90%b4%e9%9c%9c.png" alt=""  />
</p>
<br>
<p><img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-01.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-02.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-03.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-04.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-05.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-06.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-07.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-08.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-09.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-10.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-11.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-12.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-13.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>可视化 | 词嵌入模型用于计算社科领域刻板印象等信息（含代码）</title>
      <link>https://textdata.cn/blog/2023-03-03-extracts-cognitive-information-and-visualization-with-embedings/</link>
      <pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-03-extracts-cognitive-information-and-visualization-with-embedings/</guid>
      <description>语言的文字反映了人类思想的结构，使我们能够在个人之间传递思想， 而使用大规模语料训练得来的词嵌入模型蕴含着这类信息。 英文的词嵌入在社会科学中的应用教程较多，大家可以谷歌查询，我主要想丰富中文数据的教程。The words of language reflect the structure of human thought, allowing us to transfer thoughts between individuals, and word embedding models trained using large-scale corpora contain this information. There are many application tutorials of English word embedding in social science. You can search it on Google. I mainly want to enrich the tutorials of Chinese data.</description>
      <content:encoded><![CDATA[<iframe
    src="//player.bilibili.com/player.html?bvid=BV1CY4y11712&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<p>语言的文字反映了人类思想的结构，使我们能够在个人之间传递思想， 而使用大规模语料训练得来的词嵌入模型往往蕴含着这类信息。</p>
<br>
<h3 id="11-国内外社科方面的应用">1.1 国内外社科方面的应用</h3>
<p><strong>在国内社科领域， 应用词嵌入 主要用来做情感分析</strong>，大致的算法(思路)是</p>
<ol>
<li>训练词向量模型</li>
<li>根据词向量cosine或欧几里得距离，度量词语的相似性，进而扩展某种概念词典</li>
<li>检查扩充的概念词典，剔除无效词后。</li>
<li>使用整理好的概念词典，统计文本中出现该概念的词频，当做该概念的代理指标。</li>
</ol>
<p>但词嵌入在国外社科领域， 不用传统方法，使用文本数据，也能做出 **刻板印象、性别种族歧视、词语百年来语义变迁、女性高管就职后公司内性别观念变化、测量创新力(发散思维)**等议题的实证研究。</p>
<p>下图是「阶级财富性别与运动」，摘自2019年文化几何学这篇论文。</p>
<p><img loading="lazy" src="img/sport_class_fortune.png" alt=""  />
</p>
<blockquote>
<p>Kozlowski, Austin C., Matt Taddy, and James A. Evans. &ldquo;The geometry of culture: Analyzing the meanings of class through word embeddings.&rdquo; American Sociological Review 84, no. 5 (2019): 905-949.</p>
</blockquote>
<p>本文主要内容是实现这类文化几何学图的中文可视化。</p>
<br>
<h3 id="12-之前分享过的资料">1.2 之前分享过的资料</h3>
<p>之前大邓分享过的词嵌入稍有涉及，感兴趣的可以阅读我之前分享的文章</p>
<ul>
<li><a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></li>
<li><a href="https://textdata.cn/blog/2022-10-16-python-word-mover-s-distance/">Python | 词移距离(Word Mover’s Distance)</a></li>
<li><a href="https://textdata.cn/blog/wordbias/">WordBias库 | 发现偏见(刻板印象)的交互式工具</a></li>
<li><a href="https://textdata.cn/blog/embeddingsandattitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
<li><a href="https://textdata.cn/blog/whatlies_word2vec/">whatlies库 | 可视化词向量</a></li>
<li><a href="https://textdata.cn/blog//2022-11-14-pnas_naming_unrelated_words_predicts_creativity/">PNAS | 使用语义距离测量一个人的创新力(发散思维)得分</a></li>
<li><a href="https://textdata.cn/blog/embeddings_resource_usage_method/">中文词向量资源汇总 &amp; 使用方法</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
</ul>
<br>
<p>但可视化分享的不多，<strong>本文将用公开的中文预训练模型，验证可视化中文圈的群体记忆、刻板印象、偏见等信息</strong>。在此先放一张论文中两个截图， <strong>语义概念向量</strong> 一般是由语义相反的两组词构成。</p>
<p><img loading="lazy" src="img/size.png" alt=""  />

<img loading="lazy" src="img/%e4%ba%8c%e5%85%83%e6%a6%82%e5%bf%b5%e7%bb%84.png" alt=""  />
</p>
<blockquote>
<p>Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. <strong>Semantic projection recovers rich human knowledge of multiple object features from word embeddings</strong>. <em>Nature Human Behaviour</em>, pp.1-13.</p>
</blockquote>
<br>
<h3 id="概念向量的计算方法">概念向量的计算方法</h3>
<ul>
<li>二维坐标系下，点和向量都可以用二维数组(m, n)表示。同理，在n维空间中，点和向量都是n维数组。</li>
<li>将多个近义的词向量， 通过平均法创建出一个 <strong>均值端点</strong>。</li>
<li>语义完全相反的两个<strong>均值端点</strong>， 通过减法操作， 得到 <strong>概念向量</strong></li>
</ul>
<br>
<h3 id="为啥每个端点向量用多个词计算">为啥每个端点向量用多个词计算？</h3>
<p>单个词变动较大， 为了保证语义的稳定性，最好是找一组词构成概念的一个端点。</p>
<p><br><br></p>
<h2 id="二准备工作">二、准备工作</h2>
<p>下载预训练模型，可以查看这篇文章获取</p>
<p><a href="https://textdata.cn/blog/embeddings_resource_usage_method/">中文词向量资源汇总 &amp; 使用方法</a></p>
<p>之后安装好本节需要的python包</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">whatlies</span><span class="o">==</span><span class="mf">0.7.0</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">matplotlib_inline</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">gensim</span><span class="o">==</span><span class="mf">4.2.0</span>
</code></pre></div><p><br><br></p>
<h2 id="三导入预训练模型">三、导入预训练模型</h2>
<p>使用 gensim 库导入预训练模型，这里我本地保留的是预训练模型是word2vec中的sgns算法训练出来的。 导入后的数据是 KeyedVectors 类型的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1"># 微博 sgns.weibo.word.bz2 为例  </span>
<span class="n">weibo_wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.weibo.word.bz2&#39;</span><span class="p">,</span> 
                                             <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                             <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># 知乎 sgns.renmin.word.bz2</span>
<span class="n">zhihu_wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.zhihu.word.bz2&#39;</span><span class="p">,</span> 
                                              <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                              <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># 中文维基 </span>
<span class="n">wiki_wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.wiki.word.bz2&#39;</span><span class="p">,</span> 
                                              <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                              <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="31-预训练模型的词汇量">3.1 预训练模型的词汇量</h3>
<p>weibo_wv、zhihu_wv、wiki_wv是KeyedVectors类型的数据，可以直接查看词汇量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;预训练模型词汇量&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;微博: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">weibo_wv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;知乎: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">zhihu_wv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;中文维基: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">wiki_wv</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">预训练模型词汇量

微博:  195202
知乎:  259949
中文维基:  352277
</code></pre></div><br>
<h3 id="32-通用词">3.2 通用词</h3>
<p>使用不同数据集训练，得到的语言模型所含词语会有差异。这里我们查看通用词一共有多少</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wiki_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">wiki_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
<span class="n">zhihu_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
<span class="n">weibo_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">weibo_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>

<span class="c1">#交集</span>
<span class="n">common_vocab</span> <span class="o">=</span> <span class="n">wiki_vocab</span> <span class="o">&amp;</span> <span class="n">zhihu_vocab</span> <span class="o">&amp;</span>  <span class="n">weibo_vocab</span><span class="c1"># intersection</span>

<span class="nb">len</span><span class="p">(</span><span class="n">common_vocab</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">118539
</code></pre></div><br>
<h3 id="33-提取某个词的向量">3.3 提取某个词的向量</h3>
<p>以维基百科为例， 查看「幸福」的词向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#只显示向量的前20个数字</span>
<span class="n">wiki_wv</span><span class="p">[</span><span class="s1">&#39;幸福&#39;</span><span class="p">][:</span><span class="mi">20</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 0.159344,  0.280468, -0.236876, -0.198076, -0.170838,  0.027264,
           -0.349646,  0.289169, -0.421038, -0.470539,  0.247534,  0.112968,
            0.355498,  0.479956,  0.093291,  0.081054, -0.046995, -0.624586,
            0.568242,  0.16665 ], dtype=float32)
</code></pre></div><br>
<h3 id="34-查看词向量的维度">3.4 查看词向量的维度</h3>
<p>查看向量的长度（维度），以「幸福」为例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;预训练模型维度数&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;微博: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">weibo_wv</span><span class="p">[</span><span class="s2">&#34;幸福&#34;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;知乎: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">zhihu_wv</span><span class="p">[</span><span class="s2">&#34;幸福&#34;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;中文维基: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">wiki_wv</span><span class="p">[</span><span class="s2">&#34;幸福&#34;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">预训练模型维度数
微博:  300
知乎:  300
中文维基:  300
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#单个词向量的尺寸</span>
<span class="n">wiki_wv</span><span class="p">[</span><span class="s1">&#39;幸福&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(300,)
</code></pre></div><br>
<h3 id="35-计算多个词向量的均值向量">3.5 计算多个词向量的均值向量</h3>
<p>先看一下多个词提取后得到的数据形状</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(4, 300)
</code></pre></div><br>
<p>4个词，每个词都是300维的词向量。如果计算4个词向量的均值向量，返回的尺寸应该是 (300,)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">male_vector</span> <span class="o">=</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">male_vector</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(300,)
</code></pre></div><br>
<h3 id="36-最相似的词">3.6 最相似的词</h3>
<p>网上的教程经常分享最相似的词，这里我们也实验一下。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wiki_wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;于社会&#39;, 0.6210986971855164),
(&#39;社会上&#39;, 0.5995474457740784),
(&#39;社会关系&#39;, 0.5894029140472412),
(&#39;各阶层&#39;, 0.5799717903137207),
(&#39;社会制度&#39;, 0.5777087211608887),
(&#39;社会变迁&#39;, 0.5756841897964478),
(&#39;令社会&#39;, 0.575627326965332),
(&#39;社会变革&#39;, 0.5755838751792908),
(&#39;思想观念&#39;, 0.5752044916152954),
(&#39;社会存在&#39;, 0.573627769947052)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">weibo_wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;金钱至上&#39;, 0.5815222859382629),
(&#39;各阶层&#39;, 0.5668456554412842),
(&#39;福利制度&#39;, 0.5532322525978088),
(&#39;官与民&#39;, 0.5526734590530396),
(&#39;高考制度&#39;, 0.5515810251235962),
(&#39;资源分配&#39;, 0.5500271916389465),
(&#39;功利主义&#39;, 0.5484314560890198),
(&#39;分级制&#39;, 0.5450907349586487),
(&#39;功利化&#39;, 0.5432640910148621),
(&#39;法制建设&#39;, 0.5420899391174316)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;社会存在&#39;, 0.6277482509613037),
 (&#39;社会生活&#39;, 0.613935649394989),
(&#39;社会群体&#39;, 0.6123108863830566),
(&#39;社会意识&#39;, 0.6055717468261719),
(&#39;物欲横流&#39;, 0.6041101217269897),
(&#39;民主决策&#39;, 0.602908194065094),
(&#39;阶级分化&#39;, 0.59609454870224),
(&#39;社会上&#39;, 0.5932644605636597),
(&#39;于社会&#39;, 0.5919737219810486),
(&#39;法制化&#39;, 0.5820874571800232)]
</code></pre></div><p><br><br></p>
<h2 id="四-可视化">四、 可视化</h2>
<p>为了让中文可以在matplotlib正常显示， 需要先运行下方代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>

<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>

<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># 如果是其他系统，可以使用系统默认字体</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>
</code></pre></div><br>
<h3 id="41-运动的贫富和性别属性">4.1 运动的贫富和性别属性</h3>
<p>先看一个最难的例子， 后面的例子都是围绕ta展开的。</p>
<ul>
<li><strong>性别向量</strong> 由 <strong>男性均值端点向量</strong> 和 <strong>女性均值端点向量</strong> 计算得来</li>
<li><strong>贫富向量</strong> 由 <strong>富裕均值端点向量</strong> 和 <strong>贫穷均值端点向量</strong> 计算得来</li>
</ul>
<p>需要注意， 不论是 <strong>性别向量</strong>、<strong>贫富向量</strong> 还是运动词的词向量，都是 300维的向量。 如果在低维空间，例如2维坐标轴中可视化，需要做投影操作。这里需要一点大学线性代数的点乘知识。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 获取需要绘制的单词列表</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;足球&#39;</span><span class="p">,</span> <span class="s1">&#39;拳击&#39;</span><span class="p">,</span> <span class="s1">&#39;高尔夫&#39;</span><span class="p">,</span> <span class="s1">&#39;棒球&#39;</span><span class="p">,</span> <span class="s1">&#39;芭蕾&#39;</span><span class="p">]</span>

<span class="c1"># 获取词向量，并转换为 NumPy 数组</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">wiki_wv</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>

<span class="c1"># 选择两个词向量作为新坐标系的 x 轴和 y 轴</span>
<span class="n">x_axis</span> <span class="o">=</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span>  <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_axis</span> <span class="o">=</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;拮据&#39;</span><span class="p">,</span> <span class="s1">&#39;囊中羞涩&#39;</span><span class="p">,</span> <span class="s1">&#39;困难&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;贵气&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 计算每个词向量在新坐标系中的投影</span>
<span class="n">x_coords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">x_axis</span><span class="p">)</span>
<span class="n">y_coords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">y_axis</span><span class="p">)</span>

<span class="c1"># 绘制图形</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">,</span> <span class="n">y_coords</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="p">(</span><span class="n">x_coords</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_coords</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="c1"># 绘制 x 轴和 y 轴的十字线</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;贫富(贫下富上)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;运动的贫富和性别属性&#39;</span><span class="p">)</span>
<span class="c1">#plt.show()</span>


<span class="c1">#保存</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/运动的贫富和性别属性.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_25_0.png" alt="svg"  />
</p>
<br>
<h3 id="42-使用whatlies处理数据">4.2 使用whatlies处理数据</h3>
<p>上面的可视化代码太长了，使用whatlies可以简化代码量。我们把 KeyedVectors类 转为 EmbeddingSet类，这里就可以更容易的把点显示为带箭头的向量。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">whatlies</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">EmbeddingSet</span>

<span class="c1"># load vectors as whatlies EmbeddingSet</span>
<span class="n">wiki_emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="o">.</span><span class="n">from_names_X</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="n">wiki_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">,</span> 
                                     <span class="n">X</span><span class="o">=</span><span class="n">wiki_wv</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>

<span class="n">weibo_emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="o">.</span><span class="n">from_names_X</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="n">weibo_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">,</span>
                                      <span class="n">X</span> <span class="o">=</span> <span class="n">weibo_wv</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>

<span class="n">zhihu_emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="o">.</span><span class="n">from_names_X</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">,</span> 
                                      <span class="n">X</span><span class="o">=</span><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>

</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># list similar words, n=10</span>
<span class="n">wiki_emb</span><span class="o">.</span><span class="n">score_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(Emb[社会], 5.960464477539063e-08),
(Emb[于社会], 0.3789013624191284),
(Emb[社会上], 0.4004524350166321),
(Emb[社会关系], 0.410597026348114),
(Emb[各阶层], 0.42002809047698975),
(Emb[社会制度], 0.4222911596298218),
(Emb[社会变迁], 0.42431581020355225),
(Emb[令社会], 0.42437267303466797),
(Emb[社会变革], 0.424416184425354),
(Emb[思想观念], 0.4247954487800598)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">weibo_emb</span><span class="o">.</span><span class="n">score_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(Emb[社会], 0.0),
(Emb[金钱至上], 0.41847753524780273),
(Emb[各阶层], 0.43315428495407104),
(Emb[福利制度], 0.4467676877975464),
(Emb[官与民], 0.4473266005516052),
(Emb[高考制度], 0.4484187364578247),
(Emb[资源分配], 0.44997286796569824),
(Emb[功利主义], 0.4515683650970459),
(Emb[分级制], 0.45490920543670654),
(Emb[功利化], 0.4567357897758484)]
</code></pre></div><br>
<h3 id="43-whatlies默认可视化">4.3 whatlies默认可视化</h3>
<p>使用whatlies默认的效果绘制如下，但需要注意， 这里的Dimension0和Dimension1的含义是未知的。所以除了可视化， 含义解读起来比较困难。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># the default X and Y axes are the first two dimensions of the embedding vectors</span>
<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;马&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> 
                   <span class="n">color</span><span class="o">=</span><span class="s2">&#34;purple&#34;</span><span class="p">)</span>

<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;鲨鱼&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;blue&#34;</span><span class="p">)</span>

<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;鸟类&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;green&#34;</span><span class="p">)</span>
<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;人&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">)</span>
<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;蛇&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/output_30_0.svg" alt="svg"  />
</p>
<br>
<h3 id="44-使用端点向量当基向量">4.4 使用端点向量当基向量</h3>
<p>使用端点向量当基向量，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;美国&#34;</span><span class="p">,</span> <span class="s2">&#34;中国&#34;</span><span class="p">,</span> <span class="s2">&#34;俄罗斯&#34;</span><span class="p">,</span> <span class="s2">&#34;韩国&#34;</span><span class="p">]</span>

<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span><span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;弱小&#34;</span><span class="p">],</span> 
          <span class="n">y_axis</span><span class="o">=</span><span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;强大&#34;</span><span class="p">],</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">])</span>

<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;国家强弱&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/国家强弱.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_32_0.svg" alt="svg"  />
</p>
<br>
<p>按照我们的理解， 强大与弱小是方向相反的。但是如果将这两个词分别做基向量。如图所示，就体现不出方向。</p>
<p>同时，因为给定有意义的基向量作为坐标轴向量， 坐标轴含有了意义，可视化的结果可以看出语义信息的亲疏远近。</p>
<p>可以看到， 中美俄是大国强国，韩国是小国军事弱国。</p>
<br>
<h3 id="45-使用概念向量当做基向量">4.5 使用概念向量当做基向量</h3>
<p>当使用概念向量做基向量， 我们就能保留住词语之间的正反方向。避免 4.4 反义词之间无法体现方向性信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#待考察词</span>
<span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s1">&#39;足球&#39;</span><span class="p">,</span> <span class="s2">&#34;斗殴&#34;</span><span class="p">,</span> <span class="s1">&#39;高尔夫&#39;</span><span class="p">,</span> <span class="s1">&#39;篮球&#39;</span><span class="p">,</span> <span class="s1">&#39;芭蕾&#39;</span><span class="p">,</span> <span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;乒乓球&#39;</span><span class="p">,</span> <span class="s1">&#39;举重&#39;</span><span class="p">]</span>

<span class="c1">#性别概念向量</span>
<span class="n">sex_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>
<span class="c1">#贫富概念向量</span>
<span class="n">disparity_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;贵气&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;拮据&#39;</span><span class="p">,</span> <span class="s1">&#39;囊中羞涩&#39;</span><span class="p">,</span> <span class="s1">&#39;困难&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> 

<span class="c1">#概念向量 做 基向量</span>
<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span> <span class="n">sex_vector</span><span class="p">,</span> 
          <span class="n">y_axis</span><span class="o">=</span> <span class="n">disparity_vector</span><span class="p">,</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">])</span>


<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;运动中体现的贫富与性别信息&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;贫富(贫下富上)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/运动中体现的贫富与性别信息.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_34_0.svg" alt="svg"  />
</p>
<br>
<p>刚刚的图中加入了<code>男、女、贫穷、富裕</code>四个词，是为了帮助我们识别出方向来的，判断横纵坐标的含义和方向性。现在我们可以去掉这四个词，绘制更美观的图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s1">&#39;足球&#39;</span><span class="p">,</span> <span class="s1">&#39;高尔夫&#39;</span><span class="p">,</span> <span class="s1">&#39;篮球&#39;</span><span class="p">,</span> <span class="s1">&#39;芭蕾&#39;</span><span class="p">,</span> <span class="s1">&#39;乒乓球&#39;</span><span class="p">,</span> <span class="s1">&#39;举重&#39;</span><span class="p">,</span> <span class="s1">&#39;徒步&#39;</span><span class="p">]</span>

<span class="n">sex_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> 
<span class="n">disparity_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;贵气&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;拮据&#39;</span><span class="p">,</span> <span class="s1">&#39;囊中羞涩&#39;</span><span class="p">,</span> <span class="s1">&#39;困难&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> 

<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span> <span class="n">sex_vector</span><span class="p">,</span> 
          <span class="n">y_axis</span><span class="o">=</span> <span class="n">disparity_vector</span><span class="p">,</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">])</span>

<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;运动中体现的贫富与性别信息&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="c1">#plt.axis(&#39;off&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;贫富(贫下富上)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/运动中体现的贫富与性别信息.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_36_0.png" alt="svg"  />
</p>
<br>
<p>从上图可以看出， 在wiki百科中记录下的， 我们对不同运动是存在贫富、性别化的信息。这些信息根据研究场景，解读为<strong>刻板印象、态度偏好、文化记忆</strong>等。 我们再看一个例子， 把中国动物(含神兽)分别在性别维度和尺寸维度可视化。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s1">&#39;虎&#39;</span><span class="p">,</span> <span class="s1">&#39;龙&#39;</span><span class="p">,</span> <span class="s1">&#39;猫&#39;</span><span class="p">,</span> <span class="s1">&#39;燕子&#39;</span><span class="p">,</span> <span class="s1">&#39;蝴蝶&#39;</span><span class="p">]</span>

<span class="n">sex_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span><span class="o">-</span><span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>
<span class="n">size_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;大&#39;</span><span class="p">,</span> <span class="s1">&#39;庞&#39;</span><span class="p">,</span> <span class="s1">&#39;巨&#39;</span><span class="p">,</span> <span class="s1">&#39;高&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;矮&#39;</span><span class="p">,</span> <span class="s1">&#39;小&#39;</span><span class="p">,</span> <span class="s1">&#39;微&#39;</span><span class="p">,</span> <span class="s1">&#39;毫&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>


<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span> <span class="n">sex_vector</span><span class="p">,</span> 
          <span class="n">y_axis</span><span class="o">=</span> <span class="n">size_vector</span><span class="p">,</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">,</span> <span class="s2">&#34;yellow&#34;</span><span class="p">,</span> <span class="s2">&#34;grey&#34;</span><span class="p">])</span>

<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;文化中动物词蕴含的性别化和尺寸信息&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="c1">#plt.axis(&#39;off&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;尺寸(下小上大)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/文化中动物词蕴含的性别化和尺寸信息.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_38_0.svg" alt="svg"  />
</p>
<p><br><br></p>
<h2 id="代码获取">代码获取</h2>
<p>公众号:  大邓和他的Python， 同日期推文， 付费阅读获取全文教程、数据、代码~</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>simpleT5 库 | 根据英文摘要内容生成标题</title>
      <link>https://textdata.cn/blog/2023-02-23-simplet5-one-line-summary/</link>
      <pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-02-23-simplet5-one-line-summary/</guid>
      <description>T5（Text-to-Text Transfer Transformer）是一种基于 Transformer 架构的自然语言处理模型，由 Google Brain 团队开发。T5 模型采用了 encoder-decoder 架构，其中 encoder 将输入文本编码为向量，decoder 则从该向量生成目标文本。T5 模型的特点是将所有自然语言处理任务都视为“从输入文本到输出文本”的转换问题，它可以通过在任务之间共享模型参数和预训练模型来轻松地应用于各种 NLP 任务，如**文本分类、命名实体识别、文本摘要、问答系统**等。 与其他 NLP 模型不同的是，T5 模型使用了一种称为“text-to-text”方法的统一输入输出架构，使得所有 NLP 任务都能转化为文本转换问题，从而使得模型训练更加高效。</description>
      <content:encoded><![CDATA[<p>simpleT5 是基于 PyTorch 实现的 T5 模型库，旨在为用户提供一种简单、易用、可定制的 T5 模型工具。T5（Text-to-Text Transfer Transformer）是一种基于 Transformer 架构的自然语言处理模型，由 Google Brain 团队开发。T5 模型采用了 encoder-decoder 架构，其中 encoder 将输入文本编码为向量，decoder 则从该向量生成目标文本。</p>
<p><img loading="lazy" src="img/new_text_to_text.jpg" alt=""  />
</p>
<p>simpleT5 的设计目标是尽可能地减少 T5 模型的使用门槛，以方便用户在自然语言处理任务中快速应用 T5 模型，从而节省大量的模型开发时间和成本。</p>
<p>simpleT5 提供了一个简单的 API 接口，用户只需要提供输入文本和模型参数，即可轻松地使用 T5 模型进行文本转换任务，如<strong>文本摘要、机器翻译、对话系统</strong>等。simpleT5 还提供了一些预训练模型，包括 T5-small、T5-base 和 T5-large 等不同规模的模型，用户可以根据任务需求选择合适的模型。</p>
<p>除此之外，simpleT5 还提供了一些有用的工具和功能，如文本预处理、数据集加载、训练日志记录等，以帮助用户更轻松地进行模型训练和调试。simpleT5 的开发者们还提供了详细的文档和示例代码，以帮助用户更快地上手使用该库。</p>
<p>总之，simpleT5 为用户提供了一种快速、方便、可定制的 T5 模型工具，可以帮助用户在自然语言处理任务中更加高效地应用 T5 模型，节省大量的开发时间和成本。</p>
<p><br><br></p>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="n">simplet5</span>
</code></pre></div><p><br><br></p>
<h2 id="快速上手">快速上手</h2>
<p>t5模型有很多，如下图，今天以huggingface中公开的模型 <strong>snrspeaks/t5-one-line-summary为例， 展示 「根据传入的摘要内容生成对应的标题」。</strong></p>
<p><img loading="lazy" src="img/t5-models.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># pip install --upgrade simplet5</span>
<span class="kn">from</span> <span class="nn">simplet5</span> <span class="kn">import</span> <span class="n">SimpleT5</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleT5</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&#34;t5&#34;</span><span class="p">,</span><span class="s2">&#34;snrspeaks/t5-one-line-summary&#34;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    Global seed set to 42
    Downloading:   100%|          | 0.00/1.36k [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/850M [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/1.84k [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/773k [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/1.32M [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/1.74k [00:00&lt;?, ?B/s]
</code></pre></div><br>
<p>根据英文摘要生成标题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">abstract</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production 
</span><span class="s2">machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and 
</span><span class="s2">handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a 
</span><span class="s2">set of novel high-level, declarative abstractions. Overton&#39;s vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. 
</span><span class="s2">In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, 
</span><span class="s2">Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, 
</span><span class="s2">Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">abstract</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['Overton: Building, Deploying, and Monitoring Deep Machine Learning Systems']
</code></pre>
<br>
<p>根据摘要生成多个标题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">abstract</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production 
</span><span class="s2">machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and 
</span><span class="s2">handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a 
</span><span class="s2">set of novel high-level, declarative abstractions. Overton&#39;s vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. 
</span><span class="s2">In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, 
</span><span class="s2">Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, 
</span><span class="s2">Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="c1">#根据摘要生成5个标题</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">abstract</span><span class="p">,</span> 
              <span class="n">num_return_sequences</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> 
              <span class="n">num_beams</span><span class="o">=</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<pre><code>['Overton: Building, Deploying, and Monitoring Deep Machine Learning Systems',
 'Overton: Building, Deployment, and Improving Production Machine Learning Systems',
 'Overton: Building, Deploying, and Monitoring Machine Learning Systems for Engineers',
 'Overton: Building, Deploying, and Monitoring Machine Learning Systems',
 'Overton: Building, Deployment, and Monitoring Deep Machine Learning Systems']
</code></pre>
<p><br><br></p>
<h2 id="simplet5微调">simpleT5微调</h2>
<p>在 T5 模型的预训练阶段，它使用了巨大的文本语料库进行无监督的训练，以学习将输入文本转换为输出文本的能力。</p>
<p>预训练阶段结束后，T5 模型可以通过微调或迁移学习的方式用于各种下游 NLP 任务中，以实现最先进的性能表现。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">path</span> <span class="o">=</span> <span class="s2">&#34;https://raw.githubusercontent.com/Shivanandroy/T5-Finetuning-PyTorch/main/data/news_summary.csv&#34;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># simple5库传入的数据是DataFrames，必须含 &#34;source_text&#34; 和 &#34;target_text&#34;这两个字段。</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;headlines&#34;</span><span class="p">:</span><span class="s2">&#34;target_text&#34;</span><span class="p">,</span> <span class="s2">&#34;text&#34;</span><span class="p">:</span><span class="s2">&#34;source_text&#34;</span><span class="p">})</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;source_text&#39;</span><span class="p">,</span> <span class="s1">&#39;target_text&#39;</span><span class="p">]]</span>

<span class="c1"># T5 模型微调时候，source_text 数据都加入了前缀关键词summarise， 告诉 T5模型要做总结类任务的微调。</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;source_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;summarize: &#34;</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;source_text&#39;</span><span class="p">]</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<p>查看数据的形状</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_df</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">((78720, 2), (19681, 2))
</code></pre></div><br>
<p>开始进行 T5 模型的微调</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">simplet5</span> <span class="kn">import</span> <span class="n">SimpleT5</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleT5</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s2">&#34;t5&#34;</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&#34;t5-base&#34;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_df</span><span class="o">=</span><span class="n">train_df</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span>
            <span class="n">eval_df</span><span class="o">=</span><span class="n">test_df</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> 
            <span class="n">source_max_token_len</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> 
            <span class="n">target_max_token_len</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Downloading: 100%
792k/792k [00:36&lt;00:00, 21.6kB/s]

Downloading: 100%
1.39M/1.39M [00:02&lt;00:00, 641kB/s]

Downloading: 100%
1.20k/1.20k [00:00&lt;00:00, 3.50kB/s]

Downloading: 100%
892M/892M [00:32&lt;00:00, 27.4MB/s]

GPU available: True, used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M 
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Validation sanity check: 0%
0/2 [22:52&lt;?, ?it/s]
Global seed set to 42
Epoch 2: 100%
638/638 [04:07&lt;00:00, 2.57it/s, loss=1.02, v_num=0, val_loss=1.200, train_loss=1.130]
Validating: 100%
13/13 [00:01&lt;00:00, 7.43it/s]
Validating: 100%
13/13 [00:01&lt;00:00, 7.29it/s]
Validating: 100%
13/13 [00:01&lt;00:00, 7.30it/s]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># let&#39;s load the trained model for inferencing:</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&#34;t5&#34;</span><span class="p">,</span><span class="s2">&#34;outputs/SimpleT5-epoch-2-train-loss-0.9478&#34;</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">text_to_summarize</span><span class="o">=</span><span class="s2">&#34;&#34;&#34;summarize: Rahul Gandhi has replied to Goa CM Manohar Parrikar&#39;s letter, 
</span><span class="s2">which accused the Congress President of using his &#34;visit to an ailing man for political gains&#34;. 
</span><span class="s2">&#34;He&#39;s under immense pressure from the PM after our meeting and needs to demonstrate his loyalty by attacking me,&#34; 
</span><span class="s2">Gandhi wrote in his letter. Parrikar had clarified he didn&#39;t discuss Rafale deal with Rahul.
</span><span class="s2">&#34;&#34;&#34;</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">text_to_summarize</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;Rahul responds to Goa CM accusing him of using visit for political gain&#39;]
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>训练&amp;使用Glove语言模型， 可度量刻板印象等</title>
      <link>https://textdata.cn/blog/2022-11-22-glove-embeddings-model/</link>
      <pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-22-glove-embeddings-model/</guid>
      <description>训练&amp;amp;使用Glove语言模型， 可度量刻板印象等</description>
      <content:encoded><![CDATA[<p>Glove可以捕捉到词语在语料库中的全局语义信息和类比信息， 据此基于语义向量计算刻板印象、文化变迁等，Glove模型在计算社会科学中拥有很大的应用潜力。</p>
<p><img loading="lazy" src="img/wordpaths.png" alt=""  />
</p>
<p>训练Glove模型有两种实现方式</p>
<ol>
<li>C语言；  <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></li>
<li>Python语言；mittens、glove-python</li>
</ol>
<p><img loading="lazy" src="img/stanford%e8%ae%ad%e7%bb%83Glove.png" alt=""  />
</p>
<h2 id="方法比较">方法比较</h2>
<table>
<thead>
<tr>
<th style="text-align:left">方法</th>
<th style="text-align:left">优点</th>
<th style="text-align:left">缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">C语言</td>
<td style="text-align:left">速度快，现成的代码工具</td>
<td style="text-align:left">源代码仅支持英文, 需要付出较高的学习成本才能改动支持中文。 对文科生小白而言，门槛高</td>
</tr>
<tr>
<td style="text-align:left">Python语言</td>
<td style="text-align:left">mittens、glove-python等包语法简洁, 易上手</td>
<td style="text-align:left">对文科生还是有一定的门槛，代码运行速度慢</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td></td>
</tr>
</tbody>
</table>
<br>
<p>不考虑性能约束条件，更多地考虑易用性，大邓简化了Python代码，将其内置到了cntext库。</p>
<p>对词向量、词嵌入感兴趣的童鞋，可以阅读下列相关资料</p>
<ul>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
</ul>
<br>
<h2 id="glove代码">GloVe代码</h2>
<p>cntext支持中英文， 只需要7行代码，可完成导入数据、训练模型、保存结果。 这里以三体小说数据为例， 使用 <a href="santi.txt"><strong>data/santi.txt</strong></a> 。</p>
<p><strong>需要注意， santi.txt文件内文本是已经分词处理过的</strong>。这样可以在english这类西方语言模式下使用空格来区分词语的边界。</p>
<blockquote>
<p>如果使用英文数据，下面代码只需要更改数据文件的路径即可。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#设置语言和项目文件夹路径</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Glove</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="c1">#导入语料</span>
<span class="n">model</span><span class="o">.</span><span class="n">create_vocab</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/santi.txt&#39;</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">#构建词语共现矩阵</span>
<span class="n">model</span><span class="o">.</span><span class="n">cooccurrence_matrix</span><span class="p">()</span>
<span class="c1">#设置词嵌入模型的向量维度、迭代数</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_embeddings</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="c1">#存储模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;santi_glove_model&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Building prefix dict from the default dictionary ...
Step 1/4: ...Create vocabulary for Glove.

Dumping model to file cache C:\Users\Deng\AppData\Local\Temp\jieba.cache
Loading model cost 0.628 seconds.

Prefix dict has been built successfully.

Step 2/4: ...Create cooccurrence matrix.
Step 3/4: ...Train glove embeddings. 
             Note, this part takes a long time to run

Iteration 20: error 64925132.71550
Step 3/4: ... Finish! Use 316.91 s

Step 4/4: ... Save the glove embeddings to a txt file
</code></pre></div><br>
<h2 id="导入glove预训练模型">导入GloVe预训练模型</h2>
<p>训练好的GloVe模型是txt文件，可以使用gensim导入。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1"># 导入GloVe模型文件</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;output/Glove/santi_glove_model.txt&#39;</span><span class="p">,</span>  <span class="n">no_header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">#查看某词的词向量</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;宇宙&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 0.6618259 ,  0.60663235,  0.9849417 , -1.028956  ,  1.0711069 ,
       -0.8875306 , -0.52833366, -1.0125595 , -0.9628481 ,  1.0356479 ,
        0.8595257 ,  0.7454354 , -1.0468111 , -0.26285014, -1.0310447 ,
        0.9906805 ,  0.05825566, -0.85581344, -0.9932533 , -1.020438  ,
        1.0495061 , -0.6973389 ,  0.49099424, -0.80775315,  0.64256483,
        1.0157642 ,  1.0135043 , -1.0131834 ,  0.17376372,  0.89585054,
        0.30890268,  0.798895  ,  0.6653925 ,  0.908629  , -1.048273  ,
       -0.35683677,  0.06306187, -1.0267074 , -1.0494691 ,  0.42172813,
        0.24005401,  0.5934993 , -0.0696691 , -1.0360557 , -0.9797269 ,
        1.0205714 , -0.376359  , -1.0501183 ,  1.0415571 , -0.9312968 ],
      dtype=float32)
</code></pre></div><br>
<h2 id="模型的使用">模型的使用</h2>
<p>语料中所有的词语都是维度相同的向量，可以根据向量计算找近义词、反义词。可参考 之前分享的   <a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.</li>
<li>William L. Hamilton, Jure Leskovec, and Dan Jurafsky. ACL 2016. Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.</li>
<li>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.
<ul>
<li><a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></li>
</ul>
</li>
<li><a href="https://github.com/hiDaDeng/cntext">https://github.com/hiDaDeng/cntext</a></li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>FinBERT | 金融文本BERT模型，可情感分析、识别ESG和FLS类型</title>
      <link>https://textdata.cn/blog/2022-11-17-finbert-finance-bert-model/</link>
      <pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-17-finbert-finance-bert-model/</guid>
      <description>金融语言模型</description>
      <content:encoded><![CDATA[<h2 id="finbert介绍">FinBERT介绍</h2>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/uj4hm7Lr2Wo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<br>
<p>FinBERT， 是使用49亿词的英文金融语料库数据，生成的BERT预训练语言模型。语料库上大小为 49亿个词。</p>
<ul>
<li>公司报告 10-K 和 10-Q：25亿个词</li>
<li>电话会议记录：13亿个词</li>
<li>分析师报告：11亿个词</li>
</ul>
<p>FinBERT开发者在多个金融 NLP 任务上对 FinBERT 预训练模型进行了微调，均优于传统机器学习模型、深度学习模型和微调 BERT 模型。 所有经过微调的 FinBERT 模型都公开托管在 Huggingface 🤗。  目前支持包括<strong>情绪分析、ESG 分类、前瞻性陈述 (FLS) 分类</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Huang, Allen H., Hui Wang, and Yi Yang. &#34;FinBERT: A large language model for extracting information from financial text.&#34; Contemporary Accounting Research (2022).

摘要（翻译）: 我们开发了 FinBERT，这是一种适用于金融领域的最先进的大型语言模型。我们表明，FinBERT 结合了金融知识，可以更好地总结金融文本中的上下文信息。使用分析报告中研究人员标记的句子样本，我们证明 FinBERT 大大优于 Loughran 和 McDonald 词典以及其他机器学习算法，包括朴素贝叶斯、支持向量机、随机森林、卷积神经网络和长短期记忆，在情感分类中。我们的结果表明，FinBERT 擅长识别其他算法错误标记为中性的句子的正面或负面情绪，这可能是因为它使用了金融文本中的上下文信息。我们发现，FinBERT 优于其他算法，以及 Google 的原始双向编码器表示形式来自 transformers (BERT) 模型，当训练样本量较小且文本中包含一般文本中不常用的金融词时，这种优势尤为突出。 FinBERT 在识别与环境、社会和治理问题相关的讨论方面也优于其他模型。最后，我们表明，与 FinBERT 相比，其他方法低估了收益电话会议的文本信息量至少 18%。我们的结果对学术研究人员、投资专业人士和金融市场监管机构具有重要意义。
</code></pre></div><br>
<h3 id="finbert功能">FinBERT功能</h3>
<p>具体来说，FinBERT有以下内容：</p>
<ul>
<li><a href="https://huggingface.co/yiyanghkust/finbert-pretrain">FinBERT-Pretrained</a>： 针对大规模金融文本的预训练 FinBERT 模型。</li>
<li><a href="https://huggingface.co/yiyanghkust/finbert-tone">FinBERT-Sentiment</a>： 用于情感分类任务。</li>
<li><a href="https://huggingface.co/yiyanghkust/finbert-esg">FinBERT-ESG</a>： 用于 ESG 分类任务。</li>
<li><a href="https://huggingface.co/yiyanghkust/finbert-fls">FinBERT-FLS</a>： 用于前瞻性陈述（FLS）分类任务。</li>
</ul>
<br>
<h3 id="环境配置">环境配置</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install transformers==4.18.0
</code></pre></div><p>本次实验使用的transformers版本为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import transformers
transformers.__version__
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">4.18.0
</code></pre></div><br>
<h3 id="代码下载">代码下载</h3>
<p><a href="FinBERT.ipynb">点击下载</a></p>
<p><br><br></p>
<h2 id="一情感分析">一、情感分析</h2>
<p>金融文本情绪可以调动管理者、信息中介和投资者的观点和意见, 因此分析金融文本情感(情绪)是有价值的。 FinBERT-Sentiment 是一个 FinBERT 模型，它根据标准普尔 500 家公司的分析师报告中的 10,000 个手动注释的句子进行了Fine-tune(微调)。</p>
<blockquote>
<p>Fine-Tune微调 是 深度学习的一种语言处理技术，可以在前人（已有）的语言模型文件基础上加入少量新场景的文本数据进行更新训练，生成出新场景的语言模型。</p>
</blockquote>
<ul>
<li><strong>输入</strong>：金融文本。</li>
<li><strong>输出</strong>：Positive, Neutral or Negative.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1">#首次运行，因为会下载FinBERT模型，耗时会比较久</span>
<span class="n">senti_finbert</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-tone&#39;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">senti_tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-tone&#39;</span><span class="p">)</span>
<span class="n">senti_nlp</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&#34;text-classification&#34;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">senti_finbert</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">senti_tokenizer</span><span class="p">)</span>
</code></pre></div><p><br>使用3条测试文本进行测试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 待分析的文本数据</span>
<span class="n">senti_results</span> <span class="o">=</span> <span class="n">senti_nlp</span><span class="p">([</span><span class="s1">&#39;growth is strong and we have plenty of liquidity.&#39;</span><span class="p">,</span> 
                           <span class="s1">&#39;there is a shortage of capital, and we need extra financing.&#39;</span><span class="p">,</span>
                           <span class="s1">&#39;formulation patents might protect Vasotec to a limited extent.&#39;</span><span class="p">])</span>
<span class="n">senti_results</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [{&#39;label&#39;: &#39;Positive&#39;, &#39;score&#39;: 1.0},
     {&#39;label&#39;: &#39;Negative&#39;, &#39;score&#39;: 0.9952379465103149},
     {&#39;label&#39;: &#39;Neutral&#39;, &#39;score&#39;: 0.9979718327522278}]
</code></pre></div><p><br><br></p>
<h2 id="二esg分类">二、ESG分类</h2>
<p>ESG 分析可以帮助投资者确定企业的长期可持续性并识别相关风险。 FinBERT-ESG 是一个 FinBERT 模型，根据来自公司 ESG 报告和年度报告的 2,000 个手动注释句子进行微调。</p>
<ul>
<li><strong>输入</strong>：金融文本。</li>
<li><strong>输出</strong>：Environmental, Social, Governance or None.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">esg_finbert</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-esg&#39;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">esg_tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-esg&#39;</span><span class="p">)</span>
<span class="n">esg_nlp</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&#34;text-classification&#34;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">esg_finbert</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">esg_tokenizer</span><span class="p">)</span>
</code></pre></div><p><br>使用3条测试文本进行测试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">esg_results</span> <span class="o">=</span> <span class="n">esg_nlp</span><span class="p">([</span><span class="s1">&#39;Managing and working to mitigate the impact our operations have on the environment is a core element of our business.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;Rhonda has been volunteering for several years for a variety of charitable community programs.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;Cabot</span><span class="se">\&#39;</span><span class="s1">s annual statements are audited annually by an independent registered public accounting firm.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;As of December 31, 2012, the 2011 Term Loan had a principal balance of $492.5 million.&#39;</span><span class="p">])</span>

<span class="n">esg_results</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [{&#39;label&#39;: &#39;Environmental&#39;, &#39;score&#39;: 0.9805498719215393},
     {&#39;label&#39;: &#39;Social&#39;, &#39;score&#39;: 0.9906041026115417},
     {&#39;label&#39;: &#39;Governance&#39;, &#39;score&#39;: 0.6738430857658386},
     {&#39;label&#39;: &#39;None&#39;, &#39;score&#39;: 0.9960240125656128}]
</code></pre></div><p><br><br></p>
<h2 id="三fls识别">三、FLS识别</h2>
<p><strong>前瞻性陈述 (FLS)</strong> 告知投资者经理人对公司未来事件或结果的信念和意见。 从公司报告中识别前瞻性陈述可以帮助投资者进行财务分析。 FinBERT-FLS 是一个 FinBERT 模型，它基于罗素 3000 家公司年报的管理讨论和分析部分的 3,500 个手动注释的句子进行了微调。</p>
<ul>
<li><strong>输入</strong>：金融文本。</li>
<li><strong>输出</strong>：Specific-FLS(特定 FLS) , Non-specific FLS(非特定 FLS),  Not-FLS(非 FLS)。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">fls_finbert</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-fls&#39;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">fls_tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-fls&#39;</span><span class="p">)</span>

<span class="n">fls_nlp</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&#34;text-classification&#34;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">fls_finbert</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">fls_tokenizer</span><span class="p">)</span>
</code></pre></div><p><br> 使用3条测试文本进行测试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">fls_results</span> <span class="o">=</span> <span class="n">fls_nlp</span><span class="p">([</span><span class="s1">&#39;we expect the age of our fleet to enhance availability and reliability due to reduced downtime for repairs.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;on an equivalent unit of production basis, general and administrative expenses declined 24 percent from 1994 to $.67 per boe.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;we will continue to assess the need for a valuation allowance against deferred tax assets considering all available evidence obtained in future reporting periods.&#39;</span><span class="p">])</span>


<span class="n">fls_results</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [{&#39;label&#39;: &#39;Specific FLS&#39;, &#39;score&#39;: 0.7727874517440796},
     {&#39;label&#39;: &#39;Not FLS&#39;, &#39;score&#39;: 0.9905241131782532},
     {&#39;label&#39;: &#39;Non-specific FLS&#39;, &#39;score&#39;: 0.975904107093811}]
</code></pre></div><p><br><br></p>
<h2 id="文档及引用说明">文档及引用说明</h2>
<ul>
<li>
<p>文档github地址 <a href="https://github.com/yya518/FinBERT">https://github.com/yya518/FinBERT</a></p>
</li>
<li>
<p>作者博客: <a href="https://yya518.github.io/research">https://yya518.github.io/research</a></p>
</li>
</ul>
<br>
<p>Huang, Allen H., Hui Wang, and Yi Yang. &ldquo;FinBERT: A large language model for extracting information from financial text.&rdquo; <strong>Contemporary Accounting Research (2022)</strong>.</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 使用语义距离测量一个人的创新力(发散思维)得分</title>
      <link>https://textdata.cn/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/</link>
      <pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/</guid>
      <description>使用语义距离测量一个人的创新力(发散思维)得分</description>
      <content:encoded><![CDATA[<br>
<p>传统测量 <strong>被试者创造力</strong> 存在耗费时间、主观性太强、缺乏客观性，且所得到的分值是不稳定的，无法跨时间、文化、群体进行分值比较。该研究分析了创新力的两大理论，即联系理论和执行理论，即创新力是包含思维的广度和深度两方面。</p>
<ul>
<li><strong>联系理论(广度)</strong> 负责搜寻所有可能方案的集合，增加集合的规模，体现思维的广度。</li>
<li><strong>执行理论(深度)</strong> 负责寻找最佳方案，并将方案落实执行，体现思维的深度。</li>
</ul>
<p>结合Glove词嵌入技术，将每个词理解为一个技术或知识，两词语语义越相似，发散性越低。</p>
<p>文中让被试按照一定规则，随意填写10个名词，使用其中7个有效词语测量被试的创新力(发散性)思维。可以简单的把7个词理解为知识或者技术，7个词语会形成21种词语对(组合)。最后求均值可以测量出被试词语对的语义距离体现创新发散性的强度。<strong>文末含案例代码</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Olson, J.A., Nahas, J., Chmoulevitch, D., Cropper, S.J. and Webb, M.E., 2021. Naming unrelated words predicts creativity. Proceedings of the National Academy of Sciences, 118(25), p.e2022340118.
</code></pre></div><p><br><br></p>
<h2 id="一摘要">一、摘要</h2>
<p><strong>一些理论认为，有 创造力 的人能够产生更多 发散性 的想法。如果这是正确的，简单地让被试写 N 个不相关的单词，然后测量这N个词的语义距离， 作为 #发散思维 的客观衡量标准</strong>。为了验证这一假设，我们要求 8,914 名参与者说出 10 个彼此尽可能不同的单词。</p>
<p>然后计算算法估计单词之间的平均语义距离；<strong>相关词（例如 cat 和 dog）比不相关词（例如 cat 和 thimble）的距离更短。我们预测，产生更大语义距离的人也会在传统的创造力测量中得分更高</strong>。</p>
<p>在研究 1 中，我们发现语义距离与两个广泛使用的创造力测量（替代用途任务和桥接关联差距任务）之间存在中度至强相关性。在研究 2 中，参与者来自 98 个国家，语义距离仅因基本人口变量而略有不同。在一系列已知可预测创造力的问题上，语义距离与表现之间也存在正相关关系。</p>
<p>总体而言， <strong>语义距离</strong> 与已建立的 创造力测量 的相关性至少与这些测量彼此之间的相关性一样强。 因此，在我们所说的发散关联任务中命名不相关的词可以作为发散思维的简短、可靠和客观的衡量标准。</p>
<br>
<h2 id="二创新力理论">二、创新力理论</h2>
<p>想出 3 个尽可能不同的词。根据两种主要的创造力理论 (1, 2)，选择这些词依赖于产生 #远程联想 ，同时抑制 #常见联想 。</p>
<p>#联想理论 (Associative Theory)认为，有创造力的人具有语义记忆结构，可以更容易地链接远程元素 (3-6)。</p>
<p>#执行理论 (Executive Theory) 侧重于自上而下的注意力控制；创造性的解决方案来自于监测和抑制共同的联想 (2, 7)。</p>
<p>基于这些理论，我们假设 <strong>填写n个无关单词的任务</strong> 可以可靠地衡量 #语言创造力 。 <strong>创造力有两个主要的心理成分， 收敛思维和发散思维，它们在产生创意输出时协同工作</strong>。收敛性思维任务衡量评估多种刺激并得出最适当响应的能力，例如问题的最佳解决方案 (3, 8-10)。这些任务往往更容易得分，因为只有一小部分正确答案。<strong>相比之下，发散思维任务通常使用开放式问题来衡量一个人产生各种解决方案的能力</strong> (11-13)。它们通常需要更长的回答(文本)，因此更难客观评分。</p>
<br>
<h2 id="三创新力测量">三、创新力测量</h2>
<h3 id="31--替代用途任务">3.1  替代用途任务</h3>
<p>最常见的发散思维测量是 <strong>替代用途任务</strong> Alternative Uses Task (14, 15)，在该任务中，参与者生成常见物体的用途，例如回形针或鞋子。使用常用的评分方法 (16)，评分者然后根据三个组成部分来判断回答：</p>
<ul>
<li>灵活性，产生的不同用途类别的数量；</li>
<li>独创性，每次使用相对于样本的其余部分的稀有程度，这对创造力特别重要（17、18）；和</li>
<li>流畅度，一共产生了多少次使用。</li>
</ul>
<br>
<h3 id="32-离散联系任务">3.2 离散联系任务</h3>
<p>本研究作者开发了 <strong>离散联系任务</strong> (Divergent Association Task， DAT) 的网站， <strong>填写你想到的10个不相关词语， 创造力越丰富的人，填写的词语语义距离往往会更远</strong>。</p>
<p><a href="https://www.datcreativity.com/">https://www.datcreativity.com/</a></p>
<p><img loading="lazy" src="img/1_pnas_divergent_association_task_mainpage.png" alt=""  />
</p>
<h3 id="被试填写10个单词的规则">被试填写10个单词的规则</h3>
<ol>
<li>只能填写英文单词</li>
<li>只能是名词(如事情、物体、概念)</li>
<li>不能填 专有名词（例如，特定的人或地点）</li>
<li>不能填写 专业词（比如技术词）</li>
<li>自己思考这些词，不要只看周围环境的物体。</li>
</ol>
<h3 id="dat算法实现">DAT算法实现</h3>
<ol>
<li>使用Glove预训练模型</li>
<li>选前7个词(一共10个词)， 存在 21个词对（组合）</li>
<li>对21词对， 分别计算词向量的余弦距离，分别乘以100。最终求均值得到DAT得分。</li>
</ol>
<blockquote>
<p>下图是大邓第二次填写得到的DAT得分，第一次只超过了6%的人，这方法第一次准，再测就知道如何提高DAT得分。</p>
</blockquote>
<p><img loading="lazy" src="img/2_pnas_divergent_association_task_result.png" alt=""  />
</p>
<p>DAT得分范围0-200， 得分为0可能是7个有效词之间语义相同，而得分200可能是有效词之间彼此语义完全不相同。实践中，得分大多处于65~90之间，且很少超过100。</p>
<p><img loading="lazy" src="img/pnas_dat_score_low_median_high.jpg" alt=""  />
</p>
<blockquote>
<p>词嵌入技术可以把每个词转化为等长的向量，而不同词语共处于相同的语义空间中。常见的词嵌入技术有word2vec、Glove、flastText等，因为最近有学者在 <strong>替代用途任务</strong>(Alternative Uses Task）中用过Glove算法，本文采用Glove算法。本研究使用的Glove预训练模型来自Common Crawl Corpus项目，该项目拥有数十亿网页文本数据。</p>
<p>为了提供冗余， 只采用 被试者 填写的前7个词作为有效单词(DAT的被试需要填写10个词)。DAT得分是这些词之间的语义距离的平均值，具体计算方法， 7个词两两相关的组合有 42种组合， 选择其中最有可能的 21 个语义组合。</p>
</blockquote>
<br>
<h2 id="四实验">四、实验</h2>
<p>这种发散思维的操作化是基于创造力的联想和执行控制理论。 更高的分数将显示出更大的能力来利用更远程的关联 (3-5) 或抑制过度相关的关联 (2, 7)。</p>
<p>在研究 1 中，我们通过将 DAT 与其他两种创造力测量方法进行比较来检验这一假设：替代用途任务 (15) 和桥接关联差距任务 (36)。
<img loading="lazy" src="img/pnas_dat_aut_algo_valid_num.jpg" alt=""  />
</p>
<p>在研究 2 中，我们测试了这些分数如何随人口统计而变化，以及它们是否与更大数据集中与发散性思维相关的其他测量值相关 (9, 37)。 这些研究评估了语义距离是否可以作为发散思维的可靠指标。
<img loading="lazy" src="img/pnas_dat_gender_age.jpg" alt=""  />
</p>
<br>
<h2 id="五讨论">五、讨论</h2>
<p>研究结果表面， 让被试简单的填写10个不想管单词的任务可以作为 测量发散思维 的可靠衡量标准。在研究中， 将这项任务的表现与已有的两种创造力量表做了比较，具有很高的相关性。</p>
<p>总体而言支持了语义发散性，尽管这种联系背后的确切机制尚不清楚，但在创新力最主要的两个理论，即联想理论或执行理论 的联系网络中衡量网络的范围或效率。</p>
<p><strong>DAT算法表现稳定，方差不随人口统计特征变化出现显著性变化（研究2），可以在跨年龄、跨性别的情况下应用</strong>。</p>
<br>
<h3 id="51-dat的优点">5.1 DAT的优点</h3>
<ul>
<li>操作简单，快捷，客观，节约了大量的人力时间，又能保证客观性。</li>
<li>得分绝对，可比较，可以用于测量不同群体(种族、文化、性别、年龄)的创造力得分。</li>
<li>对被试友好，一般一两分钟即可完成。</li>
</ul>
<h3 id="52-dat的不足">5.2 DAT的不足</h3>
<ul>
<li>创造力有发散性和执行力，发散性负责搜选所有方案集合的规模，而执行力是从方案集中选出最优方案并将其执行。DAT测量的仅仅是发散性思维。</li>
<li>被试可能通过填写稀奇的词语提高DAT得分。</li>
<li>只有短短几分钟，被试可能很难短时间内了解实验规则。</li>
</ul>
<h3 id="53-未来展望">5.3 未来展望</h3>
<p>DAT得分取决于Glove模型、语料库(数据集), 更新词模型或语料库，被试的DAT得分会发生变化。为简单起见，本研究使用免费的预训练模型， 通过一些努力，未来研究者可以对不同时期，不同国家的语料库来训练Glove模型。随着特定单词关联或多或少的联系， 更新的模型将会自动考虑这些变化，这将允许DAT得分跨越文化跨越时代，进行创新力的比较。</p>
<p><br><br></p>
<h2 id="代码">代码</h2>
<p>代码的文档说明请点击 github仓库地址 <a href="https://github.com/jayolson/divergent-association-task">https://github.com/jayolson/divergent-association-task</a> 查看。这里仅粘贴作者源代码，源代码需要配置好才可运行。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">dat</span>

<span class="c1">## 从 https://nlp.stanford.edu/projects/glove/ 下载Glove模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">dat</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="s2">&#34;glove.840B.300d.txt&#34;</span><span class="p">,</span> <span class="s2">&#34;words.txt&#34;</span><span class="p">)</span>

<span class="c1"># 验证词语，如输入的是词组，代码会将其转为连线形式的单词</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="s2">&#34;cul de sac&#34;</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cul-de-sac
</code></pre></div><br>
<p>计算两个词语之间的语义距离</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;dog&#34;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;thimble&#34;</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.1983
0.8787
</code></pre></div><br>
<p>计算词对的DAT得分（语义cosine距离*100）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">([</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;dog&#34;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">([</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;thimble&#34;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">19.83
87.87
</code></pre></div><br>
<p>假设有三个人分别都填写10个词，选其前7个词作为有效词。有效词如下，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">low</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;arm&#34;</span><span class="p">,</span> <span class="s2">&#34;eyes&#34;</span><span class="p">,</span> <span class="s2">&#34;feet&#34;</span><span class="p">,</span> <span class="s2">&#34;hand&#34;</span><span class="p">,</span> <span class="s2">&#34;head&#34;</span><span class="p">,</span> <span class="s2">&#34;leg&#34;</span><span class="p">,</span> <span class="s2">&#34;body&#34;</span><span class="p">]</span>
<span class="n">average</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;bag&#34;</span><span class="p">,</span> <span class="s2">&#34;bee&#34;</span><span class="p">,</span> <span class="s2">&#34;burger&#34;</span><span class="p">,</span> <span class="s2">&#34;feast&#34;</span><span class="p">,</span> <span class="s2">&#34;office&#34;</span><span class="p">,</span> <span class="s2">&#34;shoes&#34;</span><span class="p">,</span> <span class="s2">&#34;tree&#34;</span><span class="p">]</span>
<span class="n">high</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;hippo&#34;</span><span class="p">,</span> <span class="s2">&#34;jumper&#34;</span><span class="p">,</span> <span class="s2">&#34;machinery&#34;</span><span class="p">,</span> <span class="s2">&#34;prickle&#34;</span><span class="p">,</span> <span class="s2">&#34;tickets&#34;</span><span class="p">,</span> <span class="s2">&#34;tomato&#34;</span><span class="p">,</span> <span class="s2">&#34;violin&#34;</span><span class="p">]</span>

<span class="c1"># Compute the DAT score (transformed average cosine distance of first 7 valid words)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">low</span><span class="p">))</span> <span class="c1"># 50</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">average</span><span class="p">))</span> <span class="c1"># 78</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">high</span><span class="p">))</span> <span class="c1"># 95</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">50
78
95
</code></pre></div><p>需要注意pnas作者公开的代码只能用在英文，且无法自己训练Glove模型。如果想基于自有数据集（中文、英文），训练自有Glove模型，需要学习</p>
<ul>
<li>如何训练Glove模型</li>
<li>如何导入训练好的Glove模型</li>
<li>如何计算中英文dat得分</li>
</ul>
<p>相关知识点已更新至我的录播课课程 <a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>魔搭 | 中文AI模型开源社区</title>
      <link>https://textdata.cn/blog/2022-11-09-chinese-modelscope-open-source/</link>
      <pubDate>Wed, 09 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-09-chinese-modelscope-open-source/</guid>
      <description>ModelScope社区成立于2022 年6月，是一个模型开源社区及创新平台，由阿里巴巴达摩院，联合CCF开源发展委员会，共同作为项目发起方。社区联合国内AI领域合作伙伴与高校机构，致力于通过开放的社区合作，构建深度学习相关的模型开源，并开源相关模型服务创新技术，推动模型应用生态的繁荣发展。</description>
      <content:encoded><![CDATA[<h2 id="关于modelscope">关于ModelScope</h2>
<p>ModelScope社区成立于 2022 年 6 月，是一个模型开源社区及创新平台，由阿里巴巴达摩院，联合CCF开源发展委员会，共同作为项目发起方。</p>
<blockquote>
<p>社区联合国内AI领域合作伙伴与高校机构，致力于通过开放的社区合作，构建深度学习相关的模型开源，并开源相关模型服务创新技术，推动模型应用生态的繁荣发展。</p>
</blockquote>
<p>期待ModelScope会有不一样的表现。</p>
<p>与ModelScope类似的网站有</p>
<ul>
<li>国际 huggingface是较早将AI模型开源的网站，用户群体庞大，社区内有丰富的数据集、模型，文档详实。</li>
<li>国内 百度飞桨是国内AI模型开源较好的网站，用户群体较大，更新活跃，但是文档质量。。。</li>
</ul>
<p>目前ModelScope刚刚上线不久，模型和数据集都不怎么多</p>
<p><img loading="lazy" src="img/model_scope_homepage.png" alt=""  />
</p>
<br>
<h2 id="heading"></h2>
<h1 id="名词解释"><strong>名词解释</strong></h1>
<p>ModelScope平台是以模型为中心的模型开源社区，与模型的使用相关，您需要先了解如下概念。</p>
<table>
<thead>
<tr>
<th><strong>基础概念</strong></th>
<th><strong>定义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>任务</td>
<td>任务（Task）指某一领域具体的应用，以用于完成特定场景的任务。例如图像分类、文本生成、语音识别等，您可根据任务的输入输出找到适合您的应用场景的任务类型，通过任务的筛选来查找您所需的模型。</td>
</tr>
<tr>
<td>模型</td>
<td>模型（Model）是指一个具体的模型实例，包括模型网络结构和相应参数。ModelScope平台提供丰富的模型信息供用户体验与使用。</td>
</tr>
<tr>
<td>模型库</td>
<td>模型库（Modelhub）是指对模型进行存储、版本管理和相关操作的模型服务，用户上传和共享的模型将存储至ModelScope的模型库中，同时用户也可在Model hub中创建属于自己的模型存储库，并沿用平台提供的模型库管理功能进行模型管理。</td>
</tr>
<tr>
<td>数据集</td>
<td>数据集（Dataset）是方便共享及访问的数据集合，可用于算法训练、测试、验证，通常以表格形式出现。按照模态可划分为文本、图像、音频、视频、多模态等。</td>
</tr>
<tr>
<td>数据集库</td>
<td>数据集库（Datasethub）用于集中管理数据，支持模型进行训练、预测等，使各类型数据具备易访问、易管理、易共享的特点。</td>
</tr>
<tr>
<td>ModelScope Library</td>
<td>ModelScope Library是ModelScope平台自研的一套Python Library框架，通过调用特定的方法，用户可以只写短短的几行代码，就可以完成模型的推理、训练和评估等任务，也可以在此基础上快速进行二次开发，实现自己的创新想法。</td>
</tr>
</tbody>
</table>
<br>
<h2 id="一模型探索">一、模型探索</h2>
<p>首先访问平台网址https://www.modelscope.cn/models， 您将看见平台上已有的所有公开模型，根据任务筛选或者关键词搜索可查找您感兴趣的模型。</p>
<p><img loading="lazy" src="img/1-model_explore.png" alt=""  />
</p>
<br>
<h2 id="二环境准备">二、环境准备</h2>
<h3 id="21-本地开发环境">2.1 本地开发环境</h3>
<p>如果您需要在本地运行模型，需要进行相应的环境安装准备，包括：</p>
<ul>
<li><strong>安装python环境</strong>。支持python3，不支持python2，建议3.7版本及以上。我们推荐您使用Anaconda进行安装。</li>
<li><strong>安装深度学习框架</strong>。ModelScope Library目前支持Tensorflow，Pytorch两大深度学习框架进行模型训练、推理。您可根据模型所需的框架选择适合的框架进行安装。</li>
<li><strong>安装ModelScope Library</strong>。我们提供两种安装方式，您可选择适合的方式进行安装。
<ul>
<li>pip安装。ModelScope提供了根据不同领域的安装包，您可根据对应的模型选择所需的安装包。</li>
<li>使用源码安装。</li>
<li>更完整的安装信息参考：环境安装指南。</li>
</ul>
</li>
</ul>
<h3 id="22-在线notebook">2.2 在线Notebook</h3>
<p>若您觉得本地安装较为复杂， ModelScope平台也提供在线的运行环境，您可直接在Notebook中运行，Notebook中提供官方镜像无需自主进行环境安装，更加方便快捷，推荐大家使用！</p>
<p>注意：该功能需要您登录后使用，新用户注册ModelScope账号并完成阿里云账号绑定后即可获得免费算力资源，详情请参阅免费额度说明 。</p>
<p><img loading="lazy" src="img/model_scode_free_online_notebook.png" alt=""  />
</p>
<p><img loading="lazy" src="img/model_scode_free_online_notebook-2.png" alt=""  />
</p>
<br>
<h2 id="三2分钟跑通模型推理">三、2分钟跑通模型推理</h2>
<p>若您准备好本地环境或者已经打开一个Notebook的预装环境实例，则根据下述代码可对该模型进行推理。 使用modelscope pipeline接口只需要两步，同样以上述中文分词模型（damo/nlp_structbert_word-segmentation_chinese-base）为例简单说明：</p>
<p>首先根据task实例化一个pipeline对象</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">modelscope.pipelines</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="n">word_segmentation</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;word-segmentation&#39;</span><span class="p">,</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;damo/nlp_structbert_word-segmentation_chinese-base&#39;</span><span class="p">)</span>
</code></pre></div><p>输入数据，拿到结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">input_str</span> <span class="o">=</span> <span class="s1">&#39;今天天气不错，适合出去游玩&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word_segmentation</span><span class="p">(</span><span class="n">input_str</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;output&#39;: &#39;今天 天气 不错 ， 适合 出去 游玩&#39;}
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>预训练词向量模型的方法、应用场景、变体延伸与实践总结</title>
      <link>https://textdata.cn/blog/2022-11-07-embeddings-theory-applicaiton-liuhuanyong/</link>
      <pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-07-embeddings-theory-applicaiton-liuhuanyong/</guid>
      <description>预训练词向量模型的方法、应用场景、变体延伸与实践总结</description>
      <content:encoded><![CDATA[<p><br><br></p>
<h2 id="关于作者">关于作者</h2>
<p>刘焕勇，liuhuanyong，现任360人工智能研究院算法专家，前中科院软件所工程师，主要研究方向为知识图谱、事件图谱在实际业务中的落地应用。<br>
得语言者得天下，得语言资源者，分得天下，得语言逻辑者，争得天下。</p>
<ul>
<li>个人主页：https://liuhuanyong.github.io</li>
<li>个人公众号：老刘说NLP</li>
</ul>
<br>
<p>当前，以预训练语言模型PLM+fintune的自然语言处理范式可谓十分火热，有大量的文章在宣传这类方法，包括梳理以NNLM为起点的整个预训练方法的发展史。</p>
<p>当前工业界，主要使用的预训练模型包括两种，一种是以wordvec为代表的预训练词向量，另一种是以BERT为代表的预训练语言模型。前者通常作为词语表示输入的初始化，后接NN/CNN/LSTM等编码层，后者既可以同样后接，也可以直接接上softmax/crf/span-pointer等进行解码。</p>
<p>本文，主要围绕预训练词向量模型这一主题，对当前预训练词向量模型的常用方法、评估与应用方式、领域的迁移变体、当前开放的训练工具和词向量文件进行介绍，并以word2vec和fasttext为例，展示一个词向量训练的案例，做到理论与实践相结合。</p>
<p><br><br></p>
<h2 id="一预训练词向量模型方法">一、预训练词向量模型方法</h2>
<p>自从进入2010年以来，神经语言模型就逐渐进入人们眼球，以NNLM为典型最初代表的神经网络模型，极大的推动了NLP这一领域的发展。</p>
<p>实际上，早期词向量的研究通常来源于语言模型，比如NNLM和RNNLM，其主要目的是语言模型，而词向量只是一个副产物。著名的harris分布式假说提供了一个局部统计信息的理论基础。</p>
<p>下面就选择其中三种典型进行介绍。</p>
<br>
<h3 id="11-word2vec">1.1 word2vec</h3>
<p>word2vec是2013年Google开源的一款用于词向量计算的工具，通过内置的语言模型训练目标，可以将中间层得到的向量权重矩阵进行抽离，形成每个词对应的向量化表示，包括CBOW、Skip-gram两种方式，前者通过周围词来预测中心词，后者以中心词来预测上下文。</p>
<p><img loading="lazy" src="img/1.png" alt=""  />
</p>
<p>经典的wordvec结构包括输入层、隐藏层和输出层，其计算流程为：</p>
<p>1、输入层存储上下文单词的onehot。假设单词向量空间dim为V，上下文单词个数为C。</p>
<p>2、所有onehot分别乘以共享的输入权重矩阵W。V*N矩阵，N为自己设定的数，初始化权重矩阵W 。</p>
<p>3、所得的向量 相加求平均作为隐层向量, size为1*N。</p>
<p>4、乘以输出权重矩阵W' N*V。</p>
<p>5、得到向量1*V，经过激活函数处理得到V-dim概率分布。</p>
<p>6、Hierarchical Softmax分类，概率最大的index所指示的单词为预测出的中间词与预测值的onehot做比较，根据误差更新权重矩阵。</p>
<p><img loading="lazy" src="img/2.png" alt=""  />
</p>
<p>这个W矩阵就是所有单词的word embedding，任何一个单词的onehot乘以这个矩阵都将得到自己的词向量。</p>
<p>通常，在训练词向量时候，会根据语料的大小来选择相应的训练方法。例如，针对小型的数据集，可以用CBOW算法，该方法对于很多分布式信息进行了平滑处理，将一整段上下文信息视为一个单一观察量，对于小型的数据集，这一处理是有帮助的。相比之下，大型数据集，可以用Skip-Gram模型，该方法将每个“上下文-目标词汇”的组合视为一个新观察量，这种做法在大型数据集中会更为有效。</p>
<br>
<h3 id="12-fasttext">1.2 fasttext</h3>
<p>fastText是Facebook于2016年开源的一个词向量计算和文本分类工具。将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。包括字符级n-gram特征的引入以及分层Softmax分类两种。</p>
<p>与CBOW一样，原本的fastText模型包括输入层、隐含层、输出层，输入都是多个经向量表示的单词，输出都是一个特定的目标，隐含层都是对多个词向量的叠加平均。不同的是，CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档，CBOW的输入单词被onehot编码过，fastText的输入特征是经embedding化的，CBOW的输出是目标词汇，fastText的输出是文档对应的类标。</p>
<p>而如果将该类标替换成中间目标词，那么就可以得到wordvec的升级版，即单纯的词向量模型。例如，word2vec把语料库中的每个单词当成原子的，它会为每个单词生成一个向量。这忽略了单词内部的形态特征。</p>
<p>fasttext使用了字符级别的n-grams来表示一个单词。对于单词“apple”，假设n的取值为3，则它的trigram有“&lt;ap”, “app”, “ppl”, “ple”, “le&gt;”，其中，&lt;表示前缀，&gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，可以用这5个trigram的向量叠加来表示“apple”的词向量。</p>
<p>因此，因为它们的n-gram可以和其它词共享，对于训练词库之外的单词，能够解决或者oov词，这也是在当前很多文本分类、推荐场景中会优先选用fastText作为训练方法。</p>
<br>
<h3 id="13-glove">1.3 Glove</h3>
<p>GloVe是斯坦福团队于2014年提出一个词向量方法，全名叫“Global Vectors”，直接利用全局的统计信息进行训练。</p>
<p>与上述两种方式靠滑动窗口来制造局部上下文不同，GloVe会用到全局的词语之间共现的统计信息，即词的出现次数，词对之间的共现概率，形成共现概率矩阵，并试图生成词向量来毕竟共现概率，利用Word2Vec的skip-gram算法的高性能来解决LDA的计算量复杂问题。</p>
<p>因此，我们可以发现，Glove需要事先统计共现概率，这也让其通常被认为是无监督学习，实际上glove还是有label的，即共现次数。与wordvec还有一处不同的是，损失函数是最小平方损失函数，权重可以做映射变换。</p>
<p><br><br></p>
<h2 id="二预训练词向量的训练参数">二、预训练词向量的训练参数</h2>
<p>词向量模型的超参数很多，不同的参数选择会取得不同的效果，并且，word2vec中有几个大家提的比较多的问题。以gensim-word2vec为例，包括以下参数：</p>
<ul>
<li>sentences： 可以是一个list，对于大语料集，可使用BrownCorpus,Text8Corpus或LineSentence构建；</li>
<li>sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法；</li>
<li>size： 特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百；</li>
<li>window： 表示当前词与预测词在一个句子中的最大距离是多少；</li>
<li>alpha: 学习速率；</li>
<li>seed： 用于随机数发生器。与初始化词向量有关；</li>
<li>min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5；</li>
<li>max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制；</li>
<li>sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)；workers参数控制训练的并行数；</li>
<li>hs: 如果为1则会采用hierarchical softmax技巧。如果设置为0（defaut），则negative sampling会被使用；</li>
<li>negative: 如果&gt;0,则会采用negativesamping，用于设置多少个noise words；</li>
<li>cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defaut）则采用均值。只有使用CBOW的时候才起作用；</li>
<li>hashfxn： hash函数来初始化权重。默认使用python的hash函数；</li>
<li>iter： 迭代次数，默认为5；</li>
<li>trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RUE_DISCARD,utis.RUE_KEEP或者utis.RUE_DEFAUT的函数；</li>
<li>sorted_vocab： 如果为1（defaut），则在分配word index 的时候会先对单词基于频率降序排序；</li>
<li>batch_words： 每一批的传递给线程的单词的数量，默认为10000。</li>
</ul>
<p>不过，如此多的参数不一定能跳得过来，因此通常会集中在以下常规参数：</p>
<p><img loading="lazy" src="img/3.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三预训练词向量的评估与应用">三、预训练词向量的评估与应用</h2>
<p>预训练词向量生产出来，需要进行性能的评估。这方面的方法包括基于评测集，或者基于具体业务使用，用业务的指标来进行评估。</p>
<h3 id="31-预训练词向量的评估">3.1 预训练词向量的评估</h3>
<p>学术上，词向量的质量通常由类比问题任务进行评估。如CA-translated包含了三个语义问题和134个中文词。CA8 是专门为中文语言设计的。它包含了 17813 个类比问题，覆盖了综合的词法和语义关联。</p>
<p>工业，则使用词向量来代替之前随机生成的词向量文件，来对自然语言处理中的文本/情感分类、实体识别、关系抽取等任务进行评估。</p>
<br>
<h3 id="32-预训练词向量的应用">3.2 预训练词向量的应用</h3>
<p>预训练词向量文件最大的价值在于解决了一个词语的初始化稠密表示，在解决当前以数值化为输入的深度或机器学习模型第一部的同时，还保留了一个词的区别性特征。</p>
<p>一方面，当前词向量可以用于近义词挖掘的重要来源，通过某个词，通过计算词与其他词之间的相似度，并设定阈值，可以迭代挖掘出大量的相关词【过程中要注意语义漂移】。而这个词，直接就可以用于当前的搜索查询扩展、领域词构建等场景。进一步的，在模型方面，还可以作为EDA数据增强工作中的重要补充。</p>
<p>另一方面，词向量可以用于当前无监督文本表示的重要方法，通过对文本进行分词，然后找到词语对应的向量，通过向量叠加的方式可以快速得到一个文本的向量表示，这一表示在诸如情感分析、句子相似度计算等任务中是实际有效的，基于文本表示，也可以进一步提升文本分类、聚类、相似query召回等使用场景性能，甚至很形象的成为了当前业务模型的baseline或者兜底模型。</p>
<p><br><br></p>
<h2 id="四预训练词向量的变体延伸">四、预训练词向量的变体延伸</h2>
<h3 id="41-gramembedding">4.1 gramEmbedding</h3>
<p>共现信息，是cbow以及skipgram的基础，其本质在于通过周围词来建模中心词或者用中心词来建模周围词。因此，通过构造不同的共现信息，可以得到不同类型的向量形式。这里取了个名字叫gramembedding，用于表示专指文本的一系列embedding变体。</p>
<p>例如，对于一个词来说，我们可以把词拆分为词word、n元序列ngram、汉字character，偏旁部首Radical，词性POS，依存关系dependency、拼音pinying。</p>
<p>单元的共现，我们同样可以进行组合，例如，构造word-word，word-ngram、ngran-ngram等，得到上下文特征（单词、n-gram、字符等）等不同粒度的词向量。</p>
<p>观察近几年的发展，词向量可以进一步分成偏旁部首向量、字符向量等。如香侬科技推出的glyce向量，引入汉字的字形特征。蚂蚁金服推出的cw2vec字符向量，将汉字拆解成偏旁、字件进行建模。</p>
<p><img loading="lazy" src="img/4.png" alt=""  />
</p>
<p>当ngram中的n为1时，可以得到字向量，n为2或者更多时，则可以得到词向量等。fasttext中，就是得到了ngram的向量，并进行加和，得到一个OOV词语的向量进行表示。</p>
<p>例如，基于skigram，分别设定词向量的维度及其他超参数，可以得到字向量,拼音向量，词向量，词性向量，通过上下文共现与PCA降维的方法可以得到依存向量。</p>
<p><img loading="lazy" src="img/5.png" alt=""  />
</p>
<p>从下面的结果可以看出，词和字向量的效果看起来还不错。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    ***********************字符向量************************
    token:刘
    (&#39;李&#39;, 0.7306396961212158),(&#39;陈&#39;, 0.7201231122016907)
    (&#39;赵&#39;, 0.6974461674690247),(&#39;杨&#39;, 0.6972213983535767)
    (&#39;吴&#39;, 0.6851627230644226),(&#39;徐&#39;, 0.6516467332839966)
    (&#39;郭&#39;, 0.6499480605125427),(&#39;蔡&#39;, 0.6175302267074585)
    (&#39;郑&#39;, 0.6092196106910706),(&#39;孙&#39;, 0.5950524210929871)
    token:丑
    (&#39;卯&#39;, 0.6074919700622559),(&#39;酉&#39;, 0.5910211801528931)
    (&#39;巳&#39;, 0.5581363439559937),(&#39;戌&#39;, 0.43932047486305237)
    (&#39;戊&#39;, 0.41449615359306335),(&#39;壬&#39;, 0.40456631779670715)
    (&#39;謤&#39;, 0.367109090089798),(&#39;绯&#39;, 0.3643313944339752),
    (&#39;寅&#39;, 0.36351141333580017),(&#39;旽&#39;, 0.3549465537071228)

    ***********************依存向量************************
    dependency rel:ATT
    (&#39;COO&#39;, 0.14239487051963806),(&#39;ADV&#39;, -0.16987691819667816)
    (&#39;RAD&#39;, -0.2357601821422577),(&#39;HED&#39;, -0.2401314228773117)
    (&#39;SBV&#39;, -0.25625932216644287),(&#39;WP&#39;, -0.27165737748146057)
    (&#39;LAD&#39;, -0.2902592420578003),(&#39;POB&#39;, -0.2990782558917999)
    (&#39;VOB&#39;, -0.37553706765174866),(&#39;IOB&#39;, -0.6669262647628784)
    dependency rel:POB
    (&#39;IOB&#39;, 0.16698899865150452),(&#39;DBL&#39;, 0.16678886115550995)
    (&#39;FOB&#39;, 0.1657436639070511),(&#39;CMP&#39;, 0.14784857630729675)
    (&#39;VOB&#39;, 0.1461176574230194),(&#39;SBV&#39;, 0.08011472970247269)
    (&#39;LAD&#39;, -0.022307466715574265),(&#39;WP&#39;, -0.022942926734685898)
    (&#39;HED&#39;, -0.037264980375766754),(&#39;RAD&#39;, -0.042251598089933395)

    ***********************拼音向量************************
    pinyin:wo
    (&#39;shei&#39;, 0.6129732131958008)(&#39;ta&#39;, 0.6081706285476685)
    (&#39;nin&#39;, 0.5819231867790222),(&#39;！&#39;, 0.5435523986816406)
    (&#39;……&#39;, 0.48428624868392944),(&#39;ai&#39;, 0.47832390666007996)
    (&#39;o&#39;, 0.4761071801185608),(&#39;。』&#39;, 0.4598163366317749)
    (&#39;...&#39;, 0.45207729935646057),(&#39;ni&#39;, 0.44975683093070984)
    pinyin:guo
    (&#39;dang&#39;, 0.3908974528312683),(&#39;yuan&#39;, 0.378823846578598)
    (&#39;zu&#39;, 0.35387369990348816),(&#39;hua&#39;, 0.3405681848526001)
    (&#39;zheng&#39;, 0.3355437219142914),(&#39;yi&#39;, 0.3333034813404083)
    (&#39;ren&#39;, 0.3194104731082916),(&#39;jun&#39;, 0.3187354505062103)
    (&#39;hui&#39;, 0.31342023611068726),(&#39;xin&#39;, 0.3096797466278076)

    ***********************词性向量************************
    word postag:a
    (&#39;d&#39;, 0.7203904986381531),(&#39;c&#39;, 0.6124969720840454)
    (&#39;v&#39;, 0.4963228106498718),(&#39;an&#39;, 0.4531499147415161)
    (&#39;uz&#39;, 0.4459834396839142),(&#39;ud&#39;, 0.42059916257858276)
    (&#39;r&#39;, 0.4090540111064911),(&#39;uj&#39;, 0.4061364233493805)
    (&#39;i&#39;, 0.38707998394966125),(&#39;l&#39;, 0.3551557660102844)
    word postag:n
    (&#39;b&#39;, 0.7030695676803589),(&#39;vn&#39;, 0.490166038274765)
    (&#39;p&#39;, 0.4858315885066986),(&#39;v&#39;, 0.4499088227748871)
    (&#39;nt&#39;, 0.44155171513557434),(&#39;f&#39;, 0.26609259843826294)
    (&#39;s&#39;, 0.2639649212360382),(&#39;l&#39;, 0.24365971982479095)
    (&#39;ns&#39;, 0.2278469204902649),(&#39;m&#39;, 0.202927365899086)
    ***********************词向量************************
    word:爱情
    (&#39;爱恋&#39;, 0.6931096315383911),(&#39;真爱&#39;, 0.6897798776626587)
    (&#39;婚姻&#39;, 0.6540514826774597),(&#39;浪漫爱情&#39;, 0.6535360813140869)
    (&#39;情感&#39;, 0.6501022577285767),(&#39;感情&#39;, 0.6403399705886841)
    (&#39;纯爱&#39;, 0.6394841074943542),(&#39;爱情故事&#39;, 0.6282097101211548)
    (&#39;校园爱情&#39;, 0.6078493595123291),(&#39;情爱&#39;, 0.5976818799972534)
    word:创新
    (&#39;技术创新&#39;, 0.7648976445198059),(&#39;不断创新&#39;, 0.7172579765319824)
    (&#39;创新型&#39;, 0.6573833227157593),(&#39;创新能力&#39;, 0.6533682942390442)
    (&#39;创新性&#39;, 0.6160774827003479),(&#39;革新&#39;, 0.6159394383430481)
    (&#39;人才培养&#39;, 0.6093565821647644),(&#39;开拓创新&#39;, 0.6015594601631165)
    (&#39;探索&#39;, 0.5987343788146973),(&#39;技术革新&#39;, 0.5949685573577881)
</code></pre></div><p>从上，也看到一些十分有趣的现象：</p>
<p>1）依存向量，依存向量中可以看出，ATT作为定中关系，在依存关系中属于定中结构，COO(联合)，ADV(状中)的相似度要比主谓SBV，动宾VOB的相似度要高。另外，作为介宾的POB，相似的有IOB，DBL，FOB，这些关系均与宾语成分相关</p>
<p>2）拼音向量，从wo，guo的拼音相似拼音来看，我们可以看到，这种相似的拼音更像是一种搭配， 很有意思，(词性参照jieba分词词性对照表)。</p>
<p>3）词性向量，从a，n的相似词性来看，也似乎更像是一种搭配现象，或许有更好的解释。</p>
<br>
<h3 id="42-domainembedding">4.2 DomainEmbedding</h3>
<p>为了更好的适配不同领域的任务，当前也有很多的公司或者任务会选择使用领域性的领域进行训练，以得到不同领域的词向量文件，这与当前各种领域的bert模型做法是类似的。当前出现了金融领域bert、法律领域的bert等。</p>
<p>代表性的，2018年推出的Chinese-Word-Vectors中提供了包含经过数十种用各领域语料（百度百科、维基百科、人民日报 1947-2017、知乎、微博、文学、金融、古汉语等）训练的词向量，涵盖各领域，且包含多种训练设置。</p>
<p><img loading="lazy" src="img/6.png" alt=""  />
</p>
<p>又如，当前PaddleNLP官方提供了61种可直接加载的预训练词向量，训练自多领域中英文语料、如百度百科、新闻语料、微博等，覆盖多种经典词向量模型（word2vec、glove、fastText）、涵盖不同维度、不同语料库大小。</p>
<br>
<h3 id="43-graphembdding">4.3 GraphEmbdding</h3>
<p>经典的deepwalk以及node2vec也是借鉴word2vec思想，学习图节点嵌入的方法。并且成为当前推荐系统中的一个重量级使用方法。</p>
<p><strong>1、Deepwalk</strong></p>
<p>通过对图中的节点进行随机游走（主要考虑深度优先遍历），形成节点之间的游走序列，并将其作为上下文，后面接入skipgram形成节点向量，从构造上来看，就是首先利用random walk来表示图结构，然后利用skip-gram模型来更新学习节点表示。</p>
<p>随机选取与其邻接的下一个结点，直至达到给定长度，这个长度作为一个参数进行指定，这个类似于word2vec中的window_size上下文窗口。</p>
<p><img loading="lazy" src="img/7.png" alt=""  />
</p>
<p><strong>2、node2vec</strong></p>
<p>node2vec综合考虑了广度优先遍历（用于捕捉局部信息）和深度优先遍历（用于捕捉全局信息）的游走，提出二阶随机游走思想，解决内容相似和结构相似的问题。</p>
<p><img loading="lazy" src="img/8.png" alt=""  />
</p>
<p>前者具有直接链接关系的两个节点，我们可以认为是内容相似的（例如两个灰色网站之间很有可能能够直接跳转，如图中的s1，s2等一阶邻居）、结构相似（例如周围邻居数量都很类似，如图中的s6和u节点，两个都有4个邻接，结构类似）。</p>
<p><img loading="lazy" src="img/9.png" alt=""  />
</p>
<p>具体实现思路也很简单：</p>
<p>我们从节点v转移到节点t，并且当前在节点t时，需要考虑下一个采样节点x。因此，可以设计一个节点到它的不同邻居的转移概率：</p>
<p><img loading="lazy" src="img/10.png" alt=""  />
</p>
<p>其中，每一步采样都会有三种状态，分别对应于上图的0，1，2三种情况：</p>
<ul>
<li><strong>1）0代表如果t和x相等，那么采样的概率为1/p；</strong></li>
<li><strong>2）1代表t与x相连，采样的概率为1；</strong></li>
<li>3）2代表t与x不相连，采样的概率为1/q**</li>
</ul>
<p>式子中的参数p作为返回参数，控制重新采样上一步已访问节点的概率。参数q，作为出入参数，控制采样的方向。</p>
<p>其中：</p>
<ul>
<li><strong>1）当q&gt;1时，接下来采样的节点倾向于节点t，偏向于广度优先；</strong></li>
<li><strong>2）当q&lt;1时，接下来采样的节点倾向于远离t，偏向于深度优先遍历。</strong></li>
<li><strong>3）当p&gt;max(q,1)时，接下来采样的节点很大概率不是之前已访问节点，这一方法使得采样偏向深度优先；</strong></li>
<li><strong>4）当p&lt;max(q,1)时，接下来采样的节点很大概率是之前已访问节点，这一方法使得采样偏向广度优先。</strong></li>
</ul>
<p>此外，在推荐场景中也有item2vec的类似延伸，例如协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达。</p>
<p><br><br></p>
<h2 id="五预训练词向量的动手实操">五、预训练词向量的动手实操</h2>
<p>纸上得来终觉浅，觉知此事要躬行，能够动手实践是加强对该概念理解的重要方式。预训练词向量，在流程上，应该包括全量训练和增量训练两种。前者可以在有大规模训练语料的情况下得到领域的向量，后者适用于小语料微调。
下面以gemsim中的wordvec和fasttext为例进行实践，大家可以看出其中的一些具体的步骤和结果。</p>
<h3 id="51-word2vec向量训练">5.1 word2vec向量训练</h3>
<h4 id="1构造训练语料">1、构造训练语料</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># coding = utf-8</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="n">cur</span> <span class="o">=</span> <span class="s1">&#39;/&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">Trainvec</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;lawsuit.json&#34;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;duanzi.txt&#34;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">build_corpus</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">train_path</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;train.txt&#34;</span><span class="p">),</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">json_obj</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">json_obj</span><span class="p">[</span><span class="s2">&#34;content&#34;</span><span class="p">]</span>
                <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
                <span class="n">cut_wds</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
                <span class="n">train_path</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cut_wds</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">train_path</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">build_update_corpus</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">train_path</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;update.txt&#34;</span><span class="p">),</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">cut_wds</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                <span class="n">train_path</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">cut_wds</span> <span class="k">if</span> <span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">train_path</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
	  <span class="n">handler</span> <span class="o">=</span> <span class="n">Trainvec</span><span class="p">()</span>
    <span class="c1">#handler.build_corpus()</span>
    <span class="n">handler</span><span class="o">.</span><span class="n">build_update_corpus</span><span class="p">()</span>
</code></pre></div><br>
<h4 id="2配置输入与输出路径">2、配置输入与输出路径</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">word2vec</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="n">cur</span> <span class="o">=</span> <span class="s1">&#39;/&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;train.txt&#34;</span><span class="p">)</span>
<span class="n">update_filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;update.txt&#34;</span><span class="p">)</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec.model&#34;</span>
<span class="n">model_update_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec_update.model&#34;</span>
<span class="n">model_vec_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec.bin&#34;</span>
<span class="n">model_update_vec_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec_update.bin&#34;</span>
</code></pre></div><br>
<h4 id="3全量数据预训练">3、全量数据预训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">full_train_embedding</span><span class="p">():</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">min_word_count</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">context</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">downsampling</span> <span class="o">=</span> <span class="mf">1e-3</span>
    <span class="c1"># 获取日志信息</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># 加载分词后的文本，使用的是Text8Corpus类</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Text8Corpus</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="c1"># 训练模型，部分参数如下</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
                              <span class="n">size</span><span class="o">=</span><span class="n">num_features</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_word_count</span><span class="p">,</span>
                              <span class="n">window</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="n">downsampling</span><span class="p">)</span>
    <span class="c1">#保存模型,除包含词-向量,还保存词频等训练所需信息</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1">#保存词向量文件,保存的模型仅包含词-向量信息</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div><p>在保存过程中，存在两种方式，保存模型,除包含词-向量,还保存词频等训练所需信息，保存词向量文件,保存的模型仅包含词-向量信息。所以我们可以看到，词向量文件，确实是word2vec模型的副产物。</p>
<br>
<h4 id="4增量数据预训练">4、增量数据预训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">增量训练</span><span class="err">，</span><span class="n">主要解决在新的文本上进行训练</span><span class="err">，</span><span class="n">也可以引入一些新的词</span><span class="err">，</span><span class="n">但这个时候</span><span class="err">，</span><span class="n">需要考虑到min_count这一过滤条件</span><span class="err">。</span>

<span class="k">def</span> <span class="nf">update_train_embedding</span><span class="p">():</span>
    <span class="c1"># 获取日志信息</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># 加载新的训练数据</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">LineSentence</span><span class="p">(</span><span class="n">update_filepath</span><span class="p">)</span>
    <span class="c1"># 加载旧模型</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1"># 更新词汇表</span>
    <span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># 训练数据</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">epochs</span><span class="p">)</span>  <span class="c1"># epoch=iter语料库的迭代次数；（默认为5）  total_examples:句子数。</span>
    <span class="c1"># 保存模型，是分成两个来训练</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_update_path</span><span class="p">)</span>
    <span class="c1"># 保存词向量文件</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_update_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div><br> 
<h4 id="5词向量结果测试">5、词向量结果测试</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">test_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s2">&#34;wordvec.model.bin&#34;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">while</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">wd</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&#34;enter an word:&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">wd</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">words</span>
</code></pre></div><p>通过运行，我们可以得到如下查询结果：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">enter an word:开心
[(&#39;高兴&#39;, 0.7237069606781006), (&#39;有缘&#39;, 0.7097823619842529), (&#39;开了花&#39;, 0.7021969556808472), (&#39;玩得&#39;, 0.6799882650375366), (&#39;快乐&#39;, 0.6698621511459351), (&#39;不亦乐乎&#39;, 0.668710470199585), (&#39;鉴宝&#39;, 0.6672042012214661), (&#39;越聊&#39;, 0.6671714782714844), (&#39;爱玩&#39;, 0.6659203767776489), (&#39;着迷&#39;, 0.6657696962356567)]
enter an word:混蛋
[(&#39;享福&#39;, 0.9413065910339355), (&#39;没良心&#39;, 0.9331107139587402), (&#39;怪不得&#39;, 0.9317291975021362), (&#39;养不活&#39;, 0.9283043742179871), (&#39;好惨&#39;, 0.9255991578102112), (&#39;看笑话&#39;, 0.9251411557197571), (&#39;逗我&#39;, 0.9232471585273743), (&#39;命苦&#39;, 0.9226915836334229), (&#39;别怪&#39;, 0.921725332736969), (&#39;我养&#39;, 0.9205465316772461)]
enter an word:巴嘎
KeyError: &#34;word &#39;巴嘎&#39; not in vocabulary&#34;
</code></pre></div><p>从上面我们可以看到，wordvec中对于词表外的词是无法查询的，为了缓解这一问题，可以通过训练时候的min_count参数调至1，以覆盖更多的词语，另一种则是进行增量训练。</p>
<br>
<h3 id="52-fasttext向量训练">5.2 fasttext向量训练</h3>
<p>与wordvec类似，fasttext也才用了类似的训练方法。</p>
<h4 id="1全量数据训练">1、全量数据训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">full_train_embedding</span><span class="p">():</span>
    <span class="n">feature_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">window_size</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">min_count</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">workers</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">corpus_file</span> <span class="o">=</span> <span class="n">datapath</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_count</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">workers</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="n">corpus_file</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
        <span class="n">corpus_file</span><span class="o">=</span><span class="n">corpus_file</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
        <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">total_words</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_total_words</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1">#保存词向量文件,保存的模型仅包含词-向量信息</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><br>
<h4 id="2增量数据训练">2、增量数据训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">update_train_embedding</span><span class="p">():</span>
    <span class="c1"># 获取日志信息</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># 加载新的训练数据</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">LineSentence</span><span class="p">(</span><span class="n">update_filepath</span><span class="p">)</span>
    <span class="c1"># 加载旧模型</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1"># 更新词汇表</span>
    <span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># 训练数据</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">epochs</span><span class="p">)</span>  <span class="c1"># epoch=iter语料库的迭代次数；（默认为5）  total_examples:句子数。</span>
    <span class="c1"># 保存模型，是分成两个来训练</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_update_path</span><span class="p">)</span>
    <span class="c1"># 保存词向量文件</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_update_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>
</code></pre></div><br>
<h4 id="3词向量结果测试">3、词向量结果测试</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">test_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="k">while</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">wd</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&#34;enter an word:&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">wd</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">return</span>
</code></pre></div><p>通过执行，我们会得到以下查询结果：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">enter an word:开心
[(&#39;开心果&#39;, 0.7953568696975708), (&#39;高兴&#39;, 0.7377268671989441), (&#39;郡县&#39;, 0.6981974244117737), (&#39;有缘&#39;, 0.6916821002960205), (&#39;折勾以&#39;, 0.687650203704834), (&#39;爱&#39;, 0.684776782989502), (&#39;愉快&#39;, 0.6840348243713379), (&#39;快乐&#39;, 0.676334023475647), (&#39;太高兴&#39;, 0.6728817224502563), (&#39;放心&#39;, 0.6692144274711609)]
enter an word:混蛋
[(&#39;侯希辰&#39;, 0.7582178115844727), (&#39;舐&#39;, 0.7578023672103882), (&#39;走眼&#39;, 0.7541716694831848), (&#39;有眼&#39;, 0.7511969804763794), (&#39;贺应勤&#39;, 0.7478049397468567), (&#39;罗敏&#39;, 0.747008204460144), (&#39;郭守桥&#39;, 0.7450246810913086), (&#39;熊芳琴&#39;, 0.7417726516723633), (&#39;找死&#39;, 0.741632342338562), (&#39;许身&#39;, 0.7414941787719727)]
enter an word:巴嘎
[(&#39;陈晓大爆&#39;, 0.3896751403808594), (&#39;董王勇&#39;, 0.36747634410858154), (&#39;李刚&#39;, 0.34988462924957275), (&#39;曾杰&#39;, 0.34452974796295166), (&#39;张文宾&#39;, 0.3370075821876526), (&#39;成浩&#39;, 0.3369928300380707), (&#39;刘晓静&#39;, 0.3348349630832672), (&#39;刘晓丹&#39;, 0.3348219394683838), (&#39;刘骏&#39;, 0.32817351818084717), (&#39;吴建明&#39;, 0.32765522599220276)]
</code></pre></div><p>与上面的wordvec无法处理OOV问题不同，对于八嘎这一词，fasttext依旧可以推断出来，关于这个中间步骤，我们可以作为单独一个问题来说明。</p>
<br>
<h4 id="4fasttext是如何解决oov问题的">4、fasttext是如何解决oov问题的</h4>
<p>通过对其源码进行阅读，可以发现fasttext针对OOV词的原始计算方式包括三个步骤，</p>
<ul>
<li>1）抽取出每个词的N-grams;</li>
<li>2）与预先存好的n-grams词库进行匹配;</li>
<li>3）将匹配到的n-gram向量进行平均，实现如下：</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.utils_any2vec</span> <span class="kn">import</span> <span class="n">_save_word2vec_format</span><span class="p">,</span> <span class="n">_load_word2vec_format</span><span class="p">,</span> <span class="n">_compute_ngrams</span><span class="p">,</span> <span class="n">_ft_hash</span>

<span class="k">def</span> <span class="nf">compute_ngrams</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">min_n</span><span class="p">,</span> <span class="n">max_n</span><span class="p">):</span>
    <span class="n">BOW</span><span class="p">,</span> <span class="n">EOW</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;&lt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&gt;&#39;</span><span class="p">)</span>  <span class="c1"># Used by FastText to attach to all words as prefix and suffix</span>
    <span class="n">extended_word</span> <span class="o">=</span> <span class="n">BOW</span> <span class="o">+</span> <span class="n">word</span> <span class="o">+</span> <span class="n">EOW</span>
    <span class="n">ngrams</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">ngram_length</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_n</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">extended_word</span><span class="p">),</span> <span class="n">max_n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">extended_word</span><span class="p">)</span> <span class="o">-</span> <span class="n">ngram_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">ngrams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">extended_word</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">ngram_length</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ngrams</span>

    <span class="k">def</span> <span class="nf">word_vec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">use_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">FastTextKeyedVectors</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">word_vec</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">use_norm</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># from gensim.models.fasttext import compute_ngrams</span>
            <span class="n">word_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vectors_ngrams</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">ngrams</span> <span class="o">=</span> <span class="n">_compute_ngrams</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_n</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">use_norm</span><span class="p">:</span>
                <span class="n">ngram_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors_ngrams_norm</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ngram_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors_ngrams</span>
            <span class="n">ngrams_found</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">:</span>
                <span class="n">ngram_hash</span> <span class="o">=</span> <span class="n">_ft_hash</span><span class="p">(</span><span class="n">ngram</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket</span>
                <span class="k">if</span> <span class="n">ngram_hash</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hash2index</span><span class="p">:</span>
                    <span class="n">word_vec</span> <span class="o">+=</span> <span class="n">ngram_weights</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hash2index</span><span class="p">[</span><span class="n">ngram_hash</span><span class="p">]]</span>
                    <span class="n">ngrams_found</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">word_vec</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">word_vec</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ngrams_found</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># No ngrams of the word are present in self.ngrams</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s1">&#39;all ngrams for word </span><span class="si">%s</span><span class="s1"> absent from model&#39;</span> <span class="o">%</span> <span class="n">word</span><span class="p">)</span>
</code></pre></div><p>例如，通过滑动窗口的方式，设定最短ngram和最长ngram，可以得到ngram集合。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&gt;&gt;&gt; from gensim.models.utils_any2vec import *
&gt;&gt;&gt; ngrams = compute_ngrams(&#39;好嗨哦&#39;,min_n = 1,max_n =3)
&gt;&gt;&gt; ngrams
[&#39;&lt;&#39;, &#39;好&#39;, &#39;嗨&#39;, &#39;哦&#39;, &#39;&gt;&#39;, &#39;&lt;好&#39;, &#39;好嗨&#39;, &#39;嗨哦&#39;, &#39;哦&gt;&#39;, &#39;&lt;好嗨&#39;, &#39;好嗨哦&#39;, &#39;嗨哦&gt;&#39;]
</code></pre></div><p>不过，可以看到的是，ngram中引入了“&lt;”和“&gt;”用于标记头和尾，这对于语言模型来说十分生动。</p>
<p><br><br></p>
<h2 id="六开源词向量训练工具与预训文件">六、开源词向量训练工具与预训文件</h2>
<p>不必重复造轮子，当前已经陆续出现了一些代表性的预训练词向量工具和词向量资源，我们可以充分利用好。</p>
<h3 id="61-开源词向量训练工具">6.1 开源词向量训练工具</h3>
<ul>
<li>ngram2vec： <a href="https://github.com/zhezhaoa/ngram2vec/">https://github.com/zhezhaoa/ngram2vec/</a></li>
<li>word2vec： <a href="https://github.com/svn2github/word2vec">https://github.com/svn2github/word2vec</a></li>
<li>fasttext： <a href="https://github.com/facebookresearch/fastText">https://github.com/facebookresearch/fastText</a></li>
<li>glove：https://github.com/stanfordnlp/GloVe</li>
</ul>
<br>
<h3 id="62-开源预训练词向量文件">6.2 开源预训练词向量文件</h3>
<ul>
<li><a href="https://github.com/Embedding/Chinese-Word-Vectors">https://github.com/Embedding/Chinese-Word-Vectors</a></li>
<li><a href="https://github.com/liuhuanyong/Word2Vector">https://github.com/liuhuanyong/Word2Vector</a></li>
<li><a href="https://github.com/liuhuanyong/ChineseEmbedding">https://github.com/liuhuanyong/ChineseEmbedding</a></li>
</ul>
<p><br><br></p>
<h2 id="七本文总结">七、本文总结</h2>
<p>本文，主要围绕预训练词向量模型这一主题，对当前预训练词向量模型的常用方法、评估与应用方式、领域的迁移变体、当前开放的训练工具和词向量文件进行介绍，并以word2vec和fasttext为例，展示一个词向量训练的案例，做到理论与实践相结合。</p>
<p>关于预训练词向量相关的文章目前已经有很多，关于更为细致的解读，可以参考其他材料。预训练词向量是bert出现之前，NLP处理业务问题的标配，绝对称得上是一个里程碑的事件，并且开创了“万物皆可embdding”的时代。</p>
<p>实际上，词向量的发展也在一定程度上验证了当前nlp的进步。</p>
<p>由最开始的基于one-hot、tf-idf、textrank等的bag-of-words，到LSA（SVD）、pLSA、LDA的主题模型词向量，再到word2vec、fastText、glove为代表的固定表征，最后到当前elmo、GPT、bert为代表的基于词向量的动态表征，都说明了语义建模中的动态属性和文本语境的多样性。</p>
<p>不过，我们需要认识的是，在此类词向量中，虽然其本质仍然是语言模型，但是它的目标不是语言模型本身，而是词向量，其所作的一系列优化，其专注于词向量本身，因此做了许多优化来提高计算效率。</p>
<p>例如，与NNLM相比，word2vec将词向量直接sum，不再拼接，并舍弃隐层；考虑到sofmax归一化需要遍历整个词汇表，采用hierarchical softmax 和negative sampling进行优化，前者生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；后者对每一个样本中每一个词都进行负例采样。</p>
<p>最后，以当前一个新的观点来结尾：</p>
<p>现在的预训练语言模型是下一代知识图谱，那么预训练词向量是什么？垫底型相关词库？大家可以想想。</p>
<p><br><br></p>
<h2 id="参考文献">参考文献</h2>
<ol>
<li><a href="https://baijiahao.baidu.com/sid=1600509930259553151">https://baijiahao.baidu.com/sid=1600509930259553151</a></li>
<li><a href="https://mp.weixin.qq.com/s/u8WZqlsIcGCU5BqPH23S4w">https://mp.weixin.qq.com/s/u8WZqlsIcGCU5BqPH23S4w</a></li>
<li><a href="https://www.jianshu.com/p/546d12898378/">https://www.jianshu.com/p/546d12898378/</a></li>
<li><a href="https://www.jianshu.com/p/471d9bfbd72f">https://www.jianshu.com/p/471d9bfbd72f</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32965521">https://zhuanlan.zhihu.com/p/32965521</a></li>
<li><a href="https://mp.weixin.qq.com/s/DvbvYppeuMaPn84SRAINcQ">https://mp.weixin.qq.com/s/DvbvYppeuMaPn84SRAINcQ</a></li>
</ol>
<br> 
<br> 
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Python | 词移距离(Word Mover&#39;s Distance)</title>
      <link>https://textdata.cn/blog/2022-10-16-python-word-mover-s-distance/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-16-python-word-mover-s-distance/</guid>
      <description>词移距离可以为我们提供短文相似度计算，距离越小，两文档相似度越高。</description>
      <content:encoded><![CDATA[


<p>词嵌入方法（word2vec、glove等）可以将每个词的语义映射到n维空间，在n维空间中，词语间距离远近可以表征语义的远近。Kusner等人(2015)提出<strong>词移距离</strong>（word mover’s distance， 后文用WMD缩写代替）借助词语向量语义距离，实现两文档间的相似度计算，距离越小，相似度越高。在会计领域中的应用可以用来度量问答场景的答非所问的程度。</p>
<p><br></p>
<div id="wmd基础" class="section level2">
<h2>WMD基础</h2>
<p>有两个文档</p>
<pre><code>doc1 = &quot;Obama speaks to the media in Illinois&quot;
doc2 = &quot;The President greets the press in Chicago.&quot;</code></pre>
<p>词向量一般是高(n)维空间，这里把n压缩到2维空间，使用matplotlib绘图。两个文档中重要的词语彼此之间存在语义相似度，</p>
<pre class="python"><code># Image from https://vene.ro/images/wmd-obama.png
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
img = mpimg.imread(&#39;img/wmd-obama.png&#39;)
imgplot = plt.imshow(img)
plt.axis(&#39;off&#39;)</code></pre>
<pre><code>## (-0.5, 2397.5, 1327.5, -0.5)</code></pre>
<pre class="python"><code>plt.show()</code></pre>
<p><img src="/blog/2022-10-16-python-word-mover-s-distance/index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p><img src="img/wmd-obama.png" /></p>
<p><br></p>
</div>
<div id="计算wmd步骤" class="section level2">
<h2>计算WMD步骤</h2>
<ol style="list-style-type: decimal">
<li>剔除文档中停止词，如the、a等无信息量词</li>
<li>导入预训练好的词嵌入(word2vec)模型(网上资源比较多，如果数据量很大，也可以自己使用gensim训练自己的词向量)</li>
<li>计算WMD</li>
</ol>
<pre class="python"><code>from nltk.corpus import stopwords
from nltk import download
download(&#39;stopwords&#39;)  # Download stopwords list.
stop_words = stopwords.words(&#39;english&#39;)

def preprocess(sentence):
    return [w for w in sentence.lower().split() if w not in stop_words]


doc1 = &quot;Obama speaks to the media in Illinois&quot;
doc2 = &quot;The President greets the press in Chicago.&quot;

doc1 = preprocess(doc1)
doc2 = preprocess(doc2)

print(doc1)
print(doc2)</code></pre>
<p>Run</p>
<pre><code>[&#39;obama&#39;, &#39;speaks&#39;, &#39;media&#39;, &#39;illinois&#39;]
[&#39;president&#39;, &#39;greets&#39;, &#39;press&#39;, &#39;chicago.&#39;]</code></pre>
<p><br></p>
<blockquote>
<p>如果运行代码出现nltk问题，可以观看视频 <a href="https://www.bilibili.com/video/BV14A411i7DB" class="uri">https://www.bilibili.com/video/BV14A411i7DB</a></p>
</blockquote>
<p>下载谷歌新闻预训练模型(word2vec-google-news-300) ,这里可以使用我提供的百度网盘</p>
<blockquote>
<p>链接：<a href="https://pan.baidu.com/s/1yzGLcMsZl3u1zigTHLdc2Q" class="uri">https://pan.baidu.com/s/1yzGLcMsZl3u1zigTHLdc2Q</a>
提取码：l63f</p>
</blockquote>
<p>这里需要</p>
<pre class="python"><code>from gensim.models import KeyedVectors

w2v_model = KeyedVectors.load(&#39;GoogleNews-vectors-negative300.bin.gz&#39;)
wmd = w2v_model.wmdistance(doc1, doc2)
print(&#39;distance :{wmd}&#39;.format(wmd=wmd))</code></pre>
<p>Run</p>
<pre><code>distance :0.8867237050133944</code></pre>
<p><br></p>
</div>
<div id="参考文献" class="section level2">
<h2>参考文献</h2>
<ul>
<li>Kusner, Matt J., Yu Sun, Nicholas I. Kolkin and Kilian Q. Weinberger. “From Word Embeddings To Document Distances.” ICML (2015).</li>
<li><a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html" class="uri">https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html</a></li>
</ul>
<p><br></p>
</div>
<div id="广而告之" class="section level2">
<h2>广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 | 多语言对齐词向量预训练模型</title>
      <link>https://textdata.cn/blog/2022-10-16-aligned-word-vectors/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-16-aligned-word-vectors/</guid>
      <description>借助该预训练模型，应该能做可做跨文化对比分析</description>
      <content:encoded><![CDATA[<h2 id="介绍">介绍</h2>
<p>Facebook研究者使用 fastText 算法，对维基百科(44种语言)语料数据进行了训练，最终生成了 44 种语言的对齐词向量。</p>
<br>
<h2 id="用途">用途</h2>
<p>wiki数据集有个优点，即由于众人分享、翻译，将不同语言的百科词条进行了翻译整理。所以facebook使用wiki训练对齐词向量有助于提升翻译准确性。与此同时，因为翻译者处于不同的语言和文化背景下，词条及词条内容必然蕴含着语言所特有的文化信息线索，有可能有助于我们挖掘跨语言的文化差异。例如中文词条<code>护士</code>和 英文词条<code>nurse</code> ，可以借助对齐词向量，比较护士这个群体在性别、种族等语义上的差异。</p>
<p>之前分享过的内容</p>
<ul>
<li><a href="https://textdata.cn/blog/embeddingsandattitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展&mdash;&mdash;基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
<li><a href="https://textdata.cn/blog/literatureembeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
</ul>
<p>不过fastText算法认为词语有不同的大小划分层次，从大到小分别是词语、词缀、字符等，使用 Joulin 等人 (2018) 中描述的 RCSLS 方法进行比对。</p>
<table>
<thead>
<tr>
<th><strong>Code</strong></th>
<th><strong>en-es</strong></th>
<th><strong>es-en</strong></th>
<th><strong>en-fr</strong></th>
<th><strong>fr-en</strong></th>
<th><strong>en-de</strong></th>
<th><strong>de-en</strong></th>
<th><strong>en-ru</strong></th>
<th><strong>ru-en</strong></th>
<th><strong>en-zh</strong></th>
<th><strong>zh-en</strong></th>
<th><strong>avg</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Joulin et al. [<a href="https://arxiv.org/abs/1804.07745">1</a>]</td>
<td>84.1</td>
<td>86.3</td>
<td>83.3</td>
<td>84.1</td>
<td><strong>79.1</strong></td>
<td>76.3</td>
<td><strong>57.9</strong></td>
<td><strong>67.2</strong></td>
<td>45.9</td>
<td>46.4</td>
<td>71.1</td>
</tr>
<tr>
<td>This implementation (10 epochs)</td>
<td>84.2</td>
<td><strong>86.6</strong></td>
<td><strong>83.9</strong></td>
<td>84.7</td>
<td>78.3</td>
<td>76.6</td>
<td>57.6</td>
<td>66.7</td>
<td><strong>47.6</strong></td>
<td><strong>47.4</strong></td>
<td>71.4</td>
</tr>
<tr>
<td>This implementation (unsup. model selection)</td>
<td><strong>84.3</strong></td>
<td><strong>86.6</strong></td>
<td><strong>83.9</strong></td>
<td><strong>85.0</strong></td>
<td>78.7</td>
<td><strong>76.7</strong></td>
<td>57.6</td>
<td>67.1</td>
<td><strong>47.6</strong></td>
<td><strong>47.4</strong></td>
<td><strong>71.5</strong></td>
</tr>
</tbody>
</table>
<p>算法得出的词向量在西方，尤其是西欧语言之间进行语义对齐，效果可能更好。而中文、日语等汉字语言，是由偏旁部首组成，与西方字母语言还是存在一定差异。上表也可以看出中英语义对齐准确率47%， 而其他语言之间对齐准确率平均为71%。</p>
<br>
<h2 id="模型资源">模型资源</h2>
<p><a href="https://fasttext.cc/docs/en/aligned-vectors.html">https://fasttext.cc/docs/en/aligned-vectors.html</a></p>
<p>对齐预训练向量模型下载链接</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Afrikaans: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.af.align.vec"><em>text</em></a></td>
<td>Arabic: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ar.align.vec"><em>text</em></a></td>
<td>Bulgarian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bg.align.vec"><em>text</em></a></td>
<td>Bengali: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bn.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Bosnian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bs.align.vec"><em>text</em></a></td>
<td>Catalan: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ca.align.vec"><em>text</em></a></td>
<td>Czech: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.cs.align.vec"><em>text</em></a></td>
<td>Danish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.da.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>German: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec"><em>text</em></a></td>
<td>Greek: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.el.align.vec"><em>text</em></a></td>
<td>English: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.en.align.vec"><em>text</em></a></td>
<td>Spanish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.es.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Estonian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.et.align.vec"><em>text</em></a></td>
<td>Persian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fa.align.vec"><em>text</em></a></td>
<td>Finnish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fi.align.vec"><em>text</em></a></td>
<td>French: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fr.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Hebrew: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.he.align.vec"><em>text</em></a></td>
<td>Hindi: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hi.align.vec"><em>text</em></a></td>
<td>Croatian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hr.align.vec"><em>text</em></a></td>
<td>Hungarian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hu.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Indonesian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.id.align.vec"><em>text</em></a></td>
<td>Italian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.it.align.vec"><em>text</em></a></td>
<td>Korean: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ko.align.vec"><em>text</em></a></td>
<td>Lithuanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.lt.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Latvian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.lv.align.vec"><em>text</em></a></td>
<td>Macedonian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.mk.align.vec"><em>text</em></a></td>
<td>Malay: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ms.align.vec"><em>text</em></a></td>
<td>Dutch: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.nl.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Norwegian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.no.align.vec"><em>text</em></a></td>
<td>Polish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.pl.align.vec"><em>text</em></a></td>
<td>Portuguese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.pt.align.vec"><em>text</em></a></td>
<td>Romanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ro.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Russian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ru.align.vec"><em>text</em></a></td>
<td>Slovak: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sk.align.vec"><em>text</em></a></td>
<td>Slovenian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sl.align.vec"><em>text</em></a></td>
<td>Albanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sq.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Swedish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sv.align.vec"><em>text</em></a></td>
<td>Tamil: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ta.align.vec"><em>text</em></a></td>
<td>Thai: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.th.align.vec"><em>text</em></a></td>
<td>Tagalog: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.tl.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Turkish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.tr.align.vec"><em>text</em></a></td>
<td>Ukrainian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.uk.align.vec"><em>text</em></a></td>
<td>Vietnamese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.vi.align.vec"><em>text</em></a></td>
<td>Chinese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.zh.align.vec"><em>text</em></a></td>
</tr>
</tbody>
</table>
<br>
<h2 id="格式">格式</h2>
<p>词向量默认使用的fastText格式</p>
<ul>
<li>第一行给了词向量的维数</li>
<li>从第二行开始，每一行由词语及对应的词向量组成。</li>
<li>数值之间使用空格间隔</li>
</ul>
<br>
<h2 id="代码">代码</h2>
<h3 id="导入模型">导入模型</h3>
<p>使用gensim导入fastText方法训练出的 预训练语言模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1">#导入刚刚下载的预训练模型</span>
<span class="c1">#该词向量模型300维</span>
<span class="n">zh_w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;wiki.zh.align.vec&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1">#英文词向量模型5G，太大了。如果内存小于16G不要使用下面命令</span>
<span class="c1">#en_w2v_model = KeyedVectors.load_word2vec_format(&#39;wiki.en.align.vec&#39;, binary=False)</span>
</code></pre></div><p>一旦导入成功，就可以进行向量计算。这里仅进行简单演示</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取某词的词向量</span>
<span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>array([ 0.0733,  0.0782,  0.0188, -0.0027, -0.0052,...,  0.0586,  0.0166,
       -0.1401, -0.0545, -0.0125,  0.0373, -0.0681,  0.063 ],
      dtype=float32)
</code></pre>
<br>
<p>在中文中， 护士职业的主要从业者为女性，反应在词向量相似度上，如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">,</span> <span class="s1">&#39;女性&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">,</span> <span class="s1">&#39;男性&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<pre><code>0.4417011
0.378651
</code></pre>
<br>
<p>更多w2v_model用法可参考 <a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="文献">文献</h2>
<p>如果使用了facebook的预训练词向量，请引用以下两篇文献。</p>
<ul>
<li>Joulin, Armand, Piotr Bojanowski, Tomas Mikolov, Hervé Jégou, and Edouard Grave. &ldquo;Loss in translation: Learning bilingual word mapping with a retrieval criterion.&rdquo; arXiv preprint arXiv:1804.07745 (2018).</li>
<li>Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. &ldquo;Enriching word vectors with subword information.&rdquo; Transactions of the association for computational linguistics 5 (2017): 135-146.</li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>R语言 | 使用word2vec词向量模型</title>
      <link>https://textdata.cn/blog/2022-10-12-r-word2vec/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-12-r-word2vec/</guid>
      <description>R语言训练和使用词向量word2vec模型</description>
      <content:encoded><![CDATA[


<p>Python的gensim库可以训练和使用word2vec模型，R语言中也有与之对应的<code>word2vec包</code>。word2vec是词嵌入技术中最常用的一种技术，如果对词嵌入不太了解，可以阅读前文</p>
<ul>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
</ul>
<p>本文需要的R包</p>
<pre><code>install.packages(c(&quot;word2vec&quot;, &quot;jiebaR&quot;, &quot;tidyverse&quot;, &quot;readtext&quot;))</code></pre>
<p><br></p>
<div id="word2vec包常用函数" class="section level2">
<h2>word2vec包常用函数</h2>
<ul>
<li>word2vec 使用文本数据训练word2vec模型</li>
<li>as.matrix 获取词向量</li>
<li>doc2vec 获取文档向量</li>
<li>predict 获取</li>
<li>write.word2vec 保存word2vec模型至文件</li>
<li>read.word2vec 读取word2vec模型文件</li>
</ul>
<p><br></p>
</div>
<div id="准备数据" class="section level2">
<h2>准备数据</h2>
<p>原始数据是从网站下载的 <code>三体.txt</code>, 未分词处理，现在需要</p>
<ol style="list-style-type: decimal">
<li>读中文取txt数据</li>
<li>保留标点符号，进行分词处理</li>
<li>分词结果重新整理为类似英文(空格间隔词语的形式)字符串</li>
<li>结果存入新的txt</li>
</ol>
<pre class="r"><code>library(jiebaR)
library(tidyverse)
library(word2vec)


#导入数据
tri_body &lt;- readtext::readtext(&#39;data/三体.txt&#39;)$text 

#分词（保留标点符号）
tokenizer &lt;- worker(symbol=T)
tri_words &lt;- segment(tri_body, tokenizer)

# 整理为英文格式（词语之间加空格）
segmented_text &lt;- stringr::str_c(tri_words, collapse = &quot; &quot;) %&gt;% c()

#写入txt
readr::write_file(segmented_text, file=&#39;data/santi.txt&#39;)</code></pre>
<p><br></p>
</div>
<div id="训练word2vec模型" class="section level2">
<h2>训练word2vec模型</h2>
<pre><code>word2vec(
  x,
  type = c(&quot;cbow&quot;, &quot;skip-gram&quot;),
  dim = 50,
  window = ifelse(type == &quot;cbow&quot;, 5L, 10L),
  iter = 5L,
  lr = 0.05,
  min_count = 5L,
  split = c(&quot; \n,.-!?:;/\&quot;#$%&amp;&#39;()*+&lt;=&gt;@[]\\^_`{|}~\t\v\f\r&quot;, &quot;.\n?!&quot;),
  stopwords = character(),
  threads = 1L,
  ...
)</code></pre>
<ul>
<li>x 英文文本数据txt文件(中文数据txt文件是分词后的txt文件，空格间隔词语)</li>
<li>type 训练方式，默认CBOW</li>
<li>dim 词向量维度，默认50维</li>
<li>window 词向量窗口，默认5</li>
<li>iter 训练迭代次数，默认5</li>
<li>split 分词、分句对应的分隔符。</li>
<li>lr 学习率，默认0.05</li>
<li>min_count 词语在语料中至少要出现5次(低于5次的词语，训练好的结果中没有该词语）</li>
<li>stopwords 停用词表，默认空字符集</li>
<li>threads 并行加速，cpu核数，默认1。为了加速训练过程，可以使用 <code>parallel::detectCores()</code> 获得本电脑的核数</li>
</ul>
<pre class="r"><code>#训练10维的词向量模型
model &lt;- word2vec(x = &#39;data/santi.txt&#39;, 
                  dim = 10,  
                  iter = 20, 
                  split = c(&quot; &quot;,  &quot;。？！；&quot;),
                  threads = parallel::detectCores()) #并行，使用cpu多核加速

emb &lt;- as.matrix(model)

#显示6个词
head(emb)</code></pre>
<pre><code>##             [,1]       [,2]        [,3]        [,4]      [,5]        [,6]
## 煮   -1.02566934 -0.9271542 -0.42417252 -0.54280633 1.8847700  0.41640753
## 报   -0.83992052  1.9440031  0.09093992  0.83522910 1.7909089  0.72149992
## 悬空 -0.06369513 -1.3519955 -2.13137460 -0.06198586 0.6096401  1.32933748
## 略    1.74687469 -0.4278547 -0.33822438  1.08505321 2.0168977 -0.07693915
## 伏   -0.68947995 -1.4147453 -1.95522511 -0.39963767 0.5269030  0.30352208
## 石柱 -0.40561640 -1.3643234  0.30329546 -0.94012892 2.1579018  0.79654717
##            [,7]       [,8]       [,9]      [,10]
## 煮   -1.1708908 -0.7624418 -0.6275516  1.2417521
## 报    0.5235919  0.8448864 -0.2960095 -0.0773837
## 悬空  0.1527163 -0.1337370 -0.1646384  1.1892601
## 略   -0.3246748 -0.9813624  0.5045205  0.2771466
## 伏    0.3166684 -1.4238008 -1.0167172 -0.0976937
## 石柱  0.2237919  0.6933151  0.7412233 -0.7918702</code></pre>
<p><br></p>
</div>
<div id="查看某词的vector" class="section level2">
<h2>查看某词的vector</h2>
<p>查看词语 <code>汪淼</code> 的vector</p>
<pre class="r"><code>emb[&quot;汪淼&quot;,]</code></pre>
<pre><code>##  [1] -0.77559733 -0.90021265  0.66555792 -0.10277803  1.89924443 -0.88817298
##  [7] -1.32665634 -0.75938725 -0.09628224  1.18008399</code></pre>
<p>查看词语 <code>地球</code> 的vector</p>
<pre class="r"><code>emb[&quot;地球&quot;,]</code></pre>
<pre><code>##  [1]  0.29645494 -0.61688840  0.91209215 -0.64530188  0.62816381 -0.72807491
##  [7]  0.50655973  2.38137436  1.19238114 -0.09610342</code></pre>
<p><br></p>
</div>
<div id="predict" class="section level2">
<h2>predict()</h2>
<p>找到语料中，词语 <code>罗辑</code> 最相似的 20个词</p>
<pre class="r"><code>predict(model, &#39;罗辑&#39;, type=&#39;nearest&#39;, top_n = 20)</code></pre>
<pre><code>## $罗辑
##    term1    term2 similarity rank
## 1   罗辑     胡文  0.9744400    1
## 2   罗辑   申玉菲  0.9678891    2
## 3   罗辑   瓦季姆  0.9550550    3
## 4   罗辑 狄奥伦娜  0.9518393    4
## 5   罗辑     蓝西  0.9472395    5
## 6   罗辑     护士  0.9471439    6
## 7   罗辑   法扎兰  0.9458703    7
## 8   罗辑   白艾思  0.9451101    8
## 9   罗辑     坎特  0.9396626    9
## 10  罗辑     白蓉  0.9387447   10
## 11  罗辑   参谋长  0.9377206   11
## 12  罗辑   弗雷斯  0.9369408   12
## 13  罗辑   第一眼  0.9357565   13
## 14  罗辑     父亲  0.9350463   14
## 15  罗辑   多少次  0.9314436   15
## 16  罗辑     门去  0.9291503   16
## 17  罗辑     维德  0.9267251   17
## 18  罗辑     褐蚁  0.9203902   18
## 19  罗辑       刚  0.9200501   19
## 20  罗辑     吴岳  0.9191605   20</code></pre>
<p>查看均值向量（多个词向量中心的）的10个近义词</p>
<pre class="r"><code>vectors &lt;- emb[c(&quot;汪淼&quot;, &quot;罗辑&quot;, &quot;叶文洁&quot;), ]
centroid_vector &lt;- colMeans(vectors)

predict(model, centroid_vector, type = &quot;nearest&quot;, top_n = 10)</code></pre>
<pre><code>##        term similarity rank
## 1      罗辑  0.9185568    1
## 2  狄奥伦娜  0.9104245    2
## 3      文洁  0.9088279    3
## 4      汪淼  0.9054156    4
## 5    白艾思  0.9046930    5
## 6      张翔  0.9026827    6
## 7      尴尬  0.8952187    7
## 8      庄颜  0.8952166    8
## 9      皇帝  0.8949283    9
## 10     父亲  0.8915347   10</code></pre>
<p><br></p>
</div>
<div id="doc2vec" class="section level2">
<h2>doc2vec()</h2>
<ul>
<li>doc2vec(object, newdata, split = ” “)
<ul>
<li>object word2vec模型对象</li>
<li>newdata 文档列表(用空格间隔的字符串列表)</li>
<li>split 默认分隔符是空格</li>
</ul></li>
</ul>
<p>将文档转为向量</p>
<pre class="r"><code>docs &lt;- c(&quot;哦 ， 对不起 ， 汪 教授 。 这是 我们 史强 队长 。&quot;, 
          &quot; 丁仪 博士 ， 您 能否 把 杨冬 的 遗书 给 汪 教授 看 一下 ？ &quot;)

doc2vec(object=model, newdata = docs, split=&#39; &#39;)</code></pre>
<pre><code>##            [,1]       [,2]       [,3]     [,4]      [,5]       [,6]       [,7]
## [1,] -1.1769752 -0.1065619  0.1983950 1.734068 0.5478012 -0.8320528 -0.2387014
## [2,] -0.4827189  0.0664595 -0.2119484 1.895074 0.6729840 -0.3008853 -0.6857539
##            [,8]      [,9]      [,10]
## [1,] -0.5519856 -2.007002  0.4182127
## [2,] -0.5976922 -2.130454 -0.4653725</code></pre>
<p><br></p>
</div>
<div id="保存word2vec模型" class="section level2">
<h2>保存word2vec模型</h2>
<p>保存模型，一般有两个目的</p>
<ul>
<li>为了分享word2vec模型</li>
<li>避免反复训练模型，节约数据分析时间</li>
</ul>
<pre class="r"><code>word2vec::write.word2vec(x = model, 
                         #新建output文件夹，将模型存入output文件夹内
                         file = &quot;output/santi_word2vec.bin&quot;)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p><br></p>
</div>
<div id="导入预训练模型" class="section level2">
<h2>导入预训练模型</h2>
<p>导入 <code>output/santi_word2vec.bin</code> 的预训练word2vec模型</p>
<pre class="r"><code>pre_trained_model &lt;- word2vec::read.word2vec(file = &quot;output/santi_word2vec.bin&quot;)
pre_trained_emb &lt;- as.matrix(pre_trained_model)
head(pre_trained_emb)</code></pre>
<pre><code>##              [,1]       [,2]       [,3]       [,4]       [,5]        [,6]
## 回荡   -1.9563367 -0.3099073 -1.2969902 -0.5719763  1.1507142 -0.05515177
## 听证会  0.2756990  1.3702289 -1.3303705 -0.1827691  0.6622804 -1.92008448
## 纲领    0.4495552  1.9311246 -0.5812275 -0.1470096 -0.2678985 -0.01694358
## 很亮    0.3621844 -1.0048453  0.7036168 -2.0917876  0.6459805  1.18436253
## 秒      1.9033701  1.6510324 -0.2616904  0.3671210  1.0618066  0.06588747
## 杰森   -1.2904713 -1.2501229  0.3380587  0.8590797  1.6798494 -0.58775252
##              [,7]       [,8]       [,9]      [,10]
## 回荡    1.1082711 -0.2064489 -0.9264346 -0.7816723
## 听证会 -1.0952694  0.6120903 -0.1326561  0.7252344
## 纲领   -0.6097277  2.1051276 -0.2405726 -0.8808851
## 很亮    0.1964065 -1.3926132 -0.4042619 -0.1645472
## 秒     -0.8347995  0.2591044  0.3594093  1.1929117
## 杰森    0.4941484 -1.1393189 -0.4687541  0.9951217</code></pre>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>NLP资源 | 汽车、金融等9大领域预训练词向量模型下载资源</title>
      <link>https://textdata.cn/blog/pretained_nlp_models/</link>
      <pubDate>Wed, 25 May 2022 10:43:10 +0600</pubDate>
      
      <guid>/blog/pretained_nlp_models/</guid>
      <description>本文主要开放汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等9大领域预训练词向量，以及字符、依存、拼音与词性4类预训练向量地址，供大家一起使用</description>
      <content:encoded><![CDATA[<p>在前面的文章中，我们介绍了关于词向量的一些基础理论和训练方法，<strong>本文主要开放汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等9大领域预训练词向量，以及字符、依存、拼音与词性4类预训练向量地址，供大家一起使用</strong>。</p>
<h2 id="一汽车房产等9大领域预训练词向量">一、汽车、房产等9大领域预训练词向量</h2>
<p>通过收集多文本分类语料库，对汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等多个领域文本进行词向量训练，得到了如下预训练词向量的结果：</p>
<table>
<thead>
<tr>
<th>领域类型</th>
<th>模型类型</th>
<th>关键词集合</th>
<th>维度数</th>
</tr>
</thead>
<tbody>
<tr>
<td>汽车</td>
<td>word_vector_auto.model.bin</td>
<td>117,510</td>
<td>200</td>
</tr>
<tr>
<td>房产</td>
<td>word_vector_house.model.bin</td>
<td>145,287</td>
<td>200</td>
</tr>
<tr>
<td>教育</td>
<td>word_vector_edu.model.bin</td>
<td>242,874</td>
<td>200</td>
</tr>
<tr>
<td>社会</td>
<td>word_vector_society.model.bin</td>
<td>221,395</td>
<td>200</td>
</tr>
<tr>
<td>娱乐</td>
<td>word_vector_ent.model.bin</td>
<td>230,665</td>
<td>200</td>
</tr>
<tr>
<td>体育</td>
<td>word_vector_sports.model.bin</td>
<td>95724</td>
<td>200</td>
</tr>
<tr>
<td>金融</td>
<td>word_vector_finance.model.bin</td>
<td>284035</td>
<td>200</td>
</tr>
<tr>
<td>科技</td>
<td>word_vector_tech.model.bin</td>
<td>108188</td>
<td>200</td>
</tr>
<tr>
<td>游戏</td>
<td>word_vector_games.model.bin</td>
<td>100821</td>
<td>200</td>
</tr>
</tbody>
</table>
<p><strong>开放地址：</strong></p>
<p><a href="https://pan.baidu.com/s/1jEHFoAmVXlB67Q28-CeTvw">https://pan.baidu.com/s/1jEHFoAmVXlB67Q28-CeTvw</a> 密码: 1pa6</p>
<h2 id="二预训练字符依存拼音与词性向量">二、预训练字符、依存、拼音与词性向量</h2>
<p>通过对字符、依存、拼音与词性进行切分，使用同样的方式，可以得到相应的预训练词向量。</p>
<table>
<thead>
<tr>
<th>向量名称</th>
<th style="text-align:center">向量含义</th>
<th style="text-align:center">词数</th>
<th style="text-align:center">维度</th>
<th style="text-align:center">例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>de_vec_10</td>
<td style="text-align:center">依存关系向量</td>
<td style="text-align:center">13</td>
<td style="text-align:center">10</td>
<td style="text-align:center">SBV, ATT</td>
</tr>
<tr>
<td>pinyin_vec_300</td>
<td style="text-align:center">汉语拼音向量</td>
<td style="text-align:center">146242</td>
<td style="text-align:center">300</td>
<td style="text-align:center">ni, hao</td>
</tr>
<tr>
<td>postag_vec_30</td>
<td style="text-align:center">汉语词性向量</td>
<td style="text-align:center">59</td>
<td style="text-align:center">300</td>
<td style="text-align:center">n,v,a,d</td>
</tr>
<tr>
<td>token_vec_300</td>
<td style="text-align:center">汉语字向量</td>
<td style="text-align:center">20029</td>
<td style="text-align:center">300</td>
<td style="text-align:center">刘,焕,勇</td>
</tr>
<tr>
<td>word_vec_300</td>
<td style="text-align:center">汉语词向量</td>
<td style="text-align:center">673266</td>
<td style="text-align:center">300</td>
<td style="text-align:center">刘焕勇</td>
</tr>
</tbody>
</table>
<p><strong>开放地址：</strong></p>
<p><a href="https://github.com/liuhuanyong/ChineseEmbedding">https://github.com/liuhuanyong/ChineseEmbedding</a></p>
<p><strong>向量效果：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> 
 ***********************字符向量************************
    token:刘
    (&#39;李&#39;, 0.7306396961212158),(&#39;陈&#39;, 0.7201231122016907)
    (&#39;赵&#39;, 0.6974461674690247),(&#39;杨&#39;, 0.6972213983535767)
    (&#39;吴&#39;, 0.6851627230644226),(&#39;徐&#39;, 0.6516467332839966)
    (&#39;郭&#39;, 0.6499480605125427),(&#39;蔡&#39;, 0.6175302267074585)
    (&#39;郑&#39;, 0.6092196106910706),(&#39;孙&#39;, 0.5950524210929871)
    token:丑
    (&#39;卯&#39;, 0.6074919700622559),(&#39;酉&#39;, 0.5910211801528931)
    (&#39;巳&#39;, 0.5581363439559937),(&#39;戌&#39;, 0.43932047486305237)
    (&#39;戊&#39;, 0.41449615359306335),(&#39;壬&#39;, 0.40456631779670715)
    (&#39;謤&#39;, 0.367109090089798),(&#39;绯&#39;, 0.3643313944339752),
    (&#39;寅&#39;, 0.36351141333580017),(&#39;旽&#39;, 0.3549465537071228)


***********************依存向量************************
    dependency rel:ATT
    (&#39;COO&#39;, 0.14239487051963806),(&#39;ADV&#39;, -0.16987691819667816)
    (&#39;RAD&#39;, -0.2357601821422577),(&#39;HED&#39;, -0.2401314228773117)
    (&#39;SBV&#39;, -0.25625932216644287),(&#39;WP&#39;, -0.27165737748146057)
    (&#39;LAD&#39;, -0.2902592420578003),(&#39;POB&#39;, -0.2990782558917999)
    (&#39;VOB&#39;, -0.37553706765174866),(&#39;IOB&#39;, -0.6669262647628784)
    dependency rel:POB
    (&#39;IOB&#39;, 0.16698899865150452),(&#39;DBL&#39;, 0.16678886115550995)
    (&#39;FOB&#39;, 0.1657436639070511),(&#39;CMP&#39;, 0.14784857630729675)
    (&#39;VOB&#39;, 0.1461176574230194),(&#39;SBV&#39;, 0.08011472970247269)
    (&#39;LAD&#39;, -0.022307466715574265),(&#39;WP&#39;, -0.022942926734685898)
    (&#39;HED&#39;, -0.037264980375766754),(&#39;RAD&#39;, -0.042251598089933395)

  
  ***********************拼音向量************************
    pinyin:wo
    (&#39;shei&#39;, 0.6129732131958008)(&#39;ta&#39;, 0.6081706285476685)
    (&#39;nin&#39;, 0.5819231867790222),(&#39;！&#39;, 0.5435523986816406)
    (&#39;……&#39;, 0.48428624868392944),(&#39;ai&#39;, 0.47832390666007996)
    (&#39;o&#39;, 0.4761071801185608),(&#39;。』&#39;, 0.4598163366317749)
    (&#39;...&#39;, 0.45207729935646057),(&#39;ni&#39;, 0.44975683093070984)
    pinyin:guo
    (&#39;dang&#39;, 0.3908974528312683),(&#39;yuan&#39;, 0.378823846578598)
    (&#39;zu&#39;, 0.35387369990348816),(&#39;hua&#39;, 0.3405681848526001)
    (&#39;zheng&#39;, 0.3355437219142914),(&#39;yi&#39;, 0.3333034813404083)
    (&#39;ren&#39;, 0.3194104731082916),(&#39;jun&#39;, 0.3187354505062103)
    (&#39;hui&#39;, 0.31342023611068726),(&#39;xin&#39;, 0.3096797466278076)

   
   ***********************词性向量************************
    word postag:a
    (&#39;d&#39;, 0.7203904986381531),(&#39;c&#39;, 0.6124969720840454)
    (&#39;v&#39;, 0.4963228106498718),(&#39;an&#39;, 0.4531499147415161)
    (&#39;uz&#39;, 0.4459834396839142),(&#39;ud&#39;, 0.42059916257858276)
    (&#39;r&#39;, 0.4090540111064911),(&#39;uj&#39;, 0.4061364233493805)
    (&#39;i&#39;, 0.38707998394966125),(&#39;l&#39;, 0.3551557660102844)
    word postag:n
    (&#39;b&#39;, 0.7030695676803589),(&#39;vn&#39;, 0.490166038274765)
    (&#39;p&#39;, 0.4858315885066986),(&#39;v&#39;, 0.4499088227748871)
    (&#39;nt&#39;, 0.44155171513557434),(&#39;f&#39;, 0.26609259843826294)
    (&#39;s&#39;, 0.2639649212360382),(&#39;l&#39;, 0.24365971982479095)
    (&#39;ns&#39;, 0.2278469204902649),(&#39;m&#39;, 0.202927365899086)
    
    ***********************词向量************************
    word:爱情
    (&#39;爱恋&#39;, 0.6931096315383911),(&#39;真爱&#39;, 0.6897798776626587)
    (&#39;婚姻&#39;, 0.6540514826774597),(&#39;浪漫爱情&#39;, 0.6535360813140869)
    (&#39;情感&#39;, 0.6501022577285767),(&#39;感情&#39;, 0.6403399705886841)
    (&#39;纯爱&#39;, 0.6394841074943542),(&#39;爱情故事&#39;, 0.6282097101211548)
    (&#39;校园爱情&#39;, 0.6078493595123291),(&#39;情爱&#39;, 0.5976818799972534)
    word:创新
    (&#39;技术创新&#39;, 0.7648976445198059),(&#39;不断创新&#39;, 0.7172579765319824)
    (&#39;创新型&#39;, 0.6573833227157593),(&#39;创新能力&#39;, 0.6533682942390442)
    (&#39;创新性&#39;, 0.6160774827003479),(&#39;革新&#39;, 0.6159394383430481)
    (&#39;人才培养&#39;, 0.6093565821647644),(&#39;开拓创新&#39;, 0.6015594601631165)
    (&#39;探索&#39;, 0.5987343788146973),(&#39;技术革新&#39;, 0.5949685573577881)
</code></pre></div><h2 id="关于作者">关于作者</h2>
<p>老刘，刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。</p>
<p>就职于360人工智能研究院、曾就职于中国科学院软件研究所。</p>
<p><strong>老刘说NLP</strong>，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 |  Python文本分析包更新</title>
      <link>https://textdata.cn/blog/cntext_tutorial/</link>
      <pubDate>Mon, 09 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_tutorial/</guid>
      <description>扩展词典、情感分析、可阅读性，内置9种情感词典，涵盖中英文</description>
      <content:encoded><![CDATA[<p><a href="https://github.com/hidadeng/cntext"><img loading="lazy" src="https://img.shields.io/badge/cntext-%e4%b8%ad%e6%96%87%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%ba%93-orange?style=for-the-badge&amp;logo=appveyor" alt=""  />
</a></p>
<p><a href="version1.2.md">旧版cntext入口</a></p>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> <strong>stats</strong>  文本统计指标
<ul>
<li><input checked="" disabled="" type="checkbox"> 词频统计</li>
<li><input checked="" disabled="" type="checkbox"> 可读性</li>
<li><input checked="" disabled="" type="checkbox"> 内置pkl词典</li>
<li><input checked="" disabled="" type="checkbox"> <strong>情感分析</strong></li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>dictionary</strong> 构建词表(典)
<ul>
<li><input checked="" disabled="" type="checkbox"> Sopmi 互信息扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> W2Vmodels 词向量扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> Glove Glove词嵌入模型</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>similarity</strong>   文本相似度
<ul>
<li><input checked="" disabled="" type="checkbox"> cos相似度</li>
<li><input checked="" disabled="" type="checkbox"> jaccard相似度</li>
<li><input checked="" disabled="" type="checkbox"> 编辑距离相似度</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>mind.py</strong> 计算文本中的认知方向（态度、偏见）</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="cntext_examples.zip">click to download</a></p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext
</code></pre></div><br>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">mind</span>
    <span class="nx">dictionary</span>
    <span class="nx">similarity</span>
    <span class="nx">stats</span>
</code></pre></div><br>
<h2 id="一stats">一、stats</h2>
<p>目前stats内置的函数有</p>
<ul>
<li><strong>readability</strong>  文本可读性</li>
<li><strong>term_freq</strong> 词频统计函数</li>
<li><strong>dict_pkl_list</strong>  获取cntext内置词典列表(pkl格式)</li>
<li><strong>load_pkl_dict</strong> 导入pkl词典文件</li>
<li><strong>sentiment</strong> 情感分析</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="11--readability">1.1  readability</h3>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p>readability(text, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text: 文本字符串数据</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p><strong>中文可读性</strong> 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>


<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 28.0,
 &#39;readability2&#39;: 0.15789473684210525,
 &#39;readability3&#39;: 14.078947368421053}
</code></pre></div><br>
<p>句子中的符号变更会影响结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.16666666666666666,
 &#39;readability3&#39;: 13.583333333333334}
</code></pre></div><p><br><br></p>
<h3 id="12--term_freq">1.2  term_freq</h3>
<p>词频统计函数，返回Counter类型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="13-dict_pkl_list">1.3 dict_pkl_list</h3>
<p>获取cntext内置词典列表(pkl格式)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取cntext内置词典列表(pkl格式)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;, 
 &#39;concreteness.pkl&#39;]
</code></pre></div><p>词典对应关系, 部分情感词典资料整理自 <a href="https://github.com/quanteda/quanteda.sentiment">quanteda.sentiment</a></p>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>DUTIR.pkl</td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td>HOWNET.pkl</td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>sentiws.pkl</td>
<td>SentimentWortschatz (SentiWS)</td>
<td>英文</td>
<td>正面词、负面词；<br>效价</td>
</tr>
<tr>
<td>ChineseFinancialFormalUnformalSentiment.pkl</td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td>ANEW.pkl</td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>词语效价信息</td>
</tr>
<tr>
<td>LSD2015.pkl</td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>NRC.pkl</td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td>geninqposneg.pkl</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HuLiu.pkl</td>
<td>Hu&amp;Liu (2004)正、负情感词典</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>AFINN.pkl</td>
<td>尼尔森 (2011) 的“新 ANEW”效价词表</td>
<td>英文</td>
<td>情感效价信息valence</td>
</tr>
<tr>
<td>LoughranMcDonald.pkl</td>
<td>会计金融LM词典</td>
<td>英文</td>
<td>金融领域正、负面情感词</td>
</tr>
<tr>
<td>ADV_CONJ.pkl</td>
<td>副词连词</td>
<td>中文</td>
<td></td>
</tr>
<tr>
<td>STOPWORDS.pkl</td>
<td></td>
<td>中、英</td>
<td>停用词</td>
</tr>
<tr>
<td>concreteness.pkl</td>
<td>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</td>
<td>English</td>
<td>word &amp; concreateness score</td>
</tr>
</tbody>
</table>
<h3 id="注意">注意:</h3>
<ul>
<li>
<p>如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</li>
<li>
<p>如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue</p>
</li>
</ul>
<br>
<h3 id="14-load_pkl_dict">1.4 load_pkl_dict</h3>
<p>导入pkl词典文件，返回字典样式数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入pkl词典文件,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, ...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...], 
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;, &#39;刿目怵心&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;, &#39;喜糖&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,]
           }
</code></pre></div><br>
<h3 id="15-sentiment">1.5 sentiment</h3>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)
使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>diction:  情感词字典；</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">],</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p>如果不适用pkl词典，可以自定义自己的词典，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<h3 id="16-sentiment_by_valence">1.6 sentiment_by_valence</h3>
<p>sentiment函数默认所有情感词权重均为1，只需要统计文本中情感词的个数，即可得到文本情感得分。</p>
<p>sentiment_by_valence(text, diction, lang=&lsquo;english&rsquo;)函数考虑了词语的效价(valence)</p>
<ul>
<li>text 待输入文本</li>
<li>diction 带效价的词典，DataFrame格式。</li>
<li>lang 语言类型&rsquo;chinese' 或 &lsquo;english&rsquo;，默认&rsquo;english'</li>
</ul>
<p>这里我们以文本具体性度量为例， <strong>concreteness.pkl</strong> 整理自 Brysbaert2014的文章。</p>
<blockquote>
<p>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># load the concreteness.pkl dictionary file</span>
<span class="n">concreteness_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;concreteness.pkl&#39;</span><span class="p">)</span>
<span class="n">concreteness_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">word</th>
<th style="text-align:right">valence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">roadsweeper</td>
<td style="text-align:right">4.85</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">traindriver</td>
<td style="text-align:right">4.54</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">tush</td>
<td style="text-align:right">4.45</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">hairdress</td>
<td style="text-align:right">3.93</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">pharmaceutics</td>
<td style="text-align:right">3.77</td>
</tr>
</tbody>
</table>
<br>
<p>先看一条文本的具体性度量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">reply</span> <span class="o">=</span> <span class="s2">&#34;I&#39;ll go look for that&#34;</span>

<span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                              <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                              <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">score</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1.85
</code></pre></div><br>
<p>很多条文本的具体性度量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">employee_replys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I&#39;ll go look for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that top&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go look for that t-shirt in grey&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt in grey&#34;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">reply</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">employee_replys</span><span class="p">):</span>
    <span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                                  <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    
    <span class="n">template</span> <span class="o">=</span> <span class="s2">&#34;Concreteness Score: </span><span class="si">{score:.2f}</span><span class="s2"> | Example-</span><span class="si">{idx}</span><span class="s2">: </span><span class="si">{exmaple}</span><span class="s2">&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="n">score</span><span class="p">,</span> 
                          <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> 
                          <span class="n">exmaple</span><span class="o">=</span><span class="n">reply</span><span class="p">))</span>
    
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Concreteness Score: 1.55 | Example-0: I&#39;ll go look for that
Concreteness Score: 1.55 | Example-1: I&#39;ll go search for that
Concreteness Score: 1.89 | Example-2: I&#39;ll go search for that top
Concreteness Score: 2.04 | Example-3: I&#39;ll go search for that t-shirt
Concreteness Score: 2.37 | Example-4: I&#39;ll go look for that t-shirt in grey
Concreteness Score: 2.37 | Example-5: I&#39;ll go search for that t-shirt in grey
</code></pre></div><br>
<p><br><br></p>
<h2 id="二dictionary">二、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="21-sopmi">2.1 SoPmi</h3>
<p>SoPmi 共现法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                   <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                   <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                   <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span>
                   <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 44.49 s
</code></pre></div><br>
<h3 id="22-w2vmodels">2.2 W2VModels</h3>
<p>W2VModels 词向量</p>
<p><strong>特别要注意代码需要设定lang语言参数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   174 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s

</code></pre></div><br>
<h3 id="需要注意">需要注意</h3>
<p>训练出的w2v模型可以后续中使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">model路径</span><span class="p">)</span>
<span class="c1">#找出word的词向量</span>
<span class="c1">#w2v_model.get_vector(word)</span>
<span class="c1">#更多w2_model方法查看</span>
<span class="c1">#help(w2_model)</span>
</code></pre></div><p>例如本代码，运行生成的结果路径<code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;technology&#39;, 0.689210832118988),
 (&#39;infrastructure&#39;, 0.669672966003418),
 (&#39;resources&#39;, 0.6695448160171509),
 (&#39;talent&#39;, 0.6627111434936523),
 (&#39;execution&#39;, 0.6549549102783203),
 (&#39;marketing&#39;, 0.6533523797988892),
 (&#39;merchandising&#39;, 0.6504817008972168),
 (&#39;diversification&#39;, 0.6479553580284119),
 (&#39;expertise&#39;, 0.6446896195411682),
 (&#39;digital&#39;, 0.6326863765716553)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.45616838, -0.7799563 ,  0.56367606, -0.8570078 ,  0.600359  ,
       -0.6588043 ,  0.31116748, -0.11956959, -0.47599426,  0.21840936,
       -0.02268819,  0.1832016 ,  0.24452794,  0.01084935, -1.4213187 ,
        0.22840202,  0.46387577,  1.198386  , -0.621511  , -0.51598716,
        0.13352732,  0.04140598, -0.23470387,  0.6402956 ,  0.20394802,
        0.10799981,  0.24908689, -1.0117126 , -2.3168423 , -0.0402851 ,
        1.6886286 ,  0.5357047 ,  0.22932841, -0.6094084 ,  0.4515793 ,
       -0.5900931 ,  1.8684244 , -0.21056202,  0.29313338, -0.221067  ,
       -0.9535679 ,  0.07325   , -0.15823542,  1.1477109 ,  0.6716076 ,
       -1.0096023 ,  0.10605699,  1.4148282 ,  0.24576302,  0.5740349 ,
        0.19984631,  0.53964925,  0.41962907,  0.41497853, -1.0322098 ,
        0.01090925,  0.54345983,  0.806317  ,  0.31737605, -0.7965337 ,
        0.9282971 , -0.8775608 , -0.26852605, -0.06743863,  0.42815775,
       -0.11774074, -0.17956367,  0.88813037, -0.46279573, -1.0841943 ,
       -0.06798118,  0.4493006 ,  0.71962464, -0.02876493,  1.0282255 ,
       -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922,
       -0.16753489,  0.14065655, -1.8609751 ,  0.03587054,  1.2792674 ,
        1.2732009 , -0.74120265, -0.98000383,  0.4521185 , -0.26387128,
        0.37045383,  0.3680011 ,  0.7197629 , -0.3570571 ,  0.8016917 ,
        0.39243212, -0.5027844 , -1.2106236 ,  0.6412354 , -0.878307  ],
      dtype=float32)
</code></pre></div><p><br><br></p>
<h3 id="23-co_occurrence_matrix">2.3 co_occurrence_matrix</h3>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence2.png" alt=""  />
</p>
<p><br><br></p>
<h3 id="24--glove">2.4  Glove</h3>
<p>构建Glove词嵌入模型，使用英文数据<code>data/brown_corpus.txt</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Glove</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">create_vocab</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/brown_corpus.txt&#39;</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">cooccurrence_matrix</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_embeddings</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4: ...Create vocabulary for Glove.
Step 2/4: ...Create cooccurrence matrix.
Step 3/4: ...Train glove embeddings. 
             Note, this part takes a long time to run
Step 3/4: ... Finish! Use 175.98 s
</code></pre></div><p>生成的Glove词嵌入文件位于<code>output/Glove</code> 。</p>
<p><br><br></p>
<h2 id="三similarity">三、similarity</h2>
<p>四种相似度计算函数</p>
<ul>
<li>cosine_sim(text1, text2)  cos余弦相似</li>
<li>jaccard_sim(text1, text2)     jaccard相似</li>
<li>minedit_sim(text1, text2)  最小编辑距离相似度；</li>
<li>simple_sim(text1, text2) 更改变动算法</li>
</ul>
<p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩啊&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.82
0.67
2.00
0.87
</code></pre></div><p><br><br></p>
<h2 id="四text2mind">四、Text2Mind</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<p>- <strong>多个对象在某概念的远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
<p>- 多个对象在某<strong>概念的分量(fen，一声)的多少</strong>， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
<p>这两种认知分别可以用向量距离、向量语义投影计算得来。</p>
<ul>
<li>tm.sematic_distance(words, c_words1, c_words2)  向量距离</li>
<li>tm.sematic_projection(words, c_words1, c_words2)  向量语义投影</li>
</ul>
<h3 id="41-tmsematic_distancewords-c_words1-c_words2">4.1 tm.sematic_distance(words, c_words1, c_words2)</h3>
<p>分别计算words与c_words1、c_words2语义距离，返回距离差值。</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果d1-d2&lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<p><strong>下载glove_w2v.6B.100d.txt</strong>链接: <a href="https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw">https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw</a> 提取码: 72l0</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#Note: this is a word2vec format model</span>
<span class="n">tm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Text2Mind</span><span class="p">(</span><span class="n">w2v_model_path</span><span class="o">=</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="n">engineers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">mans</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">womans</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">engineers</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">mans</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">womans</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><p>-0.38 意味着工程师更接近于男人，而不是女人。</p>
<br>
<h3 id="42-tmsematic_projectionwords-c_words1-c_words2">4.2 tm.sematic_projection(words, c_words1, c_words2)</h3>
<p><strong>语义投影</strong>，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，word越位于c_words2一侧。</p>
<p>下图是语义投影示例图，本文算法和图片均来自 &ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&rdquo;</p>
<p><img loading="lazy" src="img/Nature_Semantic_projection_recovering_human_knowledge_of.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">smalls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">bigs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">smalls</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">bigs</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p>在这几个动物尺寸的感知上，人类觉得老鼠体型是最小，马的体型是最大。</p>
<p><br><br></p>
<h2 id="引用说明">引用说明</h2>
<p>如果研究中使用cntext，请使用以下格式进行引用</p>
<h3 id="apalike">apalike</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Deng X., Nan P. (2022). cntext: a Python tool for text mining (version 1.7.9). DOI: 10.5281/zenodo.7063523 URL: https://github.com/hiDaDeng/cntext
</code></pre></div><h3 id="bibtex">bibtex</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@misc{YourReferenceHere,
author = {Deng, Xudong and Nan, Peng},
doi = {10.5281/zenodo.7063523},
month = {9},
title = {cntext: a Python tool for text mining},
url = {https://github.com/hiDaDeng/cntext},
year = {2022}
}
</code></pre></div><h3 id="endnote">endnote</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">%0 Generic
%A Deng, Xudong
%A Nan, Peng
%D 2022
%K text mining
%K text analysi
%K social science
%K management science
%K semantic analysis
%R 10.5281/zenodo.7063523
%T cntext: a Python tool for text mining
%U https://github.com/hiDaDeng/cntext
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>sentence-transformer库 | 句子语义向量化</title>
      <link>https://textdata.cn/blog/sentence-transformer-tutorial/</link>
      <pubDate>Mon, 09 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/sentence-transformer-tutorial/</guid>
      <description>使用sentence-transformer库BERT技术，将句子语义向量化</description>
      <content:encoded><![CDATA[<blockquote>
<p>内容摘自</p>
<p>刘焕勇博客: <a href="https://liuhuanyong.github.io/">https://liuhuanyong.github.io/</a></p>
<p>原文地址: <a href="https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q">https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q</a></p>
<p>公众号: 老刘说NLP</p>
</blockquote>
<p>word2vec、glove是两种静态的词向量模型，即每个词语只有一个固定的向量表示。但在不同语境中，词语的语义会发生变化，按道理词向量也应该动态调整。相比word2vec、glove生成的静态词向量， BERT是一种动态的技术，可以根据上下文情景，得到语义变化的词向量。</p>
<p>HuggingFace网站提供了简易可用的数据集、丰富的预训练语言模型， 通过sentence-transformer库，我们可以使用HuggingFace内的预训练模型，得到不同情景的文本的语义向量。</p>
<p>HuggingFace网站  <a href="https://huggingface.co/">https://huggingface.co/</a></p>
<p><img loading="lazy" src="img/HuggingFace.png" alt=""  />
</p>
<br>
<h2 id="动态句向量">动态句向量</h2>
<p>sentence-transformer框架提供了一种简便的方法来计算句子和段落的向量表示（也称为句子嵌入）</p>
<p><img loading="lazy" src="img/sentence-transformer.png" alt=""  />
</p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">sentence</span><span class="o">-</span><span class="n">transformers</span>
</code></pre></div><br>
<h2 id="代码">代码</h2>
<p><a href="sentence-transformer-tutorial.zip">click to download the code</a></p>
<p>使用huggingface中的distiluse-base-multilingual-cased与训练模型，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">util</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;distiluse-base-multilingual-cased&#39;</span><span class="p">)</span>
</code></pre></div><p>第一次运行上方的代码，需要运行一定的时间用于下载。下载完成后，我们使用同种语义的中英文句子，分别计算得到emb1和emb2两个句向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">emb1 = model.encode(&#39;Natural language processing is a hard task for human&#39;)

emb2 = model.encode(&#39;自然语言处理对于人类来说是个困难的任务&#39;)
emb1
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 2.58186590e-02,  4.65703346e-02,  4.25276496e-02, -1.67875513e-02,
        5.56012690e-02, -3.44308838e-02, -6.53978735e-02,  1.77450478e-02,
       -3.47155109e-02,  2.86140274e-02,  2.48657260e-02,  7.94188876e-04,
        5.09755425e-02, -1.76107027e-02, -1.04308855e-02,  7.61642214e-03,
        ...
        4.28482369e-02,  1.76657233e-02, -5.83355911e-02,  1.92921527e-03,
        2.81221420e-02,  5.24400780e-03,  2.10703332e-02,  7.96715263e-03,
       -6.80630878e-02, -2.05304120e-02, -2.43293475e-02, -1.87458862e-02],
      dtype=float32)
</code></pre></div><p>在distiluse-base-multilingual-cased这种模型中， 不同语言的同义句应该具有类似的语义，那么cos相似度应该是很大的。越接近于1越相似；越接近于0，越不相似。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">cos_sim</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">pytorch_cos_sim</span><span class="p">(</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb2</span><span class="p">)</span>
<span class="n">cos_sim</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[0.8960]])
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 从符号到嵌入：计算社会科学的两种文本表示</title>
      <link>https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/</link>
      <pubDate>Mon, 25 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/from_sysbol_to_embeddings_in_computational_social_science/</guid>
      <description>如何有效地表示数据以挖掘我们想要的计算社会科学的含义？为了探索答案，我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示。How to efficiently represent data to mine the implications we want for computational social science? To explore the answer, we conduct a thorough review of data representations for text and the web in CSS, and we summarize existing representations into two schemes, symbol-based and embedding-based</description>
      <content:encoded><![CDATA[<p>B站看到大牛刘知远关于文本分析在计算社会科学领域应用的分享，解答了我对文本表示的疑惑，看完了能对文本的特征工程加深理解，同时也能更清晰未来如何借助计算机科学技术开展社会科学研究。</p>
<blockquote>
<p><strong>全文摘抄自</strong></p>
<p>Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.</p>
</blockquote>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1qi4y1Q7qj&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<h2 id="摘要">摘要</h2>
<p><strong>计算社会科学</strong>（CSS），旨在利用计算方法来解决社会科学问题，是一个新兴和快速发展的领域。 CSS 的研究是数据驱动的，并且显着受益于在线用户生成内容和社交网络的可用性，其中包含用于调查的富文本和网络数据。然而，这些大规模、多模态的数据也给研究人员带来了很大的挑战：<strong>如何有效地表示数据以挖掘我们想要的 CSS 含义</strong>？为了探索答案，<strong>我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示</strong>，并为每个方案介绍了一系列典型的方法。随后，我们基于对来自 6 个涉及 CSS 的顶级场所的 400 多篇研究文章的调查，展示了上述表示的应用。从这些应用程序的统计数据中，<strong>我们挖掘出每种表示的强度，并发现基于嵌入的表示在过去十年中出现并获得越来越多的关注的趋势</strong>。最后，我们讨论了几个关键挑战和未来方向的开放性问题。本调查旨在为 CSS 研究人员提供对数据表示的更深入理解和更明智的应用。</p>
<p><strong>关键词</strong>：计算社会科学；基于符号的表示；基于嵌入的表示；社交网络</p>
<br>
<h2 id="一计算社会学数据分析流程">一、计算社会学数据分析流程</h2>
<p>其中第二步，数据表示目前有两大类表示(特征工程)方法</p>
<ul>
<li><strong>基于符号的文本表示</strong>；符号可以是单词(或词组)，也可以是概念(如正面情感、负面情感)</li>
<li><strong>基于嵌入(分布式)的文本表示</strong>；相比于符号法，将词(词组)看做一个点。嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(king)-v(queen)约等于v(man)-v(woman)</li>
</ul>
<p><img loading="lazy" src="img/fig1.png" alt=""  />
</p>
<br>
<h2 id="二基于符号的文本表示">二、基于符号的文本表示</h2>
<p>基于符号的文本表示一般来说默认词语是不可分的符号，每个词能根据词频统计出现次数的多与少，或是否存在。</p>
<h3 id="21-词语层面">2.1 词语层面</h3>
<ul>
<li>
<p>基于词频表示</p>
<ul>
<li>是否出现，出现标位1，反之标位0。</li>
<li>出现多少，词语出现几次，标为几个。</li>
</ul>
</li>
<li>
<p>基于特征表示，如每个词带有权重(得分)</p>
</li>
<li>
<p>基于网络表示，如词语共现网络(矩阵)</p>
</li>
</ul>
<h3 id="22-句子层面">2.2 句子层面</h3>
<ul>
<li>
<p>基于词频的表示</p>
<ul>
<li>one-hot 将文本转为向量，向量中每个数，词语出现标位1，反之标位0</li>
<li>bag-of-words，将文本转为向量，向量中每个数，词语出现n次标记为n</li>
<li>n-grams，对词组的处理，将词组看做一个单词(整体)。</li>
<li>Tf-Idf ,该算法分为tf和idf两部分。其中tf与bag-of-words类似，考虑词语出现次数。而idf还考虑词语在语料中出现场景的稀缺性程度。</li>
</ul>
</li>
<li>
<p>基于语法特征，如句法依存关系，类似于英语语法，将句子分为主谓宾、动词、名词等。</p>
</li>
<li>
<p>词典法，如使用正、负情感词典，对文本数据进行情感分析，可以得到pos和neg的各自得分</p>
</li>
</ul>
<p><img loading="lazy" src="img/fig2.png" alt=""  />
</p>
<br>
<h2 id="三基于嵌入的文本表示">三、基于嵌入的文本表示</h2>
<h3 id="31词语层面">3.1词语层面</h3>
<p>嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(best)-v(good)约等于v(worst)-v(bad)</p>
<h3 id="32-句子层面">3.2 句子层面</h3>
<p>词语是向量，那么由词语组成的句子也会加权得到一个向量。含有相似话题或含义相近的句子在多维向量空间中会比较接近。</p>
<p><img loading="lazy" src="img/fig7.png" alt=""  />
</p>
<br>
<h2 id="四任务分类文本的用法">四、任务分类：文本的用法</h2>
<p><img loading="lazy" src="img/fig16.png" alt=""  />
</p>
<p>有了文本数据，刚刚解决了如何表示文本。接下来，需要明确，我们使用文本目的是为了做哪类分析，得到哪些信息。有8种常见的文本分析图式</p>
<ul>
<li>描述性。如随时间推移，词频的发展趋势是变大的</li>
<li>相关性。</li>
<li>聚类。如lda话题分析、k-means聚类</li>
<li>相似度。两个文档转为向量后，可以通过cosine计算相似度</li>
<li>分类。机器学习分类，判断某文本隶属于哪个类别</li>
<li>回归。例如根据文本，判断某件事发生的概率</li>
<li>语言模型。</li>
<li>排序。</li>
</ul>
<br>
<h2 id="五发文趋势-符号vs嵌入">五、发文趋势-符号vs嵌入</h2>
<p>基于上一节中对应用程序的介绍，可以观察到基于符号和基于嵌入的表示在 <strong>计算社会科学</strong>中都得到了相当大的采用。为了明确研究它们的覆盖范围，我们计算了每年使用两种表示中的一种或两种的作品数量，如图 17 所示。通过比较nature、science、pnas三大顶级期刊，我们可以发现使用<strong>基于嵌入表示</strong>的文章比例在过去几年中逐渐。这表明越来越多的 计算社会科学文章 已经考虑并受益于基于嵌入表示。</p>
<p>图 18 显示了在 计算机领域ACL、WWW 和 KDD 的会议上中，发现使用基于嵌入的表示的文章数量已大大超过使用基于符号的表示的文章数量。然而，与图 17 相比，计算机科学会议中基于嵌入的表示的数量与三个多学科期刊之间存在很大差距。</p>
<p><img loading="lazy" src="img/3_top_journals.png" alt=""  />
</p>
<p><img loading="lazy" src="img/nlp.png" alt=""  />
</p>
<p>总而言之，在过去十年中，基于嵌入的表示已经出现并在 计算社会科学 中发挥着越来越重要的作用。</p>
<br>
<h2 id="六趋势解读">六、趋势解读</h2>
<p>基于它们的内部机制和现有应用，对趋势解读，我们总结出以下三个关键点。</p>
<p>基于符号的表示因其明确性和可解释性而擅长描述和关系的任务。</p>
<p>基于符号的表示中的每个值都表示一定的人类可读的含义，因此我们可以直接使用它来观察数据的分布，以及提取对象之间的关系。例如，基于频率的词表示用于观察文化变化并捕捉新闻中提及次数与公司股票交易量之间的关系。虽然基于主题模型的表示和一些基于神经的表示在一定程度上具有实际意义，但它们对于社会科学研究人员来说仍然是模糊的并且不那么引人注目。</p>
<p>由于神经网络具有强大的拟合数据和提取深度语义的能力，基于嵌入的表示在预测（例如分类和回归）和相似性任务中表现更好。一方面，神经网络通过大规模神经元的连接实现高效的输入输出映射功能。另一方面，通过多层网络的构建，实现深层语义和抽象概念的提取。现有研究表明，深层捕获相对于浅层更抽象的特征。诸如社会偏见和道德化之类的抽象概念都可以通过基于嵌入的表示来很好地衡量。虽然我们提到基于符号的表示可以通过一些定义的符号来代表抽象概念，但这种表示仍然是部分和肤浅的，很难捕捉到它们的全貌。</p>
<p>基于嵌入的表示需要更少的人力。基于符号的表示通常需要大量的专家知识来定义研究对象的特征，这是劳动密集型的。此外，对于一些没有充分特征的抽象概念或对象，它们的表现将受到限制。与它们不同的是，基于嵌入的表示是从数据中自动提取的，几乎不需要人工干预，甚至可以补充人类知识。例如，可以使用神经网络来自动恢复丢失的巴比伦文本，这即使对专家来说也是具有挑战性的。此外，基于嵌入的表示可以在没有手动定义的情况下描述语言的复杂性和歧义性。</p>
<br> 
<h2 id="七未来展望">七、未来展望</h2>
<p>尽管在过去十年中出现了从符号到嵌入的趋势，但仍有许多挑战和悬而未决的问题有待探索。展望未来，我们列出了一些与计算社会科学 中的数据表示相关的基本和潜在的未来方向。</p>
<p>预训练的语言模型。近年来，预训练的语言模型受到了相当大的关注，并在处理文本数据方面取得了巨大的成功 [100, 240]。这些模型从百科全书和书籍等海量文本数据中学习丰富的语义信息，仅在下游任务中进行微调以实现有效的基于嵌入的表示。因此，对于 计算社会科学，我们可以借助预训练的语言模型获得更通用、更健壮的文本表示。与从传统神经网络模型中学习的表示相比，这些表示不仅可以更广泛、更准确地从文本中分析社会现象，而且还可以减少那些需要大量标记数据的任务的人工注释。</p>
<p>图神经网络。通过消息传递机制，图神经网络 [461] 可以同时有效地对网络拓扑和节点/边缘特征（例如文本信息）进行建模，从而提供一个统一的框架来利用来自异构来源的信息。 计算社会科学 中的许多场景需要处理社交网络以及个人特征。因此，图神经网络技术在 计算社会科学 研究中具有很大的应用潜力，可以学习融合文本和网络信息的表示。事实上，计算机科学中的各种应用，例如自然语言处理 [418] 和推荐系统 [439]，已经采用图神经网络进行建模。</p>
<p>设计为预测和相似性。基于嵌入的表示以丰富和深层次的语义而闻名，而基于符号的表示通常保留在部分和浅层语义中。同时，基于嵌入的表示擅长预测和相似性的任务。因此，为了充分利用嵌入中的强语义，鼓励 计算社会科学 研究人员尽可能将研究问题设计为预测或相似性任务。例如，我们可以将社会偏见问题设计为性别词和中性词嵌入之间的相似性度量 [59, 133]。此外，人类语言的复杂性可以设计为一项预测任务，它以语言模型为指标查看单词或句子的预测概率[155]。</p>
<p>可解释性。诚然，基于嵌入的方法的一个缺点是缺乏可解释性。这个问题会损害与道德、安全或隐私相关的决策关键系统的应用。尽管嵌入模型，尤其是神经网络模型的可解释性尚未完全解决，但计算机科学领域的研究人员已经做出了一些努力，以提高基于神经模型的可解释性 [16]。因此，利用基于嵌入的模型和可解释性分析方法进行有效和（部分）可解释的预测将是一个有趣的方向。</p>
<br>
<h2 id="结论">结论</h2>
<p>计算社会科学作为一个新兴且有前途的跨学科领域，近年来吸引了相当多的研究兴趣。 计算社会科学 研究中广泛使用两种主要类型的数据，即文本数据和网络数据。在本次调查中，我们首先将数据表示总结为基于符号和基于嵌入的表示，并在构建这些表示时进一步介绍典型的方法。之后，我们基于来自 6 个经典期刊和会议的 400 多篇高被引文献，对这两类表示的应用进行了全面回顾。根据对这些应用的统计，发现了 计算社会科学 中基于嵌入的文本和网络表示正在出现和增长的趋势，我们进一步讨论了其中的原因。最后，我们提出了 计算社会科学 中的四个挑战和未解决的问题，它们是需要探索的基本和潜在方向。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>中文词向量资源汇总 &amp; 使用方法</title>
      <link>https://textdata.cn/blog/embeddings_resource_usage_method/</link>
      <pubDate>Thu, 21 Apr 2022 15:40:10 +0600</pubDate>
      
      <guid>/blog/embeddings_resource_usage_method/</guid>
      <description>数十种中文词向量模型资源下载&amp;amp;使用方法。Dozens of Chinese word vector model resource downloads &amp;amp; usage methods</description>
      <content:encoded><![CDATA[<br>
<h2 id="项目地址">项目地址</h2>
<p><a href="https://github.com/Embedding/Chinese-Word-Vectors">https://github.com/Embedding/Chinese-Word-Vectors</a></p>
<p>Chinese-Word-Vectors项目提供超过100种中文词向量，其中包括不同的表示方式（稠密SGNS和稀疏PPMI）、不同的上下文特征（词、N元组、字等等）、以及不同的训练语料。获取预训练词向量非常方便，下载后即可用于下游任务。</p>
<br>
<h2 id="参考文献">参考文献</h2>
<p>如果使用了本项目的词向量和CA8数据集请进行如下引用：</p>
<p>Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, <a href="http://aclweb.org/anthology/P18-2023"><em>Analogical Reasoning on Chinese Morphological and Semantic Relations</em></a>, ACL 2018.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@InProceedings{P18-2023,
  author =  &#34;Li, Shen
    and Zhao, Zhe
    and Hu, Renfen
    and Li, Wensi
    and Liu, Tao
    and Du, Xiaoyong&#34;,
  title =   &#34;Analogical Reasoning on Chinese Morphological and Semantic Relations&#34;,
  booktitle =   &#34;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&#34;,
  year =  &#34;2018&#34;,
  publisher =   &#34;Association for Computational Linguistics&#34;,
  pages =   &#34;138--143&#34;,
  location =  &#34;Melbourne, Australia&#34;,
  url =   &#34;http://aclweb.org/anthology/P18-2023&#34;
}
</code></pre></div><br>
<h3 id="不同领域">不同领域</h3>
<p>下列词向量基于不同的表示方式、不同的上下文特征以及不同领域的语料训练而成。</p>
<table align="center">
    <tr align="center">
        <td colspan="5"><b>Word2vec / Skip-Gram with Negative Sampling (SGNS)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1IiIbQGJ_AooTj5s8aZYcvA">300d</a> / PWD: 5555</td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1AmXYWVgkxrG4GokevPtNgA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZKePwxwsDdzNrfkc6WKdGQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZBVVD4mUSUuXOxlZ3V71ZA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19wQrclyynOnco3JBvnI5pA">300d</td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/19sqMz-JAhhxh3o6ecvQxQw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1upPkA8KJnxTZBfjuNDtaeQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1BvKk2QjbtQMch7EISppW2A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19Vso_k79FZb5OZCWQPAnFQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1tUghuTno5yOvOx4LXA9-wg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13yVrXeGYkxdGW3P6juiQmA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1pUqyn7mnPcUmzxT64gGpSw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1svFOwFBKnnlsqrF1t99Lnw">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/1EhtsbDa3ekzZPODWNLHcXA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FcPHv7S4vUgnL7WeWf4_PA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13CAxY5ffRFuOcHZu8VmArw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sqvrUtGBAZ7YWEsGz41DRQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VGOs0RH7DXE5vRrtw6boQA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1OQ6fQLCgqT43WTwh5fh_lg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1_xogqF9kJT6tmQHSAYrYeg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Fo27Lv_0nz8FXg-xbOz14Q">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/1zbuUJEEEpZRNHxZ7Gezzmw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/11PWBcvruXEDvKf2TiIXntg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/10bhJpaXMCUK02nHvRAttqA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FHl_bQkYucvVk-j2KG4dxA">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1ciq8iXtcrHpu3ir_VhK0zg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Oa4CkPd8o2xd6LEAaa4gmg">300d</a> / PWD: z5b4</td>
      <td><a href="https://pan.baidu.com/s/1IG8IxNp2s7vVklz-vyZR9A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1SEOKrJYS14HpqIaQT462kA">300d</a> / PWD: yenb</td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1vPSeUsSiWYXEWAuokLR0qQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sS9E7sclvS_UZcBgHN7xLQ">300d</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合<br>Baidu Netdisk / Google Drive</td>
      <td>
        <a href="https://pan.baidu.com/s/1luy-GlTdqqvJ3j-A4FcIOw">300d</a><br>
        <a href="https://drive.google.com/open?id=1Zh9ZCEu8_eSQ-qkYVQufQDNKPC4mtEKR">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1oJol-GaRMk4-8Ejpzxo6Gw">300d</a><br>
        <a href="https://drive.google.com/open?id=1WUU9LnoAjs--1E_WqcghLJ-Pp8bb38oS">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1DjIGENlhRbsVyHW-caRePg">300d</a><br>
        <a href="https://drive.google.com/open?id=1aVAK0Z2E5DkdIH6-JHbiWSL5dbAcz6c3">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/14JP1gD7hcmsWdSpTvA3vKA">300d</a><br>
        <a href="https://drive.google.com/open?id=1kSAl4_AOg3_6ayU7KRM0Nk66uGdSZdnk">300d</a>
      </td>
    </tr>
</table>
<table align="center">
    <tr align="center">
        <td colspan="5"><b>Positive Pointwise Mutual Information (PPMI)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1_itcjrQawCwcURa7WZLPOA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1cEZzN1S2senwWSyHOnL7YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1KcfFdyO0-kE9S9CwzIisfw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FXYM3CY161_4QMgiH8vasQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1MGXRrc54nITPzQ7sfEUjMA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1mtxZna8UJ7xBIxhBFntumQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dDImpAx41V73Byl2julOGA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1bsBQHXFpxMHGBexYof1_rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/1NLr1K7aapU2sYBvzbVny5g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1LJl3Br0ccGDHP0XX2k3pVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1GQQXGMn1AHh-BlifT0JD2g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1Xm9Ec3O3rJ6ayrwVwonC7g">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1ECA51CZLp9_JB_me7YZ9-Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FO39ZYy1mStERf_b53Y_yQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1lLBFBk8nn3spFAvKY9IJ6A">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1f-dLQZlZo_-B5ZKcPIc6rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/10wtgdmrTsTrjpSDvI0KzOw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b6zjvhOIqTdACSSbriisVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1w24vCfgqcoJvPxsB5VrRvw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b9BPiDRhiEZ-6ybTcovrqQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VaUP3YJC0IZKTbJ-1_8HZg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1g39PKwT0kSmpneKOgXR5YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1d8Bsuak0fyXxQOVUiNr-2w">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1D5fteBX0Vy4czEqpxXjlrQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/15O2EbToOzjNSkzJwAOk_Ug">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/11Dqywn0hfMhysto7bZS1Dw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1wY-7mfV6nwDj_tru6W9h4Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1DMW-MgLApbQnWwDd-pT_qw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1HTHhlr8zvzhTwed7dO0sDg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1jAuGJBxKqgapt__urGsBOQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/173AJfCoAV0ZA8Z31tKBdTA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dFCxke_Su3lLsuwZr7co3A">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1NJ1Gc99oE0-GV0QxBqy-qw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1YGEgyXIbw0O4NtoM1ohjdA">Sparse</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
    </tr>
</table>
<p><sup>*</sup>由于古汉语中绝大部份词均为单字词，因此只需字向量。</p>
<br>
<h2 id="语料">语料</h2>
<p>项目花费了大量精力来收集了来自多个领域的语料。所有的文本数据均移除了html和xml标记，仅保留了纯文本。之后采用了<a href="https://github.com/hankcs/HanLP">HanLP(v_1.5.3)</a>对文本进行了分词。此外，我们将繁体中文用<a href="https://github.com/BYVoid/OpenCC">Open Chinese Convert (OpenCC)</a>转换为了简体中文。更详细的语料信息如下所示：</p>
<table align="center">
	<tr align="center">
		<td><b>语料</b></td>
		<td><b>大小</b></td>
		<td><b>词数量</b></td>
		<td><b>词汇量</b></td>
		<td><b>详情</b></td>
	</tr>
	<tr align="center">
		<td>Baidu Encyclopedia<br />百度百科</td>
		<td>4.1G</td>
		<td>745M</td>
		<td>5422K</td>
		<td>中文百科<br />https://baike.baidu.com/</td>
	</tr>
	<tr align="center">
		<td>Wikipedia_zh<br />中文维基百科</td>
		<td>1.3G</td>
		<td>223M</td>
		<td>2129K</td>
		<td>中文维基百科<br />https://dumps.wikimedia.org/</td>
	</tr>
	<tr align="center">
		<td>People's Daily News<br />人民日报</td>
		<td>3.9G</td>
		<td>668M</td>
		<td>1664K</td>
		<td>人民日报新闻数据(1946-2017)<br />http://data.people.com.cn/</td>
	</tr>
	<tr align="center">
		<td>Sogou News<br />搜狗新闻</td>
		<td>3.7G</td>
		<td>649M</td>
		<td>1226K</td>
		<td>Sogou labs的新闻数据<br />http://www.sogou.com/labs/</td>
	</tr>
  <tr align="center">
    <td>Financial News<br />金融新闻</td>
    <td>6.2G</td>
    <td>1055M</td>
    <td>2785K</td>
    <td>从多个网站收集到的金融新闻</td>
  </tr>
	<tr align="center">
		<td>Zhihu_QA<br />知乎问答</td>
		<td>2.1G</td>
		<td>384M</td>
		<td>1117K</td>
		<td>中文问答数据<br />https://www.zhihu.com/</td>
	</tr>
	<tr align="center">
		<td>Weibo<br />微博</td>
		<td>0.73G</td>
		<td>136M</td>
		<td>850K</td>
		<td>NLPIR Lab提供的微博数据<br />http://www.nlpir.org/wordpress/download/weibo.7z</td>
	</tr>
	<tr align="center">
		<td>Literature<br />文学作品</td>
		<td>0.93G</td>
		<td>177M</td>
		<td>702K</td>
		<td>8599篇现代文学作品</td>
	</tr>
	<tr align="center">
		<td>Mixed-large<br />综合</td>
		<td>22.6G</td>
    <td>4037M</td>
    <td>10653K</td>
		<td>上述所有数据的汇总</td>
	</tr>
  <tr align="center">
    <td>Complete Library in Four Sections<br />四库全书</td>
    <td>1.5G</td>
    <td>714M</td>
    <td>21.8K</td>
    <td>目前最大的古代文献汇总</td>
  </tr>
</table>
上述统计结果中，所有词都被计算在内，包括低频词。
<br>
<h2 id="导入模型代码">导入模型(代码)</h2>
<p>例如我下载了多个词模型，下载得到bz2结尾的文件名，例如<code>sgns.financial.bigram.bz2</code>。</p>
<p><img loading="lazy" src="models.png" alt=""  />
</p>
<p>使用方式</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1">#以金融sgns.financial.bigram.bz2为例</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.financial.bigram.bz2&#39;</span><span class="p">,</span> 
                                          <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                          <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>


<span class="n">model</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x7fe7fad79d60&gt;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;投资&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.084635,  0.890228, -0.23223 , -0.308985,  0.058241,  0.458777,
       -0.152547, -0.413471,  0.269701, -0.078043, -0.4155  ,  0.074735,
        0.35714 ,  0.103431,  0.601784, -0.390854,  0.814801, -0.122664,
       -1.076744,  0.516941, -0.293319, -0.310251, -0.407794,  0.003898,
       -0.210962,  0.378095, -0.345955, -0.223848,  0.700162,  0.207644,
        0.426249, -0.272832, -0.110305, -0.701062, -0.173407, -0.172121,
       -0.682592,  0.593414,  0.279591, -0.408284, -0.166693,  0.753402,
        0.037375,  0.141865, -0.246024, -0.108663, -0.225255, -0.856601,
        0.381026,  0.401248,  0.012108, -0.126305, -0.374255,  0.728795,
        0.219549, -0.354029, -0.353131,  0.064867,  0.49565 , -0.503267,
       -0.304075,  0.145036,  0.688948,  0.063382, -0.223243,  0.474251,
        0.80543 ,  0.683178,  0.118159,  0.408411, -0.020066,  0.009045,
       -0.135446, -0.069633,  0.206357,  0.482845, -0.075307,  0.06433 ,
       -0.112367,  0.011816,  0.87427 , -0.120287, -0.31036 ,  0.369985,
        0.560386, -0.215248,  0.389631,  0.042943, -0.319149,  0.951551,
       -0.335188,  0.642246, -0.55546 ,  0.322397,  0.659618, -0.213124,
        0.346696, -0.342239,  0.31479 ,  0.078533, -0.345148,  0.815577,
       -0.530134,  0.303419, -0.158916, -0.190564,  0.436046, -0.112251,
       -0.339966,  0.253645,  0.181076,  0.122875, -0.310951, -0.126253,
        1.641405,  0.357906,  0.165796,  0.398656, -0.330591,  0.20328 ,
       -0.077191, -0.421248, -0.078504, -0.734519,  0.146212,  0.535727,
        0.014134,  0.040322, -0.44809 , -0.758205, -0.151237,  0.248258,
       -0.319704,  0.656033, -0.518857,  0.932356, -1.01786 , -0.46354 ,
        0.160921, -0.243597,  0.106666, -0.03404 ,  0.010672,  0.260243,
        0.899813,  0.171735, -0.108209, -0.009843, -0.18113 ,  0.302494,
        0.187285,  0.064669, -0.502041, -0.724377, -0.294312, -0.522256,
        0.334543,  0.740455, -0.357653,  0.540747,  0.256146,  0.513839,
        1.116628, -0.626111,  0.505574,  0.089774, -0.381137, -0.282352,
       -0.457542,  0.198909,  0.313638,  0.560809,  0.25295 ,  0.878158,
       -0.289311, -0.629047,  0.011103,  0.041058, -0.291302, -0.014001,
       -0.027697, -0.445817, -0.070086,  0.159816, -0.120071,  1.280489,
       -0.108866,  0.01586 , -0.505574, -0.679772, -0.343165,  0.595633,
        0.438108, -0.364066, -0.393667,  0.442285,  0.24979 , -0.191607,
        0.425692,  0.535577, -0.480332, -0.737461,  0.588498, -0.380264,
        0.151292,  0.077519, -0.221384,  0.699436,  0.401642,  0.509026,
       -0.411141,  0.206719, -0.097051, -0.451834, -0.825617,  0.602984,
        0.2853  ,  0.46055 ,  0.96472 ,  0.322712, -0.373446,  0.207944,
        0.236688,  0.566523,  0.037644,  1.241091,  0.025682,  0.373211,
        0.097712, -0.195355,  0.264579, -0.072992, -0.121629,  0.041688,
        0.213666,  0.329652, -0.015182,  0.396307,  0.117955,  0.119577,
       -0.334761, -0.135917,  0.409983,  0.512367, -0.292204,  0.302897,
       -0.325733,  0.383173, -0.92419 , -0.377535, -0.059801, -0.606275,
       -0.240482,  0.054021, -0.581386, -0.555691,  0.158354,  0.103765,
        0.107681,  0.248877, -0.597925,  0.193332,  0.844085,  0.00584 ,
        0.041622, -0.111235,  0.617778,  0.234883, -0.09562 ,  0.408324,
       -0.107121,  0.717875,  0.674794,  0.127214, -0.178357,  0.331436,
        0.417898, -0.650833, -0.428309, -0.576132,  0.210533, -0.057879,
       -0.578397,  0.468586,  0.103365, -0.403216, -0.398776,  0.094514,
       -0.130387,  0.628187, -0.463082, -0.951649,  0.561544,  0.118903,
        0.448327, -0.171685, -0.672348,  0.069471,  0.556452, -0.335425],
      dtype=float32)
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">model.similar_by_key(&#39;投资&#39;)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;长期投资&#39;, 0.5135656595230103),
 (&#39;投资规模&#39;, 0.5089880228042603),
 (&#39;智百扬&#39;, 0.49565914273262024),
 (&#39;投资总额&#39;, 0.4955061078071594),
 (&#39;洛辉&#39;, 0.489188551902771),
 (&#39;337409&#39;, 0.48917514085769653),
 (&#39;洛盛&#39;, 0.4819018244743347),
 (&#39;洛腾&#39;, 0.4728960692882538),
 (&#39;394150&#39;, 0.4704836308956146),
 (&#39;投资额&#39;, 0.4685181975364685)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">similar_by_key</span><span class="p">(</span><span class="s1">&#39;风险&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;提示&#39;, 0.6549968123435974),
 (&#39;经营风险&#39;, 0.6316577792167664),
 (&#39;景气衰退&#39;, 0.544153094291687),
 (&#39;风险分析&#39;, 0.5439289212226868),
 (&#39;遇宏观&#39;, 0.5435716509819031),
 (&#39;信用风险&#39;, 0.5345730185508728),
 (&#39;承受能力&#39;, 0.5291797518730164),
 (&#39;防范&#39;, 0.5271924138069153),
 (&#39;系统性&#39;, 0.5178108811378479),
 (&#39;不确定性&#39;, 0.5173759460449219)]
</code></pre></div><p>向量运行效果还行，感兴趣的同学也可以根据自己的数据训练word2vec模型，训练及使用的办法参照文章</p>
<p><a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>豆瓣影评 | 探索词向量妙处</title>
      <link>https://textdata.cn/blog/douban_w2v/</link>
      <pubDate>Thu, 21 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/douban_w2v/</guid>
      <description>使用cntext训练、使用词向量。</description>
      <content:encoded><![CDATA[<p>本文要点</p>
<ul>
<li>读取csv</li>
<li>cntext训练词向量模型</li>
<li>cntext扩展pos、neg词典</li>
<li>导入词向量模型</li>
<li>运用词向量模型</li>
</ul>
<br>
<br>
<h2 id="代码下载">代码下载</h2>
<p>链接: <a href="https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg">https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg</a> 提取码: og9t</p>
<p><br><br></p>
<h2 id="一读取数据">一、读取数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;douban.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;电影  : </span><span class="si">{}</span><span class="s2"> 部&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Movie_Name_CN</span><span class="o">.</span><span class="n">nunique</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;评论  : </span><span class="si">{}</span><span class="s2"> 条&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)))</span>
</code></pre></div><pre><code>电影  : 28 部
评论  : 2125056 条
</code></pre>
<p><br><br></p>
<h2 id="二训练模型">二、训练模型</h2>
<p>使用 <a href="https://textdata.cn/blog/cntext_simplification/"><em><strong>cntext</strong></em></a> 库(版本号1.9， 免费公开版)训练词向量word2vec模型,这里我把csv数据整理为txt</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext</span> <span class="kn">import</span> <span class="n">W2VModels</span>
<span class="c1">#cntext版本号1.9</span>
<span class="c1">#pip install cntext==1.9</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#训练word2vec模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>  <span class="c1">#语料数据</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;douban.txt&#39;</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...预处理    语料 ...
Step 2/4:...训练   word2vec模型
            耗时   2001 s
        
</code></pre></div><p>cntext 可以用于扩展词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;pos.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;neg.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 3/4:...准备 每个seed在word2vec模型中的相似候选词...
Step 4/4 完成! 耗时 2302 s
Step 3/4:...准备 每个seed在word2vec模型中的相似候选词...
Step 4/4 完成! 耗时 2303 s
</code></pre></div><p>在代码所在文件夹内可以找到</p>
<ul>
<li>output/w2v_candi_words/w2v.model</li>
<li>新的  pos.txt</li>
<li>新的  neg.txt</li>
</ul>
<p>新的 <em><strong>pos.txt</strong></em> 是对 <em><strong>pos.txt</strong></em> 词典的扩展。</p>
<br>
<br>
<h2 id="三导入w2v模型">三、导入w2v模型</h2>
<p>有的时候数据量特别大，模型训练十分不易。</p>
<p>这时，保存已训练好的模型，不止下次不用再同样的数据再次训练，也可分享给其他人使用。</p>
<p>训练结束后，在代码所在文件夹内可以找到 <code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span>
</code></pre></div><pre><code>&lt;gensim.models.keyedvectors.KeyedVectors at 0x7face0574880&gt;
</code></pre>
<p><em><strong>w2v_models</strong></em> 数据类型为 <em><strong>KeyedVectors</strong></em> ， 在本文中使用 <em><strong>w2v_models</strong></em> 代指 <em><strong>KeyedVectors</strong></em></p>
<p><br><br></p>
<h2 id="四玩转词向量">四、玩转词向量</h2>
<p>用户级的数据(如在线评论)感觉生成的向量会准一些，<strong>词向量的方向，近义反义在向量中都有体现</strong>。</p>
<p><img loading="lazy" src="man-woman.png" alt=""  />
</p>
<p>例如本文使用的是28部电影的2125056条影评， 一般评论内容包含电影相关信息，如电影题材、是否值的观影等。</p>
<p>而在我们训练出模型w2v_models存在一些常用的方法</p>
<ul>
<li><em><strong>w2v_model.get_vector(key)</strong></em> 获取key的词向量</li>
<li><em><strong>w2v_model.most_similar_to_given(key1, keys_list)</strong></em>  从 keys_list 中获取与 key1 最相似的词</li>
<li><em><strong>w2v_model.n_similarity(ws1, ws2)</strong></em> 两组词 <em><strong>ws1</strong></em>,  <em><strong>ws2</strong></em>  的相似度</li>
<li><em><strong>w2v_model.closer_than(key1, key2)</strong></em> 更接近于 <em><strong>key1</strong></em> 的词向量(相比于 <em><strong>key2</strong></em> )</li>
<li><em><strong>w2v_model.most_similar(positive, negative)</strong></em> 找出与 <em><strong>positive</strong></em> 同方向，与 <em><strong>negative</strong></em> 反向相反的词。</li>
</ul>
<h3 id="41-get_vectorkey">4.1 get_vector(key)</h3>
<p><em><strong>w2v_model.get_vector(key)</strong></em>  获取key的词向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取某词语的向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>array([ 0.06488553,  0.74188954,  0.25468495,  0.89755714,  1.8139195 ,
       -0.6950082 ,  0.24339403, -1.2188634 ,  0.543618  , -0.9988698 ,
        0.27471313,  0.9325699 , -0.5860608 , -0.5081917 ,  1.6423215 ,
       -0.0490295 , -0.3927043 ,  0.659067  ,  0.03185922, -1.021391  ,
       -1.3214804 , -0.28208104, -0.7819419 , -0.30637202, -1.5944146 ,
       -0.12383854, -0.70463836,  0.45689437,  1.223081  , -1.9453759 ,
       -0.5538997 , -0.9750523 , -0.10031194, -0.9568689 ,  0.30341247,
        1.1102395 ,  0.667315  , -1.1600997 , -0.26674765, -0.55144155,
       -0.3246094 ,  0.82902473, -0.47339582, -0.9009957 ,  1.7722464 ,
        0.28959563, -0.03453476,  0.4786787 , -0.48074463, -0.23090109,
       -0.49390873,  0.71246386,  2.1557336 ,  2.4899387 , -0.51481706,
        0.5579966 , -0.6973235 , -1.1408254 ,  0.72495663, -1.0326954 ,
       -0.5455598 ,  0.98941576, -1.2155218 , -0.9088408 ,  1.9184568 ,
       -0.21800426, -1.2009395 ,  0.29684314,  1.3672423 , -2.269391  ,
        0.6188098 , -0.02714545, -0.44811317,  1.4397241 , -1.0594722 ,
       -0.08088647, -0.13015983, -0.99255013,  0.62044877,  2.5046496 ,
        0.4054545 , -0.38767585, -0.6956541 ,  0.22991426,  0.5928579 ,
       -0.12684819, -0.17408212,  0.25033692, -1.4419957 , -0.27390227,
        1.166638  , -0.00624323, -1.6046506 ,  2.1633575 , -0.395548  ,
       -1.1297956 , -3.1474566 ,  0.38729438, -2.0434535 , -1.5511289 ],
      dtype=float32)
</code></pre>
<br>
<h3 id="42-most_similar_to_givenkey1-keys_list">4.2 most_similar_to_given(key1, keys_list)</h3>
<p>从 keys_list 中获取与 key1 最相似的词。例如在212w影评中，从<code>'爱情', '悬疑', '飞船', '历史', '战争'</code>找出最接近<code>'太空'</code>，最后返回<code>'飞船'</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#从 `keys_list` 中获取与 `key1` 最相似的 `key`。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar_to_given</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;太空&#39;</span><span class="p">,</span> 
                                <span class="n">keys_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;爱情&#39;</span><span class="p">,</span> <span class="s1">&#39;悬疑&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;历史&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>'飞船'
</code></pre>
<br> 
<h3 id="43-w2v_modeln_similarityws1-ws2">4.3 w2v_model.n_similarity(ws1, ws2)</h3>
<p>两组词ws1, ws2 的相似度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="n">cosine_similarity</span><span class="p">([</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;理想&#39;</span><span class="p">)],</span>  
                  <span class="p">[</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;现实&#39;</span><span class="p">)])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div><pre><code>0.5371934
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cosine算法</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.5371934
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#计算两组键之间的余弦相似度。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;赞&#39;</span><span class="p">,</span> <span class="s1">&#39;推荐&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;无聊&#39;</span><span class="p">,</span> <span class="s1">&#39;尴尬&#39;</span><span class="p">,</span> <span class="s1">&#39;垃圾&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.35008422
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> <span class="s1">&#39;梦想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">,</span> <span class="s1">&#39;生活&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.48020104
</code></pre>
<br>
<h3 id="44-w2v_modelcloser_thankey1-key2">4.4 w2v_model.closer_than(key1, key2)</h3>
<p>更接近于 <em><strong>key1</strong></em> 的词向量(相比于key2)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取所有更接近 `key1` 的键，而不是 `key2` 。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">closer_than</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> 
                      <span class="n">key2</span><span class="o">=</span><span class="s1">&#39;现实&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>['梦想', '妥协', '追梦', '愿望', '骨感']
</code></pre>
<br>
<h3 id="45-w2v_modelmost_similarpositive-negative">4.5 w2v_model.most_similar(positive, negative)</h3>
<p>找出与 <em><strong>positive</strong></em> 同方向，与 <em><strong>negative</strong></em> 反向相反的词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;过瘾&#39;</span><span class="p">],</span>
                       <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;垃圾&#39;</span><span class="p">],</span>
                       <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><pre><code>[('激动人心', 0.6859163045883179),
 ('惊心动魄', 0.6767394542694092),
 ('带感', 0.6723690032958984),
 ('惊险刺激', 0.667783796787262),
 ('刺激', 0.6445038318634033),
 ('燃', 0.6429688930511475),
 ('爽快', 0.6287934184074402),
 ('带劲', 0.6254130005836487),
 ('爽', 0.624543309211731),
 ('酣畅淋漓', 0.6140543818473816)]
</code></pre>
<br>
<h3 id="46-类比king-manwomanqueen">4.6 类比king-man+woman~queen</h3>
<p><img loading="lazy" src="kingqueenformular.png" alt=""  />
</p>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。</p>
<p>这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p><em><strong>gender_direction_1 = vector(man)-vector(woman)</strong></em></p>
<p><em><strong>gender_direction_2 = vector(king)-vector(queen)</strong></em></p>
<p>那两个性别方向应该近似，假设这里将其  <em><strong>gender_direction_1 = gender_direction_2</strong></em> ，则对于公式中任意一个词，都可以由等式中的其他三个词经过运算得到。例如</p>
<p><em><strong>vector(queen) =  vector(king)-vector(man)+vector(woman)</strong></em></p>
<p>这里构造了一个情绪的公式，计算如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 开心 - 难过 ~=  享受 - d</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;开心&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;难过&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;享受&#39;</span><span class="p">)</span>

<span class="c1">#d = a-b+c</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
</code></pre></div><pre><code>[('享受', 0.7833479046821594),
 ('开心', 0.6825607419013977),
 ('愉快', 0.6298696994781494),
 ('娱乐', 0.6215130090713501),
 ('感官', 0.6085000038146973),
 ('图个', 0.6052624583244324),
 ('图一乐', 0.6039161682128906),
 ('休闲', 0.60273677110672),
 ('视觉享受', 0.6006160378456116),
 ('轻松愉快', 0.5961319804191589)]
</code></pre>
<p>很遗憾，<em><strong>d</strong></em> 没有运算出煎熬之类的词语，但好在都是形容词，而且是快乐居多的形容词，类别是对的，就是方向是反的。</p>
<br>
<h3 id="词向量总结">词向量总结</h3>
<p>需要注意的是经典的运算 <em><strong>king-man+woman~queen</strong></em> 来自 <em><strong>Glove</strong></em>模型，而不是本文使用的 <em><strong>Word2Vec</strong></em>模型。两者相同点，<em><strong>Glove</strong></em> 与 <em><strong>Word2Vec</strong></em> 均为词嵌入 <em><strong>embeddings</strong></em> 技术。区别在于 <em><strong>Glove</strong></em> 获取的词的全局语义空间，而 <em><strong>Word2Vec</strong></em> 一般是某个词前后n个词(例如前后5个词)范围内的语义。做概念四则运算，以后如可能，建议用 <em><strong>Glove</strong></em>。</p>
<p>此外，即时使用 <em><strong>Glove</strong></em>，尽量使用概念的词组均值向量。首先要训练数据要存在这些人类认知的线索。其次，认知概念往往不是由一个词决定的，可能需要相关的很多词。例如人类社会中的<code>雄雌(没有贬义，包含了男女在内的概念)</code>，</p>
<ul>
<li>雄性概念词有<code>他、男人、男孩、父亲、爷爷、爸爸、姥爷...</code></li>
<li>雌性概念词有<code>她、女人、女孩、母亲、奶奶、妈妈、姥姥...</code></li>
<li>国王概念词有<code>查理n世、乔治、路易...</code></li>
<li>女王概念词有<code>伊丽莎白n世、维多利亚女王、叶卡捷琳娜二世...</code></li>
</ul>
<p>或许改成概念向量四则运算，公式可能更容易成立。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 |  Python文本分析包更新</title>
      <link>https://textdata.cn/blog/cntext_simplification/</link>
      <pubDate>Fri, 01 Apr 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_simplification/</guid>
      <description>扩展词典、情感分析、可阅读性，内置9种情感词典，涵盖中英文</description>
      <content:encoded><![CDATA[<p><a href="https://github.com/hidadeng/cntext"><img loading="lazy" src="https://img.shields.io/badge/cntext-%e4%b8%ad%e6%96%87%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%ba%93-orange?style=for-the-badge&amp;logo=appveyor" alt=""  />
</a></p>
<p><a href="version1.2.md">旧版cntext入口</a></p>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> <strong>stats</strong>  文本统计指标
<ul>
<li><input checked="" disabled="" type="checkbox"> 词频统计</li>
<li><input checked="" disabled="" type="checkbox"> 可读性</li>
<li><input checked="" disabled="" type="checkbox"> 内置pkl词典</li>
<li><input checked="" disabled="" type="checkbox"> <strong>情感分析</strong></li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>dictionary</strong> 构建词表(典)
<ul>
<li><input checked="" disabled="" type="checkbox"> Sopmi 互信息扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> W2Vmodels 词向量扩充词典法</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>similarity</strong>   文本相似度
<ul>
<li><input checked="" disabled="" type="checkbox"> cos相似度</li>
<li><input checked="" disabled="" type="checkbox"> jaccard相似度</li>
<li><input checked="" disabled="" type="checkbox"> 编辑距离相似度</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>mind.py</strong> 计算文本中的认知方向（态度、偏见）</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext
</code></pre></div><br>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="mf">1.8.4</span>

<span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">mind</span>
    <span class="nx">dictionary</span>
    <span class="nx">similarity</span>
    <span class="nx">stats</span>
</code></pre></div><br>
<h2 id="一stats">一、stats</h2>
<p>目前stats内置的函数有</p>
<ul>
<li><strong>readability</strong>  文本可读性</li>
<li><strong>term_freq</strong> 词频统计函数</li>
<li><strong>dict_pkl_list</strong>  获取cntext内置词典列表(pkl格式)</li>
<li><strong>load_pkl_dict</strong> 导入pkl词典文件</li>
<li><strong>sentiment</strong> 情感分析</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="11--readability">1.1  readability</h3>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p>readability(text, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text: 文本字符串数据</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p>**中文可读性 ** 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>


<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 28.0,
 &#39;readability2&#39;: 0.15789473684210525,
 &#39;readability3&#39;: 14.078947368421053}
</code></pre></div><br>
<p>句子中的符号变更会影响结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.16666666666666666,
 &#39;readability3&#39;: 13.583333333333334}
</code></pre></div><p><br><br></p>
<h3 id="12--term_freq">1.2  term_freq</h3>
<p>词频统计函数，返回Counter类型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="13-dict_pkl_list">1.3 dict_pkl_list</h3>
<p>获取cntext内置词典列表(pkl格式)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取cntext内置词典列表(pkl格式)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;]
</code></pre></div><p>词典对应关系, 部分情感词典资料整理自 <a href="https://github.com/quanteda/quanteda.sentiment">quanteda.sentiment</a></p>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>DUTIR.pkl</td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td>HOWNET.pkl</td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>sentiws.pkl</td>
<td>SentimentWortschatz (SentiWS)</td>
<td>英文</td>
<td>正面词、负面词；<br>效价</td>
</tr>
<tr>
<td>ChineseFinancialFormalUnformalSentiment.pkl</td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td>ANEW.pkl</td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>词语效价信息</td>
</tr>
<tr>
<td>LSD2015.pkl</td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>NRC.pkl</td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td>geninqposneg.pkl</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HuLiu.pkl</td>
<td>Hu&amp;Liu (2004)正、负情感词典</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>AFINN.pkl</td>
<td>尼尔森 (2011) 的“新 ANEW”效价词表</td>
<td>英文</td>
<td>情感效价信息valence</td>
</tr>
<tr>
<td>LoughranMcDonald.pkl</td>
<td>会计金融LM词典</td>
<td>英文</td>
<td>金融领域正、负面情感词</td>
</tr>
<tr>
<td>ADV_CONJ.pkl</td>
<td>副词连词</td>
<td>中文</td>
<td></td>
</tr>
<tr>
<td>STOPWORDS.pkl</td>
<td></td>
<td>中、英</td>
<td>停用词</td>
</tr>
</tbody>
</table>
<h3 id="注意">注意:</h3>
<ul>
<li>
<p>如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</li>
<li>
<p>如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue</p>
</li>
</ul>
<br>
<h3 id="14-load_pkl_dict">1.4 load_pkl_dict</h3>
<p>导入pkl词典文件，返回字典样式数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入pkl词典文件,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, ...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...], 
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;, &#39;刿目怵心&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;, &#39;喜糖&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,]
           }
</code></pre></div><br>
<h3 id="15-sentiment">1.5 sentiment</h3>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)
使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>diction:  情感词字典；</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">],</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p>如果不适用pkl词典，可以自定义自己的词典，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p><br><br></p>
<h2 id="二dictionary">二、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="21-sopmi-共现法">2.1 SoPmi 共现法</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                   <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                   <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                   <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 44.49 s
</code></pre></div><br>
<h3 id="22-w2vmodels-词向量">2.2 W2VModels 词向量</h3>
<p><strong>特别要注意代码需要设定lang语言参数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   174 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s

</code></pre></div><br>
<h3 id="需要注意">需要注意</h3>
<p>训练出的w2v模型可以后续中使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">model路径</span><span class="p">)</span>
<span class="c1">#找出word的词向量</span>
<span class="c1">#w2v_model.get_vector(word)</span>
<span class="c1">#更多w2_model方法查看</span>
<span class="c1">#help(w2_model)</span>
</code></pre></div><p>例如本代码，运行生成的结果路径<code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;technology&#39;, 0.689210832118988),
 (&#39;infrastructure&#39;, 0.669672966003418),
 (&#39;resources&#39;, 0.6695448160171509),
 (&#39;talent&#39;, 0.6627111434936523),
 (&#39;execution&#39;, 0.6549549102783203),
 (&#39;marketing&#39;, 0.6533523797988892),
 (&#39;merchandising&#39;, 0.6504817008972168),
 (&#39;diversification&#39;, 0.6479553580284119),
 (&#39;expertise&#39;, 0.6446896195411682),
 (&#39;digital&#39;, 0.6326863765716553)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.45616838, -0.7799563 ,  0.56367606, -0.8570078 ,  0.600359  ,
       -0.6588043 ,  0.31116748, -0.11956959, -0.47599426,  0.21840936,
       -0.02268819,  0.1832016 ,  0.24452794,  0.01084935, -1.4213187 ,
        0.22840202,  0.46387577,  1.198386  , -0.621511  , -0.51598716,
        0.13352732,  0.04140598, -0.23470387,  0.6402956 ,  0.20394802,
        0.10799981,  0.24908689, -1.0117126 , -2.3168423 , -0.0402851 ,
        1.6886286 ,  0.5357047 ,  0.22932841, -0.6094084 ,  0.4515793 ,
       -0.5900931 ,  1.8684244 , -0.21056202,  0.29313338, -0.221067  ,
       -0.9535679 ,  0.07325   , -0.15823542,  1.1477109 ,  0.6716076 ,
       -1.0096023 ,  0.10605699,  1.4148282 ,  0.24576302,  0.5740349 ,
        0.19984631,  0.53964925,  0.41962907,  0.41497853, -1.0322098 ,
        0.01090925,  0.54345983,  0.806317  ,  0.31737605, -0.7965337 ,
        0.9282971 , -0.8775608 , -0.26852605, -0.06743863,  0.42815775,
       -0.11774074, -0.17956367,  0.88813037, -0.46279573, -1.0841943 ,
       -0.06798118,  0.4493006 ,  0.71962464, -0.02876493,  1.0282255 ,
       -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922,
       -0.16753489,  0.14065655, -1.8609751 ,  0.03587054,  1.2792674 ,
        1.2732009 , -0.74120265, -0.98000383,  0.4521185 , -0.26387128,
        0.37045383,  0.3680011 ,  0.7197629 , -0.3570571 ,  0.8016917 ,
        0.39243212, -0.5027844 , -1.2106236 ,  0.6412354 , -0.878307  ],
      dtype=float32)
</code></pre></div><p><br><br></p>
<h2 id="23-co_occurrence_matrix">2.3 co_occurrence_matrix</h2>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三similarity">三、similarity</h2>
<p>四种相似度计算函数</p>
<ul>
<li>cosine_sim(text1, text2)  cos余弦相似</li>
<li>jaccard_sim(text1, text2)     jaccard相似</li>
<li>minedit_sim(text1, text2)  最小编辑距离相似度；</li>
<li>simple_sim(text1, text2) 更改变动算法</li>
</ul>
<p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩啊&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.82
0.67
2.00
0.87
</code></pre></div><p><br><br></p>
<h2 id="四text2mind">四、Text2Mind</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<p>- <strong>多个对象在某概念的远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
<p>- 多个对象在某<strong>概念的分量(fen，一声)的多少</strong>， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
<p>这两种认知分别可以用向量距离、向量语义投影计算得来。</p>
<ul>
<li>tm.sematic_distance(words, c_words1, c_words2)  向量距离</li>
<li>tm.sematic_projection(words, c_words1, c_words2)  向量语义投影</li>
</ul>
<h3 id="41-tmsematic_distancewords-c_words1-c_words2">4.1 tm.sematic_distance(words, c_words1, c_words2)</h3>
<p>分别计算words与c_words1、c_words2语义距离，返回距离差值。</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果d1-d2&lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<p><strong>下载glove_w2v.6B.100d.txt</strong>链接: <a href="https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw">https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw</a> 提取码: 72l0</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#Note: this is a word2vec format model</span>
<span class="n">tm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Text2Mind</span><span class="p">(</span><span class="n">w2v_model_path</span><span class="o">=</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="n">engineer</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">mans</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">womans</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">mans</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">womans</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><br>
<h3 id="42-tmsematic_projectionwords-c_words1-c_words2">4.2 tm.sematic_projection(words, c_words1, c_words2)</h3>
<p><strong>语义投影</strong>，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，word越位于c_words2一侧。</p>
<p>下图是语义投影示例图，本文算法和图片均来自 &ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&rdquo;</p>
<p><img loading="lazy" src="img/Nature_Semantic_projection_recovering_human_knowledge_of.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">smalls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">bigs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">smalls</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">bigs</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Top2Vec|主题建模和语义搜索库</title>
      <link>https://textdata.cn/blog/top2vec_tutorial/</link>
      <pubDate>Mon, 13 Dec 2021 10:43:10 +0600</pubDate>
      
      <guid>/blog/top2vec_tutorial/</guid>
      <description>Python主题建模和语义搜索库</description>
      <content:encoded><![CDATA[<p>Top2Vec 是一种用于主题建模和语义搜索的算法。**我个人从理解代码和使用代码难度来看， 对于Python小白，BERTopic更适合直接用预训练词向量，而Top2Vec更适合对小规模数据训练词向量后做主题建模。**它自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。训练 Top2Vec 模型后，您可以：</p>
<ul>
<li>获取检测到的主题数。</li>
<li>获取话题。</li>
<li>获取主题大小。</li>
<li>获取分层主题。</li>
<li>按关键字搜索主题。</li>
<li>按主题搜索文档。</li>
<li>按关键字搜索文档。</li>
<li>找出相似的词。</li>
<li>查找类似的文档。</li>
<li>使用 RESTful-Top2Vec 公开模型</li>
<li>有关其工作原理的更多详细信息，请参阅论文。</li>
</ul>
<p><strong>亮点</strong></p>
<ul>
<li>自动查找主题数。</li>
<li>不需要停用词列表。</li>
<li>不需要词干/词形还原。</li>
<li>适用于短文本。</li>
<li>创建联合嵌入的主题、文档和词向量。</li>
<li>内置搜索功能。</li>
</ul>
<p><strong>它是如何工作的？</strong></p>
<p>该算法做出的假设是，许多语义相似的文档都表明了一个潜在的主题。</p>
<p>第一步是创建文档和词向量的联合嵌入。一旦文档和单词被嵌入到一个向量空间中，算法的目标就是找到密集的文档集群，然后确定哪些单词将这些文档吸引到一起。每个密集区域是一个主题，将文档吸引到密集区域的词就是主题词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">top2vec</span><span class="o">==</span><span class="mf">1.0.27</span>
</code></pre></div><h2 id="0-代码下载">0. 代码下载</h2>
<p><a href="top2vec_tutorial.zip">click to download code</a></p>
<p><br><br></p>
<h2 id="1-导入数据">1. 导入数据</h2>
<p>使用某灾难数据集，这里是存在标注的标签，但是我们假设不用label的，仅作为评判Top2vec运行效果的标准。<a href="cnews.csv">点击cnews.csv下载</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">top2vec</span> <span class="kn">import</span> <span class="n">Top2Vec</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;STOPWORDS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;STOPWORDS&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;cnews.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>时政    120
科技    106
时尚    106
财经    105
家居    103
教育     97
娱乐     96
体育     95
房产     87
游戏     85
Name: label, dtype: int64
</code></pre>
<p><br><br></p>
<h2 id="2-清洗数据">2. 清洗数据</h2>
<p>一般而言，作中文文本分析，需要把中文分词构造成类西方语言(空格间隔词语的文本)风格。在此期间，顺便将停用词剔除。其实在用top2vec时，不剔除停用词影响也不大。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleantext&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="3-训练模型">3. 训练模型</h2>
<p>Top2vec有一下四个常用参数</p>
<p><strong>Top2vec(documents, min_count, speed, workers)</strong></p>
<ul>
<li>documents: 文档列表</li>
<li>min_count: 词语最少出现次数。低于min_count的词不加入模型中</li>
<li>speed: 训练速度，参数默认&quot;learn&quot;
<ul>
<li>&ldquo;fast-learn&rdquo;  速度最快，训练效果最差</li>
<li>&ldquo;learn&rdquo;       速度，训练效果中等</li>
<li>&ldquo;deep-learn&rdquo;  速度最慢，训练效果最佳</li>
</ul>
</li>
<li>workers: 并行运行数，该值最大取值为电脑CPU的核数。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="n">Top2Vec</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleantext&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">(),</span> 
                <span class="n">min_count</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">speed</span><span class="o">=</span><span class="s2">&#34;deep-learn&#34;</span><span class="p">,</span>  
                <span class="n">workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>2021-12-14 20:21:10,318 - top2vec - INFO - Pre-processing documents for training
2021-12-14 20:21:10,871 - top2vec - INFO - Creating joint document/word embedding
2021-12-14 20:25:06,082 - top2vec - INFO - Creating lower dimension embedding of documents
2021-12-14 20:25:14,645 - top2vec - INFO - Finding dense areas of documents
2021-12-14 20:25:14,683 - top2vec - INFO - Finding topics
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 话题个数</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_num_topics</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>9
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 各话题数量</span>
<span class="n">topic_sizes</span><span class="p">,</span> <span class="n">topic_nums</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_sizes</span><span class="p">()</span>

<span class="p">{</span><span class="s2">&#34;topic_sizes&#34;</span><span class="p">:</span><span class="n">topic_sizes</span><span class="p">,</span> 
 <span class="s2">&#34;topic_ids&#34;</span><span class="p">:</span><span class="n">topic_nums</span><span class="p">}</span>
</code></pre></div><p>Run</p>
<pre><code>{'topic_sizes': array([361, 116, 107,  99,  97,  93,  82,  25,  20]),
 'topic_ids': array([0, 1, 2, 3, 4, 5, 6, 7, 8])}
</code></pre>
<p><br><br></p>
<h2 id="4-get_topics">4. get_topics</h2>
<p>用pyecharts词云图显示<strong>话题信息</strong>， 为了简化代码，将该功能封装为函数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gen_wordcloud</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    topic_words: 主题词列表
</span><span class="s2">    word_scores: 主题特征词的权重得分(词语表征主题的能力)
</span><span class="s2">    topic_id: 主题id
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
    <span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
    <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
    
    <span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">)]</span>

    <span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">88</span><span class="p">])</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
        <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;Topic_</span><span class="si">{topic_id}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">),</span> 
                                  <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">23</span><span class="p">)),</span>
        <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="n">display</span><span class="p">(</span><span class="n">wc</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">())</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topics</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>

<span class="k">for</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">):</span>
    <span class="n">gen_wordcloud</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/vis1.png" alt=""  />

<img loading="lazy" src="img/vis2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="5-get_documents_topics">5. get_documents_topics</h2>
<p>get_documents_topics(doc_ids, num_topics=1)</p>
<ul>
<li>doc_ids: 待查询文档id列表</li>
<li>num_topics: 返回某文档可能归属话题的个数</li>
</ul>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查第一条文档的</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_documents_topics</span><span class="p">(</span><span class="n">doc_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(array([0]),
 array([0.1384481], dtype=float32),
 array([[&#39;政府&#39;, &#39;经济&#39;, &#39;政策&#39;, &#39;建设&#39;, &#39;中方&#39;, &#39;发展&#39;, &#39;促进&#39;, &#39;部门&#39;, &#39;留学&#39;, &#39;学生&#39;, &#39;会议&#39;,
         &#39;我要&#39;, &#39;事务&#39;, &#39;日电&#39;, &#39;房价&#39;, &#39;教育&#39;, &#39;国务院&#39;, &#39;温家宝&#39;, &#39;留学生&#39;, &#39;人数&#39;, &#39;移民&#39;,
         &#39;会见&#39;, &#39;推动&#39;, &#39;申请者&#39;, &#39;申请&#39;, &#39;官员&#39;, &#39;住房&#39;, &#39;房屋&#39;, &#39;加强&#39;, &#39;中国政府&#39;, &#39;购房&#39;,
         &#39;国家&#39;, &#39;支付&#39;, &#39;楼市&#39;, &#39;外交部&#39;, &#39;接收&#39;, &#39;两国&#39;, &#39;原则&#39;, &#39;各地&#39;, &#39;总理&#39;, &#39;战略&#39;,
         &#39;和平&#39;, &#39;框架&#39;, &#39;评论&#39;, &#39;有序&#39;, &#39;装修&#39;, &#39;中国&#39;, &#39;就业&#39;, &#39;友好&#39;, &#39;人力资源&#39;]],
       dtype=&#39;&lt;U9&#39;),
 array([[0.3623712 , 0.36037514, 0.35219163, 0.35109183, 0.3499857 ,
         0.34666985, 0.3426961 , 0.34161803, 0.34010434, 0.3382269 ,
         0.33710504, 0.336056  , 0.33598724, 0.33488944, 0.3303768 ,
         0.32483265, 0.324798  , 0.32201332, 0.3174801 , 0.3153757 ,
         0.3152491 , 0.31338856, 0.31334093, 0.31244045, 0.31202242,
         0.30908576, 0.3086405 , 0.30838227, 0.30605763, 0.3053521 ,
         0.30474398, 0.30268514, 0.30253592, 0.30242488, 0.30227807,
         0.3017046 , 0.30116442, 0.30062813, 0.2996228 , 0.29806197,
         0.2972776 , 0.29709277, 0.29706252, 0.29584888, 0.29578486,
         0.29524648, 0.2944737 , 0.2939484 , 0.29286712, 0.29246706]],
       dtype=float32))
</code></pre></div><p><br><br></p>
<h2 id="6-search_topics">6. search_topics</h2>
<p>根据关键词搜索话题，查某词是否属于某话题，属于该主题的概率
search_topics(keywords, num_topics, keywords_neg=None)</p>
<ul>
<li>keywords: 关键词列表</li>
<li>num_topics: 返回话题个数，按照语义相似度从高到低排序</li>
<li>keywords_neg: 反义词列表</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gen_wordcloud2</span><span class="p">(</span><span class="n">query_word</span><span class="p">,</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">,</span> <span class="n">topic_probability</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    query_word: 待查询词
</span><span class="s2">    topic_words: 主题词列表
</span><span class="s2">    word_scores: 主题特征词的权重得分(词语表征主题的能力)
</span><span class="s2">    topic_id: 主题id
</span><span class="s2">    topic_probability: 主题概率
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
    <span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
    <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
    
    <span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">)]</span>

    <span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">88</span><span class="p">])</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;Word</span><span class="si">{query_word}</span><span class="se">\n</span><span class="s2">Topic_</span><span class="si">{topic_id}</span><span class="se">\n</span><span class="s2">Probability:</span><span class="si">{probability:.2f}</span><span class="s2">&#34;&#34;&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">query_word</span><span class="o">=</span><span class="n">query_word</span><span class="p">,</span>
                                                              <span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> 
                                                              <span class="n">probability</span><span class="o">=</span><span class="n">topic_probability</span><span class="p">)</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
        <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> 
                                  <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">18</span><span class="p">)),</span>
        <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="n">display</span><span class="p">(</span><span class="n">wc</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">())</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">query_word</span> <span class="o">=</span> <span class="s2">&#34;电影&#34;</span>
<span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_topics</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="n">query_word</span><span class="p">],</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_score</span><span class="p">,</span> <span class="n">topic_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">topic_score</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">:</span>
        <span class="n">gen_wordcloud2</span><span class="p">(</span><span class="n">query_word</span><span class="o">=</span><span class="n">query_word</span><span class="p">,</span> 
                       <span class="n">topic_words</span><span class="o">=</span><span class="n">topic_words</span><span class="p">,</span> 
                       <span class="n">word_scores</span><span class="o">=</span><span class="n">word_scores</span><span class="p">,</span> 
                       <span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> <span class="n">topic_probability</span><span class="o">=</span><span class="n">topic_score</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/vis5.png" alt=""  />
</p>
<br>
<h2 id="7-query_topics">7. query_topics</h2>
<p>根据一段文本寻找最符合该文本的话题
query_topics(query, num_topics)</p>
<ul>
<li>query: 查询文本，注意是用空格间隔词语的文本</li>
<li>num_topics: 返回的话题数</li>
</ul>
<p>返回话题特征词列表， 话题特征词权重， 话题概率， 话题id</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">querytext</span> <span class="o">=</span> <span class="s1">&#39;刘晓庆 55 岁 近日 颁奖礼 刘晓庆 一袭 宝蓝色 超低 胸 V 领 长裙 亮相 轻薄 蕾丝 奢华 皮草 艳丽 色彩 翠绿&#39;</span>
<span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">query_topics</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">querytext</span><span class="p">,</span> 
                                                                       <span class="n">num_topics</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;可能归属的话题有: &#39;</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;归属于该话题的概率&#39;</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">可能归属的话题有:  [1 4]
归属于该话题的概率 [0.32036728 0.1276904 ]
</code></pre></div><br>
<h2 id="8-search_documents_by_keywords">8. search_documents_by_keywords</h2>
<p>根据关键词，筛选文档</p>
<p>search_documents_by_keywords(keywords,
num_docs,
keywords_neg=None,
return_documents=True)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#文档， 语义相关性， 文档id</span>
<span class="n">docs</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">doc_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_keywords</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;搭配&#39;</span><span class="p">],</span> 
                                                         <span class="n">num_docs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                                                         <span class="n">keywords_neg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                                                         <span class="n">return_documents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">doc_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;----------&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Document: 106, Semantic similarity: 0.4943176805973053
白色 短裙 百变 休闲 感 要点 一定 敞开 衬衫 配合 牛仔裤 休闲 感 短裤 衬衫 短 敞开 显得 好好 穿 裤子 搭配 七分裤 遮住 臀部 长度 关键 尽量 选择 艳丽 颜色 带 出 青春 感 NO.3     白色 短裙 tips :   白色 短裙 + 粉色 上衣 这是 一套 减龄 百分百 搭配 白色 短裙 本来 清纯 粉色 上衣 搭配 更加 具有活力 tips :   白色 短裙 + 抹胸 + 外套 想要 性感 一点 就加 一件 抹胸 抹胸 胸前 构造 曲线 完美 再加 外套 保暖 得体 看似 简单 一款 搭配 其实 暗地里 偷偷地 修饰 身材
----------

Document: 870, Semantic similarity: 0.4483542740345001
组图 看达人 演绎 豹纹 军装 风 导语 懂得 潮流 总是 知道 适合 今冬 流行 亮点 太 军装 豹纹 类似 民族风情 想要 知道 搭配 快 看看 时尚 达 穿 军绿色 宽松 款 大衣 不失 俏皮 味道 高腰 设计 短裙 有效 提升 腰线 衬托出 修长 美腿 豹纹 今年 冬季 抢眼 搭配 元素 加上 驼色 针织衫 灰色 围巾 暖 棕色 手 挎包 整体 色调 统一 迷人 棕色 蓝色 结合能 眼前一亮 简洁 款式 依然 突显 独特 品味 宽松 针织 外套 衬托出 优美 身形 搭配 同样 沉闷 黑色 包包 性感 丝袜 装扮 依然 透露 出 迷人 气息 立领 衬衫 加上 深黄 高腰 裤 摩登 感 十足 随意 披上 外套 更显 慵懒 个性 法式 风情
----------

Document: 450, Semantic similarity: 0.4471719563007355
街 拍 爱 招摇过市 毛茸茸 ( 组图 ) 导语 皮草 每个 冬天 可能 丢弃 每个 需要 温暖 早些 相比 人造皮 草比 真皮 草 风头 更劲 时尚 环保 大牌 秀 场上 超模 一个个 穿着 人造皮 草 “ 招摇过市 ” 之后 街头 潮人 没有 理由 拒绝 外形 酷酷 这件 气场 皮草 单品 配合默契 摇滚 风 配饰 搭配 黑色 皮草 长 背心 更显 利落 酷酷 黑色 皮草 搭配 蓝色 衬衣 不同 感觉 加上 下半身 底裤 时髦 包包 颜色 提亮 整身 装扮 抹胸 式 皮草 特点 高贵典雅 适合 搭配 连衣裙 装饰 增添 时尚 美感 复古 圆点 连衣裙 搭配 宽松 棕色 皮草 衣 名媛 感觉 典雅 淑女 短款 黑色 皮草 搭配 贴身 仔裤 搭配 长靴 潇洒 帅气 茸茸 帽子 增添 不少 甜美 感
----------
</code></pre></div><p><br><br></p>
<h2 id="9-search_documents_by_topic">9. search_documents_by_topic</h2>
<p>根据指定的topic_id， 显示该主题前num_docs个文档，显示的文档是根据概率从高到低降序显示</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看topic4的前5条文档</span>
<span class="n">topic_id</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_topic</span><span class="p">(</span><span class="n">topic_num</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> <span class="n">num_docs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Document: 905, Semantic similarity: 0.4941929578781128
-----------
现场 阿联 第三节 未 亮相   奇才 连续 3 记 重扣 逆转 比分 新浪 体育讯 北京 时间 4 2 奇才 主场 迎战 联盟 垫底 骑士 奇才 新秀 后卫 约翰 - 沃尔因 一场 对阵 热火 比赛 斗殴 禁赛 一场 伤愈 复出 安德雷 - 布 莱切 回到 首发 阵容 奇才 本赛季 首场 客场 胜利 面对 骑士 取得 当时 奇才 115 - 110 击败 对手 上半场 骑士 命中率 达到 53.8% 奇才 仅 44% 骑士 希克 森 ( 16 ) 塞 申斯 ( 12 ) 得分 双 奇才 布 莱切 ( 22 ) 麦基 ( 12 ) 埃文斯 4 投 0 仅 抢下 1 篮板 易建联 上场 7 08 2 投 0 抢下 3 篮板 异地 再战 埃文斯 终于 得分 抢断 吉后 犯规 两罚 命中 吉 随即 突破 上篮 命中 回敬 球 杰 弗斯 三分 不进 吉 抢下 篮板 上篮 再进 一球 布 莱切 中投 命中 霍林斯 篮下 出手 不进 布 莱切 抢下 篮板 此后 克劳福德 连续 突破 先是 助攻 麦基 扣篮 盖掉 戴维斯 投篮 助攻 布 莱切 扣篮 戴维斯 运球 被断 布 莱切 传给 杰 弗斯 一记 暴扣 奇才 连续 3 次 颇具 气势 扣篮 连得 6 反超 骑士 1 骑士 请求 暂停 回到 比赛 吉 上篮 不进 麦基 低位 单打 不进 布 莱切 抢下 篮板 3 得手 骑士 进攻 24 违例 奇才 越战越勇 克劳福德 身体 失去 重心 情况 仍然 将球 投进 一个打 3 骑士 连续 吉 挺身而出 三分 命中 个人 已经 得到 10 此人 本赛季 短暂 效力 奇才 麦基 中投 不进 布 莱切 抢下 前场 篮板 将球 放进 麦基 防守 领到 犯规 希克 森两罚 命中 麦基 强攻 造成 霍林斯 犯规 两罚 一中 戴维斯 三分 不进 克劳福德 跑 投 命中 戴维斯 突分 霍林斯 暴扣 命中 回过头来 克劳福德 助攻 麦基扣 劲 爆 哈兰 高迪 中投 不进 克劳福德 投篮 偏出 布 莱切 3 报价 连续 抢 篮板 进攻 最后 犯规 两罚 一中 现在 已经 得到 32 18 篮板 布 莱切 底线 遭 报价 分球 埃文斯 三分 命中 霍林斯 篮下 重扣 奇才 请求 暂停 布 莱切 继续 得分 吉布森 上篮 命中 克劳福德 中投 不进 抢下 篮板 杰 弗斯 运球 突破 犯规 两罚 命中 易建联 节 没有 登场 第三节 比赛 结束 骑士 82 - 83 奇才 ( 草头 王 )
-----------

Document: 689, Semantic similarity: 0.4917592704296112
-----------
直击 康大 内线 一柱擎天   13 优势 到手 胜利在望 新浪 体育讯 北京 时间 4 5 ( 休斯敦 时间 4 4 ) 消息 NCAA   Final   4 总决赛 休斯敦 Reliant 球馆 举行 比赛 进入 最后 6 分钟 本场 表现 十分 亮眼 康涅狄格 内线 阿莱克斯 - 奥里 瓦基接 队友 直传 空切 篮下 扣篮 得分 打成 2 + 1 目前 已经 拿下 10 9 篮板 3 封盖 巴特勒 仍然 没 解决 进攻 端的 问题 下半场 23 投 仅仅 3 屡次 外线 空挡 出手 均 打铁 告终 仅仅 入账 8 目前 康大 已经 取得 13 优势 胜利在望 ( silencer )
-----------

Document: 425, Semantic similarity: 0.47035443782806396
-----------
今日 数据 趣谈 魔兽 悲情 似 张大帅   基德 焕发 第二 春 新浪 体育讯 北京 时间 4 17 NBA 季后赛 正式 开打 进行 4 场 比赛 以下 今日 比赛 诞生 有趣 数据 今日 首场 季后赛 芝加哥 公牛 第四节 剩 4 分钟 仍以 88 - 98 落后 接下来 打出 16 - 1 攻击 波 主场 一举 逆转 印第安纳 步行者 取胜 继 2004 之后 NBA 季后赛 舞台 再次出现 终场 前 4 分钟 落后 两位数 最终 翻盘 成功 案例 2004 5 9 西部 决赛 明尼苏达 森林狼 萨克拉门托 国王 比赛 森林狼 同样 终场 前 4 分钟 仍以 78 - 88 落后 接下来 打出 16 - 1 ( 惊人 相似 ) 最终 94 - 89 逆转 取胜 今天 公牛 逆转 步行者 比赛 德里克 - 罗斯 砍 39 罚球 21 投 19 2008 洛杉矶 湖人 对阵 犹他 爵士 一场 季后赛 科比 - 布莱恩特 创下 单场 罚球 23 投 21 季后赛 纪录 罗斯 位居 全场 三分球 9 次 出手 竟无一 命中 季后赛 历史 此前 两次 类似 案例 2008 奥兰多 魔术 对阵 多伦多 猛龙 一场 比赛 拉沙德 - 刘易斯 三分球 9 投 0 一次 熟知 1994 总决赛 第七场 约翰 - 斯塔克 斯 三分 线外 11 投 0 纽约 尼克斯 负于 休斯敦 火箭 冠军 擦肩而过 今天 亚特兰大 老鹰 客场 战胜 奥兰多 魔术 比赛 老鹰 五名 球员 得分 低于 13 — — 乔 - 约翰逊 ( 25 16 投 9 ) 贾马尔 - 克劳福德 ( 23 14 投 7 ) 艾尔 - 霍福德 ( 16 14 投 7 ) 约什 - 史密斯 ( 15 12 投 6 ) 科克 - 辛里奇 ( 13 10 投 6 ) 该队 过去 199 场 季后赛 尚属 首次 老鹰队 史上 一次 出现 这种 盛况 1966 4 14 131 - 127 战胜 洛杉矶 湖人 比赛 当时 书写 纪录 五人 里奇 - 古尔林 克里夫 - 哈根 泽尔莫 - 比蒂 比尔 - 布里奇斯 乔 - 考 德维尔 今天 负于 老鹰 比赛 德怀特 - 霍华德 ( 46 ) 贾 米尔 - 尼尔森 ( 27 ) 砍 73 队友 总共 仅 拿下 20 魔术 最终 93 - 103 负于 更为 均衡 对手 NBA 历史 8 支 球队 一场 季后赛 比赛 有过 两名 球员 联手 砍 全队 至少 75% 得分 1 队 取胜 追溯到 1950 4 9 当年 总决赛 第一场 比赛 乔治 - 麦肯 得到 37 吉姆 - 波 拉德 得到 14 率领 明尼阿波利斯 湖人 68 - 66 战胜 锡 拉丘兹 民族 ( 费城 76 前身 ) 7 队则 败北 得到 46 霍华德 抢下 19 篮板 常规 时间 取得 1975 4 19 布法罗 勇敢者 ( 洛杉矶 快船 前身 ) 战胜 华盛顿 子弹 ( 华盛顿 奇才 前身 ) 一场 季后赛 效力 勇敢者 鲍勃 - 麦卡 杜 同样 没有 加时赛 情况 砍 50 21 篮板 威尔特 - 张伯伦 一场 季后赛 常规 时间 砍 46 19 篮板 球队 却输 ( 事实上 张大帅 生涯 3 场 比赛 取得 数据 竟 败北 ) 刚 谢幕 本赛季 常规赛 杰森 - 基德 仅 两场 比赛 得分 达到 20 + 1 20 对阵 湖人 比赛 砍 赛季 最高 21 今天 达拉斯 小牛 主场 战胜 波特兰 开拓者 比赛 砍 24 命中 6 记 三分球 一场 季后赛 比赛 砍 20 + 得分 刷新 常规赛 创下 赛季 新高 NBA 历史 壮举 球员 如今 38 岁 基德 年龄 最大 成为 NBA 历史 一场 季后赛 比赛 单场 命中 6 记 三分球 年龄 最大 球员 此前 纪录 雷吉 - 米勒 2002 创下 当时 36 岁 今天 小牛 战胜 开拓者 比赛 德克 - 诺维茨基 第四节 13 次 罚球 出手 命中 追平 迈克尔 - 乔丹 纪录 1990 - 91 赛季 季后赛 一场 公牛 底特律 活塞 比赛 乔丹 单节 命中 13 次 罚球 率队 105 - 97 取胜 最终 公牛 获得 赛季 总冠军 今天 迈阿密 热火 战胜 费城 76 比赛 克里斯 - 波什 得到 25 12 篮板 勒布朗 - 詹姆斯 得到 21 14 篮板 他俩 队友 参加 首场 季后赛 前 一个 赛季 各为其主 接下来 赛季 并肩作战 季后赛 首场 比赛 砍 得分 20 + 篮板 10 + 组合 波什 詹姆斯 之前 无先例 ( 魑魅 )
-----------

Document: 155, Semantic similarity: 0.45704954862594604
-----------
现场 麦蒂 返场 销魂 跳投 两 连击   小拜 纳姆 单节 11 新浪 体育讯 北京 时间 4 6 华盛顿 奇才 主场 迎战 底特律 活塞 此前 球队 已经 客场 两连胜 若能 战胜 活塞 奇才 本赛季 首次 迎来 三连胜 异地 再战 埃文斯 中投 命中率 先 得分 拜纳姆 中投 不进 克劳福德 一人 带球 运 前场 对手 尚未 落位 情况 直接 出手 投篮 命中 这种 投篮 欠缺 考虑 根本 没有 战术 配合 全 个人 手感 遇到 防守 稍 一点 球队 沃尔 抢断 埃文斯 直接 暴扣 奇才 反超 4 活塞 请求 暂停 沃尔 报价 对手 拜纳姆 得到 机会 三分 出手 命中 布 莱切 上篮 得手 门罗 助攻 威尔 考克斯 扣篮 命中 埃文斯 三分 不进 拜纳姆 突破 上篮 命中 威尔 考克斯 拿布 莱切 没有 办法 运球 进攻 威尔 考克斯 只能 伸直 手臂 不断 滑步 被布 莱切 强投 命中 活塞 拜纳姆 发力 突破 上篮 命中 布 莱切 中投 不进 拜纳姆 卷土重来 造成 沃尔 犯规 两罚 命中 个人 已经 得到 11 门罗 抢断 布 莱切 普林斯 上篮 命中 活塞 反超 3 麦基 传球 失误 奇才 请求 暂停 威尔 考克斯 篮下 强打 奇才 反击 埃文斯 上篮 命中 普林斯 糟糕 状态 继续 中投 偏出 布 莱切 运球 单打 活塞 两名 内线 屡试不爽 造成 门罗 犯规 两罚 命中 汉密尔顿 中投 不进 威尔 考克斯 抢下 前场 篮板 直接 扣篮 命中 布 莱切 继续 发威 转身 摆脱 上篮 命中 拜纳姆 三分 偏出 球 砸 远 活塞 球员 退守 不及 克劳福德 轻松 上篮 命中 沃尔 中投 不进 拜纳姆 反击 遭 侵犯 两罚 命中 个人 单节 已经 得到 11 布 莱切 对手 包夹 中投 偏出 普林斯 跑 投 命中 活塞 反超 一分 克劳福德 中投 打铁 拜纳姆 没能 命中 三分 麦蒂 回到 赛场 塞拉芬 进攻 犯规 普林斯 中投 不进 门罗 补篮 命中 麦蒂断 球 直接 中投 命中 布 莱切 走步 麦蒂 假动作 点飞 克劳福德 投篮 再进 第三节 比赛 结束 活塞 81 - 78 奇才 ( 草头 王 )
-----------

Document: 254, Semantic similarity: 0.45255911350250244
-----------
奇才 vs 步行者 前瞻 走出 客场 阴影   斗狠 东部 老八 新浪 体育讯 北京 时间 4 7 奇才队 客场 挑战 东区 第八 步行者 目前 奇才 客场 战绩 3 胜 35 负 最近 客场 两连胜 奇才队 背靠背 作战 今天 主场 107 - 105 险胜 活塞 球队 一举 拿到 赛季 最长 三连胜 实际上 这是 奇才队 2007 - 08 赛季 以后 球队 第一个 赛季 三连胜 这场 比赛 奇才 惊人 获得 35 次 罚球 沃尔一人 包办 16 次 全场 得到 26 12 次 助攻 6 篮板 4 次 抢断 布 莱切 无疑 三连胜 第一 功臣 连胜 期间 场均 得到 29 15.3 篮板 克劳福德 同样 火爆 异常 一位 前锋 首发 埃文斯 表现 低估 活塞 比赛 埃文斯 13 投 9 中射下 20 沃尔 拿到 职业生涯 首个 三连胜 “ 联盟 留下 标签 一名 菜鸟 证明 一部分 很多 想 站 球场上 一分钟 全力以赴 ” 奇才 三连胜 对手 名副其实 鱼腩 球队 无论如何 三连胜 这支 弱旅 一个 不小 激励 尤其 伤病 满营 情况 目前 球队 6 可能 赛季 结束 前 无法 归队 包括 得分王 尼克 - 杨 约什 - 霍华德 拉沙德 - 刘易斯 布克 恩戴 耶 卡 蒂尔 - 马丁 步行者 35 胜 43 负 暂居 东部 第八 目前 东部 前七 已经 锁定 季后赛 剩下 第八名 悬念 步行者 领先 第九位 山猫 2.5 个胜场 领先 10 位 雄鹿 3.5 个胜场 剩下 4 场 比赛 情况 悬念 并不大 明天 山猫 雄鹿 迎战 强敌 ( 魔术 热火 ) 步行者 机会 扩大 领先 场次 优势 球队 头号 得分手 格兰杰 状态 过去 5 场 比赛 得分 20 以下 最近 三场 三分球 12 投 3 汉斯 布鲁在 过去 6 场 比赛 陷入 挣扎 场均 9.3 5.7 篮板 ( 之前 11 场 比赛 贡献 20.6 7.8 篮板 ) 一场 比赛 步行者 12 输给 黄蜂队 主教练 沃格尔 称之为 “ 惨痛 失败 ” 本赛季 两队 战成 2 - 1 步行者 赢下 最近 两次 交锋 两场 比赛 奇才 命中率 均 低于 40% 总 失误 高达 41 次 预计 两队 首发 奇才 沃尔 克劳福德 埃文斯 布 莱切 麦基 步行者 科 里森 格兰杰 乔治 汉斯 布鲁 希伯特 ( 木瓜 丁 )
-----------
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_keywords</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;搭配&#34;</span><span class="p">,</span> <span class="s2">&#34;高跟鞋&#34;</span><span class="p">],</span> <span class="n">num_docs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><pre><code>Document: 727, Semantic similarity: 0.5883481502532959
-----------
组图 冷气 办公室   连衣裙 配小 坎肩 美国 设计师 Diane   Von   Furstenberg 曾经 感觉 女人 穿 连衣裙 女人 找到 一件 适合 dream   dress 重要 无需 费神 搭配 单穿 连身 优雅 飞扬 裙摆 似乎 告诉 女 连衣裙 玩起 High   Fashion 变脸 游戏 DKNY 绿色 连衣裙   新品 未 定价 H &amp; M 黑色 外套   新品 未 定价 Agatha 配件 新品 未 定价 C . Banner 高跟鞋   新品 未 定价 低 V 领 连衣裙 秀出 属于 性感 更好 展现出 颈部 线条 搭配 修身 剪裁 西装 短款 皮手套 极具 欧美 明星 范儿 细 高跟鞋 更好 突出 双腿 长度 整体 显得 轻盈 不少 On &amp; on 米色 连衣裙   新品 未 定价 Asobio 针织 外套   RMB   449 Kookai 金色 腰带 Jc  
-----------

Document: 435, Semantic similarity: 0.5440454483032227
-----------
组图 秋冬 优雅 妖娆   女星 爱 裸 色系 导语 裸色 优雅 代名词 女星 近来 誓 裸色 进行 到底 无论是 徐若 ? 性感 乐基儿 气质 搭配 各色 礼服 赏心悦目 娇俏 款式 更是 大饱眼福 徐若 ? 飘逸 丝带    立刻 彰显 天王 嫂 贵妇 气质 袁咏仪 翻领 西装   气质 非凡 裸色 短款 紧身 西装 皮质 面料 彰显 个性 夹带 一点 蕾丝 装饰 女性 柔美 油然而生 搭配 碎花 蛋糕 裙 气质 非凡
-----------

Document: 870, Semantic similarity: 0.523485541343689
-----------
组图 看达人 演绎 豹纹 军装 风 导语 懂得 潮流 总是 知道 适合 今冬 流行 亮点 太 军装 豹纹 类似 民族风情 想要 知道 搭配 快 看看 时尚 达 穿 军绿色 宽松 款 大衣 不失 俏皮 味道 高腰 设计 短裙 有效 提升 腰线 衬托出 修长 美腿 豹纹 今年 冬季 抢眼 搭配 元素 加上 驼色 针织衫 灰色 围巾 暖 棕色 手 挎包 整体 色调 统一 迷人 棕色 蓝色 结合能 眼前一亮 简洁 款式 依然 突显 
-----------

Document: 522, Semantic similarity: 0.4756317138671875
-----------
女星 争当 蓝色妖姬 &amp; nbsp ; 英国 气质 女演员 瑞切尔 ・ 薇 兹 时尚 点评 英国 气质 女演员 瑞切尔 · 薇 兹 ( Rachel   Weisz )   美貌 非常 头脑 修身 印花 连衣裙 搭配 抢眼 棕红色 短 夹克 非常 好看 搭配 黑色 罗马 feel 高跟鞋 特别 有潮味 时尚 点评 身材 不算 瘦 女星 Lea   Michele 搭配 起来 非常 特色 一味 地瘦 风格 满是 褶皱 裙子 非常 修身 亮眼 颜色 非常
-----------

Document: 707, Semantic similarity: 0.47334203124046326
-----------
组图 黑丝 短裙 上阵   5 旬 女星 胜过 90 红星 导语 气温 越来越低 女星 不畏 严寒 纷纷 穿着 短裙 透视装 出席 活动 一番 比拼 不难 发现 气质 年轻 难得 厉害 一起 看看 刘晓庆 55 岁 近日 颁奖礼 刘晓庆 一袭 宝蓝色 超低 胸 V 领 长裙 亮相 轻薄 蕾丝 奢华 皮草 艳丽 色彩 翠绿 首饰 配上 短小 精炼 波波 头 瞬间 减龄 15 岁 张曼玉 46 岁 一向 气质 型 美女 著称 反倒 少 繁琐 修饰 刻意 打扮 超级 简单 Lanvin   for   H &amp; M 斜肩 礼裙 搭配 一双 皮质 手套 
-----------
</code></pre>
<p><br><br></p>
<h2 id="10-get_topic_hierarchy">10. get_topic_hierarchy</h2>
<p>对话题进行分类，需要</p>
<ol>
<li>先执行model.hierarchical_topic_reduction</li>
<li>再执行model.get_topic_hierarchy。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 将话题分为2类</span>
<span class="n">model</span><span class="o">.</span><span class="n">hierarchical_topic_reduction</span><span class="p">(</span><span class="n">num_topics</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_topic_hierarchy</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>[[7, 6, 1, 8, 5, 4, 3], [2, 0]]
</code></pre>
<p><br><br></p>
<h2 id="11-similar_words">11. similar_words</h2>
<p>查找相似词， 该方法其实也可以用于扩充词典。</p>
<p>similar_words(keywords, num_words, keywords_neg=None)</p>
<ul>
<li>keywords: 待查询关键词列表</li>
<li>num_words: 返回相似词个数</li>
<li>keywords_neg: 指定反义词列表</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查找【增进】的最相似的10个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">similar_words</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;增进&#34;</span><span class="p">],</span> 
                    <span class="n">num_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                    <span class="n">keywords_neg</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>(array(['两国关系', '两国', '温家宝', '王刚', '战略', '友好', '中欧', '政治', '会见', '人民'],
       dtype='&lt;U4'),
 array([0.50498132, 0.49835259, 0.4636392 , 0.45802986, 0.45299921,
        0.44836198, 0.43550295, 0.43471974, 0.43099192, 0.42711113]))
</code></pre>
<br>
<h2 id="12-save">12. save</h2>
<p>训练不易， 记得保存模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;随便起个名字.pkl&#39;</span><span class="p">)</span>
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>KeyBERT | 关键词发现</title>
      <link>https://textdata.cn/blog/keybert_tutorial/</link>
      <pubDate>Wed, 27 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/keybert_tutorial/</guid>
      <description>使用 BERT 嵌入 和 简单余弦相似度 来查找文档中与文档本身最相似的短语，自动挖掘文本中的关键词</description>
      <content:encoded><![CDATA[<p>尽管已经有很多方法可用于关键字生成（例如，Rake、YAKE!、TF-IDF 等），但我想创建一个非常基本但功能强大的方法来提取关键字和关键短语。这就是 KeyBERT 的用武之地！它使用 <strong>BERT 嵌入</strong> 和 <strong>简单余弦相似度</strong> 来查找文档中与文档本身最相似的短语。</p>
<p>KeyBERT步骤</p>
<ol>
<li>首先使用 BERT 提取文档嵌入以获得<strong>文档级向量表示</strong>。</li>
<li>随后，为 N-gram 词/短语提取<strong>词向量</strong>。</li>
<li>然后，我们使用余弦相似度来找到与文档最相似的单词/短语。</li>
<li>最后可以将最相似的词识别为最能描述整个文档的词。</li>
</ol>
<h2 id="代码下载">代码下载</h2>
<p><a href="KeyBERT%E5%AD%A6%E4%B9%A0.ipynb">click to download the code</a></p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">keybert</span><span class="o">==</span><span class="mf">0.5.0</span>
</code></pre></div><br>
<h2 id="初始化模型">初始化模型</h2>
<p>KeyBERT库需要安装配置spacy语言模型</p>
<p>具体参考<strong>公众号：大邓和他的Python</strong> 2021-10-29 的推文 查看spacy配置方法</p>
<p>初始化模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">import</span> <span class="nn">jieba</span>


<span class="n">zh_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;zh_core_web_sm&#34;</span><span class="p">)</span>
<span class="n">bertModel</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">zh_model</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="准备数据">准备数据</h2>
<p>中文测试数据需要先分词，而后构造成类英文的语言结构(用空格间隔的文本)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 测试数据</span>
<span class="n">doc</span> <span class="o">=</span>  <span class="s2">&#34;&#34;&#34;时值10月25日抗美援朝纪念日，《长津湖》片方发布了“纪念中国人民志愿军抗美援朝出国作战71周年特别短片”，再次向伟大的志愿军致敬！
</span><span class="s2">　　电影《长津湖》全情全景地还原了71年前抗美援朝战场上那场史诗战役，志愿军奋不顾身的英勇精神令观众感叹：“岁月峥嵘英雄不灭，丹心铁骨军魂永存！”影片上映以来票房屡创新高，目前突破53亿元，暂列中国影史票房总榜第三名。
</span><span class="s2">　　值得一提的是，这部影片的很多主创或有军人的血脉，或有当兵的经历，或者家人是军人。提起这些他们也充满自豪，影片总监制黄建新称：“当兵以后会有一种特别能坚持的劲儿。”饰演雷公的胡军透露：“我父亲曾经参加过抗美援朝，还得了一个三等功。”影片历史顾问王树增表示：“我当了五十多年的兵，我的老部队就是上甘岭上下来的，那些老兵都是我的偶像。”
</span><span class="s2">　　“身先士卒卫华夏家国，血战无畏护山河无恙。”片中饰演七连连长伍千里的吴京感叹：“要永远记住这些先烈们，他们给我们带来今天的和平。感谢他们的付出，才让我们有今天的幸福生活。”饰演新兵伍万里的易烊千玺表示：“战争的残酷、碾压式的伤害，其实我们现在的年轻人几乎很难能体会到，希望大家看完电影后能明白，是那些先辈们的牺牲奉献，换来了我们的现在。”
</span><span class="s2">　　影片对战争群像的恢弘呈现，对个体命运的深切关怀，令许多观众无法控制自己的眼泪，观众称：“当看到影片中的惊险战斗场面，看到英雄们壮怀激烈的拼杀，为国捐躯的英勇无畏和无悔付出，我明白了为什么说今天的幸福生活来之不易。”（记者 王金跃）
</span><span class="s2">        &#34;&#34;&#34;</span>


<span class="n">doc</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>


<span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">keywords</span>
</code></pre></div><pre><code>[('铁骨', 0.5028),
 ('纪念日', 0.495),
 ('丹心', 0.4894),
 ('战役', 0.4869),
 ('影史', 0.473),
 ('父亲', 0.4576),
 ('票房', 0.4571),
 ('偶像', 0.4497),
 ('精神', 0.4436),
 ('家国', 0.4373)]
</code></pre>
<br>
<h2 id="常用参数">常用参数</h2>
<p><strong>bertModel.extract_keywords(docs, keyphrase_ngram_range, stop_words, top_n)</strong></p>
<ul>
<li><strong>docs</strong> 文档字符串（空格间隔词语的字符串）</li>
<li><strong>keyphrase_ngram_range</strong> 设置ngram，默认(1, 1)</li>
<li><strong>stop_words</strong> 停用词列表</li>
<li><strong>top_n</strong> 显示前n个关键词，默认5</li>
<li><strong>highlight</strong> 可视化标亮关键词，默认False</li>
<li>use_maxsum: 默认False;是否使用Max Sum Similarity作为关键词提取标准，</li>
<li>use_mmr: 默认False;是否使用Maximal Marginal Relevance (MMR) 作为关键词提取标准</li>
<li>diversity 如果use_mmr=True，可以设置该参数。参数取值范围从0到1</li>
</ul>
<br>
<p>对于<strong>keyphrase_ngram_range</strong>参数，</p>
<ul>
<li>(1, 1) 只单个词， 如&quot;抗美援朝&quot;, &ldquo;纪念日&quot;是孤立的两个词</li>
<li>(2, 2) 考虑词组， 如出现有意义的词组 &ldquo;抗美援朝 纪念日&rdquo;</li>
<li>(1, 2) 同时考虑以上两者情况</li>
</ul>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">diversity</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> 
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">keywords</span>
</code></pre></div><pre><code>[('影片 总监制', 0.5412),
 ('丹心 铁骨', 0.5339),
 ('抗美援朝 纪念日', 0.5295),
 ('长津湖 片方', 0.5252),
 ('志愿军 致敬', 0.5207),
 ('老兵 偶像', 0.5192),
 ('票房 创新', 0.5108),
 ('军人 血脉', 0.5084),
 ('家国 血战', 0.4946),
 ('家人 军人', 0.4885)]
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#可视化</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">highlight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="highlight.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">use_mmr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">diversity</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> 
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">keywords</span>
</code></pre></div><pre><code>[('影片 总监制', 0.5412),
 ('长津湖 片方', 0.5252),
 ('抗美援朝 纪念日', 0.5295),
 ('丹心 铁骨', 0.5339),
 ('志愿军 致敬', 0.5207),
 ('老兵 偶像', 0.5192),
 ('票房 创新', 0.5108),
 ('军人 血脉', 0.5084),
 ('家国 血战', 0.4946),
 ('家人 军人', 0.4885)]
</code></pre>
<br>
<h2 id="英文keybert">英文KeyBERT</h2>
<p>同样需要配置spacy，参考<strong>公众号：大邓和他的Python</strong> 2021-10-29 的推文 查看spacy配置方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">en_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;en_core_web_sm&#34;</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">         Supervised learning is the machine learning task of learning a function that
</span><span class="s2">         maps an input to an output based on example input-output pairs. It infers a
</span><span class="s2">         function from labeled training data consisting of a set of training examples.
</span><span class="s2">         In supervised learning, each example is a pair consisting of an input object
</span><span class="s2">         (typically a vector) and a desired output value (also called the supervisory signal). 
</span><span class="s2">         A supervised learning algorithm analyzes the training data and produces an inferred function, 
</span><span class="s2">         which can be used for mapping new examples. An optimal scenario will allow for the 
</span><span class="s2">         algorithm to correctly determine the class labels for unseen instances. This requires 
</span><span class="s2">         the learning algorithm to generalize from the training data to unseen situations in a 
</span><span class="s2">         &#39;reasonable&#39; way (see inductive bias).
</span><span class="s2">      &#34;&#34;&#34;</span>
<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">kw_model</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">keywords</span>
</code></pre></div><p>Run</p>
<pre><code>[('supervised learning', 0.6779),
 ('supervised', 0.6676),
 ('signal supervised', 0.6152),
 ('examples supervised', 0.6112),
 ('labeled training', 0.6013)]
</code></pre>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>BERTopic库 | 使用预训练模型做话题建模</title>
      <link>https://textdata.cn/blog/bertopic_tutorial/</link>
      <pubDate>Tue, 26 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/bertopic_tutorial/</guid>
      <description>使用BERT主题建模技术,可以对经管等领域文本数据进行主题(话题)建模。效果堪比LDA，但比LDA智能</description>
      <content:encoded><![CDATA[<p>BERT是自然语言处理领域最新的词向量技术，而BERTopic 是基于BERT词向量进行主题建模技术，它利用 Transformer 和 c-TF-IDF 来创建密集的集群，允许轻松解释主题，同时在主题描述中保留重要词。</p>
<p>BERTopic亮点</p>
<ul>
<li>支持引导式Guided</li>
<li>支持（半）监督式</li>
<li>支持动态主题。</li>
<li>支持可视化</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">bertopic</span><span class="o">==</span><span class="mf">0.10.0</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">cntext</span><span class="o">==</span><span class="mf">1.6.5</span>
</code></pre></div><p><br><br></p>
<h2 id="准备数据">准备数据</h2>
<p>这里使用的新闻数据集， 共2000条。 新闻类别涵 <code>'娱乐', '教育', '游戏', '财经', '时政', '时尚', '科技', '体育', '家居', '房产'</code>
这里假设大家不知道有10类新闻题材， 构建模型的时候不会用到label字段的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;cnews.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 新闻题材</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>

<span class="c1">#记录数</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<pre><code>['娱乐' '教育' '游戏' '财经' '时政' '时尚' '科技' '体育' '家居' '房产']
2000
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 各类题材的新闻记录数</span>
<span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">时政    120
科技    106
时尚    106
财经    105
家居    103
教育     97
娱乐     96
体育     95
房产     87
游戏     85
</code></pre></div><br>
<p>这里定义了一个清洗数据函数clean_text，需要注意BERTopic需要先将中文分词改造成类似英文文本格式（用空格间隔词语）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;STOPWORDS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;STOPWORDS&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">test</span> <span class="o">=</span> <span class="s2">&#34;云南永善县级地震已致人伤间民房受损中新网月日电据云南昭通市防震减灾局官方网站消息截至日时云南昭通永善县级地震已造成人受伤其中重伤人轻伤人已全部送医院救治民房受损户间倒塌户间个乡镇所学校不同程度受损目前被损毁电力交通通讯设施已全部抢通修复当地已调拨帐篷顶紧急转移万人月日时分云南昭通永善县发生里氏级地震震源深度公里当地震感强烈此外成都等四川多地也有明显震感&#34;</span>

<span class="n">clean_text</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&#39;云南 永善县 级 地震 已致 伤间 民房 受损 中新网 日电 云南 昭通市 防震 减灾 局 官方网站 消息 日时 云南 昭通 永善县 级 地震 造成 受伤 重伤 轻伤 送 医院 救治 民房 受损 户间 倒塌 户间 乡镇 学校 不同 程度 受损 目前 损毁 电力 交通 通讯 设施 抢通 修复 调拨 帐篷 顶 紧急 转移 万人 时分 云南 昭通 永善县 发生 里氏 级 地震 震源 深度 公里 震感 强烈 成都 四川 多地 明显 震感&#39;
</code></pre></div><p>对2000条数据进行clean_text，得到的结果存储到content字段中。</p>
<p>我的macbook内存16G, 运行时间10s</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="训练topic模型">训练Topic模型</h2>
<p>文本分析步骤包括构建特征工程和训练，在本文中，直接使用开源的预训练中文词向量，省去了特征模型的学习时间。</p>
<p>选取的与训练模型均为word2vec格式，这样方便我们使用gensim将其导入。</p>
<table>
<thead>
<tr>
<th>模型名</th>
<th>数据</th>
<th>预训练模型资源地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>sgns.zhihu.words.bz2</td>
<td>知乎</td>
<td>链接: <a href="https://pan.baidu.com/s/1BDxP28KL_23Odj9NWZGe-Q">https://pan.baidu.com/s/1BDxP28KL_23Odj9NWZGe-Q</a> 提取码: n1qq</td>
</tr>
<tr>
<td>sgns.wiki.words.bz2</td>
<td>中文维基百科</td>
<td>链接: <a href="https://pan.baidu.com/s/1B1sxHmPeIPJYiCuP1zrmMw">https://pan.baidu.com/s/1B1sxHmPeIPJYiCuP1zrmMw</a> 提取码: hofj</td>
</tr>
<tr>
<td>sgns.financial.words.bz2</td>
<td>金融</td>
<td>链接: <a href="https://pan.baidu.com/s/1L_hmGjZMY2ExBn9Vfc_eRg">https://pan.baidu.com/s/1L_hmGjZMY2ExBn9Vfc_eRg</a> 提取码: hhn6</td>
</tr>
<tr>
<td>sgns.renmin.words.bz2</td>
<td>人民日报</td>
<td>链接: <a href="https://pan.baidu.com/s/1VQIDrwZH3Y3Lpy4-smPutw">https://pan.baidu.com/s/1VQIDrwZH3Y3Lpy4-smPutw</a> 提取码: 3b53</td>
</tr>
<tr>
<td>sgns.sougou.words.bz2</td>
<td>搜狗新闻</td>
<td>链接: <a href="https://pan.baidu.com/s/15nCaeB41mwK0ZVLrukXpFQ">https://pan.baidu.com/s/15nCaeB41mwK0ZVLrukXpFQ</a> 提取码: 04en</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Note</strong>:</p>
<p>除了表格外的资源，还可以使用spacy现有的预训练模型。</p>
</blockquote>
<p>本文案例cnews.csv是新闻类数据，这里最好选择使用同样为新闻题材的文本训练出的模型，这样BERTopic效果会更精准一些。sgns.sougou.words.bz2是使用搜狗新闻数据训练的语言模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">chinese_sougou_news_models</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;sgns.sogou.word.bz2&#39;</span><span class="p">,</span> <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">chinese_sougou_news_models</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x7f93e5b8cc10&gt;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>


<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="s2">&#34;chinese (simplified)&#34;</span><span class="p">,</span> 
                       <span class="n">embedding_model</span><span class="o">=</span><span class="n">chinese_sougou_news_models</span><span class="p">,</span>
                       <span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                       <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">docs</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1">#2000条进行fit_transform需要1min</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div><pre><code>100%|██████████| 2000/2000 [01:31&lt;00:00, 21.91it/s]
2021-10-28 12:11:25,583 - BERTopic - Transformed documents to Embeddings
2021-10-28 12:11:34,582 - BERTopic - Reduced dimensionality with UMAP
2021-10-28 12:11:34,718 - BERTopic - Clustered UMAP embeddings with HDBSCAN


CPU times: user 1min 50s, sys: 7.7 s, total: 1min 57s
Wall time: 1min 43s
</code></pre>
<p><br><br></p>
<h2 id="主题模型方法">主题模型方法</h2>
<ul>
<li>topic_model.get_topic_info 查看各主题信息</li>
<li>topic_model.find_topics(term, top_n=5)  查找term最有可能所属话题</li>
<li>topic_model.get_topic(0) 查看Topic 0的特征词</li>
<li>topic_model.visualize_topics() 话题间距离的可视化</li>
<li>topic_model.visualize_distribution(probs[0]) 查看某条文本的主题分布</li>
<li>topic_model.visualize_hierarchy(top_n_topics=20) 主题层次聚类可视化</li>
<li>topic_model.visualize_barchart(topics=[1]) 显示主题1的词条形图</li>
<li>topic_model.visualize_heatmap(n_clusters=10) 主题相似度热力图</li>
<li>topic_model.visualize_term_rank() 可视化词语</li>
<li>topic_model.save()  保存主题模型</li>
<li>topic_model.reduce_topics()  压缩主题个数(合并相近的主题)</li>
</ul>
<h3 id="get_topic_info">.get_topic_info()</h3>
<p>查看BERTopic基于cnews.csv数据， 跑出的各主题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_info</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/get_topic_info.png" alt=""  />
</p>
<br>
<h3 id="find_topicsterm">.find_topics(term)</h3>
<p>查看与词语【投资】最相关的主题，返回候选的最相思的5个主题id</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#</span>
<span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&#34;投资&#34;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">similar_topics</span>
</code></pre></div><p>Run</p>
<pre><code>[3, 9, 8, 10, 4]
</code></pre>
<br>
<h3 id="get_topic">.get_topic()</h3>
<p>查看id为3的主题信息（主题词及权重）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;基金&#39;, 0.15109221307919193),
 (&#39;投资&#39;, 0.042856192509064),
 (&#39;公司&#39;, 0.039785278320496976),
 (&#39;市场&#39;, 0.037072163603417835),
 (&#39;股票&#39;, 0.03230913401086524),
 (&#39;型基金&#39;, 0.02721898070238429),
 (&#39;收益&#39;, 0.025435672141638468),
 (&#39;投资者&#39;, 0.024633503649868493),
 (&#39;经理&#39;, 0.02458550023931051),
 (&#39;发行&#39;, 0.022672639068067168)]
</code></pre></div><br>
<h3 id="visualize_topics">.visualize_topics()</h3>
<p>可视化主题间距离</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">visualize_topics1</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">()</span>
<span class="c1">#可视化结果保存至html中，可以动态显示信息</span>
<span class="n">visualize_topics1</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;visualize_topics.html&#39;</span><span class="p">)</span>
<span class="n">visualize_topics1</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_topics1.png" alt=""  />
</p>
<p><a href="img/visualize_topics1.html">点击查看visualize_topics1.html</a></p>
<br>
<h3 id="visualize_distribution">.visualize_distribution()</h3>
<p>显示第一条新闻的主题概率分布</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">first_new_topic_probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_distribution</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">first_new_topic_probs</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;first_new_topic_probs.html&#39;</span><span class="p">)</span>
<span class="n">first_new_topic_probs</span>
</code></pre></div><p><img loading="lazy" src="img/first_new_topic_probs.png" alt=""  />

<a href="img/first_new_topic_probs.html">点击查看first_new_topic_probs.html</a></p>
<p>为了理解主题的潜在层次结构，我们可以使用 scipy.cluster.hierarchy 创建聚类并可视化它们之间的关系。 这有助于合并相似主题，达到降低主题模型主题数量nr_topics。</p>
<br>
<h3 id="visualize_hierarchytop_n_topics">.visualize_hierarchy(top_n_topics)</h3>
<p>话题层次聚类可视化，模型跑出12个主题，这里就按12进行分层聚类</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_hierarchy</span><span class="p">(</span><span class="n">top_n_topics</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_hierarchy.png" alt=""  />
</p>
<br>
<h3 id="visualize_barcharttopics">.visualize_barchart(topics)</h3>
<p>显示topics的词条形图</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_barchart</span><span class="p">(</span><span class="n">topics</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_barchart.png" alt=""  />
</p>
<br>
<h3 id="visualize_heatmapn_clusters">.visualize_heatmap(n_clusters)</h3>
<p>话题相似热力图。BERTopic可将主题以embeddings形式（向量）表示， 因此我们可以应用余弦相似度来创建相似度矩阵。 每两两主题可进行余弦计算，最终结果将是一个矩阵，显示主题间的相似程度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_similar_heatmap</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_heatmap</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">topic_similar_heatmap</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;topic_similar_heatmap.html&#39;</span><span class="p">)</span>
<span class="n">topic_similar_heatmap</span>
</code></pre></div><p><img loading="lazy" src="img/topic_similar_heatmap.png" alt=""  />

<a href="img/topic_similar_heatmap.html">点击查看topic_similar_heatmap.html</a></p>
<p>通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。 从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。 此外，可以轻松地将主题表示相互比较。</p>
<br>
<h3 id="visualize_term_rank">.visualize_term_rank()</h3>
<p>通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。</p>
<p>从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。</p>
<p>此外，可以轻松地将主题表示相互比较。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">term_score_decline</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_term_rank</span><span class="p">()</span>
<span class="n">term_score_decline</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;term_score_decline.html&#39;</span><span class="p">)</span>
<span class="n">term_score_decline</span>
</code></pre></div><p><img loading="lazy" src="img/term_score_decline.png" alt=""  />

<a href="img/term_score_decline.html">点击查看term_score_decline.html</a></p>
<h3 id="update_topics">.update_topics()</h3>
<p>更新主题模型。当您训练了一个模型并查看了代表它们的主题和单词时，您可能对表示不满意。 也许您忘记删除停用词，或者您想尝试不同的 n_gram_range。 我们可以使用函数 update_topics 使用 c-TF-IDF 的新参数更新主题表示。</p>
<p>使用.update_topics()更新，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">update_topics</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div><p>topic_model得到了更新，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&#34;手机&#34;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">similar_topics</span>
</code></pre></div><p>Run</p>
<pre><code>[2, 7, 4, 1, 5]
</code></pre>
<p>查看话题2的信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;功能&#39;, 0.022132351014298786),
 (&#39;采用&#39;, 0.02136925357979149),
 (&#39;像素&#39;, 0.020797285140907094),
 (&#39;拍摄&#39;, 0.017850841110848677),
 (&#39;机身&#39;, 0.015056931248982912),
 (&#39;英寸&#39;, 0.014624438184138326),
 (&#39;佳能&#39;, 0.012857768505732597),
 (&#39;支持&#39;, 0.012600856600766349),
 (&#39;光学&#39;, 0.012462085658291079),
 (&#39;相机&#39;, 0.011832978982454568)]
</code></pre></div><p>模型保存</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Save model</span>
<span class="c1">#model.save(&#34;my_model&#34;)</span>
<span class="c1"># Load model</span>
<span class="c1">#my_model = BERTopic.load(&#34;my_model&#34;)</span>
</code></pre></div><br>
<h3 id="reduce_topics">.reduce_topics()</h3>
<p>压缩主题数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">reduce_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>2021-10-28 12:28:01,976 - BERTopic - Reduced number of topics from 20 to 11
</code></pre>
<br>
<h2 id="代码数据">代码数据</h2>
<p><a href="bertopic_tutorial.zip">click to download</a></p>
<br>
<h2 id="总结">总结</h2>
<p>本文使用中文文本数据展示BERTopic部分功能，如果对英文数据感兴趣，可以前往  <a href="https://github.com/MaartenGr/BERTopic">https://github.com/MaartenGr/BERTopic</a> 深入学习。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
