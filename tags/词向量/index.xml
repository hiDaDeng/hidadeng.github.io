<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>词向量 on 大邓和他的PYTHON</title>
    <link>/tags/%E8%AF%8D%E5%90%91%E9%87%8F/</link>
    <description>Recent content in 词向量 on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Fri, 24 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E8%AF%8D%E5%90%91%E9%87%8F/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>预训练模型 | 金融会计类word2vec， 可扩展或构建领域内概念情感词典</title>
      <link>https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/</link>
      <pubDate>Fri, 24 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/</guid>
      <description>&lt;h2 id=&#34;一介绍&#34;&gt;一、介绍&lt;/h2&gt;
&lt;p&gt;使用2001-2021年的&lt;strong&gt;管理层讨论与分析mda&lt;/strong&gt;数据(1.45G)，训练出的中国A股市场词向量模型，词汇量789539， 模型文件650M。可广泛用于经济管理等领域概念(情感)词典的构建或扩展。&lt;/p&gt;
&lt;p&gt;训练环境为内存256G的windows服务器(日常办公电脑内存16G居多)， 2.0.0版本cntext库(该版本暂不开源，最新可获取的版本为1.8.4)。在该环境下，我也尝试使用14G的年报数据，训练了两天，跑不出结果，256G的内存基本用光了。所以cntext训练模型，适合的数据规模是1G左右。模型文件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;mda01-21.200.6.bin&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mda01-21.200.6.bin.vectors.npy&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;pretained-screen.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;参数解读&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mda01-21 使用2001-2021年度的mda数据训练&lt;/li&gt;
&lt;li&gt;200 嵌入的维度数，即每个词的向量长度是200&lt;/li&gt;
&lt;li&gt;6 词语上下文的窗口是6&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为什么这样确定200和6，可以看这篇 &lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;二导入模型&#34;&gt;二、导入模型&lt;/h2&gt;
&lt;p&gt;需要用到两个自定义函数load_w2v、expand_dictionary，源代码太长，为了提高阅读体验， 放在文末。大家记得用这两个函数前一定要先导入。&lt;a href=&#34;mda_pretained_model_code.ipynb&#34;&gt;&lt;strong&gt;点击下载本文&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#先导入load_w2v、expand_dictionary函数源代码&lt;/span&gt;


&lt;span class=&#34;c1&#34;&gt;#读取模型文件&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;model/mda01-21.200.6.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Loading word2vec model...
&amp;lt;gensim.models.keyedvectors.KeyedVectors at 0x7fcc91900a90&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;三wv的使用&#34;&gt;三、wv的使用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;查看词汇量&lt;/li&gt;
&lt;li&gt;查询某词向量&lt;/li&gt;
&lt;li&gt;查看多个词的均值向量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更多内容，建议查看下gensim库的文档&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#词汇量&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;789539
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#查询某词的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([ 4.34389877e+00, -4.93447453e-01,  2.17293240e-02,  1.90846980e+00,
        8.75901580e-01, -7.95542181e-01, -1.12950909e+00,  7.44228721e-01,
        7.38122821e-01,  6.42377853e-01,  3.99175316e-01,  2.17924786e+00,
        9.30410504e-01, -3.23538423e+00, -2.91860670e-01,  1.04046893e+00,
       -1.73857129e+00, -1.12141383e+00,  3.51870751e+00, -8.69141936e-01,
        4.95228887e-01,  4.80194688e-01, -3.35257435e+00,  7.16054797e-01,
        2.29016230e-01,  2.40962386e+00, -7.40825295e-01,  2.18998361e+00,
       -3.37587762e+00, -1.30376315e+00,  5.08445930e+00, -1.68504322e+00,
       -1.60081315e+00, -8.33779454e-01, -7.58818448e-01, -1.78838921e+00,
        2.44672084e+00,  2.27579999e+00, -2.52457595e+00,  1.36214256e-01,
       -3.09675723e-01, -6.98232710e-01,  1.73018420e+00, -8.05342972e-01,
       -1.70148358e-01, -2.43612671e+00, -1.23085886e-01,  2.83124876e+00,
        3.89446110e-01, -3.16048344e-03, -2.09607935e+00, -1.49788404e+00,
        8.58029604e-01, -1.26923633e+00,  1.86084434e-01,  9.13471103e-01,
        1.53111053e+00, -2.57916182e-01,  1.83742964e+00,  1.50475979e+00,
        6.84375539e-02,  2.76320624e+00,  1.02619076e+00,  9.41017449e-01,
        1.66149962e+00, -2.49254084e+00, -7.78038025e-01, -6.52620196e-01,
       -1.59455287e+00, -4.13568115e+00,  2.78383470e+00, -5.71591198e-01,
       -8.45031738e-01,  4.54110718e+00,  1.67990357e-01,  2.12474012e+00,
       -2.25404716e+00, -8.35567772e-01,  9.91619170e-01, -2.55307484e+00,
        2.39850569e+00,  7.65280128e-01,  2.64600372e+00,  2.58998632e-01,
       -6.56729996e-01, -1.55601549e+00,  1.49751770e+00,  8.47311080e-01,
       -2.05665565e+00, -1.14815748e+00,  1.97350585e+00,  1.02964830e+00,
       -3.87644440e-01, -9.38048363e-01, -2.55545706e-01, -7.02206418e-03,
       -2.94358826e+00, -7.96167493e-01,  1.59571424e-01,  1.25497723e+00,
        7.12080002e-01, -1.34656525e+00,  1.54059935e+00, -1.12930894e+00,
       -3.66737366e+00, -7.17270374e-01, -2.69604278e+00,  1.90242791e+00,
        9.33268607e-01, -4.67624277e-01,  3.51641893e+00,  5.66355512e-02,
       -1.31763351e+00,  1.53379011e+00,  2.32190108e+00, -5.21186776e-02,
        4.06406015e-01,  4.48809415e-01, -3.68958092e+00, -1.01650321e+00,
       -1.08470261e+00, -1.93710685e+00,  2.27287245e+00, -6.63952589e-01,
        1.88207674e+00, -1.20226216e+00,  1.08953261e+00,  1.32847381e+00,
        1.38213491e+00,  1.47196710e+00, -2.06643629e+00,  1.99588931e+00,
       -1.64155555e+00, -2.24964902e-01, -2.74115324e+00, -3.16747665e+00,
        1.24095821e+00, -4.10616726e-01, -3.48466903e-01,  1.38452172e+00,
       -1.45676279e+00, -3.54911834e-02, -4.73554075e-01, -4.23114252e+00,
        1.52749741e+00,  7.25808144e-01, -4.50003862e-01, -3.16014004e+00,
        2.60309219e+00, -2.11320925e+00,  3.61347020e-01,  1.73625088e+00,
        1.57609022e+00, -2.08762145e+00,  2.18810892e+00,  1.20706499e+00,
       -1.82370770e+00,  1.22358835e+00, -8.91464829e-01, -3.30527711e+00,
       -3.72515142e-01, -6.23329699e-01,  8.11975658e-01, -8.52464736e-01,
       -9.35325995e-02, -4.06904364e+00,  1.57146180e+00,  7.85030201e-02,
        1.94540334e+00,  2.13809991e+00, -1.58913553e+00, -3.81727874e-01,
       -2.08527303e+00,  5.89691937e-01,  2.55564898e-01,  2.38364622e-01,
        3.64680409e+00,  4.18930590e-01,  1.62034535e+00, -4.63252217e-02,
        5.80206394e-01,  5.55441022e-01,  1.91946900e+00, -1.89253080e+00,
        1.77489519e+00, -3.15311766e+00,  6.48138940e-01,  1.15823770e+00,
       -2.54519200e+00, -1.03516951e-01,  1.15724599e+00, -1.83681571e+00,
       -9.87860620e-01, -1.99984312e+00,  2.76547909e-01,  8.02748859e-01,
        1.99196494e+00, -1.43310416e+00, -2.03039408e+00, -7.19777197e-02],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#查询多个词的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_mean_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;研发&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Ruj&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;array([ 0.17623448, -0.02220692, -0.01040847,  0.03616136,  0.04931263,
       -0.06220303, -0.02846557, -0.00156435,  0.04524047,  0.03185674,
        0.01104859,  0.06962118, -0.01969986, -0.10831943, -0.0524368 ,
        0.00623383, -0.04149605, -0.004912  ,  0.13154642, -0.04317038,
       -0.00407438,  0.00923527, -0.13339072,  0.01446994, -0.00153984,
        0.12378754, -0.06064663,  0.09322313, -0.07711462, -0.05880795,
        0.13697049,  0.0133168 ,  0.02769322,  0.02677607,  0.02549294,
       -0.04504526,  0.06267191,  0.02421109, -0.13401456,  0.01423616,
        0.01860182,  0.00344108,  0.04811918,  0.02748652,  0.0190251 ,
       -0.03800797,  0.01517046,  0.06439836,  0.01320594,  0.04748138,
       -0.08914943, -0.00642068,  0.01786153, -0.02472607, -0.04597819,
        0.05832303,  0.11275461, -0.0387079 ,  0.06912261,  0.05287468,
       -0.04447906,  0.10994074, -0.04371417,  0.01227543,  0.07498093,
       -0.11285575, -0.03113984, -0.01122221, -0.03913497, -0.12117577,
        0.08593786, -0.04319173, -0.01860389,  0.15636683,  0.02267851,
        0.0922839 , -0.12106322, -0.07572737,  0.0191772 , -0.00977821,
        0.00455545,  0.01378978,  0.04774487, -0.02080727,  0.01015578,
       -0.04695337,  0.0848957 , -0.01112909, -0.03210922,  0.01151857,
        0.02214565,  0.03220333, -0.02468888, -0.07493623, -0.03724978,
       -0.00716823, -0.12043905, -0.0560291 , -0.00666756,  0.03659805,
        0.0532646 , -0.05371486,  0.06905847,  0.00660356, -0.10362111,
       -0.0015829 , -0.13282564,  0.08241726,  0.00993685,  0.04208402,
        0.03087696,  0.04765649, -0.00834742,  0.07236902,  0.04473683,
       -0.02643864, -0.0050621 ,  0.04462356, -0.0832998 , -0.05533891,
        0.00664944, -0.13001585,  0.07607447, -0.00764748,  0.01410657,
       -0.03057465,  0.0250505 ,  0.09252612, -0.00784517,  0.0386237 ,
       -0.059011  ,  0.05357389, -0.04604931,  0.04388874, -0.0971131 ,
       -0.09777305,  0.02943253, -0.04103448, -0.03944859,  0.09638489,
       -0.02226706,  0.02822194, -0.0093646 , -0.11203568,  0.06142627,
        0.04761236,  0.02720375, -0.09777595,  0.04048391, -0.06758034,
       -0.01500905,  0.02439078,  0.07150253, -0.02562411,  0.02533657,
        0.00799897, -0.06416934,  0.03153701, -0.03944302, -0.04653639,
       -0.04123383, -0.01590026,  0.03051148, -0.02014856, -0.01448381,
       -0.10517117, -0.00649814,  0.02478252,  0.02855514,  0.09052269,
       -0.03505059, -0.03173327, -0.06641324,  0.06284194,  0.01993516,
        0.01349441,  0.1410133 , -0.05283241,  0.03687092, -0.02535007,
        0.00415636,  0.05841105,  0.07389537, -0.13176979,  0.06759793,
       -0.092868  ,  0.01370211,  0.06616284, -0.09137756, -0.01640504,
        0.06095972, -0.05725639, -0.04122292,  0.00598698,  0.02904861,
        0.0442962 ,  0.07399555, -0.04657119, -0.07636161,  0.03204561],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;有了每个词或者概念的向量，可以结合cntext旧版本单语言模型内的态度偏见的度量。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四扩展词典&#34;&gt;四、扩展词典&lt;/h2&gt;
&lt;p&gt;做词典法的文本分析，最重要的是有自己的领域词典。之前受限于技术难度，文科生的我也一直在用形容词的通用情感词典。现在依托word2vec技术， 可以加速人工构建的准确率和效率。&lt;/p&gt;
&lt;p&gt;下面是在 mda01-21.200.6.bin 上做的词典扩展测试，函数expand_dictionary会根据种子词选取最准确的topn个词。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#短视主义词  实验&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;抓紧&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;立刻&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;月底&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;年底&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;年终&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;争取&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;力争&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;抓紧&#39;,
 &#39;立刻&#39;,
 &#39;月底&#39;,
 &#39;年底&#39;,
 &#39;年终&#39;,
 &#39;争取&#39;,
 &#39;力争&#39;,
 &#39;争取&#39;,
 &#39;力争&#39;,
 &#39;年内&#39;,
 &#39;月底&#39;,
 &#39;年底&#39;,
 &#39;尽早&#39;,
 &#39;3月底&#39;,
 &#39;尽快&#39;,
 &#39;抓紧&#39;,
 &#39;6月份&#39;,
 &#39;4月份&#39;,
 &#39;月份&#39;,
 &#39;工作力争&#39;,
 &#39;努力争取&#39;,
 &#39;工作争取&#39;,
 &#39;10月底&#39;,
 &#39;年内实现&#39;,
 &#39;年底完成&#39;,
 &#39;中旬&#39;,
 &#39;7月份&#39;,
 &#39;9月底&#39;,
 &#39;有望&#39;,
 &#39;月底前&#39;,
 &#39;早日&#39;,
 &#39;全力&#39;,
 &#39;继续&#39;,
 &#39;月初&#39;,
 &#39;努力&#39;,
 &#39;确保&#39;,
 &#39;8月份&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;团结&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;拼搏&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;克服&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;勇攀高峰&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;友善&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;进取&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;团结&#39;,
 &#39;拼搏&#39;,
 &#39;克服&#39;,
 &#39;勇攀高峰&#39;,
 &#39;友善&#39;,
 &#39;进取&#39;,
 &#39;拼搏&#39;,
 &#39;艰苦奋斗&#39;,
 &#39;坚定信念&#39;,
 &#39;团结拼搏&#39;,
 &#39;上下同心&#39;,
 &#39;团结&#39;,
 &#39;顽强拼搏&#39;,
 &#39;勇于担当&#39;,
 &#39;团结一致&#39;,
 &#39;团结奋进&#39;,
 &#39;精诚团结&#39;,
 &#39;齐心协力&#39;,
 &#39;开拓进取&#39;,
 &#39;奋进&#39;,
 &#39;团结一心&#39;,
 &#39;实干&#39;,
 &#39;同心协力&#39;,
 &#39;团结协作&#39;,
 &#39;锐意进取&#39;,
 &#39;积极进取&#39;,
 &#39;奋力拼搏&#39;,
 &#39;拼搏精神&#39;,
 &#39;努力拼搏&#39;,
 &#39;进取&#39;,
 &#39;奋发有为&#39;,
 &#39;扎实工作&#39;,
 &#39;同心同德&#39;,
 &#39;拼搏进取&#39;,
 &#39;脚踏实地&#39;,
 &#39;励精图治&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;科技&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;研发&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;技术&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;标准&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;创新&#39;,
 &#39;科技&#39;,
 &#39;研发&#39;,
 &#39;技术&#39;,
 &#39;标准&#39;,
 &#39;创新&#39;,
 &#39;技术创新&#39;,
 &#39;技术研发&#39;,
 &#39;科技创新&#39;,
 &#39;先进技术&#39;,
 &#39;自主创新&#39;,
 &#39;前沿技术&#39;,
 &#39;关键技术&#39;,
 &#39;科研&#39;,
 &#39;新技术&#39;,
 &#39;创新性&#39;,
 &#39;研发创新&#39;,
 &#39;产品研发&#39;,
 &#39;基础研究&#39;,
 &#39;产品开发&#39;,
 &#39;集成创新&#39;,
 &#39;核心技术&#39;,
 &#39;自主研发&#39;,
 &#39;技术应用&#39;,
 &#39;技术集成&#39;,
 &#39;前沿科技&#39;,
 &#39;技术标准&#39;,
 &#39;工艺技术&#39;,
 &#39;科技成果&#39;,
 &#39;技术开发&#39;,
 &#39;尖端技术&#39;,
 &#39;工程技术&#39;,
 &#39;技术相结合&#39;,
 &#39;科学技术&#39;,
 &#39;工艺&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;竞争&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;竞争力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;竞争&#39;,
 &#39;竞争力&#39;,
 &#39;竞争能力&#39;,
 &#39;竞争优势&#39;,
 &#39;市场竞争&#39;,
 &#39;竞&#39;,
 &#39;市场竞争力&#39;,
 &#39;竞争实力&#39;,
 &#39;参与市场竞争&#39;,
 &#39;国际竞争&#39;,
 &#39;市场竞争能力&#39;,
 &#39;核心竞争力&#39;,
 &#39;激烈竞争&#39;,
 &#39;市场竞争优势&#39;,
 &#39;竞争态势&#39;,
 &#39;参与竞争&#39;,
 &#39;竞争力重要&#39;,
 &#39;竞争对手&#39;,
 &#39;创新能力&#39;,
 &#39;综合竞争力&#39;,
 &#39;价格竞争&#39;,
 &#39;之间竞争&#39;,
 &#39;核心竞争能力&#39;,
 &#39;未来市场竞争&#39;,
 &#39;国际竞争力&#39;,
 &#39;影响力竞争力&#39;,
 &#39;国际化竞争&#39;,
 &#39;行业竞争&#39;,
 &#39;综合竞争能力&#39;,
 &#39;竞争日趋激烈&#39;,
 &#39;产品竞争力&#39;,
 &#39;竞争力影响力&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;疫情&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;扩散&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;防控&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;反复&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;冲击&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;疫情&#39;,
 &#39;扩散&#39;,
 &#39;防控&#39;,
 &#39;反复&#39;,
 &#39;冲击&#39;,
 &#39;蔓延&#39;,
 &#39;疫情冲击&#39;,
 &#39;疫情爆发&#39;,
 &#39;新冠疫情&#39;,
 &#39;新冠肺炎&#39;,
 &#39;疫情蔓延&#39;,
 &#39;疫情暴发&#39;,
 &#39;肆虐&#39;,
 &#39;本次疫情&#39;,
 &#39;冲击疫情&#39;,
 &#39;新冠病毒&#39;,
 &#39;疫情扩散&#39;,
 &#39;全球蔓延&#39;,
 &#39;疫情影响&#39;,
 &#39;病毒疫情&#39;,
 &#39;肺炎疫情&#39;,
 &#39;击&#39;,
 &#39;持续蔓延&#39;,
 &#39;疫情持续&#39;,
 &#39;各地疫情&#39;,
 &#39;疫情突然&#39;,
 &#39;疫情全球&#39;,
 &#39;疫情传播&#39;,
 &#39;疫情反复&#39;,
 &#39;散发&#39;,
 &#39;变异毒株&#39;,
 &#39;疫情导致&#39;,
 &#39;疫情肆虐&#39;,
 &#39;全球疫情&#39;,
 &#39;全球新冠&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                  &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;旧&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;老&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;后&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;落后&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                  &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[&#39;旧&#39;,
 &#39;老&#39;,
 &#39;后&#39;,
 &#39;落后&#39;,
 &#39;旧&#39;,
 &#39;老&#39;,
 &#39;陈旧&#39;,
 &#39;老旧&#39;,
 &#39;淘汰&#39;,
 &#39;高能耗&#39;,
 &#39;低效率&#39;,
 &#39;设备陈旧&#39;,
 &#39;能耗高&#39;,
 &#39;老旧设备&#39;,
 &#39;落后工艺&#39;,
 &#39;进行改造&#39;,
 &#39;工艺落后&#39;,
 &#39;技术落后&#39;,
 &#39;翻新&#39;,
 &#39;更新改造&#39;,
 &#39;改造&#39;,
 &#39;更新&#39;,
 &#39;替换&#39;,
 &#39;改造更新&#39;,
 &#39;旧设备&#39;,
 &#39;污染重&#39;,
 &#39;淘汰一批&#39;,
 &#39;拆除&#39;,
 &#39;污染严重&#39;,
 &#39;简陋&#39;,
 &#39;产能落后&#39;,
 &#39;相对落后&#39;,
 &#39;产能淘汰&#39;,
 &#39;效率低下&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五源代码&#34;&gt;五、源代码&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;gensim.models&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pathlib&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    Load word2vec model
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    Args:
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        w2v_path (str): path of word2vec model
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    Returns:
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        model: word2vec model
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Loading word2vec model...&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    According to the seed word file, select the top n words with the most similar semantics and save them in the directory save_dir.
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    Args:
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        wv (Word2VecKeyedVectors): the word embedding model
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        seedwords (list): 种子词
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        topn (int, optional): Set the number of most similar words to retrieve to topn. Defaults to 100.
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;        save_dir (str, optional): the directory to save the candidate words. Defaults to &amp;#39;Word2Vec&amp;#39;.
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    Returns:
&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;simidx_scores&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;similars_candidate_idxs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#the candidate words of seedwords&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;seedidxs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#transform word to index&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;seedidx&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;seedidxs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seedidx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seedidx&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seedidxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;# sims_words such as [(&amp;#39;by&amp;#39;, 0.99984), (&amp;#39;or&amp;#39;, 0.99982), (&amp;#39;an&amp;#39;, 0.99981), (&amp;#39;up&amp;#39;, 0.99980)]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;sims_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seedidx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;#Convert words to index and store them&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;similars_candidate_idxs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sim&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sims_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;similars_candidate_idxs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similars_candidate_idxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;similars_candidate_idxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seedidxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;simidx_scores&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;score&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;simidxs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;sorted&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;simidx_scores&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reverse&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;simwords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;idx&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;simidxs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;resultwords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;resultwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seedwords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;resultwords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;simwords&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;resultwords&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;获取模型&#34;&gt;获取模型&lt;/h2&gt;
&lt;p&gt;模型训练不易， 为付费资源，如需使用请 &lt;a href=&#34;https://mp.weixin.qq.com/s/zle9-BR-ei1V8Nmul19_7w&#34;&gt;&lt;strong&gt;点击进入跳转购买链接&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;期待合作&#34;&gt;期待合作&lt;/h2&gt;
&lt;p&gt;cntext目前仍在技术迭代，版本2.0.0综合了训练语言模型&amp;amp;多语言模型对齐， 有较大的应用价值，期待有独特文本数据集交流合作。&lt;/p&gt;
&lt;p&gt;通过cntext2.0.0，理论上可以对文本所涉社会主体进行计算，适合企业文化、品牌印象、旅游目的地形象、国家形象等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同主体不同时间段， 文本中蕴含的文化态度认知变迁，&lt;/li&gt;
&lt;li&gt;或同时间段，不同主体的大样本文本蕴含的差异性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;广而告之&#34;&gt;广而告之&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/call_for_paper/&#34;&gt;长期征稿&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/we_need_you/&#34;&gt;长期招募小伙伴&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一介绍">一、介绍</h2>
<p>使用2001-2021年的<strong>管理层讨论与分析mda</strong>数据(1.45G)，训练出的中国A股市场词向量模型，词汇量789539， 模型文件650M。可广泛用于经济管理等领域概念(情感)词典的构建或扩展。</p>
<p>训练环境为内存256G的windows服务器(日常办公电脑内存16G居多)， 2.0.0版本cntext库(该版本暂不开源，最新可获取的版本为1.8.4)。在该环境下，我也尝试使用14G的年报数据，训练了两天，跑不出结果，256G的内存基本用光了。所以cntext训练模型，适合的数据规模是1G左右。模型文件</p>
<ul>
<li><strong>mda01-21.200.6.bin</strong></li>
<li><strong>mda01-21.200.6.bin.vectors.npy</strong></li>
</ul>
<p><img loading="lazy" src="pretained-screen.png" alt=""  />
</p>
<p>参数解读</p>
<ul>
<li>mda01-21 使用2001-2021年度的mda数据训练</li>
<li>200 嵌入的维度数，即每个词的向量长度是200</li>
<li>6 词语上下文的窗口是6</li>
</ul>
<p>为什么这样确定200和6，可以看这篇 <a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></p>
<br>
<br>
<h2 id="二导入模型">二、导入模型</h2>
<p>需要用到两个自定义函数load_w2v、expand_dictionary，源代码太长，为了提高阅读体验， 放在文末。大家记得用这两个函数前一定要先导入。<a href="mda_pretained_model_code.ipynb"><strong>点击下载本文</strong></a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#先导入load_w2v、expand_dictionary函数源代码</span>


<span class="c1">#读取模型文件</span>
<span class="n">wv</span> <span class="o">=</span> <span class="n">load_w2v</span><span class="p">(</span><span class="n">w2v_path</span><span class="o">=</span><span class="s1">&#39;model/mda01-21.200.6.bin&#39;</span><span class="p">)</span>
<span class="n">wv</span>
</code></pre></div><pre><code>Loading word2vec model...
&lt;gensim.models.keyedvectors.KeyedVectors at 0x7fcc91900a90&gt;
</code></pre>
<h3 id="三wv的使用">三、wv的使用</h3>
<ul>
<li>查看词汇量</li>
<li>查询某词向量</li>
<li>查看多个词的均值向量</li>
</ul>
<p>更多内容，建议查看下gensim库的文档</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#词汇量</span>
<span class="nb">len</span><span class="p">(</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>789539
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查询某词的词向量</span>
<span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;创新&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>array([ 4.34389877e+00, -4.93447453e-01,  2.17293240e-02,  1.90846980e+00,
        8.75901580e-01, -7.95542181e-01, -1.12950909e+00,  7.44228721e-01,
        7.38122821e-01,  6.42377853e-01,  3.99175316e-01,  2.17924786e+00,
        9.30410504e-01, -3.23538423e+00, -2.91860670e-01,  1.04046893e+00,
       -1.73857129e+00, -1.12141383e+00,  3.51870751e+00, -8.69141936e-01,
        4.95228887e-01,  4.80194688e-01, -3.35257435e+00,  7.16054797e-01,
        2.29016230e-01,  2.40962386e+00, -7.40825295e-01,  2.18998361e+00,
       -3.37587762e+00, -1.30376315e+00,  5.08445930e+00, -1.68504322e+00,
       -1.60081315e+00, -8.33779454e-01, -7.58818448e-01, -1.78838921e+00,
        2.44672084e+00,  2.27579999e+00, -2.52457595e+00,  1.36214256e-01,
       -3.09675723e-01, -6.98232710e-01,  1.73018420e+00, -8.05342972e-01,
       -1.70148358e-01, -2.43612671e+00, -1.23085886e-01,  2.83124876e+00,
        3.89446110e-01, -3.16048344e-03, -2.09607935e+00, -1.49788404e+00,
        8.58029604e-01, -1.26923633e+00,  1.86084434e-01,  9.13471103e-01,
        1.53111053e+00, -2.57916182e-01,  1.83742964e+00,  1.50475979e+00,
        6.84375539e-02,  2.76320624e+00,  1.02619076e+00,  9.41017449e-01,
        1.66149962e+00, -2.49254084e+00, -7.78038025e-01, -6.52620196e-01,
       -1.59455287e+00, -4.13568115e+00,  2.78383470e+00, -5.71591198e-01,
       -8.45031738e-01,  4.54110718e+00,  1.67990357e-01,  2.12474012e+00,
       -2.25404716e+00, -8.35567772e-01,  9.91619170e-01, -2.55307484e+00,
        2.39850569e+00,  7.65280128e-01,  2.64600372e+00,  2.58998632e-01,
       -6.56729996e-01, -1.55601549e+00,  1.49751770e+00,  8.47311080e-01,
       -2.05665565e+00, -1.14815748e+00,  1.97350585e+00,  1.02964830e+00,
       -3.87644440e-01, -9.38048363e-01, -2.55545706e-01, -7.02206418e-03,
       -2.94358826e+00, -7.96167493e-01,  1.59571424e-01,  1.25497723e+00,
        7.12080002e-01, -1.34656525e+00,  1.54059935e+00, -1.12930894e+00,
       -3.66737366e+00, -7.17270374e-01, -2.69604278e+00,  1.90242791e+00,
        9.33268607e-01, -4.67624277e-01,  3.51641893e+00,  5.66355512e-02,
       -1.31763351e+00,  1.53379011e+00,  2.32190108e+00, -5.21186776e-02,
        4.06406015e-01,  4.48809415e-01, -3.68958092e+00, -1.01650321e+00,
       -1.08470261e+00, -1.93710685e+00,  2.27287245e+00, -6.63952589e-01,
        1.88207674e+00, -1.20226216e+00,  1.08953261e+00,  1.32847381e+00,
        1.38213491e+00,  1.47196710e+00, -2.06643629e+00,  1.99588931e+00,
       -1.64155555e+00, -2.24964902e-01, -2.74115324e+00, -3.16747665e+00,
        1.24095821e+00, -4.10616726e-01, -3.48466903e-01,  1.38452172e+00,
       -1.45676279e+00, -3.54911834e-02, -4.73554075e-01, -4.23114252e+00,
        1.52749741e+00,  7.25808144e-01, -4.50003862e-01, -3.16014004e+00,
        2.60309219e+00, -2.11320925e+00,  3.61347020e-01,  1.73625088e+00,
        1.57609022e+00, -2.08762145e+00,  2.18810892e+00,  1.20706499e+00,
       -1.82370770e+00,  1.22358835e+00, -8.91464829e-01, -3.30527711e+00,
       -3.72515142e-01, -6.23329699e-01,  8.11975658e-01, -8.52464736e-01,
       -9.35325995e-02, -4.06904364e+00,  1.57146180e+00,  7.85030201e-02,
        1.94540334e+00,  2.13809991e+00, -1.58913553e+00, -3.81727874e-01,
       -2.08527303e+00,  5.89691937e-01,  2.55564898e-01,  2.38364622e-01,
        3.64680409e+00,  4.18930590e-01,  1.62034535e+00, -4.63252217e-02,
        5.80206394e-01,  5.55441022e-01,  1.91946900e+00, -1.89253080e+00,
        1.77489519e+00, -3.15311766e+00,  6.48138940e-01,  1.15823770e+00,
       -2.54519200e+00, -1.03516951e-01,  1.15724599e+00, -1.83681571e+00,
       -9.87860620e-01, -1.99984312e+00,  2.76547909e-01,  8.02748859e-01,
        1.99196494e+00, -1.43310416e+00, -2.03039408e+00, -7.19777197e-02],
      dtype=float32)
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查询多个词的词向量</span>
<span class="n">wv</span><span class="o">.</span><span class="n">get_mean_vector</span><span class="p">([</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;研发&#39;</span><span class="p">])</span>
</code></pre></div><p>Ruj</p>
<pre><code>array([ 0.17623448, -0.02220692, -0.01040847,  0.03616136,  0.04931263,
       -0.06220303, -0.02846557, -0.00156435,  0.04524047,  0.03185674,
        0.01104859,  0.06962118, -0.01969986, -0.10831943, -0.0524368 ,
        0.00623383, -0.04149605, -0.004912  ,  0.13154642, -0.04317038,
       -0.00407438,  0.00923527, -0.13339072,  0.01446994, -0.00153984,
        0.12378754, -0.06064663,  0.09322313, -0.07711462, -0.05880795,
        0.13697049,  0.0133168 ,  0.02769322,  0.02677607,  0.02549294,
       -0.04504526,  0.06267191,  0.02421109, -0.13401456,  0.01423616,
        0.01860182,  0.00344108,  0.04811918,  0.02748652,  0.0190251 ,
       -0.03800797,  0.01517046,  0.06439836,  0.01320594,  0.04748138,
       -0.08914943, -0.00642068,  0.01786153, -0.02472607, -0.04597819,
        0.05832303,  0.11275461, -0.0387079 ,  0.06912261,  0.05287468,
       -0.04447906,  0.10994074, -0.04371417,  0.01227543,  0.07498093,
       -0.11285575, -0.03113984, -0.01122221, -0.03913497, -0.12117577,
        0.08593786, -0.04319173, -0.01860389,  0.15636683,  0.02267851,
        0.0922839 , -0.12106322, -0.07572737,  0.0191772 , -0.00977821,
        0.00455545,  0.01378978,  0.04774487, -0.02080727,  0.01015578,
       -0.04695337,  0.0848957 , -0.01112909, -0.03210922,  0.01151857,
        0.02214565,  0.03220333, -0.02468888, -0.07493623, -0.03724978,
       -0.00716823, -0.12043905, -0.0560291 , -0.00666756,  0.03659805,
        0.0532646 , -0.05371486,  0.06905847,  0.00660356, -0.10362111,
       -0.0015829 , -0.13282564,  0.08241726,  0.00993685,  0.04208402,
        0.03087696,  0.04765649, -0.00834742,  0.07236902,  0.04473683,
       -0.02643864, -0.0050621 ,  0.04462356, -0.0832998 , -0.05533891,
        0.00664944, -0.13001585,  0.07607447, -0.00764748,  0.01410657,
       -0.03057465,  0.0250505 ,  0.09252612, -0.00784517,  0.0386237 ,
       -0.059011  ,  0.05357389, -0.04604931,  0.04388874, -0.0971131 ,
       -0.09777305,  0.02943253, -0.04103448, -0.03944859,  0.09638489,
       -0.02226706,  0.02822194, -0.0093646 , -0.11203568,  0.06142627,
        0.04761236,  0.02720375, -0.09777595,  0.04048391, -0.06758034,
       -0.01500905,  0.02439078,  0.07150253, -0.02562411,  0.02533657,
        0.00799897, -0.06416934,  0.03153701, -0.03944302, -0.04653639,
       -0.04123383, -0.01590026,  0.03051148, -0.02014856, -0.01448381,
       -0.10517117, -0.00649814,  0.02478252,  0.02855514,  0.09052269,
       -0.03505059, -0.03173327, -0.06641324,  0.06284194,  0.01993516,
        0.01349441,  0.1410133 , -0.05283241,  0.03687092, -0.02535007,
        0.00415636,  0.05841105,  0.07389537, -0.13176979,  0.06759793,
       -0.092868  ,  0.01370211,  0.06616284, -0.09137756, -0.01640504,
        0.06095972, -0.05725639, -0.04122292,  0.00598698,  0.02904861,
        0.0442962 ,  0.07399555, -0.04657119, -0.07636161,  0.03204561],
      dtype=float32)
</code></pre>
<p>有了每个词或者概念的向量，可以结合cntext旧版本单语言模型内的态度偏见的度量。</p>
<p><br><br></p>
<h2 id="四扩展词典">四、扩展词典</h2>
<p>做词典法的文本分析，最重要的是有自己的领域词典。之前受限于技术难度，文科生的我也一直在用形容词的通用情感词典。现在依托word2vec技术， 可以加速人工构建的准确率和效率。</p>
<p>下面是在 mda01-21.200.6.bin 上做的词典扩展测试，函数expand_dictionary会根据种子词选取最准确的topn个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#短视主义词  实验</span>
<span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;抓紧&#39;</span><span class="p">,</span> <span class="s1">&#39;立刻&#39;</span><span class="p">,</span> <span class="s1">&#39;月底&#39;</span><span class="p">,</span> <span class="s1">&#39;年底&#39;</span><span class="p">,</span> <span class="s1">&#39;年终&#39;</span><span class="p">,</span> <span class="s1">&#39;争取&#39;</span><span class="p">,</span> <span class="s1">&#39;力争&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['抓紧',
 '立刻',
 '月底',
 '年底',
 '年终',
 '争取',
 '力争',
 '争取',
 '力争',
 '年内',
 '月底',
 '年底',
 '尽早',
 '3月底',
 '尽快',
 '抓紧',
 '6月份',
 '4月份',
 '月份',
 '工作力争',
 '努力争取',
 '工作争取',
 '10月底',
 '年内实现',
 '年底完成',
 '中旬',
 '7月份',
 '9月底',
 '有望',
 '月底前',
 '早日',
 '全力',
 '继续',
 '月初',
 '努力',
 '确保',
 '8月份']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;团结&#39;</span><span class="p">,</span> <span class="s1">&#39;拼搏&#39;</span><span class="p">,</span>  <span class="s1">&#39;克服&#39;</span><span class="p">,</span>  <span class="s1">&#39;勇攀高峰&#39;</span><span class="p">,</span>  <span class="s1">&#39;友善&#39;</span><span class="p">,</span>  <span class="s1">&#39;进取&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['团结',
 '拼搏',
 '克服',
 '勇攀高峰',
 '友善',
 '进取',
 '拼搏',
 '艰苦奋斗',
 '坚定信念',
 '团结拼搏',
 '上下同心',
 '团结',
 '顽强拼搏',
 '勇于担当',
 '团结一致',
 '团结奋进',
 '精诚团结',
 '齐心协力',
 '开拓进取',
 '奋进',
 '团结一心',
 '实干',
 '同心协力',
 '团结协作',
 '锐意进取',
 '积极进取',
 '奋力拼搏',
 '拼搏精神',
 '努力拼搏',
 '进取',
 '奋发有为',
 '扎实工作',
 '同心同德',
 '拼搏进取',
 '脚踏实地',
 '励精图治']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;科技&#39;</span><span class="p">,</span>  <span class="s1">&#39;研发&#39;</span><span class="p">,</span>  <span class="s1">&#39;技术&#39;</span><span class="p">,</span>  <span class="s1">&#39;标准&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['创新',
 '科技',
 '研发',
 '技术',
 '标准',
 '创新',
 '技术创新',
 '技术研发',
 '科技创新',
 '先进技术',
 '自主创新',
 '前沿技术',
 '关键技术',
 '科研',
 '新技术',
 '创新性',
 '研发创新',
 '产品研发',
 '基础研究',
 '产品开发',
 '集成创新',
 '核心技术',
 '自主研发',
 '技术应用',
 '技术集成',
 '前沿科技',
 '技术标准',
 '工艺技术',
 '科技成果',
 '技术开发',
 '尖端技术',
 '工程技术',
 '技术相结合',
 '科学技术',
 '工艺']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;竞争&#39;</span><span class="p">,</span> <span class="s1">&#39;竞争力&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['竞争',
 '竞争力',
 '竞争能力',
 '竞争优势',
 '市场竞争',
 '竞',
 '市场竞争力',
 '竞争实力',
 '参与市场竞争',
 '国际竞争',
 '市场竞争能力',
 '核心竞争力',
 '激烈竞争',
 '市场竞争优势',
 '竞争态势',
 '参与竞争',
 '竞争力重要',
 '竞争对手',
 '创新能力',
 '综合竞争力',
 '价格竞争',
 '之间竞争',
 '核心竞争能力',
 '未来市场竞争',
 '国际竞争力',
 '影响力竞争力',
 '国际化竞争',
 '行业竞争',
 '综合竞争能力',
 '竞争日趋激烈',
 '产品竞争力',
 '竞争力影响力']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;疫情&#39;</span><span class="p">,</span> <span class="s1">&#39;扩散&#39;</span><span class="p">,</span> <span class="s1">&#39;防控&#39;</span><span class="p">,</span> <span class="s1">&#39;反复&#39;</span><span class="p">,</span> <span class="s1">&#39;冲击&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['疫情',
 '扩散',
 '防控',
 '反复',
 '冲击',
 '蔓延',
 '疫情冲击',
 '疫情爆发',
 '新冠疫情',
 '新冠肺炎',
 '疫情蔓延',
 '疫情暴发',
 '肆虐',
 '本次疫情',
 '冲击疫情',
 '新冠病毒',
 '疫情扩散',
 '全球蔓延',
 '疫情影响',
 '病毒疫情',
 '肺炎疫情',
 '击',
 '持续蔓延',
 '疫情持续',
 '各地疫情',
 '疫情突然',
 '疫情全球',
 '疫情传播',
 '疫情反复',
 '散发',
 '变异毒株',
 '疫情导致',
 '疫情肆虐',
 '全球疫情',
 '全球新冠']
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                  <span class="n">seedwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;旧&#39;</span><span class="p">,</span> <span class="s1">&#39;老&#39;</span><span class="p">,</span> <span class="s1">&#39;后&#39;</span><span class="p">,</span> <span class="s1">&#39;落后&#39;</span><span class="p">],</span>
                  <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['旧',
 '老',
 '后',
 '落后',
 '旧',
 '老',
 '陈旧',
 '老旧',
 '淘汰',
 '高能耗',
 '低效率',
 '设备陈旧',
 '能耗高',
 '老旧设备',
 '落后工艺',
 '进行改造',
 '工艺落后',
 '技术落后',
 '翻新',
 '更新改造',
 '改造',
 '更新',
 '替换',
 '改造更新',
 '旧设备',
 '污染重',
 '淘汰一批',
 '拆除',
 '污染严重',
 '简陋',
 '产能落后',
 '相对落后',
 '产能淘汰',
 '效率低下']
</code></pre>
<p><br><br></p>
<h2 id="五源代码">五、源代码</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>


<span class="k">def</span> <span class="nf">load_w2v</span><span class="p">(</span><span class="n">w2v_path</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    Load word2vec model
</span><span class="s2">
</span><span class="s2">    Args:
</span><span class="s2">        w2v_path (str): path of word2vec model
</span><span class="s2">
</span><span class="s2">    Returns:
</span><span class="s2">        model: word2vec model
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loading word2vec model...&#39;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">seedwords</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    According to the seed word file, select the top n words with the most similar semantics and save them in the directory save_dir.
</span><span class="s2">    
</span><span class="s2">    Args:
</span><span class="s2">        wv (Word2VecKeyedVectors): the word embedding model
</span><span class="s2">        seedwords (list): 种子词
</span><span class="s2">        topn (int, optional): Set the number of most similar words to retrieve to topn. Defaults to 100.
</span><span class="s2">        save_dir (str, optional): the directory to save the candidate words. Defaults to &#39;Word2Vec&#39;.
</span><span class="s2">    
</span><span class="s2">    Returns:
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">simidx_scores</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">similars_candidate_idxs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#the candidate words of seedwords</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span>
    <span class="n">seedidxs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1">#transform word to index</span>
    <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">seedwords</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
            <span class="n">seedidx</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">seed</span><span class="p">]</span>
            <span class="n">seedidxs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">seedidx</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">seedidx</span> <span class="ow">in</span> <span class="n">seedidxs</span><span class="p">:</span>
        <span class="c1"># sims_words such as [(&#39;by&#39;, 0.99984), (&#39;or&#39;, 0.99982), (&#39;an&#39;, 0.99981), (&#39;up&#39;, 0.99980)]</span>
        <span class="n">sims_words</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">seedidx</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="n">topn</span><span class="p">)</span>
        <span class="c1">#Convert words to index and store them</span>
        <span class="n">similars_candidate_idxs</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">dictionary</span><span class="p">[</span><span class="n">sim</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="k">for</span> <span class="n">sim</span> <span class="ow">in</span> <span class="n">sims_words</span><span class="p">])</span>
    <span class="n">similars_candidate_idxs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">similars_candidate_idxs</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">similars_candidate_idxs</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">wv</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="n">idx</span><span class="p">],</span> <span class="n">seedidxs</span><span class="p">)</span>
        <span class="n">simidx_scores</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">score</span><span class="p">))</span>
    <span class="n">simidxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">simidx_scores</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>

    <span class="n">simwords</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">simidxs</span><span class="p">][:</span><span class="n">topn</span><span class="p">]</span>

    <span class="n">resultwords</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">resultwords</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">seedwords</span><span class="p">)</span>
    <span class="n">resultwords</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">simwords</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">resultwords</span>
</code></pre></div><p><br><br></p>
<h2 id="获取模型">获取模型</h2>
<p>模型训练不易， 为付费资源，如需使用请 <a href="https://mp.weixin.qq.com/s/zle9-BR-ei1V8Nmul19_7w"><strong>点击进入跳转购买链接</strong></a></p>
<p><br><br></p>
<h2 id="期待合作">期待合作</h2>
<p>cntext目前仍在技术迭代，版本2.0.0综合了训练语言模型&amp;多语言模型对齐， 有较大的应用价值，期待有独特文本数据集交流合作。</p>
<p>通过cntext2.0.0，理论上可以对文本所涉社会主体进行计算，适合企业文化、品牌印象、旅游目的地形象、国家形象等</p>
<ul>
<li>同主体不同时间段， 文本中蕴含的文化态度认知变迁，</li>
<li>或同时间段，不同主体的大样本文本蕴含的差异性</li>
</ul>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>基于词嵌入技术的心理学研究: 方法及应用</title>
      <link>https://textdata.cn/blog/2023-03-10-psychological-research-with-word-embeddings/</link>
      <pubDate>Fri, 10 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-10-psychological-research-with-word-embeddings/</guid>
      <description>词嵌入是自然语言处理的一项基础技术。 其核心理念是根据大规模语料中词语和上下文的联系, 使用神经网络等机器学习算法自动提取有限维度的语义特征, 将每个词表示为一个低维稠密的数值向量(词向 量), 以用于后续分析。 心理学研究中, 词向量及其衍生的各种语义联系指标可用于探究人类的语义加工、认知判断、发散思维、社会偏见与刻板印象、社会与文化心理变迁等各类问题。 未来, 基于词嵌入技术的心理 学研究需要区分心理的内隐和外显成分, 深化拓展动态词向量和大型预训练语言模型(如 GPT、BERT)的应用, 并在时间和空间维度建立细粒度词向量数据库, 更多开展基于词嵌入的社会变迁和跨文化研究。 As a fundamental technique in natural language processing (NLP), word embedding quantifies a word as a low-dimensional, dense, and continuous numeric vector (i.e., word vector). Word embeddings can be obtained by using machine learning algorithms such as neural networks to predict the surrounding words given a word or vice versa (Word2Vec and FastText) or by predicting the probability of co-occurrence of multiple words (GloVe) in large-scale text corpora. Theoretically, the dimensions of a word vector reflect the pattern of how the word can be predicted in contexts; however, they also connote substantial semantic information of the word. Therefore, word embeddings can be used to analyze semantic meanings of text. In recent years, word embeddings have been increasingly applied to study human psychology, including human semantic processing, cognitive judgment, divergent thinking, social biases and stereotypes, and sociocultural changes at the societal or population level. Future research using word embeddings should (1) distinguish between implicit and explicit components of social cognition, (2) train fine-grained word vectors in terms of time and region to facilitate cross-temporal and cross-cultural research, and (3) apply contextualized word embeddings and large pre-trained language models such as GPT and BERT. To enhance the application of word embeddings in psychology。</description>
      <content:encoded><![CDATA[<p><a href="https://psychbruce.github.io/">包寒吴霜博客 https://psychbruce.github.io/</a></p>
<p><img loading="lazy" src="img/%e5%8c%85%e5%af%92%e5%90%b4%e9%9c%9c.png" alt=""  />
</p>
<br>
<p><img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-01.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-02.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-03.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-04.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-05.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-06.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-07.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-08.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-09.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-10.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-11.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-12.png" alt=""  />

<img loading="lazy" src="img/%e5%9f%ba%e4%ba%8e%e8%af%8d%e5%b5%8c%e5%85%a5%e6%8a%80%e6%9c%af%e7%9a%84%e5%bf%83%e7%90%86%e5%ad%a6%e7%a0%94%e7%a9%b6-%e6%96%b9%e6%b3%95%e5%8f%8a%e5%ba%94%e7%94%a8-13.png" alt=""  />
</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>可视化 | 词嵌入模型用于计算社科领域刻板印象等信息（含代码）</title>
      <link>https://textdata.cn/blog/2023-03-03-extracts-cognitive-information-and-visualization-with-embedings/</link>
      <pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-03-extracts-cognitive-information-and-visualization-with-embedings/</guid>
      <description>语言的文字反映了人类思想的结构，使我们能够在个人之间传递思想， 而使用大规模语料训练得来的词嵌入模型蕴含着这类信息。 英文的词嵌入在社会科学中的应用教程较多，大家可以谷歌查询，我主要想丰富中文数据的教程。The words of language reflect the structure of human thought, allowing us to transfer thoughts between individuals, and word embedding models trained using large-scale corpora contain this information. There are many application tutorials of English word embedding in social science. You can search it on Google. I mainly want to enrich the tutorials of Chinese data.</description>
      <content:encoded><![CDATA[<iframe
    src="//player.bilibili.com/player.html?bvid=BV1CY4y11712&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<p>语言的文字反映了人类思想的结构，使我们能够在个人之间传递思想， 而使用大规模语料训练得来的词嵌入模型往往蕴含着这类信息。</p>
<br>
<h3 id="11-国内外社科方面的应用">1.1 国内外社科方面的应用</h3>
<p><strong>在国内社科领域， 应用词嵌入 主要用来做情感分析</strong>，大致的算法(思路)是</p>
<ol>
<li>训练词向量模型</li>
<li>根据词向量cosine或欧几里得距离，度量词语的相似性，进而扩展某种概念词典</li>
<li>检查扩充的概念词典，剔除无效词后。</li>
<li>使用整理好的概念词典，统计文本中出现该概念的词频，当做该概念的代理指标。</li>
</ol>
<p>但词嵌入在国外社科领域， 不用传统方法，使用文本数据，也能做出 **刻板印象、性别种族歧视、词语百年来语义变迁、女性高管就职后公司内性别观念变化、测量创新力(发散思维)**等议题的实证研究。</p>
<p>下图是「阶级财富性别与运动」，摘自2019年文化几何学这篇论文。</p>
<p><img loading="lazy" src="img/sport_class_fortune.png" alt=""  />
</p>
<blockquote>
<p>Kozlowski, Austin C., Matt Taddy, and James A. Evans. &ldquo;The geometry of culture: Analyzing the meanings of class through word embeddings.&rdquo; American Sociological Review 84, no. 5 (2019): 905-949.</p>
</blockquote>
<p>本文主要内容是实现这类文化几何学图的中文可视化。</p>
<br>
<h3 id="12-之前分享过的资料">1.2 之前分享过的资料</h3>
<p>之前大邓分享过的词嵌入稍有涉及，感兴趣的可以阅读我之前分享的文章</p>
<ul>
<li><a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></li>
<li><a href="https://textdata.cn/blog/2022-10-16-python-word-mover-s-distance/">Python | 词移距离(Word Mover’s Distance)</a></li>
<li><a href="https://textdata.cn/blog/wordbias/">WordBias库 | 发现偏见(刻板印象)的交互式工具</a></li>
<li><a href="https://textdata.cn/blog/embeddingsandattitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
<li><a href="https://textdata.cn/blog/whatlies_word2vec/">whatlies库 | 可视化词向量</a></li>
<li><a href="https://textdata.cn/blog//2022-11-14-pnas_naming_unrelated_words_predicts_creativity/">PNAS | 使用语义距离测量一个人的创新力(发散思维)得分</a></li>
<li><a href="https://textdata.cn/blog/embeddings_resource_usage_method/">中文词向量资源汇总 &amp; 使用方法</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
</ul>
<br>
<p>但可视化分享的不多，<strong>本文将用公开的中文预训练模型，验证可视化中文圈的群体记忆、刻板印象、偏见等信息</strong>。在此先放一张论文中两个截图， <strong>语义概念向量</strong> 一般是由语义相反的两组词构成。</p>
<p><img loading="lazy" src="img/size.png" alt=""  />

<img loading="lazy" src="img/%e4%ba%8c%e5%85%83%e6%a6%82%e5%bf%b5%e7%bb%84.png" alt=""  />
</p>
<blockquote>
<p>Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. <strong>Semantic projection recovers rich human knowledge of multiple object features from word embeddings</strong>. <em>Nature Human Behaviour</em>, pp.1-13.</p>
</blockquote>
<br>
<h3 id="概念向量的计算方法">概念向量的计算方法</h3>
<ul>
<li>二维坐标系下，点和向量都可以用二维数组(m, n)表示。同理，在n维空间中，点和向量都是n维数组。</li>
<li>将多个近义的词向量， 通过平均法创建出一个 <strong>均值端点</strong>。</li>
<li>语义完全相反的两个<strong>均值端点</strong>， 通过减法操作， 得到 <strong>概念向量</strong></li>
</ul>
<br>
<h3 id="为啥每个端点向量用多个词计算">为啥每个端点向量用多个词计算？</h3>
<p>单个词变动较大， 为了保证语义的稳定性，最好是找一组词构成概念的一个端点。</p>
<p><br><br></p>
<h2 id="二准备工作">二、准备工作</h2>
<p>下载预训练模型，可以查看这篇文章获取</p>
<p><a href="https://textdata.cn/blog/embeddings_resource_usage_method/">中文词向量资源汇总 &amp; 使用方法</a></p>
<p>之后安装好本节需要的python包</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">whatlies</span><span class="o">==</span><span class="mf">0.7.0</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">matplotlib_inline</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">gensim</span><span class="o">==</span><span class="mf">4.2.0</span>
</code></pre></div><p><br><br></p>
<h2 id="三导入预训练模型">三、导入预训练模型</h2>
<p>使用 gensim 库导入预训练模型，这里我本地保留的是预训练模型是word2vec中的sgns算法训练出来的。 导入后的数据是 KeyedVectors 类型的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1"># 微博 sgns.weibo.word.bz2 为例  </span>
<span class="n">weibo_wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.weibo.word.bz2&#39;</span><span class="p">,</span> 
                                             <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                             <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># 知乎 sgns.renmin.word.bz2</span>
<span class="n">zhihu_wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.zhihu.word.bz2&#39;</span><span class="p">,</span> 
                                              <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                              <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># 中文维基 </span>
<span class="n">wiki_wv</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.wiki.word.bz2&#39;</span><span class="p">,</span> 
                                              <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                              <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="31-预训练模型的词汇量">3.1 预训练模型的词汇量</h3>
<p>weibo_wv、zhihu_wv、wiki_wv是KeyedVectors类型的数据，可以直接查看词汇量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;预训练模型词汇量&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;微博: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">weibo_wv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;知乎: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">zhihu_wv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;中文维基: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">wiki_wv</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">预训练模型词汇量

微博:  195202
知乎:  259949
中文维基:  352277
</code></pre></div><br>
<h3 id="32-通用词">3.2 通用词</h3>
<p>使用不同数据集训练，得到的语言模型所含词语会有差异。这里我们查看通用词一共有多少</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wiki_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">wiki_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
<span class="n">zhihu_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
<span class="n">weibo_vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">weibo_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>

<span class="c1">#交集</span>
<span class="n">common_vocab</span> <span class="o">=</span> <span class="n">wiki_vocab</span> <span class="o">&amp;</span> <span class="n">zhihu_vocab</span> <span class="o">&amp;</span>  <span class="n">weibo_vocab</span><span class="c1"># intersection</span>

<span class="nb">len</span><span class="p">(</span><span class="n">common_vocab</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">118539
</code></pre></div><br>
<h3 id="33-提取某个词的向量">3.3 提取某个词的向量</h3>
<p>以维基百科为例， 查看「幸福」的词向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#只显示向量的前20个数字</span>
<span class="n">wiki_wv</span><span class="p">[</span><span class="s1">&#39;幸福&#39;</span><span class="p">][:</span><span class="mi">20</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 0.159344,  0.280468, -0.236876, -0.198076, -0.170838,  0.027264,
           -0.349646,  0.289169, -0.421038, -0.470539,  0.247534,  0.112968,
            0.355498,  0.479956,  0.093291,  0.081054, -0.046995, -0.624586,
            0.568242,  0.16665 ], dtype=float32)
</code></pre></div><br>
<h3 id="34-查看词向量的维度">3.4 查看词向量的维度</h3>
<p>查看向量的长度（维度），以「幸福」为例</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;预训练模型维度数&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;微博: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">weibo_wv</span><span class="p">[</span><span class="s2">&#34;幸福&#34;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;知乎: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">zhihu_wv</span><span class="p">[</span><span class="s2">&#34;幸福&#34;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;中文维基: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">wiki_wv</span><span class="p">[</span><span class="s2">&#34;幸福&#34;</span><span class="p">]))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">预训练模型维度数
微博:  300
知乎:  300
中文维基:  300
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#单个词向量的尺寸</span>
<span class="n">wiki_wv</span><span class="p">[</span><span class="s1">&#39;幸福&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(300,)
</code></pre></div><br>
<h3 id="35-计算多个词向量的均值向量">3.5 计算多个词向量的均值向量</h3>
<p>先看一下多个词提取后得到的数据形状</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(4, 300)
</code></pre></div><br>
<p>4个词，每个词都是300维的词向量。如果计算4个词向量的均值向量，返回的尺寸应该是 (300,)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">male_vector</span> <span class="o">=</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">male_vector</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(300,)
</code></pre></div><br>
<h3 id="36-最相似的词">3.6 最相似的词</h3>
<p>网上的教程经常分享最相似的词，这里我们也实验一下。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wiki_wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;于社会&#39;, 0.6210986971855164),
(&#39;社会上&#39;, 0.5995474457740784),
(&#39;社会关系&#39;, 0.5894029140472412),
(&#39;各阶层&#39;, 0.5799717903137207),
(&#39;社会制度&#39;, 0.5777087211608887),
(&#39;社会变迁&#39;, 0.5756841897964478),
(&#39;令社会&#39;, 0.575627326965332),
(&#39;社会变革&#39;, 0.5755838751792908),
(&#39;思想观念&#39;, 0.5752044916152954),
(&#39;社会存在&#39;, 0.573627769947052)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">weibo_wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;金钱至上&#39;, 0.5815222859382629),
(&#39;各阶层&#39;, 0.5668456554412842),
(&#39;福利制度&#39;, 0.5532322525978088),
(&#39;官与民&#39;, 0.5526734590530396),
(&#39;高考制度&#39;, 0.5515810251235962),
(&#39;资源分配&#39;, 0.5500271916389465),
(&#39;功利主义&#39;, 0.5484314560890198),
(&#39;分级制&#39;, 0.5450907349586487),
(&#39;功利化&#39;, 0.5432640910148621),
(&#39;法制建设&#39;, 0.5420899391174316)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;社会存在&#39;, 0.6277482509613037),
 (&#39;社会生活&#39;, 0.613935649394989),
(&#39;社会群体&#39;, 0.6123108863830566),
(&#39;社会意识&#39;, 0.6055717468261719),
(&#39;物欲横流&#39;, 0.6041101217269897),
(&#39;民主决策&#39;, 0.602908194065094),
(&#39;阶级分化&#39;, 0.59609454870224),
(&#39;社会上&#39;, 0.5932644605636597),
(&#39;于社会&#39;, 0.5919737219810486),
(&#39;法制化&#39;, 0.5820874571800232)]
</code></pre></div><p><br><br></p>
<h2 id="四-可视化">四、 可视化</h2>
<p>为了让中文可以在matplotlib正常显示， 需要先运行下方代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>

<span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>  <span class="c1"># 获取操作系统类型</span>

<span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;SimHei&#39;</span><span class="p">}</span>
<span class="k">elif</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">:</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;Arial Unicode MS&#39;</span><span class="p">}</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># 如果是其他系统，可以使用系统默认字体</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">}</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;font&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>  <span class="c1"># 设置全局字体</span>
</code></pre></div><br>
<h3 id="41-运动的贫富和性别属性">4.1 运动的贫富和性别属性</h3>
<p>先看一个最难的例子， 后面的例子都是围绕ta展开的。</p>
<ul>
<li><strong>性别向量</strong> 由 <strong>男性均值端点向量</strong> 和 <strong>女性均值端点向量</strong> 计算得来</li>
<li><strong>贫富向量</strong> 由 <strong>富裕均值端点向量</strong> 和 <strong>贫穷均值端点向量</strong> 计算得来</li>
</ul>
<p>需要注意， 不论是 <strong>性别向量</strong>、<strong>贫富向量</strong> 还是运动词的词向量，都是 300维的向量。 如果在低维空间，例如2维坐标轴中可视化，需要做投影操作。这里需要一点大学线性代数的点乘知识。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 获取需要绘制的单词列表</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;足球&#39;</span><span class="p">,</span> <span class="s1">&#39;拳击&#39;</span><span class="p">,</span> <span class="s1">&#39;高尔夫&#39;</span><span class="p">,</span> <span class="s1">&#39;棒球&#39;</span><span class="p">,</span> <span class="s1">&#39;芭蕾&#39;</span><span class="p">]</span>

<span class="c1"># 获取词向量，并转换为 NumPy 数组</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">wiki_wv</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>

<span class="c1"># 选择两个词向量作为新坐标系的 x 轴和 y 轴</span>
<span class="n">x_axis</span> <span class="o">=</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span>  <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_axis</span> <span class="o">=</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;拮据&#39;</span><span class="p">,</span> <span class="s1">&#39;囊中羞涩&#39;</span><span class="p">,</span> <span class="s1">&#39;困难&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">wiki_wv</span><span class="p">[[</span><span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;贵气&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 计算每个词向量在新坐标系中的投影</span>
<span class="n">x_coords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">x_axis</span><span class="p">)</span>
<span class="n">y_coords</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">y_axis</span><span class="p">)</span>

<span class="c1"># 绘制图形</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_coords</span><span class="p">,</span> <span class="n">y_coords</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="p">(</span><span class="n">x_coords</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_coords</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="c1"># 绘制 x 轴和 y 轴的十字线</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;贫富(贫下富上)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;运动的贫富和性别属性&#39;</span><span class="p">)</span>
<span class="c1">#plt.show()</span>


<span class="c1">#保存</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/运动的贫富和性别属性.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_25_0.png" alt="svg"  />
</p>
<br>
<h3 id="42-使用whatlies处理数据">4.2 使用whatlies处理数据</h3>
<p>上面的可视化代码太长了，使用whatlies可以简化代码量。我们把 KeyedVectors类 转为 EmbeddingSet类，这里就可以更容易的把点显示为带箭头的向量。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">whatlies</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">EmbeddingSet</span>

<span class="c1"># load vectors as whatlies EmbeddingSet</span>
<span class="n">wiki_emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="o">.</span><span class="n">from_names_X</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="n">wiki_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">,</span> 
                                     <span class="n">X</span><span class="o">=</span><span class="n">wiki_wv</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>

<span class="n">weibo_emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="o">.</span><span class="n">from_names_X</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="n">weibo_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">,</span>
                                      <span class="n">X</span> <span class="o">=</span> <span class="n">weibo_wv</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>

<span class="n">zhihu_emb</span> <span class="o">=</span> <span class="n">EmbeddingSet</span><span class="o">.</span><span class="n">from_names_X</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">,</span> 
                                      <span class="n">X</span><span class="o">=</span><span class="n">zhihu_wv</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>

</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># list similar words, n=10</span>
<span class="n">wiki_emb</span><span class="o">.</span><span class="n">score_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(Emb[社会], 5.960464477539063e-08),
(Emb[于社会], 0.3789013624191284),
(Emb[社会上], 0.4004524350166321),
(Emb[社会关系], 0.410597026348114),
(Emb[各阶层], 0.42002809047698975),
(Emb[社会制度], 0.4222911596298218),
(Emb[社会变迁], 0.42431581020355225),
(Emb[令社会], 0.42437267303466797),
(Emb[社会变革], 0.424416184425354),
(Emb[思想观念], 0.4247954487800598)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">weibo_emb</span><span class="o">.</span><span class="n">score_similar</span><span class="p">(</span><span class="s2">&#34;社会&#34;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(Emb[社会], 0.0),
(Emb[金钱至上], 0.41847753524780273),
(Emb[各阶层], 0.43315428495407104),
(Emb[福利制度], 0.4467676877975464),
(Emb[官与民], 0.4473266005516052),
(Emb[高考制度], 0.4484187364578247),
(Emb[资源分配], 0.44997286796569824),
(Emb[功利主义], 0.4515683650970459),
(Emb[分级制], 0.45490920543670654),
(Emb[功利化], 0.4567357897758484)]
</code></pre></div><br>
<h3 id="43-whatlies默认可视化">4.3 whatlies默认可视化</h3>
<p>使用whatlies默认的效果绘制如下，但需要注意， 这里的Dimension0和Dimension1的含义是未知的。所以除了可视化， 含义解读起来比较困难。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># the default X and Y axes are the first two dimensions of the embedding vectors</span>
<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;马&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> 
                   <span class="n">color</span><span class="o">=</span><span class="s2">&#34;purple&#34;</span><span class="p">)</span>

<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;鲨鱼&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;blue&#34;</span><span class="p">)</span>

<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;鸟类&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;green&#34;</span><span class="p">)</span>
<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;人&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">)</span>
<span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;蛇&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&#34;arrow&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/output_30_0.svg" alt="svg"  />
</p>
<br>
<h3 id="44-使用端点向量当基向量">4.4 使用端点向量当基向量</h3>
<p>使用端点向量当基向量，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;美国&#34;</span><span class="p">,</span> <span class="s2">&#34;中国&#34;</span><span class="p">,</span> <span class="s2">&#34;俄罗斯&#34;</span><span class="p">,</span> <span class="s2">&#34;韩国&#34;</span><span class="p">]</span>

<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span><span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;弱小&#34;</span><span class="p">],</span> 
          <span class="n">y_axis</span><span class="o">=</span><span class="n">wiki_emb</span><span class="p">[</span><span class="s2">&#34;强大&#34;</span><span class="p">],</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">])</span>

<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;国家强弱&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/国家强弱.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_32_0.svg" alt="svg"  />
</p>
<br>
<p>按照我们的理解， 强大与弱小是方向相反的。但是如果将这两个词分别做基向量。如图所示，就体现不出方向。</p>
<p>同时，因为给定有意义的基向量作为坐标轴向量， 坐标轴含有了意义，可视化的结果可以看出语义信息的亲疏远近。</p>
<p>可以看到， 中美俄是大国强国，韩国是小国军事弱国。</p>
<br>
<h3 id="45-使用概念向量当做基向量">4.5 使用概念向量当做基向量</h3>
<p>当使用概念向量做基向量， 我们就能保留住词语之间的正反方向。避免 4.4 反义词之间无法体现方向性信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#待考察词</span>
<span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s1">&#39;足球&#39;</span><span class="p">,</span> <span class="s2">&#34;斗殴&#34;</span><span class="p">,</span> <span class="s1">&#39;高尔夫&#39;</span><span class="p">,</span> <span class="s1">&#39;篮球&#39;</span><span class="p">,</span> <span class="s1">&#39;芭蕾&#39;</span><span class="p">,</span> <span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;乒乓球&#39;</span><span class="p">,</span> <span class="s1">&#39;举重&#39;</span><span class="p">]</span>

<span class="c1">#性别概念向量</span>
<span class="n">sex_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>
<span class="c1">#贫富概念向量</span>
<span class="n">disparity_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;贵气&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;拮据&#39;</span><span class="p">,</span> <span class="s1">&#39;囊中羞涩&#39;</span><span class="p">,</span> <span class="s1">&#39;困难&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> 

<span class="c1">#概念向量 做 基向量</span>
<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span> <span class="n">sex_vector</span><span class="p">,</span> 
          <span class="n">y_axis</span><span class="o">=</span> <span class="n">disparity_vector</span><span class="p">,</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">])</span>


<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;运动中体现的贫富与性别信息&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;贫富(贫下富上)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/运动中体现的贫富与性别信息.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_34_0.svg" alt="svg"  />
</p>
<br>
<p>刚刚的图中加入了<code>男、女、贫穷、富裕</code>四个词，是为了帮助我们识别出方向来的，判断横纵坐标的含义和方向性。现在我们可以去掉这四个词，绘制更美观的图。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s1">&#39;足球&#39;</span><span class="p">,</span> <span class="s1">&#39;高尔夫&#39;</span><span class="p">,</span> <span class="s1">&#39;篮球&#39;</span><span class="p">,</span> <span class="s1">&#39;芭蕾&#39;</span><span class="p">,</span> <span class="s1">&#39;乒乓球&#39;</span><span class="p">,</span> <span class="s1">&#39;举重&#39;</span><span class="p">,</span> <span class="s1">&#39;徒步&#39;</span><span class="p">]</span>

<span class="n">sex_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> 
<span class="n">disparity_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;贵气&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;拮据&#39;</span><span class="p">,</span> <span class="s1">&#39;囊中羞涩&#39;</span><span class="p">,</span> <span class="s1">&#39;困难&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> 

<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span> <span class="n">sex_vector</span><span class="p">,</span> 
          <span class="n">y_axis</span><span class="o">=</span> <span class="n">disparity_vector</span><span class="p">,</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">])</span>

<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;运动中体现的贫富与性别信息&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="c1">#plt.axis(&#39;off&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;贫富(贫下富上)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/运动中体现的贫富与性别信息.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_36_0.png" alt="svg"  />
</p>
<br>
<p>从上图可以看出， 在wiki百科中记录下的， 我们对不同运动是存在贫富、性别化的信息。这些信息根据研究场景，解读为<strong>刻板印象、态度偏好、文化记忆</strong>等。 我们再看一个例子， 把中国动物(含神兽)分别在性别维度和尺寸维度可视化。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">vecs</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[</span><span class="s1">&#39;虎&#39;</span><span class="p">,</span> <span class="s1">&#39;龙&#39;</span><span class="p">,</span> <span class="s1">&#39;猫&#39;</span><span class="p">,</span> <span class="s1">&#39;燕子&#39;</span><span class="p">,</span> <span class="s1">&#39;蝴蝶&#39;</span><span class="p">]</span>

<span class="n">sex_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span><span class="o">-</span><span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>
<span class="n">size_vector</span> <span class="o">=</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;大&#39;</span><span class="p">,</span> <span class="s1">&#39;庞&#39;</span><span class="p">,</span> <span class="s1">&#39;巨&#39;</span><span class="p">,</span> <span class="s1">&#39;高&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span> <span class="o">-</span> <span class="n">wiki_emb</span><span class="p">[[</span><span class="s1">&#39;矮&#39;</span><span class="p">,</span> <span class="s1">&#39;小&#39;</span><span class="p">,</span> <span class="s1">&#39;微&#39;</span><span class="p">,</span> <span class="s1">&#39;毫&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>


<span class="n">vecs</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="o">=</span> <span class="n">sex_vector</span><span class="p">,</span> 
          <span class="n">y_axis</span><span class="o">=</span> <span class="n">size_vector</span><span class="p">,</span> 
          <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;purple&#34;</span><span class="p">,</span> <span class="s2">&#34;green&#34;</span><span class="p">,</span> <span class="s2">&#34;blue&#34;</span><span class="p">,</span> <span class="s2">&#34;red&#34;</span><span class="p">,</span> <span class="s2">&#34;yellow&#34;</span><span class="p">,</span> <span class="s2">&#34;grey&#34;</span><span class="p">])</span>

<span class="c1">#plt.show()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;文化中动物词蕴含的性别化和尺寸信息&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>

<span class="c1">#plt.axis(&#39;off&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;性别(男左女右)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;尺寸(下小上大)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s2">&#34;img/文化中动物词蕴含的性别化和尺寸信息.png&#34;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/output_38_0.svg" alt="svg"  />
</p>
<p><br><br></p>
<h2 id="代码获取">代码获取</h2>
<p>公众号:  大邓和他的Python， 同日期推文， 付费阅读获取全文教程、数据、代码~</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>simpleT5 库 | 根据英文摘要内容生成标题</title>
      <link>https://textdata.cn/blog/2023-02-23-simplet5-one-line-summary/</link>
      <pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-02-23-simplet5-one-line-summary/</guid>
      <description>T5（Text-to-Text Transfer Transformer）是一种基于 Transformer 架构的自然语言处理模型，由 Google Brain 团队开发。T5 模型采用了 encoder-decoder 架构，其中 encoder 将输入文本编码为向量，decoder 则从该向量生成目标文本。T5 模型的特点是将所有自然语言处理任务都视为“从输入文本到输出文本”的转换问题，它可以通过在任务之间共享模型参数和预训练模型来轻松地应用于各种 NLP 任务，如**文本分类、命名实体识别、文本摘要、问答系统**等。 与其他 NLP 模型不同的是，T5 模型使用了一种称为“text-to-text”方法的统一输入输出架构，使得所有 NLP 任务都能转化为文本转换问题，从而使得模型训练更加高效。</description>
      <content:encoded><![CDATA[<p>simpleT5 是基于 PyTorch 实现的 T5 模型库，旨在为用户提供一种简单、易用、可定制的 T5 模型工具。T5（Text-to-Text Transfer Transformer）是一种基于 Transformer 架构的自然语言处理模型，由 Google Brain 团队开发。T5 模型采用了 encoder-decoder 架构，其中 encoder 将输入文本编码为向量，decoder 则从该向量生成目标文本。</p>
<p><img loading="lazy" src="img/new_text_to_text.jpg" alt=""  />
</p>
<p>simpleT5 的设计目标是尽可能地减少 T5 模型的使用门槛，以方便用户在自然语言处理任务中快速应用 T5 模型，从而节省大量的模型开发时间和成本。</p>
<p>simpleT5 提供了一个简单的 API 接口，用户只需要提供输入文本和模型参数，即可轻松地使用 T5 模型进行文本转换任务，如<strong>文本摘要、机器翻译、对话系统</strong>等。simpleT5 还提供了一些预训练模型，包括 T5-small、T5-base 和 T5-large 等不同规模的模型，用户可以根据任务需求选择合适的模型。</p>
<p>除此之外，simpleT5 还提供了一些有用的工具和功能，如文本预处理、数据集加载、训练日志记录等，以帮助用户更轻松地进行模型训练和调试。simpleT5 的开发者们还提供了详细的文档和示例代码，以帮助用户更快地上手使用该库。</p>
<p>总之，simpleT5 为用户提供了一种快速、方便、可定制的 T5 模型工具，可以帮助用户在自然语言处理任务中更加高效地应用 T5 模型，节省大量的开发时间和成本。</p>
<p><br><br></p>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="n">simplet5</span>
</code></pre></div><p><br><br></p>
<h2 id="快速上手">快速上手</h2>
<p>t5模型有很多，如下图，今天以huggingface中公开的模型 <strong>snrspeaks/t5-one-line-summary为例， 展示 「根据传入的摘要内容生成对应的标题」。</strong></p>
<p><img loading="lazy" src="img/t5-models.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># pip install --upgrade simplet5</span>
<span class="kn">from</span> <span class="nn">simplet5</span> <span class="kn">import</span> <span class="n">SimpleT5</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleT5</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&#34;t5&#34;</span><span class="p">,</span><span class="s2">&#34;snrspeaks/t5-one-line-summary&#34;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    Global seed set to 42
    Downloading:   100%|          | 0.00/1.36k [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/850M [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/1.84k [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/773k [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/1.32M [00:00&lt;?, ?B/s]
    Downloading:   100%|          | 0.00/1.74k [00:00&lt;?, ?B/s]
</code></pre></div><br>
<p>根据英文摘要生成标题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">abstract</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production 
</span><span class="s2">machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and 
</span><span class="s2">handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a 
</span><span class="s2">set of novel high-level, declarative abstractions. Overton&#39;s vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. 
</span><span class="s2">In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, 
</span><span class="s2">Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, 
</span><span class="s2">Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">abstract</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>['Overton: Building, Deploying, and Monitoring Deep Machine Learning Systems']
</code></pre>
<br>
<p>根据摘要生成多个标题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">abstract</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production 
</span><span class="s2">machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and 
</span><span class="s2">handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a 
</span><span class="s2">set of novel high-level, declarative abstractions. Overton&#39;s vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. 
</span><span class="s2">In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, 
</span><span class="s2">Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, 
</span><span class="s2">Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="c1">#根据摘要生成5个标题</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">abstract</span><span class="p">,</span> 
              <span class="n">num_return_sequences</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> 
              <span class="n">num_beams</span><span class="o">=</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<pre><code>['Overton: Building, Deploying, and Monitoring Deep Machine Learning Systems',
 'Overton: Building, Deployment, and Improving Production Machine Learning Systems',
 'Overton: Building, Deploying, and Monitoring Machine Learning Systems for Engineers',
 'Overton: Building, Deploying, and Monitoring Machine Learning Systems',
 'Overton: Building, Deployment, and Monitoring Deep Machine Learning Systems']
</code></pre>
<p><br><br></p>
<h2 id="simplet5微调">simpleT5微调</h2>
<p>在 T5 模型的预训练阶段，它使用了巨大的文本语料库进行无监督的训练，以学习将输入文本转换为输出文本的能力。</p>
<p>预训练阶段结束后，T5 模型可以通过微调或迁移学习的方式用于各种下游 NLP 任务中，以实现最先进的性能表现。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">path</span> <span class="o">=</span> <span class="s2">&#34;https://raw.githubusercontent.com/Shivanandroy/T5-Finetuning-PyTorch/main/data/news_summary.csv&#34;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df1.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># simple5库传入的数据是DataFrames，必须含 &#34;source_text&#34; 和 &#34;target_text&#34;这两个字段。</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;headlines&#34;</span><span class="p">:</span><span class="s2">&#34;target_text&#34;</span><span class="p">,</span> <span class="s2">&#34;text&#34;</span><span class="p">:</span><span class="s2">&#34;source_text&#34;</span><span class="p">})</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;source_text&#39;</span><span class="p">,</span> <span class="s1">&#39;target_text&#39;</span><span class="p">]]</span>

<span class="c1"># T5 模型微调时候，source_text 数据都加入了前缀关键词summarise， 告诉 T5模型要做总结类任务的微调。</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;source_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;summarize: &#34;</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;source_text&#39;</span><span class="p">]</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<p>查看数据的形状</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_df</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">((78720, 2), (19681, 2))
</code></pre></div><br>
<p>开始进行 T5 模型的微调</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">simplet5</span> <span class="kn">import</span> <span class="n">SimpleT5</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleT5</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s2">&#34;t5&#34;</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s2">&#34;t5-base&#34;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_df</span><span class="o">=</span><span class="n">train_df</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span>
            <span class="n">eval_df</span><span class="o">=</span><span class="n">test_df</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> 
            <span class="n">source_max_token_len</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> 
            <span class="n">target_max_token_len</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> 
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Downloading: 100%
792k/792k [00:36&lt;00:00, 21.6kB/s]

Downloading: 100%
1.39M/1.39M [00:02&lt;00:00, 641kB/s]

Downloading: 100%
1.20k/1.20k [00:00&lt;00:00, 3.50kB/s]

Downloading: 100%
892M/892M [00:32&lt;00:00, 27.4MB/s]

GPU available: True, used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M 
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Validation sanity check: 0%
0/2 [22:52&lt;?, ?it/s]
Global seed set to 42
Epoch 2: 100%
638/638 [04:07&lt;00:00, 2.57it/s, loss=1.02, v_num=0, val_loss=1.200, train_loss=1.130]
Validating: 100%
13/13 [00:01&lt;00:00, 7.43it/s]
Validating: 100%
13/13 [00:01&lt;00:00, 7.29it/s]
Validating: 100%
13/13 [00:01&lt;00:00, 7.30it/s]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># let&#39;s load the trained model for inferencing:</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&#34;t5&#34;</span><span class="p">,</span><span class="s2">&#34;outputs/SimpleT5-epoch-2-train-loss-0.9478&#34;</span><span class="p">,</span> <span class="n">use_gpu</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">text_to_summarize</span><span class="o">=</span><span class="s2">&#34;&#34;&#34;summarize: Rahul Gandhi has replied to Goa CM Manohar Parrikar&#39;s letter, 
</span><span class="s2">which accused the Congress President of using his &#34;visit to an ailing man for political gains&#34;. 
</span><span class="s2">&#34;He&#39;s under immense pressure from the PM after our meeting and needs to demonstrate his loyalty by attacking me,&#34; 
</span><span class="s2">Gandhi wrote in his letter. Parrikar had clarified he didn&#39;t discuss Rafale deal with Rahul.
</span><span class="s2">&#34;&#34;&#34;</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">text_to_summarize</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;Rahul responds to Goa CM accusing him of using visit for political gain&#39;]
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">支持开票 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>训练&amp;使用Glove语言模型， 可度量刻板印象等</title>
      <link>https://textdata.cn/blog/2022-11-22-glove-embeddings-model/</link>
      <pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-22-glove-embeddings-model/</guid>
      <description>训练&amp;amp;使用Glove语言模型， 可度量刻板印象等</description>
      <content:encoded><![CDATA[<p>Glove可以捕捉到词语在语料库中的全局语义信息和类比信息， 据此基于语义向量计算刻板印象、文化变迁等，Glove模型在计算社会科学中拥有很大的应用潜力。</p>
<p><img loading="lazy" src="img/wordpaths.png" alt=""  />
</p>
<p>训练Glove模型有两种实现方式</p>
<ol>
<li>C语言；  <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></li>
<li>Python语言；mittens、glove-python</li>
</ol>
<p><img loading="lazy" src="img/stanford%e8%ae%ad%e7%bb%83Glove.png" alt=""  />
</p>
<h2 id="方法比较">方法比较</h2>
<table>
<thead>
<tr>
<th style="text-align:left">方法</th>
<th style="text-align:left">优点</th>
<th style="text-align:left">缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">C语言</td>
<td style="text-align:left">速度快，现成的代码工具</td>
<td style="text-align:left">源代码仅支持英文, 需要付出较高的学习成本才能改动支持中文。 对文科生小白而言，门槛高</td>
</tr>
<tr>
<td style="text-align:left">Python语言</td>
<td style="text-align:left">mittens、glove-python等包语法简洁, 易上手</td>
<td style="text-align:left">对文科生还是有一定的门槛，代码运行速度慢</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td></td>
</tr>
</tbody>
</table>
<br>
<p>不考虑性能约束条件，更多地考虑易用性，大邓简化了Python代码，将其内置到了cntext库。</p>
<p>对词向量、词嵌入感兴趣的童鞋，可以阅读下列相关资料</p>
<ul>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
</ul>
<br>
<h2 id="glove代码">GloVe代码</h2>
<p>cntext支持中英文， 只需要7行代码，可完成导入数据、训练模型、保存结果。 这里以三体小说数据为例， 使用 <a href="santi.txt"><strong>data/santi.txt</strong></a> 。</p>
<p><strong>需要注意， santi.txt文件内文本是已经分词处理过的</strong>。这样可以在english这类西方语言模式下使用空格来区分词语的边界。</p>
<blockquote>
<p>如果使用英文数据，下面代码只需要更改数据文件的路径即可。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#设置语言和项目文件夹路径</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Glove</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="c1">#导入语料</span>
<span class="n">model</span><span class="o">.</span><span class="n">create_vocab</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/santi.txt&#39;</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">#构建词语共现矩阵</span>
<span class="n">model</span><span class="o">.</span><span class="n">cooccurrence_matrix</span><span class="p">()</span>
<span class="c1">#设置词嵌入模型的向量维度、迭代数</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_embeddings</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="c1">#存储模型</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;santi_glove_model&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Building prefix dict from the default dictionary ...
Step 1/4: ...Create vocabulary for Glove.

Dumping model to file cache C:\Users\Deng\AppData\Local\Temp\jieba.cache
Loading model cost 0.628 seconds.

Prefix dict has been built successfully.

Step 2/4: ...Create cooccurrence matrix.
Step 3/4: ...Train glove embeddings. 
             Note, this part takes a long time to run

Iteration 20: error 64925132.71550
Step 3/4: ... Finish! Use 316.91 s

Step 4/4: ... Save the glove embeddings to a txt file
</code></pre></div><br>
<h2 id="导入glove预训练模型">导入GloVe预训练模型</h2>
<p>训练好的GloVe模型是txt文件，可以使用gensim导入。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1"># 导入GloVe模型文件</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;output/Glove/santi_glove_model.txt&#39;</span><span class="p">,</span>  <span class="n">no_header</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">#查看某词的词向量</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;宇宙&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 0.6618259 ,  0.60663235,  0.9849417 , -1.028956  ,  1.0711069 ,
       -0.8875306 , -0.52833366, -1.0125595 , -0.9628481 ,  1.0356479 ,
        0.8595257 ,  0.7454354 , -1.0468111 , -0.26285014, -1.0310447 ,
        0.9906805 ,  0.05825566, -0.85581344, -0.9932533 , -1.020438  ,
        1.0495061 , -0.6973389 ,  0.49099424, -0.80775315,  0.64256483,
        1.0157642 ,  1.0135043 , -1.0131834 ,  0.17376372,  0.89585054,
        0.30890268,  0.798895  ,  0.6653925 ,  0.908629  , -1.048273  ,
       -0.35683677,  0.06306187, -1.0267074 , -1.0494691 ,  0.42172813,
        0.24005401,  0.5934993 , -0.0696691 , -1.0360557 , -0.9797269 ,
        1.0205714 , -0.376359  , -1.0501183 ,  1.0415571 , -0.9312968 ],
      dtype=float32)
</code></pre></div><br>
<h2 id="模型的使用">模型的使用</h2>
<p>语料中所有的词语都是维度相同的向量，可以根据向量计算找近义词、反义词。可参考 之前分享的   <a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="参考资料">参考资料</h2>
<ul>
<li>冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.</li>
<li>William L. Hamilton, Jure Leskovec, and Dan Jurafsky. ACL 2016. Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.</li>
<li>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.
<ul>
<li><a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a></li>
</ul>
</li>
<li><a href="https://github.com/hiDaDeng/cntext">https://github.com/hiDaDeng/cntext</a></li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>FinBERT | 金融文本BERT模型，可情感分析、识别ESG和FLS类型</title>
      <link>https://textdata.cn/blog/2022-11-17-finbert-finance-bert-model/</link>
      <pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-17-finbert-finance-bert-model/</guid>
      <description>金融语言模型</description>
      <content:encoded><![CDATA[<h2 id="finbert介绍">FinBERT介绍</h2>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/uj4hm7Lr2Wo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<br>
<p>FinBERT， 是使用49亿词的英文金融语料库数据，生成的BERT预训练语言模型。语料库上大小为 49亿个词。</p>
<ul>
<li>公司报告 10-K 和 10-Q：25亿个词</li>
<li>电话会议记录：13亿个词</li>
<li>分析师报告：11亿个词</li>
</ul>
<p>FinBERT开发者在多个金融 NLP 任务上对 FinBERT 预训练模型进行了微调，均优于传统机器学习模型、深度学习模型和微调 BERT 模型。 所有经过微调的 FinBERT 模型都公开托管在 Huggingface 🤗。  目前支持包括<strong>情绪分析、ESG 分类、前瞻性陈述 (FLS) 分类</strong>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Huang, Allen H., Hui Wang, and Yi Yang. &#34;FinBERT: A large language model for extracting information from financial text.&#34; Contemporary Accounting Research (2022).

摘要（翻译）: 我们开发了 FinBERT，这是一种适用于金融领域的最先进的大型语言模型。我们表明，FinBERT 结合了金融知识，可以更好地总结金融文本中的上下文信息。使用分析报告中研究人员标记的句子样本，我们证明 FinBERT 大大优于 Loughran 和 McDonald 词典以及其他机器学习算法，包括朴素贝叶斯、支持向量机、随机森林、卷积神经网络和长短期记忆，在情感分类中。我们的结果表明，FinBERT 擅长识别其他算法错误标记为中性的句子的正面或负面情绪，这可能是因为它使用了金融文本中的上下文信息。我们发现，FinBERT 优于其他算法，以及 Google 的原始双向编码器表示形式来自 transformers (BERT) 模型，当训练样本量较小且文本中包含一般文本中不常用的金融词时，这种优势尤为突出。 FinBERT 在识别与环境、社会和治理问题相关的讨论方面也优于其他模型。最后，我们表明，与 FinBERT 相比，其他方法低估了收益电话会议的文本信息量至少 18%。我们的结果对学术研究人员、投资专业人士和金融市场监管机构具有重要意义。
</code></pre></div><br>
<h3 id="finbert功能">FinBERT功能</h3>
<p>具体来说，FinBERT有以下内容：</p>
<ul>
<li><a href="https://huggingface.co/yiyanghkust/finbert-pretrain">FinBERT-Pretrained</a>： 针对大规模金融文本的预训练 FinBERT 模型。</li>
<li><a href="https://huggingface.co/yiyanghkust/finbert-tone">FinBERT-Sentiment</a>： 用于情感分类任务。</li>
<li><a href="https://huggingface.co/yiyanghkust/finbert-esg">FinBERT-ESG</a>： 用于 ESG 分类任务。</li>
<li><a href="https://huggingface.co/yiyanghkust/finbert-fls">FinBERT-FLS</a>： 用于前瞻性陈述（FLS）分类任务。</li>
</ul>
<br>
<h3 id="环境配置">环境配置</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install transformers==4.18.0
</code></pre></div><p>本次实验使用的transformers版本为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import transformers
transformers.__version__
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">4.18.0
</code></pre></div><br>
<h3 id="代码下载">代码下载</h3>
<p><a href="FinBERT.ipynb">点击下载</a></p>
<p><br><br></p>
<h2 id="一情感分析">一、情感分析</h2>
<p>金融文本情绪可以调动管理者、信息中介和投资者的观点和意见, 因此分析金融文本情感(情绪)是有价值的。 FinBERT-Sentiment 是一个 FinBERT 模型，它根据标准普尔 500 家公司的分析师报告中的 10,000 个手动注释的句子进行了Fine-tune(微调)。</p>
<blockquote>
<p>Fine-Tune微调 是 深度学习的一种语言处理技术，可以在前人（已有）的语言模型文件基础上加入少量新场景的文本数据进行更新训练，生成出新场景的语言模型。</p>
</blockquote>
<ul>
<li><strong>输入</strong>：金融文本。</li>
<li><strong>输出</strong>：Positive, Neutral or Negative.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="c1">#首次运行，因为会下载FinBERT模型，耗时会比较久</span>
<span class="n">senti_finbert</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-tone&#39;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">senti_tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-tone&#39;</span><span class="p">)</span>
<span class="n">senti_nlp</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&#34;text-classification&#34;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">senti_finbert</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">senti_tokenizer</span><span class="p">)</span>
</code></pre></div><p><br>使用3条测试文本进行测试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 待分析的文本数据</span>
<span class="n">senti_results</span> <span class="o">=</span> <span class="n">senti_nlp</span><span class="p">([</span><span class="s1">&#39;growth is strong and we have plenty of liquidity.&#39;</span><span class="p">,</span> 
                           <span class="s1">&#39;there is a shortage of capital, and we need extra financing.&#39;</span><span class="p">,</span>
                           <span class="s1">&#39;formulation patents might protect Vasotec to a limited extent.&#39;</span><span class="p">])</span>
<span class="n">senti_results</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [{&#39;label&#39;: &#39;Positive&#39;, &#39;score&#39;: 1.0},
     {&#39;label&#39;: &#39;Negative&#39;, &#39;score&#39;: 0.9952379465103149},
     {&#39;label&#39;: &#39;Neutral&#39;, &#39;score&#39;: 0.9979718327522278}]
</code></pre></div><p><br><br></p>
<h2 id="二esg分类">二、ESG分类</h2>
<p>ESG 分析可以帮助投资者确定企业的长期可持续性并识别相关风险。 FinBERT-ESG 是一个 FinBERT 模型，根据来自公司 ESG 报告和年度报告的 2,000 个手动注释句子进行微调。</p>
<ul>
<li><strong>输入</strong>：金融文本。</li>
<li><strong>输出</strong>：Environmental, Social, Governance or None.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">esg_finbert</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-esg&#39;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">esg_tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-esg&#39;</span><span class="p">)</span>
<span class="n">esg_nlp</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&#34;text-classification&#34;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">esg_finbert</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">esg_tokenizer</span><span class="p">)</span>
</code></pre></div><p><br>使用3条测试文本进行测试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">esg_results</span> <span class="o">=</span> <span class="n">esg_nlp</span><span class="p">([</span><span class="s1">&#39;Managing and working to mitigate the impact our operations have on the environment is a core element of our business.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;Rhonda has been volunteering for several years for a variety of charitable community programs.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;Cabot</span><span class="se">\&#39;</span><span class="s1">s annual statements are audited annually by an independent registered public accounting firm.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;As of December 31, 2012, the 2011 Term Loan had a principal balance of $492.5 million.&#39;</span><span class="p">])</span>

<span class="n">esg_results</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [{&#39;label&#39;: &#39;Environmental&#39;, &#39;score&#39;: 0.9805498719215393},
     {&#39;label&#39;: &#39;Social&#39;, &#39;score&#39;: 0.9906041026115417},
     {&#39;label&#39;: &#39;Governance&#39;, &#39;score&#39;: 0.6738430857658386},
     {&#39;label&#39;: &#39;None&#39;, &#39;score&#39;: 0.9960240125656128}]
</code></pre></div><p><br><br></p>
<h2 id="三fls识别">三、FLS识别</h2>
<p><strong>前瞻性陈述 (FLS)</strong> 告知投资者经理人对公司未来事件或结果的信念和意见。 从公司报告中识别前瞻性陈述可以帮助投资者进行财务分析。 FinBERT-FLS 是一个 FinBERT 模型，它基于罗素 3000 家公司年报的管理讨论和分析部分的 3,500 个手动注释的句子进行了微调。</p>
<ul>
<li><strong>输入</strong>：金融文本。</li>
<li><strong>输出</strong>：Specific-FLS(特定 FLS) , Non-specific FLS(非特定 FLS),  Not-FLS(非 FLS)。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">pipeline</span>

<span class="n">fls_finbert</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-fls&#39;</span><span class="p">,</span><span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">fls_tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;yiyanghkust/finbert-fls&#39;</span><span class="p">)</span>

<span class="n">fls_nlp</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&#34;text-classification&#34;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">fls_finbert</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">fls_tokenizer</span><span class="p">)</span>
</code></pre></div><p><br> 使用3条测试文本进行测试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">fls_results</span> <span class="o">=</span> <span class="n">fls_nlp</span><span class="p">([</span><span class="s1">&#39;we expect the age of our fleet to enhance availability and reliability due to reduced downtime for repairs.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;on an equivalent unit of production basis, general and administrative expenses declined 24 percent from 1994 to $.67 per boe.&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;we will continue to assess the need for a valuation allowance against deferred tax assets considering all available evidence obtained in future reporting periods.&#39;</span><span class="p">])</span>


<span class="n">fls_results</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    [{&#39;label&#39;: &#39;Specific FLS&#39;, &#39;score&#39;: 0.7727874517440796},
     {&#39;label&#39;: &#39;Not FLS&#39;, &#39;score&#39;: 0.9905241131782532},
     {&#39;label&#39;: &#39;Non-specific FLS&#39;, &#39;score&#39;: 0.975904107093811}]
</code></pre></div><p><br><br></p>
<h2 id="文档及引用说明">文档及引用说明</h2>
<ul>
<li>
<p>文档github地址 <a href="https://github.com/yya518/FinBERT">https://github.com/yya518/FinBERT</a></p>
</li>
<li>
<p>作者博客: <a href="https://yya518.github.io/research">https://yya518.github.io/research</a></p>
</li>
</ul>
<br>
<p>Huang, Allen H., Hui Wang, and Yi Yang. &ldquo;FinBERT: A large language model for extracting information from financial text.&rdquo; <strong>Contemporary Accounting Research (2022)</strong>.</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>PNAS | 使用语义距离测量一个人的创新力(发散思维)得分</title>
      <link>https://textdata.cn/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/</link>
      <pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/</guid>
      <description>使用语义距离测量一个人的创新力(发散思维)得分</description>
      <content:encoded><![CDATA[<br>
<p>传统测量 <strong>被试者创造力</strong> 存在耗费时间、主观性太强、缺乏客观性，且所得到的分值是不稳定的，无法跨时间、文化、群体进行分值比较。该研究分析了创新力的两大理论，即联系理论和执行理论，即创新力是包含思维的广度和深度两方面。</p>
<ul>
<li><strong>联系理论(广度)</strong> 负责搜寻所有可能方案的集合，增加集合的规模，体现思维的广度。</li>
<li><strong>执行理论(深度)</strong> 负责寻找最佳方案，并将方案落实执行，体现思维的深度。</li>
</ul>
<p>结合Glove词嵌入技术，将每个词理解为一个技术或知识，两词语语义越相似，发散性越低。</p>
<p>文中让被试按照一定规则，随意填写10个名词，使用其中7个有效词语测量被试的创新力(发散性)思维。可以简单的把7个词理解为知识或者技术，7个词语会形成21种词语对(组合)。最后求均值可以测量出被试词语对的语义距离体现创新发散性的强度。<strong>文末含案例代码</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Olson, J.A., Nahas, J., Chmoulevitch, D., Cropper, S.J. and Webb, M.E., 2021. Naming unrelated words predicts creativity. Proceedings of the National Academy of Sciences, 118(25), p.e2022340118.
</code></pre></div><p><br><br></p>
<h2 id="一摘要">一、摘要</h2>
<p><strong>一些理论认为，有 创造力 的人能够产生更多 发散性 的想法。如果这是正确的，简单地让被试写 N 个不相关的单词，然后测量这N个词的语义距离， 作为 #发散思维 的客观衡量标准</strong>。为了验证这一假设，我们要求 8,914 名参与者说出 10 个彼此尽可能不同的单词。</p>
<p>然后计算算法估计单词之间的平均语义距离；<strong>相关词（例如 cat 和 dog）比不相关词（例如 cat 和 thimble）的距离更短。我们预测，产生更大语义距离的人也会在传统的创造力测量中得分更高</strong>。</p>
<p>在研究 1 中，我们发现语义距离与两个广泛使用的创造力测量（替代用途任务和桥接关联差距任务）之间存在中度至强相关性。在研究 2 中，参与者来自 98 个国家，语义距离仅因基本人口变量而略有不同。在一系列已知可预测创造力的问题上，语义距离与表现之间也存在正相关关系。</p>
<p>总体而言， <strong>语义距离</strong> 与已建立的 创造力测量 的相关性至少与这些测量彼此之间的相关性一样强。 因此，在我们所说的发散关联任务中命名不相关的词可以作为发散思维的简短、可靠和客观的衡量标准。</p>
<br>
<h2 id="二创新力理论">二、创新力理论</h2>
<p>想出 3 个尽可能不同的词。根据两种主要的创造力理论 (1, 2)，选择这些词依赖于产生 #远程联想 ，同时抑制 #常见联想 。</p>
<p>#联想理论 (Associative Theory)认为，有创造力的人具有语义记忆结构，可以更容易地链接远程元素 (3-6)。</p>
<p>#执行理论 (Executive Theory) 侧重于自上而下的注意力控制；创造性的解决方案来自于监测和抑制共同的联想 (2, 7)。</p>
<p>基于这些理论，我们假设 <strong>填写n个无关单词的任务</strong> 可以可靠地衡量 #语言创造力 。 <strong>创造力有两个主要的心理成分， 收敛思维和发散思维，它们在产生创意输出时协同工作</strong>。收敛性思维任务衡量评估多种刺激并得出最适当响应的能力，例如问题的最佳解决方案 (3, 8-10)。这些任务往往更容易得分，因为只有一小部分正确答案。<strong>相比之下，发散思维任务通常使用开放式问题来衡量一个人产生各种解决方案的能力</strong> (11-13)。它们通常需要更长的回答(文本)，因此更难客观评分。</p>
<br>
<h2 id="三创新力测量">三、创新力测量</h2>
<h3 id="31--替代用途任务">3.1  替代用途任务</h3>
<p>最常见的发散思维测量是 <strong>替代用途任务</strong> Alternative Uses Task (14, 15)，在该任务中，参与者生成常见物体的用途，例如回形针或鞋子。使用常用的评分方法 (16)，评分者然后根据三个组成部分来判断回答：</p>
<ul>
<li>灵活性，产生的不同用途类别的数量；</li>
<li>独创性，每次使用相对于样本的其余部分的稀有程度，这对创造力特别重要（17、18）；和</li>
<li>流畅度，一共产生了多少次使用。</li>
</ul>
<br>
<h3 id="32-离散联系任务">3.2 离散联系任务</h3>
<p>本研究作者开发了 <strong>离散联系任务</strong> (Divergent Association Task， DAT) 的网站， <strong>填写你想到的10个不相关词语， 创造力越丰富的人，填写的词语语义距离往往会更远</strong>。</p>
<p><a href="https://www.datcreativity.com/">https://www.datcreativity.com/</a></p>
<p><img loading="lazy" src="img/1_pnas_divergent_association_task_mainpage.png" alt=""  />
</p>
<h3 id="被试填写10个单词的规则">被试填写10个单词的规则</h3>
<ol>
<li>只能填写英文单词</li>
<li>只能是名词(如事情、物体、概念)</li>
<li>不能填 专有名词（例如，特定的人或地点）</li>
<li>不能填写 专业词（比如技术词）</li>
<li>自己思考这些词，不要只看周围环境的物体。</li>
</ol>
<h3 id="dat算法实现">DAT算法实现</h3>
<ol>
<li>使用Glove预训练模型</li>
<li>选前7个词(一共10个词)， 存在 21个词对（组合）</li>
<li>对21词对， 分别计算词向量的余弦距离，分别乘以100。最终求均值得到DAT得分。</li>
</ol>
<blockquote>
<p>下图是大邓第二次填写得到的DAT得分，第一次只超过了6%的人，这方法第一次准，再测就知道如何提高DAT得分。</p>
</blockquote>
<p><img loading="lazy" src="img/2_pnas_divergent_association_task_result.png" alt=""  />
</p>
<p>DAT得分范围0-200， 得分为0可能是7个有效词之间语义相同，而得分200可能是有效词之间彼此语义完全不相同。实践中，得分大多处于65~90之间，且很少超过100。</p>
<p><img loading="lazy" src="img/pnas_dat_score_low_median_high.jpg" alt=""  />
</p>
<blockquote>
<p>词嵌入技术可以把每个词转化为等长的向量，而不同词语共处于相同的语义空间中。常见的词嵌入技术有word2vec、Glove、flastText等，因为最近有学者在 <strong>替代用途任务</strong>(Alternative Uses Task）中用过Glove算法，本文采用Glove算法。本研究使用的Glove预训练模型来自Common Crawl Corpus项目，该项目拥有数十亿网页文本数据。</p>
<p>为了提供冗余， 只采用 被试者 填写的前7个词作为有效单词(DAT的被试需要填写10个词)。DAT得分是这些词之间的语义距离的平均值，具体计算方法， 7个词两两相关的组合有 42种组合， 选择其中最有可能的 21 个语义组合。</p>
</blockquote>
<br>
<h2 id="四实验">四、实验</h2>
<p>这种发散思维的操作化是基于创造力的联想和执行控制理论。 更高的分数将显示出更大的能力来利用更远程的关联 (3-5) 或抑制过度相关的关联 (2, 7)。</p>
<p>在研究 1 中，我们通过将 DAT 与其他两种创造力测量方法进行比较来检验这一假设：替代用途任务 (15) 和桥接关联差距任务 (36)。
<img loading="lazy" src="img/pnas_dat_aut_algo_valid_num.jpg" alt=""  />
</p>
<p>在研究 2 中，我们测试了这些分数如何随人口统计而变化，以及它们是否与更大数据集中与发散性思维相关的其他测量值相关 (9, 37)。 这些研究评估了语义距离是否可以作为发散思维的可靠指标。
<img loading="lazy" src="img/pnas_dat_gender_age.jpg" alt=""  />
</p>
<br>
<h2 id="五讨论">五、讨论</h2>
<p>研究结果表面， 让被试简单的填写10个不想管单词的任务可以作为 测量发散思维 的可靠衡量标准。在研究中， 将这项任务的表现与已有的两种创造力量表做了比较，具有很高的相关性。</p>
<p>总体而言支持了语义发散性，尽管这种联系背后的确切机制尚不清楚，但在创新力最主要的两个理论，即联想理论或执行理论 的联系网络中衡量网络的范围或效率。</p>
<p><strong>DAT算法表现稳定，方差不随人口统计特征变化出现显著性变化（研究2），可以在跨年龄、跨性别的情况下应用</strong>。</p>
<br>
<h3 id="51-dat的优点">5.1 DAT的优点</h3>
<ul>
<li>操作简单，快捷，客观，节约了大量的人力时间，又能保证客观性。</li>
<li>得分绝对，可比较，可以用于测量不同群体(种族、文化、性别、年龄)的创造力得分。</li>
<li>对被试友好，一般一两分钟即可完成。</li>
</ul>
<h3 id="52-dat的不足">5.2 DAT的不足</h3>
<ul>
<li>创造力有发散性和执行力，发散性负责搜选所有方案集合的规模，而执行力是从方案集中选出最优方案并将其执行。DAT测量的仅仅是发散性思维。</li>
<li>被试可能通过填写稀奇的词语提高DAT得分。</li>
<li>只有短短几分钟，被试可能很难短时间内了解实验规则。</li>
</ul>
<h3 id="53-未来展望">5.3 未来展望</h3>
<p>DAT得分取决于Glove模型、语料库(数据集), 更新词模型或语料库，被试的DAT得分会发生变化。为简单起见，本研究使用免费的预训练模型， 通过一些努力，未来研究者可以对不同时期，不同国家的语料库来训练Glove模型。随着特定单词关联或多或少的联系， 更新的模型将会自动考虑这些变化，这将允许DAT得分跨越文化跨越时代，进行创新力的比较。</p>
<p><br><br></p>
<h2 id="代码">代码</h2>
<p>代码的文档说明请点击 github仓库地址 <a href="https://github.com/jayolson/divergent-association-task">https://github.com/jayolson/divergent-association-task</a> 查看。这里仅粘贴作者源代码，源代码需要配置好才可运行。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">dat</span>

<span class="c1">## 从 https://nlp.stanford.edu/projects/glove/ 下载Glove模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">dat</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="s2">&#34;glove.840B.300d.txt&#34;</span><span class="p">,</span> <span class="s2">&#34;words.txt&#34;</span><span class="p">)</span>

<span class="c1"># 验证词语，如输入的是词组，代码会将其转为连线形式的单词</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="s2">&#34;cul de sac&#34;</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cul-de-sac
</code></pre></div><br>
<p>计算两个词语之间的语义距离</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;dog&#34;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;thimble&#34;</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.1983
0.8787
</code></pre></div><br>
<p>计算词对的DAT得分（语义cosine距离*100）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">([</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;dog&#34;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">([</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;thimble&#34;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">19.83
87.87
</code></pre></div><br>
<p>假设有三个人分别都填写10个词，选其前7个词作为有效词。有效词如下，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">low</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;arm&#34;</span><span class="p">,</span> <span class="s2">&#34;eyes&#34;</span><span class="p">,</span> <span class="s2">&#34;feet&#34;</span><span class="p">,</span> <span class="s2">&#34;hand&#34;</span><span class="p">,</span> <span class="s2">&#34;head&#34;</span><span class="p">,</span> <span class="s2">&#34;leg&#34;</span><span class="p">,</span> <span class="s2">&#34;body&#34;</span><span class="p">]</span>
<span class="n">average</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;bag&#34;</span><span class="p">,</span> <span class="s2">&#34;bee&#34;</span><span class="p">,</span> <span class="s2">&#34;burger&#34;</span><span class="p">,</span> <span class="s2">&#34;feast&#34;</span><span class="p">,</span> <span class="s2">&#34;office&#34;</span><span class="p">,</span> <span class="s2">&#34;shoes&#34;</span><span class="p">,</span> <span class="s2">&#34;tree&#34;</span><span class="p">]</span>
<span class="n">high</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;hippo&#34;</span><span class="p">,</span> <span class="s2">&#34;jumper&#34;</span><span class="p">,</span> <span class="s2">&#34;machinery&#34;</span><span class="p">,</span> <span class="s2">&#34;prickle&#34;</span><span class="p">,</span> <span class="s2">&#34;tickets&#34;</span><span class="p">,</span> <span class="s2">&#34;tomato&#34;</span><span class="p">,</span> <span class="s2">&#34;violin&#34;</span><span class="p">]</span>

<span class="c1"># Compute the DAT score (transformed average cosine distance of first 7 valid words)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">low</span><span class="p">))</span> <span class="c1"># 50</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">average</span><span class="p">))</span> <span class="c1"># 78</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">high</span><span class="p">))</span> <span class="c1"># 95</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">50
78
95
</code></pre></div><p>需要注意pnas作者公开的代码只能用在英文，且无法自己训练Glove模型。如果想基于自有数据集（中文、英文），训练自有Glove模型，需要学习</p>
<ul>
<li>如何训练Glove模型</li>
<li>如何导入训练好的Glove模型</li>
<li>如何计算中英文dat得分</li>
</ul>
<p>相关知识点已更新至我的录播课课程 <a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>魔搭 | 中文AI模型开源社区</title>
      <link>https://textdata.cn/blog/2022-11-09-chinese-modelscope-open-source/</link>
      <pubDate>Wed, 09 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-09-chinese-modelscope-open-source/</guid>
      <description>ModelScope社区成立于2022 年6月，是一个模型开源社区及创新平台，由阿里巴巴达摩院，联合CCF开源发展委员会，共同作为项目发起方。社区联合国内AI领域合作伙伴与高校机构，致力于通过开放的社区合作，构建深度学习相关的模型开源，并开源相关模型服务创新技术，推动模型应用生态的繁荣发展。</description>
      <content:encoded><![CDATA[<h2 id="关于modelscope">关于ModelScope</h2>
<p>ModelScope社区成立于 2022 年 6 月，是一个模型开源社区及创新平台，由阿里巴巴达摩院，联合CCF开源发展委员会，共同作为项目发起方。</p>
<blockquote>
<p>社区联合国内AI领域合作伙伴与高校机构，致力于通过开放的社区合作，构建深度学习相关的模型开源，并开源相关模型服务创新技术，推动模型应用生态的繁荣发展。</p>
</blockquote>
<p>期待ModelScope会有不一样的表现。</p>
<p>与ModelScope类似的网站有</p>
<ul>
<li>国际 huggingface是较早将AI模型开源的网站，用户群体庞大，社区内有丰富的数据集、模型，文档详实。</li>
<li>国内 百度飞桨是国内AI模型开源较好的网站，用户群体较大，更新活跃，但是文档质量。。。</li>
</ul>
<p>目前ModelScope刚刚上线不久，模型和数据集都不怎么多</p>
<p><img loading="lazy" src="img/model_scope_homepage.png" alt=""  />
</p>
<br>
<h2 id="heading"></h2>
<h1 id="名词解释"><strong>名词解释</strong></h1>
<p>ModelScope平台是以模型为中心的模型开源社区，与模型的使用相关，您需要先了解如下概念。</p>
<table>
<thead>
<tr>
<th><strong>基础概念</strong></th>
<th><strong>定义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>任务</td>
<td>任务（Task）指某一领域具体的应用，以用于完成特定场景的任务。例如图像分类、文本生成、语音识别等，您可根据任务的输入输出找到适合您的应用场景的任务类型，通过任务的筛选来查找您所需的模型。</td>
</tr>
<tr>
<td>模型</td>
<td>模型（Model）是指一个具体的模型实例，包括模型网络结构和相应参数。ModelScope平台提供丰富的模型信息供用户体验与使用。</td>
</tr>
<tr>
<td>模型库</td>
<td>模型库（Modelhub）是指对模型进行存储、版本管理和相关操作的模型服务，用户上传和共享的模型将存储至ModelScope的模型库中，同时用户也可在Model hub中创建属于自己的模型存储库，并沿用平台提供的模型库管理功能进行模型管理。</td>
</tr>
<tr>
<td>数据集</td>
<td>数据集（Dataset）是方便共享及访问的数据集合，可用于算法训练、测试、验证，通常以表格形式出现。按照模态可划分为文本、图像、音频、视频、多模态等。</td>
</tr>
<tr>
<td>数据集库</td>
<td>数据集库（Datasethub）用于集中管理数据，支持模型进行训练、预测等，使各类型数据具备易访问、易管理、易共享的特点。</td>
</tr>
<tr>
<td>ModelScope Library</td>
<td>ModelScope Library是ModelScope平台自研的一套Python Library框架，通过调用特定的方法，用户可以只写短短的几行代码，就可以完成模型的推理、训练和评估等任务，也可以在此基础上快速进行二次开发，实现自己的创新想法。</td>
</tr>
</tbody>
</table>
<br>
<h2 id="一模型探索">一、模型探索</h2>
<p>首先访问平台网址https://www.modelscope.cn/models， 您将看见平台上已有的所有公开模型，根据任务筛选或者关键词搜索可查找您感兴趣的模型。</p>
<p><img loading="lazy" src="img/1-model_explore.png" alt=""  />
</p>
<br>
<h2 id="二环境准备">二、环境准备</h2>
<h3 id="21-本地开发环境">2.1 本地开发环境</h3>
<p>如果您需要在本地运行模型，需要进行相应的环境安装准备，包括：</p>
<ul>
<li><strong>安装python环境</strong>。支持python3，不支持python2，建议3.7版本及以上。我们推荐您使用Anaconda进行安装。</li>
<li><strong>安装深度学习框架</strong>。ModelScope Library目前支持Tensorflow，Pytorch两大深度学习框架进行模型训练、推理。您可根据模型所需的框架选择适合的框架进行安装。</li>
<li><strong>安装ModelScope Library</strong>。我们提供两种安装方式，您可选择适合的方式进行安装。
<ul>
<li>pip安装。ModelScope提供了根据不同领域的安装包，您可根据对应的模型选择所需的安装包。</li>
<li>使用源码安装。</li>
<li>更完整的安装信息参考：环境安装指南。</li>
</ul>
</li>
</ul>
<h3 id="22-在线notebook">2.2 在线Notebook</h3>
<p>若您觉得本地安装较为复杂， ModelScope平台也提供在线的运行环境，您可直接在Notebook中运行，Notebook中提供官方镜像无需自主进行环境安装，更加方便快捷，推荐大家使用！</p>
<p>注意：该功能需要您登录后使用，新用户注册ModelScope账号并完成阿里云账号绑定后即可获得免费算力资源，详情请参阅免费额度说明 。</p>
<p><img loading="lazy" src="img/model_scode_free_online_notebook.png" alt=""  />
</p>
<p><img loading="lazy" src="img/model_scode_free_online_notebook-2.png" alt=""  />
</p>
<br>
<h2 id="三2分钟跑通模型推理">三、2分钟跑通模型推理</h2>
<p>若您准备好本地环境或者已经打开一个Notebook的预装环境实例，则根据下述代码可对该模型进行推理。 使用modelscope pipeline接口只需要两步，同样以上述中文分词模型（damo/nlp_structbert_word-segmentation_chinese-base）为例简单说明：</p>
<p>首先根据task实例化一个pipeline对象</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">modelscope.pipelines</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="n">word_segmentation</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;word-segmentation&#39;</span><span class="p">,</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;damo/nlp_structbert_word-segmentation_chinese-base&#39;</span><span class="p">)</span>
</code></pre></div><p>输入数据，拿到结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">input_str</span> <span class="o">=</span> <span class="s1">&#39;今天天气不错，适合出去游玩&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">word_segmentation</span><span class="p">(</span><span class="n">input_str</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;output&#39;: &#39;今天 天气 不错 ， 适合 出去 游玩&#39;}
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>预训练词向量模型的方法、应用场景、变体延伸与实践总结</title>
      <link>https://textdata.cn/blog/2022-11-07-embeddings-theory-applicaiton-liuhuanyong/</link>
      <pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-07-embeddings-theory-applicaiton-liuhuanyong/</guid>
      <description>预训练词向量模型的方法、应用场景、变体延伸与实践总结</description>
      <content:encoded><![CDATA[<p><br><br></p>
<h2 id="关于作者">关于作者</h2>
<p>刘焕勇，liuhuanyong，现任360人工智能研究院算法专家，前中科院软件所工程师，主要研究方向为知识图谱、事件图谱在实际业务中的落地应用。<br>
得语言者得天下，得语言资源者，分得天下，得语言逻辑者，争得天下。</p>
<ul>
<li>个人主页：https://liuhuanyong.github.io</li>
<li>个人公众号：老刘说NLP</li>
</ul>
<br>
<p>当前，以预训练语言模型PLM+fintune的自然语言处理范式可谓十分火热，有大量的文章在宣传这类方法，包括梳理以NNLM为起点的整个预训练方法的发展史。</p>
<p>当前工业界，主要使用的预训练模型包括两种，一种是以wordvec为代表的预训练词向量，另一种是以BERT为代表的预训练语言模型。前者通常作为词语表示输入的初始化，后接NN/CNN/LSTM等编码层，后者既可以同样后接，也可以直接接上softmax/crf/span-pointer等进行解码。</p>
<p>本文，主要围绕预训练词向量模型这一主题，对当前预训练词向量模型的常用方法、评估与应用方式、领域的迁移变体、当前开放的训练工具和词向量文件进行介绍，并以word2vec和fasttext为例，展示一个词向量训练的案例，做到理论与实践相结合。</p>
<p><br><br></p>
<h2 id="一预训练词向量模型方法">一、预训练词向量模型方法</h2>
<p>自从进入2010年以来，神经语言模型就逐渐进入人们眼球，以NNLM为典型最初代表的神经网络模型，极大的推动了NLP这一领域的发展。</p>
<p>实际上，早期词向量的研究通常来源于语言模型，比如NNLM和RNNLM，其主要目的是语言模型，而词向量只是一个副产物。著名的harris分布式假说提供了一个局部统计信息的理论基础。</p>
<p>下面就选择其中三种典型进行介绍。</p>
<br>
<h3 id="11-word2vec">1.1 word2vec</h3>
<p>word2vec是2013年Google开源的一款用于词向量计算的工具，通过内置的语言模型训练目标，可以将中间层得到的向量权重矩阵进行抽离，形成每个词对应的向量化表示，包括CBOW、Skip-gram两种方式，前者通过周围词来预测中心词，后者以中心词来预测上下文。</p>
<p><img loading="lazy" src="img/1.png" alt=""  />
</p>
<p>经典的wordvec结构包括输入层、隐藏层和输出层，其计算流程为：</p>
<p>1、输入层存储上下文单词的onehot。假设单词向量空间dim为V，上下文单词个数为C。</p>
<p>2、所有onehot分别乘以共享的输入权重矩阵W。V*N矩阵，N为自己设定的数，初始化权重矩阵W 。</p>
<p>3、所得的向量 相加求平均作为隐层向量, size为1*N。</p>
<p>4、乘以输出权重矩阵W' N*V。</p>
<p>5、得到向量1*V，经过激活函数处理得到V-dim概率分布。</p>
<p>6、Hierarchical Softmax分类，概率最大的index所指示的单词为预测出的中间词与预测值的onehot做比较，根据误差更新权重矩阵。</p>
<p><img loading="lazy" src="img/2.png" alt=""  />
</p>
<p>这个W矩阵就是所有单词的word embedding，任何一个单词的onehot乘以这个矩阵都将得到自己的词向量。</p>
<p>通常，在训练词向量时候，会根据语料的大小来选择相应的训练方法。例如，针对小型的数据集，可以用CBOW算法，该方法对于很多分布式信息进行了平滑处理，将一整段上下文信息视为一个单一观察量，对于小型的数据集，这一处理是有帮助的。相比之下，大型数据集，可以用Skip-Gram模型，该方法将每个“上下文-目标词汇”的组合视为一个新观察量，这种做法在大型数据集中会更为有效。</p>
<br>
<h3 id="12-fasttext">1.2 fasttext</h3>
<p>fastText是Facebook于2016年开源的一个词向量计算和文本分类工具。将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。包括字符级n-gram特征的引入以及分层Softmax分类两种。</p>
<p>与CBOW一样，原本的fastText模型包括输入层、隐含层、输出层，输入都是多个经向量表示的单词，输出都是一个特定的目标，隐含层都是对多个词向量的叠加平均。不同的是，CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档，CBOW的输入单词被onehot编码过，fastText的输入特征是经embedding化的，CBOW的输出是目标词汇，fastText的输出是文档对应的类标。</p>
<p>而如果将该类标替换成中间目标词，那么就可以得到wordvec的升级版，即单纯的词向量模型。例如，word2vec把语料库中的每个单词当成原子的，它会为每个单词生成一个向量。这忽略了单词内部的形态特征。</p>
<p>fasttext使用了字符级别的n-grams来表示一个单词。对于单词“apple”，假设n的取值为3，则它的trigram有“&lt;ap”, “app”, “ppl”, “ple”, “le&gt;”，其中，&lt;表示前缀，&gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，可以用这5个trigram的向量叠加来表示“apple”的词向量。</p>
<p>因此，因为它们的n-gram可以和其它词共享，对于训练词库之外的单词，能够解决或者oov词，这也是在当前很多文本分类、推荐场景中会优先选用fastText作为训练方法。</p>
<br>
<h3 id="13-glove">1.3 Glove</h3>
<p>GloVe是斯坦福团队于2014年提出一个词向量方法，全名叫“Global Vectors”，直接利用全局的统计信息进行训练。</p>
<p>与上述两种方式靠滑动窗口来制造局部上下文不同，GloVe会用到全局的词语之间共现的统计信息，即词的出现次数，词对之间的共现概率，形成共现概率矩阵，并试图生成词向量来毕竟共现概率，利用Word2Vec的skip-gram算法的高性能来解决LDA的计算量复杂问题。</p>
<p>因此，我们可以发现，Glove需要事先统计共现概率，这也让其通常被认为是无监督学习，实际上glove还是有label的，即共现次数。与wordvec还有一处不同的是，损失函数是最小平方损失函数，权重可以做映射变换。</p>
<p><br><br></p>
<h2 id="二预训练词向量的训练参数">二、预训练词向量的训练参数</h2>
<p>词向量模型的超参数很多，不同的参数选择会取得不同的效果，并且，word2vec中有几个大家提的比较多的问题。以gensim-word2vec为例，包括以下参数：</p>
<ul>
<li>sentences： 可以是一个list，对于大语料集，可使用BrownCorpus,Text8Corpus或LineSentence构建；</li>
<li>sg： 用于设置训练算法，默认为0，对应CBOW算法；sg=1则采用skip-gram算法；</li>
<li>size： 特征向量的维度，默认为100。大的size需要更多的训练数据,但是效果会更好. 推荐值为几十到几百；</li>
<li>window： 表示当前词与预测词在一个句子中的最大距离是多少；</li>
<li>alpha: 学习速率；</li>
<li>seed： 用于随机数发生器。与初始化词向量有关；</li>
<li>min_count: 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5；</li>
<li>max_vocab_size: 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制；</li>
<li>sample: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)；workers参数控制训练的并行数；</li>
<li>hs: 如果为1则会采用hierarchical softmax技巧。如果设置为0（defaut），则negative sampling会被使用；</li>
<li>negative: 如果&gt;0,则会采用negativesamping，用于设置多少个noise words；</li>
<li>cbow_mean: 如果为0，则采用上下文词向量的和，如果为1（defaut）则采用均值。只有使用CBOW的时候才起作用；</li>
<li>hashfxn： hash函数来初始化权重。默认使用python的hash函数；</li>
<li>iter： 迭代次数，默认为5；</li>
<li>trim_rule： 用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RUE_DISCARD,utis.RUE_KEEP或者utis.RUE_DEFAUT的函数；</li>
<li>sorted_vocab： 如果为1（defaut），则在分配word index 的时候会先对单词基于频率降序排序；</li>
<li>batch_words： 每一批的传递给线程的单词的数量，默认为10000。</li>
</ul>
<p>不过，如此多的参数不一定能跳得过来，因此通常会集中在以下常规参数：</p>
<p><img loading="lazy" src="img/3.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三预训练词向量的评估与应用">三、预训练词向量的评估与应用</h2>
<p>预训练词向量生产出来，需要进行性能的评估。这方面的方法包括基于评测集，或者基于具体业务使用，用业务的指标来进行评估。</p>
<h3 id="31-预训练词向量的评估">3.1 预训练词向量的评估</h3>
<p>学术上，词向量的质量通常由类比问题任务进行评估。如CA-translated包含了三个语义问题和134个中文词。CA8 是专门为中文语言设计的。它包含了 17813 个类比问题，覆盖了综合的词法和语义关联。</p>
<p>工业，则使用词向量来代替之前随机生成的词向量文件，来对自然语言处理中的文本/情感分类、实体识别、关系抽取等任务进行评估。</p>
<br>
<h3 id="32-预训练词向量的应用">3.2 预训练词向量的应用</h3>
<p>预训练词向量文件最大的价值在于解决了一个词语的初始化稠密表示，在解决当前以数值化为输入的深度或机器学习模型第一部的同时，还保留了一个词的区别性特征。</p>
<p>一方面，当前词向量可以用于近义词挖掘的重要来源，通过某个词，通过计算词与其他词之间的相似度，并设定阈值，可以迭代挖掘出大量的相关词【过程中要注意语义漂移】。而这个词，直接就可以用于当前的搜索查询扩展、领域词构建等场景。进一步的，在模型方面，还可以作为EDA数据增强工作中的重要补充。</p>
<p>另一方面，词向量可以用于当前无监督文本表示的重要方法，通过对文本进行分词，然后找到词语对应的向量，通过向量叠加的方式可以快速得到一个文本的向量表示，这一表示在诸如情感分析、句子相似度计算等任务中是实际有效的，基于文本表示，也可以进一步提升文本分类、聚类、相似query召回等使用场景性能，甚至很形象的成为了当前业务模型的baseline或者兜底模型。</p>
<p><br><br></p>
<h2 id="四预训练词向量的变体延伸">四、预训练词向量的变体延伸</h2>
<h3 id="41-gramembedding">4.1 gramEmbedding</h3>
<p>共现信息，是cbow以及skipgram的基础，其本质在于通过周围词来建模中心词或者用中心词来建模周围词。因此，通过构造不同的共现信息，可以得到不同类型的向量形式。这里取了个名字叫gramembedding，用于表示专指文本的一系列embedding变体。</p>
<p>例如，对于一个词来说，我们可以把词拆分为词word、n元序列ngram、汉字character，偏旁部首Radical，词性POS，依存关系dependency、拼音pinying。</p>
<p>单元的共现，我们同样可以进行组合，例如，构造word-word，word-ngram、ngran-ngram等，得到上下文特征（单词、n-gram、字符等）等不同粒度的词向量。</p>
<p>观察近几年的发展，词向量可以进一步分成偏旁部首向量、字符向量等。如香侬科技推出的glyce向量，引入汉字的字形特征。蚂蚁金服推出的cw2vec字符向量，将汉字拆解成偏旁、字件进行建模。</p>
<p><img loading="lazy" src="img/4.png" alt=""  />
</p>
<p>当ngram中的n为1时，可以得到字向量，n为2或者更多时，则可以得到词向量等。fasttext中，就是得到了ngram的向量，并进行加和，得到一个OOV词语的向量进行表示。</p>
<p>例如，基于skigram，分别设定词向量的维度及其他超参数，可以得到字向量,拼音向量，词向量，词性向量，通过上下文共现与PCA降维的方法可以得到依存向量。</p>
<p><img loading="lazy" src="img/5.png" alt=""  />
</p>
<p>从下面的结果可以看出，词和字向量的效果看起来还不错。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    ***********************字符向量************************
    token:刘
    (&#39;李&#39;, 0.7306396961212158),(&#39;陈&#39;, 0.7201231122016907)
    (&#39;赵&#39;, 0.6974461674690247),(&#39;杨&#39;, 0.6972213983535767)
    (&#39;吴&#39;, 0.6851627230644226),(&#39;徐&#39;, 0.6516467332839966)
    (&#39;郭&#39;, 0.6499480605125427),(&#39;蔡&#39;, 0.6175302267074585)
    (&#39;郑&#39;, 0.6092196106910706),(&#39;孙&#39;, 0.5950524210929871)
    token:丑
    (&#39;卯&#39;, 0.6074919700622559),(&#39;酉&#39;, 0.5910211801528931)
    (&#39;巳&#39;, 0.5581363439559937),(&#39;戌&#39;, 0.43932047486305237)
    (&#39;戊&#39;, 0.41449615359306335),(&#39;壬&#39;, 0.40456631779670715)
    (&#39;謤&#39;, 0.367109090089798),(&#39;绯&#39;, 0.3643313944339752),
    (&#39;寅&#39;, 0.36351141333580017),(&#39;旽&#39;, 0.3549465537071228)

    ***********************依存向量************************
    dependency rel:ATT
    (&#39;COO&#39;, 0.14239487051963806),(&#39;ADV&#39;, -0.16987691819667816)
    (&#39;RAD&#39;, -0.2357601821422577),(&#39;HED&#39;, -0.2401314228773117)
    (&#39;SBV&#39;, -0.25625932216644287),(&#39;WP&#39;, -0.27165737748146057)
    (&#39;LAD&#39;, -0.2902592420578003),(&#39;POB&#39;, -0.2990782558917999)
    (&#39;VOB&#39;, -0.37553706765174866),(&#39;IOB&#39;, -0.6669262647628784)
    dependency rel:POB
    (&#39;IOB&#39;, 0.16698899865150452),(&#39;DBL&#39;, 0.16678886115550995)
    (&#39;FOB&#39;, 0.1657436639070511),(&#39;CMP&#39;, 0.14784857630729675)
    (&#39;VOB&#39;, 0.1461176574230194),(&#39;SBV&#39;, 0.08011472970247269)
    (&#39;LAD&#39;, -0.022307466715574265),(&#39;WP&#39;, -0.022942926734685898)
    (&#39;HED&#39;, -0.037264980375766754),(&#39;RAD&#39;, -0.042251598089933395)

    ***********************拼音向量************************
    pinyin:wo
    (&#39;shei&#39;, 0.6129732131958008)(&#39;ta&#39;, 0.6081706285476685)
    (&#39;nin&#39;, 0.5819231867790222),(&#39;！&#39;, 0.5435523986816406)
    (&#39;……&#39;, 0.48428624868392944),(&#39;ai&#39;, 0.47832390666007996)
    (&#39;o&#39;, 0.4761071801185608),(&#39;。』&#39;, 0.4598163366317749)
    (&#39;...&#39;, 0.45207729935646057),(&#39;ni&#39;, 0.44975683093070984)
    pinyin:guo
    (&#39;dang&#39;, 0.3908974528312683),(&#39;yuan&#39;, 0.378823846578598)
    (&#39;zu&#39;, 0.35387369990348816),(&#39;hua&#39;, 0.3405681848526001)
    (&#39;zheng&#39;, 0.3355437219142914),(&#39;yi&#39;, 0.3333034813404083)
    (&#39;ren&#39;, 0.3194104731082916),(&#39;jun&#39;, 0.3187354505062103)
    (&#39;hui&#39;, 0.31342023611068726),(&#39;xin&#39;, 0.3096797466278076)

    ***********************词性向量************************
    word postag:a
    (&#39;d&#39;, 0.7203904986381531),(&#39;c&#39;, 0.6124969720840454)
    (&#39;v&#39;, 0.4963228106498718),(&#39;an&#39;, 0.4531499147415161)
    (&#39;uz&#39;, 0.4459834396839142),(&#39;ud&#39;, 0.42059916257858276)
    (&#39;r&#39;, 0.4090540111064911),(&#39;uj&#39;, 0.4061364233493805)
    (&#39;i&#39;, 0.38707998394966125),(&#39;l&#39;, 0.3551557660102844)
    word postag:n
    (&#39;b&#39;, 0.7030695676803589),(&#39;vn&#39;, 0.490166038274765)
    (&#39;p&#39;, 0.4858315885066986),(&#39;v&#39;, 0.4499088227748871)
    (&#39;nt&#39;, 0.44155171513557434),(&#39;f&#39;, 0.26609259843826294)
    (&#39;s&#39;, 0.2639649212360382),(&#39;l&#39;, 0.24365971982479095)
    (&#39;ns&#39;, 0.2278469204902649),(&#39;m&#39;, 0.202927365899086)
    ***********************词向量************************
    word:爱情
    (&#39;爱恋&#39;, 0.6931096315383911),(&#39;真爱&#39;, 0.6897798776626587)
    (&#39;婚姻&#39;, 0.6540514826774597),(&#39;浪漫爱情&#39;, 0.6535360813140869)
    (&#39;情感&#39;, 0.6501022577285767),(&#39;感情&#39;, 0.6403399705886841)
    (&#39;纯爱&#39;, 0.6394841074943542),(&#39;爱情故事&#39;, 0.6282097101211548)
    (&#39;校园爱情&#39;, 0.6078493595123291),(&#39;情爱&#39;, 0.5976818799972534)
    word:创新
    (&#39;技术创新&#39;, 0.7648976445198059),(&#39;不断创新&#39;, 0.7172579765319824)
    (&#39;创新型&#39;, 0.6573833227157593),(&#39;创新能力&#39;, 0.6533682942390442)
    (&#39;创新性&#39;, 0.6160774827003479),(&#39;革新&#39;, 0.6159394383430481)
    (&#39;人才培养&#39;, 0.6093565821647644),(&#39;开拓创新&#39;, 0.6015594601631165)
    (&#39;探索&#39;, 0.5987343788146973),(&#39;技术革新&#39;, 0.5949685573577881)
</code></pre></div><p>从上，也看到一些十分有趣的现象：</p>
<p>1）依存向量，依存向量中可以看出，ATT作为定中关系，在依存关系中属于定中结构，COO(联合)，ADV(状中)的相似度要比主谓SBV，动宾VOB的相似度要高。另外，作为介宾的POB，相似的有IOB，DBL，FOB，这些关系均与宾语成分相关</p>
<p>2）拼音向量，从wo，guo的拼音相似拼音来看，我们可以看到，这种相似的拼音更像是一种搭配， 很有意思，(词性参照jieba分词词性对照表)。</p>
<p>3）词性向量，从a，n的相似词性来看，也似乎更像是一种搭配现象，或许有更好的解释。</p>
<br>
<h3 id="42-domainembedding">4.2 DomainEmbedding</h3>
<p>为了更好的适配不同领域的任务，当前也有很多的公司或者任务会选择使用领域性的领域进行训练，以得到不同领域的词向量文件，这与当前各种领域的bert模型做法是类似的。当前出现了金融领域bert、法律领域的bert等。</p>
<p>代表性的，2018年推出的Chinese-Word-Vectors中提供了包含经过数十种用各领域语料（百度百科、维基百科、人民日报 1947-2017、知乎、微博、文学、金融、古汉语等）训练的词向量，涵盖各领域，且包含多种训练设置。</p>
<p><img loading="lazy" src="img/6.png" alt=""  />
</p>
<p>又如，当前PaddleNLP官方提供了61种可直接加载的预训练词向量，训练自多领域中英文语料、如百度百科、新闻语料、微博等，覆盖多种经典词向量模型（word2vec、glove、fastText）、涵盖不同维度、不同语料库大小。</p>
<br>
<h3 id="43-graphembdding">4.3 GraphEmbdding</h3>
<p>经典的deepwalk以及node2vec也是借鉴word2vec思想，学习图节点嵌入的方法。并且成为当前推荐系统中的一个重量级使用方法。</p>
<p><strong>1、Deepwalk</strong></p>
<p>通过对图中的节点进行随机游走（主要考虑深度优先遍历），形成节点之间的游走序列，并将其作为上下文，后面接入skipgram形成节点向量，从构造上来看，就是首先利用random walk来表示图结构，然后利用skip-gram模型来更新学习节点表示。</p>
<p>随机选取与其邻接的下一个结点，直至达到给定长度，这个长度作为一个参数进行指定，这个类似于word2vec中的window_size上下文窗口。</p>
<p><img loading="lazy" src="img/7.png" alt=""  />
</p>
<p><strong>2、node2vec</strong></p>
<p>node2vec综合考虑了广度优先遍历（用于捕捉局部信息）和深度优先遍历（用于捕捉全局信息）的游走，提出二阶随机游走思想，解决内容相似和结构相似的问题。</p>
<p><img loading="lazy" src="img/8.png" alt=""  />
</p>
<p>前者具有直接链接关系的两个节点，我们可以认为是内容相似的（例如两个灰色网站之间很有可能能够直接跳转，如图中的s1，s2等一阶邻居）、结构相似（例如周围邻居数量都很类似，如图中的s6和u节点，两个都有4个邻接，结构类似）。</p>
<p><img loading="lazy" src="img/9.png" alt=""  />
</p>
<p>具体实现思路也很简单：</p>
<p>我们从节点v转移到节点t，并且当前在节点t时，需要考虑下一个采样节点x。因此，可以设计一个节点到它的不同邻居的转移概率：</p>
<p><img loading="lazy" src="img/10.png" alt=""  />
</p>
<p>其中，每一步采样都会有三种状态，分别对应于上图的0，1，2三种情况：</p>
<ul>
<li><strong>1）0代表如果t和x相等，那么采样的概率为1/p；</strong></li>
<li><strong>2）1代表t与x相连，采样的概率为1；</strong></li>
<li>3）2代表t与x不相连，采样的概率为1/q**</li>
</ul>
<p>式子中的参数p作为返回参数，控制重新采样上一步已访问节点的概率。参数q，作为出入参数，控制采样的方向。</p>
<p>其中：</p>
<ul>
<li><strong>1）当q&gt;1时，接下来采样的节点倾向于节点t，偏向于广度优先；</strong></li>
<li><strong>2）当q&lt;1时，接下来采样的节点倾向于远离t，偏向于深度优先遍历。</strong></li>
<li><strong>3）当p&gt;max(q,1)时，接下来采样的节点很大概率不是之前已访问节点，这一方法使得采样偏向深度优先；</strong></li>
<li><strong>4）当p&lt;max(q,1)时，接下来采样的节点很大概率是之前已访问节点，这一方法使得采样偏向广度优先。</strong></li>
</ul>
<p>此外，在推荐场景中也有item2vec的类似延伸，例如协同过滤算法是建立在一个user-item的co-occurrence矩阵的基础上，通过行向量或列向量的相似性进行推荐。如果将同一个user购买的item视为一个context，就可以建立一个item-context的矩阵。进一步的，可以在这个矩阵上借鉴CBoW模型或Skip-gram模型计算出item的向量表达。</p>
<p><br><br></p>
<h2 id="五预训练词向量的动手实操">五、预训练词向量的动手实操</h2>
<p>纸上得来终觉浅，觉知此事要躬行，能够动手实践是加强对该概念理解的重要方式。预训练词向量，在流程上，应该包括全量训练和增量训练两种。前者可以在有大规模训练语料的情况下得到领域的向量，后者适用于小语料微调。
下面以gemsim中的wordvec和fasttext为例进行实践，大家可以看出其中的一些具体的步骤和结果。</p>
<h3 id="51-word2vec向量训练">5.1 word2vec向量训练</h3>
<h4 id="1构造训练语料">1、构造训练语料</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># coding = utf-8</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="n">cur</span> <span class="o">=</span> <span class="s1">&#39;/&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">Trainvec</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;lawsuit.json&#34;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;duanzi.txt&#34;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">build_corpus</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">train_path</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;train.txt&#34;</span><span class="p">),</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">json_obj</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">json_obj</span><span class="p">[</span><span class="s2">&#34;content&#34;</span><span class="p">]</span>
                <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
                <span class="n">cut_wds</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
                <span class="n">train_path</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cut_wds</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">train_path</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">build_update_corpus</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">train_path</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;update.txt&#34;</span><span class="p">),</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">update_filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">cut_wds</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                <span class="n">train_path</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">cut_wds</span> <span class="k">if</span> <span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">train_path</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
	  <span class="n">handler</span> <span class="o">=</span> <span class="n">Trainvec</span><span class="p">()</span>
    <span class="c1">#handler.build_corpus()</span>
    <span class="n">handler</span><span class="o">.</span><span class="n">build_update_corpus</span><span class="p">()</span>
</code></pre></div><br>
<h4 id="2配置输入与输出路径">2、配置输入与输出路径</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">word2vec</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="n">cur</span> <span class="o">=</span> <span class="s1">&#39;/&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;train.txt&#34;</span><span class="p">)</span>
<span class="n">update_filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">cur</span><span class="p">,</span> <span class="s2">&#34;update.txt&#34;</span><span class="p">)</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec.model&#34;</span>
<span class="n">model_update_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec_update.model&#34;</span>
<span class="n">model_vec_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec.bin&#34;</span>
<span class="n">model_update_vec_path</span> <span class="o">=</span> <span class="s2">&#34;wordvec_update.bin&#34;</span>
</code></pre></div><br>
<h4 id="3全量数据预训练">3、全量数据预训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">full_train_embedding</span><span class="p">():</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">min_word_count</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">num_workers</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">context</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">downsampling</span> <span class="o">=</span> <span class="mf">1e-3</span>
    <span class="c1"># 获取日志信息</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># 加载分词后的文本，使用的是Text8Corpus类</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Text8Corpus</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="c1"># 训练模型，部分参数如下</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
                              <span class="n">size</span><span class="o">=</span><span class="n">num_features</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_word_count</span><span class="p">,</span>
                              <span class="n">window</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="n">downsampling</span><span class="p">)</span>
    <span class="c1">#保存模型,除包含词-向量,还保存词频等训练所需信息</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1">#保存词向量文件,保存的模型仅包含词-向量信息</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div><p>在保存过程中，存在两种方式，保存模型,除包含词-向量,还保存词频等训练所需信息，保存词向量文件,保存的模型仅包含词-向量信息。所以我们可以看到，词向量文件，确实是word2vec模型的副产物。</p>
<br>
<h4 id="4增量数据预训练">4、增量数据预训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">增量训练</span><span class="err">，</span><span class="n">主要解决在新的文本上进行训练</span><span class="err">，</span><span class="n">也可以引入一些新的词</span><span class="err">，</span><span class="n">但这个时候</span><span class="err">，</span><span class="n">需要考虑到min_count这一过滤条件</span><span class="err">。</span>

<span class="k">def</span> <span class="nf">update_train_embedding</span><span class="p">():</span>
    <span class="c1"># 获取日志信息</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># 加载新的训练数据</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">LineSentence</span><span class="p">(</span><span class="n">update_filepath</span><span class="p">)</span>
    <span class="c1"># 加载旧模型</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1"># 更新词汇表</span>
    <span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># 训练数据</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">epochs</span><span class="p">)</span>  <span class="c1"># epoch=iter语料库的迭代次数；（默认为5）  total_examples:句子数。</span>
    <span class="c1"># 保存模型，是分成两个来训练</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_update_path</span><span class="p">)</span>
    <span class="c1"># 保存词向量文件</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_update_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div><br> 
<h4 id="5词向量结果测试">5、词向量结果测试</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">test_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s2">&#34;wordvec.model.bin&#34;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">while</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">wd</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&#34;enter an word:&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">wd</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">words</span>
</code></pre></div><p>通过运行，我们可以得到如下查询结果：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">enter an word:开心
[(&#39;高兴&#39;, 0.7237069606781006), (&#39;有缘&#39;, 0.7097823619842529), (&#39;开了花&#39;, 0.7021969556808472), (&#39;玩得&#39;, 0.6799882650375366), (&#39;快乐&#39;, 0.6698621511459351), (&#39;不亦乐乎&#39;, 0.668710470199585), (&#39;鉴宝&#39;, 0.6672042012214661), (&#39;越聊&#39;, 0.6671714782714844), (&#39;爱玩&#39;, 0.6659203767776489), (&#39;着迷&#39;, 0.6657696962356567)]
enter an word:混蛋
[(&#39;享福&#39;, 0.9413065910339355), (&#39;没良心&#39;, 0.9331107139587402), (&#39;怪不得&#39;, 0.9317291975021362), (&#39;养不活&#39;, 0.9283043742179871), (&#39;好惨&#39;, 0.9255991578102112), (&#39;看笑话&#39;, 0.9251411557197571), (&#39;逗我&#39;, 0.9232471585273743), (&#39;命苦&#39;, 0.9226915836334229), (&#39;别怪&#39;, 0.921725332736969), (&#39;我养&#39;, 0.9205465316772461)]
enter an word:巴嘎
KeyError: &#34;word &#39;巴嘎&#39; not in vocabulary&#34;
</code></pre></div><p>从上面我们可以看到，wordvec中对于词表外的词是无法查询的，为了缓解这一问题，可以通过训练时候的min_count参数调至1，以覆盖更多的词语，另一种则是进行增量训练。</p>
<br>
<h3 id="52-fasttext向量训练">5.2 fasttext向量训练</h3>
<p>与wordvec类似，fasttext也才用了类似的训练方法。</p>
<h4 id="1全量数据训练">1、全量数据训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">full_train_embedding</span><span class="p">():</span>
    <span class="n">feature_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">window_size</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">min_count</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">workers</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">corpus_file</span> <span class="o">=</span> <span class="n">datapath</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_count</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">workers</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="n">corpus_file</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
        <span class="n">corpus_file</span><span class="o">=</span><span class="n">corpus_file</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
        <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">total_words</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_total_words</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1">#保存词向量文件,保存的模型仅包含词-向量信息</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><br>
<h4 id="2增量数据训练">2、增量数据训练</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">update_train_embedding</span><span class="p">():</span>
    <span class="c1"># 获取日志信息</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1">:</span><span class="si">%(levelname)s</span><span class="s1">:</span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="c1"># 加载新的训练数据</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">LineSentence</span><span class="p">(</span><span class="n">update_filepath</span><span class="p">)</span>
    <span class="c1"># 加载旧模型</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="c1"># 更新词汇表</span>
    <span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># 训练数据</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">epochs</span><span class="p">)</span>  <span class="c1"># epoch=iter语料库的迭代次数；（默认为5）  total_examples:句子数。</span>
    <span class="c1"># 保存模型，是分成两个来训练</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_update_path</span><span class="p">)</span>
    <span class="c1"># 保存词向量文件</span>
    <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">model_update_vec_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>
</code></pre></div><br>
<h4 id="3词向量结果测试">3、词向量结果测试</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">test_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="k">while</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">wd</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&#34;enter an word:&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">wd</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">return</span>
</code></pre></div><p>通过执行，我们会得到以下查询结果：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">enter an word:开心
[(&#39;开心果&#39;, 0.7953568696975708), (&#39;高兴&#39;, 0.7377268671989441), (&#39;郡县&#39;, 0.6981974244117737), (&#39;有缘&#39;, 0.6916821002960205), (&#39;折勾以&#39;, 0.687650203704834), (&#39;爱&#39;, 0.684776782989502), (&#39;愉快&#39;, 0.6840348243713379), (&#39;快乐&#39;, 0.676334023475647), (&#39;太高兴&#39;, 0.6728817224502563), (&#39;放心&#39;, 0.6692144274711609)]
enter an word:混蛋
[(&#39;侯希辰&#39;, 0.7582178115844727), (&#39;舐&#39;, 0.7578023672103882), (&#39;走眼&#39;, 0.7541716694831848), (&#39;有眼&#39;, 0.7511969804763794), (&#39;贺应勤&#39;, 0.7478049397468567), (&#39;罗敏&#39;, 0.747008204460144), (&#39;郭守桥&#39;, 0.7450246810913086), (&#39;熊芳琴&#39;, 0.7417726516723633), (&#39;找死&#39;, 0.741632342338562), (&#39;许身&#39;, 0.7414941787719727)]
enter an word:巴嘎
[(&#39;陈晓大爆&#39;, 0.3896751403808594), (&#39;董王勇&#39;, 0.36747634410858154), (&#39;李刚&#39;, 0.34988462924957275), (&#39;曾杰&#39;, 0.34452974796295166), (&#39;张文宾&#39;, 0.3370075821876526), (&#39;成浩&#39;, 0.3369928300380707), (&#39;刘晓静&#39;, 0.3348349630832672), (&#39;刘晓丹&#39;, 0.3348219394683838), (&#39;刘骏&#39;, 0.32817351818084717), (&#39;吴建明&#39;, 0.32765522599220276)]
</code></pre></div><p>与上面的wordvec无法处理OOV问题不同，对于八嘎这一词，fasttext依旧可以推断出来，关于这个中间步骤，我们可以作为单独一个问题来说明。</p>
<br>
<h4 id="4fasttext是如何解决oov问题的">4、fasttext是如何解决oov问题的</h4>
<p>通过对其源码进行阅读，可以发现fasttext针对OOV词的原始计算方式包括三个步骤，</p>
<ul>
<li>1）抽取出每个词的N-grams;</li>
<li>2）与预先存好的n-grams词库进行匹配;</li>
<li>3）将匹配到的n-gram向量进行平均，实现如下：</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.utils_any2vec</span> <span class="kn">import</span> <span class="n">_save_word2vec_format</span><span class="p">,</span> <span class="n">_load_word2vec_format</span><span class="p">,</span> <span class="n">_compute_ngrams</span><span class="p">,</span> <span class="n">_ft_hash</span>

<span class="k">def</span> <span class="nf">compute_ngrams</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">min_n</span><span class="p">,</span> <span class="n">max_n</span><span class="p">):</span>
    <span class="n">BOW</span><span class="p">,</span> <span class="n">EOW</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;&lt;&#39;</span><span class="p">,</span> <span class="s1">&#39;&gt;&#39;</span><span class="p">)</span>  <span class="c1"># Used by FastText to attach to all words as prefix and suffix</span>
    <span class="n">extended_word</span> <span class="o">=</span> <span class="n">BOW</span> <span class="o">+</span> <span class="n">word</span> <span class="o">+</span> <span class="n">EOW</span>
    <span class="n">ngrams</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">ngram_length</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">min_n</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">extended_word</span><span class="p">),</span> <span class="n">max_n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">extended_word</span><span class="p">)</span> <span class="o">-</span> <span class="n">ngram_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">ngrams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">extended_word</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">ngram_length</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">ngrams</span>

    <span class="k">def</span> <span class="nf">word_vec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">use_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">FastTextKeyedVectors</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">word_vec</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">use_norm</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># from gensim.models.fasttext import compute_ngrams</span>
            <span class="n">word_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vectors_ngrams</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">ngrams</span> <span class="o">=</span> <span class="n">_compute_ngrams</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_n</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">use_norm</span><span class="p">:</span>
                <span class="n">ngram_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors_ngrams_norm</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ngram_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors_ngrams</span>
            <span class="n">ngrams_found</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">ngrams</span><span class="p">:</span>
                <span class="n">ngram_hash</span> <span class="o">=</span> <span class="n">_ft_hash</span><span class="p">(</span><span class="n">ngram</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket</span>
                <span class="k">if</span> <span class="n">ngram_hash</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">hash2index</span><span class="p">:</span>
                    <span class="n">word_vec</span> <span class="o">+=</span> <span class="n">ngram_weights</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hash2index</span><span class="p">[</span><span class="n">ngram_hash</span><span class="p">]]</span>
                    <span class="n">ngrams_found</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">word_vec</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">word_vec</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ngrams_found</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># No ngrams of the word are present in self.ngrams</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s1">&#39;all ngrams for word </span><span class="si">%s</span><span class="s1"> absent from model&#39;</span> <span class="o">%</span> <span class="n">word</span><span class="p">)</span>
</code></pre></div><p>例如，通过滑动窗口的方式，设定最短ngram和最长ngram，可以得到ngram集合。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&gt;&gt;&gt; from gensim.models.utils_any2vec import *
&gt;&gt;&gt; ngrams = compute_ngrams(&#39;好嗨哦&#39;,min_n = 1,max_n =3)
&gt;&gt;&gt; ngrams
[&#39;&lt;&#39;, &#39;好&#39;, &#39;嗨&#39;, &#39;哦&#39;, &#39;&gt;&#39;, &#39;&lt;好&#39;, &#39;好嗨&#39;, &#39;嗨哦&#39;, &#39;哦&gt;&#39;, &#39;&lt;好嗨&#39;, &#39;好嗨哦&#39;, &#39;嗨哦&gt;&#39;]
</code></pre></div><p>不过，可以看到的是，ngram中引入了“&lt;”和“&gt;”用于标记头和尾，这对于语言模型来说十分生动。</p>
<p><br><br></p>
<h2 id="六开源词向量训练工具与预训文件">六、开源词向量训练工具与预训文件</h2>
<p>不必重复造轮子，当前已经陆续出现了一些代表性的预训练词向量工具和词向量资源，我们可以充分利用好。</p>
<h3 id="61-开源词向量训练工具">6.1 开源词向量训练工具</h3>
<ul>
<li>ngram2vec： <a href="https://github.com/zhezhaoa/ngram2vec/">https://github.com/zhezhaoa/ngram2vec/</a></li>
<li>word2vec： <a href="https://github.com/svn2github/word2vec">https://github.com/svn2github/word2vec</a></li>
<li>fasttext： <a href="https://github.com/facebookresearch/fastText">https://github.com/facebookresearch/fastText</a></li>
<li>glove：https://github.com/stanfordnlp/GloVe</li>
</ul>
<br>
<h3 id="62-开源预训练词向量文件">6.2 开源预训练词向量文件</h3>
<ul>
<li><a href="https://github.com/Embedding/Chinese-Word-Vectors">https://github.com/Embedding/Chinese-Word-Vectors</a></li>
<li><a href="https://github.com/liuhuanyong/Word2Vector">https://github.com/liuhuanyong/Word2Vector</a></li>
<li><a href="https://github.com/liuhuanyong/ChineseEmbedding">https://github.com/liuhuanyong/ChineseEmbedding</a></li>
</ul>
<p><br><br></p>
<h2 id="七本文总结">七、本文总结</h2>
<p>本文，主要围绕预训练词向量模型这一主题，对当前预训练词向量模型的常用方法、评估与应用方式、领域的迁移变体、当前开放的训练工具和词向量文件进行介绍，并以word2vec和fasttext为例，展示一个词向量训练的案例，做到理论与实践相结合。</p>
<p>关于预训练词向量相关的文章目前已经有很多，关于更为细致的解读，可以参考其他材料。预训练词向量是bert出现之前，NLP处理业务问题的标配，绝对称得上是一个里程碑的事件，并且开创了“万物皆可embdding”的时代。</p>
<p>实际上，词向量的发展也在一定程度上验证了当前nlp的进步。</p>
<p>由最开始的基于one-hot、tf-idf、textrank等的bag-of-words，到LSA（SVD）、pLSA、LDA的主题模型词向量，再到word2vec、fastText、glove为代表的固定表征，最后到当前elmo、GPT、bert为代表的基于词向量的动态表征，都说明了语义建模中的动态属性和文本语境的多样性。</p>
<p>不过，我们需要认识的是，在此类词向量中，虽然其本质仍然是语言模型，但是它的目标不是语言模型本身，而是词向量，其所作的一系列优化，其专注于词向量本身，因此做了许多优化来提高计算效率。</p>
<p>例如，与NNLM相比，word2vec将词向量直接sum，不再拼接，并舍弃隐层；考虑到sofmax归一化需要遍历整个词汇表，采用hierarchical softmax 和negative sampling进行优化，前者生成一颗带权路径最小的哈夫曼树，让高频词搜索路劲变小；后者对每一个样本中每一个词都进行负例采样。</p>
<p>最后，以当前一个新的观点来结尾：</p>
<p>现在的预训练语言模型是下一代知识图谱，那么预训练词向量是什么？垫底型相关词库？大家可以想想。</p>
<p><br><br></p>
<h2 id="参考文献">参考文献</h2>
<ol>
<li><a href="https://baijiahao.baidu.com/sid=1600509930259553151">https://baijiahao.baidu.com/sid=1600509930259553151</a></li>
<li><a href="https://mp.weixin.qq.com/s/u8WZqlsIcGCU5BqPH23S4w">https://mp.weixin.qq.com/s/u8WZqlsIcGCU5BqPH23S4w</a></li>
<li><a href="https://www.jianshu.com/p/546d12898378/">https://www.jianshu.com/p/546d12898378/</a></li>
<li><a href="https://www.jianshu.com/p/471d9bfbd72f">https://www.jianshu.com/p/471d9bfbd72f</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32965521">https://zhuanlan.zhihu.com/p/32965521</a></li>
<li><a href="https://mp.weixin.qq.com/s/DvbvYppeuMaPn84SRAINcQ">https://mp.weixin.qq.com/s/DvbvYppeuMaPn84SRAINcQ</a></li>
</ol>
<br> 
<br> 
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Python | 词移距离(Word Mover&#39;s Distance)</title>
      <link>https://textdata.cn/blog/2022-10-16-python-word-mover-s-distance/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-16-python-word-mover-s-distance/</guid>
      <description>词移距离可以为我们提供短文相似度计算，距离越小，两文档相似度越高。</description>
      <content:encoded><![CDATA[


<p>词嵌入方法（word2vec、glove等）可以将每个词的语义映射到n维空间，在n维空间中，词语间距离远近可以表征语义的远近。Kusner等人(2015)提出<strong>词移距离</strong>（word mover’s distance， 后文用WMD缩写代替）借助词语向量语义距离，实现两文档间的相似度计算，距离越小，相似度越高。在会计领域中的应用可以用来度量问答场景的答非所问的程度。</p>
<p><br></p>
<div id="wmd基础" class="section level2">
<h2>WMD基础</h2>
<p>有两个文档</p>
<pre><code>doc1 = &quot;Obama speaks to the media in Illinois&quot;
doc2 = &quot;The President greets the press in Chicago.&quot;</code></pre>
<p>词向量一般是高(n)维空间，这里把n压缩到2维空间，使用matplotlib绘图。两个文档中重要的词语彼此之间存在语义相似度，</p>
<pre class="python"><code># Image from https://vene.ro/images/wmd-obama.png
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
img = mpimg.imread(&#39;img/wmd-obama.png&#39;)
imgplot = plt.imshow(img)
plt.axis(&#39;off&#39;)</code></pre>
<pre><code>## (-0.5, 2397.5, 1327.5, -0.5)</code></pre>
<pre class="python"><code>plt.show()</code></pre>
<p><img src="/blog/2022-10-16-python-word-mover-s-distance/index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p><img src="img/wmd-obama.png" /></p>
<p><br></p>
</div>
<div id="计算wmd步骤" class="section level2">
<h2>计算WMD步骤</h2>
<ol style="list-style-type: decimal">
<li>剔除文档中停止词，如the、a等无信息量词</li>
<li>导入预训练好的词嵌入(word2vec)模型(网上资源比较多，如果数据量很大，也可以自己使用gensim训练自己的词向量)</li>
<li>计算WMD</li>
</ol>
<pre class="python"><code>from nltk.corpus import stopwords
from nltk import download
download(&#39;stopwords&#39;)  # Download stopwords list.
stop_words = stopwords.words(&#39;english&#39;)

def preprocess(sentence):
    return [w for w in sentence.lower().split() if w not in stop_words]


doc1 = &quot;Obama speaks to the media in Illinois&quot;
doc2 = &quot;The President greets the press in Chicago.&quot;

doc1 = preprocess(doc1)
doc2 = preprocess(doc2)

print(doc1)
print(doc2)</code></pre>
<p>Run</p>
<pre><code>[&#39;obama&#39;, &#39;speaks&#39;, &#39;media&#39;, &#39;illinois&#39;]
[&#39;president&#39;, &#39;greets&#39;, &#39;press&#39;, &#39;chicago.&#39;]</code></pre>
<p><br></p>
<blockquote>
<p>如果运行代码出现nltk问题，可以观看视频 <a href="https://www.bilibili.com/video/BV14A411i7DB" class="uri">https://www.bilibili.com/video/BV14A411i7DB</a></p>
</blockquote>
<p>下载谷歌新闻预训练模型(word2vec-google-news-300) ,这里可以使用我提供的百度网盘</p>
<blockquote>
<p>链接：<a href="https://pan.baidu.com/s/1yzGLcMsZl3u1zigTHLdc2Q" class="uri">https://pan.baidu.com/s/1yzGLcMsZl3u1zigTHLdc2Q</a>
提取码：l63f</p>
</blockquote>
<p>这里需要</p>
<pre class="python"><code>from gensim.models import KeyedVectors

w2v_model = KeyedVectors.load(&#39;GoogleNews-vectors-negative300.bin.gz&#39;)
wmd = w2v_model.wmdistance(doc1, doc2)
print(&#39;distance :{wmd}&#39;.format(wmd=wmd))</code></pre>
<p>Run</p>
<pre><code>distance :0.8867237050133944</code></pre>
<p><br></p>
</div>
<div id="参考文献" class="section level2">
<h2>参考文献</h2>
<ul>
<li>Kusner, Matt J., Yu Sun, Nicholas I. Kolkin and Kilian Q. Weinberger. “From Word Embeddings To Document Distances.” ICML (2015).</li>
<li><a href="https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html" class="uri">https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html</a></li>
</ul>
<p><br></p>
</div>
<div id="广而告之" class="section level2">
<h2>广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 | 多语言对齐词向量预训练模型</title>
      <link>https://textdata.cn/blog/2022-10-16-aligned-word-vectors/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-16-aligned-word-vectors/</guid>
      <description>借助该预训练模型，应该能做可做跨文化对比分析</description>
      <content:encoded><![CDATA[<h2 id="介绍">介绍</h2>
<p>Facebook研究者使用 fastText 算法，对维基百科(44种语言)语料数据进行了训练，最终生成了 44 种语言的对齐词向量。</p>
<br>
<h2 id="用途">用途</h2>
<p>wiki数据集有个优点，即由于众人分享、翻译，将不同语言的百科词条进行了翻译整理。所以facebook使用wiki训练对齐词向量有助于提升翻译准确性。与此同时，因为翻译者处于不同的语言和文化背景下，词条及词条内容必然蕴含着语言所特有的文化信息线索，有可能有助于我们挖掘跨语言的文化差异。例如中文词条<code>护士</code>和 英文词条<code>nurse</code> ，可以借助对齐词向量，比较护士这个群体在性别、种族等语义上的差异。</p>
<p>之前分享过的内容</p>
<ul>
<li><a href="https://textdata.cn/blog/embeddingsandattitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展&mdash;&mdash;基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
<li><a href="https://textdata.cn/blog/literatureembeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
</ul>
<p>不过fastText算法认为词语有不同的大小划分层次，从大到小分别是词语、词缀、字符等，使用 Joulin 等人 (2018) 中描述的 RCSLS 方法进行比对。</p>
<table>
<thead>
<tr>
<th><strong>Code</strong></th>
<th><strong>en-es</strong></th>
<th><strong>es-en</strong></th>
<th><strong>en-fr</strong></th>
<th><strong>fr-en</strong></th>
<th><strong>en-de</strong></th>
<th><strong>de-en</strong></th>
<th><strong>en-ru</strong></th>
<th><strong>ru-en</strong></th>
<th><strong>en-zh</strong></th>
<th><strong>zh-en</strong></th>
<th><strong>avg</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Joulin et al. [<a href="https://arxiv.org/abs/1804.07745">1</a>]</td>
<td>84.1</td>
<td>86.3</td>
<td>83.3</td>
<td>84.1</td>
<td><strong>79.1</strong></td>
<td>76.3</td>
<td><strong>57.9</strong></td>
<td><strong>67.2</strong></td>
<td>45.9</td>
<td>46.4</td>
<td>71.1</td>
</tr>
<tr>
<td>This implementation (10 epochs)</td>
<td>84.2</td>
<td><strong>86.6</strong></td>
<td><strong>83.9</strong></td>
<td>84.7</td>
<td>78.3</td>
<td>76.6</td>
<td>57.6</td>
<td>66.7</td>
<td><strong>47.6</strong></td>
<td><strong>47.4</strong></td>
<td>71.4</td>
</tr>
<tr>
<td>This implementation (unsup. model selection)</td>
<td><strong>84.3</strong></td>
<td><strong>86.6</strong></td>
<td><strong>83.9</strong></td>
<td><strong>85.0</strong></td>
<td>78.7</td>
<td><strong>76.7</strong></td>
<td>57.6</td>
<td>67.1</td>
<td><strong>47.6</strong></td>
<td><strong>47.4</strong></td>
<td><strong>71.5</strong></td>
</tr>
</tbody>
</table>
<p>算法得出的词向量在西方，尤其是西欧语言之间进行语义对齐，效果可能更好。而中文、日语等汉字语言，是由偏旁部首组成，与西方字母语言还是存在一定差异。上表也可以看出中英语义对齐准确率47%， 而其他语言之间对齐准确率平均为71%。</p>
<br>
<h2 id="模型资源">模型资源</h2>
<p><a href="https://fasttext.cc/docs/en/aligned-vectors.html">https://fasttext.cc/docs/en/aligned-vectors.html</a></p>
<p>对齐预训练向量模型下载链接</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Afrikaans: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.af.align.vec"><em>text</em></a></td>
<td>Arabic: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ar.align.vec"><em>text</em></a></td>
<td>Bulgarian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bg.align.vec"><em>text</em></a></td>
<td>Bengali: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bn.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Bosnian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.bs.align.vec"><em>text</em></a></td>
<td>Catalan: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ca.align.vec"><em>text</em></a></td>
<td>Czech: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.cs.align.vec"><em>text</em></a></td>
<td>Danish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.da.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>German: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec"><em>text</em></a></td>
<td>Greek: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.el.align.vec"><em>text</em></a></td>
<td>English: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.en.align.vec"><em>text</em></a></td>
<td>Spanish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.es.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Estonian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.et.align.vec"><em>text</em></a></td>
<td>Persian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fa.align.vec"><em>text</em></a></td>
<td>Finnish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fi.align.vec"><em>text</em></a></td>
<td>French: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.fr.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Hebrew: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.he.align.vec"><em>text</em></a></td>
<td>Hindi: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hi.align.vec"><em>text</em></a></td>
<td>Croatian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hr.align.vec"><em>text</em></a></td>
<td>Hungarian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hu.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Indonesian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.id.align.vec"><em>text</em></a></td>
<td>Italian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.it.align.vec"><em>text</em></a></td>
<td>Korean: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ko.align.vec"><em>text</em></a></td>
<td>Lithuanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.lt.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Latvian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.lv.align.vec"><em>text</em></a></td>
<td>Macedonian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.mk.align.vec"><em>text</em></a></td>
<td>Malay: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ms.align.vec"><em>text</em></a></td>
<td>Dutch: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.nl.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Norwegian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.no.align.vec"><em>text</em></a></td>
<td>Polish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.pl.align.vec"><em>text</em></a></td>
<td>Portuguese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.pt.align.vec"><em>text</em></a></td>
<td>Romanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ro.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Russian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ru.align.vec"><em>text</em></a></td>
<td>Slovak: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sk.align.vec"><em>text</em></a></td>
<td>Slovenian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sl.align.vec"><em>text</em></a></td>
<td>Albanian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sq.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Swedish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.sv.align.vec"><em>text</em></a></td>
<td>Tamil: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.ta.align.vec"><em>text</em></a></td>
<td>Thai: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.th.align.vec"><em>text</em></a></td>
<td>Tagalog: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.tl.align.vec"><em>text</em></a></td>
</tr>
<tr>
<td>Turkish: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.tr.align.vec"><em>text</em></a></td>
<td>Ukrainian: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.uk.align.vec"><em>text</em></a></td>
<td>Vietnamese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.vi.align.vec"><em>text</em></a></td>
<td>Chinese: <a href="https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.zh.align.vec"><em>text</em></a></td>
</tr>
</tbody>
</table>
<br>
<h2 id="格式">格式</h2>
<p>词向量默认使用的fastText格式</p>
<ul>
<li>第一行给了词向量的维数</li>
<li>从第二行开始，每一行由词语及对应的词向量组成。</li>
<li>数值之间使用空格间隔</li>
</ul>
<br>
<h2 id="代码">代码</h2>
<h3 id="导入模型">导入模型</h3>
<p>使用gensim导入fastText方法训练出的 预训练语言模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1">#导入刚刚下载的预训练模型</span>
<span class="c1">#该词向量模型300维</span>
<span class="n">zh_w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;wiki.zh.align.vec&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1">#英文词向量模型5G，太大了。如果内存小于16G不要使用下面命令</span>
<span class="c1">#en_w2v_model = KeyedVectors.load_word2vec_format(&#39;wiki.en.align.vec&#39;, binary=False)</span>
</code></pre></div><p>一旦导入成功，就可以进行向量计算。这里仅进行简单演示</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取某词的词向量</span>
<span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>array([ 0.0733,  0.0782,  0.0188, -0.0027, -0.0052,...,  0.0586,  0.0166,
       -0.1401, -0.0545, -0.0125,  0.0373, -0.0681,  0.063 ],
      dtype=float32)
</code></pre>
<br>
<p>在中文中， 护士职业的主要从业者为女性，反应在词向量相似度上，如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">,</span> <span class="s1">&#39;女性&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zh_w2v_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;护士&#39;</span><span class="p">,</span> <span class="s1">&#39;男性&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<pre><code>0.4417011
0.378651
</code></pre>
<br>
<p>更多w2v_model用法可参考 <a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="文献">文献</h2>
<p>如果使用了facebook的预训练词向量，请引用以下两篇文献。</p>
<ul>
<li>Joulin, Armand, Piotr Bojanowski, Tomas Mikolov, Hervé Jégou, and Edouard Grave. &ldquo;Loss in translation: Learning bilingual word mapping with a retrieval criterion.&rdquo; arXiv preprint arXiv:1804.07745 (2018).</li>
<li>Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. &ldquo;Enriching word vectors with subword information.&rdquo; Transactions of the association for computational linguistics 5 (2017): 135-146.</li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>R语言 | 使用word2vec词向量模型</title>
      <link>https://textdata.cn/blog/2022-10-12-r-word2vec/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-12-r-word2vec/</guid>
      <description>R语言训练和使用词向量word2vec模型</description>
      <content:encoded><![CDATA[


<p>Python的gensim库可以训练和使用word2vec模型，R语言中也有与之对应的<code>word2vec包</code>。word2vec是词嵌入技术中最常用的一种技术，如果对词嵌入不太了解，可以阅读前文</p>
<ul>
<li><a href="https://textdata.cn/blog/wordembeddingsinsocialscience/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/">转载 | 从符号到嵌入：计算社会科学的两种文本表示</a></li>
</ul>
<p>本文需要的R包</p>
<pre><code>install.packages(c(&quot;word2vec&quot;, &quot;jiebaR&quot;, &quot;tidyverse&quot;, &quot;readtext&quot;))</code></pre>
<p><br></p>
<div id="word2vec包常用函数" class="section level2">
<h2>word2vec包常用函数</h2>
<ul>
<li>word2vec 使用文本数据训练word2vec模型</li>
<li>as.matrix 获取词向量</li>
<li>doc2vec 获取文档向量</li>
<li>predict 获取</li>
<li>write.word2vec 保存word2vec模型至文件</li>
<li>read.word2vec 读取word2vec模型文件</li>
</ul>
<p><br></p>
</div>
<div id="准备数据" class="section level2">
<h2>准备数据</h2>
<p>原始数据是从网站下载的 <code>三体.txt</code>, 未分词处理，现在需要</p>
<ol style="list-style-type: decimal">
<li>读中文取txt数据</li>
<li>保留标点符号，进行分词处理</li>
<li>分词结果重新整理为类似英文(空格间隔词语的形式)字符串</li>
<li>结果存入新的txt</li>
</ol>
<pre class="r"><code>library(jiebaR)
library(tidyverse)
library(word2vec)


#导入数据
tri_body &lt;- readtext::readtext(&#39;data/三体.txt&#39;)$text 

#分词（保留标点符号）
tokenizer &lt;- worker(symbol=T)
tri_words &lt;- segment(tri_body, tokenizer)

# 整理为英文格式（词语之间加空格）
segmented_text &lt;- stringr::str_c(tri_words, collapse = &quot; &quot;) %&gt;% c()

#写入txt
readr::write_file(segmented_text, file=&#39;data/santi.txt&#39;)</code></pre>
<p><br></p>
</div>
<div id="训练word2vec模型" class="section level2">
<h2>训练word2vec模型</h2>
<pre><code>word2vec(
  x,
  type = c(&quot;cbow&quot;, &quot;skip-gram&quot;),
  dim = 50,
  window = ifelse(type == &quot;cbow&quot;, 5L, 10L),
  iter = 5L,
  lr = 0.05,
  min_count = 5L,
  split = c(&quot; \n,.-!?:;/\&quot;#$%&amp;&#39;()*+&lt;=&gt;@[]\\^_`{|}~\t\v\f\r&quot;, &quot;.\n?!&quot;),
  stopwords = character(),
  threads = 1L,
  ...
)</code></pre>
<ul>
<li>x 英文文本数据txt文件(中文数据txt文件是分词后的txt文件，空格间隔词语)</li>
<li>type 训练方式，默认CBOW</li>
<li>dim 词向量维度，默认50维</li>
<li>window 词向量窗口，默认5</li>
<li>iter 训练迭代次数，默认5</li>
<li>split 分词、分句对应的分隔符。</li>
<li>lr 学习率，默认0.05</li>
<li>min_count 词语在语料中至少要出现5次(低于5次的词语，训练好的结果中没有该词语）</li>
<li>stopwords 停用词表，默认空字符集</li>
<li>threads 并行加速，cpu核数，默认1。为了加速训练过程，可以使用 <code>parallel::detectCores()</code> 获得本电脑的核数</li>
</ul>
<pre class="r"><code>#训练10维的词向量模型
model &lt;- word2vec(x = &#39;data/santi.txt&#39;, 
                  dim = 10,  
                  iter = 20, 
                  split = c(&quot; &quot;,  &quot;。？！；&quot;),
                  threads = parallel::detectCores()) #并行，使用cpu多核加速

emb &lt;- as.matrix(model)

#显示6个词
head(emb)</code></pre>
<pre><code>##             [,1]       [,2]        [,3]        [,4]      [,5]        [,6]
## 煮   -1.02566934 -0.9271542 -0.42417252 -0.54280633 1.8847700  0.41640753
## 报   -0.83992052  1.9440031  0.09093992  0.83522910 1.7909089  0.72149992
## 悬空 -0.06369513 -1.3519955 -2.13137460 -0.06198586 0.6096401  1.32933748
## 略    1.74687469 -0.4278547 -0.33822438  1.08505321 2.0168977 -0.07693915
## 伏   -0.68947995 -1.4147453 -1.95522511 -0.39963767 0.5269030  0.30352208
## 石柱 -0.40561640 -1.3643234  0.30329546 -0.94012892 2.1579018  0.79654717
##            [,7]       [,8]       [,9]      [,10]
## 煮   -1.1708908 -0.7624418 -0.6275516  1.2417521
## 报    0.5235919  0.8448864 -0.2960095 -0.0773837
## 悬空  0.1527163 -0.1337370 -0.1646384  1.1892601
## 略   -0.3246748 -0.9813624  0.5045205  0.2771466
## 伏    0.3166684 -1.4238008 -1.0167172 -0.0976937
## 石柱  0.2237919  0.6933151  0.7412233 -0.7918702</code></pre>
<p><br></p>
</div>
<div id="查看某词的vector" class="section level2">
<h2>查看某词的vector</h2>
<p>查看词语 <code>汪淼</code> 的vector</p>
<pre class="r"><code>emb[&quot;汪淼&quot;,]</code></pre>
<pre><code>##  [1] -0.77559733 -0.90021265  0.66555792 -0.10277803  1.89924443 -0.88817298
##  [7] -1.32665634 -0.75938725 -0.09628224  1.18008399</code></pre>
<p>查看词语 <code>地球</code> 的vector</p>
<pre class="r"><code>emb[&quot;地球&quot;,]</code></pre>
<pre><code>##  [1]  0.29645494 -0.61688840  0.91209215 -0.64530188  0.62816381 -0.72807491
##  [7]  0.50655973  2.38137436  1.19238114 -0.09610342</code></pre>
<p><br></p>
</div>
<div id="predict" class="section level2">
<h2>predict()</h2>
<p>找到语料中，词语 <code>罗辑</code> 最相似的 20个词</p>
<pre class="r"><code>predict(model, &#39;罗辑&#39;, type=&#39;nearest&#39;, top_n = 20)</code></pre>
<pre><code>## $罗辑
##    term1    term2 similarity rank
## 1   罗辑     胡文  0.9744400    1
## 2   罗辑   申玉菲  0.9678891    2
## 3   罗辑   瓦季姆  0.9550550    3
## 4   罗辑 狄奥伦娜  0.9518393    4
## 5   罗辑     蓝西  0.9472395    5
## 6   罗辑     护士  0.9471439    6
## 7   罗辑   法扎兰  0.9458703    7
## 8   罗辑   白艾思  0.9451101    8
## 9   罗辑     坎特  0.9396626    9
## 10  罗辑     白蓉  0.9387447   10
## 11  罗辑   参谋长  0.9377206   11
## 12  罗辑   弗雷斯  0.9369408   12
## 13  罗辑   第一眼  0.9357565   13
## 14  罗辑     父亲  0.9350463   14
## 15  罗辑   多少次  0.9314436   15
## 16  罗辑     门去  0.9291503   16
## 17  罗辑     维德  0.9267251   17
## 18  罗辑     褐蚁  0.9203902   18
## 19  罗辑       刚  0.9200501   19
## 20  罗辑     吴岳  0.9191605   20</code></pre>
<p>查看均值向量（多个词向量中心的）的10个近义词</p>
<pre class="r"><code>vectors &lt;- emb[c(&quot;汪淼&quot;, &quot;罗辑&quot;, &quot;叶文洁&quot;), ]
centroid_vector &lt;- colMeans(vectors)

predict(model, centroid_vector, type = &quot;nearest&quot;, top_n = 10)</code></pre>
<pre><code>##        term similarity rank
## 1      罗辑  0.9185568    1
## 2  狄奥伦娜  0.9104245    2
## 3      文洁  0.9088279    3
## 4      汪淼  0.9054156    4
## 5    白艾思  0.9046930    5
## 6      张翔  0.9026827    6
## 7      尴尬  0.8952187    7
## 8      庄颜  0.8952166    8
## 9      皇帝  0.8949283    9
## 10     父亲  0.8915347   10</code></pre>
<p><br></p>
</div>
<div id="doc2vec" class="section level2">
<h2>doc2vec()</h2>
<ul>
<li>doc2vec(object, newdata, split = ” “)
<ul>
<li>object word2vec模型对象</li>
<li>newdata 文档列表(用空格间隔的字符串列表)</li>
<li>split 默认分隔符是空格</li>
</ul></li>
</ul>
<p>将文档转为向量</p>
<pre class="r"><code>docs &lt;- c(&quot;哦 ， 对不起 ， 汪 教授 。 这是 我们 史强 队长 。&quot;, 
          &quot; 丁仪 博士 ， 您 能否 把 杨冬 的 遗书 给 汪 教授 看 一下 ？ &quot;)

doc2vec(object=model, newdata = docs, split=&#39; &#39;)</code></pre>
<pre><code>##            [,1]       [,2]       [,3]     [,4]      [,5]       [,6]       [,7]
## [1,] -1.1769752 -0.1065619  0.1983950 1.734068 0.5478012 -0.8320528 -0.2387014
## [2,] -0.4827189  0.0664595 -0.2119484 1.895074 0.6729840 -0.3008853 -0.6857539
##            [,8]      [,9]      [,10]
## [1,] -0.5519856 -2.007002  0.4182127
## [2,] -0.5976922 -2.130454 -0.4653725</code></pre>
<p><br></p>
</div>
<div id="保存word2vec模型" class="section level2">
<h2>保存word2vec模型</h2>
<p>保存模型，一般有两个目的</p>
<ul>
<li>为了分享word2vec模型</li>
<li>避免反复训练模型，节约数据分析时间</li>
</ul>
<pre class="r"><code>word2vec::write.word2vec(x = model, 
                         #新建output文件夹，将模型存入output文件夹内
                         file = &quot;output/santi_word2vec.bin&quot;)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p><br></p>
</div>
<div id="导入预训练模型" class="section level2">
<h2>导入预训练模型</h2>
<p>导入 <code>output/santi_word2vec.bin</code> 的预训练word2vec模型</p>
<pre class="r"><code>pre_trained_model &lt;- word2vec::read.word2vec(file = &quot;output/santi_word2vec.bin&quot;)
pre_trained_emb &lt;- as.matrix(pre_trained_model)
head(pre_trained_emb)</code></pre>
<pre><code>##              [,1]       [,2]       [,3]       [,4]       [,5]        [,6]
## 回荡   -1.9563367 -0.3099073 -1.2969902 -0.5719763  1.1507142 -0.05515177
## 听证会  0.2756990  1.3702289 -1.3303705 -0.1827691  0.6622804 -1.92008448
## 纲领    0.4495552  1.9311246 -0.5812275 -0.1470096 -0.2678985 -0.01694358
## 很亮    0.3621844 -1.0048453  0.7036168 -2.0917876  0.6459805  1.18436253
## 秒      1.9033701  1.6510324 -0.2616904  0.3671210  1.0618066  0.06588747
## 杰森   -1.2904713 -1.2501229  0.3380587  0.8590797  1.6798494 -0.58775252
##              [,7]       [,8]       [,9]      [,10]
## 回荡    1.1082711 -0.2064489 -0.9264346 -0.7816723
## 听证会 -1.0952694  0.6120903 -0.1326561  0.7252344
## 纲领   -0.6097277  2.1051276 -0.2405726 -0.8808851
## 很亮    0.1964065 -1.3926132 -0.4042619 -0.1645472
## 秒     -0.8347995  0.2591044  0.3594093  1.1929117
## 杰森    0.4941484 -1.1393189 -0.4687541  0.9951217</code></pre>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>NLP资源 | 汽车、金融等9大领域预训练词向量模型下载资源</title>
      <link>https://textdata.cn/blog/pretained_nlp_models/</link>
      <pubDate>Wed, 25 May 2022 10:43:10 +0600</pubDate>
      
      <guid>/blog/pretained_nlp_models/</guid>
      <description>本文主要开放汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等9大领域预训练词向量，以及字符、依存、拼音与词性4类预训练向量地址，供大家一起使用</description>
      <content:encoded><![CDATA[<p>在前面的文章中，我们介绍了关于词向量的一些基础理论和训练方法，<strong>本文主要开放汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等9大领域预训练词向量，以及字符、依存、拼音与词性4类预训练向量地址，供大家一起使用</strong>。</p>
<h2 id="一汽车房产等9大领域预训练词向量">一、汽车、房产等9大领域预训练词向量</h2>
<p>通过收集多文本分类语料库，对汽车、房产、教育、社会、娱乐、体育、金融、科技、游戏等多个领域文本进行词向量训练，得到了如下预训练词向量的结果：</p>
<table>
<thead>
<tr>
<th>领域类型</th>
<th>模型类型</th>
<th>关键词集合</th>
<th>词的规模</th>
</tr>
</thead>
<tbody>
<tr>
<td>汽车</td>
<td>word_vector_auto.model.bin</td>
<td>117,510</td>
<td>200</td>
</tr>
<tr>
<td>房产</td>
<td>word_vector_house.model.bin</td>
<td>145,287</td>
<td>200</td>
</tr>
<tr>
<td>教育</td>
<td>word_vector_edu.model.bin</td>
<td>242,874</td>
<td>200</td>
</tr>
<tr>
<td>社会</td>
<td>word_vector_society.model.bin</td>
<td>221,395</td>
<td>200</td>
</tr>
<tr>
<td>娱乐</td>
<td>word_vector_ent.model.bin</td>
<td>230,665</td>
<td>200</td>
</tr>
<tr>
<td>体育</td>
<td>word_vector_sports.model.bin</td>
<td>95724</td>
<td>200</td>
</tr>
<tr>
<td>金融</td>
<td>word_vector_finance.model.bin</td>
<td>284035</td>
<td>200</td>
</tr>
<tr>
<td>科技</td>
<td>word_vector_tech.model.bin</td>
<td>108188</td>
<td>200</td>
</tr>
<tr>
<td>游戏</td>
<td>word_vector_games.model.bin</td>
<td>100821</td>
<td>200</td>
</tr>
</tbody>
</table>
<p><strong>开放地址：</strong></p>
<p><a href="https://pan.baidu.com/s/1jEHFoAmVXlB67Q28-CeTvw">https://pan.baidu.com/s/1jEHFoAmVXlB67Q28-CeTvw</a> 密码: 1pa6</p>
<h2 id="二预训练字符依存拼音与词性向量">二、预训练字符、依存、拼音与词性向量</h2>
<p>通过对字符、依存、拼音与词性进行切分，使用同样的方式，可以得到相应的预训练词向量。</p>
<table>
<thead>
<tr>
<th>向量名称</th>
<th style="text-align:center">向量含义</th>
<th style="text-align:center">词数</th>
<th style="text-align:center">维度</th>
<th style="text-align:center">例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>de_vec_10</td>
<td style="text-align:center">依存关系向量</td>
<td style="text-align:center">13</td>
<td style="text-align:center">10</td>
<td style="text-align:center">SBV, ATT</td>
</tr>
<tr>
<td>pinyin_vec_300</td>
<td style="text-align:center">汉语拼音向量</td>
<td style="text-align:center">146242</td>
<td style="text-align:center">300</td>
<td style="text-align:center">ni, hao</td>
</tr>
<tr>
<td>postag_vec_30</td>
<td style="text-align:center">汉语词性向量</td>
<td style="text-align:center">59</td>
<td style="text-align:center">300</td>
<td style="text-align:center">n,v,a,d</td>
</tr>
<tr>
<td>token_vec_300</td>
<td style="text-align:center">汉语字向量</td>
<td style="text-align:center">20029</td>
<td style="text-align:center">300</td>
<td style="text-align:center">刘,焕,勇</td>
</tr>
<tr>
<td>word_vec_300</td>
<td style="text-align:center">汉语词向量</td>
<td style="text-align:center">673266</td>
<td style="text-align:center">300</td>
<td style="text-align:center">刘焕勇</td>
</tr>
</tbody>
</table>
<p><strong>开放地址：</strong></p>
<p><a href="https://github.com/liuhuanyong/ChineseEmbedding">https://github.com/liuhuanyong/ChineseEmbedding</a></p>
<p><strong>向量效果：</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> 
 ***********************字符向量************************
    token:刘
    (&#39;李&#39;, 0.7306396961212158),(&#39;陈&#39;, 0.7201231122016907)
    (&#39;赵&#39;, 0.6974461674690247),(&#39;杨&#39;, 0.6972213983535767)
    (&#39;吴&#39;, 0.6851627230644226),(&#39;徐&#39;, 0.6516467332839966)
    (&#39;郭&#39;, 0.6499480605125427),(&#39;蔡&#39;, 0.6175302267074585)
    (&#39;郑&#39;, 0.6092196106910706),(&#39;孙&#39;, 0.5950524210929871)
    token:丑
    (&#39;卯&#39;, 0.6074919700622559),(&#39;酉&#39;, 0.5910211801528931)
    (&#39;巳&#39;, 0.5581363439559937),(&#39;戌&#39;, 0.43932047486305237)
    (&#39;戊&#39;, 0.41449615359306335),(&#39;壬&#39;, 0.40456631779670715)
    (&#39;謤&#39;, 0.367109090089798),(&#39;绯&#39;, 0.3643313944339752),
    (&#39;寅&#39;, 0.36351141333580017),(&#39;旽&#39;, 0.3549465537071228)


***********************依存向量************************
    dependency rel:ATT
    (&#39;COO&#39;, 0.14239487051963806),(&#39;ADV&#39;, -0.16987691819667816)
    (&#39;RAD&#39;, -0.2357601821422577),(&#39;HED&#39;, -0.2401314228773117)
    (&#39;SBV&#39;, -0.25625932216644287),(&#39;WP&#39;, -0.27165737748146057)
    (&#39;LAD&#39;, -0.2902592420578003),(&#39;POB&#39;, -0.2990782558917999)
    (&#39;VOB&#39;, -0.37553706765174866),(&#39;IOB&#39;, -0.6669262647628784)
    dependency rel:POB
    (&#39;IOB&#39;, 0.16698899865150452),(&#39;DBL&#39;, 0.16678886115550995)
    (&#39;FOB&#39;, 0.1657436639070511),(&#39;CMP&#39;, 0.14784857630729675)
    (&#39;VOB&#39;, 0.1461176574230194),(&#39;SBV&#39;, 0.08011472970247269)
    (&#39;LAD&#39;, -0.022307466715574265),(&#39;WP&#39;, -0.022942926734685898)
    (&#39;HED&#39;, -0.037264980375766754),(&#39;RAD&#39;, -0.042251598089933395)

  
  ***********************拼音向量************************
    pinyin:wo
    (&#39;shei&#39;, 0.6129732131958008)(&#39;ta&#39;, 0.6081706285476685)
    (&#39;nin&#39;, 0.5819231867790222),(&#39;！&#39;, 0.5435523986816406)
    (&#39;……&#39;, 0.48428624868392944),(&#39;ai&#39;, 0.47832390666007996)
    (&#39;o&#39;, 0.4761071801185608),(&#39;。』&#39;, 0.4598163366317749)
    (&#39;...&#39;, 0.45207729935646057),(&#39;ni&#39;, 0.44975683093070984)
    pinyin:guo
    (&#39;dang&#39;, 0.3908974528312683),(&#39;yuan&#39;, 0.378823846578598)
    (&#39;zu&#39;, 0.35387369990348816),(&#39;hua&#39;, 0.3405681848526001)
    (&#39;zheng&#39;, 0.3355437219142914),(&#39;yi&#39;, 0.3333034813404083)
    (&#39;ren&#39;, 0.3194104731082916),(&#39;jun&#39;, 0.3187354505062103)
    (&#39;hui&#39;, 0.31342023611068726),(&#39;xin&#39;, 0.3096797466278076)

   
   ***********************词性向量************************
    word postag:a
    (&#39;d&#39;, 0.7203904986381531),(&#39;c&#39;, 0.6124969720840454)
    (&#39;v&#39;, 0.4963228106498718),(&#39;an&#39;, 0.4531499147415161)
    (&#39;uz&#39;, 0.4459834396839142),(&#39;ud&#39;, 0.42059916257858276)
    (&#39;r&#39;, 0.4090540111064911),(&#39;uj&#39;, 0.4061364233493805)
    (&#39;i&#39;, 0.38707998394966125),(&#39;l&#39;, 0.3551557660102844)
    word postag:n
    (&#39;b&#39;, 0.7030695676803589),(&#39;vn&#39;, 0.490166038274765)
    (&#39;p&#39;, 0.4858315885066986),(&#39;v&#39;, 0.4499088227748871)
    (&#39;nt&#39;, 0.44155171513557434),(&#39;f&#39;, 0.26609259843826294)
    (&#39;s&#39;, 0.2639649212360382),(&#39;l&#39;, 0.24365971982479095)
    (&#39;ns&#39;, 0.2278469204902649),(&#39;m&#39;, 0.202927365899086)
    
    ***********************词向量************************
    word:爱情
    (&#39;爱恋&#39;, 0.6931096315383911),(&#39;真爱&#39;, 0.6897798776626587)
    (&#39;婚姻&#39;, 0.6540514826774597),(&#39;浪漫爱情&#39;, 0.6535360813140869)
    (&#39;情感&#39;, 0.6501022577285767),(&#39;感情&#39;, 0.6403399705886841)
    (&#39;纯爱&#39;, 0.6394841074943542),(&#39;爱情故事&#39;, 0.6282097101211548)
    (&#39;校园爱情&#39;, 0.6078493595123291),(&#39;情爱&#39;, 0.5976818799972534)
    word:创新
    (&#39;技术创新&#39;, 0.7648976445198059),(&#39;不断创新&#39;, 0.7172579765319824)
    (&#39;创新型&#39;, 0.6573833227157593),(&#39;创新能力&#39;, 0.6533682942390442)
    (&#39;创新性&#39;, 0.6160774827003479),(&#39;革新&#39;, 0.6159394383430481)
    (&#39;人才培养&#39;, 0.6093565821647644),(&#39;开拓创新&#39;, 0.6015594601631165)
    (&#39;探索&#39;, 0.5987343788146973),(&#39;技术革新&#39;, 0.5949685573577881)
</code></pre></div><h2 id="关于作者">关于作者</h2>
<p>老刘，刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。</p>
<p>就职于360人工智能研究院、曾就职于中国科学院软件研究所。</p>
<p><strong>老刘说NLP</strong>，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 |  Python文本分析包更新</title>
      <link>https://textdata.cn/blog/cntext_tutorial/</link>
      <pubDate>Mon, 09 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_tutorial/</guid>
      <description>扩展词典、情感分析、可阅读性，内置9种情感词典，涵盖中英文</description>
      <content:encoded><![CDATA[<p><a href="https://github.com/hidadeng/cntext"><img loading="lazy" src="https://img.shields.io/badge/cntext-%e4%b8%ad%e6%96%87%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%ba%93-orange?style=for-the-badge&amp;logo=appveyor" alt=""  />
</a></p>
<p><a href="version1.2.md">旧版cntext入口</a></p>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> <strong>stats</strong>  文本统计指标
<ul>
<li><input checked="" disabled="" type="checkbox"> 词频统计</li>
<li><input checked="" disabled="" type="checkbox"> 可读性</li>
<li><input checked="" disabled="" type="checkbox"> 内置pkl词典</li>
<li><input checked="" disabled="" type="checkbox"> <strong>情感分析</strong></li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>dictionary</strong> 构建词表(典)
<ul>
<li><input checked="" disabled="" type="checkbox"> Sopmi 互信息扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> W2Vmodels 词向量扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> Glove Glove词嵌入模型</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>similarity</strong>   文本相似度
<ul>
<li><input checked="" disabled="" type="checkbox"> cos相似度</li>
<li><input checked="" disabled="" type="checkbox"> jaccard相似度</li>
<li><input checked="" disabled="" type="checkbox"> 编辑距离相似度</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>mind.py</strong> 计算文本中的认知方向（态度、偏见）</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p><a href="cntext_examples.zip">click to download</a></p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext
</code></pre></div><br>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">mind</span>
    <span class="nx">dictionary</span>
    <span class="nx">similarity</span>
    <span class="nx">stats</span>
</code></pre></div><br>
<h2 id="一stats">一、stats</h2>
<p>目前stats内置的函数有</p>
<ul>
<li><strong>readability</strong>  文本可读性</li>
<li><strong>term_freq</strong> 词频统计函数</li>
<li><strong>dict_pkl_list</strong>  获取cntext内置词典列表(pkl格式)</li>
<li><strong>load_pkl_dict</strong> 导入pkl词典文件</li>
<li><strong>sentiment</strong> 情感分析</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="11--readability">1.1  readability</h3>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p>readability(text, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text: 文本字符串数据</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p><strong>中文可读性</strong> 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>


<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 28.0,
 &#39;readability2&#39;: 0.15789473684210525,
 &#39;readability3&#39;: 14.078947368421053}
</code></pre></div><br>
<p>句子中的符号变更会影响结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.16666666666666666,
 &#39;readability3&#39;: 13.583333333333334}
</code></pre></div><p><br><br></p>
<h3 id="12--term_freq">1.2  term_freq</h3>
<p>词频统计函数，返回Counter类型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="13-dict_pkl_list">1.3 dict_pkl_list</h3>
<p>获取cntext内置词典列表(pkl格式)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取cntext内置词典列表(pkl格式)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;, 
 &#39;concreteness.pkl&#39;]
</code></pre></div><p>词典对应关系, 部分情感词典资料整理自 <a href="https://github.com/quanteda/quanteda.sentiment">quanteda.sentiment</a></p>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>DUTIR.pkl</td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td>HOWNET.pkl</td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>sentiws.pkl</td>
<td>SentimentWortschatz (SentiWS)</td>
<td>英文</td>
<td>正面词、负面词；<br>效价</td>
</tr>
<tr>
<td>ChineseFinancialFormalUnformalSentiment.pkl</td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td>ANEW.pkl</td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>词语效价信息</td>
</tr>
<tr>
<td>LSD2015.pkl</td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>NRC.pkl</td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td>geninqposneg.pkl</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HuLiu.pkl</td>
<td>Hu&amp;Liu (2004)正、负情感词典</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>AFINN.pkl</td>
<td>尼尔森 (2011) 的“新 ANEW”效价词表</td>
<td>英文</td>
<td>情感效价信息valence</td>
</tr>
<tr>
<td>LoughranMcDonald.pkl</td>
<td>会计金融LM词典</td>
<td>英文</td>
<td>金融领域正、负面情感词</td>
</tr>
<tr>
<td>ADV_CONJ.pkl</td>
<td>副词连词</td>
<td>中文</td>
<td></td>
</tr>
<tr>
<td>STOPWORDS.pkl</td>
<td></td>
<td>中、英</td>
<td>停用词</td>
</tr>
<tr>
<td>concreteness.pkl</td>
<td>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</td>
<td>English</td>
<td>word &amp; concreateness score</td>
</tr>
</tbody>
</table>
<h3 id="注意">注意:</h3>
<ul>
<li>
<p>如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</li>
<li>
<p>如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue</p>
</li>
</ul>
<br>
<h3 id="14-load_pkl_dict">1.4 load_pkl_dict</h3>
<p>导入pkl词典文件，返回字典样式数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入pkl词典文件,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, ...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...], 
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;, &#39;刿目怵心&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;, &#39;喜糖&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,]
           }
</code></pre></div><br>
<h3 id="15-sentiment">1.5 sentiment</h3>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)
使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>diction:  情感词字典；</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">],</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p>如果不适用pkl词典，可以自定义自己的词典，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><br>
<h3 id="16-sentiment_by_valence">1.6 sentiment_by_valence</h3>
<p>sentiment函数默认所有情感词权重均为1，只需要统计文本中情感词的个数，即可得到文本情感得分。</p>
<p>sentiment_by_valence(text, diction, lang=&lsquo;english&rsquo;)函数考虑了词语的效价(valence)</p>
<ul>
<li>text 待输入文本</li>
<li>diction 带效价的词典，DataFrame格式。</li>
<li>lang 语言类型&rsquo;chinese' 或 &lsquo;english&rsquo;，默认&rsquo;english'</li>
</ul>
<p>这里我们以文本具体性度量为例， <strong>concreteness.pkl</strong> 整理自 Brysbaert2014的文章。</p>
<blockquote>
<p>Brysbaert, M., Warriner, A. B., &amp; Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46, 904–911</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># load the concreteness.pkl dictionary file</span>
<span class="n">concreteness_df</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;concreteness.pkl&#39;</span><span class="p">)</span>
<span class="n">concreteness_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<table>
<thead>
<tr>
<th style="text-align:right"></th>
<th style="text-align:left">word</th>
<th style="text-align:right">valence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">0</td>
<td style="text-align:left">roadsweeper</td>
<td style="text-align:right">4.85</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td style="text-align:left">traindriver</td>
<td style="text-align:right">4.54</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td style="text-align:left">tush</td>
<td style="text-align:right">4.45</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td style="text-align:left">hairdress</td>
<td style="text-align:right">3.93</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td style="text-align:left">pharmaceutics</td>
<td style="text-align:right">3.77</td>
</tr>
</tbody>
</table>
<br>
<p>先看一条文本的具体性度量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">reply</span> <span class="o">=</span> <span class="s2">&#34;I&#39;ll go look for that&#34;</span>

<span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                              <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                              <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">score</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1.85
</code></pre></div><br>
<p>很多条文本的具体性度量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">employee_replys</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I&#39;ll go look for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that top&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go look for that t-shirt in grey&#34;</span><span class="p">,</span>
                   <span class="s2">&#34;I&#39;ll go search for that t-shirt in grey&#34;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">reply</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">employee_replys</span><span class="p">):</span>
    <span class="n">score</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">reply</span><span class="p">,</span> 
                                  <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> 
                                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
    
    <span class="n">template</span> <span class="o">=</span> <span class="s2">&#34;Concreteness Score: </span><span class="si">{score:.2f}</span><span class="s2"> | Example-</span><span class="si">{idx}</span><span class="s2">: </span><span class="si">{exmaple}</span><span class="s2">&#34;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">template</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="o">=</span><span class="n">score</span><span class="p">,</span> 
                          <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span> 
                          <span class="n">exmaple</span><span class="o">=</span><span class="n">reply</span><span class="p">))</span>
    
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment_by_valence</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">diction</span><span class="o">=</span><span class="n">concreteness_df</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Concreteness Score: 1.55 | Example-0: I&#39;ll go look for that
Concreteness Score: 1.55 | Example-1: I&#39;ll go search for that
Concreteness Score: 1.89 | Example-2: I&#39;ll go search for that top
Concreteness Score: 2.04 | Example-3: I&#39;ll go search for that t-shirt
Concreteness Score: 2.37 | Example-4: I&#39;ll go look for that t-shirt in grey
Concreteness Score: 2.37 | Example-5: I&#39;ll go search for that t-shirt in grey
</code></pre></div><br>
<p><br><br></p>
<h2 id="二dictionary">二、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="21-sopmi">2.1 SoPmi</h3>
<p>SoPmi 共现法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                   <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                   <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                   <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span>
                   <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 44.49 s
</code></pre></div><br>
<h3 id="22-w2vmodels">2.2 W2VModels</h3>
<p>W2VModels 词向量</p>
<p><strong>特别要注意代码需要设定lang语言参数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   174 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s

</code></pre></div><br>
<h3 id="需要注意">需要注意</h3>
<p>训练出的w2v模型可以后续中使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">model路径</span><span class="p">)</span>
<span class="c1">#找出word的词向量</span>
<span class="c1">#w2v_model.get_vector(word)</span>
<span class="c1">#更多w2_model方法查看</span>
<span class="c1">#help(w2_model)</span>
</code></pre></div><p>例如本代码，运行生成的结果路径<code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;technology&#39;, 0.689210832118988),
 (&#39;infrastructure&#39;, 0.669672966003418),
 (&#39;resources&#39;, 0.6695448160171509),
 (&#39;talent&#39;, 0.6627111434936523),
 (&#39;execution&#39;, 0.6549549102783203),
 (&#39;marketing&#39;, 0.6533523797988892),
 (&#39;merchandising&#39;, 0.6504817008972168),
 (&#39;diversification&#39;, 0.6479553580284119),
 (&#39;expertise&#39;, 0.6446896195411682),
 (&#39;digital&#39;, 0.6326863765716553)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.45616838, -0.7799563 ,  0.56367606, -0.8570078 ,  0.600359  ,
       -0.6588043 ,  0.31116748, -0.11956959, -0.47599426,  0.21840936,
       -0.02268819,  0.1832016 ,  0.24452794,  0.01084935, -1.4213187 ,
        0.22840202,  0.46387577,  1.198386  , -0.621511  , -0.51598716,
        0.13352732,  0.04140598, -0.23470387,  0.6402956 ,  0.20394802,
        0.10799981,  0.24908689, -1.0117126 , -2.3168423 , -0.0402851 ,
        1.6886286 ,  0.5357047 ,  0.22932841, -0.6094084 ,  0.4515793 ,
       -0.5900931 ,  1.8684244 , -0.21056202,  0.29313338, -0.221067  ,
       -0.9535679 ,  0.07325   , -0.15823542,  1.1477109 ,  0.6716076 ,
       -1.0096023 ,  0.10605699,  1.4148282 ,  0.24576302,  0.5740349 ,
        0.19984631,  0.53964925,  0.41962907,  0.41497853, -1.0322098 ,
        0.01090925,  0.54345983,  0.806317  ,  0.31737605, -0.7965337 ,
        0.9282971 , -0.8775608 , -0.26852605, -0.06743863,  0.42815775,
       -0.11774074, -0.17956367,  0.88813037, -0.46279573, -1.0841943 ,
       -0.06798118,  0.4493006 ,  0.71962464, -0.02876493,  1.0282255 ,
       -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922,
       -0.16753489,  0.14065655, -1.8609751 ,  0.03587054,  1.2792674 ,
        1.2732009 , -0.74120265, -0.98000383,  0.4521185 , -0.26387128,
        0.37045383,  0.3680011 ,  0.7197629 , -0.3570571 ,  0.8016917 ,
        0.39243212, -0.5027844 , -1.2106236 ,  0.6412354 , -0.878307  ],
      dtype=float32)
</code></pre></div><p><br><br></p>
<h3 id="23-co_occurrence_matrix">2.3 co_occurrence_matrix</h3>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence2.png" alt=""  />
</p>
<p><br><br></p>
<h3 id="24--glove">2.4  Glove</h3>
<p>构建Glove词嵌入模型，使用英文数据<code>data/brown_corpus.txt</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Glove</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">create_vocab</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="s1">&#39;data/brown_corpus.txt&#39;</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">cooccurrence_matrix</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_embeddings</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4: ...Create vocabulary for Glove.
Step 2/4: ...Create cooccurrence matrix.
Step 3/4: ...Train glove embeddings. 
             Note, this part takes a long time to run
Step 3/4: ... Finish! Use 175.98 s
</code></pre></div><p>生成的Glove词嵌入文件位于<code>output/Glove</code> 。</p>
<p><br><br></p>
<h2 id="三similarity">三、similarity</h2>
<p>四种相似度计算函数</p>
<ul>
<li>cosine_sim(text1, text2)  cos余弦相似</li>
<li>jaccard_sim(text1, text2)     jaccard相似</li>
<li>minedit_sim(text1, text2)  最小编辑距离相似度；</li>
<li>simple_sim(text1, text2) 更改变动算法</li>
</ul>
<p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩啊&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.82
0.67
2.00
0.87
</code></pre></div><p><br><br></p>
<h2 id="四text2mind">四、Text2Mind</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<p>- <strong>多个对象在某概念的远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
<p>- 多个对象在某<strong>概念的分量(fen，一声)的多少</strong>， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
<p>这两种认知分别可以用向量距离、向量语义投影计算得来。</p>
<ul>
<li>tm.sematic_distance(words, c_words1, c_words2)  向量距离</li>
<li>tm.sematic_projection(words, c_words1, c_words2)  向量语义投影</li>
</ul>
<h3 id="41-tmsematic_distancewords-c_words1-c_words2">4.1 tm.sematic_distance(words, c_words1, c_words2)</h3>
<p>分别计算words与c_words1、c_words2语义距离，返回距离差值。</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果d1-d2&lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<p><strong>下载glove_w2v.6B.100d.txt</strong>链接: <a href="https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw">https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw</a> 提取码: 72l0</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#Note: this is a word2vec format model</span>
<span class="n">tm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Text2Mind</span><span class="p">(</span><span class="n">w2v_model_path</span><span class="o">=</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="n">engineer</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">mans</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">womans</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">mans</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">womans</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><p>-0.38 意味着工程师更接近于男人，而不是女人。</p>
<br>
<h3 id="42-tmsematic_projectionwords-c_words1-c_words2">4.2 tm.sematic_projection(words, c_words1, c_words2)</h3>
<p><strong>语义投影</strong>，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，word越位于c_words2一侧。</p>
<p>下图是语义投影示例图，本文算法和图片均来自 &ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&rdquo;</p>
<p><img loading="lazy" src="img/Nature_Semantic_projection_recovering_human_knowledge_of.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">smalls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">bigs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">smalls</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">bigs</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p>在这几个动物尺寸的感知上，人类觉得老鼠体型是最小，马的体型是最大。</p>
<p><br><br></p>
<h2 id="引用说明">引用说明</h2>
<p>如果研究中使用cntext，请使用以下格式进行引用</p>
<h3 id="apalike">apalike</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Deng X., Nan P. (2022). cntext: a Python tool for text mining (version 1.7.9). DOI: 10.5281/zenodo.7063523 URL: https://github.com/hiDaDeng/cntext
</code></pre></div><h3 id="bibtex">bibtex</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@misc{YourReferenceHere,
author = {Deng, Xudong and Nan, Peng},
doi = {10.5281/zenodo.7063523},
month = {9},
title = {cntext: a Python tool for text mining},
url = {https://github.com/hiDaDeng/cntext},
year = {2022}
}
</code></pre></div><h3 id="endnote">endnote</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">%0 Generic
%A Deng, Xudong
%A Nan, Peng
%D 2022
%K text mining
%K text analysi
%K social science
%K management science
%K semantic analysis
%R 10.5281/zenodo.7063523
%T cntext: a Python tool for text mining
%U https://github.com/hiDaDeng/cntext
</code></pre></div><br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>sentence-transformer库 | 句子语义向量化</title>
      <link>https://textdata.cn/blog/sentence-transformer-tutorial/</link>
      <pubDate>Mon, 09 May 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/sentence-transformer-tutorial/</guid>
      <description>使用sentence-transformer库BERT技术，将句子语义向量化</description>
      <content:encoded><![CDATA[<blockquote>
<p>内容摘自</p>
<p>刘焕勇博客: <a href="https://liuhuanyong.github.io/">https://liuhuanyong.github.io/</a></p>
<p>原文地址: <a href="https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q">https://mp.weixin.qq.com/s/fkgk8l_Vd4YDU_K6G54F4Q</a></p>
<p>公众号: 老刘说NLP</p>
</blockquote>
<p>word2vec、glove是两种静态的词向量模型，即每个词语只有一个固定的向量表示。但在不同语境中，词语的语义会发生变化，按道理词向量也应该动态调整。相比word2vec、glove生成的静态词向量， BERT是一种动态的技术，可以根据上下文情景，得到语义变化的词向量。</p>
<p>HuggingFace网站提供了简易可用的数据集、丰富的预训练语言模型， 通过sentence-transformer库，我们可以使用HuggingFace内的预训练模型，得到不同情景的文本的语义向量。</p>
<p>HuggingFace网站  <a href="https://huggingface.co/">https://huggingface.co/</a></p>
<p><img loading="lazy" src="img/HuggingFace.png" alt=""  />
</p>
<br>
<h2 id="动态句向量">动态句向量</h2>
<p>sentence-transformer框架提供了一种简便的方法来计算句子和段落的向量表示（也称为句子嵌入）</p>
<p><img loading="lazy" src="img/sentence-transformer.png" alt=""  />
</p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">pip3</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">sentence</span><span class="o">-</span><span class="n">transformers</span>
</code></pre></div><br>
<h2 id="代码">代码</h2>
<p><a href="sentence-transformer-tutorial.zip">click to download the code</a></p>
<p>使用huggingface中的distiluse-base-multilingual-cased与训练模型，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">util</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;distiluse-base-multilingual-cased&#39;</span><span class="p">)</span>
</code></pre></div><p>第一次运行上方的代码，需要运行一定的时间用于下载。下载完成后，我们使用同种语义的中英文句子，分别计算得到emb1和emb2两个句向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">emb1 = model.encode(&#39;Natural language processing is a hard task for human&#39;)

emb2 = model.encode(&#39;自然语言处理对于人类来说是个困难的任务&#39;)
emb1
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 2.58186590e-02,  4.65703346e-02,  4.25276496e-02, -1.67875513e-02,
        5.56012690e-02, -3.44308838e-02, -6.53978735e-02,  1.77450478e-02,
       -3.47155109e-02,  2.86140274e-02,  2.48657260e-02,  7.94188876e-04,
        5.09755425e-02, -1.76107027e-02, -1.04308855e-02,  7.61642214e-03,
        ...
        4.28482369e-02,  1.76657233e-02, -5.83355911e-02,  1.92921527e-03,
        2.81221420e-02,  5.24400780e-03,  2.10703332e-02,  7.96715263e-03,
       -6.80630878e-02, -2.05304120e-02, -2.43293475e-02, -1.87458862e-02],
      dtype=float32)
</code></pre></div><p>在distiluse-base-multilingual-cased这种模型中， 不同语言的同义句应该具有类似的语义，那么cos相似度应该是很大的。越接近于1越相似；越接近于0，越不相似。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">cos_sim</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">pytorch_cos_sim</span><span class="p">(</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb2</span><span class="p">)</span>
<span class="n">cos_sim</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[0.8960]])
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 从符号到嵌入：计算社会科学的两种文本表示</title>
      <link>https://textdata.cn/blog/from_sysbol_to_embeddings_in_computational_social_science/</link>
      <pubDate>Mon, 25 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/from_sysbol_to_embeddings_in_computational_social_science/</guid>
      <description>如何有效地表示数据以挖掘我们想要的计算社会科学的含义？为了探索答案，我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示。How to efficiently represent data to mine the implications we want for computational social science? To explore the answer, we conduct a thorough review of data representations for text and the web in CSS, and we summarize existing representations into two schemes, symbol-based and embedding-based</description>
      <content:encoded><![CDATA[<p>B站看到大牛刘知远关于文本分析在计算社会科学领域应用的分享，解答了我对文本表示的疑惑，看完了能对文本的特征工程加深理解，同时也能更清晰未来如何借助计算机科学技术开展社会科学研究。</p>
<blockquote>
<p><strong>全文摘抄自</strong></p>
<p>Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.</p>
</blockquote>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1qi4y1Q7qj&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<h2 id="摘要">摘要</h2>
<p><strong>计算社会科学</strong>（CSS），旨在利用计算方法来解决社会科学问题，是一个新兴和快速发展的领域。 CSS 的研究是数据驱动的，并且显着受益于在线用户生成内容和社交网络的可用性，其中包含用于调查的富文本和网络数据。然而，这些大规模、多模态的数据也给研究人员带来了很大的挑战：<strong>如何有效地表示数据以挖掘我们想要的 CSS 含义</strong>？为了探索答案，<strong>我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示</strong>，并为每个方案介绍了一系列典型的方法。随后，我们基于对来自 6 个涉及 CSS 的顶级场所的 400 多篇研究文章的调查，展示了上述表示的应用。从这些应用程序的统计数据中，<strong>我们挖掘出每种表示的强度，并发现基于嵌入的表示在过去十年中出现并获得越来越多的关注的趋势</strong>。最后，我们讨论了几个关键挑战和未来方向的开放性问题。本调查旨在为 CSS 研究人员提供对数据表示的更深入理解和更明智的应用。</p>
<p><strong>关键词</strong>：计算社会科学；基于符号的表示；基于嵌入的表示；社交网络</p>
<br>
<h2 id="一计算社会学数据分析流程">一、计算社会学数据分析流程</h2>
<p>其中第二步，数据表示目前有两大类表示(特征工程)方法</p>
<ul>
<li><strong>基于符号的文本表示</strong>；符号可以是单词(或词组)，也可以是概念(如正面情感、负面情感)</li>
<li><strong>基于嵌入(分布式)的文本表示</strong>；相比于符号法，将词(词组)看做一个点。嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(king)-v(queen)约等于v(man)-v(woman)</li>
</ul>
<p><img loading="lazy" src="img/fig1.png" alt=""  />
</p>
<br>
<h2 id="二基于符号的文本表示">二、基于符号的文本表示</h2>
<p>基于符号的文本表示一般来说默认词语是不可分的符号，每个词能根据词频统计出现次数的多与少，或是否存在。</p>
<h3 id="21-词语层面">2.1 词语层面</h3>
<ul>
<li>
<p>基于词频表示</p>
<ul>
<li>是否出现，出现标位1，反之标位0。</li>
<li>出现多少，词语出现几次，标为几个。</li>
</ul>
</li>
<li>
<p>基于特征表示，如每个词带有权重(得分)</p>
</li>
<li>
<p>基于网络表示，如词语共现网络(矩阵)</p>
</li>
</ul>
<h3 id="22-句子层面">2.2 句子层面</h3>
<ul>
<li>
<p>基于词频的表示</p>
<ul>
<li>one-hot 将文本转为向量，向量中每个数，词语出现标位1，反之标位0</li>
<li>bag-of-words，将文本转为向量，向量中每个数，词语出现n次标记为n</li>
<li>n-grams，对词组的处理，将词组看做一个单词(整体)。</li>
<li>Tf-Idf ,该算法分为tf和idf两部分。其中tf与bag-of-words类似，考虑词语出现次数。而idf还考虑词语在语料中出现场景的稀缺性程度。</li>
</ul>
</li>
<li>
<p>基于语法特征，如句法依存关系，类似于英语语法，将句子分为主谓宾、动词、名词等。</p>
</li>
<li>
<p>词典法，如使用正、负情感词典，对文本数据进行情感分析，可以得到pos和neg的各自得分</p>
</li>
</ul>
<p><img loading="lazy" src="img/fig2.png" alt=""  />
</p>
<br>
<h2 id="三基于嵌入的文本表示">三、基于嵌入的文本表示</h2>
<h3 id="31词语层面">3.1词语层面</h3>
<p>嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(best)-v(good)约等于v(worst)-v(bad)</p>
<h3 id="32-句子层面">3.2 句子层面</h3>
<p>词语是向量，那么由词语组成的句子也会加权得到一个向量。含有相似话题或含义相近的句子在多维向量空间中会比较接近。</p>
<p><img loading="lazy" src="img/fig7.png" alt=""  />
</p>
<br>
<h2 id="四任务分类文本的用法">四、任务分类：文本的用法</h2>
<p><img loading="lazy" src="img/fig16.png" alt=""  />
</p>
<p>有了文本数据，刚刚解决了如何表示文本。接下来，需要明确，我们使用文本目的是为了做哪类分析，得到哪些信息。有8种常见的文本分析图式</p>
<ul>
<li>描述性。如随时间推移，词频的发展趋势是变大的</li>
<li>相关性。</li>
<li>聚类。如lda话题分析、k-means聚类</li>
<li>相似度。两个文档转为向量后，可以通过cosine计算相似度</li>
<li>分类。机器学习分类，判断某文本隶属于哪个类别</li>
<li>回归。例如根据文本，判断某件事发生的概率</li>
<li>语言模型。</li>
<li>排序。</li>
</ul>
<br>
<h2 id="五发文趋势-符号vs嵌入">五、发文趋势-符号vs嵌入</h2>
<p>基于上一节中对应用程序的介绍，可以观察到基于符号和基于嵌入的表示在 <strong>计算社会科学</strong>中都得到了相当大的采用。为了明确研究它们的覆盖范围，我们计算了每年使用两种表示中的一种或两种的作品数量，如图 17 所示。通过比较nature、science、pnas三大顶级期刊，我们可以发现使用<strong>基于嵌入表示</strong>的文章比例在过去几年中逐渐。这表明越来越多的 计算社会科学文章 已经考虑并受益于基于嵌入表示。</p>
<p>图 18 显示了在 计算机领域ACL、WWW 和 KDD 的会议上中，发现使用基于嵌入的表示的文章数量已大大超过使用基于符号的表示的文章数量。然而，与图 17 相比，计算机科学会议中基于嵌入的表示的数量与三个多学科期刊之间存在很大差距。</p>
<p><img loading="lazy" src="img/3_top_journals.png" alt=""  />
</p>
<p><img loading="lazy" src="img/nlp.png" alt=""  />
</p>
<p>总而言之，在过去十年中，基于嵌入的表示已经出现并在 计算社会科学 中发挥着越来越重要的作用。</p>
<br>
<h2 id="六趋势解读">六、趋势解读</h2>
<p>基于它们的内部机制和现有应用，对趋势解读，我们总结出以下三个关键点。</p>
<p>基于符号的表示因其明确性和可解释性而擅长描述和关系的任务。</p>
<p>基于符号的表示中的每个值都表示一定的人类可读的含义，因此我们可以直接使用它来观察数据的分布，以及提取对象之间的关系。例如，基于频率的词表示用于观察文化变化并捕捉新闻中提及次数与公司股票交易量之间的关系。虽然基于主题模型的表示和一些基于神经的表示在一定程度上具有实际意义，但它们对于社会科学研究人员来说仍然是模糊的并且不那么引人注目。</p>
<p>由于神经网络具有强大的拟合数据和提取深度语义的能力，基于嵌入的表示在预测（例如分类和回归）和相似性任务中表现更好。一方面，神经网络通过大规模神经元的连接实现高效的输入输出映射功能。另一方面，通过多层网络的构建，实现深层语义和抽象概念的提取。现有研究表明，深层捕获相对于浅层更抽象的特征。诸如社会偏见和道德化之类的抽象概念都可以通过基于嵌入的表示来很好地衡量。虽然我们提到基于符号的表示可以通过一些定义的符号来代表抽象概念，但这种表示仍然是部分和肤浅的，很难捕捉到它们的全貌。</p>
<p>基于嵌入的表示需要更少的人力。基于符号的表示通常需要大量的专家知识来定义研究对象的特征，这是劳动密集型的。此外，对于一些没有充分特征的抽象概念或对象，它们的表现将受到限制。与它们不同的是，基于嵌入的表示是从数据中自动提取的，几乎不需要人工干预，甚至可以补充人类知识。例如，可以使用神经网络来自动恢复丢失的巴比伦文本，这即使对专家来说也是具有挑战性的。此外，基于嵌入的表示可以在没有手动定义的情况下描述语言的复杂性和歧义性。</p>
<br> 
<h2 id="七未来展望">七、未来展望</h2>
<p>尽管在过去十年中出现了从符号到嵌入的趋势，但仍有许多挑战和悬而未决的问题有待探索。展望未来，我们列出了一些与计算社会科学 中的数据表示相关的基本和潜在的未来方向。</p>
<p>预训练的语言模型。近年来，预训练的语言模型受到了相当大的关注，并在处理文本数据方面取得了巨大的成功 [100, 240]。这些模型从百科全书和书籍等海量文本数据中学习丰富的语义信息，仅在下游任务中进行微调以实现有效的基于嵌入的表示。因此，对于 计算社会科学，我们可以借助预训练的语言模型获得更通用、更健壮的文本表示。与从传统神经网络模型中学习的表示相比，这些表示不仅可以更广泛、更准确地从文本中分析社会现象，而且还可以减少那些需要大量标记数据的任务的人工注释。</p>
<p>图神经网络。通过消息传递机制，图神经网络 [461] 可以同时有效地对网络拓扑和节点/边缘特征（例如文本信息）进行建模，从而提供一个统一的框架来利用来自异构来源的信息。 计算社会科学 中的许多场景需要处理社交网络以及个人特征。因此，图神经网络技术在 计算社会科学 研究中具有很大的应用潜力，可以学习融合文本和网络信息的表示。事实上，计算机科学中的各种应用，例如自然语言处理 [418] 和推荐系统 [439]，已经采用图神经网络进行建模。</p>
<p>设计为预测和相似性。基于嵌入的表示以丰富和深层次的语义而闻名，而基于符号的表示通常保留在部分和浅层语义中。同时，基于嵌入的表示擅长预测和相似性的任务。因此，为了充分利用嵌入中的强语义，鼓励 计算社会科学 研究人员尽可能将研究问题设计为预测或相似性任务。例如，我们可以将社会偏见问题设计为性别词和中性词嵌入之间的相似性度量 [59, 133]。此外，人类语言的复杂性可以设计为一项预测任务，它以语言模型为指标查看单词或句子的预测概率[155]。</p>
<p>可解释性。诚然，基于嵌入的方法的一个缺点是缺乏可解释性。这个问题会损害与道德、安全或隐私相关的决策关键系统的应用。尽管嵌入模型，尤其是神经网络模型的可解释性尚未完全解决，但计算机科学领域的研究人员已经做出了一些努力，以提高基于神经模型的可解释性 [16]。因此，利用基于嵌入的模型和可解释性分析方法进行有效和（部分）可解释的预测将是一个有趣的方向。</p>
<br>
<h2 id="结论">结论</h2>
<p>计算社会科学作为一个新兴且有前途的跨学科领域，近年来吸引了相当多的研究兴趣。 计算社会科学 研究中广泛使用两种主要类型的数据，即文本数据和网络数据。在本次调查中，我们首先将数据表示总结为基于符号和基于嵌入的表示，并在构建这些表示时进一步介绍典型的方法。之后，我们基于来自 6 个经典期刊和会议的 400 多篇高被引文献，对这两类表示的应用进行了全面回顾。根据对这些应用的统计，发现了 计算社会科学 中基于嵌入的文本和网络表示正在出现和增长的趋势，我们进一步讨论了其中的原因。最后，我们提出了 计算社会科学 中的四个挑战和未解决的问题，它们是需要探索的基本和潜在方向。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>中文词向量资源汇总 &amp; 使用方法</title>
      <link>https://textdata.cn/blog/embeddings_resource_usage_method/</link>
      <pubDate>Thu, 21 Apr 2022 15:40:10 +0600</pubDate>
      
      <guid>/blog/embeddings_resource_usage_method/</guid>
      <description>数十种中文词向量模型资源下载&amp;amp;使用方法。Dozens of Chinese word vector model resource downloads &amp;amp; usage methods</description>
      <content:encoded><![CDATA[<br>
<h2 id="项目地址">项目地址</h2>
<p><a href="https://github.com/Embedding/Chinese-Word-Vectors">https://github.com/Embedding/Chinese-Word-Vectors</a></p>
<p>Chinese-Word-Vectors项目提供超过100种中文词向量，其中包括不同的表示方式（稠密SGNS和稀疏PPMI）、不同的上下文特征（词、N元组、字等等）、以及不同的训练语料。获取预训练词向量非常方便，下载后即可用于下游任务。</p>
<br>
<h2 id="参考文献">参考文献</h2>
<p>如果使用了本项目的词向量和CA8数据集请进行如下引用：</p>
<p>Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, <a href="http://aclweb.org/anthology/P18-2023"><em>Analogical Reasoning on Chinese Morphological and Semantic Relations</em></a>, ACL 2018.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">@InProceedings{P18-2023,
  author =  &#34;Li, Shen
    and Zhao, Zhe
    and Hu, Renfen
    and Li, Wensi
    and Liu, Tao
    and Du, Xiaoyong&#34;,
  title =   &#34;Analogical Reasoning on Chinese Morphological and Semantic Relations&#34;,
  booktitle =   &#34;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&#34;,
  year =  &#34;2018&#34;,
  publisher =   &#34;Association for Computational Linguistics&#34;,
  pages =   &#34;138--143&#34;,
  location =  &#34;Melbourne, Australia&#34;,
  url =   &#34;http://aclweb.org/anthology/P18-2023&#34;
}
</code></pre></div><br>
<h3 id="不同领域">不同领域</h3>
<p>下列词向量基于不同的表示方式、不同的上下文特征以及不同领域的语料训练而成。</p>
<table align="center">
    <tr align="center">
        <td colspan="5"><b>Word2vec / Skip-Gram with Negative Sampling (SGNS)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1Rn7LtTH0n7SHyHPfjRHbkg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1XEmP_0FkQwOjipCjI2OPEw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1eeCS7uD3e_qVN8rPwmXhAw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1IiIbQGJ_AooTj5s8aZYcvA">300d</a> / PWD: 5555</td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1AmXYWVgkxrG4GokevPtNgA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZKePwxwsDdzNrfkc6WKdGQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1ZBVVD4mUSUuXOxlZ3V71ZA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19wQrclyynOnco3JBvnI5pA">300d</td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/19sqMz-JAhhxh3o6ecvQxQw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1upPkA8KJnxTZBfjuNDtaeQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1BvKk2QjbtQMch7EISppW2A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/19Vso_k79FZb5OZCWQPAnFQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1tUghuTno5yOvOx4LXA9-wg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13yVrXeGYkxdGW3P6juiQmA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1pUqyn7mnPcUmzxT64gGpSw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1svFOwFBKnnlsqrF1t99Lnw">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/1EhtsbDa3ekzZPODWNLHcXA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FcPHv7S4vUgnL7WeWf4_PA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/13CAxY5ffRFuOcHZu8VmArw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sqvrUtGBAZ7YWEsGz41DRQ">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VGOs0RH7DXE5vRrtw6boQA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1OQ6fQLCgqT43WTwh5fh_lg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1_xogqF9kJT6tmQHSAYrYeg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Fo27Lv_0nz8FXg-xbOz14Q">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/1zbuUJEEEpZRNHxZ7Gezzmw">300d</a></td>
      <td><a href="https://pan.baidu.com/s/11PWBcvruXEDvKf2TiIXntg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/10bhJpaXMCUK02nHvRAttqA">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1FHl_bQkYucvVk-j2KG4dxA">300d</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1ciq8iXtcrHpu3ir_VhK0zg">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1Oa4CkPd8o2xd6LEAaa4gmg">300d</a> / PWD: z5b4</td>
      <td><a href="https://pan.baidu.com/s/1IG8IxNp2s7vVklz-vyZR9A">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1SEOKrJYS14HpqIaQT462kA">300d</a> / PWD: yenb</td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1vPSeUsSiWYXEWAuokLR0qQ">300d</a></td>
      <td><a href="https://pan.baidu.com/s/1sS9E7sclvS_UZcBgHN7xLQ">300d</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合<br>Baidu Netdisk / Google Drive</td>
      <td>
        <a href="https://pan.baidu.com/s/1luy-GlTdqqvJ3j-A4FcIOw">300d</a><br>
        <a href="https://drive.google.com/open?id=1Zh9ZCEu8_eSQ-qkYVQufQDNKPC4mtEKR">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1oJol-GaRMk4-8Ejpzxo6Gw">300d</a><br>
        <a href="https://drive.google.com/open?id=1WUU9LnoAjs--1E_WqcghLJ-Pp8bb38oS">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/1DjIGENlhRbsVyHW-caRePg">300d</a><br>
        <a href="https://drive.google.com/open?id=1aVAK0Z2E5DkdIH6-JHbiWSL5dbAcz6c3">300d</a>
      </td>
      <td>
        <a href="https://pan.baidu.com/s/14JP1gD7hcmsWdSpTvA3vKA">300d</a><br>
        <a href="https://drive.google.com/open?id=1kSAl4_AOg3_6ayU7KRM0Nk66uGdSZdnk">300d</a>
      </td>
    </tr>
</table>
<table align="center">
    <tr align="center">
        <td colspan="5"><b>Positive Pointwise Mutual Information (PPMI)</b></td>
    </tr>
    <tr align="center">
        <td rowspan="2">语料</td>
        <td colspan="4">上下文特征</td>
    </tr>
    <tr  align="center">
      <td>词</td>
      <td>词 + N元组</td>
      <td>词 + 字</td>
      <td>词 + 字 + N元组</td>
    </tr>
    <tr  align="center">
      <td>Baidu Encyclopedia 百度百科</td>
      <td><a href="https://pan.baidu.com/s/1_itcjrQawCwcURa7WZLPOA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1cEZzN1S2senwWSyHOnL7YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1KcfFdyO0-kE9S9CwzIisfw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FXYM3CY161_4QMgiH8vasQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Wikipedia_zh 中文维基百科</td>
      <td><a href="https://pan.baidu.com/s/1MGXRrc54nITPzQ7sfEUjMA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1mtxZna8UJ7xBIxhBFntumQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dDImpAx41V73Byl2julOGA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1bsBQHXFpxMHGBexYof1_rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>People's Daily News 人民日报</td>
      <td><a href="https://pan.baidu.com/s/1NLr1K7aapU2sYBvzbVny5g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1LJl3Br0ccGDHP0XX2k3pVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1GQQXGMn1AHh-BlifT0JD2g">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1Xm9Ec3O3rJ6ayrwVwonC7g">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Sogou News 搜狗新闻</td>
      <td><a href="https://pan.baidu.com/s/1ECA51CZLp9_JB_me7YZ9-Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1FO39ZYy1mStERf_b53Y_yQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1lLBFBk8nn3spFAvKY9IJ6A">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1f-dLQZlZo_-B5ZKcPIc6rw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Financial News 金融新闻</td>
      <td><a href="https://pan.baidu.com/s/10wtgdmrTsTrjpSDvI0KzOw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b6zjvhOIqTdACSSbriisVw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1w24vCfgqcoJvPxsB5VrRvw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1b9BPiDRhiEZ-6ybTcovrqQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Zhihu_QA 知乎问答 </td>
      <td><a href="https://pan.baidu.com/s/1VaUP3YJC0IZKTbJ-1_8HZg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1g39PKwT0kSmpneKOgXR5YQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1d8Bsuak0fyXxQOVUiNr-2w">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1D5fteBX0Vy4czEqpxXjlrQ">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Weibo 微博</td>
      <td><a href="https://pan.baidu.com/s/15O2EbToOzjNSkzJwAOk_Ug">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/11Dqywn0hfMhysto7bZS1Dw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1wY-7mfV6nwDj_tru6W9h4Q">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1DMW-MgLApbQnWwDd-pT_qw">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Literature 文学作品</td>
      <td><a href="https://pan.baidu.com/s/1HTHhlr8zvzhTwed7dO0sDg">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1jAuGJBxKqgapt__urGsBOQ">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/173AJfCoAV0ZA8Z31tKBdTA">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1dFCxke_Su3lLsuwZr7co3A">Sparse</a></td>
    </tr>
    <tr  align="center">
      <td>Complete Library in Four Sections<br />四库全书<sup>*</sup></td>
      <td><a href="https://pan.baidu.com/s/1NJ1Gc99oE0-GV0QxBqy-qw">Sparse</a></td>
      <td><a href="https://pan.baidu.com/s/1YGEgyXIbw0O4NtoM1ohjdA">Sparse</a></td>
      <td>NAN</td>
      <td>NAN</td>
    </tr>
    </tr>
    <tr  align="center">
      <td>Mixed-large 综合</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
      <td>Sparse</td>
    </tr>
</table>
<p><sup>*</sup>由于古汉语中绝大部份词均为单字词，因此只需字向量。</p>
<br>
<h2 id="语料">语料</h2>
<p>项目花费了大量精力来收集了来自多个领域的语料。所有的文本数据均移除了html和xml标记，仅保留了纯文本。之后采用了<a href="https://github.com/hankcs/HanLP">HanLP(v_1.5.3)</a>对文本进行了分词。此外，我们将繁体中文用<a href="https://github.com/BYVoid/OpenCC">Open Chinese Convert (OpenCC)</a>转换为了简体中文。更详细的语料信息如下所示：</p>
<table align="center">
	<tr align="center">
		<td><b>语料</b></td>
		<td><b>大小</b></td>
		<td><b>词数量</b></td>
		<td><b>词汇量</b></td>
		<td><b>详情</b></td>
	</tr>
	<tr align="center">
		<td>Baidu Encyclopedia<br />百度百科</td>
		<td>4.1G</td>
		<td>745M</td>
		<td>5422K</td>
		<td>中文百科<br />https://baike.baidu.com/</td>
	</tr>
	<tr align="center">
		<td>Wikipedia_zh<br />中文维基百科</td>
		<td>1.3G</td>
		<td>223M</td>
		<td>2129K</td>
		<td>中文维基百科<br />https://dumps.wikimedia.org/</td>
	</tr>
	<tr align="center">
		<td>People's Daily News<br />人民日报</td>
		<td>3.9G</td>
		<td>668M</td>
		<td>1664K</td>
		<td>人民日报新闻数据(1946-2017)<br />http://data.people.com.cn/</td>
	</tr>
	<tr align="center">
		<td>Sogou News<br />搜狗新闻</td>
		<td>3.7G</td>
		<td>649M</td>
		<td>1226K</td>
		<td>Sogou labs的新闻数据<br />http://www.sogou.com/labs/</td>
	</tr>
  <tr align="center">
    <td>Financial News<br />金融新闻</td>
    <td>6.2G</td>
    <td>1055M</td>
    <td>2785K</td>
    <td>从多个网站收集到的金融新闻</td>
  </tr>
	<tr align="center">
		<td>Zhihu_QA<br />知乎问答</td>
		<td>2.1G</td>
		<td>384M</td>
		<td>1117K</td>
		<td>中文问答数据<br />https://www.zhihu.com/</td>
	</tr>
	<tr align="center">
		<td>Weibo<br />微博</td>
		<td>0.73G</td>
		<td>136M</td>
		<td>850K</td>
		<td>NLPIR Lab提供的微博数据<br />http://www.nlpir.org/wordpress/download/weibo.7z</td>
	</tr>
	<tr align="center">
		<td>Literature<br />文学作品</td>
		<td>0.93G</td>
		<td>177M</td>
		<td>702K</td>
		<td>8599篇现代文学作品</td>
	</tr>
	<tr align="center">
		<td>Mixed-large<br />综合</td>
		<td>22.6G</td>
    <td>4037M</td>
    <td>10653K</td>
		<td>上述所有数据的汇总</td>
	</tr>
  <tr align="center">
    <td>Complete Library in Four Sections<br />四库全书</td>
    <td>1.5G</td>
    <td>714M</td>
    <td>21.8K</td>
    <td>目前最大的古代文献汇总</td>
  </tr>
</table>
上述统计结果中，所有词都被计算在内，包括低频词。
<br>
<h2 id="导入模型代码">导入模型(代码)</h2>
<p>例如我下载了多个词模型，下载得到bz2结尾的文件名，例如<code>sgns.financial.bigram.bz2</code>。</p>
<p><img loading="lazy" src="models.png" alt=""  />
</p>
<p>使用方式</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1">#以金融sgns.financial.bigram.bz2为例</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;embeddings/sgns.financial.bigram.bz2&#39;</span><span class="p">,</span> 
                                          <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                          <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>


<span class="n">model</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x7fe7fad79d60&gt;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;投资&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.084635,  0.890228, -0.23223 , -0.308985,  0.058241,  0.458777,
       -0.152547, -0.413471,  0.269701, -0.078043, -0.4155  ,  0.074735,
        0.35714 ,  0.103431,  0.601784, -0.390854,  0.814801, -0.122664,
       -1.076744,  0.516941, -0.293319, -0.310251, -0.407794,  0.003898,
       -0.210962,  0.378095, -0.345955, -0.223848,  0.700162,  0.207644,
        0.426249, -0.272832, -0.110305, -0.701062, -0.173407, -0.172121,
       -0.682592,  0.593414,  0.279591, -0.408284, -0.166693,  0.753402,
        0.037375,  0.141865, -0.246024, -0.108663, -0.225255, -0.856601,
        0.381026,  0.401248,  0.012108, -0.126305, -0.374255,  0.728795,
        0.219549, -0.354029, -0.353131,  0.064867,  0.49565 , -0.503267,
       -0.304075,  0.145036,  0.688948,  0.063382, -0.223243,  0.474251,
        0.80543 ,  0.683178,  0.118159,  0.408411, -0.020066,  0.009045,
       -0.135446, -0.069633,  0.206357,  0.482845, -0.075307,  0.06433 ,
       -0.112367,  0.011816,  0.87427 , -0.120287, -0.31036 ,  0.369985,
        0.560386, -0.215248,  0.389631,  0.042943, -0.319149,  0.951551,
       -0.335188,  0.642246, -0.55546 ,  0.322397,  0.659618, -0.213124,
        0.346696, -0.342239,  0.31479 ,  0.078533, -0.345148,  0.815577,
       -0.530134,  0.303419, -0.158916, -0.190564,  0.436046, -0.112251,
       -0.339966,  0.253645,  0.181076,  0.122875, -0.310951, -0.126253,
        1.641405,  0.357906,  0.165796,  0.398656, -0.330591,  0.20328 ,
       -0.077191, -0.421248, -0.078504, -0.734519,  0.146212,  0.535727,
        0.014134,  0.040322, -0.44809 , -0.758205, -0.151237,  0.248258,
       -0.319704,  0.656033, -0.518857,  0.932356, -1.01786 , -0.46354 ,
        0.160921, -0.243597,  0.106666, -0.03404 ,  0.010672,  0.260243,
        0.899813,  0.171735, -0.108209, -0.009843, -0.18113 ,  0.302494,
        0.187285,  0.064669, -0.502041, -0.724377, -0.294312, -0.522256,
        0.334543,  0.740455, -0.357653,  0.540747,  0.256146,  0.513839,
        1.116628, -0.626111,  0.505574,  0.089774, -0.381137, -0.282352,
       -0.457542,  0.198909,  0.313638,  0.560809,  0.25295 ,  0.878158,
       -0.289311, -0.629047,  0.011103,  0.041058, -0.291302, -0.014001,
       -0.027697, -0.445817, -0.070086,  0.159816, -0.120071,  1.280489,
       -0.108866,  0.01586 , -0.505574, -0.679772, -0.343165,  0.595633,
        0.438108, -0.364066, -0.393667,  0.442285,  0.24979 , -0.191607,
        0.425692,  0.535577, -0.480332, -0.737461,  0.588498, -0.380264,
        0.151292,  0.077519, -0.221384,  0.699436,  0.401642,  0.509026,
       -0.411141,  0.206719, -0.097051, -0.451834, -0.825617,  0.602984,
        0.2853  ,  0.46055 ,  0.96472 ,  0.322712, -0.373446,  0.207944,
        0.236688,  0.566523,  0.037644,  1.241091,  0.025682,  0.373211,
        0.097712, -0.195355,  0.264579, -0.072992, -0.121629,  0.041688,
        0.213666,  0.329652, -0.015182,  0.396307,  0.117955,  0.119577,
       -0.334761, -0.135917,  0.409983,  0.512367, -0.292204,  0.302897,
       -0.325733,  0.383173, -0.92419 , -0.377535, -0.059801, -0.606275,
       -0.240482,  0.054021, -0.581386, -0.555691,  0.158354,  0.103765,
        0.107681,  0.248877, -0.597925,  0.193332,  0.844085,  0.00584 ,
        0.041622, -0.111235,  0.617778,  0.234883, -0.09562 ,  0.408324,
       -0.107121,  0.717875,  0.674794,  0.127214, -0.178357,  0.331436,
        0.417898, -0.650833, -0.428309, -0.576132,  0.210533, -0.057879,
       -0.578397,  0.468586,  0.103365, -0.403216, -0.398776,  0.094514,
       -0.130387,  0.628187, -0.463082, -0.951649,  0.561544,  0.118903,
        0.448327, -0.171685, -0.672348,  0.069471,  0.556452, -0.335425],
      dtype=float32)
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">model.similar_by_key(&#39;投资&#39;)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;长期投资&#39;, 0.5135656595230103),
 (&#39;投资规模&#39;, 0.5089880228042603),
 (&#39;智百扬&#39;, 0.49565914273262024),
 (&#39;投资总额&#39;, 0.4955061078071594),
 (&#39;洛辉&#39;, 0.489188551902771),
 (&#39;337409&#39;, 0.48917514085769653),
 (&#39;洛盛&#39;, 0.4819018244743347),
 (&#39;洛腾&#39;, 0.4728960692882538),
 (&#39;394150&#39;, 0.4704836308956146),
 (&#39;投资额&#39;, 0.4685181975364685)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">similar_by_key</span><span class="p">(</span><span class="s1">&#39;风险&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;提示&#39;, 0.6549968123435974),
 (&#39;经营风险&#39;, 0.6316577792167664),
 (&#39;景气衰退&#39;, 0.544153094291687),
 (&#39;风险分析&#39;, 0.5439289212226868),
 (&#39;遇宏观&#39;, 0.5435716509819031),
 (&#39;信用风险&#39;, 0.5345730185508728),
 (&#39;承受能力&#39;, 0.5291797518730164),
 (&#39;防范&#39;, 0.5271924138069153),
 (&#39;系统性&#39;, 0.5178108811378479),
 (&#39;不确定性&#39;, 0.5173759460449219)]
</code></pre></div><p>向量运行效果还行，感兴趣的同学也可以根据自己的数据训练word2vec模型，训练及使用的办法参照文章</p>
<p><a href="https://textdata.cn/blog/douban_w2v/">豆瓣影评 | 探索词向量妙处</a></p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>豆瓣影评 | 探索词向量妙处</title>
      <link>https://textdata.cn/blog/douban_w2v/</link>
      <pubDate>Thu, 21 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/douban_w2v/</guid>
      <description>使用cntext训练、使用词向量。</description>
      <content:encoded><![CDATA[<p>本文要点</p>
<ul>
<li>读取csv</li>
<li>cntext训练词向量模型</li>
<li>cntext扩展pos、neg词典</li>
<li>导入词向量模型</li>
<li>运用词向量模型</li>
</ul>
<br>
<h2 id="代码下载">代码下载</h2>
<p>链接: <a href="https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg">https://pan.baidu.com/s/1BFUb7myg6svTUZJfnvZfAg</a> 提取码: og9t</p>
<p><br><br></p>
<h2 id="一读取数据">一、读取数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;douban.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;电影  : </span><span class="si">{}</span><span class="s2"> 部&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Movie_Name_CN</span><span class="o">.</span><span class="n">nunique</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;评论  : </span><span class="si">{}</span><span class="s2"> 条&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)))</span>
</code></pre></div><pre><code>电影  : 28 部
评论  : 2125056 条
</code></pre>
<br>
<h2 id="二训练模型">二、训练模型</h2>
<p>使用<a href="https://textdata.cn/blog/cntext_simplification/">cntext库</a>训练词向量word2vec模型,这里我把csv数据整理为txt</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">cntext</span> <span class="kn">import</span> <span class="n">W2VModels</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#训练word2vec模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">())</span>  <span class="c1">#语料数据</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;douban.txt&#39;</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...预处理    语料 ...
Step 2/4:...训练   word2vec模型
            耗时   2001 s
        
</code></pre></div><p>cntext可以用于扩展词典</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;pos.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;neg.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 3/4:...准备 每个seed在word2vec模型中的相似候选词...
Step 4/4 完成! 耗时 2302 s
Step 3/4:...准备 每个seed在word2vec模型中的相似候选词...
Step 4/4 完成! 耗时 2303 s
</code></pre></div><p>在代码所在文件夹内可以找到</p>
<ul>
<li>output/w2v_candi_words/w2v.model</li>
<li>新的  pos.txt</li>
<li>新的  neg.txt</li>
</ul>
<p>新的pos.txt是对pos.txt词典的扩展。</p>
<br>
<br>
<h2 id="三导入w2v模型">三、导入w2v模型</h2>
<p>有的时候数据量特别大，模型训练十分不易。</p>
<p>这时，保存已训练好的模型，不止下次不用再同样的数据再次训练，也可分享给其他人使用。</p>
<p>训练结束后，在代码所在文件夹内可以找到 <code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span>
</code></pre></div><pre><code>&lt;gensim.models.keyedvectors.KeyedVectors at 0x7face0574880&gt;
</code></pre>
<p>w2v_models数据类型为KeyedVectors， 在本文中使用w2v_models代指KeyedVectors</p>
<br>
<h2 id="四玩转词向量">四、玩转词向量</h2>
<p>用户级的数据(如在线评论)感觉生成的向量会准一些，<strong>词向量的方向，近义反义在向量中都有体现</strong>。</p>
<p><img loading="lazy" src="man-woman.png" alt=""  />
</p>
<p>例如本文使用的是28部电影的2125056条影评， 一般评论内容包含电影相关信息，如电影题材、是否值的观影等。</p>
<p>而在我们训练出模型w2v_models存在一些常用的方法</p>
<ul>
<li><strong>w2v_model.get_vector(key)</strong> 获取key的词向量</li>
<li><strong>w2v_model.most_similar_to_given(key1, keys_list)</strong>  从 keys_list 中获取与 key1 最相似的词</li>
<li><strong>w2v_model.n_similarity(ws1, ws2)</strong> 两组词ws1, ws2 的相似度</li>
<li><strong>w2v_model.closer_than(key1, key2)</strong> 更接近于key1的词向量(相比于key2)</li>
<li><strong>w2v_model.most_similar(positive, negative)</strong> 找出与positive同方向，与negative反向相反的词。</li>
</ul>
<h3 id="41-get_vectorkey">4.1 get_vector(key)</h3>
<p>w2v_model.get_vector(key) 获取key的词向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取某词语的向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>array([ 0.06488553,  0.74188954,  0.25468495,  0.89755714,  1.8139195 ,
       -0.6950082 ,  0.24339403, -1.2188634 ,  0.543618  , -0.9988698 ,
        0.27471313,  0.9325699 , -0.5860608 , -0.5081917 ,  1.6423215 ,
       -0.0490295 , -0.3927043 ,  0.659067  ,  0.03185922, -1.021391  ,
       -1.3214804 , -0.28208104, -0.7819419 , -0.30637202, -1.5944146 ,
       -0.12383854, -0.70463836,  0.45689437,  1.223081  , -1.9453759 ,
       -0.5538997 , -0.9750523 , -0.10031194, -0.9568689 ,  0.30341247,
        1.1102395 ,  0.667315  , -1.1600997 , -0.26674765, -0.55144155,
       -0.3246094 ,  0.82902473, -0.47339582, -0.9009957 ,  1.7722464 ,
        0.28959563, -0.03453476,  0.4786787 , -0.48074463, -0.23090109,
       -0.49390873,  0.71246386,  2.1557336 ,  2.4899387 , -0.51481706,
        0.5579966 , -0.6973235 , -1.1408254 ,  0.72495663, -1.0326954 ,
       -0.5455598 ,  0.98941576, -1.2155218 , -0.9088408 ,  1.9184568 ,
       -0.21800426, -1.2009395 ,  0.29684314,  1.3672423 , -2.269391  ,
        0.6188098 , -0.02714545, -0.44811317,  1.4397241 , -1.0594722 ,
       -0.08088647, -0.13015983, -0.99255013,  0.62044877,  2.5046496 ,
        0.4054545 , -0.38767585, -0.6956541 ,  0.22991426,  0.5928579 ,
       -0.12684819, -0.17408212,  0.25033692, -1.4419957 , -0.27390227,
        1.166638  , -0.00624323, -1.6046506 ,  2.1633575 , -0.395548  ,
       -1.1297956 , -3.1474566 ,  0.38729438, -2.0434535 , -1.5511289 ],
      dtype=float32)
</code></pre>
<br>
<h3 id="42-most_similar_to_givenkey1-keys_list">4.2 most_similar_to_given(key1, keys_list)</h3>
<p>从 keys_list 中获取与 key1 最相似的词。例如在212w影评中，从<code>'爱情', '悬疑', '飞船', '历史', '战争'</code>找出最接近<code>'太空'</code>，最后返回<code>'飞船'</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#从 `keys_list` 中获取与 `key1` 最相似的 `key`。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar_to_given</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;太空&#39;</span><span class="p">,</span> 
                                <span class="n">keys_list</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;爱情&#39;</span><span class="p">,</span> <span class="s1">&#39;悬疑&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;历史&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>'飞船'
</code></pre>
<br> 
<h3 id="43-w2v_modeln_similarityws1-ws2">4.3 w2v_model.n_similarity(ws1, ws2)</h3>
<p>两组词ws1, ws2 的相似度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>

<span class="n">cosine_similarity</span><span class="p">([</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;理想&#39;</span><span class="p">)],</span>  
                  <span class="p">[</span><span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;现实&#39;</span><span class="p">)])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div><pre><code>0.5371934
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#cosine算法</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.5371934
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#计算两组键之间的余弦相似度。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;赞&#39;</span><span class="p">,</span> <span class="s1">&#39;推荐&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;无聊&#39;</span><span class="p">,</span> <span class="s1">&#39;尴尬&#39;</span><span class="p">,</span> <span class="s1">&#39;垃圾&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.35008422
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">n_similarity</span><span class="p">([</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> <span class="s1">&#39;梦想&#39;</span><span class="p">],</span> 
                       <span class="p">[</span><span class="s1">&#39;现实&#39;</span><span class="p">,</span> <span class="s1">&#39;生活&#39;</span><span class="p">])</span>
</code></pre></div><pre><code>0.48020104
</code></pre>
<br>
<h3 id="44-w2v_modelcloser_thankey1-key2">4.4 w2v_model.closer_than(key1, key2)</h3>
<p>更接近于key1的词向量(相比于key2)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取所有更接近 `key1` 的键，而不是 `key2` 。</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">closer_than</span><span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="s1">&#39;理想&#39;</span><span class="p">,</span> 
                      <span class="n">key2</span><span class="o">=</span><span class="s1">&#39;现实&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>['梦想', '妥协', '追梦', '愿望', '骨感']
</code></pre>
<br>
<h3 id="45-w2v_modelmost_similarpositive-negative">4.5 w2v_model.most_similar(positive, negative)</h3>
<p>找出与positive同方向，与negative反向相反的词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="s1">&#39;精彩&#39;</span><span class="p">,</span> <span class="s1">&#39;过瘾&#39;</span><span class="p">],</span>
                       <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;垃圾&#39;</span><span class="p">],</span>
                       <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><pre><code>[('激动人心', 0.6859163045883179),
 ('惊心动魄', 0.6767394542694092),
 ('带感', 0.6723690032958984),
 ('惊险刺激', 0.667783796787262),
 ('刺激', 0.6445038318634033),
 ('燃', 0.6429688930511475),
 ('爽快', 0.6287934184074402),
 ('带劲', 0.6254130005836487),
 ('爽', 0.624543309211731),
 ('酣畅淋漓', 0.6140543818473816)]
</code></pre>
<br>
<h3 id="46-类比king-manwomanqueen">4.6 类比king-man+woman~queen</h3>
<p><img loading="lazy" src="kingqueenformular.png" alt=""  />
</p>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。</p>
<p>这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p>gender_direction_1 = vector(man)-vector(woman)</p>
<p>gender_direction_2 = vector(king)-vector(queen)</p>
<p>那两个性别方向应该近似，假设这里将其gender_direction_1=gender_direction_2，则对于公式中任意一个词，都可以由等式中的其他三个词经过运算得到。例如</p>
<p>vector(queen) =  vector(king)-vector(man)+vector(woman)</p>
<p>这里构造了一个情绪的公式，计算如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 开心 - 难过 ~=  享受 - d</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;开心&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;难过&#39;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;享受&#39;</span><span class="p">)</span>

<span class="c1">#d = a-b+c</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="o">+</span><span class="n">c</span><span class="p">)</span>
</code></pre></div><pre><code>[('享受', 0.7833479046821594),
 ('开心', 0.6825607419013977),
 ('愉快', 0.6298696994781494),
 ('娱乐', 0.6215130090713501),
 ('感官', 0.6085000038146973),
 ('图个', 0.6052624583244324),
 ('图一乐', 0.6039161682128906),
 ('休闲', 0.60273677110672),
 ('视觉享受', 0.6006160378456116),
 ('轻松愉快', 0.5961319804191589)]
</code></pre>
<p>很遗憾，d没有运算出煎熬之类的词语，但好在都是形容词，而且是快乐居多的形容词，类别是对的，就是方向是反的。</p>
<br>
<h3 id="词向量总结">词向量总结</h3>
<p>需要注意的是经典的运算king-man+woman~queen来自glove模型，而不是本文使用的word2vec模型。两者相同点，glove与word2vec均为词嵌入embeddings技术。区别在于glove获取的词的全局语义空间，而word2vec一般是某个词前后n个词(例如前后5个词)范围内的语义。做概念四则运算，以后如可能，建议用glove。</p>
<p>此外，即时使用glove，尽量使用概念的词组均值向量。首先要训练数据要存在这些人类认知的线索。其次，认知概念往往不是由一个词决定的，可能需要相关的很多词。例如人类社会中的<code>雄雌(没有贬义，包含了男女在内的概念)</code>，</p>
<ul>
<li>雄性概念词有<code>他、男人、男孩、父亲、爷爷、爸爸、姥爷...</code></li>
<li>雌性概念词有<code>她、女人、女孩、母亲、奶奶、妈妈、姥姥...</code></li>
<li>国王概念词有<code>查理n世、乔治、路易...</code></li>
<li>女王概念词有<code>伊丽莎白n世、维多利亚女王、叶卡捷琳娜二世...</code></li>
</ul>
<p>或许改成概念向量四则运算，公式可能更容易成立。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext库 |  Python文本分析包更新</title>
      <link>https://textdata.cn/blog/cntext_simplification/</link>
      <pubDate>Fri, 01 Apr 2022 09:40:10 +0600</pubDate>
      
      <guid>/blog/cntext_simplification/</guid>
      <description>扩展词典、情感分析、可阅读性，内置9种情感词典，涵盖中英文</description>
      <content:encoded><![CDATA[<p><a href="https://github.com/hidadeng/cntext"><img loading="lazy" src="https://img.shields.io/badge/cntext-%e4%b8%ad%e6%96%87%e6%96%87%e6%9c%ac%e5%88%86%e6%9e%90%e5%ba%93-orange?style=for-the-badge&amp;logo=appveyor" alt=""  />
</a></p>
<p><a href="version1.2.md">旧版cntext入口</a></p>
<p>中文文本分析库，可对文本进行词频统计、词典扩充、情绪分析、相似度、可读性等</p>
<ul>
<li><a href="https://github.com/hidadeng/cntext">github地址</a> <code>https://github.com/hidadeng/cntext</code></li>
<li><a href="https://pypi.org/project/cntext/">pypi地址</a>  <code>https://pypi.org/project/cntext/</code></li>
<li><a href="https://ke.qq.com/course/482241?tuin=163164df">视频课-<strong>Python网络爬虫与文本数据分析</strong></a></li>
</ul>
<p>功能模块含</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> <strong>stats</strong>  文本统计指标
<ul>
<li><input checked="" disabled="" type="checkbox"> 词频统计</li>
<li><input checked="" disabled="" type="checkbox"> 可读性</li>
<li><input checked="" disabled="" type="checkbox"> 内置pkl词典</li>
<li><input checked="" disabled="" type="checkbox"> <strong>情感分析</strong></li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>dictionary</strong> 构建词表(典)
<ul>
<li><input checked="" disabled="" type="checkbox"> Sopmi 互信息扩充词典法</li>
<li><input checked="" disabled="" type="checkbox"> W2Vmodels 词向量扩充词典法</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>similarity</strong>   文本相似度
<ul>
<li><input checked="" disabled="" type="checkbox"> cos相似度</li>
<li><input checked="" disabled="" type="checkbox"> jaccard相似度</li>
<li><input checked="" disabled="" type="checkbox"> 编辑距离相似度</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>mind.py</strong> 计算文本中的认知方向（态度、偏见）</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip install cntext
</code></pre></div><br>
<h2 id="quickstart">QuickStart</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">help</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="nx">Help</span> <span class="nx">on</span> <span class="kn">package</span> <span class="nx">cntext</span><span class="p">:</span>

<span class="nx">NAME</span>
    <span class="nx">cntext</span>

<span class="nx">PACKAGE</span> <span class="nx">CONTENTS</span>
    <span class="nx">mind</span>
    <span class="nx">dictionary</span>
    <span class="nx">similarity</span>
    <span class="nx">stats</span>
</code></pre></div><br>
<h2 id="一stats">一、stats</h2>
<p>目前stats内置的函数有</p>
<ul>
<li><strong>readability</strong>  文本可读性</li>
<li><strong>term_freq</strong> 词频统计函数</li>
<li><strong>dict_pkl_list</strong>  获取cntext内置词典列表(pkl格式)</li>
<li><strong>load_pkl_dict</strong> 导入pkl词典文件</li>
<li><strong>sentiment</strong> 情感分析</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="11--readability">1.1  readability</h3>
<p>文本可读性，指标越大，文章复杂度越高，可读性越差。</p>
<p>readability(text, lang=&lsquo;chinese&rsquo;)</p>
<ul>
<li>text: 文本字符串数据</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<p>**中文可读性 ** 算法参考自</p>
<blockquote>
<p>徐巍,姚振晔,陈冬华.中文年报可读性：衡量与检验[J].会计研究,2021(03):28-44.</p>
<ul>
<li>readability1 &mdash;每个分句中的平均字数</li>
<li>readability2  &mdash;每个句子中副词和连词所占的比例</li>
<li>readability3  &mdash;参考Fog Index， readability3=(readability1+readability2)×0.5</li>
</ul>
</blockquote>
<p>​</p>
<p>以上三个指标越大，都说明文本的复杂程度越高，可读性越差。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>


<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 28.0,
 &#39;readability2&#39;: 0.15789473684210525,
 &#39;readability3&#39;: 14.078947368421053}
</code></pre></div><br>
<p>句子中的符号变更会影响结果</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">readability</span><span class="p">(</span><span class="n">text2</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;readability1&#39;: 27.0,
 &#39;readability2&#39;: 0.16666666666666666,
 &#39;readability3&#39;: 13.583333333333334}
</code></pre></div><p><br><br></p>
<h3 id="12--term_freq">1.2  term_freq</h3>
<p>词频统计函数，返回Counter类型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;如何看待一网文作者被黑客大佬盗号改文，因万分惭愧而停更。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">term_freq</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Counter({&#39;看待&#39;: 1,
         &#39;网文&#39;: 1,
         &#39;作者&#39;: 1,
         &#39;黑客&#39;: 1,
         &#39;大佬&#39;: 1,
         &#39;盗号&#39;: 1,
         &#39;改文因&#39;: 1,
         &#39;万分&#39;: 1,
         &#39;惭愧&#39;: 1,
         &#39;停&#39;: 1})
</code></pre></div><br>
<h3 id="13-dict_pkl_list">1.3 dict_pkl_list</h3>
<p>获取cntext内置词典列表(pkl格式)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取cntext内置词典列表(pkl格式)</span>
<span class="n">ct</span><span class="o">.</span><span class="n">dict_pkl_list</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;DUTIR.pkl&#39;,
 &#39;HOWNET.pkl&#39;,
 &#39;sentiws.pkl&#39;,
 &#39;ChineseFinancialFormalUnformalSentiment.pkl&#39;,
 &#39;ANEW.pkl&#39;,
 &#39;LSD2015.pkl&#39;,
 &#39;NRC.pkl&#39;,
 &#39;geninqposneg.pkl&#39;,
 &#39;HuLiu.pkl&#39;,
 &#39;AFINN.pkl&#39;,
 &#39;ADV_CONJ.pkl&#39;,
 &#39;LoughranMcDonald.pkl&#39;,
 &#39;STOPWORDS.pkl&#39;]
</code></pre></div><p>词典对应关系, 部分情感词典资料整理自 <a href="https://github.com/quanteda/quanteda.sentiment">quanteda.sentiment</a></p>
<table>
<thead>
<tr>
<th>pkl文件</th>
<th>词典</th>
<th>语言</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>DUTIR.pkl</td>
<td>大连理工大学情感本体库</td>
<td>中文</td>
<td>七大类情绪，<code>哀, 好, 惊, 惧, 乐, 怒, 恶</code></td>
</tr>
<tr>
<td>HOWNET.pkl</td>
<td>知网Hownet词典</td>
<td>中文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>sentiws.pkl</td>
<td>SentimentWortschatz (SentiWS)</td>
<td>英文</td>
<td>正面词、负面词；<br>效价</td>
</tr>
<tr>
<td>ChineseFinancialFormalUnformalSentiment.pkl</td>
<td>金融领域正式、非正式；积极消极</td>
<td>中文</td>
<td>formal-pos、<br>formal-neg；<br>unformal-pos、<br>unformal-neg</td>
</tr>
<tr>
<td>ANEW.pkl</td>
<td>英语单词的情感规范Affective Norms for English Words (ANEW)</td>
<td>英文</td>
<td>词语效价信息</td>
</tr>
<tr>
<td>LSD2015.pkl</td>
<td>Lexicoder Sentiment Dictionary (2015)</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>NRC.pkl</td>
<td>NRC Word-Emotion Association Lexicon</td>
<td>英文</td>
<td>细粒度情绪词；</td>
</tr>
<tr>
<td>geninqposneg.pkl</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>HuLiu.pkl</td>
<td>Hu&amp;Liu (2004)正、负情感词典</td>
<td>英文</td>
<td>正面词、负面词</td>
</tr>
<tr>
<td>AFINN.pkl</td>
<td>尼尔森 (2011) 的“新 ANEW”效价词表</td>
<td>英文</td>
<td>情感效价信息valence</td>
</tr>
<tr>
<td>LoughranMcDonald.pkl</td>
<td>会计金融LM词典</td>
<td>英文</td>
<td>金融领域正、负面情感词</td>
</tr>
<tr>
<td>ADV_CONJ.pkl</td>
<td>副词连词</td>
<td>中文</td>
<td></td>
</tr>
<tr>
<td>STOPWORDS.pkl</td>
<td></td>
<td>中、英</td>
<td>停用词</td>
</tr>
</tbody>
</table>
<h3 id="注意">注意:</h3>
<ul>
<li>
<p>如果用户情绪分析时使用DUTIR词典发表论文，请在论文中添加诸如“使用了大连理工大学信息检索研究室的情感词汇本体” 字样加以声明。参考文献中加入引文“徐琳宏,林鸿飞,潘宇,等.情感词汇本体的构造[J]. 情报学报, 2008, 27(2): 180-185.”</p>
</li>
<li>
<p>如果大家有制作的词典，可以上传至百度网盘，并在issue中留下词典的网盘链接。如词典需要使用声明，可连同文献出处一起issue</p>
</li>
</ul>
<br>
<h3 id="14-load_pkl_dict">1.4 load_pkl_dict</h3>
<p>导入pkl词典文件，返回字典样式数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入pkl词典文件,</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;DUTIR&#39;: {&#39;哀&#39;: [&#39;怀想&#39;, &#39;治丝而棼&#39;, ...],
           &#39;好&#39;: [&#39;进贤黜奸&#39;, &#39;清醇&#39;, &#39;放达&#39;, ...], 
           &#39;惊&#39;: [&#39;惊奇不已&#39;, &#39;魂惊魄惕&#39;, &#39;海外奇谈&#39;,...],
           &#39;惧&#39;: [&#39;忸忸怩怩&#39;, &#39;谈虎色变&#39;, &#39;手忙脚乱&#39;, &#39;刿目怵心&#39;,...],
           &#39;乐&#39;: [&#39;百龄眉寿&#39;, &#39;娱心&#39;, &#39;如意&#39;, &#39;喜糖&#39;,...],
           &#39;怒&#39;: [&#39;饮恨吞声&#39;, &#39;扬眉瞬目&#39;,...],
           &#39;恶&#39;: [&#39;出逃&#39;, &#39;鱼肉百姓&#39;, &#39;移天易日&#39;,]
           }
</code></pre></div><br>
<h3 id="15-sentiment">1.5 sentiment</h3>
<p>sentiment(text, diction, lang=&lsquo;chinese&rsquo;)
使用diy词典进行情感分析，计算各个情绪词出现次数; 未考虑强度副词、否定词对情感的复杂影响，</p>
<ul>
<li>text:  待分析中文文本</li>
<li>diction:  情感词字典；</li>
<li>lang: 语言类型，&ldquo;chinese&quot;或&quot;english&rdquo;，默认&quot;chinese&quot;</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>

<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
             <span class="n">diction</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;DUTIR.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;DUTIR&#39;</span><span class="p">],</span>
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;哀_num&#39;: 0,
 &#39;好_num&#39;: 0,
 &#39;惊_num&#39;: 0,
 &#39;惧_num&#39;: 0,
 &#39;乐_num&#39;: 2,
 &#39;怒_num&#39;: 0,
 &#39;恶_num&#39;: 0,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p>如果不适用pkl词典，可以自定义自己的词典，例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">diction</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;高兴&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;分享&#39;</span><span class="p">],</span>
           <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;难过&#39;</span><span class="p">,</span> <span class="s1">&#39;悲伤&#39;</span><span class="p">],</span>
           <span class="s1">&#39;adv&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;很&#39;</span><span class="p">,</span> <span class="s1">&#39;特别&#39;</span><span class="p">]}</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;我今天得奖了，很高兴，我要将快乐分享大家。&#39;</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sentiment</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> 
             <span class="n">diction</span><span class="o">=</span><span class="n">diction</span><span class="p">,</span> 
             <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;pos_num&#39;: 3,
 &#39;neg_num&#39;: 0,
 &#39;adv_num&#39;: 1,
 &#39;stopword_num&#39;: 8,
 &#39;word_num&#39;: 14,
 &#39;sentence_num&#39;: 1}
</code></pre></div><p><br><br></p>
<h2 id="二dictionary">二、dictionary</h2>
<p>本模块用于构建词表(典),含</p>
<ul>
<li>SoPmi 共现法扩充词表(典)</li>
<li>W2VModels 词向量word2vec扩充词表(典)</li>
</ul>
<h3 id="21-sopmi-共现法">2.1 SoPmi 共现法</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">sopmier</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">SoPmi</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span>
                   <span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_corpus.txt&#39;</span><span class="p">,</span>  <span class="c1">#原始数据，您的语料</span>
                   <span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/sopmi_seed_words.txt&#39;</span><span class="p">,</span> <span class="c1">#人工标注的初始种子词</span>
                   <span class="p">)</span>   

<span class="n">sopmier</span><span class="o">.</span><span class="n">sopmi</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   Corpus ...
Step 2/4:...Collect co-occurrency information ...
Step 3/4:...Calculate   mutual information ...
Step 4/4:...Save    candidate words ...
Finish! used 44.49 s
</code></pre></div><br>
<h3 id="22-w2vmodels-词向量">2.2 W2VModels 词向量</h3>
<p><strong>特别要注意代码需要设定lang语言参数</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">#初始化模型,需要设置lang参数。</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">W2VModels</span><span class="p">(</span><span class="n">cwd</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span> 
                     <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>  <span class="c1">#语料数据 w2v_corpus.txt</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_corpus.txt&#39;</span><span class="p">)</span>


<span class="c1">#根据种子词，筛选出没类词最相近的前100个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/integrity.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/innovation.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/quality.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/respect.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">seedword_txt_file</span><span class="o">=</span><span class="s1">&#39;data/w2v_seeds/teamwork.txt&#39;</span><span class="p">,</span> 
           <span class="n">topn</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Step 1/4:...Preprocess   corpus ...
Step 2/4:...Train  word2vec model
            used   174 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s
Step 3/4:...Prepare similar candidates for each seed word in the word2vec model...
Step 4/4 Finish! Used 187 s

</code></pre></div><br>
<h3 id="需要注意">需要注意</h3>
<p>训练出的w2v模型可以后续中使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">w2v</span><span class="o">.</span><span class="n">model路径</span><span class="p">)</span>
<span class="c1">#找出word的词向量</span>
<span class="c1">#w2v_model.get_vector(word)</span>
<span class="c1">#更多w2_model方法查看</span>
<span class="c1">#help(w2_model)</span>
</code></pre></div><p>例如本代码，运行生成的结果路径<code>output/w2v_candi_words/w2v.model</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;output/w2v_candi_words/w2v.model&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;technology&#39;, 0.689210832118988),
 (&#39;infrastructure&#39;, 0.669672966003418),
 (&#39;resources&#39;, 0.6695448160171509),
 (&#39;talent&#39;, 0.6627111434936523),
 (&#39;execution&#39;, 0.6549549102783203),
 (&#39;marketing&#39;, 0.6533523797988892),
 (&#39;merchandising&#39;, 0.6504817008972168),
 (&#39;diversification&#39;, 0.6479553580284119),
 (&#39;expertise&#39;, 0.6446896195411682),
 (&#39;digital&#39;, 0.6326863765716553)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#获取词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;innovation&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-0.45616838, -0.7799563 ,  0.56367606, -0.8570078 ,  0.600359  ,
       -0.6588043 ,  0.31116748, -0.11956959, -0.47599426,  0.21840936,
       -0.02268819,  0.1832016 ,  0.24452794,  0.01084935, -1.4213187 ,
        0.22840202,  0.46387577,  1.198386  , -0.621511  , -0.51598716,
        0.13352732,  0.04140598, -0.23470387,  0.6402956 ,  0.20394802,
        0.10799981,  0.24908689, -1.0117126 , -2.3168423 , -0.0402851 ,
        1.6886286 ,  0.5357047 ,  0.22932841, -0.6094084 ,  0.4515793 ,
       -0.5900931 ,  1.8684244 , -0.21056202,  0.29313338, -0.221067  ,
       -0.9535679 ,  0.07325   , -0.15823542,  1.1477109 ,  0.6716076 ,
       -1.0096023 ,  0.10605699,  1.4148282 ,  0.24576302,  0.5740349 ,
        0.19984631,  0.53964925,  0.41962907,  0.41497853, -1.0322098 ,
        0.01090925,  0.54345983,  0.806317  ,  0.31737605, -0.7965337 ,
        0.9282971 , -0.8775608 , -0.26852605, -0.06743863,  0.42815775,
       -0.11774074, -0.17956367,  0.88813037, -0.46279573, -1.0841943 ,
       -0.06798118,  0.4493006 ,  0.71962464, -0.02876493,  1.0282255 ,
       -1.1993176 , -0.38734904, -0.15875885, -0.81085825, -0.07678922,
       -0.16753489,  0.14065655, -1.8609751 ,  0.03587054,  1.2792674 ,
        1.2732009 , -0.74120265, -0.98000383,  0.4521185 , -0.26387128,
        0.37045383,  0.3680011 ,  0.7197629 , -0.3570571 ,  0.8016917 ,
        0.39243212, -0.5027844 , -1.2106236 ,  0.6412354 , -0.878307  ],
      dtype=float32)
</code></pre></div><p><br><br></p>
<h2 id="23-co_occurrence_matrix">2.3 co_occurrence_matrix</h2>
<p>词共现矩阵</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;I go to school every day by bus .&#34;</span><span class="p">,</span>
         <span class="s2">&#34;i go to theatre every night by bus&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence1.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;编程很好玩&#34;</span><span class="p">,</span>
             <span class="s2">&#34;Python是最好学的编程&#34;</span><span class="p">]</span>

<span class="n">ct</span><span class="o">.</span><span class="n">co_occurrence_matrix</span><span class="p">(</span><span class="n">documents2</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="/Users/thunderhit/Desktop/Coding/Python/diyPython/cntext/img/co_occurrence2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三similarity">三、similarity</h2>
<p>四种相似度计算函数</p>
<ul>
<li>cosine_sim(text1, text2)  cos余弦相似</li>
<li>jaccard_sim(text1, text2)     jaccard相似</li>
<li>minedit_sim(text1, text2)  最小编辑距离相似度；</li>
<li>simple_sim(text1, text2) 更改变动算法</li>
</ul>
<p>算法实现参考自 <code>Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. Lazy prices. No. w25084. National Bureau of Economic Research, 2018.</code></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span> 


<span class="n">text1</span> <span class="o">=</span> <span class="s1">&#39;编程真好玩编程真好玩&#39;</span>
<span class="n">text2</span> <span class="o">=</span> <span class="s1">&#39;游戏真好玩编程真好玩啊&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">minedit_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">simple_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.82
0.67
2.00
0.87
</code></pre></div><p><br><br></p>
<h2 id="四text2mind">四、Text2Mind</h2>
<p>词嵌入中蕴含着人类的认知信息，以往的词嵌入大多是比较一个概念中两组反义词与某对象的距离计算认知信息。</p>
<p>- <strong>多个对象在某概念的远近</strong>，职业与性别，某个职业是否存在亲近男性，而排斥女性</p>
<p>- 多个对象在某<strong>概念的分量(fen，一声)的多少</strong>， 人类语言中留存着对不同动物体积的认知记忆，如小鼠大象。动物词在词向量空间中是否能留存着这种大小的记忆</p>
<p>这两种认知分别可以用向量距离、向量语义投影计算得来。</p>
<ul>
<li>tm.sematic_distance(words, c_words1, c_words2)  向量距离</li>
<li>tm.sematic_projection(words, c_words1, c_words2)  向量语义投影</li>
</ul>
<h3 id="41-tmsematic_distancewords-c_words1-c_words2">4.1 tm.sematic_distance(words, c_words1, c_words2)</h3>
<p>分别计算words与c_words1、c_words2语义距离，返回距离差值。</p>
<p>例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">male_concept = [&#39;male&#39;, &#39;man&#39;, &#39;he&#39;, &#39;him&#39;]
female_concept = [&#39;female&#39;, &#39;woman&#39;, &#39;she&#39;, &#39;her&#39;]
software_engineer_concept  = [&#39;engineer&#39;,  &#39;programming&#39;,  &#39;software&#39;]
d1 = distance(male_concept,  software_engineer_concept)
d2 = distance(female_concept,  software_engineer_concept)
</code></pre></div><p>如果d1-d2&lt;0，说明在语义空间中，software_engineer_concept更接近male_concept，更远离female_concept。</p>
<p>换言之，在该语料中，人们对软件工程师这一类工作，对女性存在刻板印象(偏见)。</p>
<p><strong>下载glove_w2v.6B.100d.txt</strong>链接: <a href="https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw">https://pan.baidu.com/s/1MMfQ7M0YCzL9Klp4zrlHBw</a> 提取码: 72l0</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#Note: this is a word2vec format model</span>
<span class="n">tm</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Text2Mind</span><span class="p">(</span><span class="n">w2v_model_path</span><span class="o">=</span><span class="s1">&#39;glove_w2v.6B.100d.txt&#39;</span><span class="p">)</span>

<span class="n">engineer</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;program&#39;</span><span class="p">,</span> <span class="s1">&#39;software&#39;</span><span class="p">,</span> <span class="s1">&#39;computer&#39;</span><span class="p">]</span>
<span class="n">mans</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;man&#34;</span><span class="p">,</span> <span class="s2">&#34;he&#34;</span><span class="p">,</span> <span class="s2">&#34;him&#34;</span><span class="p">]</span>
<span class="n">womans</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;woman&#34;</span><span class="p">,</span> <span class="s2">&#34;she&#34;</span><span class="p">,</span> <span class="s2">&#34;her&#34;</span><span class="p">]</span>

<span class="c1">#在语义空间中，工程师更接近于男人，而不是女人。</span>
<span class="c1">#in semantic space, engineer is closer to man, other than woman.</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                    <span class="n">c_words1</span><span class="o">=</span><span class="n">mans</span><span class="p">,</span> 
                    <span class="n">c_words2</span><span class="o">=</span><span class="n">womans</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">-0.38
</code></pre></div><br>
<h3 id="42-tmsematic_projectionwords-c_words1-c_words2">4.2 tm.sematic_projection(words, c_words1, c_words2)</h3>
<p><strong>语义投影</strong>，根据两组反义词c_words1, c_words2构建一个概念(认知)向量, words中的每个词向量在概念向量中投影，即可得到认知信息。</p>
<p>分值越大，word越位于c_words2一侧。</p>
<p>下图是语义投影示例图，本文算法和图片均来自 &ldquo;Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, pp.1-13.&rdquo;</p>
<p><img loading="lazy" src="img/Nature_Semantic_projection_recovering_human_knowledge_of.png" alt=""  />
</p>
<p>例如，人类的语言中，存在尺寸、性别、年龄、政治、速度、财富等不同的概念。每个概念可以由两组反义词确定概念的向量方向。</p>
<p>以尺寸为例，动物在人类认知中可能存在体积尺寸大小差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">animals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;horse&#39;</span><span class="p">,</span>  <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;whale&#39;</span><span class="p">]</span>
<span class="n">smalls</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;small&#34;</span><span class="p">,</span> <span class="s2">&#34;little&#34;</span><span class="p">,</span> <span class="s2">&#34;tiny&#34;</span><span class="p">]</span>
<span class="n">bigs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;large&#34;</span><span class="p">,</span> <span class="s2">&#34;big&#34;</span><span class="p">,</span> <span class="s2">&#34;huge&#34;</span><span class="p">]</span>

<span class="c1"># In size conception, mouse is smallest, horse is biggest.</span>
<span class="c1"># 在大小概念上，老鼠最小，马是最大的。</span>
<span class="n">tm</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">animals</span><span class="p">,</span> 
                      <span class="n">c_words1</span><span class="o">=</span><span class="n">smalls</span><span class="p">,</span> 
                      <span class="n">c_words2</span><span class="o">=</span><span class="n">bigs</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;mouse&#39;, -1.68),
 (&#39;cat&#39;, -0.92),
 (&#39;pig&#39;, -0.46),
 (&#39;whale&#39;, -0.24),
 (&#39;horse&#39;, 0.4)]
</code></pre></div><p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Top2Vec|主题建模和语义搜索库</title>
      <link>https://textdata.cn/blog/top2vec_tutorial/</link>
      <pubDate>Mon, 13 Dec 2021 10:43:10 +0600</pubDate>
      
      <guid>/blog/top2vec_tutorial/</guid>
      <description>Python主题建模和语义搜索库</description>
      <content:encoded><![CDATA[<p>Top2Vec 是一种用于主题建模和语义搜索的算法。**我个人从理解代码和使用代码难度来看， 对于Python小白，BERTopic更适合直接用预训练词向量，而Top2Vec更适合对小规模数据训练词向量后做主题建模。**它自动检测文本中存在的主题并生成联合嵌入的主题、文档和词向量。训练 Top2Vec 模型后，您可以：</p>
<ul>
<li>获取检测到的主题数。</li>
<li>获取话题。</li>
<li>获取主题大小。</li>
<li>获取分层主题。</li>
<li>按关键字搜索主题。</li>
<li>按主题搜索文档。</li>
<li>按关键字搜索文档。</li>
<li>找出相似的词。</li>
<li>查找类似的文档。</li>
<li>使用 RESTful-Top2Vec 公开模型</li>
<li>有关其工作原理的更多详细信息，请参阅论文。</li>
</ul>
<p><strong>亮点</strong></p>
<ul>
<li>自动查找主题数。</li>
<li>不需要停用词列表。</li>
<li>不需要词干/词形还原。</li>
<li>适用于短文本。</li>
<li>创建联合嵌入的主题、文档和词向量。</li>
<li>内置搜索功能。</li>
</ul>
<p><strong>它是如何工作的？</strong></p>
<p>该算法做出的假设是，许多语义相似的文档都表明了一个潜在的主题。</p>
<p>第一步是创建文档和词向量的联合嵌入。一旦文档和单词被嵌入到一个向量空间中，算法的目标就是找到密集的文档集群，然后确定哪些单词将这些文档吸引到一起。每个密集区域是一个主题，将文档吸引到密集区域的词就是主题词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">top2vec</span><span class="o">==</span><span class="mf">1.0.27</span>
</code></pre></div><h2 id="0-代码下载">0. 代码下载</h2>
<p><a href="top2vec_tutorial.zip">click to download code</a></p>
<p><br><br></p>
<h2 id="1-导入数据">1. 导入数据</h2>
<p>使用某灾难数据集，这里是存在标注的标签，但是我们假设不用label的，仅作为评判Top2vec运行效果的标准。<a href="cnews.csv">点击cnews.csv下载</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">top2vec</span> <span class="kn">import</span> <span class="n">Top2Vec</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">jieba</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;STOPWORDS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;STOPWORDS&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;cnews.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>时政    120
科技    106
时尚    106
财经    105
家居    103
教育     97
娱乐     96
体育     95
房产     87
游戏     85
Name: label, dtype: int64
</code></pre>
<p><br><br></p>
<h2 id="2-清洗数据">2. 清洗数据</h2>
<p>一般而言，作中文文本分析，需要把中文分词构造成类西方语言(空格间隔词语的文本)风格。在此期间，顺便将停用词剔除。其实在用top2vec时，不剔除停用词影响也不大。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleantext&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="3-训练模型">3. 训练模型</h2>
<p>Top2vec有一下四个常用参数</p>
<p><strong>Top2vec(documents, min_count, speed, workers)</strong></p>
<ul>
<li>documents: 文档列表</li>
<li>min_count: 词语最少出现次数。低于min_count的词不加入模型中</li>
<li>speed: 训练速度，参数默认&quot;learn&quot;
<ul>
<li>&ldquo;fast-learn&rdquo;  速度最快，训练效果最差</li>
<li>&ldquo;learn&rdquo;       速度，训练效果中等</li>
<li>&ldquo;deep-learn&rdquo;  速度最慢，训练效果最佳</li>
</ul>
</li>
<li>workers: 并行运行数，该值最大取值为电脑CPU的核数。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="n">Top2Vec</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;cleantext&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">(),</span> 
                <span class="n">min_count</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">speed</span><span class="o">=</span><span class="s2">&#34;deep-learn&#34;</span><span class="p">,</span>  
                <span class="n">workers</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>2021-12-14 20:21:10,318 - top2vec - INFO - Pre-processing documents for training
2021-12-14 20:21:10,871 - top2vec - INFO - Creating joint document/word embedding
2021-12-14 20:25:06,082 - top2vec - INFO - Creating lower dimension embedding of documents
2021-12-14 20:25:14,645 - top2vec - INFO - Finding dense areas of documents
2021-12-14 20:25:14,683 - top2vec - INFO - Finding topics
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 话题个数</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_num_topics</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>9
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 各话题数量</span>
<span class="n">topic_sizes</span><span class="p">,</span> <span class="n">topic_nums</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topic_sizes</span><span class="p">()</span>

<span class="p">{</span><span class="s2">&#34;topic_sizes&#34;</span><span class="p">:</span><span class="n">topic_sizes</span><span class="p">,</span> 
 <span class="s2">&#34;topic_ids&#34;</span><span class="p">:</span><span class="n">topic_nums</span><span class="p">}</span>
</code></pre></div><p>Run</p>
<pre><code>{'topic_sizes': array([361, 116, 107,  99,  97,  93,  82,  25,  20]),
 'topic_ids': array([0, 1, 2, 3, 4, 5, 6, 7, 8])}
</code></pre>
<p><br><br></p>
<h2 id="4-get_topics">4. get_topics</h2>
<p>用pyecharts词云图显示<strong>话题信息</strong>， 为了简化代码，将该功能封装为函数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gen_wordcloud</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    topic_words: 主题词列表
</span><span class="s2">    word_scores: 主题特征词的权重得分(词语表征主题的能力)
</span><span class="s2">    topic_id: 主题id
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
    <span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
    <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
    
    <span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">)]</span>

    <span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">88</span><span class="p">])</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
        <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;Topic_</span><span class="si">{topic_id}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">),</span> 
                                  <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">23</span><span class="p">)),</span>
        <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="n">display</span><span class="p">(</span><span class="n">wc</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">())</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_topics</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>

<span class="k">for</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">):</span>
    <span class="n">gen_wordcloud</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/vis1.png" alt=""  />

<img loading="lazy" src="img/vis2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="5-get_documents_topics">5. get_documents_topics</h2>
<p>get_documents_topics(doc_ids, num_topics=1)</p>
<ul>
<li>doc_ids: 待查询文档id列表</li>
<li>num_topics: 返回某文档可能归属话题的个数</li>
</ul>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查第一条文档的</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_documents_topics</span><span class="p">(</span><span class="n">doc_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(array([0]),
 array([0.1384481], dtype=float32),
 array([[&#39;政府&#39;, &#39;经济&#39;, &#39;政策&#39;, &#39;建设&#39;, &#39;中方&#39;, &#39;发展&#39;, &#39;促进&#39;, &#39;部门&#39;, &#39;留学&#39;, &#39;学生&#39;, &#39;会议&#39;,
         &#39;我要&#39;, &#39;事务&#39;, &#39;日电&#39;, &#39;房价&#39;, &#39;教育&#39;, &#39;国务院&#39;, &#39;温家宝&#39;, &#39;留学生&#39;, &#39;人数&#39;, &#39;移民&#39;,
         &#39;会见&#39;, &#39;推动&#39;, &#39;申请者&#39;, &#39;申请&#39;, &#39;官员&#39;, &#39;住房&#39;, &#39;房屋&#39;, &#39;加强&#39;, &#39;中国政府&#39;, &#39;购房&#39;,
         &#39;国家&#39;, &#39;支付&#39;, &#39;楼市&#39;, &#39;外交部&#39;, &#39;接收&#39;, &#39;两国&#39;, &#39;原则&#39;, &#39;各地&#39;, &#39;总理&#39;, &#39;战略&#39;,
         &#39;和平&#39;, &#39;框架&#39;, &#39;评论&#39;, &#39;有序&#39;, &#39;装修&#39;, &#39;中国&#39;, &#39;就业&#39;, &#39;友好&#39;, &#39;人力资源&#39;]],
       dtype=&#39;&lt;U9&#39;),
 array([[0.3623712 , 0.36037514, 0.35219163, 0.35109183, 0.3499857 ,
         0.34666985, 0.3426961 , 0.34161803, 0.34010434, 0.3382269 ,
         0.33710504, 0.336056  , 0.33598724, 0.33488944, 0.3303768 ,
         0.32483265, 0.324798  , 0.32201332, 0.3174801 , 0.3153757 ,
         0.3152491 , 0.31338856, 0.31334093, 0.31244045, 0.31202242,
         0.30908576, 0.3086405 , 0.30838227, 0.30605763, 0.3053521 ,
         0.30474398, 0.30268514, 0.30253592, 0.30242488, 0.30227807,
         0.3017046 , 0.30116442, 0.30062813, 0.2996228 , 0.29806197,
         0.2972776 , 0.29709277, 0.29706252, 0.29584888, 0.29578486,
         0.29524648, 0.2944737 , 0.2939484 , 0.29286712, 0.29246706]],
       dtype=float32))
</code></pre></div><p><br><br></p>
<h2 id="6-search_topics">6. search_topics</h2>
<p>根据关键词搜索话题，查某词是否属于某话题，属于该主题的概率
search_topics(keywords, num_topics, keywords_neg=None)</p>
<ul>
<li>keywords: 关键词列表</li>
<li>num_topics: 返回话题个数，按照语义相似度从高到低排序</li>
<li>keywords_neg: 反义词列表</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">gen_wordcloud2</span><span class="p">(</span><span class="n">query_word</span><span class="p">,</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_id</span><span class="p">,</span> <span class="n">topic_probability</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    query_word: 待查询词
</span><span class="s2">    topic_words: 主题词列表
</span><span class="s2">    word_scores: 主题特征词的权重得分(词语表征主题的能力)
</span><span class="s2">    topic_id: 主题id
</span><span class="s2">    topic_probability: 主题概率
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="kn">import</span> <span class="nn">pyecharts.options</span> <span class="k">as</span> <span class="nn">opts</span>
    <span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">WordCloud</span>
    <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
    
    <span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">)]</span>

    <span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">()</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">series_name</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">data_pair</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">word_size_range</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">88</span><span class="p">])</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;Word</span><span class="si">{query_word}</span><span class="se">\n</span><span class="s2">Topic_</span><span class="si">{topic_id}</span><span class="se">\n</span><span class="s2">Probability:</span><span class="si">{probability:.2f}</span><span class="s2">&#34;&#34;&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">query_word</span><span class="o">=</span><span class="n">query_word</span><span class="p">,</span>
                                                              <span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> 
                                                              <span class="n">probability</span><span class="o">=</span><span class="n">topic_probability</span><span class="p">)</span>
    <span class="n">wc</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span>
        <span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> 
                                  <span class="n">title_textstyle_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TextStyleOpts</span><span class="p">(</span><span class="n">font_size</span><span class="o">=</span><span class="mi">18</span><span class="p">)),</span>
        <span class="n">tooltip_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TooltipOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

    <span class="n">display</span><span class="p">(</span><span class="n">wc</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">())</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">query_word</span> <span class="o">=</span> <span class="s2">&#34;电影&#34;</span>
<span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_topics</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="n">query_word</span><span class="p">],</span> <span class="n">num_topics</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_score</span><span class="p">,</span> <span class="n">topic_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">topic_wordss</span><span class="p">,</span> <span class="n">word_scoress</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">topic_score</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">:</span>
        <span class="n">gen_wordcloud2</span><span class="p">(</span><span class="n">query_word</span><span class="o">=</span><span class="n">query_word</span><span class="p">,</span> 
                       <span class="n">topic_words</span><span class="o">=</span><span class="n">topic_words</span><span class="p">,</span> 
                       <span class="n">word_scores</span><span class="o">=</span><span class="n">word_scores</span><span class="p">,</span> 
                       <span class="n">topic_id</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> <span class="n">topic_probability</span><span class="o">=</span><span class="n">topic_score</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/vis5.png" alt=""  />
</p>
<br>
<h2 id="7-query_topics">7. query_topics</h2>
<p>根据一段文本寻找最符合该文本的话题
query_topics(query, num_topics)</p>
<ul>
<li>query: 查询文本，注意是用空格间隔词语的文本</li>
<li>num_topics: 返回的话题数</li>
</ul>
<p>返回话题特征词列表， 话题特征词权重， 话题概率， 话题id</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">querytext</span> <span class="o">=</span> <span class="s1">&#39;刘晓庆 55 岁 近日 颁奖礼 刘晓庆 一袭 宝蓝色 超低 胸 V 领 长裙 亮相 轻薄 蕾丝 奢华 皮草 艳丽 色彩 翠绿&#39;</span>
<span class="n">topic_words</span><span class="p">,</span> <span class="n">word_scores</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">,</span> <span class="n">topic_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">query_topics</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">querytext</span><span class="p">,</span> 
                                                                       <span class="n">num_topics</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;可能归属的话题有: &#39;</span><span class="p">,</span> <span class="n">topic_ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;归属于该话题的概率&#39;</span><span class="p">,</span> <span class="n">topic_scores</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">可能归属的话题有:  [1 4]
归属于该话题的概率 [0.32036728 0.1276904 ]
</code></pre></div><br>
<h2 id="8-search_documents_by_keywords">8. search_documents_by_keywords</h2>
<p>根据关键词，筛选文档</p>
<p>search_documents_by_keywords(keywords,
num_docs,
keywords_neg=None,
return_documents=True)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#文档， 语义相关性， 文档id</span>
<span class="n">docs</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">doc_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_keywords</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;搭配&#39;</span><span class="p">],</span> 
                                                         <span class="n">num_docs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                                                         <span class="n">keywords_neg</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                                                         <span class="n">return_documents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">doc_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;----------&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Document: 106, Semantic similarity: 0.4943176805973053
白色 短裙 百变 休闲 感 要点 一定 敞开 衬衫 配合 牛仔裤 休闲 感 短裤 衬衫 短 敞开 显得 好好 穿 裤子 搭配 七分裤 遮住 臀部 长度 关键 尽量 选择 艳丽 颜色 带 出 青春 感 NO.3     白色 短裙 tips :   白色 短裙 + 粉色 上衣 这是 一套 减龄 百分百 搭配 白色 短裙 本来 清纯 粉色 上衣 搭配 更加 具有活力 tips :   白色 短裙 + 抹胸 + 外套 想要 性感 一点 就加 一件 抹胸 抹胸 胸前 构造 曲线 完美 再加 外套 保暖 得体 看似 简单 一款 搭配 其实 暗地里 偷偷地 修饰 身材
----------

Document: 870, Semantic similarity: 0.4483542740345001
组图 看达人 演绎 豹纹 军装 风 导语 懂得 潮流 总是 知道 适合 今冬 流行 亮点 太 军装 豹纹 类似 民族风情 想要 知道 搭配 快 看看 时尚 达 穿 军绿色 宽松 款 大衣 不失 俏皮 味道 高腰 设计 短裙 有效 提升 腰线 衬托出 修长 美腿 豹纹 今年 冬季 抢眼 搭配 元素 加上 驼色 针织衫 灰色 围巾 暖 棕色 手 挎包 整体 色调 统一 迷人 棕色 蓝色 结合能 眼前一亮 简洁 款式 依然 突显 独特 品味 宽松 针织 外套 衬托出 优美 身形 搭配 同样 沉闷 黑色 包包 性感 丝袜 装扮 依然 透露 出 迷人 气息 立领 衬衫 加上 深黄 高腰 裤 摩登 感 十足 随意 披上 外套 更显 慵懒 个性 法式 风情
----------

Document: 450, Semantic similarity: 0.4471719563007355
街 拍 爱 招摇过市 毛茸茸 ( 组图 ) 导语 皮草 每个 冬天 可能 丢弃 每个 需要 温暖 早些 相比 人造皮 草比 真皮 草 风头 更劲 时尚 环保 大牌 秀 场上 超模 一个个 穿着 人造皮 草 “ 招摇过市 ” 之后 街头 潮人 没有 理由 拒绝 外形 酷酷 这件 气场 皮草 单品 配合默契 摇滚 风 配饰 搭配 黑色 皮草 长 背心 更显 利落 酷酷 黑色 皮草 搭配 蓝色 衬衣 不同 感觉 加上 下半身 底裤 时髦 包包 颜色 提亮 整身 装扮 抹胸 式 皮草 特点 高贵典雅 适合 搭配 连衣裙 装饰 增添 时尚 美感 复古 圆点 连衣裙 搭配 宽松 棕色 皮草 衣 名媛 感觉 典雅 淑女 短款 黑色 皮草 搭配 贴身 仔裤 搭配 长靴 潇洒 帅气 茸茸 帽子 增添 不少 甜美 感
----------
</code></pre></div><p><br><br></p>
<h2 id="9-search_documents_by_topic">9. search_documents_by_topic</h2>
<p>根据指定的topic_id， 显示该主题前num_docs个文档，显示的文档是根据概率从高到低降序显示</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#查看topic4的前5条文档</span>
<span class="n">topic_id</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_topic</span><span class="p">(</span><span class="n">topic_num</span><span class="o">=</span><span class="n">topic_id</span><span class="p">,</span> <span class="n">num_docs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Document: 905, Semantic similarity: 0.4941929578781128
-----------
现场 阿联 第三节 未 亮相   奇才 连续 3 记 重扣 逆转 比分 新浪 体育讯 北京 时间 4 2 奇才 主场 迎战 联盟 垫底 骑士 奇才 新秀 后卫 约翰 - 沃尔因 一场 对阵 热火 比赛 斗殴 禁赛 一场 伤愈 复出 安德雷 - 布 莱切 回到 首发 阵容 奇才 本赛季 首场 客场 胜利 面对 骑士 取得 当时 奇才 115 - 110 击败 对手 上半场 骑士 命中率 达到 53.8% 奇才 仅 44% 骑士 希克 森 ( 16 ) 塞 申斯 ( 12 ) 得分 双 奇才 布 莱切 ( 22 ) 麦基 ( 12 ) 埃文斯 4 投 0 仅 抢下 1 篮板 易建联 上场 7 08 2 投 0 抢下 3 篮板 异地 再战 埃文斯 终于 得分 抢断 吉后 犯规 两罚 命中 吉 随即 突破 上篮 命中 回敬 球 杰 弗斯 三分 不进 吉 抢下 篮板 上篮 再进 一球 布 莱切 中投 命中 霍林斯 篮下 出手 不进 布 莱切 抢下 篮板 此后 克劳福德 连续 突破 先是 助攻 麦基 扣篮 盖掉 戴维斯 投篮 助攻 布 莱切 扣篮 戴维斯 运球 被断 布 莱切 传给 杰 弗斯 一记 暴扣 奇才 连续 3 次 颇具 气势 扣篮 连得 6 反超 骑士 1 骑士 请求 暂停 回到 比赛 吉 上篮 不进 麦基 低位 单打 不进 布 莱切 抢下 篮板 3 得手 骑士 进攻 24 违例 奇才 越战越勇 克劳福德 身体 失去 重心 情况 仍然 将球 投进 一个打 3 骑士 连续 吉 挺身而出 三分 命中 个人 已经 得到 10 此人 本赛季 短暂 效力 奇才 麦基 中投 不进 布 莱切 抢下 前场 篮板 将球 放进 麦基 防守 领到 犯规 希克 森两罚 命中 麦基 强攻 造成 霍林斯 犯规 两罚 一中 戴维斯 三分 不进 克劳福德 跑 投 命中 戴维斯 突分 霍林斯 暴扣 命中 回过头来 克劳福德 助攻 麦基扣 劲 爆 哈兰 高迪 中投 不进 克劳福德 投篮 偏出 布 莱切 3 报价 连续 抢 篮板 进攻 最后 犯规 两罚 一中 现在 已经 得到 32 18 篮板 布 莱切 底线 遭 报价 分球 埃文斯 三分 命中 霍林斯 篮下 重扣 奇才 请求 暂停 布 莱切 继续 得分 吉布森 上篮 命中 克劳福德 中投 不进 抢下 篮板 杰 弗斯 运球 突破 犯规 两罚 命中 易建联 节 没有 登场 第三节 比赛 结束 骑士 82 - 83 奇才 ( 草头 王 )
-----------

Document: 689, Semantic similarity: 0.4917592704296112
-----------
直击 康大 内线 一柱擎天   13 优势 到手 胜利在望 新浪 体育讯 北京 时间 4 5 ( 休斯敦 时间 4 4 ) 消息 NCAA   Final   4 总决赛 休斯敦 Reliant 球馆 举行 比赛 进入 最后 6 分钟 本场 表现 十分 亮眼 康涅狄格 内线 阿莱克斯 - 奥里 瓦基接 队友 直传 空切 篮下 扣篮 得分 打成 2 + 1 目前 已经 拿下 10 9 篮板 3 封盖 巴特勒 仍然 没 解决 进攻 端的 问题 下半场 23 投 仅仅 3 屡次 外线 空挡 出手 均 打铁 告终 仅仅 入账 8 目前 康大 已经 取得 13 优势 胜利在望 ( silencer )
-----------

Document: 425, Semantic similarity: 0.47035443782806396
-----------
今日 数据 趣谈 魔兽 悲情 似 张大帅   基德 焕发 第二 春 新浪 体育讯 北京 时间 4 17 NBA 季后赛 正式 开打 进行 4 场 比赛 以下 今日 比赛 诞生 有趣 数据 今日 首场 季后赛 芝加哥 公牛 第四节 剩 4 分钟 仍以 88 - 98 落后 接下来 打出 16 - 1 攻击 波 主场 一举 逆转 印第安纳 步行者 取胜 继 2004 之后 NBA 季后赛 舞台 再次出现 终场 前 4 分钟 落后 两位数 最终 翻盘 成功 案例 2004 5 9 西部 决赛 明尼苏达 森林狼 萨克拉门托 国王 比赛 森林狼 同样 终场 前 4 分钟 仍以 78 - 88 落后 接下来 打出 16 - 1 ( 惊人 相似 ) 最终 94 - 89 逆转 取胜 今天 公牛 逆转 步行者 比赛 德里克 - 罗斯 砍 39 罚球 21 投 19 2008 洛杉矶 湖人 对阵 犹他 爵士 一场 季后赛 科比 - 布莱恩特 创下 单场 罚球 23 投 21 季后赛 纪录 罗斯 位居 全场 三分球 9 次 出手 竟无一 命中 季后赛 历史 此前 两次 类似 案例 2008 奥兰多 魔术 对阵 多伦多 猛龙 一场 比赛 拉沙德 - 刘易斯 三分球 9 投 0 一次 熟知 1994 总决赛 第七场 约翰 - 斯塔克 斯 三分 线外 11 投 0 纽约 尼克斯 负于 休斯敦 火箭 冠军 擦肩而过 今天 亚特兰大 老鹰 客场 战胜 奥兰多 魔术 比赛 老鹰 五名 球员 得分 低于 13 — — 乔 - 约翰逊 ( 25 16 投 9 ) 贾马尔 - 克劳福德 ( 23 14 投 7 ) 艾尔 - 霍福德 ( 16 14 投 7 ) 约什 - 史密斯 ( 15 12 投 6 ) 科克 - 辛里奇 ( 13 10 投 6 ) 该队 过去 199 场 季后赛 尚属 首次 老鹰队 史上 一次 出现 这种 盛况 1966 4 14 131 - 127 战胜 洛杉矶 湖人 比赛 当时 书写 纪录 五人 里奇 - 古尔林 克里夫 - 哈根 泽尔莫 - 比蒂 比尔 - 布里奇斯 乔 - 考 德维尔 今天 负于 老鹰 比赛 德怀特 - 霍华德 ( 46 ) 贾 米尔 - 尼尔森 ( 27 ) 砍 73 队友 总共 仅 拿下 20 魔术 最终 93 - 103 负于 更为 均衡 对手 NBA 历史 8 支 球队 一场 季后赛 比赛 有过 两名 球员 联手 砍 全队 至少 75% 得分 1 队 取胜 追溯到 1950 4 9 当年 总决赛 第一场 比赛 乔治 - 麦肯 得到 37 吉姆 - 波 拉德 得到 14 率领 明尼阿波利斯 湖人 68 - 66 战胜 锡 拉丘兹 民族 ( 费城 76 前身 ) 7 队则 败北 得到 46 霍华德 抢下 19 篮板 常规 时间 取得 1975 4 19 布法罗 勇敢者 ( 洛杉矶 快船 前身 ) 战胜 华盛顿 子弹 ( 华盛顿 奇才 前身 ) 一场 季后赛 效力 勇敢者 鲍勃 - 麦卡 杜 同样 没有 加时赛 情况 砍 50 21 篮板 威尔特 - 张伯伦 一场 季后赛 常规 时间 砍 46 19 篮板 球队 却输 ( 事实上 张大帅 生涯 3 场 比赛 取得 数据 竟 败北 ) 刚 谢幕 本赛季 常规赛 杰森 - 基德 仅 两场 比赛 得分 达到 20 + 1 20 对阵 湖人 比赛 砍 赛季 最高 21 今天 达拉斯 小牛 主场 战胜 波特兰 开拓者 比赛 砍 24 命中 6 记 三分球 一场 季后赛 比赛 砍 20 + 得分 刷新 常规赛 创下 赛季 新高 NBA 历史 壮举 球员 如今 38 岁 基德 年龄 最大 成为 NBA 历史 一场 季后赛 比赛 单场 命中 6 记 三分球 年龄 最大 球员 此前 纪录 雷吉 - 米勒 2002 创下 当时 36 岁 今天 小牛 战胜 开拓者 比赛 德克 - 诺维茨基 第四节 13 次 罚球 出手 命中 追平 迈克尔 - 乔丹 纪录 1990 - 91 赛季 季后赛 一场 公牛 底特律 活塞 比赛 乔丹 单节 命中 13 次 罚球 率队 105 - 97 取胜 最终 公牛 获得 赛季 总冠军 今天 迈阿密 热火 战胜 费城 76 比赛 克里斯 - 波什 得到 25 12 篮板 勒布朗 - 詹姆斯 得到 21 14 篮板 他俩 队友 参加 首场 季后赛 前 一个 赛季 各为其主 接下来 赛季 并肩作战 季后赛 首场 比赛 砍 得分 20 + 篮板 10 + 组合 波什 詹姆斯 之前 无先例 ( 魑魅 )
-----------

Document: 155, Semantic similarity: 0.45704954862594604
-----------
现场 麦蒂 返场 销魂 跳投 两 连击   小拜 纳姆 单节 11 新浪 体育讯 北京 时间 4 6 华盛顿 奇才 主场 迎战 底特律 活塞 此前 球队 已经 客场 两连胜 若能 战胜 活塞 奇才 本赛季 首次 迎来 三连胜 异地 再战 埃文斯 中投 命中率 先 得分 拜纳姆 中投 不进 克劳福德 一人 带球 运 前场 对手 尚未 落位 情况 直接 出手 投篮 命中 这种 投篮 欠缺 考虑 根本 没有 战术 配合 全 个人 手感 遇到 防守 稍 一点 球队 沃尔 抢断 埃文斯 直接 暴扣 奇才 反超 4 活塞 请求 暂停 沃尔 报价 对手 拜纳姆 得到 机会 三分 出手 命中 布 莱切 上篮 得手 门罗 助攻 威尔 考克斯 扣篮 命中 埃文斯 三分 不进 拜纳姆 突破 上篮 命中 威尔 考克斯 拿布 莱切 没有 办法 运球 进攻 威尔 考克斯 只能 伸直 手臂 不断 滑步 被布 莱切 强投 命中 活塞 拜纳姆 发力 突破 上篮 命中 布 莱切 中投 不进 拜纳姆 卷土重来 造成 沃尔 犯规 两罚 命中 个人 已经 得到 11 门罗 抢断 布 莱切 普林斯 上篮 命中 活塞 反超 3 麦基 传球 失误 奇才 请求 暂停 威尔 考克斯 篮下 强打 奇才 反击 埃文斯 上篮 命中 普林斯 糟糕 状态 继续 中投 偏出 布 莱切 运球 单打 活塞 两名 内线 屡试不爽 造成 门罗 犯规 两罚 命中 汉密尔顿 中投 不进 威尔 考克斯 抢下 前场 篮板 直接 扣篮 命中 布 莱切 继续 发威 转身 摆脱 上篮 命中 拜纳姆 三分 偏出 球 砸 远 活塞 球员 退守 不及 克劳福德 轻松 上篮 命中 沃尔 中投 不进 拜纳姆 反击 遭 侵犯 两罚 命中 个人 单节 已经 得到 11 布 莱切 对手 包夹 中投 偏出 普林斯 跑 投 命中 活塞 反超 一分 克劳福德 中投 打铁 拜纳姆 没能 命中 三分 麦蒂 回到 赛场 塞拉芬 进攻 犯规 普林斯 中投 不进 门罗 补篮 命中 麦蒂断 球 直接 中投 命中 布 莱切 走步 麦蒂 假动作 点飞 克劳福德 投篮 再进 第三节 比赛 结束 活塞 81 - 78 奇才 ( 草头 王 )
-----------

Document: 254, Semantic similarity: 0.45255911350250244
-----------
奇才 vs 步行者 前瞻 走出 客场 阴影   斗狠 东部 老八 新浪 体育讯 北京 时间 4 7 奇才队 客场 挑战 东区 第八 步行者 目前 奇才 客场 战绩 3 胜 35 负 最近 客场 两连胜 奇才队 背靠背 作战 今天 主场 107 - 105 险胜 活塞 球队 一举 拿到 赛季 最长 三连胜 实际上 这是 奇才队 2007 - 08 赛季 以后 球队 第一个 赛季 三连胜 这场 比赛 奇才 惊人 获得 35 次 罚球 沃尔一人 包办 16 次 全场 得到 26 12 次 助攻 6 篮板 4 次 抢断 布 莱切 无疑 三连胜 第一 功臣 连胜 期间 场均 得到 29 15.3 篮板 克劳福德 同样 火爆 异常 一位 前锋 首发 埃文斯 表现 低估 活塞 比赛 埃文斯 13 投 9 中射下 20 沃尔 拿到 职业生涯 首个 三连胜 “ 联盟 留下 标签 一名 菜鸟 证明 一部分 很多 想 站 球场上 一分钟 全力以赴 ” 奇才 三连胜 对手 名副其实 鱼腩 球队 无论如何 三连胜 这支 弱旅 一个 不小 激励 尤其 伤病 满营 情况 目前 球队 6 可能 赛季 结束 前 无法 归队 包括 得分王 尼克 - 杨 约什 - 霍华德 拉沙德 - 刘易斯 布克 恩戴 耶 卡 蒂尔 - 马丁 步行者 35 胜 43 负 暂居 东部 第八 目前 东部 前七 已经 锁定 季后赛 剩下 第八名 悬念 步行者 领先 第九位 山猫 2.5 个胜场 领先 10 位 雄鹿 3.5 个胜场 剩下 4 场 比赛 情况 悬念 并不大 明天 山猫 雄鹿 迎战 强敌 ( 魔术 热火 ) 步行者 机会 扩大 领先 场次 优势 球队 头号 得分手 格兰杰 状态 过去 5 场 比赛 得分 20 以下 最近 三场 三分球 12 投 3 汉斯 布鲁在 过去 6 场 比赛 陷入 挣扎 场均 9.3 5.7 篮板 ( 之前 11 场 比赛 贡献 20.6 7.8 篮板 ) 一场 比赛 步行者 12 输给 黄蜂队 主教练 沃格尔 称之为 “ 惨痛 失败 ” 本赛季 两队 战成 2 - 1 步行者 赢下 最近 两次 交锋 两场 比赛 奇才 命中率 均 低于 40% 总 失误 高达 41 次 预计 两队 首发 奇才 沃尔 克劳福德 埃文斯 布 莱切 麦基 步行者 科 里森 格兰杰 乔治 汉斯 布鲁 希伯特 ( 木瓜 丁 )
-----------
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">search_documents_by_keywords</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;搭配&#34;</span><span class="p">,</span> <span class="s2">&#34;高跟鞋&#34;</span><span class="p">],</span> <span class="n">num_docs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">doc_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">document_scores</span><span class="p">,</span> <span class="n">document_ids</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Document: </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">, Semantic similarity: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-----------&#34;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><pre><code>Document: 727, Semantic similarity: 0.5883481502532959
-----------
组图 冷气 办公室   连衣裙 配小 坎肩 美国 设计师 Diane   Von   Furstenberg 曾经 感觉 女人 穿 连衣裙 女人 找到 一件 适合 dream   dress 重要 无需 费神 搭配 单穿 连身 优雅 飞扬 裙摆 似乎 告诉 女 连衣裙 玩起 High   Fashion 变脸 游戏 DKNY 绿色 连衣裙   新品 未 定价 H &amp; M 黑色 外套   新品 未 定价 Agatha 配件 新品 未 定价 C . Banner 高跟鞋   新品 未 定价 低 V 领 连衣裙 秀出 属于 性感 更好 展现出 颈部 线条 搭配 修身 剪裁 西装 短款 皮手套 极具 欧美 明星 范儿 细 高跟鞋 更好 突出 双腿 长度 整体 显得 轻盈 不少 On &amp; on 米色 连衣裙   新品 未 定价 Asobio 针织 外套   RMB   449 Kookai 金色 腰带 Jc  
-----------

Document: 435, Semantic similarity: 0.5440454483032227
-----------
组图 秋冬 优雅 妖娆   女星 爱 裸 色系 导语 裸色 优雅 代名词 女星 近来 誓 裸色 进行 到底 无论是 徐若 ? 性感 乐基儿 气质 搭配 各色 礼服 赏心悦目 娇俏 款式 更是 大饱眼福 徐若 ? 飘逸 丝带    立刻 彰显 天王 嫂 贵妇 气质 袁咏仪 翻领 西装   气质 非凡 裸色 短款 紧身 西装 皮质 面料 彰显 个性 夹带 一点 蕾丝 装饰 女性 柔美 油然而生 搭配 碎花 蛋糕 裙 气质 非凡
-----------

Document: 870, Semantic similarity: 0.523485541343689
-----------
组图 看达人 演绎 豹纹 军装 风 导语 懂得 潮流 总是 知道 适合 今冬 流行 亮点 太 军装 豹纹 类似 民族风情 想要 知道 搭配 快 看看 时尚 达 穿 军绿色 宽松 款 大衣 不失 俏皮 味道 高腰 设计 短裙 有效 提升 腰线 衬托出 修长 美腿 豹纹 今年 冬季 抢眼 搭配 元素 加上 驼色 针织衫 灰色 围巾 暖 棕色 手 挎包 整体 色调 统一 迷人 棕色 蓝色 结合能 眼前一亮 简洁 款式 依然 突显 
-----------

Document: 522, Semantic similarity: 0.4756317138671875
-----------
女星 争当 蓝色妖姬 &amp; nbsp ; 英国 气质 女演员 瑞切尔 ・ 薇 兹 时尚 点评 英国 气质 女演员 瑞切尔 · 薇 兹 ( Rachel   Weisz )   美貌 非常 头脑 修身 印花 连衣裙 搭配 抢眼 棕红色 短 夹克 非常 好看 搭配 黑色 罗马 feel 高跟鞋 特别 有潮味 时尚 点评 身材 不算 瘦 女星 Lea   Michele 搭配 起来 非常 特色 一味 地瘦 风格 满是 褶皱 裙子 非常 修身 亮眼 颜色 非常
-----------

Document: 707, Semantic similarity: 0.47334203124046326
-----------
组图 黑丝 短裙 上阵   5 旬 女星 胜过 90 红星 导语 气温 越来越低 女星 不畏 严寒 纷纷 穿着 短裙 透视装 出席 活动 一番 比拼 不难 发现 气质 年轻 难得 厉害 一起 看看 刘晓庆 55 岁 近日 颁奖礼 刘晓庆 一袭 宝蓝色 超低 胸 V 领 长裙 亮相 轻薄 蕾丝 奢华 皮草 艳丽 色彩 翠绿 首饰 配上 短小 精炼 波波 头 瞬间 减龄 15 岁 张曼玉 46 岁 一向 气质 型 美女 著称 反倒 少 繁琐 修饰 刻意 打扮 超级 简单 Lanvin   for   H &amp; M 斜肩 礼裙 搭配 一双 皮质 手套 
-----------
</code></pre>
<p><br><br></p>
<h2 id="10-get_topic_hierarchy">10. get_topic_hierarchy</h2>
<p>对话题进行分类，需要</p>
<ol>
<li>先执行model.hierarchical_topic_reduction</li>
<li>再执行model.get_topic_hierarchy。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 将话题分为2类</span>
<span class="n">model</span><span class="o">.</span><span class="n">hierarchical_topic_reduction</span><span class="p">(</span><span class="n">num_topics</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_topic_hierarchy</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<pre><code>[[7, 6, 1, 8, 5, 4, 3], [2, 0]]
</code></pre>
<p><br><br></p>
<h2 id="11-similar_words">11. similar_words</h2>
<p>查找相似词， 该方法其实也可以用于扩充词典。</p>
<p>similar_words(keywords, num_words, keywords_neg=None)</p>
<ul>
<li>keywords: 待查询关键词列表</li>
<li>num_words: 返回相似词个数</li>
<li>keywords_neg: 指定反义词列表</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查找【增进】的最相似的10个词</span>
<span class="n">model</span><span class="o">.</span><span class="n">similar_words</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;增进&#34;</span><span class="p">],</span> 
                    <span class="n">num_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                    <span class="n">keywords_neg</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>(array(['两国关系', '两国', '温家宝', '王刚', '战略', '友好', '中欧', '政治', '会见', '人民'],
       dtype='&lt;U4'),
 array([0.50498132, 0.49835259, 0.4636392 , 0.45802986, 0.45299921,
        0.44836198, 0.43550295, 0.43471974, 0.43099192, 0.42711113]))
</code></pre>
<br>
<h2 id="12-save">12. save</h2>
<p>训练不易， 记得保存模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;随便起个名字.pkl&#39;</span><span class="p">)</span>
</code></pre></div><br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>KeyBERT | 关键词发现</title>
      <link>https://textdata.cn/blog/keybert_tutorial/</link>
      <pubDate>Wed, 27 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/keybert_tutorial/</guid>
      <description>使用 BERT 嵌入 和 简单余弦相似度 来查找文档中与文档本身最相似的短语，自动挖掘文本中的关键词</description>
      <content:encoded><![CDATA[<p>尽管已经有很多方法可用于关键字生成（例如，Rake、YAKE!、TF-IDF 等），但我想创建一个非常基本但功能强大的方法来提取关键字和关键短语。这就是 KeyBERT 的用武之地！它使用 <strong>BERT 嵌入</strong> 和 <strong>简单余弦相似度</strong> 来查找文档中与文档本身最相似的短语。</p>
<p>KeyBERT步骤</p>
<ol>
<li>首先使用 BERT 提取文档嵌入以获得<strong>文档级向量表示</strong>。</li>
<li>随后，为 N-gram 词/短语提取<strong>词向量</strong>。</li>
<li>然后，我们使用余弦相似度来找到与文档最相似的单词/短语。</li>
<li>最后可以将最相似的词识别为最能描述整个文档的词。</li>
</ol>
<h2 id="代码下载">代码下载</h2>
<p><a href="KeyBERT%E5%AD%A6%E4%B9%A0.ipynb">click to download the code</a></p>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">keybert</span><span class="o">==</span><span class="mf">0.5.0</span>
</code></pre></div><br>
<h2 id="初始化模型">初始化模型</h2>
<p>KeyBERT库需要安装配置spacy语言模型</p>
<p>具体参考<strong>公众号：大邓和他的Python</strong> 2021-10-29 的推文 查看spacy配置方法</p>
<p>初始化模型</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">import</span> <span class="nn">jieba</span>


<span class="n">zh_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;zh_core_web_sm&#34;</span><span class="p">)</span>
<span class="n">bertModel</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">zh_model</span><span class="p">)</span>
</code></pre></div><br>
<h2 id="准备数据">准备数据</h2>
<p>中文测试数据需要先分词，而后构造成类英文的语言结构(用空格间隔的文本)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 测试数据</span>
<span class="n">doc</span> <span class="o">=</span>  <span class="s2">&#34;&#34;&#34;时值10月25日抗美援朝纪念日，《长津湖》片方发布了“纪念中国人民志愿军抗美援朝出国作战71周年特别短片”，再次向伟大的志愿军致敬！
</span><span class="s2">　　电影《长津湖》全情全景地还原了71年前抗美援朝战场上那场史诗战役，志愿军奋不顾身的英勇精神令观众感叹：“岁月峥嵘英雄不灭，丹心铁骨军魂永存！”影片上映以来票房屡创新高，目前突破53亿元，暂列中国影史票房总榜第三名。
</span><span class="s2">　　值得一提的是，这部影片的很多主创或有军人的血脉，或有当兵的经历，或者家人是军人。提起这些他们也充满自豪，影片总监制黄建新称：“当兵以后会有一种特别能坚持的劲儿。”饰演雷公的胡军透露：“我父亲曾经参加过抗美援朝，还得了一个三等功。”影片历史顾问王树增表示：“我当了五十多年的兵，我的老部队就是上甘岭上下来的，那些老兵都是我的偶像。”
</span><span class="s2">　　“身先士卒卫华夏家国，血战无畏护山河无恙。”片中饰演七连连长伍千里的吴京感叹：“要永远记住这些先烈们，他们给我们带来今天的和平。感谢他们的付出，才让我们有今天的幸福生活。”饰演新兵伍万里的易烊千玺表示：“战争的残酷、碾压式的伤害，其实我们现在的年轻人几乎很难能体会到，希望大家看完电影后能明白，是那些先辈们的牺牲奉献，换来了我们的现在。”
</span><span class="s2">　　影片对战争群像的恢弘呈现，对个体命运的深切关怀，令许多观众无法控制自己的眼泪，观众称：“当看到影片中的惊险战斗场面，看到英雄们壮怀激烈的拼杀，为国捐躯的英勇无畏和无悔付出，我明白了为什么说今天的幸福生活来之不易。”（记者 王金跃）
</span><span class="s2">        &#34;&#34;&#34;</span>


<span class="n">doc</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">doc</span><span class="p">))</span>


<span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">keywords</span>
</code></pre></div><pre><code>[('铁骨', 0.5028),
 ('纪念日', 0.495),
 ('丹心', 0.4894),
 ('战役', 0.4869),
 ('影史', 0.473),
 ('父亲', 0.4576),
 ('票房', 0.4571),
 ('偶像', 0.4497),
 ('精神', 0.4436),
 ('家国', 0.4373)]
</code></pre>
<br>
<h2 id="常用参数">常用参数</h2>
<p><strong>bertModel.extract_keywords(docs, keyphrase_ngram_range, stop_words, top_n)</strong></p>
<ul>
<li><strong>docs</strong> 文档字符串（空格间隔词语的字符串）</li>
<li><strong>keyphrase_ngram_range</strong> 设置ngram，默认(1, 1)</li>
<li><strong>stop_words</strong> 停用词列表</li>
<li><strong>top_n</strong> 显示前n个关键词，默认5</li>
<li><strong>highlight</strong> 可视化标亮关键词，默认False</li>
<li>use_maxsum: 默认False;是否使用Max Sum Similarity作为关键词提取标准，</li>
<li>use_mmr: 默认False;是否使用Maximal Marginal Relevance (MMR) 作为关键词提取标准</li>
<li>diversity 如果use_mmr=True，可以设置该参数。参数取值范围从0到1</li>
</ul>
<br>
<p>对于<strong>keyphrase_ngram_range</strong>参数，</p>
<ul>
<li>(1, 1) 只单个词， 如&quot;抗美援朝&quot;, &ldquo;纪念日&quot;是孤立的两个词</li>
<li>(2, 2) 考虑词组， 如出现有意义的词组 &ldquo;抗美援朝 纪念日&rdquo;</li>
<li>(1, 2) 同时考虑以上两者情况</li>
</ul>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">diversity</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> 
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">keywords</span>
</code></pre></div><pre><code>[('影片 总监制', 0.5412),
 ('丹心 铁骨', 0.5339),
 ('抗美援朝 纪念日', 0.5295),
 ('长津湖 片方', 0.5252),
 ('志愿军 致敬', 0.5207),
 ('老兵 偶像', 0.5192),
 ('票房 创新', 0.5108),
 ('军人 血脉', 0.5084),
 ('家国 血战', 0.4946),
 ('家人 军人', 0.4885)]
</code></pre>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#可视化</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">highlight</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="highlight.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 关键词提取</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">bertModel</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> 
                                      <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                                      <span class="n">stop_words</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">use_mmr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">diversity</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> 
                                      <span class="n">top_n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">keywords</span>
</code></pre></div><pre><code>[('影片 总监制', 0.5412),
 ('长津湖 片方', 0.5252),
 ('抗美援朝 纪念日', 0.5295),
 ('丹心 铁骨', 0.5339),
 ('志愿军 致敬', 0.5207),
 ('老兵 偶像', 0.5192),
 ('票房 创新', 0.5108),
 ('军人 血脉', 0.5084),
 ('家国 血战', 0.4946),
 ('家人 军人', 0.4885)]
</code></pre>
<br>
<h2 id="英文keybert">英文KeyBERT</h2>
<p>同样需要配置spacy，参考<strong>公众号：大邓和他的Python</strong> 2021-10-29 的推文 查看spacy配置方法</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keybert</span> <span class="kn">import</span> <span class="n">KeyBERT</span>
<span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">en_model</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&#34;en_core_web_sm&#34;</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">         Supervised learning is the machine learning task of learning a function that
</span><span class="s2">         maps an input to an output based on example input-output pairs. It infers a
</span><span class="s2">         function from labeled training data consisting of a set of training examples.
</span><span class="s2">         In supervised learning, each example is a pair consisting of an input object
</span><span class="s2">         (typically a vector) and a desired output value (also called the supervisory signal). 
</span><span class="s2">         A supervised learning algorithm analyzes the training data and produces an inferred function, 
</span><span class="s2">         which can be used for mapping new examples. An optimal scenario will allow for the 
</span><span class="s2">         algorithm to correctly determine the class labels for unseen instances. This requires 
</span><span class="s2">         the learning algorithm to generalize from the training data to unseen situations in a 
</span><span class="s2">         &#39;reasonable&#39; way (see inductive bias).
</span><span class="s2">      &#34;&#34;&#34;</span>
<span class="n">kw_model</span> <span class="o">=</span> <span class="n">KeyBERT</span><span class="p">()</span>
<span class="n">keywords</span> <span class="o">=</span> <span class="n">kw_model</span><span class="o">.</span><span class="n">extract_keywords</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">keyphrase_ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">keywords</span>
</code></pre></div><p>Run</p>
<pre><code>[('supervised learning', 0.6779),
 ('supervised', 0.6676),
 ('signal supervised', 0.6152),
 ('examples supervised', 0.6112),
 ('labeled training', 0.6013)]
</code></pre>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>BERTopic库 | 使用预训练模型做话题建模</title>
      <link>https://textdata.cn/blog/bertopic_tutorial/</link>
      <pubDate>Tue, 26 Oct 2021 16:40:10 +0600</pubDate>
      
      <guid>/blog/bertopic_tutorial/</guid>
      <description>使用BERT主题建模技术,可以对经管等领域文本数据进行主题(话题)建模。效果堪比LDA，但比LDA智能</description>
      <content:encoded><![CDATA[<p>BERT是自然语言处理领域最新的词向量技术，而BERTopic 是基于BERT词向量进行主题建模技术，它利用 Transformer 和 c-TF-IDF 来创建密集的集群，允许轻松解释主题，同时在主题描述中保留重要词。</p>
<p>BERTopic亮点</p>
<ul>
<li>支持引导式Guided</li>
<li>支持（半）监督式</li>
<li>支持动态主题。</li>
<li>支持可视化</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">bertopic</span><span class="o">==</span><span class="mf">0.10.0</span>
<span class="err">!</span><span class="n">pip3</span> <span class="n">install</span> <span class="n">cntext</span><span class="o">==</span><span class="mf">1.6.5</span>
</code></pre></div><p><br><br></p>
<h2 id="准备数据">准备数据</h2>
<p>这里使用的新闻数据集， 共2000条。 新闻类别涵 <code>'娱乐', '教育', '游戏', '财经', '时政', '时尚', '科技', '体育', '家居', '房产'</code>
这里假设大家不知道有10类新闻题材， 构建模型的时候不会用到label字段的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;cnews.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 新闻题材</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>

<span class="c1">#记录数</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<pre><code>['娱乐' '教育' '游戏' '财经' '时政' '时尚' '科技' '体育' '家居' '房产']
2000
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 各类题材的新闻记录数</span>
<span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">时政    120
科技    106
时尚    106
财经    105
家居    103
教育     97
娱乐     96
体育     95
房产     87
游戏     85
</code></pre></div><br>
<p>这里定义了一个清洗数据函数clean_text，需要注意BERTopic需要先将中文分词改造成类似英文文本格式（用空格间隔词语）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_pkl_dict</span><span class="p">(</span><span class="s1">&#39;STOPWORDS.pkl&#39;</span><span class="p">)[</span><span class="s1">&#39;STOPWORDS&#39;</span><span class="p">][</span><span class="s1">&#39;chinese&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>


<span class="n">test</span> <span class="o">=</span> <span class="s2">&#34;云南永善县级地震已致人伤间民房受损中新网月日电据云南昭通市防震减灾局官方网站消息截至日时云南昭通永善县级地震已造成人受伤其中重伤人轻伤人已全部送医院救治民房受损户间倒塌户间个乡镇所学校不同程度受损目前被损毁电力交通通讯设施已全部抢通修复当地已调拨帐篷顶紧急转移万人月日时分云南昭通永善县发生里氏级地震震源深度公里当地震感强烈此外成都等四川多地也有明显震感&#34;</span>

<span class="n">clean_text</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&#39;云南 永善县 级 地震 已致 伤间 民房 受损 中新网 日电 云南 昭通市 防震 减灾 局 官方网站 消息 日时 云南 昭通 永善县 级 地震 造成 受伤 重伤 轻伤 送 医院 救治 民房 受损 户间 倒塌 户间 乡镇 学校 不同 程度 受损 目前 损毁 电力 交通 通讯 设施 抢通 修复 调拨 帐篷 顶 紧急 转移 万人 时分 云南 昭通 永善县 发生 里氏 级 地震 震源 深度 公里 震感 强烈 成都 四川 多地 明显 震感&#39;
</code></pre></div><p>对2000条数据进行clean_text，得到的结果存储到content字段中。</p>
<p>我的macbook内存16G, 运行时间10s</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="训练topic模型">训练Topic模型</h2>
<p>文本分析步骤包括构建特征工程和训练，在本文中，直接使用开源的预训练中文词向量，省去了特征模型的学习时间。</p>
<p>选取的与训练模型均为word2vec格式，这样方便我们使用gensim将其导入。</p>
<table>
<thead>
<tr>
<th>模型名</th>
<th>数据</th>
<th>预训练模型资源地址</th>
</tr>
</thead>
<tbody>
<tr>
<td>sgns.zhihu.words.bz2</td>
<td>知乎</td>
<td>链接: <a href="https://pan.baidu.com/s/1BDxP28KL_23Odj9NWZGe-Q">https://pan.baidu.com/s/1BDxP28KL_23Odj9NWZGe-Q</a> 提取码: n1qq</td>
</tr>
<tr>
<td>sgns.wiki.words.bz2</td>
<td>中文维基百科</td>
<td>链接: <a href="https://pan.baidu.com/s/1B1sxHmPeIPJYiCuP1zrmMw">https://pan.baidu.com/s/1B1sxHmPeIPJYiCuP1zrmMw</a> 提取码: hofj</td>
</tr>
<tr>
<td>sgns.financial.words.bz2</td>
<td>金融</td>
<td>链接: <a href="https://pan.baidu.com/s/1L_hmGjZMY2ExBn9Vfc_eRg">https://pan.baidu.com/s/1L_hmGjZMY2ExBn9Vfc_eRg</a> 提取码: hhn6</td>
</tr>
<tr>
<td>sgns.renmin.words.bz2</td>
<td>人民日报</td>
<td>链接: <a href="https://pan.baidu.com/s/1VQIDrwZH3Y3Lpy4-smPutw">https://pan.baidu.com/s/1VQIDrwZH3Y3Lpy4-smPutw</a> 提取码: 3b53</td>
</tr>
<tr>
<td>sgns.sougou.words.bz2</td>
<td>搜狗新闻</td>
<td>链接: <a href="https://pan.baidu.com/s/15nCaeB41mwK0ZVLrukXpFQ">https://pan.baidu.com/s/15nCaeB41mwK0ZVLrukXpFQ</a> 提取码: 04en</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>Note</strong>:</p>
<p>除了表格外的资源，还可以使用spacy现有的预训练模型。</p>
</blockquote>
<p>本文案例cnews.csv是新闻类数据，这里最好选择使用同样为新闻题材的文本训练出的模型，这样BERTopic效果会更精准一些。sgns.sougou.words.bz2是使用搜狗新闻数据训练的语言模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">chinese_sougou_news_models</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;sgns.sogou.word.bz2&#39;</span><span class="p">,</span> <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">chinese_sougou_news_models</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x7f93e5b8cc10&gt;
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">bertopic</span> <span class="kn">import</span> <span class="n">BERTopic</span>


<span class="n">topic_model</span> <span class="o">=</span> <span class="n">BERTopic</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="s2">&#34;chinese (simplified)&#34;</span><span class="p">,</span> 
                       <span class="n">embedding_model</span><span class="o">=</span><span class="n">chinese_sougou_news_models</span><span class="p">,</span>
                       <span class="n">calculate_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                       <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">docs</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="c1">#2000条进行fit_transform需要1min</span>
<span class="n">topics</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div><pre><code>100%|██████████| 2000/2000 [01:31&lt;00:00, 21.91it/s]
2021-10-28 12:11:25,583 - BERTopic - Transformed documents to Embeddings
2021-10-28 12:11:34,582 - BERTopic - Reduced dimensionality with UMAP
2021-10-28 12:11:34,718 - BERTopic - Clustered UMAP embeddings with HDBSCAN


CPU times: user 1min 50s, sys: 7.7 s, total: 1min 57s
Wall time: 1min 43s
</code></pre>
<p><br><br></p>
<h2 id="主题模型方法">主题模型方法</h2>
<ul>
<li>topic_model.get_topic_info 查看各主题信息</li>
<li>topic_model.find_topics(term, top_n=5)  查找term最有可能所属话题</li>
<li>topic_model.get_topic(0) 查看Topic 0的特征词</li>
<li>topic_model.visualize_topics() 话题间距离的可视化</li>
<li>topic_model.visualize_distribution(probs[0]) 查看某条文本的主题分布</li>
<li>topic_model.visualize_hierarchy(top_n_topics=20) 主题层次聚类可视化</li>
<li>topic_model.visualize_barchart(topics=[1]) 显示主题1的词条形图</li>
<li>topic_model.visualize_heatmap(n_clusters=10) 主题相似度热力图</li>
<li>topic_model.visualize_term_rank() 可视化词语</li>
<li>topic_model.save()  保存主题模型</li>
<li>topic_model.reduce_topics()  压缩主题个数(合并相近的主题)</li>
</ul>
<h3 id="get_topic_info">.get_topic_info()</h3>
<p>查看BERTopic基于cnews.csv数据， 跑出的各主题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic_info</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/get_topic_info.png" alt=""  />
</p>
<br>
<h3 id="find_topicsterm">.find_topics(term)</h3>
<p>查看与词语【投资】最相关的主题，返回候选的最相思的5个主题id</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#</span>
<span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&#34;投资&#34;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">similar_topics</span>
</code></pre></div><p>Run</p>
<pre><code>[3, 9, 8, 10, 4]
</code></pre>
<br>
<h3 id="get_topic">.get_topic()</h3>
<p>查看id为3的主题信息（主题词及权重）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;基金&#39;, 0.15109221307919193),
 (&#39;投资&#39;, 0.042856192509064),
 (&#39;公司&#39;, 0.039785278320496976),
 (&#39;市场&#39;, 0.037072163603417835),
 (&#39;股票&#39;, 0.03230913401086524),
 (&#39;型基金&#39;, 0.02721898070238429),
 (&#39;收益&#39;, 0.025435672141638468),
 (&#39;投资者&#39;, 0.024633503649868493),
 (&#39;经理&#39;, 0.02458550023931051),
 (&#39;发行&#39;, 0.022672639068067168)]
</code></pre></div><br>
<h3 id="visualize_topics">.visualize_topics()</h3>
<p>可视化主题间距离</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">visualize_topics1</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_topics</span><span class="p">()</span>
<span class="c1">#可视化结果保存至html中，可以动态显示信息</span>
<span class="n">visualize_topics1</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;visualize_topics.html&#39;</span><span class="p">)</span>
<span class="n">visualize_topics1</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_topics1.png" alt=""  />
</p>
<p><a href="img/visualize_topics1.html">点击查看visualize_topics1.html</a></p>
<br>
<h3 id="visualize_distribution">.visualize_distribution()</h3>
<p>显示第一条新闻的主题概率分布</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">first_new_topic_probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_distribution</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">first_new_topic_probs</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;first_new_topic_probs.html&#39;</span><span class="p">)</span>
<span class="n">first_new_topic_probs</span>
</code></pre></div><p><img loading="lazy" src="img/first_new_topic_probs.png" alt=""  />

<a href="img/first_new_topic_probs.html">点击查看first_new_topic_probs.html</a></p>
<p>为了理解主题的潜在层次结构，我们可以使用 scipy.cluster.hierarchy 创建聚类并可视化它们之间的关系。 这有助于合并相似主题，达到降低主题模型主题数量nr_topics。</p>
<br>
<h3 id="visualize_hierarchytop_n_topics">.visualize_hierarchy(top_n_topics)</h3>
<p>话题层次聚类可视化，模型跑出12个主题，这里就按12进行分层聚类</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_hierarchy</span><span class="p">(</span><span class="n">top_n_topics</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_hierarchy.png" alt=""  />
</p>
<br>
<h3 id="visualize_barcharttopics">.visualize_barchart(topics)</h3>
<p>显示topics的词条形图</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_barchart</span><span class="p">(</span><span class="n">topics</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div><p><img loading="lazy" src="img/visualize_barchart.png" alt=""  />
</p>
<br>
<h3 id="visualize_heatmapn_clusters">.visualize_heatmap(n_clusters)</h3>
<p>话题相似热力图。BERTopic可将主题以embeddings形式（向量）表示， 因此我们可以应用余弦相似度来创建相似度矩阵。 每两两主题可进行余弦计算，最终结果将是一个矩阵，显示主题间的相似程度。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_similar_heatmap</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_heatmap</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">topic_similar_heatmap</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;topic_similar_heatmap.html&#39;</span><span class="p">)</span>
<span class="n">topic_similar_heatmap</span>
</code></pre></div><p><img loading="lazy" src="img/topic_similar_heatmap.png" alt=""  />

<a href="img/topic_similar_heatmap.html">点击查看topic_similar_heatmap.html</a></p>
<p>通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。 从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。 此外，可以轻松地将主题表示相互比较。</p>
<br>
<h3 id="visualize_term_rank">.visualize_term_rank()</h3>
<p>通过根据每个主题表示的 c-TF-IDF 分数创建条形图来可视化主题的选定词语。</p>
<p>从主题之间和主题内的相对 c-TF-IDF 分数中获得见解。</p>
<p>此外，可以轻松地将主题表示相互比较。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">term_score_decline</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">visualize_term_rank</span><span class="p">()</span>
<span class="n">term_score_decline</span><span class="o">.</span><span class="n">write_html</span><span class="p">(</span><span class="s1">&#39;term_score_decline.html&#39;</span><span class="p">)</span>
<span class="n">term_score_decline</span>
</code></pre></div><p><img loading="lazy" src="img/term_score_decline.png" alt=""  />

<a href="img/term_score_decline.html">点击查看term_score_decline.html</a></p>
<h3 id="update_topics">.update_topics()</h3>
<p>更新主题模型。当您训练了一个模型并查看了代表它们的主题和单词时，您可能对表示不满意。 也许您忘记删除停用词，或者您想尝试不同的 n_gram_range。 我们可以使用函数 update_topics 使用 c-TF-IDF 的新参数更新主题表示。</p>
<p>使用.update_topics()更新，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">update_topics</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">topics</span><span class="p">,</span> <span class="n">n_gram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</code></pre></div><p>topic_model得到了更新，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">similar_topics</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">find_topics</span><span class="p">(</span><span class="s2">&#34;手机&#34;</span><span class="p">,</span> <span class="n">top_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">similar_topics</span>
</code></pre></div><p>Run</p>
<pre><code>[2, 7, 4, 1, 5]
</code></pre>
<p>查看话题2的信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">topic_model</span><span class="o">.</span><span class="n">get_topic</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;功能&#39;, 0.022132351014298786),
 (&#39;采用&#39;, 0.02136925357979149),
 (&#39;像素&#39;, 0.020797285140907094),
 (&#39;拍摄&#39;, 0.017850841110848677),
 (&#39;机身&#39;, 0.015056931248982912),
 (&#39;英寸&#39;, 0.014624438184138326),
 (&#39;佳能&#39;, 0.012857768505732597),
 (&#39;支持&#39;, 0.012600856600766349),
 (&#39;光学&#39;, 0.012462085658291079),
 (&#39;相机&#39;, 0.011832978982454568)]
</code></pre></div><p>模型保存</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Save model</span>
<span class="c1">#model.save(&#34;my_model&#34;)</span>
<span class="c1"># Load model</span>
<span class="c1">#my_model = BERTopic.load(&#34;my_model&#34;)</span>
</code></pre></div><br>
<h3 id="reduce_topics">.reduce_topics()</h3>
<p>压缩主题数</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">new_topics</span><span class="p">,</span> <span class="n">new_probs</span> <span class="o">=</span> <span class="n">topic_model</span><span class="o">.</span><span class="n">reduce_topics</span><span class="p">(</span><span class="n">docs</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">nr_topics</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<pre><code>2021-10-28 12:28:01,976 - BERTopic - Reduced number of topics from 20 to 11
</code></pre>
<br>
<h2 id="代码数据">代码数据</h2>
<p><a href="bertopic_tutorial.zip">click to download</a></p>
<br>
<h2 id="总结">总结</h2>
<p>本文使用中文文本数据展示BERTopic部分功能，如果对英文数据感兴趣，可以前往  <a href="https://github.com/MaartenGr/BERTopic">https://github.com/MaartenGr/BERTopic</a> 深入学习。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://textdata.cn/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://textdata.cn/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
