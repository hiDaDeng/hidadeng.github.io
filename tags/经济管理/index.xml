<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>经济管理 on 大邓和他的PYTHON</title>
    <link>/tags/%E7%BB%8F%E6%B5%8E%E7%AE%A1%E7%90%86/</link>
    <description>Recent content in 经济管理 on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Mon, 14 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E7%BB%8F%E6%B5%8E%E7%AE%A1%E7%90%86/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PNAS | 使用语义距离测量一个人的创新力(发散思维)得分</title>
      <link>https://hidadeng.github.io/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/</link>
      <pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-14-pnas_naming_unrelated_words_predicts_creativity/</guid>
      <description>使用语义距离测量一个人的创新力(发散思维)得分</description>
      <content:encoded><![CDATA[<br>
<p>传统测量 <strong>被试者创造力</strong> 存在耗费时间、主观性太强、缺乏客观性，且所得到的分值是不稳定的，无法跨时间、文化、群体进行分值比较。该研究分析了创新力的两大理论，即联系理论和执行理论，即创新力是包含思维的广度和深度两方面。</p>
<ul>
<li><strong>联系理论(广度)</strong> 负责搜寻所有可能方案的集合，增加集合的规模，体现思维的广度。</li>
<li><strong>执行理论(深度)</strong> 负责寻找最佳方案，并将方案落实执行，体现思维的深度。</li>
</ul>
<p>结合Glove词嵌入技术，将每个词理解为一个技术或知识，两词语语义越相似，发散性越低。</p>
<p>文中让被试按照一定规则，随意填写10个名词，使用其中7个有效词语测量被试的创新力(发散性)思维。可以简单的把7个词理解为知识或者技术，7个词语会形成21种词语对(组合)。最后求均值可以测量出被试词语对的语义距离体现创新发散性的强度。<strong>文末含案例代码</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Olson, J.A., Nahas, J., Chmoulevitch, D., Cropper, S.J. and Webb, M.E., 2021. Naming unrelated words predicts creativity. Proceedings of the National Academy of Sciences, 118(25), p.e2022340118.
</code></pre></div><p><br><br></p>
<h2 id="一摘要">一、摘要</h2>
<p><strong>一些理论认为，有 创造力 的人能够产生更多 发散性 的想法。如果这是正确的，简单地让被试写 N 个不相关的单词，然后测量这N个词的语义距离， 作为 #发散思维 的客观衡量标准</strong>。为了验证这一假设，我们要求 8,914 名参与者说出 10 个彼此尽可能不同的单词。</p>
<p>然后计算算法估计单词之间的平均语义距离；<strong>相关词（例如 cat 和 dog）比不相关词（例如 cat 和 thimble）的距离更短。我们预测，产生更大语义距离的人也会在传统的创造力测量中得分更高</strong>。</p>
<p>在研究 1 中，我们发现语义距离与两个广泛使用的创造力测量（替代用途任务和桥接关联差距任务）之间存在中度至强相关性。在研究 2 中，参与者来自 98 个国家，语义距离仅因基本人口变量而略有不同。在一系列已知可预测创造力的问题上，语义距离与表现之间也存在正相关关系。</p>
<p>总体而言， <strong>语义距离</strong> 与已建立的 创造力测量 的相关性至少与这些测量彼此之间的相关性一样强。 因此，在我们所说的发散关联任务中命名不相关的词可以作为发散思维的简短、可靠和客观的衡量标准。</p>
<br>
<h2 id="二创新力理论">二、创新力理论</h2>
<p>想出 3 个尽可能不同的词。根据两种主要的创造力理论 (1, 2)，选择这些词依赖于产生 #远程联想 ，同时抑制 #常见联想 。</p>
<p>#联想理论 (Associative Theory)认为，有创造力的人具有语义记忆结构，可以更容易地链接远程元素 (3-6)。</p>
<p>#执行理论 (Executive Theory) 侧重于自上而下的注意力控制；创造性的解决方案来自于监测和抑制共同的联想 (2, 7)。</p>
<p>基于这些理论，我们假设 <strong>填写n个无关单词的任务</strong> 可以可靠地衡量 #语言创造力 。 <strong>创造力有两个主要的心理成分， 收敛思维和发散思维，它们在产生创意输出时协同工作</strong>。收敛性思维任务衡量评估多种刺激并得出最适当响应的能力，例如问题的最佳解决方案 (3, 8-10)。这些任务往往更容易得分，因为只有一小部分正确答案。<strong>相比之下，发散思维任务通常使用开放式问题来衡量一个人产生各种解决方案的能力</strong> (11-13)。它们通常需要更长的回答(文本)，因此更难客观评分。</p>
<br>
<h2 id="三创新力测量">三、创新力测量</h2>
<h3 id="31--替代用途任务">3.1  替代用途任务</h3>
<p>最常见的发散思维测量是 <strong>替代用途任务</strong> Alternative Uses Task (14, 15)，在该任务中，参与者生成常见物体的用途，例如回形针或鞋子。使用常用的评分方法 (16)，评分者然后根据三个组成部分来判断回答：</p>
<ul>
<li>灵活性，产生的不同用途类别的数量；</li>
<li>独创性，每次使用相对于样本的其余部分的稀有程度，这对创造力特别重要（17、18）；和</li>
<li>流畅度，一共产生了多少次使用。</li>
</ul>
<br>
<h3 id="32-离散联系任务">3.2 离散联系任务</h3>
<p>本研究作者开发了 <strong>离散联系任务</strong> (Divergent Association Task， DAT) 的网站， <strong>填写你想到的10个不相关词语， 创造力越丰富的人，填写的词语语义距离往往会更远</strong>。</p>
<p><a href="https://www.datcreativity.com/">https://www.datcreativity.com/</a></p>
<p><img loading="lazy" src="img/1_pnas_divergent_association_task_mainpage.png" alt=""  />
</p>
<h3 id="被试填写10个单词的规则">被试填写10个单词的规则</h3>
<ol>
<li>只能填写英文单词</li>
<li>只能是名词(如事情、物体、概念)</li>
<li>不能填 专有名词（例如，特定的人或地点）</li>
<li>不能填写 专业词（比如技术词）</li>
<li>自己思考这些词，不要只看周围环境的物体。</li>
</ol>
<h3 id="dat算法实现">DAT算法实现</h3>
<ol>
<li>使用Glove预训练模型</li>
<li>选前7个词(一共10个词)， 存在 21个词对（组合）</li>
<li>对21词对， 分别计算词向量的余弦距离，分别乘以100。最终求均值得到DAT得分。</li>
</ol>
<blockquote>
<p>下图是大邓第二次填写得到的DAT得分，第一次只超过了6%的人，这方法第一次准，再测就知道如何提高DAT得分。</p>
</blockquote>
<p><img loading="lazy" src="img/2_pnas_divergent_association_task_result.png" alt=""  />
</p>
<p>DAT得分范围0-200， 得分为0可能是7个有效词之间语义相同，而得分200可能是有效词之间彼此语义完全不相同。实践中，得分大多处于65~90之间，且很少超过100。</p>
<p><img loading="lazy" src="img/pnas_dat_score_low_median_high.jpg" alt=""  />
</p>
<blockquote>
<p>词嵌入技术可以把每个词转化为等长的向量，而不同词语共处于相同的语义空间中。常见的词嵌入技术有word2vec、Glove、flastText等，因为最近有学者在 <strong>替代用途任务</strong>(Alternative Uses Task）中用过Glove算法，本文采用Glove算法。本研究使用的Glove预训练模型来自Common Crawl Corpus项目，该项目拥有数十亿网页文本数据。</p>
<p>为了提供冗余， 只采用 被试者 填写的前7个词作为有效单词(DAT的被试需要填写10个词)。DAT得分是这些词之间的语义距离的平均值，具体计算方法， 7个词两两相关的组合有 42种组合， 选择其中最有可能的 21 个语义组合。</p>
</blockquote>
<br>
<h2 id="四实验">四、实验</h2>
<p>这种发散思维的操作化是基于创造力的联想和执行控制理论。 更高的分数将显示出更大的能力来利用更远程的关联 (3-5) 或抑制过度相关的关联 (2, 7)。</p>
<p>在研究 1 中，我们通过将 DAT 与其他两种创造力测量方法进行比较来检验这一假设：替代用途任务 (15) 和桥接关联差距任务 (36)。
<img loading="lazy" src="img/pnas_dat_aut_algo_valid_num.jpg" alt=""  />
</p>
<p>在研究 2 中，我们测试了这些分数如何随人口统计而变化，以及它们是否与更大数据集中与发散性思维相关的其他测量值相关 (9, 37)。 这些研究评估了语义距离是否可以作为发散思维的可靠指标。
<img loading="lazy" src="img/pnas_dat_gender_age.jpg" alt=""  />
</p>
<br>
<h2 id="五讨论">五、讨论</h2>
<p>研究结果表面， 让被试简单的填写10个不想管单词的任务可以作为 测量发散思维 的可靠衡量标准。在研究中， 将这项任务的表现与已有的两种创造力量表做了比较，具有很高的相关性。</p>
<p>总体而言支持了语义发散性，尽管这种联系背后的确切机制尚不清楚，但在创新力最主要的两个理论，即联想理论或执行理论 的联系网络中衡量网络的范围或效率。</p>
<p><strong>DAT算法表现稳定，方差不随人口统计特征变化出现显著性变化（研究2），可以在跨年龄、跨性别的情况下应用</strong>。</p>
<br>
<h3 id="51-dat的优点">5.1 DAT的优点</h3>
<ul>
<li>操作简单，快捷，客观，节约了大量的人力时间，又能保证客观性。</li>
<li>得分绝对，可比较，可以用于测量不同群体(种族、文化、性别、年龄)的创造力得分。</li>
<li>对被试友好，一般一两分钟即可完成。</li>
</ul>
<h3 id="52-dat的不足">5.2 DAT的不足</h3>
<ul>
<li>创造力有发散性和执行力，发散性负责搜选所有方案集合的规模，而执行力是从方案集中选出最优方案并将其执行。DAT测量的仅仅是发散性思维。</li>
<li>被试可能通过填写稀奇的词语提高DAT得分。</li>
<li>只有短短几分钟，被试可能很难短时间内了解实验规则。</li>
</ul>
<h3 id="53-未来展望">5.3 未来展望</h3>
<p>DAT得分取决于Glove模型、语料库(数据集), 更新词模型或语料库，被试的DAT得分会发生变化。为简单起见，本研究使用免费的预训练模型， 通过一些努力，未来研究者可以对不同时期，不同国家的语料库来训练Glove模型。随着特定单词关联或多或少的联系， 更新的模型将会自动考虑这些变化，这将允许DAT得分跨越文化跨越时代，进行创新力的比较。</p>
<p><br><br></p>
<h2 id="代码">代码</h2>
<p>代码的文档说明请点击 github仓库地址 <a href="https://github.com/jayolson/divergent-association-task">https://github.com/jayolson/divergent-association-task</a> 查看。这里仅粘贴作者源代码，源代码需要配置好才可运行。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">dat</span>

<span class="c1">## 从 https://nlp.stanford.edu/projects/glove/ 下载Glove模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">dat</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="s2">&#34;glove.840B.300d.txt&#34;</span><span class="p">,</span> <span class="s2">&#34;words.txt&#34;</span><span class="p">)</span>

<span class="c1"># 验证词语，如输入的是词组，代码会将其转为连线形式的单词</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="s2">&#34;cul de sac&#34;</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cul-de-sac
</code></pre></div><br>
<p>计算两个词语之间的语义距离</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;dog&#34;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;thimble&#34;</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.1983
0.8787
</code></pre></div><br>
<p>计算词对的DAT得分（语义cosine距离*100）</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">([</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;dog&#34;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span> 
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">([</span><span class="s2">&#34;cat&#34;</span><span class="p">,</span> <span class="s2">&#34;thimble&#34;</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">19.83
87.87
</code></pre></div><br>
<p>假设有三个人分别都填写10个词，选其前7个词作为有效词。有效词如下，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">low</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;arm&#34;</span><span class="p">,</span> <span class="s2">&#34;eyes&#34;</span><span class="p">,</span> <span class="s2">&#34;feet&#34;</span><span class="p">,</span> <span class="s2">&#34;hand&#34;</span><span class="p">,</span> <span class="s2">&#34;head&#34;</span><span class="p">,</span> <span class="s2">&#34;leg&#34;</span><span class="p">,</span> <span class="s2">&#34;body&#34;</span><span class="p">]</span>
<span class="n">average</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;bag&#34;</span><span class="p">,</span> <span class="s2">&#34;bee&#34;</span><span class="p">,</span> <span class="s2">&#34;burger&#34;</span><span class="p">,</span> <span class="s2">&#34;feast&#34;</span><span class="p">,</span> <span class="s2">&#34;office&#34;</span><span class="p">,</span> <span class="s2">&#34;shoes&#34;</span><span class="p">,</span> <span class="s2">&#34;tree&#34;</span><span class="p">]</span>
<span class="n">high</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;hippo&#34;</span><span class="p">,</span> <span class="s2">&#34;jumper&#34;</span><span class="p">,</span> <span class="s2">&#34;machinery&#34;</span><span class="p">,</span> <span class="s2">&#34;prickle&#34;</span><span class="p">,</span> <span class="s2">&#34;tickets&#34;</span><span class="p">,</span> <span class="s2">&#34;tomato&#34;</span><span class="p">,</span> <span class="s2">&#34;violin&#34;</span><span class="p">]</span>

<span class="c1"># Compute the DAT score (transformed average cosine distance of first 7 valid words)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">low</span><span class="p">))</span> <span class="c1"># 50</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">average</span><span class="p">))</span> <span class="c1"># 78</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">dat</span><span class="p">(</span><span class="n">high</span><span class="p">))</span> <span class="c1"># 95</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">50
78
95
</code></pre></div><p>需要注意pnas作者公开的代码只能用在英文，且无法自己训练Glove模型。如果想基于自有数据集（中文、英文），训练自有Glove模型，需要学习</p>
<ul>
<li>如何训练Glove模型</li>
<li>如何导入训练好的Glove模型</li>
<li>如何计算中英文dat得分</li>
</ul>
<p>相关知识点已更新至我的录播课课程 <a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>小规模金融并购、投资事件图谱设计概述与数据构成解析</title>
      <link>https://hidadeng.github.io/blog/2022-11-07-financial-invest-merge/</link>
      <pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-11-07-financial-invest-merge/</guid>
      <description>小规模金融并购、投资事件图谱设计概述与数据构成解析</description>
      <content:encoded><![CDATA[<h2 id="作者">作者</h2>
<p>刘焕勇，NLP开源爱好者与践行者，主页：https://liuhuanyong.github.io。</p>
<p>就职于360人工智能研究院、曾就职于中国科学院软件研究所。</p>
<p>老刘说NLP，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。</p>
<br>
<p>事件图谱是当前的一个十分有趣的话题，我们在前面的事件图谱系列文章中对事件图谱进行了论述。</p>
<p>例如文章《技术思考：面向落地应用的事件类图谱划分、关键问题及其与知识图谱的对比辨析》、《事件图谱应用：智能金融与情报分析中的七大应用潜在场景概述》、《事件图谱技术：基于触发词的事件句识别方法与关键流程总结》等。</p>
<p>同样本着技术具像化的原则，为了让大家对具体事件图谱有个清晰的直观的认识，本文我们介绍一个自建的金融事件图谱，涵盖并购和投资两大类事件类型，从金融事件图谱设计概述、投资事件图谱数据介绍以及并购事件图谱数据介绍三个角度进行论述，供大家一起参考。</p>
<br>
<h2 id="一金融事件图谱设计概述">一、金融事件图谱设计概述</h2>
<p>事件知识图谱EKG（event knowledge graph）是当前事件类图谱的一种，在这里，我更倾向于认为这个图谱本身更倾向于为一个事件知识库，而非实体知识图谱。</p>
<p><strong>事件知识图谱的工作主要围绕事件知识本身进行展开，关注点在于事件内部信息，如ACE中的8大类事件，将这几类事件中的信息进行抽取和填充就能够得到一个以特定事件类型作为分类标准的事件知识库，如婚姻事件库、爆炸事件库等。</strong></p>
<p>而相对应的，领域事件图谱显得更为重要，金融领域作为一个需求较为明显的领域，其建模能力更具代表性，例如，我们可以对事件图谱进行本体定义：</p>
<table>
<thead>
<tr>
<th style="text-align:left"><strong>事件类型</strong></th>
<th style="text-align:left"><strong>事件要素</strong></th>
<th style="text-align:left"><strong>事件关系</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">投资事件</td>
<td style="text-align:left">融资方、投资方、金额、轮次、融资时间、所属行业</td>
<td style="text-align:left">顺承/时序</td>
</tr>
<tr>
<td style="text-align:left">并购事件</td>
<td style="text-align:left">并购方、被并购方、并购状态、所属行业、涉及股权、并购开始时间、并购结束时间、是否VC/PE支持</td>
<td style="text-align:left">顺承/时序</td>
</tr>
</tbody>
</table>
<p>在这样一个本体框架之下，我们要构建起一个事件图谱，可以有两种方式：</p>
<ul>
<li>
<p><strong>从已经结构化好的数据源中直接获取</strong>。例如，目前针对投融资领域已经出现了许多垂类网站，如投资界、IT橘子中直接获取，并做清洗。这种方式最为快捷，但受制于人，其中的数据有限，并存在字段不全的问题。当我们想建成一个实时动态的金融事件图谱库，在捕捉实时数据时，及时处理时候，就需要采用抽取的思路。</p>
</li>
<li>
<p><strong>基于模型的非结构化文本抽取</strong>。为了避免方法1带来的拿来主义缺陷，我们可以转换为标准的事件抽取任务，针对实时的实时新闻流，进行论元识别、事件要素抽取。</p>
</li>
</ul>
<p>例如，给定文本：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">8日消息，总部位于墨西哥的在线批发平台Miferia获得了700万美元种子轮融资，该轮融资由贝恩资本风险投资公司和Tiger Global共同领投。Miferia批发平台将墨西哥的独立零售店与化妆品、食品和饮料以及家居装饰等类别的品牌联系起来。该平台拥有来自500多个品牌的数千种产品，每周有30多个新品牌上线。（Latamlist）
</code></pre></div><p>我们可以从中检测出融资事件：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">8月8日消息，总部位于墨西哥的在线批发平台Miferia获得了700万美元种子轮融资，该轮融资由贝恩资本风险投资公司和Tiger Global共同领投。
</code></pre></div><p>并识别出一下结构化信息：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">融资方：Miferia
金额：700万美元
轮次：种子轮以及投资方贝恩资本风险投资公司、Tiger Global；
融资时间：8月8日、所属行业：在线批发等信息
</code></pre></div><p>下图展示了一个金融领域的一个典型投资领域事件图谱：</p>
<p>其中包括“君度德瑞、新余凯信投资、深圳市立德富盈投资等投资信濠光电20%股权”、“东莞中科中广基金（领投）、中广创投、紫宸创投等投资信濠光电5%股权”两个投资事件，每个投资事件由投资方、融资方、金额、日期、轮次几个事件要素构成，<strong>而若以一个融资方为中心进行融资历程的刻画，就可以根据日期发展的先后顺序，在两个事件之间形成一条边</strong>。</p>
<p><img loading="lazy" src="img/fin_edge_networks.png" alt=""  />
</p>
<p>需要注意的是，现在的事件抽取任务中，是不包含事件名称的抽取的，但如果要星辰恶搞事件图谱，就必须保证该事件的唯一性和友好性，可以使用md5值来表示，但并不直观，图中给出了一个较好的例子，用一个短句来表示。</p>
<br>
<h2 id="二投资事件图谱数据介绍">二、投资事件图谱数据介绍</h2>
<p>我们以投资界为数据源，通过解析整理，形成了9093条投资事件，包括融资方、投资方、金额、轮次、融资时间、所属行业共5个要素。</p>
<p><img loading="lazy" src="img/fin_extract_data.png" alt=""  />
</p>
<p>数据样例：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="p">{</span>
    <span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;苏州聚源铸芯创投基金（领投）、创世一期、高捷资本等投资英彼森&#34;</span><span class="p">,</span>
    <span class="s2">&#34;event_type&#34;</span><span class="p">:</span><span class="s2">&#34;投资事件&#34;</span><span class="p">,</span>
    <span class="s2">&#34;融资方&#34;</span><span class="p">:</span><span class="s2">&#34;英彼森半导体（珠海）有限公司&#34;</span><span class="p">,</span>
    <span class="s2">&#34;投资方&#34;</span><span class="p">:[</span>
        <span class="s2">&#34;聚源资本&#34;</span><span class="p">,</span>
        <span class="s2">&#34;高捷资本&#34;</span><span class="p">,</span>
        <span class="s2">&#34;创世伙伴&#34;</span><span class="p">,</span>
        <span class="s2">&#34;绿河投资&#34;</span><span class="p">,</span>
        <span class="s2">&#34;珠海科技创投&#34;</span>
    <span class="p">],</span>
    <span class="s2">&#34;金额&#34;</span><span class="p">:</span><span class="s2">&#34;RMB数亿&#34;</span><span class="p">,</span>
    <span class="s2">&#34;轮次&#34;</span><span class="p">:</span><span class="s2">&#34;A轮&#34;</span><span class="p">,</span>
    <span class="s2">&#34;融资时间&#34;</span><span class="p">:</span><span class="s2">&#34;2021年06月29日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;所属行业&#34;</span><span class="p">:</span><span class="s2">&#34;半导体及电子设备-半导体&#34;</span>
<span class="p">}</span>

<span class="p">{</span>
    <span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;Esta Investments、DD Asset Holdings、DST China EC XI等投资滴滴集团6.08%股权&#34;</span><span class="p">,</span>
    <span class="s2">&#34;event_type&#34;</span><span class="p">:</span><span class="s2">&#34;投资事件&#34;</span><span class="p">,</span>
    <span class="s2">&#34;融资方&#34;</span><span class="p">:</span><span class="s2">&#34;滴滴&#34;</span><span class="p">,</span>
    <span class="s2">&#34;投资方&#34;</span><span class="p">:[</span>
        <span class="s2">&#34;Esta Investments&#34;</span><span class="p">,</span>
        <span class="s2">&#34;腾讯投资&#34;</span><span class="p">,</span>
        <span class="s2">&#34;THL A11&#34;</span><span class="p">,</span>
        <span class="s2">&#34;纪源资本&#34;</span><span class="p">,</span>
        <span class="s2">&#34;数字天空技术&#34;</span>
    <span class="p">],</span>
    <span class="s2">&#34;金额&#34;</span><span class="p">:</span><span class="s2">&#34;USD7.5亿&#34;</span><span class="p">,</span>
    <span class="s2">&#34;轮次&#34;</span><span class="p">:</span><span class="s2">&#34;B轮&#34;</span><span class="p">,</span>
    <span class="s2">&#34;融资时间&#34;</span><span class="p">:</span><span class="s2">&#34;2014年12月02日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;所属行业&#34;</span><span class="p">:</span><span class="s2">&#34;电信及增值业务-无线互联网服务&#34;</span>
<span class="p">}</span>

</code></pre></div><br>
<h2 id="三并购事件图谱数据介绍">三、并购事件图谱数据介绍</h2>
<p>同样的，我们得到了3865条并购事件数据，包括并购方、被并购方、并购状态、所属行业、涉及股权、并购开始时间、并购结束时间以及是否VC/PE支持等事件要素。</p>
<p><img loading="lazy" src="img/fin_extract_data2.png" alt=""  />
</p>
<p>数据样例：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="p">{</span>
    <span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;友睦口腔收购友睦三九60%股权&#34;</span><span class="p">,</span>
    <span class="s2">&#34;event_type&#34;</span><span class="p">:</span><span class="s2">&#34;并购事件&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购方&#34;</span><span class="p">:</span><span class="s2">&#34;深圳市友睦口腔股份有限公司&#34;</span><span class="p">,</span>
    <span class="s2">&#34;被并购方&#34;</span><span class="p">:</span><span class="s2">&#34;深圳友睦三九口腔门诊部有限公司&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购状态&#34;</span><span class="p">:</span><span class="s2">&#34;已完成&#34;</span><span class="p">,</span>
    <span class="s2">&#34;所属行业&#34;</span><span class="p">:</span><span class="s2">&#34;生物技术/医疗健康-医疗服务&#34;</span><span class="p">,</span>
    <span class="s2">&#34;涉及股权&#34;</span><span class="p">:</span><span class="s2">&#34;60.00 %&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购开始时间&#34;</span><span class="p">:</span><span class="s2">&#34;2017年03月21日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购结束时间&#34;</span><span class="p">:</span><span class="s2">&#34;2017年03月21日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;是否VC/PE支持&#34;</span><span class="p">:</span><span class="s2">&#34;是&#34;</span>
<span class="p">}</span>

<span class="p">{</span>
    <span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;我享科技收购我享网络&#34;</span><span class="p">,</span>
    <span class="s2">&#34;event_type&#34;</span><span class="p">:</span><span class="s2">&#34;并购事件&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购方&#34;</span><span class="p">:</span><span class="s2">&#34;上海我享网络信息科技股份有限公司&#34;</span><span class="p">,</span>
    <span class="s2">&#34;被并购方&#34;</span><span class="p">:</span><span class="s2">&#34;上海我享网络科技有限公司&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购状态&#34;</span><span class="p">:</span><span class="s2">&#34;已完成&#34;</span><span class="p">,</span>
    <span class="s2">&#34;所属行业&#34;</span><span class="p">:</span><span class="s2">&#34;互联网-电子商务-C2C&#34;</span><span class="p">,</span>
    <span class="s2">&#34;涉及股权&#34;</span><span class="p">:</span><span class="s2">&#34;N/A&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购开始时间&#34;</span><span class="p">:</span><span class="s2">&#34;2017年03月01日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;并购结束时间&#34;</span><span class="p">:</span><span class="s2">&#34;2017年03月24日&#34;</span><span class="p">,</span>
    <span class="s2">&#34;是否VC/PE支持&#34;</span><span class="p">:</span><span class="s2">&#34;是&#34;</span>
<span class="p">}</span>
</code></pre></div><br>
<h2 id="总结">总结</h2>
<p>本文我们介绍l额一个自建的金融事件图谱，涵盖并购和投资两大类事件类型，从金融事件图谱设计概述、投资事件图谱数据介绍以及并购事件图谱数据介绍三个角度进行论述，这对加深我们对事件图谱的具象化认识具有一定的意义。</p>
<p>关于具体的数据，可以关注 <strong>公众号：老刘说NLP</strong>，并加入技术社区，与技术社区的朋友一同分享获取。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Maigret库 | 查询某用户名在各平台网站的使用情况</title>
      <link>https://hidadeng.github.io/blog/2022-10-08-find-sns-account-information-with-maigret/</link>
      <pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-10-08-find-sns-account-information-with-maigret/</guid>
      <description>Maigret 能检查各网站(应用) 某 **用户名** 是否注册，并从网页收集所有可用信息,  运行过程不需要 API 密钥。目前支持超过 2500 个站点检索（完整列表），默认针对 500 个热门站点按受欢迎程度降序启动搜索。</description>
      <content:encoded><![CDATA[<p>Maigret 能检查各网站(应用) 某 <strong>用户名</strong> 是否注册，并从网页收集所有可用信息,  运行过程不需要 API 密钥。目前支持超过 2500 个站点检索（完整列表），默认针对 500 个热门站点按受欢迎程度降序启动搜索。</p>
<br>
<h2 id="主要功能">主要功能</h2>
<ul>
<li>个人资料页面解析</li>
<li>个人信息提取</li>
<li>其他个人资料链接等。</li>
<li>通过新用户名和找到的其他 id 进行递归搜索</li>
<li>按标签搜索（网站类别、国家/地区）</li>
</ul>
<br>
<h2 id="安装">安装</h2>
<p>命令行中安装maigret包</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install maigret
</code></pre></div><br>
<h2 id="使用">使用</h2>
<p>我自己有个账号名是hidadeng，就用hidadeng试试。</p>
<p>为了解用户名hidadeng使用情况，报告结果存储于html和pdf。 在命令行中执行，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">maigret hidadeng --html --pdf
</code></pre></div><p>命令行运行过程</p>
<p><img loading="lazy" src="img/hidadeng-cmd.png" alt=""  />
</p>
<br>
<h2 id="报告">报告</h2>
<p>maigret查询用户名hidadeng的使用情况、兴趣等结果可以绘制成报告。</p>
<p><a href="report_hidadeng_plain.html">点击查看hidadeng报告</a></p>
<p><img loading="lazy" src="img/hidadeng-report-1.png" alt=""  />
</p>
<p><img loading="lazy" src="img/hidadeng-report-2.png" alt=""  />
</p>
<p>效果挺准的，对hidadeng这个用户兴趣(coding、shopping)拿捏的也挺不错。</p>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Google Books Ngram Viewer显示英文词汇历史使用趋势</title>
      <link>https://hidadeng.github.io/blog/2022-09-27-r-ngramr/</link>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-09-27-r-ngramr/</guid>
      <description>显示词汇历史使用趋势</description>
      <content:encoded><![CDATA[


<p><a href="https://books.google.com/ngrams/">Google Books Ngram Viewer</a> 可以显示输入短语(你感兴趣的词组)在谷歌书籍中（例如，“British English”、“English Fiction”、“French”）中出现的频率变化趋势 。</p>
<p>网址 <a href="https://books.google.com/ngrams/" class="uri">https://books.google.com/ngrams/</a></p>
<p><img src="ngramr_hacker_programmer.png" /></p>
<p><img src="ngramr_democracy_monarchy.png" /></p>
<p><img src="us_is_us_has.png" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>文献汇总 | 量化历史学与经济学研究</title>
      <link>https://hidadeng.github.io/blog/2022-09-19-quantitative-history-economic/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-09-19-quantitative-history-economic/</guid>
      <description>通过历史数据挖掘构建有意思的新变量， 量化历史学 可能是个不错的借鉴思路。</description>
      <content:encoded><![CDATA[<p>我对经济史不太懂，标题起的可能不一定恰当，通过历史数据挖掘构建有意思的新变量， <strong>量化历史学</strong> 可能是个不错的借鉴思路。</p>
<p><br><br></p>
<h2 id="历史与经济">历史与经济</h2>
<blockquote>
<p>摘自 <a href="https://mp.weixin.qq.com/s/pnwlwQfO0EWgV8XNHzMuZg">陈志武教授统筹量化历史研究获得创纪录研究经费支持</a></p>
</blockquote>
<p>由<strong>陈志武</strong>教授统筹的跨领域研究项目获得大学教育资助委员会（教资会）辖下的研究资助局（研资局）6,732万港元（即逾850万美元） 的拨款资助，再加上香港大学的支持，此单一研究项目共获得7,480万港元 （即逾950万美元），资助金额创学院自2001年成立以来的新高。项目研究团队由来自五间教资会资助院校的成员及合作方组成，包括香港大学（港大）、香港中文大学 （中大）、香港科技大学 （科大）、岭南大学（岭大）及香港浸会大学（浸大），另外亦有来自牛津大学及中国人民大学的学者。项目成员包括：港大陈志武 （项目统筹人）、Jed O. Kaplan、梁其姿、林晨和马驰骋、白营（中大）、康文林（科大）、林展 （人民大学）、刘光临（岭大）以及马德斌 （牛津）。（按英文字母顺序排列）</p>
<p>题为 “<strong>量化历史研究：追寻现代中国发展的根源</strong>” 的研究项目获得研资局辖下第十轮 “<strong>卓越学科领域计划</strong>” 的资助，旨在透过在香港大学成立 “<strong>量化历史研究中心</strong>” ，作为协调和开展 “<strong>量化中国历史</strong>” 研究的核心学术机构，<strong>致力加深对中国历史发展的认识，挖掘历史新知识，促进历史教学，引导政策制定，改善商界运营模式</strong>。</p>
<p>学院金融学讲座教授及郑裕彤基金教授（金融学）陈志武指出：“我们很荣幸能够获得今年度卓越学科领域计划的拨款，这支持并肯定了我们为多层面中国历史研究所作出的努力。<strong>中国蕴藏丰富的历史档案和考古挖掘，其规模在世界乃独一无二， 内容几乎涵盖中国社会的所有方面：从政治到商业、法律和监管、犯罪和动乱、家庭和宗族、文化和习俗、宗教和社会组织，以及科学。近年，这些档案被数码化，为量化历史学家提供前所未有的机会去全面重新审视中国历史的各方面</strong>。作为中国的一部分，香港具有语言、文化和人力资源的优势去建立整体而全面的中国量化历史，我们深信成立量化历史研究中心，将推动香港成为全球量化历史研究的领导者。”</p>
<p><br><br></p>
<h2 id="更多资料">更多资料</h2>
<ul>
<li><a href="https://academic.oup.com/ej/article/130/631/2030/5819954"><strong>鉴古识今 – 从「科举考试」分析中国经济发展</strong></a><br>Ting Chen, James Kai-sing Kung, Chicheng Ma, Long Live <em>Keju</em>! The Persistent Effects of China’s Civil Examination System, <em><strong>The Economic Journal</strong></em>, Volume 130, Issue 631, October 2020, Pages 2030–2064, <a href="https://doi.org/10.1093/ej/ueaa043">https://doi.org/10.1093/ej/ueaa043</a></li>
</ul>
<br>
<ul>
<li>Zhiwu Chen, Chicheng Ma, Andrew J Sinclair, Banking on the Confucian Clan: Why China Developed Financial Markets so Late, <em><strong>The Economic Journal</strong></em>, Volume 132, Issue 644, May 2022, Pages 1378–1413, <a href="https://doi.org/10.1093/ej/ueab082">https://doi.org/10.1093/ej/ueab082</a></li>
</ul>
<br>
<ul>
<li><a href="https://www.bilibili.com/video/BV19L4y1A7Yn">从权贵到富贵： 中国传世明画产权变动与社会流动性研究，960-1911</a></li>
</ul>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV19L4y1A7Yn&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<ul>
<li><a href="https://www.bilibili.com/video/BV1Q34y1J7f5">陈志武 &ndash; 史前文明摇篮的长久影响 &mdash;你的故乡是兴是衰在四千多年前就确定？</a></li>
</ul>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1Q34y1J7f5&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<ul>
<li><a href="https://mp.weixin.qq.com/s/DJei28L1iZ_7rtkD0dZAQQ">李中清|《大数据与中国社会经济史》</a></li>
</ul>
<br>
<ul>
<li><a href="https://mp.weixin.qq.com/s/wpfz4WbNg-wW5IYxbBWcEQ">陈春声 | 统计分析方法在史学研究中的应用</a></li>
</ul>
<br>
<ul>
<li><a href="https://mp.weixin.qq.com/s/NxcFvQm7msQq9DrRLpMydQ">地理信息系统（GIS）与中国历史研究</a></li>
</ul>
<br>
<ul>
<li><a href="https://mp.weixin.qq.com/s/HqLvrK626bXt984dGtnR_A">周欣平：大数据与社会科学和人文科学研究</a></li>
</ul>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>国庆直播 | Python实证指标与文本分析</title>
      <link>https://hidadeng.github.io/blog/2022-09-19-text-mining-in-ms-workshop/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-09-19-text-mining-in-ms-workshop/</guid>
      <description>文本分析在经管研究中的应用</description>
      <content:encoded><![CDATA[<h2 id="报名信息">报名信息</h2>
<ul>
<li>
<p>时间：<strong>2022.10.03 ~ 2022.10.04</strong></p>
</li>
<li>
<p>地点: 小鹅通平台（线上直播）</p>
</li>
<li>
<p>报名咨询:  17816181460（同微信）（汪老师）</p>
</li>
<li>
<p>报名费：<strong>2500元</strong></p>
<ul>
<li>单位：杭州国商智库信息技术服务有限公司</li>
<li>开户银行： 中国银行杭州大学城支行</li>
<li>银行账户：6232636200100260588</li>
</ul>
</li>
</ul>
<br>
<h2 id="简介">简介</h2>
<p>在科学研究中，数据的获取及分析是最重要的也是最棘手的两个环节！</p>
<p>在<strong>前大数据时代</strong>，一般使用实验法、调查问卷、访谈或者二手数据等方式，将数据整理为结构化的表格数据，之后再使用各种计量分析方法，对这些表格数据进行分析。<strong>大数据时代</strong>，大量商业信息、社会信息以文本等非结构化、异构型数据格式存储于海量的网页中。那么对于经管为代表的人文社科类专业科研工作者而言，通过Python可以帮助学者解决使用Web数据进行科研面临的两个问题：</p>
<ol>
<li><strong>网络爬虫</strong> 解决 如何从网络世界中高效地 <strong>采集数据</strong>？</li>
<li><strong>文本分析</strong> 解决 如何从杂乱的文本数据中 <strong>构建指标</strong>？</li>
</ol>
<p>为方便大家感受到文本数据的魅力，按照是否采用某项技术(爬虫、词频、词袋、w2v建词典、w2v认知变迁)，从五个维度标记代表性的7篇论文。</p>
<table>
<thead>
<tr>
<th>文献</th>
<th>爬虫</th>
<th>定性</th>
<th>词频</th>
<th>词袋</th>
<th>W2V建词典</th>
<th>W2V认知变迁</th>
</tr>
</thead>
<tbody>
<tr>
<td>王伟 , 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>语言具体性如何影响顾客满意度</strong><br>Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” <em>Journal of Consumer Research</em> 47, no. 5 (2021): 787-806.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;<strong>Copycats vs. original mobile apps</strong>: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</td>
<td>Y</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>文本相似度</strong><br>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>胡楠, 薛付婧 and 王昊楠, 2021. <strong>管理者短视主义影响企业长期投资吗</strong>———基于文本分析和机器学习. <em>管理世界</em>, <em>37</em>(5), pp.139-156.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>Kai Li, Feng Mai, Rui Shen, Xinyan Yan, <strong>Measuring Corporate Culture Using Machine Learning</strong>, The Review of Financial Studies, 2020</td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td><strong>女性就职高管改变组织内性别偏见</strong><br>Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 119, no. 9 (2022): e2026443119.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
</tbody>
</table>
<br>
<h2 id="主讲老师">主讲老师</h2>
<p>大邓，哈尔滨工业大学(HIT)管理学院信息管理系统方向在读博士。在多所大学分享数据采集和文本分析。运营公众号：大邓和他的Python，主要分享Python、爬虫、文本分析、机器学习等内容。</p>
<br>
<h2 id="一入门语法">一、入门语法</h2>
<ul>
<li>Python跟英语一样是一门语言</li>
<li>数据类型之字符串</li>
<li>数据类型之列表元组集合</li>
<li>数据类型之字典</li>
<li>数据类型之布尔值、None</li>
<li>逻辑语句(if&amp;for&amp;tryexcept)</li>
<li>列表推导式</li>
<li>理解函数</li>
<li>常用的内置函数</li>
<li>os路径库</li>
<li>内置库csv文件库</li>
<li>常见错误汇总</li>
</ul>
<h2 id="二数据采集">二、数据采集</h2>
<ul>
<li>网络爬虫原理</li>
<li>寻找网址规律</li>
<li>获取网页-requests库</li>
<li>pyquery库解析html网页</li>
<li><strong>案例:</strong> 豆瓣小说</li>
<li>json库解析json网页</li>
<li><strong>案例:</strong> 豆瓣电影</li>
<li><strong>案例:</strong> 微博</li>
<li><strong>案例:</strong> 文件下载</li>
<li><strong>案例:</strong> 上市公司定期报告pdf批量下载</li>
<li>区分动态网站与静态网站</li>
</ul>
<h2 id="三文本初识">三、文本初识</h2>
<ul>
<li>从信息传播视角重新认识文本</li>
<li>读取各类文件中的数据</li>
<li><strong>案例:</strong>  识别图片中的文本</li>
<li>数据清洗re库</li>
<li><strong>案例:</strong> 将多个数据文件汇总至一个csv文件</li>
<li><strong>案例:</strong> 中文jieba分词、词频统计、制作词云图</li>
<li><strong>案例:</strong> 使用共现(word2vec)法扩展情感词典</li>
<li><strong>案例:</strong> 使用词典做情感分析(无权重)</li>
<li><strong>案例:</strong> 数据分析pandas库快速入门</li>
<li><strong>案例:</strong> 使用pandas对excel中的文本进行情感分析</li>
</ul>
<h2 id="四文本进阶">四、文本进阶</h2>
<ul>
<li>文本分析与机器学习</li>
<li>特征工程-认识词袋法、one-hot、Tf-Idf、word2vec</li>
<li>将文档转为机器可处理的向量</li>
<li><strong>案例:</strong> 使用情感词典和tf-idf做情感分析（有权重）</li>
<li><strong>案例:</strong> 在线评论文本分类</li>
<li><strong>案例:</strong> 使用文本相似性识别变化(政策连续性)</li>
<li><strong>案例:</strong> Kmeans聚类算法、LDA话题模型</li>
<li>文本中的人类记忆(认知)</li>
<li>如何测量人类认知偏见(刻板印象)</li>
<li><strong>案例:</strong> 词向量模型的使用方法-豆瓣影评</li>
<li>文本分析在经管社科领域中的应用概述</li>
</ul>
<br>
<h2 id="参考文献">参考文献</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]沈艳, 陈赟 and 黄卓, 2019. 文本大数据分析在经济学和金融学中的应用: 一个文献综述. *经济学 (季刊)*, *18*(4), pp.1153-1186.
[2]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J/OL].南开管理评论:1-27[2022-04-08].http://kns.cnki.net/kcms/detail/12.1288.F.20210905.1337.002.html
[3]王伟,陈伟,祝效国,王洪伟. 众筹融资成功率与语言风格的说服性-基于Kickstarter的实证研究.*管理世界*.2016;5:81-98.
[4]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[5]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
[6]Loughran T, McDonald B. Textual analysis in accounting and finance: A survey[J]. *Journal of Accounting Research*, 2016, 54(4): 1187-1230. Author links open overlay panelComputational socioeconomics
[7]Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &#34;Uniting the tribes: Using text for marketing insight.&#34; *Journal of Marketing* 84, no. 1 (2020): 1-25.
[8]Banks, George C., Haley M. Woznyj, Ryan S. Wesslen, and Roxanne L. Ross. &#34;A review of best practice recommendations for text analysis in R (and a user-friendly app).&#34; *Journal of Business and Psychology* 33, no. 4 (2018): 445-459.
[9]Cohen, Lauren, Christopher Malloy, and Quoc Nguyen. &#34;Lazy prices.&#34; *The Journal of Finance* 75, no. 3 (2020): 1371-1415.
[10]孟庆斌, 杨俊华, 鲁冰. 管理层讨论与分析披露的信息含量与股价崩盘风险——基于文本向量化方法的研究[J]. *中国工业经济*, 2017 (12): 132-150.
[11]Wang, Quan, Beibei Li, and Param Vir Singh. &#34;Copycats vs. Original Mobile Apps: A Machine Learning Copycat-Detection Method and Empirical Analysis.&#34; *Information Systems Research* 29.2 (2018): 273-291.
[12]Hoberg, Gerard, and Gordon Phillips. 2016, Text-based network industries and endogenous product differentiation,?*Journal of Political Economy* 124, 1423-1465
[13]Loughran, Tim, and Bill McDonald. &#34;When is a liability not a liability? Textual analysis, dictionaries, and 10‐Ks.&#34; *The Journal of Finance* 66, no. 1 (2011): 35-65.
[14]Fairclough, Norman. 2003. Analysing discourse: Textual analysis for social research (Psychology Press)
[15]Grimmer, Justin, and Brandon M Stewart. 2013, Text as data: The promise and pitfalls of automatic content analysis methods for political texts, *Political analysis*21, 267-297.
[16]Markowitz, D. M., &amp; Shulman, H. C. (2021). The predictive utility of word familiarity for online engagements and funding. Proceedings of the National Academy of Sciences, 118(18).
[17]Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” Journal of Consumer Research 47, no. 5 (2021): 787-806.
[18]Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.
[19]Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &#34;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&#34; *Proceedings of the National Academy of Sciences* 119, no. 9 (2022): e2026443119.
</code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>视频分享 | 文本分析在经管研究中的应用</title>
      <link>https://hidadeng.github.io/blog/2022-09-08-dufe-text-mining-in-ms/</link>
      <pubDate>Thu, 08 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-09-08-dufe-text-mining-in-ms/</guid>
      <description>文本分析在经管研究中的应用</description>
      <content:encoded><![CDATA[<h2 id="slideshttpshidadenggithubioblog2022-09-08-dufe-text-mining-in-msslideshtml"><a href="https://hidadeng.github.io/blog/2022-09-08-dufe-text-mining-in-ms/slides.html">Slides</a></h2>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1se4y1C7MV&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<p><br><br></p>
<h2 id="背景">背景</h2>
<p><img loading="lazy" src="img/multitudes-of-content-illustration.jpeg" alt=""  />
</p>
<p>在经管研究中，往往会涉及很多文本数据的编码。但是做研究面临两个问题:</p>
<h3 id="难题1--数据量大">难题1- 数据量大</h3>
<p>量太大，以至于废人力所能及。</p>
<p>时代发展，体现在数据上的特点就是数据大爆炸，过去做经管研究，使用访谈等研究方法，收录的文本内容，规模大多停留在M级。但是现在大数据时代，研究对象相关的文本数据，G级的数据量也是很常见的。</p>
<h3 id="难题2--格式乱">难题2- 格式乱</h3>
<p>信息存储技术发展，有应用不同场景的不同数据存储格式。数据可能是pdf、txt、docx，也可能是音频、视频等转录的文件。如果快捷整理，这也是个难点。</p>
<h3 id="难题3-难编码">难题3-难编码</h3>
<p>数据量少，可以人工阅读对数据进行理解和编码。但是当数据量大到无法处理的级别后，选择何种算法、各种算法技术的优缺点如何把握，对经管学者也是一个需要攻克的的技术难题。</p>
<p><img loading="lazy" src="img/consumer_org_society.png" alt=""  />
</p>
<p>难度大，但因为文本涉及的主体错综复杂，千丝万缕，所以可以研究很多对象。如个人、组织、社会之间的交互。</p>
<p><br><br></p>
<h2 id="编码解码理论">编码解码理论</h2>
<p>斯图亚特·霍尔在《电视话语的编码和解码》提出 『编码-解码理论』。该理论形成于70年代冷战时期，冷战中不两大阵营为了维护各自的社会稳定，为了在意识形态宣传中取胜，都在宣传工作中投入了重金。</p>
<p>当时的宣传工具是单向的广播模式，媒体作为统治阶级的喉舌，要将统治阶级的偏好、价值观等进行加工，生产相应意识形态内容。</p>
<p>而普罗大众，作为内容的接受者， 一成长于该特定意识形态的社会，同时又有一定的自我意识，所以对于一个宣传内容可能会有三种反应，表里都认同、表认同里不认同、表里都不认同。</p>
<p><img loading="lazy" src="img/SenderReceiver.png" alt=""  />
</p>
<h3 id="使用文本想清楚两个问题">使用文本想清楚两个问题</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- How text reflects its Sender？
- How text impacts its Receiver？
</code></pre></div><h3 id="使用文本明晰三个角度">使用文本明晰三个角度</h3>
<p>我做的研究使用的文本数据，涉及哪些角色、作用力方向、感兴趣的内容。</p>
<ul>
<li>角色: Sender or Receiver</li>
<li>方向: Reflect or Impact</li>
<li>内容: Sender的意识(认知、偏好、&hellip;)   vs  Receiver的意识(认知、偏好、&hellip;)</li>
</ul>
<p>下面是经管领域研究部分汇总，每个学者根据自己学科研究对象，应该能在4*4的矩阵中找到自己对应的位置</p>
<p><img loading="lazy" src="img/%e7%94%9f%e4%ba%a7%e4%b8%8e%e6%b6%88%e8%b4%b9.png" alt=""  />
</p>
<blockquote>
<p>Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel. &ldquo;Uniting the tribes: Using text for marketing insight.&rdquo; Journal of Marketing 84, no. 1 (2020): 1-25.</p>
</blockquote>
<p><br><br></p>
<h2 id="人工编码与机器编码">人工编码与机器编码</h2>
<p><img loading="lazy" src="img/unstructrueddata.png" alt=""  />
</p>
<p>做研究需要有干净的数据做实证分析，最为理想的是表数据，例如excel文件，每一行代表一条记录，每一列代表一个字段。编码的作用就是将非机构化的、脏乱的数据整理为干净整洁的表数据。</p>
<p>要明确编码方法的优点和缺点，在合理的适用范围使用。对于文本数据的编码，需要理解人工和机器两种编码方式的优缺点</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th>分析方法</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">人工编码</td>
<td>质性（扎根）</td>
<td>少量数据，深刻洞见。</td>
<td>难以应对大数据；<br>编码标准不统一；</td>
</tr>
<tr>
<td style="text-align:left">机器编码</td>
<td>词频、向量相似度、向量距离</td>
<td>标准如一;<br>适合大规模文本挖掘；</td>
<td>需要破坏文本的结构，<br>丧失了部分信息量</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="机器编码-将文本转为数字或向量">机器编码-将文本转为数字或向量</h2>
<ul>
<li>
<p>符号法(每个词对应一个数字)</p>
<ul>
<li>词典(词频)法</li>
<li>词袋法、TF-IDF</li>
</ul>
</li>
<li>
<p>词嵌入(每个词对应一个向量)</p>
</li>
</ul>
<p>符号法算法假设词语彼此是语义不相关的，目的是把 <strong>文本</strong> 转为某个数字或<strong>向量</strong>。</p>
<p>而词嵌入算法假设不同的词语是由n维个语义组成的线性组合，目的是把 <strong>词语</strong> 转为<strong>向量</strong>。</p>
<br>
<h3 id="符号法">符号法</h3>
<p>符号法就是数某个词或某类词的出现次数(或占比)。符合法是计算机NLP领域的专业叫法，在经管社科领域，最常见的文本分析软件<a href="https://hidadeng.github.io/blog/liwc_python_text_mining/">LIWC</a>其实也是符号法。而LIWC全(Linguistic Inquiry and Word Count，即语义查询与词频统计。</p>
<p><img loading="lazy" src="img/symbol-representation-1.png" alt=""  />
</p>
<h3 id="符号法的应用">符号法的应用</h3>
<table>
<thead>
<tr>
<th>概念</th>
<th>测量方法</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>认真(努力)</strong></td>
<td>测量文本中词语的个数</td>
</tr>
<tr>
<td><strong>情感</strong></td>
<td>使用情感词典，统计文本中正面词占比</td>
</tr>
<tr>
<td><strong>可读性</strong></td>
<td>文本中高难度(或专业性)词占比</td>
</tr>
<tr>
<td><strong>客观性</strong></td>
<td>文本中某个值的方差，如情感<br>- A<code>产品不错， 包装破损， 态度很好， 综合还是推荐大家购买!</code> [5, 1, 5, 4]<br>- B<code>产品垃圾，使用垃圾， 包装破损， 差评!!</code> [1,  1,  1,  1]<br>A的方差更大，更客观</td>
</tr>
<tr>
<td><strong>相似性(政策稳定性)</strong></td>
<td>cosine(text_vector1, text_vector2)</td>
</tr>
<tr>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>
<br>
<h3 id="词嵌入">词嵌入</h3>
<p>词嵌入技术有 Word2Vec、Glove，这类技术是挖掘出每个词的上下文语境，通俗的说法就是让计算机，对同样的文章数据，做千万次、上亿次完形填空。这样每个词语都有独特的上下文语义，并以n维向量形式表示，所以词嵌入也可以称之为词向量。</p>
<p><strong>向量模型有近义词相近、概念类似的平行两个特点</strong>。分别举几个例子，方便大家理解。</p>
<p>语义空间是n维，为了便于理解，将其压缩至二维空间。中学的向量大家都比较熟悉，在二维坐标中空间中，两个点的连线可以组成新的向量，相同的向量是平行的。</p>
<p>而在下图的2维语义空间中，good、best语义更接近，所以空间距离更近。同理bad、worst更近。</p>
<p>而vector(good, best)、vector(bad, worst)这两个向量均表示<code>原形-&gt;最高级</code>, 语义向量会近似平行。</p>
<p>同理， vector(good, bad)、 vector(best, worst)两个向量表示 <code>好-&gt;差</code>，语义向量也会近似平行。</p>
<p><img loading="lazy" src="img/embeddings-based.png" alt=""  />
</p>
<br>
<h3 id="词嵌入与认知">词嵌入与认知</h3>
<p>刚刚词嵌入的语义空间中的几个例子，其实就体现了语言的记忆。语义记录了使用该语言的人的记忆。不同的组织，对于同一种概念，会有不同的偏好。例如， Nature2022使用大规模语料数据训练出的词向量，发现语言中残存着人类的某些认知记忆。</p>
<p>通过构建概念词组对儿，在空间中投影，就可以挖掘出词语的在该概念中的分值。例如，使用</p>
<ul>
<li>SMALL = [small, tiny, little&hellip;]</li>
<li>BIG = [big, mega, large&hellip;]</li>
</ul>
<p>每个词都是一个n维的向量，SMALL或BIG都能计算出一个均值向量。大家记得中学的向量投影不，Nature2022就使用这个朴素的方法测量每个动物名称所蕴含的人类尺寸认知。</p>
<p><img loading="lazy" src="img/Concept_Words_Project.png" alt=""  />
</p>
<blockquote>
<p>Grand, G., Blank, I.A., Pereira, F. and Fedorenko, E., 2022. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Nature Human Behaviour, pp.1-13.</p>
</blockquote>
<br>
<h3 id="技术对比">技术对比</h3>
<p>这里做个表格对比，大家自己感受下三种技术的异同。</p>
<table>
<thead>
<tr>
<th>技术</th>
<th>技术</th>
<th>维度类比</th>
<th>任务</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>符号法-字典</strong>（词频）</td>
<td>数个数</td>
<td>原子</td>
<td>统计每句话里的名词个数</td>
<td>sent_num1 = 2<br>sent_num2 = 1</td>
</tr>
<tr>
<td><strong>符号法-词袋</strong></td>
<td>bag of words<br>one-hot<br>Tf-idf</td>
<td>分子</td>
<td>转化为词向量, 计算两个句子相似度。</td>
<td>vec1 = [1, 1, 1, 1, 1, 0]<br>vec2 = [0, 1, 0, 1, 0, 1]<br>similarity = cosine(vec1, vec2)</td>
</tr>
<tr>
<td><strong>词嵌入</strong></td>
<td>word2vec、<br>glove等</td>
<td>中子、质子、电子</td>
<td>词语相似度。(语义上大小相近，方向相反; 态度、偏见)</td>
<td>mom = [0.2, 0.7, 0.1]<br/>dad   = [0.3, 0.5, -0.2]</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="经管-文本分析-文献">经管-文本分析-文献</h2>
<p>在这里我把技术细分为词频、词袋、w2v建词典、w2v认知变迁四个维度，整理了经管7篇论文。大家可以阅读这7篇论文，掌握文本分析的应用场景。</p>
<table>
<thead>
<tr>
<th>文献</th>
<th>定性</th>
<th>词频</th>
<th>词袋</th>
<th>W2V建词典</th>
<th>W2V认知变迁</th>
</tr>
</thead>
<tbody>
<tr>
<td>王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</td>
<td>Y</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://hidadeng.github.io/blog/jcr_concreteness_computation/">语言具体性如何影响顾客满意度</a><br>Packard, Grant, and Jonah Berger. “How concrete language shapes customer satisfaction.” <em>Journal of Consumer Research</em> 47, no. 5 (2021): 787-806.</td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://hidadeng.github.io/blog/2019-12-08-lazy-prices/">文本相似度</a><br>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>胡楠, 薛付婧 and 王昊楠, 2021. <a href="https://hidadeng.github.io/blog/text_mining_in_2021_management_world/">管理者短视主义</a>影响企业长期投资吗———基于文本分析和机器学习. <em>管理世界</em>, <em>37</em>(5), pp.139-156.</td>
<td></td>
<td>Y</td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>Kai Li, Feng Mai, Rui Shen, Xinyan Yan, <a href="https://github.com/MS20190155/Measuring-Corporate-Culture-Using-Machine-Learning">Measuring Corporate Culture Using Machine Learning</a>, The Review of Financial Studies, 2020</td>
<td></td>
<td></td>
<td>Y</td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>女性就职高管改变组织内性别偏见<br>Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 119, no. 9 (2022): e2026443119.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
<tr>
<td>使用词嵌入技术，量化近百年以来性别和族群的刻板印象<br>Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. &ldquo;Word embeddings quantify 100 years of gender and ethnic stereotypes.&rdquo; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="案例">案例</h2>
<h3 id="案例1-众筹语言风格">案例1-众筹语言风格</h3>
<p>王伟, 陈伟, 祝效国 and 王洪伟, 2016. 众筹融资成功率与语言风格的说服性&ndash;基于 Kickstarter 的实证研究. <em>管理世界</em>, (5), pp.81-98.</p>
<blockquote>
<p>众筹融资效果决定着众筹平台的兴衰。 众筹行为很大程度上是由投资者的主观因素决定的，而影响主观判断的一个重要因素就是语言的说服性。 而这又是一种典型的用 户产生内容（UGC），项目发起者可以采用任意类型的语言风格对项目进行描述。 不同的语 言风格会改变投资者对项目前景的感知，进而影响他们的投资意愿。 首先，依据 Aristotle 修 辞三元组以及 Hovland 说服模型，采用扎根理论，将众筹项目的语言说服风格分为 5 类：诉诸可信、诉诸情感、诉诸逻辑、诉诸回报和诉诸夸张。</p>
<p>然后，<strong>借助文本挖掘方法，构建说服风格语料库，并对项目摘要进行分类。</strong></p>
<p>最后，建立语言说服风格对项目筹资影响的计量模型，并对 <strong>Kickstarter 平台上的 128345 个项目进行实证分析</strong>。 总体来说，由于项目性质的差异，不同 的项目类别对应于不同的最佳说服风格。</p>
</blockquote>
<p><img loading="lazy" src="img/%e4%bc%97%e7%ad%b9-%e7%a7%8d%e5%ad%90%e8%af%8d.png" alt=""  />

<img loading="lazy" src="img/%e4%bc%97%e7%ad%b9-%e6%b5%81%e7%a8%8b%e5%9b%be.png" alt=""  />
</p>
<br>
<h3 id="案例2-山寨-vs-原创">案例2 山寨 vs 原创</h3>
<p>Wang, Quan, Beibei Li, and Param Vir Singh. &ldquo;Copycats vs. original mobile apps: A machine learning copycat-detection method and empirical analysis.&rdquo; Information Systems Research 29, no. 2 (2018): 273-291.</p>
<blockquote>
<p><strong>进行此类研究的主要威慑因素是缺乏一种客观的方法来识别应用程序是模仿者还是原创者。通过结合自然语言处理，潜在语义分析，基于网络的聚类和图像分析等机器学习技术，我们提出了一种将应用识别为原创app或模仿app，可检测两种模仿者的方法：欺骗性和非欺骗性。</strong></p>
<p>根据检测结果，我们进行了经济计量分析，以确定五年间在iOS App Store中发布的<strong>5,141个开发人员的10,100个动作游戏应用程序</strong>样本中，模仿app对原创app需求的影响。我们的结果表明，特定模仿者对原始应用需求的影响取决于模仿者的质量和欺骗程度。高质量的非欺骗性复制品会对原件产生负面影响。相比之下，低质量，欺骗性的模仿者正面影响了对原创app的需求。</p>
<p>结果表明，从总体上讲，模仿app对原创app需求的影响在统计上是微不足道的。<strong>我们的研究通过提供一种识别模仿app的方法</strong>，并提供模仿app对原创app需求影响的证据，为越来越多的移动应用消费文献做出了贡献。</p>
</blockquote>
<p><img loading="lazy" src="img/copycat.png" alt=""  />
</p>
<br>
<h3 id="案例3-lazy-prices文本相似性">案例3 Lazy prices文本相似性</h3>
<p>Cohen, L., Malloy, C. and Nguyen, Q., 2020. <a href="https://hidadeng.github.io/blog/2019-12-08-lazy-prices/">Lazy prices</a>. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</p>
<blockquote>
<p>之前的研究认为，尽管投资者一次对包含重大变化的财务报表的发布作出了迅时反应，但随着时间的流逝，这种公告作用是会减弱的(Brown and Tucker, 2011 and Feldman et al., 2010)。这表示10-K报告会随着时间推移，信息价值大打折扣。尽管我们复现了这个事实，即与常规文件的变更没有重大的公告效应，但我们认为，前人的研究忽略了更重要部分(如MD&amp;A)对对资产价格的影响。</p>
</blockquote>
<blockquote>
<p>确切的说，<strong>并不是报告的披露效应的信息价值变低了，而是投资者越来越难以发现报告中微妙的信息变化， 比如因为报告变得越来越冗杂。投资者只有看到某些新闻后，才会逐渐意识到之前公司报告内容变化的的真正价值</strong>。</p>
<p>使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对<strong>报告进行积极更改</strong>时，这种行为<strong>蕴含着</strong>公司未来运营的<strong>重要信号</strong>。</p>
<p><strong>财务报告的语言和结构的变化也对公司的未来收益产生重大影响</strong>：做空&quot;变化&quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。</p>
</blockquote>
<p><img loading="lazy" src="img/lazy-prices-1.png" alt=""  />

<img loading="lazy" src="img/lazy-prices-2.png" alt=""  />
</p>
<br>
<h3 id="案例3-女性就职高管改变组织内性别刻板印象-pnas2022">案例3-女性就职高管改变组织内性别刻板印象 PNAS2022</h3>
<p>Lawson, M. Asher, Ashley E. Martin, Imrul Huda, and Sandra C. Matz. &ldquo;Hiring women into senior leadership positions is associated with a reduction in gender stereotypes in organizational language.&rdquo; <em>Proceedings of the National Academy of Sciences</em> 119, no. 9 (2022): e2026443119.</p>
<blockquote>
<p>女性在领导职位上的代表性仍然不足。这种代表性不足至少部分是由将男性与成就导向的代理特征（例如，自信和果断）联系起来的性别刻板印象驱动的。这些刻板印象在语言中得到表达和延续，女性被描述的方式比男性少。目前的研究表明，任命女性担任高层管理人员可以减轻这些以语言表达的根深蒂固的刻板印象。我们使用自然语言处理技术分析了超过 <strong>43,000 份包含 12.3 亿字的文档，发现聘用女性首席执行官和董事会成员与组织使用语言的变化有关，因此女性的语义变得更类似于代理的语义</strong>。换句话说，雇用女性担任领导职务有助于将女性与对领导成功至关重要的特征联系起来。重要的是，我们的研究结果表明，通过增加女性代表来改变组织语言可能会为女性提供摆脱双重束缚的途径：当女性领导人被任命担任权力职位时，女性与代理的积极方面（例如，独立和自信）在语言上，不以减少与社区的联系（例如，善良和关怀）为代价。总而言之，我们的研究结果表明，女性代表不仅是目的，而且是系统地改变阴险的性别刻板印象并克服女性被认为是有能力或可爱的权衡的一种手段。</p>
<p>本文使用的词向量， 刻画研究对象的文化认知，是依对象依时间而变化的。</p>
</blockquote>
<p><img loading="lazy" src="img/hiring_women.png" alt=""  />
</p>
<br>
<h3 id="案例4--使用词嵌入技术量化近百年以来性别和族群的刻板印象-pnas2018">案例4- 使用词嵌入技术，量化近百年以来性别和族群的刻板印象 PNAS2018</h3>
<p>{{ &lt; bilibili BV1b4411X7i1 &gt;}}</p>
<br>
<h2 id="关于合作与交流">关于合作与交流</h2>
<p>如果正在推进的项目，需要用到文本分析，欢迎交流与合作 ~</p>
<p>可加微信372335839， 备注【姓名-学校-专业】, 说明来意。如果想系统获取技术细节，课程 <a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a> 内都有的技术代码和讲解，欢迎了解。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>ManagementScience | 使用网络算法识别创新的颠覆性与否</title>
      <link>https://hidadeng.github.io/blog/2022-09-07-management-science-disrupt-science-and-technology/</link>
      <pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/blog/2022-09-07-management-science-disrupt-science-and-technology/</guid>
      <description>The CD index is a new approach to finding important points in evolving networks. When applied to large-scale data sets like U.S. patent citations, the index is useful for identifying influential innovations and other features of technological change.</description>
      <content:encoded><![CDATA[


<p>颠覆式创新是一个很火的概念，在创新创业、科学学等研究中，每个专利、论文的正文中都会引用关系，而引用关系会构成一个引用网络。</p>
<p>那么创新如何从网络形态进行区分，如何计算网络节点的创新程度，本文列举两篇与此相关的论文，分别是 Management science 和 Science 。</p>
<p><br><br></p>
<div id="文献摘要" class="section level2">
<h2>文献摘要</h2>
<p><strong>Funk, Russell J., and Jason Owen-Smith. “A dynamic network measure of technological change.” <em>Management science</em> 63, no. 3 (2017): 791-817.</strong></p>
<p>该文使用网络分析方法研究技术变革，论文认为 <strong>颠覆性的新发明，通过将发明者的注意力转移到或远离这些发明所依赖的知识，来重塑相互关联的技术网络。即更广的视野或更久远的视角，往往有利于颠覆性创新的产生</strong>。<strong>基于该思路，本文开发了新发明的颠覆性与否的计算指标cdindex</strong>。我们将这些指标应用于大学研究商业化的分析，并发现 <strong>联邦研究资金推动校园产生颠覆性创新，而商业联系会有利于巩固现状的创新</strong>。通过量化新技术，我们提出的指数允许基于专利的创新研究捕捉概念上重要的现象， 这些现象无法通过既定措施检测到。该测量方法提供了支持创新、创业、技术战略、科学政策和社会网络理论研究的理论发展的经验见解。</p>
<blockquote>
<p>Abstract: This article outlines a network approach to the study of technological change. We propose that new inventions reshape networks of interlinked technologies by shifting inventors’ attention to or away from the knowledge on which those inventions build. Using this approach, we develop novel indexes of the extent to which a new invention consolidates or destabilizes existing technology streams. We apply these indexes in analyses of university research commercialization and ﬁnd that, although federal research funding pushes campuses to create inventions that are more destabilizing, deeper commercial ties lead them to produce technologies that consolidate the status quo. By quantifying the eﬀects that new technologies have on their predecessors, the indexes we propose allow patent-based studies of innovation to capture conceptually important phenomena that are not detectable with established measures. The measurement approach presented here oﬀers empirical insights that support theoretical development in studies of innovation, entrepreneurship, technology strategy, science policy, and social network theory.</p>
</blockquote>
<p><br></p>
<p><strong>Wu, Lingfei, Dashun Wang, and James A. Evans. “Large teams develop and small teams disrupt science and technology.” Nature 566, no. 7744 (2019): 378-382.</strong></p>
<p>当今科学和技术最普遍的趋势之一是各个领域的大型团队的增长，因为孤独的研究人员和小型团队的流行程度正在减少 。团队规模的增加归因于科学活动的专业化、通信技术的改进 或需要跨学科解决方案的现代问题的复杂性。团队规模的这种转变引发了一个问题，即大团队所产生的科技特征是否以及如何不同于小团队。分析了 1954-2014 年期间超过 6500 万篇论文、专利和软件产品，证明在此期间，<strong>较小的团队倾向于将拉长到更大的时间尺度，借鉴过去，用新的想法和机会来颠覆科学和技术；而较大的团队倾向于聚焦于当前流行的，完善当前现有的</strong>。不论团队大小，均对于蓬勃发展的科学技术生态至关重要，并表明，为实现这一目标，科学政策应旨在支持团队规模的多样性。</p>
<blockquote>
<p>Abstract: One of the most universal trends in science and technology today is the growth of large teams in all areas, as solitary researchers and small teams diminish in prevalence. Increases in team size have been attributed to the specialization of scientific activities,
improvements in communication technology, or the complexity
of modern problems that require interdisciplinary solutions.This shift in team size raises the question of whether and how the character of the science and technology produced by large teams differs from that of small teams. Here we analyse more than 65 million papers, patents and software products that span the period 1954–2014, and demonstrate that across this period smaller teams have tended to disrupt science and technology with new ideas and opportunities, whereas larger teams have tended to develop existing ones. Work from larger teams builds on morerecent and popular developments, and attention to their work comes
immediately. By contrast, contributions by smaller teams search more deeply into the past, are viewed as disruptive to science and technology and succeed further into the future—if at all. Observed differences between small and large teams are magnified for higherimpact work, with small teams known for disruptive work and large teams for developing work. Differences in topic and research design
account for a small part of the relationship between team size and disruption; most of the effect occurs at the level of the individual, as people move between smaller and larger teams. These results demonstrate that both small and large teams are essential to a flourishing ecology of science and technology, and suggest that, to achieve this, science policies should aim to support a diversity of team sizes.</p>
</blockquote>
<p><br><br></p>
</div>
<div id="算法对比" class="section level2">
<h2>算法对比</h2>
<p>我没阅读两篇论文，仅就颠覆性与否的计算方法和图例，感觉算法实现差不多。</p>
<div class="figure">
<img src="img/cdindex-managent_science_2017.png" alt="" />
<p class="caption">上图为2017年Management Science的插图</p>
</div>
<p><br></p>
<div class="figure">
<img src="img/disruption_nature_2019.png" alt="" />
<p class="caption">上图为2019年Nature的插图</p>
</div>
<p><br><br></p>
</div>
<div id="代码数据" class="section level2">
<h2>代码数据</h2>
<p>下面分别为Management2017和Nature2019的主页，均含数据和代码。</p>
<p><a href="http://russellfunk.org/cdindex/"><img src="img/cdindex-homepage.png" /></a></p>
<p><br></p>
<p><a href="https://lingfeiwu.github.io/smallTeams/"><img src="img/nature2019-disrupt-homepage.png" /></a></p>
<p><br><br></p>
</div>
<div id="算法实现" class="section level2">
<h2>算法实现</h2>
<p>按照时间优先原则，本文就只分享Management2017论文作者Funk, Russell开源了cdindex库 (开发语言C和Python) ，安装</p>
<p><br></p>
<pre><code>pip3 install cdindex</code></pre>
<p>将Management2017 cdindex算法图 标注为如下图， 下图中左右两个网络节点是相同的，只需构造一套节点，两套边数据即可完成实验。</p>
<p><img src="img/cdindex-managent_science_2017_demo.png" /></p>
<p><br></p>
<p>我们就直接上代码</p>
<pre class="python"><code>import cdindex
import datetime

#节点，理解为专利号或者论文doi号；同时节点有先后时间属性
vertices = [{&quot;name&quot;: &quot;x1&quot;, &quot;time&quot;: datetime.datetime(1990, 1, 1)},
           {&quot;name&quot;: &quot;x2&quot;, &quot;time&quot;: datetime.datetime(1990, 1, 1)},
           {&quot;name&quot;: &quot;x3&quot;, &quot;time&quot;: datetime.datetime(1990, 1, 1)},
           {&quot;name&quot;: &quot;x4&quot;, &quot;time&quot;: datetime.datetime(1990, 1, 1)},
        
           {&quot;name&quot;: &quot;y&quot;, &quot;time&quot;: datetime.datetime(1991, 1, 1)},
          
           {&quot;name&quot;: &quot;z1&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)},
           {&quot;name&quot;: &quot;z2&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)},
           {&quot;name&quot;: &quot;z3&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)},
           {&quot;name&quot;: &quot;z4&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)},
           {&quot;name&quot;: &quot;z5&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)},
           {&quot;name&quot;: &quot;z6&quot;, &quot;time&quot;: datetime.datetime(1995, 1, 1)}]
           
    
#edges_1边关系
#edges_1中的y为颠覆型
edges_1 = [{&quot;source&quot;: &quot;z1&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z2&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z3&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z4&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z5&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z6&quot;, &quot;target&quot;: &quot;y&quot;},
           
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x1&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x2&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x3&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x4&quot;}]


#edges_2边关系 
#edges_2中的y为巩固型
edges_2 = [{&quot;source&quot;: &quot;z1&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z2&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z3&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z4&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z5&quot;, &quot;target&quot;: &quot;y&quot;},
           {&quot;source&quot;: &quot;z6&quot;, &quot;target&quot;: &quot;y&quot;},
           
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x1&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x2&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x3&quot;},
           {&quot;source&quot;: &quot;y&quot;, &quot;target&quot;: &quot;x4&quot;},
          
          {&quot;source&quot;: &quot;z1&quot;, &quot;target&quot;: &quot;x1&quot;},
          {&quot;source&quot;: &quot;z2&quot;, &quot;target&quot;: &quot;x1&quot;},
          {&quot;source&quot;: &quot;z3&quot;, &quot;target&quot;: &quot;x2&quot;},
           
          {&quot;source&quot;: &quot;z4&quot;, &quot;target&quot;: &quot;x3&quot;},
          {&quot;source&quot;: &quot;z5&quot;, &quot;target&quot;: &quot;x3&quot;},
          {&quot;source&quot;: &quot;z5&quot;, &quot;target&quot;: &quot;x4&quot;},
          {&quot;source&quot;: &quot;z6&quot;, &quot;target&quot;: &quot;x4&quot;}]



# 构建两个网络
graph1 = cdindex.Graph() #颠覆型
graph2 = cdindex.Graph() #发展型

# 添加节点
for vertex in vertices:
    graph1.add_vertex(vertex[&quot;name&quot;], cdindex.timestamp_from_datetime(vertex[&quot;time&quot;]))
    graph2.add_vertex(vertex[&quot;name&quot;], cdindex.timestamp_from_datetime(vertex[&quot;time&quot;]))

# 添加引用关系
for edge in edges_1:
    graph1.add_edge(edge[&quot;source&quot;], edge[&quot;target&quot;])
for edge in edges_2:
    graph2.add_edge(edge[&quot;source&quot;], edge[&quot;target&quot;])
    
    
#y研究发布后1825天内，引用y的论文(专利)列入网络。
t_delta = int(datetime.timedelta(days=1825).total_seconds())

#计算cdindex得分
score1 = graph1.cdindex(&quot;y&quot;, t_delta)
score2 = graph2.cdindex(&quot;y&quot;, t_delta)

print(&#39;左侧-网络中的y节点的cdinex得分: {}, 节点y 为颠覆性创新&#39;.format(score1))</code></pre>
<pre><code>## 左侧-网络中的y节点的cdinex得分: 1.0, 节点y 为颠覆性创新</code></pre>
<p><br></p>
<pre class="python"><code>print(&#39;右侧-网络中的y节点的cdinex得分: {}, 节点y 为发展性创新&#39;.format(score2))</code></pre>
<pre><code>## 右侧-网络中的y节点的cdinex得分: -1.0, 节点y 为发展性创新</code></pre>
<p><br><br></p>
</div>
<div id="cdindex" class="section level2">
<h2>cdindex</h2>
<p>对比Python的结果，与论文计算过程，完全一致。cdindex内部实现我不太熟悉，如果想了解cdindex内部实现，可前往 <a href="https://github.com/russellfunk/cdindex" class="uri">https://github.com/russellfunk/cdindex</a> 阅读cdindex库的源码。
<img src="img/cdindex-managent_science_2017.png" /></p>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>实战 | 构建基于客户细分的 K-Means 聚类算法！</title>
      <link>https://hidadeng.github.io/blog/customer_segment_with_kmeans/</link>
      <pubDate>Thu, 09 Jun 2022 18:43:10 +0600</pubDate>
      
      <guid>/blog/customer_segment_with_kmeans/</guid>
      <description>客群细分对于企业了解目标受众非常重要。根据受众群体的不同，我们可以给采取不同的营销策略。目前有许多无监督的机器学习算法可以帮助公司识别他们的用户群并创建消费群体。</description>
      <content:encoded><![CDATA[<p>客群细分对于企业了解目标受众非常重要。根据受众群体的不同，我们可以给采取不同的营销策略。目前有许多无监督的机器学习算法可以帮助公司识别他们的用户群并创建消费群体。</p>
<p>在本文中，我将分享一种目前比较流行的 K-Means 聚类的无监督学习技术。K-Means的目标是将所有可用的数据分组为彼此不同的不重叠的子组。K-Means聚类是数据科学家用来帮助公司进行客户细分的常用技术。</p>
<p>在本文中，你将了解以下内容：</p>
<ul>
<li>K-Means聚类的数据预处理</li>
<li>从头构建K-Means聚类算法</li>
<li>用于评估聚类模型性能的指标</li>
<li>可视化构建簇类</li>
<li>簇类构建的解读与分析</li>
</ul>
<h2 id="代码下载">代码下载</h2>
<p><a href="customer_segment_with_kmeans.zip">点击下载</a></p>
<br>
<h2 id="预备知识">预备知识</h2>
<p>在开始之前安装以下库：pandas、numpy、matplotlib、seaborn、sciket learn、kneed。完成后，我们就可以开始制作模型了！</p>
<p>本文中要的数据集可以文末下载，运行以下代码行以导入必要的库并读取数据集：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Mall_Customers.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CustomerID</th>
      <th>Gender</th>
      <th>Age</th>
      <th>Annual Income (k$)</th>
      <th>Spending Score (1-100)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Male</td>
      <td>19</td>
      <td>15</td>
      <td>39</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>Male</td>
      <td>21</td>
      <td>15</td>
      <td>81</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Female</td>
      <td>20</td>
      <td>16</td>
      <td>6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Female</td>
      <td>23</td>
      <td>16</td>
      <td>77</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>Female</td>
      <td>31</td>
      <td>17</td>
      <td>40</td>
    </tr>
  </tbody>
</table>
</div>
<p>数据集中有五个变量。CustomerID是数据集中每个客户的唯一标识符，我们可以删除这个变量。它没有为我们提供任何有用的集群信息。由于 gender 是一个分类变量，它需要编码并转换成数字。</p>
<p>在输入模型之前，其他所有变量都将按正态分布进行缩放。我们将标准化这些变量，平均值为0，标准偏差为1。</p>
<br>
<h2 id="标准化变量">标准化变量</h2>
<p>首先，让我们标准化数据集中的所有变量，使它们在相同的范围内。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">col_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Annual Income (k$)&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Spending Score (1-100)&#39;</span><span class="p">]</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col_names</span><span class="p">]</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">scaled_features</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">col_names</span><span class="p">)</span>
<span class="n">scaled_features</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Annual Income (k$)</th>
      <th>Age</th>
      <th>Spending Score (1-100)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.738999</td>
      <td>-1.424569</td>
      <td>-0.434801</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.738999</td>
      <td>-1.281035</td>
      <td>1.195704</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.700830</td>
      <td>-1.352802</td>
      <td>-1.715913</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.700830</td>
      <td>-1.137502</td>
      <td>1.040418</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.662660</td>
      <td>-0.563369</td>
      <td>-0.395980</td>
    </tr>
  </tbody>
</table>
</div>
<p>我们可以看到所有的变量都被转换了，现在都以零为中心。</p>
<br>
<h2 id="热编码">热编码</h2>
<p>变量&quot;gender&quot;是分类变量，我们需要把它转换成一个数值变量，可以用pd.get_dummies()来处理。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">gender</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Gender&#39;</span><span class="p">]</span>
<span class="n">newdf</span> <span class="o">=</span> <span class="n">scaled_features</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gender</span><span class="p">)</span>

<span class="n">newdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">newdf</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix_sep</span><span class="o">=</span><span class="s1">&#39;_&#39;</span><span class="p">,</span> <span class="n">dummy_na</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">newdf</span> <span class="o">=</span> <span class="n">newdf</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Gender_Male&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">newdf</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Annual Income (k$)</th>
      <th>Age</th>
      <th>Spending Score (1-100)</th>
      <th>Gender_Female</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.738999</td>
      <td>-1.424569</td>
      <td>-0.434801</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.738999</td>
      <td>-1.281035</td>
      <td>1.195704</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1.700830</td>
      <td>-1.352802</td>
      <td>-1.715913</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.700830</td>
      <td>-1.137502</td>
      <td>1.040418</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-1.662660</td>
      <td>-0.563369</td>
      <td>-0.395980</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<p>可以看到，性别变量已经发生了变化，从数据框中删除了“Gender_Male”。这是因为不需要再保留变量了。</p>
<br>
<h2 id="建立聚类模型">建立聚类模型</h2>
<p>让我们构建一个 K-means 聚类模型，并将其拟合到数据集中的所有变量上，我们用肘部图可视化聚类模型的性能，它会告诉我们在构建模型时使用的「最佳聚类数」。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">SSE</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">cluster</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">newdf</span><span class="p">)</span>
    <span class="n">SSE</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="c1"># converting the results into a dataframe and plotting them</span>

<span class="n">frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Cluster&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="s1">&#39;SSE&#39;</span><span class="p">:</span><span class="n">SSE</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">frame</span><span class="p">[</span><span class="s1">&#39;Cluster&#39;</span><span class="p">],</span> <span class="n">frame</span><span class="p">[</span><span class="s1">&#39;SSE&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of clusters&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Inertia&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>Text(0, 0.5, 'Inertia')
</code></pre>
<p>​ <br>
<img loading="lazy" src="output_9_1.png" alt="png"  />

​</p>
<p>根据上面的「肘部图」，我们可以看到最佳聚类数为「4」</p>
<br>
<h2 id="轮廓系数">轮廓系数</h2>
<p>轮廓系数或轮廓分数是用于评估该算法创建的簇的质量的方法。轮廓分数在-1到+1之间。轮廓分数越高，模型越好。轮廓分数度量同一簇中所有数据点之间的距离。这个距离越小，轮廓分数就越好。</p>
<p>让我们计算一下我们刚刚建立的模型的轮廓分数：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span>
<span class="c1"># First, build a model with 4 clusters</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;k-means++&#39;</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">newdf</span><span class="p">)</span>

<span class="c1"># Now, print the silhouette score of this model</span>

<span class="nb">print</span><span class="p">(</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">newdf</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))</span>
</code></pre></div><pre><code>0.35027020434653977
</code></pre>
<p>轮廓线得分约为「0.35」。这是一个不错的模型，但我们可以做得更好，并尝试获得更高的簇群分离。</p>
<p>在我们尝试这样做之前，让我们将刚刚构建的聚类可视化，以了解模型的运行情况：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">newdf</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:])</span>

<span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;label&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">clusters</span>
 
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> 
           <span class="n">df</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> 
           <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> 
           <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> 
           <span class="n">df</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> 
           <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> 
           <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> 
           <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">185</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_13_0.png" alt="png"  />

​</p>
<p>从上图可以看出，簇类分离度不是很大。红点与蓝色混合，绿色与黄色重叠，这与轮廓分数一起向我们表明该模型表现不佳。现在，让我们创建一个比这个模型具有更好集群可分离性的新模型。</p>
<br>
<h2 id="建立聚类模型2">建立聚类模型2</h2>
<p>对于这个模型，让我们做一些特征选择。我们可以使用一种叫做主成分分析（PCA）的技术。</p>
<p>PCA 是一种帮助我们降低数据集维数的技术。现在，让我们在数据集上运行PCA：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">principalComponents</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">newdf</span><span class="p">)</span>

<span class="n">features</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;PCA features&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;variance %&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="n">PCA_components</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">principalComponents</span><span class="p">)</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_15_0.png" alt="png"  />

​</p>
<p>这张图表显示了每个主成分分析的组成，以及它的方差。我们可以看到前两个主成分解释了大约70%的数据集方差。我们可以将这两个组件输入到模型中再次构建模型，并选择要使用的簇的数量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">inertias</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">PCA_components</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">inertias</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">inertias</span><span class="p">,</span> <span class="s1">&#39;-o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;number of clusters, k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;inertia&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">ks</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="output_17_0.png" alt="png"  />

​</p>
<p>同样，看起来「最佳簇数是4」。我们可以用4个簇来计算此模型的轮廓分数：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">PCA_components</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">])</span>

<span class="c1"># silhouette score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">PCA_components</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))</span>
</code></pre></div><pre><code>0.6025604455573874
</code></pre>
<p>这个模型的轮廓分数是「0.42」，这比我们之前创建的模型要好。我们可以像前面一样可视化此模型：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">clusters</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">PCA_components</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;label&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">clusters</span>
 
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">21</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="o">.</span><span class="n">Age</span><span class="p">[</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="o">.</span><span class="n">Age</span><span class="p">[</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="o">.</span><span class="n">Age</span><span class="p">[</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newdf</span><span class="o">.</span><span class="n">Age</span><span class="p">[</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Annual Income (k$)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> <span class="n">newdf</span><span class="p">[</span><span class="s2">&#34;Spending Score (1-100)&#34;</span><span class="p">][</span><span class="n">newdf</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">185</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p>​ <br>
<img loading="lazy" src="output_21_0.png" alt="png"  />

​</p>
<br> 
<h2 id="模型1与模型2">模型1与模型2</h2>
<p>让我们比较一下这个模型和第一个模型的聚类可分性：</p>
<p>第二个模型中的簇比第一个模型中的簇分离得好得多。此外，第二个模型的轮廓分数要高得多。基于这些原因，我们可以选择第二个模型进行分析。</p>
<br>
<h2 id="聚类分析">聚类分析</h2>
<p>首先，让我们将簇类映射回数据集，并查看数据帧。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Mall_Customers.csv&#39;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;CustomerID&#39;</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># map back clusters to dataframe</span>

<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">PCA_components</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">])</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">frame</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred</span>
<span class="n">frame</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Gender</th>
      <th>Age</th>
      <th>Annual Income (k$)</th>
      <th>Spending Score (1-100)</th>
      <th>cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Male</td>
      <td>19</td>
      <td>15</td>
      <td>39</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Male</td>
      <td>21</td>
      <td>15</td>
      <td>81</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Female</td>
      <td>20</td>
      <td>16</td>
      <td>6</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Female</td>
      <td>23</td>
      <td>16</td>
      <td>77</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Female</td>
      <td>31</td>
      <td>17</td>
      <td>40</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
<p>数据帧中的每一行现在都分配给一个集群。要比较不同群集的属性，请查找每个群集上所有变量的平均值：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">avg_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;cluster&#39;</span><span class="p">],</span> <span class="n">as_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">avg_df</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cluster</th>
      <th>Age</th>
      <th>Annual Income (k$)</th>
      <th>Spending Score (1-100)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>25.521739</td>
      <td>26.304348</td>
      <td>78.565217</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>51.681818</td>
      <td>62.125000</td>
      <td>33.750000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>32.904762</td>
      <td>84.380952</td>
      <td>80.500000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>26.659574</td>
      <td>53.106383</td>
      <td>40.042553</td>
    </tr>
  </tbody>
</table>
</div>
<p>如果我们将这些簇可视化，我们可以更容易地解释它们。运行以下代码以获得每个变量的不同可视化效果：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;Age&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">avg_df</span><span class="p">)</span>
</code></pre></div><pre><code>&lt;AxesSubplot:xlabel='cluster', ylabel='Age'&gt;
</code></pre>
<p>​ <br>
<img loading="lazy" src="output_27_1.png" alt="png"  />

​</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;Spending Score (1-100)&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">avg_df</span><span class="p">)</span>
</code></pre></div><pre><code>&lt;AxesSubplot:xlabel='cluster', ylabel='Spending Score (1-100)'&gt;
</code></pre>
<p>​ <br>
<img loading="lazy" src="output_28_1.png" alt="png"  />

​</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;Annual Income (k$)&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">avg_df</span><span class="p">)</span>
</code></pre></div><pre><code>&lt;AxesSubplot:xlabel='cluster', ylabel='Annual Income (k$)'&gt;
</code></pre>
<p>​ <br>
<img loading="lazy" src="output_29_1.png" alt="png"  />

​</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span><span class="s1">&#39;Gender&#39;</span><span class="p">])[</span><span class="s1">&#39;Gender&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">())</span>
<span class="n">df2</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Gender</th>
    </tr>
    <tr>
      <th>cluster</th>
      <th>Gender</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="2" valign="top">0</th>
      <th>Female</th>
      <td>14</td>
    </tr>
    <tr>
      <th>Male</th>
      <td>9</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">1</th>
      <th>Female</th>
      <td>47</td>
    </tr>
    <tr>
      <th>Male</th>
      <td>41</td>
    </tr>
    <tr>
      <th>2</th>
      <th>Female</th>
      <td>23</td>
    </tr>
  </tbody>
</table>
</div>
<p>各细分市场的主要特点</p>
<p><strong>簇类0</strong>:</p>
<ul>
<li>年平均收入高，支出低。</li>
<li>平均年龄在40岁左右，性别以男性为主。</li>
</ul>
<p><strong>簇类1</strong>：</p>
<ul>
<li>中低收入，平均消费能力。</li>
<li>平均年龄在50岁左右，性别以女性为主。</li>
</ul>
<p><strong>簇类2</strong>：</p>
<ul>
<li>平均收入低，消费分数高。</li>
<li>平均年龄在25岁左右，性别以女性为主。</li>
</ul>
<p><strong>簇类3</strong>：</p>
<ul>
<li>平均收入高，消费分数高。</li>
<li>平均年龄在30岁左右，性别以女性为主。</li>
</ul>
<p>值得注意的是，计算年龄中位数将有助于更好地了解每个集群内的年龄分布。</p>
<p>而且，女性在整个数据集中的代表性更高，这就是为什么大多数集群中女性的数量比男性多。我们可以找到每个性别相对于整个数据集中的数字的百分比，以便更好地了解性别分布。</p>
<br>
<h2 id="为每个簇类构建角色">为每个簇类构建角色</h2>
<p>作为一名数据科学家，能够用你的分析讲述一个故事是一项重要的技能，这将帮助你的客户或利益相关者更容易理解你的发现。下面是一个基于创建的簇类构建消费者角色的示例：</p>
<p><strong>簇类0</strong></p>
<p>这个角色由对金钱非常谨慎的中年人组成。尽管与所有其他群体中的个人相比，他们的平均收入最高，但花费最少。这可能是因为他们有经济责任——比如为孩子的高等教育存钱。</p>
<p>建议：促销、优惠券和折扣代码将吸引这一领域的个人，因为他们倾向于少花钱。</p>
<p><strong>簇类1</strong></p>
<p>这部分人包括一个年龄较大的群体。他们挣的少，花的少，而且可能正在为退休储蓄。</p>
<p>建议：针对这些人的营销可以向这一领域的人推广医疗保健相关产品。</p>
<p><strong>簇类2</strong></p>
<p>这一部分由较年轻的年龄组组成。这部分人最有可能是第一批求职者。与其他人相比，他们赚的钱最少。然而，这些人都是热情的年轻人，他们喜欢过上好的生活方式，而且往往超支消费。</p>
<p>建议：由于这些年轻人花费很多，给他们提供旅游优惠券或酒店折扣可能是个好主意。为他们提供折扣的顶级服装和化妆品品牌也将很好地为这一部分。</p>
<p><strong>簇类3</strong></p>
<p>这部分人是由中年人组成的。这些人努力工作，积累了大量财富。他们也花大量的钱来过好的生活。</p>
<p>建议：由于他们的消费能力和人口结构，这些人很可能会寻找房产购买或投资。</p>
<br>
<h2 id="结论">结论</h2>
<p>在本文中，我已经详细的建立了一个用于客户细分的 K-Means 聚类模型。我们还探讨了聚类分析，并分析了每个聚类中个体的行为。最后，我们看了一些可以根据集群中每个人的属性提供的业务建议。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>转载 | 从符号到嵌入：计算社会科学的两种文本表示</title>
      <link>https://hidadeng.github.io/blog/from_sysbol_to_embeddings_in_computational_social_science/</link>
      <pubDate>Mon, 25 Apr 2022 10:40:10 +0600</pubDate>
      
      <guid>/blog/from_sysbol_to_embeddings_in_computational_social_science/</guid>
      <description>如何有效地表示数据以挖掘我们想要的计算社会科学的含义？为了探索答案，我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示</description>
      <content:encoded><![CDATA[<p>B站看到大牛刘知远关于文本分析在计算社会科学领域应用的分享，解答了我对文本表示的疑惑，看完了能对文本的特征工程加深理解，同时也能更清晰未来如何借助计算机科学技术开展社会科学研究。</p>
<blockquote>
<p><strong>全文摘抄自</strong></p>
<p>Chen, H., Yang, C., Zhang, X., Liu, Z., Sun, M. and Jin, J., 2021. From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science. Journal of Social Computing, 2(2), pp.103-156.</p>
</blockquote>
<iframe
    src="//player.bilibili.com/player.html?bvid=BV1qi4y1Q7qj&page=1"
    scrolling="no"
    height="500px"
    width="800px"
    frameborder="no"
    framespacing="0"
    allowfullscreen="true"
>
</iframe>

<br>
<h2 id="摘要">摘要</h2>
<p><strong>计算社会科学</strong>（CSS），旨在利用计算方法来解决社会科学问题，是一个新兴和快速发展的领域。 CSS 的研究是数据驱动的，并且显着受益于在线用户生成内容和社交网络的可用性，其中包含用于调查的富文本和网络数据。然而，这些大规模、多模态的数据也给研究人员带来了很大的挑战：<strong>如何有效地表示数据以挖掘我们想要的 CSS 含义</strong>？为了探索答案，<strong>我们对 CSS 中文本和网络的数据表示进行了彻底的回顾，我们将现有的表示总结为两个方案，即基于符号的表示和基于嵌入的表示</strong>，并为每个方案介绍了一系列典型的方法。随后，我们基于对来自 6 个涉及 CSS 的顶级场所的 400 多篇研究文章的调查，展示了上述表示的应用。从这些应用程序的统计数据中，<strong>我们挖掘出每种表示的强度，并发现基于嵌入的表示在过去十年中出现并获得越来越多的关注的趋势</strong>。最后，我们讨论了几个关键挑战和未来方向的开放性问题。本调查旨在为 CSS 研究人员提供对数据表示的更深入理解和更明智的应用。</p>
<p><strong>关键词</strong>：计算社会科学；基于符号的表示；基于嵌入的表示；社交网络</p>
<br>
<h2 id="一计算社会学数据分析流程">一、计算社会学数据分析流程</h2>
<p>其中第二步，数据表示目前有两大类表示(特征工程)方法</p>
<ul>
<li><strong>基于符号的文本表示</strong>；符号可以是单词(或词组)，也可以是概念(如正面情感、负面情感)</li>
<li><strong>基于嵌入(分布式)的文本表示</strong>；相比于符号法，将词(词组)看做一个点。嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(king)-v(queen)约等于v(man)-v(woman)</li>
</ul>
<p><img loading="lazy" src="img/fig1.png" alt=""  />
</p>
<br>
<h2 id="二基于符号的文本表示">二、基于符号的文本表示</h2>
<p>基于符号的文本表示一般来说默认词语是不可分的符号，每个词能根据词频统计出现次数的多与少，或是否存在。</p>
<h3 id="21-词语层面">2.1 词语层面</h3>
<ul>
<li>
<p>基于词频表示</p>
<ul>
<li>是否出现，出现标位1，反之标位0。</li>
<li>出现多少，词语出现几次，标为几个。</li>
</ul>
</li>
<li>
<p>基于特征表示，如每个词带有权重(得分)</p>
</li>
<li>
<p>基于网络表示，如词语共现网络(矩阵)</p>
</li>
</ul>
<h3 id="22-句子层面">2.2 句子层面</h3>
<ul>
<li>
<p>基于词频的表示</p>
<ul>
<li>one-hot 将文本转为向量，向量中每个数，词语出现标位1，反之标位0</li>
<li>bag-of-words，将文本转为向量，向量中每个数，词语出现n次标记为n</li>
<li>n-grams，对词组的处理，将词组看做一个单词(整体)。</li>
<li>Tf-Idf ,该算法分为tf和idf两部分。其中tf与bag-of-words类似，考虑词语出现次数。而idf还考虑词语在语料中出现场景的稀缺性程度。</li>
</ul>
</li>
<li>
<p>基于语法特征，如句法依存关系，类似于英语语法，将句子分为主谓宾、动词、名词等。</p>
</li>
<li>
<p>词典法，如使用正、负情感词典，对文本数据进行情感分析，可以得到pos和neg的各自得分</p>
</li>
</ul>
<p><img loading="lazy" src="img/fig2.png" alt=""  />
</p>
<br>
<h2 id="三基于嵌入的文本表示">三、基于嵌入的文本表示</h2>
<h3 id="31词语层面">3.1词语层面</h3>
<p>嵌入表示认为词是存在更多浅藏含义，存在亲疏远近，是可以比较的词向量。词向量可以有v(best)-v(good)约等于v(worst)-v(bad)</p>
<h3 id="32-句子层面">3.2 句子层面</h3>
<p>词语是向量，那么由词语组成的句子也会加权得到一个向量。含有相似话题或含义相近的句子在多维向量空间中会比较接近。</p>
<p><img loading="lazy" src="img/fig7.png" alt=""  />
</p>
<br>
<h2 id="四任务分类文本的用法">四、任务分类：文本的用法</h2>
<p><img loading="lazy" src="img/fig16.png" alt=""  />
</p>
<p>有了文本数据，刚刚解决了如何表示文本。接下来，需要明确，我们使用文本目的是为了做哪类分析，得到哪些信息。有8种常见的文本分析图式</p>
<ul>
<li>描述性。如随时间推移，词频的发展趋势是变大的</li>
<li>相关性。</li>
<li>聚类。如lda话题分析、k-means聚类</li>
<li>相似度。两个文档转为向量后，可以通过cosine计算相似度</li>
<li>分类。机器学习分类，判断某文本隶属于哪个类别</li>
<li>回归。例如根据文本，判断某件事发生的概率</li>
<li>语言模型。</li>
<li>排序。</li>
</ul>
<br>
<h2 id="五发文趋势-符号vs嵌入">五、发文趋势-符号vs嵌入</h2>
<p>基于上一节中对应用程序的介绍，可以观察到基于符号和基于嵌入的表示在 <strong>计算社会科学</strong>中都得到了相当大的采用。为了明确研究它们的覆盖范围，我们计算了每年使用两种表示中的一种或两种的作品数量，如图 17 所示。通过比较nature、science、pnas三大顶级期刊，我们可以发现使用<strong>基于嵌入表示</strong>的文章比例在过去几年中逐渐。这表明越来越多的 计算社会科学文章 已经考虑并受益于基于嵌入表示。</p>
<p>图 18 显示了在 计算机领域ACL、WWW 和 KDD 的会议上中，发现使用基于嵌入的表示的文章数量已大大超过使用基于符号的表示的文章数量。然而，与图 17 相比，计算机科学会议中基于嵌入的表示的数量与三个多学科期刊之间存在很大差距。</p>
<p><img loading="lazy" src="img/3_top_journals.png" alt=""  />
</p>
<p><img loading="lazy" src="img/nlp.png" alt=""  />
</p>
<p>总而言之，在过去十年中，基于嵌入的表示已经出现并在 计算社会科学 中发挥着越来越重要的作用。</p>
<br>
<h2 id="六趋势解读">六、趋势解读</h2>
<p>基于它们的内部机制和现有应用，对趋势解读，我们总结出以下三个关键点。</p>
<p>基于符号的表示因其明确性和可解释性而擅长描述和关系的任务。</p>
<p>基于符号的表示中的每个值都表示一定的人类可读的含义，因此我们可以直接使用它来观察数据的分布，以及提取对象之间的关系。例如，基于频率的词表示用于观察文化变化并捕捉新闻中提及次数与公司股票交易量之间的关系。虽然基于主题模型的表示和一些基于神经的表示在一定程度上具有实际意义，但它们对于社会科学研究人员来说仍然是模糊的并且不那么引人注目。</p>
<p>由于神经网络具有强大的拟合数据和提取深度语义的能力，基于嵌入的表示在预测（例如分类和回归）和相似性任务中表现更好。一方面，神经网络通过大规模神经元的连接实现高效的输入输出映射功能。另一方面，通过多层网络的构建，实现深层语义和抽象概念的提取。现有研究表明，深层捕获相对于浅层更抽象的特征。诸如社会偏见和道德化之类的抽象概念都可以通过基于嵌入的表示来很好地衡量。虽然我们提到基于符号的表示可以通过一些定义的符号来代表抽象概念，但这种表示仍然是部分和肤浅的，很难捕捉到它们的全貌。</p>
<p>基于嵌入的表示需要更少的人力。基于符号的表示通常需要大量的专家知识来定义研究对象的特征，这是劳动密集型的。此外，对于一些没有充分特征的抽象概念或对象，它们的表现将受到限制。与它们不同的是，基于嵌入的表示是从数据中自动提取的，几乎不需要人工干预，甚至可以补充人类知识。例如，可以使用神经网络来自动恢复丢失的巴比伦文本，这即使对专家来说也是具有挑战性的。此外，基于嵌入的表示可以在没有手动定义的情况下描述语言的复杂性和歧义性。</p>
<br> 
<h2 id="七未来展望">七、未来展望</h2>
<p>尽管在过去十年中出现了从符号到嵌入的趋势，但仍有许多挑战和悬而未决的问题有待探索。展望未来，我们列出了一些与计算社会科学 中的数据表示相关的基本和潜在的未来方向。</p>
<p>预训练的语言模型。近年来，预训练的语言模型受到了相当大的关注，并在处理文本数据方面取得了巨大的成功 [100, 240]。这些模型从百科全书和书籍等海量文本数据中学习丰富的语义信息，仅在下游任务中进行微调以实现有效的基于嵌入的表示。因此，对于 计算社会科学，我们可以借助预训练的语言模型获得更通用、更健壮的文本表示。与从传统神经网络模型中学习的表示相比，这些表示不仅可以更广泛、更准确地从文本中分析社会现象，而且还可以减少那些需要大量标记数据的任务的人工注释。</p>
<p>图神经网络。通过消息传递机制，图神经网络 [461] 可以同时有效地对网络拓扑和节点/边缘特征（例如文本信息）进行建模，从而提供一个统一的框架来利用来自异构来源的信息。 计算社会科学 中的许多场景需要处理社交网络以及个人特征。因此，图神经网络技术在 计算社会科学 研究中具有很大的应用潜力，可以学习融合文本和网络信息的表示。事实上，计算机科学中的各种应用，例如自然语言处理 [418] 和推荐系统 [439]，已经采用图神经网络进行建模。</p>
<p>设计为预测和相似性。基于嵌入的表示以丰富和深层次的语义而闻名，而基于符号的表示通常保留在部分和浅层语义中。同时，基于嵌入的表示擅长预测和相似性的任务。因此，为了充分利用嵌入中的强语义，鼓励 计算社会科学 研究人员尽可能将研究问题设计为预测或相似性任务。例如，我们可以将社会偏见问题设计为性别词和中性词嵌入之间的相似性度量 [59, 133]。此外，人类语言的复杂性可以设计为一项预测任务，它以语言模型为指标查看单词或句子的预测概率[155]。</p>
<p>可解释性。诚然，基于嵌入的方法的一个缺点是缺乏可解释性。这个问题会损害与道德、安全或隐私相关的决策关键系统的应用。尽管嵌入模型，尤其是神经网络模型的可解释性尚未完全解决，但计算机科学领域的研究人员已经做出了一些努力，以提高基于神经模型的可解释性 [16]。因此，利用基于嵌入的模型和可解释性分析方法进行有效和（部分）可解释的预测将是一个有趣的方向。</p>
<br>
<h2 id="结论">结论</h2>
<p>计算社会科学作为一个新兴且有前途的跨学科领域，近年来吸引了相当多的研究兴趣。 计算社会科学 研究中广泛使用两种主要类型的数据，即文本数据和网络数据。在本次调查中，我们首先将数据表示总结为基于符号和基于嵌入的表示，并在构建这些表示时进一步介绍典型的方法。之后，我们基于来自 6 个经典期刊和会议的 400 多篇高被引文献，对这两类表示的应用进行了全面回顾。根据对这些应用的统计，发现了 计算社会科学 中基于嵌入的文本和网络表示正在出现和增长的趋势，我们进一步讨论了其中的原因。最后，我们提出了 计算社会科学 中的四个挑战和未解决的问题，它们是需要探索的基本和潜在方向。</p>
<br>
<br>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文本相似 | Lazy Prices公司年报内容变动预示重大风险</title>
      <link>https://hidadeng.github.io/blog/2019-12-08-lazy-prices/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-12-08-lazy-prices/</guid>
      <description>description用于SEO优化</description>
      <content:encoded><![CDATA[<h2 id="文献">文献</h2>
<p>Cohen, L., Malloy, C. and Nguyen, Q., 2020. Lazy prices. <em>The Journal of Finance</em>, <em>75</em>(3), pp.1371-1415.</p>
<br>
<h2 id="摘要">摘要</h2>
<p>使用1995年-2014年所有美国公司季度和年度申报的完整历史记录，研究发现当公司对<strong>报告进行积极更改</strong>时，这种行为<strong>蕴含着</strong>公司未来运营的<strong>重要信号</strong>。</p>
<p><strong>财务报告的语言和结构的变化也对公司的未来收益产生重大影响</strong>：做空&quot;变化&quot;的公司（持有的公司，如果其报告发生变化的，做空该公司股票），买入“不变化”的公司，使用这样的投资组合策略，在2006年的每月alpha值高达1.88%的收益（每年超过22％）。报告中涉及执行官（CEO和CFO）团队的话语风格的变化，或者有关诉讼(风险部分)的话语的变化，都对投资的未来收益有重要作用。</p>
<p>研究发现，对10-K的变化可以预测未来的收益、获利能力、未来的新闻公告，甚至未来的公司破产。同时，不做任何变化的公司将获得显著的异常收益。与资产价格典型的反应不足研究不同，我们发现没有任何与这些变化相关的公告效应–仅在后来通过新闻，事件或收益披露信息时才产生回报–暗示投资者并未注意到整个公众领域的这些变化。</p>
<br>
<h2 id="研究背景">研究背景</h2>
<p>之前的研究认为，尽管投资者一次对包含重大变化的财务报表的发布作出了迅时反应，但随着时间的流逝，这种公告作用是会减弱的(Brown and Tucker, 2011 and Feldman et al., 2010)。这表示10-K报告会随着时间推移，信息价值大打折扣。尽管我们复现了这个事实，即与常规文件的变更没有重大的公告效应，但我们认为，前人的研究忽略了更重要部分(如MD&amp;A)对对资产价格的影响。</p>
<p>确切的说，<strong>并不是报告的披露效应的信息价值变低了，而是投资者越来越难以发现报告中微妙的信息变化， 比如因为报告变得越来越冗杂。投资者只有看到某些新闻后，才会逐渐意识到之前公司报告内容变化的的真正价值</strong>。</p>
<p>例如Baxter公司</p>
<ul>
<li>纽约时报在 <strong>2010年4月23日</strong> 发了一条FDA将有对输液泵(infusion pumps)更严格对审批管理规定的新闻，<strong>新闻中提到了Baxter公司</strong>。新闻公布当天，<strong>Baxter股价大跌</strong>。</li>
<li><strong>10天</strong>后的（2010年5月4日），Baxter宣布<strong>召回问题的输液泵产品</strong>，股价当天再次大跌。</li>
</ul>
<p><img loading="lazy" src="img/lazy-prices-1.png" alt=""  />
</p>
<p>两次负面新闻导致Baxter股价大跌超过20%，最有意思的是Baxter公司一个多月前（<strong>2010年2月23日</strong>）10-k报告中 <strong>提到</strong> 了与这两条新闻类似的 <strong>线索</strong>。</p>
<p><img loading="lazy" src="img/clues_from_report.png" alt=""  />
</p>
<p>截图中写着 <strong>Baxter的产品COLLEGUE未来可能面脸额外的处罚，而且相关销售面临着FDA、OIG、DOI和FTC越来越严格的审批，面临的执法强度也越来越大</strong>。</p>
<p>因纽约时报发布的消息，股价大跌。但是大跌之前Baxter的10-k报告中似乎提示未来公司可能面临的风险，但是投资者怎么没有注意到这个重要线索呢？</p>
<br>
<h2 id="数据获取与分析方法">数据获取与分析方法</h2>
<p>这篇文章用到了很多 文本数据挖掘 方法，如</p>
<ul>
<li>数据采集(报告下载和信息监测)</li>
<li>正则表达式（数据分割与抽取）</li>
<li>文本相似度(计算报告变化程度)</li>
</ul>
<p>我大致说下这几部分技术在这篇论文中的应用。</p>
<h3 id="1-数据采集">1. 数据采集</h3>
<p>这篇论文研究者认为，只有投资者意识到本期报告和上一期报告做对比，才能发现报告变化，进而对股价有影响。所以当有新公告公布后，投资者是否下载本期报告的同时顺带着下载上一期报告，下载量又是多少。</p>
<p>下载量可以从Freedom of Information Act下载，</p>
<p><img loading="lazy" src="img/download_data.png" alt=""  />
</p>
<p>可以拿到的信息包括:</p>
<ul>
<li>报告文件</li>
<li>报告下载时间</li>
<li>报告下载的IP地址(可以通过这个ip来当作投资者的id)</li>
</ul>
<br>
<h3 id="2-正则表达式">2. 正则表达式</h3>
<p>一个公司报告文件会有不同部分，我们需要将不同的部分分别识别出来。这里用到正则表达式，可以进行快速的数据清洗和数据抽取。</p>
<p><img loading="lazy" src="img/regular_expression.png" alt=""  />
</p>
<br>
<h3 id="3-文本相似度">3. 文本相似度</h3>
<p>文本转为向量后就可以进行相似度计算,</p>
<p><img loading="lazy" src="img/similar-1.png" alt=""  />

<img loading="lazy" src="img/similar-2.png" alt=""  />

<img loading="lazy" src="img/similar-3.png" alt=""  />
</p>
<p>这里使用我开发的cntext包，可以实现cosine和jaccard相似度的计算。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">A</span> <span class="o">=</span> <span class="s1">&#39;We expect demand to increase.&#39;</span>
<span class="n">B</span> <span class="o">=</span> <span class="s1">&#39;We expect worldwide demand to increase.&#39;</span>
<span class="n">C</span> <span class="o">=</span> <span class="s1">&#39;We expect weakness in sales&#39;</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">jaccard_sim</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">0.40
0.83
</code></pre></div><p>如果对Baxter公司多个年度对报告进行相似度计算，绘制成图就会发现2010年与前后变化很大。相似度越低，说明公司报告前后变化很大，应该引起投资者注意，如果能注意到就会避免纽约时报导致到股价暴跌。如下图</p>
<p><img loading="lazy" src="img/lazy-prices-2.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="案例实现">案例实现</h2>
<p>由于没有完全一样的数据，这里使用政府工作报告数据类比，使用cosine相似度画出趋势线条。</p>
<p>使用相似性识别变化的时间点</p>
<h3 id="准备数据">准备数据</h3>
<p>政府工作报告 <a href="http://www.gov.cn/guowuyuan/zfgzbg.htm">http://www.gov.cn/guowuyuan/zfgzbg.htm</a></p>
<p>prc_reports.xlsx 链接:https://pan.baidu.com/s/1sVU3mkEcP7Z3_hbG5AVNUA 密码:zjrq</p>
<p>将下载好后的 prc_reports.xlsx 文件放置于 .ipynb文件 所在的文件夹内。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_excel</span><span class="p">(</span><span class="s1">&#39;prc_reports.xlsx&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<h3 id="计算相似度">计算相似度</h3>
<p>运行时间大概30s， 运算结果是列表数据 cosines</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">cosines</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">#row  Series</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="n">text1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;report&#39;</span><span class="p">]</span>
    <span class="n">text2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;report2&#39;</span><span class="p">]</span>
    <span class="n">simi</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">cosine_sim</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">)</span>
    <span class="n">cosines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">simi</span><span class="p">)</span>
    
<span class="n">cosines</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[0.44&#39;, &#39;0.39&#39;, &#39;0.35&#39;, ... &#39;0.62&#39;, &#39;0.61&#39;, &#39;0.60&#39;]
</code></pre></div><h3 id="绘制柱状图">绘制柱状图</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pyecharts.charts</span> <span class="kn">import</span> <span class="n">Bar</span>
<span class="kn">from</span> <span class="nn">pyecharts</span> <span class="kn">import</span> <span class="n">options</span> <span class="k">as</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">pyecharts.globals</span> <span class="kn">import</span> <span class="n">CurrentConfig</span><span class="p">,</span> <span class="n">NotebookType</span>
<span class="n">CurrentConfig</span><span class="o">.</span><span class="n">NOTEBOOK_TYPE</span> <span class="o">=</span> <span class="n">NotebookType</span><span class="o">.</span><span class="n">JUPYTER_NOTEBOOK</span>

<span class="n">bar</span> <span class="o">=</span> <span class="n">Bar</span><span class="p">()</span>

<span class="n">bar</span><span class="o">.</span><span class="n">add_xaxis</span><span class="p">(</span><span class="n">xaxis_data</span><span class="o">=</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">])</span>
<span class="n">bar</span><span class="o">.</span><span class="n">add_yaxis</span><span class="p">(</span><span class="s2">&#34;相似度&#34;</span><span class="p">,</span> 
               <span class="n">cosines</span><span class="p">,</span> 
               <span class="n">label_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">LabelOpts</span><span class="p">(</span><span class="n">is_show</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

<span class="n">bar</span><span class="o">.</span><span class="n">set_global_opts</span><span class="p">(</span><span class="n">title_opts</span><span class="o">=</span><span class="n">opts</span><span class="o">.</span><span class="n">TitleOpts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&#34;政府工作报告相似度可视化&#34;</span><span class="p">))</span>
<span class="n">bar</span><span class="o">.</span><span class="n">load_javascript</span><span class="p">()</span>

<span class="n">bar</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;政府工作报告相似度可视化1.html&#39;</span><span class="p">)</span>
<span class="n">bar</span><span class="o">.</span><span class="n">render_notebook</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/vis_res.png" alt=""  />
</p>
<h3 id="解读">解读</h3>
<p>从图中可以看到除1959年异常外，其他方面能挖掘出很多信息。从相似度整体趋势，</p>
<p>1959-1992 第一阶段，
1992-至今 第二阶段</p>
<p>1992年附近，第一次确立社会主义市场经济制度。之后的岁月里一直围绕着经济建设高速发展。</p>
<p>同时也可以看出在第一阶段前期相似度异常的低，可以理解为新中国初建，百废待兴，对于建设者而言，组着和管理这个国家的政府也在学习如何建设新中国。而90年代后，相似度越来越高，体现了政府越来越熟悉如何治理国家，如何搞经济建设。</p>
<p><br><br></p>
<h2 id="广而告之">广而告之</h2>
<ul>
<li><a href="https://hidadeng.github.io/blog/call_for_paper/">长期征稿</a></li>
<li><a href="https://hidadeng.github.io/blog/we_need_you/">长期招募小伙伴</a></li>
<li><a href="https://hidadeng.github.io/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
