<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>cntext on 大邓和他的PYTHON</title>
    <link>/tags/cntext/</link>
    <description>Recent content in cntext on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 23 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="/tags/cntext/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>资源 | 中文 GloVe&amp;Word2Vec 词向量模型列表</title>
      <link>https://textdata.cn/blog/2025-04-18-chinese-pretrained-word-embeddings/</link>
      <pubDate>Fri, 18 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2025-04-18-chinese-pretrained-word-embeddings/</guid>
      <description>中文语料预训练模型列表， 使用 cntext2.x 训练出的预训练语言模型， 主要分 GloVe 和 Word2Vec 两种。</description>
      <content:encoded><![CDATA[<p>中文语料预训练模型列表， 使用 cntext2.x 训练出的预训练语言模型， 主要分 GloVe 和 Word2Vec 两种。</p>
<br>
<h2 id="一中文预训练模型">一、中文预训练模型</h2>
<p>使用 <a href="https://cntext.readthedocs.io/zh-cn/latest/intro.html">cntext2.x</a> 训练得到的中文预训练模型资源，汇总如下</p>
<p>对中文语料进行了近义测试和类比测试， 其中斯皮尔曼秩系数(Spearman&rsquo;s Rank Coeficient) 取值[-1,1], 取值越大表示模型越符合人类的认知。</p>
<p>类比测试有首都国家（CapitalOfCountries）、省会省份（CityInProvince）、家人关系（FamilyRelationship）、社会科学(管理、经济、心理等 SocialScience) 的类别准确率测试。</p>
<br>
<table>
<thead>
<tr>
<th>数据集</th>
<th>词向量</th>
<th>网盘</th>
<th>斯皮尔曼秩系数</th>
<th>首都国家(%)</th>
<th>省会省份(%)</th>
<th>家人关系(%)</th>
<th>社会科学(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://textdata.cn/blog/2023-12-17-gov-anual-report-dataset/">中国政府工作报告</a></td>
<td><strong><em>人民政府(国省市)工作报告-GloVe.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1IdK8RU9L8mp6I2nhcoSmyA?pwd=ht2s">https://pan.baidu.com/s/1IdK8RU9L8mp6I2nhcoSmyA?pwd=ht2s</a></td>
<td>0.38</td>
<td>30.73</td>
<td>98.86</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-12-17-gov-anual-report-dataset/">中国政府工作报告</a></td>
<td><strong><em>人民政府(国省市)工作报告-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1GoTjMbUcYS4jN6w4GqlqBA?pwd=qb5b">https://pan.baidu.com/s/1GoTjMbUcYS4jN6w4GqlqBA?pwd=qb5b</a></td>
<td>0.35</td>
<td>30.06</td>
<td>96.00</td>
<td>0.00</td>
<td>16.67</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-05-07-china-law-judgment-documents-datasets/">中国裁判文书网</a></td>
<td><strong><em>裁判文书-GloVe.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1a0Fisvnkl8UaQZrHP7olCQ?pwd=8w49">https://pan.baidu.com/s/1a0Fisvnkl8UaQZrHP7olCQ?pwd=8w49</a></td>
<td>0.37</td>
<td>7.69</td>
<td>98.86</td>
<td>75.53</td>
<td>25.00</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-12-22-renmin-gov-leader-comment-board/">留言板</a></td>
<td><strong><em>留言板-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1n7vwCOBnrye1CYrt_IBqZA?pwd=9m42">https://pan.baidu.com/s/1n7vwCOBnrye1CYrt_IBqZA?pwd=9m42</a></td>
<td>0.45</td>
<td>19.33</td>
<td>100</td>
<td>61.40</td>
<td>20%</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/">A 股年报</a></td>
<td><strong><em>mda01-23-GloVe.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1vXvbomHjOaFBeEz7GV0R6A?pwd=y6hd">https://pan.baidu.com/s/1vXvbomHjOaFBeEz7GV0R6A?pwd=y6hd</a></td>
<td>0.34</td>
<td>78.13</td>
<td>100</td>
<td>0</td>
<td>37.50</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/">A 股年报</a></td>
<td><strong><em>mda01-23-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/11V1RyqH_cKE9eju0Mm-1TQ?pwd=kcwx">https://pan.baidu.com/s/11V1RyqH_cKE9eju0Mm-1TQ?pwd=kcwx</a></td>
<td>0.41</td>
<td>27.27</td>
<td>97.14</td>
<td>10</td>
<td>44.44</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2024-01-21-hk-stock-market-anual-report/">港股年报</a></td>
<td><strong><em>英文港股年报-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1ISGAoZnA_1Ben6M2DCliOQ?pwd=nagx">https://pan.baidu.com/s/1ISGAoZnA_1Ben6M2DCliOQ?pwd=nagx</a></td>
<td>&mdash;</td>
<td>&mdash;</td>
<td>&mdash;</td>
<td>&mdash;</td>
<td>&mdash;</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2024-01-21-hk-stock-market-anual-report/">港股年报</a></td>
<td><strong><em>中文港股年报-Word2Vec.200.15.bin</em></strong></td>
<td>hhttps://pan.baidu.com/s/1smMcrPtIP8g635YABCodig?pwd=sjdj</td>
<td>0.35</td>
<td>25.20</td>
<td>79.43</td>
<td>18.59</td>
<td>25</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-12-14-daily-news-dataset/">人民日报</a></td>
<td><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">年份 Word2Vec</a></td>
<td><a href="https://pan.baidu.com/s/1Ru_wxu9egsmhM7lATjSlgQ?pwd=bcea">https://pan.baidu.com/s/1Ru_wxu9egsmhM7lATjSlgQ?pwd=bcea</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-12-14-daily-news-dataset/">人民日报</a></td>
<td><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">对齐模型 Aligned_Word2Vec</a></td>
<td><a href="https://pan.baidu.com/s/1IVgP0MyQpez0hpoJyEyFdA?pwd=7qsu">https://pan.baidu.com/s/1IVgP0MyQpez0hpoJyEyFdA?pwd=7qsu</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/">专利申请</a></td>
<td><strong><em>专利摘要-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1FHI_J7wU9eQGRckD12QB5g?pwd=6rr2">https://pan.baidu.com/s/1FHI_J7wU9eQGRckD12QB5g?pwd=6rr2</a></td>
<td>0.46</td>
<td>3.78</td>
<td>25.14</td>
<td>33.33</td>
<td>37.50</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">专利申请</a></td>
<td><strong><em>province_w2vs 分省份训练词向量</em></strong></td>
<td><a href="https://pan.baidu.com/s/1eBFTIZcv2DWssLiaRnCqZQ?pwd=ikpu">https://pan.baidu.com/s/1eBFTIZcv2DWssLiaRnCqZQ?pwd=ikpu</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">专利申请</a></td>
<td><strong><em>year_w2vs 分年份训练词向量</em></strong></td>
<td><a href="https://pan.baidu.com/s/1lrVkML92cVJdHQa1HQyAwA?pwd=4gqa">https://pan.baidu.com/s/1lrVkML92cVJdHQa1HQyAwA?pwd=4gqa</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>大众点评评论语料</td>
<td><strong><em>大众点评-评论-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/15He728XGzoXDFYrUWDTaqQ?pwd=eg6x">https://pan.baidu.com/s/15He728XGzoXDFYrUWDTaqQ?pwd=eg6x</a></td>
<td>0.34</td>
<td>50.31</td>
<td>89.71</td>
<td>70.00</td>
<td>0.00</td>
</tr>
<tr>
<td>中文歌词</td>
<td><strong><em>中文歌词-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1h1g1mOACmpCwn5pz8jR3vQ?pwd=ub2z">https://pan.baidu.com/s/1h1g1mOACmpCwn5pz8jR3vQ?pwd=ub2z</a></td>
<td>0.06</td>
<td>0.00</td>
<td>0.00</td>
<td>0.9</td>
<td>0.00</td>
</tr>
<tr>
<td>英文歌词</td>
<td><strong><em>英文歌词-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1ycy-BTSa8zqW_xbIoshy6Q?pwd=hu1v">https://pan.baidu.com/s/1ycy-BTSa8zqW_xbIoshy6Q?pwd=hu1v</a></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2025-03-05-consumer-complaint-dataset/">黑猫消费者投诉</a></td>
<td><strong><em>消费者黑猫投诉-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1FOI2BIVRojOswdKfqaNbsw?pwd=catc">https://pan.baidu.com/s/1FOI2BIVRojOswdKfqaNbsw?pwd=catc</a></td>
<td>0.32</td>
<td>16.18</td>
<td>68</td>
<td>28.57</td>
<td>0.00</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset">豆瓣影评</a></td>
<td><strong><em>douban-movie-1000w-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1uq6Ti7HbEWyT4CgktKrMng?pwd=63jg">https://pan.baidu.com/s/1uq6Ti7HbEWyT4CgktKrMng?pwd=63jg</a></td>
<td>0.43</td>
<td>39.02</td>
<td>28.57</td>
<td>92.65</td>
<td>25.00</td>
</tr>
<tr>
<td><a href="https://textdata.cn/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec">B 站</a></td>
<td><strong><em>B 站签名-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1OtBU9BzitcNxkmPzhzH6FQ?pwd=m3iv">https://pan.baidu.com/s/1OtBU9BzitcNxkmPzhzH6FQ?pwd=m3iv</a></td>
<td>0.34</td>
<td>25.56</td>
<td>33.71</td>
<td>44.17</td>
<td>0.00</td>
</tr>
<tr>
<td><a href="https://github.com/Viscount/IUI-Paper">B 站弹幕</a></td>
<td><strong><em>B 站弹幕-Word2Vec.200.15.bin</em></strong></td>
<td><a href="https://pan.baidu.com/s/1LNDLed5uP3KnUMmrKf_uhg?pwd=x4t8">https://pan.baidu.com/s/1LNDLed5uP3KnUMmrKf_uhg?pwd=x4t8</a></td>
<td>0.42</td>
<td>11.67</td>
<td>65.81</td>
<td>44.17</td>
<td>25.00</td>
</tr>
</tbody>
</table>
<p><br><br></p>
<h2 id="二cntext2x">二、cntext2.x</h2>
<p>cntext2.x 是中英文文本分析库，内置有多重词典和常用函数。 常见的文本分析代码行数在数十行，而 cntext2.x 力求将代码量控制在 2~5 行。</p>
<h3 id="21-训练模型">2.1 训练模型</h3>
<p>训练模型步骤:</p>
<ol>
<li>构建语料</li>
<li>训练模型</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 大邓Mac 96G内存， 12核使用的代码。</span>
<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;留言板.txt&#39;</span><span class="p">,</span>
                  <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                  <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                  <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span>
                  <span class="n">chunksize</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
                  <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/renmin_board_cache.txt Not Found or Empty, Preprocessing Corpus
Reading Preprocessed Corpus from output/renmin_board_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 2692 s.
Output Saved To: output/留言板-Word2Vec.200.15.bin
</code></pre></div><br>
<p>cntext2.x 训练模型的教程可参考</p>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/">使用 1 亿 B 站用户签名训练 word2vec 词向量</a></li>
<li><a href="https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/">使用 1985 年-2025 年专利申请摘要训练 Word2Vec 模型</a></li>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">使用 MD&amp;A2001-2023 语料训练 Word2Vec/GloVe 模型</a></li>
<li><a href="https://textdata.cn/blog/2025-04-17-training-a-glove-model-using-china-judgements-corpus/">使用裁判文书语料训练 GloVe 词向量</a></li>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">使用 5000w 专利申请数据集按年份(按省份)训练词向量</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">使用人民网领导留言板语料训练 Word2Vec 模型</a></li>
</ul>
<br>
<h3 id="22-评估模型">2.2 评估模型</h3>
<p>使用近义法和类比法， 判断模型的表现。详情可查看<a href="https://cntext.readthedocs.io/zh-cn/latest/model.html">文档</a></p>
<br>
<p><strong>近义测试</strong></p>
<p>cntext2.x 内置 537 条近义实验数据， 可直接使用。</p>
<p><img loading="lazy" src="img/01-similar.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">evaluate_similarity</span><span class="p">(</span><span class="n">w2v</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   426    |    111     |            0.45            |
+----------+------------+----------------------------+
</code></pre></div><p>Spearman’s Rank Coeficient 系数取值[-1, 1], 取值越大， 说明模型表现越好。</p>
<br>
<p><strong>类比测试</strong></p>
<ul>
<li>雅典之于希腊，似如巴格达之于伊拉克。</li>
<li>哈尔滨之于黑龙江，似如长沙之于湖南。</li>
<li>国王之于王后，似如男人之于女人。</li>
</ul>
<p><img loading="lazy" src="img/02-analogy-woman.png" alt=""  />
</p>
<p>cntext2.x 内置 1194 条类比， 格式如下</p>
<p><img loading="lazy" src="img/03-analogy.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">evaluate_analogy</span><span class="p">(</span><span class="n">wv</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|██████████████| 1198/1198 [00:11&lt;00:00, 99.91it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   238    |    439     |   19.33    |   2.74   |
|   CityInProvince   |   175    |     0      |   100.00   |   1.01   |
| FamilyRelationship |   272    |     0      |   61.40    |   1.96   |
|   SocialScience    |    10    |     60     |   20.00    |   1.50   |
+--------------------+----------+------------+------------+----------+
</code></pre></div><ul>
<li>CapitalOfCountries 留言板语料在此项表现较差， 应该是语料中常见国家首度的提及较少。</li>
<li>CityInProvince 留言板语料在此项表现如此优异，应该是语料中省份、省会地域词经常出现。</li>
<li>FamilyRelationship 留言板中应该少不了家长里短， 所以此项准确率还可以。 以<a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">年报 MD&amp;A</a>为例，此处准确率只有 10%, 而<a href="https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/">豆瓣影评</a>该处准确率高达 92.65%。</li>
<li>SocialScience 留言板语料在此项表现一般， 应该是语料中常见的社会科学词语提及较少。</li>
</ul>
<p>整体而言，语料训练的效果很不错，抓住了数据场景的独特性语义。</p>
<p><br><br></p>
<h2 id="三模型使用">三、模型使用</h2>
<h3 id="31-读取模型">3.1 读取模型</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="n">ct</span>

<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/留言板-Word2Vec.200.15.bin&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;维度数:&#39;</span><span class="p">,</span> <span class="n">w2v</span><span class="o">.</span><span class="n">vector_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;词汇量: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w2v</span><span class="p">))</span>
<span class="n">w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading output/留言板-Word2Vec.200.15.bin...
维度数: 200
词汇量:  1050245
&lt;gensim.models.keyedvectors.KeyedVectors at 0x328d737a0&gt;
</code></pre></div><br>
<h3 id="32-keyedvectors-的操作方法或属性">3.2 KeyedVectors 的操作方法(或属性)</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><em>KeyedVectors.index_to_key</em></strong></td>
<td>获取词汇表中的所有单词。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.key_to_index</em></strong></td>
<td>获取单词到索引的映射。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.vector_size</em></strong></td>
<td>获取 GloVe 模型中任意词向量的维度。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.get_vector(word)</em></strong></td>
<td>获取给定单词的词向量。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.similar_by_word(word, topn=10)</em></strong></td>
<td>获取某词语最相似的 10 个近义词。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.similar_by_vector(vector, topn=10)</em></strong></td>
<td>获取词向量最相似的 10 个近义词。</td>
</tr>
<tr>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>
<br>
<h3 id="33-查看词表">3.3 查看词表</h3>
<p>因为词表有 <strong><em>1050245</em></strong> 个词， 为了方便，这里只显示前 20 个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># 词表带顺序的
list(w2v.index_to_key)[:20]
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;问题&#39;,
 &#39;进行&#39;,
 &#39;您好&#39;,
 &#39;工作&#39;,
 &#39;小区&#39;,
 &#39;反映&#39;,
 &#39;领导&#39;,
 &#39;情况&#39;,
 &#39;相关&#39;,
 &#39;留言&#39;,
 &#39;没有&#39;,
 &#39;感谢您&#39;,
 &#39;网友&#39;,
 &#39;业主&#39;,
 &#39;办理&#39;,
 &#39;公司&#39;,
 &#39;建设&#39;,
 &#39;回复&#39;,
 &#39;支持&#39;,
 &#39;部门&#39;]
</code></pre></div><br>
<p>查看词表映射</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">w2v.key_to_index
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;问题&#39;: 0,
 &#39;进行&#39;: 1,
 &#39;您好&#39;: 2,
 &#39;工作&#39;: 3,
 &#39;小区&#39;: 4,
 &#39;反映&#39;: 5,
 &#39;领导&#39;: 6,
 ...
  &#39;连续&#39;: 995,
 &#39;稳定&#39;: 996,
 &#39;市住建局&#39;: 997,
 &#39;降低&#39;: 998,
 &#39;会同&#39;: 999,
 ...}
</code></pre></div><br>
<h3 id="34-获取某词的向量">3.4 获取某词的向量</h3>
<p>查找某词对应的词向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># w2v[&#39;问题&#39;]</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;问题&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-6.2813835 ,  1.5916584 , -0.48086444, -2.6446412 , 10.031776  ,
       -0.11915778, -5.039283  , -2.1107564 ,  1.1351422 , -2.881387  ,
        4.2890835 , -1.1337206 ,  3.7850847 , -3.640467  , -0.96282107,
        ...
        ...
        1.1314462 , -2.5386178 , -2.3993561 , -2.0407596 ,  0.95457   ,
        3.03732   , -2.033116  , -0.20390491,  3.5368073 ,  6.5452943 ,
        2.1186016 ,  0.79572505,  2.5855987 ,  0.88565165, -1.812104  ],
      dtype=float32)
</code></pre></div><p>受限于篇幅，这里显示词向量的部分数值。</p>
<br>
<p>需要注意，如果查询的词不存在于模型词表，则会出现报错。例如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">word = &#39;这是一个不存在的词&#39;
w2v.get_vector(word)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[130], line 2
      1 word = &#39;这是一个不存在的词&#39;
----&gt; 2 w2v.wv.get_vector(word)

File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/models/keyedvectors.py:446, in KeyedVectors.get_vector(self, key, norm)
    422 def get_vector(self, key, norm=False):
    423     &#34;&#34;&#34;Get the key&#39;s vector, as a 1D numpy array.
    424
    425     Parameters
   (...)
    444
    445     &#34;&#34;&#34;
--&gt; 446     index = self.get_index(key)
    447     if norm:
    448         self.fill_norms()

File /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/models/keyedvectors.py:420, in KeyedVectors.get_index(self, key, default)
    418     return default
    419 else:
--&gt; 420     raise KeyError(f&#34;Key &#39;{key}&#39; not present&#34;)

KeyError: &#34;Key &#39;这是一个不存在的词&#39; not present&#34;

</code></pre></div><br>
<h3 id="35-近义词">3.5 近义词</h3>
<p>根据词语查寻近义词，返回最相似的 10 个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="s1">&#39;问题&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;情况&#39;, 0.6178732514381409),
 (&#39;现象&#39;, 0.5385990142822266),
 (&#39;此类情况&#39;, 0.418301522731781),
 (&#39;留言&#39;, 0.4179410934448242),
 (&#39;一事&#39;, 0.40703579783439636),
 (&#39;事项&#39;, 0.39551448822021484),
 (&#39;事情&#39;, 0.3860214948654175),
 (&#39;情形&#39;, 0.38478103280067444),
 (&#39;事件&#39;, 0.36725184321403503),
 (&#39;现像&#39;, 0.3665226995944977)]
</code></pre></div><br>
<p>根据语义向量查寻近义词，返回最相似的 10 个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">question_vector</span> <span class="o">=</span> <span class="n">w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;问题&#39;</span><span class="p">)</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">question_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;问题&#39;, 1.0),
 (&#39;情况&#39;, 0.6178732514381409),
 (&#39;现象&#39;, 0.5385990142822266),
 (&#39;此类情况&#39;, 0.4183014929294586),
 (&#39;留言&#39;, 0.4179410934448242),
 (&#39;一事&#39;, 0.40703579783439636),
 (&#39;事项&#39;, 0.39551448822021484),
 (&#39;事情&#39;, 0.3860214948654175),
 (&#39;情形&#39;, 0.38478103280067444),
 (&#39;事件&#39;, 0.36725184321403503)]
</code></pre></div><br>
<h3 id="36-计算多个词的中心向量">3.6 计算多个词的中心向量</h3>
<p>我们可以计算「经济」、「建设」、「发展」的中心向量 eco_vector。 并试图寻找中心向量 eco_vector 的最相似的 10 个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">eco_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_centroid</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v</span><span class="p">,</span>
                                  <span class="n">words</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;经济&#39;</span><span class="p">,</span> <span class="s1">&#39;建设&#39;</span><span class="p">,</span> <span class="s1">&#39;发展&#39;</span><span class="p">])</span>


<span class="c1"># 寻找 eco_vector 语义最相似的10个词</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">eco_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;发展&#39;, 0.8317984938621521),
 (&#39;建设&#39;, 0.7508440613746643),
 (&#39;经济&#39;, 0.6406075954437256),
 (&#39;经济社会发展&#39;, 0.6385446786880493),
 (&#39;发展壮大&#39;, 0.6317417621612549),
 (&#39;化发展&#39;, 0.5961641073226929),
 (&#39;大力发展&#39;, 0.585274338722229),
 (&#39;经济腾飞&#39;, 0.5823679566383362),
 (&#39;产业&#39;, 0.5820372700691223),
 (&#39;高质量发展&#39;, 0.5803337097167969)]
</code></pre></div><p>语义捕捉的很准。</p>
<br>
<h3 id="37-概念轴">3.7 概念轴</h3>
<p>男性概念向量由多个男性词的向量加总求均值得到，女性概念向量算法类似。当性质或方向明显相反的两个概念向量相减， 得到的新的向量，我们可以称之为**<em>概念轴向量 Concept Axis</em>**。</p>
<p>将几个城市词的词向量在[冷热概念轴向量]进行投影，得到的数值越大，表示越接近于 c_words2，越寒冷。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 数值越大，表示越接近于c_words2，越寒冷。</span>
<span class="n">ct</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v</span><span class="p">,</span>
                     <span class="n">words</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;杭州&#39;</span><span class="p">,</span> <span class="s1">&#39;哈尔滨&#39;</span><span class="p">,</span> <span class="s1">&#39;广州&#39;</span><span class="p">,</span> <span class="s1">&#39;潍坊&#39;</span><span class="p">],</span>
                     <span class="n">poswords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;寒冷&#39;</span><span class="p">,</span> <span class="s1">&#39;冰雪&#39;</span><span class="p">],</span>
                     <span class="n">negwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;炎热&#39;</span><span class="p">,</span> <span class="s1">&#39;酷暑&#39;</span><span class="p">],</span>
                     <span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;杭州&#39;, -2.52), (&#39;广州&#39;, -2.06), (&#39;潍坊&#39;, 2.18), (&#39;哈尔滨&#39;, 2.78)]
</code></pre></div><br>
<p>投影体现出城市的冷热， 体现了语言模型中蕴含着人类的认知(文化、偏见、记忆)。 类似的概念轴，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 尺寸(大, 小)
- 湿度(干燥,潮湿)
- 财富(富裕, 贫穷)
- 性别(男, 女)
- 等
</code></pre></div><p>其实任意概念的向量也可看做概念轴，即该概念向量与 0 向量相减。只不过两组性质方向相反的方式得到的概念轴， 在语义上更稳定。</p>
<p><br><br></p>
<h2 id="相关资料">相关资料</h2>
<ul>
<li><a href="https://textdata.cn/blog/management_python_course/">视频课 | Python 实证指标构建与文本分析</a></li>
<li><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)数据挖掘文献资料汇总</a></li>
<li><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集汇总</a></li>
<li><a href="https://textdata.cn/blog/2025-02-14-using-online-large-model-api-to-transform-text-data-into-structured-data/">教程 | 使用 Ollama 与大模型将文本数据转化为结构化数据</a></li>
<li><a href="https://textdata.cn/blog/">https://textdata.cn/</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>文化几何学：通过词嵌入分析反映文本背后的社会文化(变迁)</title>
      <link>https://textdata.cn/blog/2025-04-23-word-embedding-reflect-human-attitude/</link>
      <pubDate>Wed, 23 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2025-04-23-word-embedding-reflect-human-attitude/</guid>
      <description>中文语料预训练模型列表， 使用 cntext2.x 训练出的预训练语言模型， 主要分 GloVe 和 Word2Vec 两种。</description>
      <content:encoded><![CDATA[<p>人类在留下语言、文字的过程中，也留下了自己的偏见、态度等主观认知信息（偏见、态度）。词嵌入做为一种词向量模型，可以隐含上下文的情景信息，态度及偏见很容易保留在词向量的某些维度中。通过词向量距离的测算，就可以间接测得不同群体 对 某概念(组织、群体、品牌、地域等)的态度偏见。</p>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载 | 大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
</ul>
<p>虽然现在LLM(大语言模型)很火，但其底层架构Transformer正是基于词向量(分布式语义表示)发展而来。LLM虽然能生成流畅的文本，但其&quot;黑箱&quot;特性使得我们难以直接分析其中蕴含的社会偏见。相比之下，传统的词嵌入模型(维度通常在50-300之间)虽然维度较高难以直观理解，但通过线性代数等数学工具，我们可以精确测量和分析词向量空间中的文化偏见和态度倾向。</p>
<p>下图所示， 在大众点评语料的词向量中蕴含着一些文化(态度或刻板印象)， 如提起<strong>旅行</strong>这件事， 大家脑海里首先想到的是一群年轻女性探索有趣的世界，世界那么大我想去看看。 而 <strong>高尔夫球</strong>， 在大家认知里是一群男性老板通过该活动社交谈生意。 </p>
<p><img loading="lazy" src="img/04-hobby.png" alt=""  />
</p>
<br>
<h2 id="一文献">一、文献</h2>
<p>这篇文献挺老的，但是算法思路目前很有启发。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Kozlowski, Austin C., Matt Taddy, and James A. Evans. &#34;The geometry of culture: Analyzing the meanings of class through word embeddings.&#34; American Sociological Review 84, no. 5 (2019): 905-949.
</code></pre></div><h3 id="摘要">摘要</h3>
<p>词嵌入模型是研究文化的有效工具，并以历史分析中对社会阶级的共同理解为实证案例。词嵌入模型通过将语义关系表示为高维空间中的向量关系，提供了一种与当代文化理论一致的关系模型。在这些空间中，词差异（如“富裕-贫穷”）所诱导的维度对应于文化意义的维度，而词在这些维度上的投影反映了广泛共享的文化关联——我们通过调查验证了这一点。通过分析过去一百年来出版的数百万本书籍的文本，我们发现阶级的标志在经济转型中不断变化，但阶级的基本文化维度保持显著稳定。值得注意的是，教育成为与富裕紧密相关的因素，独立于其与高雅品味的关联。</p>
<br>
<h3 id="研究目的">研究目的</h3>
<p>验证词嵌入模型是否能够准确捕捉文化维度（如富裕、性别、种族等），并通过与人类评估的文化关联数据进行对比，证明其在社会学分析中的有效性。</p>
<br>
<h3 id="研究设计">研究设计</h3>
<ol>
<li><strong>语料准备</strong>:  基于Google Ngram语料库训练，包含1900-2012年间出版的数百万本书籍的文本数据。</li>
<li><strong>​词嵌入模型</strong>​​: 基于语料训练词嵌入模型(GloVe或Word2Vec），这篇文章使用的Word2Vec。 词嵌入算法将文本中的单词表示为高维空间中的向量。这些向量基于单词在文本中的上下文关系，共享相似上下文的单词在空间中位置相近。</li>
<li><strong>​文化维度的识别</strong>​​: 通过计算反义词对的平均值来识别文化维度。例如，通过计算“rich”和“poor”等反义词对的差值，构建“富裕-贫穷”这一文化维度。</li>
<li><strong>词向量投影</strong>​​: 将单词对应的词向量投影到特定的概念轴（文化维度，如性别、财富）向量上，计算其在该维度上的关联强度。 投影值通过余弦相似度衡量，正值表示与某一文化维度（如富裕）的正向关联，负值表示负向关联。</li>
<li><strong>验证方法</strong>​​: 要求受访者对59个词汇（如“banker”“jazz”“nurse”）在三个维度（阶级、种族、性别）上进行评分(0~100)。例如“在0到100的范围内，您认为‘芭蕾舞’在多大程度上属于上层阶级？”； 通过调查数据验证词嵌入模型在捕捉文化关联方面的有效性。比较词嵌入模型与人类评估的文化关联数据，计算Pearson相关系数。</li>
<li>​<strong>静态​结果可视化​</strong>​: 通过图表展示运动词在不同文化维度(性别维度、财富维度)上的投影结果。</li>
</ol>
<p>以上步骤证明了词嵌入投影算法捕捉人类社会文化(文本中蕴含的文化线索)的有效性，接下来按每10年构建一个语料(1900~2010)， 训练出不同年代的词向量。财富维度与六种阶级维度(教育、培养、地位、道德、职业、性别)余弦相似度的关系。 下图是六个维度的正反义词对儿。</p>
<p><img loading="lazy" src="img/01-word-pairs.png" alt=""  />
</p>
<br>
<h3 id="研究结果">研究结果</h3>
<ol>
<li><strong>​文化维度的有效性​</strong>​: 词嵌入模型在捕捉文化关联方面表现出色，与人类评估的相关系数在0.53到0.90之间。性别维度的关联最强，种族维度的关联较弱。</li>
<li><strong>多维度的阶级结构</strong>​​: 阶级的文化维度形成了一个复杂但稳定的语义结构，包括财富、地位、教育、道德等维度。这些维度在高维空间中相互关联，无法通过低维空间准确表示。</li>
<li><strong>社会阶级的文化维度演变​</strong>​: 分析结果显示，社会阶级的文化维度在二十世纪保持稳定，但具体的文化标记（如职业名称）发生了显著变化。教育和富裕之间的关联逐渐增强，成为阶级划分的重要标志。
<img loading="lazy" src="img/02-apa-proj.png" alt=""  />

<img loading="lazy" src="img/03-evolution.png" alt=""  />
</li>
</ol>
<p><br><br></p>
<h2 id="二实验准备">二、实验准备</h2>
<h3 id="21-训练模型">2.1 训练模型</h3>
<p>使用 <a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">cntext2.x</a> 训练出的预训练语言模型， 具体可参考</p>
<p>不考虑时间(语义演变)， 只训练一个模型
<a href="https://textdata.cn/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/">词向量 | 使用1亿B站用户签名训练word2vec词向量</a>
<a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">词向量 | 使用Stanford Glove代码训练中文语料的Glove模型</a>
<a href="https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/">词向量 | 使用1985年-2025年专利申请摘要训练 Word2Vec 模型</a>
<a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">词向量 | 使用 MD&amp;A2001-2023 语料训练 Word2Vec/GloVe 模型</a>
<a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量 | 使用人民网领导留言板语料训练 Word2Vec 模型</a></p>
<br>
<p>考虑时间因素， 按某个时间间隔(如每10年)，训练一个年代向量</p>
<p><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></p>
<br>
<p>如果觉得训练太麻烦， 大邓将已经训练好的模型免费提供给大家。
<a href="https://github.com/hiDaDeng/Chinese-Pretrained-Word-Embeddings">免费资源 | cntext2.x 训练出的免费公开词向量</a></p>
<h3 id="22-读取模型">2.2 读取模型</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 模型下载地址</span>
<span class="c1"># https://github.com/hiDaDeng/Chinese-Pretrained-Word-Embeddings</span>
<span class="n">wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;大众点评-评论-GloVe.200.15.bin&#39;</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="23-获取词向量">2.3 获取词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;模型词汇量: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">wv</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">模型词汇量: 278565
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;富有&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;富有&#39;</span><span class="p">])</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(200,)
array([-0.52744 , -0.108866, -0.119827, -0.644396, -0.342953, -0.503506,
       -0.453796, -0.213651,  0.041335,  0.345231,  0.4752  , -0.026904,
       -0.026971, -0.249429, -1.115758,  0.351041, -0.304552,  0.40272 ,
       ......
       ......
       -0.061966,  0.384454,  0.280508, -0.005171, -0.236791,  0.171627,
        0.151691, -0.295215,  0.233423, -0.146419, -0.210322, -0.338783,
        0.214728, -0.101312,  0.489487, -0.257294,  0.732999,  0.057721,
       -0.286473,  0.394552], dtype=float32)
</code></pre></div><p>词向量的维度是200，即每个词的语义是由200个数字组成的向量所表示。</p>
<br>
<h3 id="24-计算概念轴向量">2.4 计算概念轴向量</h3>
<p>概念轴向量为例，如何计算呢？以性别为例，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1. 先找出性别(男、女)的正反义词对儿
2. 分别计算正词的多个向量、负词的多个词向量
3. 求得正均值向量、负均值向量
4. 两者相减、归一化处理后得到性别概念向量。  
</code></pre></div><p>大邓将这些步骤封装到cntext2.x中，只需要将词语传入即可</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 性别概念轴向量</span>
<span class="n">gender_poss</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;男人&#39;</span><span class="p">,</span> <span class="s1">&#39;男性&#39;</span><span class="p">,</span> <span class="s1">&#39;丈夫&#39;</span><span class="p">,</span> <span class="s1">&#39;他&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">,</span> <span class="s1">&#39;祖父&#39;</span><span class="p">,</span> <span class="s1">&#39;爸爸&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;儿子&#39;</span><span class="p">,</span> <span class="s1">&#39;兄弟&#39;</span><span class="p">]</span>
<span class="n">gender_negs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;女人&#39;</span><span class="p">,</span> <span class="s1">&#39;女性&#39;</span><span class="p">,</span> <span class="s1">&#39;妻子&#39;</span><span class="p">,</span> <span class="s1">&#39;她&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">,</span> <span class="s1">&#39;祖母&#39;</span><span class="p">,</span> <span class="s1">&#39;妈妈&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;女儿&#39;</span><span class="p">,</span> <span class="s1">&#39;姐妹&#39;</span><span class="p">]</span>
<span class="n">gender_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">generate_concept_axis</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                                         <span class="n">poswords</span><span class="o">=</span><span class="n">gender_poss</span><span class="p">,</span> 
                                         <span class="n">negwords</span><span class="o">=</span><span class="n">gender_negs</span><span class="p">)</span>


<span class="c1"># 财富概念轴向量</span>
<span class="n">affluence_poss</span> <span class="o">=</span><span class="p">[</span><span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;有钱&#39;</span><span class="p">,</span> <span class="s1">&#39;成功&#39;</span><span class="p">,</span> <span class="s1">&#39;发达&#39;</span><span class="p">,</span> <span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;优势&#39;</span><span class="p">,</span> <span class="s1">&#39;高贵&#39;</span><span class="p">,</span> <span class="s1">&#39;高端&#39;</span><span class="p">,</span> <span class="s1">&#39;昂贵&#39;</span><span class="p">,</span> <span class="s1">&#39;华丽&#39;</span><span class="p">,</span> <span class="s1">&#39;精致&#39;</span><span class="p">,</span> <span class="s1">&#39;奢侈&#39;</span><span class="p">,</span> <span class="s1">&#39;奢华&#39;</span><span class="p">,</span> <span class="s1">&#39;充裕&#39;</span><span class="p">,</span> <span class="s1">&#39;豪华&#39;</span><span class="p">]</span>
<span class="n">affluence_negs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;没钱&#39;</span><span class="p">,</span> <span class="s1">&#39;失败&#39;</span><span class="p">,</span> <span class="s1">&#39;落后&#39;</span><span class="p">,</span> <span class="s1">&#39;贫困&#39;</span><span class="p">,</span> <span class="s1">&#39;劣势&#39;</span><span class="p">,</span> <span class="s1">&#39;卑贱&#39;</span><span class="p">,</span> <span class="s1">&#39;低端&#39;</span><span class="p">,</span> <span class="s1">&#39;廉价&#39;</span><span class="p">,</span> <span class="s1">&#39;朴素&#39;</span><span class="p">,</span> <span class="s1">&#39;粗糙&#39;</span><span class="p">,</span> <span class="s1">&#39;廉价&#39;</span><span class="p">,</span> <span class="s1">&#39;节俭&#39;</span><span class="p">,</span> <span class="s1">&#39;匮乏&#39;</span><span class="p">,</span> <span class="s1">&#39;破旧&#39;</span><span class="p">]</span>
<span class="n">affluence_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">generate_concept_axis</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                                            <span class="n">poswords</span><span class="o">=</span><span class="n">affluence_poss</span><span class="p">,</span> 
                                            <span class="n">negwords</span><span class="o">=</span><span class="n">affluence_negs</span><span class="p">)</span>

<span class="c1"># 查看性别概念轴向量</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gender_vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gender_vector</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">(200,)
[-0.11656909 -0.19618881 -0.01077267  0.04915987  0.00569247  0.05462526
 -0.009799    0.00770712  0.05658354  0.04547084  0.03688154 -0.02133968
 -0.0706896   0.08739712  0.11174724 -0.02057768  0.03183764  0.01165388
  ......
  ......
  0.0101583   0.09426635 -0.09078085 -0.13099451 -0.02234778  0.03765206
  0.1083525   0.07751778  0.04983377  0.03304265 -0.05442946  0.11609897
 -0.10463558  0.00224418  0.00210647 -0.04888193  0.01931083  0.07366373
 -0.01534469  0.06682201]
</code></pre></div><p>注意:</p>
<ol>
<li>词向量、 概念轴向量维度是相同的，在本文案例中都是200.</li>
<li>注意概念正反义词对方向的确定， 方向决定了对计算结果正负号数字的解读。 例如性别概念轴维度，将男性确定为正义词， 任意词的词向量与性别概念轴计算投影(或余弦相似度)， 数值越大， 说明该词与男性的相关性越大。</li>
</ol>
<h3 id="25-计算投影">2.5 计算投影</h3>
<p>cntext2.x封装了投影计算，只需要传入词语或词向量即可。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">project_word</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">cosine</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div><p>在向量空间中， 计算词语 a 在词语 b 上的投影。</p>
<ul>
<li><strong>wv</strong> 语料 txt 文件路径</li>
<li><strong>a</strong> 词语 a 字符串或列表</li>
<li><strong>b</strong> 词语字符串、词语列表、或某概念向量</li>
<li><strong>cosine</strong> 是否使用余弦相似度， 默认为False， 函数计算结果为a在b上的投影值。 如果为True， 函数计算结果为a与b的余弦相似度。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 获取词向量文件</span>
<span class="c1"># https://github.com/hiDaDeng/Chinese-Pretrained-Word-Embeddings</span>
<span class="n">dm_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;douban-movie-1000w-Word2Vec.200.15.bin&#39;</span><span class="p">)</span>

<span class="n">b</span><span class="o">=</span><span class="s1">&#39;苗条&#39;</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;性感&#39;</span><span class="p">,</span><span class="s1">&#39;美丽&#39;</span><span class="p">,</span> <span class="s1">&#39;可爱&#39;</span><span class="p">,</span> <span class="s1">&#39;丑陋&#39;</span><span class="p">]:</span>
    <span class="n">proj</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">project_word</span><span class="p">(</span><span class="n">dm_w2v</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;[</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s1">]在[</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1">]投影值: </span><span class="si">{</span><span class="n">proj</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>


<span class="n">b</span><span class="o">=</span><span class="s1">&#39;修长&#39;</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;性感&#39;</span><span class="p">,</span><span class="s1">&#39;美丽&#39;</span><span class="p">,</span> <span class="s1">&#39;可爱&#39;</span><span class="p">,</span> <span class="s1">&#39;丑陋&#39;</span><span class="p">]:</span>
    <span class="n">proj</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">project_word</span><span class="p">(</span><span class="n">dm_w2v</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;[</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s1">]在[</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1">]投影值: </span><span class="si">{</span><span class="n">proj</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[性感]在[苗条]投影值: 14.172947883605957
[美丽]在[苗条]投影值: 7.0944623947143555
[可爱]在[苗条]投影值: 6.935092926025391
[丑陋]在[苗条]投影值: 1.235807180404663

[性感]在[修长]投影值: 14.599699974060059
[美丽]在[修长]投影值: 9.360642433166504
[可爱]在[修长]投影值: 4.740543842315674
[丑陋]在[修长]投影值: 4.010622501373291
</code></pre></div><p>可以看到， 在豆瓣电影语料中， 在[苗条、修长]维度的认知中，都认为</p>
<ul>
<li>[性感]意味着身材最瘦长</li>
<li>[美丽]次之、[可爱]略显不那么修长苗条</li>
<li>[丑陋]意味着基本与[苗条、修长]无关，数值最小。</li>
</ul>
<br>
<h2 id="三实验可视化">三、实验可视化</h2>
<h3 id="31-静态可视化">3.1 静态可视化</h3>
<p>不考虑时间因素，将所有语料训练得出一个词向量， 在这个词向量基础上进行语义投影可视化。</p>
<p>这里用大众点评评论语料训练出的词向量为例，进行爱好词、品牌词、美食词在性别维度、财富维度的投影。看看这些词（爱好词、品牌词、美食词）是否体现出性别差异、财富差异。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">ct</span><span class="o">.</span><span class="n">matplotlib_chinese</span><span class="p">()</span> <span class="c1"># 确保中文显示</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>  <span class="c1"># 使用内置的 ggplot 风格作为基础</span>



<span class="c1"># ====== 用户已经完成的数据准备部分（假设已运行）======</span>
<span class="n">wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;大众点评-评论-GloVe.200.15.bin&#39;</span><span class="p">)</span>
<span class="n">gender_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">generate_concept_axis</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span>
                                          <span class="n">poswords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;男人&#39;</span><span class="p">,</span> <span class="s1">&#39;男性&#39;</span><span class="p">,</span> <span class="s1">&#39;丈夫&#39;</span><span class="p">,</span> <span class="s1">&#39;他&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">,</span> <span class="s1">&#39;祖父&#39;</span><span class="p">,</span> <span class="s1">&#39;爸爸&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;儿子&#39;</span><span class="p">,</span> <span class="s1">&#39;兄弟&#39;</span><span class="p">],</span>
                                          <span class="n">negwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;女人&#39;</span><span class="p">,</span> <span class="s1">&#39;女性&#39;</span><span class="p">,</span> <span class="s1">&#39;妻子&#39;</span><span class="p">,</span> <span class="s1">&#39;她&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">,</span> <span class="s1">&#39;祖母&#39;</span><span class="p">,</span> <span class="s1">&#39;妈妈&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;女儿&#39;</span><span class="p">,</span> <span class="s1">&#39;姐妹&#39;</span><span class="p">])</span>
<span class="n">affluence_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">generate_concept_axis</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span>
                                          <span class="n">poswords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;富有&#39;</span><span class="p">,</span> <span class="s1">&#39;有钱&#39;</span><span class="p">,</span> <span class="s1">&#39;成功&#39;</span><span class="p">,</span> <span class="s1">&#39;发达&#39;</span><span class="p">,</span> <span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;优势&#39;</span><span class="p">,</span> <span class="s1">&#39;高贵&#39;</span><span class="p">,</span> <span class="s1">&#39;高端&#39;</span><span class="p">,</span> <span class="s1">&#39;昂贵&#39;</span><span class="p">,</span> <span class="s1">&#39;华丽&#39;</span><span class="p">,</span> <span class="s1">&#39;精致&#39;</span><span class="p">,</span> <span class="s1">&#39;奢侈&#39;</span><span class="p">,</span> <span class="s1">&#39;奢华&#39;</span><span class="p">,</span> <span class="s1">&#39;充裕&#39;</span><span class="p">,</span> <span class="s1">&#39;豪华&#39;</span><span class="p">],</span>
                                          <span class="n">negwords</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;没钱&#39;</span><span class="p">,</span> <span class="s1">&#39;失败&#39;</span><span class="p">,</span> <span class="s1">&#39;落后&#39;</span><span class="p">,</span> <span class="s1">&#39;贫困&#39;</span><span class="p">,</span> <span class="s1">&#39;劣势&#39;</span><span class="p">,</span> <span class="s1">&#39;卑贱&#39;</span><span class="p">,</span> <span class="s1">&#39;低端&#39;</span><span class="p">,</span> <span class="s1">&#39;廉价&#39;</span><span class="p">,</span> <span class="s1">&#39;朴素&#39;</span><span class="p">,</span> <span class="s1">&#39;粗糙&#39;</span><span class="p">,</span> <span class="s1">&#39;廉价&#39;</span><span class="p">,</span> <span class="s1">&#39;节俭&#39;</span><span class="p">,</span> <span class="s1">&#39;匮乏&#39;</span><span class="p">,</span> <span class="s1">&#39;破旧&#39;</span><span class="p">])</span>


<span class="n">words</span> <span class="o">=</span>  <span class="p">[</span><span class="s2">&#34;象棋&#34;</span><span class="p">,</span> <span class="s2">&#34;麻将&#34;</span><span class="p">,</span> <span class="s2">&#34;围棋&#34;</span><span class="p">,</span> <span class="s2">&#34;高尔夫&#34;</span><span class="p">,</span> <span class="s2">&#34;武术&#34;</span><span class="p">,</span> <span class="s2">&#34;潜水&#34;</span><span class="p">,</span> <span class="s2">&#34;书法&#34;</span><span class="p">,</span> <span class="s2">&#34;瑜伽&#34;</span><span class="p">,</span> <span class="s2">&#34;羽毛球&#34;</span><span class="p">,</span> <span class="s2">&#34;马术&#34;</span><span class="p">,</span> <span class="s2">&#34;网球&#34;</span><span class="p">,</span> <span class="s2">&#34;美妆&#34;</span><span class="p">,</span> <span class="s2">&#34;旅行&#34;</span><span class="p">]</span>
<span class="c1"># words =  [&#34;烧烤&#34;, &#34;寿司&#34;, &#34;牛排&#34;, &#34;白酒&#34;, &#34;啤酒&#34;, &#34;麻辣烫&#34;, &#34;汉堡&#34;, &#34;煎饼&#34;, &#34;包子&#34;, &#34;小米粥&#34;, &#34;沙拉&#34;, &#34;披萨&#34;]</span>
<span class="c1"># words =  [&#34;阿玛尼&#34;, &#34;coach&#34;, &#34;lv&#34;, &#34;耐克&#34;, &#34;阿迪&#34;, &#34;爱马仕&#34;, &#34;优衣库&#34;, &#34;海澜&#34;]</span>

<span class="n">gender_proj</span> <span class="o">=</span> <span class="p">[</span><span class="n">ct</span><span class="o">.</span><span class="n">project_word</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">word</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">gender_vector</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
<span class="n">affluence_proj</span> <span class="o">=</span> <span class="p">[</span><span class="n">ct</span><span class="o">.</span><span class="n">project_word</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">word</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">affluence_vector</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
<span class="c1"># ========================================================</span>


<span class="c1"># ====== 绘图部分 ======</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span> <span class="c1"># 获取当前 axes 对象，方便后续操作，特别是设置限制</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;爱好的(性别-财富)刻板印象&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.titlesize&#39;</span><span class="p">])</span> <span class="c1"># 使用样式中定义的标题字号和 padding</span>

<span class="c1"># 设置图表显示范围，略大于数据范围，为轴标签和箭头留出空间</span>
<span class="c1"># 先根据数据计算一个合理的范围，再根据需求调整</span>
<span class="n">x_data_min</span><span class="p">,</span> <span class="n">x_data_max</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">affluence_proj</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">affluence_proj</span><span class="p">)</span>
<span class="n">y_data_min</span><span class="p">,</span> <span class="n">y_data_max</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">gender_proj</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">gender_proj</span><span class="p">)</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="n">x_data_max</span> <span class="o">-</span> <span class="n">x_data_min</span>
<span class="n">y_range</span> <span class="o">=</span> <span class="n">y_data_max</span> <span class="o">-</span> <span class="n">y_data_min</span>

<span class="c1"># 可以设置固定范围，或者根据数据范围动态计算</span>
<span class="n">x_lims</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_data_min</span> <span class="o">-</span> <span class="n">x_range</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">x_data_max</span> <span class="o">+</span> <span class="n">x_range</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">y_lims</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_data_min</span> <span class="o">-</span> <span class="n">y_range</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">y_data_max</span> <span class="o">+</span> <span class="n">y_range</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="c1"># 或者设置一个固定的、对称的范围，例如：</span>
<span class="n">max_abs_x</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">x_data_min</span><span class="p">),</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x_data_max</span><span class="p">))</span>
<span class="n">max_abs_y</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">y_data_min</span><span class="p">),</span> <span class="nb">abs</span><span class="p">(</span><span class="n">y_data_max</span><span class="p">))</span>
<span class="n">plot_lim</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_abs_x</span><span class="p">,</span> <span class="n">max_abs_y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.3</span> <span class="c1"># 确保范围包含所有点并有余量</span>
<span class="n">x_lims</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">plot_lim</span><span class="p">,</span> <span class="n">plot_lim</span><span class="p">)</span>
<span class="n">y_lims</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">plot_lim</span><span class="p">,</span> <span class="n">plot_lim</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">x_lims</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">y_lims</span><span class="p">)</span>


<span class="c1"># 绘制散点</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">affluence_proj</span><span class="p">,</span> <span class="n">gender_proj</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span> <span class="c1"># 调整点大小和颜色，使其清晰</span>

<span class="c1"># 绘制中心轴线 (0,0)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 生成网格线 (使用更新后的图表限制)</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">xx</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">yy</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray_r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>



<span class="c1"># 添加轴端点标签和箭头 (更像图 3 的风格)</span>
<span class="n">arrow_length_x</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.95</span> <span class="c1"># 箭头长度为范围的 90%</span>
<span class="n">arrow_length_y</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.95</span>
<span class="n">head_width_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.015</span> <span class="c1"># 箭头头部宽度根据轴范围调整</span>
<span class="n">head_width_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.015</span>

<span class="c1"># Affluence 轴 (X) - 贫穷到富有</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">arrow_length_x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="n">head_width_y</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="n">head_width_x</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">length_includes_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">arrow_length_x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="n">head_width_y</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="n">head_width_x</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">length_includes_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># Gender 轴 (Y) - 女性到男性</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">arrow_length_y</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="n">head_width_x</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="n">head_width_y</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">length_includes_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">arrow_length_y</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="n">head_width_x</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="n">head_width_y</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">length_includes_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>


<span class="c1"># 添加词语标签</span>
<span class="c1"># 遍历数据点和词语</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">affluence_proj</span><span class="p">,</span> <span class="n">gender_proj</span><span class="p">,</span> <span class="n">words</span><span class="p">)):</span>
     <span class="c1"># 可以尝试 xytext 偏移来控制标签位置</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s2">&#34;offset points&#34;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">])</span>


<span class="c1"># 设置轴标题</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Affluence (</span><span class="si">{</span><span class="nb">chr</span><span class="p">(</span><span class="mi">8592</span><span class="p">)</span><span class="si">}</span><span class="s1"> 贫穷 | 富有 </span><span class="si">{</span><span class="nb">chr</span><span class="p">(</span><span class="mi">8594</span><span class="p">)</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.labelsize&#39;</span><span class="p">])</span> <span class="c1"># 使用箭头符号更直观</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Gender (</span><span class="si">{</span><span class="nb">chr</span><span class="p">(</span><span class="mi">8595</span><span class="p">)</span><span class="si">}</span><span class="s1"> 女性化程度 | 男性化程度 </span><span class="si">{</span><span class="nb">chr</span><span class="p">(</span><span class="mi">8593</span><span class="p">)</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.labelsize&#39;</span><span class="p">])</span> <span class="c1"># 使用箭头符号更直观</span>


<span class="c1"># 样式中已经设置了网格，如果想自定义，可以取消注释下一行</span>
<span class="c1">#plt.grid(True, linestyle=&#39;--&#39;, alpha=0.5)</span>

<span class="c1"># 确保图表元素的布局紧凑</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="c1"># 显示图表</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/04-hobby.png" alt=""  />

爱好词在性别维度向量、财富维度向量投影结果解读。</p>
<ul>
<li>性别维度，旅行、瑜伽、美妆这几个爱好词与女性强相关。高尔夫、网球、羽毛球、麻将、书法与男性强相关。</li>
<li>财富维度， 富裕的爱好有瑜伽、高尔夫、旅行、网球、麻将、羽毛球。而贫穷的爱好有象棋、美妆、武术、围棋，似乎不用太花钱。</li>
</ul>
<p><img loading="lazy" src="img/05-food.png" alt=""  />
</p>
<p>美食词在性别维度向量、财富维度向量投影结果解读。饮食方面投影主要分布在图的右侧。</p>
<ul>
<li>性别维度，啤酒、白酒、烧烤很男性； 而披萨、沙拉、寿司、牛排很女性。</li>
<li>财富维度，牛排、寿计较富裕， 而小米粥很贫穷。 食物的财富维度区分度较低。</li>
</ul>
<p>总的来说， 食物中，好吃的、贵的跟女性关联度远大于男性。</p>
<p><img loading="lazy" src="img/06-brand.png" alt=""  />

品牌词在性别维度向量、财富维度向量投影结果解读。</p>
<ul>
<li>性别维度, 耐克、阿迪、海澜之家、阿玛尼与男性关联度更高。而优衣库、lv、爱马仕、coach与女性的关联度更高。</li>
<li>财富维度, lv、coach、爱马仕在语义上与富裕强相关，而耐克、阿迪、海澜之家与贫穷强相关。</li>
</ul>
<p>总之，静态的分析，通过大众点评评论语料， 可以体现出目前社会消费领域中， 对于品牌、美食、爱好的认知、文化、刻板印象。  如果有不同年代的语料， 就可以挖掘文化的变化。</p>
<br>
<h3 id="32-考虑时间因素">3.2 考虑时间因素</h3>
<p>以人民日报语料为例， 每10年训练一个词向量， 观察不同年份的语义的变化。 训练代码可阅读 <a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a> 。  文中以语义距离来刻画文化，即</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">distance = distance(女, 成功) - distance(男, 成功)
</code></pre></div><ul>
<li>如果distance趋近于0， 男女在成功概念上语义接近， 无明显刻板印象。</li>
<li>但是当distance明显大于0， 当人们聊到成功概念时，更容易联想到男性，而不是女性。</li>
</ul>
<br>
<p><strong>性别与成就</strong></p>
<p><img loading="lazy" src="img/07-gender.png" alt=""  />
</p>
<p>从图中可以看到， 新中国初期， 我国的女性解放运动在全世界都是领先的，成果十分卓著。 而今耳熟能详的口号恰好说明当时的宣传已经刻入每个中国人的认知中，如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 谁说女子不如男
- 不爱红装爱武装
- 女人撑起半边天
...
</code></pre></div><p>提到「成功概念」时，在新中国初期，由于破除性别刻板印象，宣传更加中性， 立榜样考虑了性别的平衡。而随着时间推移，口号式的宣传运动沉寂后， 历史的惯性(传统文化的基因)可能会重新复活， 提到「成功概念」时，社会更容易将「成功」与「男性」联系起来。</p>
<br>
<p><strong>性别与责任</strong></p>
<p>成就与男性有更高的关联， 背后是否意味着传统文化建构的社会要求男性承担远多于女性的责任。</p>
<p><img loading="lazy" src="img/08-responsibility.png" alt=""  />
</p>
<p>从图中可以看出，在大多数年份， distance是大于0的，即 提到「责任」概念时，社会更容易联想到「男性」，而不是「女性」。</p>
<p><br><br></p>
<h2 id="相关资料">相关资料</h2>
<ul>
<li><a href="https://textdata.cn/blog/management_python_course/">视频课 | Python 实证指标构建与文本分析</a></li>
<li><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)数据挖掘文献资料汇总</a></li>
<li><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集汇总</a></li>
<li><a href="https://textdata.cn/blog/2025-02-14-using-online-large-model-api-to-transform-text-data-into-structured-data/">教程 | 使用 Ollama 与大模型将文本数据转化为结构化数据</a></li>
<li><a href="https://textdata.cn/blog/">https://textdata.cn/</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>案例 |  使用裁判文书语料训练GloVe词向量</title>
      <link>https://textdata.cn/blog/2025-04-17-training-a-glove-model-using-china-judgements-corpus/</link>
      <pubDate>Thu, 17 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2025-04-17-training-a-glove-model-using-china-judgements-corpus/</guid>
      <description>&lt;p&gt;前阵子分享了 &lt;a href=&#34;https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/&#34;&gt;实验 | 使用 Stanford Glove 代码训练中文语料的 Glove 模型&lt;/a&gt; ，后来我偷偷的修改了这篇技术文， 将 C 代码封装到 cntext2.x 内， 原来训练代码行几十行， 现在只需要两行就可以训练 GloVe 模型。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GloVe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;语料文件.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一检查数据&#34;&gt;一、检查数据&lt;/h2&gt;
&lt;p&gt;裁判文书数据集，每个月份存储到一个 csv， 每个年份有一个对应的文件夹。下图是 2021 年的文件夹截图
&lt;img loading=&#34;lazy&#34; src=&#34;img/2021.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;csv 字段格式是一致的，我们只需要找一个文件，尝试着读取前 5 行，查看数据中有哪些字段。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2013/2013-01.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nrows&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;文书内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二训练词向量&#34;&gt;二、训练词向量&lt;/h2&gt;
&lt;h3 id=&#34;21-构造语料&#34;&gt;2.1 构造语料&lt;/h3&gt;
&lt;p&gt;我们只从 csv 中选取 &amp;ldquo;&lt;strong&gt;文书内容&lt;/strong&gt;&amp;rdquo; ，并将其存储到语料 txt 文件中。但全部裁判文书数据量高达 300G， 我希望文本语料控制在 10G 左右。&lt;/p&gt;
&lt;p&gt;2010/2011/2013 这三个年度的数据只有几百 M， 数据全部保留。 剩下的年份，设置不同的抽样比例，尽可能将每年生成的语料 txt 文件控制在 1G 左右。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;年份&lt;/th&gt;
&lt;th&gt;解压后文件大小&lt;/th&gt;
&lt;th&gt;抽样比例&lt;/th&gt;
&lt;th&gt;语料 txt 大小&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2010&lt;/td&gt;
&lt;td&gt;761M&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;684M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2011&lt;/td&gt;
&lt;td&gt;452M&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;396M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2012&lt;/td&gt;
&lt;td&gt;757M&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;665M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2013&lt;/td&gt;
&lt;td&gt;5.13G&lt;/td&gt;
&lt;td&gt;20%&lt;/td&gt;
&lt;td&gt;984M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2014&lt;/td&gt;
&lt;td&gt;23.7G&lt;/td&gt;
&lt;td&gt;4%&lt;/td&gt;
&lt;td&gt;905M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2015&lt;/td&gt;
&lt;td&gt;33.6G&lt;/td&gt;
&lt;td&gt;3%&lt;/td&gt;
&lt;td&gt;968M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2016&lt;/td&gt;
&lt;td&gt;39.9G&lt;/td&gt;
&lt;td&gt;2.4%&lt;/td&gt;
&lt;td&gt;914M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2017&lt;/td&gt;
&lt;td&gt;44.6G&lt;/td&gt;
&lt;td&gt;2.2%&lt;/td&gt;
&lt;td&gt;882M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2018&lt;/td&gt;
&lt;td&gt;24.8G&lt;/td&gt;
&lt;td&gt;4%&lt;/td&gt;
&lt;td&gt;875M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2019&lt;/td&gt;
&lt;td&gt;48.3G&lt;/td&gt;
&lt;td&gt;2%&lt;/td&gt;
&lt;td&gt;833M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2020&lt;/td&gt;
&lt;td&gt;91.2G&lt;/td&gt;
&lt;td&gt;1%&lt;/td&gt;
&lt;td&gt;779M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2021&lt;/td&gt;
&lt;td&gt;32.3G&lt;/td&gt;
&lt;td&gt;3%&lt;/td&gt;
&lt;td&gt;816M&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tqdm&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tqdm&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 年份、抽样比例&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_fracs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2010&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2011&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2012&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2013&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2014&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.04&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2015&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.03&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2016&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2017&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.022&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2018&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.04&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2019&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2020&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.01&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2021&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.03&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;



&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;裁判文书.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frac&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tqdm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_fracs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.csv&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;csvfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;# 为节省内存开销，&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;# 只读 csv 中的 “文书内容” 一个字段，&lt;/span&gt;
            &lt;span class=&#34;c1&#34;&gt;# 且设置 chunksize 分批次读取&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;csvf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;usecols&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;文书内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunksize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dropna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;文书内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;inplace&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;文书内容&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-训练-glove&#34;&gt;2.2 训练 GloVe&lt;/h3&gt;
&lt;p&gt;使用 cntext2.x 对代码进行了优化， 几个 G 的语料在 cntext 内预处理时候不会一次性读取全部内容，所以一般情况不会出现内存溢出问题。&lt;/p&gt;
&lt;p&gt;基于语料 &lt;strong&gt;&lt;em&gt;裁判文书.txt&lt;/em&gt;&lt;/strong&gt; 训练 GloVe 词嵌入语言模型 ，参数 window_size=15, vector_size=200, 结果会自动保存到 output 文件夹内。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-corpus.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;使用 &lt;strong&gt;&lt;em&gt;cntext2.1.6&lt;/em&gt;&lt;/strong&gt;， 代码如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GloVe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;裁判文书.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Mac(Linux) System, Enable Parallel Processing
Cache output/裁判文书_cache.txt Not Found, Preprocessing Corpus
Processing Corpus: 100%|██████████████████| 2502938/2502938 [26:37&amp;lt;00:00, 1566.54it/s]
Reading Preprocessed Corpus from output/裁判文书_cache.txt
Start Training GloVe
GloVe Training Cost 1223s.
Output Saved To: output/裁判文书-Word2Vec.200.15.bin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;训练总耗时 1223s， 约 20 分钟。模型保存在 &lt;strong&gt;&lt;em&gt;output/裁判文书-Word2Vec.200.15.bin&lt;/em&gt;&lt;/strong&gt;， 该模型文件大小约为 1.58G。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-评估模型&#34;&gt;2.3 评估模型&lt;/h3&gt;
&lt;p&gt;使用近义法和类比法， 判断模型的表现。详情可查看&lt;a href=&#34;https://cntext.readthedocs.io/zh-cn/latest/model.html&#34;&gt;文档&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evaluate_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt
Processing Similarity Test: 100%|███████████| 537/537 [00:00&amp;lt;00:00, 131978.28it/s]

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&amp;#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   432    |    105     |            0.37            |
+----------+------------+----------------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;近义测试&lt;/strong&gt;: Spearman&amp;rsquo;s Rank Coeficient 系数取值[-1, 1], 取值越大， 说明模型表现越好。&lt;br&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evaluate_analogy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|████████████████| 1198/1198 [00:48&amp;lt;00:00, 24.75it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   507    |    170     |    7.69    |   4.38   |
|   CityInProvince   |   175    |     0      |   98.86    |   1.39   |
| FamilyRelationship |   272    |     0      |   73.53    |   1.56   |
|   SocialScience    |    8     |     62     |   25.00    |   7.00   |
+--------------------+----------+------------+------------+----------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;类比测试&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CapitalOfCountries 裁判文书语料在此项表现较差， 应该是数据库中涉外的案件较少。&lt;/li&gt;
&lt;li&gt;CityInProvince 裁判文书语料在此项表现如此优异，是因为几乎全为国内案件， 而案件描述一般会交待案发的省市等信息。&lt;/li&gt;
&lt;li&gt;FamilyRelationship 裁判文书语料中表现较好， 可能很多的案件会描述案件相关社会关系。&lt;/li&gt;
&lt;li&gt;SocialScience 裁判文书语料在此项表现一般， 应该是语料中常见的社会科学词语提及较少。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整体而言，语料训练的效果很不错，抓住了数据场景的独特性语义。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三使用-glove&#34;&gt;三、使用 GloVe&lt;/h2&gt;
&lt;h3 id=&#34;31-导入模型&#34;&gt;3.1 导入模型&lt;/h3&gt;
&lt;p&gt;使用 cntext2.1.6 读取很简单， 代码如下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/裁判文书-Word2Vec.200.15.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;模型词汇量: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&amp;lt;class &amp;#39;gensim.models.keyedvectors.KeyedVectors&amp;#39;&amp;gt;
模型词汇量: 2099102
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-keyedvectors-的操作方法或属性&#34;&gt;3.2 KeyedVectors 的操作方法(或属性)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方法&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.index_to_key&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取词汇表中的所有单词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.key_to_index&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取单词到索引的映射。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.vector_size&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取 GloVe 模型中任意词向量的维度。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.get_vector(word)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取给定单词的词向量。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.similar_by_word(word, topn=10)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取某词语最相似的 10 个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.similar_by_vector(vector, topn=10)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取词向量最相似的 10 个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h2 id=&#34;33-词表&#34;&gt;3.3 词表&lt;/h2&gt;
&lt;p&gt;查看词表&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;被告&amp;#39;,
 &amp;#39;原告&amp;#39;,
 &amp;#39;本院&amp;#39;,
 &amp;#39;公司&amp;#39;,
 &amp;#39;规定&amp;#39;,
 &amp;#39;执行&amp;#39;,
 ...
]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;p&gt;查看词汇映射表&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;被告&amp;#39;: 0,
 &amp;#39;原告&amp;#39;: 1,
 &amp;#39;本院&amp;#39;: 2,
 &amp;#39;公司&amp;#39;: 3,
 &amp;#39;规定&amp;#39;: 4,
 &amp;#39;执行&amp;#39;: 5,
 ...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;34-查看词向量&#34;&gt;3.4 查看词向量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查询某词的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;经济&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([ 2.909250e-01,  9.074450e-01,  5.231860e-01,  5.381490e-01,
       -2.813620e-01,  2.661690e-01,  1.045510e-01, -4.516240e-01,
       -2.186710e-01,  1.867590e-01, -4.870700e-01, -1.803480e-01,
       -6.361140e-01, -8.739630e-01,  3.418450e-01,  7.470900e-02,
        ......
        ......
        2.636230e-01, -2.538920e-01, -2.442900e-02,  5.847510e-01,
        5.135750e-01, -4.009650e-01, -3.629850e-01,  2.332400e-01,
       -3.069630e-01, -4.182810e-01,  3.937240e-01, -8.510000e-01,
        7.894350e-01,  3.969710e-01,  7.895660e-01,  4.881190e-01],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查询多个词的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_mean_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;经济&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;犯罪&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Ruj&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([ 0.02923387,  0.04620265,  0.03790346,  0.01160904, -0.02162073,
        0.01537724,  0.02025648, -0.03336571, -0.00447518, -0.00529976,
       -0.02856204,  0.01545951,  0.00780857, -0.05398807,  0.02195465,
        0.03140446, -0.02007412,  0.08278576, -0.027172  , -0.00272319,
       ......
        0.0291778 ,  0.03382879, -0.00913138,  0.04487584,  0.06375133,
        0.032144  , -0.02788475,  0.05068161,  0.0122064 ,  0.01759091,
       -0.05560436,  0.00272704, -0.01176615, -0.08875326,  0.00767812,
       -0.00486504,  0.10119167, -0.01212235,  0.06018812,  0.02998512],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;35-近义词&#34;&gt;3.5 近义词&lt;/h3&gt;
&lt;p&gt;根据词语查找最相似的 10 个词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;动机&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;主观&amp;#39;, 0.6688777804374695),
 (&amp;#39;意图&amp;#39;, 0.6248725652694702),
 (&amp;#39;恶性&amp;#39;, 0.6005507111549377),
 (&amp;#39;蓄意&amp;#39;, 0.5913136005401611),
 (&amp;#39;卑劣&amp;#39;, 0.5908187627792358),
 (&amp;#39;作案动机&amp;#39;, 0.5703221559524536),
 (&amp;#39;心态&amp;#39;, 0.5640602707862854),
 (&amp;#39;故意&amp;#39;, 0.5533956289291382),
 (&amp;#39;显而易见&amp;#39;, 0.5524264574050903),
 (&amp;#39;恶意&amp;#39;, 0.5509642958641052)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;根据某词的词向量查询最相似的 10 个词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;动机&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;动机&amp;#39;, 0.9999999403953552),
 (&amp;#39;主观&amp;#39;, 0.6688777804374695),
 (&amp;#39;意图&amp;#39;, 0.6248724460601807),
 (&amp;#39;恶性&amp;#39;, 0.600550651550293),
 (&amp;#39;蓄意&amp;#39;, 0.5913134813308716),
 (&amp;#39;卑劣&amp;#39;, 0.5908187627792358),
 (&amp;#39;作案动机&amp;#39;, 0.5703221559524536),
 (&amp;#39;心态&amp;#39;, 0.5640602707862854),
 (&amp;#39;故意&amp;#39;, 0.5533955693244934),
 (&amp;#39;显而易见&amp;#39;, 0.5524263381958008)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;多个词求得均值向量&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;purpose_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_mean_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;动机&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;意图&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;目的&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;g_wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;purpose_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;意图&amp;#39;, 0.9032057523727417),
 (&amp;#39;目的&amp;#39;, 0.8639562726020813),
 (&amp;#39;动机&amp;#39;, 0.8277378678321838),
 (&amp;#39;主观&amp;#39;, 0.7455390095710754),
 (&amp;#39;恶意&amp;#39;, 0.7291366457939148),
 (&amp;#39;故意&amp;#39;, 0.7236210107803345),
 (&amp;#39;客观&amp;#39;, 0.7146263122558594),
 (&amp;#39;企图&amp;#39;, 0.7049675583839417),
 (&amp;#39;行为&amp;#39;, 0.6962229609489441),
 (&amp;#39;掩盖&amp;#39;, 0.6917882561683655),
 (&amp;#39;所谓&amp;#39;, 0.6809536218643188),
 (&amp;#39;并非&amp;#39;, 0.667915403842926),
 (&amp;#39;手段&amp;#39;, 0.6663289666175842),
 (&amp;#39;利益&amp;#39;, 0.6568542718887329),
 (&amp;#39;这种&amp;#39;, 0.6558799743652344),
 (&amp;#39;欺骗&amp;#39;, 0.6545097231864929),
 (&amp;#39;违背&amp;#39;, 0.6538694500923157),
 (&amp;#39;真相&amp;#39;, 0.6527130007743835),
 (&amp;#39;显然&amp;#39;, 0.6525647640228271),
 (&amp;#39;实质&amp;#39;, 0.6521809101104736)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四获取模型&#34;&gt;四、获取模型&lt;/h2&gt;
&lt;p&gt;内容创作不易， 本文为付费内容，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 裁判文书-GloVe.200.15.bin   https://pan.baidu.com/s/1a0Fisvnkl8UaQZrHP7olCQ?pwd=8w49

- 更多词向量模型               https://cntext.readthedocs.io/zh-cn/latest/embeddings.html

- 100元                      cntext-2.1.6-py3-none-any.whl  加微信 372335839， 备注「姓名-学校-专业」
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
      <content:encoded><![CDATA[<p>前阵子分享了 <a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用 Stanford Glove 代码训练中文语料的 Glove 模型</a> ，后来我偷偷的修改了这篇技术文， 将 C 代码封装到 cntext2.x 内， 原来训练代码行几十行， 现在只需要两行就可以训练 GloVe 模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="n">g_wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">GloVe</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;语料文件.txt&#39;</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="一检查数据">一、检查数据</h2>
<p>裁判文书数据集，每个月份存储到一个 csv， 每个年份有一个对应的文件夹。下图是 2021 年的文件夹截图
<img loading="lazy" src="img/2021.png" alt=""  />
</p>
<br>
<p>csv 字段格式是一致的，我们只需要找一个文件，尝试着读取前 5 行，查看数据中有哪些字段。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;2013/2013-01.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;文书内容&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二训练词向量">二、训练词向量</h2>
<h3 id="21-构造语料">2.1 构造语料</h3>
<p>我们只从 csv 中选取 &ldquo;<strong>文书内容</strong>&rdquo; ，并将其存储到语料 txt 文件中。但全部裁判文书数据量高达 300G， 我希望文本语料控制在 10G 左右。</p>
<p>2010/2011/2013 这三个年度的数据只有几百 M， 数据全部保留。 剩下的年份，设置不同的抽样比例，尽可能将每年生成的语料 txt 文件控制在 1G 左右。</p>
<table>
<thead>
<tr>
<th>年份</th>
<th>解压后文件大小</th>
<th>抽样比例</th>
<th>语料 txt 大小</th>
</tr>
</thead>
<tbody>
<tr>
<td>2010</td>
<td>761M</td>
<td>100%</td>
<td>684M</td>
</tr>
<tr>
<td>2011</td>
<td>452M</td>
<td>100%</td>
<td>396M</td>
</tr>
<tr>
<td>2012</td>
<td>757M</td>
<td>100%</td>
<td>665M</td>
</tr>
<tr>
<td>2013</td>
<td>5.13G</td>
<td>20%</td>
<td>984M</td>
</tr>
<tr>
<td>2014</td>
<td>23.7G</td>
<td>4%</td>
<td>905M</td>
</tr>
<tr>
<td>2015</td>
<td>33.6G</td>
<td>3%</td>
<td>968M</td>
</tr>
<tr>
<td>2016</td>
<td>39.9G</td>
<td>2.4%</td>
<td>914M</td>
</tr>
<tr>
<td>2017</td>
<td>44.6G</td>
<td>2.2%</td>
<td>882M</td>
</tr>
<tr>
<td>2018</td>
<td>24.8G</td>
<td>4%</td>
<td>875M</td>
</tr>
<tr>
<td>2019</td>
<td>48.3G</td>
<td>2%</td>
<td>833M</td>
</tr>
<tr>
<td>2020</td>
<td>91.2G</td>
<td>1%</td>
<td>779M</td>
</tr>
<tr>
<td>2021</td>
<td>32.3G</td>
<td>3%</td>
<td>816M</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># 年份、抽样比例</span>
<span class="n">year_fracs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;2010&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2011&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2012&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;2013&#39;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2014&#39;</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2015&#39;</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;2016&#39;</span><span class="p">,</span> <span class="mf">0.024</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2017&#39;</span><span class="p">,</span> <span class="mf">0.022</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2018&#39;</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;2019&#39;</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2020&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;2021&#39;</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">)</span>
    <span class="p">]</span>



<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;裁判文书.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">corpus_file</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">year</span><span class="p">,</span> <span class="n">frac</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">year_fracs</span><span class="p">):</span>
        <span class="n">csvfs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">csvf</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">csvf</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">year</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.csv&#39;</span> <span class="ow">in</span> <span class="n">csvf</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">csvf</span> <span class="ow">in</span> <span class="n">csvfs</span><span class="p">:</span>
            <span class="c1"># 为节省内存开销，</span>
            <span class="c1"># 只读 csv 中的 “文书内容” 一个字段，</span>
            <span class="c1"># 且设置 chunksize 分批次读取</span>
            <span class="n">chunk_dfs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csvf</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;文书内容&#39;</span><span class="p">],</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">chunk_df</span> <span class="ow">in</span> <span class="n">chunk_dfs</span><span class="p">:</span>
                <span class="n">chunk_df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;文书内容&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chunk_df</span><span class="p">[</span><span class="s1">&#39;文书内容&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
                <span class="n">corpus_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><br>
<h3 id="22-训练-glove">2.2 训练 GloVe</h3>
<p>使用 cntext2.x 对代码进行了优化， 几个 G 的语料在 cntext 内预处理时候不会一次性读取全部内容，所以一般情况不会出现内存溢出问题。</p>
<p>基于语料 <strong><em>裁判文书.txt</em></strong> 训练 GloVe 词嵌入语言模型 ，参数 window_size=15, vector_size=200, 结果会自动保存到 output 文件夹内。</p>
<p><img loading="lazy" src="img/01-corpus.png" alt=""  />
</p>
<p>使用 <strong><em>cntext2.1.6</em></strong>， 代码如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">g_wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">GloVe</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;裁判文书.txt&#39;</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/裁判文书_cache.txt Not Found, Preprocessing Corpus
Processing Corpus: 100%|██████████████████| 2502938/2502938 [26:37&lt;00:00, 1566.54it/s]
Reading Preprocessed Corpus from output/裁判文书_cache.txt
Start Training GloVe
GloVe Training Cost 1223s.
Output Saved To: output/裁判文书-Word2Vec.200.15.bin
</code></pre></div><p>训练总耗时 1223s， 约 20 分钟。模型保存在 <strong><em>output/裁判文书-Word2Vec.200.15.bin</em></strong>， 该模型文件大小约为 1.58G。</p>
<br>
<h3 id="23-评估模型">2.3 评估模型</h3>
<p>使用近义法和类比法， 判断模型的表现。详情可查看<a href="https://cntext.readthedocs.io/zh-cn/latest/model.html">文档</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">evaluate_similarity</span><span class="p">(</span><span class="n">g_wv</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt
Processing Similarity Test: 100%|███████████| 537/537 [00:00&lt;00:00, 131978.28it/s]

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   432    |    105     |            0.37            |
+----------+------------+----------------------------+
</code></pre></div><p><strong>近义测试</strong>: Spearman&rsquo;s Rank Coeficient 系数取值[-1, 1], 取值越大， 说明模型表现越好。<br></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">evaluate_analogy</span><span class="p">(</span><span class="n">g_wv</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|████████████████| 1198/1198 [00:48&lt;00:00, 24.75it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   507    |    170     |    7.69    |   4.38   |
|   CityInProvince   |   175    |     0      |   98.86    |   1.39   |
| FamilyRelationship |   272    |     0      |   73.53    |   1.56   |
|   SocialScience    |    8     |     62     |   25.00    |   7.00   |
+--------------------+----------+------------+------------+----------+
</code></pre></div><p><strong>类比测试</strong>:</p>
<ul>
<li>CapitalOfCountries 裁判文书语料在此项表现较差， 应该是数据库中涉外的案件较少。</li>
<li>CityInProvince 裁判文书语料在此项表现如此优异，是因为几乎全为国内案件， 而案件描述一般会交待案发的省市等信息。</li>
<li>FamilyRelationship 裁判文书语料中表现较好， 可能很多的案件会描述案件相关社会关系。</li>
<li>SocialScience 裁判文书语料在此项表现一般， 应该是语料中常见的社会科学词语提及较少。</li>
</ul>
<p>整体而言，语料训练的效果很不错，抓住了数据场景的独特性语义。</p>
<p><br><br></p>
<h2 id="三使用-glove">三、使用 GloVe</h2>
<h3 id="31-导入模型">3.1 导入模型</h3>
<p>使用 cntext2.1.6 读取很简单， 代码如下</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">g_wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/裁判文书-Word2Vec.200.15.bin&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">g_wv</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;模型词汇量: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">g_wv</span><span class="o">.</span><span class="n">wv</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;class &#39;gensim.models.keyedvectors.KeyedVectors&#39;&gt;
模型词汇量: 2099102
</code></pre></div><br>
<h3 id="32-keyedvectors-的操作方法或属性">3.2 KeyedVectors 的操作方法(或属性)</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><em>KeyedVectors.index_to_key</em></strong></td>
<td>获取词汇表中的所有单词。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.key_to_index</em></strong></td>
<td>获取单词到索引的映射。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.vector_size</em></strong></td>
<td>获取 GloVe 模型中任意词向量的维度。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.get_vector(word)</em></strong></td>
<td>获取给定单词的词向量。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.similar_by_word(word, topn=10)</em></strong></td>
<td>获取某词语最相似的 10 个近义词。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.similar_by_vector(vector, topn=10)</em></strong></td>
<td>获取词向量最相似的 10 个近义词。</td>
</tr>
</tbody>
</table>
<br>
<h2 id="33-词表">3.3 词表</h2>
<p>查看词表</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">g_wv</span><span class="o">.</span><span class="n">index_to_key</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;被告&#39;,
 &#39;原告&#39;,
 &#39;本院&#39;,
 &#39;公司&#39;,
 &#39;规定&#39;,
 &#39;执行&#39;,
 ...
]
</code></pre></div><br>
<br>
<p>查看词汇映射表</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">g_wv</span><span class="o">.</span><span class="n">key_to_index</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;被告&#39;: 0,
 &#39;原告&#39;: 1,
 &#39;本院&#39;: 2,
 &#39;公司&#39;: 3,
 &#39;规定&#39;: 4,
 &#39;执行&#39;: 5,
 ...
}
</code></pre></div><br>
<h3 id="34-查看词向量">3.4 查看词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查询某词的词向量</span>
<span class="n">g_wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;经济&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 2.909250e-01,  9.074450e-01,  5.231860e-01,  5.381490e-01,
       -2.813620e-01,  2.661690e-01,  1.045510e-01, -4.516240e-01,
       -2.186710e-01,  1.867590e-01, -4.870700e-01, -1.803480e-01,
       -6.361140e-01, -8.739630e-01,  3.418450e-01,  7.470900e-02,
        ......
        ......
        2.636230e-01, -2.538920e-01, -2.442900e-02,  5.847510e-01,
        5.135750e-01, -4.009650e-01, -3.629850e-01,  2.332400e-01,
       -3.069630e-01, -4.182810e-01,  3.937240e-01, -8.510000e-01,
        7.894350e-01,  3.969710e-01,  7.895660e-01,  4.881190e-01],
      dtype=float32)
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查询多个词的词向量</span>
<span class="n">g_wv</span><span class="o">.</span><span class="n">get_mean_vector</span><span class="p">([</span><span class="s1">&#39;经济&#39;</span><span class="p">,</span> <span class="s1">&#39;犯罪&#39;</span><span class="p">])</span>
</code></pre></div><p>Ruj</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 0.02923387,  0.04620265,  0.03790346,  0.01160904, -0.02162073,
        0.01537724,  0.02025648, -0.03336571, -0.00447518, -0.00529976,
       -0.02856204,  0.01545951,  0.00780857, -0.05398807,  0.02195465,
        0.03140446, -0.02007412,  0.08278576, -0.027172  , -0.00272319,
       ......
        0.0291778 ,  0.03382879, -0.00913138,  0.04487584,  0.06375133,
        0.032144  , -0.02788475,  0.05068161,  0.0122064 ,  0.01759091,
       -0.05560436,  0.00272704, -0.01176615, -0.08875326,  0.00767812,
       -0.00486504,  0.10119167, -0.01212235,  0.06018812,  0.02998512],
      dtype=float32)
</code></pre></div><br>
<h3 id="35-近义词">3.5 近义词</h3>
<p>根据词语查找最相似的 10 个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">g_wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="s1">&#39;动机&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;主观&#39;, 0.6688777804374695),
 (&#39;意图&#39;, 0.6248725652694702),
 (&#39;恶性&#39;, 0.6005507111549377),
 (&#39;蓄意&#39;, 0.5913136005401611),
 (&#39;卑劣&#39;, 0.5908187627792358),
 (&#39;作案动机&#39;, 0.5703221559524536),
 (&#39;心态&#39;, 0.5640602707862854),
 (&#39;故意&#39;, 0.5533956289291382),
 (&#39;显而易见&#39;, 0.5524264574050903),
 (&#39;恶意&#39;, 0.5509642958641052)]
</code></pre></div><br>
<p>根据某词的词向量查询最相似的 10 个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">g_wv</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">g_wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;动机&#39;</span><span class="p">),</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;动机&#39;, 0.9999999403953552),
 (&#39;主观&#39;, 0.6688777804374695),
 (&#39;意图&#39;, 0.6248724460601807),
 (&#39;恶性&#39;, 0.600550651550293),
 (&#39;蓄意&#39;, 0.5913134813308716),
 (&#39;卑劣&#39;, 0.5908187627792358),
 (&#39;作案动机&#39;, 0.5703221559524536),
 (&#39;心态&#39;, 0.5640602707862854),
 (&#39;故意&#39;, 0.5533955693244934),
 (&#39;显而易见&#39;, 0.5524263381958008)]
</code></pre></div><br>
<p>多个词求得均值向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">purpose_vector</span> <span class="o">=</span> <span class="n">g_wv</span><span class="o">.</span><span class="n">get_mean_vector</span><span class="p">([</span><span class="s1">&#39;动机&#39;</span><span class="p">,</span>  <span class="s1">&#39;意图&#39;</span><span class="p">,</span> <span class="s1">&#39;目的&#39;</span><span class="p">])</span>
<span class="n">g_wv</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">purpose_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;意图&#39;, 0.9032057523727417),
 (&#39;目的&#39;, 0.8639562726020813),
 (&#39;动机&#39;, 0.8277378678321838),
 (&#39;主观&#39;, 0.7455390095710754),
 (&#39;恶意&#39;, 0.7291366457939148),
 (&#39;故意&#39;, 0.7236210107803345),
 (&#39;客观&#39;, 0.7146263122558594),
 (&#39;企图&#39;, 0.7049675583839417),
 (&#39;行为&#39;, 0.6962229609489441),
 (&#39;掩盖&#39;, 0.6917882561683655),
 (&#39;所谓&#39;, 0.6809536218643188),
 (&#39;并非&#39;, 0.667915403842926),
 (&#39;手段&#39;, 0.6663289666175842),
 (&#39;利益&#39;, 0.6568542718887329),
 (&#39;这种&#39;, 0.6558799743652344),
 (&#39;欺骗&#39;, 0.6545097231864929),
 (&#39;违背&#39;, 0.6538694500923157),
 (&#39;真相&#39;, 0.6527130007743835),
 (&#39;显然&#39;, 0.6525647640228271),
 (&#39;实质&#39;, 0.6521809101104736)]
</code></pre></div><p><br><br></p>
<h2 id="四获取模型">四、获取模型</h2>
<p>内容创作不易， 本文为付费内容，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 裁判文书-GloVe.200.15.bin   https://pan.baidu.com/s/1a0Fisvnkl8UaQZrHP7olCQ?pwd=8w49

- 更多词向量模型               https://cntext.readthedocs.io/zh-cn/latest/embeddings.html

- 100元                      cntext-2.1.6-py3-none-any.whl  加微信 372335839， 备注「姓名-学校-专业」
</code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>使用 5000w 专利申请数据集按年份(按省份)训练词向量</title>
      <link>https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/</link>
      <pubDate>Fri, 04 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-20-word2vec-by-year-by-province/</guid>
      <description>&lt;p&gt;想用 &lt;a href=&#34;https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/&#34;&gt;中国专利申请数据集&lt;/a&gt;，按年份(或按省份)训练词向量的同学，可以好好看本文，能节省你几十个小时时间。
&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一检查数据&#34;&gt;一、检查数据&lt;/h2&gt;
&lt;p&gt;这个数据集很大， 如图所示，文件动辄几G&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-data-screen.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;之前分享过 &lt;a href=&#34;&#34;&gt;&lt;/a&gt; , 面对巨大csv文件，我们要了解内部有哪些字段、字段的含义， 只读取需要的字段，减轻电脑内存压力， 让你能轻松应对几倍于内存的巨大csv文件。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 以山东省.csv 为例， 只读第一行(前1行)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;山东省.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nrows&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-shandong_df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;字段展示的不全，完整的字段应该有&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Index([&amp;#39;专利名称&amp;#39;, &amp;#39;专利类型&amp;#39;, &amp;#39;申请人&amp;#39;, &amp;#39;申请人类型&amp;#39;, &amp;#39;申请人地址&amp;#39;, &amp;#39;申请人国家&amp;#39;, &amp;#39;申请人省份&amp;#39;, &amp;#39;申请人城市&amp;#39;,
       &amp;#39;申请人区县&amp;#39;, &amp;#39;申请号&amp;#39;, &amp;#39;申请日&amp;#39;, &amp;#39;申请年份&amp;#39;, &amp;#39;公开公告号&amp;#39;, &amp;#39;公开公告日&amp;#39;, &amp;#39;公开公告年份&amp;#39;, &amp;#39;授权公告号&amp;#39;,
       &amp;#39;授权公告日&amp;#39;, &amp;#39;授权公告年份&amp;#39;, &amp;#39;IPC分类号&amp;#39;, &amp;#39;IPC主分类号&amp;#39;, &amp;#39;发明人&amp;#39;, &amp;#39;摘要文本&amp;#39;, &amp;#39;主权项内容&amp;#39;, &amp;#39;当前权利人&amp;#39;,
       &amp;#39;当前专利权人地址&amp;#39;, &amp;#39;专利权人类型&amp;#39;, &amp;#39;统一社会信用代码&amp;#39;, &amp;#39;引证次数&amp;#39;, &amp;#39;被引证次数&amp;#39;, &amp;#39;自引次数&amp;#39;, &amp;#39;他引次数&amp;#39;,
       &amp;#39;被自引次数&amp;#39;, &amp;#39;被他引次数&amp;#39;, &amp;#39;家族引证次数&amp;#39;, &amp;#39;家族被引证次数&amp;#39;],
      dtype=&amp;#39;object&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;训练词向量主要用文本数据， 在本案例中， 需要的字段 [&lt;strong&gt;专利摘要&lt;/strong&gt;] 。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二构造语料&#34;&gt;二、构造语料&lt;/h2&gt;
&lt;p&gt;在 [5000万专利申请全量数据1985-2025年] 文件夹中，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;新建 [province_corpus] 和 [year_corpus] 两个文件夹&lt;/li&gt;
&lt;li&gt;新建 [code.ipynb]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;构造语料对电脑的性能要求不高， 不论你的电脑是什么配置，基本都能运行， 而且耗时在能接受的范围。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;21-文件树结构&#34;&gt;2.1 文件树结构&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;5000万专利申请全量数据1985-2025年
  |---中国专利数据库.csv.gz
  |---code.ipynb
  |---province_corpus
     |---安徽省.txt
     |---浙江省.txt
     |---...
  |---year_corpus
     |---2025.txt
     |---2024.txt
     |---...
  |---provin_w2vs
        |---安徽省-Word2Vec.200.15.bin
        |---山东省-Word2Vec.200.15.bin
        |---...
  |---year_w2vs
        |---2025-Word2Vec.200.15.bin
        |---2022-Word2Vec.100.6.bin.syn1neg.npy
        |---2022-Word2Vec.100.6.bin.wv.vectors.npy
        |---...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-构造语料代码&#34;&gt;2.2 构造语料代码&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pathlib&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tqdm&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tqdm&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 读取csv文件， 只读取需要的字段&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 按 100000 行分块读取， 避免内存溢出&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;中国专利数据库.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                 &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                 &lt;span class=&#34;n&#34;&gt;usecols&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请人省份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;申请日&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;专利名称&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;摘要文本&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
                 &lt;span class=&#34;n&#34;&gt;chunksize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tqdm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请日&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_datetime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请日&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;# 新建 province_corpus 和 year_corpus文件夹&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;province_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;province_corpus&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;province_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mkdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parents&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exist_ok&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;year_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year_corpus&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;year_dir&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mkdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;parents&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exist_ok&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;c1&#34;&gt;# 按省份和年份构造语料&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Grouper&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请日&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;freq&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;YE&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;year_file&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;date&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.txt&amp;#34;&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;a+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;yf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;y_text_series&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利名称&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;摘要文本&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;y_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_text_series&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;yf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;groupby&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;申请人省份&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;prov_file&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;province_dir&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prov&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;.txt&amp;#34;&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prov_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;a+&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;prov_text_series&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利名称&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;摘要文本&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;prov_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prov_text_series&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;pf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prov_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2025
2024
...


上海市
云南省
...
安徽省

CPU times: total: 27min 55s
Wall time: 39min 10s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;构造语料用 40 分钟时间，得到文件夹province_corpus和year_corpus。
&lt;img loading=&#34;lazy&#34; src=&#34;img/03-province-corpus.png&#34; alt=&#34;&#34;  /&gt;

&lt;img loading=&#34;lazy&#34; src=&#34;img/03-year-corpus.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三训练word2vec&#34;&gt;三、训练word2vec&lt;/h2&gt;
&lt;p&gt;需要注意， 训练word2vec需要耗费很大的计算能力， 训练时间需要一两三。 本文使用的 &lt;em&gt;&lt;strong&gt;cntext2.1.5&lt;/strong&gt;&lt;/em&gt; 版本，需要付费获取 &lt;em&gt;&lt;strong&gt;cntext-2.1.5-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;
&lt;h3 id=&#34;31-安装cntext&#34;&gt;3.1 安装cntext&lt;/h3&gt;
&lt;p&gt;将 &lt;em&gt;&lt;strong&gt;cntext-2.1.5-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 放置于电脑桌面， 打开 &lt;strong&gt;命令行cmd&lt;/strong&gt; (Mac打开&lt;strong&gt;terminal&lt;/strong&gt;)， 输入&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
pip install cntext-2.1.5-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;有部分使用win的同学，如果按照操作没有安装成功，再试试&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd Desktop
pip install cntext-2.1.5-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-开始训练&#34;&gt;3.2 开始训练&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;

&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;glob&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pathlib&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 分年份训练&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;glob&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;glob&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year_corpus/*.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# 训练word2vec，自动保存到output文件夹内&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Word2Vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                &lt;span class=&#34;n&#34;&gt;only_binary&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 将output文件夹重命名为year_w2vs&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rename&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;year_w2vs&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;


&lt;span class=&#34;c1&#34;&gt;# 分省份训练&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov_f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;glob&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;glob&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;province_corpus/*.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# 训练word2vec，自动保存到output文件夹内&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Word2Vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prov_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                &lt;span class=&#34;n&#34;&gt;only_binary&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;span class=&#34;c1&#34;&gt;# 将output文件夹重命名为province_w2vs&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rename&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;province_w2vs&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Windows System, Unable Parallel Processing
Cache output\1985_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|████████████████████████████████████████████████████████| 10009/10009 [00:13&amp;lt;00:00, 734.66it/s]
Reading Preprocessed Corpus from output\1985_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 17 s. 
Output Saved To: output\1985-Word2Vec.200.15.bin

......
......

Windows System, Unable Parallel Processing
Cache output\2025_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|████████████████████████████████████████████████████████| 10009/10009 [00:13&amp;lt;00:00, 734.66it/s]
Reading Preprocessed Corpus from output\2025_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 17 s. 
Output Saved To: output\2025-Word2Vec.200.15.bin


Windows System, Unable Parallel Processing
Cache output\上海市_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|██████████████████████████████████████████████████| 2456943/2456943 [03:42&amp;lt;00:00, 11048.35it/s]
Reading Preprocessed Corpus from output\上海市_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 1400 s. 
Output Saved To: output\上海市-Word2Vec.200.15.bin
......
......

Windows System, Unable Parallel Processing
Cache output\黑龙江省_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|█████████████████████████████████████████████████████| 544329/544329 [01:07&amp;lt;00:00, 8114.12it/s]
Reading Preprocessed Corpus from output\黑龙江省_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 320 s. 
Output Saved To: output\黑龙江省-Word2Vec.200.15.bin


CPU times: total: 21354 s
Wall time: 21758 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;训练省份词向量大概用了 6 小时，模型文件保存在 &lt;em&gt;&lt;strong&gt;provin_w2vs&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;year_w2vs&lt;/strong&gt;&lt;/em&gt; 文件夹内。
&lt;img loading=&#34;lazy&#34; src=&#34;img/05-province-w2vs.png&#34; alt=&#34;&#34;  /&gt;

&lt;img loading=&#34;lazy&#34; src=&#34;img/06-years-w2vs.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三使用word2vec&#34;&gt;三、使用word2vec&lt;/h2&gt;
&lt;h3 id=&#34;31-导入模型&#34;&gt;3.1 导入模型&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;output/provin_w2vs&lt;/code&gt; 和 &lt;code&gt;output/year_w2vs&lt;/code&gt; 内有多个模型， 单个的模型文件大约几十M ~ 几百M， &lt;strong&gt;但不建议一次性导入进来&lt;/strong&gt;。大邓的电脑内存96G，为了省事，就一次性全导入了。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;glob&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tqdm&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tqdm&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 导入各省份词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;provin_w2vs_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;provin_w2v_fs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;glob&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;glob&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;province_w2vs/*.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tqdm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;provin_w2vs_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;


&lt;span class=&#34;c1&#34;&gt;# 导入各年份词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_w2vs_&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;year_w2v_fs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;glob&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;glob&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;year_w2vs/*.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tqdm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;year_w2vs_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;  3%|██▍                                                                                | 1/34 [00:03&amp;lt;01:57,  3.57s/it]
Loading province_w2vs\上海市-Word2Vec.200.15.bin...
  6%|████▉                                                                              | 2/34 [00:04&amp;lt;01:11,  2.23s/it]
Loading province_w2vs\云南省-Word2Vec.200.15.bin...
......
97%|███████████████████████████████████████████████████████████████████████████████▌  | 33/34 [01:07&amp;lt;00:01,  1.10s/it]
Loading province_w2vs\香港特别行政区-Word2Vec.200.15.bin...
100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [01:09&amp;lt;00:00,  2.04s/it]
Loading province_w2vs\黑龙江省-Word2Vec.200.15.bin...


  2%|██                                                                                 | 1/41 [00:00&amp;lt;00:05,  7.10it/s]
Loading year_w2vs\1985-Word2Vec.200.15.bin...
Loading year_w2vs\1986-Word2Vec.200.15.bin...
 10%|████████                                                                           | 4/41 [00:00&amp;lt;00:05,  6.80it/s]
 ......
 100%|██████████████████████████████████████████████████████████████████████████████████| 41/41 [01:11&amp;lt;00:00,  1.75s/it]
Loading year_w2vs\2025-Word2Vec.200.15.bin...

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-查看词汇量&#34;&gt;3.2 查看词汇量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pathlib&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;省份Word2vec词汇量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2vs_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;province&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stem&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;province&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 词汇量: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;省份Word2vec词汇量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;上海市&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;640941&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;云南省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;205193&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;内蒙古自治区&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;138507&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;北京市&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;783162&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;台湾省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;242630&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;吉林省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;185587&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;四川省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;494241&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;天津市&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;373286&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;宁夏回族自治区&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;91592&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;安徽省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;540111&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;山东省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;722886&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;山西省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;188013&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;广东省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1010230&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;广西壮族自治区&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;190128&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;新疆维吾尔自治区&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;110063&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;江苏省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;983871&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;江西省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;256695&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;河北省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;326042&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;河南省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;415905&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;浙江省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;795041&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;海南省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;74657&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;湖北省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;412827&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;湖南省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;400262&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;澳门特别行政区&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;7806&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;甘肃省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;148753&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;福建省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;480456&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;西藏自治区&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;23115&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;贵州省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;186345&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;辽宁省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;347563&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;重庆市&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;358991&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;陕西省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;381781&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;青海省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;53325&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;香港特别行政区&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;71947&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;黑龙江省&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;词汇量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;253129&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;年份word2vec词汇量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2vs_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stem&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;年份word2vec词汇量
1985: 15494
1986: 17945
1987: 23625
1988: 27740
1989: 27394
1990: 32920
1991: 37584
1992: 45393
1993: 48326
1994: 46725
1995: 46138
1996: 50117
1997: 53625
1998: 57187
1999: 65154
2000: 78368
2001: 95927
2002: 123513
2003: 145087
2004: 158694
2005: 185840
2006: 215856
2007: 240167
2008: 279364
2009: 334179
2010: 382888
2011: 449648
2012: 508506
2013: 621644
2014: 625248
2015: 685487
2016: 732443
2017: 760332
2018: 776968
2019: 789104
2020: 817553
2021: 799388
2022: 734045
2023: 596784
2024: 516263
2025: 21230
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;33-语义检查-省份&#34;&gt;3.3 语义检查-省份&lt;/h3&gt;
&lt;p&gt;先检查省份， 查看与[&amp;lsquo;创新&amp;rsquo;, &amp;lsquo;新颖&amp;rsquo;]最相似的5个词，通过语义捕捉准确与否，粗略判断Word2vec训练效果的好坏。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2vs_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;province&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;provin_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stem&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;wordweigths&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provin_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;新颖&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wordweigths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;province&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;province&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;: NA&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;上海市: 独特 巧妙 创新性 理念 新颖结构合理
云南省: 独特 巧妙 精巧 科学合理 新颖合理
内蒙古自治区: 理念 独特 巧妙 合理使用方便 全新
北京市: 巧妙 独特 全新 借鉴 新颖使用方便
台湾省: 全新 独特 精巧 简洁 巧妙
吉林省: 理念 思路 现代 全新 巧妙
四川省: 巧妙 独特 全新 理念 合理使用方便
天津市: 独特 巧妙 合理使用方便 巧妙使用方便 全新
宁夏回族自治区: 更具 多样 丰富 性价比 市场前景
安徽省: 巧妙 独特 巧妙使用方便 合理 合理结构紧凑
山东省: 巧妙 精巧 新颖结构合理 巧妙结构合理 全新
山西省: 独特 全新 科学 现代 已有
广东省: 巧妙 独特 创新性 合理 精巧
广西壮族自治区: 独特 巧妙 合理使用方便 精巧 合理实用性
新疆维吾尔自治区: 合理使用方便 巧妙 简单合理 科学合理 合理
江苏省: 巧妙 独特 合理 全新 科学
江西省: 独特 科学合理 巧妙 简洁 精巧
河北省: 巧妙 新颖使用方便 精巧 独特 理念
河南省: 巧妙 科学合理 独特 新颖使用方便 巧妙使用方便
浙江省: 独特 巧妙 科学 精巧 合理
海南省: 思路 科学合理 人性化 科学 独特
湖北省: 巧妙 巧妙合理 科学合理 独特 新颖结构合理
湖南省: 巧妙 精巧 独特 新颖独特 巧妙结构合理
澳门特别行政区: 撞击 边坡 溜槽 耐高温 材料制成
甘肃省: 独特 全新 理念 现代 普及
福建省: 巧妙 新颖使用方便 独特 巧妙结构合理 全新
西藏自治区: 既能 十分 疲劳 范围广 更加人性化
贵州省: 巧妙 独特 科学合理 合理使用方便 精巧
辽宁省: 巧妙 巧妙结构合理 新颖结构合理 新颖独特 独创
重庆市: 合理使用方便 科学合理 精巧 全新 新颖使用方便
陕西省: 巧妙 独特 新颖结构合理 合理 合理使用方便
青海省: 突破 织机 现行 经济环保 杆织机
香港特别行政区: 全新 更具 美感 市场 丰富
黑龙江省: 独特 精巧 科学合理 巧妙 构思
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从上面的运行结果看， 除青海省，剩下的绝大多数的省份的Word2vec都很准确的捕捉到了专利摘要的语义信息。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;34-语义检查-年份&#34;&gt;3.4 语义检查-年份&lt;/h3&gt;
&lt;p&gt;查看与[&amp;lsquo;创新&amp;rsquo;, &amp;lsquo;新颖&amp;rsquo;]最相似的5个词，通过语义捕捉准确与否，粗略判断Word2vec训练效果的好坏。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v_fs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2vs_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year_w2v_f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;stem&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;wordweigths&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;新颖&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wordweigths&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;: NA&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;1985: 公知 专门 特别适用 提出一种 机器人
1986: 植入 理发 135 婴儿车 街道
1987: 落后 儿童智力开发 低档 胶鞋 指甲
1988: 目前市场 低档 捕鱼 证件 普遍使用
1989: 价廉物美 课堂教学 普及型 大众 得心应手
1990: 单纯 精简 多方面 机等 应用领域
1991: 普及型 前途 大众 现代 现代化
1992: 构思 保留传统 不失为 全新 机之
1993: 完美 现代科技 崭新 式样 边墙
1994: 样式 别致 造型新颖 显得 华贵
1995: 多样 结实耐用 独特 别致 高档
1996: 花样 耐冲击浮标 极其 式样新颖 应用范围
1997: 实用美观 室内外装饰 标准化 多变 形象逼真
1998: 现代 改革 开发 高雅 创造
1999: 现代 市场 越来越 款式 大方
2000: 全新 娱乐性趣味性 多样化 多方面 各种各样
2001: 完美 现代 新颖别致 多样化 体现
2002: 全新 现代 实为 科学 大众
2003: 全新 突破传统 多样化 体现 科学
2004: 全新 科技 现代 市场 科学合理
2005: 创意 理念 全新 科学 现代
2006: 全新 构思 理念 新颖性 独特
2007: 全新 突破传统 巧妙 独特 现代
2008: 独特 巧妙 全新 新颖独特 设计理念
2009: 独特 巧妙 全新 科学 新颖独特
2010: 独特 新颖独特 精巧 巧妙 科学合理
2011: 独特 精巧 新颖独特 科学合理 巧妙
2012: 独特 巧妙 新颖独特 精巧 科学合理
2013: 独特 新颖独特 精巧 科学合理 巧妙
2014: 巧妙 独特 科学合理 巧妙合理 精巧
2015: 巧妙 独特 巧妙合理 新颖结构合理 科学合理
2016: 巧妙 结合现在 巧妙合理 独特 全新
2017: 巧妙 科学合理 独特 科学 合理
2018: 巧妙 独特 合理 科学 全新
2019: 巧妙 独特 合理 全新 精巧
2020: 巧妙 独特 合理 精巧 设计
2021: 巧妙 精巧 合理 新颖结构合理 全新
2022: 巧妙 全新 独特 创新性 精巧
2023: 巧妙 独特 全新 创新性 精巧
2024: 巧妙 创新性 独特 精巧 全新
2025: 相结合 双重 独特 多重 优势
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;也试了其他的词语，好像 1998 年之后的捕捉的语义是准确的。&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;四研究潜力-语义变迁研究方法介绍&#34;&gt;四、研究潜力: 语义变迁研究方法介绍&lt;/h2&gt;
&lt;p&gt;假设语义都很准的话， 是可以研究 &lt;strong&gt;语义变迁&lt;/strong&gt; 或者 &lt;strong&gt;语义差异&lt;/strong&gt; 的。 但需要注意， 不能直接使用两个年份或者两个省份的中word1和word2的距离来体现语义的变迁或者语义的差异。 如果想做省份间差异或者某省份随时间的变化， 需要用到 &lt;strong&gt;对齐算法&lt;/strong&gt;， 常用的算法是 &lt;strong&gt;正交Procrustes矩阵对齐&lt;/strong&gt;， 使得同省份不同年份或者通年份不同省份的word2vec都有相同的语义空间。&lt;/p&gt;
&lt;h3 id=&#34;41-正交procrustes算法&#34;&gt;4.1 正交Procrustes算法&lt;/h3&gt;
&lt;p&gt;正交Procrustes矩阵对齐是一种将两个预训练语言模型的词向量矩阵对齐的方法，使得它们在相同的语义空间中表示。具体来说，它通过计算一个正交矩阵，将两个词向量矩阵进行线性变换，使得它们的Frobenius范数之和最小，从而实现对齐。 在 cntext2.x中实现了Procrustes对齐函数 &lt;em&gt;&lt;strong&gt;ct.procrustes_align()&lt;/strong&gt;&lt;/em&gt;，具体可阅读 &lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/&#34;&gt;文本分析库cntext2.x使用手册&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;42-语义变迁流程图&#34;&gt;4.2 语义变迁流程图&lt;/h3&gt;
&lt;p&gt;语义变迁类研究的流程图可参考 &lt;a href=&#34;https://github.com/Living-with-machines/DiachronicEmb-BigHistData&#34;&gt;DiachronicEmb-BigHistData&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/w2v-time-shifting-flowchart.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;大邓在 &lt;a href=&#34;https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/&#34;&gt;可视化 | 人民日报语料反映七十年文化演变&lt;/a&gt; 实现了历时语义对齐， 可以看出70 年整个中国社会的认知变迁。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;43-识别语义变化时间点&#34;&gt;4.3 识别语义变化时间点&lt;/h3&gt;
&lt;p&gt;该项目研究了1800-1910期间， 每10年为一个单位训练词向量， 探究词语变化。以 &lt;em&gt;&lt;strong&gt;railway&lt;/strong&gt;&lt;/em&gt; 和  &lt;em&gt;&lt;strong&gt;traffic&lt;/strong&gt;&lt;/em&gt; 为例, 先用 余弦相似度(cosine-similarity)算法识别词语语义变化的时间点，如下图&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/consine-sim-cpdetection.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;44-绘制语义变化轨迹&#34;&gt;4.4 绘制语义变化轨迹&lt;/h3&gt;
&lt;p&gt;语义变化轨迹&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/railway-time-shifting.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五获取资源&#34;&gt;五、获取资源&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 免费  专利摘要-Word2Vec.200.15.bin  https://pan.baidu.com/s/1CgBjy96hDKM2GKQY4G6kYA?pwd=ba92

- 免费  province_w2vs                https://pan.baidu.com/s/1eBFTIZcv2DWssLiaRnCqZQ?pwd=ikpu

- 免费  year_w2vs                    https://pan.baidu.com/s/1lrVkML92cVJdHQa1HQyAwA?pwd=4gqa

- 100元  cntext-2.1.5-py3-none-any.whl 如有需要，请加微信 372335839， 备注「姓名-学校-专业」
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
      <content:encoded><![CDATA[<p>想用 <a href="https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/">中国专利申请数据集</a>，按年份(或按省份)训练词向量的同学，可以好好看本文，能节省你几十个小时时间。
<br><br></p>
<h2 id="一检查数据">一、检查数据</h2>
<p>这个数据集很大， 如图所示，文件动辄几G</p>
<p><img loading="lazy" src="img/01-data-screen.png" alt=""  />
</p>
<p>之前分享过 <a href=""></a> , 面对巨大csv文件，我们要了解内部有哪些字段、字段的含义， 只读取需要的字段，减轻电脑内存压力， 让你能轻松应对几倍于内存的巨大csv文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># 以山东省.csv 为例， 只读第一行(前1行)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;山东省.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/02-shandong_df.png" alt=""  />
</p>
<br>
<p>字段展示的不全，完整的字段应该有</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">df</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Index([&#39;专利名称&#39;, &#39;专利类型&#39;, &#39;申请人&#39;, &#39;申请人类型&#39;, &#39;申请人地址&#39;, &#39;申请人国家&#39;, &#39;申请人省份&#39;, &#39;申请人城市&#39;,
       &#39;申请人区县&#39;, &#39;申请号&#39;, &#39;申请日&#39;, &#39;申请年份&#39;, &#39;公开公告号&#39;, &#39;公开公告日&#39;, &#39;公开公告年份&#39;, &#39;授权公告号&#39;,
       &#39;授权公告日&#39;, &#39;授权公告年份&#39;, &#39;IPC分类号&#39;, &#39;IPC主分类号&#39;, &#39;发明人&#39;, &#39;摘要文本&#39;, &#39;主权项内容&#39;, &#39;当前权利人&#39;,
       &#39;当前专利权人地址&#39;, &#39;专利权人类型&#39;, &#39;统一社会信用代码&#39;, &#39;引证次数&#39;, &#39;被引证次数&#39;, &#39;自引次数&#39;, &#39;他引次数&#39;,
       &#39;被自引次数&#39;, &#39;被他引次数&#39;, &#39;家族引证次数&#39;, &#39;家族被引证次数&#39;],
      dtype=&#39;object&#39;)
</code></pre></div><br>
<p>训练词向量主要用文本数据， 在本案例中， 需要的字段 [<strong>专利摘要</strong>] 。</p>
<p><br><br></p>
<h2 id="二构造语料">二、构造语料</h2>
<p>在 [5000万专利申请全量数据1985-2025年] 文件夹中，</p>
<ol>
<li>新建 [province_corpus] 和 [year_corpus] 两个文件夹</li>
<li>新建 [code.ipynb]</li>
</ol>
<p>构造语料对电脑的性能要求不高， 不论你的电脑是什么配置，基本都能运行， 而且耗时在能接受的范围。</p>
<br>
<h3 id="21-文件树结构">2.1 文件树结构</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">5000万专利申请全量数据1985-2025年
  |---中国专利数据库.csv.gz
  |---code.ipynb
  |---province_corpus
     |---安徽省.txt
     |---浙江省.txt
     |---...
  |---year_corpus
     |---2025.txt
     |---2024.txt
     |---...
  |---provin_w2vs
        |---安徽省-Word2Vec.200.15.bin
        |---山东省-Word2Vec.200.15.bin
        |---...
  |---year_w2vs
        |---2025-Word2Vec.200.15.bin
        |---2022-Word2Vec.100.6.bin.syn1neg.npy
        |---2022-Word2Vec.100.6.bin.wv.vectors.npy
        |---...
</code></pre></div><br>
<h3 id="22-构造语料代码">2.2 构造语料代码</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># 读取csv文件， 只读取需要的字段</span>
<span class="c1"># 按 100000 行分块读取， 避免内存溢出</span>
<span class="n">chunk_dfs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;中国专利数据库.csv.gz&#39;</span><span class="p">,</span> 
                 <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">,</span> 
                 <span class="n">usecols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;申请人省份&#39;</span><span class="p">,</span> <span class="s1">&#39;申请日&#39;</span><span class="p">,</span> <span class="s1">&#39;专利名称&#39;</span><span class="p">,</span> <span class="s1">&#39;摘要文本&#39;</span><span class="p">],</span>
                 <span class="n">chunksize</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>

<span class="k">for</span> <span class="n">chunk_df</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">chunk_dfs</span><span class="p">):</span>
    <span class="n">chunk_df</span><span class="p">[</span><span class="s1">&#39;申请日&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">chunk_df</span><span class="p">[</span><span class="s1">&#39;申请日&#39;</span><span class="p">])</span>

    <span class="c1"># 新建 province_corpus 和 year_corpus文件夹</span>
    <span class="n">province_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;province_corpus&#39;</span><span class="p">)</span>
    <span class="n">province_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">year_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;year_corpus&#39;</span><span class="p">)</span>
    <span class="n">year_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># 按省份和年份构造语料</span>
    <span class="k">for</span> <span class="n">date</span><span class="p">,</span> <span class="n">year_df</span> <span class="ow">in</span> <span class="n">chunk_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Grouper</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;申请日&#39;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;YE&#39;</span><span class="p">)):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">date</span><span class="o">.</span><span class="n">year</span><span class="p">)</span>
        <span class="n">year_file</span> <span class="o">=</span> <span class="n">year_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">date</span><span class="o">.</span><span class="n">year</span><span class="si">}</span><span class="s2">.txt&#34;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">year_file</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">yf</span><span class="p">:</span>
            <span class="n">y_text_series</span> <span class="o">=</span> <span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;专利名称&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">year_df</span><span class="p">[</span><span class="s1">&#39;摘要文本&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
            <span class="n">y_text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">y_text_series</span><span class="p">)</span>
            <span class="n">yf</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">y_text</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">prov</span><span class="p">,</span> <span class="n">prov_df</span> <span class="ow">in</span> <span class="n">chunk_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;申请人省份&#39;</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">prov</span><span class="p">)</span>
        <span class="n">prov_file</span> <span class="o">=</span> <span class="n">province_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">prov</span><span class="si">}</span><span class="s2">.txt&#34;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">prov_file</span><span class="p">,</span> <span class="s1">&#39;a+&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pf</span><span class="p">:</span>
            <span class="n">prov_text_series</span> <span class="o">=</span> <span class="n">prov_df</span><span class="p">[</span><span class="s1">&#39;专利名称&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">prov_df</span><span class="p">[</span><span class="s1">&#39;摘要文本&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
            <span class="n">prov_text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">prov_text_series</span><span class="p">)</span>
            <span class="n">pf</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">prov_text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2025
2024
...


上海市
云南省
...
安徽省

CPU times: total: 27min 55s
Wall time: 39min 10s
</code></pre></div><p>构造语料用 40 分钟时间，得到文件夹province_corpus和year_corpus。
<img loading="lazy" src="img/03-province-corpus.png" alt=""  />

<img loading="lazy" src="img/03-year-corpus.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三训练word2vec">三、训练word2vec</h2>
<p>需要注意， 训练word2vec需要耗费很大的计算能力， 训练时间需要一两三。 本文使用的 <em><strong>cntext2.1.5</strong></em> 版本，需要付费获取 <em><strong>cntext-2.1.5-py3-none-any.whl</strong></em>。</p>
<h3 id="31-安装cntext">3.1 安装cntext</h3>
<p>将 <em><strong>cntext-2.1.5-py3-none-any.whl</strong></em> 放置于电脑桌面， 打开 <strong>命令行cmd</strong> (Mac打开<strong>terminal</strong>)， 输入</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
pip install cntext-2.1.5-py3-none-any.whl
</code></pre></div><p>有部分使用win的同学，如果按照操作没有安装成功，再试试</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd Desktop
pip install cntext-2.1.5-py3-none-any.whl
</code></pre></div><br>
<h3 id="32-开始训练">3.2 开始训练</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>

<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># 分年份训练</span>
<span class="k">for</span> <span class="n">year_f</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;year_corpus/*.txt&#39;</span><span class="p">):</span>
    <span class="c1"># 训练word2vec，自动保存到output文件夹内</span>
    <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span> <span class="o">=</span> <span class="n">year_f</span><span class="p">,</span> 
                <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> 
                <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> 
                <span class="n">only_binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 将output文件夹重命名为year_w2vs</span>
<span class="n">os</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="s1">&#39;year_w2vs&#39;</span><span class="p">)</span>


<span class="c1"># 分省份训练</span>
<span class="k">for</span> <span class="n">prov_f</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;province_corpus/*.txt&#39;</span><span class="p">):</span>
    <span class="c1"># 训练word2vec，自动保存到output文件夹内</span>
    <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span> <span class="o">=</span> <span class="n">prov_f</span><span class="p">,</span> 
                <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> 
                <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> 
                <span class="n">only_binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
<span class="c1"># 将output文件夹重命名为province_w2vs</span>
<span class="n">os</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="s1">&#39;province_w2vs&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Windows System, Unable Parallel Processing
Cache output\1985_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|████████████████████████████████████████████████████████| 10009/10009 [00:13&lt;00:00, 734.66it/s]
Reading Preprocessed Corpus from output\1985_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 17 s. 
Output Saved To: output\1985-Word2Vec.200.15.bin

......
......

Windows System, Unable Parallel Processing
Cache output\2025_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|████████████████████████████████████████████████████████| 10009/10009 [00:13&lt;00:00, 734.66it/s]
Reading Preprocessed Corpus from output\2025_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 17 s. 
Output Saved To: output\2025-Word2Vec.200.15.bin


Windows System, Unable Parallel Processing
Cache output\上海市_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|██████████████████████████████████████████████████| 2456943/2456943 [03:42&lt;00:00, 11048.35it/s]
Reading Preprocessed Corpus from output\上海市_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 1400 s. 
Output Saved To: output\上海市-Word2Vec.200.15.bin
......
......

Windows System, Unable Parallel Processing
Cache output\黑龙江省_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|█████████████████████████████████████████████████████| 544329/544329 [01:07&lt;00:00, 8114.12it/s]
Reading Preprocessed Corpus from output\黑龙江省_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 320 s. 
Output Saved To: output\黑龙江省-Word2Vec.200.15.bin


CPU times: total: 21354 s
Wall time: 21758 s
</code></pre></div><p>训练省份词向量大概用了 6 小时，模型文件保存在 <em><strong>provin_w2vs</strong></em> 和 <em><strong>year_w2vs</strong></em> 文件夹内。
<img loading="lazy" src="img/05-province-w2vs.png" alt=""  />

<img loading="lazy" src="img/06-years-w2vs.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="三使用word2vec">三、使用word2vec</h2>
<h3 id="31-导入模型">3.1 导入模型</h3>
<p><code>output/provin_w2vs</code> 和 <code>output/year_w2vs</code> 内有多个模型， 单个的模型文件大约几十M ~ 几百M， <strong>但不建议一次性导入进来</strong>。大邓的电脑内存96G，为了省事，就一次性全导入了。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># 导入各省份词向量</span>
<span class="n">provin_w2vs_</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">provin_w2v_fs</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;province_w2vs/*.bin&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">provin_w2v_f</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">provin_w2v_fs</span><span class="p">):</span>
    <span class="n">provin_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">provin_w2v_f</span><span class="p">)</span>
    <span class="n">provin_w2vs_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">provin_w2v</span><span class="p">)</span>


<span class="c1"># 导入各年份词向量</span>
<span class="n">year_w2vs_</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">year_w2v_fs</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;year_w2vs/*.bin&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">year_w2v_f</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">year_w2v_fs</span><span class="p">):</span>
    <span class="n">year_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">year_w2v_f</span><span class="p">)</span>
    <span class="n">year_w2vs_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">year_w2v</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">  3%|██▍                                                                                | 1/34 [00:03&lt;01:57,  3.57s/it]
Loading province_w2vs\上海市-Word2Vec.200.15.bin...
  6%|████▉                                                                              | 2/34 [00:04&lt;01:11,  2.23s/it]
Loading province_w2vs\云南省-Word2Vec.200.15.bin...
......
97%|███████████████████████████████████████████████████████████████████████████████▌  | 33/34 [01:07&lt;00:01,  1.10s/it]
Loading province_w2vs\香港特别行政区-Word2Vec.200.15.bin...
100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [01:09&lt;00:00,  2.04s/it]
Loading province_w2vs\黑龙江省-Word2Vec.200.15.bin...


  2%|██                                                                                 | 1/41 [00:00&lt;00:05,  7.10it/s]
Loading year_w2vs\1985-Word2Vec.200.15.bin...
Loading year_w2vs\1986-Word2Vec.200.15.bin...
 10%|████████                                                                           | 4/41 [00:00&lt;00:05,  6.80it/s]
 ......
 100%|██████████████████████████████████████████████████████████████████████████████████| 41/41 [01:11&lt;00:00,  1.75s/it]
Loading year_w2vs\2025-Word2Vec.200.15.bin...

</code></pre></div><br>
<h3 id="32-查看词汇量">3.2 查看词汇量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;省份Word2vec词汇量&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">provin_w2v_f</span><span class="p">,</span> <span class="n">provin_w2v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">provin_w2v_fs</span><span class="p">,</span> <span class="n">provin_w2vs_</span><span class="p">):</span>
    <span class="n">province</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">provin_w2v_f</span><span class="p">)</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">province</span><span class="si">}</span><span class="s1"> 词汇量: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">provin_w2v</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">省份Word2vec词汇量</span>
<span class="n">上海市</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">640941</span>
<span class="n">云南省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">205193</span>
<span class="n">内蒙古自治区</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">138507</span>
<span class="n">北京市</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">783162</span>
<span class="n">台湾省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">242630</span>
<span class="n">吉林省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">185587</span>
<span class="n">四川省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">494241</span>
<span class="n">天津市</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">373286</span>
<span class="n">宁夏回族自治区</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">91592</span>
<span class="n">安徽省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">540111</span>
<span class="n">山东省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">722886</span>
<span class="n">山西省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">188013</span>
<span class="n">广东省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">1010230</span>
<span class="n">广西壮族自治区</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">190128</span>
<span class="n">新疆维吾尔自治区</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">110063</span>
<span class="n">江苏省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">983871</span>
<span class="n">江西省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">256695</span>
<span class="n">河北省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">326042</span>
<span class="n">河南省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">415905</span>
<span class="n">浙江省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">795041</span>
<span class="n">海南省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">74657</span>
<span class="n">湖北省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">412827</span>
<span class="n">湖南省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">400262</span>
<span class="n">澳门特别行政区</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">7806</span>
<span class="n">甘肃省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">148753</span>
<span class="n">福建省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">480456</span>
<span class="n">西藏自治区</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">23115</span>
<span class="n">贵州省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">186345</span>
<span class="n">辽宁省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">347563</span>
<span class="n">重庆市</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">358991</span>
<span class="n">陕西省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">381781</span>
<span class="n">青海省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">53325</span>
<span class="n">香港特别行政区</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">71947</span>
<span class="n">黑龙江省</span> <span class="n">词汇量</span><span class="p">:</span> <span class="mi">253129</span>
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;年份word2vec词汇量&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">year_w2v_f</span><span class="p">,</span> <span class="n">year_w2v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">year_w2v_fs</span><span class="p">,</span> <span class="n">year_w2vs_</span><span class="p">):</span>
    <span class="n">year</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">year_w2v_f</span><span class="p">)</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">year_w2v</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">年份word2vec词汇量
1985: 15494
1986: 17945
1987: 23625
1988: 27740
1989: 27394
1990: 32920
1991: 37584
1992: 45393
1993: 48326
1994: 46725
1995: 46138
1996: 50117
1997: 53625
1998: 57187
1999: 65154
2000: 78368
2001: 95927
2002: 123513
2003: 145087
2004: 158694
2005: 185840
2006: 215856
2007: 240167
2008: 279364
2009: 334179
2010: 382888
2011: 449648
2012: 508506
2013: 621644
2014: 625248
2015: 685487
2016: 732443
2017: 760332
2018: 776968
2019: 789104
2020: 817553
2021: 799388
2022: 734045
2023: 596784
2024: 516263
2025: 21230
</code></pre></div><br>
<h3 id="33-语义检查-省份">3.3 语义检查-省份</h3>
<p>先检查省份， 查看与[&lsquo;创新&rsquo;, &lsquo;新颖&rsquo;]最相似的5个词，通过语义捕捉准确与否，粗略判断Word2vec训练效果的好坏。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">provin_w2v_f</span><span class="p">,</span> <span class="n">provin_w2v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">provin_w2v_fs</span><span class="p">,</span> <span class="n">provin_w2vs_</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">province</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">provin_w2v_f</span><span class="p">)</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">wordweigths</span> <span class="o">=</span> <span class="n">provin_w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;新颖&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">p</span> <span class="ow">in</span> <span class="n">wordweigths</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">province</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">province</span><span class="si">}</span><span class="s1">: NA&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">上海市: 独特 巧妙 创新性 理念 新颖结构合理
云南省: 独特 巧妙 精巧 科学合理 新颖合理
内蒙古自治区: 理念 独特 巧妙 合理使用方便 全新
北京市: 巧妙 独特 全新 借鉴 新颖使用方便
台湾省: 全新 独特 精巧 简洁 巧妙
吉林省: 理念 思路 现代 全新 巧妙
四川省: 巧妙 独特 全新 理念 合理使用方便
天津市: 独特 巧妙 合理使用方便 巧妙使用方便 全新
宁夏回族自治区: 更具 多样 丰富 性价比 市场前景
安徽省: 巧妙 独特 巧妙使用方便 合理 合理结构紧凑
山东省: 巧妙 精巧 新颖结构合理 巧妙结构合理 全新
山西省: 独特 全新 科学 现代 已有
广东省: 巧妙 独特 创新性 合理 精巧
广西壮族自治区: 独特 巧妙 合理使用方便 精巧 合理实用性
新疆维吾尔自治区: 合理使用方便 巧妙 简单合理 科学合理 合理
江苏省: 巧妙 独特 合理 全新 科学
江西省: 独特 科学合理 巧妙 简洁 精巧
河北省: 巧妙 新颖使用方便 精巧 独特 理念
河南省: 巧妙 科学合理 独特 新颖使用方便 巧妙使用方便
浙江省: 独特 巧妙 科学 精巧 合理
海南省: 思路 科学合理 人性化 科学 独特
湖北省: 巧妙 巧妙合理 科学合理 独特 新颖结构合理
湖南省: 巧妙 精巧 独特 新颖独特 巧妙结构合理
澳门特别行政区: 撞击 边坡 溜槽 耐高温 材料制成
甘肃省: 独特 全新 理念 现代 普及
福建省: 巧妙 新颖使用方便 独特 巧妙结构合理 全新
西藏自治区: 既能 十分 疲劳 范围广 更加人性化
贵州省: 巧妙 独特 科学合理 合理使用方便 精巧
辽宁省: 巧妙 巧妙结构合理 新颖结构合理 新颖独特 独创
重庆市: 合理使用方便 科学合理 精巧 全新 新颖使用方便
陕西省: 巧妙 独特 新颖结构合理 合理 合理使用方便
青海省: 突破 织机 现行 经济环保 杆织机
香港特别行政区: 全新 更具 美感 市场 丰富
黑龙江省: 独特 精巧 科学合理 巧妙 构思
</code></pre></div><p>从上面的运行结果看， 除青海省，剩下的绝大多数的省份的Word2vec都很准确的捕捉到了专利摘要的语义信息。</p>
<br>
<h3 id="34-语义检查-年份">3.4 语义检查-年份</h3>
<p>查看与[&lsquo;创新&rsquo;, &lsquo;新颖&rsquo;]最相似的5个词，通过语义捕捉准确与否，粗略判断Word2vec训练效果的好坏。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">year_w2v_f</span><span class="p">,</span> <span class="n">year_w2v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">year_w2v_fs</span><span class="p">,</span> <span class="n">year_w2vs_</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">year</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">year_w2v_f</span><span class="p">)</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">wordweigths</span> <span class="o">=</span> <span class="n">year_w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;新颖&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span><span class="n">p</span> <span class="ow">in</span> <span class="n">wordweigths</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">: NA&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1985: 公知 专门 特别适用 提出一种 机器人
1986: 植入 理发 135 婴儿车 街道
1987: 落后 儿童智力开发 低档 胶鞋 指甲
1988: 目前市场 低档 捕鱼 证件 普遍使用
1989: 价廉物美 课堂教学 普及型 大众 得心应手
1990: 单纯 精简 多方面 机等 应用领域
1991: 普及型 前途 大众 现代 现代化
1992: 构思 保留传统 不失为 全新 机之
1993: 完美 现代科技 崭新 式样 边墙
1994: 样式 别致 造型新颖 显得 华贵
1995: 多样 结实耐用 独特 别致 高档
1996: 花样 耐冲击浮标 极其 式样新颖 应用范围
1997: 实用美观 室内外装饰 标准化 多变 形象逼真
1998: 现代 改革 开发 高雅 创造
1999: 现代 市场 越来越 款式 大方
2000: 全新 娱乐性趣味性 多样化 多方面 各种各样
2001: 完美 现代 新颖别致 多样化 体现
2002: 全新 现代 实为 科学 大众
2003: 全新 突破传统 多样化 体现 科学
2004: 全新 科技 现代 市场 科学合理
2005: 创意 理念 全新 科学 现代
2006: 全新 构思 理念 新颖性 独特
2007: 全新 突破传统 巧妙 独特 现代
2008: 独特 巧妙 全新 新颖独特 设计理念
2009: 独特 巧妙 全新 科学 新颖独特
2010: 独特 新颖独特 精巧 巧妙 科学合理
2011: 独特 精巧 新颖独特 科学合理 巧妙
2012: 独特 巧妙 新颖独特 精巧 科学合理
2013: 独特 新颖独特 精巧 科学合理 巧妙
2014: 巧妙 独特 科学合理 巧妙合理 精巧
2015: 巧妙 独特 巧妙合理 新颖结构合理 科学合理
2016: 巧妙 结合现在 巧妙合理 独特 全新
2017: 巧妙 科学合理 独特 科学 合理
2018: 巧妙 独特 合理 科学 全新
2019: 巧妙 独特 合理 全新 精巧
2020: 巧妙 独特 合理 精巧 设计
2021: 巧妙 精巧 合理 新颖结构合理 全新
2022: 巧妙 全新 独特 创新性 精巧
2023: 巧妙 独特 全新 创新性 精巧
2024: 巧妙 创新性 独特 精巧 全新
2025: 相结合 双重 独特 多重 优势
</code></pre></div><p>也试了其他的词语，好像 1998 年之后的捕捉的语义是准确的。</p>
<br>
<h2 id="四研究潜力-语义变迁研究方法介绍">四、研究潜力: 语义变迁研究方法介绍</h2>
<p>假设语义都很准的话， 是可以研究 <strong>语义变迁</strong> 或者 <strong>语义差异</strong> 的。 但需要注意， 不能直接使用两个年份或者两个省份的中word1和word2的距离来体现语义的变迁或者语义的差异。 如果想做省份间差异或者某省份随时间的变化， 需要用到 <strong>对齐算法</strong>， 常用的算法是 <strong>正交Procrustes矩阵对齐</strong>， 使得同省份不同年份或者通年份不同省份的word2vec都有相同的语义空间。</p>
<h3 id="41-正交procrustes算法">4.1 正交Procrustes算法</h3>
<p>正交Procrustes矩阵对齐是一种将两个预训练语言模型的词向量矩阵对齐的方法，使得它们在相同的语义空间中表示。具体来说，它通过计算一个正交矩阵，将两个词向量矩阵进行线性变换，使得它们的Frobenius范数之和最小，从而实现对齐。 在 cntext2.x中实现了Procrustes对齐函数 <em><strong>ct.procrustes_align()</strong></em>，具体可阅读 <a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">文本分析库cntext2.x使用手册</a></p>
<br>
<h3 id="42-语义变迁流程图">4.2 语义变迁流程图</h3>
<p>语义变迁类研究的流程图可参考 <a href="https://github.com/Living-with-machines/DiachronicEmb-BigHistData">DiachronicEmb-BigHistData</a></p>
<p><img loading="lazy" src="img/w2v-time-shifting-flowchart.png" alt=""  />
</p>
<p>大邓在 <a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a> 实现了历时语义对齐， 可以看出70 年整个中国社会的认知变迁。</p>
<br>
<h3 id="43-识别语义变化时间点">4.3 识别语义变化时间点</h3>
<p>该项目研究了1800-1910期间， 每10年为一个单位训练词向量， 探究词语变化。以 <em><strong>railway</strong></em> 和  <em><strong>traffic</strong></em> 为例, 先用 余弦相似度(cosine-similarity)算法识别词语语义变化的时间点，如下图</p>
<p><img loading="lazy" src="img/consine-sim-cpdetection.png" alt=""  />
</p>
<br>
<h3 id="44-绘制语义变化轨迹">4.4 绘制语义变化轨迹</h3>
<p>语义变化轨迹</p>
<p><img loading="lazy" src="img/railway-time-shifting.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="五获取资源">五、获取资源</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 免费  专利摘要-Word2Vec.200.15.bin  https://pan.baidu.com/s/1CgBjy96hDKM2GKQY4G6kYA?pwd=ba92

- 免费  province_w2vs                https://pan.baidu.com/s/1eBFTIZcv2DWssLiaRnCqZQ?pwd=ikpu

- 免费  year_w2vs                    https://pan.baidu.com/s/1lrVkML92cVJdHQa1HQyAwA?pwd=4gqa

- 100元  cntext-2.1.5-py3-none-any.whl 如有需要，请加微信 372335839， 备注「姓名-学校-专业」
</code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>可视化 | 人民日报语料反映七十年文化演变</title>
      <link>https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/</guid>
      <description>使用人民日报1946-2023年之间的新闻数据，通过语义距离刻画文化的变迁。</description>
      <content:encoded><![CDATA[<h2 id="一引言">一、引言</h2>
<p>社会文化是一个不断演变的复杂系统，受到历史、科技、经济和社会变革等多种因素的影响。随着时代的推移，人们的语言使用和文化认知也经历着变迁，反映着社会的发展脉络。在这个背景下，使用Word2Vec等词嵌入技术来研究社会文化变迁和刻板印象的重要性日益凸显。</p>
<p>Word2Vec作为一种词向量表示方法，通过将词汇映射到高维空间中的向量，有效地捕捉了词语之间的语义关系。这使得我们能够以全新的方式理解语言的演变和文化认知的转变。通过对比不同时期的Word2Vec模型，我们可以深入挖掘语言的时代特征，捕捉到文化观念、价值观念以及社会角色的演变。</p>
<p>研究社会文化变迁和刻板印象，不仅有助于解构历史时刻下的社会结构和文化动态，还能为我们提供深刻的洞察力，揭示出社会变迁中潜在的驱动力和趋势。这种研究有助于建构更为全面、客观的历史记忆，帮助我们更好地理解人类行为背后的深层次原因。</p>
<p><br><br></p>
<h2 id="二训练模型">二、训练模型</h2>
<h3 id="21-获取数据">2.1 获取数据</h3>
<ul>
<li><a href="https://textdata.cn/blog/2023-12-14-daily-news-dataset/">新闻数据集 | 含 人民日报/经济日报/光明日报 等数十家媒体(2024.05)</a></li>
<li><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">文本分析库 cntext2.x 获取方式&amp;使用手册</a></li>
</ul>
<br>
<h3 id="22--构造语料">2.2  构造语料</h3>
<p>本使用的 <em><strong>rmrb.csv.gz</strong></em> 对该数据集感兴趣的同学，可点击查看  <a href="https://textdata.cn/blog/2023-12-14-daily-news-dataset/">新闻数据集 | 含 人民日报/经济日报/光明日报 等数十家媒体(2024.05)</a>  。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">])</span>

<span class="c1"># 每5年构造一个语料txt文件</span>
<span class="k">for</span> <span class="n">date</span><span class="p">,</span> <span class="n">freq_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Grouper</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;5YE&#39;</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">date</span><span class="p">)</span>
    <span class="n">corpus_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;corpus&#39;</span><span class="p">)</span>
    <span class="n">corpus_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">corpus_file</span> <span class="o">=</span> <span class="n">corpus_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">date</span><span class="o">.</span><span class="n">year</span><span class="si">}</span><span class="s2">.txt&#34;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">corpus_file</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">text_series</span> <span class="o">=</span> <span class="n">freq_df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="n">raw_text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text_series</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1946-12-31 00:00:00
1951-12-31 00:00:00
1956-12-31 00:00:00
1961-12-31 00:00:00
1966-12-31 00:00:00
1971-12-31 00:00:00
1976-12-31 00:00:00
1981-12-31 00:00:00
1986-12-31 00:00:00
1991-12-31 00:00:00
1996-12-31 00:00:00
2001-12-31 00:00:00
2006-12-31 00:00:00
2011-12-31 00:00:00
2016-12-31 00:00:00
2021-12-31 00:00:00
2026-12-31 00:00:00
CPU times: user 2.64 s, sys: 1.54 s, total: 4.18 s
Wall time: 5.29 s
</code></pre></div><p><img loading="lazy" src="img/01-corpus.jpg" alt=""  />
</p>
<p>语料txt命名规则， 实际上每个 <em><strong>year.txt</strong></em> 是存储了 <em><strong>year-5</strong></em>  ~  <em><strong>year</strong></em> 期间的新闻数据。</p>
<p><em><strong>1946.txt</strong></em> 内实际上只存储了 <em><strong>1946.5.15</strong></em> ~ <em><strong>1946.12.31</strong></em> 之间半年多的数据， 由于数据量太小，后续训练出的 <em><strong>word2vec</strong></em> 模型，其语义大概率不准。</p>
<p><em><strong>2006.txt</strong></em> 存储了 <em><strong>2002.1.1. ~ 2006.12.31</strong></em> 之间所有的数据</p>
<p>而 <em><strong>2026.txt</strong></em> 则存储了 <em><strong>2022.1.1 ~ 2026.12.31</strong></em> 之间所有的数据</p>
<br>
<h2 id="三训练word2vec">三、训练word2vec</h2>
<h3 id="31-配置环境">3.1 配置环境</h3>
<p>安装方法，将 <em><strong>cntext-2.1.5-py3-none-any.whl</strong></em> 放置于桌面， 打开命令行 <em><strong>cmd</strong></em> （mac是terminal), 依次执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
pip3 install cntext-2.1.5-py3-none-any.whl
</code></pre></div><blockquote>
<p><em><strong>cntext-2.1.5-py3-none-any.whl</strong></em>， 100元 ； 如需 cntext2.x 可加微信 372335839 获取。</p>
</blockquote>
<br>
<h3 id="32-开始训练">3.2 开始训练</h3>
<p>训练代码比较简单，已经封装到 <strong>cntext2.x</strong>， 只需几行代码即可。且 cntext2.x 对代码进行了优化， 训练速度更快， 内存占用更小。</p>
<p>训练环境 Mac 内存 96G， 大家回去可以试试 16G、32G，应该也能跑通。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">glob</span>


<span class="c1"># 获取corpus文件夹内的所有语料txt文件的文件路径</span>
<span class="n">corpus_files</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;corpus/*.txt&#39;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">corpus_file</span> <span class="ow">in</span> <span class="n">corpus_files</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">corpus_file</span><span class="p">)</span>
    <span class="c1"># 结果自动保存到output文件夹内</span>
    <span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="n">corpus_file</span><span class="p">,</span>
                 <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                 <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                 <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>  
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-corpus/1946.txt" data-lang="corpus/1946.txt">Mac(Linux) System, Enable Parallel Processing
Cache output/1946_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|███████████████████| 5954/5954 [00:07&lt;00:00, 757.68it/s]
Reading Preprocessed Corpus from output/1946_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 16 s. 
Output Saved To: output/1946-Word2Vec.200.15.bin
......
......
......
corpus/2026.txt
Mac(Linux) System, Enable Parallel Processing
Cache output/2026_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|██████████████| 105037/105037 [00:34&lt;00:00, 3075.29it/s]
Reading Preprocessed Corpus from output/2026_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 194. 
Output Saved To: output/2026-Word2Vec.200.15.bin
CPU times: user 2h 38min 4s, sys: 4min 41s, total: 2h 42min 45s
Wall time: 1h 5min 39s
</code></pre></div><p><img loading="lazy" src="img/02-word2vec.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四检查模型">四、检查模型</h2>
<p>现在我们要检查模型， 为了方便，我就随机抽查 1946/1981/2001/2026， 查看这四个模型关于「工业」的近义词，看模型语义捕捉的准不准。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">mfiles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;output/1946-Word2Vec.200.15.bin&#39;</span><span class="p">,</span>
          <span class="s1">&#39;output/1981-Word2Vec.200.15.bin&#39;</span><span class="p">,</span>
          <span class="s1">&#39;output/2001-Word2Vec.200.15.bin&#39;</span><span class="p">,</span>
          <span class="s1">&#39;output/2026-Word2Vec.200.15.bin&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">mfile</span> <span class="ow">in</span> <span class="n">mfiles</span><span class="p">:</span>
    <span class="n">w2v_model</span>  <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">mfile</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">mfile</span><span class="p">)</span>
    <span class="n">word_scores</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;工业&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">word_scores</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">output/1946-Word2Vec.200.15.bin
市场 0.9601176381111145
重工业 0.9589242935180664
工业部门 0.9484396576881409
物价 0.9464751482009888
商业 0.9423016309738159
工业原料 0.9378510117530823
焦煤 0.9368941783905029
物价暴涨 0.9348677396774292
第一年 0.9331346154212952
农产品 0.9329909682273865
输入 0.9329512119293213
农业 0.9327669143676758
水平 0.9323830008506775
通货 0.9320995807647705
国民经济 0.9268764853477478
投资 0.9261932373046875
输出 0.9258642792701721
钢铁工厂 0.925421953201294
工业生产 0.9251945614814758
十三亿 0.9251589775085449

Loading output/1981-Word2Vec.200.15.bin...
output/1981-Word2Vec.200.15.bin
工业部门 0.6696006655693054
重工业 0.6490920782089233
建筑业 0.6461381316184998
轻工业 0.6443966627120972
工业生产 0.6364479064941406
机器制造业 0.6220380067825317
化学工业 0.6116607785224915
钢铁工业 0.5941601991653442
加工工业 0.5932750701904297
电子工业 0.5880091190338135
轻纺工业 0.5786471366882324
食品工业 0.5777474045753479
重工业轻工业 0.5734774470329285
民用工业 0.5729294419288635
消费品生产 0.5721379518508911
纺织业 0.56629878282547
农业轻工业 0.5642068982124329
机器制造 0.5622154474258423
制造业 0.5620284676551819
化工 0.5588406324386597

Loading output/2001-Word2Vec.200.15.bin...
output/2001-Word2Vec.200.15.bin
重工业 0.6766582727432251
工业生产 0.6742461323738098
制造业 0.641242504119873
轻工业 0.615958571434021
传统产业 0.6039909720420837
加工工业 0.5936708450317383
机械电子 0.5892737507820129
工业部门 0.5891364216804504
轻工 0.5785651803016663
化学工业 0.5783289670944214
纺织 0.5708677172660828
支柱行业 0.5655868053436279
钢铁工业 0.5648497939109802
化工 0.5617026686668396
机械工业 0.5609593987464905
振兴国防科技 0.5588745474815369
纺织业 0.5520373582839966
工业体系 0.5505329370498657
工业总产值 0.5477191805839539
冶金纺织 0.5463222861289978

Loading output/2026-Word2Vec.200.15.bin...
output/2026-Word2Vec.200.15.bin
制造业 0.6705414056777954
工业生产 0.6067013144493103
智能制造 0.5936543941497803
轻工业 0.5885797142982483
钢铁行业 0.5884692072868347
化工 0.5675483345985413
钢铁企业 0.5637045502662659
工业互联网 0.559167742729187
装备制造业 0.5545477271080017
制造 0.5482359528541565
建筑业 0.5467448234558105
冶金 0.5400071740150452
规模工业 0.5395020246505737
重工业 0.537196695804596
钢铁 0.5245063304901123
工业遗产 0.5208563804626465
钢铁工业 0.5142995715141296
改数 0.512413740158081
纺织业 0.5109716653823853
规上工业 0.5082385540008545
</code></pre></div><p>从四个年代，我们可以看到中国人民对于 「<strong>工业</strong>」的认识发生了变化， 相比建国初期的一穷二白，工农业等领域经济凋敝； 而2026年的「<strong>工业</strong>」已实现工业现代化，更加注制造业、智能制造、工业互联网、装备制造业等概念。</p>
<p><br><br></p>
<h2 id="五对齐模型">五、对齐模型</h2>
<h3 id="51--为什么要进行对齐">5.1  为什么要进行对齐?</h3>
<p>Word2Vec是一种词嵌入（word embedding）算法，它将词语映射到高维空间中的向量，使得语义相近的词在该空间中距离较近。然而，不同年份的Word2Vec模型在训练时可能受到不同的语料库、训练参数等因素的影响，导致它们的向量空间之间存在一定的差异，所以不能直接拿不同年年份模型直接进行语义比较。</p>
<p><strong>Procrustes对齐算法目的是通过线性变换来使两个向量空间尽可能地对齐，以便进行比较</strong>。这个过程涉及到对两个向量空间进行旋转、缩放和平移等变换，使它们在某种意义上尽量一致。</p>
<p>具体原因包括：</p>
<ol>
<li><strong>词汇漂移（Lexical Drift）：</strong> 随着时间的推移，词汇的含义和使用可能发生变化，导致不同年份的语料库中的词语存在一定的漂移。Procrustes分析可以在一定程度上对齐这种漂移。</li>
<li><strong>训练参数不同：</strong> Word2Vec模型的训练参数，如窗口大小、迭代次数等，可能在不同年份有所不同，导致生成的向量空间差异较大。</li>
<li><strong>语料库的差异：</strong> 不同年份的语料库可能覆盖的主题、文体等存在差异，这也会影响词向量的学习结果。</li>
</ol>
<p>通过Procrustes对齐，可以在一定程度上解决这些问题，使得不同年份的Word2Vec模型在语义上更具可比性。这有助于在跨时间的语料库中进行一致的语义分析。</p>
<br>
<h3 id="52-对齐之后">5.2 对齐之后</h3>
<p>对齐后的Word2Vec模型进行的语义变迁研究：</p>
<ol>
<li><strong>词义演变：</strong> 比较不同年份相同词汇的词向量，观察其在向量空间中的位置变化，分析词义在语义空间中的演变趋势。</li>
<li><strong>语境变迁：</strong> 考察同一词语在不同年份的上下文中的变化，了解词语在不同语境下的语义演变情况。</li>
<li><strong>主题变迁：</strong> 通过对齐后的向量空间，分析不同年份语料库中词语的主题分布变化，探讨社会、文化因素对语言使用的影响。</li>
<li><strong>时代特征分析：</strong> 通过对比不同年份的模型，识别出每个时期在词向量空间中的独特特征，从而揭示时代背景对语义的影响。</li>
<li><strong>探索新兴词汇：</strong> 通过对比不同年份的模型，发现在语义空间中新兴词汇的出现和演变，了解新兴概念和文化趋势。</li>
</ol>
<p>总的来说，通过对齐Word2Vec模型，你可以更准确地比较不同年份的语料库，深入研究语义的演变和语言使用的变迁。这有助于揭示社会、文化、科技等方面的发展对语言表达的影响。</p>
<br>
<h3 id="53-对齐代码">5.3 对齐代码</h3>
<p>使用 <strong>cntext2.1.5</strong>，未公开，需微信大邓 372335839 购买获取。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">glob</span>

<span class="c1"># 基准模型</span>
<span class="n">base_wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/2026-Word2Vec.200.15.bin&#39;</span><span class="p">)</span>

<span class="c1">#将其他模型与基准模型对齐</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;output/*.bin&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="n">other_wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="n">procrusted_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">procrustes_align</span><span class="p">(</span><span class="n">base_wv</span><span class="o">=</span><span class="n">base_wv</span><span class="p">,</span>
                                         <span class="n">other_wv</span><span class="o">=</span><span class="n">other_wv</span><span class="p">)</span>
    <span class="c1"># win</span>
    <span class="c1">#year = file.split(&#39;\\&#39;)[-1][:4]</span>
    
    <span class="c1"># mac</span>
    <span class="n">year</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">4</span><span class="p">]</span>
    
    <span class="n">output_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;Aligned_Word2Vec&#39;</span><span class="p">)</span>
    <span class="n">output_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">procrusted_w2v</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Aligned_Word2Vec/</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">.200.15.bin&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading output/2026-Word2Vec.200.15.bin...
output/1956-Word2Vec.200.15.bin
Loading output/1956-Word2Vec.200.15.bin...

output/2026-Word2Vec.200.15.bin
Loading output/2026-Word2Vec.200.15.bin...

output/2021-Word2Vec.200.15.bin
Loading output/2021-Word2Vec.200.15.bin...

output/1951-Word2Vec.200.15.bin
Loading output/1951-Word2Vec.200.15.bin...

output/1946-Word2Vec.200.15.bin
Loading output/1946-Word2Vec.200.15.bin...

output/2001-Word2Vec.200.15.bin
Loading output/2001-Word2Vec.200.15.bin...

output/1981-Word2Vec.200.15.bin
Loading output/1981-Word2Vec.200.15.bin...

output/1971-Word2Vec.200.15.bin
Loading output/1971-Word2Vec.200.15.bin...

output/1976-Word2Vec.200.15.bin
Loading output/1976-Word2Vec.200.15.bin...

output/2006-Word2Vec.200.15.bin
Loading output/2006-Word2Vec.200.15.bin...

output/1986-Word2Vec.200.15.bin
Loading output/1986-Word2Vec.200.15.bin...

output/1961-Word2Vec.200.15.bin
Loading output/1961-Word2Vec.200.15.bin...

output/2011-Word2Vec.200.15.bin
Loading output/2011-Word2Vec.200.15.bin...

output/1991-Word2Vec.200.15.bin
Loading output/1991-Word2Vec.200.15.bin...

output/2016-Word2Vec.200.15.bin
Loading output/2016-Word2Vec.200.15.bin...

output/1996-Word2Vec.200.15.bin
Loading output/1996-Word2Vec.200.15.bin...

output/1966-Word2Vec.200.15.bin
Loading output/1966-Word2Vec.200.15.bin...

CPU times: user 1min 8s, sys: 49.7 s, total: 1min 58s
Wall time: 46.3 s
</code></pre></div><p><img loading="lazy" src="img/03-align.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="六实验-文化变迁">六、实验-文化变迁</h2>
<p>时代的宣传必然在语义中深刻的影响社会认知，不同时代语料中必然蕴含着不同的文化特征，如语义距离的变化。这里我演示 两个对立词组分别与目标词组进行语义距离计算， 根据语义距离反应刻板印象态度偏见，其实这也反映了文化变迁。</p>
<h3 id="61-性别与成功">6.1 性别与成功</h3>
<p>男性、女性与成功之间的语义距离</p>
<p><strong>cntext2.1.5</strong> 内置了两种算法， 语义投影和语义距离，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">distance = distance(女, 成功) - distance(男, 成功)
</code></pre></div><p>如果distance趋近于0， 男女在成功概念上语义接近， 无明显刻板印象。</p>
<p>但是当distance明显大于0， 当人们聊到成功概念时，更容易联想到男性，而不是女性。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import pandas as pd
import glob

gender_suceess_data = []

words = [&#39;成功&#39;, &#39;成就&#39;, &#39;胜利&#39;]
c_words1 = [&#39;女&#39;, &#39;女人&#39;, &#39;她&#39;, &#39;母亲&#39;, &#39;女儿&#39;, &#39;奶奶&#39;]
c_words2 = [&#39;男&#39;, &#39;男人&#39;, &#39;他&#39;, &#39;父亲&#39;, &#39;儿子&#39;, &#39;爷爷&#39;]

# 当前代码所处文件 与 Aligned_Word2Vec 处于同一文件夹内
mfiles = sorted(glob.glob(&#39;Aligned_Word2Vec/*.bin&#39;))
for file in mfiles:
    distance = ct.sematic_distance(wv=ct.load_w2v(file),
                                   words=words, 
                                   c_words1=c_words1, 
                                   c_words2=c_words2)
    data = dict()
    data[&#39;year&#39;] = file.split(&#39;/&#39;)[-1][:4]
    data[&#39;distance&#39;] = distance
    gender_suceess_data.append(data)
    

gender_success_df = pd.DataFrame(gender_suceess_data)
gender_success_df
</code></pre></div><p><img loading="lazy" src="img/04-df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">ct</span><span class="o">.</span><span class="n">matplotlib_chinese</span><span class="p">()</span> <span class="c1">#为正常显示中文</span>

<span class="n">gender_suceess_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;成功&#39;</span><span class="p">,</span> <span class="s1">&#39;成就&#39;</span><span class="p">,</span> <span class="s1">&#39;胜利&#39;</span><span class="p">]</span>
<span class="n">c_words1</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女人&#39;</span><span class="p">,</span> <span class="s1">&#39;她&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;女儿&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]</span>
<span class="n">c_words2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男人&#39;</span><span class="p">,</span> <span class="s1">&#39;他&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;儿子&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]</span>

<span class="n">mfiles</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;Aligned_Word2Vec/*.bin&#39;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">mfiles</span><span class="p">:</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">file</span><span class="p">),</span>
                                   <span class="n">words</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> 
                                   <span class="n">c_words1</span><span class="o">=</span><span class="n">c_words1</span><span class="p">,</span> 
                                   <span class="n">c_words2</span><span class="o">=</span><span class="n">c_words2</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">4</span><span class="p">]</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;distance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">distance</span>
    <span class="n">gender_suceess_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    
<span class="n">gender_success_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">gender_suceess_data</span><span class="p">)</span>
<span class="n">gender_success_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;人民日报在「成就」概念的文化变迁&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;大于0表示社会更容易将成功与男性联系起来&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/04-gender.png" alt=""  />
</p>
<p>从图中可以看到， 新中国初期， 我国的女性解放运动在全世界都是领先的，成果十分卓著。而今耳熟能详的口号恰好说明当时的宣传已经刻入每个中国人的认知中，如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 谁说女子不如男
- 不爱红装爱武装
- 女人撑起半边天
...
</code></pre></div><br>
<p>提到「成功概念」时，在新中国初期，由于破除性别刻板印象，宣传更加中性， 立榜样考虑了性别的平衡。而随着时间推移，口号式的宣传运动沉寂后， 历史的惯性(传统文化的基因)可能会重新复活， 提到「成功概念」时，社会更容易将「成功」与「男性」联系起来。</p>
<br>
<h3 id="52-性别与责任">5.2 性别与责任</h3>
<p>成就与男性有更高的关联， 背后是否意味着传统文化建构的社会要求男性承担远多于女性的责任。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python">
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">ct</span><span class="o">.</span><span class="n">matplotlib_chinese</span><span class="p">()</span> <span class="c1">#为正常显示中文</span>

<span class="n">gender_responsibility_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">words</span> <span class="o">=</span>   <span class="p">[</span><span class="s1">&#39;责任&#39;</span><span class="p">,</span> <span class="s1">&#39;重担&#39;</span><span class="p">,</span> <span class="s1">&#39;担当&#39;</span><span class="p">]</span>
<span class="n">c_words1</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女人&#39;</span><span class="p">,</span> <span class="s1">&#39;她&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;女儿&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]</span>
<span class="n">c_words2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男人&#39;</span><span class="p">,</span> <span class="s1">&#39;他&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;儿子&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]</span>

<span class="n">mfiles</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;Aligned_Word2Vec/*.bin&#39;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">mfiles</span><span class="p">:</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">file</span><span class="p">),</span>
                                   <span class="n">words</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> 
                                   <span class="n">c_words1</span><span class="o">=</span><span class="n">c_words1</span><span class="p">,</span> 
                                   <span class="n">c_words2</span><span class="o">=</span><span class="n">c_words2</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">4</span><span class="p">]</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;distance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">distance</span>
    <span class="n">gender_responsibility_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    
<span class="n">gender_responsibility_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">gender_responsibility_data</span><span class="p">)</span>
<span class="n">gender_responsibility_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;人民日报在「责任」语义的文化变迁&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;大于0表示社会更容易将「责任」与男性联系起来&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/05-responsibility.png" alt=""  />
</p>
<p>从图中可以看出，在大多数年份， distance是大于0的，即 提到「责任」概念时，社会更容易联想到「男性」，而不是「女性」。</p>
<p><br><br></p>
<h2 id="七获取资源">七、获取资源</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 免费   Word2Vec          https://pan.baidu.com/s/1Ru_wxu9egsmhM7lATjSlgQ?pwd=bcea

- 免费   Aligned_Word2Vec  https://pan.baidu.com/s/1IVgP0MyQpez0hpoJyEyFdA?pwd=7qsu

- 100元  cntext-2.1.5-py3-none-any.whl  如有需要，加微信 372335839 ，备注「姓名-学校-专业」
</code></pre></div><br>
<br>
<h2 id="八相关内容">八、相关内容</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.
[2]Hamilton, William L., Jure Leskovec, and Dan Jurafsky. &#34;Diachronic word embeddings reveal statistical laws of semantic change.&#34; arXiv preprint arXiv:1605.09096 (2016).
[3]Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. &#34;Word embeddings quantify 100 years of gender and ethnic stereotypes.&#34; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.
[3]Aceves, Pedro, and James A. Evans. “Mobilizing conceptual spaces: How word embedding models can inform measurement and theory within organization science.” Organization Science (2023).
[4]Kozlowski, A.C., Taddy, M. and Evans, J.A., 2019. The geometry of culture: Analyzing the meanings of class through word embeddings. American Sociological Review, 84(5), pp.905-949.
</code></pre></div><br>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量  | 使用<strong>人民网领导留言板</strong>语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用 Stanford Glove 代码训练中文语料的 GloVe 模型</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>词向量 | 使用 MD&amp;A2001-2023 语料训练 Word2Vec/GloVe 模型</title>
      <link>https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/</guid>
      <description>&lt;h2 id=&#34;一数据集&#34;&gt;一、数据集&lt;/h2&gt;
&lt;h3 id=&#34;11-数据概况&#34;&gt;1.1 数据概况&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/&#34;&gt;&lt;strong&gt;数据集 | 2001-2023 年 A 股上市公司年报&amp;amp;管理层讨论与分析&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据名称: 2001-2023年A股上市公司年报&amp;amp;管理层讨论与分析
数据来源: 上海证券交易所、深圳证券交易所
数据格式: csv、txt
公司数量: 5606
MD&amp;amp;A数量: 60079
会计年度: 2001-2023
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;12-读取-mda-数据&#34;&gt;1.2 读取 md&amp;amp;a 数据&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 读取前5行数据&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mda01-23.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compression&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;gzip&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nrows&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# gz解压后读取csv&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# df = pd.read_csv(&amp;#39;mda01-23.csv&amp;#39;, nrows=5)&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;60079
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;二训练-word2vec--glove-模型&#34;&gt;二、训练 Word2Vec &amp;amp; GloVe 模型&lt;/h2&gt;
&lt;h3 id=&#34;21-准备语料&#34;&gt;2.1 准备语料&lt;/h3&gt;
&lt;p&gt;从 &lt;strong&gt;mda01-23.csv.gz&lt;/strong&gt; 数据中抽取出所有文本，写入到 &lt;strong&gt;mda01-23.txt&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mda01-23.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;最终得到 2.88G 的语料文件。&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;22-配置-cntext-环境&#34;&gt;2.2 配置 cntext 环境&lt;/h2&gt;
&lt;p&gt;使用 2.1.6 版本 cntext 库(该版本暂不开源，需付费购买)。 将得到的 &lt;strong&gt;cntext-2.1.6-py3-none-any.whl&lt;/strong&gt; 文件放置于电脑桌面， win 系统打开&lt;strong&gt;cmd&lt;/strong&gt;(Mac 打开 terminal)， 输入如下命令(将工作环境切换至桌面)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;个别 Win 用户如无效，试试&lt;code&gt;cd Desktop&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;继续在 cmd (terminal) 中执行如下命令安装 cntext2.1.6&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install cntext-2.1.6-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;23-开始训练&#34;&gt;2.3 开始训练&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Word2Vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mda01-23.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 语料文件&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;             &lt;span class=&#34;c1&#34;&gt;# 中文语料&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;            &lt;span class=&#34;c1&#34;&gt;# 嵌入的维度数&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;             &lt;span class=&#34;c1&#34;&gt;# 词语上下文的窗口大小&lt;/span&gt;



&lt;span class=&#34;n&#34;&gt;glove_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GloVe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mda01-23.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                       &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                       &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                       &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Mac(Linux) System, Enable Parallel Processing
Cache output/mda01-23_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|█████████| 12437725/12437725 [03:54&amp;lt;00:00, 52930.57it/s]
Reading Preprocessed Corpus from output/mda01-23_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 1370 s.
Output Saved To: output/mda01-23-Word2Vec.200.15.bin


Mac(Linux) System, Enable Parallel Processing
Cache output/mda01-23_cache.txt Found, Skip Preprocessing Corpus
Start Training GloVe
BUILDING VOCABULARY
Using vocabulary of size 452721.
......
04/03/25 - 04:07.06PM, iter: 001, cost: 0.112966
04/03/25 - 04:07.56PM, iter: 002, cost: 0.079845
......
04/03/25 - 04:18.06PM, iter: 014, cost: 0.048427
04/03/25 - 04:18.56PM, iter: 015, cost: 0.047962

GloVe Training Cost 1229 s.
Output Saved To: output/mda01-23-GloVe.200.15.bin
CPU times: user 1h 19min 15s, sys: 58.5 s, total: 1h 20min 14s
Wall time: 43min 20s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;经过 80 分钟， 训练出的中国 A 股管理层讨论与分析的 GloVe 和 Word2Vec 词向量模型(如下截图)，词汇量 914058， 模型文件 1.49G。模型可广泛用于经济管理等领域概念(情感)词典的构建或扩展。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;mda01-23_cache.txt&lt;/strong&gt; 缓存文件&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mda01-23-Word2Vec.200.15.bin&lt;/strong&gt; Word2Vec 模型文件&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;mda01-23-GloVe.200.15.bin&lt;/strong&gt; GloVe 模型文件&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/pretained-screen.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;三使用模型&#34;&gt;三、使用模型&lt;/h2&gt;
&lt;h3 id=&#34;31-导入模型&#34;&gt;3.1 导入模型&lt;/h3&gt;
&lt;p&gt;使用 &lt;strong&gt;&lt;em&gt;ct.load_w2v(w2v_path)&lt;/em&gt;&lt;/strong&gt; 来导入刚刚训练好的模型 &lt;strong&gt;&lt;em&gt;mda01-23-GloVe.200.15.bin&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__version__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;   &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/mda01-23-Word2Vec.200.15.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;glove_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/mda01-23-GloVe.200.15.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2.1.6
Loading output/mda01-23-Word2Vec.200.15.bin...
Loading output/mda01-23-GloVe.200.15.bin...
&amp;lt;gensim.models.keyedvectors.KeyedVectors at 0x633060fe0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-评估模型&#34;&gt;3.2 评估模型&lt;/h3&gt;
&lt;p&gt;使用近义法和类比法， 判断模型的表现。详情可查看&lt;a href=&#34;https://cntext.readthedocs.io/zh-cn/latest/model.html&#34;&gt;文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;以 word2vec 为例，评估模型表现&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evaluate_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evaluate_analogy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&amp;#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   421    |    116     |            0.41            |
+----------+------------+----------------------------+


类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|██████████████| 1198/1198 [00:11&amp;lt;00:00, 99.91it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   407    |    270     |   27.27    |   3.96   |
|   CityInProvince   |   175    |     0      |   97.14    |   1.29   |
| FamilyRelationship |    90    |    182     |   10.00    |   3.89   |
|   SocialScience    |    9     |     61     |   44.44    |   3.00   |
+--------------------+----------+------------+------------+----------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;近义测试&lt;/strong&gt;: Spearman&amp;rsquo;s Rank Coeficient 系数取值[-1, 1], 取值越大， 说明模型表现越好。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;类比测试&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CapitalOfCountries 中文 md&amp;amp;a 语料在此项表现较差， 应该是语料中常见国家首度的提及较少。也体现了大多数企业没有国际化。盲猜美股的 CapitalOfCountries 表现应该好于 A 股。&lt;/li&gt;
&lt;li&gt;CityInProvince 中文 md&amp;amp;a 语料在此项表现如此优异，说明 A 股多数企业扎根于中国大地， 年报 md&amp;amp;a 中提及次数很多。&lt;/li&gt;
&lt;li&gt;FamilyRelationship 中文 md&amp;amp;a 语料中主要体现的是公司组织层面，较少提及家庭关系词语，所以类别表现一般是很容易理解的。&lt;/li&gt;
&lt;li&gt;SocialScience 中文 md&amp;amp;a 语料在此项表现一般， 应该是语料中常见的社会科学词语提及较少。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整体而言，语料训练的效果很不错，抓住了数据场景的独特性语义。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;33-keyedvectors-的操作方法或属性&#34;&gt;3.3 KeyedVectors 的操作方法(或属性)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方法&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.index_to_key&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取词汇表中的所有单词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.key_to_index&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取单词到索引的映射。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.vector_size&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取 GloVe 模型中任意词向量的维度。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.get_vector(word)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取给定单词的词向量。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.similar_by_word(word, topn=10)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取某词语最相似的 10 个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.similar_by_vector(vector, topn=10)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取词向量最相似的 10 个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;34-查看词汇量维度数&#34;&gt;3.4 查看词汇量&amp;amp;维度数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 词汇量&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Word2Vec词汇量: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;GloVe词汇量: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;glove_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Word2Vec维度数: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;GloVe维度数: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;glove_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Word2Vec词汇量:  779451
GloVe词汇量:     452722
Word2Vec维度数:  200
GloVe维度数:     200
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;35-词表&#34;&gt;3.5 词表&lt;/h3&gt;
&lt;p&gt;查看词表&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;公司&amp;#39;,
 &amp;#39;适用&amp;#39;,
 &amp;#39;情况&amp;#39;,
 &amp;#39;项目&amp;#39;,
 ...
 &amp;#39;电源&amp;#39;,
 &amp;#39;模块&amp;#39;,
 &amp;#39;治疗&amp;#39;,
 &amp;#39;实行&amp;#39;,
 ...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;查看词汇映射表&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;公司&amp;#39;: 0,
 &amp;#39;适用&amp;#39;: 1,
 &amp;#39;情况&amp;#39;: 2,
 &amp;#39;项目&amp;#39;: 3,
 &amp;#39;产品&amp;#39;: 4,
 ......
&amp;#39;电源&amp;#39;: 996,
 &amp;#39;模块&amp;#39;: 997,
 &amp;#39;治疗&amp;#39;: 998,
 &amp;#39;实行&amp;#39;: 999,
 ...}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;36-查看词向量&#34;&gt;3.6 查看词向量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查询某词的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([ 2.1754024 , -1.1083757 ,  0.30669013, -2.3222647 ,  2.037556  ,
       -0.4029445 ,  3.6833916 , -1.520377  ,  2.046346  , -1.2697963 ,
       -4.6910505 ,  0.77117187,  1.1461644 ,  0.44298795,  0.6784688 ,
        3.3559523 ,  0.24663335, -1.2482047 , -0.9346108 , -3.0777013 ,
        ......
       -3.6354382 ,  0.05906389,  0.34168765, -0.7054434 ,  1.1509504 ,
        1.8190739 , -1.1612972 , -1.4397353 ,  1.2453864 ,  2.280641  ,
        0.16765192,  0.07346256,  3.5366366 , -3.6461854 , -0.9496986 ,
        2.38728   ,  0.20706034,  1.9512706 ,  0.138616  , -1.5360951 ],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查询多个词的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_mean_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;研发&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Ruj&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([ 0.09148444, -0.06439913, -0.01015558, -0.05306313,  0.06175516,
       -0.06248198,  0.0741367 , -0.1192503 ,  0.01363031, -0.04997339,
       -0.14710814,  0.02335552,  0.02538575,  0.04013668,  0.01318196,
        0.02532444,  0.04894971, -0.02153242, -0.08227678, -0.07488775,
       ......
       -0.12517202, -0.01881655,  0.00918441, -0.0136063 , -0.00371204,
        0.06221166, -0.03297246, -0.03030303,  0.0700142 ,  0.0314462 ,
       -0.00345534,  0.01589244,  0.08589543, -0.04257936,  0.00832741,
        0.04352532,  0.0469989 ,  0.02008099,  0.04311348,  0.00275607],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;37-近义词&#34;&gt;3.7 近义词&lt;/h3&gt;
&lt;p&gt;根据词语查找最相似的 10 个词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;技术创新&amp;#39;, 0.700412929058075),
 (&amp;#39;不断创新&amp;#39;, 0.6930481791496277),
 (&amp;#39;创新型&amp;#39;, 0.6269345283508301),
 (&amp;#39;创新能力&amp;#39;, 0.5974201560020447),
 (&amp;#39;引领&amp;#39;, 0.5780265927314758),
 (&amp;#39;革新&amp;#39;, 0.5736942291259766),
 (&amp;#39;科技进步&amp;#39;, 0.5656147599220276),
 (&amp;#39;硬核&amp;#39;, 0.558936357498169),
 (&amp;#39;创新性&amp;#39;, 0.5329084992408752),
 (&amp;#39;前沿&amp;#39;, 0.5278463959693909)]

​
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;根据某词的词向量查询最相似的 10 个词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;creativeness_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;creativeness_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;创新&amp;#39;, 1.0),
 (&amp;#39;技术创新&amp;#39;, 0.700412929058075),
 (&amp;#39;不断创新&amp;#39;, 0.6930481195449829),
 (&amp;#39;创新型&amp;#39;, 0.6269344687461853),
 (&amp;#39;创新能力&amp;#39;, 0.5974200963973999),
 (&amp;#39;引领&amp;#39;, 0.5780266523361206),
 (&amp;#39;革新&amp;#39;, 0.5736941695213318),
 (&amp;#39;科技进步&amp;#39;, 0.5656147599220276),
 (&amp;#39;硬核&amp;#39;, 0.558936357498169),
 (&amp;#39;创新性&amp;#39;, 0.5329084992408752)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;多个词求得均值向量&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;AI_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_mean_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ai&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;机器学习&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;人工智能&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;自然语言处理&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;AI_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;ai&amp;#39;, 0.9102671146392822),
 (&amp;#39;机器学习&amp;#39;, 0.8870545625686646),
 (&amp;#39;自然语言处理&amp;#39;, 0.8581846356391907),
 (&amp;#39;人工智能&amp;#39;, 0.850341260433197),
 (&amp;#39;ai模型&amp;#39;, 0.8282461762428284),
 (&amp;#39;语言模型&amp;#39;, 0.8115222454071045),
 (&amp;#39;深度学习&amp;#39;, 0.8071558475494385),
 (&amp;#39;nlp&amp;#39;, 0.798158586025238),
 (&amp;#39;自然语言理解&amp;#39;, 0.7791630625724792),
 (&amp;#39;gpt&amp;#39;, 0.7678513526916504),
 (&amp;#39;生成式&amp;#39;, 0.7635747194290161),
 (&amp;#39;知识图谱&amp;#39;, 0.7630875706672668),
 (&amp;#39;语义&amp;#39;, 0.7626250982284546),
 (&amp;#39;模态模型&amp;#39;, 0.7623038291931152),
 (&amp;#39;自然语言&amp;#39;, 0.7621716856956482),
 (&amp;#39;神经网络&amp;#39;, 0.7459751963615417),
 (&amp;#39;训练模型&amp;#39;, 0.7420169711112976),
 (&amp;#39;ai算法&amp;#39;, 0.7381570339202881),
 (&amp;#39;语音识别&amp;#39;, 0.7319735884666443),
 (&amp;#39;推理&amp;#39;, 0.7291040420532227)]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;短视主义词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;short_term_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_mean_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;尽快&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;年内&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;马上&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;short_term_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;年内&amp;#39;, 0.7409026026725769),
 (&amp;#39;尽快&amp;#39;, 0.7231807112693787),
 (&amp;#39;尽早&amp;#39;, 0.6633163094520569),
 (&amp;#39;马上&amp;#39;, 0.654857337474823),
 (&amp;#39;早日&amp;#39;, 0.6193257570266724),
 (&amp;#39;即将&amp;#39;, 0.5834046602249146),
 (&amp;#39;争取早日&amp;#39;, 0.5398548245429993),
 (&amp;#39;按期&amp;#39;, 0.5317867994308472),
 (&amp;#39;抓紧&amp;#39;, 0.5302024483680725),
 (&amp;#39;力争尽早&amp;#39;, 0.5286926627159119),
 (&amp;#39;争取&amp;#39;, 0.5223618745803833),
 (&amp;#39;今年年底&amp;#39;, 0.5185986161231995),
 (&amp;#39;最后&amp;#39;, 0.5065357685089111),
 (&amp;#39;后续&amp;#39;, 0.5003851056098938),
 (&amp;#39;力争早日&amp;#39;, 0.49779534339904785),
 (&amp;#39;争取尽早&amp;#39;, 0.49219441413879395),
 (&amp;#39;争取尽快&amp;#39;, 0.48603734374046326),
 (&amp;#39;力争&amp;#39;, 0.4822418689727783),
 (&amp;#39;如期&amp;#39;, 0.48014208674430847),
 (&amp;#39;冲刺&amp;#39;, 0.46000415086746216)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四扩展词典&#34;&gt;四、扩展词典&lt;/h2&gt;
&lt;p&gt;做词典法的文本分析，最重要的是有自己的领域词典。之前受限于技术难度，文科生的我也一直在用形容词的通用情感词典。现在依托 word2vec 技术， 可以加速人工构建的准确率和效率。&lt;/p&gt;
&lt;p&gt;下面是在 &lt;strong&gt;&lt;em&gt;mda01-23-Word2Vec.200.15.bin&lt;/em&gt;&lt;/strong&gt; 上做的词典扩展测试，函数 &lt;strong&gt;&lt;em&gt;ct.expand_dictionary(wv, seeddict, topn=100)&lt;/em&gt;&lt;/strong&gt; 会根据种子词选取最准确的 topn 个词。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;wv&lt;/em&gt;&lt;/strong&gt; 预训练模型，数据类型为 gensim.models.keyedvectors.KeyedVectors。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;seeddict&lt;/em&gt;&lt;/strong&gt; 参数类似于种子词；格式为 PYTHON 字典；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;topn&lt;/em&gt;&lt;/strong&gt; 返回 topn 个语义最接近 seeddict 的词，默认 100.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;假设现在有种子词 seeddicts， 内含我构建的 &lt;strong&gt;&lt;em&gt;短视词&lt;/em&gt;&lt;/strong&gt;、 &lt;strong&gt;&lt;em&gt;创新词&lt;/em&gt;&lt;/strong&gt;、 &lt;strong&gt;&lt;em&gt;竞争词&lt;/em&gt;&lt;/strong&gt;， 我希望生成最终各含 30 个词的候选词表 txt 文件。&lt;/p&gt;
&lt;p&gt;可以使用 &lt;strong&gt;&lt;em&gt;ct.expand_dictionary&lt;/em&gt;&lt;/strong&gt; 进行如下操作&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;seeddicts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;短视词&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;抓紧&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;立刻&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;月底&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;年底&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;年终&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;争取&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;力争&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;创新词&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;科技&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;研发&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;s1&#34;&gt;&amp;#39;技术&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;标准&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;竞争词&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;竞争&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;竞争力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                     &lt;span class=&#34;n&#34;&gt;seeddict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seeddicts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                     &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Finish! 短视词 candidates saved to output/短视词.txt
Finish! 创新词 candidates saved to output/创新词.txt
Finish! 竞争词 candidates saved to output/竞争词.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-expand.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;六获取模型&#34;&gt;六、获取模型&lt;/h2&gt;
&lt;p&gt;内容创作不易， 本文为付费内容，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 免费     mda01-23-Word2Vec.200.15.bin   链接: https://pan.baidu.com/s/13r8ZiwmzaiIx691vzqNKDA?pwd=vxuy 提取码: vxuy

- 免费     mda01-23-GloVe.200.15.bin 链接: https://pan.baidu.com/s/1Qi3oyE5S9OOon2GxpqP1Ew?pwd=dt3s 提取码: dt3s

- 更多免费词向量      https://cntext.readthedocs.io/zh-cn/latest/embeddings.html

- 100元    cntext-6-py3-none-any.whl  加微信 372335839， 备注「姓名-学校-专业」
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;p&gt;相关文献&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216.
[1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.
[3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/the_text_analysis_list_about_ms/&#34;&gt;LIST | 社科(经管)文本挖掘文献汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/text_analysis_code_list_about_ms/&#34;&gt;LIST | 文本分析代码汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见 39 个 FAQ 汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/datasets_available_for_management_science/&#34;&gt;LIST | 可供社科(经管)领域使用的数据集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;Python 实证指标构建与文本分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/&#34;&gt;推荐 | 文本分析库 cntext2.x 使用手册&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/&#34;&gt;使用 3751w 专利申请数据集按年份(按省份)训练词向量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/&#34;&gt;预训练模型 | 使用 1000w 专利摘要训练 word2vec 模型，可用于开发词典&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/&#34;&gt;词向量 | 使用人民网领导留言板语料训练 Word2Vec 模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/&#34;&gt;实验 | 使用 Stanford Glove 代码训练中文语料的 Glove 模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/&#34;&gt;可视化 | 人民日报语料反映七十年文化演变&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一数据集">一、数据集</h2>
<h3 id="11-数据概况">1.1 数据概况</h3>
<p><a href="https://textdata.cn/blog/2023-03-23-china-a-share-market-dataset-mda-from-01-to-21/"><strong>数据集 | 2001-2023 年 A 股上市公司年报&amp;管理层讨论与分析</strong></a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据名称: 2001-2023年A股上市公司年报&amp;管理层讨论与分析
数据来源: 上海证券交易所、深圳证券交易所
数据格式: csv、txt
公司数量: 5606
MD&amp;A数量: 60079
会计年度: 2001-2023
</code></pre></div><h3 id="12-读取-mda-数据">1.2 读取 md&amp;a 数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># 读取前5行数据</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;mda01-23.csv.gz&#39;</span><span class="p">,</span> <span class="n">compression</span><span class="o">=</span><span class="s1">&#39;gzip&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># gz解压后读取csv</span>
<span class="c1"># df = pd.read_csv(&#39;mda01-23.csv&#39;, nrows=5)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">60079
</code></pre></div><p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<br>
<h2 id="二训练-word2vec--glove-模型">二、训练 Word2Vec &amp; GloVe 模型</h2>
<h3 id="21-准备语料">2.1 准备语料</h3>
<p>从 <strong>mda01-23.csv.gz</strong> 数据中抽取出所有文本，写入到 <strong>mda01-23.txt</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;mda01-23.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</code></pre></div><p>最终得到 2.88G 的语料文件。</p>
<br>
<h2 id="22-配置-cntext-环境">2.2 配置 cntext 环境</h2>
<p>使用 2.1.6 版本 cntext 库(该版本暂不开源，需付费购买)。 将得到的 <strong>cntext-2.1.6-py3-none-any.whl</strong> 文件放置于电脑桌面， win 系统打开<strong>cmd</strong>(Mac 打开 terminal)， 输入如下命令(将工作环境切换至桌面)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>个别 Win 用户如无效，试试<code>cd Desktop</code> 。</p>
<p>继续在 cmd (terminal) 中执行如下命令安装 cntext2.1.6</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext-2.1.6-py3-none-any.whl
</code></pre></div><br>
<h3 id="23-开始训练">2.3 开始训练</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;mda01-23.txt&#39;</span><span class="p">,</span> <span class="c1"># 语料文件</span>
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span>             <span class="c1"># 中文语料</span>
                        <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>            <span class="c1"># 嵌入的维度数</span>
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>             <span class="c1"># 词语上下文的窗口大小</span>



<span class="n">glove_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">GloVe</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;mda01-23.txt&#39;</span><span class="p">,</span>
                       <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span>
                       <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/mda01-23_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|█████████| 12437725/12437725 [03:54&lt;00:00, 52930.57it/s]
Reading Preprocessed Corpus from output/mda01-23_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 1370 s.
Output Saved To: output/mda01-23-Word2Vec.200.15.bin


Mac(Linux) System, Enable Parallel Processing
Cache output/mda01-23_cache.txt Found, Skip Preprocessing Corpus
Start Training GloVe
BUILDING VOCABULARY
Using vocabulary of size 452721.
......
04/03/25 - 04:07.06PM, iter: 001, cost: 0.112966
04/03/25 - 04:07.56PM, iter: 002, cost: 0.079845
......
04/03/25 - 04:18.06PM, iter: 014, cost: 0.048427
04/03/25 - 04:18.56PM, iter: 015, cost: 0.047962

GloVe Training Cost 1229 s.
Output Saved To: output/mda01-23-GloVe.200.15.bin
CPU times: user 1h 19min 15s, sys: 58.5 s, total: 1h 20min 14s
Wall time: 43min 20s
</code></pre></div><p>经过 80 分钟， 训练出的中国 A 股管理层讨论与分析的 GloVe 和 Word2Vec 词向量模型(如下截图)，词汇量 914058， 模型文件 1.49G。模型可广泛用于经济管理等领域概念(情感)词典的构建或扩展。</p>
<ul>
<li><strong>mda01-23_cache.txt</strong> 缓存文件</li>
<li><strong>mda01-23-Word2Vec.200.15.bin</strong> Word2Vec 模型文件</li>
<li><strong>mda01-23-GloVe.200.15.bin</strong> GloVe 模型文件</li>
</ul>
<p><img loading="lazy" src="img/pretained-screen.png" alt=""  />
</p>
<br>
<br>
<h2 id="三使用模型">三、使用模型</h2>
<h3 id="31-导入模型">3.1 导入模型</h3>
<p>使用 <strong><em>ct.load_w2v(w2v_path)</em></strong> 来导入刚刚训练好的模型 <strong><em>mda01-23-GloVe.200.15.bin</em></strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="n">w2v_model</span>   <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/mda01-23-Word2Vec.200.15.bin&#39;</span><span class="p">)</span>
<span class="n">glove_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/mda01-23-GloVe.200.15.bin&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2.1.6
Loading output/mda01-23-Word2Vec.200.15.bin...
Loading output/mda01-23-GloVe.200.15.bin...
&lt;gensim.models.keyedvectors.KeyedVectors at 0x633060fe0&gt;
</code></pre></div><br>
<h3 id="32-评估模型">3.2 评估模型</h3>
<p>使用近义法和类比法， 判断模型的表现。详情可查看<a href="https://cntext.readthedocs.io/zh-cn/latest/model.html">文档</a></p>
<p>以 word2vec 为例，评估模型表现</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">evaluate_similarity</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">)</span>

<span class="n">ct</span><span class="o">.</span><span class="n">evaluate_analogy</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   421    |    116     |            0.41            |
+----------+------------+----------------------------+


类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|██████████████| 1198/1198 [00:11&lt;00:00, 99.91it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   407    |    270     |   27.27    |   3.96   |
|   CityInProvince   |   175    |     0      |   97.14    |   1.29   |
| FamilyRelationship |    90    |    182     |   10.00    |   3.89   |
|   SocialScience    |    9     |     61     |   44.44    |   3.00   |
+--------------------+----------+------------+------------+----------+
</code></pre></div><p><strong>近义测试</strong>: Spearman&rsquo;s Rank Coeficient 系数取值[-1, 1], 取值越大， 说明模型表现越好。</p>
<br>
<p><strong>类比测试</strong>:</p>
<ul>
<li>CapitalOfCountries 中文 md&amp;a 语料在此项表现较差， 应该是语料中常见国家首度的提及较少。也体现了大多数企业没有国际化。盲猜美股的 CapitalOfCountries 表现应该好于 A 股。</li>
<li>CityInProvince 中文 md&amp;a 语料在此项表现如此优异，说明 A 股多数企业扎根于中国大地， 年报 md&amp;a 中提及次数很多。</li>
<li>FamilyRelationship 中文 md&amp;a 语料中主要体现的是公司组织层面，较少提及家庭关系词语，所以类别表现一般是很容易理解的。</li>
<li>SocialScience 中文 md&amp;a 语料在此项表现一般， 应该是语料中常见的社会科学词语提及较少。</li>
</ul>
<p>整体而言，语料训练的效果很不错，抓住了数据场景的独特性语义。</p>
<br>
<h3 id="33-keyedvectors-的操作方法或属性">3.3 KeyedVectors 的操作方法(或属性)</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><em>KeyedVectors.index_to_key</em></strong></td>
<td>获取词汇表中的所有单词。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.key_to_index</em></strong></td>
<td>获取单词到索引的映射。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.vector_size</em></strong></td>
<td>获取 GloVe 模型中任意词向量的维度。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.get_vector(word)</em></strong></td>
<td>获取给定单词的词向量。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.similar_by_word(word, topn=10)</em></strong></td>
<td>获取某词语最相似的 10 个近义词。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.similar_by_vector(vector, topn=10)</em></strong></td>
<td>获取词向量最相似的 10 个近义词。</td>
</tr>
</tbody>
</table>
<h3 id="34-查看词汇量维度数">3.4 查看词汇量&amp;维度数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 词汇量</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Word2Vec词汇量: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;GloVe词汇量: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">glove_model</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Word2Vec维度数: &#39;</span><span class="p">,</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">vector_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;GloVe维度数: &#39;</span><span class="p">,</span> <span class="n">glove_model</span><span class="o">.</span><span class="n">vector_size</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Word2Vec词汇量:  779451
GloVe词汇量:     452722
Word2Vec维度数:  200
GloVe维度数:     200
</code></pre></div><br>
<h3 id="35-词表">3.5 词表</h3>
<p>查看词表</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">index_to_key</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;公司&#39;,
 &#39;适用&#39;,
 &#39;情况&#39;,
 &#39;项目&#39;,
 ...
 &#39;电源&#39;,
 &#39;模块&#39;,
 &#39;治疗&#39;,
 &#39;实行&#39;,
 ...]
</code></pre></div><br>
<p>查看词汇映射表</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">key_to_index</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;公司&#39;: 0,
 &#39;适用&#39;: 1,
 &#39;情况&#39;: 2,
 &#39;项目&#39;: 3,
 &#39;产品&#39;: 4,
 ......
&#39;电源&#39;: 996,
 &#39;模块&#39;: 997,
 &#39;治疗&#39;: 998,
 &#39;实行&#39;: 999,
 ...}
</code></pre></div><br>
<h3 id="36-查看词向量">3.6 查看词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查询某词的词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;创新&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 2.1754024 , -1.1083757 ,  0.30669013, -2.3222647 ,  2.037556  ,
       -0.4029445 ,  3.6833916 , -1.520377  ,  2.046346  , -1.2697963 ,
       -4.6910505 ,  0.77117187,  1.1461644 ,  0.44298795,  0.6784688 ,
        3.3559523 ,  0.24663335, -1.2482047 , -0.9346108 , -3.0777013 ,
        ......
       -3.6354382 ,  0.05906389,  0.34168765, -0.7054434 ,  1.1509504 ,
        1.8190739 , -1.1612972 , -1.4397353 ,  1.2453864 ,  2.280641  ,
        0.16765192,  0.07346256,  3.5366366 , -3.6461854 , -0.9496986 ,
        2.38728   ,  0.20706034,  1.9512706 ,  0.138616  , -1.5360951 ],
      dtype=float32)
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查询多个词的词向量</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">get_mean_vector</span><span class="p">([</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;研发&#39;</span><span class="p">])</span>
</code></pre></div><p>Ruj</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 0.09148444, -0.06439913, -0.01015558, -0.05306313,  0.06175516,
       -0.06248198,  0.0741367 , -0.1192503 ,  0.01363031, -0.04997339,
       -0.14710814,  0.02335552,  0.02538575,  0.04013668,  0.01318196,
        0.02532444,  0.04894971, -0.02153242, -0.08227678, -0.07488775,
       ......
       -0.12517202, -0.01881655,  0.00918441, -0.0136063 , -0.00371204,
        0.06221166, -0.03297246, -0.03030303,  0.0700142 ,  0.0314462 ,
       -0.00345534,  0.01589244,  0.08589543, -0.04257936,  0.00832741,
        0.04352532,  0.0469989 ,  0.02008099,  0.04311348,  0.00275607],
      dtype=float32)
</code></pre></div><br>
<h3 id="37-近义词">3.7 近义词</h3>
<p>根据词语查找最相似的 10 个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v_model</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;技术创新&#39;, 0.700412929058075),
 (&#39;不断创新&#39;, 0.6930481791496277),
 (&#39;创新型&#39;, 0.6269345283508301),
 (&#39;创新能力&#39;, 0.5974201560020447),
 (&#39;引领&#39;, 0.5780265927314758),
 (&#39;革新&#39;, 0.5736942291259766),
 (&#39;科技进步&#39;, 0.5656147599220276),
 (&#39;硬核&#39;, 0.558936357498169),
 (&#39;创新性&#39;, 0.5329084992408752),
 (&#39;前沿&#39;, 0.5278463959693909)]

​
</code></pre></div><br>
<p>根据某词的词向量查询最相似的 10 个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">creativeness_vector</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;创新&#39;</span><span class="p">)</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">creativeness_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;创新&#39;, 1.0),
 (&#39;技术创新&#39;, 0.700412929058075),
 (&#39;不断创新&#39;, 0.6930481195449829),
 (&#39;创新型&#39;, 0.6269344687461853),
 (&#39;创新能力&#39;, 0.5974200963973999),
 (&#39;引领&#39;, 0.5780266523361206),
 (&#39;革新&#39;, 0.5736941695213318),
 (&#39;科技进步&#39;, 0.5656147599220276),
 (&#39;硬核&#39;, 0.558936357498169),
 (&#39;创新性&#39;, 0.5329084992408752)]
</code></pre></div><br>
<p>多个词求得均值向量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">AI_vector</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_mean_vector</span><span class="p">([</span><span class="s1">&#39;ai&#39;</span><span class="p">,</span>  <span class="s1">&#39;机器学习&#39;</span><span class="p">,</span> <span class="s1">&#39;人工智能&#39;</span><span class="p">,</span> <span class="s1">&#39;自然语言处理&#39;</span><span class="p">])</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">AI_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;ai&#39;, 0.9102671146392822),
 (&#39;机器学习&#39;, 0.8870545625686646),
 (&#39;自然语言处理&#39;, 0.8581846356391907),
 (&#39;人工智能&#39;, 0.850341260433197),
 (&#39;ai模型&#39;, 0.8282461762428284),
 (&#39;语言模型&#39;, 0.8115222454071045),
 (&#39;深度学习&#39;, 0.8071558475494385),
 (&#39;nlp&#39;, 0.798158586025238),
 (&#39;自然语言理解&#39;, 0.7791630625724792),
 (&#39;gpt&#39;, 0.7678513526916504),
 (&#39;生成式&#39;, 0.7635747194290161),
 (&#39;知识图谱&#39;, 0.7630875706672668),
 (&#39;语义&#39;, 0.7626250982284546),
 (&#39;模态模型&#39;, 0.7623038291931152),
 (&#39;自然语言&#39;, 0.7621716856956482),
 (&#39;神经网络&#39;, 0.7459751963615417),
 (&#39;训练模型&#39;, 0.7420169711112976),
 (&#39;ai算法&#39;, 0.7381570339202881),
 (&#39;语音识别&#39;, 0.7319735884666443),
 (&#39;推理&#39;, 0.7291040420532227)]

</code></pre></div><br>
<p>短视主义词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">short_term_vector</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">get_mean_vector</span><span class="p">([</span><span class="s1">&#39;尽快&#39;</span><span class="p">,</span>  <span class="s1">&#39;年内&#39;</span><span class="p">,</span> <span class="s1">&#39;马上&#39;</span><span class="p">])</span>
<span class="n">w2v_model</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">short_term_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;年内&#39;, 0.7409026026725769),
 (&#39;尽快&#39;, 0.7231807112693787),
 (&#39;尽早&#39;, 0.6633163094520569),
 (&#39;马上&#39;, 0.654857337474823),
 (&#39;早日&#39;, 0.6193257570266724),
 (&#39;即将&#39;, 0.5834046602249146),
 (&#39;争取早日&#39;, 0.5398548245429993),
 (&#39;按期&#39;, 0.5317867994308472),
 (&#39;抓紧&#39;, 0.5302024483680725),
 (&#39;力争尽早&#39;, 0.5286926627159119),
 (&#39;争取&#39;, 0.5223618745803833),
 (&#39;今年年底&#39;, 0.5185986161231995),
 (&#39;最后&#39;, 0.5065357685089111),
 (&#39;后续&#39;, 0.5003851056098938),
 (&#39;力争早日&#39;, 0.49779534339904785),
 (&#39;争取尽早&#39;, 0.49219441413879395),
 (&#39;争取尽快&#39;, 0.48603734374046326),
 (&#39;力争&#39;, 0.4822418689727783),
 (&#39;如期&#39;, 0.48014208674430847),
 (&#39;冲刺&#39;, 0.46000415086746216)]
</code></pre></div><p><br><br></p>
<h2 id="四扩展词典">四、扩展词典</h2>
<p>做词典法的文本分析，最重要的是有自己的领域词典。之前受限于技术难度，文科生的我也一直在用形容词的通用情感词典。现在依托 word2vec 技术， 可以加速人工构建的准确率和效率。</p>
<p>下面是在 <strong><em>mda01-23-Word2Vec.200.15.bin</em></strong> 上做的词典扩展测试，函数 <strong><em>ct.expand_dictionary(wv, seeddict, topn=100)</em></strong> 会根据种子词选取最准确的 topn 个词。</p>
<ul>
<li><strong><em>wv</em></strong> 预训练模型，数据类型为 gensim.models.keyedvectors.KeyedVectors。</li>
<li><strong><em>seeddict</em></strong> 参数类似于种子词；格式为 PYTHON 字典；</li>
<li><strong><em>topn</em></strong> 返回 topn 个语义最接近 seeddict 的词，默认 100.</li>
</ul>
<br>
<p>假设现在有种子词 seeddicts， 内含我构建的 <strong><em>短视词</em></strong>、 <strong><em>创新词</em></strong>、 <strong><em>竞争词</em></strong>， 我希望生成最终各含 30 个词的候选词表 txt 文件。</p>
<p>可以使用 <strong><em>ct.expand_dictionary</em></strong> 进行如下操作</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">seeddicts</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;短视词&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;抓紧&#39;</span><span class="p">,</span> <span class="s1">&#39;立刻&#39;</span><span class="p">,</span> <span class="s1">&#39;月底&#39;</span><span class="p">,</span> <span class="s1">&#39;年底&#39;</span><span class="p">,</span> <span class="s1">&#39;年终&#39;</span><span class="p">,</span> <span class="s1">&#39;争取&#39;</span><span class="p">,</span> <span class="s1">&#39;力争&#39;</span><span class="p">],</span>
    <span class="s1">&#39;创新词&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;科技&#39;</span><span class="p">,</span>  <span class="s1">&#39;研发&#39;</span><span class="p">,</span>  <span class="s1">&#39;技术&#39;</span><span class="p">,</span> <span class="s1">&#39;标准&#39;</span><span class="p">],</span>
    <span class="s1">&#39;竞争词&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;竞争&#39;</span><span class="p">,</span> <span class="s1">&#39;竞争力&#39;</span><span class="p">],</span>
    <span class="p">}</span>

<span class="n">ct</span><span class="o">.</span><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="p">,</span>
                     <span class="n">seeddict</span> <span class="o">=</span> <span class="n">seeddicts</span><span class="p">,</span>
                     <span class="n">topn</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Finish! 短视词 candidates saved to output/短视词.txt
Finish! 创新词 candidates saved to output/创新词.txt
Finish! 竞争词 candidates saved to output/竞争词.txt
</code></pre></div><p><img loading="lazy" src="img/03-expand.jpg" alt=""  />
</p>
<p><br><br></p>
<h2 id="六获取模型">六、获取模型</h2>
<p>内容创作不易， 本文为付费内容，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 免费     mda01-23-Word2Vec.200.15.bin   链接: https://pan.baidu.com/s/13r8ZiwmzaiIx691vzqNKDA?pwd=vxuy 提取码: vxuy

- 免费     mda01-23-GloVe.200.15.bin 链接: https://pan.baidu.com/s/1Qi3oyE5S9OOon2GxpqP1Ew?pwd=dt3s 提取码: dt3s

- 更多免费词向量      https://cntext.readthedocs.io/zh-cn/latest/embeddings.html

- 100元    cntext-6-py3-none-any.whl  加微信 372335839， 备注「姓名-学校-专业」
</code></pre></div><p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<p>相关文献</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[0]刘景江,郑畅然,洪永淼.机器学习如何赋能管理学研究？——国内外前沿综述和未来展望[J].管理世界,2023,39(09):191-216.
[1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.
[3]胡楠,薛付婧,王昊楠.管理者短视主义影响企业长期投资吗？——基于文本分析和机器学习[J].管理世界,2021,37(05):139-156+11+19-21.
[4]Kai Li, Feng Mai, Rui Shen, Xinyan Yan, Measuring Corporate Culture Using Machine Learning, *The Review of Financial Studies*,2020
</code></pre></div><br>
<ul>
<li><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)文本挖掘文献汇总</a></li>
<li><a href="https://textdata.cn/blog/text_analysis_code_list_about_ms/">LIST | 文本分析代码汇总</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见 39 个 FAQ 汇总</a></li>
<li><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">Python 实证指标构建与文本分析</a></li>
<li><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">推荐 | 文本分析库 cntext2.x 使用手册</a></li>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">使用 3751w 专利申请数据集按年份(按省份)训练词向量</a></li>
<li><a href="https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/">预训练模型 | 使用 1000w 专利摘要训练 word2vec 模型，可用于开发词典</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量 | 使用人民网领导留言板语料训练 Word2Vec 模型</a></li>
<li><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用 Stanford Glove 代码训练中文语料的 Glove 模型</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>词向量 | 使用1985年-2025年专利申请摘要训练 Word2Vec 模型</title>
      <link>https://textdata.cn/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-10-training-word2vec-model-using-china-3751w-patent-application-dataset/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/&#34;&gt;&lt;strong&gt;5112万条专利申请数据集(1985-2025年)&lt;/strong&gt;&lt;/a&gt; 中随机抽取了30%的 「&lt;strong&gt;专利摘要&lt;/strong&gt;」，构成6.14G的训练语料(千万级别)， 耗时6小时，训练得到word2vec模型。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;需要注意， 100%全部语料有30+G， 训练时间非常长。&lt;/p&gt;
&lt;p&gt;没办法，我不会优化代码性能，所以只能抽取 30% 的文本数据来训练word2vec ，语料体积大概10G。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;p&gt;本文需要用到新cntext，因为bug较多， 直接上传到PyPi，将导致之前制作的课程和公众号推文相关内容全部重新一遍。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一语料构建&#34;&gt;一、语料构建&lt;/h2&gt;
&lt;p&gt;随机抽取20%的记录，构成千万专利文本摘要训练语料。&lt;/p&gt;
&lt;p&gt;为了防止电脑内存爆炸， 对任意单个大csv文件，分批次读取，每次读10w行。最终将专利摘要文本保存到txt文件中，编码方式为utf-8。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果想开发一些词典，可以跳过此部分内容，并不影响代码运行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/screen-datasets.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 将代码放在csv数据文件夹内&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;c1&#34;&gt;# 获得当前文件夹内所有的csv文件路径&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;中国专利数据库.csv.gz&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                            &lt;span class=&#34;n&#34;&gt;usecols&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利名称&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;摘要文本&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; 
                            &lt;span class=&#34;n&#34;&gt;chunksize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_dfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
        &lt;span class=&#34;c1&#34;&gt;# 剔除专利摘要为空的记录&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;sample_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;chunk_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;frac&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;raw_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;摘要文本&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;))&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;最终得到的 &lt;strong&gt;专利摘要.txt&lt;/strong&gt;  文件有 10G&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二训练word2vec&#34;&gt;二、训练word2vec&lt;/h2&gt;
&lt;h3 id=&#34;21-安装&#34;&gt;2.1 安装&lt;/h3&gt;
&lt;p&gt;将 &lt;em&gt;&lt;strong&gt;cntext-2.1.6-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 放置于桌面，打开 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal) 中使用 &lt;em&gt;&lt;strong&gt;pip3&lt;/strong&gt;&lt;/em&gt; 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install cntext-2.1.6-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文末有 &lt;em&gt;&lt;strong&gt;cntext-2.1.6-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 获取方式&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-训练-word2vec&#34;&gt;2.2 训练 Word2Vec&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# cntext为2.1.6&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Word2Vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;专利摘要.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 词向量维度&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# 窗口大小&lt;/span&gt;
                        &lt;span class=&#34;n&#34;&gt;chunksize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 每次读取10000行&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Mac(Linux) System, Enable Parallel Processing
Cache output/专利摘要_cache.txt Not Found or Empty, Preprocessing Corpus

Reading Preprocessed Corpus from output/专利摘要_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 8816 s. 
Output Saved To: output/专利摘要-Word2Vec.200.15.bin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;整理训练过程 2.5 小时， 训练结束后得到 &lt;em&gt;&lt;strong&gt;output&lt;/strong&gt;&lt;/em&gt; 文件夹， 里面有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;output/专利摘要-Word2Vec.200.15.bin&lt;/strong&gt;&lt;/em&gt;  模型文件&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;专利摘要_cache.txt&lt;/strong&gt;&lt;/em&gt;                   训练缓存文件&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-评估模型&#34;&gt;2.3 评估模型&lt;/h3&gt;
&lt;p&gt;使用近义法和类比法， 判断模型的表现。详情可查看&lt;a href=&#34;https://cntext.readthedocs.io/zh-cn/latest/model.html&#34;&gt;文档&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evaluate_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evaluate_analogy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&amp;#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   427    |    110     |            0.46            |
+----------+------------+----------------------------+


类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|██████████████| 1198/1198 [00:11&amp;lt;00:00, 99.91it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   238    |    439     |    3.78    |   5.67   |
|   CityInProvince   |   175    |     0      |   25.14    |   4.48   |
| FamilyRelationship |   156    |    116     |   33.33    |   2.29   |
|   SocialScience    |    8     |     62     |   37.50    |   2.33   |
+--------------------+----------+------------+------------+----------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;近义测试&lt;/strong&gt;: Spearman&amp;rsquo;s Rank Coeficient系数取值[-1, 1], 取值越大， 说明模型表现越好。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;类比测试&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CapitalOfCountries   专利语料在此项表现很差， 应该是语料中常见国家首度的提及较少。&lt;/li&gt;
&lt;li&gt;CityInProvince       专利语料在此项好于CapitalOfCountries， 毕竟在中国大地进行科创。&lt;/li&gt;
&lt;li&gt;FamilyRelationship   专利语料中没想到在此项准确率中显著大于0， 我原本以为准确率为0，毕竟专利摘要中出现家人管理不太技术。没想到没想到啊。 发明可能类似于电影非诚勿扰里解决人类问题的例子，发明很雷人。&lt;/li&gt;
&lt;li&gt;SocialScience        专利语料在此项表现一般， 应该是语料中常见的社会科学词语提及较少。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整体而言，在四个维度准确率较低。 但是需要说明， 这四个维度是大邓自己收集的，评判模型类比表现维度有很多， 有可能专利摘要在别的类比维度上表现会很好。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三使用词向量&#34;&gt;三、使用词向量&lt;/h2&gt;
&lt;h3 id=&#34;31-录入模型&#34;&gt;3.1 录入模型&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/专利摘要-Word2Vec.200.15.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Loading 专利摘要-Word2Vec.200.15.bin...
&amp;lt;gensim.models.keyedvectors.KeyedVectors at 0x32b079340&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-keyedvectors的操作方法或属性&#34;&gt;3.2 KeyedVectors的操作方法(或属性)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方法&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.index_to_key&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取词汇表中的所有单词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.key_to_index&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取单词到索引的映射。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.vector_size&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取GloVe模型中任意词向量的维度。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.get_vector(word)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取给定单词的词向量。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.most_similar(words, topn=10)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取某类词(list)最相似的10个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.similar_by_word(word, topn=10)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取某词语最相似的10个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.similar_by_vector(vector, topn=10)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取词向量最相似的10个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h3 id=&#34;33-词汇量维度数&#34;&gt;3.3 词汇量&amp;amp;维度数&lt;/h3&gt;
&lt;p&gt;查看模型中的词汇量&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;print(f&amp;#39;词汇量: {len(w2v)}&amp;#39;)
print(f&amp;#39;维度数: {w2v.vector_size}&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;词汇量: 1059801
维度数: 200
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;34-查看词向量&#34;&gt;3.4 查看词向量&lt;/h3&gt;
&lt;p&gt;查看任意词的词向量，例如“&lt;strong&gt;”人工智能”&lt;/strong&gt;”&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查看 ”人工智能” 的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;人工智能&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([-1.1817173 , -2.1371903 , -3.0181015 ,  1.7000161 , -3.081852  ,
        3.554449  , -0.22385244,  3.6647737 , -3.7086377 , -1.4868759 ,
       -0.7706527 ,  5.9335155 ,  2.8328223 , -1.7995875 , -6.051175  ,
       -0.91756725, -4.15509   , -1.6975762 , -4.5753274 , -3.022245  ,
       ......
       -2.0807118 , -3.4522808 ,  4.29429   , -1.712142  , -1.6512033 ,
        2.625037  , -3.4015207 ,  1.3526493 , -0.7858534 , -1.6782432 ,
       -3.1669524 , -2.6371615 , -1.5394825 ,  3.101744  ,  0.44502366,
       -1.4104489 , -0.01298253, -4.217453  , -0.92512876,  0.10754411],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;35-最相似词&#34;&gt;3.5 最相似词&lt;/h3&gt;
&lt;p&gt;与&amp;rsquo;创新&#39;, &amp;lsquo;颠覆&amp;rsquo;最相似的20个词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;# 词语列表中可传入任意多个词，
# 大邓词穷，只想到这两个相似的种子词
w2v.most_similar([&amp;#39;创新&amp;#39;, &amp;#39;颠覆&amp;#39;], topn=20)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;革新&amp;#39;, 0.7983665466308594),
 (&amp;#39;改革&amp;#39;, 0.7454208731651306),
 (&amp;#39;变革&amp;#39;, 0.7136300206184387),
 (&amp;#39;全新&amp;#39;, 0.707391619682312),
 (&amp;#39;彻底改变&amp;#39;, 0.7064372301101685),
 (&amp;#39;创造性&amp;#39;, 0.6960274577140808),
 (&amp;#39;颠覆性&amp;#39;, 0.6874485611915588),
 (&amp;#39;有别于&amp;#39;, 0.6775000095367432),
 (&amp;#39;加以改进&amp;#39;, 0.6736693978309631),
 (&amp;#39;摒弃&amp;#39;, 0.6716011762619019),
 (&amp;#39;独创&amp;#39;, 0.6609643697738647),
 (&amp;#39;颠覆传统&amp;#39;, 0.6604534983634949),
 (&amp;#39;开创&amp;#39;, 0.6531570553779602),
 (&amp;#39;核心技术&amp;#39;, 0.6419240236282349),
 (&amp;#39;彻底颠覆&amp;#39;, 0.6397384405136108),
 (&amp;#39;技术创新&amp;#39;, 0.6390863060951233),
 (&amp;#39;突破性&amp;#39;, 0.6368305087089539),
 (&amp;#39;大胆&amp;#39;, 0.6357517242431641),
 (&amp;#39;技术革新&amp;#39;, 0.6347700357437134),
 (&amp;#39;沿用&amp;#39;, 0.6328355669975281)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;刚刚的运行，体现模型很好的学习到了专利摘要中的语义关系。&lt;/p&gt;
&lt;p&gt;如果我想开发三个词典，分别是 &lt;strong&gt;创新&lt;/strong&gt;、&lt;strong&gt;成本&lt;/strong&gt;、&lt;strong&gt;质量&lt;/strong&gt; ，想直接将结果保存到txt中，可以运行如下代码&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;seeds&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新概念&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;创新&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;颠覆&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
         &lt;span class=&#34;s1&#34;&gt;&amp;#39;成本概念&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;成本&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
         &lt;span class=&#34;s1&#34;&gt;&amp;#39;质量概念&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;质量&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;expand_dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;          &lt;span class=&#34;c1&#34;&gt;# word2vec词向量&lt;/span&gt;
                     &lt;span class=&#34;n&#34;&gt;seeddict&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seeds&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 种子词字典&lt;/span&gt;
                     &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;         &lt;span class=&#34;c1&#34;&gt;# 保留20个最相似的词&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Finish! 创新概念 candidates saved to output/创新概念.txt
Finish! 成本概念 candidates saved to output/成本概念.txt
Finish! 质量概念 candidates saved to output/质量概念.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/similar-words.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四获取资源&#34;&gt;四、获取资源&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 免费     专利摘要-Word2Vec.200.15.bin 链接: https://pan.baidu.com/s/1LKebAWL5fzjUVo_MR7dVug?pwd=a56c 提取码: a56c

- 100元   cntext-2.1.6-py3-none-any.whl   如有需要，加微信 372335839， 备注「姓名-学校-专业」
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
      <content:encoded><![CDATA[<p><a href="https://textdata.cn/blog/2023-04-13-3571w-patent-dataset-in-china-mainland/"><strong>5112万条专利申请数据集(1985-2025年)</strong></a> 中随机抽取了30%的 「<strong>专利摘要</strong>」，构成6.14G的训练语料(千万级别)， 耗时6小时，训练得到word2vec模型。</p>
<blockquote>
<p>需要注意， 100%全部语料有30+G， 训练时间非常长。</p>
<p>没办法，我不会优化代码性能，所以只能抽取 30% 的文本数据来训练word2vec ，语料体积大概10G。</p>
</blockquote>
<br>
<p>本文需要用到新cntext，因为bug较多， 直接上传到PyPi，将导致之前制作的课程和公众号推文相关内容全部重新一遍。</p>
<p><br><br></p>
<h2 id="一语料构建">一、语料构建</h2>
<p>随机抽取20%的记录，构成千万专利文本摘要训练语料。</p>
<p>为了防止电脑内存爆炸， 对任意单个大csv文件，分批次读取，每次读10w行。最终将专利摘要文本保存到txt文件中，编码方式为utf-8。</p>
<blockquote>
<p>如果想开发一些词典，可以跳过此部分内容，并不影响代码运行。</p>
</blockquote>
<p><img loading="lazy" src="img/screen-datasets.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 将代码放在csv数据文件夹内</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;专利摘要.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">corpus_file</span><span class="p">:</span>
    <span class="c1"># 获得当前文件夹内所有的csv文件路径</span>
    <span class="n">chunk_dfs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;中国专利数据库.csv.gz&#39;</span><span class="p">,</span> 
                            <span class="n">usecols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;专利名称&#39;</span><span class="p">,</span> <span class="s1">&#39;摘要文本&#39;</span><span class="p">],</span> 
                            <span class="n">chunksize</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">chunk_df</span> <span class="ow">in</span> <span class="n">chunk_dfs</span><span class="p">:</span>
        <span class="c1"># 剔除专利摘要为空的记录</span>
        <span class="n">sample_df</span> <span class="o">=</span> <span class="n">chunk_df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="n">raw_text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;摘要文本&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;))</span>
        <span class="n">corpus_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span>
</code></pre></div><p>最终得到的 <strong>专利摘要.txt</strong>  文件有 10G<br><br></p>
<h2 id="二训练word2vec">二、训练word2vec</h2>
<h3 id="21-安装">2.1 安装</h3>
<p>将 <em><strong>cntext-2.1.6-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext-2.1.6-py3-none-any.whl
</code></pre></div><p>文末有 <em><strong>cntext-2.1.6-py3-none-any.whl</strong></em> 获取方式</p>
<br>
<h3 id="22-训练-word2vec">2.2 训练 Word2Vec</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># cntext为2.1.6</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;专利摘要.txt&#39;</span><span class="p">,</span>
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span>
                        <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="c1"># 词向量维度</span>
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span><span class="c1"># 窗口大小</span>
                        <span class="n">chunksize</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span> <span class="c1"># 每次读取10000行</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/专利摘要_cache.txt Not Found or Empty, Preprocessing Corpus

Reading Preprocessed Corpus from output/专利摘要_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 8816 s. 
Output Saved To: output/专利摘要-Word2Vec.200.15.bin
</code></pre></div><p>整理训练过程 2.5 小时， 训练结束后得到 <em><strong>output</strong></em> 文件夹， 里面有</p>
<ul>
<li><em><strong>output/专利摘要-Word2Vec.200.15.bin</strong></em>  模型文件</li>
<li><em><strong>专利摘要_cache.txt</strong></em>                   训练缓存文件</li>
</ul>
<br>
<h3 id="23-评估模型">2.3 评估模型</h3>
<p>使用近义法和类比法， 判断模型的表现。详情可查看<a href="https://cntext.readthedocs.io/zh-cn/latest/model.html">文档</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">evaluate_similarity</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">)</span>

<span class="n">ct</span><span class="o">.</span><span class="n">evaluate_analogy</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   427    |    110     |            0.46            |
+----------+------------+----------------------------+


类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|██████████████| 1198/1198 [00:11&lt;00:00, 99.91it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   238    |    439     |    3.78    |   5.67   |
|   CityInProvince   |   175    |     0      |   25.14    |   4.48   |
| FamilyRelationship |   156    |    116     |   33.33    |   2.29   |
|   SocialScience    |    8     |     62     |   37.50    |   2.33   |
+--------------------+----------+------------+------------+----------+
</code></pre></div><p><strong>近义测试</strong>: Spearman&rsquo;s Rank Coeficient系数取值[-1, 1], 取值越大， 说明模型表现越好。</p>
<br>
<p><strong>类比测试</strong>:</p>
<ul>
<li>CapitalOfCountries   专利语料在此项表现很差， 应该是语料中常见国家首度的提及较少。</li>
<li>CityInProvince       专利语料在此项好于CapitalOfCountries， 毕竟在中国大地进行科创。</li>
<li>FamilyRelationship   专利语料中没想到在此项准确率中显著大于0， 我原本以为准确率为0，毕竟专利摘要中出现家人管理不太技术。没想到没想到啊。 发明可能类似于电影非诚勿扰里解决人类问题的例子，发明很雷人。</li>
<li>SocialScience        专利语料在此项表现一般， 应该是语料中常见的社会科学词语提及较少。</li>
</ul>
<p>整体而言，在四个维度准确率较低。 但是需要说明， 这四个维度是大邓自己收集的，评判模型类比表现维度有很多， 有可能专利摘要在别的类比维度上表现会很好。</p>
<p><br><br></p>
<h2 id="三使用词向量">三、使用词向量</h2>
<h3 id="31-录入模型">3.1 录入模型</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/专利摘要-Word2Vec.200.15.bin&#39;</span><span class="p">)</span>
<span class="n">w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading 专利摘要-Word2Vec.200.15.bin...
&lt;gensim.models.keyedvectors.KeyedVectors at 0x32b079340&gt;
</code></pre></div><br>
<h3 id="32-keyedvectors的操作方法或属性">3.2 KeyedVectors的操作方法(或属性)</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>KeyedVectors.index_to_key</strong></em></td>
<td>获取词汇表中的所有单词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.key_to_index</strong></em></td>
<td>获取单词到索引的映射。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.vector_size</strong></em></td>
<td>获取GloVe模型中任意词向量的维度。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.get_vector(word)</strong></em></td>
<td>获取给定单词的词向量。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.most_similar(words, topn=10)</strong></em></td>
<td>获取某类词(list)最相似的10个近义词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_word(word, topn=10)</strong></em></td>
<td>获取某词语最相似的10个近义词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_vector(vector, topn=10)</strong></em></td>
<td>获取词向量最相似的10个近义词。</td>
</tr>
</tbody>
</table>
<br>
<h3 id="33-词汇量维度数">3.3 词汇量&amp;维度数</h3>
<p>查看模型中的词汇量</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">print(f&#39;词汇量: {len(w2v)}&#39;)
print(f&#39;维度数: {w2v.vector_size}&#39;)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">词汇量: 1059801
维度数: 200
</code></pre></div><br>
<h3 id="34-查看词向量">3.4 查看词向量</h3>
<p>查看任意词的词向量，例如“<strong>”人工智能”</strong>”</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查看 ”人工智能” 的词向量</span>
<span class="n">w2v</span><span class="p">[</span><span class="s1">&#39;人工智能&#39;</span><span class="p">]</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-1.1817173 , -2.1371903 , -3.0181015 ,  1.7000161 , -3.081852  ,
        3.554449  , -0.22385244,  3.6647737 , -3.7086377 , -1.4868759 ,
       -0.7706527 ,  5.9335155 ,  2.8328223 , -1.7995875 , -6.051175  ,
       -0.91756725, -4.15509   , -1.6975762 , -4.5753274 , -3.022245  ,
       ......
       -2.0807118 , -3.4522808 ,  4.29429   , -1.712142  , -1.6512033 ,
        2.625037  , -3.4015207 ,  1.3526493 , -0.7858534 , -1.6782432 ,
       -3.1669524 , -2.6371615 , -1.5394825 ,  3.101744  ,  0.44502366,
       -1.4104489 , -0.01298253, -4.217453  , -0.92512876,  0.10754411],
      dtype=float32)
</code></pre></div><br>
<h3 id="35-最相似词">3.5 最相似词</h3>
<p>与&rsquo;创新', &lsquo;颠覆&rsquo;最相似的20个词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"># 词语列表中可传入任意多个词，
# 大邓词穷，只想到这两个相似的种子词
w2v.most_similar([&#39;创新&#39;, &#39;颠覆&#39;], topn=20)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;革新&#39;, 0.7983665466308594),
 (&#39;改革&#39;, 0.7454208731651306),
 (&#39;变革&#39;, 0.7136300206184387),
 (&#39;全新&#39;, 0.707391619682312),
 (&#39;彻底改变&#39;, 0.7064372301101685),
 (&#39;创造性&#39;, 0.6960274577140808),
 (&#39;颠覆性&#39;, 0.6874485611915588),
 (&#39;有别于&#39;, 0.6775000095367432),
 (&#39;加以改进&#39;, 0.6736693978309631),
 (&#39;摒弃&#39;, 0.6716011762619019),
 (&#39;独创&#39;, 0.6609643697738647),
 (&#39;颠覆传统&#39;, 0.6604534983634949),
 (&#39;开创&#39;, 0.6531570553779602),
 (&#39;核心技术&#39;, 0.6419240236282349),
 (&#39;彻底颠覆&#39;, 0.6397384405136108),
 (&#39;技术创新&#39;, 0.6390863060951233),
 (&#39;突破性&#39;, 0.6368305087089539),
 (&#39;大胆&#39;, 0.6357517242431641),
 (&#39;技术革新&#39;, 0.6347700357437134),
 (&#39;沿用&#39;, 0.6328355669975281)]
</code></pre></div><br>
<p>刚刚的运行，体现模型很好的学习到了专利摘要中的语义关系。</p>
<p>如果我想开发三个词典，分别是 <strong>创新</strong>、<strong>成本</strong>、<strong>质量</strong> ，想直接将结果保存到txt中，可以运行如下代码</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">seeds</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;创新概念&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;创新&#39;</span><span class="p">,</span> <span class="s1">&#39;颠覆&#39;</span><span class="p">],</span>
         <span class="s1">&#39;成本概念&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;成本&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">],</span>
         <span class="s1">&#39;质量概念&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;质量&#39;</span><span class="p">]}</span>

<span class="n">ct</span><span class="o">.</span><span class="n">expand_dictionary</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">w2v</span><span class="p">,</span>          <span class="c1"># word2vec词向量</span>
                     <span class="n">seeddict</span><span class="o">=</span><span class="n">seeds</span><span class="p">,</span>  <span class="c1"># 种子词字典</span>
                     <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>         <span class="c1"># 保留20个最相似的词</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Finish! 创新概念 candidates saved to output/创新概念.txt
Finish! 成本概念 candidates saved to output/成本概念.txt
Finish! 质量概念 candidates saved to output/质量概念.txt
</code></pre></div><p><img loading="lazy" src="img/similar-words.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四获取资源">四、获取资源</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 免费     专利摘要-Word2Vec.200.15.bin 链接: https://pan.baidu.com/s/1LKebAWL5fzjUVo_MR7dVug?pwd=a56c 提取码: a56c

- 100元   cntext-2.1.6-py3-none-any.whl   如有需要，加微信 372335839， 备注「姓名-学校-专业」
</code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>词向量 | 使用1亿B站用户签名训练word2vec词向量</title>
      <link>https://textdata.cn/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/</guid>
      <description>&lt;h2 id=&#34;一用户签名&#34;&gt;一、用户签名&lt;/h2&gt;
&lt;p&gt;1 亿 B 站用户群体十分庞大，文本中蕴含着这个群体的认知信息(如兴趣、身份、座右铭等)，如果能用签名训练 word2vec 词向量模型，说不定就有利用这个模型，对每个用户签名进行量化, 对用户进行分类。 本文要解决&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;构建语料训练出模型&lt;/li&gt;
&lt;li&gt;简单看看模型训练效果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二准备语料&#34;&gt;二、准备语料&lt;/h2&gt;
&lt;p&gt;Kaggle 网有 1 亿 B 站用户数据集，下载地址&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/datasets/beats0/bilibili-user&#34;&gt;https://www.kaggle.com/datasets/beats0/bilibili-user&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;之前分享过 &lt;a href=&#34;https://textdata.cn/blog/2023-05-10-100m-bilibili-user-info-dataset/&#34;&gt;数据集 | 哔哩哔哩 1 亿用户数据&lt;/a&gt; ， 阅读此文可以熟悉 pandas 的一些基本操作，如数据读取、文本操作等。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 从kaggle下载B站1亿用户数据&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 查看前5行&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;User.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nrows&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;将 9093092 个非空签名汇总到 &lt;strong&gt;&lt;em&gt;B 站用户签名语料.txt&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;B站签名.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;raw_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;sign&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;代码运行后，得到 320M 的 &lt;strong&gt;&lt;em&gt;B 站签名.txt&lt;/em&gt;&lt;/strong&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三训练-word2vec&#34;&gt;三、训练 Word2Vec&lt;/h2&gt;
&lt;p&gt;我使用的自己 &lt;strong&gt;未公开&lt;/strong&gt; 的 cntext 2.1.6 版本， Bug 频出，等调整好了再公开。&lt;/p&gt;
&lt;h3 id=&#34;31-安装-cntext&#34;&gt;3.1 安装 cntext&lt;/h3&gt;
&lt;p&gt;将 cntext-2.1.6-py3-none-any.whl 放置于桌面，打开 &lt;strong&gt;cmd&lt;/strong&gt; (苹果电脑打开 terminal)， 输入 cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 &lt;strong&gt;cmd&lt;/strong&gt; (苹果电脑打开 terminal) 中使用 pip3 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install cntext-2.1.6-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文末有 **_cntext-2.1.6-py3-none-any.whl _**获取方式&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-训练-word2vec&#34;&gt;3.2 训练 word2vec&lt;/h3&gt;
&lt;p&gt;cntext 训练时候 Word2Vec 模型参数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;corpus_file&lt;/em&gt;&lt;/strong&gt; 语料 txt 文件路径； 刚刚准备的 &lt;strong&gt;&lt;em&gt;B 站用户签名语料.txt&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;window_size&lt;/em&gt;&lt;/strong&gt; 上下文窗口大小(上下文语义)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;vector_size&lt;/em&gt;&lt;/strong&gt; 向量维度数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;chunksize&lt;/em&gt;&lt;/strong&gt; 每次语料 txt 文件中读取的行数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;lang&lt;/em&gt;&lt;/strong&gt; 语言的语言&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# cntext2.1.6未公开，获取2.1.6请阅读文末获取方式&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Word2Vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;B站用户签名语料.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;window&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Mac(Linux) System, Enable Parallel Processing
Cache output/B站签名_cache.txt Not Found or Empty, Preprocessing Corpus

Reading Preprocessed Corpus from output/B站签名_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 275 s.
Output Saved To: output/B站签名-Word2Vec.200.15.bin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;耗时 275s， 模型训练完成！需要注意， output 文件夹内有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;B 站签名-Word2Vec.200.15.bin&lt;/em&gt;&lt;/strong&gt; 模型文件&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;B 站签名_cache.txt&lt;/em&gt;&lt;/strong&gt; 训练缓存文件&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&#34;33-评估模型&#34;&gt;3.3 评估模型&lt;/h3&gt;
&lt;p&gt;使用近义法和类比法， 判断模型的表现。详情可查看&lt;a href=&#34;https://cntext.readthedocs.io/zh-cn/latest/model.html&#34;&gt;文档&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evaluate_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evaluate_analogy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&amp;#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   434    |    103     |            0.34            |
+----------+------------+----------------------------+


类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|██████████████| 1198/1198 [00:11&amp;lt;00:00, 99.91it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   360    |    317     |   25.56    |   4.02   |
|   CityInProvince   |   175    |     0      |   33.71    |   4.64   |
| FamilyRelationship |   240    |     32     |   44.17    |   1.93   |
|   SocialScience    |    2     |     68     |    0.00    |   NaN    |
+--------------------+----------+------------+------------+----------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;近义测试&lt;/strong&gt;: Spearman&amp;rsquo;s Rank Coeficient 系数取值[-1, 1], 取值越大， 说明模型表现越好。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;类比测试&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CapitalOfCountries B 站用户签名语料在此项表现大于 0，说明很多签名里会出现国家首都这类信息。&lt;/li&gt;
&lt;li&gt;CityInProvince B 站用户签名语料在此项表现大于 0，说明很多签名里会出现省份省会这类信息。考虑到用户几乎全为中国人，所以此项准确率高于 CapitalOfCountries。&lt;/li&gt;
&lt;li&gt;FamilyRelationship B 站用户签名语料体现的是一个个鲜活的中国人，签名中必然含有更多的人际关系， 所以此项准确率是四个项目中最高的。&lt;/li&gt;
&lt;li&gt;SocialScience B 站用户签名语料在此项表现最差， 应该是语料中常见的社会科学词语提及很少。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整体而言，模型效果一般，但是不是算法代码问题，而是语料出的问题。毕竟每个用户的签名一般都是一句话，太短，信息太少。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四使用-word2vec&#34;&gt;四、使用 word2vec&lt;/h2&gt;
&lt;h3 id=&#34;41-读取模型&#34;&gt;4.1 读取模型&lt;/h3&gt;
&lt;p&gt;使用 gensim 录入模型 &lt;strong&gt;&lt;em&gt;B 站用户签名语料-Word2Vec.100.15.bin&lt;/em&gt;&lt;/strong&gt; ,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;gensim.models&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/B站签名-Word2Vec.200.15.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;模型词汇量: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;模型词汇量:  244491
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-查询某词的词向量&#34;&gt;4.2 查询某词的词向量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;w2v[&amp;#39;高冷&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([ 1.05914783e+00,  4.51383203e-01, -1.34764791e+00, -9.42894161e-01,
        5.28594255e-01,  8.05936933e-01, -1.59555584e-01,  2.42719814e-01,
       -6.04722261e-01, -9.25606042e-02,  9.69056904e-01,  8.85407850e-02,
       -1.67851341e+00,  3.26303959e-01,  6.52321458e-01,  5.77043407e-02,
       -4.24268842e-02, -2.64299393e-01,  5.24512887e-01,  2.15208486e-01,
       -2.09263057e-01, -4.55661058e-01,  8.78976703e-01, -1.24363959e+00,
       -1.71196852e-02, -9.03965294e-01, -6.52690083e-02,  2.47650072e-02,
       -2.82155067e-01,  9.09134224e-02,  9.13890541e-01, -1.40862179e+00,
       -1.31956196e+00, -5.29659569e-01,  1.23605825e-01, -4.00647372e-01,
        4.94630456e-01,  2.81695575e-01,  1.71391249e-01,  1.23341233e-01,
       -7.70617545e-01,  5.81079908e-02, -4.89788234e-01,  2.14924827e-01,
       -7.73121595e-01, -6.66803181e-01, -1.31617844e+00,  1.18301921e-01,
        6.22543573e-01, -8.07524860e-01, -4.36694354e-01,  2.95946062e-01,
        3.10503364e-01, -4.93252903e-01,  1.27962172e-01,  1.97043195e-01,
        6.61175609e-01, -1.80842638e-01,  1.13270843e+00, -5.34760773e-01,
        9.13145125e-01,  5.48191011e-01,  7.68198539e-03,  1.17955339e+00,
       -1.96015276e-02,  9.14144278e-01, -9.06695664e-01,  4.39731702e-02,
       -3.87832075e-01,  4.72544342e-01,  4.95476156e-01, -1.21628530e-01,
       -4.41256445e-03,  1.82375580e-01, -7.00045705e-01,  4.34259921e-01,
        2.00862193e+00, -5.61490715e-01, -7.67120644e-02,  5.78972995e-01,
       -7.80492842e-01, -5.01321375e-01, -5.50926566e-01, -8.99926543e-01,
       -1.66289490e-02,  1.77679747e-01,  4.23889339e-01,  1.40111005e+00,
       -7.63866380e-02, -8.86032939e-01, -1.08106744e+00,  3.31989765e-01,
        3.78885448e-01, -1.23718023e+00,  2.09680721e-01,  2.39727721e-01,
        2.46049106e-01,  2.32866824e-01, -6.65583909e-02,  1.09542537e+00,
       -5.44713318e-01,  7.68220305e-01, -1.56612769e-02,  3.48719925e-01,
        2.91741371e-01,  1.88722059e-01, -2.12467611e-01,  8.20825279e-01,
       -1.74725935e-01, -8.05535197e-01, -1.41250715e-01, -7.84179568e-01,
       -8.00660312e-01, -1.12991728e-01, -2.16052849e-02, -1.07448053e+00,
        2.53552765e-01, -1.28611282e-01, -1.16868567e+00, -6.08788371e-01,
        4.30017859e-02, -5.11076570e-01,  6.43583059e-01,  3.11966389e-01,
       -1.63116843e-01,  3.58751595e-01,  5.16831456e-03,  5.09353161e-01,
        1.61675465e+00,  6.42039478e-01, -1.07160270e+00, -2.34255135e-01,
       -7.27983773e-01,  1.20267116e-01, -1.11912894e+00,  1.49096262e+00,
       -1.48015752e-01,  6.85670376e-02, -1.70197403e+00,  2.16349974e-01,
        1.32302952e+00,  5.39037228e-01, -8.35760951e-01, -7.43441284e-01,
        6.55625939e-01, -5.07541537e-01, -5.40877655e-02, -5.38533449e-01,
       -2.57937461e-01,  8.67499232e-01, -6.53150141e-01, -1.32043970e+00,
       -5.84588587e-01,  1.24599323e-01, -8.35753500e-01, -2.68954426e-01,
        3.67542468e-02,  1.61010170e+00,  7.27127492e-01,  1.35515738e+00,
       -2.76694775e-01,  2.69006938e-01,  4.81265247e-01, -6.30314708e-01,
       -3.66074532e-01,  3.03934813e-01,  1.92417920e+00,  4.67498928e-01,
       -1.83004290e-01,  1.01947844e+00, -5.52489638e-01,  1.59275869e-03,
        4.84914184e-01,  1.33545566e+00, -9.75372076e-01,  2.25273356e-01,
        6.02540433e-01,  7.07564950e-01,  1.36330187e-01, -4.34346311e-02,
        4.53452200e-01,  1.58401883e+00, -6.68083191e-01, -1.30876124e+00,
       -1.19713686e-01, -9.80615169e-02, -2.04207993e+00,  8.29822361e-01,
       -4.08902228e-01, -4.70339246e-02,  1.00982547e+00,  1.64084151e-01,
        4.62104648e-01, -2.28677273e-01, -5.95047355e-01, -2.71069705e-01,
        6.27930462e-01, -8.85554433e-01, -1.79520398e-01, -3.44800770e-01],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;43-查看近义词&#34;&gt;4.3 查看近义词&lt;/h3&gt;
&lt;p&gt;通过给定词语，查看其近义词，可以了解模型训练的好坏。语义捕捉的合理，说明语料合理，模型训练的好。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 列表中可以传入任意多个词，这里大邓偷懒，都只传入了一两个词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;女汉纸&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;汉纸&amp;#39;, 0.8672055602073669),
 (&amp;#39;腹黑&amp;#39;, 0.8662189841270447),
 (&amp;#39;文艺清新&amp;#39;, 0.849425733089447),
 (&amp;#39;闷骚&amp;#39;, 0.8427557945251465),
 (&amp;#39;神经大条&amp;#39;, 0.8329920768737793),
 (&amp;#39;汉子&amp;#39;, 0.8232208490371704),
 (&amp;#39;宅基&amp;#39;, 0.8224843144416809),
 (&amp;#39;猥琐大叔&amp;#39;, 0.8214939832687378),
 (&amp;#39;偶是&amp;#39;, 0.8164061307907104),
 (&amp;#39;腐宅&amp;#39;, 0.8117423057556152),
 (&amp;#39;宅女腐女&amp;#39;, 0.8073472380638123),
 (&amp;#39;软妹&amp;#39;, 0.7999386787414551),
 (&amp;#39;萌妹&amp;#39;, 0.7999064326286316),
 (&amp;#39;小女生&amp;#39;, 0.7998836040496826),
 (&amp;#39;天蝎女&amp;#39;, 0.7971166372299194),
 (&amp;#39;傲娇受&amp;#39;, 0.7964810132980347),
 (&amp;#39;天蝎&amp;#39;, 0.7957624197006226),
 (&amp;#39;天蝎座&amp;#39;, 0.7915034890174866),
 (&amp;#39;女纸&amp;#39;, 0.7912994623184204),
 (&amp;#39;双鱼座&amp;#39;, 0.7900263667106628)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;犯二&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;脱线&amp;#39;, 0.8355404734611511),
 (&amp;#39;神经质&amp;#39;, 0.8035165667533875),
 (&amp;#39;神经大条&amp;#39;, 0.7816897630691528),
 (&amp;#39;发神经&amp;#39;, 0.780509352684021),
 (&amp;#39;人来疯&amp;#39;, 0.7794896960258484),
 (&amp;#39;精分&amp;#39;, 0.7705598473548889),
 (&amp;#39;毒舌&amp;#39;, 0.7692195773124695),
 (&amp;#39;犯病&amp;#39;, 0.7659722566604614),
 (&amp;#39;闷骚&amp;#39;, 0.7620697617530823),
 (&amp;#39;迷糊&amp;#39;, 0.7608135342597961),
 (&amp;#39;智商在线&amp;#39;, 0.7525709867477417),
 (&amp;#39;抽疯&amp;#39;, 0.7491970658302307),
 (&amp;#39;欢脱&amp;#39;, 0.7444456219673157),
 (&amp;#39;深井&amp;#39;, 0.7416326403617859),
 (&amp;#39;抽风&amp;#39;, 0.7321327924728394),
 (&amp;#39;精分患者&amp;#39;, 0.7319940328598022),
 (&amp;#39;装嫩&amp;#39;, 0.7267141342163086),
 (&amp;#39;蒙圈&amp;#39;, 0.7262043952941895),
 (&amp;#39;神经&amp;#39;, 0.7257982492446899),
 (&amp;#39;假正经&amp;#39;, 0.7215201258659363)]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;内向&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;外向&amp;#39;, 0.8474423885345459),
 (&amp;#39;慢热&amp;#39;, 0.8272333741188049),
 (&amp;#39;不爱说话&amp;#39;, 0.8249834775924683),
 (&amp;#39;不善言辞&amp;#39;, 0.80635666847229),
 (&amp;#39;腼腆&amp;#39;, 0.7940059304237366),
 (&amp;#39;孤僻&amp;#39;, 0.7929618954658508),
 (&amp;#39;开朗&amp;#39;, 0.7585728168487549),
 (&amp;#39;闷骚&amp;#39;, 0.745791494846344),
 (&amp;#39;神经质&amp;#39;, 0.7454176545143127),
 (&amp;#39;多愁善感&amp;#39;, 0.7348753809928894),
 (&amp;#39;胆小&amp;#39;, 0.7213962078094482),
 (&amp;#39;沉默寡言&amp;#39;, 0.7145323157310486),
 (&amp;#39;随和&amp;#39;, 0.7115553617477417),
 (&amp;#39;敏感&amp;#39;, 0.7103193402290344),
 (&amp;#39;水瓶座&amp;#39;, 0.7092751264572144),
 (&amp;#39;大大咧咧&amp;#39;, 0.7085798382759094),
 (&amp;#39;高冷&amp;#39;, 0.7084994912147522),
 (&amp;#39;性格开朗&amp;#39;, 0.7064590454101562),
 (&amp;#39;耿直&amp;#39;, 0.7048951983451843),
 (&amp;#39;做作&amp;#39;, 0.704330325126648)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;五获取资源&#34;&gt;五、获取资源&lt;/h2&gt;
&lt;p&gt;内容整理不易， 本文内容分免费和付费部分。 免费部分可以直接下载数据、构建语料、使用 word2vec 模型。&lt;/p&gt;
&lt;p&gt;付费部分主要是 cntext，用于训练 word2vec 模型。 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 免费     1亿用户数据集 https://www.kaggle.com/datasets/beats0/bilibili-user

- 免费     B站签名-Word2Vec.200.15.bin  链接: https://pan.baidu.com/s/1ILVwu6gGGGP0IHv-vsjgvw?pwd=em99 提取码: em99

- 100元   获得cntext-2.1.6-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cntext.readthedocs.io/&#34;&gt;文本分析库 cntext2.x 使用手册 https://cntext.readthedocs.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/&#34;&gt;实验 | 使用 Stanford Glove 代码训练中文语料的 Glove 模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/&#34;&gt;词向量 | 使用人民网领导留言板语料训练 Word2Vec 模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/&#34;&gt;使用 5000w 专利申请数据集按年份(按省份)训练词向量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/&#34;&gt;使用 1000w 条豆瓣影评训练 Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见 39 个 FAQ 汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/&#34;&gt;转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/&#34;&gt;OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一用户签名">一、用户签名</h2>
<p>1 亿 B 站用户群体十分庞大，文本中蕴含着这个群体的认知信息(如兴趣、身份、座右铭等)，如果能用签名训练 word2vec 词向量模型，说不定就有利用这个模型，对每个用户签名进行量化, 对用户进行分类。 本文要解决</p>
<ul>
<li>构建语料训练出模型</li>
<li>简单看看模型训练效果</li>
</ul>
<p><br><br></p>
<h2 id="二准备语料">二、准备语料</h2>
<p>Kaggle 网有 1 亿 B 站用户数据集，下载地址</p>
<blockquote>
<p><a href="https://www.kaggle.com/datasets/beats0/bilibili-user">https://www.kaggle.com/datasets/beats0/bilibili-user</a></p>
</blockquote>
<p>之前分享过 <a href="https://textdata.cn/blog/2023-05-10-100m-bilibili-user-info-dataset/">数据集 | 哔哩哔哩 1 亿用户数据</a> ， 阅读此文可以熟悉 pandas 的一些基本操作，如数据读取、文本操作等。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 从kaggle下载B站1亿用户数据</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># 查看前5行</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;User.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<p>将 9093092 个非空签名汇总到 <strong><em>B 站用户签名语料.txt</em></strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;B站签名.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">raw_text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;sign&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span>
</code></pre></div><p>代码运行后，得到 320M 的 <strong><em>B 站签名.txt</em></strong> 。</p>
<p><br><br></p>
<h2 id="三训练-word2vec">三、训练 Word2Vec</h2>
<p>我使用的自己 <strong>未公开</strong> 的 cntext 2.1.6 版本， Bug 频出，等调整好了再公开。</p>
<h3 id="31-安装-cntext">3.1 安装 cntext</h3>
<p>将 cntext-2.1.6-py3-none-any.whl 放置于桌面，打开 <strong>cmd</strong> (苹果电脑打开 terminal)， 输入 cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <strong>cmd</strong> (苹果电脑打开 terminal) 中使用 pip3 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext-2.1.6-py3-none-any.whl
</code></pre></div><p>文末有 **_cntext-2.1.6-py3-none-any.whl _**获取方式</p>
<br>
<h3 id="32-训练-word2vec">3.2 训练 word2vec</h3>
<p>cntext 训练时候 Word2Vec 模型参数</p>
<ul>
<li><strong><em>corpus_file</em></strong> 语料 txt 文件路径； 刚刚准备的 <strong><em>B 站用户签名语料.txt</em></strong></li>
<li><strong><em>window_size</em></strong> 上下文窗口大小(上下文语义)</li>
<li><strong><em>vector_size</em></strong> 向量维度数</li>
<li><strong><em>chunksize</em></strong> 每次语料 txt 文件中读取的行数</li>
<li><strong><em>lang</em></strong> 语言的语言</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># cntext2.1.6未公开，获取2.1.6请阅读文末获取方式</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;B站用户签名语料.txt&#39;</span><span class="p">,</span>
                    <span class="n">window</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
                    <span class="n">vector_size</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
                    <span class="n">window_size</span> <span class="o">=</span> <span class="mi">15</span>
                    <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/B站签名_cache.txt Not Found or Empty, Preprocessing Corpus

Reading Preprocessed Corpus from output/B站签名_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 275 s.
Output Saved To: output/B站签名-Word2Vec.200.15.bin
</code></pre></div><p>耗时 275s， 模型训练完成！需要注意， output 文件夹内有</p>
<ul>
<li><strong><em>B 站签名-Word2Vec.200.15.bin</em></strong> 模型文件</li>
<li><strong><em>B 站签名_cache.txt</em></strong> 训练缓存文件</li>
</ul>
<br>
<h3 id="33-评估模型">3.3 评估模型</h3>
<p>使用近义法和类比法， 判断模型的表现。详情可查看<a href="https://cntext.readthedocs.io/zh-cn/latest/model.html">文档</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">evaluate_similarity</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">ct</span><span class="o">.</span><span class="n">evaluate_analogy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   434    |    103     |            0.34            |
+----------+------------+----------------------------+


类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|██████████████| 1198/1198 [00:11&lt;00:00, 99.91it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   360    |    317     |   25.56    |   4.02   |
|   CityInProvince   |   175    |     0      |   33.71    |   4.64   |
| FamilyRelationship |   240    |     32     |   44.17    |   1.93   |
|   SocialScience    |    2     |     68     |    0.00    |   NaN    |
+--------------------+----------+------------+------------+----------+
</code></pre></div><p><strong>近义测试</strong>: Spearman&rsquo;s Rank Coeficient 系数取值[-1, 1], 取值越大， 说明模型表现越好。</p>
<br>
<p><strong>类比测试</strong>:</p>
<ul>
<li>CapitalOfCountries B 站用户签名语料在此项表现大于 0，说明很多签名里会出现国家首都这类信息。</li>
<li>CityInProvince B 站用户签名语料在此项表现大于 0，说明很多签名里会出现省份省会这类信息。考虑到用户几乎全为中国人，所以此项准确率高于 CapitalOfCountries。</li>
<li>FamilyRelationship B 站用户签名语料体现的是一个个鲜活的中国人，签名中必然含有更多的人际关系， 所以此项准确率是四个项目中最高的。</li>
<li>SocialScience B 站用户签名语料在此项表现最差， 应该是语料中常见的社会科学词语提及很少。</li>
</ul>
<p>整体而言，模型效果一般，但是不是算法代码问题，而是语料出的问题。毕竟每个用户的签名一般都是一句话，太短，信息太少。</p>
<p><br><br></p>
<h2 id="四使用-word2vec">四、使用 word2vec</h2>
<h3 id="41-读取模型">4.1 读取模型</h3>
<p>使用 gensim 录入模型 <strong><em>B 站用户签名语料-Word2Vec.100.15.bin</em></strong> ,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/B站签名-Word2Vec.200.15.bin&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;模型词汇量: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w2v</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">模型词汇量:  244491
</code></pre></div><br>
<h3 id="42-查询某词的词向量">4.2 查询某词的词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">w2v[&#39;高冷&#39;]
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 1.05914783e+00,  4.51383203e-01, -1.34764791e+00, -9.42894161e-01,
        5.28594255e-01,  8.05936933e-01, -1.59555584e-01,  2.42719814e-01,
       -6.04722261e-01, -9.25606042e-02,  9.69056904e-01,  8.85407850e-02,
       -1.67851341e+00,  3.26303959e-01,  6.52321458e-01,  5.77043407e-02,
       -4.24268842e-02, -2.64299393e-01,  5.24512887e-01,  2.15208486e-01,
       -2.09263057e-01, -4.55661058e-01,  8.78976703e-01, -1.24363959e+00,
       -1.71196852e-02, -9.03965294e-01, -6.52690083e-02,  2.47650072e-02,
       -2.82155067e-01,  9.09134224e-02,  9.13890541e-01, -1.40862179e+00,
       -1.31956196e+00, -5.29659569e-01,  1.23605825e-01, -4.00647372e-01,
        4.94630456e-01,  2.81695575e-01,  1.71391249e-01,  1.23341233e-01,
       -7.70617545e-01,  5.81079908e-02, -4.89788234e-01,  2.14924827e-01,
       -7.73121595e-01, -6.66803181e-01, -1.31617844e+00,  1.18301921e-01,
        6.22543573e-01, -8.07524860e-01, -4.36694354e-01,  2.95946062e-01,
        3.10503364e-01, -4.93252903e-01,  1.27962172e-01,  1.97043195e-01,
        6.61175609e-01, -1.80842638e-01,  1.13270843e+00, -5.34760773e-01,
        9.13145125e-01,  5.48191011e-01,  7.68198539e-03,  1.17955339e+00,
       -1.96015276e-02,  9.14144278e-01, -9.06695664e-01,  4.39731702e-02,
       -3.87832075e-01,  4.72544342e-01,  4.95476156e-01, -1.21628530e-01,
       -4.41256445e-03,  1.82375580e-01, -7.00045705e-01,  4.34259921e-01,
        2.00862193e+00, -5.61490715e-01, -7.67120644e-02,  5.78972995e-01,
       -7.80492842e-01, -5.01321375e-01, -5.50926566e-01, -8.99926543e-01,
       -1.66289490e-02,  1.77679747e-01,  4.23889339e-01,  1.40111005e+00,
       -7.63866380e-02, -8.86032939e-01, -1.08106744e+00,  3.31989765e-01,
        3.78885448e-01, -1.23718023e+00,  2.09680721e-01,  2.39727721e-01,
        2.46049106e-01,  2.32866824e-01, -6.65583909e-02,  1.09542537e+00,
       -5.44713318e-01,  7.68220305e-01, -1.56612769e-02,  3.48719925e-01,
        2.91741371e-01,  1.88722059e-01, -2.12467611e-01,  8.20825279e-01,
       -1.74725935e-01, -8.05535197e-01, -1.41250715e-01, -7.84179568e-01,
       -8.00660312e-01, -1.12991728e-01, -2.16052849e-02, -1.07448053e+00,
        2.53552765e-01, -1.28611282e-01, -1.16868567e+00, -6.08788371e-01,
        4.30017859e-02, -5.11076570e-01,  6.43583059e-01,  3.11966389e-01,
       -1.63116843e-01,  3.58751595e-01,  5.16831456e-03,  5.09353161e-01,
        1.61675465e+00,  6.42039478e-01, -1.07160270e+00, -2.34255135e-01,
       -7.27983773e-01,  1.20267116e-01, -1.11912894e+00,  1.49096262e+00,
       -1.48015752e-01,  6.85670376e-02, -1.70197403e+00,  2.16349974e-01,
        1.32302952e+00,  5.39037228e-01, -8.35760951e-01, -7.43441284e-01,
        6.55625939e-01, -5.07541537e-01, -5.40877655e-02, -5.38533449e-01,
       -2.57937461e-01,  8.67499232e-01, -6.53150141e-01, -1.32043970e+00,
       -5.84588587e-01,  1.24599323e-01, -8.35753500e-01, -2.68954426e-01,
        3.67542468e-02,  1.61010170e+00,  7.27127492e-01,  1.35515738e+00,
       -2.76694775e-01,  2.69006938e-01,  4.81265247e-01, -6.30314708e-01,
       -3.66074532e-01,  3.03934813e-01,  1.92417920e+00,  4.67498928e-01,
       -1.83004290e-01,  1.01947844e+00, -5.52489638e-01,  1.59275869e-03,
        4.84914184e-01,  1.33545566e+00, -9.75372076e-01,  2.25273356e-01,
        6.02540433e-01,  7.07564950e-01,  1.36330187e-01, -4.34346311e-02,
        4.53452200e-01,  1.58401883e+00, -6.68083191e-01, -1.30876124e+00,
       -1.19713686e-01, -9.80615169e-02, -2.04207993e+00,  8.29822361e-01,
       -4.08902228e-01, -4.70339246e-02,  1.00982547e+00,  1.64084151e-01,
        4.62104648e-01, -2.28677273e-01, -5.95047355e-01, -2.71069705e-01,
        6.27930462e-01, -8.85554433e-01, -1.79520398e-01, -3.44800770e-01],
      dtype=float32)
</code></pre></div><br>
<h3 id="43-查看近义词">4.3 查看近义词</h3>
<p>通过给定词语，查看其近义词，可以了解模型训练的好坏。语义捕捉的合理，说明语料合理，模型训练的好。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 列表中可以传入任意多个词，这里大邓偷懒，都只传入了一两个词</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;女汉纸&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;汉纸&#39;, 0.8672055602073669),
 (&#39;腹黑&#39;, 0.8662189841270447),
 (&#39;文艺清新&#39;, 0.849425733089447),
 (&#39;闷骚&#39;, 0.8427557945251465),
 (&#39;神经大条&#39;, 0.8329920768737793),
 (&#39;汉子&#39;, 0.8232208490371704),
 (&#39;宅基&#39;, 0.8224843144416809),
 (&#39;猥琐大叔&#39;, 0.8214939832687378),
 (&#39;偶是&#39;, 0.8164061307907104),
 (&#39;腐宅&#39;, 0.8117423057556152),
 (&#39;宅女腐女&#39;, 0.8073472380638123),
 (&#39;软妹&#39;, 0.7999386787414551),
 (&#39;萌妹&#39;, 0.7999064326286316),
 (&#39;小女生&#39;, 0.7998836040496826),
 (&#39;天蝎女&#39;, 0.7971166372299194),
 (&#39;傲娇受&#39;, 0.7964810132980347),
 (&#39;天蝎&#39;, 0.7957624197006226),
 (&#39;天蝎座&#39;, 0.7915034890174866),
 (&#39;女纸&#39;, 0.7912994623184204),
 (&#39;双鱼座&#39;, 0.7900263667106628)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;犯二&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;脱线&#39;, 0.8355404734611511),
 (&#39;神经质&#39;, 0.8035165667533875),
 (&#39;神经大条&#39;, 0.7816897630691528),
 (&#39;发神经&#39;, 0.780509352684021),
 (&#39;人来疯&#39;, 0.7794896960258484),
 (&#39;精分&#39;, 0.7705598473548889),
 (&#39;毒舌&#39;, 0.7692195773124695),
 (&#39;犯病&#39;, 0.7659722566604614),
 (&#39;闷骚&#39;, 0.7620697617530823),
 (&#39;迷糊&#39;, 0.7608135342597961),
 (&#39;智商在线&#39;, 0.7525709867477417),
 (&#39;抽疯&#39;, 0.7491970658302307),
 (&#39;欢脱&#39;, 0.7444456219673157),
 (&#39;深井&#39;, 0.7416326403617859),
 (&#39;抽风&#39;, 0.7321327924728394),
 (&#39;精分患者&#39;, 0.7319940328598022),
 (&#39;装嫩&#39;, 0.7267141342163086),
 (&#39;蒙圈&#39;, 0.7262043952941895),
 (&#39;神经&#39;, 0.7257982492446899),
 (&#39;假正经&#39;, 0.7215201258659363)]

</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;内向&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;外向&#39;, 0.8474423885345459),
 (&#39;慢热&#39;, 0.8272333741188049),
 (&#39;不爱说话&#39;, 0.8249834775924683),
 (&#39;不善言辞&#39;, 0.80635666847229),
 (&#39;腼腆&#39;, 0.7940059304237366),
 (&#39;孤僻&#39;, 0.7929618954658508),
 (&#39;开朗&#39;, 0.7585728168487549),
 (&#39;闷骚&#39;, 0.745791494846344),
 (&#39;神经质&#39;, 0.7454176545143127),
 (&#39;多愁善感&#39;, 0.7348753809928894),
 (&#39;胆小&#39;, 0.7213962078094482),
 (&#39;沉默寡言&#39;, 0.7145323157310486),
 (&#39;随和&#39;, 0.7115553617477417),
 (&#39;敏感&#39;, 0.7103193402290344),
 (&#39;水瓶座&#39;, 0.7092751264572144),
 (&#39;大大咧咧&#39;, 0.7085798382759094),
 (&#39;高冷&#39;, 0.7084994912147522),
 (&#39;性格开朗&#39;, 0.7064590454101562),
 (&#39;耿直&#39;, 0.7048951983451843),
 (&#39;做作&#39;, 0.704330325126648)]
</code></pre></div><br>
<br>
<h2 id="五获取资源">五、获取资源</h2>
<p>内容整理不易， 本文内容分免费和付费部分。 免费部分可以直接下载数据、构建语料、使用 word2vec 模型。</p>
<p>付费部分主要是 cntext，用于训练 word2vec 模型。 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 免费     1亿用户数据集 https://www.kaggle.com/datasets/beats0/bilibili-user

- 免费     B站签名-Word2Vec.200.15.bin  链接: https://pan.baidu.com/s/1ILVwu6gGGGP0IHv-vsjgvw?pwd=em99 提取码: em99

- 100元   获得cntext-2.1.6-py3-none-any.whl
</code></pre></div><p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://cntext.readthedocs.io/">文本分析库 cntext2.x 使用手册 https://cntext.readthedocs.io/</a></li>
<li><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用 Stanford Glove 代码训练中文语料的 Glove 模型</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量 | 使用人民网领导留言板语料训练 Word2Vec 模型</a></li>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">使用 5000w 专利申请数据集按年份(按省份)训练词向量</a></li>
<li><a href="https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/">使用 1000w 条豆瓣影评训练 Word2Vec</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见 39 个 FAQ 汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>实验 | 使用Stanford Glove代码训练中文语料的Glove模型</title>
      <link>https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/</link>
      <pubDate>Fri, 28 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/</guid>
      <description>&lt;h2 id=&#34;一简介&#34;&gt;一、简介&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;Stanford GloVe&lt;/a&gt;（Global Vectors for Word Representation）算法作为一种融合全局统计信息与局部上下文窗口的词嵌入模型，相较于Word2Vec仅依赖局部上下文，GloVe利用全局统计信息，能更精准地反映词频分布特征。例如，在高维词向量（如200D）中，GloVe在词语类比任务中准确率达75%，并在命名实体识别任务中优于其他词嵌入模型。因其高效的语义表征能力，在社会学、管理学等领域展现出广泛的应用价值。 相关词嵌入文献资料可阅读&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/&#34;&gt;OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/&#34;&gt;转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-09-literature-about-embeddings/&#34;&gt;文献汇总 | 词嵌入 与 社会科学中的偏见(态度)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-01-embeddings-and-attitude/&#34;&gt;词嵌入测量不同群体对某概念的态度(偏见)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二环境准备&#34;&gt;二、环境准备&lt;/h2&gt;
&lt;p&gt;cntext2.x 内置了 GloVe 训练所需的环境，支持 win 和 mac。&lt;/p&gt;
&lt;p&gt;获取&lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/&#34;&gt;cntext2.x&lt;/a&gt; 的安装文件 &lt;em&gt;&lt;strong&gt;cntext-2.1.5-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt;，并将该whl文件放置于桌面。执行以下安装命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
pip install cntext-2.1.5-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GloVe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dict_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stopwords_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;min_count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_memory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;4.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_max&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;corpus_file&lt;/strong&gt;&lt;/em&gt;: 输入语料文件路径（文本格式）。该文件为分词后的语料文件。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;lang&lt;/strong&gt;&lt;/em&gt;: 语料文件的语言类型，默认为 &amp;lsquo;chinese&amp;rsquo;。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;dict_file&lt;/strong&gt;&lt;/em&gt;: 自定义词典txt文件路径，默认为None。utf-8编码。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;stopwords_file&lt;/strong&gt;&lt;/em&gt;: 停用词文件路径，默认为 None。utf-8编码。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;vector_size&lt;/strong&gt;&lt;/em&gt;: 词向量维度，默认 100。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;window_size&lt;/strong&gt;&lt;/em&gt;: 上下文窗口大小，默认 15。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;min_count&lt;/strong&gt;&lt;/em&gt;: 忽略出现次数低于此值的单词，默认 5。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;max_memory&lt;/strong&gt;&lt;/em&gt;: 可供使用的最大内存大小，单位为GB，默认 4;  该参数越大，训练越快。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;max_iter&lt;/strong&gt;&lt;/em&gt;: 训练的最大迭代次数，默认 15。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;x_max&lt;/strong&gt;&lt;/em&gt;: 共现矩阵中元素的最大计数值，默认 10。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三训练中文glove&#34;&gt;三、训练中文GloVe&lt;/h2&gt;
&lt;p&gt;我们其实只需要设置 &lt;em&gt;&lt;strong&gt;corpus_file&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;lang&lt;/strong&gt;&lt;/em&gt;， 但为了让大家知道&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上下文的窗口大小 &lt;em&gt;&lt;strong&gt;window_size&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;训练出模型词语的维度数 &lt;em&gt;&lt;strong&gt;vector_size&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 简化版调用。训练window_size=100维， vector_size=15&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# glove_wv = ct.GloVe(corpus_file=&amp;#39;data/三体.txt&amp;#39;, lang=&amp;#39;chinese&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 正常调用。训练window_size=15维， vector_size=50&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;glove_wv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GloVe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/三体.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;only_binary&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 同时保存txt和bin两种格式的模型文件&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;glove_wv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Mac(Linux) System, Enable Parallel Processing
Cache output/三体_cache.txt Not Found or Empty, Preprocessing Corpus
Start Training GloVe
BUILDING VOCABULARY
Using vocabulary of size 6975.

COUNTING COOCCURRENCES
Merging cooccurrence files: processed 2106999 lines.

Using random seed 1743474106
SHUFFLING COOCCURRENCES
Merging temp files: processed 2106999 lines.

TRAINING MODEL
Read 2106999 lines.
Using random seed 1743474106
04/01/25 - 10:21.46AM, iter: 001, cost: 0.055981
04/01/25 - 10:21.46AM, iter: 002, cost: 0.050632
......
04/01/25 - 10:21.48AM, iter: 014, cost: 0.030047
04/01/25 - 10:21.48AM, iter: 015, cost: 0.029100

GloVe Training Cost 9 s. 
Output Saved To: output/三体-GloVe.50.15.txt
Output Saved To: output/三体-GloVe.50.15.bin
&amp;lt;gensim.models.keyedvectors.KeyedVectors at 0x331517440&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-glove.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;四使用中文glove模型&#34;&gt;四、使用中文GloVe模型&lt;/h2&gt;
&lt;h3 id=&#34;41-加载模型&#34;&gt;4.1 加载模型&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 加载word2vec模型.txt文件&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/三体-GloVe.50.15.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&amp;lt;gensim.models.keyedvectors.KeyedVectors at 0x336ff8dd0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-keyedvectors的操作方法或属性&#34;&gt;4.2 KeyedVectors的操作方法(或属性)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方法&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.index_to_key&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取词汇表中的所有单词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.key_to_index&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取单词到索引的映射。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.vector_size&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取GloVe模型中任意词向量的维度。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.get_vector(word)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取给定单词的词向量。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.similar_by_word(word, topn=10)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取某词语最相似的10个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.similar_by_vector(vector, topn=10)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取词向量最相似的10个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;421-词表&#34;&gt;4.2.1 词表&lt;/h3&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;的&amp;#39;,
 &amp;#39;了&amp;#39;,
 &amp;#39;在&amp;#39;,
...
 &amp;#39;引力&amp;#39;,
 &amp;#39;所说&amp;#39;,
 &amp;#39;星际&amp;#39;,
 ...]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;422-词表映射&#34;&gt;4.2.2 词表映射&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;的&amp;#39;: 0,
 &amp;#39;了&amp;#39;: 1,
 &amp;#39;在&amp;#39;: 2,
...
 &amp;#39;引力&amp;#39;: 997,
 &amp;#39;所说&amp;#39;: 998,
 &amp;#39;星际&amp;#39;: 999,
 ...}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;423-向量维度数&#34;&gt;4.2.3 向量维度数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;词表有 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 个词&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;向量是 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 维&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;词表有 4365 个词
向量是 50 维
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;424-获取词向量&#34;&gt;4.2.4 获取词向量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查看「降临」的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;降临&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([ 0.672314,  0.020081,  0.653733,  0.598732, -0.680517, -0.049689,
       -0.16845 , -0.06759 , -0.147955,  0.024006,  0.264551, -0.050127,
        0.252063, -0.475633,  0.103722, -0.012481,  0.040755,  1.154912,
        0.742695,  0.048619, -0.514424, -1.184054,  0.515892, -0.1034  ,
        0.368755, -0.690357, -0.784287, -0.505814,  0.035807, -0.166354,
       -0.26149 ,  0.015089,  0.10626 , -0.215666, -0.374001, -0.123558,
        0.422617, -0.075277, -0.316387, -0.484295,  0.059687,  0.132621,
        0.192094, -0.591919,  0.236281,  0.164198, -0.058724,  1.285457,
        0.905606, -0.52032 ], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;425-近义词&#34;&gt;4.2.5 近义词&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;三体&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;叛军&amp;#39;, 0.7699569463729858),
 (&amp;#39;更新&amp;#39;, 0.7687217593193054),
 (&amp;#39;地球&amp;#39;, 0.760529100894928),
 (&amp;#39;全集&amp;#39;, 0.7575182914733887),
 (&amp;#39;最快&amp;#39;, 0.7426372170448303),
 (&amp;#39;世界&amp;#39;, 0.7262137532234192),
 (&amp;#39;最新&amp;#39;, 0.7219281792640686),
 (&amp;#39;游戏&amp;#39;, 0.7180070877075195),
 (&amp;#39;危机&amp;#39;, 0.7020451426506042),
 (&amp;#39;教&amp;#39;, 0.7012627720832825)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;426-计算多个词的中心向量&#34;&gt;4.2.6 计算多个词的中心向量&lt;/h3&gt;
&lt;p&gt;我们可以计算「三体」、「降临」、「组织」、「拯救」的中心向量eto_vector。 并试图寻找中心向量eto_vector的最相似的10个词。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;eto_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;semantic_centroid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;三体&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;降临&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;组织&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;拯救&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eto_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 寻找 eto_vector 语义最相似的10个词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eto_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[ 0.6267875   0.08975425  0.48438451  0.405128   -0.49928901  0.11347825
 -0.90057975  0.11877625 -0.27053049  0.344603    0.4368495  -0.3839495
  0.02633176 -0.138534    0.2531555  -0.0060905  -0.48776849  0.75548999
  0.72575876 -0.446079   -0.30361701 -1.039792    0.457687   -0.4286315
  0.44577325 -0.39119426 -0.4783935  -0.2596135  -0.32513325 -0.10315975
 -0.42880575 -0.48328425  0.129438   -0.17085625 -0.13454625 -0.070053
  0.68060375  0.16736924 -0.15664874 -0.20528575  0.385481    0.206432
  0.18913225 -0.93453825  0.58597099  0.60727924  0.009064    0.87661726
  0.65814423 -0.356567  ]

[(&amp;#39;降临&amp;#39;, 0.8707027435302734),
 (&amp;#39;组织&amp;#39;, 0.8625670671463013),
 (&amp;#39;三体&amp;#39;, 0.8621653914451599),
 (&amp;#39;派&amp;#39;, 0.8343338966369629),
 (&amp;#39;拯救&amp;#39;, 0.8301094174385071),
 (&amp;#39;叛军&amp;#39;, 0.784512460231781),
 (&amp;#39;地球&amp;#39;, 0.7536635398864746),
 (&amp;#39;世界&amp;#39;, 0.7245718836784363),
 (&amp;#39;外部&amp;#39;, 0.7078365087509155),
 (&amp;#39;入侵&amp;#39;, 0.6962169408798218)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;熟悉三体的朋友应该能联想到背叛人类的ETO(地球三体组织)有两个派别，分别是拯救派和降临派。&lt;/p&gt;
&lt;p&gt;ETO开发了一款虚拟现实游戏，它向参与者展示了三体世界的真实情况，包括其恶劣的自然条件、三体文明的历史及其科技水平等。通过参与这个游戏，玩家们能够逐渐了解三体世界的真相，并最终决定是否要加入到支持三体文明入侵地球的行列中来。&lt;/p&gt;
&lt;p&gt;这个游戏不仅充当了信息传递的媒介，也是甄别志同道合者的工具，让那些对人类社会现状不满、渴望变革的人们找到了组织，进而成为了背叛人类的叛军一员。在这个过程中，“三体游戏”起到了关键的作用，是连接地球人与三体世界的重要桥梁。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cntext.readthedocs.io/&#34;&gt;文本分析库cntext2.x使用手册 https://cntext.readthedocs.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/&#34;&gt;词向量 | 使用人民网领导留言板语料训练Word2Vec模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/&#34;&gt;使用 5000w 专利申请数据集按年份(按省份)训练词向量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/&#34;&gt;使用 1000w 条豆瓣影评训练 Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/&#34;&gt;转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/&#34;&gt;OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一简介">一、简介</h2>
<p><a href="https://nlp.stanford.edu/projects/glove/">Stanford GloVe</a>（Global Vectors for Word Representation）算法作为一种融合全局统计信息与局部上下文窗口的词嵌入模型，相较于Word2Vec仅依赖局部上下文，GloVe利用全局统计信息，能更精准地反映词频分布特征。例如，在高维词向量（如200D）中，GloVe在词语类比任务中准确率达75%，并在命名实体识别任务中优于其他词嵌入模型。因其高效的语义表征能力，在社会学、管理学等领域展现出广泛的应用价值。 相关词嵌入文献资料可阅读</p>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
<li><a href="https://textdata.cn/blog/2022-04-01-embeddings-and-attitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
</ul>
<p><br><br></p>
<h2 id="二环境准备">二、环境准备</h2>
<p>cntext2.x 内置了 GloVe 训练所需的环境，支持 win 和 mac。</p>
<p>获取<a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">cntext2.x</a> 的安装文件 <em><strong>cntext-2.1.5-py3-none-any.whl</strong></em>，并将该whl文件放置于桌面。执行以下安装命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
pip install cntext-2.1.5-py3-none-any.whl
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">GloVe</span><span class="p">(</span><span class="n">corpus_file</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span> <span class="n">dict_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stopwords_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_memory</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">x_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><ul>
<li><em><strong>corpus_file</strong></em>: 输入语料文件路径（文本格式）。该文件为分词后的语料文件。</li>
<li><em><strong>lang</strong></em>: 语料文件的语言类型，默认为 &lsquo;chinese&rsquo;。</li>
<li><em><strong>dict_file</strong></em>: 自定义词典txt文件路径，默认为None。utf-8编码。</li>
<li><em><strong>stopwords_file</strong></em>: 停用词文件路径，默认为 None。utf-8编码。</li>
<li><em><strong>vector_size</strong></em>: 词向量维度，默认 100。</li>
<li><em><strong>window_size</strong></em>: 上下文窗口大小，默认 15。</li>
<li><em><strong>min_count</strong></em>: 忽略出现次数低于此值的单词，默认 5。</li>
<li><em><strong>max_memory</strong></em>: 可供使用的最大内存大小，单位为GB，默认 4;  该参数越大，训练越快。</li>
<li><em><strong>max_iter</strong></em>: 训练的最大迭代次数，默认 15。</li>
<li><em><strong>x_max</strong></em>: 共现矩阵中元素的最大计数值，默认 10。</li>
</ul>
<p><br><br></p>
<h2 id="三训练中文glove">三、训练中文GloVe</h2>
<p>我们其实只需要设置 <em><strong>corpus_file</strong></em> 和 <em><strong>lang</strong></em>， 但为了让大家知道</p>
<ul>
<li>上下文的窗口大小 <em><strong>window_size</strong></em></li>
<li>训练出模型词语的维度数 <em><strong>vector_size</strong></em></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 简化版调用。训练window_size=100维， vector_size=15</span>
<span class="c1"># glove_wv = ct.GloVe(corpus_file=&#39;data/三体.txt&#39;, lang=&#39;chinese&#39;)</span>

<span class="c1"># 正常调用。训练window_size=15维， vector_size=50</span>
<span class="n">glove_wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">GloVe</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;data/三体.txt&#39;</span><span class="p">,</span> 
                    <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span>
                    <span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                    <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                    <span class="n">only_binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># 同时保存txt和bin两种格式的模型文件</span>

<span class="n">glove_wv</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/三体_cache.txt Not Found or Empty, Preprocessing Corpus
Start Training GloVe
BUILDING VOCABULARY
Using vocabulary of size 6975.

COUNTING COOCCURRENCES
Merging cooccurrence files: processed 2106999 lines.

Using random seed 1743474106
SHUFFLING COOCCURRENCES
Merging temp files: processed 2106999 lines.

TRAINING MODEL
Read 2106999 lines.
Using random seed 1743474106
04/01/25 - 10:21.46AM, iter: 001, cost: 0.055981
04/01/25 - 10:21.46AM, iter: 002, cost: 0.050632
......
04/01/25 - 10:21.48AM, iter: 014, cost: 0.030047
04/01/25 - 10:21.48AM, iter: 015, cost: 0.029100

GloVe Training Cost 9 s. 
Output Saved To: output/三体-GloVe.50.15.txt
Output Saved To: output/三体-GloVe.50.15.bin
&lt;gensim.models.keyedvectors.KeyedVectors at 0x331517440&gt;

</code></pre></div><p><img loading="lazy" src="img/05-glove.png" alt=""  />
</p>
<br>
<h2 id="四使用中文glove模型">四、使用中文GloVe模型</h2>
<h3 id="41-加载模型">4.1 加载模型</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 加载word2vec模型.txt文件</span>
<span class="n">wv_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/三体-GloVe.50.15.bin&#39;</span><span class="p">)</span>
<span class="n">wv_model</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x336ff8dd0&gt;
</code></pre></div><br>
<h3 id="42-keyedvectors的操作方法或属性">4.2 KeyedVectors的操作方法(或属性)</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>KeyedVectors.index_to_key</strong></em></td>
<td>获取词汇表中的所有单词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.key_to_index</strong></em></td>
<td>获取单词到索引的映射。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.vector_size</strong></em></td>
<td>获取GloVe模型中任意词向量的维度。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.get_vector(word)</strong></em></td>
<td>获取给定单词的词向量。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_word(word, topn=10)</strong></em></td>
<td>获取某词语最相似的10个近义词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_vector(vector, topn=10)</strong></em></td>
<td>获取词向量最相似的10个近义词。</td>
</tr>
<tr>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>
<h3 id="421-词表">4.2.1 词表</h3>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wv_model</span><span class="o">.</span><span class="n">index_to_key</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;的&#39;,
 &#39;了&#39;,
 &#39;在&#39;,
...
 &#39;引力&#39;,
 &#39;所说&#39;,
 &#39;星际&#39;,
 ...]

</code></pre></div><br>
<h3 id="422-词表映射">4.2.2 词表映射</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wv_model</span><span class="o">.</span><span class="n">key_to_index</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;的&#39;: 0,
 &#39;了&#39;: 1,
 &#39;在&#39;: 2,
...
 &#39;引力&#39;: 997,
 &#39;所说&#39;: 998,
 &#39;星际&#39;: 999,
 ...}
</code></pre></div><br>
<h3 id="423-向量维度数">4.2.3 向量维度数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;词表有 </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">wv_model</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span><span class="si">}</span><span class="s1"> 个词&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;向量是 </span><span class="si">{</span><span class="n">wv_model</span><span class="o">.</span><span class="n">vector_size</span><span class="si">}</span><span class="s1"> 维&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">词表有 4365 个词
向量是 50 维
</code></pre></div><br>
<h3 id="424-获取词向量">4.2.4 获取词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查看「降临」的词向量</span>
<span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;降临&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 0.672314,  0.020081,  0.653733,  0.598732, -0.680517, -0.049689,
       -0.16845 , -0.06759 , -0.147955,  0.024006,  0.264551, -0.050127,
        0.252063, -0.475633,  0.103722, -0.012481,  0.040755,  1.154912,
        0.742695,  0.048619, -0.514424, -1.184054,  0.515892, -0.1034  ,
        0.368755, -0.690357, -0.784287, -0.505814,  0.035807, -0.166354,
       -0.26149 ,  0.015089,  0.10626 , -0.215666, -0.374001, -0.123558,
        0.422617, -0.075277, -0.316387, -0.484295,  0.059687,  0.132621,
        0.192094, -0.591919,  0.236281,  0.164198, -0.058724,  1.285457,
        0.905606, -0.52032 ], dtype=float32)
</code></pre></div><br>
<h3 id="425-近义词">4.2.5 近义词</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="s1">&#39;三体&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;叛军&#39;, 0.7699569463729858),
 (&#39;更新&#39;, 0.7687217593193054),
 (&#39;地球&#39;, 0.760529100894928),
 (&#39;全集&#39;, 0.7575182914733887),
 (&#39;最快&#39;, 0.7426372170448303),
 (&#39;世界&#39;, 0.7262137532234192),
 (&#39;最新&#39;, 0.7219281792640686),
 (&#39;游戏&#39;, 0.7180070877075195),
 (&#39;危机&#39;, 0.7020451426506042),
 (&#39;教&#39;, 0.7012627720832825)]
</code></pre></div><br>
<h3 id="426-计算多个词的中心向量">4.2.6 计算多个词的中心向量</h3>
<p>我们可以计算「三体」、「降临」、「组织」、「拯救」的中心向量eto_vector。 并试图寻找中心向量eto_vector的最相似的10个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">eto_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_centroid</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> <span class="n">words</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;三体&#39;</span><span class="p">,</span> <span class="s1">&#39;降临&#39;</span><span class="p">,</span> <span class="s1">&#39;组织&#39;</span><span class="p">,</span> <span class="s1">&#39;拯救&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">eto_vector</span><span class="p">)</span>
<span class="c1"># 寻找 eto_vector 语义最相似的10个词</span>
<span class="n">wv</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">eto_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[ 0.6267875   0.08975425  0.48438451  0.405128   -0.49928901  0.11347825
 -0.90057975  0.11877625 -0.27053049  0.344603    0.4368495  -0.3839495
  0.02633176 -0.138534    0.2531555  -0.0060905  -0.48776849  0.75548999
  0.72575876 -0.446079   -0.30361701 -1.039792    0.457687   -0.4286315
  0.44577325 -0.39119426 -0.4783935  -0.2596135  -0.32513325 -0.10315975
 -0.42880575 -0.48328425  0.129438   -0.17085625 -0.13454625 -0.070053
  0.68060375  0.16736924 -0.15664874 -0.20528575  0.385481    0.206432
  0.18913225 -0.93453825  0.58597099  0.60727924  0.009064    0.87661726
  0.65814423 -0.356567  ]

[(&#39;降临&#39;, 0.8707027435302734),
 (&#39;组织&#39;, 0.8625670671463013),
 (&#39;三体&#39;, 0.8621653914451599),
 (&#39;派&#39;, 0.8343338966369629),
 (&#39;拯救&#39;, 0.8301094174385071),
 (&#39;叛军&#39;, 0.784512460231781),
 (&#39;地球&#39;, 0.7536635398864746),
 (&#39;世界&#39;, 0.7245718836784363),
 (&#39;外部&#39;, 0.7078365087509155),
 (&#39;入侵&#39;, 0.6962169408798218)]
</code></pre></div><br>
<p>熟悉三体的朋友应该能联想到背叛人类的ETO(地球三体组织)有两个派别，分别是拯救派和降临派。</p>
<p>ETO开发了一款虚拟现实游戏，它向参与者展示了三体世界的真实情况，包括其恶劣的自然条件、三体文明的历史及其科技水平等。通过参与这个游戏，玩家们能够逐渐了解三体世界的真相，并最终决定是否要加入到支持三体文明入侵地球的行列中来。</p>
<p>这个游戏不仅充当了信息传递的媒介，也是甄别志同道合者的工具，让那些对人类社会现状不满、渴望变革的人们找到了组织，进而成为了背叛人类的叛军一员。在这个过程中，“三体游戏”起到了关键的作用，是连接地球人与三体世界的重要桥梁。</p>
<p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://cntext.readthedocs.io/">文本分析库cntext2.x使用手册 https://cntext.readthedocs.io/</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量 | 使用人民网领导留言板语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">使用 5000w 专利申请数据集按年份(按省份)训练词向量</a></li>
<li><a href="https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/">使用 1000w 条豆瓣影评训练 Word2Vec</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>教程 | 使用大模型将文本数据转化为结构化数据</title>
      <link>https://textdata.cn/blog/2025-02-14-using-online-large-model-api-to-transform-text-data-into-structured-data/</link>
      <pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2025-02-14-using-online-large-model-api-to-transform-text-data-into-structured-data/</guid>
      <description>实验数据为外卖评论， 今天咱们做个有难度的文本分析任务，从不同维度(味道、速度、服务)对外卖评论进行打分(-1.0~1.0)文本分析（也称为文本挖掘或自然语言处理，NLP）是指使用计算机算法和技术从大量文本数据中提取有价值信息的过程。文本分析的目标是从非结构化的文本数据中识别模式、提取关键信息、理解语义，并将其转化为结构化数据以便进一步分析和应用。</description>
      <content:encoded><![CDATA[<p>实验数据为外卖评论， 今天咱们做个有难度的文本分析任务，从不同维度(味道、速度、服务)对外卖评论进行打分(-1.0~1.0)。</p>
<p><img loading="lazy" src="img/06-df.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="一文本分析">一、文本分析</h2>
<p><strong>文本分析</strong>（也称为<strong>文本挖掘</strong>或<strong>自然语言处理</strong>，NLP）是指使用计算机算法和技术从大量文本数据中提取有价值信息的过程。文本分析的目标是从非结构化的文本数据中识别模式、提取关键信息、理解语义，并将其转化为结构化数据以便进一步分析和应用。 常用的文本分析方法有:</p>
<ul>
<li>词频统计</li>
<li>情感分析</li>
<li>文本分类</li>
<li>话题分析</li>
<li>&hellip;</li>
</ul>
<p><img loading="lazy" src="img/text-2-structured-data.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="二大模型云服务商">二、大模型云服务商</h2>
<p>随着 chatGPT、deepseek、通义千问这类大语言模型(<strong><em>LLM</em></strong>, large language model)的出现， 它们增强了文本理解能力，能够更精准的把握文本中的语义和情绪等信息，使得文本分析任务实现难度大大降低。</p>
<p>一般大模型服务提供商，有免费开源和封闭付费两种服务。</p>
<ul>
<li>免费模型， 可通过 <strong>Ollama</strong> 本地部署。部署教程可参考 <a href="https://textdata.cn/blog/2024-06-14-how-to-download-large-language-model-with-ollama/"><strong>教程 | 如何使用 Ollama 下载 &amp; 使用本地大语言模型</strong></a></li>
<li>付费模型， 账户有钱的情况下， 通过联网调用大模型厂商的 API 接口。</li>
</ul>
<p>使用 Python 代码， 联网调用大模型的 API，我们首先需要确定三个</p>
<ul>
<li><strong><em>BASE_URL</em></strong> 服务提供商运行大模型的网址。 如果是本地离线， BASE_URL = ''</li>
<li><strong><em>API_KEY</em></strong> 调用服务所需密钥，类似于钥匙</li>
<li><strong><em>MODEL_NAME</em></strong> 调用哪种模型(名字)</li>
</ul>
<p>阿里云不需要注册，支付宝扫码登录，即可调用市面上常见的大模型，如**<em>通义千问 qwen</em><strong>、</strong><em>Llama</em><strong>、</strong><em>deepseek</em><strong>、</strong><em>chatGLM</em><strong>等。现在我们以阿里云服务商为例， 依次获取</strong><em>BASE_URL</em><strong>、</strong><em>API_KEY</em><strong>、</strong><em>MODEL_NAME</em>**。</p>
<br>
<h3 id="21-充钱">2.1 充钱</h3>
<p><a href="https://billing-cost.console.aliyun.com/home">阿里云</a>替咱们在云服务商运行大模型，肯定不能是免费的。 所以先检查下账号里是否有钱，没钱了记得充值哦。 点击链接 <a href="https://billing-cost.console.aliyun.com/home">https://billing-cost.console.aliyun.com/home</a></p>
<p><img loading="lazy" src="img/02-charge.png" alt=""  />
</p>
<br>
<h3 id="22-base_url">2.2 BASE_URL</h3>
<p>阿里云运行大模型的网址 <strong><em>BASE_URL</em></strong> 为 <code>https://dashscope.aliyuncs.com/compatible-mode/v1</code></p>
<br>
<h3 id="23-api_key">2.3 API_KEY</h3>
<p>点击 <a href="https://bailian.console.aliyun.com/">阿里云百炼 https://bailian.console.aliyun.com/</a>，打开后点击右上角<img loading="lazy" src="https://help-static-aliyun-doc.aliyuncs.com/assets/img/zh-CN/0278981271/p824758.png" alt="image"  />
图标，在下拉菜单中单击<strong>API-KEY</strong>。</p>
<p><img loading="lazy" src="img/01-bai-lian.png" alt=""  />
</p>
<br>
<p>在左侧导航栏，选择 <strong>全部 API-KEY</strong> 或 <strong>我的 API-KEY</strong> ，然后<strong>创建</strong>（图中位置 ①）或<strong>查看</strong>（图中位置 ②）<strong><em>API Key</em></strong>。</p>
<p><img loading="lazy" src="img/02-api-key.png" alt=""  />
</p>
<br>
<p><strong>注意:</strong> 请不要将 <strong><em>API Key</em></strong> 以任何方式公开，避免因未经授权的使用造成安全风险或资金损失。</p>
<br>
<h3 id="24-model_name">2.4 MODEL_NAME</h3>
<p><a href="https://help.aliyun.com/zh/model-studio/getting-started/models">通义千问的模型列表 https://help.aliyun.com/zh/model-studio/getting-started/models</a>， 根据任务需要，选择适合的模型。</p>
<p><img loading="lazy" src="img/03-model-name.png" alt=""  />
</p>
<p>上图仅展示了阿里云服务提供的部分大模型， 以通义千问旗舰模型为例， <strong><em>MODEL_NAME</em><strong>模型名分别为</strong><em>qwen-max</em></strong>、<strong><em>qwen-plus</em></strong>、<strong><em>qwen-turbo</em></strong>、<strong><em>qwen-long</em></strong>。</p>
<br>
<h2 id="三环境配置">三、环境配置</h2>
<p>在 Python 中调用大模型， 不论是本地离线 API 还是云服务 API， 先要配置好相应的环境。 本文使用**<em>Ollama+cntext2.x</em>**</p>
<h3 id="31-安装软件-ollama">3.1 安装软件 Ollama</h3>
<p><a href="https://ollama.ai/"><strong>Ollama</strong></a>是一款开源应用程序，可让您使用 MacOS、Linux 和 Windows 上的命令行界面在本地运行、创建和共享大型语言模型。</p>
<p>Ollama 可以直接从其库中访问各种 LLM，只需一个命令即可下载。下载后，只需执行一个命令即可开始使用。这对于工作量围绕终端窗口的用户非常有帮助。Ollama 的安装、配置、使用的详细教程可阅读 <a href="https://textdata.cn/blog/2024-06-14-how-to-download-large-language-model-with-ollama/"><strong>教程 | 如何使用 Ollama 下载 &amp; 使用本地大语言模型</strong></a></p>
<p><img loading="lazy" src="img/04-ollama-gui.png" alt=""  />
</p>
<br>
<h3 id="32-安装-cntext2x">3.2 安装 cntext2.x</h3>
<p><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">cntext2.x</a>是大邓开发的文本分析库， 内置了丰富的文本分析函数， 如词频统计、词典法情感分析、经济政策不确定性 epu 等， 大大降低了文本分析难度。 以本文大模型文本分析为例， Python 源代码需要 <strong>80+</strong> 行， 经过大邓封装， 使用 cntext2.x 内置函数 <strong><em>text_analysis_by_llm</em></strong> 仅需要不到 <strong>5</strong> 行代码。</p>
<p><strong><em>安装包 cntext-2.1.4-py3-none-any.whl</em></strong> 是付费内容(100 元)， 如需使用<strong>加微信: 372335839</strong>，备注「<strong>姓名-学校-专业-cntext</strong>」</p>
<p>所有 <strong><em>cntext2.x</em></strong> 安装方法类似， 以目前 <strong><em>cntext2.1.4</em></strong> 为例，将 <strong><em>cntext-2.1.4-py3-none-any.whl</em></strong> 放置于桌面，打开 <strong><em>cmd</em></strong> (苹果电脑打开 terminal)， 输入 <strong><em>cd desktop</em></strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <strong><em>cmd</em></strong> (苹果电脑打开 terminal) 中使用 <strong><em>pip3</em></strong> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext-2.1.4-py3-none-any.whl
</code></pre></div><p>需要注意， <strong>cntext2.x 使用环境为 Python3.8 及以上版本</strong>； 文章开头和文章末都有 <strong><em>cntext-2.1.4-py3-none-any.whl</em></strong> 获取方式说明。</p>
<p><br><br></p>
<h2 id="四实验代码">四、实验代码</h2>
<p><strong>实验数据为外卖评论， 今天咱们做个有难度的任务，从不同维度(味道、速度、服务)对外卖评论进行打分(-1.0~1.0)</strong>。</p>
<br>
<h3 id="41-读取数据">4.1 读取数据</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#构造实验数据</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;速度非常快，口味非常好， 服务非常棒！&#39;</span><span class="p">,</span>
        <span class="s1">&#39;送餐时间还是比较久&#39;</span><span class="p">,</span>
        <span class="s1">&#39;送单很快，菜也不错赞&#39;</span><span class="p">,</span>
        <span class="s1">&#39;太难吃了&#39;</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;comment&#39;</span><span class="p">])</span>

<span class="c1">#假设有外卖评论数据集data.csv， 文件内有字段comment， 直接读取数据。</span>
<span class="c1">#df = pd.read_csv(&#39;data.csv&#39;)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/05-df.png" alt=""  />
</p>
<br>
<h3 id="42-小实验">4.2 小实验</h3>
<h4 id="421-本地模型">4.2.1 本地模型</h4>
<p>使用 <strong><em>cntext2.x</em></strong> 调用本地电脑安装的大模型进行文本分析，不需要设置**<em>BASE_URL</em><strong>、</strong><em>API_KEY 这两个</em>**参数。</p>
<p>本节使用本地安装的模型， 先在命令行**<em>cmd</em>** (mac 对应 terminal) 中检查本地已安装的模型。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ollama list
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">NAME                       ID              SIZE      MODIFIED
qwen2.5:7b                 845dbda0ea48    4.7 GB    7 days ago
qwen2.5:3b                 357c53fb659c    1.9 GB    7 days ago
qwen2.5:0.5b               a8b0c5157701    397 MB    7 days ago
qwen2.5:1.5b               65ec06548149    986 MB    7 days ago
deepseek-r1:1.5b           a42b25d8c10a    1.1 GB    7 days ago
deepseek-r1:7b             0a8c26691023    4.7 GB    7 days ago
nomic-embed-text:latest    0a109f422b47    274 MB    9 months ago
</code></pre></div><br>
<p>在 <strong><em>cmd</em></strong> 中使用命令 <strong><em>ollama serve</em></strong> 启动本地服务。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ollama</span> <span class="n">serve</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2025/02/14 16:00:18 routes.go:1259: INFO server config env=&#34;map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/deng/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]&#34;
time=2025-02-07T16:00:18.551+08:00 level=INFO source=images.go:757 msg=&#34;total blobs: 11&#34;
time=2025-02-07T16:00:18.551+08:00 level=INFO source=images.go:764 msg=&#34;total unused blobs removed: 0&#34;
[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.

[GIN-debug] [WARNING] Running in &#34;debug&#34; mode. Switch to &#34;release&#34; mode in production.
 - using env:	export GIN_MODE=release
 - using code:	gin.SetMode(gin.ReleaseMode)
er.(*Server).GenerateRoutes.func1 (5 handlers)
......
time=2025-02-14T16:00:18.553+08:00 level=INFO source=routes.go:1339 msg=&#34;Dynamic LLM libraries&#34; runners=[metal]
time=2025-02-14T16:00:18.577+08:00 level=INFO source=types.go:131 msg=&#34;inference compute&#34; id=0 library=metal variant=&#34;&#34; compute=&#34;&#34; driver=0.0 name=&#34;&#34; total=&#34;72.0 GiB&#34; available=&#34;72.0 GiB&#34;

</code></pre></div><p><strong><em>cmd</em></strong> 之中出现上方信息，证明服务已经启动。 如果之前已经启动服务， 会看到信息</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Error: listen tcp 127.0.0.1:11434: bind: address already in use
</code></pre></div><p>接下来，我们在 Python 中调用模型 <strong><em>qwen2.5:7b</em></strong></p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">PROMPT</span> <span class="o">=</span> <span class="s1">&#39;从口味taste、速度speed、服务service三个维度， 对外卖评论内容进行文本分析， 分别返回不同维度的分值(分值范围-1.0 ~ 1.0)&#39;</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s1">&#39;qwen2.5:7b&#39;</span>

<span class="c1">#味道、速度、服务</span>
<span class="n">OUTPUT_FORMAT</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;taste&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="s1">&#39;speed&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="s1">&#39;service&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">}</span>

<span class="n">COMMENT_CONTENT</span> <span class="o">=</span> <span class="s1">&#39;太难吃了&#39;</span>

<span class="c1">#使用</span>
<span class="c1">#result = ct.analysis_by_llm(text=COMMENT_CONTENT,</span>
<span class="c1">#或</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">text_analysis_by_llm</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">COMMENT_CONTENT</span><span class="p">,</span>
                                 <span class="n">prompt</span><span class="o">=</span><span class="n">PROMPT</span><span class="p">,</span>
                                 <span class="n">model_name</span><span class="o">=</span><span class="n">MODEL_NAME</span><span class="p">,</span>
                                 <span class="n">output_format</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;taste&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                                                <span class="s1">&#39;speed&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                                                <span class="s1">&#39;service&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">},</span>
                                 <span class="n">max_retries</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                 <span class="n">return_df</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">result</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;taste&#39;: -1.0, &#39;speed&#39;: 0.0, &#39;service&#39;: 0.0}
</code></pre></div><br>
<h4 id="422-云服务商-api">4.2.2 云服务商 API</h4>
<p>使用 <strong><em>cntext2.x</em></strong> 调用云服务商大模型进行文本分析，需要设置**<em>BASE_URL</em><strong>、</strong><em>API_KEY</em>**等参数。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">PROMPT</span> <span class="o">=</span> <span class="s1">&#39;从口味taste、速度speed、服务service三个维度， 对外卖评论内容进行文本分析， 分别返回不同维度的分值(分值范围-1.0 ~ 1.0)&#39;</span>
<span class="n">BASE_URL</span> <span class="o">=</span> <span class="s1">&#39;https://dashscope.aliyuncs.com/compatible-mode/v1&#39;</span>
<span class="n">API_KEY</span> <span class="o">=</span> <span class="s1">&#39;你的API-KEY&#39;</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s1">&#39;qwen-max&#39;</span>

<span class="c1">#味道、速度、服务</span>
<span class="n">OUTPUT_FORMAT</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;taste&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="s1">&#39;speed&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="s1">&#39;service&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">}</span>

<span class="n">COMMENT_CONTENT</span> <span class="o">=</span> <span class="s1">&#39;太难吃了&#39;</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">analysis_by_llm</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">COMMENT_CONTENT</span><span class="p">,</span>
                            <span class="n">prompt</span><span class="o">=</span><span class="n">PROMPT</span><span class="p">,</span>
                            <span class="n">base_url</span><span class="o">=</span><span class="n">BASE_URL</span><span class="p">,</span>
                            <span class="n">api_key</span><span class="o">=</span><span class="n">API_KEY</span><span class="p">,</span>
                            <span class="n">model_name</span><span class="o">=</span><span class="n">MODEL_NAME</span><span class="p">,</span>
                            <span class="n">output_format</span><span class="o">=</span><span class="n">OUTPUT_FORMAT</span><span class="p">,</span>
                            <span class="n">max_retries</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                            <span class="n">return_df</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">result</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;taste&#39;: -1.0, &#39;speed&#39;: 0.0, &#39;service&#39;: 0.0}
</code></pre></div><p>小实验成功，现在设计分析函数， 对所有的评论进行分析，输出 dataframe 格式，保存到 csv 中。</p>
<br>
<h3 id="43-设计分析函数">4.3 设计分析函数</h3>
<p>使用 <strong>cntext2.x</strong> 中的大模型文本分析函数 <strong><em>analysis_by_llm(text, prompt, base_url, api_key, model_name, temperature, output_format, max_retries, return_df)</em></strong></p>
<ul>
<li><strong><em>text</em></strong>: 待分析的文本</li>
<li><strong><em>prompt</em></strong> 提示 Prompt, 默认 prompt=&ldquo;根据评论内容，返回文本的情感类别(pos、neg)&rdquo;, 可判断文本 pos 或 neg</li>
<li><strong><em>base_url</em></strong>: 大模型 API 接口， 默认 base_url=''， 默认使用的本地 Ollama 搭建服务的 API 接口；</li>
<li><strong><em>api_key</em></strong>: 大模型 API 对应的 KEY， 默认 api_key='' 表示使用的本地 Ollama 搭建服务</li>
<li><strong><em>model_name</em></strong>: 模型名；默认使用 model_name=&ldquo;qwen2.5:3b&rdquo;</li>
<li><strong><em>temperature</em></strong>: 控制模型输出结果的随机性，默认 temperature=0； 取值范围 0 到无穷, 常用的范围[0, 1]。虽然理论上可以设置大于 1 的值，但这样会导致输出过于随机，通常不推荐这样做。需要结合任务确定取值
<ul>
<li>高准确性一致性任务，如情感分析、文本分类、事实性回答， 建议 temperature=0</li>
<li>高创造性和多样性任务， 如故事写作、头脑风暴等， 建议 temperature=0.7</li>
<li>实验性或探索性任务，较高的 <code>temperature</code> 值（如 1.0 以上，但一般不推荐超过 2.0）</li>
</ul>
</li>
<li><strong><em>output_format</em></strong>: 设置分析结果的输出格式; 默认 output_format = {&lsquo;label&rsquo;: str, &lsquo;score&rsquo;: float}, 输出结果为字典， 含字段类别字段 label 和数值字段 score</li>
<li><strong><em>max_retries</em></strong>: 最大失败次数， 默认 max_retries=3</li>
<li><strong><em>return_df</em></strong>: 返回结果是否为 dataframe， 默认 False</li>
</ul>
<p>以调用云服务商大模型为例， 设计**<em>llm_analysis</em>**</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1">#分析函数</span>
<span class="k">def</span> <span class="nf">llm_analysis</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">analysis_by_llm</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
                                <span class="n">prompt</span><span class="o">=</span> <span class="s1">&#39;从口味taste、速度speed、服务service三个维度， 对外卖评论内容进行文本分析， 分别返回不同维度的分值(分值范围-1.0 ~ 1.0)&#39;</span><span class="p">,</span>
                                <span class="n">base_url</span><span class="o">=</span><span class="s1">&#39;https://dashscope.aliyuncs.com/compatible-mode/v1&#39;</span><span class="p">,</span>
                                <span class="n">api_key</span><span class="o">=</span><span class="s1">&#39;你的API-KEY&#39;</span><span class="p">,</span>
                                <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;qwen-max&#39;</span><span class="p">,</span>
                                <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                <span class="n">output_format</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;taste&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="s1">&#39;speed&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="s1">&#39;service&#39;</span><span class="p">:</span> <span class="nb">float</span><span class="p">}</span>
                               <span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>


<span class="c1">#批量运算</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;comment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">llm_analysis</span><span class="p">)</span>
<span class="n">res_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">df2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#保存分析结果</span>
<span class="n">res_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;result.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">res_df</span>
</code></pre></div><p><img loading="lazy" src="img/06-df.png" alt=""  />
</p>
<br>
<br>
<h2 id="五获取-cntext2x">五、获取 cntext2.x</h2>
<p>安装包**<em>cntext-2.1.4-py3-none-any.whl</em>** 是付费内容(<strong><em>100 元</em></strong>)， 如需使用<strong>加微信: 372335839</strong>，备注「<strong>姓名-学校-专业-cntext</strong>」</p>
<p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2025-02-17-gpt-is-an-effective-tool-for-multilingual-psychological-text-analysis/"><strong>PNAS | GPT 是多语言心理文本分析的有效工具</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-06-14-how-to-download-large-language-model-with-ollama/"><strong>教程 | 如何使用 Ollama 下载 &amp; 使用本地大语言模型</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-08-06-using-the-ollama-local-large-model-to-predict-the-sentiment-category-of-online-comments/"><strong>实验 | 使用本地大模型预测在线评论情感类别和分值</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-08-07-structured-outputs-with-ollama/"><strong>实验 | 如何使 Ollama 结构化输出 JSON 样式的结果</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/"><strong>推荐 | 文本分析库 cntext2.x 使用手册</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/"><strong>实验 | 使用本地大模型从文本中提取结构化信息</strong></a></li>
<li><a href="https://textdata.cn/blog/2024-07-10-using-large-language-model-to-build-diy-dictionary/">实验 | 使用 Ollama 本地大模型 DIY 制作单词书教案 PDF</a></li>
<li><a href="https://textdata.cn/blog/2024-08-05-create-a-blog-writer-multi-agent-system-using-crewai-and-ollama/">实验 | 使用 Crewai 和 Ollama 构建智能体(AI Agent)帮我撰写博客文章</a></li>
</ul>
<p><br><br></p>
<h2 id="精选内容">精选内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集汇总</a></li>
<li><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)数据挖掘文献资料汇总</a></li>
<li><a href="https://textdata.cn/blog/2024-06-16-scrapegraph-ai/">网络爬虫 | 使用 scrapegraph-ai(大模型方案)自动采集网页数据</a></li>
<li><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">推荐 | 文本分析库 cntext2.x 使用手册</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python 实证指标构建与文本分析</a></li>
<li><a href="https://textdata.cn/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/">实验 | 使用本地大模型从文本中提取结构化信息</a>
<br>
<br></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>实验 | 使用本地大模型从文本中提取结构化信息</title>
      <link>https://textdata.cn/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/</link>
      <pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-06-14-using-large-language-model-to-extract-structure-data-from-raw-text/</guid>
      <description>&lt;p&gt;非结构文本、图片、视频等数据是待挖掘的数据矿藏， 在经管、社科等研究领域中谁拥有了&lt;em&gt;&lt;strong&gt;从非结构提取结构化信息的能力&lt;/strong&gt;&lt;/em&gt;，谁就拥有科研上的数据优势。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一需求&#34;&gt;一、需求&lt;/h2&gt;
&lt;p&gt;现在有很多个电子发票PDF文件， 使用自动化工具帮我们批量自动从发票PDF提取出格式化信息。如从发票&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-raw-pdf.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;提取出DICT_DATA&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;DICT_DATA = {
    &amp;#34;开票日期&amp;#34;: &amp;#34;2023年01月06日&amp;#34;,
    &amp;#34;应税货物(或服务)名称&amp;#34;: &amp;#34;*信息技术服务*技术服务费&amp;#34;,
    &amp;#34;价税合计(大写)&amp;#34;: &amp;#34;&amp;#34;,
    &amp;#34;税率&amp;#34;: &amp;#34;6%&amp;#34;,
    &amp;#34;备注&amp;#34;: &amp;#34;230106163474406331&amp;#34;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二ollama介绍&#34;&gt;二、Ollama介绍&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://ollama.ai/&#34;&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/a&gt;是一款开源应用程序，可让您使用 MacOS、Linux 和 Windows 上的命令行界面在本地运行、创建和共享大型语言模型。&lt;/p&gt;
&lt;p&gt;Ollama 可以直接从其库中访问各种 LLM，只需一个命令即可下载。下载后，只需执行一个命令即可开始使用。这对于工作量围绕终端窗口的用户非常有帮助。如果他们被困在某个地方，他们可以在不切换到另一个浏览器窗口的情况下获得答案。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;21-特点和优点&#34;&gt;2.1 特点和优点&lt;/h3&gt;
&lt;p&gt;这就是为什么 OLLAMA 是您的工具包中必备的工具：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;简单&lt;/strong&gt; ：OLLAMA 提供简单的设置过程。您无需拥有机器学习博士学位即可启动和运行它。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成本效益&lt;/strong&gt; ：在本地运行模型意味着您无需支付云成本。您的钱包会感谢您。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隐私&lt;/strong&gt; ：使用 OLLAMA，所有数据处理都在您的本地机器上进行。这对于用户隐私来说是一个巨大的胜利。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多功能性&lt;/strong&gt; ：OLLAMA 不只是为 Python 爱好者准备的。它的灵活性使其可以用于各种应用程序，包括 Web 开发。&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-使用-ollama-进行-llm-选择&#34;&gt;2.2 使用 Ollama 进行 LLM 选择&lt;/h3&gt;
&lt;p&gt;默认情况下，Openai Models 在 CrewAI 中用作 llm。有经费、有网络、不担心数据泄露等条件下,  力求达到最佳性能，可考虑使用 GPT-4 或 OpenAI 稍便宜的 GPT-3.5。&lt;/p&gt;
&lt;p&gt;但本文是要 &lt;strong&gt;本地部署&lt;/strong&gt;， 因此我们将使用 Meta Llama 3，这是迄今为止功能最强大的公开 LLM。Meta Llama 3 是 Meta Inc. 开发的模型系列，是最新推出的模型，具有 8B 和 70B 两种参数大小（预训练或指令调整）。Llama 3 指令调整模型针对对话/聊天用例进行了微调和优化，并且在常见基准测试中胜过许多可用的开源聊天模型。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/02-llama3-performance.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/03-llama3-performance.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-安装ollama&#34;&gt;2.3 安装ollama&lt;/h3&gt;
&lt;p&gt;点击前往网站 &lt;a href=&#34;https://ollama.com/&#34;&gt;https://ollama.com/&lt;/a&gt; ，下载ollama软件，支持win、Mac、linux&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/04-ollama-gui.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;24-下载llm&#34;&gt;2.4 下载LLM&lt;/h3&gt;
&lt;p&gt;ollama软件目前支持多种大模型， 如阿里的（qwen、qwen2）、meta的(llama3、llama3.1)，&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-ollama-model.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;以llama3为例，根据自己电脑显存性能， 选择适宜的版本。如果不知道选什么，那就试着安装，不合适不能用再删除即可。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/06-ollama-llama3.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行模型下载(安装)命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ollama pull llama3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;等待 &lt;strong&gt;llama3:8b&lt;/strong&gt; 下载完成。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;25-安装python包&#34;&gt;2.5 安装python包&lt;/h3&gt;
&lt;p&gt;在python中调用ollama服务，需要ollama包。&lt;/p&gt;
&lt;p&gt;打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行安装命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install ollama
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;26-启动ollama服务&#34;&gt;2.6 启动ollama服务&lt;/h3&gt;
&lt;p&gt;在Python中调用本地ollama服务，需要先启动本地ollama服务， 打开电脑命令行cmd(mac是terminal), 执行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ollama serve
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2024/06/14 14:52:24 routes.go:1011: INFO server config env=&amp;#34;map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/Users/deng/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]&amp;#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:725 msg=&amp;#34;total blobs: 18&amp;#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:732 msg=&amp;#34;total unused blobs removed: 0&amp;#34;
time=2024-06-14T14:52:24.743+08:00 level=INFO source=routes.go:1057 msg=&amp;#34;Listening on 127.0.0.1:11434 (version 0.1.44)&amp;#34;
time=2024-06-14T14:52:24.744+08:00 level=INFO source=payload.go:30 msg=&amp;#34;extracting embedded files&amp;#34; dir=/var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/ollama4239159529/runners
time=2024-06-14T14:52:24.772+08:00 level=INFO source=payload.go:44 msg=&amp;#34;Dynamic LLM libraries [metal]&amp;#34;
time=2024-06-14T14:52:24.796+08:00 level=INFO source=types.go:71 msg=&amp;#34;inference compute&amp;#34; id=0 library=metal compute=&amp;#34;&amp;#34; driver=0.0 name=&amp;#34;&amp;#34; total=&amp;#34;72.0 GiB&amp;#34; available=&amp;#34;72.0 GiB&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;cmd(mac是terminal)看到如上的信息，说明本地ollama服务已开启。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三实验&#34;&gt;三、实验&lt;/h2&gt;
&lt;h3 id=&#34;31-代码结构&#34;&gt;3.1 代码结构&lt;/h3&gt;
&lt;p&gt;点击下载 &lt;a href=&#34;project.zip&#34;&gt;本文代码&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;project
   |
  - 代码.ipynb   #代码
  - prompt.txt  #提示模板
  - data
      |--- 1.pdf #实验的发票
  - result.csv   #结果
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;32-读取pdf&#34;&gt;3.2 读取pdf&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-raw-pdf.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#cntext版本为2.1.3，非开源， #需联系大邓372335839获取&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_pdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/1.pdf&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__version__&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;2.1.3

&amp;#39; 北京增值税电子普通发票发票代码： \n发票号码： 69453658\n开票日期： 2023年01月06日\n校 验 码： \n购\n买\n方名        称： 哈尔滨所以然信息技术有限公司\n密\n码\n区030898/5&amp;lt;32&amp;gt;*/0*440/63+79*08\n纳税人识别号： 91230109MABT7KBC4M /&amp;lt;54&amp;lt;1*6+49&amp;lt;-*+*&amp;gt;7&amp;lt;-8*04&amp;lt;+01\n地 址、电 话： 68+160026-45904*2&amp;lt;+3+15503&amp;gt;2\n开户行及账号： 98*2/*-*480145+-19*0917-1*61\n货物或应税劳务、服务名称 规格型号 单 位 数 量 单 价 金 额 税率 税 额\n*信息技术服务*技术服务费 1248.113208 248.11 6% 14.89\n合      计 ￥248.11 ￥14.89\n价税合计（大写）\n  贰佰陆拾叁元整             （小写）￥263.00\n销\n售\n方名        称： \n备\n注230106163474406331\n纳税人识别号： 91110108MA01WFY0X6\n地 址、电 话： \n开户行及账号： \n  收款人： 复核： 开票人： 销售方：（章）&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;34-提取信息&#34;&gt;3.4 提取信息&lt;/h3&gt;
&lt;p&gt;使用ollama服务中的大模型 &lt;em&gt;&lt;strong&gt;llama3:8b&lt;/strong&gt;&lt;/em&gt; , 需要大模型提示信息及数据。这是我实验里设计的提示信息prompt&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;prompt = open(&amp;#39;prompt.txt&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;).read()
print(prompt)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;发票文本内容
--- 
{TEXT} 
---

以 JSON 格式回答。 JSON 应包含如下信息， 依次为&amp;#34;开票日期&amp;#34;, &amp;#34;应税货物(或服务)名称&amp;#34;, &amp;#34;价税合计(大写)&amp;#34;, &amp;#34;税率&amp;#34;, &amp;#34;备注&amp;#34;; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;

&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ollama&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#cntext版本为2.1.3，非开源， 需联系大邓372335839获取&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#读取发票pdf&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_pdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/1.pdf&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#读取prompt&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;prompt.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;llama3:8b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
      &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;system&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
      &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;eval&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;```&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;```&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;CPU times: user 20.5 ms, sys: 2.34 ms, total: 22.9 ms
Wall time: 3.58 s

{&amp;#39;开票日期&amp;#39;: &amp;#39;2023年01月06日&amp;#39;,
 &amp;#39;应税货物(或服务)名称&amp;#39;: &amp;#39;*信息技术服务*技术服务费&amp;#39;,
 &amp;#39;价税合计(大写)&amp;#39;: &amp;#39;贰佰陆拾叁元整&amp;#39;,
 &amp;#39;税率&amp;#39;: &amp;#39;6%&amp;#39;,
 &amp;#39;备注&amp;#39;: &amp;#39;230106163474406331&amp;#39;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;33-封装成函数extract_info&#34;&gt;3.3 封装成函数extract_info&lt;/h3&gt;
&lt;p&gt;实验成功，我们将其封装为函数&lt;em&gt;&lt;strong&gt;extract_info&lt;/strong&gt;&lt;/em&gt;， 为增强代码的鲁棒性， 函数内设置了异常处理机制，最多可重试3次。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ollama&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;re&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;extract_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_retries&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attempt&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_retries&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;llama3:8b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
                    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;system&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
                    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
                &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
            &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

            &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;eval&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;```&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;```&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;
        
        &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attempt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_retries&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;An error occurred: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;. Retrying (&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;attempt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_retries&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;)...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;
  
&lt;span class=&#34;c1&#34;&gt;#读取prompt&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;prompt.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;extract_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;result与之前无异， 为了节省版面，这里就不显示result。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;34-批量提取&#34;&gt;3.4 批量提取&lt;/h3&gt;
&lt;p&gt;假设data文件夹内有成百上千的发票(实际上只有一张发票)， 对data文件夹进行批量信息提取，结果存储为csv。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;

&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ollama&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#cntext版本为2.1.3，非开源， 需联系大邓372335839获取&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;extract_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_retries&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attempt&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_retries&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ollama&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;chat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;llama3:8b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                &lt;span class=&#34;n&#34;&gt;messages&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
                    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;system&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
                    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;role&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
                &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
            &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

            &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;eval&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;```&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;```&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;result&lt;/span&gt;
        
        &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;Exception&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attempt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_retries&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;An error occurred: &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;. Retrying (&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;attempt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_retries&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;)...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
                &lt;span class=&#34;k&#34;&gt;raise&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;
                
  
&lt;span class=&#34;c1&#34;&gt;#当前代码所在的代码文件与data文件夹处于同一个文件夹内&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#获取data内所有pdf的路径&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;pdf_files&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;file&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;os&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;listdir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;.pdf&amp;#39;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;#读取prompt&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;prompt.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;dict_datas&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pdf_file&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pdf_files&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_pdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pdf_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dict_data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;extract_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;dict_datas&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dict_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dict_datas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;result.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;CPU times: user 32 ms, sys: 2.17 ms, total: 15.2 ms
Wall time: 3.8 s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四讨论&#34;&gt;四、讨论&lt;/h2&gt;
&lt;p&gt;本文只使用了一张发票进行实验， 实际上准确率没有这么高， 识别错误字段集中在销售方纳税识别号(案例没有展示销售方纳税识别号的识别)。原因主要是ct.read_pdf读入pdf时，文本比较杂乱。对大模型的语义理解有一定的挑战。目前大模型已经支持文本、图片、音频、视频、网址， 所以各位看官，不用等太久，就可克服此问题。&lt;/p&gt;
&lt;p&gt;大模型会对每个输入，给出正确概率最大的回答，因此大模型提取数据时存在一定的错误识别风险。为降低该风险，尽量选择特别特殊、显眼，例如三张发票的&lt;strong&gt;价税合计(大写)&lt;/strong&gt;,  因为信息是特殊的中文大写数字， 在所有文本中是最醒目最特别的文本信息，这样大模型处理这类信息时会给这类信息尽可能高的权重，增大回答的准确率。&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;精选内容&#34;&gt;精选内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/datasets_available_for_management_science/&#34;&gt;LIST | 可供社科(经管)领域使用的数据集汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/the_text_analysis_list_about_ms/&#34;&gt;LIST | 社科(经管)数据挖掘文献资料汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-06-16-scrapegraph-ai/&#34;&gt;网络爬虫 | 使用scrapegraph-ai(大模型方案)自动采集网页数据&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/&#34;&gt;推荐 | 文本分析库cntext2.x使用手册&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/management_python_course/&#34;&gt;付费视频课 | Python实证指标构建与文本分析&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<p>非结构文本、图片、视频等数据是待挖掘的数据矿藏， 在经管、社科等研究领域中谁拥有了<em><strong>从非结构提取结构化信息的能力</strong></em>，谁就拥有科研上的数据优势。</p>
<p><br><br></p>
<h2 id="一需求">一、需求</h2>
<p>现在有很多个电子发票PDF文件， 使用自动化工具帮我们批量自动从发票PDF提取出格式化信息。如从发票</p>
<p><img loading="lazy" src="img/01-raw-pdf.png" alt=""  />
</p>
<p>提取出DICT_DATA</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">DICT_DATA = {
    &#34;开票日期&#34;: &#34;2023年01月06日&#34;,
    &#34;应税货物(或服务)名称&#34;: &#34;*信息技术服务*技术服务费&#34;,
    &#34;价税合计(大写)&#34;: &#34;&#34;,
    &#34;税率&#34;: &#34;6%&#34;,
    &#34;备注&#34;: &#34;230106163474406331&#34;
}
</code></pre></div><p><br><br></p>
<h2 id="二ollama介绍">二、Ollama介绍</h2>
<p><a href="https://ollama.ai/"><strong>Ollama</strong></a>是一款开源应用程序，可让您使用 MacOS、Linux 和 Windows 上的命令行界面在本地运行、创建和共享大型语言模型。</p>
<p>Ollama 可以直接从其库中访问各种 LLM，只需一个命令即可下载。下载后，只需执行一个命令即可开始使用。这对于工作量围绕终端窗口的用户非常有帮助。如果他们被困在某个地方，他们可以在不切换到另一个浏览器窗口的情况下获得答案。</p>
<br>
<h3 id="21-特点和优点">2.1 特点和优点</h3>
<p>这就是为什么 OLLAMA 是您的工具包中必备的工具：</p>
<ul>
<li><strong>简单</strong> ：OLLAMA 提供简单的设置过程。您无需拥有机器学习博士学位即可启动和运行它。</li>
<li><strong>成本效益</strong> ：在本地运行模型意味着您无需支付云成本。您的钱包会感谢您。</li>
<li><strong>隐私</strong> ：使用 OLLAMA，所有数据处理都在您的本地机器上进行。这对于用户隐私来说是一个巨大的胜利。</li>
<li><strong>多功能性</strong> ：OLLAMA 不只是为 Python 爱好者准备的。它的灵活性使其可以用于各种应用程序，包括 Web 开发。</li>
</ul>
<br>
<h3 id="22-使用-ollama-进行-llm-选择">2.2 使用 Ollama 进行 LLM 选择</h3>
<p>默认情况下，Openai Models 在 CrewAI 中用作 llm。有经费、有网络、不担心数据泄露等条件下,  力求达到最佳性能，可考虑使用 GPT-4 或 OpenAI 稍便宜的 GPT-3.5。</p>
<p>但本文是要 <strong>本地部署</strong>， 因此我们将使用 Meta Llama 3，这是迄今为止功能最强大的公开 LLM。Meta Llama 3 是 Meta Inc. 开发的模型系列，是最新推出的模型，具有 8B 和 70B 两种参数大小（预训练或指令调整）。Llama 3 指令调整模型针对对话/聊天用例进行了微调和优化，并且在常见基准测试中胜过许多可用的开源聊天模型。</p>
<p><img loading="lazy" src="img/02-llama3-performance.png" alt=""  />
</p>
<p><img loading="lazy" src="img/03-llama3-performance.png" alt=""  />
</p>
<br>
<h3 id="23-安装ollama">2.3 安装ollama</h3>
<p>点击前往网站 <a href="https://ollama.com/">https://ollama.com/</a> ，下载ollama软件，支持win、Mac、linux</p>
<p><img loading="lazy" src="img/04-ollama-gui.png" alt=""  />
</p>
<br>
<h3 id="24-下载llm">2.4 下载LLM</h3>
<p>ollama软件目前支持多种大模型， 如阿里的（qwen、qwen2）、meta的(llama3、llama3.1)，</p>
<p><img loading="lazy" src="img/05-ollama-model.png" alt=""  />
</p>
<br>
<p>以llama3为例，根据自己电脑显存性能， 选择适宜的版本。如果不知道选什么，那就试着安装，不合适不能用再删除即可。</p>
<p><img loading="lazy" src="img/06-ollama-llama3.png" alt=""  />
</p>
<br>
<p>打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行模型下载(安装)命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ollama pull llama3
</code></pre></div><p>等待 <strong>llama3:8b</strong> 下载完成。</p>
<br>
<h3 id="25-安装python包">2.5 安装python包</h3>
<p>在python中调用ollama服务，需要ollama包。</p>
<p>打开电脑命令行cmd(mac是terminal),  网络是连网状态，执行安装命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install ollama
</code></pre></div><br>
<h3 id="26-启动ollama服务">2.6 启动ollama服务</h3>
<p>在Python中调用本地ollama服务，需要先启动本地ollama服务， 打开电脑命令行cmd(mac是terminal), 执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ollama serve
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2024/06/14 14:52:24 routes.go:1011: INFO server config env=&#34;map[OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/Users/deng/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]&#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:725 msg=&#34;total blobs: 18&#34;
time=2024-06-14T14:52:24.742+08:00 level=INFO source=images.go:732 msg=&#34;total unused blobs removed: 0&#34;
time=2024-06-14T14:52:24.743+08:00 level=INFO source=routes.go:1057 msg=&#34;Listening on 127.0.0.1:11434 (version 0.1.44)&#34;
time=2024-06-14T14:52:24.744+08:00 level=INFO source=payload.go:30 msg=&#34;extracting embedded files&#34; dir=/var/folders/y0/4gqxky0s2t94x1c1qhlwr6100000gn/T/ollama4239159529/runners
time=2024-06-14T14:52:24.772+08:00 level=INFO source=payload.go:44 msg=&#34;Dynamic LLM libraries [metal]&#34;
time=2024-06-14T14:52:24.796+08:00 level=INFO source=types.go:71 msg=&#34;inference compute&#34; id=0 library=metal compute=&#34;&#34; driver=0.0 name=&#34;&#34; total=&#34;72.0 GiB&#34; available=&#34;72.0 GiB&#34;
</code></pre></div><p>cmd(mac是terminal)看到如上的信息，说明本地ollama服务已开启。</p>
<p><br><br></p>
<h2 id="三实验">三、实验</h2>
<h3 id="31-代码结构">3.1 代码结构</h3>
<p>点击下载 <a href="project.zip">本文代码</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">project
   |
  - 代码.ipynb   #代码
  - prompt.txt  #提示模板
  - data
      |--- 1.pdf #实验的发票
  - result.csv   #结果
</code></pre></div><br>
<h3 id="32-读取pdf">3.2 读取pdf</h3>
<p><img loading="lazy" src="img/01-raw-pdf.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="c1">#cntext版本为2.1.3，非开源， #需联系大邓372335839获取</span>

<span class="n">text</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;data/1.pdf&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ct</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="n">text</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">2.1.3

&#39; 北京增值税电子普通发票发票代码： \n发票号码： 69453658\n开票日期： 2023年01月06日\n校 验 码： \n购\n买\n方名        称： 哈尔滨所以然信息技术有限公司\n密\n码\n区030898/5&lt;32&gt;*/0*440/63+79*08\n纳税人识别号： 91230109MABT7KBC4M /&lt;54&lt;1*6+49&lt;-*+*&gt;7&lt;-8*04&lt;+01\n地 址、电 话： 68+160026-45904*2&lt;+3+15503&gt;2\n开户行及账号： 98*2/*-*480145+-19*0917-1*61\n货物或应税劳务、服务名称 规格型号 单 位 数 量 单 价 金 额 税率 税 额\n*信息技术服务*技术服务费 1248.113208 248.11 6% 14.89\n合      计 ￥248.11 ￥14.89\n价税合计（大写）\n  贰佰陆拾叁元整             （小写）￥263.00\n销\n售\n方名        称： \n备\n注230106163474406331\n纳税人识别号： 91110108MA01WFY0X6\n地 址、电 话： \n开户行及账号： \n  收款人： 复核： 开票人： 销售方：（章）&#39;
</code></pre></div><br>
<h3 id="34-提取信息">3.4 提取信息</h3>
<p>使用ollama服务中的大模型 <em><strong>llama3:8b</strong></em> , 需要大模型提示信息及数据。这是我实验里设计的提示信息prompt</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">prompt = open(&#39;prompt.txt&#39;, encoding=&#39;utf-8&#39;).read()
print(prompt)
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">发票文本内容
--- 
{TEXT} 
---

以 JSON 格式回答。 JSON 应包含如下信息， 依次为&#34;开票日期&#34;, &#34;应税货物(或服务)名称&#34;, &#34;价税合计(大写)&#34;, &#34;税率&#34;, &#34;备注&#34;; 
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>

<span class="kn">import</span> <span class="nn">ollama</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="c1">#cntext版本为2.1.3，非开源， 需联系大邓372335839获取</span>

<span class="c1">#读取发票pdf</span>
<span class="n">content</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;data/1.pdf&#39;</span><span class="p">)</span>
<span class="c1">#读取prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;prompt.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;llama3:8b&#39;</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
      <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span><span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">},</span>
      <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span><span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">content</span><span class="p">},</span>
    <span class="p">])</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
<span class="n">result</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;```</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">```&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">result</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">CPU times: user 20.5 ms, sys: 2.34 ms, total: 22.9 ms
Wall time: 3.58 s

{&#39;开票日期&#39;: &#39;2023年01月06日&#39;,
 &#39;应税货物(或服务)名称&#39;: &#39;*信息技术服务*技术服务费&#39;,
 &#39;价税合计(大写)&#39;: &#39;贰佰陆拾叁元整&#39;,
 &#39;税率&#39;: &#39;6%&#39;,
 &#39;备注&#39;: &#39;230106163474406331&#39;}
</code></pre></div><br>
<h3 id="33-封装成函数extract_info">3.3 封装成函数extract_info</h3>
<p>实验成功，我们将其封装为函数<em><strong>extract_info</strong></em>， 为增强代码的鲁棒性， 函数内设置了异常处理机制，最多可重试3次。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">ollama</span>
<span class="kn">import</span> <span class="nn">re</span>


<span class="k">def</span> <span class="nf">extract_info</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">max_retries</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_retries</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="s1">&#39;llama3:8b&#39;</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
                    <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">},</span>
                    <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">text</span><span class="p">}</span>
                <span class="p">]</span>
            <span class="p">)</span>

            <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
            <span class="n">result</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;```</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">```&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">result</span>
        
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">attempt</span> <span class="o">&lt;</span> <span class="n">max_retries</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;An error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. Retrying (</span><span class="si">{</span><span class="n">attempt</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">max_retries</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">)...&#34;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">e</span>
  
<span class="c1">#读取prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;prompt.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">extract_info</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>
<span class="n">result</span>
</code></pre></div><p>result与之前无异， 为了节省版面，这里就不显示result。</p>
<br>
<h3 id="34-批量提取">3.4 批量提取</h3>
<p>假设data文件夹内有成百上千的发票(实际上只有一张发票)， 对data文件夹进行批量信息提取，结果存储为csv。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">ollama</span>
<span class="c1">#cntext版本为2.1.3，非开源， 需联系大邓372335839获取</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>


<span class="k">def</span> <span class="nf">extract_info</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">max_retries</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_retries</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">ollama</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="s1">&#39;llama3:8b&#39;</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
                    <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;system&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">},</span>
                    <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">text</span><span class="p">}</span>
                <span class="p">]</span>
            <span class="p">)</span>

            <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
            <span class="n">result</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;```</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">```&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">result</span>
        
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">attempt</span> <span class="o">&lt;</span> <span class="n">max_retries</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;An error occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. Retrying (</span><span class="si">{</span><span class="n">attempt</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">max_retries</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">)...&#34;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">e</span>
                
  
<span class="c1">#当前代码所在的代码文件与data文件夹处于同一个文件夹内</span>
<span class="c1">#获取data内所有pdf的路径</span>
<span class="n">pdf_files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;data/</span><span class="si">{</span><span class="n">file</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;.pdf&#39;</span> <span class="ow">in</span> <span class="n">file</span><span class="p">]</span>
<span class="c1">#读取prompt</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;prompt.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">dict_datas</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">pdf_file</span> <span class="ow">in</span> <span class="n">pdf_files</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="n">pdf_file</span><span class="p">)</span>
    <span class="n">dict_data</span> <span class="o">=</span> <span class="n">extract_info</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>
    <span class="n">dict_datas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dict_data</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dict_datas</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;result.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">CPU times: user 32 ms, sys: 2.17 ms, total: 15.2 ms
Wall time: 3.8 s
</code></pre></div><p><img loading="lazy" src="img/05-df.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四讨论">四、讨论</h2>
<p>本文只使用了一张发票进行实验， 实际上准确率没有这么高， 识别错误字段集中在销售方纳税识别号(案例没有展示销售方纳税识别号的识别)。原因主要是ct.read_pdf读入pdf时，文本比较杂乱。对大模型的语义理解有一定的挑战。目前大模型已经支持文本、图片、音频、视频、网址， 所以各位看官，不用等太久，就可克服此问题。</p>
<p>大模型会对每个输入，给出正确概率最大的回答，因此大模型提取数据时存在一定的错误识别风险。为降低该风险，尽量选择特别特殊、显眼，例如三张发票的<strong>价税合计(大写)</strong>,  因为信息是特殊的中文大写数字， 在所有文本中是最醒目最特别的文本信息，这样大模型处理这类信息时会给这类信息尽可能高的权重，增大回答的准确率。</p>
<br>
<br>
<h2 id="精选内容">精选内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/datasets_available_for_management_science/">LIST | 可供社科(经管)领域使用的数据集汇总</a></li>
<li><a href="https://textdata.cn/blog/the_text_analysis_list_about_ms/">LIST | 社科(经管)数据挖掘文献资料汇总</a></li>
<li><a href="https://textdata.cn/blog/2024-06-16-scrapegraph-ai/">网络爬虫 | 使用scrapegraph-ai(大模型方案)自动采集网页数据</a></li>
<li><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">推荐 | 文本分析库cntext2.x使用手册</a></li>
<li><a href="https://textdata.cn/blog/management_python_course/">付费视频课 | Python实证指标构建与文本分析</a>
<br>
<br></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>cntext2.x | 新增读取pdf/docx| 提取MD&amp;A | 文本可视化等功能</title>
      <link>https://textdata.cn/blog/2024-05-14-add-readpdf-readdocx-lexical-dispersion-plot/</link>
      <pubDate>Tue, 14 May 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-05-14-add-readpdf-readdocx-lexical-dispersion-plot/</guid>
      <description>&lt;h2 id=&#34;一cntext&#34;&gt;一、cntext&lt;/h2&gt;
&lt;h3 id=&#34;11-新增函数&#34;&gt;1.1 新增函数&lt;/h3&gt;
&lt;p&gt;cntext2.1.2新增函数有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;get_cntext_path()&lt;/strong&gt;&lt;/em&gt;  查看cntext2.x的安装路径&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;read_pdf()/read_docx()&lt;/strong&gt;&lt;/em&gt;  读取 pdf、docx文件&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;extract_mda()&lt;/strong&gt;&lt;/em&gt; 提取中文年报文本中的管理层讨论与分析&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;matplotlib_chinese()&lt;/strong&gt;&lt;/em&gt; 支持matplotlib显示中文&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;lexical_dispersion_plot1()&lt;/strong&gt;&lt;/em&gt; 词汇分散图&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;lexical_dispersion_plot2()&lt;/strong&gt;&lt;/em&gt; 词汇分散图&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;已购买cntext2.x的用户，可私信找到大邓获取最新版本安装包！&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id=&#34;12-安装&#34;&gt;1.2 安装&lt;/h3&gt;
&lt;p&gt;所有 cntext2.x 安装方法类似， 以目前 cntext2.1.2 为例，将 cntext-2.1.2-py3-none-any.whl 放置于桌面，打开 cmd (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 cmd (苹果电脑打开terminal) 中使用 pip3 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install distinctiveness
pip3 install cntext-2.1.2-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文章开头和文章末都有*** cntext-2.1.2-py3-none-any.whl*** 获取方式说明。&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;二实验&#34;&gt;二、实验&lt;/h2&gt;
&lt;h3 id=&#34;21-get_cntext_path&#34;&gt;2.1 get_cntext_path()&lt;/h3&gt;
&lt;p&gt;如果你熟悉PYTHON，想对cntext内进行修改， 可以使用该函数找到cntext安装路径。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_cntext_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cntext
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;不同电脑返回的位置是不同的，以上路径是大邓Mac中cntext2.x的安装路径&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;h3 id=&#34;22-read_docx&#34;&gt;2.2 read_docx()&lt;/h3&gt;
&lt;p&gt;读取 docx文件。 自己diy一个 test.docx , 在文件内写一个句子，测一测&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_docx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;test.docx&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;这是来自docx文件里的内容
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;23-read_pdf&#34;&gt;2.3 read_pdf()&lt;/h3&gt;
&lt;p&gt;读取 pdf文件&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;%%&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#读取格力电器2023会计年度的年报文件&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_pdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;格力电器2023.pdf&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;CPU times: user 5.5 s, sys: 48.9 ms, total: 5.55 s
Wall time: 5.55 s

\n珠海格力电器股份有限公司 2023年年度报告全文  \n珠海格力电器股份有限公司  \n2023年年度报告  \n \n \n二〇二四年四月 \n珠海格力电器股份有限公司 2023年年度报告全文  \n 第 2 页 共 249 页 第一节 重要提示、目录和释义  \n公司董事会、监事会及董事、监事、高级管理人员保证年度报告内容\n的真实、准确、完整，不存在虚假记载、误导性陈述或重大遗漏，并承担\n个别和连带的法律责任。  \n公司负责人董明珠、主管会计工作负责人廖建雄及会计机构负责人\n（会计主管人员）刘炎姿声明：保证本年度报告中财务报告的真实、准确、\n完整。 \n所有董事均已出席了审议本报告的董事会会议。  \n本报告中所涉及的未来计划、发展战略等前瞻性陈述，不构成公司对\n投资者的实质承诺，投资者及相关人士均应当对此保持足够的风险认识，\n并且应当理解计划、预测与承诺之间的差异，敬请注意投资风险，理性投\n资。 \n公司经本次董事会审议通过的利润分配预案为：拟以本利润分配预案\n披露时享有利润分配权的股本总额  5,521,943,646 股（总股本\n5,631,405,741 股扣除公司回购账户持有的股份 109,462,095 股）为基数，\n向全体股东每 10股派发现金红利 23.80元（含税），送红股 0股（含\n税），不以公积金转增股本。  \n   \n珠海格力电器股份有限公司 2023年年度报告全文  \n 第 3 页 共 249 页 目录 \n第一节 重要提示、目录和释义  ................................ ..........................  2 \n第二节 公司简介和主要财务指标  ................................ ........................  6 \n第三节 管理层讨论与分析  ................................ ...............................  10 \n第四节 公司治理  ................................ ................................ ........  42 \n第五节 环境和社会责任  ................................ ..

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;24-extract_mda&#34;&gt;2.4 extract_mda()&lt;/h3&gt;
&lt;p&gt;提取A股年报中的MD&amp;amp;A文本内容。如果返回&#39;&#39;,则提取失败。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;ct.extract_mda(text, kws_pattern=&amp;#39;&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;text 中国A股年报原始文本&lt;/li&gt;
&lt;li&gt;kws_pattern 管理层讨论与分析章节识别关键词的模板。cntext内置的kws_pattern内容如下&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;kws_pattern = &amp;#39;董事会报告|董事会报告与管理讨论|企业运营与管理评述|经营总结与分析|管理层评估与未来展望|董事局报告|管理层讨论与分析|经营情况讨论与分析|经营业绩分析|业务回顾与展望|公司经营分析|管理层评论与分析|执行摘要与业务回顾|业务运营分析&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;基本上2015年之后，识别命中率在90%以上。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#读取格力电器2023会计年度的年报文件&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_pdf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;格力电器2023.pdf&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#提取mda&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mda_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;extract_mda&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;mda_text&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;管理层讨论与分析  \n一、报告期内公司所处行业情况  \n（一）行业发展现状  \n1.消费领域 ——家电行业稳定增长，空调市场恢复明显  \n2023年，中国经济保持了整体恢复向好的态势，激发消费是稳增长的重中之重。国家鼓励和推动消费品以旧换\n新，促进消费经济大循环，加速更新需求释放，推动高能效产品设备销售和出口增长，进一步激发绿色消费潜力。  \n1）家电行业稳定增长  \n2023年，国内经济恢复明显，家电行业稳定增长。根据全国家用电器工业信息中心发布的《 2023年中国家电\n行业年度报告》，家电行业外销明显增长，出口规模为 6,174亿元，同比增长 9.9%；国内市场实现稳步增长，销售\n规模为7,736亿元，同比增长 1.7%。 \n2）空调市场规模实现较好恢复  \n2023年，空调市场恢复明显。根据奥维云网（ AVC）零售推总数据， 2023年空调市场实现零售额 2,117亿元，\n同比增长 7.5%，零售量 6,085万台，同比增长 6.5%。根据产业在线数据， 2023年，家用空调生产 16,869.2 万台，\n同比增长 11.1%，销售17,044.0 万台，同比增长 11.2%，其中内销出货 9,959.7万台，同比增长 13.8%，出口出货\n7,084.3万台，同比增长 7.8%，内外销实现双增长。  \n2.工业领域 ——工业经济稳中向上态势  \n根据工信部数据， 2023年，我国规模以上工业增加值同比增长 4.6%，同比提升 1个百分点，其中制造业规模\n以上工业增加值同比增长 5.0%。 \n智能制造产业规模日益增长。从《中国制造 2025》再到《“十四五”智能制造发展规划》，均以发展先进智能\n制造业为核心目标，布局规划制造强国的推进路径。我国已 初步形成以自动化生产线、智能检测与装配装备、智能\n控制系统、工业机器人等为代表的智能制造产业体系，产业规模日益增长。中商产业研究院预计， 2023年我国智能\n制造装备市场规模将超过 2.97万亿元。前瞻产业研究院预测，到 2027年，我国智能制造行业市场规模将达到 6.6\n万亿元，其中智能制造装备市场规模约 5.4万亿元，智能制造系统解决方案市场规模约 1.2万亿元。 2023年，国内\n加快推动传统产业技术改造升级，加大智能制造推广力度，组建成  62家“灯塔工厂”，占全球“灯塔工厂”总数\n的40%，培育了 421家国家级智能制造示范 工厂，万余家省级数字化车间和智能工厂。  \n空调核心零部件产业规模增长明显。根据产业在线数据， 2023年，空调转子压缩机市场高速发展，全年产量达\n到2.61亿台，同比增长 12.2%；全年销售量达到 2.62亿台，成为行业新巅峰。内销市场，转子压缩机表现出色，\n全年保持正向增长，预计内销为 2.27亿台，同比增长 14.3%；外销市场，全年预计出口 3,564.7万台，同比增长\n2.1%。受益于 2023年下游空调市场销售规模的增长，空调电机行业产销规模同步提升，达到 4.22亿台，同比增长\n6.8%；内销市场出货约为 3.5亿台，同 比增长8.4%；出口市场出货约为 0.7亿台，同比持平。压缩机和电机产业规\n模的增长，为整个空调行业的发展提供了有力支持。  \n
.......
.......
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;25-matplotlib_chinese&#34;&gt;2.5 matplotlib_chinese()&lt;/h3&gt;
&lt;p&gt;matplotlib默认不支持中文可视化， cntext新增该函数，可以解决中文可视化问题&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;matplotlib_chinese&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;中文图表&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fontsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/27-chinese-matplotlib.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;26-lexical_dispersion_plot1&#34;&gt;2.6 lexical_dispersion_plot1()&lt;/h3&gt;
&lt;p&gt;词汇分散图可视化， 对某一个文本text， 可视化不同目标类别词targets_dict在文本中出现位置&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lexical_dispersion_plot1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;targets_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;特定词汇在不同文本来源的相对离散图&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prop&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;text&lt;/strong&gt;&lt;/em&gt;: 文本数据&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;targets_dict&lt;/strong&gt;&lt;/em&gt;:  目标类别词字典； targets_dict={&amp;lsquo;pos&amp;rsquo;: [&amp;lsquo;开心&amp;rsquo;, &amp;lsquo;快乐&amp;rsquo;], &amp;lsquo;neg&amp;rsquo;: [&amp;lsquo;悲伤&amp;rsquo;, &amp;lsquo;难过&amp;rsquo;]}&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;lang&lt;/strong&gt;&lt;/em&gt;: 文本数据texts_dict的语言类型，默认&amp;rsquo;chinese&#39;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;figsize&lt;/strong&gt;&lt;/em&gt;: 图的长宽尺寸. 默认 (8, 5).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;title&lt;/strong&gt;&lt;/em&gt; : 图的标题；&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;prop&lt;/strong&gt;&lt;/em&gt;: 横坐标字符位置是否为相对位置. 默认True，横坐标索引值取值范围0 ~ 100&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;p&gt;点击下载 &lt;a href=&#34;https://textdata.cn/data/%E4%B8%89%E4%BD%93.txt&#34;&gt;&lt;strong&gt;三体.txt&lt;/strong&gt;&lt;/a&gt;、&lt;a href=&#34;https://textdata.cn/data/%E5%9F%BA%E5%9C%B0.txt&#34;&gt;&lt;strong&gt;基地.txt&lt;/strong&gt;&lt;/a&gt;两本小说文件。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;roles_dict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;汪淼&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;汪淼&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;叶文洁&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;叶文洁&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
    &lt;span class=&#34;s2&#34;&gt;&amp;#34;罗辑&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;罗辑&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;santi_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;三体.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lexical_dispersion_plot1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;santi_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#文本数据&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;targets_dict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;roles_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#角色&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#尺寸大小&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#中文数据&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;《三体》小说角色出现位置&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#标题&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;prop&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;#相对位置(横坐标轴取值范围0-100)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/23-lexical_dispersion_plot1-relative.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lexical_dispersion_plot1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;santi_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#文本数据&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;targets_dict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;roles_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#角色&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#尺寸大小&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;#中文数据&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;《三体》小说角色出现位置&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#标题&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;prop&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;#绝对位置(横坐标轴取值范围与小说文本长度有关)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/24-lexical_dispersion_plot1-absolute.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;#diy了一个小词典&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;senti_dict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;pos&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;开心&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;幸福&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;快乐&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;安宁&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;希望&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;neg&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;紧张&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;恐惧&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;害怕&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;绝望&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;santi_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;三体.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lexical_dispersion_plot1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;santi_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                            &lt;span class=&#34;n&#34;&gt;targets_dict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;senti_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                            &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; 
                            &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                            &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;《三体》情绪词出现位置&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;prop&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/25-santi_sentiment.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;27--lexical_dispersion_plot2&#34;&gt;2.7  lexical_dispersion_plot2()&lt;/h3&gt;
&lt;p&gt;词汇分散图可视化， 对某几个文本texts_dict， 可视化某些目标词targets在文本中出现相对位置(0~100)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lexical_dispersion_plot2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;texts_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;targets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;特定词汇在不同文本来源的相对离散图&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;texts_dict&lt;/strong&gt;&lt;/em&gt;: 多个文本的字典数据。形如{&amp;lsquo;source1&amp;rsquo;: &amp;lsquo;source1的文本内容&amp;rsquo;, &amp;lsquo;source2&amp;rsquo;: &amp;lsquo;source2的文本内容&amp;rsquo;}&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;targets&lt;/strong&gt;&lt;/em&gt;: 目标词列表&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;lang&lt;/strong&gt;&lt;/em&gt;: 文本数据texts_dict的语言类型，默认&amp;rsquo;chinese&#39;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;figsize&lt;/strong&gt;&lt;/em&gt;: 图的长宽尺寸. 默认 (8, 5).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;title&lt;/strong&gt;&lt;/em&gt; : 图的标题；&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;targets&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;太空&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;宇宙&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;texts_dict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;三体&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;三体.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(),&lt;/span&gt;
              &lt;span class=&#34;s1&#34;&gt;&amp;#39;基地&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;基地.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()}&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;lexical_dispersion_plot2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;texts_dict&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;texts_dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;targets&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;targets&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                            &lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; 
                            &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#34;太空/宇宙&amp;#34;词语出现位置&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/26-santi_base.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;获取资料&#34;&gt;获取资料&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 100元 cntext-2.1.2-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;加微信 &lt;strong&gt;372335839&lt;/strong&gt;， 备注「姓名-学校-专业」。 已购买cntext2.x的用户，可私信找到大邓获取最新版本安装包！&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一cntext">一、cntext</h2>
<h3 id="11-新增函数">1.1 新增函数</h3>
<p>cntext2.1.2新增函数有</p>
<ul>
<li><em><strong>get_cntext_path()</strong></em>  查看cntext2.x的安装路径</li>
<li><em><strong>read_pdf()/read_docx()</strong></em>  读取 pdf、docx文件</li>
<li><em><strong>extract_mda()</strong></em> 提取中文年报文本中的管理层讨论与分析</li>
<li><em><strong>matplotlib_chinese()</strong></em> 支持matplotlib显示中文</li>
<li><em><strong>lexical_dispersion_plot1()</strong></em> 词汇分散图</li>
<li><em><strong>lexical_dispersion_plot2()</strong></em> 词汇分散图</li>
</ul>
<p>已购买cntext2.x的用户，可私信找到大邓获取最新版本安装包！</p>
<p><br><br></p>
<h3 id="12-安装">1.2 安装</h3>
<p>所有 cntext2.x 安装方法类似， 以目前 cntext2.1.2 为例，将 cntext-2.1.2-py3-none-any.whl 放置于桌面，打开 cmd (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 cmd (苹果电脑打开terminal) 中使用 pip3 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install distinctiveness
pip3 install cntext-2.1.2-py3-none-any.whl
</code></pre></div><p>文章开头和文章末都有*** cntext-2.1.2-py3-none-any.whl*** 获取方式说明。</p>
<br>
<h2 id="二实验">二、实验</h2>
<h3 id="21-get_cntext_path">2.1 get_cntext_path()</h3>
<p>如果你熟悉PYTHON，想对cntext内进行修改， 可以使用该函数找到cntext安装路径。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">ct</span><span class="o">.</span><span class="n">get_cntext_path</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/cntext
</code></pre></div><blockquote>
<p>不同电脑返回的位置是不同的，以上路径是大邓Mac中cntext2.x的安装路径</p>
</blockquote>
<br>
<h3 id="22-read_docx">2.2 read_docx()</h3>
<p>读取 docx文件。 自己diy一个 test.docx , 在文件内写一个句子，测一测</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">text</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_docx</span><span class="p">(</span><span class="s1">&#39;test.docx&#39;</span><span class="p">)</span>

<span class="n">text</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">这是来自docx文件里的内容
</code></pre></div><br>
<h3 id="23-read_pdf">2.3 read_pdf()</h3>
<p>读取 pdf文件</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#读取格力电器2023会计年度的年报文件</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;格力电器2023.pdf&#39;</span><span class="p">)</span>

<span class="n">text</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">CPU times: user 5.5 s, sys: 48.9 ms, total: 5.55 s
Wall time: 5.55 s

\n珠海格力电器股份有限公司 2023年年度报告全文  \n珠海格力电器股份有限公司  \n2023年年度报告  \n \n \n二〇二四年四月 \n珠海格力电器股份有限公司 2023年年度报告全文  \n 第 2 页 共 249 页 第一节 重要提示、目录和释义  \n公司董事会、监事会及董事、监事、高级管理人员保证年度报告内容\n的真实、准确、完整，不存在虚假记载、误导性陈述或重大遗漏，并承担\n个别和连带的法律责任。  \n公司负责人董明珠、主管会计工作负责人廖建雄及会计机构负责人\n（会计主管人员）刘炎姿声明：保证本年度报告中财务报告的真实、准确、\n完整。 \n所有董事均已出席了审议本报告的董事会会议。  \n本报告中所涉及的未来计划、发展战略等前瞻性陈述，不构成公司对\n投资者的实质承诺，投资者及相关人士均应当对此保持足够的风险认识，\n并且应当理解计划、预测与承诺之间的差异，敬请注意投资风险，理性投\n资。 \n公司经本次董事会审议通过的利润分配预案为：拟以本利润分配预案\n披露时享有利润分配权的股本总额  5,521,943,646 股（总股本\n5,631,405,741 股扣除公司回购账户持有的股份 109,462,095 股）为基数，\n向全体股东每 10股派发现金红利 23.80元（含税），送红股 0股（含\n税），不以公积金转增股本。  \n   \n珠海格力电器股份有限公司 2023年年度报告全文  \n 第 3 页 共 249 页 目录 \n第一节 重要提示、目录和释义  ................................ ..........................  2 \n第二节 公司简介和主要财务指标  ................................ ........................  6 \n第三节 管理层讨论与分析  ................................ ...............................  10 \n第四节 公司治理  ................................ ................................ ........  42 \n第五节 环境和社会责任  ................................ ..

</code></pre></div><br>
<h3 id="24-extract_mda">2.4 extract_mda()</h3>
<p>提取A股年报中的MD&amp;A文本内容。如果返回'',则提取失败。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">ct.extract_mda(text, kws_pattern=&#39;&#39;)
</code></pre></div><ul>
<li>text 中国A股年报原始文本</li>
<li>kws_pattern 管理层讨论与分析章节识别关键词的模板。cntext内置的kws_pattern内容如下</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">kws_pattern = &#39;董事会报告|董事会报告与管理讨论|企业运营与管理评述|经营总结与分析|管理层评估与未来展望|董事局报告|管理层讨论与分析|经营情况讨论与分析|经营业绩分析|业务回顾与展望|公司经营分析|管理层评论与分析|执行摘要与业务回顾|业务运营分析&#39;
</code></pre></div><br>
<p>基本上2015年之后，识别命中率在90%以上。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#读取格力电器2023会计年度的年报文件</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">read_pdf</span><span class="p">(</span><span class="s1">&#39;格力电器2023.pdf&#39;</span><span class="p">)</span>

<span class="c1">#提取mda</span>
<span class="n">mda_text</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">extract_mda</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="n">mda_text</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">管理层讨论与分析  \n一、报告期内公司所处行业情况  \n（一）行业发展现状  \n1.消费领域 ——家电行业稳定增长，空调市场恢复明显  \n2023年，中国经济保持了整体恢复向好的态势，激发消费是稳增长的重中之重。国家鼓励和推动消费品以旧换\n新，促进消费经济大循环，加速更新需求释放，推动高能效产品设备销售和出口增长，进一步激发绿色消费潜力。  \n1）家电行业稳定增长  \n2023年，国内经济恢复明显，家电行业稳定增长。根据全国家用电器工业信息中心发布的《 2023年中国家电\n行业年度报告》，家电行业外销明显增长，出口规模为 6,174亿元，同比增长 9.9%；国内市场实现稳步增长，销售\n规模为7,736亿元，同比增长 1.7%。 \n2）空调市场规模实现较好恢复  \n2023年，空调市场恢复明显。根据奥维云网（ AVC）零售推总数据， 2023年空调市场实现零售额 2,117亿元，\n同比增长 7.5%，零售量 6,085万台，同比增长 6.5%。根据产业在线数据， 2023年，家用空调生产 16,869.2 万台，\n同比增长 11.1%，销售17,044.0 万台，同比增长 11.2%，其中内销出货 9,959.7万台，同比增长 13.8%，出口出货\n7,084.3万台，同比增长 7.8%，内外销实现双增长。  \n2.工业领域 ——工业经济稳中向上态势  \n根据工信部数据， 2023年，我国规模以上工业增加值同比增长 4.6%，同比提升 1个百分点，其中制造业规模\n以上工业增加值同比增长 5.0%。 \n智能制造产业规模日益增长。从《中国制造 2025》再到《“十四五”智能制造发展规划》，均以发展先进智能\n制造业为核心目标，布局规划制造强国的推进路径。我国已 初步形成以自动化生产线、智能检测与装配装备、智能\n控制系统、工业机器人等为代表的智能制造产业体系，产业规模日益增长。中商产业研究院预计， 2023年我国智能\n制造装备市场规模将超过 2.97万亿元。前瞻产业研究院预测，到 2027年，我国智能制造行业市场规模将达到 6.6\n万亿元，其中智能制造装备市场规模约 5.4万亿元，智能制造系统解决方案市场规模约 1.2万亿元。 2023年，国内\n加快推动传统产业技术改造升级，加大智能制造推广力度，组建成  62家“灯塔工厂”，占全球“灯塔工厂”总数\n的40%，培育了 421家国家级智能制造示范 工厂，万余家省级数字化车间和智能工厂。  \n空调核心零部件产业规模增长明显。根据产业在线数据， 2023年，空调转子压缩机市场高速发展，全年产量达\n到2.61亿台，同比增长 12.2%；全年销售量达到 2.62亿台，成为行业新巅峰。内销市场，转子压缩机表现出色，\n全年保持正向增长，预计内销为 2.27亿台，同比增长 14.3%；外销市场，全年预计出口 3,564.7万台，同比增长\n2.1%。受益于 2023年下游空调市场销售规模的增长，空调电机行业产销规模同步提升，达到 4.22亿台，同比增长\n6.8%；内销市场出货约为 3.5亿台，同 比增长8.4%；出口市场出货约为 0.7亿台，同比持平。压缩机和电机产业规\n模的增长，为整个空调行业的发展提供了有力支持。  \n
.......
.......
</code></pre></div><br>
<h3 id="25-matplotlib_chinese">2.5 matplotlib_chinese()</h3>
<p>matplotlib默认不支持中文可视化， cntext新增该函数，可以解决中文可视化问题</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">plt</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">matplotlib_chinese</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;中文图表&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/27-chinese-matplotlib.png" alt=""  />
</p>
<br>
<h3 id="26-lexical_dispersion_plot1">2.6 lexical_dispersion_plot1()</h3>
<p>词汇分散图可视化， 对某一个文本text， 可视化不同目标类别词targets_dict在文本中出现位置</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">lexical_dispersion_plot1</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">targets_dict</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;特定词汇在不同文本来源的相对离散图&#39;</span><span class="p">,</span> <span class="n">prop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><ul>
<li><em><strong>text</strong></em>: 文本数据</li>
<li><em><strong>targets_dict</strong></em>:  目标类别词字典； targets_dict={&lsquo;pos&rsquo;: [&lsquo;开心&rsquo;, &lsquo;快乐&rsquo;], &lsquo;neg&rsquo;: [&lsquo;悲伤&rsquo;, &lsquo;难过&rsquo;]}</li>
<li><em><strong>lang</strong></em>: 文本数据texts_dict的语言类型，默认&rsquo;chinese'.</li>
<li><em><strong>figsize</strong></em>: 图的长宽尺寸. 默认 (8, 5).</li>
<li><em><strong>title</strong></em> : 图的标题；</li>
<li><em><strong>prop</strong></em>: 横坐标字符位置是否为相对位置. 默认True，横坐标索引值取值范围0 ~ 100</li>
</ul>
<br>
<p>点击下载 <a href="https://textdata.cn/data/%E4%B8%89%E4%BD%93.txt"><strong>三体.txt</strong></a>、<a href="https://textdata.cn/data/%E5%9F%BA%E5%9C%B0.txt"><strong>基地.txt</strong></a>两本小说文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">roles_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;汪淼&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;汪淼&#39;</span><span class="p">],</span>
    <span class="s2">&#34;叶文洁&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;叶文洁&#39;</span><span class="p">],</span>
    <span class="s2">&#34;罗辑&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;罗辑&#39;</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">santi_text</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;三体.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">lexical_dispersion_plot1</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">santi_text</span><span class="p">,</span>  <span class="c1">#文本数据</span>
                            <span class="n">targets_dict</span> <span class="o">=</span> <span class="n">roles_dict</span><span class="p">,</span> <span class="c1">#角色</span>
                            <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>  <span class="c1">#尺寸大小</span>
                            <span class="n">lang</span> <span class="o">=</span> <span class="s1">&#39;chinese&#39;</span><span class="p">,</span>  <span class="c1">#中文数据</span>
                            <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;《三体》小说角色出现位置&#39;</span><span class="p">,</span> <span class="c1">#标题</span>
                            <span class="n">prop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>    <span class="c1">#相对位置(横坐标轴取值范围0-100)</span>
<span class="n">ax</span>
</code></pre></div><p><img loading="lazy" src="img/23-lexical_dispersion_plot1-relative.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">lexical_dispersion_plot1</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">santi_text</span><span class="p">,</span>  <span class="c1">#文本数据</span>
                            <span class="n">targets_dict</span> <span class="o">=</span> <span class="n">roles_dict</span><span class="p">,</span> <span class="c1">#角色</span>
                            <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>  <span class="c1">#尺寸大小</span>
                            <span class="n">lang</span> <span class="o">=</span> <span class="s1">&#39;chinese&#39;</span><span class="p">,</span>  <span class="c1">#中文数据</span>
                            <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;《三体》小说角色出现位置&#39;</span><span class="p">,</span> <span class="c1">#标题</span>
                            <span class="n">prop</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>    <span class="c1">#绝对位置(横坐标轴取值范围与小说文本长度有关)</span>
</code></pre></div><p><img loading="lazy" src="img/24-lexical_dispersion_plot1-absolute.png" alt=""  />
</p>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1">#diy了一个小词典</span>
<span class="n">senti_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;pos&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;开心&#39;</span><span class="p">,</span> <span class="s1">&#39;幸福&#39;</span><span class="p">,</span> <span class="s1">&#39;快乐&#39;</span><span class="p">,</span> <span class="s1">&#39;安宁&#39;</span><span class="p">,</span> <span class="s1">&#39;希望&#39;</span><span class="p">],</span>
    <span class="s1">&#39;neg&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;紧张&#39;</span><span class="p">,</span> <span class="s1">&#39;恐惧&#39;</span><span class="p">,</span> <span class="s1">&#39;害怕&#39;</span><span class="p">,</span> <span class="s1">&#39;绝望&#39;</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">santi_text</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;三体.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">lexical_dispersion_plot1</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">santi_text</span><span class="p">,</span> 
                            <span class="n">targets_dict</span> <span class="o">=</span> <span class="n">senti_dict</span><span class="p">,</span> 
                            <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> 
                            <span class="n">lang</span> <span class="o">=</span> <span class="s1">&#39;chinese&#39;</span><span class="p">,</span> 
                            <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;《三体》情绪词出现位置&#39;</span><span class="p">,</span>
                            <span class="n">prop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span>
</code></pre></div><p><img loading="lazy" src="img/25-santi_sentiment.png" alt=""  />
</p>
<br>
<h3 id="27--lexical_dispersion_plot2">2.7  lexical_dispersion_plot2()</h3>
<p>词汇分散图可视化， 对某几个文本texts_dict， 可视化某些目标词targets在文本中出现相对位置(0~100)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">lexical_dispersion_plot2</span><span class="p">(</span><span class="n">texts_dict</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;特定词汇在不同文本来源的相对离散图&#39;</span><span class="p">)</span>
</code></pre></div><ul>
<li><em><strong>texts_dict</strong></em>: 多个文本的字典数据。形如{&lsquo;source1&rsquo;: &lsquo;source1的文本内容&rsquo;, &lsquo;source2&rsquo;: &lsquo;source2的文本内容&rsquo;}</li>
<li><em><strong>targets</strong></em>: 目标词列表</li>
<li><em><strong>lang</strong></em>: 文本数据texts_dict的语言类型，默认&rsquo;chinese'.</li>
<li><em><strong>figsize</strong></em>: 图的长宽尺寸. 默认 (8, 5).</li>
<li><em><strong>title</strong></em> : 图的标题；</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;太空&#39;</span><span class="p">,</span> <span class="s1">&#39;宇宙&#39;</span><span class="p">]</span>

<span class="n">texts_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;三体&#39;</span><span class="p">:</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;三体.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">(),</span>
              <span class="s1">&#39;基地&#39;</span><span class="p">:</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;基地.txt&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()}</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">lexical_dispersion_plot2</span><span class="p">(</span><span class="n">texts_dict</span> <span class="o">=</span> <span class="n">texts_dict</span><span class="p">,</span>
                            <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">,</span> 
                            <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> 
                            <span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;&#34;太空/宇宙&#34;词语出现位置&#39;</span><span class="p">,</span>
                            <span class="n">lang</span> <span class="o">=</span> <span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
<span class="n">ax</span>
</code></pre></div><p><img loading="lazy" src="img/26-santi_base.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="获取资料">获取资料</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 100元 cntext-2.1.2-py3-none-any.whl
</code></pre></div><p>加微信 <strong>372335839</strong>， 备注「姓名-学校-专业」。 已购买cntext2.x的用户，可私信找到大邓获取最新版本安装包！</p>
<p><br><br></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>豆瓣影评 | 探索词向量妙处</title>
      <link>https://textdata.cn/blog/douban_w2v/</link>
      <pubDate>Sun, 21 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/douban_w2v/</guid>
      <description>使用cntext训练、使用词向量。</description>
      <content:encoded><![CDATA[<p>本文要点</p>
<ul>
<li>读取 <em><strong>csv</strong></em></li>
<li>准备语料</li>
<li><em><strong>cntext2.x</strong></em> 训练词向量模型</li>
<li>运用词向量模型</li>
</ul>
<br>
<br>
<h2 id="一读取数据">一、读取数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;douban.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;电影  : </span><span class="si">{}</span><span class="s2"> 部&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Movie_Name_CN</span><span class="o">.</span><span class="n">nunique</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;评论  : </span><span class="si">{}</span><span class="s2"> 条&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    电影  : 28 部
    评论  : 2125056 条
</code></pre></div><p><br><br></p>
<h2 id="二准备语料">二、准备语料</h2>
<p>提取文本，去除非中文字符，保存为txt文件</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;douban.csv&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;douban.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">raw_text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Comment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="三训练模型">三、训练模型</h2>
<h3 id="31-安装cntext2x">3.1 安装cntext2x</h3>
<p>将 <em><strong>cntext-2.1.6-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext-2.1.6-py3-none-any.whl
</code></pre></div><p>文末有 <em><strong>cntext-2.1.6-py3-none-any.whl</strong></em> 获取方式</p>
<br>
<h3 id="32-训练模型">3.2 训练模型</h3>
<p>使用 <a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/"><em><strong>cntext2.x</strong></em></a> 库(版本号2.1.6) 训练词向量word2vec模型, 这里我把 csv 数据整理为 txt</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 训练</span>
<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span> <span class="o">=</span> <span class="s1">&#39;douban.txt&#39;</span><span class="p">,</span>  
                        <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span> 
                        <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                        <span class="n">only_binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># 只保存二进制模型文件</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/douban_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 11150it [00:07, 5759.05it/s]
Reading Preprocessed Corpus from output/douban_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 2001 s. 
Output Saved To: output/douban-Word2Vec.200.15.bin
</code></pre></div><br>
<p>在代码所在文件夹内可以找到</p>
<ul>
<li>output/douban-Word2Vec.200.15.bin</li>
<li>新的  pos.txt</li>
<li>新的  neg.txt</li>
</ul>
<p>新的 <em><strong>pos.txt</strong></em> 是对 <em><strong>pos.txt</strong></em> 词典的扩展。</p>
<br>
<h3 id="24-评估模型">2.4 评估模型</h3>
<p>使用近义法和类比法， 判断模型的表现。详情可查看<a href="https://cntext.readthedocs.io/zh-cn/latest/model.html">文档</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">evaluate_similarity</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">)</span>

<span class="n">ct</span><span class="o">.</span><span class="n">evaluate_analogy</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   459    |     78     |            0.43            |
+----------+------------+----------------------------+


类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|██████████████| 1198/1198 [00:11&lt;00:00, 99.91it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   615    |     62     |   39.02    |   2.98   |
|   CityInProvince   |   175    |     0      |   28.57    |   4.74   |
| FamilyRelationship |   272    |     0      |   92.65    |   1.48   |
|   SocialScience    |    8     |     62     |   25.00    |   6.00   |
+--------------------+----------+------------+------------+----------+
</code></pre></div><p><strong>近义测试</strong>: Spearman&rsquo;s Rank Coeficient系数取值[-1, 1], 取值越大， 说明模型表现越好。</p>
<br>
<p><strong>类比测试</strong>:</p>
<ul>
<li>CapitalOfCountries  豆瓣影评语料在此项表现尚可，可能目前电影库中有一定比例的外国素材。</li>
<li>CityInProvince      豆瓣影评语料在此项表现较差，不太可能是中国素材太少，可能大多数省市以类似汉东省的形式出现。这是我的猜测。 <a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">人民网留言板语料Word2Vec</a>中，该项准确率100%。</li>
<li>FamilyRelationship  豆瓣影评体现的是电影相关内容，而电影永远的主题是人性， 内容少不了家长里短，七大姑八大姨，所以此项准确率高达92.65%。 以<a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">年报MD&amp;A</a>为例，此处准确率只有10%。</li>
<li>SocialScience       豆瓣影评语料在此项表现一般， 应该是语料中常见的社会科学词语提及较少。</li>
</ul>
<p>整体而言，语料训练的效果很不错，抓住了数据场景的独特性语义。</p>
<p><br><br></p>
<h2 id="四使用word2vec">四、使用Word2Vec</h2>
<h3 id="41-导入word2vec模型文件">4.1 导入Word2Vec模型文件</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
 
<span class="c1"># 导入模型，请注意路径。</span>
<span class="c1"># 「当前代码」 与 「output」 同处于一个文件夹内</span>

<span class="n">dm_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/douban-Word2Vec.200.15.bin&#39;</span><span class="p">)</span>
<span class="c1"># dm_w2v = ct.load_w2v(&#39;output/douban-Word2Vec.200.15.txt&#39;)</span>

<span class="n">dm_w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading output/output/douban-Word2Vec.200.15.bin...
&lt;gensim.models.keyedvectors.KeyedVectors at 0x314193830&gt;
</code></pre></div><br>
<h3 id="42-keyedvectors的操作方法或属性">4.2 KeyedVectors的操作方法(或属性)</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>KeyedVectors.index_to_key</strong></em></td>
<td>获取词汇表中的所有单词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.key_to_index</strong></em></td>
<td>获取单词到索引的映射。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.vector_size</strong></em></td>
<td>获取GloVe模型中任意词向量的维度。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.get_vector(word)</strong></em></td>
<td>获取给定单词的词向量。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_word(word, topn=10)</strong></em></td>
<td>获取某词语最相似的10个近义词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_vector(vector, topn=10)</strong></em></td>
<td>获取词向量最相似的10个近义词。</td>
</tr>
</tbody>
</table>
<br>
<h3 id="44-查看词表">4.4 查看词表</h3>
<p>查看词表所有单词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">index_to_key</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;电影&#39;,
 &#39;一个&#39;,
 &#39;没有&#39;,
 &#39;喜欢&#39;,
 ...
 &#39;跟着&#39;,
 &#39;意识&#39;,
 &#39;态度&#39;,
 ...]
</code></pre></div><p>为了方便查看， 这里只展示部分数据。</p>
<br>
<h3 id="45-词表映射">4.5 词表映射</h3>
<p>查看单词到索引的映射</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">key_to_index</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;电影&#39;: 0,
 &#39;一个&#39;: 1,
 &#39;没有&#39;: 2,
...
&#39;跟着&#39;: 997,
 &#39;意识&#39;: 998,
 &#39;态度&#39;: 999,
 ...}
</code></pre></div><br>
<h3 id="46-向量维度数">4.6 向量维度数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;词表有 </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span><span class="si">}</span><span class="s1"> 个词&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;向量是 </span><span class="si">{</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">vector_size</span><span class="si">}</span><span class="s1"> 维&#39;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">词表有 426646 个词
向量是 200 维
</code></pre></div><br>
<h3 id="47-获取词向量">4.7 获取词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-1.24090052e+00, -6.79377019e-01,  1.42518425e+00, -1.46615291e+00,
       -9.53197628e-02,  6.50456071e-01, -2.97696137e+00,  2.20916629e+00,
        6.12876177e-01,  1.63172066e+00,  4.91760701e-01, -9
        ......
        ......
         -1.42494082e+00,  2.49131727e+00, -6.27597034e-01, -7.91438043e-01,
       -4.54898655e-01,  1.37747681e+00, -4.20672953e-01, -1.53694853e-01,
        1.04936564e+00,  2.18786263e+00, -8.07472587e-01, -8.32003877e-02],
      dtype=float32)
</code></pre></div><br>
<h3 id="48-近义词">4.8 近义词</h3>
<p>根据词语查看近义词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 近义词</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;相当给力&#39;, 0.6180022358894348),
 (&#39;太给力&#39;, 0.6019443273544312),
 (&#39;带劲&#39;, 0.5840415954589844),
 (&#39;不给力&#39;, 0.5774183869361877),
 (&#39;过瘾&#39;, 0.5616626739501953),
 (&#39;牛叉&#39;, 0.553788959980011),
 (&#39;出彩&#39;, 0.5414286851882935),
 (&#39;精彩&#39;, 0.5332293510437012),
 (&#39;看得过瘾&#39;, 0.5250197649002075),
 (&#39;大赞&#39;, 0.5205727219581604)]
</code></pre></div><br>
<p>根据向量查找最相似的近义词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">word_vector</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">word_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;给力&#39;, 1.0),
 (&#39;相当给力&#39;, 0.6180021166801453),
 (&#39;太给力&#39;, 0.6019443273544312),
 (&#39;带劲&#39;, 0.5840415954589844),
 (&#39;不给力&#39;, 0.5774183869361877),
 (&#39;过瘾&#39;, 0.5616626739501953),
 (&#39;牛叉&#39;, 0.5537890195846558),
 (&#39;出彩&#39;, 0.5414287447929382),
 (&#39;精彩&#39;, 0.5332292914390564),
 (&#39;看得过瘾&#39;, 0.5250197649002075)]
</code></pre></div><br>
<h3 id="49-计算多个词的中心向量">4.9 计算多个词的中心向量</h3>
<p>我们可以计算「宇宙」、「飞船」、「战争」的宇宙语义向量（中心向量）。 并试图寻找中心向量 <em><strong>universe_vector</strong></em> 的最相似的10个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 几个词语构建的宇宙语义向量</span>
<span class="n">universe_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_centroid</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">dm_w2v</span><span class="p">,</span> 
                                       <span class="n">words</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;宇宙&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>


<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">universe_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;宇宙&#39;, 0.7568532228469849),
 (&#39;星系&#39;, 0.7090039253234863),
 (&#39;飞船&#39;, 0.7080673575401306),
 (&#39;人类文明&#39;, 0.6973789930343628),
 (&#39;战舰&#39;, 0.6890057325363159),
 (&#39;母舰&#39;, 0.6864359974861145),
 (&#39;星球&#39;, 0.6799622774124146),
 (&#39;卫星&#39;, 0.6799139976501465),
 (&#39;星际&#39;, 0.6789332032203674),
 (&#39;空间站&#39;, 0.6780815124511719),
 (&#39;地球&#39;, 0.6769616603851318),
 (&#39;外太空&#39;, 0.6683873534202576),
 (&#39;核战&#39;, 0.6669113039970398),
 (&#39;外星飞船&#39;, 0.6592534780502319),
 (&#39;木星&#39;, 0.6586896777153015),
 (&#39;能源&#39;, 0.6562989950180054),
 (&#39;战争&#39;, 0.6556441187858582),
 (&#39;巨兽&#39;, 0.6544537544250488),
 (&#39;月球&#39;, 0.6525537967681885),
 (&#39;一艘&#39;, 0.6521110534667969)]
</code></pre></div><p>语义捕捉的很准哦。</p>
<h3 id="410-类比-king-man--woman--queen">4.10 类比 king-man + woman ~ queen</h3>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p><img loading="lazy" src="img/kingqueenformular.png" alt=""  />
</p>
<h4 id="4101-传统类比">4.10.1 传统类比</h4>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p>$$
Vector1 \approx vector(国王)-vector(男人)
$$</p>
<p>$$
Vector2 \approx vector(王后)-vector(女人)
$$</p>
<p>那两个向量方向应该近似，即 <em><strong>Vector1</strong></em>  约等于 <em><strong>Vector2</strong></em> ，将其看做等式就得到如下公式：</p>
<p>$$
vector(国王)-vector(男人) \approx vector(王后) - vector(女人)
$$</p>
<p>现在我们检查三个语义向量计算出的新的向量是否有与queen相关的语义信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">men_vector</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;男人&#39;</span><span class="p">)</span>
<span class="n">women_vector</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;女人&#39;</span><span class="p">)</span> 
<span class="n">king_vector</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;国王&#39;</span><span class="p">)</span> 

<span class="c1"># 假设 king- queen 近似等于 man -woman </span>
<span class="c1"># result 近似等于 king - queen + women</span>
<span class="n">result_vector</span> <span class="o">=</span> <span class="n">king_vector</span> <span class="o">-</span> <span class="n">men_vector</span> <span class="o">+</span> <span class="n">women_vector</span>
<span class="c1"># 现在检查 result_vector 的语义应该与queen相关</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">result_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;国王&#39;, 0.8276543617248535),
 (&#39;王后&#39;, 0.754295289516449),
 (&#39;皇后&#39;, 0.6877321004867554),
 (&#39;公主&#39;, 0.6311503052711487),
 (&#39;王位&#39;, 0.6292931437492371),
 (&#39;皇帝&#39;, 0.6280742287635803),
 (&#39;王妃&#39;, 0.6235458850860596),
 (&#39;伊丽莎白一世&#39;, 0.6158717274665833),
 (&#39;君主&#39;, 0.6151927709579468),
 (&#39;公爵&#39;, 0.6111372113227844),
 (&#39;女王&#39;, 0.6068686246871948),
 (&#39;登基&#39;, 0.606802225112915),
 (&#39;皇子&#39;, 0.5979987382888794),
 (&#39;侍卫&#39;, 0.594831109046936),
 (&#39;夫人&#39;, 0.5942187309265137),
 (&#39;王室&#39;, 0.5891965627670288),
 (&#39;女皇&#39;, 0.5889874696731567),
 (&#39;继位&#39;, 0.5818601846694946),
 (&#39;皇室&#39;, 0.5812580585479736),
 (&#39;王冠&#39;, 0.5733407139778137)]
</code></pre></div><h4 id="4102-新算法">4.10.2 新算法</h4>
<p><em><strong>most_similar_cosmul</strong></em> 使用了一种基于 <strong>乘法组合</strong> 的相似度计算方法，而不是简单的向量加减法。其核心公式如下：
$$
\text{Similarity}(w, \text{positive}, \text{negative}) = \frac{\prod_{p \in \text{positive}} \cos(w, p)}{\prod_{n \in \text{negative}} \cos(w, n)}
$$
对于给定的正样本词集合 P 和负样本词集合 N，目标是找到一个词 w，使得得分最大化。</p>
<p>参照如下的例子
$$
vector(王后)   \approx  vector(国王) + vector(女人) -vector(男人)<br>
$$</p>
<p>其中正向目标词有 <em><strong>国王</strong></em> 和 <em><strong>女人</strong></em>， 负向词有 <em><strong>男人</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 类比函数</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">most_similar_cosmul</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;国王&#39;</span><span class="p">,</span> <span class="s1">&#39;女人&#39;</span><span class="p">],</span>   <span class="c1">#</span>
                           <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;男人&#39;</span><span class="p">],</span> 
                           <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;王后&#39;, 0.9907146692276001),
 (&#39;皇后&#39;, 0.9572808146476746),
 (&#39;公主&#39;, 0.9137295484542847),
 (&#39;王妃&#39;, 0.9079920649528503),
 (&#39;皇帝&#39;, 0.905644953250885),
 (&#39;伊丽莎白一世&#39;, 0.9031068682670593),
 (&#39;女王&#39;, 0.8956636190414429),
 (&#39;王位&#39;, 0.8942943215370178),
 (&#39;登基&#39;, 0.8899738192558289),
 (&#39;君主&#39;, 0.8883361220359802),
 (&#39;公爵&#39;, 0.8862053751945496),
 (&#39;王室&#39;, 0.8842172622680664),
 (&#39;夫人&#39;, 0.8840034604072571),
 (&#39;女皇&#39;, 0.8824913501739502),
 (&#39;侍卫&#39;, 0.8815361857414246),
 (&#39;皇子&#39;, 0.8785887360572815),
 (&#39;皇室&#39;, 0.8755369186401367),
 (&#39;继位&#39;, 0.8736834526062012),
 (&#39;驾崩&#39;, 0.8675689101219177),
 (&#39;波旁王朝&#39;, 0.8671858906745911)]
</code></pre></div><p>可以看到返回前2的词直接表明了词语是王后皇后，与公式推算结果一般无二。</p>
<p><br><br></p>
<h2 id="五获取资料">五、获取资料</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 免费词向量      https://cntext.readthedocs.io/zh-cn/latest/embeddings.html

- 1000w-douban-movies.zip  链接: https://pan.baidu.com/s/1V8FUA9_qwHBW-utoOcV11w?pwd=t3sa 提取码: t3sa 

- 442w-douban-movies.zip
链接: https://pan.baidu.com/s/1bhJls4P33a6EwZ6guhiw_A?pwd=qi28 提取码: qi28

- 212w-douban-movie.zip
链接: https://pan.baidu.com/s/1vaOKOJPA3F4ipBrdZygtLA?pwd=gfvd 提取码: gfvd 

- 100元  cntext-2.1.6-py3-none-any.whl   如有需要，加微信 372335839， 备注「姓名-学校-专业」
</code></pre></div><p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://cntext.readthedocs.io/">文本分析库cntext2.x使用手册 https://cntext.readthedocs.io/</a></li>
<li><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用Stanford Glove代码训练中文语料的Glove模型</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量 | 使用人民网领导留言板语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">使用 5000w 专利申请数据集按年份(按省份)训练词向量</a></li>
<li><a href="https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/">使用 1000w 条豆瓣影评训练 Word2Vec</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 |  使用 1000w 条豆瓣影评训练 Word2Vec</title>
      <link>https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/</link>
      <pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/</guid>
      <description>&lt;p&gt;本文内容&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;介绍豆瓣影评数据集&lt;/li&gt;
&lt;li&gt;构造语料训练 &lt;strong&gt;&lt;em&gt;Word2Vec&lt;/em&gt;&lt;/strong&gt; 模型&lt;/li&gt;
&lt;li&gt;获取数据 &lt;strong&gt;&lt;em&gt;cntext2.x&lt;/em&gt;&lt;/strong&gt; &amp;amp; &lt;strong&gt;&lt;em&gt;Word2Vec&lt;/em&gt;&lt;/strong&gt; 模型文件&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一豆瓣影评数据集&#34;&gt;一、豆瓣影评数据集&lt;/h2&gt;
&lt;h3 id=&#34;11-数据集介绍&#34;&gt;1.1 数据集介绍&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据集: douba-movie-1000w

数据源: 豆瓣电影

记录数:
   - 电影 10269 部
   - 影评 10310989 条

体积: 1.35G
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;该数据集正好弥补下国内公开电影数据集的空缺， 数据已经过初步清洗，可用于推荐系统、情感分析、知识图谱、新闻传播学、社会学文化变迁等多个领域(或主题)。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;12-读取数据&#34;&gt;1.2 读取数据&lt;/h3&gt;
&lt;p&gt;下载 &lt;strong&gt;&lt;em&gt;douba-movie-1000w.zip&lt;/em&gt;&lt;/strong&gt; 解压后，可以看到数据集中有一个 &lt;strong&gt;&lt;em&gt;all_movies_with_id.csv&lt;/em&gt;&lt;/strong&gt; 文件。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;all_movies_with_id.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;13-所含字段&#34;&gt;1.3 所含字段&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;col&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; - &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt; - ID
 - Movie_Name  电影名
 - Score  豆瓣电影评分(1-10)
 - Review_People  评论者人数
 - Star_Distribution  评论评分分布(1-5, 含多个数值，数值以%间隔)
 - Craw_Date 爬虫运行日期
 - Username 豆瓣评论者用户名
 - Date 影评日期
 - Star  影评评分(1-5)
 - Comment 影评内容
 - Comment_Distribution 影评评分分布
 - Like 影评获得的喜欢数
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二-构造语料训练-word2vec&#34;&gt;二、 构造语料&amp;amp;训练 Word2Vec&lt;/h2&gt;
&lt;h3 id=&#34;21-构造语料&#34;&gt;2.1 构造语料&lt;/h3&gt;
&lt;p&gt;将字段 &lt;strong&gt;&lt;em&gt;Comment&lt;/em&gt;&lt;/strong&gt; 中所有文本汇总到 &lt;strong&gt;&lt;em&gt;douban-movie-1000w.txt&lt;/em&gt;&lt;/strong&gt;,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 读取数据，只读取字段sign&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;all_movies_with_id.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;usecols&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;sign&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 将sign列中的所有文本汇总到douban-movie-1000w.txt&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;douban-movie-1000w.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Comment&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 删除df和text变量，释放内存&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;del&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;del&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-配置-cntext216&#34;&gt;2.2 配置 cntext2.1.6&lt;/h3&gt;
&lt;p&gt;将 &lt;strong&gt;&lt;em&gt;cntext-2.1.6-py3-none-any.whl&lt;/em&gt;&lt;/strong&gt; 放置于桌面，打开 &lt;strong&gt;&lt;em&gt;cmd&lt;/em&gt;&lt;/strong&gt; (苹果电脑打开 terminal)， 输入 cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 &lt;strong&gt;&lt;em&gt;cmd&lt;/em&gt;&lt;/strong&gt; (苹果电脑打开 terminal) 中使用 &lt;strong&gt;&lt;em&gt;pip3&lt;/em&gt;&lt;/strong&gt; 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install cntext-2.1.6-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文末有 &lt;strong&gt;&lt;em&gt;cntext-2.1.6-py3-none-any.whl&lt;/em&gt;&lt;/strong&gt; 获取方式&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-训练-word2vec&#34;&gt;2.3 训练 Word2Vec&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# cntext为2.1.6&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 训练Word2Vec模型&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Word2Vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;douban-movie-1000w.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                 &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                 &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                 &lt;span class=&#34;n&#34;&gt;min_count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Mac(Linux) System, Enable Parallel Processing
Cache output/douban-movie-1000w_cache.txt Not Found or Empty, Preprocessing Corpus
Reading Preprocessed Corpus from output/douban-movie-1000w_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 2965 s.
Output Saved To: output/douban-movie-1000w.200.15.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;经过半个小时的训练， 得到&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型文件 &lt;strong&gt;&lt;em&gt;output/douban-movie-1000w-Word2Vec.200.15.txt&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;模型文件 &lt;strong&gt;&lt;em&gt;output/douban-movie-1000w-Word2Vec.200.15.bin&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;缓存文件 &lt;strong&gt;&lt;em&gt;output/douban-movie-1000w_cache.txt&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中模型文件有 &lt;strong&gt;&lt;em&gt;txt&lt;/em&gt;&lt;/strong&gt; 和 &lt;strong&gt;&lt;em&gt;bin&lt;/em&gt;&lt;/strong&gt; 两种格式， 信息量完全等同。 &lt;strong&gt;&lt;em&gt;txt&lt;/em&gt;&lt;/strong&gt; 可以用记事本打开查看， 而 &lt;strong&gt;&lt;em&gt;bin&lt;/em&gt;&lt;/strong&gt; 则是二进制文件， 体积更小。 已训练好的模型， 因 bin 格式体积更小， 便于分享。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;24-评估模型&#34;&gt;2.4 评估模型&lt;/h3&gt;
&lt;p&gt;使用近义法和类比法， 判断模型的表现。详情可查看&lt;a href=&#34;https://cntext.readthedocs.io/zh-cn/latest/model.html&#34;&gt;文档&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evaluate_similarity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evaluate_analogy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&amp;#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   459    |     78     |            0.43            |
+----------+------------+----------------------------+


类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|██████████████| 1198/1198 [00:11&amp;lt;00:00, 99.91it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   615    |     62     |   39.02    |   2.98   |
|   CityInProvince   |   175    |     0      |   28.57    |   4.74   |
| FamilyRelationship |   272    |     0      |   92.65    |   1.48   |
|   SocialScience    |    8     |     62     |   25.00    |   6.00   |
+--------------------+----------+------------+------------+----------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;近义测试&lt;/strong&gt;: Spearman&amp;rsquo;s Rank Coeficient 系数取值[-1, 1], 取值越大， 说明模型表现越好。&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;类比测试&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CapitalOfCountries 豆瓣影评语料在此项表现尚可，可能目前电影库中有一定比例的外国素材。&lt;/li&gt;
&lt;li&gt;CityInProvince 豆瓣影评语料在此项表现较差，不太可能是中国素材太少，可能大多数省市以类似汉东省的形式出现。这是我的猜测。 &lt;a href=&#34;https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/&#34;&gt;人民网留言板语料 Word2Vec&lt;/a&gt;中，该项准确率 100%。&lt;/li&gt;
&lt;li&gt;FamilyRelationship 豆瓣影评体现的是电影相关内容，而电影永远的主题是人性， 内容少不了家长里短，七大姑八大姨，所以此项准确率高达 92.65%。 以&lt;a href=&#34;https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/&#34;&gt;年报 MD&amp;amp;A&lt;/a&gt;为例，此处准确率只有 10%。&lt;/li&gt;
&lt;li&gt;SocialScience 豆瓣影评语料在此项表现一般， 应该是语料中常见的社会科学词语提及较少。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整体而言，语料训练的效果很不错，抓住了数据场景的独特性语义。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四使用-word2vec&#34;&gt;四、使用 Word2Vec&lt;/h2&gt;
&lt;h3 id=&#34;41-导入-word2vec-模型文件&#34;&gt;4.1 导入 Word2Vec 模型文件&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 导入模型，请注意路径。&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 「当前代码」 与 「output」 同处于一个文件夹内&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/douban-movie-1000w-Word2Vec.200.15.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# dm_w2v = ct.load_w2v(&amp;#39;output/douban-movie-1000w-Word2Vec.200.15.bin&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Loading output/douban-movie-1000w-Word2Vec.200.15.txt...
&amp;lt;gensim.models.keyedvectors.KeyedVectors at 0x314193830&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-keyedvectors-的操作方法或属性&#34;&gt;4.2 KeyedVectors 的操作方法(或属性)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方法&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.index_to_key&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取词汇表中的所有单词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.key_to_index&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取单词到索引的映射。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.vector_size&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取 GloVe 模型中任意词向量的维度。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.get_vector(word)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取给定单词的词向量。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.similar_by_word(word, topn=10)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取某词语最相似的 10 个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;em&gt;KeyedVectors.similar_by_vector(vector, topn=10)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;获取词向量最相似的 10 个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h3 id=&#34;43-查看词表&#34;&gt;4.3 查看词表&lt;/h3&gt;
&lt;p&gt;查看词表所有单词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;电影&amp;#39;,
 &amp;#39;一个&amp;#39;,
 &amp;#39;没有&amp;#39;,
 &amp;#39;喜欢&amp;#39;,
 ...
 &amp;#39;跟着&amp;#39;,
 &amp;#39;意识&amp;#39;,
 &amp;#39;态度&amp;#39;,
 ...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;为了方便查看， 这里只展示部分数据。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;44-词表映射&#34;&gt;4.4 词表映射&lt;/h3&gt;
&lt;p&gt;查看单词到索引的映射&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;电影&amp;#39;: 0,
 &amp;#39;一个&amp;#39;: 1,
 &amp;#39;没有&amp;#39;: 2,
...
&amp;#39;跟着&amp;#39;: 997,
 &amp;#39;意识&amp;#39;: 998,
 &amp;#39;态度&amp;#39;: 999,
 ...}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;45-向量维度数&#34;&gt;4.5 向量维度数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;词表有 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 个词&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;向量是 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 维&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;词表有 426646 个词
向量是 200 维
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;46-获取词向量&#34;&gt;4.6 获取词向量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([-1.24090052e+00, -6.79377019e-01,  1.42518425e+00, -1.46615291e+00,
       -9.53197628e-02,  6.50456071e-01, -2.97696137e+00,  2.20916629e+00,
        6.12876177e-01,  1.63172066e+00,  4.91760701e-01, -9
        ......
        ......
         -1.42494082e+00,  2.49131727e+00, -6.27597034e-01, -7.91438043e-01,
       -4.54898655e-01,  1.37747681e+00, -4.20672953e-01, -1.53694853e-01,
        1.04936564e+00,  2.18786263e+00, -8.07472587e-01, -8.32003877e-02],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;47-近义词&#34;&gt;4.7 近义词&lt;/h3&gt;
&lt;p&gt;根据词语查看近义词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 近义词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;相当给力&amp;#39;, 0.6180022358894348),
 (&amp;#39;太给力&amp;#39;, 0.6019443273544312),
 (&amp;#39;带劲&amp;#39;, 0.5840415954589844),
 (&amp;#39;不给力&amp;#39;, 0.5774183869361877),
 (&amp;#39;过瘾&amp;#39;, 0.5616626739501953),
 (&amp;#39;牛叉&amp;#39;, 0.553788959980011),
 (&amp;#39;出彩&amp;#39;, 0.5414286851882935),
 (&amp;#39;精彩&amp;#39;, 0.5332293510437012),
 (&amp;#39;看得过瘾&amp;#39;, 0.5250197649002075),
 (&amp;#39;大赞&amp;#39;, 0.5205727219581604)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;根据向量查找最相似的近义词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;word_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;给力&amp;#39;, 1.0),
 (&amp;#39;相当给力&amp;#39;, 0.6180021166801453),
 (&amp;#39;太给力&amp;#39;, 0.6019443273544312),
 (&amp;#39;带劲&amp;#39;, 0.5840415954589844),
 (&amp;#39;不给力&amp;#39;, 0.5774183869361877),
 (&amp;#39;过瘾&amp;#39;, 0.5616626739501953),
 (&amp;#39;牛叉&amp;#39;, 0.5537890195846558),
 (&amp;#39;出彩&amp;#39;, 0.5414287447929382),
 (&amp;#39;精彩&amp;#39;, 0.5332292914390564),
 (&amp;#39;看得过瘾&amp;#39;, 0.5250197649002075)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;48-计算多个词的中心向量&#34;&gt;4.8 计算多个词的中心向量&lt;/h3&gt;
&lt;p&gt;我们可以计算「宇宙」、「飞船」、「战争」的宇宙语义向量（中心向量）。 并试图寻找中心向量 &lt;strong&gt;&lt;em&gt;universe_vector&lt;/em&gt;&lt;/strong&gt; 的最相似的 10 个词。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 几个词语构建的宇宙语义向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;universe_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;semantic_centroid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                       &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;宇宙&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;飞船&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;战争&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;universe_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;宇宙&amp;#39;, 0.7568532228469849),
 (&amp;#39;星系&amp;#39;, 0.7090039253234863),
 (&amp;#39;飞船&amp;#39;, 0.7080673575401306),
 (&amp;#39;人类文明&amp;#39;, 0.6973789930343628),
 (&amp;#39;战舰&amp;#39;, 0.6890057325363159),
 (&amp;#39;母舰&amp;#39;, 0.6864359974861145),
 (&amp;#39;星球&amp;#39;, 0.6799622774124146),
 (&amp;#39;卫星&amp;#39;, 0.6799139976501465),
 (&amp;#39;星际&amp;#39;, 0.6789332032203674),
 (&amp;#39;空间站&amp;#39;, 0.6780815124511719),
 (&amp;#39;地球&amp;#39;, 0.6769616603851318),
 (&amp;#39;外太空&amp;#39;, 0.6683873534202576),
 (&amp;#39;核战&amp;#39;, 0.6669113039970398),
 (&amp;#39;外星飞船&amp;#39;, 0.6592534780502319),
 (&amp;#39;木星&amp;#39;, 0.6586896777153015),
 (&amp;#39;能源&amp;#39;, 0.6562989950180054),
 (&amp;#39;战争&amp;#39;, 0.6556441187858582),
 (&amp;#39;巨兽&amp;#39;, 0.6544537544250488),
 (&amp;#39;月球&amp;#39;, 0.6525537967681885),
 (&amp;#39;一艘&amp;#39;, 0.6521110534667969)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;语义捕捉的很准哦。&lt;/p&gt;
&lt;h3 id=&#34;49-概念轴&#34;&gt;4.9 概念轴&lt;/h3&gt;
&lt;p&gt;男性概念向量由多个男性词的向量加总求均值得到，女性概念向量算法类似。当性质或方向明显相反的两个概念向量相减， 得到的新的向量，我们可以称之为**&lt;em&gt;概念轴向量 Concept Axis&lt;/em&gt;**。常见的概念轴，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 尺寸(大, 小)
- 湿度(干燥,潮湿)
- 性别(男, 女)
- 财富(富裕, 贫穷)
- 等
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其实任意概念的向量也可看做概念轴，即该概念向量与 0 向量相减。只不过两组性质方向相反的方式得到的概念轴， 在语义上更稳定。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 定义词语列表&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;phy_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;游泳&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;跑步&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;篮球&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;羽毛球&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;马拉松&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;马术&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;徒步&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;rich_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;富裕&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;财富&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;金钱&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;豪宅&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;豪车&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;奢侈品&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;投资&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;股票&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;基金&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;黄金&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;钻石&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;游艇&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;私人飞机&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;企业家&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;富豪&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;成功&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;繁荣&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;奢华&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;贵族&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;高收入&amp;#39;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;poor_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;贫穷&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;贫困&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;饥饿&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;失业&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;低收入&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;简陋&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;破旧&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;乞丐&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;流浪&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;欠债&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;破产&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;困境&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;艰难&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;挣扎&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;匮乏&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;落后&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;无助&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;绝望&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;赤贫&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;温饱&amp;#39;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;phy_project_on_fortune&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sematic_projection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                               &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;phy_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                               &lt;span class=&#34;n&#34;&gt;poswords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rich_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                               &lt;span class=&#34;n&#34;&gt;negwords&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;poor_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                               &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;phy_project_on_fortune&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;跑步&amp;#39;, -1.82),
 (&amp;#39;徒步&amp;#39;, -0.82),
 (&amp;#39;游泳&amp;#39;, -0.19),
 (&amp;#39;羽毛球&amp;#39;, 0.57),
 (&amp;#39;马拉松&amp;#39;, 0.62),
 (&amp;#39;马术&amp;#39;, 1.15),
 (&amp;#39;篮球&amp;#39;, 4.0)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;计算结果中， 数值越大越接近于 c_words2, 越小越接近于 c_words1 。 可以看到在财富概念轴向量上的投影， 篮球不太准，但是其他几项基本上看出运动的贫富性。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;410-类比-king-man--woman--queen&#34;&gt;4.10 类比 king-man + woman ~ queen&lt;/h3&gt;
&lt;p&gt;每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/king-queen-formular.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;这两个词相减，按感觉应该得到的是性别方向，雄性-&amp;gt;雌性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;gender_direction_1 = vector(man)-vector(woman)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;gender_direction_2 = vector(king)-vector(queen)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;那两个性别方向应该近似，即 gender_direction_1 约等于 gender_direction_2 ，将其看做等式就得到如下公式：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;vector(理应近似 queen) = vector(king)-vector(men)+vector(women)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;现在我们检查三个语义向量计算出的新的向量是否有与 queen 相关的语义信息。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;semactic_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;semantic_centroid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                  &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;men_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;semactic_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;男&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;男孩&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;男人&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;他&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;父亲&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;爸爸&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;爷爷&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;women_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;semactic_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;女&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;女孩&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;女人&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;她&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;母亲&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;妈妈&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;奶奶&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;king_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;semactic_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;国王&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;皇帝&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;帝王&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;大帝&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 假设 king- queen 约等于 man -woman&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# result 近似等于 king - queen + women&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;result_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;king_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;men_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;women_vector&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 现在检查 result_vector 的语义应该与queen相关&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;皇帝&amp;#39;, 0.8448051810264587),
 (&amp;#39;王后&amp;#39;, 0.8056979179382324),
 (&amp;#39;国王&amp;#39;, 0.8004385232925415),
 (&amp;#39;帝王&amp;#39;, 0.7693961262702942),
 (&amp;#39;君主&amp;#39;, 0.7663125991821289),
 (&amp;#39;皇后&amp;#39;, 0.7614380717277527),
 (&amp;#39;太后&amp;#39;, 0.7463700175285339),
 (&amp;#39;妃子&amp;#39;, 0.7433678507804871),
 (&amp;#39;君王&amp;#39;, 0.7407413125038147),
 (&amp;#39;皇子&amp;#39;, 0.7380139231681824),
 (&amp;#39;王位&amp;#39;, 0.7319545745849609),
 (&amp;#39;皇上&amp;#39;, 0.7215542197227478),
 (&amp;#39;登基&amp;#39;, 0.7210745215415955),
 (&amp;#39;大臣&amp;#39;, 0.714862048625946),
 (&amp;#39;伊丽莎白一世&amp;#39;, 0.702217698097229),
 (&amp;#39;王朝&amp;#39;, 0.7000151872634888),
 (&amp;#39;宫女&amp;#39;, 0.6997070908546448),
 (&amp;#39;驾崩&amp;#39;, 0.6992778182029724),
 (&amp;#39;王妃&amp;#39;, 0.6981185078620911),
 (&amp;#39;昏君&amp;#39;, 0.6974363923072815)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看到三个语义向量四则运算出的 result_vector 与 queen 仍具有较高的相关性。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五获取资料&#34;&gt;五、获取资料&lt;/h2&gt;
&lt;p&gt;除了本文介绍的这个 1000w 条影评数据集， 大邓还有 2 个类似的豆瓣影评数据集，影评记录量 212w 和 442 w 条。 两个数据集下载链接我都公开，感兴趣的可以都下载下来。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 免费  douba-movie-1000w 链接: https://pan.baidu.com/s/15C0fn7oyYEFvuQtPO8tw8Q?pwd=1g7m 提取码: 1g7m
- 免费 douban-movie-1000w-Word2Vec.200.15.bin
链接: https://pan.baidu.com/s/1fK8LhLmK4_xq-eHzNn42lg?pwd=2hwr 提取码: 2hwr
- 免费 douban-movie-442w 链接: https://pan.baidu.com/s/1T_LPuxEZ_W8xfYcxV7rW5Q?pwd=a683 提取码: a683
- 免费 douban-movie-212w 链接: :https://pan.baidu.com/s/1VBwnOqfMPu_Y48bMlQ4oiw?pwd=t8id
 提取码: t8id

- 免费词向量      https://cntext.readthedocs.io/zh-cn/latest/embeddings.html

- 100元  cntext-2.1.6-py3-none-any.whl   如有需要，加微信 ***372335839***， 备注「姓名-学校-专业」
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/&#34;&gt;数据集 | 3394w 条豆瓣书评数据集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/&#34;&gt;实验 | 使用 Stanford Glove 代码训练中文语料的 GloVe 模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/&#34;&gt;可视化 | 人民日报语料反映七十年文化演变&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/&#34;&gt;词向量 | 使用 MD&amp;amp;A2001-2023 语料训练 Word2Vec 模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/&#34;&gt;实验 | 使用 Stanford Glove 代码训练中文语料的 Glove 模型&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
</description>
      <content:encoded><![CDATA[<p>本文内容</p>
<ol>
<li>介绍豆瓣影评数据集</li>
<li>构造语料训练 <strong><em>Word2Vec</em></strong> 模型</li>
<li>获取数据 <strong><em>cntext2.x</em></strong> &amp; <strong><em>Word2Vec</em></strong> 模型文件</li>
</ol>
<p><br><br></p>
<h2 id="一豆瓣影评数据集">一、豆瓣影评数据集</h2>
<h3 id="11-数据集介绍">1.1 数据集介绍</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据集: douba-movie-1000w

数据源: 豆瓣电影

记录数:
   - 电影 10269 部
   - 影评 10310989 条

体积: 1.35G
</code></pre></div><p>该数据集正好弥补下国内公开电影数据集的空缺， 数据已经过初步清洗，可用于推荐系统、情感分析、知识图谱、新闻传播学、社会学文化变迁等多个领域(或主题)。</p>
<br>
<h3 id="12-读取数据">1.2 读取数据</h3>
<p>下载 <strong><em>douba-movie-1000w.zip</em></strong> 解压后，可以看到数据集中有一个 <strong><em>all_movies_with_id.csv</em></strong> 文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;all_movies_with_id.csv&#39;</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<br>
<h3 id="13-所含字段">1.3 所含字段</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39; - </span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> - ID
 - Movie_Name  电影名
 - Score  豆瓣电影评分(1-10)
 - Review_People  评论者人数
 - Star_Distribution  评论评分分布(1-5, 含多个数值，数值以%间隔)
 - Craw_Date 爬虫运行日期
 - Username 豆瓣评论者用户名
 - Date 影评日期
 - Star  影评评分(1-5)
 - Comment 影评内容
 - Comment_Distribution 影评评分分布
 - Like 影评获得的喜欢数
</code></pre></div><p><br><br></p>
<h2 id="二-构造语料训练-word2vec">二、 构造语料&amp;训练 Word2Vec</h2>
<h3 id="21-构造语料">2.1 构造语料</h3>
<p>将字段 <strong><em>Comment</em></strong> 中所有文本汇总到 <strong><em>douban-movie-1000w.txt</em></strong>,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># 读取数据，只读取字段sign</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;all_movies_with_id.csv&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sign&#39;</span><span class="p">])</span>

<span class="c1"># 将sign列中的所有文本汇总到douban-movie-1000w.txt</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;douban-movie-1000w.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Comment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># 删除df和text变量，释放内存</span>
<span class="k">del</span> <span class="n">df</span>
<span class="k">del</span> <span class="n">text</span>

</code></pre></div><br>
<h3 id="22-配置-cntext216">2.2 配置 cntext2.1.6</h3>
<p>将 <strong><em>cntext-2.1.6-py3-none-any.whl</em></strong> 放置于桌面，打开 <strong><em>cmd</em></strong> (苹果电脑打开 terminal)， 输入 cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <strong><em>cmd</em></strong> (苹果电脑打开 terminal) 中使用 <strong><em>pip3</em></strong> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext-2.1.6-py3-none-any.whl
</code></pre></div><p>文末有 <strong><em>cntext-2.1.6-py3-none-any.whl</em></strong> 获取方式</p>
<br>
<h3 id="23-训练-word2vec">2.3 训练 Word2Vec</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># cntext为2.1.6</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 训练Word2Vec模型</span>
<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;douban-movie-1000w.txt&#39;</span><span class="p">,</span>
                 <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                 <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                 <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/douban-movie-1000w_cache.txt Not Found or Empty, Preprocessing Corpus
Reading Preprocessed Corpus from output/douban-movie-1000w_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 2965 s.
Output Saved To: output/douban-movie-1000w.200.15.txt
</code></pre></div><p>经过半个小时的训练， 得到</p>
<ul>
<li>模型文件 <strong><em>output/douban-movie-1000w-Word2Vec.200.15.txt</em></strong></li>
<li>模型文件 <strong><em>output/douban-movie-1000w-Word2Vec.200.15.bin</em></strong></li>
<li>缓存文件 <strong><em>output/douban-movie-1000w_cache.txt</em></strong></li>
</ul>
<p>其中模型文件有 <strong><em>txt</em></strong> 和 <strong><em>bin</em></strong> 两种格式， 信息量完全等同。 <strong><em>txt</em></strong> 可以用记事本打开查看， 而 <strong><em>bin</em></strong> 则是二进制文件， 体积更小。 已训练好的模型， 因 bin 格式体积更小， 便于分享。</p>
<br>
<h3 id="24-评估模型">2.4 评估模型</h3>
<p>使用近义法和类比法， 判断模型的表现。详情可查看<a href="https://cntext.readthedocs.io/zh-cn/latest/model.html">文档</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">evaluate_similarity</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">)</span>

<span class="n">ct</span><span class="o">.</span><span class="n">evaluate_analogy</span><span class="p">(</span><span class="n">w2v_model</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">近义测试: similarity.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/similarity.txt

评估结果：
+----------+------------+----------------------------+
| 发现词语 | 未发现词语 | Spearman&#39;s Rank Coeficient |
+----------+------------+----------------------------+
|   459    |     78     |            0.43            |
+----------+------------+----------------------------+


类比测试: analogy.txt
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cntext/model/evaluate_data/analogy.txt
Processing Analogy Test: 100%|██████████████| 1198/1198 [00:11&lt;00:00, 99.91it/s]

评估结果：
+--------------------+----------+------------+------------+----------+
|      Category      | 发现词语 | 未发现词语 | 准确率 (%) | 平均排名 |
+--------------------+----------+------------+------------+----------+
| CapitalOfCountries |   615    |     62     |   39.02    |   2.98   |
|   CityInProvince   |   175    |     0      |   28.57    |   4.74   |
| FamilyRelationship |   272    |     0      |   92.65    |   1.48   |
|   SocialScience    |    8     |     62     |   25.00    |   6.00   |
+--------------------+----------+------------+------------+----------+
</code></pre></div><p><strong>近义测试</strong>: Spearman&rsquo;s Rank Coeficient 系数取值[-1, 1], 取值越大， 说明模型表现越好。</p>
<br>
<p><strong>类比测试</strong>:</p>
<ul>
<li>CapitalOfCountries 豆瓣影评语料在此项表现尚可，可能目前电影库中有一定比例的外国素材。</li>
<li>CityInProvince 豆瓣影评语料在此项表现较差，不太可能是中国素材太少，可能大多数省市以类似汉东省的形式出现。这是我的猜测。 <a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">人民网留言板语料 Word2Vec</a>中，该项准确率 100%。</li>
<li>FamilyRelationship 豆瓣影评体现的是电影相关内容，而电影永远的主题是人性， 内容少不了家长里短，七大姑八大姨，所以此项准确率高达 92.65%。 以<a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">年报 MD&amp;A</a>为例，此处准确率只有 10%。</li>
<li>SocialScience 豆瓣影评语料在此项表现一般， 应该是语料中常见的社会科学词语提及较少。</li>
</ul>
<p>整体而言，语料训练的效果很不错，抓住了数据场景的独特性语义。</p>
<p><br><br></p>
<h2 id="四使用-word2vec">四、使用 Word2Vec</h2>
<h3 id="41-导入-word2vec-模型文件">4.1 导入 Word2Vec 模型文件</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 导入模型，请注意路径。</span>
<span class="c1"># 「当前代码」 与 「output」 同处于一个文件夹内</span>

<span class="n">dm_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/douban-movie-1000w-Word2Vec.200.15.txt&#39;</span><span class="p">)</span>
<span class="c1"># dm_w2v = ct.load_w2v(&#39;output/douban-movie-1000w-Word2Vec.200.15.bin&#39;)</span>

<span class="n">dm_w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading output/douban-movie-1000w-Word2Vec.200.15.txt...
&lt;gensim.models.keyedvectors.KeyedVectors at 0x314193830&gt;
</code></pre></div><br>
<h3 id="42-keyedvectors-的操作方法或属性">4.2 KeyedVectors 的操作方法(或属性)</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><em>KeyedVectors.index_to_key</em></strong></td>
<td>获取词汇表中的所有单词。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.key_to_index</em></strong></td>
<td>获取单词到索引的映射。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.vector_size</em></strong></td>
<td>获取 GloVe 模型中任意词向量的维度。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.get_vector(word)</em></strong></td>
<td>获取给定单词的词向量。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.similar_by_word(word, topn=10)</em></strong></td>
<td>获取某词语最相似的 10 个近义词。</td>
</tr>
<tr>
<td><strong><em>KeyedVectors.similar_by_vector(vector, topn=10)</em></strong></td>
<td>获取词向量最相似的 10 个近义词。</td>
</tr>
</tbody>
</table>
<br>
<h3 id="43-查看词表">4.3 查看词表</h3>
<p>查看词表所有单词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">index_to_key</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;电影&#39;,
 &#39;一个&#39;,
 &#39;没有&#39;,
 &#39;喜欢&#39;,
 ...
 &#39;跟着&#39;,
 &#39;意识&#39;,
 &#39;态度&#39;,
 ...]
</code></pre></div><p>为了方便查看， 这里只展示部分数据。</p>
<br>
<h3 id="44-词表映射">4.4 词表映射</h3>
<p>查看单词到索引的映射</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">key_to_index</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;电影&#39;: 0,
 &#39;一个&#39;: 1,
 &#39;没有&#39;: 2,
...
&#39;跟着&#39;: 997,
 &#39;意识&#39;: 998,
 &#39;态度&#39;: 999,
 ...}
</code></pre></div><br>
<h3 id="45-向量维度数">4.5 向量维度数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;词表有 </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span><span class="si">}</span><span class="s1"> 个词&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;向量是 </span><span class="si">{</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">vector_size</span><span class="si">}</span><span class="s1"> 维&#39;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">词表有 426646 个词
向量是 200 维
</code></pre></div><br>
<h3 id="46-获取词向量">4.6 获取词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-1.24090052e+00, -6.79377019e-01,  1.42518425e+00, -1.46615291e+00,
       -9.53197628e-02,  6.50456071e-01, -2.97696137e+00,  2.20916629e+00,
        6.12876177e-01,  1.63172066e+00,  4.91760701e-01, -9
        ......
        ......
         -1.42494082e+00,  2.49131727e+00, -6.27597034e-01, -7.91438043e-01,
       -4.54898655e-01,  1.37747681e+00, -4.20672953e-01, -1.53694853e-01,
        1.04936564e+00,  2.18786263e+00, -8.07472587e-01, -8.32003877e-02],
      dtype=float32)
</code></pre></div><br>
<h3 id="47-近义词">4.7 近义词</h3>
<p>根据词语查看近义词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 近义词</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;相当给力&#39;, 0.6180022358894348),
 (&#39;太给力&#39;, 0.6019443273544312),
 (&#39;带劲&#39;, 0.5840415954589844),
 (&#39;不给力&#39;, 0.5774183869361877),
 (&#39;过瘾&#39;, 0.5616626739501953),
 (&#39;牛叉&#39;, 0.553788959980011),
 (&#39;出彩&#39;, 0.5414286851882935),
 (&#39;精彩&#39;, 0.5332293510437012),
 (&#39;看得过瘾&#39;, 0.5250197649002075),
 (&#39;大赞&#39;, 0.5205727219581604)]
</code></pre></div><br>
<p>根据向量查找最相似的近义词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">word_vector</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">word_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;给力&#39;, 1.0),
 (&#39;相当给力&#39;, 0.6180021166801453),
 (&#39;太给力&#39;, 0.6019443273544312),
 (&#39;带劲&#39;, 0.5840415954589844),
 (&#39;不给力&#39;, 0.5774183869361877),
 (&#39;过瘾&#39;, 0.5616626739501953),
 (&#39;牛叉&#39;, 0.5537890195846558),
 (&#39;出彩&#39;, 0.5414287447929382),
 (&#39;精彩&#39;, 0.5332292914390564),
 (&#39;看得过瘾&#39;, 0.5250197649002075)]
</code></pre></div><br>
<h3 id="48-计算多个词的中心向量">4.8 计算多个词的中心向量</h3>
<p>我们可以计算「宇宙」、「飞船」、「战争」的宇宙语义向量（中心向量）。 并试图寻找中心向量 <strong><em>universe_vector</em></strong> 的最相似的 10 个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 几个词语构建的宇宙语义向量</span>
<span class="n">universe_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_centroid</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">dm_w2v</span><span class="p">,</span>
                                       <span class="n">words</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;宇宙&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>


<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">universe_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;宇宙&#39;, 0.7568532228469849),
 (&#39;星系&#39;, 0.7090039253234863),
 (&#39;飞船&#39;, 0.7080673575401306),
 (&#39;人类文明&#39;, 0.6973789930343628),
 (&#39;战舰&#39;, 0.6890057325363159),
 (&#39;母舰&#39;, 0.6864359974861145),
 (&#39;星球&#39;, 0.6799622774124146),
 (&#39;卫星&#39;, 0.6799139976501465),
 (&#39;星际&#39;, 0.6789332032203674),
 (&#39;空间站&#39;, 0.6780815124511719),
 (&#39;地球&#39;, 0.6769616603851318),
 (&#39;外太空&#39;, 0.6683873534202576),
 (&#39;核战&#39;, 0.6669113039970398),
 (&#39;外星飞船&#39;, 0.6592534780502319),
 (&#39;木星&#39;, 0.6586896777153015),
 (&#39;能源&#39;, 0.6562989950180054),
 (&#39;战争&#39;, 0.6556441187858582),
 (&#39;巨兽&#39;, 0.6544537544250488),
 (&#39;月球&#39;, 0.6525537967681885),
 (&#39;一艘&#39;, 0.6521110534667969)]
</code></pre></div><p>语义捕捉的很准哦。</p>
<h3 id="49-概念轴">4.9 概念轴</h3>
<p>男性概念向量由多个男性词的向量加总求均值得到，女性概念向量算法类似。当性质或方向明显相反的两个概念向量相减， 得到的新的向量，我们可以称之为**<em>概念轴向量 Concept Axis</em>**。常见的概念轴，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 尺寸(大, 小)
- 湿度(干燥,潮湿)
- 性别(男, 女)
- 财富(富裕, 贫穷)
- 等
</code></pre></div><p>其实任意概念的向量也可看做概念轴，即该概念向量与 0 向量相减。只不过两组性质方向相反的方式得到的概念轴， 在语义上更稳定。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 定义词语列表</span>
<span class="n">phy_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;游泳&#39;</span><span class="p">,</span> <span class="s1">&#39;跑步&#39;</span><span class="p">,</span> <span class="s1">&#39;篮球&#39;</span><span class="p">,</span> <span class="s1">&#39;羽毛球&#39;</span><span class="p">,</span> <span class="s1">&#39;马拉松&#39;</span><span class="p">,</span> <span class="s1">&#39;马术&#39;</span><span class="p">,</span> <span class="s1">&#39;徒步&#39;</span><span class="p">]</span>

<span class="n">rich_words</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">,</span> <span class="s1">&#39;金钱&#39;</span><span class="p">,</span> <span class="s1">&#39;豪宅&#39;</span><span class="p">,</span> <span class="s1">&#39;豪车&#39;</span><span class="p">,</span>
    <span class="s1">&#39;奢侈品&#39;</span><span class="p">,</span> <span class="s1">&#39;投资&#39;</span><span class="p">,</span> <span class="s1">&#39;股票&#39;</span><span class="p">,</span> <span class="s1">&#39;基金&#39;</span><span class="p">,</span> <span class="s1">&#39;黄金&#39;</span><span class="p">,</span>
    <span class="s1">&#39;钻石&#39;</span><span class="p">,</span> <span class="s1">&#39;游艇&#39;</span><span class="p">,</span> <span class="s1">&#39;私人飞机&#39;</span><span class="p">,</span> <span class="s1">&#39;企业家&#39;</span><span class="p">,</span> <span class="s1">&#39;富豪&#39;</span><span class="p">,</span>
    <span class="s1">&#39;成功&#39;</span><span class="p">,</span> <span class="s1">&#39;繁荣&#39;</span><span class="p">,</span> <span class="s1">&#39;奢华&#39;</span><span class="p">,</span> <span class="s1">&#39;贵族&#39;</span><span class="p">,</span> <span class="s1">&#39;高收入&#39;</span>
<span class="p">]</span>

<span class="n">poor_words</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;贫困&#39;</span><span class="p">,</span> <span class="s1">&#39;饥饿&#39;</span><span class="p">,</span> <span class="s1">&#39;失业&#39;</span><span class="p">,</span> <span class="s1">&#39;低收入&#39;</span><span class="p">,</span>
    <span class="s1">&#39;简陋&#39;</span><span class="p">,</span> <span class="s1">&#39;破旧&#39;</span><span class="p">,</span> <span class="s1">&#39;乞丐&#39;</span><span class="p">,</span> <span class="s1">&#39;流浪&#39;</span><span class="p">,</span> <span class="s1">&#39;欠债&#39;</span><span class="p">,</span>
    <span class="s1">&#39;破产&#39;</span><span class="p">,</span> <span class="s1">&#39;困境&#39;</span><span class="p">,</span> <span class="s1">&#39;艰难&#39;</span><span class="p">,</span> <span class="s1">&#39;挣扎&#39;</span><span class="p">,</span> <span class="s1">&#39;匮乏&#39;</span><span class="p">,</span>
    <span class="s1">&#39;落后&#39;</span><span class="p">,</span> <span class="s1">&#39;无助&#39;</span><span class="p">,</span> <span class="s1">&#39;绝望&#39;</span><span class="p">,</span> <span class="s1">&#39;赤贫&#39;</span><span class="p">,</span> <span class="s1">&#39;温饱&#39;</span>
<span class="p">]</span>

<span class="n">phy_project_on_fortune</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">wv</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="p">,</span>
                                               <span class="n">words</span> <span class="o">=</span> <span class="n">phy_words</span><span class="p">,</span>
                                               <span class="n">poswords</span> <span class="o">=</span><span class="n">rich_words</span><span class="p">,</span>
                                               <span class="n">negwords</span> <span class="o">=</span><span class="n">poor_words</span><span class="p">,</span>
                                               <span class="p">)</span>

<span class="n">phy_project_on_fortune</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;跑步&#39;, -1.82),
 (&#39;徒步&#39;, -0.82),
 (&#39;游泳&#39;, -0.19),
 (&#39;羽毛球&#39;, 0.57),
 (&#39;马拉松&#39;, 0.62),
 (&#39;马术&#39;, 1.15),
 (&#39;篮球&#39;, 4.0)]
</code></pre></div><p>计算结果中， 数值越大越接近于 c_words2, 越小越接近于 c_words1 。 可以看到在财富概念轴向量上的投影， 篮球不太准，但是其他几项基本上看出运动的贫富性。</p>
<br>
<h3 id="410-类比-king-man--woman--queen">4.10 类比 king-man + woman ~ queen</h3>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p><img loading="lazy" src="img/king-queen-formular.png" alt=""  />
</p>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p><strong><em>gender_direction_1 = vector(man)-vector(woman)</em></strong></p>
<p><strong><em>gender_direction_2 = vector(king)-vector(queen)</em></strong></p>
<p>那两个性别方向应该近似，即 gender_direction_1 约等于 gender_direction_2 ，将其看做等式就得到如下公式：</p>
<p><strong><em>vector(理应近似 queen) = vector(king)-vector(men)+vector(women)</em></strong></p>
<p>现在我们检查三个语义向量计算出的新的向量是否有与 queen 相关的语义信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">semactic_vector</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_centroid</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span>
                                  <span class="n">words</span><span class="o">=</span><span class="n">words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vector</span>


<span class="n">men_vector</span> <span class="o">=</span> <span class="n">semactic_vector</span><span class="p">(</span><span class="n">dm_w2v</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;男人&#39;</span><span class="p">,</span> <span class="s1">&#39;他&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爸爸&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">])</span>
<span class="n">women_vector</span> <span class="o">=</span> <span class="n">semactic_vector</span><span class="p">(</span><span class="n">dm_w2v</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;女人&#39;</span><span class="p">,</span> <span class="s1">&#39;她&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;妈妈&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">])</span>
<span class="n">king_vector</span> <span class="o">=</span> <span class="n">semactic_vector</span><span class="p">(</span><span class="n">dm_w2v</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;国王&#39;</span><span class="p">,</span> <span class="s1">&#39;皇帝&#39;</span><span class="p">,</span> <span class="s1">&#39;帝王&#39;</span><span class="p">,</span> <span class="s1">&#39;大帝&#39;</span><span class="p">])</span>
<span class="c1"># 假设 king- queen 约等于 man -woman</span>
<span class="c1"># result 近似等于 king - queen + women</span>
<span class="n">result_vector</span> <span class="o">=</span> <span class="n">king_vector</span> <span class="o">-</span> <span class="n">men_vector</span> <span class="o">+</span> <span class="n">women_vector</span>
<span class="c1"># 现在检查 result_vector 的语义应该与queen相关</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">result_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;皇帝&#39;, 0.8448051810264587),
 (&#39;王后&#39;, 0.8056979179382324),
 (&#39;国王&#39;, 0.8004385232925415),
 (&#39;帝王&#39;, 0.7693961262702942),
 (&#39;君主&#39;, 0.7663125991821289),
 (&#39;皇后&#39;, 0.7614380717277527),
 (&#39;太后&#39;, 0.7463700175285339),
 (&#39;妃子&#39;, 0.7433678507804871),
 (&#39;君王&#39;, 0.7407413125038147),
 (&#39;皇子&#39;, 0.7380139231681824),
 (&#39;王位&#39;, 0.7319545745849609),
 (&#39;皇上&#39;, 0.7215542197227478),
 (&#39;登基&#39;, 0.7210745215415955),
 (&#39;大臣&#39;, 0.714862048625946),
 (&#39;伊丽莎白一世&#39;, 0.702217698097229),
 (&#39;王朝&#39;, 0.7000151872634888),
 (&#39;宫女&#39;, 0.6997070908546448),
 (&#39;驾崩&#39;, 0.6992778182029724),
 (&#39;王妃&#39;, 0.6981185078620911),
 (&#39;昏君&#39;, 0.6974363923072815)]
</code></pre></div><p>可以看到三个语义向量四则运算出的 result_vector 与 queen 仍具有较高的相关性。</p>
<p><br><br></p>
<h2 id="五获取资料">五、获取资料</h2>
<p>除了本文介绍的这个 1000w 条影评数据集， 大邓还有 2 个类似的豆瓣影评数据集，影评记录量 212w 和 442 w 条。 两个数据集下载链接我都公开，感兴趣的可以都下载下来。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 免费  douba-movie-1000w 链接: https://pan.baidu.com/s/15C0fn7oyYEFvuQtPO8tw8Q?pwd=1g7m 提取码: 1g7m
- 免费 douban-movie-1000w-Word2Vec.200.15.bin
链接: https://pan.baidu.com/s/1fK8LhLmK4_xq-eHzNn42lg?pwd=2hwr 提取码: 2hwr
- 免费 douban-movie-442w 链接: https://pan.baidu.com/s/1T_LPuxEZ_W8xfYcxV7rW5Q?pwd=a683 提取码: a683
- 免费 douban-movie-212w 链接: :https://pan.baidu.com/s/1VBwnOqfMPu_Y48bMlQ4oiw?pwd=t8id
 提取码: t8id

- 免费词向量      https://cntext.readthedocs.io/zh-cn/latest/embeddings.html

- 100元  cntext-2.1.6-py3-none-any.whl   如有需要，加微信 ***372335839***， 备注「姓名-学校-专业」
</code></pre></div><p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/">数据集 | 3394w 条豆瓣书评数据集</a></li>
<li><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用 Stanford Glove 代码训练中文语料的 GloVe 模型</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></li>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">词向量 | 使用 MD&amp;A2001-2023 语料训练 Word2Vec 模型</a></li>
<li><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用 Stanford Glove 代码训练中文语料的 Glove 模型</a></li>
</ul>
<br>
]]></content:encoded>
    </item>
    
    <item>
      <title>word_in_context | 查看某类词的上下文，更好的理解文本数据</title>
      <link>https://textdata.cn/blog/2023-03-19-word-in-context/</link>
      <pubDate>Sun, 19 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-03-19-word-in-context/</guid>
      <description>通过一个单词所处的语境，我们可以了解该单词的含义。**该谚语源于英国语言学家 J.R. Firth 的理论，他认为单词的含义是由其周围的语境和与之相伴的其他单词所决定的，因此我们需要通过单词出现的上下文来理解其含义。这一理论在语言学、自然语言处理等领域有着广泛的应用。之前分享过 [ 使用正则表达式、文本向量化、线性回归算法从md&amp;amp;a数据中计算 「企业融资约束指标」 ]， 使用的是正则表达式识别融资约束文本。但是正则表达式设计十分复杂且有难度，在此之前，如果能够查看某些融资关键词附近上下文， 可帮助研究者更全面地了解数据集中关键词的使用情况和语境，更好的设计正则表达式，亦或许意外找出新的有价值的线索。</description>
      <content:encoded><![CDATA[<p>Firth（1957）有一句名言，理解一个词要从ta身边入手。</p>
<blockquote>
<p>You shall know a word by the company it keeps</p>
</blockquote>
<p>通过一个单词所处的语境，我们可以了解该单词的含义。<strong>该谚语源于英国语言学家 J.R. Firth 的理论，他认为单词的含义是由其周围的语境和与之相伴的其他单词所决定的，因此我们需要通过单词出现的上下文来理解其含义。这一理论在语言学、自然语言处理等领域有着广泛的应用</strong>。之前分享过</p>
<p><img loading="lazy" src="img/39faq-firth_words.png" alt=""  />
</p>
<p>之前分享过 <a href="https://textdata.cn/blog/2023-12-31-using-regex-to-compute-the-financial_constraints"> 使用正则表达式、文本向量化、线性回归算法从md&amp;a数据中计算 「企业融资约束指标」 </a>， 使用的是正则表达式识别融资约束文本。但是正则表达式设计十分复杂且有难度，在此之前，如果能够查看某些融资关键词附近上下文， 可帮助研究者更全面地了解数据集中关键词的使用情况和语境，更好的设计正则表达式，亦或许意外找出新的有价值的线索。</p>
<br>
<h2 id="代码">代码</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">jieba</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="k">def</span> <span class="nf">word_in_context</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">keywords</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    Given text and keywords, the task is to find the text where the keyword appears
</span><span class="s2">    Args:
</span><span class="s2">        text (str): input document, string format
</span><span class="s2">        keywords (list): keywords
</span><span class="s2">        window (int): return the text where the keyword appears, default is 3, meaning return 3 word.
</span><span class="s2">        lang (str, optional): setting the lang, only support chinese and english. Defaults to &#39;chinese&#39;.
</span><span class="s2">
</span><span class="s2">    Returns:
</span><span class="s2">        list contains multiple dictionaries, where each dictionary contains the sentence, keyword, and the sentence where the keyword appears
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="k">if</span> <span class="n">lang</span><span class="o">==</span><span class="s1">&#39;chinese&#39;</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&#34;你应该安装nltk和对应的nltk_data, 请看B站https://www.bilibili.com/video/BV14A411i7DB&#34;</span><span class="p">)</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">]</span>
    <span class="n">kw_idxss</span> <span class="o">=</span> <span class="p">[[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">keyword</span><span class="p">]</span> <span class="k">for</span> <span class="n">keyword</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">]</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">keyword</span><span class="p">,</span> <span class="n">kw_idxs</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">keywords</span><span class="p">,</span> <span class="n">kw_idxss</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">kw_idxs</span><span class="p">:</span>
            <span class="n">half</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span> <span class="o">-</span> <span class="n">half</span><span class="p">)</span>
            <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">half</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">row</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;keyword&#39;</span><span class="p">:</span> <span class="n">keyword</span><span class="p">,</span> 
                   <span class="s1">&#39;context&#39;</span><span class="p">:</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">start</span><span class="p">:</span> <span class="n">end</span><span class="p">])</span> <span class="k">if</span> <span class="n">lang</span><span class="o">==</span><span class="s1">&#39;chinese&#39;</span> <span class="k">else</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">start</span><span class="p">:</span> <span class="n">end</span><span class="p">])</span>
                      <span class="p">}</span>
            <span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>


</code></pre></div><p><br><br></p>
<h2 id="练习">练习</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#测试代码，假设zh_text是年报文本，从找找出丝网词相关词的上下文</span>
<span class="n">zh_text</span> <span class="o">=</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">【插入一条自家广告】大邓自己家的家，
</span><span class="s2">安平县多隆丝网制品，生产销售不锈钢轧花网、
</span><span class="s2">电焊网、石笼网、刀片刺绳、冲孔网等丝网制品。
</span><span class="s2">联系人 邓颖静 0318-7686899
</span><span class="s2">
</span><span class="s2">人生苦短，我学Python
</span><span class="s2">在社科中，可以用Python做文本分析
</span><span class="s2">Python是一门功能强大的编程语言，广泛应用在经管社科领域。
</span><span class="s2">可以做网络爬虫、文本分析、LDA话题模型、相似度分析等。
</span><span class="s2">
</span><span class="s2">今年经济不景气，形势异常严峻。
</span><span class="s2">由于疫情不景气，静默管理， 产品积压， 公司经营困难。
</span><span class="s2">保就业促就业，任务十分艰巨。
</span><span class="s2">&#34;&#34;&#34;</span>

<span class="c1">#【产品词】上下文</span>
<span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">zh_text</span><span class="p">,</span> 
                <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;石笼&#39;</span><span class="p">],</span> 
                <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><table>
<thead>
<tr>
<th>keyword</th>
<th>context</th>
</tr>
</thead>
<tbody>
<tr>
<td>石笼</td>
<td>电焊网、石笼网、刀片刺绳</td>
</tr>
</tbody>
</table>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#【经营】上下文</span>
<span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">zh_text</span><span class="p">,</span> 
                <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;经营&#39;</span><span class="p">],</span> 
                <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><table>
<thead>
<tr>
<th>keyword</th>
<th>context</th>
</tr>
</thead>
<tbody>
<tr>
<td>经营</td>
<td>&gt;积压， 公司经营困难。\n保</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">#【Python】上下文</span>
<span class="n">word_in_context</span><span class="p">(</span><span class="n">text</span> <span class="o">=</span> <span class="n">zh_text</span><span class="p">,</span> 
                <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;python&#39;</span><span class="p">],</span> 
                <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><table>
<thead>
<tr>
<th>keyword</th>
<th>context</th>
</tr>
</thead>
<tbody>
<tr>
<td>python</td>
<td>人生苦短，我学python\n在社科中</td>
</tr>
<tr>
<td>python</td>
<td>中，可以用python做文本分析\n</td>
</tr>
<tr>
<td>python</td>
<td>做文本分析\npython是一门功能强大的</td>
</tr>
</tbody>
</table>
<p><br><br></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
