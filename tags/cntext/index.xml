<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>cntext on 大邓和他的PYTHON</title>
    <link>/tags/cntext/</link>
    <description>Recent content in cntext on 大邓和他的PYTHON</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 03 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="/tags/cntext/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>可视化 | 人民日报语料反映七十年文化演变</title>
      <link>https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/</guid>
      <description>使用人民日报1946-2023年之间的新闻数据，通过语义距离刻画文化的变迁。</description>
      <content:encoded><![CDATA[<h2 id="一引言">一、引言</h2>
<p>社会文化是一个不断演变的复杂系统，受到历史、科技、经济和社会变革等多种因素的影响。随着时代的推移，人们的语言使用和文化认知也经历着变迁，反映着社会的发展脉络。在这个背景下，使用Word2Vec等词嵌入技术来研究社会文化变迁和刻板印象的重要性日益凸显。</p>
<p>Word2Vec作为一种词向量表示方法，通过将词汇映射到高维空间中的向量，有效地捕捉了词语之间的语义关系。这使得我们能够以全新的方式理解语言的演变和文化认知的转变。通过对比不同时期的Word2Vec模型，我们可以深入挖掘语言的时代特征，捕捉到文化观念、价值观念以及社会角色的演变。</p>
<p>研究社会文化变迁和刻板印象，不仅有助于解构历史时刻下的社会结构和文化动态，还能为我们提供深刻的洞察力，揭示出社会变迁中潜在的驱动力和趋势。这种研究有助于建构更为全面、客观的历史记忆，帮助我们更好地理解人类行为背后的深层次原因。</p>
<p><br><br></p>
<h2 id="二训练模型">二、训练模型</h2>
<h3 id="21-获取数据">2.1 获取数据</h3>
<ul>
<li><a href="https://textdata.cn/blog/2023-12-14-daily-news-dataset/">新闻数据集 | 含 人民日报/经济日报/光明日报 等数十家媒体(2024.05)</a></li>
<li><a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">文本分析库 cntext2.x 获取方式&amp;使用手册</a></li>
</ul>
<br>
<h3 id="22--构造语料">2.2  构造语料</h3>
<p>本使用的 <em><strong>rmrb.csv.gz</strong></em> 对该数据集感兴趣的同学，可点击查看  <a href="https://textdata.cn/blog/2023-12-14-daily-news-dataset/">新闻数据集 | 含 人民日报/经济日报/光明日报 等数十家媒体(2024.05)</a>  。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">])</span>

<span class="c1"># 每5年构造一个语料txt文件</span>
<span class="k">for</span> <span class="n">date</span><span class="p">,</span> <span class="n">freq_df</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Grouper</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;5YE&#39;</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">date</span><span class="p">)</span>
    <span class="n">corpus_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;corpus&#39;</span><span class="p">)</span>
    <span class="n">corpus_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">corpus_file</span> <span class="o">=</span> <span class="n">corpus_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">date</span><span class="o">.</span><span class="n">year</span><span class="si">}</span><span class="s2">.txt&#34;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">corpus_file</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">text_series</span> <span class="o">=</span> <span class="n">freq_df</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="n">raw_text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text_series</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">1946-12-31 00:00:00
1951-12-31 00:00:00
1956-12-31 00:00:00
1961-12-31 00:00:00
1966-12-31 00:00:00
1971-12-31 00:00:00
1976-12-31 00:00:00
1981-12-31 00:00:00
1986-12-31 00:00:00
1991-12-31 00:00:00
1996-12-31 00:00:00
2001-12-31 00:00:00
2006-12-31 00:00:00
2011-12-31 00:00:00
2016-12-31 00:00:00
2021-12-31 00:00:00
2026-12-31 00:00:00
CPU times: user 2.64 s, sys: 1.54 s, total: 4.18 s
Wall time: 5.29 s
</code></pre></div><p><img loading="lazy" src="img/01-corpus.jpg" alt=""  />
</p>
<p>语料txt命名规则， 实际上每个 <em><strong>year.txt</strong></em> 是存储了 <em><strong>year-5</strong></em>  ~  <em><strong>year</strong></em> 期间的新闻数据。</p>
<p><em><strong>1946.txt</strong></em> 内实际上只存储了 <em><strong>1946.5.15</strong></em> ~ <em><strong>1946.12.31</strong></em> 之间半年多的数据， 由于数据量太小，后续训练出的 <em><strong>word2vec</strong></em> 模型，其语义大概率不准。</p>
<p><em><strong>2006.txt</strong></em> 存储了 <em><strong>2002.1.1. ~ 2006.12.31</strong></em> 之间所有的数据</p>
<p>而 <em><strong>2026.txt</strong></em> 则存储了 <em><strong>2022.1.1 ~ 2026.12.31</strong></em> 之间所有的数据</p>
<br>
<h2 id="三训练word2vec">三、训练word2vec</h2>
<h3 id="31-配置环境">3.1 配置环境</h3>
<p>安装方法，将 <em><strong>cntext-2.1.5-py3-none-any.whl</strong></em> 放置于桌面， 打开命令行 <em><strong>cmd</strong></em> （mac是terminal), 依次执行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
pip3 install cntext-2.1.5-py3-none-any.whl
</code></pre></div><blockquote>
<p><em><strong>cntext-2.1.5-py3-none-any.whl</strong></em>， 100元 ； 如需 cntext2.x 可加微信 372335839 获取。</p>
</blockquote>
<br>
<h3 id="32-开始训练">3.2 开始训练</h3>
<p>训练代码比较简单，已经封装到 <strong>cntext2.x</strong>， 只需几行代码即可。且 cntext2.x 对代码进行了优化， 训练速度更快， 内存占用更小。</p>
<p>训练环境 Mac 内存 96G， 大家回去可以试试 16G、32G，应该也能跑通。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">glob</span>


<span class="c1"># 获取corpus文件夹内的所有语料txt文件的文件路径</span>
<span class="n">corpus_files</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;corpus/*.txt&#39;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">corpus_file</span> <span class="ow">in</span> <span class="n">corpus_files</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">corpus_file</span><span class="p">)</span>
    <span class="c1"># 结果自动保存到output文件夹内</span>
    <span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="n">corpus_file</span><span class="p">,</span>
                 <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                 <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                 <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-corpus/1946.txt" data-lang="corpus/1946.txt">Mac(Linux) System, Enable Parallel Processing
Cache output/1946_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|███████████████████| 5954/5954 [00:07&lt;00:00, 757.68it/s]
Reading Preprocessed Corpus from output/1946_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 16 s. 
Output Saved To: output/1946-Word2Vec.200.15.txt
......
......
......
corpus/2026.txt
Mac(Linux) System, Enable Parallel Processing
Cache output/2026_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 100%|██████████████| 105037/105037 [00:34&lt;00:00, 3075.29it/s]
Reading Preprocessed Corpus from output/2026_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 194. 
Output Saved To: output/2026-Word2Vec.200.15.txt
CPU times: user 2h 38min 4s, sys: 4min 41s, total: 2h 42min 45s
Wall time: 1h 5min 39s
</code></pre></div><p><img loading="lazy" src="img/02-word2vec.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="四检查模型">四、检查模型</h2>
<p>现在我们要检查模型， 为了方便，我就随机抽查 1946/1981/2001/2026， 查看这四个模型关于「工业」的近义词，看模型语义捕捉的准不准。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">mfiles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;output/1946-Word2Vec.200.15.bin&#39;</span><span class="p">,</span>
          <span class="s1">&#39;output/1981-Word2Vec.200.15.bin&#39;</span><span class="p">,</span>
          <span class="s1">&#39;output/2001-Word2Vec.200.15.bin&#39;</span><span class="p">,</span>
          <span class="s1">&#39;output/2026-Word2Vec.200.15.bin&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">mfile</span> <span class="ow">in</span> <span class="n">mfiles</span><span class="p">:</span>
    <span class="n">w2v_model</span>  <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">mfile</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">mfile</span><span class="p">)</span>
    <span class="n">word_scores</span> <span class="o">=</span> <span class="n">w2v_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;工业&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">word_scores</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">output/1946-Word2Vec.200.15.bin
市场 0.9601176381111145
重工业 0.9589242935180664
工业部门 0.9484396576881409
物价 0.9464751482009888
商业 0.9423016309738159
工业原料 0.9378510117530823
焦煤 0.9368941783905029
物价暴涨 0.9348677396774292
第一年 0.9331346154212952
农产品 0.9329909682273865
输入 0.9329512119293213
农业 0.9327669143676758
水平 0.9323830008506775
通货 0.9320995807647705
国民经济 0.9268764853477478
投资 0.9261932373046875
输出 0.9258642792701721
钢铁工厂 0.925421953201294
工业生产 0.9251945614814758
十三亿 0.9251589775085449

Loading output/1981-Word2Vec.200.15.bin...
output/1981-Word2Vec.200.15.bin
工业部门 0.6696006655693054
重工业 0.6490920782089233
建筑业 0.6461381316184998
轻工业 0.6443966627120972
工业生产 0.6364479064941406
机器制造业 0.6220380067825317
化学工业 0.6116607785224915
钢铁工业 0.5941601991653442
加工工业 0.5932750701904297
电子工业 0.5880091190338135
轻纺工业 0.5786471366882324
食品工业 0.5777474045753479
重工业轻工业 0.5734774470329285
民用工业 0.5729294419288635
消费品生产 0.5721379518508911
纺织业 0.56629878282547
农业轻工业 0.5642068982124329
机器制造 0.5622154474258423
制造业 0.5620284676551819
化工 0.5588406324386597

Loading output/2001-Word2Vec.200.15.bin...
output/2001-Word2Vec.200.15.bin
重工业 0.6766582727432251
工业生产 0.6742461323738098
制造业 0.641242504119873
轻工业 0.615958571434021
传统产业 0.6039909720420837
加工工业 0.5936708450317383
机械电子 0.5892737507820129
工业部门 0.5891364216804504
轻工 0.5785651803016663
化学工业 0.5783289670944214
纺织 0.5708677172660828
支柱行业 0.5655868053436279
钢铁工业 0.5648497939109802
化工 0.5617026686668396
机械工业 0.5609593987464905
振兴国防科技 0.5588745474815369
纺织业 0.5520373582839966
工业体系 0.5505329370498657
工业总产值 0.5477191805839539
冶金纺织 0.5463222861289978

Loading output/2026-Word2Vec.200.15.bin...
output/2026-Word2Vec.200.15.bin
制造业 0.6705414056777954
工业生产 0.6067013144493103
智能制造 0.5936543941497803
轻工业 0.5885797142982483
钢铁行业 0.5884692072868347
化工 0.5675483345985413
钢铁企业 0.5637045502662659
工业互联网 0.559167742729187
装备制造业 0.5545477271080017
制造 0.5482359528541565
建筑业 0.5467448234558105
冶金 0.5400071740150452
规模工业 0.5395020246505737
重工业 0.537196695804596
钢铁 0.5245063304901123
工业遗产 0.5208563804626465
钢铁工业 0.5142995715141296
改数 0.512413740158081
纺织业 0.5109716653823853
规上工业 0.5082385540008545
</code></pre></div><p>从四个年代，我们可以看到中国人民对于 「<strong>工业</strong>」的认识发生了变化， 相比建国初期的一穷二白，工农业等领域经济凋敝； 而2026年的「<strong>工业</strong>」已实现工业现代化，更加注制造业、智能制造、工业互联网、装备制造业等概念。</p>
<p><br><br></p>
<h2 id="五对齐模型">五、对齐模型</h2>
<h3 id="51--为什么要进行对齐">5.1  为什么要进行对齐?</h3>
<p>Word2Vec是一种词嵌入（word embedding）算法，它将词语映射到高维空间中的向量，使得语义相近的词在该空间中距离较近。然而，不同年份的Word2Vec模型在训练时可能受到不同的语料库、训练参数等因素的影响，导致它们的向量空间之间存在一定的差异，所以不能直接拿不同年年份模型直接进行语义比较。</p>
<p><strong>Procrustes对齐算法目的是通过线性变换来使两个向量空间尽可能地对齐，以便进行比较</strong>。这个过程涉及到对两个向量空间进行旋转、缩放和平移等变换，使它们在某种意义上尽量一致。</p>
<p>具体原因包括：</p>
<ol>
<li><strong>词汇漂移（Lexical Drift）：</strong> 随着时间的推移，词汇的含义和使用可能发生变化，导致不同年份的语料库中的词语存在一定的漂移。Procrustes分析可以在一定程度上对齐这种漂移。</li>
<li><strong>训练参数不同：</strong> Word2Vec模型的训练参数，如窗口大小、迭代次数等，可能在不同年份有所不同，导致生成的向量空间差异较大。</li>
<li><strong>语料库的差异：</strong> 不同年份的语料库可能覆盖的主题、文体等存在差异，这也会影响词向量的学习结果。</li>
</ol>
<p>通过Procrustes对齐，可以在一定程度上解决这些问题，使得不同年份的Word2Vec模型在语义上更具可比性。这有助于在跨时间的语料库中进行一致的语义分析。</p>
<br>
<h3 id="52-对齐之后">5.2 对齐之后</h3>
<p>对齐后的Word2Vec模型进行的语义变迁研究：</p>
<ol>
<li><strong>词义演变：</strong> 比较不同年份相同词汇的词向量，观察其在向量空间中的位置变化，分析词义在语义空间中的演变趋势。</li>
<li><strong>语境变迁：</strong> 考察同一词语在不同年份的上下文中的变化，了解词语在不同语境下的语义演变情况。</li>
<li><strong>主题变迁：</strong> 通过对齐后的向量空间，分析不同年份语料库中词语的主题分布变化，探讨社会、文化因素对语言使用的影响。</li>
<li><strong>时代特征分析：</strong> 通过对比不同年份的模型，识别出每个时期在词向量空间中的独特特征，从而揭示时代背景对语义的影响。</li>
<li><strong>探索新兴词汇：</strong> 通过对比不同年份的模型，发现在语义空间中新兴词汇的出现和演变，了解新兴概念和文化趋势。</li>
</ol>
<p>总的来说，通过对齐Word2Vec模型，你可以更准确地比较不同年份的语料库，深入研究语义的演变和语言使用的变迁。这有助于揭示社会、文化、科技等方面的发展对语言表达的影响。</p>
<br>
<h3 id="53-对齐代码">5.3 对齐代码</h3>
<p>使用 <strong>cntext2.1.5</strong>，未公开，需微信大邓 372335839 购买获取。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">glob</span>

<span class="c1"># 基准模型</span>
<span class="n">base_wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/2026-Word2Vec.200.15.bin&#39;</span><span class="p">)</span>

<span class="c1">#将其他模型与基准模型对齐</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;output/*.bin&#39;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="n">other_wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="n">procrusted_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">procrustes_align</span><span class="p">(</span><span class="n">base_wv</span><span class="o">=</span><span class="n">base_wv</span><span class="p">,</span>
                                         <span class="n">other_wv</span><span class="o">=</span><span class="n">other_wv</span><span class="p">)</span>
    <span class="c1"># win</span>
    <span class="c1">#year = file.split(&#39;\\&#39;)[-1][:4]</span>
    
    <span class="c1"># mac</span>
    <span class="n">year</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">4</span><span class="p">]</span>
    
    <span class="n">output_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s1">&#39;Aligned_Word2Vec&#39;</span><span class="p">)</span>
    <span class="n">output_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">procrusted_w2v</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Aligned_Word2Vec/</span><span class="si">{</span><span class="n">year</span><span class="si">}</span><span class="s1">.200.15.bin&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading output/2026-Word2Vec.200.15.bin...
output/1956-Word2Vec.200.15.bin
Loading output/1956-Word2Vec.200.15.bin...

output/2026-Word2Vec.200.15.bin
Loading output/2026-Word2Vec.200.15.bin...

output/2021-Word2Vec.200.15.bin
Loading output/2021-Word2Vec.200.15.bin...

output/1951-Word2Vec.200.15.bin
Loading output/1951-Word2Vec.200.15.bin...

output/1946-Word2Vec.200.15.bin
Loading output/1946-Word2Vec.200.15.bin...

output/2001-Word2Vec.200.15.bin
Loading output/2001-Word2Vec.200.15.bin...

output/1981-Word2Vec.200.15.bin
Loading output/1981-Word2Vec.200.15.bin...

output/1971-Word2Vec.200.15.bin
Loading output/1971-Word2Vec.200.15.bin...

output/1976-Word2Vec.200.15.bin
Loading output/1976-Word2Vec.200.15.bin...

output/2006-Word2Vec.200.15.bin
Loading output/2006-Word2Vec.200.15.bin...

output/1986-Word2Vec.200.15.bin
Loading output/1986-Word2Vec.200.15.bin...

output/1961-Word2Vec.200.15.bin
Loading output/1961-Word2Vec.200.15.bin...

output/2011-Word2Vec.200.15.bin
Loading output/2011-Word2Vec.200.15.bin...

output/1991-Word2Vec.200.15.bin
Loading output/1991-Word2Vec.200.15.bin...

output/2016-Word2Vec.200.15.bin
Loading output/2016-Word2Vec.200.15.bin...

output/1996-Word2Vec.200.15.bin
Loading output/1996-Word2Vec.200.15.bin...

output/1966-Word2Vec.200.15.bin
Loading output/1966-Word2Vec.200.15.bin...

CPU times: user 1min 8s, sys: 49.7 s, total: 1min 58s
Wall time: 46.3 s
</code></pre></div><p><img loading="lazy" src="img/03-align.png" alt=""  />
</p>
<p><br><br></p>
<h2 id="六实验-文化变迁">六、实验-文化变迁</h2>
<p>时代的宣传必然在语义中深刻的影响社会认知，不同时代语料中必然蕴含着不同的文化特征，如语义距离的变化。这里我演示 两个对立词组分别与目标词组进行语义距离计算， 根据语义距离反应刻板印象态度偏见，其实这也反映了文化变迁。</p>
<h3 id="61-性别与成功">6.1 性别与成功</h3>
<p>男性、女性与成功之间的语义距离</p>
<p><strong>cntext2.1.5</strong> 内置了两种算法， 语义投影和语义距离，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">distance = distance(女, 成功) - distance(男, 成功)
</code></pre></div><p>如果distance趋近于0， 男女在成功概念上语义接近， 无明显刻板印象。</p>
<p>但是当distance明显大于0， 当人们聊到成功概念时，更容易联想到男性，而不是女性。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">import pandas as pd
import glob

gender_suceess_data = []

words = [&#39;成功&#39;, &#39;成就&#39;, &#39;胜利&#39;]
c_words1 = [&#39;女&#39;, &#39;女人&#39;, &#39;她&#39;, &#39;母亲&#39;, &#39;女儿&#39;, &#39;奶奶&#39;]
c_words2 = [&#39;男&#39;, &#39;男人&#39;, &#39;他&#39;, &#39;父亲&#39;, &#39;儿子&#39;, &#39;爷爷&#39;]

# 当前代码所处文件 与 Aligned_Word2Vec 处于同一文件夹内
mfiles = sorted(glob.glob(&#39;Aligned_Word2Vec/*.bin&#39;))
for file in mfiles:
    distance = ct.sematic_distance(wv=ct.load_w2v(file),
                                   words=words, 
                                   c_words1=c_words1, 
                                   c_words2=c_words2)
    data = dict()
    data[&#39;year&#39;] = file.split(&#39;/&#39;)[-1][:4]
    data[&#39;distance&#39;] = distance
    gender_suceess_data.append(data)
    

gender_success_df = pd.DataFrame(gender_suceess_data)
gender_success_df
</code></pre></div><p><img loading="lazy" src="img/04-df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">ct</span><span class="o">.</span><span class="n">matplotlib_chinese</span><span class="p">()</span> <span class="c1">#为正常显示中文</span>

<span class="n">gender_suceess_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;成功&#39;</span><span class="p">,</span> <span class="s1">&#39;成就&#39;</span><span class="p">,</span> <span class="s1">&#39;胜利&#39;</span><span class="p">]</span>
<span class="n">c_words1</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女人&#39;</span><span class="p">,</span> <span class="s1">&#39;她&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;女儿&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]</span>
<span class="n">c_words2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男人&#39;</span><span class="p">,</span> <span class="s1">&#39;他&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;儿子&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]</span>

<span class="n">mfiles</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;Aligned_Word2Vec/*.bin&#39;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">mfiles</span><span class="p">:</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">file</span><span class="p">),</span>
                                   <span class="n">words</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> 
                                   <span class="n">c_words1</span><span class="o">=</span><span class="n">c_words1</span><span class="p">,</span> 
                                   <span class="n">c_words2</span><span class="o">=</span><span class="n">c_words2</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">4</span><span class="p">]</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;distance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">distance</span>
    <span class="n">gender_suceess_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    
<span class="n">gender_success_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">gender_suceess_data</span><span class="p">)</span>
<span class="n">gender_success_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;人民日报在「成就」概念的文化变迁&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;大于0表示社会更容易将成功与男性联系起来&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/04-gender.png" alt=""  />
</p>
<p>从图中可以看到， 新中国初期， 我国的女性解放运动在全世界都是领先的，成果十分卓著。而今耳熟能详的口号恰好说明当时的宣传已经刻入每个中国人的认知中，如</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 谁说女子不如男
- 不爱红装爱武装
- 女人撑起半边天
...
</code></pre></div><br>
<p>提到「成功概念」时，在新中国初期，由于破除性别刻板印象，宣传更加中性， 立榜样考虑了性别的平衡。而随着时间推移，口号式的宣传运动沉寂后， 历史的惯性(传统文化的基因)可能会重新复活， 提到「成功概念」时，社会更容易将「成功」与「男性」联系起来。</p>
<br>
<h3 id="52-性别与责任">5.2 性别与责任</h3>
<p>成就与男性有更高的关联， 背后是否意味着传统文化建构的社会要求男性承担远多于女性的责任。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python">
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;svg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">scienceplots</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;science&#39;</span><span class="p">,</span> <span class="s1">&#39;no-latex&#39;</span><span class="p">,</span> <span class="s1">&#39;cjk-sc-font&#39;</span><span class="p">])</span>
<span class="n">ct</span><span class="o">.</span><span class="n">matplotlib_chinese</span><span class="p">()</span> <span class="c1">#为正常显示中文</span>

<span class="n">gender_responsibility_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">words</span> <span class="o">=</span>   <span class="p">[</span><span class="s1">&#39;责任&#39;</span><span class="p">,</span> <span class="s1">&#39;重担&#39;</span><span class="p">,</span> <span class="s1">&#39;担当&#39;</span><span class="p">]</span>
<span class="n">c_words1</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女人&#39;</span><span class="p">,</span> <span class="s1">&#39;她&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;女儿&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">]</span>
<span class="n">c_words2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男人&#39;</span><span class="p">,</span> <span class="s1">&#39;他&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;儿子&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">]</span>

<span class="n">mfiles</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;Aligned_Word2Vec/*.bin&#39;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">mfiles</span><span class="p">:</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sematic_distance</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="n">file</span><span class="p">),</span>
                                   <span class="n">words</span><span class="o">=</span><span class="n">words</span><span class="p">,</span> 
                                   <span class="n">c_words1</span><span class="o">=</span><span class="n">c_words1</span><span class="p">,</span> 
                                   <span class="n">c_words2</span><span class="o">=</span><span class="n">c_words2</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="mi">4</span><span class="p">]</span>
    <span class="n">data</span><span class="p">[</span><span class="s1">&#39;distance&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">distance</span>
    <span class="n">gender_responsibility_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    
<span class="n">gender_responsibility_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">gender_responsibility_data</span><span class="p">)</span>
<span class="n">gender_responsibility_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;人民日报在「责任」语义的文化变迁&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;大于0表示社会更容易将「责任」与男性联系起来&#39;</span><span class="p">)</span>
</code></pre></div><p><img loading="lazy" src="img/05-responsibility.png" alt=""  />
</p>
<p>从图中可以看出，在大多数年份， distance是大于0的，即 提到「责任」概念时，社会更容易联想到「男性」，而不是「女性」。</p>
<br>
<br>
<h2 id="七相关内容">七、相关内容</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[1]冉雅璇,李志强,刘佳妮,张逸石.大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用[J].南开管理评论:1-27.
[2]Hamilton, William L., Jure Leskovec, and Dan Jurafsky. &#34;Diachronic word embeddings reveal statistical laws of semantic change.&#34; arXiv preprint arXiv:1605.09096 (2016).
[3]Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. &#34;Word embeddings quantify 100 years of gender and ethnic stereotypes.&#34; Proceedings of the National Academy of Sciences 115, no. 16 (2018): E3635-E3644.
[3]Aceves, Pedro, and James A. Evans. “Mobilizing conceptual spaces: How word embedding models can inform measurement and theory within organization science.” Organization Science (2023).
[4]Kozlowski, A.C., Taddy, M. and Evans, J.A., 2019. The geometry of culture: Analyzing the meanings of class through word embeddings. American Sociological Review, 84(5), pp.905-949.
</code></pre></div><br>
<ul>
<li>
<p><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量  | 使用<strong>人民网领导留言板</strong>语料训练Word2Vec模型</a></p>
</li>
<li>
<p><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用 Stanford Glove 代码训练中文语料的 GloVe 模型</a></p>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>实验 | 使用Stanford Glove代码训练中文语料的Glove模型</title>
      <link>https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/</link>
      <pubDate>Fri, 28 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/</guid>
      <description>&lt;h2 id=&#34;一简介&#34;&gt;一、简介&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34;&gt;Stanford GloVe&lt;/a&gt;（Global Vectors for Word Representation）算法作为一种融合全局统计信息与局部上下文窗口的词嵌入模型，相较于Word2Vec仅依赖局部上下文，GloVe利用全局统计信息，能更精准地反映词频分布特征。例如，在高维词向量（如200D）中，GloVe在词语类比任务中准确率达75%，并在命名实体识别任务中优于其他词嵌入模型。因其高效的语义表征能力，在社会学、管理学等领域展现出广泛的应用价值。 相关词嵌入文献资料可阅读&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/&#34;&gt;OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/&#34;&gt;转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-09-literature-about-embeddings/&#34;&gt;文献汇总 | 词嵌入 与 社会科学中的偏见(态度)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-01-embeddings-and-attitude/&#34;&gt;词嵌入测量不同群体对某概念的态度(偏见)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二环境准备&#34;&gt;二、环境准备&lt;/h2&gt;
&lt;p&gt;cntext2.x 内置了 GloVe 训练所需的环境，支持 win 和 mac。&lt;/p&gt;
&lt;p&gt;获取&lt;a href=&#34;https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/&#34;&gt;cntext2.x&lt;/a&gt; 的安装文件 &lt;em&gt;&lt;strong&gt;cntext-2.1.5-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt;，并将该whl文件放置于桌面。执行以下安装命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
pip install cntext-2.1.5-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GloVe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dict_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;stopwords_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;min_count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_memory&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;4.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_iter&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_max&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;corpus_file&lt;/strong&gt;&lt;/em&gt;: 输入语料文件路径（文本格式）。该文件为分词后的语料文件。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;lang&lt;/strong&gt;&lt;/em&gt;: 语料文件的语言类型，默认为 &amp;lsquo;chinese&amp;rsquo;。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;dict_file&lt;/strong&gt;&lt;/em&gt;: 自定义词典txt文件路径，默认为None。utf-8编码。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;stopwords_file&lt;/strong&gt;&lt;/em&gt;: 停用词文件路径，默认为 None。utf-8编码。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;vector_size&lt;/strong&gt;&lt;/em&gt;: 词向量维度，默认 100。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;window_size&lt;/strong&gt;&lt;/em&gt;: 上下文窗口大小，默认 15。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;min_count&lt;/strong&gt;&lt;/em&gt;: 忽略出现次数低于此值的单词，默认 5。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;max_memory&lt;/strong&gt;&lt;/em&gt;: 可供使用的最大内存大小，单位为GB，默认 4;  该参数越大，训练越快。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;max_iter&lt;/strong&gt;&lt;/em&gt;: 训练的最大迭代次数，默认 15。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;x_max&lt;/strong&gt;&lt;/em&gt;: 共现矩阵中元素的最大计数值，默认 10。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三训练中文glove&#34;&gt;三、训练中文GloVe&lt;/h2&gt;
&lt;p&gt;我们其实只需要设置 &lt;em&gt;&lt;strong&gt;corpus_file&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;lang&lt;/strong&gt;&lt;/em&gt;， 但为了让大家知道&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上下文的窗口大小 &lt;em&gt;&lt;strong&gt;window_size&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;训练出模型词语的维度数 &lt;em&gt;&lt;strong&gt;vector_size&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 简化版调用。训练window_size=100维， vector_size=15&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# glove_wv = ct.GloVe(corpus_file=&amp;#39;data/三体.txt&amp;#39;, lang=&amp;#39;chinese&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 正常调用。训练window_size=15维， vector_size=50&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;glove_wv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GloVe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data/三体.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;glove_wv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Mac(Linux) System, Enable Parallel Processing
Cache output/三体_cache.txt Not Found or Empty, Preprocessing Corpus
Start Training GloVe
BUILDING VOCABULARY
Using vocabulary of size 6975.

COUNTING COOCCURRENCES
Merging cooccurrence files: processed 2106999 lines.

Using random seed 1743474106
SHUFFLING COOCCURRENCES
Merging temp files: processed 2106999 lines.

TRAINING MODEL
Read 2106999 lines.
Using random seed 1743474106
04/01/25 - 10:21.46AM, iter: 001, cost: 0.055981
04/01/25 - 10:21.46AM, iter: 002, cost: 0.050632
......
04/01/25 - 10:21.48AM, iter: 014, cost: 0.030047
04/01/25 - 10:21.48AM, iter: 015, cost: 0.029100

GloVe Training Cost 9 s. 
Output Saved To: output/三体-GloVe.50.15.txt
&amp;lt;gensim.models.keyedvectors.KeyedVectors at 0x331517440&amp;gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/05-glove.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h2 id=&#34;四使用中文glove模型&#34;&gt;四、使用中文GloVe模型&lt;/h2&gt;
&lt;h3 id=&#34;41-加载模型&#34;&gt;4.1 加载模型&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 加载word2vec模型.txt文件&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/三体-GloVe.50.15.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&amp;lt;gensim.models.keyedvectors.KeyedVectors at 0x336ff8dd0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-keyedvectors的操作方法或属性&#34;&gt;4.2 KeyedVectors的操作方法(或属性)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方法&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.index_to_key&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取词汇表中的所有单词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.key_to_index&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取单词到索引的映射。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.vector_size&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取GloVe模型中任意词向量的维度。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.get_vector(word)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取给定单词的词向量。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.similar_by_word(word, topn=10)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取某词语最相似的10个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.similar_by_vector(vector, topn=10)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取词向量最相似的10个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;421-词表&#34;&gt;4.2.1 词表&lt;/h3&gt;
&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;的&amp;#39;,
 &amp;#39;了&amp;#39;,
 &amp;#39;在&amp;#39;,
...
 &amp;#39;引力&amp;#39;,
 &amp;#39;所说&amp;#39;,
 &amp;#39;星际&amp;#39;,
 ...]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;422-词表映射&#34;&gt;4.2.2 词表映射&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;的&amp;#39;: 0,
 &amp;#39;了&amp;#39;: 1,
 &amp;#39;在&amp;#39;: 2,
...
 &amp;#39;引力&amp;#39;: 997,
 &amp;#39;所说&amp;#39;: 998,
 &amp;#39;星际&amp;#39;: 999,
 ...}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;423-向量维度数&#34;&gt;4.2.3 向量维度数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;词表有 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 个词&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;向量是 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv_model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 维&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;词表有 4365 个词
向量是 50 维
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;424-获取词向量&#34;&gt;4.2.4 获取词向量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查看「降临」的词向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;降临&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([ 0.672314,  0.020081,  0.653733,  0.598732, -0.680517, -0.049689,
       -0.16845 , -0.06759 , -0.147955,  0.024006,  0.264551, -0.050127,
        0.252063, -0.475633,  0.103722, -0.012481,  0.040755,  1.154912,
        0.742695,  0.048619, -0.514424, -1.184054,  0.515892, -0.1034  ,
        0.368755, -0.690357, -0.784287, -0.505814,  0.035807, -0.166354,
       -0.26149 ,  0.015089,  0.10626 , -0.215666, -0.374001, -0.123558,
        0.422617, -0.075277, -0.316387, -0.484295,  0.059687,  0.132621,
        0.192094, -0.591919,  0.236281,  0.164198, -0.058724,  1.285457,
        0.905606, -0.52032 ], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;425-近义词&#34;&gt;4.2.5 近义词&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;三体&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;叛军&amp;#39;, 0.7699569463729858),
 (&amp;#39;更新&amp;#39;, 0.7687217593193054),
 (&amp;#39;地球&amp;#39;, 0.760529100894928),
 (&amp;#39;全集&amp;#39;, 0.7575182914733887),
 (&amp;#39;最快&amp;#39;, 0.7426372170448303),
 (&amp;#39;世界&amp;#39;, 0.7262137532234192),
 (&amp;#39;最新&amp;#39;, 0.7219281792640686),
 (&amp;#39;游戏&amp;#39;, 0.7180070877075195),
 (&amp;#39;危机&amp;#39;, 0.7020451426506042),
 (&amp;#39;教&amp;#39;, 0.7012627720832825)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;426-计算多个词的中心向量&#34;&gt;4.2.6 计算多个词的中心向量&lt;/h3&gt;
&lt;p&gt;我们可以计算「三体」、「降临」、「组织」、「拯救」的中心向量eto_vector。 并试图寻找中心向量eto_vector的最相似的10个词。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;eto_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;semantic_centroid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;三体&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;降临&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;组织&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;拯救&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eto_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 寻找 eto_vector 语义最相似的10个词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eto_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[ 0.6267875   0.08975425  0.48438451  0.405128   -0.49928901  0.11347825
 -0.90057975  0.11877625 -0.27053049  0.344603    0.4368495  -0.3839495
  0.02633176 -0.138534    0.2531555  -0.0060905  -0.48776849  0.75548999
  0.72575876 -0.446079   -0.30361701 -1.039792    0.457687   -0.4286315
  0.44577325 -0.39119426 -0.4783935  -0.2596135  -0.32513325 -0.10315975
 -0.42880575 -0.48328425  0.129438   -0.17085625 -0.13454625 -0.070053
  0.68060375  0.16736924 -0.15664874 -0.20528575  0.385481    0.206432
  0.18913225 -0.93453825  0.58597099  0.60727924  0.009064    0.87661726
  0.65814423 -0.356567  ]

[(&amp;#39;降临&amp;#39;, 0.8707027435302734),
 (&amp;#39;组织&amp;#39;, 0.8625670671463013),
 (&amp;#39;三体&amp;#39;, 0.8621653914451599),
 (&amp;#39;派&amp;#39;, 0.8343338966369629),
 (&amp;#39;拯救&amp;#39;, 0.8301094174385071),
 (&amp;#39;叛军&amp;#39;, 0.784512460231781),
 (&amp;#39;地球&amp;#39;, 0.7536635398864746),
 (&amp;#39;世界&amp;#39;, 0.7245718836784363),
 (&amp;#39;外部&amp;#39;, 0.7078365087509155),
 (&amp;#39;入侵&amp;#39;, 0.6962169408798218)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;熟悉三体的朋友应该能联想到背叛人类的ETO(地球三体组织)有两个派别，分别是拯救派和降临派。&lt;/p&gt;
&lt;p&gt;ETO开发了一款虚拟现实游戏，它向参与者展示了三体世界的真实情况，包括其恶劣的自然条件、三体文明的历史及其科技水平等。通过参与这个游戏，玩家们能够逐渐了解三体世界的真相，并最终决定是否要加入到支持三体文明入侵地球的行列中来。&lt;/p&gt;
&lt;p&gt;这个游戏不仅充当了信息传递的媒介，也是甄别志同道合者的工具，让那些对人类社会现状不满、渴望变革的人们找到了组织，进而成为了背叛人类的叛军一员。在这个过程中，“三体游戏”起到了关键的作用，是连接地球人与三体世界的重要桥梁。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cntext.readthedocs.io/&#34;&gt;文本分析库cntext2.x使用手册 https://cntext.readthedocs.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/&#34;&gt;词向量 | 使用人民网领导留言板语料训练Word2Vec模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/&#34;&gt;使用 5000w 专利申请数据集按年份(按省份)训练词向量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/&#34;&gt;使用 1000w 条豆瓣影评训练 Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/&#34;&gt;转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/&#34;&gt;OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一简介">一、简介</h2>
<p><a href="https://nlp.stanford.edu/projects/glove/">Stanford GloVe</a>（Global Vectors for Word Representation）算法作为一种融合全局统计信息与局部上下文窗口的词嵌入模型，相较于Word2Vec仅依赖局部上下文，GloVe利用全局统计信息，能更精准地反映词频分布特征。例如，在高维词向量（如200D）中，GloVe在词语类比任务中准确率达75%，并在命名实体识别任务中优于其他词嵌入模型。因其高效的语义表征能力，在社会学、管理学等领域展现出广泛的应用价值。 相关词嵌入文献资料可阅读</p>
<ul>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-09-literature-about-embeddings/">文献汇总 | 词嵌入 与 社会科学中的偏见(态度)</a></li>
<li><a href="https://textdata.cn/blog/2022-04-01-embeddings-and-attitude/">词嵌入测量不同群体对某概念的态度(偏见)</a></li>
</ul>
<p><br><br></p>
<h2 id="二环境准备">二、环境准备</h2>
<p>cntext2.x 内置了 GloVe 训练所需的环境，支持 win 和 mac。</p>
<p>获取<a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/">cntext2.x</a> 的安装文件 <em><strong>cntext-2.1.5-py3-none-any.whl</strong></em>，并将该whl文件放置于桌面。执行以下安装命令</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
pip install cntext-2.1.5-py3-none-any.whl
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">ct</span><span class="o">.</span><span class="n">GloVe</span><span class="p">(</span><span class="n">corpus_file</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span> <span class="n">dict_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stopwords_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_memory</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">x_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><ul>
<li><em><strong>corpus_file</strong></em>: 输入语料文件路径（文本格式）。该文件为分词后的语料文件。</li>
<li><em><strong>lang</strong></em>: 语料文件的语言类型，默认为 &lsquo;chinese&rsquo;。</li>
<li><em><strong>dict_file</strong></em>: 自定义词典txt文件路径，默认为None。utf-8编码。</li>
<li><em><strong>stopwords_file</strong></em>: 停用词文件路径，默认为 None。utf-8编码。</li>
<li><em><strong>vector_size</strong></em>: 词向量维度，默认 100。</li>
<li><em><strong>window_size</strong></em>: 上下文窗口大小，默认 15。</li>
<li><em><strong>min_count</strong></em>: 忽略出现次数低于此值的单词，默认 5。</li>
<li><em><strong>max_memory</strong></em>: 可供使用的最大内存大小，单位为GB，默认 4;  该参数越大，训练越快。</li>
<li><em><strong>max_iter</strong></em>: 训练的最大迭代次数，默认 15。</li>
<li><em><strong>x_max</strong></em>: 共现矩阵中元素的最大计数值，默认 10。</li>
</ul>
<p><br><br></p>
<h2 id="三训练中文glove">三、训练中文GloVe</h2>
<p>我们其实只需要设置 <em><strong>corpus_file</strong></em> 和 <em><strong>lang</strong></em>， 但为了让大家知道</p>
<ul>
<li>上下文的窗口大小 <em><strong>window_size</strong></em></li>
<li>训练出模型词语的维度数 <em><strong>vector_size</strong></em></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 简化版调用。训练window_size=100维， vector_size=15</span>
<span class="c1"># glove_wv = ct.GloVe(corpus_file=&#39;data/三体.txt&#39;, lang=&#39;chinese&#39;)</span>

<span class="c1"># 正常调用。训练window_size=15维， vector_size=50</span>
<span class="n">glove_wv</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">GloVe</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;data/三体.txt&#39;</span><span class="p">,</span> 
                    <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span>
                    <span class="n">vector_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                    <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="n">glove_wv</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/三体_cache.txt Not Found or Empty, Preprocessing Corpus
Start Training GloVe
BUILDING VOCABULARY
Using vocabulary of size 6975.

COUNTING COOCCURRENCES
Merging cooccurrence files: processed 2106999 lines.

Using random seed 1743474106
SHUFFLING COOCCURRENCES
Merging temp files: processed 2106999 lines.

TRAINING MODEL
Read 2106999 lines.
Using random seed 1743474106
04/01/25 - 10:21.46AM, iter: 001, cost: 0.055981
04/01/25 - 10:21.46AM, iter: 002, cost: 0.050632
......
04/01/25 - 10:21.48AM, iter: 014, cost: 0.030047
04/01/25 - 10:21.48AM, iter: 015, cost: 0.029100

GloVe Training Cost 9 s. 
Output Saved To: output/三体-GloVe.50.15.txt
&lt;gensim.models.keyedvectors.KeyedVectors at 0x331517440&gt;

</code></pre></div><p><img loading="lazy" src="img/05-glove.png" alt=""  />
</p>
<br>
<h2 id="四使用中文glove模型">四、使用中文GloVe模型</h2>
<h3 id="41-加载模型">4.1 加载模型</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 加载word2vec模型.txt文件</span>
<span class="n">wv_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/三体-GloVe.50.15.txt&#39;</span><span class="p">)</span>
<span class="n">wv_model</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">&lt;gensim.models.keyedvectors.KeyedVectors at 0x336ff8dd0&gt;
</code></pre></div><br>
<h3 id="42-keyedvectors的操作方法或属性">4.2 KeyedVectors的操作方法(或属性)</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>KeyedVectors.index_to_key</strong></em></td>
<td>获取词汇表中的所有单词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.key_to_index</strong></em></td>
<td>获取单词到索引的映射。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.vector_size</strong></em></td>
<td>获取GloVe模型中任意词向量的维度。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.get_vector(word)</strong></em></td>
<td>获取给定单词的词向量。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_word(word, topn=10)</strong></em></td>
<td>获取某词语最相似的10个近义词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_vector(vector, topn=10)</strong></em></td>
<td>获取词向量最相似的10个近义词。</td>
</tr>
<tr>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>
<h3 id="421-词表">4.2.1 词表</h3>
<br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wv_model</span><span class="o">.</span><span class="n">index_to_key</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;的&#39;,
 &#39;了&#39;,
 &#39;在&#39;,
...
 &#39;引力&#39;,
 &#39;所说&#39;,
 &#39;星际&#39;,
 ...]

</code></pre></div><br>
<h3 id="422-词表映射">4.2.2 词表映射</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wv_model</span><span class="o">.</span><span class="n">key_to_index</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;的&#39;: 0,
 &#39;了&#39;: 1,
 &#39;在&#39;: 2,
...
 &#39;引力&#39;: 997,
 &#39;所说&#39;: 998,
 &#39;星际&#39;: 999,
 ...}
</code></pre></div><br>
<h3 id="423-向量维度数">4.2.3 向量维度数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;词表有 </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">wv_model</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span><span class="si">}</span><span class="s1"> 个词&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;向量是 </span><span class="si">{</span><span class="n">wv_model</span><span class="o">.</span><span class="n">vector_size</span><span class="si">}</span><span class="s1"> 维&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">词表有 4365 个词
向量是 50 维
</code></pre></div><br>
<h3 id="424-获取词向量">4.2.4 获取词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 查看「降临」的词向量</span>
<span class="n">wv</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;降临&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 0.672314,  0.020081,  0.653733,  0.598732, -0.680517, -0.049689,
       -0.16845 , -0.06759 , -0.147955,  0.024006,  0.264551, -0.050127,
        0.252063, -0.475633,  0.103722, -0.012481,  0.040755,  1.154912,
        0.742695,  0.048619, -0.514424, -1.184054,  0.515892, -0.1034  ,
        0.368755, -0.690357, -0.784287, -0.505814,  0.035807, -0.166354,
       -0.26149 ,  0.015089,  0.10626 , -0.215666, -0.374001, -0.123558,
        0.422617, -0.075277, -0.316387, -0.484295,  0.059687,  0.132621,
        0.192094, -0.591919,  0.236281,  0.164198, -0.058724,  1.285457,
        0.905606, -0.52032 ], dtype=float32)
</code></pre></div><br>
<h3 id="425-近义词">4.2.5 近义词</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">wv</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="s1">&#39;三体&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;叛军&#39;, 0.7699569463729858),
 (&#39;更新&#39;, 0.7687217593193054),
 (&#39;地球&#39;, 0.760529100894928),
 (&#39;全集&#39;, 0.7575182914733887),
 (&#39;最快&#39;, 0.7426372170448303),
 (&#39;世界&#39;, 0.7262137532234192),
 (&#39;最新&#39;, 0.7219281792640686),
 (&#39;游戏&#39;, 0.7180070877075195),
 (&#39;危机&#39;, 0.7020451426506042),
 (&#39;教&#39;, 0.7012627720832825)]
</code></pre></div><br>
<h3 id="426-计算多个词的中心向量">4.2.6 计算多个词的中心向量</h3>
<p>我们可以计算「三体」、「降临」、「组织」、「拯救」的中心向量eto_vector。 并试图寻找中心向量eto_vector的最相似的10个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">eto_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_centroid</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> <span class="n">words</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;三体&#39;</span><span class="p">,</span> <span class="s1">&#39;降临&#39;</span><span class="p">,</span> <span class="s1">&#39;组织&#39;</span><span class="p">,</span> <span class="s1">&#39;拯救&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">eto_vector</span><span class="p">)</span>
<span class="c1"># 寻找 eto_vector 语义最相似的10个词</span>
<span class="n">wv</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">eto_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[ 0.6267875   0.08975425  0.48438451  0.405128   -0.49928901  0.11347825
 -0.90057975  0.11877625 -0.27053049  0.344603    0.4368495  -0.3839495
  0.02633176 -0.138534    0.2531555  -0.0060905  -0.48776849  0.75548999
  0.72575876 -0.446079   -0.30361701 -1.039792    0.457687   -0.4286315
  0.44577325 -0.39119426 -0.4783935  -0.2596135  -0.32513325 -0.10315975
 -0.42880575 -0.48328425  0.129438   -0.17085625 -0.13454625 -0.070053
  0.68060375  0.16736924 -0.15664874 -0.20528575  0.385481    0.206432
  0.18913225 -0.93453825  0.58597099  0.60727924  0.009064    0.87661726
  0.65814423 -0.356567  ]

[(&#39;降临&#39;, 0.8707027435302734),
 (&#39;组织&#39;, 0.8625670671463013),
 (&#39;三体&#39;, 0.8621653914451599),
 (&#39;派&#39;, 0.8343338966369629),
 (&#39;拯救&#39;, 0.8301094174385071),
 (&#39;叛军&#39;, 0.784512460231781),
 (&#39;地球&#39;, 0.7536635398864746),
 (&#39;世界&#39;, 0.7245718836784363),
 (&#39;外部&#39;, 0.7078365087509155),
 (&#39;入侵&#39;, 0.6962169408798218)]
</code></pre></div><br>
<p>熟悉三体的朋友应该能联想到背叛人类的ETO(地球三体组织)有两个派别，分别是拯救派和降临派。</p>
<p>ETO开发了一款虚拟现实游戏，它向参与者展示了三体世界的真实情况，包括其恶劣的自然条件、三体文明的历史及其科技水平等。通过参与这个游戏，玩家们能够逐渐了解三体世界的真相，并最终决定是否要加入到支持三体文明入侵地球的行列中来。</p>
<p>这个游戏不仅充当了信息传递的媒介，也是甄别志同道合者的工具，让那些对人类社会现状不满、渴望变革的人们找到了组织，进而成为了背叛人类的叛军一员。在这个过程中，“三体游戏”起到了关键的作用，是连接地球人与三体世界的重要桥梁。</p>
<p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://cntext.readthedocs.io/">文本分析库cntext2.x使用手册 https://cntext.readthedocs.io/</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量 | 使用人民网领导留言板语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">使用 5000w 专利申请数据集按年份(按省份)训练词向量</a></li>
<li><a href="https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/">使用 1000w 条豆瓣影评训练 Word2Vec</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>豆瓣影评 | 探索词向量妙处</title>
      <link>https://textdata.cn/blog/douban_w2v/</link>
      <pubDate>Sun, 21 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/douban_w2v/</guid>
      <description>使用cntext训练、使用词向量。</description>
      <content:encoded><![CDATA[<p>本文要点</p>
<ul>
<li>读取 <em><strong>csv</strong></em></li>
<li>准备语料</li>
<li><em><strong>cntext2.x</strong></em> 训练词向量模型</li>
<li>运用词向量模型</li>
</ul>
<br>
<br>
<h2 id="一读取数据">一、读取数据</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;douban.csv&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div><p><img loading="lazy" src="img/df.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;电影  : </span><span class="si">{}</span><span class="s2"> 部&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Movie_Name_CN</span><span class="o">.</span><span class="n">nunique</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&#34;评论  : </span><span class="si">{}</span><span class="s2"> 条&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">    电影  : 28 部
    评论  : 2125056 条
</code></pre></div><p><br><br></p>
<h2 id="二准备语料">二、准备语料</h2>
<p>提取文本，去除非中文字符，保存为txt文件</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;douban.csv&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;douban.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">raw_text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Comment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span>
</code></pre></div><p><br><br></p>
<h2 id="三训练模型">三、训练模型</h2>
<h3 id="31-安装cntext2x">3.1 安装cntext2x</h3>
<p>将 <em><strong>cntext-2.1.5-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext-2.1.5-py3-none-any.whl
</code></pre></div><p>文末有 <em><strong>cntext-2.1.5-py3-none-any.whl</strong></em> 获取方式</p>
<br>
<h3 id="32-训练模型">3.2 训练模型</h3>
<p>使用 <a href="https://textdata.cn/blog/2024-04-27-cntext2x-usage-tutorial/"><em><strong>cntext2.x</strong></em></a> 库(版本号2.1.5) 训练词向量word2vec模型, 这里我把 csv 数据整理为 txt</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 训练</span>
<span class="n">w2v_model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span> <span class="o">=</span> <span class="s1">&#39;douban.txt&#39;</span><span class="p">,</span>  
                        <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> 
                        <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">,</span> 
                        <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> 
                        <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span> 
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/douban_cache.txt Not Found or Empty, Preprocessing Corpus
Processing Corpus: 11150it [00:07, 5759.05it/s]
Reading Preprocessed Corpus from output/douban_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 2001 s. 
Output Saved To: output/douban-Word2Vec.200.15.txt
</code></pre></div><br>
<p>在代码所在文件夹内可以找到</p>
<ul>
<li>output/douban-Word2Vec.200.15.txt</li>
<li>output/douban-Word2Vec.200.15.bin</li>
<li>新的  pos.txt</li>
<li>新的  neg.txt</li>
</ul>
<p>新的 <em><strong>pos.txt</strong></em> 是对 <em><strong>pos.txt</strong></em> 词典的扩展。</p>
<br>
<p><br><br></p>
<h2 id="四使用word2vec">四、使用Word2Vec</h2>
<h3 id="41-导入word2vec模型文件">4.1 导入Word2Vec模型文件</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
 
<span class="c1"># 导入模型，请注意路径。</span>
<span class="c1"># 「当前代码」 与 「output」 同处于一个文件夹内</span>

<span class="n">dm_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/douban-Word2Vec.200.15.bin&#39;</span><span class="p">)</span>
<span class="c1"># dm_w2v = ct.load_w2v(&#39;output/douban-Word2Vec.200.15.txt&#39;)</span>

<span class="n">dm_w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading output/output/douban-Word2Vec.200.15.bin...
&lt;gensim.models.keyedvectors.KeyedVectors at 0x314193830&gt;
</code></pre></div><br>
<h3 id="42-keyedvectors的操作方法或属性">4.2 KeyedVectors的操作方法(或属性)</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>KeyedVectors.index_to_key</strong></em></td>
<td>获取词汇表中的所有单词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.key_to_index</strong></em></td>
<td>获取单词到索引的映射。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.vector_size</strong></em></td>
<td>获取GloVe模型中任意词向量的维度。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.get_vector(word)</strong></em></td>
<td>获取给定单词的词向量。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_word(word, topn=10)</strong></em></td>
<td>获取某词语最相似的10个近义词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_vector(vector, topn=10)</strong></em></td>
<td>获取词向量最相似的10个近义词。</td>
</tr>
</tbody>
</table>
<br>
<h3 id="44-查看词表">4.4 查看词表</h3>
<p>查看词表所有单词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">index_to_key</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;电影&#39;,
 &#39;一个&#39;,
 &#39;没有&#39;,
 &#39;喜欢&#39;,
 ...
 &#39;跟着&#39;,
 &#39;意识&#39;,
 &#39;态度&#39;,
 ...]
</code></pre></div><p>为了方便查看， 这里只展示部分数据。</p>
<br>
<h3 id="45-词表映射">4.5 词表映射</h3>
<p>查看单词到索引的映射</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">key_to_index</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;电影&#39;: 0,
 &#39;一个&#39;: 1,
 &#39;没有&#39;: 2,
...
&#39;跟着&#39;: 997,
 &#39;意识&#39;: 998,
 &#39;态度&#39;: 999,
 ...}
</code></pre></div><br>
<h3 id="46-向量维度数">4.6 向量维度数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;词表有 </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span><span class="si">}</span><span class="s1"> 个词&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;向量是 </span><span class="si">{</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">vector_size</span><span class="si">}</span><span class="s1"> 维&#39;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">词表有 426646 个词
向量是 200 维
</code></pre></div><br>
<h3 id="47-获取词向量">4.7 获取词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-1.24090052e+00, -6.79377019e-01,  1.42518425e+00, -1.46615291e+00,
       -9.53197628e-02,  6.50456071e-01, -2.97696137e+00,  2.20916629e+00,
        6.12876177e-01,  1.63172066e+00,  4.91760701e-01, -9
        ......
        ......
         -1.42494082e+00,  2.49131727e+00, -6.27597034e-01, -7.91438043e-01,
       -4.54898655e-01,  1.37747681e+00, -4.20672953e-01, -1.53694853e-01,
        1.04936564e+00,  2.18786263e+00, -8.07472587e-01, -8.32003877e-02],
      dtype=float32)
</code></pre></div><br>
<h3 id="48-近义词">4.8 近义词</h3>
<p>根据词语查看近义词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 近义词</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;相当给力&#39;, 0.6180022358894348),
 (&#39;太给力&#39;, 0.6019443273544312),
 (&#39;带劲&#39;, 0.5840415954589844),
 (&#39;不给力&#39;, 0.5774183869361877),
 (&#39;过瘾&#39;, 0.5616626739501953),
 (&#39;牛叉&#39;, 0.553788959980011),
 (&#39;出彩&#39;, 0.5414286851882935),
 (&#39;精彩&#39;, 0.5332293510437012),
 (&#39;看得过瘾&#39;, 0.5250197649002075),
 (&#39;大赞&#39;, 0.5205727219581604)]
</code></pre></div><br>
<p>根据向量查找最相似的近义词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">word_vector</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">word_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;给力&#39;, 1.0),
 (&#39;相当给力&#39;, 0.6180021166801453),
 (&#39;太给力&#39;, 0.6019443273544312),
 (&#39;带劲&#39;, 0.5840415954589844),
 (&#39;不给力&#39;, 0.5774183869361877),
 (&#39;过瘾&#39;, 0.5616626739501953),
 (&#39;牛叉&#39;, 0.5537890195846558),
 (&#39;出彩&#39;, 0.5414287447929382),
 (&#39;精彩&#39;, 0.5332292914390564),
 (&#39;看得过瘾&#39;, 0.5250197649002075)]
</code></pre></div><br>
<h3 id="49-计算多个词的中心向量">4.9 计算多个词的中心向量</h3>
<p>我们可以计算「宇宙」、「飞船」、「战争」的宇宙语义向量（中心向量）。 并试图寻找中心向量 <em><strong>universe_vector</strong></em> 的最相似的10个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 几个词语构建的宇宙语义向量</span>
<span class="n">universe_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_centroid</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">dm_w2v</span><span class="p">,</span> 
                                       <span class="n">words</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;宇宙&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>


<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">universe_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;宇宙&#39;, 0.7568532228469849),
 (&#39;星系&#39;, 0.7090039253234863),
 (&#39;飞船&#39;, 0.7080673575401306),
 (&#39;人类文明&#39;, 0.6973789930343628),
 (&#39;战舰&#39;, 0.6890057325363159),
 (&#39;母舰&#39;, 0.6864359974861145),
 (&#39;星球&#39;, 0.6799622774124146),
 (&#39;卫星&#39;, 0.6799139976501465),
 (&#39;星际&#39;, 0.6789332032203674),
 (&#39;空间站&#39;, 0.6780815124511719),
 (&#39;地球&#39;, 0.6769616603851318),
 (&#39;外太空&#39;, 0.6683873534202576),
 (&#39;核战&#39;, 0.6669113039970398),
 (&#39;外星飞船&#39;, 0.6592534780502319),
 (&#39;木星&#39;, 0.6586896777153015),
 (&#39;能源&#39;, 0.6562989950180054),
 (&#39;战争&#39;, 0.6556441187858582),
 (&#39;巨兽&#39;, 0.6544537544250488),
 (&#39;月球&#39;, 0.6525537967681885),
 (&#39;一艘&#39;, 0.6521110534667969)]
</code></pre></div><p>语义捕捉的很准哦。</p>
<h3 id="410-类比-king-man--woman--queen">4.10 类比 king-man + woman ~ queen</h3>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p><img loading="lazy" src="img/kingqueenformular.png" alt=""  />
</p>
<h4 id="4101-传统类比">4.10.1 传统类比</h4>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p>$$
Vector1 \approx vector(国王)-vector(男人)
$$</p>
<p>$$
Vector2 \approx vector(王后)-vector(女人)
$$</p>
<p>那两个向量方向应该近似，即 <em><strong>Vector1</strong></em>  约等于 <em><strong>Vector2</strong></em> ，将其看做等式就得到如下公式：</p>
<p>$$
vector(国王)-vector(男人) \approx vector(王后) - vector(女人)
$$</p>
<p>现在我们检查三个语义向量计算出的新的向量是否有与queen相关的语义信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">men_vector</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;男人&#39;</span><span class="p">)</span>
<span class="n">women_vector</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;女人&#39;</span><span class="p">)</span> 
<span class="n">king_vector</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;国王&#39;</span><span class="p">)</span> 

<span class="c1"># 假设 king- queen 近似等于 man -woman </span>
<span class="c1"># result 近似等于 king - queen + women</span>
<span class="n">result_vector</span> <span class="o">=</span> <span class="n">king_vector</span> <span class="o">-</span> <span class="n">men_vector</span> <span class="o">+</span> <span class="n">women_vector</span>
<span class="c1"># 现在检查 result_vector 的语义应该与queen相关</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">result_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;国王&#39;, 0.8276543617248535),
 (&#39;王后&#39;, 0.754295289516449),
 (&#39;皇后&#39;, 0.6877321004867554),
 (&#39;公主&#39;, 0.6311503052711487),
 (&#39;王位&#39;, 0.6292931437492371),
 (&#39;皇帝&#39;, 0.6280742287635803),
 (&#39;王妃&#39;, 0.6235458850860596),
 (&#39;伊丽莎白一世&#39;, 0.6158717274665833),
 (&#39;君主&#39;, 0.6151927709579468),
 (&#39;公爵&#39;, 0.6111372113227844),
 (&#39;女王&#39;, 0.6068686246871948),
 (&#39;登基&#39;, 0.606802225112915),
 (&#39;皇子&#39;, 0.5979987382888794),
 (&#39;侍卫&#39;, 0.594831109046936),
 (&#39;夫人&#39;, 0.5942187309265137),
 (&#39;王室&#39;, 0.5891965627670288),
 (&#39;女皇&#39;, 0.5889874696731567),
 (&#39;继位&#39;, 0.5818601846694946),
 (&#39;皇室&#39;, 0.5812580585479736),
 (&#39;王冠&#39;, 0.5733407139778137)]
</code></pre></div><h4 id="4102-新算法">4.10.2 新算法</h4>
<p><em><strong>most_similar_cosmul</strong></em> 使用了一种基于 <strong>乘法组合</strong> 的相似度计算方法，而不是简单的向量加减法。其核心公式如下：
$$
\text{Similarity}(w, \text{positive}, \text{negative}) = \frac{\prod_{p \in \text{positive}} \cos(w, p)}{\prod_{n \in \text{negative}} \cos(w, n)}
$$
对于给定的正样本词集合 P 和负样本词集合 N，目标是找到一个词 w，使得得分最大化。</p>
<p>参照如下的例子
$$
vector(王后)   \approx  vector(国王) + vector(女人) -vector(男人)<br>
$$</p>
<p>其中正向目标词有 <em><strong>国王</strong></em> 和 <em><strong>女人</strong></em>， 负向词有 <em><strong>男人</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 类比函数</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">most_similar_cosmul</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;国王&#39;</span><span class="p">,</span> <span class="s1">&#39;女人&#39;</span><span class="p">],</span>   <span class="c1">#</span>
                           <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;男人&#39;</span><span class="p">],</span> 
                           <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;王后&#39;, 0.9907146692276001),
 (&#39;皇后&#39;, 0.9572808146476746),
 (&#39;公主&#39;, 0.9137295484542847),
 (&#39;王妃&#39;, 0.9079920649528503),
 (&#39;皇帝&#39;, 0.905644953250885),
 (&#39;伊丽莎白一世&#39;, 0.9031068682670593),
 (&#39;女王&#39;, 0.8956636190414429),
 (&#39;王位&#39;, 0.8942943215370178),
 (&#39;登基&#39;, 0.8899738192558289),
 (&#39;君主&#39;, 0.8883361220359802),
 (&#39;公爵&#39;, 0.8862053751945496),
 (&#39;王室&#39;, 0.8842172622680664),
 (&#39;夫人&#39;, 0.8840034604072571),
 (&#39;女皇&#39;, 0.8824913501739502),
 (&#39;侍卫&#39;, 0.8815361857414246),
 (&#39;皇子&#39;, 0.8785887360572815),
 (&#39;皇室&#39;, 0.8755369186401367),
 (&#39;继位&#39;, 0.8736834526062012),
 (&#39;驾崩&#39;, 0.8675689101219177),
 (&#39;波旁王朝&#39;, 0.8671858906745911)]
</code></pre></div><p>可以看到返回前2的词直接表明了词语是王后皇后，与公式推算结果一般无二。</p>
<p><br><br></p>
<h2 id="五获取资料">五、获取资料</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 免费 douban-Word2Vec.200.15.bin
链接: https://pan.baidu.com/s/1Ca_84dUStxuONpc2kNOUfg?pwd=kvwa 提取码: kvwa

- 100元  cntext-2.1.5-py3-none-any.whl   如有需要，加微信 372335839， 备注「姓名-学校-专业」
</code></pre></div><p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://cntext.readthedocs.io/">文本分析库cntext2.x使用手册 https://cntext.readthedocs.io/</a></li>
<li><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用Stanford Glove代码训练中文语料的Glove模型</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量 | 使用人民网领导留言板语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">使用 5000w 专利申请数据集按年份(按省份)训练词向量</a></li>
<li><a href="https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/">使用 1000w 条豆瓣影评训练 Word2Vec</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>数据集 |  使用 1000w 条豆瓣影评训练 Word2Vec</title>
      <link>https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/</link>
      <pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/</guid>
      <description>&lt;p&gt;本文内容&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;介绍豆瓣影评数据集&lt;/li&gt;
&lt;li&gt;构造语料训练 &lt;em&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt;&lt;/em&gt; 模型&lt;/li&gt;
&lt;li&gt;获取数据 &lt;em&gt;&lt;strong&gt;cntext2.x&lt;/strong&gt;&lt;/em&gt; &amp;amp; &lt;em&gt;&lt;strong&gt;Word2Vec&lt;/strong&gt;&lt;/em&gt; 模型文件&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;一豆瓣影评数据集&#34;&gt;一、豆瓣影评数据集&lt;/h2&gt;
&lt;h3 id=&#34;11-数据集介绍&#34;&gt;1.1 数据集介绍&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;数据集: douba-movie-1000w

数据源: 豆瓣电影
   
记录数:
   - 电影 10269 部
   - 影评 10310989 条
   
体积: 1.35G 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;该数据集正好弥补下国内公开电影数据集的空缺， 数据已经过初步清洗，可用于推荐系统、情感分析、知识图谱、新闻传播学、社会学文化变迁等多个领域(或主题)。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;12-读取数据&#34;&gt;1.2 读取数据&lt;/h3&gt;
&lt;p&gt;下载 &lt;em&gt;&lt;strong&gt;douba-movie-1000w.zip&lt;/strong&gt;&lt;/em&gt; 解压后，可以看到数据集中有一个 &lt;em&gt;&lt;strong&gt;all_movies_with_id.csv&lt;/strong&gt;&lt;/em&gt; 文件。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;all_movies_with_id.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/01-df.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;13-所含字段&#34;&gt;1.3 所含字段&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;col&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; - &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;col&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt; - ID
 - Movie_Name  电影名
 - Score  豆瓣电影评分(1-10)
 - Review_People  评论者人数
 - Star_Distribution  评论评分分布(1-5, 含多个数值，数值以%间隔)
 - Craw_Date 爬虫运行日期
 - Username 豆瓣评论者用户名
 - Date 影评日期
 - Star  影评评分(1-5)
 - Comment 影评内容
 - Comment_Distribution 影评评分分布
 - Like 影评获得的喜欢数
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二-构造语料训练word2vec&#34;&gt;二、 构造语料&amp;amp;训练Word2Vec&lt;/h2&gt;
&lt;h3 id=&#34;21-构造语料&#34;&gt;2.1 构造语料&lt;/h3&gt;
&lt;p&gt;将字段 &lt;em&gt;&lt;strong&gt;Comment&lt;/strong&gt;&lt;/em&gt; 中所有文本汇总到 &lt;em&gt;&lt;strong&gt;douban-movie-1000w.txt&lt;/strong&gt;&lt;/em&gt;,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 读取数据，只读取字段sign&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;all_movies_with_id.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;usecols&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;sign&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 将sign列中的所有文本汇总到douban-movie-1000w.txt&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;douban-movie-1000w.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Comment&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 删除df和text变量，释放内存&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;del&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;del&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;22-配置cntext215&#34;&gt;2.2 配置cntext2.1.5&lt;/h3&gt;
&lt;p&gt;将 &lt;em&gt;&lt;strong&gt;cntext-2.1.5-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 放置于桌面，打开 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 &lt;em&gt;&lt;strong&gt;cmd&lt;/strong&gt;&lt;/em&gt;  (苹果电脑打开terminal) 中使用 &lt;em&gt;&lt;strong&gt;pip3&lt;/strong&gt;&lt;/em&gt; 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install cntext-2.1.5-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文末有 &lt;em&gt;&lt;strong&gt;cntext-2.1.5-py3-none-any.whl&lt;/strong&gt;&lt;/em&gt; 获取方式&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;23-训练word2vec&#34;&gt;2.3 训练Word2Vec&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# cntext为2.1.5&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 训练Word2Vec模型&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Word2Vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;douban-movie-1000w.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                 &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                 &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                 &lt;span class=&#34;n&#34;&gt;min_count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Mac(Linux) System, Enable Parallel Processing
Cache output/douban-movie-1000w_cache.txt Not Found or Empty, Preprocessing Corpus
Reading Preprocessed Corpus from output/douban-movie-1000w_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 2965 s. 
Output Saved To: output/douban-movie-1000w.200.15.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;经过半个小时的训练， 得到&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型文件 &lt;em&gt;&lt;strong&gt;output/douban-movie-1000w-Word2Vec.200.15.txt&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;模型文件 &lt;em&gt;&lt;strong&gt;output/douban-movie-1000w-Word2Vec.200.15.bin&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;缓存文件 &lt;em&gt;&lt;strong&gt;output/douban-movie-1000w_cache.txt&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中模型文件有 &lt;em&gt;&lt;strong&gt;txt&lt;/strong&gt;&lt;/em&gt; 和 &lt;em&gt;&lt;strong&gt;bin&lt;/strong&gt;&lt;/em&gt; 两种格式， 信息量完全等同。 &lt;em&gt;&lt;strong&gt;txt&lt;/strong&gt;&lt;/em&gt; 可以用记事本打开查看， 而 &lt;em&gt;&lt;strong&gt;bin&lt;/strong&gt;&lt;/em&gt; 则是二进制文件， 体积更小。 已训练好的模型， 因 bin 格式体积更小， 便于分享。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四使用word2vec&#34;&gt;四、使用Word2Vec&lt;/h2&gt;
&lt;h3 id=&#34;41-导入word2vec模型文件&#34;&gt;4.1 导入Word2Vec模型文件&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
 
&lt;span class=&#34;c1&#34;&gt;# 导入模型，请注意路径。&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 「当前代码」 与 「output」 同处于一个文件夹内&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/douban-movie-1000w-Word2Vec.200.15.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# dm_w2v = ct.load_w2v(&amp;#39;output/douban-movie-1000w-Word2Vec.200.15.bin&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Loading output/douban-movie-1000w-Word2Vec.200.15.txt...
&amp;lt;gensim.models.keyedvectors.KeyedVectors at 0x314193830&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-keyedvectors的操作方法或属性&#34;&gt;4.2 KeyedVectors的操作方法(或属性)&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方法&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.index_to_key&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取词汇表中的所有单词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.key_to_index&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取单词到索引的映射。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.vector_size&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取GloVe模型中任意词向量的维度。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.get_vector(word)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取给定单词的词向量。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.similar_by_word(word, topn=10)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取某词语最相似的10个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;KeyedVectors.similar_by_vector(vector, topn=10)&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;获取词向量最相似的10个近义词。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
&lt;h3 id=&#34;44-查看词表&#34;&gt;4.4 查看词表&lt;/h3&gt;
&lt;p&gt;查看词表所有单词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index_to_key&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[&amp;#39;电影&amp;#39;,
 &amp;#39;一个&amp;#39;,
 &amp;#39;没有&amp;#39;,
 &amp;#39;喜欢&amp;#39;,
 ...
 &amp;#39;跟着&amp;#39;,
 &amp;#39;意识&amp;#39;,
 &amp;#39;态度&amp;#39;,
 ...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;为了方便查看， 这里只展示部分数据。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;45-词表映射&#34;&gt;4.5 词表映射&lt;/h3&gt;
&lt;p&gt;查看单词到索引的映射&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;{&amp;#39;电影&amp;#39;: 0,
 &amp;#39;一个&amp;#39;: 1,
 &amp;#39;没有&amp;#39;: 2,
...
&amp;#39;跟着&amp;#39;: 997,
 &amp;#39;意识&amp;#39;: 998,
 &amp;#39;态度&amp;#39;: 999,
 ...}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;46-向量维度数&#34;&gt;4.6 向量维度数&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;词表有 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;key_to_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 个词&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;向量是 &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; 维&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;词表有 426646 个词
向量是 200 维
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;47-获取词向量&#34;&gt;4.7 获取词向量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([-1.24090052e+00, -6.79377019e-01,  1.42518425e+00, -1.46615291e+00,
       -9.53197628e-02,  6.50456071e-01, -2.97696137e+00,  2.20916629e+00,
        6.12876177e-01,  1.63172066e+00,  4.91760701e-01, -9
        ......
        ......
         -1.42494082e+00,  2.49131727e+00, -6.27597034e-01, -7.91438043e-01,
       -4.54898655e-01,  1.37747681e+00, -4.20672953e-01, -1.53694853e-01,
        1.04936564e+00,  2.18786263e+00, -8.07472587e-01, -8.32003877e-02],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;48-近义词&#34;&gt;4.8 近义词&lt;/h3&gt;
&lt;p&gt;根据词语查看近义词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 近义词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;相当给力&amp;#39;, 0.6180022358894348),
 (&amp;#39;太给力&amp;#39;, 0.6019443273544312),
 (&amp;#39;带劲&amp;#39;, 0.5840415954589844),
 (&amp;#39;不给力&amp;#39;, 0.5774183869361877),
 (&amp;#39;过瘾&amp;#39;, 0.5616626739501953),
 (&amp;#39;牛叉&amp;#39;, 0.553788959980011),
 (&amp;#39;出彩&amp;#39;, 0.5414286851882935),
 (&amp;#39;精彩&amp;#39;, 0.5332293510437012),
 (&amp;#39;看得过瘾&amp;#39;, 0.5250197649002075),
 (&amp;#39;大赞&amp;#39;, 0.5205727219581604)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;p&gt;根据向量查找最相似的近义词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;word_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;给力&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;给力&amp;#39;, 1.0),
 (&amp;#39;相当给力&amp;#39;, 0.6180021166801453),
 (&amp;#39;太给力&amp;#39;, 0.6019443273544312),
 (&amp;#39;带劲&amp;#39;, 0.5840415954589844),
 (&amp;#39;不给力&amp;#39;, 0.5774183869361877),
 (&amp;#39;过瘾&amp;#39;, 0.5616626739501953),
 (&amp;#39;牛叉&amp;#39;, 0.5537890195846558),
 (&amp;#39;出彩&amp;#39;, 0.5414287447929382),
 (&amp;#39;精彩&amp;#39;, 0.5332292914390564),
 (&amp;#39;看得过瘾&amp;#39;, 0.5250197649002075)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;49-计算多个词的中心向量&#34;&gt;4.9 计算多个词的中心向量&lt;/h3&gt;
&lt;p&gt;我们可以计算「宇宙」、「飞船」、「战争」的宇宙语义向量（中心向量）。 并试图寻找中心向量 &lt;em&gt;&lt;strong&gt;universe_vector&lt;/strong&gt;&lt;/em&gt; 的最相似的10个词。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 几个词语构建的宇宙语义向量&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;universe_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;semantic_centroid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                                       &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;宇宙&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;飞船&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;战争&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;universe_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;宇宙&amp;#39;, 0.7568532228469849),
 (&amp;#39;星系&amp;#39;, 0.7090039253234863),
 (&amp;#39;飞船&amp;#39;, 0.7080673575401306),
 (&amp;#39;人类文明&amp;#39;, 0.6973789930343628),
 (&amp;#39;战舰&amp;#39;, 0.6890057325363159),
 (&amp;#39;母舰&amp;#39;, 0.6864359974861145),
 (&amp;#39;星球&amp;#39;, 0.6799622774124146),
 (&amp;#39;卫星&amp;#39;, 0.6799139976501465),
 (&amp;#39;星际&amp;#39;, 0.6789332032203674),
 (&amp;#39;空间站&amp;#39;, 0.6780815124511719),
 (&amp;#39;地球&amp;#39;, 0.6769616603851318),
 (&amp;#39;外太空&amp;#39;, 0.6683873534202576),
 (&amp;#39;核战&amp;#39;, 0.6669113039970398),
 (&amp;#39;外星飞船&amp;#39;, 0.6592534780502319),
 (&amp;#39;木星&amp;#39;, 0.6586896777153015),
 (&amp;#39;能源&amp;#39;, 0.6562989950180054),
 (&amp;#39;战争&amp;#39;, 0.6556441187858582),
 (&amp;#39;巨兽&amp;#39;, 0.6544537544250488),
 (&amp;#39;月球&amp;#39;, 0.6525537967681885),
 (&amp;#39;一艘&amp;#39;, 0.6521110534667969)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;语义捕捉的很准哦。&lt;/p&gt;
&lt;h3 id=&#34;410-概念轴&#34;&gt;4.10 概念轴&lt;/h3&gt;
&lt;p&gt;男性概念向量由多个男性词的向量加总求均值得到，女性概念向量算法类似。当性质或方向明显相反的两个概念向量相减， 得到的新的向量，我们可以称之为&lt;em&gt;&lt;strong&gt;概念轴向量Concept Axis&lt;/strong&gt;&lt;/em&gt;。常见的概念轴，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 尺寸(大, 小)
- 湿度(干燥,潮湿)
- 性别(男, 女)
- 财富(富裕, 贫穷)
- 等
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其实任意概念的向量也可看做概念轴，即该概念向量与0向量相减。只不过两组性质方向相反的方式得到的概念轴， 在语义上更稳定。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 定义词语列表&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;phy_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;游泳&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;跑步&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;篮球&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;羽毛球&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;马拉松&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;马术&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;徒步&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;rich_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;富裕&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;财富&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;金钱&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;豪宅&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;豪车&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;奢侈品&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;投资&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;股票&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;基金&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;黄金&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;钻石&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;游艇&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;私人飞机&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;企业家&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;富豪&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;成功&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;繁荣&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;奢华&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;贵族&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;高收入&amp;#39;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;poor_words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;贫穷&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;贫困&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;饥饿&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;失业&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;低收入&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;简陋&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;破旧&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;乞丐&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;流浪&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;欠债&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;破产&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;困境&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;艰难&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;挣扎&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;匮乏&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
    &lt;span class=&#34;s1&#34;&gt;&amp;#39;落后&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;无助&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;绝望&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;赤贫&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;温饱&amp;#39;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;phy_project_on_fortune&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sematic_projection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                               &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;phy_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                                               &lt;span class=&#34;n&#34;&gt;c_words1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;poor_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                                               &lt;span class=&#34;n&#34;&gt;c_words2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rich_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;phy_project_on_fortune&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;跑步&amp;#39;, -1.82),
 (&amp;#39;徒步&amp;#39;, -0.82),
 (&amp;#39;游泳&amp;#39;, -0.19),
 (&amp;#39;羽毛球&amp;#39;, 0.57),
 (&amp;#39;马拉松&amp;#39;, 0.62),
 (&amp;#39;马术&amp;#39;, 1.15),
 (&amp;#39;篮球&amp;#39;, 4.0)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;计算结果中， 数值越大越接近于c_words2,  越小越接近于c_words1 。 可以看到在财富概念轴向量上的投影，  篮球不太准，但是其他几项基本上看出运动的贫富性。&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;411-类比-king-man--woman--queen&#34;&gt;4.11 类比 king-man + woman ~ queen&lt;/h3&gt;
&lt;p&gt;每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/king-queen-formular.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;这两个词相减，按感觉应该得到的是性别方向，雄性-&amp;gt;雌性。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;gender_direction_1 = vector(man)-vector(woman)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;gender_direction_2 = vector(king)-vector(queen)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;那两个性别方向应该近似，即 gender_direction_1 约等于 gender_direction_2 ，将其看做等式就得到如下公式：&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;vector(理应近似queen) = vector(king)-vector(men)+vector(women)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;现在我们检查三个语义向量计算出的新的向量是否有与queen相关的语义信息。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;semactic_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;semantic_centroid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                                  &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vector&lt;/span&gt;


&lt;span class=&#34;n&#34;&gt;men_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;semactic_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;男&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;男孩&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;男人&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;他&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;父亲&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;爸爸&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;爷爷&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;women_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;semactic_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;女&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;女孩&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;女人&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;她&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;母亲&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;妈妈&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;奶奶&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;king_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;semactic_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;国王&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;皇帝&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;帝王&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;大帝&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 假设 king- queen 约等于 man -woman &lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# result 近似等于 king - queen + women&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;result_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;king_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;men_vector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;women_vector&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# 现在检查 result_vector 的语义应该与queen相关&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;dm_w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;similar_by_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;result_vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;皇帝&amp;#39;, 0.8448051810264587),
 (&amp;#39;王后&amp;#39;, 0.8056979179382324),
 (&amp;#39;国王&amp;#39;, 0.8004385232925415),
 (&amp;#39;帝王&amp;#39;, 0.7693961262702942),
 (&amp;#39;君主&amp;#39;, 0.7663125991821289),
 (&amp;#39;皇后&amp;#39;, 0.7614380717277527),
 (&amp;#39;太后&amp;#39;, 0.7463700175285339),
 (&amp;#39;妃子&amp;#39;, 0.7433678507804871),
 (&amp;#39;君王&amp;#39;, 0.7407413125038147),
 (&amp;#39;皇子&amp;#39;, 0.7380139231681824),
 (&amp;#39;王位&amp;#39;, 0.7319545745849609),
 (&amp;#39;皇上&amp;#39;, 0.7215542197227478),
 (&amp;#39;登基&amp;#39;, 0.7210745215415955),
 (&amp;#39;大臣&amp;#39;, 0.714862048625946),
 (&amp;#39;伊丽莎白一世&amp;#39;, 0.702217698097229),
 (&amp;#39;王朝&amp;#39;, 0.7000151872634888),
 (&amp;#39;宫女&amp;#39;, 0.6997070908546448),
 (&amp;#39;驾崩&amp;#39;, 0.6992778182029724),
 (&amp;#39;王妃&amp;#39;, 0.6981185078620911),
 (&amp;#39;昏君&amp;#39;, 0.6974363923072815)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看到三个语义向量四则运算出的 result_vector 与queen仍具有较高的相关性。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;五获取资料&#34;&gt;五、获取资料&lt;/h2&gt;
&lt;p&gt;除了本文介绍的这个 1000w 条影评数据集， 大邓还有2个类似的豆瓣影评数据集，影评记录量 212w和442 w 条。 两个数据集下载链接我都公开，感兴趣的可以都下载下来。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 免费  douba-movie-1000w 链接: https://pan.baidu.com/s/15C0fn7oyYEFvuQtPO8tw8Q?pwd=1g7m 提取码: 1g7m
- 免费 douban-movie-1000w-Word2Vec.200.15.bin
链接: https://pan.baidu.com/s/1fK8LhLmK4_xq-eHzNn42lg?pwd=2hwr 提取码: 2hwr
- 免费 douban-movie-442w 链接: https://pan.baidu.com/s/1T_LPuxEZ_W8xfYcxV7rW5Q?pwd=a683 提取码: a683
- 免费 douban-movie-212w 链接: :https://pan.baidu.com/s/1VBwnOqfMPu_Y48bMlQ4oiw?pwd=t8id 
 提取码: t8id 

- 100元  cntext-2.1.5-py3-none-any.whl   如有需要，加微信 ***372335839***， 备注「姓名-学校-专业」
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/&#34;&gt;数据集 | 3394w条豆瓣书评数据集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/&#34;&gt;实验 | 使用Stanford Glove代码训练中文语料的GloVe模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/&#34;&gt;可视化 | 人民日报语料反映七十年文化演变&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/&#34;&gt;词向量 | 使用MD&amp;amp;A2001-2023语料训练Word2Vec模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/&#34;&gt;实验 | 使用Stanford Glove代码训练中文语料的Glove模型&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;</description>
      <content:encoded><![CDATA[<p>本文内容</p>
<ol>
<li>介绍豆瓣影评数据集</li>
<li>构造语料训练 <em><strong>Word2Vec</strong></em> 模型</li>
<li>获取数据 <em><strong>cntext2.x</strong></em> &amp; <em><strong>Word2Vec</strong></em> 模型文件</li>
</ol>
<p><br><br></p>
<h2 id="一豆瓣影评数据集">一、豆瓣影评数据集</h2>
<h3 id="11-数据集介绍">1.1 数据集介绍</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">数据集: douba-movie-1000w

数据源: 豆瓣电影
   
记录数:
   - 电影 10269 部
   - 影评 10310989 条
   
体积: 1.35G 
</code></pre></div><p>该数据集正好弥补下国内公开电影数据集的空缺， 数据已经过初步清洗，可用于推荐系统、情感分析、知识图谱、新闻传播学、社会学文化变迁等多个领域(或主题)。</p>
<br>
<h3 id="12-读取数据">1.2 读取数据</h3>
<p>下载 <em><strong>douba-movie-1000w.zip</strong></em> 解压后，可以看到数据集中有一个 <em><strong>all_movies_with_id.csv</strong></em> 文件。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;all_movies_with_id.csv&#39;</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p><img loading="lazy" src="img/01-df.png" alt=""  />
</p>
<br>
<h3 id="13-所含字段">1.3 所含字段</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39; - </span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"> - ID
 - Movie_Name  电影名
 - Score  豆瓣电影评分(1-10)
 - Review_People  评论者人数
 - Star_Distribution  评论评分分布(1-5, 含多个数值，数值以%间隔)
 - Craw_Date 爬虫运行日期
 - Username 豆瓣评论者用户名
 - Date 影评日期
 - Star  影评评分(1-5)
 - Comment 影评内容
 - Comment_Distribution 影评评分分布
 - Like 影评获得的喜欢数
</code></pre></div><p><br><br></p>
<h2 id="二-构造语料训练word2vec">二、 构造语料&amp;训练Word2Vec</h2>
<h3 id="21-构造语料">2.1 构造语料</h3>
<p>将字段 <em><strong>Comment</strong></em> 中所有文本汇总到 <em><strong>douban-movie-1000w.txt</strong></em>,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># 读取数据，只读取字段sign</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;all_movies_with_id.csv&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sign&#39;</span><span class="p">])</span>

<span class="c1"># 将sign列中的所有文本汇总到douban-movie-1000w.txt</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;douban-movie-1000w.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Comment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="c1"># 删除df和text变量，释放内存</span>
<span class="k">del</span> <span class="n">df</span>
<span class="k">del</span> <span class="n">text</span>

</code></pre></div><br>
<h3 id="22-配置cntext215">2.2 配置cntext2.1.5</h3>
<p>将 <em><strong>cntext-2.1.5-py3-none-any.whl</strong></em> 放置于桌面，打开 <em><strong>cmd</strong></em>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <em><strong>cmd</strong></em>  (苹果电脑打开terminal) 中使用 <em><strong>pip3</strong></em> 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext-2.1.5-py3-none-any.whl
</code></pre></div><p>文末有 <em><strong>cntext-2.1.5-py3-none-any.whl</strong></em> 获取方式</p>
<br>
<h3 id="23-训练word2vec">2.3 训练Word2Vec</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># cntext为2.1.5</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="c1"># 训练Word2Vec模型</span>
<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;douban-movie-1000w.txt&#39;</span><span class="p">,</span>
                 <span class="n">vector_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                 <span class="n">window_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                 <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/douban-movie-1000w_cache.txt Not Found or Empty, Preprocessing Corpus
Reading Preprocessed Corpus from output/douban-movie-1000w_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 2965 s. 
Output Saved To: output/douban-movie-1000w.200.15.txt
</code></pre></div><p>经过半个小时的训练， 得到</p>
<ul>
<li>模型文件 <em><strong>output/douban-movie-1000w-Word2Vec.200.15.txt</strong></em></li>
<li>模型文件 <em><strong>output/douban-movie-1000w-Word2Vec.200.15.bin</strong></em></li>
<li>缓存文件 <em><strong>output/douban-movie-1000w_cache.txt</strong></em></li>
</ul>
<p>其中模型文件有 <em><strong>txt</strong></em> 和 <em><strong>bin</strong></em> 两种格式， 信息量完全等同。 <em><strong>txt</strong></em> 可以用记事本打开查看， 而 <em><strong>bin</strong></em> 则是二进制文件， 体积更小。 已训练好的模型， 因 bin 格式体积更小， 便于分享。</p>
<p><br><br></p>
<h2 id="四使用word2vec">四、使用Word2Vec</h2>
<h3 id="41-导入word2vec模型文件">4.1 导入Word2Vec模型文件</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
 
<span class="c1"># 导入模型，请注意路径。</span>
<span class="c1"># 「当前代码」 与 「output」 同处于一个文件夹内</span>

<span class="n">dm_w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/douban-movie-1000w-Word2Vec.200.15.txt&#39;</span><span class="p">)</span>
<span class="c1"># dm_w2v = ct.load_w2v(&#39;output/douban-movie-1000w-Word2Vec.200.15.bin&#39;)</span>

<span class="n">dm_w2v</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Loading output/douban-movie-1000w-Word2Vec.200.15.txt...
&lt;gensim.models.keyedvectors.KeyedVectors at 0x314193830&gt;
</code></pre></div><br>
<h3 id="42-keyedvectors的操作方法或属性">4.2 KeyedVectors的操作方法(或属性)</h3>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>KeyedVectors.index_to_key</strong></em></td>
<td>获取词汇表中的所有单词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.key_to_index</strong></em></td>
<td>获取单词到索引的映射。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.vector_size</strong></em></td>
<td>获取GloVe模型中任意词向量的维度。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.get_vector(word)</strong></em></td>
<td>获取给定单词的词向量。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_word(word, topn=10)</strong></em></td>
<td>获取某词语最相似的10个近义词。</td>
</tr>
<tr>
<td><em><strong>KeyedVectors.similar_by_vector(vector, topn=10)</strong></em></td>
<td>获取词向量最相似的10个近义词。</td>
</tr>
</tbody>
</table>
<br>
<h3 id="44-查看词表">4.4 查看词表</h3>
<p>查看词表所有单词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">index_to_key</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[&#39;电影&#39;,
 &#39;一个&#39;,
 &#39;没有&#39;,
 &#39;喜欢&#39;,
 ...
 &#39;跟着&#39;,
 &#39;意识&#39;,
 &#39;态度&#39;,
 ...]
</code></pre></div><p>为了方便查看， 这里只展示部分数据。</p>
<br>
<h3 id="45-词表映射">4.5 词表映射</h3>
<p>查看单词到索引的映射</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">key_to_index</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">{&#39;电影&#39;: 0,
 &#39;一个&#39;: 1,
 &#39;没有&#39;: 2,
...
&#39;跟着&#39;: 997,
 &#39;意识&#39;: 998,
 &#39;态度&#39;: 999,
 ...}
</code></pre></div><br>
<h3 id="46-向量维度数">4.6 向量维度数</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;词表有 </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span><span class="si">}</span><span class="s1"> 个词&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;向量是 </span><span class="si">{</span><span class="n">dm_w2v</span><span class="o">.</span><span class="n">vector_size</span><span class="si">}</span><span class="s1"> 维&#39;</span><span class="p">)</span>

</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">词表有 426646 个词
向量是 200 维
</code></pre></div><br>
<h3 id="47-获取词向量">4.7 获取词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([-1.24090052e+00, -6.79377019e-01,  1.42518425e+00, -1.46615291e+00,
       -9.53197628e-02,  6.50456071e-01, -2.97696137e+00,  2.20916629e+00,
        6.12876177e-01,  1.63172066e+00,  4.91760701e-01, -9
        ......
        ......
         -1.42494082e+00,  2.49131727e+00, -6.27597034e-01, -7.91438043e-01,
       -4.54898655e-01,  1.37747681e+00, -4.20672953e-01, -1.53694853e-01,
        1.04936564e+00,  2.18786263e+00, -8.07472587e-01, -8.32003877e-02],
      dtype=float32)
</code></pre></div><br>
<h3 id="48-近义词">4.8 近义词</h3>
<p>根据词语查看近义词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 近义词</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;相当给力&#39;, 0.6180022358894348),
 (&#39;太给力&#39;, 0.6019443273544312),
 (&#39;带劲&#39;, 0.5840415954589844),
 (&#39;不给力&#39;, 0.5774183869361877),
 (&#39;过瘾&#39;, 0.5616626739501953),
 (&#39;牛叉&#39;, 0.553788959980011),
 (&#39;出彩&#39;, 0.5414286851882935),
 (&#39;精彩&#39;, 0.5332293510437012),
 (&#39;看得过瘾&#39;, 0.5250197649002075),
 (&#39;大赞&#39;, 0.5205727219581604)]
</code></pre></div><br>
<p>根据向量查找最相似的近义词</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">word_vector</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="o">.</span><span class="n">get_vector</span><span class="p">(</span><span class="s1">&#39;给力&#39;</span><span class="p">)</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">word_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;给力&#39;, 1.0),
 (&#39;相当给力&#39;, 0.6180021166801453),
 (&#39;太给力&#39;, 0.6019443273544312),
 (&#39;带劲&#39;, 0.5840415954589844),
 (&#39;不给力&#39;, 0.5774183869361877),
 (&#39;过瘾&#39;, 0.5616626739501953),
 (&#39;牛叉&#39;, 0.5537890195846558),
 (&#39;出彩&#39;, 0.5414287447929382),
 (&#39;精彩&#39;, 0.5332292914390564),
 (&#39;看得过瘾&#39;, 0.5250197649002075)]
</code></pre></div><br>
<h3 id="49-计算多个词的中心向量">4.9 计算多个词的中心向量</h3>
<p>我们可以计算「宇宙」、「飞船」、「战争」的宇宙语义向量（中心向量）。 并试图寻找中心向量 <em><strong>universe_vector</strong></em> 的最相似的10个词。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 几个词语构建的宇宙语义向量</span>
<span class="n">universe_vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_centroid</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">dm_w2v</span><span class="p">,</span> 
                                       <span class="n">words</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;宇宙&#39;</span><span class="p">,</span> <span class="s1">&#39;飞船&#39;</span><span class="p">,</span> <span class="s1">&#39;战争&#39;</span><span class="p">])</span>


<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">universe_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;宇宙&#39;, 0.7568532228469849),
 (&#39;星系&#39;, 0.7090039253234863),
 (&#39;飞船&#39;, 0.7080673575401306),
 (&#39;人类文明&#39;, 0.6973789930343628),
 (&#39;战舰&#39;, 0.6890057325363159),
 (&#39;母舰&#39;, 0.6864359974861145),
 (&#39;星球&#39;, 0.6799622774124146),
 (&#39;卫星&#39;, 0.6799139976501465),
 (&#39;星际&#39;, 0.6789332032203674),
 (&#39;空间站&#39;, 0.6780815124511719),
 (&#39;地球&#39;, 0.6769616603851318),
 (&#39;外太空&#39;, 0.6683873534202576),
 (&#39;核战&#39;, 0.6669113039970398),
 (&#39;外星飞船&#39;, 0.6592534780502319),
 (&#39;木星&#39;, 0.6586896777153015),
 (&#39;能源&#39;, 0.6562989950180054),
 (&#39;战争&#39;, 0.6556441187858582),
 (&#39;巨兽&#39;, 0.6544537544250488),
 (&#39;月球&#39;, 0.6525537967681885),
 (&#39;一艘&#39;, 0.6521110534667969)]
</code></pre></div><p>语义捕捉的很准哦。</p>
<h3 id="410-概念轴">4.10 概念轴</h3>
<p>男性概念向量由多个男性词的向量加总求均值得到，女性概念向量算法类似。当性质或方向明显相反的两个概念向量相减， 得到的新的向量，我们可以称之为<em><strong>概念轴向量Concept Axis</strong></em>。常见的概念轴，</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 尺寸(大, 小)
- 湿度(干燥,潮湿)
- 性别(男, 女)
- 财富(富裕, 贫穷)
- 等
</code></pre></div><p>其实任意概念的向量也可看做概念轴，即该概念向量与0向量相减。只不过两组性质方向相反的方式得到的概念轴， 在语义上更稳定。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 定义词语列表</span>
<span class="n">phy_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;游泳&#39;</span><span class="p">,</span> <span class="s1">&#39;跑步&#39;</span><span class="p">,</span> <span class="s1">&#39;篮球&#39;</span><span class="p">,</span> <span class="s1">&#39;羽毛球&#39;</span><span class="p">,</span> <span class="s1">&#39;马拉松&#39;</span><span class="p">,</span> <span class="s1">&#39;马术&#39;</span><span class="p">,</span> <span class="s1">&#39;徒步&#39;</span><span class="p">]</span>

<span class="n">rich_words</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;富裕&#39;</span><span class="p">,</span> <span class="s1">&#39;财富&#39;</span><span class="p">,</span> <span class="s1">&#39;金钱&#39;</span><span class="p">,</span> <span class="s1">&#39;豪宅&#39;</span><span class="p">,</span> <span class="s1">&#39;豪车&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;奢侈品&#39;</span><span class="p">,</span> <span class="s1">&#39;投资&#39;</span><span class="p">,</span> <span class="s1">&#39;股票&#39;</span><span class="p">,</span> <span class="s1">&#39;基金&#39;</span><span class="p">,</span> <span class="s1">&#39;黄金&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;钻石&#39;</span><span class="p">,</span> <span class="s1">&#39;游艇&#39;</span><span class="p">,</span> <span class="s1">&#39;私人飞机&#39;</span><span class="p">,</span> <span class="s1">&#39;企业家&#39;</span><span class="p">,</span> <span class="s1">&#39;富豪&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;成功&#39;</span><span class="p">,</span> <span class="s1">&#39;繁荣&#39;</span><span class="p">,</span> <span class="s1">&#39;奢华&#39;</span><span class="p">,</span> <span class="s1">&#39;贵族&#39;</span><span class="p">,</span> <span class="s1">&#39;高收入&#39;</span>
<span class="p">]</span>

<span class="n">poor_words</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;贫穷&#39;</span><span class="p">,</span> <span class="s1">&#39;贫困&#39;</span><span class="p">,</span> <span class="s1">&#39;饥饿&#39;</span><span class="p">,</span> <span class="s1">&#39;失业&#39;</span><span class="p">,</span> <span class="s1">&#39;低收入&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;简陋&#39;</span><span class="p">,</span> <span class="s1">&#39;破旧&#39;</span><span class="p">,</span> <span class="s1">&#39;乞丐&#39;</span><span class="p">,</span> <span class="s1">&#39;流浪&#39;</span><span class="p">,</span> <span class="s1">&#39;欠债&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;破产&#39;</span><span class="p">,</span> <span class="s1">&#39;困境&#39;</span><span class="p">,</span> <span class="s1">&#39;艰难&#39;</span><span class="p">,</span> <span class="s1">&#39;挣扎&#39;</span><span class="p">,</span> <span class="s1">&#39;匮乏&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;落后&#39;</span><span class="p">,</span> <span class="s1">&#39;无助&#39;</span><span class="p">,</span> <span class="s1">&#39;绝望&#39;</span><span class="p">,</span> <span class="s1">&#39;赤贫&#39;</span><span class="p">,</span> <span class="s1">&#39;温饱&#39;</span>
<span class="p">]</span>

<span class="n">phy_project_on_fortune</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">sematic_projection</span><span class="p">(</span><span class="n">wv</span> <span class="o">=</span> <span class="n">dm_w2v</span><span class="p">,</span>
                                               <span class="n">words</span> <span class="o">=</span> <span class="n">phy_words</span><span class="p">,</span> 
                                               <span class="n">c_words1</span> <span class="o">=</span><span class="n">poor_words</span><span class="p">,</span>
                                               <span class="n">c_words2</span> <span class="o">=</span><span class="n">rich_words</span><span class="p">)</span>

<span class="n">phy_project_on_fortune</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;跑步&#39;, -1.82),
 (&#39;徒步&#39;, -0.82),
 (&#39;游泳&#39;, -0.19),
 (&#39;羽毛球&#39;, 0.57),
 (&#39;马拉松&#39;, 0.62),
 (&#39;马术&#39;, 1.15),
 (&#39;篮球&#39;, 4.0)]
</code></pre></div><p>计算结果中， 数值越大越接近于c_words2,  越小越接近于c_words1 。 可以看到在财富概念轴向量上的投影，  篮球不太准，但是其他几项基本上看出运动的贫富性。</p>
<br>
<h3 id="411-类比-king-man--woman--queen">4.11 类比 king-man + woman ~ queen</h3>
<p>每个词是高维向量空间中的一个点， 两个点可以组成有方向的向量，而向量可以比较方向。这里是推理过程，受限于数据，公式不一定完全成立， 但是思维可以类比。</p>
<p><img loading="lazy" src="img/king-queen-formular.png" alt=""  />
</p>
<p>这两个词相减，按感觉应该得到的是性别方向，雄性-&gt;雌性。</p>
<p><em><strong>gender_direction_1 = vector(man)-vector(woman)</strong></em></p>
<p><em><strong>gender_direction_2 = vector(king)-vector(queen)</strong></em></p>
<p>那两个性别方向应该近似，即 gender_direction_1 约等于 gender_direction_2 ，将其看做等式就得到如下公式：</p>
<p><em><strong>vector(理应近似queen) = vector(king)-vector(men)+vector(women)</strong></em></p>
<p>现在我们检查三个语义向量计算出的新的向量是否有与queen相关的语义信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">semactic_vector</span><span class="p">(</span><span class="n">wv</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">semantic_centroid</span><span class="p">(</span><span class="n">wv</span><span class="o">=</span><span class="n">wv</span><span class="p">,</span> 
                                  <span class="n">words</span><span class="o">=</span><span class="n">words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vector</span>


<span class="n">men_vector</span> <span class="o">=</span> <span class="n">semactic_vector</span><span class="p">(</span><span class="n">dm_w2v</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;男&#39;</span><span class="p">,</span> <span class="s1">&#39;男孩&#39;</span><span class="p">,</span> <span class="s1">&#39;男人&#39;</span><span class="p">,</span> <span class="s1">&#39;他&#39;</span><span class="p">,</span> <span class="s1">&#39;父亲&#39;</span><span class="p">,</span> <span class="s1">&#39;爸爸&#39;</span><span class="p">,</span> <span class="s1">&#39;爷爷&#39;</span><span class="p">])</span>
<span class="n">women_vector</span> <span class="o">=</span> <span class="n">semactic_vector</span><span class="p">(</span><span class="n">dm_w2v</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;女&#39;</span><span class="p">,</span> <span class="s1">&#39;女孩&#39;</span><span class="p">,</span> <span class="s1">&#39;女人&#39;</span><span class="p">,</span> <span class="s1">&#39;她&#39;</span><span class="p">,</span> <span class="s1">&#39;母亲&#39;</span><span class="p">,</span> <span class="s1">&#39;妈妈&#39;</span><span class="p">,</span> <span class="s1">&#39;奶奶&#39;</span><span class="p">])</span>
<span class="n">king_vector</span> <span class="o">=</span> <span class="n">semactic_vector</span><span class="p">(</span><span class="n">dm_w2v</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;国王&#39;</span><span class="p">,</span> <span class="s1">&#39;皇帝&#39;</span><span class="p">,</span> <span class="s1">&#39;帝王&#39;</span><span class="p">,</span> <span class="s1">&#39;大帝&#39;</span><span class="p">])</span>
<span class="c1"># 假设 king- queen 约等于 man -woman </span>
<span class="c1"># result 近似等于 king - queen + women</span>
<span class="n">result_vector</span> <span class="o">=</span> <span class="n">king_vector</span> <span class="o">-</span> <span class="n">men_vector</span> <span class="o">+</span> <span class="n">women_vector</span>
<span class="c1"># 现在检查 result_vector 的语义应该与queen相关</span>
<span class="n">dm_w2v</span><span class="o">.</span><span class="n">similar_by_vector</span><span class="p">(</span><span class="n">result_vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;皇帝&#39;, 0.8448051810264587),
 (&#39;王后&#39;, 0.8056979179382324),
 (&#39;国王&#39;, 0.8004385232925415),
 (&#39;帝王&#39;, 0.7693961262702942),
 (&#39;君主&#39;, 0.7663125991821289),
 (&#39;皇后&#39;, 0.7614380717277527),
 (&#39;太后&#39;, 0.7463700175285339),
 (&#39;妃子&#39;, 0.7433678507804871),
 (&#39;君王&#39;, 0.7407413125038147),
 (&#39;皇子&#39;, 0.7380139231681824),
 (&#39;王位&#39;, 0.7319545745849609),
 (&#39;皇上&#39;, 0.7215542197227478),
 (&#39;登基&#39;, 0.7210745215415955),
 (&#39;大臣&#39;, 0.714862048625946),
 (&#39;伊丽莎白一世&#39;, 0.702217698097229),
 (&#39;王朝&#39;, 0.7000151872634888),
 (&#39;宫女&#39;, 0.6997070908546448),
 (&#39;驾崩&#39;, 0.6992778182029724),
 (&#39;王妃&#39;, 0.6981185078620911),
 (&#39;昏君&#39;, 0.6974363923072815)]
</code></pre></div><p>可以看到三个语义向量四则运算出的 result_vector 与queen仍具有较高的相关性。</p>
<p><br><br></p>
<h2 id="五获取资料">五、获取资料</h2>
<p>除了本文介绍的这个 1000w 条影评数据集， 大邓还有2个类似的豆瓣影评数据集，影评记录量 212w和442 w 条。 两个数据集下载链接我都公开，感兴趣的可以都下载下来。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 免费  douba-movie-1000w 链接: https://pan.baidu.com/s/15C0fn7oyYEFvuQtPO8tw8Q?pwd=1g7m 提取码: 1g7m
- 免费 douban-movie-1000w-Word2Vec.200.15.bin
链接: https://pan.baidu.com/s/1fK8LhLmK4_xq-eHzNn42lg?pwd=2hwr 提取码: 2hwr
- 免费 douban-movie-442w 链接: https://pan.baidu.com/s/1T_LPuxEZ_W8xfYcxV7rW5Q?pwd=a683 提取码: a683
- 免费 douban-movie-212w 链接: :https://pan.baidu.com/s/1VBwnOqfMPu_Y48bMlQ4oiw?pwd=t8id 
 提取码: t8id 

- 100元  cntext-2.1.5-py3-none-any.whl   如有需要，加微信 ***372335839***， 备注「姓名-学校-专业」
</code></pre></div><p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://textdata.cn/blog/2024-04-17-douban-book-3394w-ratings-comments-dataset/">数据集 | 3394w条豆瓣书评数据集</a></li>
<li><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用Stanford Glove代码训练中文语料的GloVe模型</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-visualize-the-culture-change-using-people-daily-dataset/">可视化 | 人民日报语料反映七十年文化演变</a></li>
<li><a href="https://textdata.cn/blog/2023-03-24-load-w2v-and-expand-your-concpet-dicitonary/">词向量 | 使用MD&amp;A2001-2023语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用Stanford Glove代码训练中文语料的Glove模型</a></li>
</ul>
<br>]]></content:encoded>
    </item>
    
    <item>
      <title>词向量 | 使用1亿B站用户签名训练word2vec词向量</title>
      <link>https://textdata.cn/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/blog/2023-11-12-using-100m-bilibili-user-sign-data-to-training-word2vec/</guid>
      <description>&lt;h2 id=&#34;一用户签名&#34;&gt;一、用户签名&lt;/h2&gt;
&lt;p&gt;1亿B站用户群体十分庞大，文本中蕴含着这个群体的认知信息(如兴趣、身份、座右铭等)，如果能用签名训练word2vec词向量模型，说不定就有利用这个模型，对每个用户签名进行量化,  对用户进行分类。 本文要解决&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;构建语料训练出模型&lt;/li&gt;
&lt;li&gt;简单看看模型训练效果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;二准备语料&#34;&gt;二、准备语料&lt;/h2&gt;
&lt;p&gt;Kaggle网有1亿B站用户数据集，下载地址&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/datasets/beats0/bilibili-user&#34;&gt;https://www.kaggle.com/datasets/beats0/bilibili-user&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;之前分享过 &lt;a href=&#34;https://textdata.cn/blog/2023-05-10-100m-bilibili-user-info-dataset/&#34;&gt;数据集 | 哔哩哔哩 1 亿用户数据&lt;/a&gt; ， 阅读此文可以熟悉pandas的一些基本操作，如数据读取、文本操作等。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 从kaggle下载B站1亿用户数据&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# 查看前5行&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;User.csv&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nrows&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;img/df2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;将 9093092 个非空签名汇总到 &lt;em&gt;&lt;strong&gt;B站用户签名语料.txt&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;B站签名.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;raw_text&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;sign&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fillna&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;代码运行后，得到 320M  的 &lt;em&gt;&lt;strong&gt;B站签名.txt&lt;/strong&gt;&lt;/em&gt;  。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;三训练-word2vec&#34;&gt;三、训练 Word2Vec&lt;/h2&gt;
&lt;p&gt;我使用的自己 &lt;strong&gt;未公开&lt;/strong&gt; 的cntext 2.1.5版本， Bug频出，等调整好了再公开。&lt;/p&gt;
&lt;h3 id=&#34;31-安装cntext&#34;&gt;3.1 安装cntext&lt;/h3&gt;
&lt;p&gt;将 cntext-2.1.5-py3-none-any.whl 放置于桌面，打开 &lt;strong&gt;cmd&lt;/strong&gt;  (苹果电脑打开terminal)， 输入cd desktop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;cd desktop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;之后在 &lt;strong&gt;cmd&lt;/strong&gt;  (苹果电脑打开terminal) 中使用pip3 安装&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;pip3 install cntext-2.1.5-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文末有 ***cntext-2.1.5-py3-none-any.whl ***获取方式&lt;/p&gt;
&lt;br&gt;
&lt;h3 id=&#34;32-训练word2vec&#34;&gt;3.2 训练word2vec&lt;/h3&gt;
&lt;p&gt;cntext 训练时候Word2Vec模型参数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;corpus_file&lt;/strong&gt;&lt;/em&gt; 语料txt文件路径； 刚刚准备的 &lt;em&gt;&lt;strong&gt;B站用户签名语料.txt&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;window_size&lt;/strong&gt;&lt;/em&gt;   上下文窗口大小(上下文语义)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;vector_size&lt;/strong&gt;&lt;/em&gt; 向量维度数&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;chunksize&lt;/strong&gt;&lt;/em&gt;  每次语料txt文件中读取的行数&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;lang&lt;/strong&gt;&lt;/em&gt; 语言的语言&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# cntext2.1.5未公开，获取2.1.5请阅读文末获取方式&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Word2Vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus_file&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;B站用户签名语料.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;n&#34;&gt;window&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;n&#34;&gt;vector_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;200&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
                    &lt;span class=&#34;n&#34;&gt;window_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;
                    &lt;span class=&#34;n&#34;&gt;lang&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;chinese&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;Mac(Linux) System, Enable Parallel Processing
Cache output/B站签名_cache.txt Not Found or Empty, Preprocessing Corpus

Reading Preprocessed Corpus from output/B站签名_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 275 s. 
Output Saved To: output/B站签名-Word2Vec.200.15.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;耗时 275s， 模型训练完成！需要注意， output文件夹内有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;B站签名-Word2Vec.200.15.txt&lt;/strong&gt;&lt;/em&gt;        模型文件&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;B站签名-Word2Vec.200.15.bin&lt;/strong&gt;&lt;/em&gt;        模型文件&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;B站签名_cache.txt&lt;/strong&gt;&lt;/em&gt;                  训练缓存文件&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;txt 和 bin 内容方面完全相同，只是格式不同。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;四使用word2vec&#34;&gt;四、使用word2vec&lt;/h2&gt;
&lt;h3 id=&#34;41-读取模型&#34;&gt;4.1 读取模型&lt;/h3&gt;
&lt;p&gt;使用gensim录入模型 &lt;em&gt;&lt;strong&gt;B站用户签名语料-Word2Vec.100.15.txt&lt;/strong&gt;&lt;/em&gt; ,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;gensim.models&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KeyedVectors&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;cntext&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;ct&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ct&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;output/B站签名-Word2Vec.200.15.txt&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;# w2v = ct.load_w2v(&amp;#39;output/B站签名-Word2Vec.200.15.bin&amp;#39;)&lt;/span&gt;

&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;模型词汇量: &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;模型词汇量:  244491
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;42-查询某词的词向量&#34;&gt;4.2 查询某词的词向量&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;w2v[&amp;#39;高冷&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;array([ 1.05914783e+00,  4.51383203e-01, -1.34764791e+00, -9.42894161e-01,
        5.28594255e-01,  8.05936933e-01, -1.59555584e-01,  2.42719814e-01,
       -6.04722261e-01, -9.25606042e-02,  9.69056904e-01,  8.85407850e-02,
       -1.67851341e+00,  3.26303959e-01,  6.52321458e-01,  5.77043407e-02,
       -4.24268842e-02, -2.64299393e-01,  5.24512887e-01,  2.15208486e-01,
       -2.09263057e-01, -4.55661058e-01,  8.78976703e-01, -1.24363959e+00,
       -1.71196852e-02, -9.03965294e-01, -6.52690083e-02,  2.47650072e-02,
       -2.82155067e-01,  9.09134224e-02,  9.13890541e-01, -1.40862179e+00,
       -1.31956196e+00, -5.29659569e-01,  1.23605825e-01, -4.00647372e-01,
        4.94630456e-01,  2.81695575e-01,  1.71391249e-01,  1.23341233e-01,
       -7.70617545e-01,  5.81079908e-02, -4.89788234e-01,  2.14924827e-01,
       -7.73121595e-01, -6.66803181e-01, -1.31617844e+00,  1.18301921e-01,
        6.22543573e-01, -8.07524860e-01, -4.36694354e-01,  2.95946062e-01,
        3.10503364e-01, -4.93252903e-01,  1.27962172e-01,  1.97043195e-01,
        6.61175609e-01, -1.80842638e-01,  1.13270843e+00, -5.34760773e-01,
        9.13145125e-01,  5.48191011e-01,  7.68198539e-03,  1.17955339e+00,
       -1.96015276e-02,  9.14144278e-01, -9.06695664e-01,  4.39731702e-02,
       -3.87832075e-01,  4.72544342e-01,  4.95476156e-01, -1.21628530e-01,
       -4.41256445e-03,  1.82375580e-01, -7.00045705e-01,  4.34259921e-01,
        2.00862193e+00, -5.61490715e-01, -7.67120644e-02,  5.78972995e-01,
       -7.80492842e-01, -5.01321375e-01, -5.50926566e-01, -8.99926543e-01,
       -1.66289490e-02,  1.77679747e-01,  4.23889339e-01,  1.40111005e+00,
       -7.63866380e-02, -8.86032939e-01, -1.08106744e+00,  3.31989765e-01,
        3.78885448e-01, -1.23718023e+00,  2.09680721e-01,  2.39727721e-01,
        2.46049106e-01,  2.32866824e-01, -6.65583909e-02,  1.09542537e+00,
       -5.44713318e-01,  7.68220305e-01, -1.56612769e-02,  3.48719925e-01,
        2.91741371e-01,  1.88722059e-01, -2.12467611e-01,  8.20825279e-01,
       -1.74725935e-01, -8.05535197e-01, -1.41250715e-01, -7.84179568e-01,
       -8.00660312e-01, -1.12991728e-01, -2.16052849e-02, -1.07448053e+00,
        2.53552765e-01, -1.28611282e-01, -1.16868567e+00, -6.08788371e-01,
        4.30017859e-02, -5.11076570e-01,  6.43583059e-01,  3.11966389e-01,
       -1.63116843e-01,  3.58751595e-01,  5.16831456e-03,  5.09353161e-01,
        1.61675465e+00,  6.42039478e-01, -1.07160270e+00, -2.34255135e-01,
       -7.27983773e-01,  1.20267116e-01, -1.11912894e+00,  1.49096262e+00,
       -1.48015752e-01,  6.85670376e-02, -1.70197403e+00,  2.16349974e-01,
        1.32302952e+00,  5.39037228e-01, -8.35760951e-01, -7.43441284e-01,
        6.55625939e-01, -5.07541537e-01, -5.40877655e-02, -5.38533449e-01,
       -2.57937461e-01,  8.67499232e-01, -6.53150141e-01, -1.32043970e+00,
       -5.84588587e-01,  1.24599323e-01, -8.35753500e-01, -2.68954426e-01,
        3.67542468e-02,  1.61010170e+00,  7.27127492e-01,  1.35515738e+00,
       -2.76694775e-01,  2.69006938e-01,  4.81265247e-01, -6.30314708e-01,
       -3.66074532e-01,  3.03934813e-01,  1.92417920e+00,  4.67498928e-01,
       -1.83004290e-01,  1.01947844e+00, -5.52489638e-01,  1.59275869e-03,
        4.84914184e-01,  1.33545566e+00, -9.75372076e-01,  2.25273356e-01,
        6.02540433e-01,  7.07564950e-01,  1.36330187e-01, -4.34346311e-02,
        4.53452200e-01,  1.58401883e+00, -6.68083191e-01, -1.30876124e+00,
       -1.19713686e-01, -9.80615169e-02, -2.04207993e+00,  8.29822361e-01,
       -4.08902228e-01, -4.70339246e-02,  1.00982547e+00,  1.64084151e-01,
        4.62104648e-01, -2.28677273e-01, -5.95047355e-01, -2.71069705e-01,
        6.27930462e-01, -8.85554433e-01, -1.79520398e-01, -3.44800770e-01],
      dtype=float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;h3 id=&#34;43-查看近义词&#34;&gt;4.3 查看近义词&lt;/h3&gt;
&lt;p&gt;通过给定词语，查看其近义词，可以了解模型训练的好坏。语义捕捉的合理，说明语料合理，模型训练的好。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 列表中可以传入任意多个词，这里大邓偷懒，都只传入了一两个词&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;女汉纸&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;汉纸&amp;#39;, 0.8672055602073669),
 (&amp;#39;腹黑&amp;#39;, 0.8662189841270447),
 (&amp;#39;文艺清新&amp;#39;, 0.849425733089447),
 (&amp;#39;闷骚&amp;#39;, 0.8427557945251465),
 (&amp;#39;神经大条&amp;#39;, 0.8329920768737793),
 (&amp;#39;汉子&amp;#39;, 0.8232208490371704),
 (&amp;#39;宅基&amp;#39;, 0.8224843144416809),
 (&amp;#39;猥琐大叔&amp;#39;, 0.8214939832687378),
 (&amp;#39;偶是&amp;#39;, 0.8164061307907104),
 (&amp;#39;腐宅&amp;#39;, 0.8117423057556152),
 (&amp;#39;宅女腐女&amp;#39;, 0.8073472380638123),
 (&amp;#39;软妹&amp;#39;, 0.7999386787414551),
 (&amp;#39;萌妹&amp;#39;, 0.7999064326286316),
 (&amp;#39;小女生&amp;#39;, 0.7998836040496826),
 (&amp;#39;天蝎女&amp;#39;, 0.7971166372299194),
 (&amp;#39;傲娇受&amp;#39;, 0.7964810132980347),
 (&amp;#39;天蝎&amp;#39;, 0.7957624197006226),
 (&amp;#39;天蝎座&amp;#39;, 0.7915034890174866),
 (&amp;#39;女纸&amp;#39;, 0.7912994623184204),
 (&amp;#39;双鱼座&amp;#39;, 0.7900263667106628)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;犯二&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;脱线&amp;#39;, 0.8355404734611511),
 (&amp;#39;神经质&amp;#39;, 0.8035165667533875),
 (&amp;#39;神经大条&amp;#39;, 0.7816897630691528),
 (&amp;#39;发神经&amp;#39;, 0.780509352684021),
 (&amp;#39;人来疯&amp;#39;, 0.7794896960258484),
 (&amp;#39;精分&amp;#39;, 0.7705598473548889),
 (&amp;#39;毒舌&amp;#39;, 0.7692195773124695),
 (&amp;#39;犯病&amp;#39;, 0.7659722566604614),
 (&amp;#39;闷骚&amp;#39;, 0.7620697617530823),
 (&amp;#39;迷糊&amp;#39;, 0.7608135342597961),
 (&amp;#39;智商在线&amp;#39;, 0.7525709867477417),
 (&amp;#39;抽疯&amp;#39;, 0.7491970658302307),
 (&amp;#39;欢脱&amp;#39;, 0.7444456219673157),
 (&amp;#39;深井&amp;#39;, 0.7416326403617859),
 (&amp;#39;抽风&amp;#39;, 0.7321327924728394),
 (&amp;#39;精分患者&amp;#39;, 0.7319940328598022),
 (&amp;#39;装嫩&amp;#39;, 0.7267141342163086),
 (&amp;#39;蒙圈&amp;#39;, 0.7262043952941895),
 (&amp;#39;神经&amp;#39;, 0.7257982492446899),
 (&amp;#39;假正经&amp;#39;, 0.7215201258659363)]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;w2v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;内向&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;[(&amp;#39;外向&amp;#39;, 0.8474423885345459),
 (&amp;#39;慢热&amp;#39;, 0.8272333741188049),
 (&amp;#39;不爱说话&amp;#39;, 0.8249834775924683),
 (&amp;#39;不善言辞&amp;#39;, 0.80635666847229),
 (&amp;#39;腼腆&amp;#39;, 0.7940059304237366),
 (&amp;#39;孤僻&amp;#39;, 0.7929618954658508),
 (&amp;#39;开朗&amp;#39;, 0.7585728168487549),
 (&amp;#39;闷骚&amp;#39;, 0.745791494846344),
 (&amp;#39;神经质&amp;#39;, 0.7454176545143127),
 (&amp;#39;多愁善感&amp;#39;, 0.7348753809928894),
 (&amp;#39;胆小&amp;#39;, 0.7213962078094482),
 (&amp;#39;沉默寡言&amp;#39;, 0.7145323157310486),
 (&amp;#39;随和&amp;#39;, 0.7115553617477417),
 (&amp;#39;敏感&amp;#39;, 0.7103193402290344),
 (&amp;#39;水瓶座&amp;#39;, 0.7092751264572144),
 (&amp;#39;大大咧咧&amp;#39;, 0.7085798382759094),
 (&amp;#39;高冷&amp;#39;, 0.7084994912147522),
 (&amp;#39;性格开朗&amp;#39;, 0.7064590454101562),
 (&amp;#39;耿直&amp;#39;, 0.7048951983451843),
 (&amp;#39;做作&amp;#39;, 0.704330325126648)]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;
&lt;br&gt;
&lt;h2 id=&#34;五获取资源&#34;&gt;五、获取资源&lt;/h2&gt;
&lt;p&gt;内容整理不易， 本文内容分免费和付费部分。 免费部分可以直接下载数据、构建语料、使用word2vec模型。&lt;/p&gt;
&lt;p&gt;付费部分主要是cntext，用于训练 word2vec 模型。 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;- 免费 1亿用户数据集 https://www.kaggle.com/datasets/beats0/bilibili-user

- 免费 B站签名-Word2Vec.200.15.bin  链接: https://pan.baidu.com/s/1ILVwu6gGGGP0IHv-vsjgvw?pwd=em99 提取码: em99 

- 100元   获得cntext-2.1.5-py3-none-any.whl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关内容&#34;&gt;相关内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cntext.readthedocs.io/&#34;&gt;文本分析库cntext2.x使用手册 https://cntext.readthedocs.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/&#34;&gt;实验 | 使用Stanford Glove代码训练中文语料的Glove模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/&#34;&gt;词向量 | 使用人民网领导留言板语料训练Word2Vec模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/&#34;&gt;使用 5000w 专利申请数据集按年份(按省份)训练词向量&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/&#34;&gt;使用 1000w 条豆瓣影评训练 Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/&#34;&gt;词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/&#34;&gt;转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/&#34;&gt;OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      <content:encoded><![CDATA[<h2 id="一用户签名">一、用户签名</h2>
<p>1亿B站用户群体十分庞大，文本中蕴含着这个群体的认知信息(如兴趣、身份、座右铭等)，如果能用签名训练word2vec词向量模型，说不定就有利用这个模型，对每个用户签名进行量化,  对用户进行分类。 本文要解决</p>
<ul>
<li>构建语料训练出模型</li>
<li>简单看看模型训练效果</li>
</ul>
<p><br><br></p>
<h2 id="二准备语料">二、准备语料</h2>
<p>Kaggle网有1亿B站用户数据集，下载地址</p>
<blockquote>
<p><a href="https://www.kaggle.com/datasets/beats0/bilibili-user">https://www.kaggle.com/datasets/beats0/bilibili-user</a></p>
</blockquote>
<p>之前分享过 <a href="https://textdata.cn/blog/2023-05-10-100m-bilibili-user-info-dataset/">数据集 | 哔哩哔哩 1 亿用户数据</a> ， 阅读此文可以熟悉pandas的一些基本操作，如数据读取、文本操作等。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 从kaggle下载B站1亿用户数据</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># 查看前5行</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;User.csv&#39;</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">df</span>
</code></pre></div><p>Run</p>
<p><img loading="lazy" src="img/df2.png" alt=""  />
</p>
<br>
<p>将 9093092 个非空签名汇总到 <em><strong>B站用户签名语料.txt</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;B站签名.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">raw_text</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;sign&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span>
</code></pre></div><p>代码运行后，得到 320M  的 <em><strong>B站签名.txt</strong></em>  。</p>
<p><br><br></p>
<h2 id="三训练-word2vec">三、训练 Word2Vec</h2>
<p>我使用的自己 <strong>未公开</strong> 的cntext 2.1.5版本， Bug频出，等调整好了再公开。</p>
<h3 id="31-安装cntext">3.1 安装cntext</h3>
<p>将 cntext-2.1.5-py3-none-any.whl 放置于桌面，打开 <strong>cmd</strong>  (苹果电脑打开terminal)， 输入cd desktop</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">cd desktop
</code></pre></div><p>之后在 <strong>cmd</strong>  (苹果电脑打开terminal) 中使用pip3 安装</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">pip3 install cntext-2.1.5-py3-none-any.whl
</code></pre></div><p>文末有 ***cntext-2.1.5-py3-none-any.whl ***获取方式</p>
<br>
<h3 id="32-训练word2vec">3.2 训练word2vec</h3>
<p>cntext 训练时候Word2Vec模型参数</p>
<ul>
<li><em><strong>corpus_file</strong></em> 语料txt文件路径； 刚刚准备的 <em><strong>B站用户签名语料.txt</strong></em></li>
<li><em><strong>window_size</strong></em>   上下文窗口大小(上下文语义)</li>
<li><em><strong>vector_size</strong></em> 向量维度数</li>
<li><em><strong>chunksize</strong></em>  每次语料txt文件中读取的行数</li>
<li><em><strong>lang</strong></em> 语言的语言</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># cntext2.1.5未公开，获取2.1.5请阅读文末获取方式</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus_file</span><span class="o">=</span><span class="s1">&#39;B站用户签名语料.txt&#39;</span><span class="p">,</span> 
                    <span class="n">window</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span> 
                    <span class="n">vector_size</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> 
                    <span class="n">window_size</span> <span class="o">=</span> <span class="mi">15</span>
                    <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;chinese&#39;</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">Mac(Linux) System, Enable Parallel Processing
Cache output/B站签名_cache.txt Not Found or Empty, Preprocessing Corpus

Reading Preprocessed Corpus from output/B站签名_cache.txt
Start Training Word2Vec
Word2Vec Training Cost 275 s. 
Output Saved To: output/B站签名-Word2Vec.200.15.txt
</code></pre></div><p>耗时 275s， 模型训练完成！需要注意， output文件夹内有</p>
<ul>
<li><em><strong>B站签名-Word2Vec.200.15.txt</strong></em>        模型文件</li>
<li><em><strong>B站签名-Word2Vec.200.15.bin</strong></em>        模型文件</li>
<li><em><strong>B站签名_cache.txt</strong></em>                  训练缓存文件</li>
</ul>
<p>txt 和 bin 内容方面完全相同，只是格式不同。</p>
<p><br><br></p>
<h2 id="四使用word2vec">四、使用word2vec</h2>
<h3 id="41-读取模型">4.1 读取模型</h3>
<p>使用gensim录入模型 <em><strong>B站用户签名语料-Word2Vec.100.15.txt</strong></em> ,</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>
<span class="kn">import</span> <span class="nn">cntext</span> <span class="k">as</span> <span class="nn">ct</span>

<span class="n">w2v</span> <span class="o">=</span> <span class="n">ct</span><span class="o">.</span><span class="n">load_w2v</span><span class="p">(</span><span class="s1">&#39;output/B站签名-Word2Vec.200.15.txt&#39;</span><span class="p">)</span>
<span class="c1"># w2v = ct.load_w2v(&#39;output/B站签名-Word2Vec.200.15.bin&#39;)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;模型词汇量: &#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w2v</span><span class="p">))</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">模型词汇量:  244491
</code></pre></div><br>
<h3 id="42-查询某词的词向量">4.2 查询某词的词向量</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">w2v[&#39;高冷&#39;]
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">array([ 1.05914783e+00,  4.51383203e-01, -1.34764791e+00, -9.42894161e-01,
        5.28594255e-01,  8.05936933e-01, -1.59555584e-01,  2.42719814e-01,
       -6.04722261e-01, -9.25606042e-02,  9.69056904e-01,  8.85407850e-02,
       -1.67851341e+00,  3.26303959e-01,  6.52321458e-01,  5.77043407e-02,
       -4.24268842e-02, -2.64299393e-01,  5.24512887e-01,  2.15208486e-01,
       -2.09263057e-01, -4.55661058e-01,  8.78976703e-01, -1.24363959e+00,
       -1.71196852e-02, -9.03965294e-01, -6.52690083e-02,  2.47650072e-02,
       -2.82155067e-01,  9.09134224e-02,  9.13890541e-01, -1.40862179e+00,
       -1.31956196e+00, -5.29659569e-01,  1.23605825e-01, -4.00647372e-01,
        4.94630456e-01,  2.81695575e-01,  1.71391249e-01,  1.23341233e-01,
       -7.70617545e-01,  5.81079908e-02, -4.89788234e-01,  2.14924827e-01,
       -7.73121595e-01, -6.66803181e-01, -1.31617844e+00,  1.18301921e-01,
        6.22543573e-01, -8.07524860e-01, -4.36694354e-01,  2.95946062e-01,
        3.10503364e-01, -4.93252903e-01,  1.27962172e-01,  1.97043195e-01,
        6.61175609e-01, -1.80842638e-01,  1.13270843e+00, -5.34760773e-01,
        9.13145125e-01,  5.48191011e-01,  7.68198539e-03,  1.17955339e+00,
       -1.96015276e-02,  9.14144278e-01, -9.06695664e-01,  4.39731702e-02,
       -3.87832075e-01,  4.72544342e-01,  4.95476156e-01, -1.21628530e-01,
       -4.41256445e-03,  1.82375580e-01, -7.00045705e-01,  4.34259921e-01,
        2.00862193e+00, -5.61490715e-01, -7.67120644e-02,  5.78972995e-01,
       -7.80492842e-01, -5.01321375e-01, -5.50926566e-01, -8.99926543e-01,
       -1.66289490e-02,  1.77679747e-01,  4.23889339e-01,  1.40111005e+00,
       -7.63866380e-02, -8.86032939e-01, -1.08106744e+00,  3.31989765e-01,
        3.78885448e-01, -1.23718023e+00,  2.09680721e-01,  2.39727721e-01,
        2.46049106e-01,  2.32866824e-01, -6.65583909e-02,  1.09542537e+00,
       -5.44713318e-01,  7.68220305e-01, -1.56612769e-02,  3.48719925e-01,
        2.91741371e-01,  1.88722059e-01, -2.12467611e-01,  8.20825279e-01,
       -1.74725935e-01, -8.05535197e-01, -1.41250715e-01, -7.84179568e-01,
       -8.00660312e-01, -1.12991728e-01, -2.16052849e-02, -1.07448053e+00,
        2.53552765e-01, -1.28611282e-01, -1.16868567e+00, -6.08788371e-01,
        4.30017859e-02, -5.11076570e-01,  6.43583059e-01,  3.11966389e-01,
       -1.63116843e-01,  3.58751595e-01,  5.16831456e-03,  5.09353161e-01,
        1.61675465e+00,  6.42039478e-01, -1.07160270e+00, -2.34255135e-01,
       -7.27983773e-01,  1.20267116e-01, -1.11912894e+00,  1.49096262e+00,
       -1.48015752e-01,  6.85670376e-02, -1.70197403e+00,  2.16349974e-01,
        1.32302952e+00,  5.39037228e-01, -8.35760951e-01, -7.43441284e-01,
        6.55625939e-01, -5.07541537e-01, -5.40877655e-02, -5.38533449e-01,
       -2.57937461e-01,  8.67499232e-01, -6.53150141e-01, -1.32043970e+00,
       -5.84588587e-01,  1.24599323e-01, -8.35753500e-01, -2.68954426e-01,
        3.67542468e-02,  1.61010170e+00,  7.27127492e-01,  1.35515738e+00,
       -2.76694775e-01,  2.69006938e-01,  4.81265247e-01, -6.30314708e-01,
       -3.66074532e-01,  3.03934813e-01,  1.92417920e+00,  4.67498928e-01,
       -1.83004290e-01,  1.01947844e+00, -5.52489638e-01,  1.59275869e-03,
        4.84914184e-01,  1.33545566e+00, -9.75372076e-01,  2.25273356e-01,
        6.02540433e-01,  7.07564950e-01,  1.36330187e-01, -4.34346311e-02,
        4.53452200e-01,  1.58401883e+00, -6.68083191e-01, -1.30876124e+00,
       -1.19713686e-01, -9.80615169e-02, -2.04207993e+00,  8.29822361e-01,
       -4.08902228e-01, -4.70339246e-02,  1.00982547e+00,  1.64084151e-01,
        4.62104648e-01, -2.28677273e-01, -5.95047355e-01, -2.71069705e-01,
        6.27930462e-01, -8.85554433e-01, -1.79520398e-01, -3.44800770e-01],
      dtype=float32)
</code></pre></div><br>
<h3 id="43-查看近义词">4.3 查看近义词</h3>
<p>通过给定词语，查看其近义词，可以了解模型训练的好坏。语义捕捉的合理，说明语料合理，模型训练的好。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 列表中可以传入任意多个词，这里大邓偷懒，都只传入了一两个词</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;女汉纸&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;汉纸&#39;, 0.8672055602073669),
 (&#39;腹黑&#39;, 0.8662189841270447),
 (&#39;文艺清新&#39;, 0.849425733089447),
 (&#39;闷骚&#39;, 0.8427557945251465),
 (&#39;神经大条&#39;, 0.8329920768737793),
 (&#39;汉子&#39;, 0.8232208490371704),
 (&#39;宅基&#39;, 0.8224843144416809),
 (&#39;猥琐大叔&#39;, 0.8214939832687378),
 (&#39;偶是&#39;, 0.8164061307907104),
 (&#39;腐宅&#39;, 0.8117423057556152),
 (&#39;宅女腐女&#39;, 0.8073472380638123),
 (&#39;软妹&#39;, 0.7999386787414551),
 (&#39;萌妹&#39;, 0.7999064326286316),
 (&#39;小女生&#39;, 0.7998836040496826),
 (&#39;天蝎女&#39;, 0.7971166372299194),
 (&#39;傲娇受&#39;, 0.7964810132980347),
 (&#39;天蝎&#39;, 0.7957624197006226),
 (&#39;天蝎座&#39;, 0.7915034890174866),
 (&#39;女纸&#39;, 0.7912994623184204),
 (&#39;双鱼座&#39;, 0.7900263667106628)]
</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;犯二&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;脱线&#39;, 0.8355404734611511),
 (&#39;神经质&#39;, 0.8035165667533875),
 (&#39;神经大条&#39;, 0.7816897630691528),
 (&#39;发神经&#39;, 0.780509352684021),
 (&#39;人来疯&#39;, 0.7794896960258484),
 (&#39;精分&#39;, 0.7705598473548889),
 (&#39;毒舌&#39;, 0.7692195773124695),
 (&#39;犯病&#39;, 0.7659722566604614),
 (&#39;闷骚&#39;, 0.7620697617530823),
 (&#39;迷糊&#39;, 0.7608135342597961),
 (&#39;智商在线&#39;, 0.7525709867477417),
 (&#39;抽疯&#39;, 0.7491970658302307),
 (&#39;欢脱&#39;, 0.7444456219673157),
 (&#39;深井&#39;, 0.7416326403617859),
 (&#39;抽风&#39;, 0.7321327924728394),
 (&#39;精分患者&#39;, 0.7319940328598022),
 (&#39;装嫩&#39;, 0.7267141342163086),
 (&#39;蒙圈&#39;, 0.7262043952941895),
 (&#39;神经&#39;, 0.7257982492446899),
 (&#39;假正经&#39;, 0.7215201258659363)]

</code></pre></div><br>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">([</span><span class="s1">&#39;内向&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><p>Run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">[(&#39;外向&#39;, 0.8474423885345459),
 (&#39;慢热&#39;, 0.8272333741188049),
 (&#39;不爱说话&#39;, 0.8249834775924683),
 (&#39;不善言辞&#39;, 0.80635666847229),
 (&#39;腼腆&#39;, 0.7940059304237366),
 (&#39;孤僻&#39;, 0.7929618954658508),
 (&#39;开朗&#39;, 0.7585728168487549),
 (&#39;闷骚&#39;, 0.745791494846344),
 (&#39;神经质&#39;, 0.7454176545143127),
 (&#39;多愁善感&#39;, 0.7348753809928894),
 (&#39;胆小&#39;, 0.7213962078094482),
 (&#39;沉默寡言&#39;, 0.7145323157310486),
 (&#39;随和&#39;, 0.7115553617477417),
 (&#39;敏感&#39;, 0.7103193402290344),
 (&#39;水瓶座&#39;, 0.7092751264572144),
 (&#39;大大咧咧&#39;, 0.7085798382759094),
 (&#39;高冷&#39;, 0.7084994912147522),
 (&#39;性格开朗&#39;, 0.7064590454101562),
 (&#39;耿直&#39;, 0.7048951983451843),
 (&#39;做作&#39;, 0.704330325126648)]
</code></pre></div><br>
<br>
<h2 id="五获取资源">五、获取资源</h2>
<p>内容整理不易， 本文内容分免费和付费部分。 免费部分可以直接下载数据、构建语料、使用word2vec模型。</p>
<p>付费部分主要是cntext，用于训练 word2vec 模型。 如果对本文感兴趣，可加微信 372335839， 备注「姓名-学校-专业」</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">- 免费 1亿用户数据集 https://www.kaggle.com/datasets/beats0/bilibili-user

- 免费 B站签名-Word2Vec.200.15.bin  链接: https://pan.baidu.com/s/1ILVwu6gGGGP0IHv-vsjgvw?pwd=em99 提取码: em99 

- 100元   获得cntext-2.1.5-py3-none-any.whl
</code></pre></div><p><br><br></p>
<h2 id="相关内容">相关内容</h2>
<ul>
<li><a href="https://cntext.readthedocs.io/">文本分析库cntext2.x使用手册 https://cntext.readthedocs.io/</a></li>
<li><a href="https://textdata.cn/blog/2025-03-28-train_a_glove_model_on_chinese_corpus_using_stanfordnlp/">实验 | 使用Stanford Glove代码训练中文语料的Glove模型</a></li>
<li><a href="https://textdata.cn/blog/2023-12-28-train-word2vec-using-renmin-gov-leader-board-dataset/">词向量 | 使用人民网领导留言板语料训练Word2Vec模型</a></li>
<li><a href="https://textdata.cn/blog/2023-11-20-word2vec-by-year-by-province/">使用 5000w 专利申请数据集按年份(按省份)训练词向量</a></li>
<li><a href="https://textdata.cn/blog/2024-04-16-douban-movie-1000w-ratings-comments-dataset/">使用 1000w 条豆瓣影评训练 Word2Vec</a></li>
<li><a href="https://textdata.cn/blog/2023-03-15-39faq-about-word-embeddings-for-social-science/">词嵌入技术在社会科学领域进行数据挖掘常见39个FAQ汇总</a></li>
<li><a href="https://textdata.cn/blog/2022-04-07-word-embeddings-in-social-science/">转载|大数据时代下社会科学研究方法的拓展——基于词嵌入技术的文本分析的应用</a></li>
<li><a href="https://textdata.cn/blog/2023-11-03-organization-science-with-word-embeddings/">OS2022 | 概念空间 | 词嵌入模型如何为组织科学中的测量和理论提供信息</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
